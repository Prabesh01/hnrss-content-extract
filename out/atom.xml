<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-06T06:49:09.318870+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45812756</id><title>An eBPF Loophole: Using XDP for Egress Traffic</title><updated>2025-11-06T06:49:20.305885+00:00</updated><content>&lt;doc fingerprint="27b14038c8213ec6"&gt;
  &lt;main&gt;
    &lt;p&gt;On this page:&lt;/p&gt;
    &lt;head rend="h4"&gt;TL;DR:&lt;/head&gt;
    &lt;p&gt;XDP (eXpress Data Path) is the fastest packet processing framework in linux - but it only works for incoming (ingress) traffic. We discovered how to use it for outgoing (egress) traffic by exploiting a loophole in how the linux kernel determines packet direction. Our technique delivers 10x better performance than current solutions, works with existing Docker/Kubernetes containers, and requires zero kernel modifications.&lt;/p&gt;
    &lt;p&gt;This post not only expands on the overall implementation but also outlines how existing container and VM workloads can immediately take advantage with minimal effort and zero infrastructure changes.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Line-Rate Problem&lt;/head&gt;
    &lt;p&gt;At Loophole Labs, we live migrate everything - containers, VMs, and even network connections.&lt;/p&gt;
    &lt;p&gt;During a migration every single packet for a workload needs to be intercepted, modified, encapsulated, encrypted, and rerouted to its new destination - all without the application noticing. Our scale requires us to be able to move workloads across clouds at hundreds of gigabits per second - and with that sort of performance requirement, every single CPU cycle matters.&lt;/p&gt;
    &lt;p&gt;All of this is to say, we need to be able to process packets at line-rate (however much the underlying network can support, whether that's 20Gbps or 200Gbps), and there's really only one approach that lets us do that:&lt;/p&gt;
    &lt;p&gt;Linux Packet Processing Performance Comparison&lt;/p&gt;
    &lt;p&gt;In Linux, the gold standard for high-performance packet processing is XDP (eXpress Data Path). By intercepting packets as soon as they arrive at the network driver (before reaching the kernel) XDP is able to achieve line-rate speeds in most environments.&lt;/p&gt;
    &lt;p&gt;Our own benchmarks above show how easily we were able to reach line-rate with XDP, not to mention the fact that major companies like Meta, Cloudflare, and GCore have already been using it for more than 5 years now to handle 10s of millions of packets per second.&lt;/p&gt;
    &lt;head rend="h2"&gt;XDP's Main Limitation&lt;/head&gt;
    &lt;p&gt;Unfortunately XDP has one fundamental flaw that everyone accepts as fact: it only works for ingress (incoming) traffic. This isn't a bug or an oversight - it's the entire identity of XDP, one of the main characteristics that define it. XDP only processes packets on ingress. Period.&lt;/p&gt;
    &lt;p&gt;For routers and load balancers, this limitation is perfectly fine: every packet they handle arrives from an external interface, making it all ingress from the kernel's perspective.&lt;/p&gt;
    &lt;p&gt;Our network plane, on the other hand, has to run on the same compute nodes as the workloads that we're live migrating. And when these workloads generate packets - initiating connections, sending responses, etc. - that's considered egress traffic by the host kernel. XDP simply does not work in this scenario.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Traffic Control Isn't Good Enough&lt;/head&gt;
    &lt;p&gt;A popular method for handling egress packets is Traffic Control (TC), another eBPF-based mechanism that allows for packet processing at both ingress and egress. TC is already commonly used for traffic shaping, queuing, filtering, and policing outbound traffic. In fact, it's the de facto standard in the Kubernetes ecosystem - CNIs like Cilium and Calico all rely on TC for egress control because, until now, XDP for egress simply wasn't possible.&lt;/p&gt;
    &lt;p&gt;Given all of this, TC might seem like an obvious choice for our use case as well, but it has a fundamental flaw of its own:&lt;/p&gt;
    &lt;p&gt;Performance.&lt;/p&gt;
    &lt;p&gt;We haven't been able to process more than 21Gbps with TC on egress (or more than 23Gbps on ingress),which makes it a non-starter for our needs. The reason why TC suffers from this performance bottleneck is due to how (and more importantly when) the linux kernel runs the TC program:&lt;/p&gt;
    &lt;p&gt;TC Program Flow Diagram&lt;/p&gt;
    &lt;p&gt;As shown in the diagram above, TC programs operate quite late in the networking stack, after packets have already spent some time travelling through the linux kernel. By the time a packet reaches the TC hook, the kernel has already processed it through various subsystems for routing, firewalling, and even connection tracking. This means that we've wasted quite a few CPU cycles before our TC program even runs.&lt;/p&gt;
    &lt;p&gt;Another major limitation of TC is that it works on socket buffers (called &lt;code&gt;struct sk_buff&lt;/code&gt; in the
linux kernel) which are allocated per-packet. This structure -
while necessary for linux's own packet handling - comes with a significant performance hit due to the allocations
themselves as well as the additional memory copies required to populate it. This all becomes doubly problematic when
you're trying to process millions of packets every second.&lt;/p&gt;
    &lt;p&gt;XDP, on the other hand, not only operates directly on the raw packet memory (because it runs directly in the network drivers before the packet even reaches the linux kernel) but does so at the earliest point in the packet's lifecycle, meaning almost no CPU cycles have been spent by the time our XDP program starts running. All this results in zero-copy packet processing, meaning packets can be inspected, modified, and redirected with the absolute minimum overhead possible.&lt;/p&gt;
    &lt;p&gt;For us XDP is a hard requirement, and while the industry seems to have accepted that this is impossible, we haven't.&lt;/p&gt;
    &lt;head rend="h2"&gt;When Does an XDP Program Run?&lt;/head&gt;
    &lt;p&gt;One of our core beliefs at Loophole Labs is that every so-called "limitation" imposed by modern infrastructure is really just a problem we haven't solved yet. In the spirit of this, we decided to go digging through the linux kernel source in an attempt to understand exactly why and how the kernel decides to classify a packet as "ingress" in the first place.&lt;/p&gt;
    &lt;p&gt;As it turns out, linux doesn't actually classify the packet at all. When a packet arrives at a physical network interface, the network card writes the contents into an RX ring buffer - a memory region allocated by the device driver that the network card can write to directly via DMA (Direct Memory Access).&lt;/p&gt;
    &lt;p&gt;Next, the network card uses an interrupt to signal the device driver that there's a packet available for processing. The device driver then copies the packet from the ring buffer into its RX queue. And this is exactly when the XDP program runs: directly on the packet in the RX queue. This process is illustrated in the diagram below:&lt;/p&gt;
    &lt;p&gt;XDP Program Flow Diagram&lt;/p&gt;
    &lt;p&gt;If this entire process makes one thing clear, it's that there is very little work being done in between the packet arriving at the physical interface and it being ready for the XDP program to run. The RX queue is the trigger that tells the linux kernel how to "classify" the packet as ingress and whether it should run the XDP hook.&lt;/p&gt;
    &lt;p&gt;As we saw in this diagram, the RX queue is not used at all for egress packets, and this simple limitation is the cause of all our headaches.&lt;/p&gt;
    &lt;p&gt;Now that we know all this, how can we get around it? As it turns out, we don't have to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Egress Look Like Ingress&lt;/head&gt;
    &lt;p&gt;We were reading through the various linux interface docs, hoping to find some little insight into our predicament, when an interesting virtual interface caught our eye: Virtual Ethernet.&lt;/p&gt;
    &lt;p&gt;A Virtual Ethernet (&lt;code&gt;veth&lt;/code&gt;) interface is a pair of network interfaces that act as a direct tunnel between each
other. When a packet is transmitted from the TX queue of one side of the &lt;code&gt;veth&lt;/code&gt; pair, a pointer to the packet's memory is
simply moved to the RX queue of the other interface. This makes the packet appear as if it were received by a physical
network interface with very low overhead.&lt;/p&gt;
    &lt;p&gt;Yep, you read that right - &lt;code&gt;veth&lt;/code&gt; interfaces have an RX queue that's used when receiving a packet from the other side.&lt;/p&gt;
    &lt;p&gt;To illustrate this better, let's take an example setup like the one below. We have two applications running in their own network namespaces, with two &lt;code&gt;veth&lt;/code&gt; pairs (&lt;code&gt;veth0-A&lt;/code&gt; and &lt;code&gt;veth0-B&lt;/code&gt;) being used to route traffic out of the namespaces.&lt;/p&gt;
    &lt;p&gt;XDP for Egress Traffic Flow Diagram&lt;/p&gt;
    &lt;p&gt;The key insight here is that if we send outgoing traffic through one end of the &lt;code&gt;veth&lt;/code&gt; pairs (&lt;code&gt;veth0-A&lt;/code&gt; in the diagram
above), then from the perspective of the second interface (&lt;code&gt;veth1-A&lt;/code&gt;), the packet arrives at the
RX queue of an interface, and is now considered ingress traffic. And, since XDP programs can be attached to any
interface‚Äôs RX queue, our XDP hook will automatically run on that egress packet.&lt;/p&gt;
    &lt;p&gt;Furthermore, if we run our XDP programs in native mode like in the diagram above, packets can be processed with zero-copy and will bypass the linux kernel entirely when we use &lt;code&gt;XDP_REDIRECT&lt;/code&gt; to route directly to the TX queue of the
&lt;code&gt;eth0&lt;/code&gt; interface.&lt;/p&gt;
    &lt;p&gt;What makes this discovery even more powerful is that modern container runtimes - Docker, Kubernetes, containerd - already use &lt;code&gt;veth&lt;/code&gt; pairs and network namespaces for container networking. Every container you're running right now is
already connected through &lt;code&gt;veth&lt;/code&gt; interfaces, and it looks exactly like the diagram above.&lt;/p&gt;
    &lt;p&gt;That's right - not only can we use XDP for egress traffic in any of these environments, but we can do it without having to change them in any way.&lt;/p&gt;
    &lt;head rend="h2"&gt;If You Skip the Kernel, You Have to be the Kernel&lt;/head&gt;
    &lt;p&gt;Unfortunately, while implementing this technique seemed straightforward at first, we quickly hit a snag while benchmarking. Our packets kept getting dropped after our XDP program ran, and at first we couldn't figure out why.&lt;/p&gt;
    &lt;p&gt;We decided to run &lt;code&gt;tcpdump&lt;/code&gt; on the receiving host and realized the packets weren't even making it over the network.
Next, we decided to run &lt;code&gt;tcpdump&lt;/code&gt; on the switch handling the packets, and that's when we realized what we'd
missed.&lt;/p&gt;
    &lt;p&gt;As it turns out, when you bypass the kernel's networking stack, you inherit its responsibilities.&lt;/p&gt;
    &lt;p&gt;Normally, when a packet is sent out via the linux kernel, it handles the routing, checksumming, and ARP resolution for us. But we of course have bypassed the kernel's networking stack entirely, meaning now we have to take full responsibility for ensuring packets are properly formed and can actually reach their next hop.&lt;/p&gt;
    &lt;p&gt;Our network plane already handles proper routing for us, but we'd missed both checksumming and ARP resolution.&lt;/p&gt;
    &lt;head rend="h3"&gt;Checksum Calculations in XDP&lt;/head&gt;
    &lt;p&gt;XDP programs unfortunately are not provided with the same checksum helpers that TC programs get. For NAT (Network Address Translation) or any other packet header modifications, you need to recalculate checksums manually - and when performance matters, the trick is to use incremental checksum updates rather than full recalculations:&lt;/p&gt;
    &lt;head rend="h3"&gt;ARP Resolution&lt;/head&gt;
    &lt;p&gt;The linux kernel normally handles ARP to resolve IP addresses to MAC addresses and automatically sets the destination MAC address in the &lt;code&gt;ethernet&lt;/code&gt; layer of the outgoing packet. With XDP however, we need to maintain our own ARP table
and pass in the destination MAC ourselves:&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;To validate our overall approach, we set up iPerf3 containers between two 200Gbps-capable EC2 instances in the same AWS VPC. We purposely reduced the MTU to 1500 since traffic to the public internet generally can't use jumbo frames in the first place.&lt;/p&gt;
    &lt;p&gt;We used the exact same container networking setup for all three tests - the same standard network namespaces with &lt;code&gt;veth&lt;/code&gt; pairs that Docker and every other container runtime uses by default. The only thing we changed was how packets
were routed from the container's &lt;code&gt;veth&lt;/code&gt; interface to the host's physical interface:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;iptables: The default that everyone uses today - &lt;code&gt;PREROUTING&lt;/code&gt;chains to move traffic out of the namespace&lt;/item&gt;
      &lt;item&gt;Traffic Control: Using a TC egress program on the &lt;code&gt;veth&lt;/code&gt;interfaces&lt;/item&gt;
      &lt;item&gt;XDP: Our technique - Using an XDP program attached to the &lt;code&gt;veth&lt;/code&gt;interfaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We also decided to benchmark both the generic and native XDP drivers implemented for &lt;code&gt;veth&lt;/code&gt; interfaces.&lt;/p&gt;
    &lt;p&gt;We'll let the results speak for themselves:&lt;/p&gt;
    &lt;p&gt;iPerf3 Benchmark With Various Routing Strategies&lt;/p&gt;
    &lt;p&gt;The first two results are exactly what we expect - iptables introduces the most overhead because it routes through the linux kernel, and traffic control performs better but still operates post-socket-buffer and can't come close to achieving line-rate.&lt;/p&gt;
    &lt;p&gt;With the &lt;code&gt;generic&lt;/code&gt; XDP driver, however, we see something surprising: worse performance than our TC program. After a
little digging we realized this actually makes sense. The &lt;code&gt;generic&lt;/code&gt; XDP driver does not run on the RX queue and
instead, like TC, runs after the socket buffer has been allocated. The worse performance is the result of running the
XDP program in the same place as TC but without any of the optimizations that TC benefits from.&lt;/p&gt;
    &lt;p&gt;With the &lt;code&gt;native&lt;/code&gt; XDP driver (which is available in
linux 4.19+)
we finally see the results we've been looking for - we're routing just shy of line-rate at about 194Gbps, 12.4 times the
throughput of iptables and about 9.2x the throughput of TC.&lt;/p&gt;
    &lt;p&gt;One final thing to note here are the error bars, which were significantly smaller with &lt;code&gt;native&lt;/code&gt; XDP. This makes
sense since the bulk of our performance improvements come from bypassing the linux kernel and doing less work. iPerf3,
iptables, and the linux kernel are all constantly fighting for the CPU which results in inconsistent throughput.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Drop-In Fast Path for Containers&lt;/head&gt;
    &lt;p&gt;One of the most exciting aspects of this discovery is how immediately applicable it is. We setup our benchmarks to replicate how containers already use network namespaces and &lt;code&gt;veth&lt;/code&gt; pairs. This means we can dramatically accelerate
container networking without changing how containers work or how they're orchestrated.&lt;/p&gt;
    &lt;p&gt;Consider what happens every time a containerized application sends a packet today: it traverses through iptables rules, gets NAT'd, maybe goes through connection tracking, and finally makes it out to the network. All of this happens in the kernel, consuming precious CPU cycles that could be used by actual applications.&lt;/p&gt;
    &lt;p&gt;With XDP on veth interfaces, we can bypass all of that overhead. The packet goes straight from the container's namespace through our XDP program to the physical interface. No iptables. No conntrack. Just pure, line-rate packet routing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We're Building&lt;/head&gt;
    &lt;p&gt;While our primary use case at Loophole Labs is live migration - where this technique enables us to transparently reroute connections at line rates during migrations - we recognize the broader impact this can have on container networking as a whole.&lt;/p&gt;
    &lt;p&gt;That's why we're working on a Docker network plugin that implements this technique. It'll be a drop-in replacement for Docker's default bridge network driver, except it uses XDP instead of iptables for packet routing.&lt;/p&gt;
    &lt;p&gt;For simpler container deployments that don't need the full complexity of Kubernetes networking (microservices, development environments, or edge computing nodes) this could mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Doubling network throughput without any hardware upgrades&lt;/item&gt;
      &lt;item&gt;Dramatically reducing CPU usage for network-heavy workloads&lt;/item&gt;
      &lt;item&gt;Eliminating iptables as a bottleneck in container-to-container communication&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We plan to open source this plugin soon, but the beauty of this technique is that you don't need to wait for us. Everything you need to implement this yourself is described in this post. The &lt;code&gt;veth&lt;/code&gt; pairs are already there, all that's
left is writing the XDP programs to route your packets.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bigger Picture&lt;/head&gt;
    &lt;p&gt;Loophole Labs was built on a very simple premise: Better Building Blocks = Better Applications.&lt;/p&gt;
    &lt;p&gt;This discovery - that XDP can process egress traffic by taking advantage of &lt;code&gt;veth&lt;/code&gt; interfaces - is the best
representation of just that, a better building block that results in significantly better applications.&lt;/p&gt;
    &lt;p&gt;While we'll be open-sourcing the Docker network plugin for those who want to take advantage of XDP's egress performance for themselves, this discovery also powers something much bigger: Architect, our live migration platform.&lt;/p&gt;
    &lt;p&gt;Architect uses this XDP technique (as well as other breakthrough implementations for disk &amp;amp; memory checkpointing) to seamlessly live migrate your containers, VMs, and even active network connections between any clouds or regions - all without your users noticing.&lt;/p&gt;
    &lt;p&gt;If you're interested in diving deeper into the technical details or implementing XDP egress in your own infrastructure, join our Discord where our engineering team hangs out and answers questions from the community. Trust me, we love talking about this stuff.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to Use Live Migration?&lt;/head&gt;
    &lt;p&gt;Join our waitlist to be among the first to dramatically reduce your infrastructure costs while improving reliability:&lt;/p&gt;
    &lt;head rend="h4"&gt;‚îÄ‚îÄ/~\ Architect&lt;/head&gt;
    &lt;head rend="h4"&gt;‚îÄ‚îÄOptimize cluster costs and maximize node utilization, all without modifying your applications or your infrastructure.&lt;/head&gt;
    &lt;head rend="h3"&gt;Going to KubeCon NA 2025?&lt;/head&gt;
    &lt;p&gt;If you are in Atlanta for KubeCon NA 2025 (November 10-13), stop by Booth #1752 to see live demos of workloads migrating between clouds. We'll show you exactly how this XDP technique combines with our other innovations to make the impossible, possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://loopholelabs.io/blog/xdp-for-egress-traffic"/><published>2025-11-04T16:26:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45815419</id><title>I was right about dishwasher pods and now I can prove it [video]</title><updated>2025-11-06T06:49:19.587846+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=DAX2_mPr9W8"/><published>2025-11-04T20:16:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45822539</id><title>NY school phone ban has made lunch loud again</title><updated>2025-11-06T06:49:19.373973+00:00</updated><content>&lt;doc fingerprint="366ad004a8c395ce"&gt;
  &lt;main&gt;
    &lt;p&gt;These days, lunchtime at Benjamin N. Cardozo High School in Queens is a boisterous affair, a far cry from before the smartphone ban went into effect, when most students spent their spare time scrolling and teachers said you could hear a pin drop.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis year's gotten way louder,‚Äù said Jimena Garcia, 15. ‚ÄúSometimes I would take naps in the lunchroom, but now I can't because of the noise. But it's fun.‚Äù&lt;/p&gt;
    &lt;p&gt;On a recent fall afternoon, Garcia and her friends crowded around a lunch table in the large cafeteria playing Jenga, occasionally shrieking and gasping as the tower began to lean and fall.&lt;/p&gt;
    &lt;p&gt;The faculty donated board games to help ease kids into the phone-free era. Student volunteers oversaw a table stacked with games: checkers, chess, Yahtzee, Scrabble, Clue, Life and Trivial Pursuit. For many of the kids, it was their first time playing the games, and they said they were enjoying it.&lt;/p&gt;
    &lt;p&gt;‚Äú I do like how this phone ban is allowing students to just connect with each other, make new friendships,‚Äù said 17-year-old Alyssa Ko, the school president. ‚ÄúBecause some people use their phone to just hide away.‚Äù&lt;/p&gt;
    &lt;p&gt;The ban prohibits all internet-enabled devices throughout the school day, although there are exceptions for some students with disabilities, kids learning English who need translation apps, and in cases where a teacher says a device can be used for a lesson.&lt;/p&gt;
    &lt;p&gt;Schools were given flexibility to choose their own storage plans, and Cardozo, which rolled out metal detectors this fall after a student was found with a gun, requires its 3,100 students to keep their phones in internet-blocking magnetic pouches. Other schools have installed storage lockers, or have kids keep their phones zipped up in backpacks.&lt;/p&gt;
    &lt;p&gt;As students adjust to lo-fi life, teachers seem pleased with the results. According to an October survey from the state teachers union, New York State United Teachers, 89% of school staff members said the new policies improved their schools' environments, and 76% said kids are more engaged in lessons.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhen students put down their phones, they pick up books ‚Äî and build friendships,‚Äù said NYSUT president Melinda Person.&lt;/p&gt;
    &lt;p&gt;The initial feedback reflects national trends. New York is one of 31 states, plus Washington, D.C. that have banned smartphones in schools, according to an EdWeek tracker. In a national survey from the University of Pennsylvania, teachers said banning phones helps kids focus.&lt;/p&gt;
    &lt;p&gt;‚ÄúTeachers seem really happy with the changes that they're seeing in the classroom with the electronic device ban,‚Äù Cardozo principal Meagan Colby said. ‚ÄúThey're telling us that there's a lot more student interaction, a lot more discussion among students, a lot better focus. Overall productivity in the classroom and engagement is higher.‚Äù&lt;/p&gt;
    &lt;p&gt;Senior Raya Osagie, 16, said she has to ‚Äúthink more in class‚Äù because she used to Google answers or use artificial intelligence. ‚ÄúNow when we get computers, I actually have to [do] deep research instead of going straight to AI,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;Students said they‚Äôve also seen their classmates reading physical books more.&lt;/p&gt;
    &lt;p&gt;In the cafeteria, Ryan Tripathi, 16, was paging through ‚ÄúLord of the Flies,‚Äù which he said is slow-going. ‚ÄúI'm just not used to reading,‚Äù he said. ‚ÄúI‚Äôm usually on my phone.‚Äù&lt;/p&gt;
    &lt;p&gt;Tripathi said it‚Äôs good that people are reading more and classroom discussions have become more lively. But he said he‚Äôs ‚Äúnot the biggest fan‚Äù of the smartphone ban. ‚ÄùSometimes you just want to go on your phone and you don't have the ability to do that anymore,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;Enakshi Barua, 14, said she‚Äôs also opposed to the ban, on principle.&lt;/p&gt;
    &lt;p&gt;‚Äú Students are distracted by the phones, but I also don‚Äôt believe they should take away our privileges," Barua said. "I feel like the trust isn‚Äôt there between the students and teachers. So I feel like that should be built instead of banning the phones.‚Äù&lt;/p&gt;
    &lt;p&gt;At Cardozo, a few kids break the rules, teachers said. Some either put ‚Äúburner‚Äù phones in their pouches or bang them open. Shanna Burrows, who oversees restorative justice at the school, said staff members are collecting around 30 contraband phones a day. There‚Äôs a strike system with escalating punishments that include keeping phones for days, weeks or months, and meetings with parents. Under the state law, schools are not allowed to suspend students solely for smartphone-ban infractions.&lt;/p&gt;
    &lt;p&gt;Students said they have found other ways to push boundaries, like passing old-fashioned notes. ‚Äú Especially when you're trying to talk but not have the teacher notice ‚Ä¶ it would just be [that] we‚Äôd send a text message or write on our notes app,‚Äù Ko said. ‚ÄúPassing notes is more common now.‚Äù&lt;/p&gt;
    &lt;p&gt;Ko said other analog activities have also made a comeback, including cards, hangman, tic-tac-toe and Polaroid cameras. ‚ÄúThere are just a lot of memories that we make throughout high school that we want to capture,‚Äù she said. ‚ÄúI actually have a lot of Polaroids on my wall.‚Äù&lt;/p&gt;
    &lt;p&gt;Tiana Millen, assistant principal of climate and culture, said she‚Äôd also like to see another analog technology make a comeback: the clock.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey don't know how to read the clocks," she said. "So I make jokes with them and say, ‚ÄòWe're going to have classes just on how to read the clocks.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;If they did, she said, they‚Äôd see they‚Äôre getting to class on time more than they used to; hallway traffic is moving better now that kids aren‚Äôt so focused on their phones.&lt;/p&gt;
    &lt;p&gt;This story has been updated.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gothamist.com/news/ny-smartphone-ban-has-made-lunch-loud-again"/><published>2025-11-05T13:20:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45822559</id><title>Radiant Computer</title><updated>2025-11-06T06:49:18.410764+00:00</updated><content>&lt;doc fingerprint="899b796b21f8ecf7"&gt;
  &lt;main&gt;&lt;p&gt;Radiant Computer&lt;/p&gt;&lt;p&gt;Radiant's purpose is to explore what personal computing could be when designed from first principles.&lt;/p&gt;We believe the current trajectory of personal computing is leading us to a less free world, and that only a new computing movement rooted in human dignity, creation and autonomy can change its course.&lt;p&gt;Radiant is a computer reimagined from the ground up, a clean-slate design free from the historical baggage that plagues modern systems, and free from Big Tech's influence.&lt;/p&gt;&lt;p&gt;It's a computer designed to help you learn, create, play, and explore. It's a space to focus, free from distractions. A return to the simple joy of computing: just you and your ideas.&lt;/p&gt;&lt;p&gt;Computers today are designed around engagement and surveillance business models rather than user needs. App stores are filled with adware. Operating systems prioritize data collection over user agency. Social media algorithms optimize for addiction. Big Tech fundamentally reshaped computing from a tool for human empowerment into an attention extraction machine.&lt;/p&gt;&lt;p&gt;Radiant proposes an alternative vision for computing. It doesn't ship with a web browser; it has its own network reminiscent of the early Internet: no social media, no scripts, no trackers. It's a from-scratch system that doesn't retrace the footsteps of the contemporary OS. It's a new paradigm for personal computing that uses modern advances mindfully and deliberately. It's fully open, from hardware to software. It's an offline-first space, designed for focus and creation.&lt;/p&gt;&lt;p&gt;Code is computing's native medium; the material you shape to build your own tools, stories, and spaces. Radiant is designed to make that accessible to everyone. It's a tool for personal computing where every application and every surface, exists as code you can read, edit, and extend. It's a system you can truly own. One that is designed to bring the joy of computing to everyone.&lt;/p&gt;&lt;p&gt;Writing software doesn't have to be daunting, but the platforms and tools we're stuck with make it so. Radiant aims to change that and truly empower users to create. Furthermore, advances in generative A.I. will make coding accessible to a much broader audience. One of our goals is to explore how an A.I.-native computer system can enhance the creative process, all while keeping data private.&lt;/p&gt;&lt;p&gt;Radiant belongs to a more humane future: a personal computer that welcomes curiosity, invites experimentation, and keeps the power in your hands. This is personal computing for the next generation.&lt;/p&gt;&lt;p&gt;Find out more about the Radiant system, or read our design principles and tenets.&lt;/p&gt;&lt;p&gt;Radiant is an ongoing research project in personal computing. If this resonates with you, write us at üìß letters@radiant.computer or follow us on ü¶ã Bluesky @radiant.computer.&lt;/p&gt;&lt;p&gt;You can learn more about Radiant by browsing our log or notes.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://radiant.computer"/><published>2025-11-05T13:22:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45823141</id><title>The shadows lurking in the equations</title><updated>2025-11-06T06:49:18.075018+00:00</updated><content>&lt;doc fingerprint="370746cbee2f5ea1"&gt;
  &lt;main&gt;
    &lt;p&gt;For all the history of computational mathematical visualization, graphing equations has been done in binary mode - where graphs show only where an equation is EXACTLY equal. But when you only see in black-and-white, some things are invisible. For all this time, lurking beneath the error == 0 surface, mathematical shadows have been lurking in the equations.&lt;/p&gt;
    &lt;p&gt;FuzzyGraph, on the other hand, visualizes equations in Non-Binary mode - showing not only where an equation are exactly equal, but also where the equation nearly equal and where the equation is far from equal (where the error is high). Sometimes, these high error areas form clear visual shadow-like features.&lt;/p&gt;
    &lt;p&gt;Let's look at some examples...&lt;/p&gt;
    &lt;p&gt;Here is the "Slash Dot" Equation ( \( \frac{y}{x^2+y^2} = \frac{x+1}{x^2+y^2} \)) as both a conventional and fuzzy graph...&lt;/p&gt;
    &lt;p&gt;Note the giant black hole that is present in the Fuzzy/Non-Binary graph, but invisible in conventional/Binary graphing. This "black hole" feature represents a region of high error in the equation.&lt;/p&gt;
    &lt;p&gt;Let's look at another example: \(y = \frac{x}{x^2 + y^2} \) &lt;/p&gt;
    &lt;p&gt;Notice that the black hole eye-looking features are COMPLETELY INVISIBLE in the conventional/binary mode of graphing.&lt;/p&gt;
    &lt;p&gt;To get a better idea of what these black hole things are, let's look at a simpler example. First let's look at the opposite of a black hole - a simple star/particle example: \( x^2 + y^2 = 0 \). For this equation, there is only 1 solution: (0, 0). So if you graph this in a conventional graphing app, it will only show a single dot at (0, 0). But in FuzzyGraph, it looks like a fuzzy particle or something.&lt;/p&gt;
    &lt;p&gt;But now, let's invert this to get the "Black Hole Equation": \( \frac{1}{x^2+y^2} = 0 \)...&lt;/p&gt;
    &lt;p&gt;In this case, there is absolutely nothing to show on a conventional graph, as there are actual solutions to this equations. However, there is still a mathematical topography which can be visualized (as can be seen in the fuzzy graph).&lt;/p&gt;
    &lt;p&gt;Not all of the Shadows are like black holes.&lt;/p&gt;
    &lt;p&gt;In this example, let's start by combining 2 lines together: \(y=x\) and \(y=-x\).&lt;/p&gt;
    &lt;p&gt;We can visually add 2 equations together by refactoring them so they are both equal to 0, and then multiplying the two refactored equations together. \(y=x\) can be changed to \(y-x=0\), and \(y=-x\) can be refactored to \(y+x=0\).&lt;/p&gt;
    &lt;p&gt;We can then combine 2 into a single equation these like this: \( (y-x) \times (y+x) = 0 \)&lt;/p&gt;
    &lt;p&gt;And now, let's invert one of the equations using division: \( \frac{x-y}{x+y} = 0 \)&lt;/p&gt;
    &lt;p&gt;So as you can see, the line that was inverted (under the division line) is now a Shadow Line. And this seems like a more "correct" way to visualize this than as the conventional graph shows it (which is indistinguishable from the simpler equation, \(y-x=0\)).&lt;/p&gt;
    &lt;p&gt;This equation works almost exactly as the previous. And like before, let's start with multiplication to combine 2 equations (in this case, a circle and a vertical line equation): \( x \times (x^2+y^2-1) = 0 \).&lt;/p&gt;
    &lt;p&gt;But now, let's invert the circle by using division, which makes the equation: \( \frac{x}{x^2+y^2-1} = 0 \).&lt;/p&gt;
    &lt;p&gt;Note that the Shadow Circle is invisible in the conventional graph. In fact, the conventional graph looks identical to a conventional graph of the \(x=0\) equation (as if the denominator was not there).&lt;/p&gt;
    &lt;p&gt;In all of these previous examples, the "shadows" have represented areas of high error. But in this last example, we'll see some hidden details that represent areas of low error - areas that are nearly solutions to the equation.&lt;/p&gt;
    &lt;p&gt;Consider the equation, \( y=4 sin(x)+ sin(2.7y) \), as both a conventional graph and a fuzzy graph:&lt;/p&gt;
    &lt;p&gt;Note the floating dots in the fuzzy graph version that are not there in the conventional/binary graph. These are like underwater islands - underwater mountains that are just below the surface of the water (or in this case, the \( error == 0 \) surface). These hidden islands represent area that are near-solutions to the equation (which are only visible in FuzzyGraph).&lt;/p&gt;
    &lt;p&gt;Their presense hints that we can tweak the equation slightly to cause them to burst above the surface of the water (which should also make them visible in conventional graphs).&lt;/p&gt;
    &lt;p&gt;So let's change the equation from: &lt;lb/&gt; \( y=4 sin(x)+ sin(2.7y) \) to: &lt;lb/&gt; \( y=4 sin(x)+ sin(2.8y) \)...&lt;/p&gt;
    &lt;p&gt;And as you can see, those previously-hidden islands are now visible in the conventional graph.&lt;/p&gt;
    &lt;p&gt;So Fuzzy/non-binary graphing can help us see features of the mathematical topography that are completely invisible with conventional/binary.&lt;/p&gt;
    &lt;p&gt;Date published: 2025-11-05&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gods.art/articles/equation_shadows.html"/><published>2025-11-05T14:21:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45823186</id><title>Carice TC2 ‚Äì A non-digital electric car</title><updated>2025-11-06T06:49:15.878733+00:00</updated><content>&lt;doc fingerprint="e68401540d9d7dbb"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;the 100% electric Carice TC2&lt;/head&gt;
    &lt;head rend="h1"&gt;a real retro head-turner&lt;/head&gt;
    &lt;p&gt;Reserve your spot now on next year‚Äôs production batch. Spaces limited.&lt;/p&gt;
    &lt;head rend="h1"&gt;the 100% electric carice TC2&lt;/head&gt;
    &lt;head rend="h2"&gt;a real retro head-turner&lt;/head&gt;
    &lt;p&gt;Reserve your spot now on next year‚Äôs production batch. Spaces limited.&lt;/p&gt;
    &lt;head rend="h6"&gt;The new experience&lt;/head&gt;
    &lt;head rend="h2"&gt;carice TC2&lt;/head&gt;
    &lt;p&gt;Meet the all-new electric Carice TC2: extremely lightweight, dynamic and elegant. When you get into your Carice, you escape and for a moment you forget about the everyday hassle. Whether you are the one driving the Carice or the passenger being driven around, it will be hard to hide that smile. With a TC2 you own something extraordinary, a piece of art.&lt;/p&gt;
    &lt;head rend="h6"&gt;The new experience&lt;/head&gt;
    &lt;head rend="h2"&gt;Carice TC2&lt;/head&gt;
    &lt;p&gt;We are busy with anything and everything, all the time. When you get into your Carice, you escape and for a moment you forget about the everyday hassle. You just relax and enjoy the drive. Whether you are the one driving the Carice or the passenger being driven around, it will be hard to hide that smile. The TC2 is a piece of art, just for you to enjoy.&lt;/p&gt;
    &lt;head rend="h6"&gt;why A Carice&lt;/head&gt;
    &lt;head rend="h2"&gt;the ultimate freedom&lt;/head&gt;
    &lt;p&gt;The result of years of hard work and dedication is the striking Carice TC2: it is the ultimate car to enjoy that sunny day in style and enjoy your drive and unwind.&lt;/p&gt;
    &lt;head rend="h5"&gt;all electric&lt;/head&gt;
    &lt;p&gt;The Carice TC2 is fully electric and has no emissions. This car is built to be fun for everybody ‚Äì not just the driver. It is our mission to combine 21st-century technology with the look and feel of the cars of the past.&lt;/p&gt;
    &lt;head rend="h5"&gt;the essence&lt;/head&gt;
    &lt;p&gt;If you just take away unnecessary things for long enough, you will get back to the essence of driving. The Carice TC2 is elegant, stylish and at the same time uncomplicated. This delivers electric driving in its most pure and elementary form.&lt;/p&gt;
    &lt;head rend="h5"&gt;featherlight&lt;/head&gt;
    &lt;p&gt;Because the Carice TC2 is available from 590 kg including battery pack, it handles exceptionally dynamic yet comfortable. Moreover, power consumption is very low due to this weight. Therefore, the TC2 delivers a driving experience like no other. Very compact, yet big enough!&lt;/p&gt;
    &lt;head rend="h3"&gt;‚Äì time to forget about time ‚Äì&lt;/head&gt;
    &lt;head rend="h6"&gt;Seen on&lt;/head&gt;
    &lt;head rend="h6"&gt;Seen on&lt;/head&gt;
    &lt;head rend="h6"&gt;About us&lt;/head&gt;
    &lt;head rend="h2"&gt;carice craftsmanship&lt;/head&gt;
    &lt;p&gt;Built and designed from the ground up in the Netherlands by people with a lifelong love of classic cars, the TC2 is made to resemble the playful and elegant looks of every car that you loved as a kid. This passion for cars translates into a high level of attention to detail and commitment to meet your needs. Carice is expanding their extensive history in automotive design and development every day. Find out about our latest events and achievements here.&lt;/p&gt;
    &lt;head rend="h6"&gt;gallery&lt;/head&gt;
    &lt;head rend="h2"&gt;modern classic&lt;/head&gt;
    &lt;p&gt;From the eye-catching dashboard, the classically styled steering wheel to the matching upholstery: everything in a Carice TC2 is made to stand out in all its simplicity. With a Carice you don‚Äôt just own another car: you get something extraordinary, a piece of art.&lt;/p&gt;
    &lt;head rend="h6"&gt;configure&lt;/head&gt;
    &lt;head rend="h2"&gt;configure your carice&lt;/head&gt;
    &lt;p&gt;To personalize your TC2, you can choose from a wide range of different colors for the paint, upholstery and rooftop. There is always a combination that fits your style.&lt;/p&gt;
    &lt;head rend="h2"&gt;specifications&lt;/head&gt;
    &lt;p&gt;There is no better way to experience the Carice TC2 than by seeing it and driving it. The elegant lines, attention to detail and phenomenal handling can‚Äôt be captured in a list, but some features can. You can find them below.&lt;/p&gt;
    &lt;head rend="h5"&gt;sizes and masses&lt;/head&gt;
    &lt;head rend="h5"&gt;battery&lt;/head&gt;
    &lt;head rend="h5"&gt;other&lt;/head&gt;
    &lt;p&gt;* Some specifications may differ, depending on the individual configuration of the TC2&lt;/p&gt;
    &lt;head rend="h6"&gt;contact&lt;/head&gt;
    &lt;head rend="h2"&gt;send us a message&lt;/head&gt;
    &lt;head rend="h6"&gt;FAQ&lt;/head&gt;
    &lt;head rend="h2"&gt;frequently asked questions&lt;/head&gt;
    &lt;head rend="h5"&gt;when can i order my Carice TC2?&lt;/head&gt;
    &lt;p&gt;You can already order your Carice. If you are interested you can contact us via the links on the website and the contact form to register your interest or book a test drive.&lt;/p&gt;
    &lt;head rend="h5"&gt;is the Carice TC2 a new car?&lt;/head&gt;
    &lt;p&gt;Yes! We have been designing and developing the TC2 ourselves from the ground up, and are now manufacturing the first series of TC2‚Äôs in the Netherlands. After more than 10 years of developing, testing and optimizing an extremely lightweight chassis around our electric drivetrain, you can now get a phenomenal handling and elegant TC2 yourself and enjoy driving in its most elementary form.&lt;/p&gt;
    &lt;head rend="h5"&gt;what is the price of a Carice TC2?&lt;/head&gt;
    &lt;p&gt;Prices for a TC2 start at ‚Ç¨44.500 excluding taxes (‚Ç¨53.854 including 21% btw/Dutch tax).&lt;/p&gt;
    &lt;head rend="h5"&gt;in what countries can i drive the Carice?&lt;/head&gt;
    &lt;p&gt;The Carice TC2 complies with the European regulations and can therefore be driven in all EU countries and countries that adopt those regulations, like Switzerland, the United Kingdom, Monaco and Norway.&lt;/p&gt;
    &lt;head rend="h5"&gt;what is the estimated delivery time?&lt;/head&gt;
    &lt;p&gt;At the moment we are making the TC2 exclusively on order, as every car is configured differently. We can provide you with an estimation on the delivery time and you can reserve a spot on the production list by placing an order.&lt;/p&gt;
    &lt;head rend="h5"&gt;how can i configure my Carice?&lt;/head&gt;
    &lt;p&gt;There are a lot of options for you to choose between. Different colors, wheels, upholstery, soft top, accessories, battery pack, charging gear and so on. If you are interested in buying a Carice TC2, please get in touch.&lt;/p&gt;
    &lt;head rend="h5"&gt;what is the range of a Carice TC2&lt;/head&gt;
    &lt;p&gt;Depending on the configuration of your TC2, you can drive more than 300km, which can bring you to the most beautiful places.&lt;/p&gt;
    &lt;head rend="h5"&gt;i have decided: i want one soon! how to proceed?&lt;/head&gt;
    &lt;p&gt;The current production batch is sold out, but there are a few cars left for the next production run. If you are sure you want one, you can secure one of these cars by paying a deposit of 75% of the purchasing price. Please contact us for the details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.caricecars.com/"/><published>2025-11-05T14:25:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45823831</id><title>Ruby and Its Neighbors: Smalltalk</title><updated>2025-11-06T06:49:15.627168+00:00</updated><content>&lt;doc fingerprint="34329b3385e7083a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Ruby And Its Neighbors: Smalltalk&lt;/head&gt;
    &lt;p&gt;Last time, we talked about Perl as an influence on Ruby, this time, we‚Äôll talk about the other major influence on Ruby: Smalltalk.&lt;/p&gt;
    &lt;p&gt;Smalltalk had a different kind of influence, since almost nothing of Smalltalk‚Äôs syntax made into Ruby. But many of the details of how objects work are directly inspired by Smalltalk, including the idea that every piece of data is part of the object system.&lt;/p&gt;
    &lt;p&gt;Also unlike Perl, I spent a good couple of years working in Smalltalk, and it is one of my favorite languages that I‚Äôll never likely use in anger again.&lt;/p&gt;
    &lt;head rend="h2"&gt;(A Personal) History of Smalltalk&lt;/head&gt;
    &lt;p&gt;Smalltalk originated in the same Xerox PARC team that invented the windowed interface, ethernet, and the laser printer, and who knows what else, they may have invented ice cream and rainbows.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a whole story about what project Smalltalk was invented to be a part of, and a whole alternate history of computing and how people interact with computers that we are going to largely ignore. (If you are interested, start by searching for ‚ÄúAlan Kay Dynabook‚Äù.)&lt;/p&gt;
    &lt;p&gt;Smalltalk went through a few different iterations in the 1970s, but the version that we know today is a direct descendent of Smalltalk-80, which was the first version released to the wider world.&lt;/p&gt;
    &lt;p&gt;For most of the 80s and 90s, Smalltalk was something that doesn‚Äôt really exist today ‚Äì a programming language and environment that companies paid money to use. Lots of money. The major player was ParcPlace, which was a spinoff of Xerox that provided Smalltalk tools. Their commercial product was originally called ObjectWorks, later changed to VisualWorks, and eventually sold off and presumably slowly losing customers after the late 90s.&lt;/p&gt;
    &lt;p&gt;Smalltalk was pretty big in the industry for a while. Most of the aviation industry ran on it in the 90s, the big payroll project that was the basis for Extreme Programming was a Smalltalk project, there was reasonably high demand for Smalltalk programmers through at least the mid 1990s. I taught an undergrad OO class in Smalltalk in 1997 and 1998 to students that wanted to be learning C++, and I remember telling them that Smalltalk programmers were paid more.&lt;/p&gt;
    &lt;p&gt;I first encountered Smalltalk as a grad student in about 1993, where Georgia Tech used ObjectWorks to teach Smalltalk and Object-Oriented programming (there‚Äôs a whole other sidebar about how Object-Oriented languages came to prominence in the 90s, and the arguments over that but again, another day). ObjectWorks was pricey, and there was also a lower-cost vendor called Digitalk, and eventually I also used a product called Smalltalk Agents, which has apparently totally vanished from the entire internet.&lt;/p&gt;
    &lt;p&gt;In 1995, a bunch of the original Xerox Smalltalk team was together at Apple, and they decided to release an open-source Smalltalk VM. What they did was very interesting. They wrote a very, very small kernel in very vanilla C, and then 95% of the environment was then built in Smalltalk on top of that. Oh, and even the vanilla C was written in Smalltalk, they wrote a Smalltalk to C compiler. They called their new Smalltalk ‚ÄúSqueak‚Äù, which made a lot more sense when they all moved en masse to Disney.&lt;/p&gt;
    &lt;p&gt;The fact that Squeak was largely written in itself made it fairly easy to port to new systems, and it was quickly available on just about anything with a microchip.&lt;/p&gt;
    &lt;p&gt;I‚Äôm pretty sure that I first saw Squeak at the OOPSLA conference in 1997. (Object-Oriented Programming, Systems, Languages &amp;amp; Applications, since you asked) At this conference I somehow got to do a team-building exercise with Adelde Goldberg from the original Xerox PARC team, which is not relevant to anything but seemed very cool at the time. I was already using Smalltalk in my projects, but Squeak was immediately interesting and my extended research team started doing cool stuff. Like, what I‚Äôm pretty sure was the first Wiki tool outside the original C2 Wiki, was written in Squeak. (Apparently at least one is still running).&lt;/p&gt;
    &lt;head rend="h2"&gt;Smalltalk‚Äôs Environment&lt;/head&gt;
    &lt;p&gt;It‚Äôs important to understand that Smalltalk‚Äôs development is a different evolutionary tree from nearly every currently popular programming language, in that Smalltalk is in no way, shape, or form influenced by Unix or C. Perl, Ruby, Python, JavaScript, Swift, Kotlin and on and on, all come from a universe where they expect to run Unix libraries, and where C syntax is normal. The Unix philosophy of ‚Äúsmall pieces, loosely joined‚Äù is not a part of Smalltalk‚Äôs DNA at all.&lt;/p&gt;
    &lt;p&gt;Smalltalk is basically its own operating system, and the syntax is different from C-style languages in ways big and small. For example, the first element of an array is‚Ä¶ 1. Which, when you think about how people count, actually makes sense.&lt;/p&gt;
    &lt;p&gt;It‚Äôs hard to separate Smalltalk the language from Smalltalk the environment, although I suppose technically you could have the language without the whole shebang (and I think there was a GNU Smalltalk that tried this), really the environment is part of the appeal.&lt;/p&gt;
    &lt;p&gt;Your main interfaces to the smalltalk system are a Workspace and a Browser. A workspace is analogous to REPL session, you can type in arbitrary Smalltalk code and have the system ‚Äúdo it‚Äù to execute the code, ‚Äúprint it‚Äù to execute the code and output the result. There are some other actions like ‚Äúdebug it‚Äù or ‚Äúinspect it‚Äù, but that‚Äôs the basic idea. Unlike a Unix REPL, there‚Äôs no prompt, and you don‚Äôt automatically invoke code by hitting return, you have to select code and then invoke the menu item or the keyboard shortcut for the code you want to act on.&lt;/p&gt;
    &lt;p&gt;The Browser is where you write code. There a a few different versions in most Smalltalks, here‚Äôs the main one, this is from a modern Smalltalk called Cuis.&lt;/p&gt;
    &lt;p&gt;At the top, we have four window panes ‚Äì left to right we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Categories ‚Äì groups of classes that are related in some way. Cuis nicely puts each group in a pulldown list. Categories have no particular syntactic meaning, they are just there to make browsing easier.&lt;/item&gt;
      &lt;item&gt;Classes ‚Äì one entry for each class in the currently selected category, at the bottom of this pane is a toggle for ‚Äúclass‚Äù vs. ‚Äúinstance‚Äù which determines what kinds of messages are shown in the next two panes.&lt;/item&gt;
      &lt;item&gt;Protocols ‚Äì a protocol is a user-defined group of messages. Smalltalk internally uses ‚Äúmessage‚Äù rather than ‚Äúmethod‚Äù because of how Alan Kay thinks about objects. Again, protocols are for the programmer, not the system.&lt;/item&gt;
      &lt;item&gt;Messages ‚Äì each messages in the currently selected protocol is listed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The bottom pane is the code editor, and if a message is selected in the code pane, its code is displayed there.&lt;/p&gt;
    &lt;p&gt;You probably have questions:&lt;/p&gt;
    &lt;p&gt;Does this mean that you can see the source code for the entire Smalltalk system?&lt;/p&gt;
    &lt;p&gt;Yes, yes it does.&lt;/p&gt;
    &lt;p&gt;**Can you modify any code in the system? **&lt;/p&gt;
    &lt;p&gt;Yes, yes you can.&lt;/p&gt;
    &lt;p&gt;Even, like, deep system code?&lt;/p&gt;
    &lt;p&gt;Yes.&lt;/p&gt;
    &lt;p&gt;Isn‚Äôt that dangerous?&lt;/p&gt;
    &lt;p&gt;As a Ruby developer, you should know that it‚Äôs only as dangerous as the developers who use it.&lt;/p&gt;
    &lt;p&gt;How do you edit a message?&lt;/p&gt;
    &lt;p&gt;Just display the existing message in the browser, edit the message and select ‚Äúsave‚Äù. The Smalltalk system will parse the code, stop if there are syntax errors, but if not, the updated method will be saved to the system. A side effect is you can‚Äôt save code that isn‚Äôt syntactically parsable, even as a draft.&lt;/p&gt;
    &lt;p&gt;Okay, but how do you create a message?&lt;/p&gt;
    &lt;p&gt;The ‚Äúreal‚Äù way is to select a protocol but not a message, and Smalltalk will put a template in the edit window. Write your message in the editor and save it. Alternately, you can just change the name of a message in the edit window, and a new method with that name will be created, without deleting the old message.&lt;/p&gt;
    &lt;p&gt;And how do you create a class?&lt;/p&gt;
    &lt;p&gt;Similarly.&lt;/p&gt;
    &lt;p&gt;If you select a category and not a class, you‚Äôll get this in the code editor pane:&lt;/p&gt;
    &lt;code&gt;Object subclass: #NameOfSubclass
	instanceVariableNames: ''
	classVariableNames: ''
	poolDictionaries: ''
	category: 'Kernel-Chronology'
&lt;/code&gt;
    &lt;p&gt;The thing to note is that this is not actually template, it‚Äôs actually code: a message, waiting for you to fill in the arguments, replacing &lt;code&gt;#NameOfSubclass&lt;/code&gt; and adding the instance variables and so on. You don‚Äôt save this, you ‚Äúdo it‚Äù, just like if you were in a workspace. The message call is evaluated, and Smalltalk creates a new class.&lt;/p&gt;
    &lt;p&gt;But wait, if all the code is in the image and isn‚Äôt in text files, how do people work together and share code?&lt;/p&gt;
    &lt;p&gt;Don‚Äôt worry about it.&lt;/p&gt;
    &lt;p&gt;Seriously, though, worry about it.&lt;/p&gt;
    &lt;p&gt;This has always been a problem. Smalltalk allows you to share ‚Äúchange sets‚Äù, effectively the code differences between one point and another. Classically, one person would export their change set, and other team members would import it. Different Smalltalks have built up more sophisticated tools over time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Smalltalk‚Äôs Syntax&lt;/head&gt;
    &lt;p&gt;Smalltalk‚Äôs syntax is very simple, relative to Ruby and Perl.&lt;/p&gt;
    &lt;p&gt;Wait a sec, I literally wrote this for a chapter in a book about Smalltalk literally 25 years ago, here‚Äôs a slight paraphrase:&lt;/p&gt;
    &lt;p&gt;Every line of Smalltalk is evaluated the same way.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Every variable is an object. There are no basic types that are not objects.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every expression is a message being passed to an object, there is basically no expression syntax that is not a message.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;All messages return a value. (The return value is specified by&lt;/p&gt;&lt;code&gt;^&lt;/code&gt;, if the method does not specify a return value, it implicitly returns&lt;code&gt;self&lt;/code&gt;, the instance that received the message.)&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There are three kinds of messages:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Unary messages like &lt;code&gt;3 negated&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Binary messages like &lt;code&gt;a + b&lt;/code&gt;, these actually are messages you can define, there is a small set of them, and they are special cases in the parser.&lt;/item&gt;
          &lt;item&gt;Keyword messages such as &lt;code&gt;anArray at: 3 put: 7&lt;/code&gt;. This syntax got used by ObjectiveC and later Swift, so you may be familiar with it. It‚Äôs&lt;code&gt;receiver &amp;lt;messagepart&amp;gt;: &amp;lt;argument&amp;gt;&lt;/code&gt;where you can have multiple message parts. If you are referring to the message, typically you just say the message parts, so this message would be called&lt;code&gt;at:put:&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Unary messages like &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smalltalk does not have operator precedence. All code is evaluated strictly from left to right. Unary messages first, binary messages second, keyword messages last. Parenthesis can be used to force order of operations or to make things clearer.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The assignment operator is&lt;/p&gt;&lt;code&gt;:=&lt;/code&gt;(Smalltalk uses&lt;code&gt;=&lt;/code&gt;for boolean equality), the right hand side is evaluated and the value is assigned to the result of the left hand side.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And that‚Äôs basically it, with a couple of ways to create literals like strings, arrays, dictionaries, local variables, and blocks.&lt;/p&gt;
    &lt;p&gt;So, for 10 points and control of the board, what does this do?&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse := 3 squared + 4 squared sqrt&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The unary messages are evaluated first:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse := 9 + 16 sqrt&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;There‚Äôs still a unary message&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse := 9 + 4&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Now we can do the binary message:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse = 13&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Oops.&lt;/p&gt;
    &lt;p&gt;To get what you actually want, you need parentheses:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse := (3 squared + 4 squared) sqrt&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Smalltalk does not have special syntax for loops, all loop behavior is defined by methods on &lt;code&gt;Array&lt;/code&gt; and the like, very similar to Ruby‚Äôs &lt;code&gt;Enumerable&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Smalltalk does not have special syntax to create messages or classes. Message creation is managed by the editor (which internally calls a message that adds the new code), class creation is just another method ‚Äì in Squeak, that method is &lt;code&gt;Object#subclass:instanceVariableNames:classVariableNames:poolDictionaries:category:&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Smalltalk does not have special syntax for boolean logic, all logic behavior is defined by the classes &lt;code&gt;True&lt;/code&gt; and &lt;code&gt;False&lt;/code&gt;. Ruby sort of does this, but Ruby does have &lt;code&gt;if&lt;/code&gt; as special syntax. Smalltalk does not, you‚Äôd write a Smalltalk conditional as just another message:&lt;/p&gt;
    &lt;code&gt;(x &amp;gt; 10) ifTrue: [ x squared ] ifFalse: [ x sqrt ]
&lt;/code&gt;
    &lt;p&gt;The square brackets are blocks, and behave very similar to Ruby blocks, except that you can treat them as just normal variables and normal arguments. You can even, as in this case, have multiple arguments that take blocks.&lt;/p&gt;
    &lt;p&gt;The implementation if the method &lt;code&gt;ifTrue:ifFalse&lt;/code&gt; is simple. For the &lt;code&gt;True&lt;/code&gt; class, it just takes the true block and executes it by passing it the message &lt;code&gt;value&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;ifTrue: trueAlternativeBlock ifFalse: falseAlternativeBlock
	^trueAlternativeBlock value
&lt;/code&gt;
    &lt;p&gt;And for the false class, the exact opposite:&lt;/p&gt;
    &lt;code&gt;ifTrue: trueAlternativeBlock ifFalse: falseAlternativeBlock
	^falseAlternativeBlock value
&lt;/code&gt;
    &lt;p&gt;Smalltalk doesn‚Äôt have a &lt;code&gt;case&lt;/code&gt; or &lt;code&gt;switch&lt;/code&gt; statement, typically if you want behavior like that you‚Äôd define a dictionary of keys to blocks or you would use the object system and polymorphism and double dispatch.&lt;/p&gt;
    &lt;head rend="h2"&gt;Smalltalk‚Äôs Object Model&lt;/head&gt;
    &lt;p&gt;There‚Äôs a lot about Smalltalk‚Äôs object model that sound familiar to a Ruby developer:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There‚Äôs a base class called &lt;code&gt;Object&lt;/code&gt;that everything inherits from.&lt;/item&gt;
      &lt;item&gt;Instance variables are private. Getters and setters default to having the same name as the instance variable.&lt;/item&gt;
      &lt;item&gt;Method lookup happens at the point of the method call.&lt;/item&gt;
      &lt;item&gt;Classes are instances of the class &lt;code&gt;Class&lt;/code&gt;(sort of).&lt;/item&gt;
      &lt;item&gt;There‚Äôs a thing called a ‚ÄúMetaclass‚Äù&lt;/item&gt;
      &lt;item&gt;There‚Äôs a method that‚Äôs the method of last resort ‚Äì in Ruby, it‚Äôs &lt;code&gt;method_missing&lt;/code&gt;, but in Smalltalk it‚Äôs called&lt;code&gt;doesNotUnderstand&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are a couple of differences as well&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smalltalk‚Äôs meta classes are structured differently, I explained this once and I‚Äôm not sure I ever want to explain it again.&lt;/item&gt;
      &lt;item&gt;Smalltalk doesn‚Äôt have multiple inheritance or mixins or modules or anything like that. Although there have been some attempts to add these features, the traditional Smalltalk way to do this is through delegation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But overall, Smalltalk and Ruby are similar enough that a huge amount of Kent Beck‚Äôs Smalltalk Best Practice Patterns is applicable to Ruby as long as you translate the syntax.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Happened?&lt;/head&gt;
    &lt;p&gt;Unlike Perl, I actually did use Smallalk to build a few real applications that had real users. I miss it a lot.&lt;/p&gt;
    &lt;p&gt;I find that when I try to explain Smalltalk to people, it‚Äôs easy to explain the syntax and the object model. What‚Äôs hard to explain is how it is to work in a Smalltalk environment.&lt;/p&gt;
    &lt;p&gt;You‚Äôve likely used powerful coding editors and terminals. Smalltalk is just different. You are in the running environment.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tests start instantly, and in general run very fast. There‚Äôs a dedicated test runner window. Some smalltalk integrate tests with the regular browser, so you can see test status from the code browsers.&lt;/item&gt;
      &lt;item&gt;Debugging is amazing, you can investigate the state of any object in the system, you can change that state, you can easily execute arbitrary code. You can have a test halt on exception, update the code and re-run from the point failure. It‚Äôs hard to describe how fluid it is, especially since I‚Äôm no longer expert enough to do it fluently.&lt;/item&gt;
      &lt;item&gt;While the editor doesn‚Äôt have all the niceties of the IDE‚Äôs you are used to, it‚Äôs very powerful in its own way. If you save code with a message name that does not appear in the image at all, Smalltalk will typically ask you if you want to define it right there. A lot of the things we ask a Language Server to do, Smalltalk just kind of does, because the image has access to everything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the all-encompassing nature of the environment was also Smalltalk‚Äôs downfall. As more and more of the general computing environment became Unix and the ‚Äúsmall pieces loosely joined‚Äù philosophy, Smalltalk got harder and harder to integrate. Smalltalk isn‚Äôt a scripting language, it was late to develop connectivity to external databases, its model of team interaction is fundamentally different from Unix source control. The image-based system has some drawbacks ‚Äì you do get amazing access to the system, but it can be hard to tell where your code ends and the system begins. Code could depend on the state of the image in ways that were hard to replicate in deploys.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Did Ruby Take From Smalltalk?&lt;/head&gt;
    &lt;p&gt;Smalltalk‚Äôs legacy in Ruby is primarily the object model ‚Äì the idea that everything is an object and everything is manageable via method calls, and that message calls are evaluated at the point of call, as late as possible. Ruby takes that idea and translates it into a syntax that is more familiar to programmers used to C/Perl/Java.&lt;/p&gt;
    &lt;p&gt;I‚Äôm not sure this is exactly on point as far as Smalltalk‚Äôs influence on Ruby, but my Ruby style has always been very aggressive about creating new classes and objects. I‚Äôm quite confident that a reason for that style is that I came from Smalltalk first and not Java, Smalltalk style is much more amenable to small classes.&lt;/p&gt;
    &lt;p&gt;On my first largish Smalltalk project, users were simulating a chemical plant‚Äôs pipe system by placing tiles with pipes in them, and I frequently needed to do logic based on relative directions. I clearly remember creating a &lt;code&gt;Direction&lt;/code&gt; class with basically four live instances, &lt;code&gt;up&lt;/code&gt;, &lt;code&gt;down&lt;/code&gt;, &lt;code&gt;left&lt;/code&gt;, &lt;code&gt;right&lt;/code&gt;, and just enough logic inside to say that &lt;code&gt;up.turn_left&lt;/code&gt; equals &lt;code&gt;left&lt;/code&gt;, but &lt;code&gt;down.turn_left&lt;/code&gt; equals &lt;code&gt;right&lt;/code&gt;. It was useful enough that I remember how much fun it was to build it even  now, nearly thirty years later.&lt;/p&gt;
    &lt;p&gt;Of all the other programming languages I‚Äôve used, Ruby is the one that most clearly encourages that style of coding.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://noelrappin.com/blog/2025/11/ruby-and-its-neighbors-smalltalk/"/><published>2025-11-05T15:24:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45824864</id><title>Why aren't smart people happier?</title><updated>2025-11-06T06:49:14.957643+00:00</updated><content>&lt;doc fingerprint="2b2a121caf818e99"&gt;
  &lt;main&gt;
    &lt;p&gt;Adam Mastroianni is the author of Experimental History. He studies how people perceive and misperceive their social worlds. His work has been featured in Science, Nature, and The Tonight Show with Jimmy Fallon. He has a PhD in psychology from Harvard and a certificate of completion from 137 different escape rooms. He‚Äôs originally from Monroeville, Ohio (pop. 1,400) and currently lives in New York City.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a definition of intelligence that lots of psychologists can get behind:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Intelligence is a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. It is not merely book learning, a narrow academic skill, or test-taking smarts. Rather, it reflects a broader and deeper capability for comprehending our surroundings-‚Äúcatching on,‚Äù ‚Äúmaking sense‚Äù of things, or ‚Äúfiguring out‚Äù what to do [‚Ä¶] Intelligence, so defined, can be measured, and intelligence tests measure it well.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Intelligence sounds pretty great. Who doesn‚Äôt want to ‚Äúcatch on‚Äù and ‚Äúmake sense‚Äù? Hell, ‚Äúfiguring out‚Äù what to do is pretty much all of life!&lt;/p&gt;
    &lt;p&gt;Naturally, people with more of this mental horsepower must live happier lives. When they encounter a problem, they should use their superior problem-solving ability to solve it. Smarter people should do a better job making plans and getting what they want, and they should learn more from their mistakes and subsequently make fewer of them. All of this should add up to a life that makes smart people go ‚Äúthis life rules!‚Äù&lt;/p&gt;
    &lt;p&gt;So smarter people are happier, right?&lt;/p&gt;
    &lt;p&gt;Well, this meta-analysis says no. Another says maybe a teeny tiny bit. This large, nationally-representative study from the UK finds that people who score the lowest on an intelligence test are a little less happy than everyone else, but that‚Äôs pretty much it.&lt;/p&gt;
    &lt;p&gt;I also pulled data from the General Social Survey, which includes (a) a short vocabulary test that seems to correlate reasonably well with longer intelligence tests (you can try it here), and (b) a simple measure of happiness: ‚ÄúTaken all together, how would you say things are these days‚Äîwould you say that you are very happy, pretty happy, or not too happy?‚Äù Across 50 years of data and 30,346 people, the folks who scored higher on the vocab test were a tiny bit less happy (r = -.06, p &amp;lt; .001).&lt;/p&gt;
    &lt;head rend="h1"&gt;WHAT‚ÄôS GOING ON HERE?&lt;/head&gt;
    &lt;p&gt;Maybe our tests are bad. The psychological study of intelligence has a long, bleak history of racism and prejudice against poor people (‚Äúthree generations of imbeciles are enough‚Äù), so we should be skeptical coming in. Psychologists have been trying to construct bias-free tests for a long time, but it‚Äôs hard. Plus, people score higher on IQ tests when you pay them for performance, so what looks like a test of intelligence may in part be a test of how hard you‚Äôre willing to try.&lt;/p&gt;
    &lt;p&gt;But even if intelligence tests only measure something like ‚Äúability to succeed in an unfair society‚Äù or ‚Äúwillingness to try hard,‚Äù it only deepens the mystery. Shouldn‚Äôt those people end up with happier lives, however unfair that may be?&lt;/p&gt;
    &lt;p&gt;And the tests likely do tap something more than just privilege and effort. There‚Äôs plenty of skepticism toward intelligence tests in psychology, but even the biggest skeptics agree that IQ can predict things like how well you do in school and what kind of job you get, even accounting for all the criticisms. So why doesn‚Äôt it also predict living a life that you like?&lt;/p&gt;
    &lt;head rend="h1"&gt;SPEARING SPEARMAN&lt;/head&gt;
    &lt;p&gt;I think there‚Äôs one guy to blame for this big mystery, and his name is Charles Spearman.&lt;/p&gt;
    &lt;p&gt;Way back in 1904, Spearman noticed something weird: the same kids who did well in one subject in school tended to do well in other subjects, too. The correlations were never perfect, of course, but they were pretty darn high, even across subjects that seemed pretty different from each other, like French and math. How come?&lt;/p&gt;
    &lt;p&gt;Spearman figured there must be some general mental ability that humans use to solve all kinds of problems. He later wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This continued tendency to success of the same person throughout all variations of both form and subject-matter‚Äîthat is to say throughout all conscious aspects of cognition whatever‚Äîappears only explicable by some factor lying deeper than the phenomena of consciousness.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Helpfully, he also drew us a picture:&lt;/p&gt;
    &lt;p&gt;This is, I think, exactly where everything went wrong with the study of intelligence for the next 119 years. It‚Äôs not that Spearman‚Äôs results were inaccurate‚Äîin fact, they‚Äôve been replicated over and over. At this point, pretty much every paper on intelligence has to start out like this review from 2006:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In the study of intelligence, one empirical phenomenon is well established: Test scores on cognitive tasks show a positive manifold, that is, they are invariably positively intercorrelated, albeit to varying degrees. This implies that people who score well on one cognitive test are likely to score well on other cognitive tests. The positive manifold is a robust phenomenon.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Spearman‚Äôs stats were sound, but his interpretation was wrong. He did not, as he claimed, observe a ‚Äúcontinued tendency to success throughout all variations of both form and subject-matter,‚Äù nor has anybody else. It merely looks as if we‚Äôve varied all the forms and the subject-matters because we have the wrong theory about what makes them different.&lt;/p&gt;
    &lt;p&gt;We think tests of math, vocabulary, French, music, etc. are all different because some are about words and others are about numbers and others are about sounds. But psychology, like all sciences, is all about discovering the differences between seemingly similar things, and discovering the similarities between seemingly different things. If psychologists ever had to march into battle, a good candidate for our crests may be the famous M√ºller-Lyer illusion, the two lines that look like they‚Äôre different lengths but aren‚Äôt:&lt;/p&gt;
    &lt;p&gt;Just like those lines, I think all of our various tests of intelligence aren‚Äôt as different as they seem. They‚Äôre all full of problems that have a few important things in common:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;There are stable relationships between the variables.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There‚Äôs no disagreement about whether the problems are problems, or whether they‚Äôve been solved.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There have clear boundaries; there is a finite amount of relevant information and possible actions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The problems are repeatable. Although the details may change, the process for solving the problems does not.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think a good name for problems like these is well-defined. Well-defined problems can be very difficult, but they aren‚Äôt mystical. You can write down instructions for solving them. And you can put them on a test. In fact, standardized tests items must be well-defined problems, because they require indisputable answers. Matching a word to its synonym, finding the area of a trapezoid, putting pictures in the correct order‚Äîall common tasks on IQ tests‚Äîare well-defined problems.&lt;/p&gt;
    &lt;p&gt;Spearman was right that people differ in their ability to solve well-defined problems. But he was wrong that well-defined problems are the only kind of problems. ‚ÄúWhy can‚Äôt I find someone to spend my life with?‚Äù ‚ÄúShould I be a dentist or a dancer?‚Äù and ‚ÄúHow do I get my child to stop crying?‚Äù are all important but poorly defined problems. ‚ÄúHow can we all get along?‚Äù is not a multiple-choice question. Neither is ‚ÄúWhat do I do when my parents get old?‚Äù And getting better at rotating shapes or remembering state capitals is not going to help you solve them.&lt;/p&gt;
    &lt;p&gt;We all share some blame with Spearman, of course, because everybody talks about smarts as if they‚Äôre one thing. Google ‚Äúsmartest people in the world‚Äù and most of the results will be physicists, mathematicians, computer scientists, and chess masters. These are all difficult problems, but they are well-defined, and that makes it easy to rank people. The best chess player in the world is the one who can beat everybody else. The best mathematician is the one who can solve the problems that nobody else could solve. That makes it seem like the best chess players and mathematicians are not just the smartest in their fields, but the smartest in the whole world.&lt;/p&gt;
    &lt;head rend="h1"&gt;THE POORLY DEFINED PROBLEM OF BEING ALIVE&lt;/head&gt;
    &lt;p&gt;There is, unfortunately no good word for ‚Äúskill at solving poorly defined problems.‚Äù Insight, creativity, agency, self-knowledge‚Äîthey‚Äôre all part of it, but not all of it. Wisdom comes the closest, but it suggests a certain fustiness and grandeur, and poorly defined problems aren‚Äôt just dramatic questions like ‚Äúhow do you live a good life‚Äù; they‚Äôre also everyday questions like ‚Äúhow do you host a good party‚Äù and ‚Äúhow do you figure out what to do today.‚Äù&lt;/p&gt;
    &lt;p&gt;One way to spot people who are good at solving poorly defined problems is to look for people who feel good about their lives; ‚Äúhow do I live a life I like‚Äù is a humdinger of a poorly defined problem. The rules aren‚Äôt stable: what makes you happy may make me miserable. The boundaries aren‚Äôt clear: literally anything I do could make me more happy or less happy. The problems are not repeatable: what made me happy when I was 21 may not make me happy when I‚Äôm 31. Nobody else can be completely sure whether I‚Äôm happy or not, and sometimes I‚Äôm not even sure. In fact, some people might claim that I‚Äôm not really happy, no matter what I say, unless I accept Jesus into my heart or reach nirvana or fall in love‚Äîif I think I‚Äôm happy before all that, I‚Äôm simply mistaken about what happiness is!&lt;/p&gt;
    &lt;p&gt;This is why the people who score well on intelligence tests and win lots of chess games are no happier than the people who flunk the tests and lose at chess: well-defined and poorly defined problems require completely different problem-solving skills. Life ain‚Äôt chess! Nobody agrees on the rules, the pieces do whatever they want, and the board covers the whole globe, as well as the inside of your head and possibly several metaphysical planes as well.&lt;/p&gt;
    &lt;head rend="h1"&gt;IF YOU‚ÄôRE SO SMART, WHY ARE YOU SO DUMB?&lt;/head&gt;
    &lt;p&gt;Here‚Äôs another way of looking at it.&lt;/p&gt;
    &lt;p&gt;Say you want to test people‚Äôs math ability. You design a test, administer it to a bunch of people, do all your psychometrics, etc. You‚Äôre feeling pretty good about your math test. And then you find that some of the people who ace your test later say things like ‚Äútwo plus two is 19‚Äù and ‚Äú88 is the biggest number.‚Äù You‚Äôd feel pretty embarrassed about your math test because it‚Äôs clearly not measuring mathematical ability, if it‚Äôs measuring anything at all.&lt;/p&gt;
    &lt;p&gt;This is exactly the situation we‚Äôre in with tests that claim to measure people‚Äôs ‚Äúreasoning‚Äù and ‚Äúproblem-solving ability.‚Äù Christopher Langan, a guy who can score eye-popping numbers on IQ tests, believes that 9/11 was an inside job meant specifically to distract the public from his theories, and he claims that banks won‚Äôt give him a loan because he‚Äôs white. John Sununu supposedly has IQ of 176, but he still had to resign from being George H.W. Bush‚Äôs chief of staff because he flew to his dentist appointments using military jets. Bobby Fischer is one of the greatest chess players of all time, but he also claimed that Hitler was a good dude, the Holocaust didn‚Äôt happen, and ‚ÄúJews murder Christian children for their blood and they‚Äôre doing it even today.‚Äù Then there‚Äôs the ever-lengthening list of professors at elite universities who have been disciplined or dismissed for doing things like sexually harassing colleagues and students or completely making up data or hanging out with a known pedophile. These are supposed to be some of the smartest people in the world, endowed with exceptional problem-solving abilities. And yet they‚Äôre still unable to solve basic but poorly defined problems like ‚Äúmaintain a basic grip on reality‚Äù and ‚Äúbe a good person‚Äù and ‚Äúdon‚Äôt make any life-altering blunders.‚Äù&lt;/p&gt;
    &lt;head rend="h1"&gt;GAZE UPON OUR WORKS AND DESPAIR&lt;/head&gt;
    &lt;p&gt;And here‚Äôs another way of looking at it.&lt;/p&gt;
    &lt;p&gt;Over the last generation, we have solved tons of well-defined problems. We eradicated smallpox and polio. We landed on the moon. We built better cars, refrigerators, and televisions. We even got ~15 IQ points smarter! And how did our incredible success make us feel?&lt;/p&gt;
    &lt;p&gt;Well:&lt;/p&gt;
    &lt;p&gt;All that progress didn‚Äôt make us a bit happier. I think there‚Äôs an important lesson here: if solving a bunch of well-defined problems did not make our predecessors happier, it probably won‚Äôt make us happier, either. The barrier between you and everlasting bliss is probably not the size of your television, nor your ability to solve Raven‚Äôs Progressive Matrices.&lt;/p&gt;
    &lt;p&gt;(To be clear, I still think it‚Äôs good we did all this. Polio sucks and going to the moon is awesome.)&lt;/p&gt;
    &lt;p&gt;I wish we knew more about how to make that bright green line go up, but we just haven‚Äôt yet defined the problem of ‚Äúliving a happy life‚Äù. We know that if you‚Äôre starving, lonely, or in pain, you‚Äôll probably get happier if you get food, friends, and relief. After that, the returns diminish very quickly. You could read all the positive psychology you want, take the online version of The Science of Wellbeing (‚ÄùYale‚Äôs Most Popular Course Ever!‚Äù), read my post on hacking the hedonic treadmill, meditate, exercise, and keep a gratitude journal‚Äîand after all that, maybe you‚Äôll be a smidge happier. Whatever else you think will put a big, permanent smile on your face, you‚Äôre probably wrong.&lt;/p&gt;
    &lt;p&gt;So if you‚Äôre really looking for a transformative change in your happiness, you might be better off reading something ancient. The great thinkers of the distant past seemed obsessed with figuring out how to live good lives: Socrates, Plato, Aristotle, Epicurus, Buddha, Confucius, Jesus, Marcus Aurelius, St. Augustine, even up through Thoreau and Vivekananda. But at some point, this kind of stuff apparently fell out of fashion.&lt;/p&gt;
    &lt;p&gt;And hey, maybe that‚Äôs because there‚Äôs just no more progress to make on the poorly defined problem of ‚Äúhow do we live.‚Äù But most well-defined problems were once defined poorly. For example, ‚Äúhow do we land on the moon‚Äù was a hopelessly poorly defined problem for most of human history. It only makes sense if you know that the moon is a big rock you can land on and not, say, a god floating in the sky. We slowly put some definitions around that problem, and then one day we sent an actual dude to the moon and he walked around and was like ‚ÄúI‚Äôm on the moon now.‚Äù If we can do that, maybe we can also figure out how to live good lives. It certainly seems worth it to keep trying.&lt;/p&gt;
    &lt;head rend="h1"&gt;BUT AREN‚ÄôT THERE MULTIPLE INTELLIGENCES?&lt;/head&gt;
    &lt;p&gt;I‚Äôm not the first to propose that ‚Äúgeneral‚Äù intelligence is more than one thing. Pretty much as soon as Spearman started claiming that intelligence is mainly one thing, other people started saying that intelligence is actually many things. (That‚Äôs science, baby!) Today, the most popular version of this theory claims there‚Äôs something like eight intelligences, ranging from ‚Äúvisual-spatial‚Äù to ‚Äúbodily-kinesthetic.‚Äù I‚Äôm sympathetic to this take because it tries to account for all the different weird and wonderful things that humans can do. But it‚Äôs got two big problems.&lt;/p&gt;
    &lt;p&gt;Problem #1: People very rarely try to find any evidence for it. And when they do, they find that the people who score high on one of the many intelligences tend to score high on the others, too, just as Spearman would‚Äôve predicted a hundred years ago.&lt;/p&gt;
    &lt;p&gt;Problem #2: When you label every human activity as its own intelligence, you give up any hope of understanding anything about the structure of problems in the world or how people solve them. We can make up whatever categories we want; they aren‚Äôt given by God. The only reason to use some categories and not others is that some categories are useful and others aren‚Äôt.&lt;/p&gt;
    &lt;p&gt;For instance, we could have created a periodic table that organized the elements alphabetically, or by color, or by how good they taste. Instead we organize them by atomic number, not because it‚Äôs their ‚Äútrue‚Äù order, but because it‚Äôs useful. It helps us realize things like, ‚ÄúHey, we‚Äôve got a number 62 and a number 64‚ÄîI wonder if there‚Äôs a number 63 out there. We should go looking for it.‚Äù&lt;/p&gt;
    &lt;p&gt;So we should pick the way of categorizing intelligence that gives us the most bang for our buck. ‚ÄúIntelligence is many things‚Äù can‚Äôt explain why people perform similarly across supposedly different tests, and ‚Äúintelligence is mostly one thing‚Äù can‚Äôt answer a basic question like ‚Äúwhy smart people aren‚Äôt happier?‚Äù But we can handle both of those challenges when we split intelligence into skill at solving well-defined and poorly defined problems.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs not all we can do.&lt;/p&gt;
    &lt;head rend="h1"&gt;OH BOY HERE COMES THE PART ABOUT AI&lt;/head&gt;
    &lt;p&gt;People think of AI as a big glob of problem-solving ability. If you make the glob bigger, it can solve harder problems. That‚Äôs certainly been true so far: gigantic globs of AI can now drive cars, defeat our greatest chess players, and predict how proteins will fold.&lt;/p&gt;
    &lt;p&gt;All this has happened very quickly, which may make it seem like we‚Äôre careening toward a ‚Äúgeneral‚Äù artificial intelligence that can do all the things humans can. But if you split problems into well-defined and poorly defined, you‚Äôll notice that all of AI‚Äôs progress has been on defined problems. That‚Äôs what artificial intelligence does. In order to get AI to solve a problem, we have to give it data to learn from, and picking that data requires defining the problem.&lt;/p&gt;
    &lt;p&gt;That doesn‚Äôt mean the problems AI has solved so far are stupid or trivial. They‚Äôre really important and interesting! They‚Äôre just all well-defined problems. And we should expect that pattern to continue: for any well-defined problem, AI will eventually outperform humans. But for poorly defined problems, AI is hopeless. To solve those, we need humans running around doing weird human stuff.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat about GPT-3‚Äîit can write movie scripts! And what about DALLE-2‚Äîit can paint pictures!‚Äù These AIs perform a clever trick: they make it seem like they‚Äôre solving poorly defined problems when, under the hood, they‚Äôre really solving well-defined problems. GPT-3 doesn‚Äôt actually write movie scripts; it predicts what words should come next. DALLE-2 doesn‚Äôt actually paint pictures; it matches words to images. These problems aren‚Äôt easy to solve‚Äîthat‚Äôs why you need such a big glob of AI. But they obey clear, unchanging rules, they have bright boundaries, and you know precisely when you‚Äôve solved them. They are well-defined problems. (This is also why AI art isn‚Äôt art).&lt;/p&gt;
    &lt;p&gt;If you booted up a super-smart AI in ancient Greece, fed it all human knowledge, and asked it how to land on the moon, it would respond ‚ÄúYou can‚Äôt land on the moon. The moon is a god floating in the sky.‚Äù How would you get it to realize the moon is actually a big rock? That‚Äôs a great, poorly defined problem, and I don‚Äôt expect AI to solve it anytime soon.&lt;/p&gt;
    &lt;head rend="h1"&gt;SHOUTOUT TO MY GRANDMA&lt;/head&gt;
    &lt;p&gt;Here‚Äôs one last advantage of dividing intelligence into well-defined problem-solving and poorly defined problem-solving: it reminds us to give some respect where respect is due.&lt;/p&gt;
    &lt;p&gt;We‚Äôve got no problem fawning over people who are good at solving well-defined problems. They get to be called ‚Äúprofessor‚Äù and ‚Äúdoctor.‚Äù We pay them lots of money to teach us stuff. They get to join exclusive clubs like Mensa and the Prometheus Society. (By the way, Mensa‚Äôs page explaining IQ doesn‚Äôt mention anything about the dark history of using intelligence tests to hurt people, and you might expect a bunch of smarty-pantses to, you know, use their brains to discuss things with a bit more nuance. But what do I know, I‚Äôm just a big dummy.)&lt;/p&gt;
    &lt;p&gt;People who are good at solving poorly defined problems don‚Äôt get the same kind of kudos. They don‚Äôt get any special titles or clubs. There is no test they can take that will spit out a big, honking number that will make everybody respect them.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs a shame. My grandma does not know how to use the ‚Äúinput‚Äù button on her TV‚Äôs remote control, but she does know how to raise a family full of good people who love each other, how to carry on through a tragedy, and how to make the perfect pumpkin pie. We sometimes condescendingly refer to this kind of wisdom as ‚Äúfolksy‚Äù or ‚Äúhomespun,‚Äù as if answering multiple-choice questions is real intelligence, and living a good, full life is just some down-home, gee-whiz, cutesy thing that little old ladies do.&lt;/p&gt;
    &lt;p&gt;Excluding this kind of intelligence from our definitions doesn‚Äôt just hurt our grandmas‚Äîit hurts us too. If you don‚Äôt value the ability to solve poorly defined problems, you‚Äôll never get more of it. You won‚Äôt seek out people who have that ability and try to learn from them, nor will you listen to them when they have something important to say. You‚Äôll spend your whole life trying to solve problems with cleverness when what you really need is wisdom. And you‚Äôll wonder why it never really seems to work. All of your optimizing, your straining to achieve and advance, your ruthless crusade to eliminate all of the well-defined problems from your life‚Äîit doesn‚Äôt actually seem make your life any better.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre stuck trying to solve poorly defined problems with your slick, well-defined problem-solving skills and you‚Äôre lucky enough to have a grandma like mine still on this Earth, my god, go see her. Shut up and listen to her for a while. And once you‚Äôve learned something, maybe ask her if she needs help with her TV.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theseedsofscience.pub/p/why-arent-smart-people-happier"/><published>2025-11-05T16:32:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45825965</id><title>ChatGPT terms disallow its use in providing legal and medical advice to others</title><updated>2025-11-06T06:49:14.481906+00:00</updated><content>&lt;doc fingerprint="ec8cff953dd634b6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ChatGPT users can‚Äôt use service for tailored legal and medical advice, OpenAI says&lt;/head&gt;
    &lt;p&gt;Updated:&lt;/p&gt;
    &lt;p&gt;Published:&lt;/p&gt;
    &lt;head rend="h3"&gt;Here Are The 60 Best Advent Calendars For 2025 You Can Get In Canada (So Far)&lt;/head&gt;
    &lt;head rend="h3"&gt;I‚Äôve been Using This Canadian Shampoo And Conditioner For Over A Month, And It‚Äôs Totally Changed My Scalp And Hair Health&lt;/head&gt;
    &lt;head rend="h3"&gt;20 Foolproof Gifts To Order If You Want To Get Your Holiday Shopping Done Early&lt;/head&gt;
    &lt;head rend="h3"&gt;I Tried It: A Laundry Basket So Smart It Solved Our Biggest Household Argument&lt;/head&gt;
    &lt;head rend="h3"&gt;20 Things From Amazon Canada That CTV Shopping Trends Readers Loved Ordering In October&lt;/head&gt;
    &lt;head rend="h3"&gt;How To Choose The Best Vacuum Sealer For You (And A Few Of Our Favourite Models For 2025)&lt;/head&gt;
    &lt;head rend="h3"&gt;13 Budget-Friendly Beauty Products That Are Dupes Of More Expensive Items&lt;/head&gt;
    &lt;head rend="h3"&gt;12 Products For Damaged Hair That‚Äôll Help Bring Your Fried Tresses Back To Life&lt;/head&gt;
    &lt;head rend="h3"&gt;15 Of The Best Korean Beauty Skincare Finds For Fall 2025&lt;/head&gt;
    &lt;head rend="h3"&gt;27 Of The Absolute Best Last-Minute Beauty Discounts To Take Advantage Of Before The Amazon Prime Big Deal Days Sale Ends&lt;/head&gt;
    &lt;p&gt;The Shopping Trends team is independent of the journalists at CTV News. We may earn a commission when you use our links to shop. Read about us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ctvnews.ca/sci-tech/article/openai-updates-policies-so-chatgpt-wont-provide-medical-or-legal-advice/"/><published>2025-11-05T18:11:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45826266</id><title>Dillo, a multi-platform graphical web browser</title><updated>2025-11-06T06:49:13.907238+00:00</updated><content>&lt;doc fingerprint="9650cf58848088e4"&gt;
  &lt;main&gt;
    &lt;p&gt;Dillo is a multi-platform graphical web browser, known for its speed and small footprint, that is developed with a focus on personal security and privacy. It is built with the FLTK 1.3 GUI toolkit.&lt;/p&gt;
    &lt;p&gt;Screenshot of the Dillo Website rendered in Dillo:&lt;/p&gt;
    &lt;p&gt;To install Dillo follow the installation guide.&lt;/p&gt;
    &lt;p&gt;This repository contains mostly the original code of Dillo with some minor patches. Additional patches or pull requests are welcome.&lt;/p&gt;
    &lt;p&gt;See also other related forks: dillo-plus, dilloNG, D+ browser and Mobilized Dillo.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;As of December 2023, the host &lt;code&gt;dillo.org&lt;/code&gt; is no longer under control
of Dillo developers. A copy of the old website is archived in
GitHub Pages and the Wayback Machine (May 2022).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/dillo-browser/dillo"/><published>2025-11-05T18:40:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45826348</id><title>The state of SIMD in Rust in 2025</title><updated>2025-11-06T06:49:13.724581+00:00</updated><content>&lt;doc fingerprint="3f28d332981d8b8c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The state of SIMD in Rust in 2025&lt;/head&gt;
    &lt;p&gt;If you‚Äôre already familiar with SIMD, the table below is all you need.&lt;/p&gt;
    &lt;p&gt;And if you‚Äôre not, you will understand the table by the end of this article!&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs SIMD? Why SIMD?&lt;/head&gt;
    &lt;p&gt;Hardware that does arithmetic is cheap, so any CPU made this century has plenty of it. But you still only have one instruction decoding block and it is hard to get it to go fast, so the arithmetic hardware is vastly underutilized.&lt;/p&gt;
    &lt;p&gt;To get around the instruction decoding bottleneck, you can feed the CPU a batch of numbers all at once for a single arithmetic operation like addition. Hence the name: ‚Äúsingle instruction, multiple data,‚Äù or SIMD for short.&lt;/p&gt;
    &lt;p&gt;Instead of adding two numbers together, you can add two batches or ‚Äúvectors‚Äù of numbers and it takes about the same amount of time.&lt;/p&gt;
    &lt;p&gt;On recent x86 chips these batches can be up to 512 bits in size, so in theory you can get an 8x speedup for math on &lt;code&gt;u64&lt;/code&gt; or a 64x speedup on &lt;code&gt;u8&lt;/code&gt;!&lt;/p&gt;
    &lt;head rend="h2"&gt;Instruction sets&lt;/head&gt;
    &lt;p&gt;Historically, SIMD instructions were added after the CPU architecture was already designed, so SIMD is an extension with its own marketing name on each architecture.&lt;/p&gt;
    &lt;p&gt;ARM calls theirs ‚ÄúNEON‚Äù, and all 64-bit ARM CPUs have it.&lt;/p&gt;
    &lt;p&gt;WebAssembly doesn‚Äôt have a marketing department, so they just call theirs ‚ÄúWebAssembly 128-bit packed SIMD extension‚Äù.&lt;/p&gt;
    &lt;p&gt;64-bit x86 shipped with one called ‚ÄúSSE2‚Äù which has basic instructions for 128-bit vectors, but later they added a whole menagerie of extensions on top of that, with SSE 4.2 adding more operations, AVX and AVX2 adding 256-bit vectors and AVX-512 adding 512-bit vectors.&lt;/p&gt;
    &lt;p&gt;The word ‚Äúlater‚Äù in the above paragraph creates a problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does this CPU have that instruction?&lt;/head&gt;
    &lt;p&gt;If you‚Äôre running a program on an x86 CPU, it‚Äôs not a given that the CPU has any particular SIMD extension. So by default the compiler isn‚Äôt allowed to use instructions beyond SSE2 because that won‚Äôt work on all x86 CPUs.&lt;/p&gt;
    &lt;p&gt;There are two ways around this problem.&lt;/p&gt;
    &lt;p&gt;If you work for a company that only ever runs their binaries on their own servers or on a public cloud, you can just assert that they‚Äôre all recent enough to at least have AVX2 that was introduced over 10 years ago, and have the program crash or misbehave if it ever runs on anything without AVX2:&lt;/p&gt;
    &lt;code&gt;RUSTFLAGS='-C target-cpu=x86‚Äì64-v3' cargo build --release&lt;/code&gt;
    &lt;p&gt;However, if you are distributing the binaries for other people to run, that‚Äôs not really an option.&lt;/p&gt;
    &lt;p&gt;Instead you can do something called function multiversioning: compile the same function multiple times for different SIMD extensions, and when the program actually runs, check what features the CPU supports and select the appropriate version based on that.&lt;/p&gt;
    &lt;p&gt;Fortunately, this problem only exists on x86.&lt;/p&gt;
    &lt;p&gt;ARM made its NEON mandatory in all 64-bit CPUs and then didn‚Äôt bother expanding the width beyond 128 bits. (Technically SVE exists, but in 2025 it is still mostly on paper, and Rust support for it is still in progress).&lt;/p&gt;
    &lt;p&gt;WebAssembly makes you compile two different binaries, one with SIMD and one without, and use JavaScript to check if the browser supports SIMD.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solution space&lt;/head&gt;
    &lt;p&gt;There are four approaches to SIMD in Rust, in ascending order of effort:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Automatic vectorization&lt;/item&gt;
      &lt;item&gt;Fancy iterators&lt;/item&gt;
      &lt;item&gt;Portable SIMD abstractions&lt;/item&gt;
      &lt;item&gt;Raw intrinsics&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Automatic vectorization&lt;/head&gt;
    &lt;p&gt;The easiest approach to SIMD is letting the compiler do it for you.&lt;/p&gt;
    &lt;p&gt;It works surprisingly well, as long as you structure your code in a way that is amenable to vectorization. This article covers it:&lt;/p&gt;
    &lt;p&gt;You can check if it‚Äôs working with cargo-show-asm or godbolt.org, but your benchmarks are the ultimate judge of the results.&lt;/p&gt;
    &lt;p&gt;Sadly there is a limit on the complexity of the code that the compiler will vectorize, and it may change between compiler versions. If something vectorizes today that doesn‚Äôt necessarily mean it still will in a year from now.&lt;/p&gt;
    &lt;p&gt;The other drawback of this method is that the optimizer won‚Äôt even touch anything involving floats (&lt;code&gt;f32&lt;/code&gt; and &lt;code&gt;f64&lt;/code&gt; types). It‚Äôs not permitted to change any observable outputs of the program, and reordering float operations may alter the result due to precision loss. (There is a way to tell the compiler not to worry about precision loss, but it‚Äôs currently nightly-only).&lt;/p&gt;
    &lt;p&gt;So right now, if you need to process floats, autovectorization is a no-go unless you can use nightly builds of the Rust compiler.&lt;/p&gt;
    &lt;p&gt;(Floats are cursed even without SIMD. Something as simple as summing an array of them in a usable way turns out to be really hard).&lt;/p&gt;
    &lt;p&gt;There is no built-in way to multiversion functions, but the multiversion crate works great with autovectorization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fancy iterators&lt;/head&gt;
    &lt;p&gt;Just like rayon lets you run your iterators in parallel by swapping &lt;code&gt;.iter()&lt;/code&gt; with &lt;code&gt;.par_iter()&lt;/code&gt;, there have been attempts to do the same for SIMD. After all, what is SIMD but another kind of parallelism?&lt;/p&gt;
    &lt;p&gt;This is the approach that the faster crate takes. That crate has been abandoned for years, and it doesn‚Äôt look like this approach has panned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Portable SIMD abstractions&lt;/head&gt;
    &lt;p&gt;The idea is to let you write your algorithm by explicitly operating on chunks of data, something like &lt;code&gt;[f32; 8]&lt;/code&gt; but wrapped in a custom type, and then provide custom implementations of operations like &lt;code&gt;+&lt;/code&gt; that compile down into SIMD instructions.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;std::simd&lt;/code&gt; is exactly that. It supports all instruction sets LLVM supports, so its platform support is unparalleled. It pairs well with the multiversion crate. Sadly it‚Äôs nightly-only and will remain such for the foreseeable future, so it‚Äôs unusable in most situations.&lt;/p&gt;
    &lt;p&gt;The wide crate is a mature, established option. It supports NEON, WASM and all the x86 instruction sets. But it doesn‚Äôt support multiversioning at all, save for very exotic and limited approaches like cargo-multivers.&lt;/p&gt;
    &lt;p&gt;The pulp crate is a great design with built-in multiversioning, and is quite mature and complete. It powers faer, so its performance is clearly proven. The drawbacks are that it doesn‚Äôt support WASM, and that on x86 it only supports targeting AVX2 and AVX-512 but not the older extensions. But AVX2 was introduced in 2012 and in the Steam hardware survey 95% systems have it, so that might not be a big deal.&lt;/p&gt;
    &lt;p&gt;The macerator crate is a fork of pulp with vastly expanded instruction set support. It supports all x86 extensions, WASM, NEON, and even the LoongArch SIMD extensions. It‚Äôs used only by burn-ndarray, and even there it‚Äôs an optional dependency. It sounds great on paper, but it‚Äôs oddly obscure and therefore unproven. I‚Äôd probably write code using pulp, then replace it with macerator and see if everything still works and runs as fast as it should.&lt;/p&gt;
    &lt;p&gt;The fearless_simd crate is largely a copy of pulp‚Äôs design made for use in vello. It‚Äôs far less mature than pulp, but it‚Äôs under active development. As of this writing it supports NEON, WASM and SSE4.2, but not the newer x86 extensions. Seems too immature just yet, but something to keep an eye on.&lt;/p&gt;
    &lt;p&gt;simdeez is a rather old crate that supports all instruction sets except AVX-512 and comes with built-in multiversioning. What gives me pause is that despite existing for many years, it‚Äôs still barely used. Everyone else who needed SIMD built their own instead of using it. And its README says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Currently things are well fleshed out for i32, i64, f32, and f64 types.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So I guess the other types aren‚Äôt complete?&lt;/p&gt;
    &lt;p&gt;TL;DR: use &lt;code&gt;std::simd&lt;/code&gt; if you don‚Äôt mind nightly, &lt;code&gt;wide&lt;/code&gt; if you don‚Äôt need multiversioning, and otherwise &lt;code&gt;pulp&lt;/code&gt; or &lt;code&gt;macerator&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If it‚Äôs not 2025 when you‚Äôre reading this, check out &lt;code&gt;fearless_simd&lt;/code&gt;, because &lt;code&gt;std::simd&lt;/code&gt; is still in nightly in your glorious future, isn‚Äôt it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Raw intrinsics&lt;/head&gt;
    &lt;p&gt;If you want to get really close to the metal, there are always the raw intrinsics, just one step removed from the processor instructions.&lt;/p&gt;
    &lt;p&gt;The problem looming over any use of raw intrinsics is that you have to manually write them for every platform and instruction set you‚Äôre targeting. Whereas &lt;code&gt;std::simd&lt;/code&gt; or &lt;code&gt;wide&lt;/code&gt; let you write your logic once and compile it down to the assembly automatically, with intrinsics you have to write a separate implementation for every single platform and instruction set (SSE, AVX, NEON‚Ä¶) you care to support. That‚Äôs a lot of code!&lt;/p&gt;
    &lt;p&gt;It‚Äôs really not helped by the fact that they are all named something like &lt;code&gt;_mm256_srli_epi32&lt;/code&gt; and your code ends up as a long list of calls to these arcanely named functions. And wrappers that help readability introduce their own problems, such as clashes with multiversioning or unsafe code or arcane macros.&lt;/p&gt;
    &lt;p&gt;You also have to build your own multiversioning. Or rather, you have to manually dispatch to the dedicated implementation you have manually written for each instruction set. &lt;code&gt;std::is_x86_feature_detected!&lt;/code&gt; macro takes care of the feature detection, but it is somewhat slow. In some cases it is beneficial to detect available features exactly once and then cache the results, but you have to implement that manually too.&lt;/p&gt;
    &lt;p&gt;On the bright side, this year writing intrinsics got markedly less awful. Most of them are no longer &lt;code&gt;unsafe&lt;/code&gt; to call in Rust 1.86 and later, and the safe_unaligned_simd crate provides safe wrappers for the rest.&lt;/p&gt;
    &lt;p&gt;So at least this approach is no longer &lt;code&gt;unsafe&lt;/code&gt; on top of all the other problems it has!&lt;/p&gt;
    &lt;head rend="h2"&gt;Which one is right for you?&lt;/head&gt;
    &lt;p&gt;The right tool for the job ultimately depends on the use case.&lt;/p&gt;
    &lt;p&gt;Want zero dependencies and little up-front hassle? Autovectorization. Porting existing C code or targeting very specific hardware? Intrinsics. Anything else? Portable SIMD abstraction.&lt;/p&gt;
    &lt;p&gt;And now that you made it this far, you can understand the table at the top of the article, which will help guide your decision!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://shnatsel.medium.com/the-state-of-simd-in-rust-in-2025-32c263e5f53d"/><published>2025-11-05T18:45:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45826995</id><title>New gel restores dental enamel and could revolutionise tooth repair</title><updated>2025-11-06T06:49:12.285288+00:00</updated><content>&lt;doc fingerprint="29d491bb3e858cc8"&gt;
  &lt;main&gt;Tuesday, 04 November 2025&lt;div&gt;&lt;p&gt;A new material has been used to create a gel that can repair and regenerate tooth enamel, opening up new possibilities for effective and long-lasting preventive and restorative dental treatment.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Scientists from the University of Nottingham‚Äôs School of Pharmacy and Department of Chemical and Environmental Engineering, in collaboration with an international team of researchers, have developed a bioinspired material that has the potential to regenerate demineralized or eroded enamel, strengthen healthy enamel, and prevent future decay. The findings have been published today in Nature Communications.&lt;/p&gt;&lt;p&gt;The gel can be rapidly applied to teeth in the same way dentists currently apply standard fluoride treatments. However, this new protein-based gel is fluoride free and works by mimicking key features of the natural proteins that guide the growth of dental enamel in infancy. When applied, the gel creates a thin and robust layer that impregnates teeth, filling holes and cracks in them. It then functions as a scaffold that takes calcium and phosphate ions from saliva and promotes the controlled growth of new mineral in a process called epitaxial mineralization. This enables the new mineral to be organized and integrated to the underlying natural tissue while recovering both the structure and properties of natural healthy enamel.&lt;/p&gt;&lt;p&gt;The new material can also be applied on top of exposed dentine, growing an enamel-like layer on top of dentine, which has many benefits including treating hypersensitivity or enhancing the bonding of dental restorations.&lt;/p&gt;&lt;p&gt;Enamel degradation is a major contributor to tooth decay and is associated to dental problems affecting almost 50% of the world‚Äôs population. These problems can lead to infections and tooth loss, and can also be associated with conditions such as diabetes and cardiovascular disease. Enamel does not naturally regenerate; once you lose it is gone forever. There is currently no solution available that can effectively regrow enamel. Current treatments such as fluoride varnishes and remineralisation solutions only alleviate the symptoms of lost enamel.&lt;/p&gt;&lt;p&gt;Dr Abshar Hasan, a Postdoctoral Fellow and leading author of the study, said: ‚ÄúDental enamel has a unique structure, which gives enamel its remarkable properties that protect our teeth throughout life against physical, chemical, and thermal insults. When our material is applied to demineralized or eroded enamel, or exposed dentine, the material promotes the growth of crystals in an integrated and organized manner, recovering the architecture of our natural healthy enamel."&lt;/p&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt; We have tested the mechanical properties of these regenerated tissues under conditions simulating ‚Äòreal-life situations‚Äô such as tooth brushing, chewing, and exposure to acidic foods, and found that the regenerated enamel behaves just like healthy enamel. &lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt; We are very excited because the technology has been designed with the clinician and patient in mind. It is safe, can be easily and rapidly applied, and it is scalable. Also, the technology is versatile, which opens the opportunity to be translated into multiple types of products to help patients of all ages suffering from a variety of dental problems associated with loss of enamel and exposed dentine. We have started this process with our start-up company Mintech-Bio and hope to have a first product out next year; this innovation could soon be helping patients worldwide. &lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h2"&gt;Story credits&lt;/head&gt;&lt;p&gt;More information is available from Professor Alvaro Mata on Alvaro.Mata@nottingham.ac.uk&lt;/p&gt;&lt;div&gt;&lt;p&gt;Notes to editors:&lt;/p&gt;&lt;p&gt;About the University of Nottingham&lt;/p&gt;&lt;p&gt;Ranked 97 in the world and 17th in the UK by the QS World University Rankings, the University of Nottingham is a founding member of Russell Group of research-intensive universities. Studying at the University of Nottingham is a life-changing experience, and we pride ourselves on unlocking the potential of our students. We have a pioneering spirit, expressed in the vision of our founder Sir Jesse Boot, which has seen us lead the way in establishing campuses in China and Malaysia - part of a globally connected network of education, research and industrial engagement.&lt;/p&gt;&lt;p&gt;Nottingham was crowned Sports University of the Year by The Times and Sunday Times Good University Guide 2024 ‚Äì the third time it has been given the honour since 2018 ‚Äì and by the Daily Mail University Guide 2024.&lt;/p&gt;&lt;p&gt;The university is among the best universities in the UK for the strength of our research, positioned seventh for research power in the UK according to REF 2021. The birthplace of discoveries such as MRI and ibuprofen, our innovations transform lives and tackle global problems such as sustainable food supplies, ending modern slavery, developing greener transport, and reducing reliance on fossil fuels.&lt;/p&gt;&lt;p&gt;The university is a major employer and industry partner - locally and globally - and our graduates are the third most targeted by the UK's top employers, according to The Graduate Market in 2024 report by High Fliers Research.&lt;/p&gt;&lt;p&gt;We lead the Universities for Nottingham initiative, in partnership with Nottingham Trent University, a pioneering collaboration between the city‚Äôs two world-class institutions to improve levels of prosperity, opportunity, sustainability, health and wellbeing for residents in the city and region we are proud to call home.&lt;/p&gt;&lt;p&gt; More news‚Ä¶ &lt;/p&gt;&lt;/div&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nottingham.ac.uk/news/new-gel-restores-dental-enamel-and-could-revolutionise-tooth-repair"/><published>2025-11-05T19:44:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45827190</id><title>Solarpunk is happening in Africa</title><updated>2025-11-06T06:49:12.139641+00:00</updated><content/><link href="https://climatedrift.substack.com/p/why-solarpunk-is-already-happening"/><published>2025-11-05T20:00:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45827661</id><title>A Lost IBM PC/at Model? Analyzing a Newfound Old Bios</title><updated>2025-11-06T06:49:11.808632+00:00</updated><content>&lt;doc fingerprint="a7009d1325a483dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Lost IBM PC/AT Model? Analyzing a Newfound Old BIOS&lt;/head&gt;
    &lt;p&gt;Something intriguing turned up recently over at the Vintage Computer Federation Forums. Member GearTechWolf occasionally rescues and dumps random ROM chips that show up on eBay, and makes the contents available so they aren't lost to the ages. One of his hauls turned up two pairs of EPROMs labeled "IBM" in plain dot-&lt;/p&gt;
    &lt;p&gt;They came with no further identification, and no hints about their origins, or what machines they may have come from. And just to establish that proper setting of suspense and mystery, neither pair could be content-&lt;/p&gt;
    &lt;p&gt;Much poking and prying commenced. I didn't delve very deeply into the '25/05/90' odd/&lt;/p&gt;
    &lt;p&gt;The more intriguing one for me was the '1981, 1985' duo (yellow labels in the photo). A cursory look in a hex viewer revealed the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;EPROM 6448246 has the even addresses, 6448238 the odd addresses.&lt;/item&gt;
      &lt;item&gt;The internal part numbers are 6480442 and 6480441, respectively.&lt;/item&gt;
      &lt;item&gt;The BIOS date stamp in the standard location (F000:FFF5) is 03/08/85.&lt;/item&gt;
      &lt;item&gt;At F000:330A, there's yet another date stamp - 02/14/85.&lt;/item&gt;
      &lt;item&gt;The model byte (in the second-to-last position) is FCh.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In 1985, the &lt;code&gt;FCh&lt;/code&gt; model byte could only mean the 5170 (PC/&lt;/p&gt;
    &lt;p&gt;My first thought was that this may have come from one of those more shadowy members of the 5170 family: perhaps the AT/370, the 3270 AT/G(X), or the rack-mounted 7532 Industrial AT. But known examples of those carry the same firmware sets as the plain old 5170, so their BIOS extensions (if any) came in the shape of extra adapter ROMs. Whatever this thing was - some other 5170-type machine, a prototype, or even just a custom patch - it seemed I'd have to inquire within for any further clues.&lt;/p&gt;
    &lt;head rend="h3"&gt;The PC/AT BIOS: Known Versions&lt;/head&gt;
    &lt;p&gt;This was a good time to brush up on the three official revisions of the AT BIOS: how to tell them apart, and how they correspond to hardware options. The following table was compiled mostly from the pages at Minus Zero Degrees (IBM 5170 BIOS Revisions) and PC DOS Retro (IBM PC BIOS version history), and from the info in Ralf Brown's Interrupt List. Where the sources weren't in total agreement, I went with what seemed to conform with IBM's published source code listings.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;PC/AT BIOS revision&lt;/cell&gt;
        &lt;cell role="head"&gt;Rev. 1&lt;/cell&gt;
        &lt;cell role="head"&gt;Rev. 2&lt;/cell&gt;
        &lt;cell role="head"&gt;Rev. 3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Date (US format)&lt;/cell&gt;
        &lt;cell&gt;01/10/84&lt;/cell&gt;
        &lt;cell&gt;06/10/85&lt;/cell&gt;
        &lt;cell&gt;11/15/85&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P/N (internal/&lt;/cell&gt;
        &lt;cell&gt;U27, even: 6181028&lt;p&gt;U47, odd: 6181029&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;U27, even: 6480090&lt;p&gt;U47, odd: 6480091&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;U27, even: 62X0820&lt;p&gt;U47, odd: 62X0821&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;P/N (EPROM label)&lt;/cell&gt;
        &lt;cell&gt;U17/U27, even: 6181024/5&lt;p&gt;U37/&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;U27, even: 6448896&lt;p&gt;U47, odd: 6448897&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;U27, even: 61X9266&lt;p&gt;U47, odd: 61X9265&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ID bytes: Model, Submodel, Revision level&lt;/cell&gt;
        &lt;cell&gt;FCh, N/A, N/A a&lt;/cell&gt;
        &lt;cell&gt;FCh, 00h, 01h&lt;/cell&gt;
        &lt;cell&gt;FCh, 01h, 00h&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Technical Reference (with source code listing)&lt;/cell&gt;
        &lt;cell&gt;March 1984&lt;/cell&gt;
        &lt;cell&gt;September 1985&lt;/cell&gt;
        &lt;cell&gt;March 1986&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;PC/AT model and mainboard type&lt;/cell&gt;
        &lt;cell&gt;068, 099 (Type 1)&lt;/cell&gt;
        &lt;cell&gt;239 (Type 2)&lt;/cell&gt;
        &lt;cell&gt;319, 339 (Type 3)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CPU clock supported&lt;/cell&gt;
        &lt;cell&gt;6 MHz (not tested in POST)&lt;/cell&gt;
        &lt;cell&gt;6 MHz (tested in POST)&lt;/cell&gt;
        &lt;cell&gt;8 MHz (tested in POST)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Keyboards supported&lt;/cell&gt;
        &lt;cell&gt;84-key AT&lt;/cell&gt;
        &lt;cell&gt;84-key AT b&lt;/cell&gt;
        &lt;cell&gt;84-key AT&lt;p&gt;101/102-&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Floppy drive types supported&lt;/cell&gt;
        &lt;cell&gt;360 KB&lt;p&gt;1.2 MB&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;360 KB&lt;p&gt;1.2 MB&lt;/p&gt;&lt;p&gt;720 KB&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;360 KB&lt;p&gt;1.2 MB&lt;/p&gt;&lt;p&gt;720 KB&lt;/p&gt;&lt;p&gt;1.44 MB&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Hard drive types supported&lt;/cell&gt;
        &lt;cell&gt;14 (types 01-14)&lt;/cell&gt;
        &lt;cell&gt;22 (types 01-14, 16-23)&lt;/cell&gt;
        &lt;cell&gt;22 (types 01-14, 16-23)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Checks for "multiple data rate" drive controller? c&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That's the low-down on what we have to compare against. In this yet-unknown revision, both of the date stamps within the BIOS image (03/08/85 and 02/14/85) place it in-&lt;/p&gt;
    &lt;head rend="h6"&gt;Are You AT Enough?&lt;/head&gt;
    &lt;p&gt;Before I embarked on some actual reverse-&lt;/p&gt;
    &lt;p&gt;Since I don't have an actual IBM 5170, that's as close to an "AT compatibility test" as I could get, but this firmware appeared to pass muster:&lt;/p&gt;
    &lt;p&gt;Our 'rev. 1.5' BIOS makes it through POST with no issues. The IBM AT Advanced Diagnostics v2.07 disk loads up, and dutifully reports the firmware's P/N and date string; System Checkout (which lists the installed hardware) and the SETUP procedure (which is where you configure it) both run as expected.&lt;/p&gt;
    &lt;p&gt;Evidently this is some manner of PC/AT BIOS, or close enough to make Diagnostics happy. Poking at it under an emulator isn't going to tell us a whole lot more than this, however: we don't know exactly what this revision assumes about the hardware, so we can't expect to tell compatibility issues from configuration errors, or just sketchy emulation. At this juncture (if you'll excuse the imagery) we might as well roll up our sleeves and start rummaging through the entrails.&lt;/p&gt;
    &lt;head rend="h3"&gt;Findings&lt;/head&gt;
    &lt;p&gt;For more detailed notes about the disassembly itself, have a look at the repository. I'll just mention that the goal was to figure out just where and how this 'rev. 1.5' BIOS differs from the others, and this would have been much more difficult if it wasn't for two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The published source listings for all official AT BIOS versions, from the respective editions of the PC/AT Technical Reference - available on Bitsavers (and on the Internet Archive: #1, #2, #3).&lt;/item&gt;
      &lt;item&gt;The excellent reconstructions of the source code by Vernon Brooks over at PC DOS Retro, which have the listings in plain text (and they can be successfully reassembled, too).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I didn't go quite as far as trying to reconstruct a version that actually builds, but thanks to the above I believe I have things mostly figured out, so here's my analysis.&lt;/p&gt;
    &lt;head rend="h6"&gt;The Code Base&lt;/head&gt;
    &lt;p&gt;Just to get this out of the way, this is very clearly not a custom patch, or a little localized modification. Most obviously, all offsets/&lt;/p&gt;
    &lt;p&gt;In fact, with some (important) differences which will be detailed below, overall the code base appears to be something you'd expect if you were looking at a snapshot of some interim state between rev. 1 and 2. Some sections are closer to their rev. 1 counterparts, others to rev. 2; many contain elements of both, or follow the general logic of one version while still showing certain practices more common in the other.&lt;/p&gt;
    &lt;p&gt;For instance, a routine may perform essentially the same thing it did in rev. 1, but include certain optimizations which are mostly found only in rev. 2, such as immediate multi-&lt;code&gt;SHL BL, 2&lt;/code&gt; in place of two &lt;code&gt;SHL BL, 1&lt;/code&gt; instructions),3 or updating segment registers with &lt;code&gt;PUSH&lt;/code&gt;/&lt;code&gt;POP&lt;/code&gt; (instead of &lt;code&gt;MOV&lt;/code&gt;ing the value through a go-&lt;/p&gt;
    &lt;p&gt;Other routines appear more or less in the same form they have in rev. 2, while signs of the rev. 1 coding style still persist. For example, all the I/O required to access CMOS memory is done inline, as opposed to rev. 2 which calls two new routines for this purpose (&lt;code&gt;CMOS_READ&lt;/code&gt; and &lt;code&gt;CMOS_WRITE&lt;/code&gt;).  Or the encoding of jumps: two-byte (short) jumps are often found padded with a &lt;code&gt;NOP&lt;/code&gt; instruction, like in rev. 1, something that no longer happens in the later iterations.4&lt;/p&gt;
    &lt;p&gt;The more interesting parts, of course, are the sections which are unique to 'rev. 1.5' and don't have direct counterparts elsewhere, but I'll be getting to them in a bit. Those aside, the whole thing does look like an authentic version of the BIOS code, caught in some intermediate state of development between revisions 1 and 2 - including some (but not all) of the changes that later made it to the second revision, as well as a few modifications that didn't.&lt;/p&gt;
    &lt;head rend="h6"&gt;Top-Level Organization (and Build Environment)&lt;/head&gt;
    &lt;p&gt;The other editions of the AT BIOS were all generated from multiple source files. Here we only have the final image, but by comparing the overall structure against the other versions, we can deduce the breakdown into separate source modules. The arrangement of the code and data here is the same as in rev. 1, which suggests that 'rev. 1.5' was built before the restructuring that can be seen in the second revision.&lt;/p&gt;
    &lt;p&gt;Rev. 1 was apparently assembled with MASM v1.0, but rev. 2 switched to v2.0, as we're told by the page titles in IBM's source listing. If the structural overhaul was down to that change, then '1.5' was likely still built with MASM v1.0... a form of cruel and unusual punishment if there ever was one, but perhaps they had some inside scoops from Microsoft on how to deal with all the errata in that famously bug-&lt;/p&gt;
    &lt;head rend="h3"&gt;Functionality (and Hardware Support) Comparison&lt;/head&gt;
    &lt;p&gt;Now for some of the actual similarities and differences between 'rev 1.5' and its older/&lt;/p&gt;
    &lt;p&gt;So it wasn't a complete surprise to find quite a few similarities with rev. 2. For instance,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It supports 720K (3.5" DSDD) floppy disks, officially introduced only in rev. 2.&lt;/item&gt;
      &lt;item&gt;21 different hard drive types are available: 01-14 and 16-22, just one short of rev. 2 (and 3), which add drive type 23. Only types 01-14 were recognized in rev. 1 (15 is always reserved).&lt;/item&gt;
      &lt;item&gt;It implements INT 15h function C0h ("Get System Configuration"), which didn't exist in rev. 1, but was present in rev. 2 (and in all later PC compatible BIOSes).&lt;/item&gt;
      &lt;item&gt;Keyboard support is more or less the same as in rev. 2: only the 84-key AT keyboard is (fully) supported, but some code for the 101/102-key Enhanced Keyboard is already present. The hardware IRQ handler (INT 09h) attempts to detect it, and uses its expanded scan code tables, but the enhanced INT 16h services are not available.5&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In certain other respects, however, there's more in common with the first revision:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It doesn't attempt to verify the 286's clock speed, a test that was added to POST in revisions 2 and 3 (at checkpoint 11h).67&lt;/item&gt;
      &lt;item&gt;When testing the floppy/&lt;wbr&gt;hard drive controller ("combo card" in IBM-&lt;/wbr&gt;&lt;wbr&gt;speak), it won't throw up a "601 diskette error" if it cannot find the "multiple data rate capability" indication bit. What this means in practice is that more third-party controllers should be supported.&lt;/wbr&gt;&lt;/item&gt;
      &lt;item&gt;POST checkpoint codes 02 and 03 mean the same things as in rev. 1 (respectively, these tests verify the CMOS Shutdown Byte and the BIOS ROM checksum). The later revisions swap these two tests around.6&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then you've got those peculiar sections where 'rev. 1.5' does its own thing entirely. The most significant ones handle RAM testing and parity errors: this also provides our biggest clue about just what sort of AT this firmware came from, so I'll expand on this down below.&lt;/p&gt;
    &lt;p&gt;For now, just a few notes about some of the above:&lt;/p&gt;
    &lt;head rend="h6"&gt;The System Configuration Table&lt;/head&gt;
    &lt;p&gt;This is where we find the machine ID bytes: model, sub-&lt;/p&gt;
    &lt;p&gt;The odd part is that it returns sub-&lt;/p&gt;
    &lt;quote&gt;F000:E6F5 08 00 CONF_TBL dw 8 ; LENGTH OF FOLLOWING TABLE F000:E6F7 FC db MODEL_BYTE ; SYSTEM MODEL BYTE F000:E6F8 01 db SUB_MODEL_BYTE ; SYSTEM SUB MODEL TYPE BYTE F000:E6F8 ; [* 1, like rev. 3 (0 in rev. 2) *] F000:E6F9 00 db BIOS_LEVEL ; BIOS REVISION LEVEL F000:E6F9 ; [* 0, like rev. 3 (1 in rev. 2) *] F000:E6FA 70 db 1110000b ; 10000000 = DMA CHANNEL 3 USE BY BIOS F000:E6FA ; 01000000 = CASCADED INTERRUPT LEVEL 2 F000:E6FA ; 00100000 = REAL TIME CLOCK AVAILABLE F000:E6FA ; 00010000 = KEYBOARD SCAN CODE HOOK 1AH&lt;/quote&gt;
    &lt;head rend="h6"&gt;Floppy Drive Support&lt;/head&gt;
    &lt;p&gt;This is one area where the 'rev 1.5' code base diverges from all other revisions, but the actual functionality doesn't seem to be too different from rev. 2. It looks more as if 3.5" 720 KB media support was shoehorned into the rev. 1 code (which only handled 1.2 MB and 360 KB disks and drives). This somewhat over-&lt;/p&gt;
    &lt;p&gt;Rev. 2 refactored and simplified the floppy code, in part by implementing what the comments call a "new architecture", along with two routines (&lt;code&gt;XLAT_NEW&lt;/code&gt; and &lt;code&gt;XLAT_OLD&lt;/code&gt;) which convert such data fields to a new internal format when entering a BIOS function, then back to the old format on exit.  The floppy code in 'rev 1.5' is therefore noticeably messy compared to the other revisions, and in places it seems to use certain "reserved" state bits for temporary purposes which I haven't fully grokked yet (see &lt;code&gt;set_dskstate_*&lt;/code&gt; in the disassembly).&lt;/p&gt;
    &lt;p&gt;Still, the interface already has the familiar form it would retain later. For instance, INT 13h function 17h (Set Disk Type for Format), named &lt;code&gt;FORMAT_SET&lt;/code&gt; in the code (address F000:&lt;/p&gt;
    &lt;p&gt;Function 08h (Get Drive Parameters), which in rev. 1 was only available for hard drives, works for floppies here. It returns the same data in the same registers as rev. 2 and onward, although it's executed differently. The data is populated from a table at F000:&lt;/p&gt;
    &lt;quote&gt;F000:EF62 00 F0 d720_seg dw 0F000h F000:EF64 C7 EF d720_off dw offset DISK_BASE ; [* 720k: ptr to DISK_BASE *] F000:EF66 09 d720_spt db 9 ; [* 720K: sectors/track *] F000:EF67 4F 00 d720_mxt dw 79 ; [* 720K: max tracks *] F000:EF69 01 d720_mxh db 1 ; [* 720K: max heads *]&lt;/quote&gt;
    &lt;p&gt;720 KB floppy support might just be another innovation of this firmware. The first PC-&lt;/p&gt;
    &lt;head rend="h3"&gt;It's All About that Base RAM&lt;/head&gt;
    &lt;p&gt;At last, the interesting part: what this BIOS does about memory - and how this appears to hint at a machine that isn't quite your garden-&lt;/p&gt;
    &lt;head rend="h6"&gt;System Board RAM (and the Keyboard Controller)&lt;/head&gt;
    &lt;p&gt;A bit of background to keep in mind here: through the AT's lifetime, IBM never saw fit to release a model with room for more than 512K of RAM on the system board, unlike the XT (and the XT Model 286). A 128K expansion board can be used to bring a 512K system up to the 640K "base RAM" (AKA conventional RAM) limit.&lt;/p&gt;
    &lt;p&gt;Now, most of the AT's configuration options are kept in CMOS memory, but a couple of things still have to be set the old way - as with the PC and XT, via motherboard switches and jumpers. These settings can be read from the 8042 keyboard controller's input port (by sending C0h to port 64h, then reading port 60h).&lt;/p&gt;
    &lt;p&gt;During the POST procedure, the 5170 BIOS reads these switch settings and stores them in the BIOS Data Area at address 0040:0012 - a byte that was previously unused, except on the PCjr. The AT BIOS listings label this byte &lt;code&gt;MFG_TST&lt;/code&gt;, although the manufacturing test jumper status is just one of the bits used.&lt;/p&gt;
    &lt;p&gt;One of these settings (determined by jumper J18) specifies the amount of RAM on the system board. Type 2 and type 3 AT motherboards come with the full complement of 512 KB; on a Type 1 board, either 256 or 512 KB may be populated, so on these early 5170s this setting can take either value.&lt;/p&gt;
    &lt;p&gt;In the data byte obtained from the 8042 input port, that's what bit 4 indicates. Seems simple enough: as far as the official documentation is concerned, this is the only jumper or switch setting that has anything to do with the amount of on-&lt;/p&gt;
    &lt;p&gt;But hold on: in the definition table, you will also notice a bit 3. In all three versions of the PC/AT Technical Reference, bit 3 is marked "undefined" or "reserved".&lt;/p&gt;
    &lt;head rend="h6"&gt;The "Base Planar Memory Extension"&lt;/head&gt;
    &lt;p&gt;The BIOS code listings include IBM's comments for all symbolic constants, most of them at the very start (&lt;code&gt;POSTEQU.INC&lt;/code&gt; in the reconstructed source files). There, the sources for revisions 2 and 3 (but not for rev. 1!) sneak one more entry into the list of bits in the keyboard controller's input port: 10&lt;/p&gt;
    &lt;code&gt;
               C  ;--------- 8042 INPUT PORT BIT DEFINITION SAVED IN @MFG_TST --------------------
 = 0008        C  BASE_MEM8       EQU     00001000B       ; BASE PLANAR R/W MEMORY EXTENSION 640/X 
 = 0010        C  BASE_MEM        EQU     00010000B       ; BASE PLANAR R/W MEMORY SIZE 256/512
 = 0020        C  MFG_LOOP        EQU     00100000B       ; LOOP POST JUMPER BIT FOP MANUFACTURING
 = 0040        C  DSP_JMP         EQU     01000000B       ; DISPLAY TYPE SWITCH JUMPER BIT
 = 0080        C  KEY_BD_INHIB    EQU     10000000B       ; KEYBOARD INHIBIT SWITCH BIT
&lt;/code&gt;
    &lt;p&gt;This &lt;code&gt;BASE_MEM8&lt;/code&gt; would be our "reserved" bit 3.  Note how it's described: "base planar R/W memory extension 640&lt;/p&gt;
    &lt;p&gt;Anyway, the POST process does just what that block comment says on the tin: around checkpoint 11, it reads the switch settings, which were temporarily stored in the &lt;code&gt;DMA_PAGE+1&lt;/code&gt; register a bit earlier.  Then it strips off the unneeded bits, and saves the result to the &lt;code&gt;@MFG_TST&lt;/code&gt; byte in the BIOS Data Area.&lt;/p&gt;
    &lt;p&gt;BIOS revisions 2 and 3, which know about bit 3 (as &lt;code&gt;BASE_MEM8&lt;/code&gt;), take care to preserve it - again, unlike rev. 1:&lt;/p&gt;
    &lt;quote&gt;;----- GET THE INPUT BUFFER (SWITCH SETTINGS) 05CB E4 82 IN AL,DMA_PAGE+1 ; GET THE SWITCH SETTINGS 05CD 24 F8 AND AL,KEY_BD_INHIB+DSP_JMP+MFG_LOOP+BASE_MEM+BASE_MEM8 ; STRIP BITS 05CF A2 12 00 MOV @MFG_TST,AL ; SAVE SETTINGS&lt;/quote&gt;
    &lt;p&gt;"But what do they actually do with this bit once they've read it?", you ask. To channel Trade Master Greenish, "that's a good question, with a very interesting answer": they do precisely nothing whatsoever with it at any point. Whether in the POST process or elsewhere, this piece of information is consulted a grand total of zero times.&lt;/p&gt;
    &lt;p&gt;I should mention that I couldn't find any unofficial explanation of this bit, either. None of the usual references and books have anything better to say about it than "reserved" or "undefined", and that includes such ne plus ultra sources as Ralf Brown, or The Undocumented PC by Frank van Gilluwe. A most curious state of affairs.&lt;/p&gt;
    &lt;p&gt;So what could "base planar memory extension 640&lt;/p&gt;
    &lt;head rend="h3"&gt;The 640/X Factor&lt;/head&gt;
    &lt;p&gt;At this point you can likely guess where this is going: the 'rev 1.5' AT BIOS does pay attention to this undocumented '640/X' bit. It's checked in a couple of places, but always in the same context, and it's a rather enlightening one: RAM parity checking.&lt;/p&gt;
    &lt;head rend="h6"&gt;A Perfunctory Parity Primer&lt;/head&gt;
    &lt;p&gt;For every byte of memory in the IBM PC-&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RAM on the system board triggers "Parity Check", which sets bit 7 of port 61h (if enabled by setting bit 2).&lt;/item&gt;
      &lt;item&gt;RAM expansion cards trigger "I/O Channel Check", which sets bit 6 of port 61h (if enabled by setting bit 3).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These commonly-&lt;/p&gt;
    &lt;head rend="h6"&gt;Many NMIs Bring Much Honor&lt;/head&gt;
    &lt;p&gt;When the NMI service routine is invoked, it tries to determine what has roused it from its slumber. If the cause was a parity error, it will display the message "PARITY CHECK 1" (if the source was on-&lt;/p&gt;
    &lt;p&gt;Interestingly, this test doesn't try to match the specific type of parity error that raised the NMI: for each 64K region, it decides which of the two parity check signals it's going to look for. Those switch settings in &lt;code&gt;MFG_TST&lt;/code&gt; play into this choice, so we can get some insight by looking at the logic. In all versions of the AT BIOS, the relevant code is in &lt;code&gt;NMI_INT_1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For comparison, the Rev. 1 BIOS does it as follows. The diagram is somewhat simplified, but this is the general logic:&lt;/p&gt;
    &lt;p&gt;That is, below 256K it always watches for on-&lt;/p&gt;
    &lt;p&gt;In revisions 2 and 3, the NMI handler's memory test is less revealing: no matter the RAM address, it always watches for both types of parity error, and treats either one as a good enough reproduction. Likely, the designers decided that being picky about it wasn't worth the extra code, since the test already disregards the type of error which caused the NMI to fire in the first place.&lt;/p&gt;
    &lt;p&gt;'Rev. 1.5', however, still insists on being pedantic. Which is fortunate for us, because the change in logic from rev. 1 is very instructive:&lt;/p&gt;
    &lt;p&gt;The crucial difference is in the region above 512K, where the '640/X' flag (AKA &lt;code&gt;BASE_MEM8&lt;/code&gt;) comes in. When the machine has 512K on the system board and the '640/X' bit is set, the test in revision "1.5" will expect on-board parity errors in this block of RAM.&lt;/p&gt;
    &lt;p&gt;If following a bunch of arrows around isn't your idea of a good time, this format may be easier on the eyes:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Region&lt;/cell&gt;
        &lt;cell role="head"&gt;Rev. 1 checks for&lt;/cell&gt;
        &lt;cell role="head"&gt;Rev. '1.5' checks for&lt;/cell&gt;
        &lt;cell role="head"&gt;Rev. 2, 3 check for&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;0K&lt;/cell&gt;
        &lt;cell&gt;On-board RAM error&lt;/cell&gt;
        &lt;cell&gt;On-board RAM error&lt;/cell&gt;
        &lt;cell&gt;(Any parity error)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;256K&lt;/cell&gt;
        &lt;cell&gt;[256K system]: I/O channel error&lt;p&gt;[512K system]: On-board RAM error&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;[256K+'640/X']: I/O channel error&lt;p&gt;[otherwise]: On-board RAM error&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;(Any parity error)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;512K&lt;/cell&gt;
        &lt;cell&gt;I/O channel error&lt;/cell&gt;
        &lt;cell&gt;[512K+'640/X']: On-board RAM error&lt;p&gt;[otherwise]: I/O channel error&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;(Any parity error)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;"Yeah, yeah, get to the point": what does this mean, then?&lt;/p&gt;
    &lt;p&gt;Well, let's imagine that the matching motherboard had something like an extra 128K bank of RAM, plus a switch or a jumper to indicate that it was populated. If the '640/X' bit reflected the state of that switch, this would all make sense - and so would the "base planar" terminology from IBM's later listings.&lt;/p&gt;
    &lt;p&gt;What doesn't make quite as much sense is the behavior of the "256K on board" setting: whether it's used in tandem with the '640/&lt;/p&gt;
    &lt;head rend="h6"&gt;The POST Memory Test Loop... and the &amp;gt;1MB Oddity&lt;/head&gt;
    &lt;p&gt;There's another place where the 'rev. 1.5' BIOS refers to the status of the '640/X' flag: the cold-&lt;/p&gt;
    &lt;p&gt;For conventional memory, when determining the type of parity check to expect for each range of addresses, the cold-&lt;/p&gt;
    &lt;p&gt;In rev. 1, all non-&lt;/p&gt;
    &lt;p&gt;But for some reason, 'rev. 1.5' here appears to reserve special treat&lt;/p&gt;
    &lt;quote&gt;F000:0FFC E21_C1M: ; [* 1MB boundary *] F000:0FFC C6 06 64 00 10 mov byte ptr ds:DS_TEMP+BASE_HI_BYTE, 16 F000:1001 C6 06 4C 00 10 mov byte ptr ds:ES_TEMP+BASE_HI_BYTE, 16 F000:1006 B0 40 mov al, IO_CHECK ; [* I/O Check mask (&amp;gt;1M on exp. card) *] F000:1008 E6 87 out DMA_PAGE+6, al ; [* temporary storage *] F000:100A 1E push ds F000:100B F000:100B ; [* this rev. only: get hardware configuration again *] F000:100B F000:100B B8 18 00 mov ax, RSDA_PTR ; [* system data area for POST *] F000:100E 8E D8 mov ds, ax F000:1010 A0 12 00 mov al, ds:@MFG_TST ; [* get mfg test config *] F000:1013 1F pop ds F000:1014 24 18 and al, BASE_MEM+BASE_MEM8 ; [* Planar RAM configuration bits: *] F000:1016 3C 10 cmp al, BASE_MEM ; [* bit 4 (512k planar) ONLY? *] F000:1018 75 04 jnz short E21_C1M5 ; [* no: keep I/O Check mask *] F000:101A B0 80 mov al, PARITY_CHECK ; [* yes: use Parity check mask (planar) *] F000:101C E6 87 out DMA_PAGE+6, al ; [* and save to temporary storage *] F000:101E F000:101E ; [* this rev. only: check for 1.5MB boundary (24*64K)? *] F000:101E F000:101E E21_C1M5: F000:101E 80 3E 64 00 18 cmp byte ptr ds:DS_TEMP+BASE_HI_BYTE, 24 F000:1023 72 04 jb short NEXT1 ; [* continue if below 1.5MB *] F000:1025 B0 40 mov al, IO_CHECK ; [* reset to I/O Check mask above 1.5MB *] F000:1027 E6 87 out DMA_PAGE+6, al ; [* temporary storage *]&lt;/quote&gt;
    &lt;p&gt;The logic here seems to be this: if the 1--&lt;/p&gt;
    &lt;p&gt;What that could mean is anybody's guess. But if we take this at face value, then our theoretical motherboard may have had two selectable configurations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;640K on board (the usual 512K plus 128K extra), which would then fill up conventional memory to the limit; OR,&lt;/item&gt;
      &lt;item&gt;1024K on board, set up as 512K base plus 512K extended, with the latter mapped between 1 and 1.5 MB.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the first case, you'd set the hypothetical switch or jumper one way, causing the '640/X' bit (AKA &lt;code&gt;BASE_MEM8&lt;/code&gt;) to be set.  In the second case, the switch goes the other way, which would clear it.  With a full meg on board, the logic implies that the 128K expansion board can still be used, bringing your base RAM up to 640K.&lt;/p&gt;
    &lt;head rend="h6"&gt;The PCB Real-Estate Question&lt;/head&gt;
    &lt;p&gt;One may wonder where a megabyte of RAM might go on the 5170 mainboard (or on some plausible variant of it). But that's not too far fetched if we assume 256 kbit DRAM chips, like the Type 2 and 3 AT boards.&lt;/p&gt;
    &lt;p&gt;In fact, looking at those later mainboards, the layout around that single 512K RAM bank seems rather cozy and spacious, which sort of stands out next to the cramped organization of the rest of the board. Those two rows of chips are flanked on both sides by curiously empty space - coincidentally, it looks like there's just enough room there to double the chip count with a minimal change in design.&lt;/p&gt;
    &lt;p&gt;If we roll with this observation, we can just about arrive at a scenario where the first redesign of the 5170 mainboard - corresponding to our 'rev. 1.5' AT BIOS on the timeline - could have accommodated as much as 1024K of memory. For reasons of their own, the powers that be at IBM decided not to pursue this as a finished product, and the next AT models to hit the market (with the Type 2 mainboard, and the rev. 2 BIOS) were stripped down to one bank of 512K, leaving all that board real-&lt;/p&gt;
    &lt;p&gt;Since the 'rev. 1.5' BIOS code seems to imply a selectable cofiguration of either 1 MB or 640 KB, could such a RAM subsystem support either setup at the flip of a switch? Supposing one 16-&lt;/p&gt;
    &lt;head rend="h3"&gt;Skyrocket: The Real Deal?&lt;/head&gt;
    &lt;p&gt;Conjectures are fun and all, but can this hypothetical PC/AT variant be identified with anything like an actual real-&lt;/p&gt;
    &lt;p&gt;The thread in question is 'OT: "Skyrocket" the AT that never was', posted to comp.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Sorting through the piles a bit I've (re)discovered a bit of ancient AT history -- the machine that had the internal codename "Skyrocket". Looks just like a normal AT with the primary difference being there is 640K on the planar rather than 512K as shipped in all the retail models.&lt;/p&gt;
      &lt;p&gt;How did I happen across this rather unique piece of history? Well, I was in the right place at the right time when the Boca Raton site was being shutdown and thrown to the wind. IBM was firesaleing off all sorts of gear to the employees and I'd bought a stack of AT's for $10/&lt;/p&gt;
      &lt;wbr&gt;each. On closer examination, one of the stack turned out to be the rare Skyrocket...&lt;/wbr&gt;
    &lt;/quote&gt;
    &lt;p&gt;Further along the thread, we get more details:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Skyrocket's planar is traditional "big" AT style (not the shortie 339 or XT-286 style), but uses normal DIP's not the old Mostek/&lt;/p&gt;
      &lt;wbr&gt;TI DIL's (positioned on the usual place the double deckers would be on the planar though). It's got a two rows of 41256 and two of 4164 type DIP.&lt;/wbr&gt;
      &lt;p&gt;I believe Skyrocket probably predates XT-286, and may have been concurrent with 339 at some point. When the project was shelved, some number of advanced prototype had been produced and were distributed about the site on an IUO basis - I had one on my desk in 86' during OS/2 1.0 development (real 339's were rare and all being shipped out to customers&lt;/p&gt;
      &lt;g&gt;). Where the "AT" badge would be is a metalic emblem shooting star logo, which leads me to believe it was fairly well along when it was shot down...&lt;/g&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;My sense is that the Skyrocket machine was tanked because the 339 planar was ready, cost less to manufacture, and with PS/2 ready to be unveiled, it just wasn't worth the bother to get the extra 128K onto the planar for a machine that would have a distinctly limited lifespan.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That DRAM layout is just as predicted above for the 640 KB option: one Type 2/3-&lt;/p&gt;
    &lt;head rend="h6"&gt;...Or is it?&lt;/head&gt;
    &lt;p&gt;Now, I'm not completely convinced beyond a shadow of a doubt that this is what we have here. Given what I can make of this BIOS, the particulars of this cancelled AT prototype do fit... but not exactly like a glove.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The most obvious smoking gun is of course the 640K system board, but there's also that alternative 1 MB option implied by the cold-&lt;/p&gt;
        &lt;wbr&gt;boot RAM test loop, which Tony doesn't mention.&lt;/wbr&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;He makes it sound like "Skyrocket" came along fairly late in the 5170's lifespan, possibly concurrent with the Model 339/&lt;/p&gt;
        &lt;wbr&gt;Type 3 AT, and not all that long before the launch of the PS/2. But the date stamps in this 'rev. 1.5' BIOS (as well as the code!) date it to before the Type 2 boards.&lt;/wbr&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A "traditional big AT style" planar (which I take to mean a 'Type 1' form factor) does fit the time frame; but an 8 MHz CPU clock wouldn't be my first guess with that sort of thing, since even the Type 2 still limped along at 6 MHz.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But on the other hand:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The option to switch between 640 KB and 1 MB configurations may be technically supported in the 'rev. 1.5' BIOS, but perhaps the mechanism to do this (along with the extra address decoding logic) was simply never implemented in the prototype system board... or maybe there just weren't enough 256 kbit DRAM parts to go around, and all the "Skyrocket" ATs ended up with a fixed 640 KB on board.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ROM date stamps never correlate very well with actual availability anyway, and we have to keep in mind that the 5170 had less than 3 years of official lifetime as a product. Within that period, I could easily imagine various projects going on simultaneously, with all sorts of fun bureaucratic delays to ensure maximum confusion.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I don't see anything in the 'rev. 1.5' BIOS code to suggest that it can't be an 8 MHz system. In fact nothing seems to indicate any particular clock rate, because it's missing the CPU speed test which was added in rev. 2. There are a few instances of speed-&lt;/p&gt;
        &lt;wbr&gt;dependent "busy wait" loops scattered throughout the code, but the counter values used for those weren't even consistent between the two 6 MHz BIOS editions, so they tell us very little.&lt;/wbr&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since there's nothing to absolutely preclude this BIOS from being the "Skyrocket" firmware, and the '640/X' business with RAM capacity sure seems to be a good match, I'll invoke Occam's trusty razor and say it's probably "Skyrocket"... or at least something very closely related to it.&lt;/p&gt;
    &lt;p&gt;With that talk about "a metallic emblem shooting star logo" in place of the "AT" one, I couldn't resist making this little mock-&lt;/p&gt;
    &lt;p&gt;Whichever 5170 variant this BIOS came from, it wasn't a released product, but it makes sense for a prototype that made it as far as "internal use" at IBM: the EPROMs have part numbers (on printed labels and in the data), which seems to hint that this project got to a respectable stage in the development cycle before they canned it.&lt;/p&gt;
    &lt;p&gt;I'm given to understand that Tony Ingenoso is regrettably no longer with us, so we won't be able to verify whether the BIOS ROMs in his "Skyrocket" are the same as this 'rev. 1.5'. But maybe someone else out there will be able to shed some more light here.&lt;/p&gt;
    &lt;p&gt;Amusingly, it's a lucky thing that this firmware still retains rev. 1's pickiness about specific parity error types - it could have been less fussy about it (like the later revisions), and our only clue that there was anything funny about the RAM setup would have gone down the chute. But it didn't, and now we can also explain that little riddle from the rev. 2 and 3 BIOS listings, where they mention the undocumented "640/X" bit: evidently someone done goofed, and simply forgot to clean up the source files.&lt;/p&gt;
    &lt;p&gt;"Skyrocket" or not, this was a diverting little game of connect-&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The same model byte was used by other IBM systems, namely the XT-286 and the 7552 "Gearbox" industrial computer, but these only arrived in late 1986. See the model byte table from Ralf Brown's Interrupt List. [‚Üë]&lt;/item&gt;
      &lt;item&gt;This was done to support certain very early, "badly behaved" software, which accessed BIOS functions and structures by calling absolute addresses instead of using the interrupt services - some authors had the PC mixed up with an Apple II, apparently. IBM explains this in the comments for the 'compatibility' section, for instance in the second PC AT Technical Reference (Sep. 1985), p.5-182 (which does a great job conveying the intended tone of disapproval and contempt).&lt;lb/&gt;This practice was carried on by third-&lt;wbr&gt;party BIOS vendors: see ROM Address Compatibility Table in System BIOS for IBM PC/XT/AT Computers and Compatibles (Phoenix Technologies, Ltd., 1989), p.58. [‚Üë]&lt;/wbr&gt;&lt;/item&gt;
      &lt;item&gt;Immediate multi-bit shifts and rotates were a new feature of the 80286 (or more accurately, of the 80186, but IBM skipped that one). The rev. 1 AT BIOS tends to stick to 8088-&lt;wbr&gt;compatible instruction forms - this likely has to do with MASM 1.0, which didn't support any 286 instructions whatsoever, so when they do appear they're implemented using macros. [‚Üë]&lt;/wbr&gt;&lt;/item&gt;
      &lt;item&gt;This too is an artifact of early MASM versions, which used a two-&lt;wbr&gt;pass assembly process. In the first pass, symbols haven't been resolved yet, and jumps within a segment are encoded as near (3-byte) jumps. The second pass applies the resolved addresses, and if the target is within range, it'll go with the short (2-byte) form. That would shift things around, but the two-pass process can't deal with offsets being changed at this point, so the third byte is simply replaced with a&lt;/wbr&gt;&lt;code&gt;NOP&lt;/code&gt;(90h).&lt;lb/&gt;To get around this you can explicitly specify "JMP SHORT", and the first pass will use the two-byte form. Of course, this gets you an error if the target isn't within the short jump range, but IBM evidently did the legwork for the second AT BIOS revision. For more about this (and other ancient MASM quirks), see this writeup at OS/2 Museum. [‚Üë]&lt;/item&gt;
      &lt;item&gt;The Type 2 AT Technical Reference (Sep. 1985) doesn't mention the Enhanced Keyboard at all, other than the hints for this rudimentary support in the BIOS listing - where the comments refer to it only as "KBX". Perhaps the notion of a "keyboard X" gave off the right mixture of suspense and enigma, but the eagle-&lt;wbr&gt;eyed would have noticed references to keys like F11 and F12.&lt;/wbr&gt;&lt;lb/&gt;The Enhanced Keyboard was officially introduced with the 7531/7532 Industrial Computer, which used the 5170 AT BIOS. I'm not sure which revision(s), but the 7531/7532 Technical Reference from July '85 includes the source for rev. 1, even though it fully describes the new keyboard. Makes no sense to me... but St. Augustine would have said, "if you understand it, it is not IBM". [‚Üë]&lt;/item&gt;
      &lt;item&gt;For the sequencing and meaning of POST checkpoint codes on the 5170, see the IBM 5170 - POST Codes page at Minus Zero Degrees. [‚Üë]&lt;/item&gt;
      &lt;item&gt;Infamously, the speed test prevented owners of the 6-MHz Model 239 AT (with its rev. 2 BIOS) from overclocking it by replacing the clock oscillator, which was often done with earlier ATs. The common conspiracy theory says that IBM did this purely out of greed, since they didn't want a "too-fast" 5170 to bite into the sales figures of more expensive systems (which?), or of the "official" faster ATs they were planning to introduce shortly.&lt;lb/&gt;I'm not sure I buy that. The fact is that the 286 CPUs supplied with these models were only rated for 6 MHz, and the stability of the rest of the system wasn't guaranteed beyond that either, since the bus ran off the same clock. The AT had already caused PR problems for IBM, with those early CMI hard drives going teats-up en masse, and I suppose they weren't going to risk any more reliability issues if they could help it. [‚Üë]&lt;/item&gt;
      &lt;item&gt;The original IBM JX BIOS could only handle 40 tracks, so the total usable capacity was 360K; the earliest JX BIOS that could handle the full 720K has a 1986 date code. See "System Specifications" on the IBM JX Information Page. [‚Üë]&lt;/item&gt;
      &lt;item&gt;Keyboard Controller: Input Port Bit Definitions, The IBM Personal Computer AT Technical Reference. Rev. 1 (Mar. 1984): p.1-44; rev. 2 (Sep. 1985): p.1-55; rev. 3 (Mar. 1986): p.1-55 [‚Üë]&lt;/item&gt;
      &lt;item&gt;The IBM Personal Computer AT Technical Reference. Rev. 2 (Sep. 1985): p.5-32; rev. 3 (Mar. 1986), p.5-22 [‚Üë]&lt;/item&gt;
      &lt;item&gt;CMOS RAM Configuration Information, The IBM Personal Computer AT Technical Reference rev. 1 (Mar. 1984), p.1-54 [‚Üë]&lt;/item&gt;
      &lt;item&gt;For more about the ISA parity checking/&lt;wbr&gt;NMI mechanism in the AT architechture (and later), see Non-&lt;/wbr&gt;&lt;wbr&gt;Maskable Interrupt Requests&lt;/wbr&gt;, ISA System Architecture: Third Edition (Mindshare, Inc. 1995), pp. 399-401. [‚Üë]&lt;/item&gt;
      &lt;item&gt;Intel's 8-bit Above Board (1984), for one, allowed either 64 kbit or 256 kbit chips in all sockets; the type used was indicated by setting a DIP switch. Further description is at DOS Days.&lt;lb/&gt;The 5170's 16-bit memory (and bus) architecture is pondered in more detail by GloriousCow of MartyPC fame, in his article Exploring 16-bit Bus Access on the PC/AT. [‚Üë]&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://int10h.org/blog/2025/11/lost-ibm-at-model-bios-analysis/"/><published>2025-11-05T20:40:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45829210</id><title>The Basic Laws of Human Stupidity (1987) [pdf]</title><updated>2025-11-06T06:49:10.865091+00:00</updated><content/><link href="https://gandalf.fee.urv.cat/professors/AntonioQuesada/Curs1920/Cipolla_laws.pdf"/><published>2025-11-05T22:58:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45830223</id><title>Recursive macros in C, demystified (once the ugly crying stops)</title><updated>2025-11-06T06:49:10.524997+00:00</updated><content>&lt;doc fingerprint="2ec9b45278e499bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Recursive macros in C, demystified (once the ugly crying stops üò≠)&lt;/head&gt;
    &lt;p&gt;In which it becomes clear, the C Preprocessor was designed by &lt;del&gt;a&lt;/del&gt; Kafka &lt;del&gt;fan&lt;/del&gt;&lt;/p&gt;
    &lt;p&gt;So you have heard rumors whispered between peers, that a rare few people somehow manage to make compile-time recursion work in C? And you want to have some insight into how that might be possible??&lt;/p&gt;
    &lt;p&gt;I should warn you, you‚Äôre risking your sanity‚Ä¶ but I‚Äôll indulge you.&lt;/p&gt;
    &lt;p&gt;Wait, did I really just say that? I must be a glutton for punishment, because the macro system is, by far, the thing I like least about C.&lt;/p&gt;
    &lt;p&gt;C has many advantages that have led to its longevity (60 years as perhaps the most important language). It has many quirks due to its age, most of which are easy to look past. But despite 30 years writing C, I still bristle at C‚Äôs macro system.&lt;/p&gt;
    &lt;p&gt;That‚Äôs not just because there are many languages (including C++) with more modern takes on compile-time execution. Macros appear simple, but have subtleties that make them poorly suited for anything other than light wrappers.&lt;/p&gt;
    &lt;p&gt;Still, being C‚Äôs only compile-time execution capability (currently), it is still both critical and important. Critical, in that many venerable critical systems heavily depend on them, and wouldn‚Äôt compile without them. Important, in that it‚Äôs often the only way to abstract out complexity that would lead to safety or security issues if exposed, such as automatically adding sentinels or static type checks.&lt;/p&gt;
    &lt;p&gt;C macros being hard to use does discourage their overuse. It‚Äôs easy for too much abstraction to make it too difficult for other people to maintain the code, so in some ways, as painful as they are, I can find some things to appreciate, and do occasionally find reason to use them for something non-trivial.&lt;/p&gt;
    &lt;p&gt;But it doesn‚Äôt take much for a macro to be non-trivial, because, while C macros can look like functions, they cannot be called recursively (at least, not easily, as we will see).&lt;/p&gt;
    &lt;p&gt;I have never been able to find out why recursion is limited in C macros. It could have started off intentional, but compile time execution wasn‚Äôt really on people‚Äôs minds then; the challenge was abstracting over many platform differences as cheaply as possible.&lt;/p&gt;
    &lt;p&gt;I suspect the system evolved as needed in the early days, without really thinking about it as something that perhaps should support recursion. Certainly at some point, the question would get raised; but it‚Äôs easy to imagine:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The organic evolution of the macro system coupled with early success made it brittle, and hard to evolve.&lt;/item&gt;
      &lt;item&gt;People were worried about build issues like hanging compile times due to infinite loops, or crashing with no diagnostics, due to infinite recursion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Depending on which of these two was more prominent, I could equally imagine the lack of recursion being an accident, or being an intentional choice.&lt;/p&gt;
    &lt;p&gt;However it happened, that die was cast in a completely different era‚Äîpeople have definitely woken up to the value of pushing as much into compile time as possible.&lt;/p&gt;
    &lt;p&gt;Either way, it all seems archaic and unnecessary now.&lt;/p&gt;
    &lt;p&gt;So, let‚Äôs roll up our sleeves and learn to cope with the issue!&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;If you have anything interesting you need done with the preprocessor, you probably need to generalize over multiple items.&lt;/p&gt;
    &lt;p&gt;Maybe you want to add automatic sanity checks around parameters, or add automatic type checking. You might want to pre-fill arrays, or generate a list of functions based on some data. Generally, we should be able to do such things at compile time, and in cases like type checking, it is often incredibly challenging to defer the work till runtime. So the lack of a good compile time solution for such things is problematic.&lt;/p&gt;
    &lt;p&gt;I tend to reach for macros when they can remove the potential for human error; for example, calling an API wrong. That can indeed be adding automatic casts, enforcing that null terminators get added on variable argument arrays, etc.&lt;/p&gt;
    &lt;p&gt;In all of those scenarios, we would need something to move macros toward Turing completeness at compile time. But macros do not advertise support for either of the things we‚Äôd be looking for there: iteration, and recursion.&lt;/p&gt;
    &lt;p&gt;Thankfully, we can get there. But it‚Äôs not going to be easy.&lt;/p&gt;
    &lt;p&gt;To frame our discussion, let‚Äôs pick a simple, but highly valuable goal. We‚Äôre going to build a macro that counts the number of variable arguments in a function-like macro that accepts variable arguments.&lt;/p&gt;
    &lt;p&gt;Why that problem? Because from there it‚Äôs a short jump to dealing with some common issues, where we can remove large sources of human error:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Variadic functions (aka varargs) are error prone, because the implementation has to figure out where the arguments stop; the language doesn‚Äôt give you a way to know. The caller has to remember to follow your convention (like adding a null terminator). And that convention can often back fire, for example, when a null value is a valid argument . Being able to count the number of variadic arguments allows us to provide a single, general approach to dealing with this, and to not put the burden on the caller to have to count correctly.&lt;/item&gt;
      &lt;item&gt;There are plenty of cases where we‚Äôd want to apply a transformation to each argument of a variable argument function, like statically checking that parameters are all the same type, or automatically adding a layer of sanity checking to a third party API, so that the people calling your function can remain blissfully unaware. These don‚Äôt directly require counting, but once we can count recursively, it‚Äôs a small change to give ourselves a more general purpose &lt;code&gt;map&lt;/code&gt;construct to make such transformations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You might be surprised that the language doesn‚Äôt provide a way to count variable arguments in a macro, especially if you noticed the recent addition of a pre-defined macro called &lt;code&gt;__COUNTER__&lt;/code&gt; in the draft C2Y standard. The new &lt;code&gt;__COUNTER__&lt;/code&gt; macro is intended to make it easier to provide uniqueness for situations where macros need to generate identifiers and labels; it definitely isn‚Äôt for counting variadic macro arguments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Apparently math is hard?&lt;/head&gt;
    &lt;p&gt;If there‚Äôs no primitive to count variadic macro arguments, well, we need to create one, right? And if we have to create one, that means it should be pretty easy, one would hope?&lt;/p&gt;
    &lt;head rend="h1"&gt;ü¶óü¶óü¶ó&lt;/head&gt;
    &lt;p&gt;I see you‚Äôre skeptical. But an optimist who wanted to delve into compile-time coding in C for the first time might give it a go. But they will quickly find that the obvious approach below, that feels like it should work, absolutely does not work:&lt;/p&gt;
    &lt;code&gt;// The first macro... counts one argument, then we'd like it to recurse.
#define _COUNT_ONE(x, ...) + 1 _COUNT_TOP(__VA_ARGS__)
#define _COUNT_TOP(...)    __VA_OPT__(_COUNT_ONE(__VA_ARGS__))
#define COUNT(...)        (_COUNT_TOP(__VA_ARGS__) + 0)
&lt;/code&gt;
    &lt;p&gt;Try to call this code, and‚Ä¶&lt;/p&gt;
    &lt;head rend="h1"&gt;ü§Æ&lt;/head&gt;
    &lt;p&gt;Yup, it‚Äôll barf.&lt;/p&gt;
    &lt;p&gt;What this doe-eyed attempt is trying to do, is generate an expression (at compile time) of the form &lt;code&gt;(+ 1 + 1 + ‚Ä¶ + 0)&lt;/code&gt;, by iterating over each argument at a call site (via recursion). It doesn‚Äôt care what the arguments are, it just wants to generate a &lt;code&gt;+ 1&lt;/code&gt; for each valid argument, and a &lt;code&gt;+ 0&lt;/code&gt; at the end, both to make it a valid expression and to handle the case where there are no arguments passed.&lt;/p&gt;
    &lt;p&gt;If we‚Äôre successful, the addition will all happen at compile time, and will be what C calls an integer constant expression. That means, the compiler will, at compile time, fold this into a single static integer. So, even if it feels inefficient, there is no run-time cost involved.&lt;/p&gt;
    &lt;p&gt;Why three macros? That seems a bit excessive, right?&lt;/p&gt;
    &lt;p&gt;Unfortunately, C currently doesn‚Äôt have an easy way to do the equivalent of an &lt;code&gt;if()&lt;/code&gt; statement at compile time. It‚Äôs possible to create something with macros, but that‚Äôs just extra hackery.&lt;/p&gt;
    &lt;p&gt;Instead, we split the primary body into its own macro‚Äî &lt;code&gt;_COUNT_ONE()&lt;/code&gt;, which adds the &lt;code&gt;1 +&lt;/code&gt; and then triggers recursion (we wish, anyway; again, the recursion part won‚Äôt work this easily).&lt;/p&gt;
    &lt;p&gt;The intent the behind &lt;code&gt;_COUNT_TOP()&lt;/code&gt; macro is evaluating an exit condition for our recursion. Specifically, we want to stop when we have no more arguments left in the function. The builtin macro &lt;code&gt;__VA_OPT__()&lt;/code&gt; allows us to do exactly that‚Äî the text inside the parentheses gets expanded only if there are arguments. And when there are no arguments, the text inside the parentheses is discarded.&lt;/p&gt;
    &lt;p&gt;This gives us a lightweight way to separate the one argument case from the two argument case, without need for a kind of &lt;code&gt;if&lt;/code&gt; statement; the 0-case won‚Äôt generate a recursive call. To combine this with &lt;code&gt;_COUNT_ONE()&lt;/code&gt;, we‚Äôd need a primitive that allowed us to specify a replacement that only expands when there are no arguments, which doesn‚Äôt come out of the box.&lt;/p&gt;
    &lt;p&gt;The outer &lt;code&gt;COUNT()&lt;/code&gt; macro could be factored out trivially; I leave it because it keeps what‚Äôs going on a bit clearer. This macro is the actual entry point, adds the ‚Äò0‚Äô a single time the end, and wraps the whole thing in parentheses, which helps avoid running afoul of operator precedence rules.&lt;/p&gt;
    &lt;p&gt;If we directly combine it with &lt;code&gt;_COUNT_TOP()&lt;/code&gt; in the obvious way, it would compute the right thing, but it would do it by adding the &lt;code&gt;+ 0&lt;/code&gt; after EVERY term, and parenthesizing the expression in a way that would come off as odd if you were looking at the resulting C code. For instance, we‚Äôd be aiming for a three argument function to generate:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;( + 1 ( + 1 ( + 1 ( + 0 ) + 0 ) + 0 ) + 0 )&lt;/code&gt;
If we were to generate this code, it‚Äôd be ugly, but most people would declare success and move on. But unfortunately, the above code does not work, and will never work, no matter how many iterations of the standard are released between now and the heat death of the universe‚Äî changing the behavior would be too likely to impact plenty of real code.&lt;/p&gt;
    &lt;p&gt;For example, let‚Äôs attempt to use the above implementation like so:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int
main()
{
    printf("COUNT() = %d\n", COUNT(1, 2, 3));
    return 0;
}
&lt;/code&gt;
    &lt;p&gt;With &lt;code&gt;clang&lt;/code&gt;, I get:&lt;/p&gt;
    &lt;code&gt;tmp.c:8:30: error: expected ')'
    8 |     printf("COUNT() = %d\n", COUNT(1, 2, 3));
      |                              ^
tmp.c:4:28: note: expanded from macro 'COUNT'
    4 | #define COUNT(...)        (_COUNT_TOP(__VA_ARGS__) + 0)
      |                            ^
tmp.c:3:39: note: expanded from macro '_COUNT_TOP'
    3 | #define _COUNT_TOP(...)    __VA_OPT__(_COUNT_ONE(__VA_ARGS__))
      |                                       ^
tmp.c:2:32: note: expanded from macro '_COUNT_ONE'
    2 | #define _COUNT_ONE(x, ...) + 1 _COUNT_TOP(__VA_ARGS__)
      |                                ^
tmp.c:39:30: note: to match this '('
tmp.c:4:27: note: expanded from macro 'COUNT'
    4 | #define COUNT(...)        (_COUNT_TOP(__VA_ARGS__) + 0)
&lt;/code&gt;
    &lt;p&gt;Wow, that‚Äôs a lot of error messages, saying little that makes sense.&lt;/p&gt;
    &lt;p&gt;I‚Äôm sure you can already get the feeling that, when you have a problem with your macros, it‚Äôs incredibly challenging to translate the resulting errors into what‚Äôs actually wrong. Here, it‚Äôs complaining about balancing parentheses, and with just a bit more complexity in our macros, it‚Äôd be incredibly easy for someone to spend 10 minutes trying to figure out where the parenthesis is missing, when no parenthesis is missing whatsoever.&lt;/p&gt;
    &lt;p&gt;Or, we could have taken advantage of the fact that C is perfectly happy to accept &lt;code&gt;+ + 0&lt;/code&gt;, and write &lt;code&gt;1 +&lt;/code&gt; instead of &lt;code&gt;+ 1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Making that microscopic change, merely transposing two tokens, completely changes the error &lt;code&gt;clang&lt;/code&gt; produces:&lt;/p&gt;
    &lt;code&gt;tmp.c:8:30: error: call to undeclared function '_COUNT_TOP'; ISO C99 and later do not support implicit function declarations [-Wimplicit-function-declaration]
   8 |     printf("COUNT() = %d\n", COUNT(1, 2, 3));
&lt;/code&gt;
    &lt;p&gt;Hey, at least that message is concise. Never mind that it‚Äôs totally different, and also unrelated to the real issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Not counting on it&lt;/head&gt;
    &lt;p&gt;The C Preprocessor (which I will usually call CPP) is responsible for macro expansion and processing lines with a leading &lt;code&gt;#&lt;/code&gt;. As we‚Äôve said, it does not fully support recursion. As you might expect, that‚Äôs the core of the actual problem in our first attempt. Yet, the preprocessor happily thinks it did its job. We‚Äôll see in more detail what‚Äôs going on, but the crux of this particular problem is how recursion is disallowed, not that it IS disallowed.&lt;/p&gt;
    &lt;p&gt;The problem here is that C macros are their own programming language, being used to generate C code. The macro language doesn‚Äôt model most of the interesting parts of the language, and it is quite easy to produce code that the preprocessor finds acceptable, that the compiler cannot understand (as we will see).&lt;/p&gt;
    &lt;p&gt;In both these cases, the preprocessor feels like it‚Äôs done its job, and passes off its work to the C compiler. The C compiler gets the generated code, and has no idea that macros were used. It calls the error as it sees it.&lt;/p&gt;
    &lt;p&gt;This disconnect between the preprocessor and the compiler is one of the things that makes macros in C so unfriendly.&lt;/p&gt;
    &lt;p&gt;If a macro expansion (basically the same as an ‚Äòevaluation‚Äô) is recursive, the CPP decides ‚Äúthey can‚Äôt possibly have wanted recursion here, because that might loop forever, so this must be plain old text I have to substitute‚Äù. As a result, the emitted code will still contain the unexpanded macro.&lt;/p&gt;
    &lt;p&gt;How can we confirm this? If we can‚Äôt understand the resulting transformations, we‚Äôre going to end up stumbling around in the dark.&lt;/p&gt;
    &lt;p&gt;Many developers don‚Äôt know how to see what the C preprocessor actually produces. If the preprocessor successfully exits, we can see it‚Äôs output by stopping the compiler after the preprocessing phase, generally with the &lt;code&gt;-E&lt;/code&gt; flag. If we don‚Äôt give a file name (via the &lt;code&gt;-o&lt;/code&gt; flag), we should see the results on the terminal. And at least in the case of &lt;code&gt;clang&lt;/code&gt;, we will even get output up to the point that we did something so wrong that the preprocessor gives us an error.&lt;/p&gt;
    &lt;p&gt;For the code above, running &lt;code&gt;cc -E tmp.c&lt;/code&gt; works without errors, and dumps the output of CPP to my terminal.&lt;/p&gt;
    &lt;p&gt;That consists of a lot of stuff you might not expect to see. The output contains our code after the preprocessor has fully expanded it. But that full expansion includes the results of it preprocessing all of the header files we pulled in, which in our case was &lt;code&gt;stdio.h&lt;/code&gt;, and any cascading dependencies it might have.&lt;/p&gt;
    &lt;p&gt;However, our code is easy to find in that noise. The last thing output will be our fully translated &lt;code&gt;main()&lt;/code&gt; function, ready to be input into the C compiler. For the case where we add &lt;code&gt;+1&lt;/code&gt; at the beginning of the macro, we will see:&lt;/p&gt;
    &lt;code&gt;int
main()
{
    printf("COUNT() = %d\n", (+1 _COUNT_TOP(2, 3) + 0));
    return 0;
}
&lt;/code&gt;
    &lt;p&gt;Here, the compiler doesn‚Äôt have to try to look up the symbol &lt;code&gt;_COUNT_TOP&lt;/code&gt;; it knows that it doesn‚Äôt make sense to have a function call after a number with no operator in between.&lt;/p&gt;
    &lt;p&gt;When we reverse the &lt;code&gt;+&lt;/code&gt; and the &lt;code&gt;1&lt;/code&gt;, the line of code is valid, as long as there‚Äôs a function C can resolve called &lt;code&gt;_COUNT_TOP&lt;/code&gt;. Because there isn‚Äôt, the compiler bails.&lt;/p&gt;
    &lt;p&gt;That explains why we get two different errors, for such a minor change.&lt;/p&gt;
    &lt;p&gt;Because CPP and the compiler itself are oblivious to the execution of the other, and because we have to live with the fact that recursive macros aren‚Äôt errors to the CPP (silently passing them through unexpanded), it‚Äôs quite a bit of work for any compiler to even try to tell you that your problem here is attempting to use recursion in a macro. It could be done, and maybe it should be done, because otherwise, compilers are effectively trying to gaslight you into believing you have a syntax error of some sort.&lt;/p&gt;
    &lt;p&gt;Because the preprocessor is much more permissive than the compiler, without the compiler having any awareness of macros even existing is perhaps the most significant reason why writing non-trivial macros is so hard to do.&lt;/p&gt;
    &lt;p&gt;Again, we can hack our way around the recursion problem. The semantics are arcane and intricate enough that, even knowing the rules (and doing macro work with a copy of the standard at the ready), macro development is incredibly challenging the second you have any problem at all. Decades later, I often feel like I‚Äôm stumbling around in the dark when a macro I write blows up on me.&lt;/p&gt;
    &lt;p&gt;If this is all too intimidating, absolutely we can plagiarize our way to success. Though, personally, I really prefer not to cut-and-paste code from Stack Overflow, especially if it‚Äôs code I don‚Äôt understand. Similarly, while Claude and I are casual acquaintances, I do not trust his code. It‚Äôs my unwillingness to using code I don‚Äôt understand in production that keeps me learning and growing. Instead, I avoid non-trivial macros, unless (as I said above), I make an exception when they will be a huge net positive for helping the developer, usually by removing potential failure modes, or with significant clarity improvements. Meaning, if you can use them to provide an abstraction that makes the code more robust, and is also not going to be hard to maintain if the need arises, then I‚Äôd consider it (even if you have to get Claude to write it). Here, automating size detection statically feels like a good enough use case for my tastes, because macros will not only make variadic functions easier to write, but also make it far easier to call them correctly.&lt;/p&gt;
    &lt;p&gt;So, I‚Äôd like to help those interested understand how to navigate through the pain, and shine a light on it, in the hopes that this is another area of the language that the modern standards committee can make massively better.&lt;/p&gt;
    &lt;head rend="h2"&gt;That doesn‚Äôt count&lt;/head&gt;
    &lt;p&gt;If we want to know how to circumvent the recursion restriction, we probably need to understand the detection mechanism we are attempting to evade.&lt;/p&gt;
    &lt;p&gt;It sure would be nice if we could debug by having our compiler give us intermediate expansions, up until the point that it breaks. This is not directly built into any compiler as far as I know. And my experience with macro debuggers has been that they have a hard time matching compiler semantics. I dusted one of them off when working on this article, and it was easy to get it to expand macros as valid that CPP barfed on, and vice versa. Despite the lack of tooling, I‚Äôll walk through the expansion process in detail so we can all understand.&lt;/p&gt;
    &lt;p&gt;The rules for C macro evaluation are hard to explain in a way that‚Äôs simultaneously precise and clear. But for function-like macros, the main process of evaluating a macro boils down to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Replace the macro text. Stashing aside the arguments used to call the macro, we replace the full macro with the textual body, within the larger token stream we‚Äôre processing.&lt;/item&gt;
      &lt;item&gt;Add placeholder tokens. Instances of ‚Äòarguments‚Äô in the body get replaced with placeholder tokens, to prevent them from being evaluated as macros in any nested argument expansion we might have. This includes &lt;code&gt;__VA_ARGS__&lt;/code&gt;and&lt;code&gt;__VA_OPT__()&lt;/code&gt;invocations.&lt;/item&gt;
      &lt;item&gt;Evaluate preprocessor operators in the body that take operands , particularly &lt;code&gt;#&lt;/code&gt;and&lt;code&gt;##&lt;/code&gt;. We don‚Äôt make good use of these operators in this article, so we won‚Äôt cover in too much depth. Note, however, that&lt;code&gt;__VA_OPT__()&lt;/code&gt;is also a preprocessor operator that takes arguments. We inhibited expansion within the replacement text, when we were evaluating operators, but at the end of this phase, we put it back; it can get expanded in the next step.&lt;/item&gt;
      &lt;item&gt;Rescanning the body. The body is then scanned for more macros to expand, starting from our cursor in the token stream. The ‚Äúinput‚Äù head moves forward a token at a time, until we mind a macro to expand, or reach the end of the macro we‚Äôre evaluating. When we find a macro to expand, we recursively apply the algorithm.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are some pretty large subtleties here. As long as a macro invocation starts in the scope of a rescan, the scanning head position can move past the end of the original macro. For example, consider this basic scenario:&lt;/p&gt;
    &lt;code&gt;#define CONCAT(X, Y)  X ## Y

int PRINT_INT = 100;

int
main()
{
    printf("%d\n", CONCAT(PRINT_, INT));
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;##&lt;/code&gt; operator, seen used in the &lt;code&gt;CONCAT&lt;/code&gt; macro, appends two tokens together, turning them into a single preprocessor token.&lt;/p&gt;
    &lt;p&gt;When the preprocessor evaluates &lt;code&gt;CONCAT()&lt;/code&gt;, it will result in the token &lt;code&gt;PRINT_INT&lt;/code&gt;. If &lt;code&gt;PRINT_INT&lt;/code&gt; were a macro, the preprocessor would do further expansion. But it is not, so the preprocessor outputs &lt;code&gt;PRINT_INT&lt;/code&gt;. The C compiler does not complain, because it sees a variable named &lt;code&gt;PRINT_INT&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So far, depending on your background, the semantics may or may not being intuitive. But either way, what do you think should happen in this slightly more complex scenario?&lt;/p&gt;
    &lt;code&gt;#define CONCAT(X, Y)  X ## Y

#define PRINT_INT(N)  printf("look an integer %d\n", (N));

int PRINT_INT = 100;

int
main()
{
    printf("%d\n", CONCAT(PRINT_, INT));
    CONCAT(PRINT_,INT)(100);
}
&lt;/code&gt;
    &lt;p&gt;It would be reasonable to think there‚Äôs an error here, but this code will compile and run. Why? In both cases, the preprocessor will generate the token &lt;code&gt;PRINT_INT&lt;/code&gt;. In the first case, everything will happen the same way it did in our first example, and the compiler will see the variable &lt;code&gt;PRINT_INT&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;But, with the second use of &lt;code&gt;CONCAT&lt;/code&gt;, the preprocessor will see that there is a parenthesis immediately following the token &lt;code&gt;PRINT_INT&lt;/code&gt;. Since it has a function-like macro with that name, it will prefer the macro interpretation.&lt;/p&gt;
    &lt;p&gt;That‚Äôs true, even though we didn‚Äôt directly write &lt;code&gt;PRINT_INT&lt;/code&gt; in the code, there. The effective result of the preprocessor‚Äôs expansion would look like this:&lt;/p&gt;
    &lt;code&gt;#define CONCAT(X, Y)  X ## Y

#define PRINT_INT(N)  printf("look an integer %d\n", (N));

int PRINT_INT = 100;

int
main()
{
    printf("%d\n", 100);
    printf("look an integer %d\n", 100);
}
&lt;/code&gt;
    &lt;p&gt;That‚Äôs because C‚Äôs macros come in two flavors, with slightly different semantics:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Function-like macros, which take arguments, and syntactically LOOK like functions. As we see here, if the preprocessor sees a token with the same name as a function like macro, but it‚Äôs not used like a function like macro, it will pass it through, letting the C compiler resolve the token.&lt;/item&gt;
      &lt;item&gt;Object-like macros, the definitions of which look like variables, and do not take arguments. If the C preprocessor sees a left parenthesis after an object-like macro, that token will just be passed through directly to the compiler.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That is why the following code does NOT error. In fact, it runs quite happily:&lt;/p&gt;
    &lt;code&gt;#define OBJECT_LIKE_MACRO printf
#include &amp;lt;stdio.h&amp;gt;
int main()
{
  OBJECT_LIKE_MACRO("Hello, world!\n");
}
&lt;/code&gt;
    &lt;p&gt;Meaning, if we we have an object-like macro and it looks like we‚Äôre trying to call it, the C preprocessor isn‚Äôt going to call it. It‚Äôs just going to pass the token through, and let the compiler figure it out.&lt;/p&gt;
    &lt;p&gt;As you can see, the preprocessor‚Äôs philosophy is to do its job, and nothing else.&lt;/p&gt;
    &lt;p&gt;Another preprocessor subtlety that‚Äôs easy to miss, yet important to understand is:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;‚ÑπÔ∏èÔ∏è&lt;/cell&gt;
        &lt;cell&gt;Arguments to function-like macros are expanded at the call site. For expansions that trigger inside the body where those arguments are used, the contents of the expanded arguments will not be available to be part of any additional expansion.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The preprocessor‚Äôs approach of substituting arguments with placeholder tokens during evaluation is pretty effective at stopping a bunch of accidental recursion that would be non-intuitive. Although, it‚Äôs not the problem for the recursion we‚Äôre trying to solve. The barrier we‚Äôre hitting is a subtlety that we haven‚Äôt discussed yet, but we‚Äôll get to it soon enough.&lt;/p&gt;
    &lt;p&gt;Before that, let‚Äôs solidify our understanding by walking through the relevant steps with our invocation of &lt;code&gt;COUNT(1)&lt;/code&gt; .&lt;/p&gt;
    &lt;p&gt;We‚Äôve learned that, when &lt;code&gt;COUNT()&lt;/code&gt; has an invocation of &lt;code&gt;_COUNT_TOP()&lt;/code&gt;, the replacement text cannot lead to recursion. The expansion endures the rescan, and nothing attempts to expand it.&lt;/p&gt;
    &lt;p&gt;The rule that‚Äôs preventing the expansion is effectively an explicit anti-recursion rule. Formally, when we replace a macro, that macro is marked as currently being replaced. The mark stays, until that replacement is totally finished, including its rescan.&lt;/p&gt;
    &lt;p&gt;And, unfortunately, marked macros are ineligible for replacement. Note the rescan ineligibility is IN THE CALLING ENVIRONMENT. That‚Äôs going to cause us some grief in a few minutes.&lt;/p&gt;
    &lt;p&gt;For whatever reason, the original C89 standards committee referred to a macro marked as ineligible for expansion as painted blue.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;‚ùì&lt;/cell&gt;
        &lt;cell&gt;Why the term painted blue? Perhaps the standards committee at the time realized how miserable it was going to make future C developers? It‚Äôs not a term mentioned in the standard, but is often used when people try to explain the whole process.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;I‚Äôm blue, because I can‚Äôt count on you&lt;/head&gt;
    &lt;p&gt;We can work around the problem, despite there being no way in C to opt out of your macro getting painted blue. But the work-around is going to be hard fought.&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs give ourselves a cause to be optimistic: the restriction that‚Äôs preventing &lt;code&gt;_COUNT_TOP()&lt;/code&gt; from recursively expanding is relaxed when the second &lt;code&gt;_COUNT_TOP()&lt;/code&gt; gets replaced. If, instead, the restriction stayed in full force the entire time we‚Äôve evaluating &lt;code&gt;COUNT()&lt;/code&gt;, then you would not be able to do the following:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#define H4X(x) # x // convert to string
#define DUPE(token) H4X(token) H4X(token)

int 
main() {
    printf("%s\n", DUPE(h4x0r));
}
&lt;/code&gt;
    &lt;p&gt;But that will absolutely work. The preprocessor will output exactly what I intended:&lt;/p&gt;
    &lt;code&gt;int
main()
{
    printf("%s\n", "h4x0r" "h4x0r");
}
&lt;/code&gt;
    &lt;p&gt;In C, two string literals next to each other are merged into a single literal at compile time, so this program prints:&lt;/p&gt;
    &lt;code&gt;h4x0rh4x0r
&lt;/code&gt;
    &lt;p&gt;This leads us to believe that the blue paint wears off when the rescan moves past an expanded macro, after we‚Äôve processed its replacement.&lt;/p&gt;
    &lt;p&gt;So our hypothesis right now might be that &lt;code&gt;_COUNT_ONE()&lt;/code&gt; expanding inside &lt;code&gt;COUNT()&lt;/code&gt; is fine, as long as it happens after the processing head moves past the start of where the macro was, after expansion.&lt;/p&gt;
    &lt;p&gt;If that‚Äôs the case, then all we need to do now is add another layer of indirection, right? Let the calling macro evaluate its recursive call on rescan!&lt;/p&gt;
    &lt;p&gt;That would make sense! We‚Äôll rewrite our attempt at recursion to add a proxy layer to call &lt;code&gt;_COUNT_TOP()&lt;/code&gt;, knowing it cannot re-expand.&lt;/p&gt;
    &lt;code&gt;#define _COUNT_ONE(x, ...) + 1 _COUNT_TOP(__VA_ARGS__)
#define _COUNT_TOP(...)    __VA_OPT__(_COUNT_ONE(__VA_ARGS__))
#define _COUNT_PROXY(...)  (_COUNT_TOP(__VA_ARGS__) + 0)
#define COUNT(...)         _COUNT_PROXY(__VA_ARGS__)
&lt;/code&gt;
    &lt;p&gt;We‚Äôd now expect the replacement when our evaluation gets back up to &lt;code&gt;COUNT(1, 2, 3)&lt;/code&gt; to look like:&lt;/p&gt;
    &lt;code&gt;(+1 _COUNT_TOP(2, 3) + 0))
&lt;/code&gt;
    &lt;p&gt;So now, we‚Äôre thinking that escaped the paint, and &lt;code&gt;_COUNT_TOP()&lt;/code&gt; will further expand, right? ü¶óü¶óü¶ó&lt;/p&gt;
    &lt;p&gt;It‚Äôs time to shatter our youthful optimism with the bitter pill of experience. Yes, we‚Äôve added an extra indirection, but here‚Äôs what it expands to:&lt;/p&gt;
    &lt;code&gt;(+1 _COUNT_TOP(2, 3) + 0))
&lt;/code&gt;
    &lt;head rend="h2"&gt;ü§Ø&lt;/head&gt;
    &lt;p&gt;That‚Äôs the same as our intermediate expansion, but nothing further was done to the macro! I thought we made our way past the paint?&lt;/p&gt;
    &lt;p&gt;Clearly, there are subtleties to the rules somewhere. Clearly, the author is a jerk, and must have intentionally failed to mention it above.&lt;/p&gt;
    &lt;p&gt;It‚Äôs true I am a jerk, and it‚Äôs also true that I failed to mention the restriction that‚Äôs biting us right now. That‚Äôs because it‚Äôs a part of the journey‚Äî no tutorial or explanation I‚Äôve ever seen made it clear to me that restrictions on expanded macros can survive past the call site.&lt;/p&gt;
    &lt;p&gt;So maybe I‚Äôm just obtuse, and torturing others because I once suffered long ago. Let‚Äôs go look at the relevant text in the C23 standard, shall we? It‚Äôll either be enlightening‚Ä¶ or further torture.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;‚ÑπÔ∏èÔ∏è&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;After all parameters in the replacement list have been substituted and # and ## processing has taken place, all place-marker preprocessing tokens are removed. The resulting preprocessing token sequence is then rescanned, along with all subsequent preprocessing tokens of the source file, for more macro names to replace.&lt;/p&gt;
          &lt;p&gt;If the name of the macro being replaced is found during this scan of the replacement list (not including the rest of the source file‚Äôs preprocessing tokens), it is not replaced. Furthermore, if any nested replacements encounter the name of the macro being replaced, it is not replaced. These non-replaced macro name preprocessing tokens are no longer available for further replacement even if they are later (re)examined in contexts in which that macro name preprocessing token would otherwise have been replaced.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The sentence I bolded, I think we clearly understood; I did explain it above. The two sentences afterward, which I put in italics, is where the restriction that‚Äôs hurting us is specified.&lt;/p&gt;
    &lt;p&gt;For years, I took that to mean, ‚ÄúEven at the top level where the macro was called, no matter how many times subsequent replacement text gets rescanned, the called macro is still ineligible for expansion.‚Äù&lt;/p&gt;
    &lt;p&gt;What am I doing wrong? Why does the blue paint leave and then come back? This makes no sense. Right? Right?&lt;/p&gt;
    &lt;head rend="h2"&gt;üò≠üò≠&lt;/head&gt;
    &lt;p&gt;(I warned you, there‚Äôd be ugly crying. Better me than you though; I clearly deserve it).&lt;/p&gt;
    &lt;head rend="h2"&gt;Close n-counter&lt;/head&gt;
    &lt;p&gt;Perhaps it would be obvious to most readers that I had misinterpreted the standard. Perhaps, but even once I finally realized that my original interpretation couldn‚Äôt possibly be right, I still feel the above text from the standard is bit under-specified.&lt;/p&gt;
    &lt;p&gt;Specifically, what are the boundaries for ‚Äúnested replacement‚Äù? Clearly it‚Äôs not the case that once a macro is called, a parent of the calling macro can never replace it again. So does it mean, ‚Äúthe resulting text can never, in any way be involved in an expansion which produces an expansion ever again?‚Äù requiring full taint tracking of the replacement through all future transformations as long as the text could possibly be rescanned, no matter what?&lt;/p&gt;
    &lt;p&gt;Or, does it only apply to any text with the name of the function we called, and the second that changes, it could change back?&lt;/p&gt;
    &lt;p&gt;Or maybe there are some different semantics?&lt;/p&gt;
    &lt;p&gt;Let‚Äôs roll up our sleeves, with another little experiment, to help us determine how we should interpret the above test. What we‚Äôd like to see, is, can &lt;code&gt;_COUNT_ONE()&lt;/code&gt; produce a macro invocation with a different name, that we don‚Äôt try to expand until after leaving the context in which it and &lt;code&gt;_COUNT_TOP()&lt;/code&gt; are painted, and then somehow replace that macro with &lt;code&gt;_COUNT_TOP()&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;If we can make that happen, then we‚Äôll test to see if we can call the resulting function is callable. Let‚Äôs junk our previous experiment, and go back to where we were before:&lt;/p&gt;
    &lt;code&gt;#define _COUNT_ONE(x, ...) + 1 _COUNT_TOP(__VA_ARGS__)
#define _COUNT_TOP(...)    __VA_OPT__(_COUNT_ONE(__VA_ARGS__))
#define COUNT(...)        (_COUNT_TOP(__VA_ARGS__) + 0)
&lt;/code&gt;
    &lt;p&gt;Now, we want to try to change the value of &lt;code&gt;_COUNT_TOP&lt;/code&gt; to something else to escape detection. It‚Äôs got to be a valid macro, but one that we‚Äôre NOT going to end up expanding when rescanning &lt;code&gt;_COUNT_ONE()&lt;/code&gt; or &lt;code&gt;_COUNT_TOP()&lt;/code&gt;. If we call it &lt;code&gt;_COUNT_INDIRECT&lt;/code&gt;, we don‚Äôt want &lt;code&gt;_COUNT_INDIRECT(2,3)&lt;/code&gt; to evaluate until we pop all the way back up into &lt;code&gt;_COUNT()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Sounds like a tall order, but there‚Äôs are a couple of facts we‚Äôve already learned, that can help us:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We know that, if we have a function-like macro named &lt;code&gt;X()&lt;/code&gt;, the preprocessor does not consider a bare&lt;code&gt;X&lt;/code&gt;with no parenthesis next to it to be an invocation of&lt;code&gt;X&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Rescans start at the replacement site, they don‚Äôt back up tokens.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So perhaps we can separate the function name and the arguments for a while, and somehow bring them together in a way where we could rescan it inside &lt;code&gt;COUNT()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Having the function name separate from an argument list will both keep it from running, and will help us avoid a rescan the first time we get the left parenthesis to plop down in the right place.&lt;/p&gt;
    &lt;p&gt;We just need a way to add a spacer of some sort that goes away at rescan, to keep apart the name &lt;code&gt;_COUNT_ONE&lt;/code&gt; from its argument list. That is, we‚Äôd conceptually like to do:&lt;/p&gt;
    &lt;code&gt;#define _COUNT_ONE(x, ...) \
   + 1 __VA_OPT__(_COUNT_ONE &amp;lt;&amp;lt;SPACER&amp;gt;&amp;gt; (__VA_ARGS__))
&lt;/code&gt;
    &lt;p&gt;But we‚Äôre going to change the name of &lt;code&gt;_COUNT_ONE&lt;/code&gt; to &lt;code&gt;_COUNT_INDRECT&lt;/code&gt;. If we just do:&lt;/p&gt;
    &lt;code&gt;#define _COUNT_INDIRECT _COUNT_ONE
&lt;/code&gt;
    &lt;p&gt;Then we‚Äôre going to re-generate &lt;code&gt;_COUNT_ONE&lt;/code&gt; on the rescan, which is painted blue.&lt;/p&gt;
    &lt;p&gt;So it‚Äôs actually &lt;code&gt;_COUNT_INDIRECT&lt;/code&gt; where we currently have the dire need to postpone evaluating it.&lt;/p&gt;
    &lt;p&gt;Therefore, we need to turn &lt;code&gt;_COUNT_INDIRECT&lt;/code&gt; into a function, and keep THAT identifier separated from the parentheses that trigger it, via our to-be-written spacer. So this is the definition we want instead:&lt;/p&gt;
    &lt;code&gt;#define _COUNT_INDIRECT() _COUNT_ONE
&lt;/code&gt;
    &lt;p&gt;We‚Äôll also need to add the empty parameter list to invoke it on the other side of our spacer. So here‚Äôs what we really need &lt;code&gt;_COUNT_ONE&lt;/code&gt; to look like:&lt;/p&gt;
    &lt;code&gt;#define _COUNT_ONE(x, ...) \
   + 1 __VA_OPT__(_COUNT_INDIRECT &amp;lt;&amp;lt;SPACER&amp;gt;&amp;gt; ()(__VA_ARGS__))
&lt;/code&gt;
    &lt;p&gt;As we cascade up with replacements, we want the spacer to disappear, leaving:&lt;/p&gt;
    &lt;code&gt;_COUNT_INDIRECT()(2,3)
&lt;/code&gt;
    &lt;p&gt;Remember, in the contents where we replace the spacer, we will have advanced the input head past &lt;code&gt;_COUNT_INDIRECT&lt;/code&gt;. So if the spacer expands to nothing, the rescan will know that &lt;code&gt;()&lt;/code&gt; isn‚Äôt a replaceable macro, and go on with its day. But it will leave &lt;code&gt;_COUNT_INDIRECT&lt;/code&gt; next to the &lt;code&gt;()&lt;/code&gt; so it can be called above, to generate the correct name.&lt;/p&gt;
    &lt;head rend="h2"&gt;You can count on me being empty inside&lt;/head&gt;
    &lt;p&gt;It‚Äôs not hard to get something to expand to the empty string.&lt;/p&gt;
    &lt;p&gt;And then, once we get back up to &lt;code&gt;COUNT()&lt;/code&gt;, we will evaluate &lt;code&gt;_COUNT_INDIRECT()&lt;/code&gt;, which will leave us with &lt;code&gt;_COUNT_ONE(2,3)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Once we get that far, will tinker to find if there are any conditions where we can re-invoke &lt;code&gt;_COUNT_ONE()&lt;/code&gt;. Because hey, we already know there are, we just might not know WHAT they are.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not hard to create a spacer. We can define a macro named &lt;code&gt;EMPTY&lt;/code&gt; like this:&lt;/p&gt;
    &lt;code&gt;#define EMPTY
&lt;/code&gt;
    &lt;p&gt;And use that as our spacer. If you‚Äôve ever peeked into someone‚Äôs recursive macros and been dumbfounded with what you saw, there‚Äôs a good chance you saw a macro named &lt;code&gt;EMPTY&lt;/code&gt; and at the time were thinking, ‚Äúwhat the heck could that possibly do?‚Äù&lt;/p&gt;
    &lt;p&gt;Now you know. But when you did see it, it was probably defined as a function-like macro instead:&lt;/p&gt;
    &lt;code&gt;#define EMPTY()
&lt;/code&gt;
    &lt;p&gt;Using it to postpone evaluation is as easy as:&lt;/p&gt;
    &lt;code&gt;_COUNT_INDIRECT EMPTY() () (2, 3)
&lt;/code&gt;
    &lt;p&gt;Why would we use a function-like macro for our spacer? Doing so makes it easy for us to control how long we want to wait before we‚Äôre able to evaluate what we‚Äôre separating.&lt;/p&gt;
    &lt;p&gt;Specifically, let‚Äôs say we nest our recursive call several levels down. Every level, we use the same trick recursively, separating &lt;code&gt;EMPTY()&lt;/code&gt; apart‚Ä¶ using another &lt;code&gt;EMPTY()&lt;/code&gt; invocation.&lt;/p&gt;
    &lt;p&gt;For instance, if we need to postpone a total of three layers, we could write:&lt;/p&gt;
    &lt;code&gt;_COUNT_INDIRECT EMPTY EMPTY EMPTY() () () () (2, 3)
&lt;/code&gt;
    &lt;p&gt;Honestly, &lt;code&gt;EMPTY()&lt;/code&gt; is a confusing name that detracts from what‚Äôs happening. we can encapsulate this into a more readable macro‚Ä¶ or macros, one for each level we might want to postpone evaluation:&lt;/p&gt;
    &lt;code&gt;#define POSTPONE1(macro_name, args) macro_name EMPTY() args
#define POSTPONE2(macro_name, args) macro_name EMPTY EMPTY()() args
#define POSTPONE3(macro_name, args) \
                               macro_name EMPTY EMPTY EMPTY()()() args
&lt;/code&gt;
    &lt;p&gt;We‚Äôll only need the first of these by the way; but now you know, if you never have a use case where you have deeper nesting (though if you do, maybe worry your macros are getting too complex to be readable?)&lt;/p&gt;
    &lt;p&gt;Our &lt;code&gt;POSTPONE1&lt;/code&gt; macro encapsulates the unintuitive &lt;code&gt;EMPTY()&lt;/code&gt; madness for us, allowing us to instead write:&lt;/p&gt;
    &lt;code&gt;#define _COUNT_ONE(x, ...) \
                    + 1 __VA_OPT__(POSTPONE1(_COUNT_INDIRECT, ())(__VA_ARGS__))
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;EMPTY()&lt;/code&gt; invocation is hidden inside &lt;code&gt;POSTPONE1()&lt;/code&gt; That makes the code we use here, it‚Äôs more explicit that we‚Äôre going to postpone expanding &lt;code&gt;_COUNT_INDIRECT&lt;/code&gt;. We‚Äôve even added the args we want to call it with as a second parameter, to make it more clear what we‚Äôre doing, instead of having a bunch of consecutive argument lists detached from their identifier, which many engineers find alien and incomprehensible.&lt;/p&gt;
    &lt;p&gt;So far, the rest of what we have is:&lt;/p&gt;
    &lt;code&gt;#define _COUNT_TOP(...)   __VA_OPT__(_COUNT_ONE(__VA_ARGS__))
#define COUNT(...)        (_COUNT_TOP(__VA_ARGS__) + 0)
#define _COUNT_INDIRECT() _COUNT_ONE
&lt;/code&gt;
    &lt;p&gt;Let‚Äôs now see if it is getting the right text up to the top, even though we won‚Äôt yet try to get it to expand (and thus, we will get a compiler error). If we invoke as &lt;code&gt;COUNT(1, 2)&lt;/code&gt; again, CPP will generate the following expansion:&lt;/p&gt;
    &lt;code&gt;(+1 _COUNT_INDIRECT ()(2) + 0)
&lt;/code&gt;
    &lt;p&gt;That‚Ä¶ looks exactly like what we were hoping to see. As we wanted, the pieces came together, and &lt;code&gt;_COUNT_INDIRECT()&lt;/code&gt; did NOT get evaluated during the rescan process. If we had made a mistake, and the rescan had happened, it would have been replaced with &lt;code&gt;_COUNT_ONE&lt;/code&gt;, which we already know we could not trigger for re-evaluation.&lt;/p&gt;
    &lt;p&gt;Okay, let‚Äôs now see what happens if we try to force the expansion of the above, to finally get an answer to our question as to the true scope of blue paint.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs wrap the body of &lt;code&gt;COUNT()&lt;/code&gt; with a call to a passthrough macro, which we‚Äôll name &lt;code&gt;EVAL()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;#define EVAL(...)  __VA_ARGS__
#define COUNT(...) EVAL((_COUNT_TOP(__VA_ARGS__) + 0))
&lt;/code&gt;
    &lt;p&gt;Based on our rules above, &lt;code&gt;EVAL()&lt;/code&gt; will substitute, and then rescan. So this passthrough macro forces the rescan we want.&lt;/p&gt;
    &lt;p&gt;The test case for our macro should be:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int
main()
{
    printf("%d\n", COUNT(1, 2));
}
&lt;/code&gt;
    &lt;p&gt;You might notice, this actually compiles. And if you run it, it gives the right answer.&lt;/p&gt;
    &lt;head rend="h1"&gt;ü•π&lt;/head&gt;
    &lt;p&gt;Wow, are we done?&lt;/p&gt;
    &lt;head rend="h3"&gt;ü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£üòÇü§£&lt;/head&gt;
    &lt;p&gt;While we may be farther than expected, we don‚Äôt have something that will fully work. What‚Äôs important is that we‚Äôve shown that blue paint FULLY gets removed from a macro when:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We have finished all rescans of that macro where it was called; and&lt;/item&gt;
      &lt;item&gt;EITHER the macro does not appear in the replacement text, or we completely exit the recursive expansion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, we still have a problem. If you change your invocation to &lt;code&gt;COUNT(1, 2, 3)&lt;/code&gt; then your code will no longer compile. Instead, it will expand to:&lt;/p&gt;
    &lt;code&gt;(+1 +1 _COUNT_INDIRECT ()(3) + 0))
&lt;/code&gt;
    &lt;p&gt;What‚Äôs happening should be clear at this point: while we are iterating over arguments, we are stopping after the second iteration.&lt;/p&gt;
    &lt;p&gt;Great, that‚Äôs easy to fix. We can do so by‚Ä¶&lt;/p&gt;
    &lt;p&gt;Passing the output of &lt;code&gt;EVAL()&lt;/code&gt; to another call of &lt;code&gt;EVAL()&lt;/code&gt;, like so:&lt;/p&gt;
    &lt;code&gt;#define COUNT(...) EVAL(EVAL((_COUNT_TOP(__VA_ARGS__) + 0)))
&lt;/code&gt;
    &lt;p&gt;Before we address the obvious complaint, I‚Äôd like to point out that the extra &lt;code&gt;EVAL()&lt;/code&gt; doesn‚Äôt mess up the results if we change our test invocation to &lt;code&gt;COUNT()&lt;/code&gt; or &lt;code&gt;COUNT(1)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;EVAL()&lt;/code&gt; will expand, but it doesn‚Äôt break anything when there are no macros in the text passed to it that are eligible for expansion. Once the last possible expansion happens, it will just keep copying its arguments to the replacement text, per the algorithm above; the associated rescans have nothing to do.&lt;/p&gt;
    &lt;p&gt;Given that, it‚Äôs common to say, ‚ÄúNobody needs more than 10 arguments!‚Äù and do something like:&lt;/p&gt;
    &lt;code&gt;#define _E(...) __VA_ARGS__
#define EVAL(...) _E(_E(_E(_E(_E(_E(_E(_E(_E(_E(__VA_ARGS__))))))))))
&lt;/code&gt;
    &lt;p&gt;Although, more commonly, you‚Äôd probably see people do more expansions, perhaps enough to accommodate 20 or 50 arguments. Even there, I‚Äôve seen some Microsoft APIs that convince me, that‚Äôs too low a limit, so I‚Äôd suggest at least 100 iterations.&lt;/p&gt;
    &lt;p&gt;We can use a much smarter approach that can still fairly compactly get us as many expansions as we think we might ever need.&lt;/p&gt;
    &lt;p&gt;I‚Äôd guess that, if we saw a function with 1024 arguments, it would be explicitly TRYING to break our macro. So let‚Äôs do the base 2 version of the Spinal Tap ‚Äúone more‚Äù, and get to 1025 expansions:&lt;/p&gt;
    &lt;code&gt;#define _E1(...)    __VA_ARGS__
#define _E8(...)    _E1(_E1(_E1(_E1(_E1(_E1(_E1(_E1(__VA_ARGS__))))))))
#define _E64(...)   _E8(_E8(_E8(_E8(_E8(_E8(_E8(_E8(__VA_ARGS__))))))))
#define _E256(...)  _E64(_E64(_E64(_E64(__VA_ARGS__))))
#define _E1024(...) _E256(_E256(_E256(_E256(__VA_ARGS__))))
#define EVAL(...)   _E1024(__VA_ARGS__)
&lt;/code&gt;
    &lt;p&gt;The last expansion really is gratuitous; we could name &lt;code&gt;_E1024()&lt;/code&gt; to&lt;code&gt;EVAL()&lt;/code&gt; and stop on the power of two, but what fun is that?&lt;/p&gt;
    &lt;p&gt;Before writing this article, I generally stopped at 256 iterations, but I was curious as to whether the compiler‚Äôs CPP was smart enough to skip unnecessary evaluations, given it‚Äôs a common idiom. I ramped the number of iterations up to 65,636, and built a minimal program that would trigger 100 different calls to eval. On a Macbook Pro using &lt;code&gt;clang&lt;/code&gt;, that many expansions took .13 seconds; when setting EVAL to 256 expansions, compiling took .06 seconds. When using &lt;code&gt;gcc&lt;/code&gt;, both times were a bit more than twice as expensive.&lt;/p&gt;
    &lt;p&gt;So no, we definitely shouldn‚Äôt keep going until we get to 2^64; it‚Äôs unlikely to work. But, I‚Äôve never noticed compile time impact due to 256 iterations, even in programs using recursive macros extensively, and can recommend it, but it also seems 2^16 expansions is totally acceptable for cases where you might need it (probably when you‚Äôre iterating over something other than call arguments).&lt;/p&gt;
    &lt;p&gt;NOW we can declare victory.&lt;/p&gt;
    &lt;p&gt;Hang on, I‚Äôm going to go cry again, but this time tears of joy. üò≠üò≠&lt;/p&gt;
    &lt;head rend="h3"&gt;You can count me out&lt;/head&gt;
    &lt;p&gt;While the &lt;code&gt;EVAL&lt;/code&gt;approach works, you may find it feels kludgy. Why ask the preprocessor to do all that additional work? maybe it can recognize the idiom and short-circuit a bunch of work with an &lt;code&gt;EVAL()&lt;/code&gt;? Is there really no better way?
There is a technique that facilitates the kind of compile-time recursion we‚Äôre trying to do here, without unnecessary layers of expansion.
The basic idea is related to the concept of continuations; every macro passes a bag of state to the ‚Äònext‚Äô function-like macro that should get called, doing it in a way that allows us to BOTH dodge blue paint, AND terminate without oblivious expansions.
However:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The approach is MUCH more complicated than what we‚Äôve done so far (and I hope anyone reading should agree what we‚Äôve done today is already much too complicated).&lt;/item&gt;
      &lt;item&gt;Having used the continuation approach, I find it too brittle (as implementable in CPP), and significantly harder to debug than more traditional recursion work-arounds.&lt;/item&gt;
      &lt;item&gt;The extra evals in the approach we‚Äôre using tend to be cheap enough, that the massive amount of extra complexity buys you virtually nothing.&lt;/item&gt;
      &lt;item&gt;I suspect the continuation technique cannot be done without relying on undefined behavior (specifically, how the compiler chooses to resolve cases where there are multiple possible ambiguous valid expansions). But, it‚Äôs still really cool, and if you‚Äôre interested (and really want to risk your head going ü§Ø), check out this brainf‚Äî interpreter written entirely in C macros.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I should note, there are still other ways we could avoid such deep compile-time recursion, or even all recursion if we aren‚Äôt trying to get close to an arbitrary number of arguments. It‚Äôs all far uglier (and way more challenging to understand), and what we‚Äôre doing is performant, enough so that added ugliness isn‚Äôt merited, IMHO.&lt;/p&gt;
    &lt;p&gt;Though, one thing I do recommend you do differently from what we‚Äôve done today, is that you should add a common prefix to all your names, to remove the risk of name collisions ‚Äî many libraries already define things like &lt;code&gt;EVAL()&lt;/code&gt; and &lt;code&gt;EMPTY()&lt;/code&gt;for themselves, and you never know what might make its way into your system, and cause chaos.&lt;/p&gt;
    &lt;p&gt;You can‚Äôt be bothered? Okay, I‚Äôm a pushover (and feel strongly about the issue). So I‚Äôve provided a complete version for you at the end of this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Count your blessings&lt;/head&gt;
    &lt;p&gt;This journey has taken us further down the macro rabbit hole than anyone should ever have to go, yet we didn‚Äôt have to sacrifice ALL of our remaining sanity (it helps that I was already tapped out). As much as C actively worked to thwart us from our goal, we got there; now we have a good tool for better compile-time checking of C code in a number of cases, such as when we want to support variable argument functions. And, with incredibly minor changes, we can re-use the code to operate on each argument, to support other things we might want to automate to make our code more robust. For instance, we could add automatic casts or calls to runtime type checkers, or so on. All you really need to do is, take the text we insert (generating the addition that the compiler can easily fold into a count), and replace it with an invocation of a macro that the caller passes in, passing that macro the current argument. Yes, the C standard committee has good reason to disallow recursion‚Äî removing the restriction would undoubtedly break existing code. Yet, the difficulty of the whole exercise hopefully demonstrates the need for some quality-of-life improvements in C2Y, all of which can be done without significant backward compatibility risk.&lt;/p&gt;
    &lt;p&gt;Most of all, I‚Äôd love to be able to do far more meaningful work at compile time much more sanely, minimizing my use of macros (as I know many others would too). For that, the language should extend &lt;code&gt;constexpr&lt;/code&gt; capabilities, giving us good, full-fledged &lt;code&gt;constexpr&lt;/code&gt; functions, with as few limitations as possible. That may be a tall order for C2Y, given the complexity of that change.
But even still, I‚Äôd want to improve the macro system too:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add a builtin macro, &lt;code&gt;__VA_COUNT__&lt;/code&gt;. We need it to avoid null truncation problems for varargs, and shouldn‚Äôt have to keep reinventing the wheel (or doling out&lt;code&gt;__VA_ARGS__&lt;/code&gt;in our code like it‚Äôs Halloween candy).&lt;/item&gt;
      &lt;item&gt;Add another builtin macro, &lt;code&gt;__VA_EMPTY__(...)&lt;/code&gt;, which would be the inverse of&lt;code&gt;__VA_OPT__(...)&lt;/code&gt;; its arguments would only get expanded only when the are NO variadic arguments. This would make it even easier to ensure people have good tools to easily terminate the kind of recursion we did today. Assuming&lt;code&gt;constexpr&lt;/code&gt;functions take longer to do well, A&lt;code&gt;__VA_EMPTY__&lt;/code&gt;macro can also bridge the gap of not having a good compile-time IF available for complex use cases; it would be much easier to cobble together a reasonably robust one one in macros than it is today by pairing it with&lt;code&gt;__VA_OPT__&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Add a &lt;code&gt;__VA_EVAL__(...)&lt;/code&gt;which simply expands its contents, with two semantic changes to the process that other macro goes through:&lt;list rend="ol"&gt;&lt;item&gt;When an expansion fully finishes, the entire macro gets rescanned again, as many times as necessary, until the full expansion reaches a fix-point (meaning, no more expansions are possible). That would allow us to worry about artificial limits on &lt;code&gt;EVAL()&lt;/code&gt;macros; they‚Äôd just stop when they should stop.&lt;/item&gt;&lt;item&gt;Every top-level rescan should remove all blue paint generated during one expansion, before starting the next expansion (alternately, it could forego further painting macros in the first place).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;When an expansion fully finishes, the entire macro gets rescanned again, as many times as necessary, until the full expansion reaches a fix-point (meaning, no more expansions are possible). That would allow us to worry about artificial limits on &lt;/item&gt;
      &lt;item&gt;A more general purpose &lt;code&gt;__MAP__(body_macro, state, ...)&lt;/code&gt;would be useful (though, to be fair, much less of a problem to build robustly if the rest of the above were present).&lt;/item&gt;
      &lt;item&gt;While waiting for &lt;code&gt;constexr&lt;/code&gt;functions, it would be valuable to have a preprocessor built-in&lt;code&gt;__SHA256__&lt;/code&gt;that‚Ä¶ replaces the contents passed to it (after expansion) with the SHA256 hash of those contents. I‚Äôve seen more than a few cases where this would be incredibly useful for saving both startup costs, and ongoing costs. No? Why can‚Äôt we have nice things??!! Interestingly, doing a compile-time only implementation of&lt;code&gt;SHA-256&lt;/code&gt;using only C macros may seem possible, but I‚Äôve built it (for strings one block in length), and it completely overwhelms both GCC and Clang, even with significantly reduced rounds.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the committee were to adopt most of the above, it sure would make one of the ugliest legacies the language has needed to carry forward far more tolerable.&lt;/p&gt;
    &lt;p&gt;We mere mortals would be able to get the gist of expansion rules more easily, if we could tell ourselves, ‚Äúmacro recursion is disallowed unless you use &lt;code&gt;__EVAL__()&lt;/code&gt;‚Äù. That removes a huge source of confusion, but still leaves us with the problem of getting termination conditions right, which are a bit tricky.
Adding &lt;code&gt;__VA_EMPTY__()&lt;/code&gt; makes it pretty easy for someone to get the exit condition right in the common case where our recursion is being used to iterate over the arguments passed to function-like macros. &lt;code&gt;__EVAL__()&lt;/code&gt; would then essentially be able to function as a ‚Äúdo what I mean‚Äù operator, for most of the things people bang their head against when trying to write useful, robust macros to hide C‚Äôs unnecessary complexity. With just these two things, you would be able to implement &lt;code&gt;__VA_COUNT__(‚Ä¶)&lt;/code&gt;fairly simply:&lt;/p&gt;
    &lt;code&gt;#define __REST__(x, ...) __VA_ARGS__
#define __VA_COUNT__(...) \
    __VA_EMPTY__(0) \
    __VA_OPT__(1 + __EVAL__(__VA_COUNT__(__REST__(__VA_ARGS__))))
&lt;/code&gt;
    &lt;p&gt;Sure, it‚Äôs still a little obtuse, but it‚Äôs far more sane than our final product.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does this count?&lt;/head&gt;
    &lt;p&gt;I said there was no price for my full implementation, and there‚Äôs not. But, if you really feel obliged, then spend a minute indulging me.&lt;/p&gt;
    &lt;p&gt;I can count the times I‚Äôve written the word ‚Äúcount‚Äù more than 100 times in a week on one finger. The entire time I‚Äôve been working on this article, I keep thinking about what might be my favorite dad joke ever, but I spent far more time in ‚Äúmacro hell‚Äù than I like. So, before we talk about our final implementation, I‚Äôm going to indulge myself. It will only make sense to people who grew up with US kids programming, and does not in any way contribute to this topic:&lt;/p&gt;
    &lt;p&gt;Person A: ‚ÄúWho‚Äôs your favorite vampire?‚Äù&lt;/p&gt;
    &lt;p&gt;Person B: ‚ÄúWithout a doubt, the one who lives on Sesame Street.‚Äù&lt;/p&gt;
    &lt;p&gt;Person A: ‚ÄúThe puppet? He doesn‚Äôt count!‚Äù&lt;/p&gt;
    &lt;p&gt;Person B: ‚ÄúI assure you, he does.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ö°Ô∏èHa! Ha! Ha! ‚ö°Ô∏è&lt;/head&gt;
    &lt;code&gt;              oooOOOooo
           oOOOOOOOOOOOOOo
         oOO"           "OO
    ____oOO  ====   ====  OOo____ 
    \   OO'      ! !.---. 'OO   /
     \  OO   &amp;lt;0&amp;gt; ! !!&amp;lt;0&amp;gt;!  OO  /
      \ Oo       ! !'---'  oO /
       \o        \_/        o/
        .' _______________ '.
      ,'   :   V     V   :   '.
    ,'      -_         _-      '.
  ,'          "oOOOOOo"          '.
,'              OOOOO              '.
-----------     "OOO"     -----------
                 "O"             
&lt;/code&gt;
    &lt;p&gt;Okay, if you sat through that (or were smart enough to skip it), you‚Äôve earned the full code.&lt;/p&gt;
    &lt;p&gt;As I mentioned above, I added a &lt;code&gt;H4X0R_&lt;/code&gt; prefix to everything to avoid name collisions. Additionally, internal helpers have a leading underscore. The notion of &lt;code&gt;_&lt;/code&gt; indicating an internal variable has a long heritage (it is particularly prominent in Python, for example).&lt;/p&gt;
    &lt;p&gt;Our new top-level count macro is named &lt;code&gt;H4X0R_VA_COUNT()&lt;/code&gt;; I added the &lt;code&gt;VA&lt;/code&gt; to indicate we‚Äôre counting variable arguments (but didn‚Äôt want to make it too verbose, either).&lt;/p&gt;
    &lt;p&gt;But I actually made some other changes from what we did above.&lt;/p&gt;
    &lt;p&gt;Specifically, I first built a macro, &lt;code&gt;H4X0R_MAP(macro, ...)&lt;/code&gt;. The implementation of this new macro is structured in pretty much the same way as our &lt;code&gt;COUNT()&lt;/code&gt; macro above, at least in terms of the recursion. Our major change, besides the names is that, whereas for each argument, &lt;code&gt;COUNT()&lt;/code&gt; ignores the argument and replaces it with &lt;code&gt;+1&lt;/code&gt;, &lt;code&gt;H4X0R_MAP()&lt;/code&gt; takes the argument, and passes it to a macro supplied by the caller. That caller-supplied macro gets passed the value of the parameter currently being visited.&lt;/p&gt;
    &lt;p&gt;That makes it trivial to create &lt;code&gt;H4X0R_VA_COUNT()&lt;/code&gt;; we just have to call our new macro, and pass in a body macro that simply does &lt;code&gt;+1&lt;/code&gt; (ignoring the parameter value).&lt;/p&gt;
    &lt;p&gt;This new macro gives us more flexibility, hopefully lessening the need to write future recursive macros.&lt;/p&gt;
    &lt;code&gt;// To keep this compact width-wise (given the prefix), we only do two 
// expansions per line, and stop at 256 expansions. Extend as desired.

#define H4X0R_EVAL1(...)    __VA_ARGS__
#define H4X0R_EVAL2(...)    H4X0R_EVAL1(H4X0R_EVAL1(__VA_ARGS__))
#define H4X0R_EVAL4(...)    H4X0R_EVAL2(H4X0R_EVAL2(__VA_ARGS__))
#define H4X0R_EVAL8(...)    H4X0R_EVAL4(H4X0R_EVAL4(__VA_ARGS__))
#define H4X0R_EVAL16(...)   H4X0R_EVAL8(H4X0R_EVAL8(__VA_ARGS__))
#define H4X0R_EVAL32(...)   H4X0R_EVAL16(H4X0R_EVAL16(__VA_ARGS__))
#define H4X0R_EVAL64(...)   H4X0R_EVAL32(H4X0R_EVAL32(__VA_ARGS__))
#define H4X0R_EVAL128(...)  H4X0R_EVAL64(H4X0R_EVAL64(__VA_ARGS__))
#define H4X0R_EVAL(...)     H4X0R_EVAL128(H4X0R_EVAL128(__VA_ARGS__))

#define H4X0R_EMPTY()
#define H4X0R_POSTPONE1(macro) macro H4X0R_EMPTY()

// MAP(); If you remove the macro parameter, and replace the call
// `macro(x)` with `+1`, you'll see this is structurally the same
// as COUNT() above.
#define H4X0R_MAP(macro, ...) \
    __VA_OPT__(H4X0R_EVAL(_H4X0R_MAP_ONE(macro, __VA_ARGS__)))
#define _H4X0R_MAP_ONE(macro, x, ...) macro(x) \
    __VA_OPT__(H4X0R_POSTPONE1(_H4X0R_MAP_INDIRECT)()(macro, __VA_ARGS__))
#define _H4X0R_MAP_INDIRECT() _H4X0R_MAP_ONE

// A re-implementation of count on top of H4X0R_MAP()... it's simple now!
#define _H4X0R_COUNT_BODY(x) +1
#define H4X0R_VA_COUNT(...)  \
                           (H4X0R_MAP(_H4X0R_COUNT_BODY, __VA_ARGS__) + 0)
&lt;/code&gt;
    &lt;p&gt;The new &lt;code&gt;H4X0R_MAP()&lt;/code&gt; call gives us a more general purpose way to apply transformations to a list of individual arguments. This can help us automate static type checking per-argument, for instance, when we‚Äôre implementing variable-argument functions and want to statically ensure all items at the call site have the same time (as done in the variadic function arguments implementation I wrote about last week.&lt;/p&gt;
    &lt;p&gt;For a use case like that, we need to add the commas back in from the original call site. But we won‚Äôt want to add the comma at the end of last argument, because the C compiler will complain. There are plenty of different ways to handle this problem, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If the output is being passed to a C varargs function where the same arguments we‚Äôre iterating over will have a correct &lt;code&gt;count&lt;/code&gt;parameter before the values, we can simply add a dummy value (probably 0 since that‚Äôll pass through easily in most contexts). This one is trivial when appropriate, but I don‚Äôt love it, as it isn‚Äôt always appropriate.&lt;/item&gt;
      &lt;item&gt;We can rework our map implementation to call a different callback for the last parameter, or to call a separate callback in between parameters. This is definitely more workable, but requires more recursive macro work, overcomplicating things. And then, the extra parameter can easily be forgotten by callers, which would lead to confusing errors (any time you make a mistake calling a macro, but the error isn‚Äôt detected until the compilation step, we should expect it to be confusing).&lt;/item&gt;
      &lt;item&gt;We can let the caller handle the first argument separately, and then for any subsequent arguments, add the comma at the beginning, separating it from the previous argument that we know must be present.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I personally think the third option is the best compromise. Let‚Äôs look at a simple little example. While I normally would want to add more static type checking, sometimes the need might arise to convert a bunch of items of different types into &lt;code&gt;void *&lt;/code&gt; .&lt;/p&gt;
    &lt;p&gt;If we only need to do it for integer types and pointers, it‚Äôs not too hard to use a union to statically convert one item at a time to &lt;code&gt;void *&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We will create a temporary union, with two fields, one being our largest standard unsigned int type (&lt;code&gt;unsigned long long&lt;/code&gt;, guaranteed to be at least 64 bits), and a &lt;code&gt;void *&lt;/code&gt;. We will use a cast to assign to the first field, allowing us to insert any integer or pointer type into the anonymous union:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shorter integers will happily promote to a larger size.&lt;/item&gt;
      &lt;item&gt;Pointers will convert, since they are never more than 64 bits in size today.&lt;/item&gt;
      &lt;item&gt;The first field being unsigned prevents unwanted sign extension, when we insert signed values that are kept in smaller values.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We then will take the value out of the anonymous union, using the &lt;code&gt;void *&lt;/code&gt; . The compiler is smart enough to not generate a real temporary object for this conversion.&lt;/p&gt;
    &lt;code&gt;#define _H4X0R_CONVERT_ONE(arg)                  \
    ((union { unsigned long long u; void *v; }){ \
         .u = (unsigned long long)arg,           \
     }).v    
&lt;/code&gt;
    &lt;p&gt;Next, let‚Äôs build a macro to return the first parameter item in a variable list:&lt;/p&gt;
    &lt;code&gt;#define H4X0R_FIRST(...)     __VA_OPT__(_H4X0R_FIRST(__VA_ARGS__))
#define _H4X0R_FIRST(x, ...) x
&lt;/code&gt;
    &lt;p&gt;The reason for two macros here, is that we want to be able to tolerate cases where no arguments are provided.&lt;/p&gt;
    &lt;p&gt;We then can do the same thing to get all the rest of the arguments, after carving off the first argument:&lt;/p&gt;
    &lt;code&gt;#define H4X0R_REST(...)     __VA_OPT__(_H4X0R_REST(__VA_ARGS__))
#define _H4X0R_REST(x, ...) __VA_ARGS__
&lt;/code&gt;
    &lt;p&gt;When we process the first argument, we‚Äôll be able to call &lt;code&gt;_H4X0R_CONVERT_ONE()&lt;/code&gt; directly. For the rest of the arguments, we‚Äôre going to want to pass a macro to process the argument, which can reuse &lt;code&gt;_H4X0R_CONVERT_ONE()&lt;/code&gt;, but does need to add a comma before the argument. So it‚Äôs as simple as:&lt;/p&gt;
    &lt;code&gt;#define _H4X0R_CONVERT_LATER_ARG(arg) , _H4X0R_CONVERT_ONE(arg)
&lt;/code&gt;
    &lt;p&gt;We‚Äôll want to pass that macro to our map implementation. Here‚Äôs the call to our map function:&lt;/p&gt;
    &lt;code&gt;#define _H4X0R_CONVERT_LIST(...) H4X0R_MAP(_H4X0R_CONVERT_LATER_ARG, __VA_ARGS__)
&lt;/code&gt;
    &lt;p&gt;To stitch this all together, we just need to peal off the first argument from the rest, and concatenate the two resulting pieces. The only caveat is that we need to be careful to avoid adding a spurious comma or semicolons in between the two bits we need. When in doubt, use &lt;code&gt;cc -E&lt;/code&gt; to review what‚Äôs getting produced.&lt;/p&gt;
    &lt;code&gt;#define H4X0R_VA_VOID_STAR_CONVERT(...)          \
    _H4X0R_CONVERT_ONE(H4X0R_FIRST(__VA_ARGS__)) \
    _H4X0R_CONVERT_LIST(H4X0R_REST(__VA_ARGS__))
&lt;/code&gt;
    &lt;p&gt;A dumb test case to show this working:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int
main()
{
    printf("%d items: (%p, %p, %p)\n",
           H4X0R_VA_COUNT(1, 2, 3),
           H4X0R_VA_VOID_STAR_CONVERT(1, 2, 3));
}
&lt;/code&gt;
    &lt;p&gt;As you may expect, this prints:&lt;/p&gt;
    &lt;code&gt;3 items: (0x1, 0x2, 0x3)
&lt;/code&gt;
    &lt;p&gt;With our knowledge, and our final &lt;code&gt;H4X0R_MAP()&lt;/code&gt; and &lt;code&gt;H4X0R_COUNT()&lt;/code&gt; implementations, we can now do some significant transformations easily most people find mind boggling.&lt;/p&gt;
    &lt;p&gt;I find it astounding, the level of difficulty required to understand enough about C macros to be able to build a primitive as basic as counting arguments statically at a call site.&lt;/p&gt;
    &lt;p&gt;There‚Äôs such a high level of complexity involved, that it‚Äôs even more amazing to me, that once we got the understanding we needed, we could implement both macros in a mere 18 lines of code, exactly half of that being the rote &lt;code&gt;H4X0R_EVAL()&lt;/code&gt;implementation.&lt;/p&gt;
    &lt;p&gt;And in my view, by choosing comprehensible names, despite all the hurdles we‚Äôve had to overcome, the final result looks almost trivial. Yet, the code alone does not hint at the knowledge you need to write it.&lt;/p&gt;
    &lt;p&gt;I‚Äôm confident that nearly anyone who lacks a solid understanding of those basics who tries to gain understanding by trying to rebuild this code, but using it has a guide (perhaps to a slightly different use case), will be inflicting gruesome self-torture on themselves. They might have preferred to code their algorithm in the Brainf‚Äî implementation mentioned above!&lt;/p&gt;
    &lt;p&gt;Why is this stuff so hard after decades of standardization? Is the committee secretly a cabal of Rust zealots, that built time travel and went back to 1989, to maximize their torture of C developers?? Maybe that was the inspiration for the movie 12 Monkeys ü§î Sounds plausible. Or did they just beam that into my head?! Where did I leave my tin foil hat?&lt;/p&gt;
    &lt;p&gt;Happy hacking (but hopefully not on C macros)!&lt;/p&gt;
    &lt;p&gt;‚Äî L33 T. (with a &lt;code&gt;#&lt;/code&gt;-ing headache)&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;This post started as an out-of-control sidebar in my post on variable argument functions in C.&lt;/p&gt;
    &lt;p&gt;I owe a huge debt of gratitude to Robert Seacord; his early feedback gave me the clarity I needed to (hopefully) help other C programmers over what is probably one of the biggest hurdles in the language‚Ä¶ people who become C developers eventually master pointers, but to many senior developers, macro recursion has remained a dark art, requiring a magic incantation borrowed from Stack Overflow, Claude or the like.&lt;/p&gt;
    &lt;p&gt;Ivan O‚ÄôDay also was critical here, BUT is responsible for me taking several extra weeks to get the article out once it was in decent shape‚Ä¶ because he helped me see that I really needed the step-by-step example of a macro expansion, which I then had to find time to do!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://h4x0r.org/big-mac-ro-attack/"/><published>2025-11-06T01:09:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45830770</id><title>End of Japanese community</title><updated>2025-11-06T06:49:10.329759+00:00</updated><content>&lt;doc fingerprint="d673c7910d1bb0ed"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript is disabled in your browser. Please enable JavaScript to proceed. A required part of this site couldn‚Äôt load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://support.mozilla.org/en-US/forums/contributors/717446"/><published>2025-11-06T02:38:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45830808</id><title>Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer</title><updated>2025-11-06T06:49:10.265167+00:00</updated><content>&lt;doc fingerprint="364b6a729478ef42"&gt;
  &lt;main&gt;
    &lt;p&gt;* Denotes Equal Contribution&lt;/p&gt;
    &lt;p&gt;‚Äî&lt;/p&gt;
    &lt;p&gt;Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present Brain-IT, a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters &amp;amp; subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i) high-level semantic features, which steer the diffusion model toward the correct semantic content of the image; and (ii) low-level structural features, which help to initialize the diffusion process with the correct coarse layout of the image. BIT‚Äôs design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current state-of-the-art approaches both visually and by standard objective metrics. Moreover, with only 1 hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40 hour recordings.&lt;/p&gt;
    &lt;p&gt;The Brain Interaction Transformer (BIT) transforms fMRI signals into Semantic and VGG features using a shared Voxel-to-Cluster (V2C) mapping. Two branches are applied: (i) the Low-Level branch reconstructs a coarse image from VGG features, used to initialize the (ii) Semantic branch, which uses semantic features to guide the diffusion model. Each voxel from every subject is mapped to a functional cluster shared across subjects, enabling integration within and across brains. Our Brain-IT pipeline thus reconstructs images directly from fMRI activations by first predicting meaningful image features with BIT, then refining them through a diffusion model guided by semantic conditioning and a Deep Image Prior (DIP) that ensures structural fidelity.&lt;/p&gt;
    &lt;p&gt;The BIT model predicts image features from voxel activations (fMRI). The Brain Tokenizer maps the fMRI activations into Brain Tokens, which are representations of the aggregated information from all the voxels of a single cluster (one token per cluster). The Cross-Transformer Module integrates information from the Brain Tokens to refine their representation, and employs query tokens to retrieve information from the Brain Tokens and transform it into image features, with each query token predicting a single output image feature.&lt;/p&gt;
    &lt;p&gt;Brain-IT demonstrates strong semantic fidelity and structural accuracy across multiple evaluation metrics. Importantly, with just 1 hour of data, Brain-IT is comparable to prior methods trained on the full 40 hours.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://AmitZalcher.github.io/Brain-IT/"/><published>2025-11-06T02:46:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45830829</id><title>Ratatui ‚Äì App Showcase</title><updated>2025-11-06T06:49:09.999286+00:00</updated><content>&lt;doc fingerprint="74cad9da50b51b8f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;App Showcase&lt;/head&gt;
    &lt;p&gt;Atuin replaces your existing shell history with a SQLite database, and records additional context for your commands.&lt;/p&gt;
    &lt;p&gt;This is a CLI utility for displaying current network utilization by process, connection and remote IP/hostname&lt;/p&gt;
    &lt;p&gt;Perform binary analysis in your terminal.&lt;/p&gt;
    &lt;p&gt;A customizable cross-platform graphical process/system monitor for the terminal&lt;/p&gt;
    &lt;p&gt;Play crossword puzzles in your terminal.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;csvlens&lt;/code&gt; is A command line CSV file viewer. It is like less but made for CSV.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;dua&lt;/code&gt; is a disk space analysis tool designed for speed, leveraging parallel processing to quickly
provide detailed disk usage information and allowing for faster deletion of unnecessary data
compared to the standard ‚Äòrm‚Äô command.&lt;/p&gt;
    &lt;p&gt;A command line tool that executes make target using fuzzy finder with preview window&lt;/p&gt;
    &lt;p&gt;TUI for git written in rust&lt;/p&gt;
    &lt;p&gt;gpg-tui is a Terminal User Interface for GnuPG.&lt;/p&gt;
    &lt;p&gt;Ranger-like terminal file manager written in Rust&lt;/p&gt;
    &lt;p&gt;A material design color palette for the terminal.&lt;/p&gt;
    &lt;p&gt;A mine sweeping game written in Rust&lt;/p&gt;
    &lt;p&gt;Oatmeal is a terminal UI chat application that speaks with LLMs, complete with slash commands and fancy chat bubbles. It features agnostic backends to allow switching between the powerhouse of ChatGPT, or keeping things private with Ollama. While Oatmeal works great as a stand alone terminal application, it works even better paired with an editor like Neovim!&lt;/p&gt;
    &lt;p&gt;oha is a tiny program that sends some load to a web application and show realtime tui&lt;/p&gt;
    &lt;p&gt;A simple TUI to view &amp;amp; control docker containers&lt;/p&gt;
    &lt;p&gt;Unlock the power of APIs with simplicity and speed, right from your terminal. View OpenAPI documentations in your terminal.&lt;/p&gt;
    &lt;p&gt;A lightweight and terminal-based tool for interacting with databases.&lt;/p&gt;
    &lt;p&gt;An application to manage markdown notes from your terminal and compile them to HTML&lt;/p&gt;
    &lt;p&gt;A simple oscilloscope/vectorscope/spectroscope for your terminal&lt;/p&gt;
    &lt;p&gt;Terminal HTTP/REST client&lt;/p&gt;
    &lt;p&gt;A CLI-based AI coding agent for local dev, scripts/CI, and automation.&lt;/p&gt;
    &lt;p&gt;A terminal user interface for taskwarrior&lt;/p&gt;
    &lt;p&gt;Television is a fast and versatile fuzzy finder TUI.&lt;/p&gt;
    &lt;p&gt;It lets you quickly search through any kind of data source (files, git repositories, environment variables, docker images, you name it) using a fuzzy matching algorithm and is designed to be easily extensible.&lt;/p&gt;
    &lt;p&gt;A network diagnostic tool that combines the functionality of traceroute and ping and is designed to assist with the analysis of networking issues.&lt;/p&gt;
    &lt;p&gt;A hackable, minimal, fast TUI file explorer&lt;/p&gt;
    &lt;p&gt;Blazing fast terminal file manager written in Rust, based on async I/O&lt;/p&gt;
    &lt;p&gt;Y≈çzefu is an interactive TUI application for exploring data of a Kafka cluster.&lt;/p&gt;
    &lt;p&gt;It is an alternative tool to AKHQ, Redpanda Console, or the Kafka plugin for JetBrains IDEs. It includes a search query language inspired by SQL, providing fine-grained filtering capabilities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ratatui.rs/showcase/apps/"/><published>2025-11-06T02:50:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45831541</id><title>I may have found a way to spot U.S. at-sea strikes before they're announced</title><updated>2025-11-06T06:49:09.780686+00:00</updated><content/><link href="https://old.reddit.com/r/OSINT/comments/1opjjyv/i_may_have_found_a_way_to_spot_us_atsea_strikes/"/><published>2025-11-06T04:37:45+00:00</published></entry></feed>