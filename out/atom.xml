<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-08-30T15:33:22.778448+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45044155</id><title>Andrew Ng says bottleneck in AI startups isn't coding – it's product management</title><updated>2025-08-30T15:33:30.184735+00:00</updated><content>&lt;doc fingerprint="a760e073117e202f"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI sped up coding. Now, the real challenge for startups is product management.&lt;/item&gt;
      &lt;item&gt;If a prototype takes a day, waiting a week for user feedback is "really painful," said Andrew Ng.&lt;/item&gt;
      &lt;item&gt;The former Google Brain scientist said his teams are "increasingly relying on gut" to make faster decisions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AI has made coding the easy part. The hard part now is product management, said Andrew Ng.&lt;/p&gt;
    &lt;p&gt;The Stanford professor and former Google Brain scientist said on an episode of the "No Priors" podcast published Thursday that AI-assisted coding has compressed the startup loop.&lt;/p&gt;
    &lt;p&gt;Things that used to take six engineers three months to build, "my friends and I, we'll just build on a weekend," Ng said.&lt;/p&gt;
    &lt;p&gt;"The bottleneck is deciding what do we actually want to build," he added.&lt;/p&gt;
    &lt;p&gt;In the past, a prototype might take three weeks to develop, so waiting another week for user feedback wasn't a big deal. But today, when a prototype can be built in a single day, "if you have to wait a week for user feedback, that's really painful," Ng said.&lt;/p&gt;
    &lt;p&gt;That mismatch is forcing teams to make faster product decisions — and Ng said his teams are "increasingly relying on gut."&lt;/p&gt;
    &lt;p&gt;The best product managers bring "deep customer empathy," he said. It's not enough to crunch data on user behavior. They need to form a mental model of the ideal customer.&lt;/p&gt;
    &lt;p&gt;It's the ability to "synthesize lots of signals to really put yourself in the other person's shoes to then very rapidly make product decisions," he added.&lt;/p&gt;
    &lt;p&gt;Ng did not respond to a request for comment from Business Insider.&lt;/p&gt;
    &lt;head rend="h2"&gt;The great product manager debate&lt;/head&gt;
    &lt;p&gt;Ng's comments come as the debate continues in the startup world over the role of product managers.&lt;/p&gt;
    &lt;p&gt;Product managers have been referred to — both affectionately and critically — as "mini-CEOs" of the products they oversee. They act as a bridge among engineers, sales teams, customer service, and other departments, ensuring that products align with user needs.&lt;/p&gt;
    &lt;p&gt;Some tech leaders have said that product managers are key in the age of AI.&lt;/p&gt;
    &lt;p&gt;Microsoft's chief technology officer, Kevin Scott, said on an episode of the "Twenty Minute VC" podcast published in March that product managers play a crucial role in setting up "feedback loops" to make AI agents better.&lt;/p&gt;
    &lt;p&gt;But others argue that product managers add little value.&lt;/p&gt;
    &lt;p&gt;Surge AI CEO Edwin Chen said on an episode of the "No Priors" podcast published last month that product managers don't make sense early on in a company's early days.&lt;/p&gt;
    &lt;p&gt;Microsoft wants to increase the number of engineers relative to product or program managers, Business Insider's Ashley Stewart reported in March.&lt;/p&gt;
    &lt;p&gt;The call for executives to go "founder mode" — a concept coined by the Y Combinator cofounder Paul Graham and touted by Airbnb CEO Brian Chesky — has some leaders questioning whether they should delegate product decisions to product managers.&lt;/p&gt;
    &lt;p&gt;In 2023, Chesky merged product management with marketing, and Snap told The Information in the same year that it laid off 20 product managers to help speed up the company's decision-making.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.businessinsider.com/andrew-ng-product-management-bottleneck-coding-ai-startups-2025-8"/></entry><entry><id>https://news.ycombinator.com/item?id=45063559</id><title>Grok Code Fast 1</title><updated>2025-08-30T15:33:30.089187+00:00</updated><content/><link href="https://x.ai/news/grok-code-fast-1"/></entry><entry><id>https://news.ycombinator.com/item?id=45064329</id><title>Deploying DeepSeek on 96 H100 GPUs</title><updated>2025-08-30T15:33:29.761391+00:00</updated><content>&lt;doc fingerprint="ec5d845b04b8c994"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deploying DeepSeek with PD Disaggregation and Large-Scale Expert Parallelism on 96 H100 GPUs&lt;/head&gt;
    &lt;p&gt;by: The SGLang Team, May 05, 2025&lt;/p&gt;
    &lt;p&gt;DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which uses Multi-head Latent Attention (MLA) and Mixture of Experts (MoE), require an advanced system for efficient serving at scale. In this blog, we explain how we match DeepSeek's inference system performance with SGLang.&lt;/p&gt;
    &lt;p&gt;Our implementation, shown in the figure above, runs on 12 nodes in the Atlas Cloud, each equipped with 8 H100 GPUs. It uses prefill-decode disaggregation and large-scale expert parallelism (EP), achieving a speed of 52.3k input tokens per second and 22.3k output tokens per second per node for 2000-token input sequences. To the best of our knowledge, this represents the first open-source implementation to nearly match the throughput reported in the official DeepSeek blog at large scale. By deploying this implementation locally, it translates to a cost of $0.20/1M output tokens, which is about one-fifth the cost of the official DeepSeek Chat API. Compared to vanilla tensor parallelism using the same resources, this optimized strategy improves the output throuhgput by up to 5x. This blog dives into our parallelism design, optimization methods, and results. All components of our work are fully open-source, allowing others to explore and build on our efforts. The instructions for reproducing our experiments are fully available here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlight&lt;/head&gt;
    &lt;p&gt;✅ SGLang now supports prefill-decode (PD) disaggregation and large-scale EP, including the full functionality of DeepEP, DeepGEMM, and EPLB.&lt;/p&gt;
    &lt;p&gt;✅ Leveraging these new features, our team successfully replicated DeepSeek's inference system using 12 nodes, each with 8 H100 GPUs. In total, SGLang achieves a throughput of 52.3k input tokens per second and 22.3k output tokens per second per node for input sequences of 2000 tokens.&lt;/p&gt;
    &lt;p&gt;✅ This blog explains technical details of our approach, focusing on optimizations for efficiency, peak memory usage reduction, and workload balancing. The profile results show that our implementation achieves nearly on-par performance with the official DeepSeek’s report.&lt;/p&gt;
    &lt;p&gt;✅ All experiments and code are fully open-sourced for community access and further development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parallelism Design&lt;/item&gt;
      &lt;item&gt;Prefill and Decode Disaggregation&lt;/item&gt;
      &lt;item&gt;Large-scale Expert Parallelism&lt;/item&gt;
      &lt;item&gt;Evaluation&lt;/item&gt;
      &lt;item&gt;Toolkits&lt;/item&gt;
      &lt;item&gt;Limitations and Future Work&lt;/item&gt;
      &lt;item&gt;Conclusion&lt;/item&gt;
      &lt;item&gt;Acknowledgment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Parallelism Design&lt;/head&gt;
    &lt;p&gt;Efficient parallelism is essential to manage the computational complexity and memory demands of DeepSeek's architecture. This section outlines our approach to optimizing key components: attention layers, dense feed-forward networks (FFNs), sparse FFNs, and the language model (LM) head. Each component leverages tailored parallelism strategies to enhance scalability, memory efficiency, and performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Attention Layers&lt;/head&gt;
    &lt;p&gt;DeepSeek employs Multi-head Latent Attention (MLA) to effectively model complex dependencies within input sequences. To optimize this mechanism, we implement DP Attention, a data parallelism strategy that eliminates KV cache duplication across devices, significantly reducing memory overhead. Introduced in SGLang v0.4, this approach has been extended to support hybrid data and tensor parallelism, offering flexibility for processing small batch sizes efficiently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dense FFNs&lt;/head&gt;
    &lt;p&gt;Despite using only three dense FFN layers, DeepSeek-V3's computation can significantly increase peak memory usage, potentially leading to system crashes if not carefully managed. To address this, we adopt Data Parallelism (DP) over tensor parallelism (TP), leveraging the following advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enhanced Scalability: With an intermediate dimension of 18,432, high TP degrees (e.g., TP32) result in inefficient fragmentation into small-unit segments (e.g., 576 units), which are not divisible by 128—a common alignment boundary for modern GPUs such as H100. This misalignment hampers computational efficiency and memory utilization. DP provides a more scalable solution by avoiding fragmentation, ensuring balanced workload distribution across devices.&lt;/item&gt;
      &lt;item&gt;Optimized Memory Efficiency: Traditionally, TP reduces memory usage as worker size increases, but this advantage diminishes under DP attention. In a pure TP setup, memory demand for a single-layer Transformer model scales with DP size as: $$\text{Memory}=\frac{N_{\text{param}}}{\text{TP}}+(1+k)N_{\text{hidden_state}}\cdot \text{DP}\notag$$ Here, $N_{\text{hidden_state}}=n_\text{token}\times n_\text{hidden_size}$ is the size of the hidden state on each device (DP rank), $N_{\text{param}}=n_\text{intermediate_size}\times n_\text{hidden_size}$ is the number of model parameters, and $k$ is a coefficient representing extra memory overhead from CUDA Graph duplication. By assuming $\text{DP}=\text{TP}$, this memory usage function is minimized when $\text{TP}=\sqrt{\frac{N_{\text{param}}}{(1+k)N_{\text{hidden_state}}}}$. DeepSeek-V3 uses an intermediate size of 18,432. During the prefill phase, CUDA Graph is typically disabled, so $k = 0$. However, the token size per device can easily exceed 2,048, resulting in an optimal TP size of 3 or less. In the decode phase, a practical configuration might use 128 tokens per device and set $k = 3$. In this case, the memory-optimal TP size is 6. In both phases, a lower TP degree minimizes memory usage per device. As a result, DP may offer a more memory-efficient approach for scaling compared to relying solely on TP.&lt;/item&gt;
      &lt;item&gt;Minimized Communication Overhead: In pure TP, each FFN necessitates two all-reduce operations, resulting in substantial communication overhead. By leveraging DP, we optimize this process to a single reduce-scatter following the prior attention layer and an all-gather before the next, reducing communication costs by 50%. Furthermore, when attention is also computed under pure DP, inter-device communication is entirely eliminated, significantly enhancing overall efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The integration of DP dense FFN with DP attention is illustrated in the left figure below. Users can enable this feature by setting &lt;code&gt;--moe-dense-tp-size=1&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sparse FFNs&lt;/head&gt;
    &lt;p&gt;In DeepSeek-V3's Mixture of Experts (MoE) architecture, sparse FFNs require substantial expert weights, creating a significant memory bottleneck. To address this, we implement Expert Parallelism (EP), which distributes expert weights across multiple devices. This approach effectively scales memory capacity while maintaining high performance, though it does introduce challenges like irregular all-to-all communication and workload imbalance.&lt;/p&gt;
    &lt;p&gt;The figure in the right figure above illustrates our EP implementation using the DeepEP framework, with further details on our EP design and optimizations provided in the following sections.&lt;/p&gt;
    &lt;head rend="h3"&gt;LM Head&lt;/head&gt;
    &lt;p&gt;The LM head computes output probabilities over a large vocabulary, a resource-intensive operation traditionally handled with vocabulary parallelism to aggregate token logits from TP groups. To enhance scalability and efficiency, we adopt Data Parallelism (DP), mirroring our dense FFN strategy. This reduces memory overhead and simplifies communication across devices, delivering a more streamlined solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prefill and Decode Disaggregation&lt;/head&gt;
    &lt;p&gt;LLM inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce Prefill and Decode (PD) Disaggregation in SGLang.&lt;/p&gt;
    &lt;head rend="h3"&gt;Issues with Unified Scheduling&lt;/head&gt;
    &lt;p&gt;The conventional unified engine, which processes prefill and decode batches together, results in three significant problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prefill Interruption: Incoming prefill batches frequently interrupt ongoing decode batches, causing substantial delays in token generation.&lt;/item&gt;
      &lt;item&gt;DP Attention Imbalance: In DP attention, one DP worker may process a prefill batch while another handles a decode batch simultaneously, leading to increased decode latency.&lt;/item&gt;
      &lt;item&gt;Incompatible with DeepEP: As we will discuss in a later section, DeepEP executes different dispatch modes for prefill and decode, making unified scheduling imcompatible with DeepEP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PD Disaggregation resolves these by separating the two stages, enabling tailored optimizations for each.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation Details&lt;/head&gt;
    &lt;p&gt;The PD Disaggregation design in SGLang, depicted in the diagram below, interleaves execution between a Prefill Server and a Decode Server:&lt;/p&gt;
    &lt;p&gt;Upon receiving an input request, the workflow proceeds as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A Prefill Server and a Decode Server pair via a handshake, establishing a local sender and receiver, respectively.&lt;/item&gt;
      &lt;item&gt;The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin the model forward pass and compute the KV caches.&lt;/item&gt;
      &lt;item&gt;Once computed, the data transfers to the Decode Server, which handles iterative token generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This separation ensures each phase operates under optimal conditions, maximizing GPU resource utilization. To further enhance performance, our implementation incorporates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-blocking Transfer: Data send and receive operations run in a background thread, keeping the scheduler’s event loop uninterrupted.&lt;/item&gt;
      &lt;item&gt;RDMA-Based Transfer: Remote Direct Memory Access (RDMA) leverages queue pairs for connections and scatter-gather elements (SGE) for efficient transfer of non-contiguous memory chunks.&lt;/item&gt;
      &lt;item&gt;Flexible API Integration: SGLang offers adaptable APIs that integrate high-performance RDMA libraries like Mooncake and NIXL, streamlining data transfers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details can be found in our design document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Large-scale Expert Parallelism&lt;/head&gt;
    &lt;head rend="h3"&gt;Expert Parallelism with DeepEP&lt;/head&gt;
    &lt;p&gt;DeepEP, implemented by the DeepSeek team, is a communication library designed to streamline EP in MoE models. It tackles the challenge of efficiently routing tokens to specific experts across multiple GPUs. By providing optimized communication kernels, DeepEP reduces latency and boosts throughput, making it ideal for large-scale inference tasks.&lt;/p&gt;
    &lt;p&gt;DeepEP provides two specialized dispatch modes to address varying workload demands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Normal Dispatch: Optimized for handling long input sequences, such as during the prefill phase, this mode prioritizes maximum computational throughput. However, it generates symbolic shapes that are incompatible with CUDA Graph, rendering it less effective for the decode phase, where kernel launch overhead becomes a significant bottleneck.&lt;/item&gt;
      &lt;item&gt;Low-Latency Dispatch: Tailored for generating output tokens during the decode phase, this mode prioritizes minimal delay to ensure real-time performance. It supports CUDA Graph but requires preallocating a fixed memory size. If the memory demand exceeds this preallocation, a runtime error occurs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In SGLang, the integration of DeepEP provides auto mode that dynamically selects between these two dispatch modes based on the workload. However, without PD disaggregation, the auto mode faces a limitation: it cannot simultaneously support both normal dispatch (for prefill) and low-latency dispatch (for decode) within the same communication group. This restriction hinders its compatibility with DP attention, which is crucial for memory-efficient inference. The compatibility of each mode is outlined in the table below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Long Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Long Output&lt;/cell&gt;
        &lt;cell role="head"&gt;DP Attention&lt;/cell&gt;
        &lt;cell role="head"&gt;CUDA Graph&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Normal&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Low-Latency&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Auto&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;PD disaggregation addresses this by separating prefill and decode phases, allowing normal dispatch for the prefill phase and low-latency dispatch for the decode phase, both under DP attention. This integration optimizes resource utilization and enhances overall performance by aligning the dispatch mode with the specific needs of each phase.&lt;/p&gt;
    &lt;head rend="h3"&gt;DeepGEMM Integration&lt;/head&gt;
    &lt;p&gt;DeepGEMM is another high-efficient library developed by the DeepSeek team, specifically designed to optimize computations in MoE models. It provides two specialized functions for handling MoE-related matrix multiplications (Grouped GEMMs), each tailored to different phases of the inference process.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grouped GEMMs (contiguous layout): This kernel is designed for dynamic input shapes, making it ideal for the prefill phase of MoE inference. It processes inputs where the data for different experts is concatenated contiguously, allowing for flexible handling of varying input sizes.&lt;/item&gt;
      &lt;item&gt;Grouped GEMMs (masked layout): This kernel assumes a fixed input shape and uses a mask tensor to compute only the valid portions of the input. It is compatible with CUDA Graph, which optimizes kernel launches, making it well-suited for the decode phase where reducing overhead is critical.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DeepGEMM integrates smoothly with the dispatch modes of DeepEP:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For the contiguous layout kernel, which is used with normal dispatch in the prefill phase, an additional step is required. Since normal dispatch outputs a symbolic shape, a permutation is needed to transform the output into the contiguous format expected by the kernel. We referred to the LightLLM project and implemented a custom Triton kernel for efficient permutation. This kernel ensures that the output from normal dispatch is correctly rearranged, enabling smooth integration with the contiguous GEMM kernel.&lt;/item&gt;
      &lt;item&gt;The masked layout kernel pairs seamlessly with DeepEP’s low-latency dispatch, as both are optimized for the decode phase and support CUDA Graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SGLang also integrates DeepGEMM for MoE computation under tensor parallelism. Additionally, DeepGEMM provides a highly efficient general GeMM kernel, which can be activated in SGLang by setting the environment variable &lt;code&gt;SGL_ENABLE_JIT_DEEPGEMM&lt;/code&gt; to 1, offering even greater computational efficiency for non-MoE operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Two-batch Overlap&lt;/head&gt;
    &lt;p&gt;In multi-node environments, limited communication bandwidth can significantly increase overall latency. To tackle this challenge, we implemented Two-batch Overlap (TBO) following DeepSeek's system design. TBO splits a single batch into two micro-batches, allowing computation and communication to overlap, which also lowers peak memory usage by halving the effective batch size. However, putting TBO into practice introduces specific implementation difficulties.&lt;/p&gt;
    &lt;head rend="h5"&gt;Implementation Challenges&lt;/head&gt;
    &lt;p&gt;Although DeepSeek released the design framework of TBO, there are two slight implementation challenges.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code Complexity: Directly coding TBO can lead to duplicated logic for managing multiple micro-batches. This increases the complexity of the codebase, making it harder to maintain and prone to errors, especially as the number of micro-batches or overlapping scenarios grows.&lt;/item&gt;
      &lt;item&gt;Synchronization Issues in the Prefill Phase: Achieving effective overlap between computation and communication needs consideration when the normal dispatch in DeepEP block the CPU. This blocking behavior can stall the pipeline, leaving the GPU idle and undermining the performance benefits of TBO.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Abstraction for Clean Implementation&lt;/head&gt;
    &lt;p&gt;To create a more maintainable and reusable codebase, we use an abstraction layer consisting of operations and yield points. This method simplifies development by allowing us to write code as if handling a single micro-batch, while strategically pausing execution by inserting yield points to let other micro-batches proceed. It eliminates code duplication, reduces the potential need for variable postfixes, and efficiently manages cases where some executions complete at a layer's end while others have not. Additionally, it supports easy adaptation to different overlapping region choices or future enhancements, like a three-batch overlap, with minimal code changes. Below is a concise demonstration of this approach:&lt;/p&gt;
    &lt;code&gt;operations = [
    self._forward_attn,
    YieldOperation(),  # Pause execution for other micro-batches
    self._forward_dispatch,
    self._forward_mlp,
    YieldOperation(),  # Another pause point
    self._forward_combine,
]

# Process a single micro-batch without duplicating code
def _forward_attn(self, state):
    state.hidden_states = self.self_attn(state.hidden_states, ...)
&lt;/code&gt;
    &lt;head rend="h5"&gt;Prefill Overlapping Implementation&lt;/head&gt;
    &lt;p&gt;We refine the launch order during the prefill phase to avoid CPU-blocking via the dispatch operation in DeepEP, even though we are using its asynchronous mode. Specifically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The dispatch operation blocks the CPU until the GPU receives metadata from other ranks to allocate correctly sized tensors.&lt;/item&gt;
      &lt;item&gt;An improper implementation would leave the computation stream idle during this period, as no computation tasks are submitted to the GPU.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To optimize, we prioritize submitting computation tasks to the GPU before launching CPU-blocking communication. This ensures the GPU remains active during communication. As illustrated in the figure below, TBO with a proper launch order, indicated by bolded borders, avoids bubble caused by a CPU-blocking operation (i.e., normal dispatch).&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert Parallelism Load Balancer&lt;/head&gt;
    &lt;p&gt;In MoE models, EP often leads to uneven workload distribution across GPUs. This imbalance forces the system to wait for the slowest GPU computation or communication, wasting compute cycles and increasing memory usage due to expert activations. As the number of GPUs (EP size) increases, the imbalance issue gets more severe.&lt;/p&gt;
    &lt;p&gt;To address this, DeepSeek developed the Expert Parallelism Load Balancer (EPLB). EPLB takes expert distribution statistics as input and computes an optimal arrangement of experts to minimize imbalance. Users can allocate redundant experts (e.g., 32 additional experts), which, when combined with the original 256, create a pool of 288 experts. This pool allows EPLB to strategically place or replicate experts—for instance, duplicating the most frequently used expert multiple times or grouping a moderately used expert with rarely used ones on a single GPU.&lt;/p&gt;
    &lt;p&gt;Beyond balancing workloads, EPLB offers greater flexibility in parallelism design. With the original 256 experts, parallelism sizes are restricted to powers of two. EPLB’s use of 288 experts enables more diverse configurations, such as parallelism sizes of 12 or 72.&lt;/p&gt;
    &lt;p&gt;In the figure below, we demonstrate the effects of scale and EPLB algorithm to the imbalance issue via simulation. We compute GPU balancedness as the ratio between mean computation time and maximum computation time for a MoE layer among GPUs, and we use the number of tokens for a GPU to estimate the computation time for it. As can be seen, utilization rate decreases when the system scales with the number of nodes, and enabling EPLB significantly improves the utilization.&lt;/p&gt;
    &lt;head rend="h5"&gt;EPLB for Real-World Serving&lt;/head&gt;
    &lt;p&gt;For EPLB to be effective, the input distribution must closely match the actual serving workload. Two strategies enhance this alignment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Increasing Batch Size: Larger batches reduce random fluctuations in expert usage, which improves balance, which can be achieved by scaling the cluster or using techniques like Multi-Token Prediction (MTP).&lt;/item&gt;
      &lt;item&gt;Periodic Rebalancing: Regularly updating the expert arrangement leverages temporal locality but requires efficient reloading of experts. This necessitates minimizing the cost of expert reloading operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with EPLB, some imbalance is inevitable, making further optimization a valuable future direction.&lt;/p&gt;
    &lt;head rend="h5"&gt;Implementation of Rebalancing&lt;/head&gt;
    &lt;p&gt;SGLang implements expert rebalancing in three stages to ensure efficiency and minimal disruption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;System Loading Stage: Weights are optionally preloaded from disk to main memory for faster rebalancing or kept on disk with memory mapping (mmap) for reduced memory usage.&lt;/item&gt;
      &lt;item&gt;Rebalance Preparation Stage: Required weights are asynchronously transferred to device memory in the background, utilizing free DMA hardware engines without interrupting ongoing GPU operations.&lt;/item&gt;
      &lt;item&gt;Rebalance Execution Stage: A device-to-device copy updates the weights. This step can be further optimized through physical memory rebinding techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This staged approach ensures that rebalancing is both efficient and non-disruptive, maintaining system performance during updates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluation&lt;/head&gt;
    &lt;head rend="h3"&gt;End-to-end Performance&lt;/head&gt;
    &lt;head rend="h5"&gt;Experimental Setup&lt;/head&gt;
    &lt;p&gt;We evaluated the end-to-end performance of different configurations of SGLang using DeepSeek-V3 on a cluster of 12 nodes, connected via InfiniBand and each equipped with 8 H100 GPUs. This evaluation highlights the throughput improvements enabled by our advanced optimization techniques. We compared the following four settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SGLang with TP16 x 6: Every two nodes are paired with an independent group, running DeepSeek-V3 inference with a TP size of 16 and DP attention.&lt;/item&gt;
      &lt;item&gt;SGLang with PD Disaggregation: This version incorporates PD disaggregation and full EP optimization. For the EPLB, we adopt a distribution matching the input/output data, as real-time serving statistics are unavailable.&lt;/item&gt;
      &lt;item&gt;SGLang with PD Disaggregation and simulated MTP: To simulate MTP’s effects, we firstly double the batch size and halve the Key-Value KV cache length to maintain the same workload for GroupedGeMM computation and memory access. Moreover, we insert dummy kernels after the real attention computation to ensure the attention phase takes the same time as in DeepSeek’s profile, accurately reflecting the slowdown caused by MTP’s attention mechanism. We conservatively assume a 70% acceptance rate under MTP.&lt;/item&gt;
      &lt;item&gt;DeepSeek Profile Results: Throughput estimates are derived from DeepSeek’s official profiling data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Performance Analysis of Prefill and Decode Phases&lt;/head&gt;
    &lt;p&gt;To accommodate varying workload demands, we independently evaluated the prefill (P) and decode (D) phases, assuming unlimited resources for the non-tested phase to isolate and maximize the load on the tested nodes—mirroring the setup used by DeepSeek. The results are summarized below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prefill Phase: On 4 nodes (4×8×H100, EP32), the system achieved per-node throughputs of 57,674, 54,543, and 50,302 tokens per second for prompt lengths of 1K, 2K, and 4K, respectively. As shown in the bar chart below, this represents up to a 3.3× improvement over the TP16 baseline, largely attributable to the optimized GroupedGeMM kernel (DeepGEMM) and two-batch overlap. Assuming a perfectly balanced workload, our system’s throughput is within 5.6% of DeepSeek's official profile.&lt;/item&gt;
      &lt;item&gt;Decode Phase: Evaluated on 9 nodes (9×8×H100, EP72; half the scale of DeepSeek), the system achieved 22,282 tokens/sec per node for 2K inputs—representing a 5.2× speedup over the TP16 baseline. Under simulated MTP conditions—with attention kernels intentionally slowed to reflect real-world latency—the system sustained a high throughput of 17,373 tokens/sec per node for 4K inputs, just 6.6% below DeepSeek’s official profile. As shown in the figure on the right, these performance gains are largely attributed to 4× larger batch sizes enabled by EP, which enhances scalability by significantly reducing per-GPU memory consumption of model weights.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Profile Results&lt;/head&gt;
    &lt;p&gt;This section compares SGLang’s performance with DeepSeek’s inference system, aligning our experimental setup as closely as possible to DeepSeek’s production environment. We analyze overall throughput and detailed kernel breakdowns, benchmarking against DeepSeek’s blog and public profile data.&lt;/p&gt;
    &lt;head rend="h5"&gt;Overall Throughput&lt;/head&gt;
    &lt;p&gt;For prefill, we tested a scenario with 16,384 tokens per device and an input length of 4,096. Due to uncertainty in DeepSeek’s expert distribution, we evaluated two cases: one with default expert distribution and another with simulated perfect EPLB (random expert selection following group-limited routing semantics) as a performance upper bound.&lt;/p&gt;
    &lt;p&gt;The results are presented below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;DeepSeek Blog (excl. cache hit)&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek Profile&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang (Default)&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang + Simulated Perfect EPLB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Input Length&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput (per node)&lt;/cell&gt;
        &lt;cell&gt;32,206&lt;/cell&gt;
        &lt;cell&gt;62,713&lt;/cell&gt;
        &lt;cell&gt;50,302&lt;/cell&gt;
        &lt;cell&gt;59,337&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;DeepSeek’s profile reports a throughput roughly twice that of its production environment. SGLang with default expert imbalance is 20% slower than DeepSeek’s profile, while the simulated perfect EPLB case narrows the gap to 6%.&lt;/p&gt;
    &lt;p&gt;For decode, the results are shown below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;DeepSeek Blog&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek Profile&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang (Default)&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang + Simulated MTP (Slow Attention)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
        &lt;cell&gt;256&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;KV Cache Length&lt;/cell&gt;
        &lt;cell&gt;4,989&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;2,000&lt;/cell&gt;
        &lt;cell&gt;4,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Number of Nodes&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput (per node)&lt;/cell&gt;
        &lt;cell&gt;14,800&lt;/cell&gt;
        &lt;cell&gt;18,598&lt;/cell&gt;
        &lt;cell&gt;22,282&lt;/cell&gt;
        &lt;cell&gt;17,373&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Using half the nodes of DeepSeek, SGLang with simulated MTP is only slightly slower than DeepSeek’s profile. In a higher batch size setting (256 sequences, 2,000 input length), SGLang achieves 22,282 tokens per second per node, demonstrating strong scalability.&lt;/p&gt;
    &lt;head rend="h5"&gt;Detail Breakdown&lt;/head&gt;
    &lt;p&gt;The figure below breaks down kernel execution times for prefill, including unit test results as a theoretical upper bound:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default EPLB: Communication kernels exhibit longer execution times and higher variance compared to DeepSeek’s profile, likely due to greater expert imbalance. This leads to extended computation stream bubbles, slowing down overall performance.&lt;/item&gt;
      &lt;item&gt;Simulated Perfect EPLB: This setup aligns more closely with DeepSeek’s profile, though discrepancies remain, indicating potential areas for optimization.&lt;/item&gt;
      &lt;item&gt;Comparison with Unit Tests: Both DeepSeek and SGLang have a communication time slower than unit test results, while the latter is achievable when disabling TBO, revealing a potential optimization direction if communication is the bottleneck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SGLang’s decode kernel breakdown aligns closely with DeepSeek’s, as shown below:&lt;/p&gt;
    &lt;p&gt;Key observations include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Combine Time Discrepancy: SGLang’s combine operation appears 2x slower than DeepSeek’s due to shorter attention computation, causing communication kernels to busy-wait. In the simulated slow attention experiment, combine time matches DeepSeek’s, confirming this hypothesis.&lt;/item&gt;
      &lt;item&gt;MoE Performance: SGLang’s MoE kernels are 25% slower, possibly because DeepSeek’s 18 nodes (versus our 9) distribute experts more efficiently, reducing memory access overhead for GEMM operations.&lt;/item&gt;
      &lt;item&gt;Dispatch Optimization Potential: Both DeepSeek and SGLang show dispatch times of ~0.17ms per layer, but unit tests with DeepEP reveal a potential of 0.06ms occupying SMs. Currently, dispatch spends significant time busy-waiting for data. Inserting slow dummy kernels between send/receive operations reduces dispatch time to 0.09ms, and in-flight duration analysis using unit test data suggests further improvements are possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While minor enhancements remain—primarily in kernel fusion under "Other Kernels"—SGLang’s decode performance is largely aligned with DeepSeek’s, with prefill optimization as the next focus.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ablation Study: Two-batch Overlap&lt;/head&gt;
    &lt;head rend="h5"&gt;Impact of Batch Size and Attention Time&lt;/head&gt;
    &lt;p&gt;This section investigates TBO performance across varying batch sizes and simulated MTP scenarios.&lt;/p&gt;
    &lt;p&gt;TBO delivers two significant benefits in the prefill phase, as evidenced by throughput comparisons and memory usage optimizations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for Larger Batch Sizes: In the vanilla configuration, each device processes up to 8,192 tokens before encountering out-of-memory (OOM) errors at 16,384 tokens. TBO mitigates this by optimizing memory usage for input tokens, enabling inference with batches as large as 16,384 tokens per device. This further boosts performance to 40.5% increase when comparing the TBO flag with all other configurations made optimal.&lt;/item&gt;
      &lt;item&gt;Enhanced Throughput: By overlapping computation (e.g., attention and MLP phases) with communication (e.g., DeepEP Combine and Dispatch), TBO achieves a 27% to 35% throughput increase compared to the vanilla setup, even when processing the same token count per device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TBO’s impact in the decode phase varies by scenario, with performance tied to batch size and attention processing time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real Test Cases: Speedup in practical scenarios is contingent on batch size exceeding a threshold between 64 and 128 tokens. Below this, TBO yields minimal or negative gains (e.g., -27% at 32 tokens/device), as small decode batch sizes hinder kernel efficiency. The speedup reaches 25.5% at 256 tokens with a performance of 22,310 tokens per second.&lt;/item&gt;
      &lt;item&gt;Simulated MTP Scenario: TBO provides the most substantial speedup in simulated MTP cases when processing 128 requests to generate 256 tokens per decode step. This is due to prolonged attention processing time, which aligns computation (e.g., DP Attention layers) with DeepEP communication overhead (e.g., combine and dispatch steps). The evaluation shows a 35% speedup at 128 sequences/device, with throughput 17,552 tokens per second compared to 12,929 without TBO.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Detail Breakdown&lt;/head&gt;
    &lt;p&gt;We evaluated three prefill scenarios: TBO with 16k tokens per batch, TBO with 8k tokens, and no-TBO with 8k tokens. The figure below reveals key insights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TBO Efficiency: Comparing the 8k cases, TBO improves overall efficiency by overlapping computation and communication, as expected.&lt;/item&gt;
      &lt;item&gt;Batch Size Impact: Reducing the batch size from 16k to 8k with TBO results in a slight slowdown, reflecting diminished kernel efficiency with smaller batches.&lt;/item&gt;
      &lt;item&gt;Kernel Performance: Interestingly, the no-TBO 8k case outperforms the TBO 16k case in per-kernel speed, despite both having an effective batch size of 8k for kernels. This may stem from reduced streaming multiprocessors (SMs) with TBO, potential noisy neighbor effects during overlap, or kernel incompatibility between computation and communication. These findings suggest future optimization directions for SGLang.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the decode phase, we analyzed three configurations: TBO with a batch size of 256, no-TBO with 256, and no-TBO with 128. The time breakdown is shown below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TBO vs. No-TBO (Batch Size 256): Without TBO, communication time increases significantly due to the lack of overlap. However, computation kernels, particularly GEMM, benefit from a larger effective batch size, resulting in faster execution.&lt;/item&gt;
      &lt;item&gt;TBO (256) vs. No-TBO (128): Comparing cases with the same kernel batch size, only non-overlapped communication slows down in the no-TBO setup, while computation remains consistent. Unlike prefill, decode communication kernels either fully utilize SMs (during send/receive) or none (during inflight waiting), avoiding resource contention with computation kernels.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ablation Study: EPLB&lt;/head&gt;
    &lt;p&gt;This section evaluates the impact of the EPLB on system performance through overall throughput analysis and detailed case studies. Given EPLB's sensitivity to workload distribution and distribution shifts in production environments, we focus on qualitative and generalizable insights rather than real-world performance, which requires production data.&lt;/p&gt;
    &lt;head rend="h5"&gt;Overall Results&lt;/head&gt;
    &lt;p&gt;The figure below illustrates EPLB's effect on throughput in large-scale settings. EPLB delivers a significant speedup of 1.49x (prefill) and 2.54x (decode), as expected, due to its ability to mitigate workload imbalances across GPUs. As the number of ranks scales, imbalances grow, and EPLB effectively addresses this in our large-scale experiments, leading to notable throughput improvements.&lt;/p&gt;
    &lt;head rend="h5"&gt;Case Study: Workload Imbalance Versus Overall Throughput&lt;/head&gt;
    &lt;p&gt;To explore the relationship between workload imbalance and throughput, we conducted a case study using a decode experiment with 1800 input tokens, 100 output tokens, and a batch size of 256. Throughput and balancedness (average token count divided by maximum token count across experts) were plotted against decoding steps:&lt;/p&gt;
    &lt;p&gt;The results reveal a strong correlation between balancedness and throughput, emphasizing the importance of maintaining high balancedness for optimal performance.&lt;/p&gt;
    &lt;head rend="h5"&gt;Case Study: Expert Distribution Statistics&lt;/head&gt;
    &lt;p&gt;The following figure presents expert distribution statistics for prefill and decode sample data:&lt;/p&gt;
    &lt;p&gt;Key observations include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Imbalance in Expert Usage: Most experts are infrequently used, while a small subset is heavily utilized, underscoring the inherent imbalance in MoE models.&lt;/item&gt;
      &lt;item&gt;Prefill vs. Decode Differences: Although prefill and decode distributions share similarities, notable differences exist. This supports the use of PD disaggregation, which enables distinct expert placements for each phase, optimizing performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These findings highlight EPLB's role in addressing workload imbalances and the value of tailoring expert placement to phase-specific demands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Toolkits&lt;/head&gt;
    &lt;head rend="h3"&gt;Disposable Tensor&lt;/head&gt;
    &lt;p&gt;Memory management in PyTorch can be challenging due to persistent object references, especially in GPU-intensive workflows where CUDA memory is a scarce resource. Consider the following example:&lt;/p&gt;
    &lt;code&gt;def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    del hidden_state  # Attempt to free memory, but no effect due to external reference
    return linear2(nn.ReLU(intermediate_state))

hidden_state = ffn(hidden_state, linear1, linear2)
&lt;/code&gt;
    &lt;p&gt;In this code, &lt;code&gt;del hidden_state&lt;/code&gt; is intended to release the memory occupied by &lt;code&gt;hidden_state&lt;/code&gt; after &lt;code&gt;intermediate_state&lt;/code&gt; is computed. However, as &lt;code&gt;hidden_state&lt;/code&gt; is still referenced outside the function, the &lt;code&gt;del&lt;/code&gt; operation has no effect. This increases peak memory usage, risking performance slowdowns or out-of-memory errors.&lt;/p&gt;
    &lt;p&gt;SGLang addresses this with the DisposableTensor class, a subclass of &lt;code&gt;torch.Tensor&lt;/code&gt; which introduces a dispose() method to explicitly and immediately release a tensor’s memory, circumventing Python’s reference counting limitations. Here’s how it works:&lt;/p&gt;
    &lt;code&gt;def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    hidden_state.dispose()  # Immediately releases CUDA memory
    return linear2(nn.ReLU(intermediate_state))

# Wrap the tensor in DisposableTensor
hidden_state = DisposableTensor(hidden_state)
hidden_state = ffn(hidden_state, linear1, linear2)
&lt;/code&gt;
    &lt;p&gt;By wrapping &lt;code&gt;hidden_state&lt;/code&gt; in a &lt;code&gt;DisposableTensor&lt;/code&gt; and calling &lt;code&gt;dispose()&lt;/code&gt; when it’s no longer needed, the CUDA memory is freed right away. This ensures that memory is released as soon as the tensor’s role in the computation is complete, reducing peak memory usage and improving overall efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert Workload Extraction and Simulation&lt;/head&gt;
    &lt;p&gt;SGLang also includes a toolset for analyzing and simulating expert workload distribution in MoE models. This feature enables users to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dump Expert Workload Statistics: Extract either accumulated statistics or per-batch workload data. Accumulated stats support the EPLB manager for real-time optimization, while per-batch data provides granular insights for analysis and simulation.&lt;/item&gt;
      &lt;item&gt;Simulate Expert Utilization: Model expert balance across various configurations without requiring costly hardware or repeated trials. For instance, users can gather workload data from a modest setup (e.g., 2x8xH100 or 8xH200) and simulate the performance for a large-scale 22-node deployment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This simulation capability allows users to evaluate how factors like rebalancing frequency, node count, or batch size impact system performance. It’s a cost-effective way to fine-tune configurations before scaling up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations and Future Work&lt;/head&gt;
    &lt;p&gt;While our implementation of SGLang for DeepSeek-V3 inference demonstrates significant throughput improvements, several limitations and areas for future enhancement remain:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Latency Optimization: The current focus on throughput leaves Time to First Token (TTFT) at 2–5 seconds and Inter-Token Latency (ITL) at approximately 100ms, requiring further optimizations for real-time use cases.&lt;/item&gt;
      &lt;item&gt;Sequence Length Constraints: Limited to shorter sequences due to the use of 96 GPUs. Expanding GPU resources would support longer sequences, essential for specific applications.&lt;/item&gt;
      &lt;item&gt;Multi-Token Prediction (MTP) Integration: SGLang supports MTP but lacks full integration with DP attention, reducing efficiency in mixed parallelism configurations.&lt;/item&gt;
      &lt;item&gt;EPLB Distribution: The experiments in this blog utilizes in-distribution data for Expert Parallelism Load Balancer (EPLB), which may not reflect real-world variability. Future work should experiment performances when having distribution shifts.&lt;/item&gt;
      &lt;item&gt;Flexible Tensor Parallelism (TP) Sizes: For DeepSeek-V3, memory-optimal TP sizes for dense FFNs are small but larger than 1. Currently, SGLang only supports pure TP or DP, leading to suboptimal memory use. Flexible TP options are needed.&lt;/item&gt;
      &lt;item&gt;Blackwell Support: Currently, our implementation supports only the NVIDIA Hopper architecture. We are actively working to extend compatibility to the next-generation Blackwell architecture. If you are interested in supporting or sponsoring this development, welcome to contact lmsys.org@gmail.com.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;By leveraging PD disaggregation, EP, and a carefully crafted parallelism design, we’ve reproduced DeepSeek’s inference framework in SGLang with exceptional performance. Our open-source efforts—achieving 52.3k input tokens per second and 22.3k output tokens per second—demonstrate SGLang’s power for large-scale LLM inference. We invite the community to explore, replicate, and extend this work to push the boundaries of efficient AI deployment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgment&lt;/head&gt;
    &lt;p&gt;We would like to express our heartfelt gratitude to the following teams and collaborators:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SGLang Core Team and Community Contributors — Jingyi Chen, Cheng Wan, Liangsheng Yin, Baizhou Zhang, Ke Bao, Jiexin Liang, Xiaoyu Zhang, Yanbo Yang, Fan Yin, Chao Wang, Laixin Xie, Runkai Tao, Yuhong Guo, Kaihong Zhang, Lei Yu, Yu-Hsuan Tseng, Qilin Tian, Peng Zhang, Yi Zhang, Yineng Zhang, Byron Hsu, and many others.&lt;/item&gt;
      &lt;item&gt;Atlas Cloud Team — Jerry Tang, Wei Xu, Simon Xue, Harry He, Eva Ma, and colleagues — for providing a 96-device NVIDIA H100 cluster and offering responsive engineering support.&lt;/item&gt;
      &lt;item&gt;NVIDIA Solution Architect Team — Xuting Zhou, Jinyan Chen, and colleagues — for their work on the seamless integration of expert parallelism.&lt;/item&gt;
      &lt;item&gt;NVIDIA Enterprise Product Team — Trevor Morris, Elfie Guo, Kaixi Hou, Kushan Ahmadian, and colleagues — for optimizing the DeepSeek R1 kernels.&lt;/item&gt;
      &lt;item&gt;LinkedIn Team — Biao He, Qingquan Song, Chunan Zeng, Yun Dai, Yubo Wang, and colleagues — for optimizing the Flash-Attention 3 backend.&lt;/item&gt;
      &lt;item&gt;Mooncake Team — Shangming Cai, Teng Ma, Mingxing Zhang, and colleagues — for their collaboration on PD disaggregation in SGLang.&lt;/item&gt;
      &lt;item&gt;FlashInfer Team — Zihao Ye, Yong Wu, Yaxing Cai — for additional DeepSeek R1 kernel optimizations.&lt;/item&gt;
      &lt;item&gt;Dynamo Team - Kyle Kranen, Vikram Sharma Mailthody, and colleagues - for extra support on PD disaggregation in SGLang.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you all for your invaluable support and collaboration.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;Related PRs: #1970 #2925 #4068 #4165 #4232 #4390 #4435 #4521 #4654 #4767 #4770 #4836 #4880 #4957 #5068 #5085 #5295 #5415 #5432 #5435 #5530 #5558 #5561 #5626 #5657 #5805 #5819 #5890 DeepEP#142&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lmsys.org/blog/2025-05-05-large-scale-ep/"/></entry><entry><id>https://news.ycombinator.com/item?id=45065705</id><title>Essential Coding Theory [pdf]</title><updated>2025-08-30T15:33:29.268571+00:00</updated><content/><link href="https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf"/></entry><entry><id>https://news.ycombinator.com/item?id=45066060</id><title>Wikipedia as a Graph</title><updated>2025-08-30T15:33:29.157303+00:00</updated><content/><link href="https://wikigrapher.com/paths"/></entry><entry><id>https://news.ycombinator.com/item?id=45066395</id><title>John Carmack's arguments against building a custom XR OS at Meta</title><updated>2025-08-30T15:33:28.751390+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/ID_AA_Carmack/status/1961172409920491849"/></entry><entry><id>https://news.ycombinator.com/item?id=45068091</id><title>Do the simplest thing that could possibly work</title><updated>2025-08-30T15:33:28.538653+00:00</updated><content>&lt;doc fingerprint="7d78985f2d453935"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Do the simplest thing that could possibly work&lt;/head&gt;
    &lt;p&gt;When designing software systems, do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;p&gt;It’s surprising how far you can take this piece of advice. I genuinely think you can do this all the time. You can follow this approach for fixing bugs, for maintaining existing systems, and for architecting new ones.&lt;/p&gt;
    &lt;p&gt;A lot of engineers design by trying to think of the “ideal” system: something well-factored, near-infinitely scalable, elegantly distributed, and so on. I think this is entirely the wrong way to go about software design. Instead, spend that time understanding the current system deeply, then do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple can be underwhelming&lt;/head&gt;
    &lt;p&gt;System design requires competence with a lot of different tools: app servers, proxies, databases, caches, queues, and so on. As they gain familiarity with these tools, junior engineers naturally want to use them. It’s fun to construct systems out of many different components! And it feels very satisfying to draw boxes and arrows on a whiteboard - like you’re doing real engineering.&lt;/p&gt;
    &lt;p&gt;However, as with many skills, real mastery often involves learning when to do less, not more. The fight between an ambitious novice and an old master is a well-worn cliche in martial arts movies: the novice is a blur of motion, flipping and spinning. The master is mostly still. But somehow the novice’s attacks never seem to quite connect, and the master’s eventual attack is decisive.&lt;/p&gt;
    &lt;p&gt;In software, this means that great software design looks underwhelming. It doesn’t look like anything much is happening at all. You can tell you’re in the presence of great software design because you start having thoughts like “oh, I didn’t realise the problem was that easy” or “oh nice, you don’t actually have to do anything difficult”.&lt;/p&gt;
    &lt;p&gt;Unicorn is great software design, because it delivers all the most important guarantees in a web server (request isolation, horizontal scaling, crash recovery) by leaning on Unix primitives1. The industry-standard Rails REST API is great software design, because it gives you exactly what you need for a CRUD app in the most boring way possible. I don’t think any of these are impressive software. But they’re impressive feats of design, because they do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;p&gt;You should do that too! Suppose you’ve got a Golang application that you want to add some kind of rate limiting to. What’s the simplest thing that could possibly work? Your first idea might be to add some kind of persistent storage (say, Redis) to track per-user request counts with a leaky-bucket algorithm. That would work! But do you need a whole new piece of infrastructure? What if instead you kept those per-user request counts in-memory? Sure, you’d lose some rate limiting data when the application is restarted, but does that matter? Actually, are you sure your edge proxy2 doesn’t support rate limiting already? Could you just write a couple of lines in a config file instead of implementing the feature at all?&lt;/p&gt;
    &lt;p&gt;Maybe your edge proxy doesn’t support rate limiting. Maybe you can’t track it in-memory because you have too many server instances running in parallel, so the tightest rate limit you could enforce that way is too wide. Maybe it’s a dealbreaker if you ever lose rate limiting data, because people are hammering your service that hard. In that case, the simplest thing that could possibly work is adding persistent storage, so you should go and do that. But if you could do one of the easier approaches, wouldn’t you want to?&lt;/p&gt;
    &lt;p&gt;You really can build a whole application from scratch this way: start with the absolute simplest thing, and then only extend it when you have new requirements that force you to. It sounds silly, but it works. Think of it as taking YAGNI as the ultimate design principle: above single-responsibility, above choosing the best tool for the job, and above “good design”.&lt;/p&gt;
    &lt;head rend="h3"&gt;What’s wrong with doing the simplest thing?&lt;/head&gt;
    &lt;p&gt;Of course, there are three big problems with always doing the simplest thing that could possibly work. The first is that, by not anticipating future requirements, you end up with an inflexible system or a big ball of mud. The second is that it’s not clear what “simplest” means, so at worst I’m saying “to design well, always do good design”. The third is that you ought to be building systems that can scale, not systems that just work right now. Let’s take those objections in order.&lt;/p&gt;
    &lt;head rend="h4"&gt;Big balls of mud&lt;/head&gt;
    &lt;p&gt;To some engineers, “do the simplest thing that could possibly work” sounds like I’m telling them to stop doing engineering. If the simplest thing is usually a quick kludge, does that mean this advice will inevitably lead to a complete mess? We’ve all seen codebases with hacks stacked on top of hacks, and they definitely don’t look like good design.&lt;/p&gt;
    &lt;p&gt;But are hacks simple? I actually don’t think so. The problem with a hack or a kludge is precisely that it isn’t simple: that it adds complexity to the codebase by introducing another thing you have to always remember. Hacks are just easier to think of. Figuring out the proper fix is hard because it requires having to understand the entire codebase (or large sections of it). In fact, the proper fix is almost always much simpler than the hack.&lt;/p&gt;
    &lt;p&gt;It is not easy to do the simplest thing that could possibly work. When you’re looking at a problem, the first few solutions that come to mind are unlikely to be the simplest ones. Figuring out the simplest solution requires considering many different approaches. In other words, it requires doing engineering.&lt;/p&gt;
    &lt;head rend="h4"&gt;What is simplicity?&lt;/head&gt;
    &lt;p&gt;Engineers disagree a lot about what constitutes simple code. If “simplest” already means “with good design”, is it just a tautology to say “you should do the simplest thing that could possibly work?” In other words, is Unicorn really simpler than Puma3? Is adding in-memory rate limiting really simpler than using Redis? Here’s a rough, intuitive definition of simplicity4:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple systems have fewer “moving pieces”: fewer things you have to think about when you’re working with them&lt;/item&gt;
      &lt;item&gt;Simple systems are less internally-connected. They are composed from components with clear, straightforward interfaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unix processes are simpler than threads (and thus Unicorn is simpler than Puma) because processes are less connected: they do not share memory. This makes a lot of sense to me! But I don’t think it gives you the tools to figure out what’s simpler in every case.&lt;/p&gt;
    &lt;p&gt;What about in-memory rate limiting vs Redis? On the one hand, in-memory is simpler because you don’t have to think about all the things involved in standing up a separate service with persistent memory. On the other hand, Redis is simpler because the rate limiting guarantees it offers are more straightforward - you don’t have to worry about the case where one server instance thinks a user is rate limited and another one doesn’t.&lt;/p&gt;
    &lt;p&gt;When I’m not sure what “seems” simpler to me, I like to use this tiebreaker: simple systems are stable. If you’re comparing two states of a software system, and one will require more ongoing work if no requirements change, the other one is simpler. Redis must be deployed and maintained, it can have its own incidents, it requires its own monitoring, it requires a separate deployment in any new environments the service finds itself in, and so on. Thus in-memory rate limiting is simpler than Redis5.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why wouldn’t you want to be scalable?&lt;/head&gt;
    &lt;p&gt;A certain type of engineer is now screaming to themselves “but in-memory rate limiting won’t scale!” Doing the simplest thing that could possibly work will emphatically not deliver the most web-scale system. It will deliver a system that works well at the current scale. Is this irresponsible engineering?&lt;/p&gt;
    &lt;p&gt;No. In my view, the cardinal sin of big tech SaaS engineering is an obsession with scale. I’ve seen so much unavoidable pain caused by over-engineering systems to prepare for several orders of magnitude more than the current scale.&lt;/p&gt;
    &lt;p&gt;The main reason to not try this is that it doesn’t work. In my experience, for any non-trivial codebase, you can’t anticipate how it will behave at several orders of magnitude more traffic, because you don’t know ahead of time where all the bottlenecks are going to be. At most you can try to make sure you’re ready for 2x or 5x the current traffic, and then stand by to deal with problems as they come in.&lt;/p&gt;
    &lt;p&gt;The other reason not to try this is that it makes your codebase inflexible. It’s fun to decouple your service into two pieces so they can be scaled independently (I have seen this happen maybe ten times, and I have seen them actually be usefully scaled independently maybe once). But that makes certain features very hard to implement, because they now require coordination over the wire. In the worst case, they require transactions over the wire, which is a genuinely hard engineering problem. Most of the time you just don’t have to do any of this!&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;The longer I spend working in tech, the less optimistic I become about our collective ability to predict where a system is going. It’s hard enough to get your head around where a system currently is. And in fact, that’s the main practical difficulty in doing good design: getting an accurate big-picture understanding of the system. Most design is done without that understanding, and most design is thus pretty bad.&lt;/p&gt;
    &lt;p&gt;There are, broadly speaking, two ways to develop software. The first is to predict what your requirements might look like six months or a year from now, and then design the best system for that purpose. The second is to design the best system for what your requirements actually look like right now: in other words, to do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;p&gt;edit: this article has gotten some comments on Hacker News.&lt;/p&gt;
    &lt;p&gt;One interesting comment thread says that simplicity of architecture doesn’t matter at scale, because the complexity of “state space exploration in implementation” (I think that means something like what I wrote about here) dominates any other complexity. I disagree - the more complex your feature interactions become, the more important a simple architecture becomes, because your “complexity budget” is almost exhausted.&lt;/p&gt;
    &lt;p&gt;I also want to credit Ward Cunningham and Kent Beck for inventing the expression - I genuinely thought I’d just come up with the wording myself, but I almost certainly just remembered it. Oops! Thanks to the HN user ternaryoperator for pointing this out.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;It’s just Unix sockets and forked processes! I love Unicorn.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Every tech company has some kind of edge proxy.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I do like Puma and think it’s a good web server. There are definitely use cases where you’d pick it over Unicorn (though in those cases I would personally think hard about using a different language than Ruby).&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I’m influenced here by Rich Hickey’s great talk Simple Made Easy. I don’t agree with all of it (I think familiarity does in fact contribute to simplicity in practice) but it’s definitely worth watching.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Of course, if the system has to scale horizontally more than a little bit, in-memory rate limiting won’t work and must be replaced with something like Redis. But in my experience a Golang service can scale a lot without having to scale horizontally to more than a handful of replicas.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;August 28, 2025 │ Tags: software design, shipping&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/the-simplest-thing-that-could-possibly-work/"/></entry><entry><id>https://news.ycombinator.com/item?id=45068986</id><title>The Theoretical Limitations of Embedding-Based Retrieval</title><updated>2025-08-30T15:33:28.265961+00:00</updated><content>&lt;doc fingerprint="7d9ca80b5cf390ce"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Information Retrieval&lt;/head&gt;&lt;p&gt; [Submitted on 28 Aug 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:On the Theoretical Limitations of Embedding-Based Retrieval&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.IR&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2508.21038"/></entry><entry><id>https://news.ycombinator.com/item?id=45070793</id><title>Why Romania excels in international Olympiads</title><updated>2025-08-30T15:33:28.002424+00:00</updated><content>&lt;doc fingerprint="b679f819390b9407"&gt;
  &lt;main&gt;
    &lt;p&gt;Olympiads are international student intellectual competitions in which students from across the world go toe-to-toe answering questions in mathematics, physics, informatics, chemistry, and more. The best performers tend to be from countries like China, the United States, India, and Japan. But, somehow, the southeastern European country of Romania also frequently tops the list.&lt;/p&gt;
    &lt;p&gt;Since 2020, Romania’s performance in the International Mathematical Olympiad (IMO) has been nothing short of amazing. In 2022, Romania came in fifth overall, fourth in 2023, and twelfth in 2024. In 2023, Romania placed fourth globally and first in Europe at the International Physics Olympiad, seventeenth globally and third in Europe at the International Olympiad in Informatics, sixth globally and second in Europe in the European Girls’ Mathematical Olympiad, first in the Balkan Mathematical Olympiad—which also included France, Italy, and the United Kingdom—and first in the Central European Olympiad in Informatics. Romania also performed well in the International Chemistry Olympiad and many others.&lt;/p&gt;
    &lt;p&gt;It’s an understatement to call Romania’s skill in Olympiads merely “overperformance”. Romania’s lackluster performance in international assessments and its relatively small population size of just over 19 million people makes the things they do in Olympiads downright miraculous.&lt;/p&gt;
    &lt;p&gt;Average Romanian educational performance is unimpressive. Romanian youth routinely perform below the average of OECD countries and near the bottom of the pack of European nations. Romania has a poor-to-mediocre showing whether you include or exclude migrants from the calculations, and its scores on assessments like the PISA aren’t low due to being tainted by bias in the examinations. Romania genuinely underperforms. But underperformance is not the impression you would get if you only knew of Romanian education from Olympiads.&lt;/p&gt;
    &lt;p&gt;One possibility is that Romanian students have more variable performance on international assessments than students in other countries. No dice: they aren’t much more variable than the student populations in other countries, and a handful of comparably-sized nations with worse Olympiad performance are more variable. Another possibility is that, for some reason, there’s a fat right tail in Romanian educational performance. If this is true, it just doesn’t show up in any existing data. Given the fact that international assessments indicate Romania’s sampling tends to be population-representative, we should have a strong prior against this possibility. Romanian test scores tend to be distributed along a symmetrical bell curve.&lt;/p&gt;
    &lt;p&gt;Yet another possibility is that Romania has an undersampled ethnic group that overperforms, but whose schools aren’t tested very well. The only group this might be is Romanian Jews and using them as an explanation is problematic for two reasons. The first is that there are too few to realistically explain Romanian Olympiad performance. The second is that we know the identities of Olympiad participants from Romania, and they don’t seem to be Jewish.&lt;/p&gt;
    &lt;p&gt;Something else, something more mysterious, explains why Romania is such an outlier in international intellectual competitions. That thing is, in fact, the unique design of the Romanian educational system.&lt;/p&gt;
    &lt;p&gt;In the late 19th century, Romanian prince regnant Alexandru Ioan Cuza attempted to raise the status of the nation by instituting a mass literacy campaign centered around building free schools that children were compelled to attend. This effort was largely a failure, with literacy failing to break 50% by the 1930s. But World War II precipitated change. In 1948, Romania’s new governing communist party began to bring about serious educational reform at a breakneck pace.The Education Law of 1948 was passed to provoke a military-grade offensive against illiteracy, involving the mass participation of the literate from all walks of life in uplifting the poor, the abandoned, and those who simply shunned education. By the end of the 1950s, illiteracy was practically eradicated among Romania’s youth.&lt;/p&gt;
    &lt;p&gt;The education system that existed in Romania’s communist period was modeled on the system in place in the Soviet Union, and it included a fair helping of political propaganda in addition to physical labor. The system also overproduced schools, resulting in shoddy but widely available facilities dotting the country. Like the Soviet school system, Romania’s was marked by increasing lengths of compulsory education, poor availability of qualified teachers and educational supplies, high budgetary costs, and an extreme level of credential inflation.&lt;/p&gt;
    &lt;p&gt;After the fall of communism, the new democratic government went on to shutter many of these schools and to immediately lower compulsory schooling requirements to put an end to the bureaucratic nightmare that Soviet influence had saddled the country with. In the following years, how Romania wished to ration scarce governmental resources for education was a matter of intense debate, and out of that debate came a strong sentiment that, whatever the system, Romanian education would be structured competitively.&lt;/p&gt;
    &lt;p&gt;Nowadays, the most prestigious Romanian high schools are the National Colleges, or Colegiu Național. These schools are often international and frequently uphold old educational traditions sometimes dating back more than a century. Below these schools are the Liceu Teoretic, which are the norm, offering standard educations. Romania also has three military colleges—Colegiu Militar—managed directly by the Ministry of National Defense. There are also schools focused on service, technical schools, vocational schools, and apprenticeship programs. The brightest students get their pick among these schools after they take the national placement test, the Evaluarea Națională, when they are graduating the 8th grade around ages fourteen to fifteen.&lt;/p&gt;
    &lt;p&gt;The high school placement test is a standardized test covering Romanian language and literature as well as mathematics. Performance on the examination is reported publicly when students are issued a score on a one-to-ten scale with precision to two decimal places. A student who receives a high grade—say 9.65—would have their pick from most any school, whereas a student scoring 5.00 or below would usually be constrained to a less academically-focused form of education like a vocational program. Most students elect to go to the best school they are able to test into, and so the degree of sorting across schools is very high. To make this setup even more extreme, there is also often—but not universally—sorting within schools, as students select into educational tracks. This is done directly when applying to schools.&lt;/p&gt;
    &lt;p&gt;At the end of the Romanian high school experience, there is a graduation test, the Bacalaureat, or bac. This test is marked like the entrance examination and, to pass, students must obtain a score of at least five in the subjects they have elected to take. This testing includes written and oral examinations, assessments of foreign language and computer skills, and, for ethnic minorities, assessment of their skill with their maternal language other than Romanian. The need for a given score on this examination can range from requiring just passing to requiring a high score, depending on the university one intends to attend, if that is their goal.&lt;/p&gt;
    &lt;p&gt;The design of Romania’s educational system makes it perhaps the most stratified educational system in the world. The fact that they have a centralized repository containing all student and teacher educational data makes their system perfect for a high-powered evaluation of exactly what happens when a country opts to hyper-stratify education.&lt;/p&gt;
    &lt;p&gt;One of the cruel parts of the Romanian system is that, though sorting is nationally available, students do not have equal opportunities to sort. Students located in smaller towns have fewer high school options to select from unless they’re among the few who opt into a military academy, which means joining the military. The extent of sorting is far more intense in areas with larger numbers of schools. In a recent paper, the Romanian economist Andrei Munteanu provided an illustration of how this works: essentially, the fewer schools in a locale, the more each individual school contains students with a wider range of ability and, the more schools in a locale, the more each individual school will be stratified into low, middle, or high ability.&lt;/p&gt;
    &lt;p&gt;This combined sorting between schools and tracks means that low-ability students get stuck with other low-ability students, and high-ability students are surrounded by other high-ability students. In effect, peer groups throughout high school are extremely homogeneous. This matters because then low-performing students drag down low-performing students, and high performers cause each other to rise. Romania’s educational system has causal peer impacts on student performance on the graduation test that are very large in both directions, but primarily where there are opportunities for sorting to take place.&lt;/p&gt;
    &lt;p&gt;But peer effects are not everything to Romania’s exceptional Olympiad performance; they are just the fertile ground in which exceptional performance is fostered. The next part has to do with teachers. Like students, Romania’s teachers must take tests to be able to do what they want to do. Teachers naturally prefer to lecture smarter students, and the smartest teachers have their pick of the schools, and even of the tracks. In a paper with extremely robust results, researchers from the last decade described this as such:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[Teachers] with higher certification standards are more likely to work at better-ranked schools. This sorting persists even within schools as one moves from a weaker to a stronger track, and even within tracks as one moves from a weaker to a stronger class.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The best teachers also opt into towns with more schools. It’s apparent, then, that teachers prefer teaching in the highest-achieving places they can be, both within and between towns. The effect of teacher-student ability pairing is accentuated even more by incentives to compete. The government of Romania is not unique in providing monetary rewards for those who win Olympiads, those who teach winners of Olympiads, or those schools Olympiad winners attend, but they are unique in having all the previously-mentioned institutional characteristics on top of providing comprehensive monetary incentives for Olympiad achievement.&lt;/p&gt;
    &lt;p&gt;Romania’s immense success in Olympiads and the widely recognized importance of Olympiad wins for signaling student human capital has also spawned a small number of private schools that advertise their prominence and tutoring capabilities. Many teachers also recommend to parents that they obtain additional tutoring for their brighter pupils, and tutoring services are commonplace. The commonality of tutoring for Olympiad winners is a global constant, whereas the things distinguishing Romania are not.&lt;/p&gt;
    &lt;p&gt;Two notable factors do not increase performance in the same direction. These are very slight decrements in funding allocated to the highest-ability schools, and when parents reduce the time they spend helping their students with homework, conditional on their kids matching into better schools. Another potential factor that militates against the synchrony of resource allocation in Romania is that children in more selective schools report feeling marginalized because they realize that they’re not as strong of students as they believed. The decrements in funding are likely to be unproblematic, because higher-scoring schools tend to be larger and more urban, lending them economies of scale. Due to this, they may have effectively more funding.&lt;/p&gt;
    &lt;p&gt;With all the pieces on the board, the key to Romania’s Olympiad success is three-fold: put the best students in the same classrooms, put the best teachers with the best students, and then incentivize schools, teachers, and students each to win Olympiads.&lt;/p&gt;
    &lt;p&gt;This system has proved amazingly fruitful. Given its underlying human capital, the poverty from its communist legacy, and its modest population size, Romania should not perform the way it does in academic Olympiads. And yet it does. The trade-off for Romania, however, is palpable.&lt;/p&gt;
    &lt;p&gt;Large portions of Romania’s Olympiad winners leave the country. Because Romania is a member state of the European Union, the people the country has put great effort into training and credentialing are easily able to leave the country and acquire jobs elsewhere.&lt;/p&gt;
    &lt;p&gt;Losing the right tail to brain drain is damaging for many countries, but it’s arguably worse for Romania because its educational system is so zero-sum: the top performers do better, while the low-performers do worse. This sorting does not “lift all boats,” as it were. In Romania, the system makes for an incredibly well-trained right tail and a neglected left tail, and that left tail might hurt more than the right tail is helped, if effects on test scores are any indication. On its own, Romania’s system might be a stellar boon to the country. But with free movement of talent between countries, Romania ends up subsidizing talent discovery for other countries with less apt educational systems.&lt;/p&gt;
    &lt;p&gt;Most of the growth we see around us is due to the innovations of the right tail, and if they do better, we all do better. Though I doubt Romania’s schooling raises the intelligence of the right tail, even raising aptitude is worth something, because we must get capable people to the frontiers of their respective fields in order to innovate, and Romania has fostered a system that seems to do just that. Moreover, even if Olympiad training does not make those on the right tail more capable but instead simply prepares them better, then it can still have large, socially beneficial effects simply through providing Romania’s highly capable people with a means of having their talents recognized internationally.&lt;/p&gt;
    &lt;p&gt;But these benefits are returned only very indirectly to Romania, if at all on net. Rather than changing Romania’s educational system or closing the borders, the right solution is for more nations to choose to be like Romania, getting a lot more juice out of their smart kids by designing a system just for them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.palladiummag.com/2025/08/29/why-romania-excels-in-international-olympiads/"/></entry><entry><id>https://news.ycombinator.com/item?id=45072160</id><title>From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms</title><updated>2025-08-30T15:33:27.855754+00:00</updated><content>&lt;doc fingerprint="3518d6c6043b3b90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms&lt;/head&gt;
    &lt;head rend="h2"&gt;What is attention?&lt;/head&gt;
    &lt;p&gt;In any autoregressive model, the prediction of the future tokens is based on some preceding context. However, not all the tokens within this context equally contribute to the prediction, because some tokens might be more relevant than others. The attention mechanism addresses this by allowing the model to concentrate on the important context words selectively, while generating each output word or token. Consider the popular example that explains the attention mechanism.&lt;/p&gt;
    &lt;p&gt;“The animal didn’t cross the street because it was too tired”.&lt;/p&gt;
    &lt;p&gt;In this sentence, the pronoun “it” could refer to either “animal” or “street”. Attention helps the model to associate “it” with “animal” rather than “street” by weighing the relative importance of each word. This helps the model to understand the relationships between words and capture the contextual meaning in various NLP tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;How is attention calculated?&lt;/head&gt;
    &lt;p&gt;There are various types of attention mechanisms today, beginning with the Multi-Head Attention (MHA), which introduced the attention concept in the seminal paper. More recently, advanced variants like Multi-Latent Head Attention (MHLA) have been employed in popular models like Deepseek. This blog aims to cover the fundamentals of each attention mechanism, including the core ideas, advantages, limitations, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Concepts in Attention Mechanisms&lt;/head&gt;
    &lt;p&gt;Before diving into specific types of attention, we need to understand some fundamental concepts that underpin all the various attention mechanisms.&lt;/p&gt;
    &lt;p&gt;The main idea behind the attention mechanism is to dynamically weigh, and focus on relevant parts of inputs. Attention is required in both the encoding and decoding stages. But in this blog, we will be discussing this from a decoder's point of view.&lt;/p&gt;
    &lt;p&gt;During each generation step, we need to understand the attention weights, which help us to get a better contextual representation for the next word prediction. At its core, attention operates through three fundamental components — queries, keys, and values — that work together with attention scores to create a flexible, context-aware vector representation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Query (Q): The query is a vector that represents the current token for which the model wants to compute attention.&lt;/item&gt;
      &lt;item&gt;Key (K): Keys are vectors that represent the elements in the context against which the query is compared, to determine the relevance.&lt;/item&gt;
      &lt;item&gt;Attention Scores: These are computed using Query and Key vectors to determine the amount of attention to be paid to each context token.&lt;/item&gt;
      &lt;item&gt;Value (V): Values are the vectors that represent the actual contextual information. After calculating the attention scores using Query and Key vectors, these scores are applied against Value vectors to get the final context vector&lt;/item&gt;
      &lt;item&gt;KV Caching: Since the key and value vectors are for previous tokens, we can skip this computation for those tokens that are already calculated. KV caching stores the precomputed keys and values from the previous computations, which helps in faster decoding in autoregressive models by reusing the cached vectors. However, the Query vectors cannot be cached, since they are calculated for the current token.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To understand how each of these vectors are scores are calculated you can refer to this blog.&lt;/p&gt;
    &lt;p&gt;The high-level concepts remain consistent across all types of attention mechanisms. However, the key difference lies in how efficiently each of them executes the attention process without compromising on performance. Innovations focus on computational speed, reducing memory usage, improving scalability across longer sequences, etc.&lt;/p&gt;
    &lt;p&gt;Now, let's dive into each of these techniques&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Head Attention (MHA)&lt;/head&gt;
    &lt;p&gt;In multi-head attention, for computing the attention weights for the ith token, first, a query vector is calculated for that token. To calculate the attention weights for the token, this query vector is compared with all the preceding tokens. For that, key vectors are calculated for all the preceding tokens. These comparisons will generate an attention score, which is then used to produce a weighted score for each token using the corresponding value vectors.&lt;/p&gt;
    &lt;p&gt;In multi-head attention, this process is repeated in parallel across multiple attention “heads”. Each head has its own query, value, and key vectors, using which it calculates the relationship between the words. The final output context vector will be the concatenated output from all the attention heads.&lt;/p&gt;
    &lt;p&gt;Now, this seems straightforward. However, as the context grows, the number of Key and Value vectors will increase dramatically, because these vectors need to be calculated and stored for all the context tokens. For a sequence length of n, each query vector must be compared against all n key vectors and then perform the weighted combination using n value vectors. This results in a quadratic complexity in both computation and memory.&lt;/p&gt;
    &lt;p&gt;KV cache can help in reducing the computation and memory overhead during inference. But as the context grows, the size of the cache grows linearly with sequence length to store all the keys and values for all the preceding tokens. KV cache reduces the redundant computations, but will not reduce the fundamental cost of attending to all the previous tokens.&lt;/p&gt;
    &lt;p&gt;Models using MHA – Bert, RoBerta, T5, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Query Attention (MQA)&lt;/head&gt;
    &lt;p&gt;A significant challenge with MHA was the high computational and memory overhead associated with storing and processing separate Key and Value vectors for each attention head.&lt;/p&gt;
    &lt;p&gt;MQA addresses this problem by using multiple query heads but sharing a common set of Key and Value vectors across all the heads. In other words, there are still “h” distinct Query projections using which the model attends the current token from multiple perspectives. But the same Key and Value vectors are used for every head.&lt;/p&gt;
    &lt;p&gt;This approach will greatly reduce the memory bandwidth requirements without significantly sacrificing the model performance. By sharing the Key and Value vectors, MQA enables an efficient inference, especially for Large language models with long context lengths.&lt;/p&gt;
    &lt;p&gt;Here, the Key and Value vectors need to be calculated only once for a token instead of “h” times, which reduces the computation cost of Key/Value projection. But note that for calculating the attention score, each query head is still multiplied by the Key vectors and then weighed using the Value vectors. So this remains the same.&lt;/p&gt;
    &lt;p&gt;Also, with MQA only one set of Key-Value pairs needs to be cached, regardless of the number of Query heads. This lets the KV cache size grow gradually as the sequence length grows, leading to much lower memory requirements when compared to MHA&lt;/p&gt;
    &lt;p&gt;Models using MQA – PaLM, Falcon&lt;/p&gt;
    &lt;head rend="h2"&gt;Grouped Query Attention (GQA)&lt;/head&gt;
    &lt;p&gt;Grouped Query attention offers a balance between the MHA and MQA. As we saw earlier, traditional MHA requires significant memory and computation overhead due to separate Key-Value vectors for each Query head, and the computation overhead even increases as the number of heads increases. MQA addresses this by having a shared Key-Value, which reduces the computation cost and memory, but it may impact the model performance.&lt;/p&gt;
    &lt;p&gt;GQA offers a compromise between these two extremes. Instead of having a common Key-Value for all the heads, GQA divides the Query heads into “g” groups and lets each group share a common Key and Value head. We can say, MHA and MQA come as two extreme cases of GQA, with g=1 leading to MQA and g=h leading to MHA. This approach reduces the memory and computational requirements compared to MHA while retaining a better performance than MQA.&lt;/p&gt;
    &lt;p&gt;Models using GQA – Llama2, Llama3, Mistral&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Head Latent Attention (MHLA)&lt;/head&gt;
    &lt;p&gt;While GQA performs better than MQA, but still may not match MHA’s performance in some complex tasks.&lt;/p&gt;
    &lt;p&gt;MHLA is a recent innovation in transformer architecture introduced in models like DeepSeek. Its main goal is to dramatically reduce memory usage and accelerate inference, especially for large language models (LLMs), without loss in model performance.&lt;/p&gt;
    &lt;p&gt;The idea is to attain a performance near MHA. So we need to consider separate Key value heads for each attention head, like in MHA, but also improve the inference speed by reducing the memory overhead for storing the large amounts of Key value vectors.&lt;/p&gt;
    &lt;p&gt;MHLA addresses the challenge of high memory usage and slow inference by compressing the Key and Value representations into a much smaller latent space using low-rank projections. Specifically, instead of storing the full Key and Value vectors for every token and head, MHLA applies a linear transformation that projects these vectors into a lower-dimensional space.&lt;/p&gt;
    &lt;p&gt;So during the inference:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A down-projection weight matrix W(DKV) is introduced and is multiplied with the input sequence to obtain a compressed latent vector C(KV) for keys and Values. This latent vector is stored in cache, which is significantly smaller in size when compared to the full key and Value vectors&lt;/item&gt;
      &lt;item&gt;This is then multiplied by an up-projection matrix W(UK) and W(UV) to get the Key and Value vectors&lt;/item&gt;
      &lt;item&gt;Additionally, the matrix W(KR) is used to produce a decoupled Key that carries the Rotary Positional embedding&lt;/item&gt;
      &lt;item&gt;Additionally, the same process is done for attention Queries as well, which will reduce the activation memory during training&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MHLA supports switching between two computation paradigms for different stages. During the training stage, which is computationally intensive, it operates similarly to MHA, where the computational overhead is slightly lower than conventional MHA. During inference, it can seamlessly switch to a paradigm similar to MQA. Here, the cached KV head interacts with all query heads to produce the final output.&lt;/p&gt;
    &lt;p&gt;Models using MHLA– Deepseek- V2, Deep seek V2&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In addition to the topics discussed, there are various innovative methods that are designed to optimise the challenges of the traditional attention technique. Some of these include sparse attention, efficient attention, memory augmented attention, etc. These approaches reflect the focus on ongoing research for making the attention more scalable, faster, and adaptable across various tasks and requirements.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this post! Let me know if you liked it, have questions, or spotted an error. Please feel free to contact or follow me through LinkedIn, Twitter, or Medium.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24"/></entry><entry><id>https://news.ycombinator.com/item?id=45073791</id><title>A blog does not need “analytics”</title><updated>2025-08-30T15:33:27.369408+00:00</updated><content>&lt;doc fingerprint="ab06e3171d8543af"&gt;
  &lt;main&gt;
    &lt;p&gt;It is my increasingly firm belief that computers are tools that strip us of our humanity, converting our behaviour and thoughts into data that can be captured and commodified by other machines and their owners. They are not âjust toolsâ â how can they be when we consider who made them and for what purposes, and how they continue to be used in our social and work lives?&lt;/p&gt;
    &lt;p&gt;Many of us work in jobs that involve using computers and we are no doubt dealing with lots of data. We also have mortgages to pay and children to put through marketised education systems that have shifted the cost from the state to individuals. Living is an expensive, precarious undertaking. We have little to no freedom in forming the shape of our jobs, and the use of analytics â a term rooted in the military, surveillance and finance â in some form or other.&lt;/p&gt;
    &lt;p&gt;If you work in digital marketing, youâll deal in depersonalising, martial and downright hostile language every day. We undertake campaigns, formulate a strategy and tactics, track website visitors â often without them knowing â and convert prospects. Data is a key weapon in this war.&lt;/p&gt;
    &lt;p&gt;How useful this activity is for anyone apart from Google is the subject of a different article. But consider whether you want â let alone need â to be doing this to your readers on your personal website.&lt;/p&gt;
    &lt;p&gt;When I started this blog â 17 years ago! â I installed WordPressâs analytics plugin, which told me how many people were visiting, what they looked at and who was linking to me. I didnât ask whether I could track this information, and no-one volunteered it. Regardless of whether it was anonymised (yet another grisly marketing term), I was surveilling my readers.&lt;/p&gt;
    &lt;p&gt;I ditched analytics quite early in this blogâs history when I realised that knowing about my website visitors had zero effect on anything in the real world. For example, when Smashing Magazine linked to a WordPress theme Iâd published I got thousands of visitors over a couple of days. The only practical outcome was that a couple of people approached me to redesign their websites. This would have happened regardless of whether I knew that Iâd been linked to, and how many clicks this link had generated.&lt;/p&gt;
    &lt;p&gt;The other reason you might put analytics on your site is to know when someone links to your writing. Again, if the linker doesnât intend to tell you, then youâre surveilling. You do not need to know every time your writing is mentioned.&lt;/p&gt;
    &lt;p&gt;If you link to someone else and youâd like them to know, there are ways to automate a notification, through webmentions, for example, or you can do the radical thing and actually tell them by sending an email or message on social media.&lt;/p&gt;
    &lt;p&gt;We should resist the urge to data-fy and commodify our personal websites. Unintuitively, that means smaller, more closed communities and networks where communication is intentional rather than automated and surveilled. Youâll never know everything about your website, or its readers, and nor should you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.thisdaysportion.com/posts/contra-analytics/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074071</id><title>Nokia’s legendary font makes for a great user interface font</title><updated>2025-08-30T15:33:27.009057+00:00</updated><content>&lt;doc fingerprint="1c9023579ba34fd7"&gt;
  &lt;main&gt;
    &lt;p&gt;If you’re of a certain age (and not American), there’s a specific corporate font you’re most likely aware of. You may not know its exact name, and you may not actively remember it, but once you see it, you know exactly what you’re looking at. The font’s called Nokia Sans (and Nokia Serif), and it was used by pretty much every single Nokia device between roughly 2002 and 2013 or so, when it was replaced by a very bland font made by Bruno Maag (with help from the person who designed Comic Sans) that they used after that.&lt;/p&gt;
    &lt;p&gt;I can’t remember why, exactly, but I got majorly nostalgic for Nokia’s characteristic, recognisable font, and decided to see if it would work as a user interface font. Now, the font is still owned by Nokia and I couldn’t find a proper place to download it, but I eventually stumbled upon a site that had each individual variant listed for download. I downloaded each of them, installed them using KDE’s font installation method, and tried it out as my user interface font.&lt;/p&gt;
    &lt;p&gt;You’ll quickly discover you shouldn’t use the regular variant, but should instead opt for the Nokia Sans Wide variant. Back in 2011, when Nokia originally announced it was replacing Nokia Sans, the creator of the font, Erik Spiekermann, responded to the announcement on his blog. Apparently, one of the major reasons for Nokia to change fonts was that they claimed Nokia Sans wouldn’t work as a user interface font, but Spiekermann obviously disagrees, pointing specifically to the Wide variant. In fact, Spiekermann does not pull any punches.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;After 10 years it was high time to look at Nokia’s typefaces as the dominant visual voice of the brand but whoever decided on a completely new direction was either not aware of what was available or was persuaded by Bruno Maag to start over. Bruno may not create the most memorable typefaces, but he certainly knows how to sell them. And technically, their fonts are excellent. Too bad they didn’t have the confidence to work with me on an update. Instead they’re throwing out ten years of brand recognition in favour of blandness.&lt;/p&gt;↫ Erik Spiekermann&lt;/quote&gt;
    &lt;p&gt;I was pleasently surprised by just how nice the font looks when used as a general user interface font. It’s extremely legible at a variety of sizes, and has a ton of character without becoming gimmicky or overbearing. What originally started as mere curiosity has now become my UI font of choice on all my machines, finally displacing Inter after many years of uncontested service. Of course, all of this is deeply personal and 95% an issue of taste, but I wanted to write about it to see if I’m just entirely crazy, or if there’s some method to my madness.&lt;/p&gt;
    &lt;p&gt;Do note that I’m using high DPI displays, and KDE on Wayland, and that all of this may look different on Windows or macOS, or on displays with lower DPI. One of Inter’s strengths is that it renders great on both high and lower DPI displays, but since I don’t have any lower DPI displays anymore, I can’t test it in such an environment. I’m also not entirely sure about the legal status of downloading fonts like this, but I am fairly sure you’re at least allowed to use non-free fonts for personal, non-commercial use, but please don’t quote me on that. Since downloading each variant of these Nokia fonts is annoying, I’d love to create and upload a zip file containing all of them, but I’m sure that’s illegal.&lt;/p&gt;
    &lt;p&gt;I’m not a font connoisseur, so I may be committing a huge faux pas here? Not that I care, but reading about font nerds losing their minds over things I never even noticed is always highly entertaining.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.osnews.com/story/143222/it-turns-out-nokias-legendary-font-makes-for-a-great-general-user-interface-font/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074115</id><title>F-Stack – A network development kit with high performance based on DPDK</title><updated>2025-08-30T15:33:25.802690+00:00</updated><content>&lt;doc fingerprint="5ffa111a148a3cee"&gt;
  &lt;main&gt;
    &lt;p&gt;Physical limits of NIC, 10M CC, 5M RPS, 1M CPS&lt;/p&gt;
    &lt;p&gt;Kernel bypass, lockless, no scheduling, no interruption&lt;/p&gt;
    &lt;p&gt;Easy adaption for any application&lt;/p&gt;
    &lt;p&gt;With the rapid development of NIC, the poor performance of data packets processing with Linux kernel has become the bottleneck. However, the rapid development of the Internet needs high performance of network processing, kernel bypass has caught more and more attention. There are various similar technologies appear, such as DPDK, NETMAP and PF_RING. The main idea of kernel bypass is that Linux is only used to deal with control flow, all data streams are processed in user space. Therefore, kernel bypass can avoid performance bottlenecks caused by kernel packet copy, thread scheduling, system calls and interrupt. Furthermore, kernel bypass can achieve higher performance with multi optimizing methods. Within various techniques, DPDK has been widely used because of its more thorough isolation from kernel scheduling and active community support.&lt;/p&gt;
    &lt;p&gt;F-Stack is an open source network framework with high performance based on DPDKï¼ include an user space TCP/IP stack(port FreeBSD 11.0 stable), Posix API(Socket, Epoll, Kqueue), Progamming SDK(Coroutine) and some apps(Nginx, Redis) interface.&lt;/p&gt;
    &lt;p&gt;Currently, there are various products in Tencent Cloud has used the F-Stack, such as DKDNS(DNSPod's authorization DNS server), HttpDNS (D+), COS access module, CDN access module, etc..&lt;/p&gt;
    &lt;p&gt;Now let's start using F-Stack to launch a HTTP server on EC2 with Nginx.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.f-stack.org/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074147</id><title>Agent Client Protocol</title><updated>2025-08-30T15:33:25.515258+00:00</updated><content>&lt;doc fingerprint="7d89da1d4bcc10c1"&gt;
  &lt;main&gt;
    &lt;p&gt;The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).The protocol is still under development, but it should be complete enough to build interesting user experiences using it.&lt;/p&gt;
    &lt;p&gt;AI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users. This creates several problems:&lt;/p&gt;
    &lt;p&gt;Integration overhead: Every new agent-editor combination requires custom work&lt;/p&gt;
    &lt;p&gt;Limited compatibility: Agents work with only a subset of available editors&lt;/p&gt;
    &lt;p&gt;Developer lock-in: Choosing an agent often means accepting their available interfaces&lt;/p&gt;
    &lt;p&gt;ACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents. This decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.&lt;/p&gt;
    &lt;p&gt;ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://agentclientprotocol.com/overview/introduction"/></entry><entry><id>https://news.ycombinator.com/item?id=45074157</id><title>FBI cyber cop: Salt Typhoon pwned 'nearly every American'</title><updated>2025-08-30T15:33:25.263820+00:00</updated><content>&lt;doc fingerprint="f3b8b8159e0ca529"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FBI cyber cop: Salt Typhoon pwned 'nearly every American'&lt;/head&gt;
    &lt;head rend="h2"&gt;Plus millions of other people across 80+ countries&lt;/head&gt;
    &lt;p&gt;China's Salt Typhoon cyberspies hoovered up information belonging to millions of people in the United States over the course of the years-long intrusion into telecommunications networks, according to a top FBI cyber official.&lt;/p&gt;
    &lt;p&gt;"There's a good chance this espionage campaign has stolen information from nearly every American," Michael Machtinger, deputy assistant director for the FBI's cyber division, told The Register.&lt;/p&gt;
    &lt;p&gt;"There's a thought among the public that if you don't work in a sensitive area that the PRC might be interested in for its traditional espionage activities, then you are safe, they will not target you," he said, during a Thursday interview with The Register. "As we have seen from Salt Typhoon, this is no longer an assumption that anyone can afford to make."&lt;/p&gt;
    &lt;p&gt;The Beijing-backed spying campaign began at least in 2019 but wasn't uncovered by US authorities until last fall. On Wednesday, US law enforcement and intelligence agencies along with those from 12 other countries warned the ongoing espionage activity expanded far beyond nine American telcos and government networks. According to Machtinger, at least 80 countries were hit by the digital intrusions.&lt;/p&gt;
    &lt;p&gt;Around 200 American organizations were compromised by the espionage activity, Machtinger said, including the previously disclosed telecommunications firms such as Verizon and AT&amp;amp;T.&lt;/p&gt;
    &lt;p&gt;Yesterday's joint security alert also pointed the allies' collective finger at three China-based entities affiliated with Salt Typhoon: Sichuan Juxinhe Network Technology, Beijing Huanyu Tianqiong Information Technology, and Sichuan Zhixin Ruijie Network Technology. These companies, and likely others, provide cyber products and services to China's Ministry of State Security and People's Liberation Army, the governments said.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;What the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;"This is one of the most consequential cyber espionage breaches that we've ever seen in the United States," Machtinger said.&lt;/p&gt;
    &lt;p&gt;"What this really underscores is that what the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space," he added. "And that should really set off alarm bells for us — not only in the United States. The scale of indiscriminate targeting is unlike what we've seen in the past."&lt;/p&gt;
    &lt;p&gt;This indiscriminate targeting, as the FBI and White House security officials have previously noted, allowed Beijing’s snoops to geo-locate millions of mobile phone users, monitor their internet traffic, and, in some cases, record their phone calls. Victims reportedly included President Donald Trump and Vice President JD Vance.&lt;/p&gt;
    &lt;p&gt;Machtinger declined to confirm whether Trump and Vance were among those surveilled, but did say that victims included more than 100 current and former presidential administration officials.&lt;/p&gt;
    &lt;p&gt;"As we look at the impact on the different sets of victims," he said, Salt Typhoon collected "bulk information from millions of Americans."&lt;/p&gt;
    &lt;p&gt;For the more targeted group of individuals, "most of whom are very high-profile, current and former presidential administration officials, and campaign appointees from both major political parties," the data collection went much deeper, Machtinger added. "Down to intercepting actual content."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you thought China's Salt Typhoon was booted off critical networks, think again&lt;/item&gt;
      &lt;item&gt;China's Salt Typhoon spies spotted on US govt networks before telcos, CISA boss says&lt;/item&gt;
      &lt;item&gt;This is the FBI, open up. China's Volt Typhoon is on your network&lt;/item&gt;
      &lt;item&gt;How does China keep stealing our stuff, wonders DoD group responsible for keeping foreign agents out&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition to Salt Typhoon, the feds over the past year have issued warnings about other Chinese cyber operations. These include Volt Typhoon intruders, who infected hundreds of outdated routers to build a botnet and break into US critical infrastructure facilities. The Beijing-backed crew, we would later learn, was prepositioning itself and readying destructive cyberattacks.&lt;/p&gt;
    &lt;p&gt;Another China-linked crew, Silk Typhoon has spent more than a decade compromising IT and cloud providers to steal sensitive data from their government, technology, education, and legal and professional services customers.&lt;/p&gt;
    &lt;p&gt;China is not the only source of threats, Machtinger noted. Russia, Iran, North Korea, plus along with home-grown and international cybercriminals and ransomware crooks, assault computers and networks of both individuals and organizations, every day.&lt;/p&gt;
    &lt;p&gt;"These actors are going to continue their efforts, and they're going to get more sophisticated," Machtinger said. "We need to make sure that we, a nation, are taking cybersecurity seriously, updating systems, removing end-of-life devices, and making it as hard and costly as possible for the myriad of actors that are out there to successfully compromise." ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/08/28/fbi_cyber_cop_salt_typhoon/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074248</id><title>Cognitive Load is what matters</title><updated>2025-08-30T15:33:24.786007+00:00</updated><content>&lt;doc fingerprint="373978120d250fed"&gt;
  &lt;main&gt;&lt;p&gt;Readable version | Chinese translation | Korean translation | Turkish translation&lt;/p&gt;&lt;p&gt;It is a living document, last update: August 2025. Your contributions are welcome!&lt;/p&gt;&lt;p&gt;There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.&lt;/p&gt;&lt;p&gt;Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high cognitive load. It's not some fancy abstract concept, but rather a fundamental human constraint. It's not imagined, it's there and we can feel it.&lt;/p&gt;&lt;p&gt;Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Cognitive load is how much a developer needs to think in order to complete a task.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly four such chunks in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.&lt;/p&gt;&lt;p&gt;Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, the author had created a high cognitive load for us.&lt;/p&gt;&lt;p&gt;We should reduce the cognitive load in our projects as much as possible.&lt;/p&gt;&lt;p&gt;Intrinsic - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.&lt;/p&gt;&lt;p&gt;Extraneous - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.&lt;/p&gt;&lt;p&gt;Let's jump straight to the concrete practical examples of extraneous cognitive load.&lt;/p&gt;&lt;p&gt;We will refer to the level cognitive load as follows:&lt;code&gt;🧠&lt;/code&gt;: fresh working memory, zero cognitive load&lt;code&gt;🧠++&lt;/code&gt;: two facts in our working memory, cognitive load increased&lt;code&gt;🤯&lt;/code&gt;: cognitive overload, more than 4 facts&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Our brain is much more complex and unexplored, but we can go with this simplistic model.&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;if val &amp;gt; someConstant // 🧠+
    &amp;amp;&amp;amp; (condition2 || condition3) // 🧠+++, prev cond should be true, one of c2 or c3 has be true
    &amp;amp;&amp;amp; (condition4 &amp;amp;&amp;amp; !condition5) { // 🤯, we are messed up by this point
    ...
}&lt;/code&gt;&lt;p&gt;Introduce intermediate variables with meaningful names:&lt;/p&gt;&lt;code&gt;isValid = val &amp;gt; someConstant
isAllowed = condition2 || condition3
isSecure = condition4 &amp;amp;&amp;amp; !condition5 
// 🧠, we don't need to remember the conditions, there are descriptive variables
if isValid &amp;amp;&amp;amp; isAllowed &amp;amp;&amp;amp; isSecure {
    ...
}&lt;/code&gt;&lt;code&gt;if isValid { // 🧠+, okay nested code applies to valid input only
    if isSecure { // 🧠++, we do stuff for valid and secure input only
        stuff // 🧠+++
    }
} &lt;/code&gt;&lt;p&gt;Compare it with the early returns:&lt;/p&gt;&lt;code&gt;if !isValid
    return
 
if !isSecure
    return

// 🧠, we don't really care about earlier returns, if we are here then all good

stuff // 🧠+&lt;/code&gt;&lt;p&gt;We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.&lt;/p&gt;&lt;p&gt;We are asked to change a few things for our admin users: &lt;code&gt;🧠&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;AdminController extends UserController extends GuestController extends BaseController&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Ohh, part of the functionality is in &lt;code&gt;BaseController&lt;/code&gt;, let's have a look: &lt;code&gt;🧠+&lt;/code&gt;&lt;lb/&gt; Basic role mechanics got introduced in &lt;code&gt;GuestController&lt;/code&gt;: &lt;code&gt;🧠++&lt;/code&gt;&lt;lb/&gt; Things got partially altered in &lt;code&gt;UserController&lt;/code&gt;: &lt;code&gt;🧠+++&lt;/code&gt;&lt;lb/&gt; Finally we are here, &lt;code&gt;AdminController&lt;/code&gt;, let's code stuff! &lt;code&gt;🧠++++&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Oh, wait, there's &lt;code&gt;SuperuserController&lt;/code&gt; which extends &lt;code&gt;AdminController&lt;/code&gt;. By modifying &lt;code&gt;AdminController&lt;/code&gt; we can break things in the inherited class, so let's dive in &lt;code&gt;SuperuserController&lt;/code&gt; first: &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Prefer composition over inheritance. We won't go into detail - there's plenty of material out there.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Method, class and module are interchangeable in this context&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.&lt;/p&gt;&lt;p&gt;Deep module - simple interface, complex functionality&lt;lb/&gt; Shallow module - interface is relatively complex to the small functionality it provides&lt;/p&gt;&lt;p&gt;Having too many shallow modules can make it difficult to understand the project. Not only do we have to keep in mind each module responsibilities, but also all their interactions. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, linear thinking is more natural to us humans.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Information hiding is paramount, and we don't hide as much complexity in shallow modules.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.&lt;/p&gt;&lt;p&gt;Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The best components are those that provide powerful functionality yet have a simple interface.&lt;/p&gt;&lt;lb/&gt;John K. Ousterhout&lt;/quote&gt;&lt;p&gt;The interface of the UNIX I/O is very simple. It has only five basic calls:&lt;/p&gt;&lt;code&gt;open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)&lt;/code&gt;&lt;p&gt;A modern implementation of this interface has hundreds of thousands of lines of code. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;This deep module example is taken from the book A Philosophy of Software Design by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper On the Criteria To Be Used in Decomposing Systems into Modules. Both are essential reads. Other related readings: A Philosophy of Software Design vs Clean Code, It's probably time to stop recommending Clean Code, Small Functions considered Harmful.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.&lt;/p&gt;&lt;p&gt;All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So MetricsProviderFactoryFactory seems to be just fine. The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that? Something went wrong.&lt;/p&gt;&lt;p&gt;We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A module should be responsible to one, and only one, user or stakeholder.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.&lt;/p&gt;&lt;p&gt;But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.&lt;/p&gt;&lt;p&gt;This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.&lt;/p&gt;&lt;p&gt;I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. Diagnostic difficulty in integration space skyrocketed. Both time to market and cognitive load were unacceptably high. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". Hello, you got to stop dreaming big.&lt;/p&gt;&lt;p&gt;The Tanenbaum-Torvalds debate argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.&lt;/p&gt;&lt;p&gt;A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.&lt;/p&gt;&lt;p&gt;We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.&lt;/p&gt;&lt;p&gt;If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, when you come back later, you would have to recreate that thought process!&lt;/p&gt;&lt;p&gt;You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;These statements are made by none other than Rob Pike.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Reduce cognitive load by limiting the number of choices.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Language features are OK, as long as they are orthogonal to each other.&lt;/p&gt;&lt;head&gt;Thoughts from an engineer with 20 years of C++ experience ⭐️&lt;/head&gt;&lt;p&gt;I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!&lt;/p&gt;&lt;p&gt;I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.&lt;/p&gt;&lt;p&gt;Like, can you imagine, the token&lt;/p&gt;&lt;code&gt;||&lt;/code&gt; has a different meaning in &lt;code&gt;requires ((!P&amp;lt;T&amp;gt; || !Q&amp;lt;T&amp;gt;))&lt;/code&gt; and in &lt;code&gt;requires (!(P&amp;lt;T&amp;gt; || Q&amp;lt;T&amp;gt;))&lt;/code&gt;. The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.&lt;p&gt;You can't allocate space for a trivial type and just&lt;/p&gt;&lt;code&gt;memcpy&lt;/code&gt; a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.&lt;p&gt;Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you will face that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03.&lt;/p&gt;&lt;code&gt;🤯&lt;/code&gt;&lt;p&gt;There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, but if the value is known statically, then...&lt;/p&gt;&lt;code&gt;🤯&lt;/code&gt;&lt;p&gt;This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons (extraneous cognitive load).&lt;/p&gt;&lt;p&gt;I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.&lt;/p&gt;&lt;p&gt;By no means I am trying to blame C++. I love the language. It's just that I am tired now.&lt;/p&gt;&lt;p&gt;Thanks to 0xd34df00d for writing.&lt;/p&gt;&lt;p&gt;On the backend we return:&lt;code&gt;401&lt;/code&gt; for expired jwt token&lt;code&gt;403&lt;/code&gt; for not enough access&lt;code&gt;418&lt;/code&gt; for banned users&lt;/p&gt;&lt;p&gt;The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:&lt;code&gt;401&lt;/code&gt; is for expired jwt token // &lt;code&gt;🧠+&lt;/code&gt;, ok just temporary remember it&lt;code&gt;403&lt;/code&gt; is for not enough access // &lt;code&gt;🧠++&lt;/code&gt;&lt;code&gt;418&lt;/code&gt; is for banned users // &lt;code&gt;🧠+++&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Frontend developers would (hopefully) introduce some kind &lt;code&gt;numeric status -&amp;gt; meaning&lt;/code&gt; dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.&lt;/p&gt;&lt;p&gt;Then QA engineers come into play: "Hey, I got &lt;code&gt;403&lt;/code&gt; status, is that expired token or not enough access?"
QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.&lt;/p&gt;&lt;p&gt;Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:&lt;/p&gt;&lt;code&gt;{
    "code": "jwt_has_expired"
}&lt;/code&gt;&lt;p&gt;Cognitive load on the frontend side: &lt;code&gt;🧠&lt;/code&gt; (fresh, no facts are held in mind)&lt;lb/&gt; Cognitive load on the QA side: &lt;code&gt;🧠&lt;/code&gt;&lt;/p&gt;&lt;p&gt;The same rule applies to all sorts of numeric statuses (in the database or wherever) - prefer self-describing strings. We are not in the era of 640K computers to optimise for memory.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;People spend time arguing between&lt;/p&gt;&lt;code&gt;401&lt;/code&gt;and&lt;code&gt;403&lt;/code&gt;, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.&lt;/quote&gt;&lt;p&gt;P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like "login" and "permissions" to reduce the cognitive load.&lt;/p&gt;&lt;p&gt;Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.&lt;/p&gt;&lt;p&gt;Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.&lt;/p&gt;&lt;p&gt;Rob Pike once said:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A little copying is better than a little dependency.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.&lt;/p&gt;&lt;p&gt;All your dependencies are your code. Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (because things go wrong) is painful.&lt;/p&gt;&lt;p&gt;There's a lot of "magic" in frameworks. By relying too heavily on a framework, we force all upcoming developers to learn that "magic" first. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.&lt;/p&gt;&lt;p&gt;Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;By no means do we advocate to invent everything from scratch!&lt;/p&gt;&lt;p&gt;We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.&lt;/p&gt;&lt;p&gt;There is a certain engineering excitement about all this stuff.&lt;/p&gt;&lt;p&gt;I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Abstraction is supposed to hide complexity, here it just adds indirection. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture’s layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.&lt;/p&gt;&lt;p&gt;If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and implicit interfaces.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;With a sufficient number of users of an API,&lt;/p&gt;&lt;lb/&gt;it does not matter what you promise in the contract:&lt;lb/&gt;all observable behaviors of your system&lt;lb/&gt;will be depended on by somebody.&lt;/quote&gt;&lt;p&gt;We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. We spent the next 10 months on dealing with out-of-order events and other challenges. It's now funny to say that abstractions helps us replace components quickly.&lt;/p&gt;&lt;p&gt;So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future? Plus, in most cases, that future of replacing some core component never happens.&lt;/p&gt;&lt;p&gt;These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. Discuss.&lt;/p&gt;&lt;p&gt;Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.&lt;/p&gt;&lt;p&gt;Layers of abstraction aren't free of charge, they are to be held in our limited working memory.&lt;/p&gt;&lt;p&gt;Domain-driven design has some great points, although it is often misinterpreted. People say "We write code in DDD", which is a bit strange, because DDD is about problem space, not about solution space.&lt;/p&gt;&lt;p&gt;Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.&lt;/p&gt;&lt;p&gt;Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The problem is that familiarity is not the same as simplicity. They feel the same — that same ease of moving through a space without much mental effort — but for very different reasons. Every “clever” (read: “self-indulgent”) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get “the new kid” to critique the code before they get too institutionalised!&lt;/p&gt;&lt;p&gt;It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.&lt;/p&gt;&lt;p&gt;In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: “When there are only 2 or 3 conditionals, adding another one doesn’t make any difference. By the time there are 20 or 30 conditionals, adding another one doesn’t make any difference!”&lt;/p&gt;&lt;p&gt;There is no “simplifying force” acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.&lt;/p&gt;&lt;p&gt;Thanks to Dan North for his comment.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.&lt;/p&gt;&lt;p&gt;The more mental models there are to learn, the longer it takes for a new developer to deliver value.&lt;/p&gt;&lt;p&gt;Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.&lt;/p&gt;&lt;p&gt;If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Our architecture is a standard CRUD app architecture, a Python monolith on top of Postgres&lt;/item&gt;&lt;item&gt;How Instagram scaled to 14 million users with only 3 engineers&lt;/item&gt;&lt;item&gt;The companies where we were like ”woah, these folks are smart as hell” for the most part failed&lt;/item&gt;&lt;item&gt;One function that wires up the entire system. If you want to know how the system works - go read it&lt;/item&gt;&lt;/list&gt;&lt;p&gt;These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.&lt;/p&gt;&lt;p&gt;Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.&lt;/p&gt;&lt;p&gt;Imagine for a moment that what we inferred in the second chapter isn’t actually true. If that’s the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. Do not do this to your colleagues.&lt;/p&gt;&lt;p&gt;We should reduce any cognitive load above and beyond what is intrinsic to the work we do.&lt;/p&gt;&lt;head&gt;Comments&lt;/head&gt;&lt;p&gt;Rob Pike&lt;lb/&gt;Nice article.&lt;/p&gt;&lt;p&gt;Andrej Karpathy (ChatGPT, Tesla)&lt;lb/&gt;Nice post on software engineering. Probably the most true, least practiced viewpoint.&lt;/p&gt;&lt;p&gt;Elon Musk&lt;lb/&gt;True.&lt;/p&gt;&lt;p&gt;Addy Osmani (Chrome, the most complex software system in the world)&lt;lb/&gt;I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.&lt;/p&gt;&lt;p&gt;The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."&lt;/p&gt;&lt;p&gt;What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.&lt;/p&gt;&lt;p&gt;And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.&lt;/p&gt;&lt;p&gt;Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.&lt;/p&gt;&lt;p&gt;One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.&lt;/p&gt;&lt;p&gt;Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility.&lt;/p&gt;&lt;p&gt;Sometimes the simplest solution is the best one, even in a complex system.&lt;/p&gt;&lt;p&gt;antirez (Redis)&lt;lb/&gt;Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.&lt;/p&gt;&lt;p&gt;A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.&lt;/p&gt;&lt;p&gt;Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).&lt;/p&gt;&lt;p&gt;These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D&lt;/p&gt;&lt;p&gt;A developer from the internet&lt;lb/&gt;You would not hire me... I sell myself on my track record of released enterprise projects.&lt;/p&gt;&lt;p&gt;I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.&lt;/p&gt;&lt;p&gt;I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.&lt;/p&gt;&lt;p&gt;I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.&lt;/p&gt;&lt;p&gt;Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/zakirullin/cognitive-load"/></entry><entry><id>https://news.ycombinator.com/item?id=45074399</id><title>Show HN: OpenAnimation – KMP app for exploring and editing Lottie animations</title><updated>2025-08-30T15:33:24.412791+00:00</updated><content>&lt;doc fingerprint="7f5af4ecaf93d78a"&gt;
  &lt;main&gt;
    &lt;p&gt;✨ Check out the live web version: openanimation.web.app ✨&lt;/p&gt;
    &lt;p&gt;Discover and draw inspiration from a curated collection of beautiful Lottie animations, all powered by Kotlin Multiplatform.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🎨 Animation Color Scheme Picker - Add ability for users to customize animation color palettes&lt;/item&gt;
      &lt;item&gt;👥 Community Animations - Implement support for user-submitted animations with voting system&lt;/item&gt;
      &lt;item&gt;⚡ Rendering Optimization - Improve animation playback performance across platforms&lt;/item&gt;
      &lt;item&gt;📊 Performance Metrics - Add analytics for rendering times and resource usage&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/orispok/OpenAnimationApp"/></entry><entry><id>https://news.ycombinator.com/item?id=45074467</id><title>AI models need a virtual machine</title><updated>2025-08-30T15:33:23.867086+00:00</updated><content>&lt;doc fingerprint="bd5fd5d284b383c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Applications using AI embed the AI model in a framework that interfaces between the model and the rest of the system, providing needed services such as tool calling, context retrieval, etc. Software for early chatbots took user input, called the LLM, and returned the result to the user; essentially just a read-eval-print loop. But, as the capabilities of LLMs have evolved and extension mechanisms, such as MCP were defined, the complexities of the control software that calls the LLM have increased. AI software systems require the same qualities that an operating system provides, including security, isolation, extensibility, and portability. For example, when an AI model needs to be given a file as part of its context, access control must be established that determines if the model should be allowed to view that file. We believe it is time to consider standardizing the ways in which the AI models are embedded into software and think of that control software layer as a virtual machine, where one of the machine instructions, albeit a super-powerful one, is to call the LLM.&lt;/p&gt;
    &lt;p&gt;Our approach decouples model development from integration logic, allowing any model to “plug in” to a rich software ecosystem that includes tools, security controls, memory abstractions, etc. Similar to the impact that the Java Virtual Machine had, creating a specification of a VM for the AI orchestrator could enable a “write once, run anywhere” execution environment for AI models while at the same time providing familiar constraints and governance to maintain security and privacy in existing software systems. Below we outline related work in this direction, the motivation behind it, and the key benefits of an AI Model VM.&lt;/p&gt;
    &lt;p&gt;Introduction&lt;/p&gt;
    &lt;p&gt;AI models are being leveraged in existing software as application copilots, embedded in IDEs, and with the rise of the MCP protocol, are increasingly able to use tools, implement agents, etc. This rapid evolution of valuable use cases brings with it a greater need to ensure that the AI-powered applications maintain privacy, are secure, and operate correctly. Guarantees of security and privacy are best provided if the underlying system is secure by design and not added on to systems as an afterthought. We take the Java Virtual Machine (JVM) as our inspiration in making the case for the importance of a standard AI Virtual Machine. The Java Virtual Machine guarantees memory safety by design, defines access control policies, and prevents code injection with bytecode verification. These properties allow Java programs running on the JVM to be executed with trust despite being shipped remotely, enabling “write once, run anywhere” software distribution.&lt;/p&gt;
    &lt;p&gt;How does the JVM relate to applications that use AI models? We used the following example to explain:&lt;/p&gt;
    &lt;p&gt;The diagram illustrates the role of the software layer that interacts with an AI model, which we call the Model Virtual Machine (MVM). That layer intermediates between the model and the rest of the world. For example, a chatbot user might type a prompt (1) that the MVM then sends unmodified to the AI model (2). In practice, the MVM will add additional context, including the system prompt, chat history, to the AI model input as well. The AI model generates a response, which in the example requires a specific tool to be called (3). This response has a specific format that is mutually agreed upon between the model and the MVM, such as MCP. In our example, because it is important to restrict the model from making undesired tool calls, the MVM first consults the list of allowed tools (4) before deciding to call the tool the model requested (5). This check (4) guarantees that the model doesn’t make unauthorized tool calls. Every commercial system using AI models requires some version of this control software.&lt;/p&gt;
    &lt;p&gt;We make the analogy that the interface with the LLM should be a virtual machine. If that is the case, what are the instructions that the machine can execute? Here are examples of operations that existing AI model interfaces have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Certifying, loading, initializing, and unloading a given AI model&lt;/item&gt;
      &lt;item&gt;Calling a model with context&lt;/item&gt;
      &lt;item&gt;Parsing the output from the model&lt;/item&gt;
      &lt;item&gt;Certifying, loading, initializing, and unloading tools&lt;/item&gt;
      &lt;item&gt;Calling a tool&lt;/item&gt;
      &lt;item&gt;Parsing the results from a tool call&lt;/item&gt;
      &lt;item&gt;Storing the results from a tool call into memory&lt;/item&gt;
      &lt;item&gt;Asking the user for input&lt;/item&gt;
      &lt;item&gt;Adding content to a history memory&lt;/item&gt;
      &lt;item&gt;Standard control constructs such as conditionals, sequencing, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A VM would support all of these operations in a well-typed context where constraints are placed on the calls made, the arguments passed, etc.&lt;/p&gt;
    &lt;p&gt;Existing Work Informs What is Needed&lt;/p&gt;
    &lt;p&gt;Some of the required elements of a well-specified interface are emerging in AI systems explored in academic work and in applications that are widely deployed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI’s Structured Tool Calling Protocols: OpenAI introduced a JSON-based function calling API that lets models invoke code-defined functions in a structured way. This approach, along with OpenAI’s plugin system (which uses OpenAPI specifications for tools), showed how structured tool-calling protocols can reduce ambiguity and simplify integration.&lt;/item&gt;
      &lt;item&gt;Anthropic’s Model Context Protocol (MCP, 2024): MCP is an open protocol for connecting AI assistants to external data and tools, explicitly aiming to be a universal interface. “Think of MCP like a USB-C port for AI applications,” Anthropic explains. Instead of every service having a custom AI integration, MCP provides a common schema and client-server approach. Despite being relatively new, MCP adoption, including in large companies, has been rapid.&lt;/item&gt;
      &lt;item&gt;Secure Orchestrators – FIDES &amp;amp; AC4A (2025): Security remains a weak point in current AI systems. Two recent projects propose runtime-level controls. FIDES (by Microsoft Research) enforces information-flow policies on agents by tracking data confidentiality labels and adding new agent actions like “inspect” to limit what agents can access (where a quarantined LLM can safely summarize restricted data) (paper). AC4A (Access Control for Agents) (manuscript in preparation) takes an OS-style approach: All tools and data are organized into hierarchies (like files and folders), and the agent must request read/write access for each resource. AC4A’s runtime intercepts every agent action and blocks anything not permitted, forcing a least-privilege operation mode. These projects show how a standard AI VM could include built-in security and access control, just as modern operating systems do. Even with strong access controls built into a VM specification, AI models present new security challenges that need to be considered in the design. For example, an AI model, when prevented from accessing a particular item of data, might use its chain-of-thought reasoning to devise ways to gather accessible data that allows it to infer the inaccessible item. As such, security researchers have to devise new mitigations to prevent AI models taking adversarial actions even with the virtual machine constraints.&lt;/item&gt;
      &lt;item&gt;Open-Source Agent Runtimes: Several projects are actively building general-purpose runtimes for AI. For example, langchain and Semantic Kernel provide numerous common runtime services that make writing reliable AI-enabled applications easier. The AI Controller Interface (AICI) (later renamed llguidance), integrates a lightweight VM into the model-serving pipeline, allowing developers to script and constrain model behavior at a low level (e.g., control of generations token-by-token).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Defining a specification for a VM interface for AI systems from these emerging approaches will require more than an agreement on protocols and APIs. Because AI systems derive their behavior from training data, model training data must reflect the specification of the VM interface so that the models and the VM model interface can co-evolve. This will enable otherwise diverse models to exhibit broadly compatible behavior with respect to the VM interface specification.&lt;/p&gt;
    &lt;p&gt;Benefits of a Well-Specified AI Model VM&lt;/p&gt;
    &lt;p&gt;As mentioned, many applications that leverage AI models require reliability, privacy, and security. In addition, new models are developed almost daily and updating the model being used by an application is often necessary. Given this confluence of factors, creating robust AI software presents significant engineering challenges. We believe that a specification of the interface between the AI model and the surrounding software that interfaces to it will address some of these challenges.&lt;/p&gt;
    &lt;p&gt;The need for an AI Model VM specification is driven by several clear motivations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Separation of Concerns: An interface specification enforces a clean separation between model logic and integration logic. This means models become interchangeable components. You could swap in a new model (or move an agent to a different platform) and, as long as both adhere to the standard, everything still works. Likewise, virtual machine implementors can increase the performance, security, and tooling of the virtual machine while maintaining compatibility with the AI model interfaces.&lt;/item&gt;
      &lt;item&gt;Built-in Safety and Governance: A VM specification can enforce safety by design. By routing all tool usage and external access through a well-defined interface, it becomes easier to apply permission checks, audit logs, and fail-safes. As shown by projects like AC4A, the VM can act as a gatekeeper, restricting what models can do unless explicitly authorized. This creates a safer deployment solution for powerful AI systems: even if the model behaves unpredictably, the VM layer can contain its effects. Standards bodies could even define security requirements (e.g., certain calls must always require user confirmation), creating a shared foundation of trust. Similar to the benefits of signed assemblies in the Common Language Runtime, have a certification process around loading and unloading models and tools ensures the end-to-end security of the supply chain.&lt;/item&gt;
      &lt;item&gt;Transparent Performance &amp;amp; Resource Tracking: A VM specification could also give developers visibility to runtime diagnostics. Post-execution manifests could report model performance, resource consumption, and data access level which helps developers evaluate overall efficiency and performance. Benchmarks for accuracy, utility, and responsiveness can be supported directly in the VM interface across models and platforms.&lt;/item&gt;
      &lt;item&gt;Verifiability of Model Output: Leveraging a VM specification, experts can explore integrating formal methods to verify their model behavior. Techniques such as zero-knowledge proofs could confirm the integrity of model outputs without sensitive internal logic. While still emerging, this possibility hints at new levels of trust and accountability in AI systems and should be carefully considered during development.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;We argue that a well-specified AI Model Virtual Machine is needed. Developments occurring in multiple directions, including work from tech companies, startups, and academia, all motivate the need for a VM specification that lets AI models safely and seamlessly interact with the world around them. The motivation is clear – reducing complexity and unlocking interoperability – and the potential benefits range from technical (faster development, modular upgrades) to strategic (cross-platform AI ecosystems, improved safety). From enforcing controls for security and privacy, to potentially formal proof capabilities for trust, the opportunities are wide-ranging. Learning a lesson from older generations of software virtualization, a VM specification can increase AI systems portability, interoperability, security, and reliability. The purpose of this document is to highlight these issues and start engaging with the community on building a consensus that such a specification is needed and what it should include.&lt;/p&gt;
    &lt;p&gt;Biographies:&lt;/p&gt;
    &lt;p&gt;Shraddha Barke is a Senior Researcher at Microsoft Research in Redmond, Washington in the Research in Software Engineering (RiSE) group. Her research interests include AI for proof generation, training AI models for program-reasoning tasks using RL and improving the reliability of AI agents.&lt;/p&gt;
    &lt;p&gt;Betül Durak is a Principal Researcher at Microsoft Research in Redmond, Washington in Security, Privacy, and Cryptography group. Her research interests broadly include security analysis as well as secure and private protocol designs motivated from real world problems.&lt;/p&gt;
    &lt;p&gt;Dan Grossman is a Professor at the University of Washington and the Vice Director of the Paul G. Allen School of Computer Science &amp;amp; Engineering. His research interests are in programming languages, particularly in applying programming languages concepts and analyses to emerging domains.&lt;/p&gt;
    &lt;p&gt;Peli de Halleux is a Principal Research Software Developer Engineer in Redmond, Washington working in the Research in Software Engineering (RiSE) group. His research interests include empowering individuals to build LLM-powered applications more efficiently.&lt;/p&gt;
    &lt;p&gt;Emre Kıcıman is a Senior Principal Research Manager and Head of Research for Copilot Tuning at Microsoft. His research interests include causal methods, the security of AI, and applications of LLM and AI-based systems, together with their implications for people and society.&lt;/p&gt;
    &lt;p&gt;Reshabh K Sharma is a PhD student at the University of Washington. His research lies at the intersection of PL/SE and LLMs, focusing on developing infrastructure and tools to create better LLM-based system that are easier to develop reliably and correctly.&lt;/p&gt;
    &lt;p&gt;Ben Zorn is a Partner Researcher at Microsoft Research in Redmond, Washington working in (and previously having co-managed) the Research in Software Engineering (RiSE) group. His research interests include programming language design and implementation, end-user programing, and AI software including technology for ensuring responsible AI.&lt;/p&gt;
    &lt;p&gt;Disclaimer: These posts are written by individual contributors to share their thoughts on the SIGPLAN blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGPLAN or its parent organization, ACM.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/"/></entry><entry><id>https://news.ycombinator.com/item?id=45074895</id><title>Condor's Cuzco RISC-V Core at Hot Chips 2025</title><updated>2025-08-30T15:33:23.670706+00:00</updated><content>&lt;doc fingerprint="c2da901fa4165780"&gt;
  &lt;main&gt;
    &lt;p&gt;Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive. Andes formed Condor in 2023, so Condor is a relatively young player on the RISC-V scene. However, Andes does have RISC-V design experience prior to Condor’s formation with a few RISC-V cores under their belt from years past.&lt;/p&gt;
    &lt;p&gt;Condor is presenting their Cuzco core at Hot Chips 2025. This core is a heavyweight within the RISC-V scene, with wide out-of-order execution and a modern branch predictor and some new time based tricks. It’s in the same segment as high performance RISC-V designs like SiFive’s P870 and Veyron’s V1. Like those cores, Cuzco should stand head and shoulders above currently in-silicon RISC-V cores like Alibaba T-HEAD’s C910 and SiFive’s P550.&lt;/p&gt;
    &lt;p&gt;Besides being a wide out-of-order design, Cuzco uses mostly static scheduling in the backend to save power and reduce complexity. Condor calls this a “time-based” scheduling scheme. I’ll cover more on this later, but it’s important to note that this is purely an implementation detail. It doesn’t require ISA modifications or special treatment from the compiler for optimal performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Core Overview&lt;/head&gt;
    &lt;p&gt;Cuzco is a 8-wide out-of-order core with a 256 entry ROB and clock speed targets around 2 GHz SS (Slow-Slow) to 2.5 GHz (Typical-Typical) on TSMC’s 5nm process. The pipeline has 12 stages counting from instruction fetch to data cache access completion. However, a 10 cycle mispredict penalty probably more accurately describes the core’s pipeline length relative to its competitors.&lt;/p&gt;
    &lt;p&gt;As a licensed core, Cuzco is meant to be highly configurable to widen its target market. The core is built from a variable number of execution slices. Customization options also include L2 TLB size, off-cluster bus widths, and L2/L3 capacity. Condor can also adjust the size of various internal core structures to meet customer performance requirements. Cuzco cores are arranged into clusters with up to eight cores. Clusters interface with the system via a CHI bus, so customers can bring their own network-on-chip (NoC) to hit higher core counts via multi-cluster setups.&lt;/p&gt;
    &lt;head rend="h1"&gt;Frontend&lt;/head&gt;
    &lt;p&gt;Cuzco’s frontend starts with a sophisticated branch predictor, as is typical for modern cores targeting any reasonable performance level. Conditional branches are handled via a TAGE-SC-L predictor. TAGE stands for Tagged Geometric, a technique that uses multiple tables each handling a different history length. It seeks to efficiently use branch predictor storage by selecting the most appropriate history length for each branch, as opposed to older techniques that use a fixed history length. The SC (Statistical Corrector) part handles the small subset of branches where TAGE doesn’t work well, and can invert the prediction if it sees TAGE often getting things wrong under certain circumstances. Finally, L indicates a loop predictor. A loop predictor is simply a set of counters that come into play for branches that are taken a certain number of times, then not taken once. If the branch predictor detects such loop behavior, the loop predictor can let it avoid mispredicting on the last iteration of the loop. Basically, TAGE-SC-L is an augmented version of the basic TAGE predictor.&lt;/p&gt;
    &lt;p&gt;AMD’s Zen 2, Ampere’s AmpereOne, and Qualcomm’s Oryon also use TAGE predictors of some sort, and achieve excellent branch prediction accuracy. AMD, Ampere, and Qualcomm also likely augment the basic TAGE prediction strategy in some way. How Cuzco’s TAGE predictor performs will depend on how large its history tables are, as well as how well the predictor is tuned (selection of index vs tag bits, history lengths, distribution of storage budget across TAGE tables, etc). For Cuzco’s part, they’ve disclosed that the TAGE predictor’s base component uses a 16K entry table of bimodal counters.&lt;/p&gt;
    &lt;p&gt;Branch target caching on Cuzco is provided by a 8K entry branch target buffer (BTB) split into two levels. Condor’s slides show the BTB hit/miss occurring on the cycle after instruction cache access starts, so a taken branch likely creates a single pipeline bubble. Returns are predicted using a 32 entry return stack. Cuzco also has an indirect branch predictor, which is typical on modern CPUs.&lt;/p&gt;
    &lt;p&gt;Cuzco’s instruction fetch logic feeds from a 64 KB 8-way set associative instruction cache, and speeds up address translations with a 64 entry fully associative TLB. The instruction fetch stages pull an entire 64B cacheline into the ICQ (instruction cache queue), and then pull instructions from that into an instruction queue (XIQ). The decoders feed from the XIQ, and can handle up to eight instructions per cycle.&lt;/p&gt;
    &lt;head rend="h1"&gt;Rename and Allocate&lt;/head&gt;
    &lt;p&gt;Much of the action in Condor’s presentation relates to the rename and allocate stage, which acts as a bridge between the frontend and out-of-order backend. In most out-of-order cores, the renamer carries out register renaming and allocates resources in the backend. Then, the backend dynamically schedules instructions as their dependencies become available. Cuzco’s renamer goes a step further and predicts instruction schedules as well.&lt;/p&gt;
    &lt;p&gt;One parallel to this is Nvidia’s static scheduling in Kepler and subsequent GPU architectures. Both simplify scheduling by telling an instruction to execute a certain number of cycles in the future, rather than having hardware dynamically check for dependencies. But Nvidia does this in their compiler because GPU ISAs aren’t standardized. Cuzco still uses hardware to create dynamic schedules, but moves that job into the rename/allocate stage rather than the schedulers in the backend. Schedulers can be expensive structures in conventional out-of-order CPUs, because they have to check whether instructions are ready to execute every cycle. On Cuzco, the backend schedulers can simply wait a specified number of cycles, and then issue an instruction knowing the dependencies will be ready by then.&lt;/p&gt;
    &lt;p&gt;To carry out time-based scheduling, Cuzco maintains a Time Resource Matrix (TRM), which tracks utilization of various resources like execution ports, functional units, and data buses for a certain number of cycles in the future. The TRM can look 256 cycles into the future, which keeps storage requirements under control. Because searching a 256 row matrix in hardware would be extremely expensive, Cuzco only looks for available resources in a small window after an instruction’s dependencies are predicted to be ready. Condor found searching a window of eight cycles provided a good tradeoff. Because the renamer can handle up to eight instructions, it at most has to access 64 rows in the TRM per cycle. If the renamer can’t find free resources in the search window, the instruction will be stalled at the ID2 stage.&lt;/p&gt;
    &lt;p&gt;Another potential limitation is the TRM size, which could be a limitation for long latency instructions. However, the longest latency instructions tend to be loads that miss cache. Cuzco always assumes a L1D hit for TRM scheduling, and uses replay to handle L1D misses. That means stalls at ID2 from TRM size limitations should also be rare.&lt;/p&gt;
    &lt;p&gt;Compared to a hypothetical “greedy” setup, where the core is able to create a perfect schedule with execution resource limitations in mind, limiting the TRM search window decreases performance by a few percent. Condor notes that creating a core to match the “greedy” figure may not even be possible. A conventional out-of-order core wouldn’t have TRM-related restrictions, but may face difficulties creating an optimal schedule for other reasons. For example, a distributed scheduler may have several micro-ops become ready in one scheduling queue, and face “false” delays even though free execution units may be available on other scheduling queues.&lt;/p&gt;
    &lt;p&gt;Static scheduling only works when instruction latencies are known ahead of time. Some instructions have variable latency, like loads that can miss caches or TLBs, encounter bank conflicts, or require store forwarding. As mentioned before, Cuzco uses instruction replay to handle variable latency instructions and the associated dynamic behavior. The renamer does take some measures to reduce replays, like checking to see if a load gets its address from the same register as a prior store. However, it doesn’t attempt to predict memory dependencies like Intel’s Core 2, and also doesn’t try to predict whether a load will miss cache.&lt;/p&gt;
    &lt;head rend="h1"&gt;Out-of-Order Backend&lt;/head&gt;
    &lt;p&gt;Out of order execution in Cuzco is relatively simple, because the rename/allocate stage takes care of figuring out when instructions will execute. Each instruction is simply held within the schedulers until a specified number of cycles pass, after which it’s sent for execution. If the rename/allocate stage guesses wrong, replay gets handled via “poison” bits. The erroneously executed instruction’s result data is effectively marked as poisoned, and any instructions consuming that data will get re-executed. Replaying instructions costs power and wastes execution throughput, so replays should ideally be a rare event. 70.07 replays per 1000 instructions feels like a bit of a high figure, but likely isn’t a major problem because execution resources are rarely a limitation in an out-of-order core. Taking about 7% more execution resources may be an acceptable tradeoff, considering most modern chips rarely use their core width in a sustained fashion.&lt;/p&gt;
    &lt;p&gt;Execution resources are grouped into slices, each of which have a pair of pipelines. A slice can execute all of the core’s supported RISC-V instructions, making it easy to scale execution resources by changing slice count. Each slice consists of a set of execution queues (XEQs), which hold micro-ops waiting for a functional unit. Cuzco has XEQs per functional unit, unlike conventional designs that tend to have a scheduling queue that feeds all functional units attached to an execution port. Four register read ports supply operands to the slice, and two write ports handle result writeback. Bus conflicts are handled by the TRM as well. A slice cannot execute more than two micro-ops per cycle, even doing so would not oversubscribe the register read ports. For example, a slice can’t issue an integer add, a branch, and a load in the same cycle even though that would only require four register inputs.&lt;/p&gt;
    &lt;p&gt;XEQs are sized to match workload characteristics, much like tuning a distributed scheduler. While XEQ sizes can be set to match customer requirements, Condor was able to give some figures for a baseline configuration. ALUs get 16 entry queues, while branches and address generation units (LS) get 8 entry queues. XEQ sizes are adjustable in powers of two, from 2 to 32 entries. There’s generally a single cycle of latency for forwarding between slices. The core can be configured to do zero cycle cross-slice forwarding, but that would be quite difficult to pull off.&lt;/p&gt;
    &lt;p&gt;On the vector side, Cuzco supports 256/512-bit VLENs via multiple micro-ops, which are distributed across the execution slices. Execution units are natively 64 bits wide. There’s one FMA unit per slice, so peak FP32 throughput is eight FMA operations per cycle, or 16 FLOPS when counting the add and multiply as separate operations. FP adds execute with 2 cycle latency, while FP multiplies and multiply-adds have four cycle latency. The two cycle FP add latency is nice to see, and matches recent cores like Neoverse N1 and Intel’s Golden Cove, albeit at much lower clocks.&lt;/p&gt;
    &lt;head rend="h1"&gt;Load/Store&lt;/head&gt;
    &lt;p&gt;Cuzco’s load/store unit has a 64 entry load queue, a 64 entry store queue, and a 64 entry queue for data cache misses. Loads can leave the load queue after accessing the data cache, likely creating behavior similar to AMD’s Zen series where the out-of-order backend can have far more loads pending retirement than the documented load queue capacity would suggest. The core has four load/store pipelines in a four slice configuration, or one pipeline per slice. Maximum load bandwidth is 64B/cycle, achievable with vector loads.&lt;/p&gt;
    &lt;p&gt;The L1D is physically indexed and physically addressed (PIPT), so address translation has to complete before L1D access.To speed up address translation, Cuzco has a 64 entry fully associative data TLB. The L2 TLB is 4-way set associative, and can have 1K, 2K, or 4K entries. Cuzco’s core private, unified L2 cache has configurable capacity as well. An example 2 MB L2 occupies 1.04 mm2 on TSMC 5nm.&lt;/p&gt;
    &lt;p&gt;Eight cores per cluster share a L3 cache, which is split into slices to handle bandwidth demands from multiple cores. Each slice can deliver 64B/cycle, and slice count matches core count. Thus Cuzco enjoys 64B/cycle of load bandwidth throughout the cache hierarchy, of course with the caveat that L3 bandwidth may be lower if accesses from different cores clash into the same slice. Cores and L3 slices within a cluster are linked by a crossbar. The L3 cache can run at up to core clock. Requests to the system head out through a 64B/cycle CHI interface. System topology beyond the cluster is up to the implementer.&lt;/p&gt;
    &lt;p&gt;Replays for cache misses are carried out by rescheduling the data consumer to a later time when data is predicted to be ready. Thus a L3 hit would cause a consuming instruction to be executed three times - once for the predicted L1D hit, once for the predicted L2 hit, and a final time for the L3 hit with the correct data.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;High performance CPU design has settled down over the past couple decades, and converged on an out-of-order execution model. There’s no denying that out-of-order execution is difficult. Numerous alternatives have been tried through the years but didn’t have staying power. Intel’s Itanium sought to use an ISA-based approach, but failed to unseat the company’s own x86 cores that used out-of-order execution. Nvidia’s Denver tried to dynamically compile ARM instructions into microcode bundles, but that approach was not carried forward. All successful high performance designs today generally use the same out-of-order execution strategy, albeit with plenty of variation. That’s driven by the requirements of ISA compatibility, and the need to deliver high single threaded performance across a broad range of applications. Breaking from the mould is obviously fraught with peril.&lt;/p&gt;
    &lt;p&gt;Condor seeks to break from the mould, but does so deep in the core in a way that should be invisible to software a functional perspective, and mostly invisible from a performance perspective. The core runs RISC-V instructions and thus benefits from that software ecosystem, unlike Itanium. It doesn’t rely on a compiled microcode cache like Denver, so it doesn’t end up running in a degraded performance beyond what a typical OoO core would see when dealing with poor code locality. Finally, instruction replay effectively creates dynamic schedules and handles cache misses&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot"/></entry><entry><id>https://news.ycombinator.com/item?id=45074967</id><title>Adafruit Fruit Jam – An RP2350 mini computer running classic Macintosh</title><updated>2025-08-30T15:33:23.154196+00:00</updated><content>&lt;doc fingerprint="8d06c53510b2409a"&gt;
  &lt;main&gt;
    &lt;p&gt;The Adafruit Fruit Jam is a credit card-sized RP2350-powered mini computer that is designed to run classic Macintosh through the uMac emulator. It supports System 2.0 up to System 7.5.5, 720p video via DVI, audio, and USB keyboard and mouse.&lt;/p&gt;
    &lt;p&gt;Built around the Raspberry Pi RP2350 MCU, the Fruit Jam board also features an ESP32-C6 wireless module and offers DVI output via the RP2350’s HSTX interface, USB-C for bootloading, a microSD card slot for storage, and an onboard TLV320DAC3100 I2S audio DAC for stereo headphones and a mono speaker. There’s also a 16-pin GPIO header, NeoPixels LEDs, tactile switches, and STEMMA QT and JST connectors for expansion. These features make this board suitable for retro emulation, educational projects, and lightweight standalone computing.&lt;/p&gt;
    &lt;p&gt;Adafruit Fruit Jam specifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microcontroller – Raspberry Pi RP2350B &lt;list rend="ul"&gt;&lt;item&gt;CPU &lt;list rend="ul"&gt;&lt;item&gt;Dual-core Arm Cortex-M33 @ 150 MHz with Arm Trustzone, Secure boot OR&lt;/item&gt;&lt;item&gt;Dual-core RISC-V Hazard3 @ 150 MHz&lt;/item&gt;&lt;item&gt;Either two cores can be used.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Memory – 520 KB on-chip SRAM&lt;/item&gt;&lt;item&gt;Package – QFN-80&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;CPU &lt;/item&gt;
      &lt;item&gt;Memory – 8MB PSRAM&lt;/item&gt;
      &lt;item&gt;Storage &lt;list rend="ul"&gt;&lt;item&gt;16MB SPI Flash&lt;/item&gt;&lt;item&gt;MicroSD card slot&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Video Output – DVI via HSTX port&lt;/item&gt;
      &lt;item&gt;Audio &lt;list rend="ul"&gt;&lt;item&gt;On-board TLV320DAC3100 I2S Audio DAC&lt;/item&gt;&lt;item&gt;Stereo headphone output&lt;/item&gt;&lt;item&gt;Mono speaker output (mini speaker included)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;USB &lt;list rend="ul"&gt;&lt;item&gt;1x USB Type-C port for power, bootloading, USB client&lt;/item&gt;&lt;item&gt;Dual USB Type-A host ports (keyboard, mouse, game controllers)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Expansion &lt;list rend="ul"&gt;&lt;item&gt;STEMMA QT I²C connector&lt;/item&gt;&lt;item&gt;3-pin JST STEMMA connector&lt;/item&gt;&lt;item&gt;16-pin GPIO header (10x A/D GPIOs, 5V, 3.3V, GND)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Debug – PicoProbe debug port&lt;/item&gt;
      &lt;item&gt;Misc &lt;list rend="ul"&gt;&lt;item&gt;5x NeoPixel RGB LEDs&lt;/item&gt;&lt;item&gt;3x tactile switches&lt;/item&gt;&lt;item&gt;ON/OFF switch&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Dimensions – 85.725mm x 53.975mm (credit-card size meets ISO/IEC 7810 ID-1)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In terms of software support, this development board supports CircuitPython, Arduino IDE, and the Pico SDK (C/C++). The board can also run emulators such as uMac, CP/M (RunCPM), and MCUME for retro platforms. It also supports games, multimedia projects, and custom applications. More information can be found on Fruit Jam’s primary guide page. You can also check out the Fruit Jam Mac Emulator guide for detailed instructions on setting up and running classic Macintosh systems on the board.&lt;/p&gt;
    &lt;p&gt;Previously, we have seen Olimex RP2350pc, a similar Raspberry Pi RP2350-based development board design to run and emulate Apple OS, and run Reload emulator to emulate Apple ][, Apple ][e, Oric-Atmos, Pravetz, and Puldin 8-bit computers. It can also run Paul Robson’s RP2350pc user library to allow compilers and OS to be created with a unified, BIOS-like API. The Adafruit Fruit Jam should be able to support similar retro-computing emulators and libraries, but with more features.&lt;/p&gt;
    &lt;p&gt;The Adafruit Fruit Jam board is sold for $39.95 on the company’s online store. In the package, you will get a protective top plate with nylon screws, a mini speaker, and a bumper kit for stability. At the time of writing, only 23 units are available, and the stock is expected to sell out soon.&lt;/p&gt;
    &lt;p&gt;Debashis Das is a technical content writer and embedded engineer with over five years of experience in the industry. With expertise in Embedded C, PCB Design, and SEO optimization, he effectively blends difficult technical topics with clear communication&lt;/p&gt;
    &lt;p&gt;Support CNX Software! Donate via cryptocurrencies, become a Patron on Patreon, or purchase goods on Amazon or Aliexpress. We also use affiliate links in articles to earn commissions if you make a purchase after clicking on those links.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnx-software.com/2025/08/27/adafruit-fruit-jam-a-rp2350-mini-computer-running-classic-macintosh/"/></entry></feed>