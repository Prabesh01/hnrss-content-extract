<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-24T20:35:57.460621+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45359378</id><title>US airlines are pushing to remove protections for passengers and add more fees</title><updated>2025-09-24T20:36:06.858485+00:00</updated><content>&lt;doc fingerprint="567c9a5a01baa55b"&gt;
  &lt;main&gt;
    &lt;p&gt;Home¬ªAIRLINE NEWS¬ª American Joins Delta, Southwest, United and Other US Airlines Push to Strip Away Travelers‚Äô Rights and Add More Fees by Rolling Back Key Protections in New Deregulation Move&lt;/p&gt;
    &lt;p&gt;American Joins Delta, Southwest, United and Other US Airlines Push to Strip Away Travelers‚Äô Rights and Add More Fees by Rolling Back Key Protections in New Deregulation Move&lt;/p&gt;
    &lt;p&gt;American Airlines joins with Delta, Southwest, United, and other US airlines are pushing to remove key protections for passengers and add more fees by rolling back rules, claiming it will lower costs and boost competition, but it may leave travelers with fewer rights and more hidden charges. Under the guise of lowering costs and boosting competition, these changes are likely to result in the erosion of consumer rights ‚Äì the right to cancel a ticket with an automatic refund, transparency of pricing, and the right to sit with your family on the same reservation. Airlines claim that removing these ‚Äòprotections‚Äô will decrease airfare and increase competition on the routes. The prospects for travelers, on the other hand, will likely be more fees, less certainty of receiving the service paid for, and diminished responsibility from the airlines for service failures. If these projections become reality, deregulation will aggravate the air travel experience for the consumer making it more expensive and more opaque.&lt;/p&gt;
    &lt;p&gt;The Airline Industry‚Äôs Deregulatory Push&lt;/p&gt;
    &lt;p&gt;The U.S. airline industry is pushing for a significant rollback of consumer protections, which many see as a major step backward for air travel. Airline lobbyists, representing carriers like American, Delta, Southwest, United, and the Airlines for America (A4A) association, have laid out a detailed agenda that would fundamentally alter the landscape of air travel, making it more difficult for passengers to know what they‚Äôre actually paying for and less likely to receive compensation when things go wrong.&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;This agenda centers on weakening or eliminating four major consumer protections:&lt;/p&gt;
    &lt;p&gt;Automatic Refunds for Cancellations: Airlines want to remove the requirement to provide automatic refunds when flights are cancelled or significantly altered. Passengers may instead receive only vouchers or no compensation at all, leaving them without recourse in the event of a major flight disruption.&lt;/p&gt;
    &lt;p&gt;Transparency of Fees: The airlines also aim to strip away rules that require them to disclose all fees (like baggage, seat assignments, and service charges) upfront. Instead of the clear, itemized pricing system that passengers currently rely on, airlines could hide fees until later in the booking process, making the true cost of a ticket much higher than expected.&lt;/p&gt;
    &lt;p&gt;Family Seating Guarantees: Under current regulations, airlines must ensure that families with young children are seated together without additional charges. This would no longer be guaranteed under the new proposal, meaning families could face extra costs just to sit next to one another.&lt;/p&gt;
    &lt;p&gt;Accessibility Protections for Disabled Passengers: The deregulation proposal also targets protections for disabled passengers, weakening their access to support and assistance during air travel.&lt;/p&gt;
    &lt;p&gt;The Airline Industry‚Äôs Argument: Deregulation as a Path to Lower Prices&lt;/p&gt;
    &lt;p&gt;The airline industry‚Äôs argument for deregulation is grounded in a belief that removing these protections will lead to lower prices, more competition, and better services for consumers. Airline lobbyists argue that deregulation, which began with the Airline Deregulation Act of 1978, has led to increased competition, lower airfares, and more choices for passengers.&lt;/p&gt;
    &lt;p&gt;However, while some might agree that competition can drive prices down, there‚Äôs a serious concern that deregulation could lead to more surprise charges and less accountability for airlines. Instead of benefiting consumers, deregulation may open the door for airlines to charge excessive fees for basic services, which are often hidden until later in the booking process. This could leave passengers paying far more than they anticipated and receiving less value for their money.&lt;/p&gt;
    &lt;p&gt;The Airlines‚Äô Detailed Deregulatory Agenda&lt;/p&gt;
    &lt;p&gt;The Airlines for America (A4A), the industry group representing major U.S. airlines, has strongly supported deregulation, arguing that it has benefited both airlines and passengers since the 1970s. In a recent document, A4A outlined their full deregulatory wish list, which includes the following key points:&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;Support for Deregulation: A4A strongly advocates for the continuation of deregulation, claiming that it has led to lower prices, increased competition, and better services for consumers. The group argues that removing regulations would allow airlines to better compete in the market and reinvest in improving services for passengers.&lt;/p&gt;
    &lt;p&gt;Criticism of the Biden Administration‚Äôs Regulatory Actions:&lt;/p&gt;
    &lt;p&gt;Ancillary Fee Transparency: A4A opposes the U.S. Department of Transportation‚Äôs (DOT) rules requiring airlines to disclose ancillary fees upfront, arguing that these rules exceed the DOT‚Äôs authority and don‚Äôt provide any clear benefits to consumers.&lt;/p&gt;
    &lt;p&gt;Refund Rules: A4A calls for the repeal or revision of refund rules that, according to them, go beyond what‚Äôs required by law, imposing unnecessary costs on airlines without providing any clear benefit to the public.&lt;/p&gt;
    &lt;p&gt;Flight Delay and Cancellations: The group also criticizes DOT‚Äôs policies on flight delays and cancellations, claiming the rules unfairly penalize airlines, particularly when disruptions are caused by factors beyond their control.&lt;/p&gt;
    &lt;p&gt;Deregulatory Priorities: A4A outlines several changes they would like to see the DOT pursue:&lt;/p&gt;
    &lt;p&gt;Rescinding Unlawful Regulations: A4A seeks the repeal of certain regulations, such as family seating and mobility aid assistance rules, which they argue exceed DOT‚Äôs authority.&lt;/p&gt;
    &lt;p&gt;Limiting Refund Rules: The group wants to limit DOT‚Äôs authority on flight refund rules, particularly for minor operational changes, such as changes to flight numbers or itineraries that don‚Äôt cause harm to passengers.&lt;/p&gt;
    &lt;p&gt;Economic Impact of Deregulation: A4A points to the success of deregulation, citing a rise in low-cost carriers, which have made air travel more affordable. They also highlight the significant decrease in airfare prices, which has directly benefited consumers.&lt;/p&gt;
    &lt;p&gt;Investment in Airline Operations: A4A argues that deregulation has allowed airlines to reinvest in their services, improving customer satisfaction and innovation.&lt;/p&gt;
    &lt;p&gt;Support for Technological Innovation: The airline industry is also backing the use of technology, including artificial intelligence (AI) and biometrics, to improve operational efficiency and the customer experience.&lt;/p&gt;
    &lt;p&gt;Why Deregulation is a Concern for Passengers&lt;/p&gt;
    &lt;p&gt;While the airline industry argues that deregulation will lead to lower prices and more competition, critics are skeptical. Here are the key reasons why deregulation could harm consumers:&lt;/p&gt;
    &lt;p&gt;More Hidden Fees: If airlines are no longer required to disclose fees upfront, passengers may face a barrage of surprise charges, from baggage fees to seat selection costs. The cost of air travel could increase significantly, even if base fares appear lower.&lt;/p&gt;
    &lt;p&gt;No Guarantees for Families: The proposal to eliminate the guarantee that families will be seated together without extra charges could lead to more stress for families travelling with young children. Parents may find themselves paying additional fees just to sit next to their kids.&lt;/p&gt;
    &lt;p&gt;Less Accountability for Cancellations: Airlines would have more power to decide whether or not to refund passengers for flight cancellations. This could lead to more vouchers, rather than cash refunds, leaving passengers at the mercy of airlines‚Äô own policies.&lt;/p&gt;
    &lt;p&gt;Weaker Protections for Disabled Passengers: The weakening of accessibility regulations could make it more difficult for passengers with disabilities to access the services and assistance they need during their travel.&lt;/p&gt;
    &lt;p&gt;Less Competition, Not More: While deregulation advocates claim it will lead to more competition, the reality is that fewer protections for consumers could allow major airlines to exploit passengers without fear of consequences. Smaller carriers may also struggle to compete on an uneven playing field.&lt;/p&gt;
    &lt;p&gt;The Risk of Over-Regulation vs. Consumer Protection&lt;/p&gt;
    &lt;p&gt;The battle between over-regulation and consumer protection is a complex issue. While it‚Äôs true that some regulations may stifle innovation, the protections that are in place help ensure fair treatment and transparency for consumers. The question is not whether airlines should be regulated, but how much regulation is necessary to strike a balance between profitability and protecting passengers.&lt;/p&gt;
    &lt;p&gt;In Europe, stricter regulations have led to fewer delays and cancellations, and the market remains competitive with budget airlines thriving under the current system. The fear is that deregulation in the U.S. could result in a situation where airlines dominate the market, and passengers are left with fewer rights and more fees.&lt;/p&gt;
    &lt;p&gt;What Passengers Can Do&lt;/p&gt;
    &lt;p&gt;As a passenger, it‚Äôs important to stay informed about these changes and advocate for your rights. Here are some steps you can take:&lt;/p&gt;
    &lt;p&gt;Stay Informed: Keep up with the latest news and updates on airline regulations.&lt;/p&gt;
    &lt;p&gt;Contact Your Representatives: Let your senators and congress members know how you feel about the deregulation of the airline industry.&lt;/p&gt;
    &lt;p&gt;Know Your Rights: Understand the protections you currently have and how they might change.&lt;/p&gt;
    &lt;p&gt;American Airlines, Delta, Southwest, United, and other U.S. airlines are pushing to remove key protections for passengers and add more fees by rolling back regulations, arguing that it will lower costs and increase competition, but it could also lead to fewer rights and more hidden charges for travelers.&lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;The deregulation push by U.S. airlines is a major threat to passenger rights. While airlines argue that deregulation will lead to cheaper fares and more competition, the reality is likely to be more fees, less transparency, and fewer protections for passengers. If successful, this move could turn back the clock to a time when flying was riddled with hidden charges and unfair treatment.&lt;/p&gt;
    &lt;p&gt;The future of air travel depends on consumers, advocacy groups, and lawmakers standing up for passenger rights. The airline industry may be pushing for deregulation, but it‚Äôs up to the public to ensure that the changes made are in the best interest of all passengers, not just the airlines. The battle is not just about cheaper tickets, but about ensuring that air travel remains fair, transparent, and accountable for everyone.&lt;/p&gt;
    &lt;p&gt;We use cookies on our website to give you the most relevant experience by remembering your preferences and repeat visits. By clicking ‚ÄúAccept‚Äù, you consent to the use of ALL the cookies.&lt;/p&gt;
    &lt;p&gt;This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.&lt;/p&gt;
    &lt;p&gt;Necessary cookies are absolutely essential for the website to function properly. These cookies ensure basic functionalities and security features of the website, anonymously.&lt;/p&gt;
    &lt;p&gt;Cookie&lt;/p&gt;
    &lt;p&gt;Duration&lt;/p&gt;
    &lt;p&gt;Description&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-analytics&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Analytics".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-functional&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;The cookie is set by GDPR cookie consent to record the user consent for the cookies in the category "Functional".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-necessary&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookies is used to store the user consent for the cookies in the category "Necessary".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-others&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Other.&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-performance&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Performance".&lt;/p&gt;
    &lt;p&gt;viewed_cookie_policy&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;The cookie is set by the GDPR Cookie Consent plugin and is used to store whether or not user has consented to the use of cookies. It does not store any personal data.&lt;/p&gt;
    &lt;p&gt;Functional cookies help to perform certain functionalities like sharing the content of the website on social media platforms, collect feedbacks, and other third-party features.&lt;/p&gt;
    &lt;p&gt;Performance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors.&lt;/p&gt;
    &lt;p&gt;Analytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics the number of visitors, bounce rate, traffic source, etc.&lt;/p&gt;
    &lt;p&gt;Advertisement cookies are used to provide visitors with relevant ads and marketing campaigns. These cookies track visitors across websites and collect information to provide customized ads.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.travelandtourworld.com/news/article/american-joins-delta-southwest-united-and-other-us-airlines-push-to-strip-away-travelers-rights-and-add-more-fees-by-rolling-back-key-protections-in-new-deregulation-move/"/><published>2025-09-24T12:30:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359524</id><title>Learning Persian with Anki, ChatGPT and YouTube</title><updated>2025-09-24T20:36:06.800758+00:00</updated><content>&lt;doc fingerprint="9290b055cdb636d2"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôve been learning Persian (Farsi) for a while now, and I‚Äôm using a bunch of tools for it. The central one is certainly Anki, a spaced repetition app to train memory. I‚Äôm creating my own never-ending deck of cards, with different types of content, for different purposes. The most frequent type of cards is grammar focused phrases (very rarely single words) coming sometimes from my own daily life, but also very often directly from videos of the Persian Learning YouTube channel, created by Majid, a very talented and nice Persian teacher, in my opinion.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take an example, suppose there is this slide in one of Majid‚Äôs videos:&lt;/p&gt;
    &lt;p&gt;From this, I will extract three screenshots (with the MacOS screenshot tool). First, to create a card of type ‚Äúbasic‚Äù (one side). I use this type of card to exercise my reading, which is very difficult and remains stubbornly slow, even though I know the 32 letters of the Persian alphabet quite well by now. But the different ways of writing them (which varies by their position in the word) and the fact that the vowels are not present makes it an enduringly challenging task.&lt;/p&gt;
    &lt;p&gt;The next type of card I create with the two remaining screenshots is ‚Äúbasic and reversed‚Äù, which actually creates two cards (one for each direction), one with some romanized phrase, and the other with the English or French translation:&lt;/p&gt;
    &lt;p&gt;When I review these cards in my daily Anki routine, this is where ChatGPT enters into play. First I have set a ‚ÄúPersian‚Äù project with these instructions:&lt;/p&gt;
    &lt;p&gt;With this project, every time I have a doubt or don‚Äôt remember something in Anki, I just take a screenshot and paste it in the project:&lt;/p&gt;
    &lt;p&gt;With this, I have an instant refresher on any notion, in any context. Sometimes I need to do this over and over, before it gels into a deeper, more instant and visceral ‚Äúknowledge‚Äù.&lt;/p&gt;
    &lt;p&gt;The next set of techniques is also based on YouTube. I use a Chrome extension called Dual Subtitles (which only works of course with videos having actual dual sources of subtitles):&lt;/p&gt;
    &lt;p&gt;The dual subtitles serve a couple of purposes: first as a source of new Anki cards (I create the cards directly, again with screenshots in the clipboard).&lt;/p&gt;
    &lt;p&gt;I also use the Tweaks for YouTube extension, which allows me to get extra keyboard shortcuts, to go back and forward only 1 second, instead of the built-in 5 seconds.&lt;/p&gt;
    &lt;p&gt;With these YouTube extensions, I have developed this particular ‚Äútechnique‚Äù to improve my vocal understanding:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I listen at 75% speed&lt;/item&gt;
      &lt;item&gt;I use the ‚Äúdual subtitles‚Äù browser extension to have both the Farsi and English subtitles at the same time (I set the Farsi one slightly bigger)&lt;/item&gt;
      &lt;item&gt;Every time a new sentence appears, I read it very quickly first in English (I pause if I need to), and then I listen carefully to the voice, to let the meaning and sound of Farsi infuse my mind (this part is very subtle but the most important: you must ‚Äúfeel‚Äù that you understand, and this feeling must cover even the words that you don‚Äôt know; because the meaning of the sentence is currently present and active in your mind, because you just read the English part, I believe that its mapping with the Farsi words that you then hear is particularly efficient, at least that‚Äôs my theory)&lt;/item&gt;
      &lt;item&gt;I also read the Farsi script, to improve my understanding, and disambiguate certain words for which it‚Äôs hard for me to hear what is exactly said&lt;/item&gt;
      &lt;item&gt;I repeat out loud what has been said also, which is quite important&lt;/item&gt;
      &lt;item&gt;Most importantly: I repeat this process (for a single video) over and over, in order to reach a stage where I genuinely understand what is said, in real-time, which is a very powerful and exhilarating feeling.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cjauvin.github.io/posts/learning-persian/"/><published>2025-09-24T12:45:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359604</id><title>How to Lead in a Room Full of Experts</title><updated>2025-09-24T20:36:06.321364+00:00</updated><content>&lt;doc fingerprint="f3751d93156404b7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Here is a realization I made recently. I'm sitting in a room full of smart people. On one side are developers who understand the ins and outs of our microservice architecture. On the other are the front-end developers who can debug React in their sleep. In front of me is the product team that has memorized every possible user path that exists on our website. And then, there is me. The lead developer. I don't have the deepest expertise on any single technology.&lt;/p&gt;
      &lt;p&gt;So what exactly is my role when I'm surrounded by experts? Well, that's easy. I have all the answers.&lt;/p&gt;
      &lt;head rend="h2"&gt;Technical Leadership&lt;/head&gt;
      &lt;p&gt;OK. Technically, I don't have all the answers. But I know exactly where to find them and connect the pieces together. &lt;/p&gt;
      &lt;p&gt;When the backend team explains why a new authentication service would take three weeks to build, I'm not thinking about the OAuth flows or JWT token validation. Instead, I think about how I can communicate it to the product team who expects it done "sometime this week." When the product team requests a "simple" feature, I'm thinking about the 3 teams that need to be involved to update the necessary microservices.&lt;/p&gt;
      &lt;p&gt;Leadership in technical environments isn't about being the smartest person in the room. It's about being the most effective translator.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is a Social Skill&lt;/head&gt;
      &lt;p&gt;I often get "eye rolls" when I say this to developers: You are not going to convince anyone with facts. In a room full of experts, your technical credibility gets you a seat at the table, but your social skills determine whether anything productive happens once you're there.&lt;/p&gt;
      &lt;p&gt;Where ideally you will provide documentation that everyone can read and understand, in reality, you need to talk to get people to understand. People can get animated when it comes to the tools they use. When the database team and the API team are talking past each other about response times, your role isn't to lay down the facts. Instead it's to read the room and find a way to address technical constraints and unclear requirements. It means knowing when to let a heated technical debate continue because it's productive, and when to intervene because it's become personal.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is Remembering the Goal&lt;/head&gt;
      &lt;p&gt;When you are an expert in your field, you love to dive deep. It's what makes you experts. But someone needs to keep one eye on the forest while everyone else is examining the trees.&lt;/p&gt;
      &lt;p&gt;I've sat through countless meetings where engineers debated the merits of different caching strategies while the real issue was that we hadn't clearly defined what "fast enough" meant for the user experience. The technical discussion was fascinating, but it wasn't moving us toward shipping.&lt;/p&gt;
      &lt;p&gt;As a leader, your job isn't to have sophisticated technical opinions. It's to ask how this "discussion" can move us closer to solving our actual problem.&lt;/p&gt;
      &lt;p&gt;When you understand a problem, and you have a room full of experts, the solution often emerges from the discussion. But someone needs to clearly articulate what problem we're actually trying to solve.&lt;/p&gt;
      &lt;p&gt;When a product team says customers are reporting the app is too slow, that's not a clear problem. It's a symptom. It might be that users are not noticing when the shopping cart is loaded, or that maybe we have an event that is not being triggered at the right time. Or maybe the app feels sluggish during peak hours. Each of those problems has different solutions, different priorities, and different trade-offs. Each expert might be looking at the problem with their own lense, and may miss the real underlying problem.&lt;/p&gt;
      &lt;p&gt;Your role as a leader is to make sure the problem is translated in a way the team can clearly understand the problem.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is Saying "I Don't Know"&lt;/head&gt;
      &lt;p&gt;By definition, leading is knowing the way forward. But in reality, in a room full of experts, pretending to know everything makes you look like an idiot.&lt;/p&gt;
      &lt;p&gt;Instead, "I don't know, but let's figure it out" becomes a superpower. It gives your experts permission to share uncertainty. It models intellectual humility. And it keeps the focus on moving forward rather than defending ego. It's also an opportunity to let your experts shine.&lt;/p&gt;
      &lt;p&gt;Nothing is more annoying than a lead who needs to be the smartest person in every conversation. Your database expert spent years learning how to optimize queries - let them be the hero when performance issues arise. Your security specialist knows threat models better than you, give them the floor when discussing architecture decisions.&lt;/p&gt;
      &lt;p&gt;Make room for some productive discussion. When two experts disagree about implementation approaches, your job isn't to pick the "right" answer. It's to help frame the decision in terms of trade-offs, timeline, and user impact.&lt;/p&gt;
      &lt;p&gt;Your value isn't in having all the expertise. It's in recognizing which expertise is needed when, and creating space for the right people to contribute their best work. &lt;/p&gt;
      &lt;head rend="h3"&gt;The Translation Challenge&lt;/head&gt;
      &lt;p&gt;There was this fun blog post I read recently about how non-developers read tutorials written by developers. What sounds natural to you, can be complete gibberish to someone else. As a lead, you constantly need to think about your audience. You need to learn multiple languages to communicate the same thing:&lt;/p&gt;
      &lt;p&gt;Developer language: "The authentication service has a dependency on the user service, and if we don't implement proper circuit breakers, we'll have cascading failures during high load."&lt;/p&gt;
      &lt;p&gt;Product language: "If our login system goes down, it could take the entire app with it. We need to build in some safeguards, which will add about a week to the timeline but prevent potential outages."&lt;/p&gt;
      &lt;p&gt;Executive language: "We're prioritizing system reliability over feature velocity for this sprint. This reduces risk of user-facing downtime that could impact revenue."&lt;/p&gt;
      &lt;p&gt;All three statements describe the same technical decision, but each is crafted for its audience. Your experts shouldn't have to learn product speak, and your product team shouldn't need to understand circuit breaker patterns. But someone needs to bridge that gap.&lt;/p&gt;
      &lt;head rend="h2"&gt;Beyond "Because, that's why!"&lt;/head&gt;
      &lt;p&gt;"I'm the lead, and we are going to do it this way." That's probably the worst way to make a decision. That might work in the short term, but it erodes trust and kills the collaborative culture that makes expert teams thrive.&lt;/p&gt;
      &lt;p&gt;Instead, treat your teams like adults and communicate the reason behind your decision:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;"We're choosing the more conservative approach because the cost of being wrong is high, and we can iterate later."&lt;/item&gt;
        &lt;item&gt;"I know this feels like extra work, but it aligns with our architectural goals and will save us time on the next three features."&lt;/item&gt;
        &lt;item&gt;"This isn't the most elegant solution, but it's the one we can ship confidently within our timeline."&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;The more comfortable you become with not being the expert, the more effective you become as a leader.&lt;/p&gt;
      &lt;p&gt;When you stop trying to out-expert the experts, you can focus on what expert teams actually need:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Clear problem definitions&lt;/item&gt;
        &lt;item&gt;Context for decision-making&lt;/item&gt;
        &lt;item&gt;Translation between different perspectives&lt;/item&gt;
        &lt;item&gt;Protection from unnecessary complexity&lt;/item&gt;
        &lt;item&gt;Space to do their best work&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Your role isn't to have all the answers. It's to make sure the right questions get asked, the right people get heard, and the right decisions get made for the right reasons.&lt;/p&gt;
      &lt;p&gt;Technical leadership in expert environments is less about command and control, and more about connection and context. You're not the conductor trying to play every instrument. You're the one helping the orchestra understand what song they're playing together.&lt;/p&gt;
      &lt;p&gt;That's a much more interesting challenge than trying to be the smartest person in the room.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://idiallo.com/blog/how-to-lead-in-a-room-full-of-experts"/><published>2025-09-24T12:52:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45360475</id><title>Just let me select text</title><updated>2025-09-24T20:36:05.548273+00:00</updated><content>&lt;doc fingerprint="cb024a5731ccf78e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Just Let Me Select Text&lt;/head&gt;By Artyom Bologov&lt;head rend="h2"&gt;Untranslatable Bios #&lt;/head&gt;&lt;p&gt;I‚Äôm lonely. Like everyone-ish else. Naturally, I‚Äôm on Bumble. (Because Tinder is a rape-friendly lure trap.) When work calls get boring I inevitably start swiping (mostly left üò¢)&lt;/p&gt;&lt;p&gt;There are lots of tourists in Armenia in the summer. From all over the world really. Speaking a stupefying range of languages. With bios and prompt answers in these numerous languages. Not necessarily discernible to me due to my language learning stagnation.&lt;/p&gt;&lt;p&gt;So there‚Äôs this profile of a pretty German girl. With bio and prompts in (an undeniably beautiful) German. Speaking English, she made the decision to use her mother tongue for the bio. A totally valid choice.&lt;/p&gt;&lt;p&gt;So I want to know the story she tells with her profile:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Select her bio,&lt;/item&gt;&lt;item&gt;copy it,&lt;/item&gt;&lt;item&gt;paste into a translator,&lt;/item&gt;&lt;item&gt;look up the exact meaning of some mistranslated German word,&lt;/item&gt;&lt;item&gt;and realize the unexpected poetic meaning she put into these 300 chars.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Except‚Ä¶ I can‚Äôt do that. The text is not selectable/copyable in Bumble app. I have to do a bunch of relatively unsurmountable steps to do what should‚Äôve taken half a minute. Like screenshot the profile and scrape the text with iOS Photos text recognition. Or use some OCR (web)app elsewhere. It‚Äôs‚Ä¶ discouraging. Thus I give up and swipe left. A shame‚Äîshe was beautiful at the very least!&lt;/p&gt;&lt;head rend="h2"&gt;Media #&lt;/head&gt;&lt;p&gt;By making the text in your UI non-selectable, you turn it into‚Ä¶ an image essentially? Images, audio, video, and interactive JS-heavy pages are multidimentional media. Not really manipulable and referenceable in any reasonable way. (Not even with Media Fragments‚Äîthey were turned down by everyone.) You lose a whole dimension (ü•Å) of functionality and benefit by going with such media or their semblance text.&lt;/p&gt;&lt;p&gt; Podcasts are not easy to roll back to useful part. Video transcripts don‚Äôt make sense without the visuals. Web graphics are opaque &lt;code&gt;&amp;lt;canvas&amp;gt;&lt;/code&gt;-es you can‚Äôt gut.

&lt;/p&gt;&lt;p&gt;Text is copyable. Text is translatable. Text is accessible (as in a11y.) Text is lightweight. Text is fundamental to how we people process information.&lt;/p&gt;&lt;p&gt;That‚Äôs why we still use text in our UIs. We want to convey the meaning. We strive to provide unambiguous instructions. We need to be understood. So why make the text harder to process and understand?&lt;/p&gt;&lt;head rend="h2"&gt;Stop It #&lt;/head&gt;&lt;p&gt;Whenever you disable text selection/copying on your UI, you commit a crime against the user. Crime against comprehension. Crime against accessibility. Crime against the meaning. Stop incapacitating your users, allow them to finally use the text.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aartaka.me/select-text.html"/><published>2025-09-24T13:56:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45360824</id><title>Smartphone Cameras Go Hyperspectral</title><updated>2025-09-24T20:36:05.354694+00:00</updated><content>&lt;doc fingerprint="77951f9c0a9da747"&gt;
  &lt;main&gt;
    &lt;p&gt;The human eye is mostly sensitive to only three bands of the electromagnetic spectrum‚Äîred, green, and blue (RGB)‚Äîin the visible range. In contrast, off-the-shelf smartphone camera sensors are potentially hyperspectral in nature, meaning that each pixel is sensitive to far more spectral bands. Now scientists have found a simple way for any conventional smartphone camera to serve as a hyperspectral sensor‚Äîby placing a card with a chart on it within its view. The new patent-pending technique may find applications in defense, security, medicine, forensics, agriculture, environmental monitoring, industrial quality control, and food and beverage quality analysis, the researchers add.&lt;/p&gt;
    &lt;p&gt;‚ÄúAt the heart of this work is a simple but powerful idea‚Äîa photo is never just an image,‚Äù says Semin Kwon, a postdoctoral research associate of biomedical engineering Purdue University in West Lafayette, Ind. ‚ÄúEvery photo carries hidden spectral information waiting to be uncovered. By extracting it, we can turn everyday photography into science.‚Äù&lt;/p&gt;
    &lt;p&gt;Using a smartphone camera and a spectral color chart, researchers can image the transmission spectrum of high-end whiskey, thus determining its authenticity. Semin Kwon/Purdue University&lt;/p&gt;
    &lt;p&gt;Every molecule has a unique spectral signature‚Äîthe degree to which it absorbs or reflects each wavelength of light. The extreme sensitivity to distinguishing color seen in scientific-grade hyperspectral sensors can help them identify chemicals based on their spectral signatures, for applications in a wide range of industries, such as medical diagnostics, distinguishing authentic versus counterfeit whiskey, monitoring air quality, and nondestructive analysis of pigments in artwork, says Young Kim, a professor of biomedical engineering at Purdue.&lt;/p&gt;
    &lt;p&gt;Previous research has pursued a number of different ways to recover spectral details from conventional smartphone RGB camera data. However, machine learning models developed for this purpose typically rely heavily on the task-specific data on which they are trained. This limits their generalizability and makes them susceptible to errors resulting from variations in lighting, image file formats, and more. Another possible avenue involved special hardware attachments, but these can prove expensive and bulky.&lt;/p&gt;
    &lt;p&gt;In the new study, the scientists designed a special color reference chart that can be printed on a card. They also developed an algorithm that can analyze smartphone pictures taken with this card and account for factors such as lighting conditions. This strategy can extract hyperspectral data from raw images with a sensitivity of 1.6 nanometers of difference in wavelength of visible light, comparable to scientific-grade spectrometers.&lt;/p&gt;
    &lt;p&gt;‚ÄúIn short, this technique could turn an ordinary smartphone into a pocket spectrometer,‚Äù Kim says.&lt;/p&gt;
    &lt;p&gt;The scientists are currently pursuing applications for their new technique in digital and mobile-health applications in both domestic and resource-limited settings. ‚ÄúWe are truly excited that this opens the door to making spectroscopy both affordable and accessible,‚Äù Kwon says.&lt;/p&gt;
    &lt;p&gt;The scientists recently detailed their findings in the journal IEEE Transactions on Image Processing.&lt;/p&gt;
    &lt;p&gt;Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/hyperspectral-imaging"/><published>2025-09-24T14:20:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361344</id><title>The Lambda Calculus (2023)</title><updated>2025-09-24T20:36:05.042838+00:00</updated><content>&lt;doc fingerprint="2e83231764ffdbf1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Lambda Calculus&lt;/head&gt;&lt;p&gt;The \(\lambda\)-calculus is, at heart, a simple notation for functions and application. The main ideas are applying a function to an argument and forming functions by abstraction. The syntax of basic \(\lambda\)-calculus is quite sparse, making it an elegant, focused notation for representing functions. Functions and arguments are on a par with one another. The result is a non-extensional theory of functions as rules of computation, contrasting with an extensional theory of functions as sets of ordered pairs. Despite its sparse syntax, the expressiveness and flexibility of the \(\lambda\)-calculus make it a cornucopia of logic and mathematics. This entry develops some of the central highlights of the field and prepares the reader for further study of the subject and its applications in philosophy, linguistics, computer science, and logic.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;1. Introduction&lt;/item&gt;&lt;item&gt;2. Syntax&lt;/item&gt;&lt;item&gt;3. Brief history of \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;4. Reduction&lt;/item&gt;&lt;item&gt;5. \(\lambda\)-theories&lt;/item&gt;&lt;item&gt;6. Consistency of the \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;7. Semantics of \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;8. Extensions and Variations&lt;/item&gt;&lt;item&gt;9. Applications&lt;/item&gt;&lt;item&gt;Bibliography&lt;/item&gt;&lt;item&gt;Academic Tools&lt;/item&gt;&lt;item&gt;Other Internet Resources&lt;/item&gt;&lt;item&gt;Related Entries&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;1. Introduction&lt;/head&gt;&lt;p&gt;The \(\lambda\)-calculus is an elegant notation for working with applications of functions to arguments. To take a mathematical example, suppose we are given a simple polynomial such as \(x^2 -2\cdot x+5\). What is the value of this expression when \(x = 2\)? We compute this by ‚Äòplugging in‚Äô 2 for \(x\) in the expression: we get \(2^2 -2\cdot 2+5\), which we can further reduce to get the answer 5. To use the \(\lambda\)-calculus to represent the situation, we start with the \(\lambda\)-term&lt;/p&gt;\[ \lambda x[x^2 -2\cdot x+5]. \]&lt;p&gt;The \(\lambda\) operators allows us to abstract over \(x\). One can intuitively read ‚Äò\(\lambda x[x^2 -2\cdot x+5]\)‚Äô as an expression that is waiting for a value \(a\) for the variable \(x\). When given such a value \(a\) (such as the number 2), the value of the expression is \(a^2 -2\cdot a+5\). The ‚Äò\(\lambda\)‚Äô on its own has no significance; it merely binds the variable \(x\), guarding it, as it were, from outside interference. The terminology in \(\lambda\)-calculus is that we want to apply this expression to an argument, and get a value. We write ‚Äò\(Ma\)‚Äô to denote the application of the function \(M\) to the argument \(a\). Continuing with the example, we get:&lt;/p&gt;\[\begin{align} (\lambda x[x^2 -2\cdot x+5])2 \rhd 2^2&amp;amp; -2\cdot 2+5 &amp;amp;\langle \text{Substitute 2 for } x\rangle \\ &amp;amp;= 4-4+5 &amp;amp;\langle\text{Arithmetic}\rangle \\ &amp;amp;= 5 &amp;amp;\langle\text{Arithmetic}\rangle \end{align}\]&lt;p&gt;The first step of this calculation, plugging in ‚Äò2‚Äô for occurrences of \(x\) in the expression ‚Äò\(x^2 - 2\cdot x + 5\)‚Äô, is the passage from an abstraction term to another term by the operation of substitution. The remaining equalities are justified by computing with natural numbers.&lt;/p&gt;&lt;p&gt;This example suggests the central principle of the \(\lambda\)-calculus, called \(\beta\)-reduction, which is also sometimes called \(\beta\)-conversion:&lt;/p&gt;\[ \tag{\(\beta\)} (\lambda x[M])N \rhd M[x := N] \]&lt;p&gt;The understanding is that we can reduce or contract \((\rhd)\) an application \((\lambda xM)N\) of an abstraction term (the left-hand side, \(\lambda xM)\) to something (the right-hand side, \(N)\) by simply plugging in \(N\) for the occurrences of \(x\) inside \(M\) (that‚Äôs what the notation ‚Äò\(M[x := N]\)‚Äô expresses). \(\beta\)-reduction, or \(\beta\)-conversion, is the heart of the \(\lambda\)-calculus. When one actually applies \(\beta\)-reduction to reduce a term, there is an important proviso that has to be observed. But this will be described in Section 2.1, when we discuss bound and free variables.&lt;/p&gt;&lt;head rend="h3"&gt;1.1 Multi-argument operations&lt;/head&gt;&lt;p&gt;What about functions of multiple arguments? Can the \(\lambda\)-calculus represent operations such as computing the length of the hypotenuse of a right triangle:&lt;/p&gt;&lt;p&gt;Hypotenuse of a right triangle with legs of length \(x\) and \(y \Rightarrow \sqrt{x^2 + y^2}\).&lt;/p&gt;&lt;p&gt;The length-of-hypotenuse operation maps two positive real numbers \(x\) and \(y\) to another positive real number. One can represent such multiple-arity operations using the apparatus of the \(\lambda\)-calculus by viewing the operation as taking one input at a time. Thus, the operation can be seen as taking one input, \(x\), a positive real number, and producing as its value not a number, but an operation: namely, the operation that takes a positive real number \(y\) as input and produces as output the positive real number \(\sqrt{x^2 + y^2}\). One could summarize the discussion by saying that the operation, hypotenuse-length, that computes the length of the hypotenuse of a right triangle given the lengths \(a\) and \(b\) of its legs, is:&lt;/p&gt;&lt;p&gt;hypotenuse-length \(:= \lambda a[\lambda b[\sqrt{a^2 + b^2}]]\)&lt;/p&gt;&lt;p&gt;By the principle of \(\beta\)-reduction, we have, for example, that hypotenuse-length 3, the application of hypotenuse-length to 3, is \(\lambda b[\sqrt{3^2 + b^2}]\), which is a function of that is ‚Äòwaiting‚Äô for another argument. The \(\lambda\)-term hypotenuse-length 3 can be viewed as a function that computes the length of the hypotenuse of a right triangle one of whose legs has length 3. We find, finally, that (hypotenuse-length 3)4‚Äîthe application of hypotenuse-length to 3 and then to 4‚Äîis 5, as expected.&lt;/p&gt;&lt;p&gt;Another way to understand the reduction of many-place functions to one-place functions is to imagine a machine \(M\) that initially starts out by loading the first \(a\) of multiple arguments \(a, b,\ldots\) into memory. If one then suspends the machine after it has loaded the first argument into memory, one can view the result as another machine M\(_a\) that is awaiting one fewer input; the first argument is now fixed.&lt;/p&gt;&lt;head rend="h3"&gt;1.2 Non-Extensionality&lt;/head&gt;&lt;p&gt;An important philosophical issue concerning the \(\lambda\)-calculus is the question of its underlying concept of functions. In set theory, a function is standardly understood as a set of argument-value pairs. More specifically, a function is understood as a set \(f\) of ordered pairs satisfying the property that \((x,y) \in f\) and \((x,z) \in f\) implies \(y = z\). If \(f\) is a function and \((x,y) \in f\), this means that the function f assigns the value \(y\) to the argument \(x\). This is the concept of functions-as-sets. Consequently, the notion of equality of functions-as-sets is equality qua sets, which, under the standard principle of extensionality, entails that two functions are equal precisely when they contain the same ordered pairs. In other words, two functions are identical if and only if they assign the same values to the same arguments. In this sense, functions-as-sets are extensional objects.&lt;/p&gt;&lt;p&gt;In contrast, the notion of a function at work in \(\lambda\)-calculus is one where functions are understood as rules: a function is given by a rule for how to determine its values from its arguments. More specifically, we can view a \(\lambda\)-term \(\lambda x[M]\) as a description of an operation that, given \(x\), produces \(M\); the body \(M\) of the abstraction term is, essentially, a rule for what to do with \(x\). This is the conception of functions-as-rules. Intuitively, given rules \(M\) and \(N\), we cannot in general decide whether \(\lambda x[M]\) is equal to \(\lambda x[N]\). The two terms might ‚Äòbehave‚Äô the same (have the same value given the same arguments), but it may not be clear what resources are needed for showing the equality of the terms. In this sense, functions-as-rules are non-extensional objects.&lt;/p&gt;&lt;p&gt;To distinguish the extensional concept of functions-as-sets from the non-extensional concept of functions-as-rules, the latter is often referred to as an ‚Äòintensional‚Äô function concept, in part because of the ostensibly intensional concept of a rule involved. This terminology is particularly predominant in the community of mathematical logicians and philosophers of mathematics working on the foundations of mathematics. But from the perspective of the philosophy of language, the terminology can be somewhat misleading, since in this context, the extensional-intensional distinction has a slightly different meaning.&lt;/p&gt;&lt;p&gt;In the standard possible-worlds framework of philosophical semantics, we would distinguish between an extensional and an intensional function concept as follows. Let us say that two functions are extensionally equivalent at a world if and only if they assign the same values to the same arguments at that world. And let us say that two functions are intensionally equivalent if and only if they assign the same values to the same arguments at every possible-world. To illustrate, consider the functions highest-mountain-on-earth and highest-mountain-in-the-Himalayas, where highest-mountain-on-earth assigns the highest mountain on earth as the value to every argument and highest-mountain-in-the-Himalayas assigns the highest mountain in the Himalayas as the value to every argument. The two functions are extensionally equivalent (at the actual world), but not intensionally so. At the actual world, the two functions assign the same value to every argument, namely Mt. Everest. Now consider a world where Mt. Everest is not the highest mountain on earth, but say, Mt. Rushmore is. Suppose further that this is so, just because Mt. Rushmore is 30.000 feet/9.100 m higher than it is at the actual world, while Mt. Everest, with its roughly 29.000 feet/8.800 m, is still the highest mountain in the Himalayas. At that world, highest-mountain-on-earth now assigns Mt. Rushmore as the value to every argument, while highest-mountain-in-the-Himalayas still assigns Mt. Everest to every object. In other words, highest-mountain-on-earth and highest-mountain-in-the-Himalayas are extensionally equivalent (at the actual world) but not intensionally equivalent.&lt;/p&gt;&lt;p&gt;A function concept may now be called extensional if and only if it requires functions that are extensionally equivalent at the actual world to be identical. And a function concept may be classified as intensional if and only if it requires intensionally equivalent functions to be identical. Note that these classifications are conceptually different from the distinctions commonly used in the foundations of mathematics. On the terminology used in the foundations of mathematics, functions-as-sets are classified as extensional since they use the axiom of extensionality as their criterion of identity, and functions-as-rules are classified as intensional because they rely on the ostensibly intensional concept of a rule. In the present possible-worlds terminology, function concepts are classified as extensional or intensional based of their behavior at possible-worlds.&lt;/p&gt;&lt;p&gt;An issue from which conceptual confusion might arise is that the two terminologies potentially pass different verdicts on the function concept at work in the \(\lambda\)-calculus. To see this, consider the following two functions:&lt;/p&gt;\[\begin{align} \addone &amp;amp;:= \lambda x[x+1] \\ \addtwosubtractone &amp;amp;:= \lambda x[[x+2]-1] \end{align}\]&lt;p&gt;These two functions are clearly extensionally equivalent: they assign the same value to the same input at the actual world. Moreover, given standard assumptions in possible worlds semantics, the two functions are also intensionally equivalent. If we assume that mathematical facts, like facts about addition and subtraction, are necessary in the sense that they are the same at every possible world, then we get that the two functions give the same value to the arguments at every possible world. So, an intensional function concept would require the two functions to be identical. In the \(\lambda\)-calculus, however, it‚Äôs not clear at all that we should identify the two functions. Formally speaking, without the help of some other principle, we cannot show that the two \(\lambda\)-terms denote the same function. Moreover, informally speaking, on the conception of functions-as-rules, it‚Äôs not even clear that we should identify them: the two terms involve genuinely different rules, and so we might be tempted to say that they denote different functions.&lt;/p&gt;&lt;p&gt;A function concept that allows for intensionally equivalent functions to be distinct is called hyperintensional. The point is that in possible-worlds terminology, the function concept at work in the \(\lambda\)-calculus may be regarded not as intentional but hyperintensional‚Äîin contrast to what the terminology common in the foundations of mathematics says. Note that it‚Äôs unclear how an intensional semantic framework, like the possible-worlds framework, could even in principle account for a non-intensional function concept. On the semantics of the \(\lambda\)-calculus, see section 7. The point here was simply to clarify any conceptual confusions that might arise from different terminologies at play in philosophical discourse.&lt;/p&gt;&lt;p&gt;The hyperintensionality of the \(\lambda\)-calculus is particularly important when it comes to its applications as a theory of not only functions, but more generally \(n\)-ary relations. On this, see section 9.3. It is effectively the hyperintensionality of the \(\lambda\)-calculus that makes it an attractive tool in this context. It should be noted, however, that the \(\lambda\)-calculus can be made extensional (as well as intensional) by postulating additional laws concerning the equality of \(\lambda\)-terms. On this, see section 5.&lt;/p&gt;&lt;head rend="h2"&gt;2. Syntax&lt;/head&gt;&lt;p&gt;The official syntax of the \(\lambda\)-calculus is quite simple; it is contained in the next definition.&lt;/p&gt;&lt;p&gt;Definition For the alphabet of the language of the \(\lambda\)-calculus we take the left and right parentheses, left and right square brackets, the symbol ‚Äò\(\lambda\)‚Äô, and an infinite set of variables. The class of \(\lambda\)-terms is defined inductively as follows:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Every variable is a \(\lambda\)-term.&lt;/item&gt;&lt;item&gt;If \(M\) and \(N\) are \(\lambda\)-terms, then so is \((MN)\).&lt;/item&gt;&lt;item&gt;If \(M\) is a \(\lambda\)-term and \(x\) is a variable, then \((\lambda x[M])\) is a \(\lambda\)-term.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;By ‚Äòterm‚Äô we always mean ‚Äò\(\lambda\)-term‚Äô. Terms formed according to rule (2) are called application terms. Terms formed according to rule (3) are called abstraction terms.&lt;/p&gt;&lt;p&gt;As is common when dealing with formal languages that have grouping symbols (the left and right parenthesis, in our case), some parentheses will be omitted when it is safe to do so (that is, when they can be reintroduced in only one sensible way). Juxtaposing more than two \(\lambda\)-terms is, strictly speaking, illegal. To avoid the tedium of always writing all needed parentheses, we adopt the following convention:&lt;/p&gt;&lt;p&gt;Convention (association to the left): When more than two terms \(M_1 M_2 M_3 \ldots M_n\) are juxtaposed we can recover the missing parentheses by associating to the left: reading from left to right, group \(M_1\) and \(M_2\) together, yielding \((M_1 M_2)M_3 \ldots M_n\); then group \((M_1 M_2)\) with \(M_3\): \(((M_1 M_2)M_3)\ldots M_n\), and so forth.&lt;/p&gt;&lt;p&gt;The convention thus gives a unique reading to any sequence of \(\lambda\)-terms whose length is greater than 2.&lt;/p&gt;&lt;head rend="h3"&gt;2.1 Variables, bound and free&lt;/head&gt;&lt;p&gt;The function of \(\lambda\) in an abstraction term \((\lambda x[M]\)) is that it binds the variable appearing immediately after it in the term \(M\). Thus \(\lambda\) is analogous to the universal and existential quantifiers \(\forall\) and \(\exists\) of first-order logic. One can define, analogously, the notions of free and bound variable in the expected way, as follows.&lt;/p&gt;&lt;p&gt;Definition The syntactic functions \(\mathbf{FV}\) and \(\mathbf{BV}\) (for ‚Äòfree variable‚Äô and ‚Äòbound variable‚Äô, respectively) are defined on the set of \(\lambda\)-terms by structural induction thus:&lt;/p&gt;&lt;p&gt;For every variable \(x\), term \(M\), and term \(N\):&lt;/p&gt;&lt;p&gt;If \(\mathbf{FV}(M) = \varnothing\) then \(M\) is called a combinator.&lt;/p&gt;&lt;p&gt;Clause (3) in the two definitions supports the intention that \(\lambda\) binds variables (ensures that they are not free). Note the difference between \(\mathbf{BV}\) and \(\mathbf{FV}\) for variables.&lt;/p&gt;&lt;p&gt;As is typical in other subjects where the concepts appear, such as first-order logic, one needs to be careful about the issue; a casual attitude about substitution can lead to syntactic difficulties.[1] We can defend a casual attitude by adopting the convention that we are interested not in terms themselves, but in a certain equivalence class of terms. We now define substitution, and then lay down a convention that allows us to avoid such difficulties.&lt;/p&gt;&lt;p&gt;Definition (substitution) We write ‚Äò\(M[x := N]\)‚Äô to denote the substitution of \(N\) for the free occurrences of \(x\) in \(M\). A precise definition[2] by recursion on the set of \(\lambda\)-terms is as follows: for all terms \(A\), \(B\), and \(M\), and for all variables \(x\) and \(y\), we define&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;\(x[x := M] \equiv M\)&lt;/item&gt;&lt;item&gt;\(y[x := M] \equiv y\) (\(y\) distinct from \(x)\)&lt;/item&gt;&lt;item&gt;\((AB)[x := M] \equiv A[x := M]B[x := M]\)&lt;/item&gt;&lt;item&gt;\((\lambda x[A])[x := M] \equiv \lambda x[A]\)&lt;/item&gt;&lt;item&gt;\((\lambda y[A])[x := M] \equiv \lambda y[A[x := M]]\) (\(y\) distinct from \(x)\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Clause (1) of the definition simply says that if we are to substitute \(M\) for \(x\) and we are dealing simply with \(x\), then the result is just \(M\). Clause (2) says that nothing happens when we are dealing (only) with a variable different from \(x\) but we are to substitute something for \(x\). Clause (3) tells us that substitution unconditionally distributes over applications. Clauses (4) and (5) concern abstraction terms and parallel clauses (1) and (2) (or rather, clauses (2) and (1), in opposite order): If the bound variable \(z\) of the abstraction term \(\lambda z[A]\) is identical to the variable \(x\) for which we are to do a substitution, then we do not perform any substitution (that is, substitution ‚Äústops‚Äù). This coheres with the intention that \(M[x := N]\) is supposed to denote the substitution of \(N\) for the free occurrences of \(x\) in \(M\). If \(M\) is an abstraction term \(\lambda x[A]\) whose bound variable is \(x\), then \(x\) does not occurr freely in \(M\), so there is nothing to do. This explains clause 4. Clause (5), finally, says that if the bound variable of an abstraction term differs from \(x\), then at least \(x\) has the ‚Äúchance ‚Äù to occur freely in the abstraction term, and substitution continues into the body of the abstraction term.&lt;/p&gt;&lt;p&gt;Definition (change of bound variables, \(\alpha\)-convertibility). The term \(N\) is obtained from the term \(M\) by a change of bound variables if, roughly, any abstraction term \(\lambda x[A]\) inside \(M\) has been replaced by \(\lambda y[A[x := y]]\).&lt;/p&gt;&lt;p&gt;Let us say that terms \(M\) and \(N\) are \(\alpha\)-convertible if there is a sequence of changes of bound variables starting from \(M\) and ending at \(N\).&lt;/p&gt;&lt;p&gt;Axiom. \(\beta\)-conversion (stated with a no-capture proviso):&lt;/p&gt;&lt;p&gt; \( (\lambda x[M])N \rhd M[x := N]\), &lt;lb/&gt; provided no variable that occurrs free in \(N\) becomes bound after its substitution into \(M\).&lt;/p&gt;&lt;p&gt;Roughly, we need to adhere to the principle that free variables ought to remain free; when an occurrence of a variable is threatened to become bound by a substitution, simply perform enough \(\alpha\)-conversions to sidestep the problem. If we keep this in mind, we can work with \(\lambda\)-calculus without worrying about these nettlesome syntactic difficulties. So, for example, we can‚Äôt apply the function \(\lambda x[\lambda y[x(y-5)]]\) to the argument \(2y\) because upon substitution of ‚Äú\(2y\)‚Äù for ‚Äú\(x\)‚Äù, the ‚Äú\(y\)‚Äù in ‚Äú\(2y\)‚Äù would be captured by the variable-binding operator ‚Äú\(\lambda y\)‚Äù. Such a substitution would yield a function different from the one intended. However, we can first transform \(\lambda x[\lambda y[x(y-5)]]\) to \(\lambda x[\lambda z[x(z-5)]]\) by \(\alpha\)-conversion, and then apply this latter function to the argument \(2y\). So whereas the following is not a valid use of \(\beta\)-conversion: \[ (\lambda x[\lambda y[x(y-5)]])2y \rhd \lambda y[2y(y-5)]\] we can validly use \(\beta\)-conversion to conclude: \[ (\lambda x[\lambda z[x(z-5)]])2y \rhd \lambda z[2y(z-5)]\] This example helps one to see why the proviso to \(\beta\)-conversion is so important. The proviso is really no different from the one used in the statement of an axiom of the predicate calculus, namely: \(\forall x\phi \to \phi^{\tau}_x\), provided no variable that is free in the term \(\tau\) before the substitution becomes bound after the substitution.&lt;/p&gt;&lt;p&gt;The syntax of \(\lambda\)-calculus is quite flexible. One can form all sorts of terms, even self-applications such as \(xx\). Such terms appear at first blush to be suspicious; one might suspect that using such terms could lead to inconsistency, and in any case one might find oneself reaching for a tool with which to forbid such terms. If one were to view functions and sets of ordered pairs of a certain kind, then the \(x\) in \(xx\) would be a function (set of ordered pairs) that contains as an element a pair \((x,y)\) whose first element would be \(x\) itself. But no set can contain itself in this way, lest the axiom of foundation (or regularity) be violated. Thus, from a set theoretical perspective such terms are clearly dubious. Below one can find a brief sketch of one such tool, type theory. But in fact such terms do not lead to inconsistency and serve a useful purpose in the context of \(\lambda\)-calculus. Moreover, forbidding such terms, as in type theory, does not come for free (e.g., some of the expressiveness of untyped \(\lambda\)-calculus is lost).&lt;/p&gt;&lt;head rend="h3"&gt;2.2 Combinators&lt;/head&gt;&lt;p&gt;As defined earlier, a combinator is a \(\lambda\)-term with no free variables. One can intuitively understand combinators as ‚Äòcompletely specified‚Äô operations, since they have no free variables. There are a handful of combinators that have proven useful in the history of \(\lambda\)-calculus; the next table highlights some of these special combinators. Many more could be given (and obviously there are infinitely many combinators), but the following have concise definitions and have proved their utility. Below is a table of some standard \(\lambda\)-terms and combinators.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Name&lt;/cell&gt;&lt;cell&gt;Definition &amp;amp; Comments&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[xz(yz)]]]\) &lt;p&gt;Keep in mind that ‚Äò\(xz(yz)\)‚Äô is to be understood as the application \((xz)(yz)\) of \(xz\) to \(yz. \bS\) can thus be understood as a substitute-and-apply operator: \(z\) ‚Äòintervenes‚Äô between \(x\) and \(y\): instead of applying \(x\) to \(y\), we apply \(xz\) to \(yz\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{K}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[x]]\) &lt;p&gt;The value of \(\mathbf{K}M\) is the constant function whose value for any argument is simply \(M.\)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{I}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[x]\) &lt;p&gt;The identity function.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{B}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[x(yz)]]]\) &lt;p&gt;Recall that ‚Äò\(xyz\)‚Äô is to be understood as \((xy)z\), so this combinator is not a trivial identity function.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{C}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[xzy]]]\) &lt;p&gt;Swaps an argument.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{T}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[x]]\) &lt;p&gt;Truth value true. Identical to \(\mathbf{K}\). We shall see later how these representations of truth values plays a role in the blending of logic and \(\lambda\)-calculus.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{F}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[y]]\) &lt;p&gt;Truth value false.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\boldsymbol{\omega}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[xx]\) &lt;p&gt;Self-application combinator&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\boldsymbol{\Omega}\)&lt;/cell&gt;&lt;cell&gt;\(\boldsymbol{\omega \omega}\) &lt;p&gt;Self-application of the self-application combinator. Reduces to itself.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{Y}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda f[(\lambda x[f(xx)])(\lambda x[f(xx)]\))] &lt;p&gt;Curry‚Äôs paradoxical combinator. For every \(\lambda\)-term \(X\), we have: \[\begin{align} \mathbf{Y}X &amp;amp;\rhd (\lambda x[X(xx)])(\lambda x[X(xx)]) \\ &amp;amp;\rhd X((\lambda x[X(xx)])(\lambda x[X(xx)])) \end{align}\] The first step in the reduction shows that \(\mathbf{Y}\)X reduces to the application term \((\lambda x[X(xx)])(\lambda x[X(xx)]\)), which is recurring in the third step. Thus, \(\mathbf{Y}\) has the curious property that \(\mathbf{Y}\)X and X\((\mathbf{Y}\)X) reduce to a common term.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\boldsymbol{\Theta}\)&lt;/cell&gt;&lt;cell&gt;\((\lambda x[\lambda f[f(xxf)]])(\lambda x[\lambda f[f(xxf)]]\)) &lt;p&gt;Turing‚Äôs fixed-point combinator. For every \(\lambda\)-term \(X\), \(\boldsymbol{\Theta}X\) reduces to \(X(\boldsymbol{\Theta}X)\), which one can confirm by hand. (Curry‚Äôs paradoxical combinator \(\mathbf{Y}\) does not have this property.)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Below is a table of notational conventions employed in this entry.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Notation&lt;/cell&gt;&lt;cell&gt;Reading &amp;amp; Comments&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(MN\)&lt;/cell&gt;&lt;cell&gt;The application of the function \(M\) to the argument \(N\). &lt;p&gt;Usually, parentheses are used to separate the function from the argument, like so: ‚Äò\(M(N)\)‚Äô. However, in \(\lambda\)-calculus and kindred subjects the parentheses are used as grouping symbols. Thus, it is safe to write the function and the argument adjacent to one other.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(PQR\)&lt;/cell&gt;&lt;cell&gt;The application of the function \(PQ\)‚Äîwhich is itself the application of the function \(P\) to the argument \(Q\)‚Äîto \(R\). &lt;p&gt;If we do not use parentheses to separate function and argument, how are we to disambiguate expressions that involve three or more terms, such as ‚Äò\(PQR\)‚Äô? Recall our convention that we are to understand such officially illegal expressions by working from left to right, always putting parentheses around adjacent terms. Thus, ‚Äò\(PQR\)‚Äô is to be understood as \((PQ)R\). ‚Äò\(PQRS\)‚Äô is \(((PQ)R)S\). The expression ‚Äò\((PQ)R\)‚Äô is disambiguated; by our convention, it is identical to \(PQR\). The expression ‚Äò\(P(QR)\)‚Äô is also explicitly disambiguated; it is distinct from \(PQR\) because it is the application of \(P\) to the argument \(QR\) (which is itself the application of the function \(Q\) to the argument \(R)\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\((\lambda x[M])\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\) term that binds the variable \(x\) in the \(\boldsymbol{body}\) term \(M\). &lt;p&gt;The official vocabulary of the \(\lambda\)-calculus consists of the symbol ‚Äò\(\lambda\)‚Äô, left ‚Äò(‚Äôand right ‚Äò)‚Äô parentheses, and a set of variables (assumed to be distinct from the three symbols ‚Äò\(\lambda\)‚Äô, ‚Äò(‚Äô, and ‚Äò)‚Äô lest we have syntactic chaos).&lt;/p&gt;&lt;p&gt;Alternative notation. It is not necessary to include two kinds of grouping symbols (parentheses and square brackets) in the syntax. Parentheses or square brackets alone would obviously suffice. The two kinds of brackets are employed in this entry for the sake of readability. Given the two kinds of grouping symbols, we could economize further and omit the parentheses from abstraction terms, so that ‚Äò\((\lambda x[M]\))‚Äô would be written as ‚Äò\(\lambda x[M]\)‚Äô.&lt;/p&gt;&lt;p&gt;Some authors write ‚Äò\(\lambda x.M\)‚Äô or ‚Äò\(\lambda x\cdot M\)‚Äô, with a full stop or a centered dot separating the bound variable from the body of the abstraction term. As with the square brackets, these devices are intended to assist reading \(\lambda\)-terms; they are usually not part of the official syntax. (One sees this device used in earlier works of logic, such as Principia Mathematica, where the function of the symbol . in expressions such as ‚Äò\(\forall x\).\(\phi\)‚Äô is to get us to read the whole of the formula \(\phi\) as under the scope of the \(\forall x\).)&lt;/p&gt;&lt;p&gt;Some authors write abstraction terms without any device separating the bound variable from the body: such terms are crisply written as, e.g., ‚Äò\(\lambda xx\)‚Äô, ‚Äò\(\lambda yx\)‚Äô. The practice is not without its merits: it is about as concise as one can ask for, and permits an even simpler official syntax of the \(\lambda\)-calculus. But this practice is not flawless. In ‚Äò\(\lambda xyz\)‚Äô, is the bound variable \(x\) or is it \(xy\)? Usually the names of variables are single letters, and theoretically this is clearly sufficient. But it seems unduly restrictive to forbid the practice of giving longer names to variables; indeed, such constructions arise naturally in computer programming languages.&lt;/p&gt;&lt;p&gt;For the sake of uniformity, we will adopt the square bracket notation in this entry. (Incidentally, this notation is used in (Turing, 1937).)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(M[x := A]\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\)-term that is obtained by substituting the \(\lambda\)-term A for all free occurrences of \(x\) inside \(M\). &lt;p&gt;A bewildering array of notations to represent substitution can be found in the literature on \(\lambda\)-calculus and kindred subjects:&lt;/p&gt;\[ M[x/A], M[A/x], M_{x}^A, M_{A}^x, [x/A]M,\ldots \]&lt;p&gt;Which notation to use for substitution seems to be a personal matter. In this entry we use a linear notation, eschewing superscripts and subscripts. The practice of representing substitution with ‚Äò:=‚Äô comes from computer science, where ‚Äò:=‚Äô is read in some programming languages as assigning a value to a variable.&lt;/p&gt;&lt;p&gt;As with the square brackets employed to write abstraction terms, the square brackets employed to write substitution are not officially part of the syntax of the \(\lambda\)-calculus. \(M\) and A are terms, \(x\) is a variable; \(M[x := A]\) is another term.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(M \equiv N\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\)-terms \(M\) and \(N\) are identical: understood as sequences of symbols, \(M\) and \(N\) have the same length and corresponding symbols of the sequences are identical. &lt;p&gt;The syntactic identity relation \(\equiv\) is not part of the official syntax of \(\lambda\)-calculus; this relation between \(\lambda\)-terms belongs to the metatheory of \(\lambda\)-calculus. It is clearly a rather strict notion of equality between \(\lambda\)-terms. Thus, it is not the case (if \(x\) and \(y\) are distinct variables) that \(\lambda x[x] \equiv \lambda y[y]\), even though these two terms clearly ‚Äòbehave‚Äô in the same way in the sense that both are expressions of the identity operation \(x \Rightarrow x\). Later we will develop formal theories of equality of \(\lambda\)-terms with the aim of capturing this intuitive equality of \(\lambda x[x]\) and \(\lambda y[y]\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;3. Brief history of \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;\(\lambda\)-calculus arose from the study of functions as rules. Already the essential ingredients of the subject can be found in Frege‚Äôs pioneering work (Frege, 1893). Frege observed, as we did above, that in the study of functions it is sufficient to focus on unary functions (i.e., functions that take exactly one argument). (The procedure of viewing a multiple-arity operation as a sequence of abstractions that yield an equivalent unary operation is called currying the operation. Perhaps it would be more historically accurate to call the operation fregeing, but there are often miscarriages of justice in the appellation of mathematical ideas.) In the 1920s, the mathematician Moses Sch√∂nfinkel took the subject further with his study of so-called combinators. As was common in the early days of the subject, Sch√∂nfinkel was interested in the kinds of transformations that one sees in formal logic, and his combinators were intended to be a contribution to the foundations of formal logic. By analogy with the reduction that one sees in classical propositional logic with the Sheffer stroke, Sch√∂finkel established the astonishing result that the all functions (in the sense of all transformations) could be given in terms of the combinators \(\mathbf{K}\) and \(\bS\); later we will see the definition of these combinators.&lt;/p&gt;&lt;p&gt;Theorem For every term \(M\) made up of \(\mathbf{K}\) and \(\bS\) and the variable \(x\), there exists a term \(F\) (built only from \(\mathbf{K}\) and \(\bS)\) such that we can derive \(Fx = M\).&lt;/p&gt;&lt;p&gt;(The proof that these two suffice to represent all functions is beyond the scope of this entry. For further discussion, see the entry on combinatory logic.) One can prove the theorem constructively: there is an algorithm that, given \(M\), produces the required \(F\). Church called this \(F\) ‚Äò\(\lambda x[M]\)‚Äô (Church, 1932).[3] From this perspective, the \(\beta\)-rule can be justified: if ‚Äò\(\lambda x[M]\)‚Äô is to be a function \(F\) satisfying \(Fx = M\), then \(\lambda x[M]\)x should transform to \(M\). This is just a special case of the more general principle that for all \(N, (\lambda x[M])N\) should transform to \(M[x := N]\).&lt;/p&gt;&lt;p&gt;Although today we have more clearly delimited systems of abstraction and rewriting, in its early days \(\lambda\)-calculus and combinatory logic (√† la Sch√∂nfinkel) were bound up with investigations of foundations of mathematics. In the hands of Curry, Church, Kleene, and Rosser (some of the pioneers in the subject) the focus was on defining mathematical objects and carrying out logical reasoning inside the these new systems. It turned out that these early attempts at so-called illative \(\lambda\)-calculus and combinatory logic were inconsistent. Curry isolated and polished the inconsistency; the result is now known as Curry‚Äôs paradox. See the entry on Curry‚Äôs paradox and appendix B of (Barendregt, 1985).&lt;/p&gt;&lt;p&gt;The \(\lambda\)-calculus earns a special place in the history of logic because it was the source of the first undecidable problem. The problem is: given \(\lambda\)-terms \(M\) and \(N\), determine whether \(M = N\). (A theory of equational reasoning about \(\lambda\)-terms has not yet been defined; the definition will come later.) This problem was shown to be undecidable.&lt;/p&gt;&lt;p&gt;Another early problem in the \(\lambda\)-calculus was whether it is consistent at all. In this context, inconsistency means that all terms are equal: one can reduce any \(\lambda\)-term \(M\) to any other \(\lambda\)-term \(N\). That this is not the case is an early result of \(\lambda\)-calculus. Initially one had results showing that certain terms were not interconvertible (e.g., \(\mathbf{K}\) and \(\bS)\); later, a much more powerful result, the so-called Church-Rosser theorem, helped shed more light on \(\beta\)-conversion and could be used to give quick proofs of the non-inter-convertibility of whole classes of \(\lambda\)-terms. See below for more detailed discussion of consistency.&lt;/p&gt;&lt;p&gt;The \(\lambda\)-calculus was a somewhat obscure formalism until the 1960s, when, at last, a ‚Äòmathematical‚Äô semantics was found. Its relation to programming languages was also clarified. Till then the only models of \(\lambda\)-calculus were ‚Äòsyntactic‚Äô, that is, were generated in the style of Henkin and consisted of equivalence classes of \(\lambda\)-terms (for suitable notions of equivalence). Applications in the semantics of natural language, thanks to developments by Montague and other linguists, helped to ‚Äòspread the word‚Äô about the subject. Since then the \(\lambda\)-calculus enjoys a respectable place in mathematical logic, computer science, linguistics (see, e.g., Heim and Kratzer 1998), and kindred fields.&lt;/p&gt;&lt;head rend="h2"&gt;4. Reduction&lt;/head&gt;&lt;p&gt;Various notions of reduction for \(\lambda\)-terms are available, but the principal one is \(\beta\)-reduction, which we have already seen earlier. Earlier we used the notation ‚Äò\(\rhd\)‚Äô; we can be more precise. In this section we discuss \(\beta\)-reduction and some extensions.&lt;/p&gt;&lt;p&gt;Definition (one-step \(\beta\)-reduction \(\rhd_{\beta ,1})\) For \(\lambda\)-terms \(A\) and \(B\), we say that \(A\) \(\beta\)-reduces in one step to \(B\), written \(A \rhd_{\beta ,1} B\), just in case there exists an (occurrence of a) subterm \(C\) of \(A\), a variable \(x\), and \(\lambda\)-terms \(M\) and \(N\) such that \(C \equiv(\lambda x[M])N\) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M[x := N]\).&lt;/p&gt;&lt;p&gt;Here are some examples of \(\beta\)-reduction:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The variable \(x\) does not \(\beta\)-reduce to anything. (It does not have the right shape: it is simply a variable, not an application term whose left-hand side is an abstraction term.)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;\((\lambda x[x])a \rhd_{\beta ,1} a\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;If \(x\) and \(y\) are distinct variables, then \((\lambda x[y])a \rhd_{\beta ,1} y\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \((\lambda x[(\lambda y[xy])a])b]\) \(\beta\)-reduces in one step to two different \(\lambda\)-terms:&lt;/p&gt;\[ (\lambda x[(\lambda y[xy])a])b \rhd_{\beta ,1} (\lambda y[by])a \]&lt;p&gt;and&lt;/p&gt;\[ (\lambda x[(\lambda y[xy])a])b \rhd_{\beta ,1} (\lambda x[xa])b \]&lt;p&gt;Moreover, one can check that these two terms \(\beta\)-reduce in one step to a common term: \(ba\). We thus have:&lt;/p&gt;&lt;td&gt;\((\lambda y[by])a\)&lt;/td&gt;&lt;td&gt;\(\nearrow\)&lt;/td&gt;&lt;td&gt;\(\searrow\)&lt;/td&gt;&lt;td&gt;\((\lambda x[(\lambda y[xy])a])b\)&lt;/td&gt;&lt;td&gt;\(ba\)&lt;/td&gt;&lt;td&gt;\(\searrow\)&lt;/td&gt;&lt;td&gt;\(\nearrow\)&lt;/td&gt;&lt;td&gt;\((\lambda x[xa])b\)&lt;/td&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;As with any binary relation, one can ask many questions about the relation \(\rhd_{\beta ,1}\) holding between \(\lambda\)-terms, and one can define various derived notions in terms of \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction sequence from a \(\lambda\)-term \(A\) to a \(\lambda\)-term \(B\) is a finite sequence \(s_1 , \ldots s_n\) of \(\lambda\)-terms starting with \(A\), ending with \(B\), and whose adjacent terms \((s_k,s_{k+1})\) satisfy the property that \(s_k \rhd_{\beta ,1} s_{k+1}\).&lt;/p&gt;&lt;p&gt;More generally, any sequence \(s\)‚Äîfinite or infinite‚Äîstarting with a \(\lambda\)-term \(A\) is said to be a \(\beta\)-reduction sequence commencing with \(A\) provided that the adjacent terms \((s_k,s_{k+1})\) of \(s\) satisfy the property that \(s_k \rhd_{\beta ,1} s_{k+1}\).&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Continuing with \(\beta\)-reduction Example 1, there are no \(\beta\)-reduction sequences at all commencing with the variable \(x\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Continuing with \(\beta\)-reduction Example 2, the two-term sequence&lt;/p&gt;\[ (\lambda x[x])a, a \]&lt;p&gt;is a \(\beta\)-reduction sequence from \((\lambda x[x])a\) to \(a\). If \(a\) is a variable, then this \(\beta\)-reduction sequence cannot be prolonged, and there are no other \(\beta\)-reduction sequences commencing with \((\lambda x[x])a\); thus, the set of \(\beta\)-reduction sequences commencing with \((\lambda x[x])a\) is finite and contains no infinite sequences.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;The combinator \(\boldsymbol{\Omega}\) has the curious property that \(\Omega \rhd_{\beta ,1} \Omega\). Every term of every \(\beta\)-reduction sequence commencing with \(\boldsymbol{\Omega}\) (finite or infinite) is equal to \(\boldsymbol{\Omega}\).&lt;/item&gt;&lt;item&gt;&lt;p&gt;Consider the term \(\mathbf{K}a\boldsymbol{\Omega}\). There are infinitely many reduction sequences commencing with this term:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \ldots\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If \(a\) is a variable, one can see that all finite reduction sequences commencing with \(\bK a\boldsymbol{\Omega}\) end at \(a\), and there is exactly one infinite reduction sequence.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Definition A \(\beta\)-redex of a \(\lambda\)-term \(M\) is (an occurrence of) a subterm of \(M\) of the form \((\lambda x[P])Q\). (‚Äòredex‚Äô comes from ‚Äòreducible expression.) A \(\beta\)-redex is simply a candidate for an application of \(\beta\)-reduction. Doing so, one contracts the \(\beta\)-redex. A term is said to be in \(\beta\)-normal form if it has no \(\beta\)-redexes.&lt;/p&gt;&lt;p&gt;(Can a term have multiple \(\beta\)-normal forms? The answer is literally ‚Äòyes‚Äô, but substantially the answer is ‚Äòno‚Äô: If a \(M\) and \(M'\) are \(\beta\)-normal forms of some term, then \(M\) is \(\alpha\)-convertible to \(M'\) Thus, \(\beta\)-normal forms are unique up to changes of bound variables.)&lt;/p&gt;&lt;p&gt;So far we have focused only on one step of \(\beta\)-reduction. One can combine multiple \(\beta\)-reduction steps into one by taking the transitive closure of the relation \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;Definition For \(\lambda\)-terms \(A\) and \(B\), one says that \(A\) \(\beta\)-reduces to \(B\), written \(A \rhd_{\beta} B\), if either \(A \equiv B\) or there exists a finite \(\beta\)-reduction sequence from \(A\) to \(B\).&lt;/p&gt;&lt;p&gt;Definition A term \(M\) has a \(\beta\)-normal form if there exists a term \(N\) such that \(N\) is in \(\beta\)-normal form an \(M \rhd_{\beta} N\).&lt;/p&gt;&lt;p&gt;Reducibility as defined is a one-way relation: it is generally not true that if \(A \rhd_{\beta} B\), then \(B \rhd_{\beta} A\). However, depending on one‚Äôs purposes, one may wish to treat \(A\) and \(B\) as equivalent if either \(A\) reduces to \(B\) or \(B\) reduces to \(A\). Doing so amounts to considering the reflexive, symmetric, and transitive closure of the relation \(\rhd_{\beta ,1,}\).&lt;/p&gt;&lt;p&gt;Definition For \(\lambda\)-terms \(A\) and \(B\), we say that \(A =_{\beta} B\) if either \(A \equiv B\) or there exists a sequence \(s_1 , \ldots s_n\) starting with \(A\), ending with \(B\), and whose adjacent terms \((s_k,s_{k+1})\) are such that either \(s_k \rhd_{\beta ,1} s_{k+1}\) or \(s_{k+1} \rhd_{\beta ,1} s_k\).&lt;/p&gt;&lt;head rend="h3"&gt;4.1 Other notions of reduction&lt;/head&gt;&lt;p&gt;We have thus far developed the theory of \(\beta\)-reduction. This is by no means the only notion of reduction available in the \(\lambda\)-calculus. In addition to \(\beta\)-reduction, a standard relation between \(\lambda\)-terms is that of \(\eta\)-reduction:&lt;/p&gt;&lt;p&gt;Definition (one-step \(\eta\)-reduction) For \(\lambda\)-terms \(A\) and \(B\), we say that \(A\) \(\beta \eta\)-reduces in one step to \(B\), written \(A \rhd_{\beta \eta ,1} B\), just in case there exists an (occurrence of a) subterm \(C\) of \(A\), a variable \(x\), and \(\lambda\)-terms \(M\) and \(N\) such that either&lt;/p&gt;&lt;p&gt;\(C \equiv(\lambda x[M])N\) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M[x := N]\)&lt;/p&gt;&lt;p&gt;or&lt;/p&gt;&lt;p&gt;\(C \equiv(\lambda x[Mx]\)) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M\).&lt;/p&gt;&lt;p&gt;The first clause in the definition of \(\rhd_{\beta \eta ,1}\) ensures that the relation extends the relation of one-step \(\beta\)-reduction. As we did for the relation of one-step \(\beta\)-reduction, we can replay the development for \(\eta\)-reduction. Thus, one has the notion of an \(\eta\)-redex, and from \(\rhd_{\eta ,1}\) one can define the relation \(\rhd_{\eta}\) between \(\lambda\)-terms as the reflexive and transitive closure of \(\rhd_{\eta ,1}\), which captures zero-or-more-steps of \(\eta\)-reduction. Then one defines \(=_{\eta}\) as the symmetric and transitive closure of \(\rhd_{\eta}\).&lt;/p&gt;&lt;p&gt;If \(A \rhd_{\eta ,1} B\), then the length of \(B\) is strictly smaller than that of \(A\). Thus, there can be no infinite \(\eta\)-reductions. This is not the case of \(\beta\)-reduction, as we saw above in \(\beta\)-reduction sequence examples 3 and 4.&lt;/p&gt;&lt;p&gt;One can combine notions of reduction. One useful combination is to blend \(\beta\)- and \(\eta\)-reduction.&lt;/p&gt;&lt;p&gt;Definition (one-step \(\beta \eta\)-reduction) \(\lambda x[Mx] \rhd_{\beta \eta ,1} M\) and \((\lambda x[M]N)) \rhd_{\beta \eta ,1} M[x := N]\). A \(\lambda\)-term \(A\) \(\beta \eta\)-reduces in one step to a \(\lambda\)-term \(B\) just in case either \(A\) \(\beta\)-reduces to \(B\) in one step or \(A\) \(\eta\)-reduces to \(B\) in one step.&lt;/p&gt;&lt;p&gt;Again, one can replay the basic concepts of reduction, as we did for \(\beta\)-reduction, for this new notion of reduction \(\beta \eta\).&lt;/p&gt;&lt;head rend="h3"&gt;4.2 Reduction strategies&lt;/head&gt;&lt;p&gt;Recall that a term is said to be in \(\beta\)-normal form if it has no \(\beta\)-redexes, that is, subterms of the shape \((\lambda x[M]\))N. A term has a \(\beta\)-normal form if it can be reduced to a term in \(\beta\)-normal form. It should be intuitively clear that if a term has a \(\beta\)-normal form, then we can find one by exhaustively contracting all all \(\beta\)-redexes of the term, then exhaustively contracting all \(\beta\)-redexes of all resulting terms, and so forth. To say that a term has a \(\beta\)-normal form amounts to saying that this blind search for one will eventually terminate.&lt;/p&gt;&lt;p&gt;Blind search for \(\beta\)-normal forms is not satisfactory. In addition to be aesthetically unpleasant, it can be quite inefficient: there may not be any need to exhaustively contract all \(\beta\)-redexes. What is wanted is a strategy‚Äîpreferably, a computable one‚Äîfor finding a \(\beta\)-normal form. The problem is to effectively decide, if there are multiple \(\beta\)-redexes of a term, which ought to be reduced.&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction strategy is a function whose domain is the set of all \(\lambda\)-terms and whose value on a term \(M\) not in \(\beta\)-normal form is a redex subterm of \(M\), and whose value on all terms M in \(\beta\)-normal form is simply \(M\).&lt;/p&gt;&lt;p&gt;In other words, a \(\beta\)-reduction strategy selects, whenever a term has multiple \(\beta\)-redexes, which one should be contracted. (If a term is in \(\beta\)-normal form, then nothing is to be done, which is why we require in the definition of \(\beta\)-reduction strategy that it does not change any term in \(\beta\)-normal form.) One can represent a strategy \(S\) as a relation \(\rhd_S\) on \(\lambda\)-terms, with the understanding that \(M \rhd_S N\) provided that \(N\) is obtained from \(M\) in one step by adhering to the strategy S. When viewed as relations, strategies constitute a subrelation of \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;A \(\beta\)-reduction strategy may or may not have the property that adhering to the strategy will ensure that we (eventually) reach a \(\beta\)-normal form, if one exists.&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction strategy \(S\) is normalizing if for all \(\lambda\)-terms \(M\), if \(M\) has a \(\beta\)-normal form \(N\), then the sequence \(M, S(M), S(S(M)),\ldots\) terminates at \(N\).&lt;/p&gt;&lt;p&gt;Some \(\beta\)-reduction strategies are normalizing, but others are not.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The rightmost strategy, whereby we always choose to reduce the rightmost \(\beta\)-redex (if there are any \(\beta\)-redexes) is not normalizing. Consider, for example, the term KI\(\Omega\). This term has two \(\beta\)-redexes: itself, and \(\Omega\) (which, recall, is the term \(\omega\omega\equiv(\lambda\)x[\(xx])(\lambda\)x[\(xx]\))). By working with left-hand \(\beta\)-redexes, we can \(\beta\)-reduce KI\(\Omega\) to \(\mathbf{I}\) in two steps. If we insist on working with the rightmost \(\beta\)-redex \(\Omega\) we reduce KI(\(\Omega\)) to \(\mathbf{KI}\)(\(\Omega \)), then \(\mathbf{KI}\)(\(\Omega\)), ‚Ä¶.&lt;/item&gt;&lt;item&gt;The leftmost strategy, whereby we always choose to reduce the leftmost \(\beta\)-redex (if there are any \(\beta\)-redexes) is normalizing. The proof of this fact is beyond the scope of this entry; see (Barendregt, 1985, section 13.2) for details.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Once we have defined a reduction strategy, it is natural to ask whether one can improve it. If a term has a \(\beta\)-normal form, then a strategy will discover a normal form; but might there be a shorter \(\beta\)-reduction sequence that reaches the same normal form (or a term that is \(\alpha\)-convertible to that normal form)? This is the question of optimality. Defining optimal strategies and showing that they are optimal is generally considerably more difficult than simply defining a strategy. For more discussion, see (Barendregt, 1984 chapter 10).&lt;/p&gt;&lt;p&gt;For the sake of concreteness, we have discussed only \(\beta\)-reduction strategies. But in the definitions above the notion of reduction \(\beta\) is but one possibility. For any notion \(R\) of reduction we have the associated theory of \(R\)-reduction strategies, and one can replay the problems of normalizability, optimality, etc., for \(R\).&lt;/p&gt;&lt;head rend="h2"&gt;5. \(\lambda\)-theories&lt;/head&gt;&lt;p&gt;We discussed earlier how the \(\lambda\)-calculus is a non-extensional theory of functions. If, in the non-extensional spirit, we understand \(\lambda\)-terms as descriptions, how should we treat equality of \(\lambda\)-terms? Various approaches are available. In this section, let us treat the equality relation = as a primitive, undefined relation holding between two \(\lambda\)-terms, and try to axiomatize the properties that equality should have. The task is to identity axioms and formulate suitable rules of inference concerning the equality of \(\lambda\)-terms.&lt;/p&gt;&lt;p&gt;Some obvious properties of equality, having nothing to do with \(\lambda\)-calculus, are as follows:&lt;/p&gt;\[\tag{Reflexivity} \frac{}{X=X} \] \[\tag{Symmetry} \frac{X=Y}{Y=X} \] \[\tag{Transitivity} \frac{X=Y \quad Y=Z}{X=Z} \]&lt;p&gt;As is standard in proof theory, the way to read these rules of inference is that above the horizontal rule \(\frac{}{\phantom{X=X}}\) are the premises of the rule (which are equations) and the equation below the horizontal rule is the conclusion of the rule of inference. In the case of the reflexivity rule, nothing is written above the horizontal rule. We understand such a case as saying that, for all terms \(X\), we may infer the equation \(X = X\) from no premises.&lt;/p&gt;&lt;head rend="h3"&gt;5.1 The basic theory \(\lambda\)&lt;/head&gt;&lt;p&gt;The three rules of inference listed in the previous section governing equality have nothing to do with the \(\lambda\)-calculus. The following lists rules of inference that relate the undefined notion of equality and the two term-building operations of the \(\lambda\)-calculus, application and abstraction.&lt;/p&gt;\[ \frac{M=N}{AM=AN} \quad \frac{M=N}{MA=NA} \] \[ \tag{\(\boldsymbol{\xi}\)} \frac{M=N}{\lambda x[M] = \lambda x[N]} \]&lt;p&gt;Together, these rules of inference say that = is a congruence relation on the set of \(\lambda\)-terms: it ‚Äòpreserves‚Äô both the application and abstraction term-building operations&lt;/p&gt;&lt;p&gt;The final rule of inference, \(\beta\)-conversion, is the most important:&lt;/p&gt;\[\tag{\(\boldsymbol{\beta}\)} \frac{}{(\lambda x[M])A = M[x := A]} \]&lt;p&gt;As before with the reflexivity rule, the rule \(\boldsymbol{\beta}\) has no premises: for any variable \(x\) and any terms \(M\) and \(A\), one can infer the equation \((\lambda x[M])A = M[x := A]\) at any point in a formal derivation in the theory \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;head rend="h3"&gt;5.2 Extending the basic theory \(\lambda\)&lt;/head&gt;&lt;p&gt;A number of extensions to \(\boldsymbol{\lambda}\) are available. Consider, for example, the rule (\(\boldsymbol{\eta}\)), which expresses the principle of \(\eta\)-reduction as a rule of inference:&lt;/p&gt;\[\tag{\(\boldsymbol{\eta}\)} \frac{}{\lambda x[Mx] = M} \text{ provided } x \not\in \mathbf{FV}(M) \]&lt;p&gt;Rule \(\boldsymbol{\eta}\) tells us that a certain kind of abstraction is otiose: it is safe to identify \(M\) with the function that, given an argument \(x\), applies \(M\) to \(x\). Through this rule we can also see that all terms are effectively functions. One can intuitively justify this rule using the principle of \(\beta\)-reduction.&lt;/p&gt;\[\tag{\(\mathbf{Ext}\)} \frac{Mx=Nx}{M=N}\text{ provided } x \not\in \mathbf{FV}(M) \cup \mathbf{FV}(N) \]&lt;p&gt;One can view rule \(\mathbf{Ext}\) as a kind of generalization principle. If we have derived that \(Mx = Nx\), but \(x\) figures in neither \(M\) nor \(N\), then we have effectively shown that \(M\) and \(N\) are alike. Compare this principle to the principle of universal generalization in first-order logic: if we have derived \(\phi(x)\) from a set \(\Gamma\) of hypotheses in which \(x\) is not free, then we can conclude that \(\Gamma\) derives \(\forall x\phi\).&lt;/p&gt;&lt;p&gt;Another productive principle in the \(\lambda\)-calculus permits us to identify terms that ‚Äòact‚Äô the same:&lt;/p&gt;\[\tag{\(\boldsymbol{\omega}\)} \frac{\text{For all terms }x, Mx=Nx}{M=N} \]&lt;p&gt;The rule \(\boldsymbol{\omega}\) has infinitely many hypotheses: on the assumption that \(Mx = Nx\), no matter what \(x\) may be, then we can conclude that \(M = N\). The \(\boldsymbol{\omega}\) rule is an analogue in the \(\lambda\)-calculus of the rule of inference under the same name in formal number theory, according to which one can conclude the universal formula \(\forall x\phi\) provided one has proofs for \(\phi(x := \mathbf{0}), \phi(x := \mathbf{1}),\ldots\) . Note that unlike the rule \(\mathbf{Ext}\), the condition that \(x\) not occur freely in \(M\) or \(N\) does not arise.&lt;/p&gt;&lt;head rend="h2"&gt;6. Consistency of the \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;Is the \(\lambda\)-calculus consistent? The question might not be well-posed. The \(\lambda\)-calculus is not a logic for reasoning about propositions; there is no apparent notion of contradiction \((\bot)\) or a method of forming absurd propositions (e.g., \(p \wedge \neg p)\). Thus ‚Äòinconsistency‚Äô of the \(\lambda\)-calculus cannot mean that \(\bot\), or some formula tantamount to \(\bot\), is derivable. A suitable notion of ‚Äòconsistent‚Äô is, however, available. Intuitively, a logic is inconsistent if it permits us to derive too much. The theory \(\lambda\) is a theory of equations. We can thus take inconsistency of \(\lambda\) to mean: all equations are derivable. Such a property, if it were true of \(\lambda\), would clearly show that \(\lambda\) is of little use as a formal theory.&lt;/p&gt;&lt;p&gt;Early formulations of the idea of \(\lambda\)-calculus by A. Church were indeed inconsistent; see (Barendregt, 1985, appendix 2) or (Rosser, 1985) for a discussion. To take a concrete problem: how do we know that the equation \(\bK = \mathbf{I}\) is not a theorem of \(\lambda\)? The two terms are obviously intuitively distinct. \(\bK\) is a function of two arguments, whereas \(\mathbf{I}\) is a function of one argument. If we could show that \(\bK = \mathbf{I}\), then we could show that \(\mathbf{KK} = \mathbf{IK}\), whence \(\mathbf{KK} = \bK\) would be a theorem of \(\lambda\), along with many other equations that strike us as intuitively unacceptable. But when we‚Äôre investigating a formal theory such as \(\lambda\), intuitive unacceptability by no means implies underivability. What is missing is a deeper understanding of \(\beta\)-reduction.&lt;/p&gt;&lt;p&gt;An early result that gave such an understanding is known as the Church-Rosser theorem:&lt;/p&gt;&lt;p&gt;Theorem (Church-Rosser) If \(P \rhd_{\beta} Q\) and \(P \rhd_{\beta}\) R, then there exists a term \(S\) such that both \(Q \rhd_{\beta} S\) and \(R \rhd_{\beta} S\).&lt;/p&gt;&lt;p&gt;(The proof of this theorem is quite non-trivial and is well-beyond the scope of this entry.) The result is a deep fact about \(\beta\)-reduction. It says that no matter how we diverge from \(P\) by \(\beta\)-reductions, we can always converge again to a common term.&lt;/p&gt;&lt;p&gt;The Church-Rosser theorem gives us, among other things, that the plain \(\lambda\)-calculus‚Äîthat is, the theory \(\lambda\) of equations between \(\lambda\)-terms‚Äîis consistent, in the sense that not all equations are derivable.&lt;/p&gt;&lt;p&gt;As an illustration, we can use the Church-Rosser theorem to solve the earlier problem of showing that the two terms \(\bK\) and \(\mathbf{I}\) are not identified by \(\lambda\). The two terms are in \(\beta\)-normal form, so from them there are no \(\beta\)-reduction sequences at all. If \(\bK = \mathbf{I}\) were a theorem of \(\lambda\), then there would be a term \(M\) from which there is a \(\beta\)-reduction path to both \(\mathbf{I}\) and \(\bK\). The Church-Rosser theorem then implies the two paths diverging from \(M\) can be merged. But this is impossible, since \(\bK\) and \(\mathbf{I}\) are distinct \(\beta\)-normal forms.&lt;/p&gt;&lt;p&gt;The Church-Rosser theorem implies the existence of \(\beta\)-reduction sequences commencing from \(\bK\) and from \(\mathbf{I}\) that end at a common term. But there are no \(\beta\)-reduction sequences at all commencing from \(\mathbf{I}\), because it is in \(\beta\)-normal form, and likewise for \(\bK\).&lt;/p&gt;&lt;p&gt;Theorem \(\lambda\) is consistent, in the sense that not every equation is a theorem.&lt;/p&gt;&lt;p&gt;To prove the theorem, it is sufficient to produce one underivable equation. We have already worked through an example: we used the Church-Rosser theorem to show that the equation \(\bK = \mathbf{I}\) is not a theorem of \(\lambda\). Of course, there‚Äôs nothing special about these two terms. A significant generalization of this result is available: if \(M\) and \(N\) in \(\beta\)-normal form but \(M\) is distinct from \(N\), then the equation \(M = N\) is not a theorem of \(\lambda\). (This simple condition for underivability does not generally hold if we add additional rules of inference to \(\lambda\).)&lt;/p&gt;&lt;p&gt;The theories \(\lambda \eta\) and \(\lambda \omega\) are likewise consistent. One can prove these consistency results along the lines of the consistency proof for \(\lambda\) by extending the Church-Rosser theorem to the wider senses of derivability of these theories.&lt;/p&gt;&lt;head rend="h2"&gt;7. Semantics of \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;As we‚Äôve said at the outset, the \(\lambda\)-calculus is, at heart, about functions and their applications. But it is surprisingly difficult to cash this idea out in semantic terms. A natural approach would be to try to associate with every \(\lambda\)-term \(M\) a function \(f_M\) over some domain \(D\) and to interpret application terms \((MN)\) using function application as \(f_M(f_N).\) But this idea quickly runs into difficulties. To begin with, it‚Äôs easy to see that, in this context, we can‚Äôt use the standard set-theoretic concept of functions-as-sets (see section 1.2 of this entry). According to this concept, remember, a function \(f\) is a set of argument-value pairs, where every argument gets assigned a unique value. The problem arises in the context of self-applications. Remember from section 2.1 that the untyped \(\lambda\)-calculus allows \(\lambda\)-terms such as \((xx)\), which intuitively apply \(x\) to itself. On the semantic picture we‚Äôre exploring, we can obtain the associated function \(f_{(xx)}\) for the term \((xx)\) by taking the function \(f_x\) for \(x\) and applying it to itself:&lt;/p&gt;\[ f_{(xx)}=f_x(f_x) \]&lt;p&gt;But following functions-as-sets, this would mean that the set \(f_x\) needs to contain an argument-value pair that has \(f_x\) as its first component and \(f_{(xx)}\) as the second:&lt;/p&gt;\[ f_{x}=\{\ldots, (f_{x},f_{(xx)})), \ldots\} \]&lt;p&gt;But this would make \(f_x\) a non-well-founded object: defining \(f_x\) would involve \(f_x\) itself. In fact, sets like this are excluded in standard axiomatic set theory by the axiom of foundation (also known as the axiom of regularity). ‚ÄîThis is further semantic evidence that the concept of a function underlying the \(\lambda\)-calculus can‚Äôt be the extensional functions-as-sets concept.&lt;/p&gt;&lt;p&gt;But the problem runs even deeper than that. Even when we use a non-extensional notion of a function, such as the functions-as-rules conception (see again section 1.2), we run into difficulties. In the untyped \(\lambda\)-calculus, everything can both be function and an argument to functions. Correspondingly, we should want our domain \(D\) to include, in some sense, the function space \(D^D\), which contains all and only the functions with both arguments and values from \(D\). To see this:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Every element of \(D\) can be a function that applies to elements of \(D\), and what‚Äôs returned can then be again be an argument for elements of \(D\). So, every element of \(D\) intuitively corresponds to a member of \(D^D\).&lt;/item&gt;&lt;item&gt;If, in turn, we take a member of \(D^D\), i.e., a function with arguments and values from \(D\), this is precisely the kind of thing we want to include in our domain \(D\). So, intuitively, we want every member of \(D^D\) to correspond to a member of \(D\).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In short, we want there to be a one-to-one correspondence between our domain and its own function space, i.e., we want them to satisfy the ‚Äòequation‚Äô \(X\cong X^X\). But this is impossible since it contradicts Cantor‚Äôs theorem.&lt;/p&gt;&lt;p&gt;Given these difficulties, the question arises whether it‚Äôs possible to give a set-theoretic model for the \(\lambda\)-calculus in the first place? It turns out that it is. D. Scott was the first to describe such a model in an unpublished manuscript from 1969. This model, \(D_\infty\), solves the aforementioned problems with Cantor‚Äôs theorem by suitably restricting the function space \(D^D\), by only letting some members of \(D^D\) correspond to members of \(D\). Covering Scott‚Äôs construction goes beyond the scope of this entry, since it involves advanced tools from algebra and topology; see (Meyer 1982), (Barendregt, 1985, chapter 18.2), or (Hindley and Seldin, 2008, chapter 16) for details. Instead, we‚Äôll discuss the more general question: What is a model for the \(\lambda\)-calculus? That is, leaving aside for a moment the question whether sets are functions, rules, or something altogether different, we ask what kind of mathematical structure a model for the \(\lambda\)-calculus is in the first place.&lt;/p&gt;&lt;head rend="h3"&gt;7.1 \(\lambda\)-Models&lt;/head&gt;&lt;p&gt;It turns out that there are multiple, essentially equivalent, ways of defining the notion of a model for the \(\lambda\)-calculus; see (Barendregt, 1985, chapter 5) or (Hindley and Seldin, 2008, chapter 15). In the following, we‚Äôll discuss what we consider the most palatable notion for philosophers familiar with the standard semantics for first-order logic (see, e.g., the entry on Classical Logic ), the so-called syntactical \(\lambda\)-models. These models first appear in the work of (Hindley and Longo, 1980), (Koymans, 1982), and (Meyer 1982). They derive their name from the fact that their clauses closely correspond to the syntactic rules of the calculus \(\boldsymbol{\lambda}\). This is somewhat unsatisfactory and motivates ‚Äòsyntax-free‚Äô definitions (see below). At the same time, the syntactical \(\lambda\)-models provide a fairly transparent and accessible route into the world of \(\lambda\)-models. In addition, despite their conceptual shortcomings, syntactical models have proven a technically useful tool in the semantical study of the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;In order to avoid the set-theoretic problems mentioned above, most definitions of \(\lambda\)-models use so-called applicative structures. The idea is to treat the denotations of \(\lambda\)-terms not as set-theoretic functions, but as unanalyzed, first-order ‚Äòfunction-objects‚Äô, instead. Correspondingly, then, we treat function application as an unanalyzed binary operation on these function-objects:&lt;/p&gt;&lt;p&gt;Definition An applicative structure is a pair \((D,\cdot)\), where \(D\) is some set and \(\cdot\) a binary operation on \(D\). To avoid trivial models, we usually assume that \(D\) has at least two elements.&lt;/p&gt;&lt;p&gt;Applicative structures are, in a sense, first-order models of function spaces that satisfy the problematic equation \(X\cong X^X\). \(\lambda\)-models, in turn, are defined over them.&lt;/p&gt;&lt;p&gt;For the definition of our \(\lambda\)-models, we work with valuations‚Äîa concept familiar from first-order semantics. Valuations assign denotations to the variables and are used primarily in the semantic clauses for the \(\lambda\)-operator. Additionally, they can be used to express general claims over the domain, in a way that is familiar from the semantics for the first-order quantifiers \(\exists x\) and \(\forall x\).&lt;/p&gt;&lt;p&gt;Definition A valuation in an applicative structure \((D,\cdot)\) is a function \(\rho\) that assigns an element \(\rho(x)\in D\) to every variable \(x\).&lt;/p&gt;&lt;p&gt;As a useful piece of notation, for \(\rho\) a valuation in some applicative structure \((D,\cdot)\), \(x\) a variable, and \(d\in D\) an object, we define the valuation \(\rho[x\mapsto d]\) by saying that: \[\rho[x\mapsto d](y)=\begin{cases} d &amp;amp; \text{ if }y=x\\ \rho(y) &amp;amp; \text{otherwise}\end{cases}\] That is, \(\rho[x\mapsto d]\) is the result of changing the value of \(x\) to be \(d\), while leaving all other other values under \(\rho\) unchanged.&lt;/p&gt;&lt;p&gt;Definition A syntactical \(\lambda\)-model is a triple \(\mathfrak{M}=(D,\cdot,\llbracket \ \rrbracket)\), where \((D,\cdot)\) is an applicative structure and \(\llbracket \ \rrbracket\) is a function that assigns to every \(\lambda\)-term M and valuation \(\rho\) a denotation \(\llbracket M\rrbracket_\rho\in D\) subject to the following constraints:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;\(\llbracket x\rrbracket_\rho=\rho(x)\)&lt;/item&gt;&lt;item&gt;\(\llbracket MN\rrbracket_\rho=\llbracket M\rrbracket_\rho\cdot \llbracket N\rrbracket_\rho\)&lt;/item&gt;&lt;item&gt;\(\llbracket \lambda xM\rrbracket_\rho\cdot d=\llbracket M\rrbracket_{\rho[x\mapsto d]}\), for all \(d\in D\)&lt;/item&gt;&lt;item&gt;\(\llbracket \lambda xM\rrbracket_\rho = \llbracket \lambda xN\rrbracket_\rho\), whenever for all \(d\in D\), we have \(\llbracket M\rrbracket_{\rho[x\mapsto d]}=\llbracket N\rrbracket_{\rho[x\mapsto d]}\)&lt;/item&gt;&lt;item&gt;\(\llbracket M\rrbracket_\rho=\llbracket M\rrbracket_\sigma\), whenever \(\rho(x)=\sigma(x)\) for all \(x\in \mathbf{FV}(M)\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Intuitively, in a model \(\mathfrak{M}\), \(\llbracket M\rrbracket_\rho\) is the function-object denoted by the \(\lambda\)-term \(M\) under the valuation \(\rho\).&lt;/p&gt;&lt;p&gt;It is now straight-forward to define what it means for a \(\lambda\)-model \(\mathfrak{M}\) to satisfy an equation \(M=N\), symbolically \(\mathfrak{M}\vDash M=N\):&lt;/p&gt;&lt;p&gt;Definition (satisfaction).&lt;/p&gt;\[\mathfrak{M}\vDash M=N\text{ iff for all }\rho\text{, we have } \llbracket M\rrbracket_\rho=\llbracket N\rrbracket_\rho\]&lt;p&gt;In words: an equation \(M=N\) holds in a model \(\mathfrak{M}\) just in case the \(\lambda\)-terms \(M\) and \(N\) have the same denotation under every valuation in the underlying applicative structure.&lt;/p&gt;&lt;p&gt;Note that clauses 3. and 4. from the definition of a syntactical \(\lambda\)-model directly mirror the \(\boldsymbol{\lambda}\)-rules \(\boldsymbol{\beta}\) and \(\boldsymbol{\xi}\), respectively (see section 5.1 above). This is the ‚Äòsyntactic‚Äô nature of our models. While this might be semantically unsatisfactory (see below), it makes it relatively straight-forward to prove a soundness theorem for the semantics provided by the syntactical \(\lambda\)-models; see (Barendregt, 1985, Theorem 5.3.4) and (Hindley and Seldin, 2008. Theorem 15.12):&lt;/p&gt;&lt;p&gt;Theorem For all terms \(M,N\), if \(M=N\) is derivable in \(\boldsymbol{\lambda}\), then for all syntactical \(\lambda\)-models \(\mathfrak{M}\), we have that \(\mathfrak{M}\vDash M=N\).&lt;/p&gt;&lt;p&gt;This theorem provides a first ‚Äòsanity-check‚Äô for the semantics. But note that, so far, we haven‚Äôt shown that there exist any syntactical \(\lambda\)-models at all.&lt;/p&gt;&lt;p&gt;This worry is addressed by constructing so-called ‚Äòterm models‚Äô, which are not unlike the well-known Henkin constructions from first-order semantics. In order to define these models, we first need the notion of a \(\boldsymbol{\lambda}\)-equivalence class for a given \(\lambda\)-term \(M\). This class contains precisely the terms that \(\boldsymbol{\lambda}\) proves identical to \(M\):&lt;/p&gt;\[ [M]_{\boldsymbol{\lambda}}=\{N:\boldsymbol{\lambda}\text{ proves }M=N\} \]&lt;p&gt;We then define the term model for \(\boldsymbol{\lambda}\), \(\mathfrak{T}\), by setting:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(D=\{[M]_\boldsymbol{\lambda}:M\text{ is a }\lambda\text{-term}\}\)&lt;/item&gt;&lt;item&gt;\([M]_\boldsymbol{\lambda}\cdot [N]_\boldsymbol{\lambda}=[MN]_\boldsymbol{\lambda}\)&lt;/item&gt;&lt;item&gt;\(\llbracket M\rrbracket_\rho=[M[x_1:=N_1]\ldots[x_n:=N_n]]_\boldsymbol{\lambda}\), where \(\mathbf{FV}(M)=\{x_1,\ldots,x_n\}\) and \(\rho(x_1)=N_1, \ldots,\rho(x_n)=N_n\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;It is easily seen that this indeed defines a syntactical \(\lambda\)-model. In fact, it is easily checked that in the term model for \(\boldsymbol{\lambda}\), we have that:&lt;/p&gt;\[ \mathfrak{T}\vDash M=N\text{ iff }\boldsymbol{\lambda}\text{ derives }M=N. \]&lt;p&gt;This paves a way for a very simple completeness proof for \(\boldsymbol{\lambda}\) with respect to the class of syntactical \(\lambda\)-models; see (Meyer, 1982, 98‚Äì99) for one of the few explicit mentions of this kind of result in the literature:&lt;/p&gt;&lt;p&gt;Theorem For all terms \(M,N\), if for all syntactical \(\lambda\)-models \(\mathfrak{M}\), we have that \(\mathfrak{M}\vDash M=N\), then \(M=N\) is derivable in \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;p&gt;The proof is a simple proof by contraposition, which uses the term model \(\mathfrak{T}\) as a countermodel to any non-derivable identity in \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;p&gt;But there are reasons to be dissatisfied with the syntactical \(\lambda\)-models as a semantics for the \(\lambda\)-calculus. For one, by virtue of clauses 3. and 4. mirroring rules \(\boldsymbol{\beta}\) and \(\boldsymbol{\xi}\), the soundness result is ‚Äòbaked into‚Äô the semantics, as it were. This is unsatisfactory from a semantic perspective since it means that via the syntactical \(\lambda\)-models, we don‚Äôt really learn anything directly about what conditions an applicative structure needs to satisfy in order to adequately model the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;A related worry is that the clauses 3. and 4. are not recursive in nature. That is, they don‚Äôt allow us to compute the denotation of a complex \(\lambda\)-term from the denotations of its parts and information about the syntactic operation used to combine them. In our syntax (see section 2), there are two ways of constructing complex \(\lambda\)-terms: application terms of the form \(MN\) and abstraction terms of the form \((\lambda x[M])\). Clause 1. of our syntactical \(\lambda\)-models is a recursive clause for the syntactical application operation, but we don‚Äôt have a recursive clause for the syntactical operation of \(\lambda\)-abstraction. Clauses 3. and 4. are rather conditions on the denotation function \(\llbracket \ \rrbracket\) than recursive clauses. This is unsatisfactory since it means that we‚Äôre not really given a compositional semantics for the \(\lambda\)-operator by the syntactical \(\lambda\)-models.&lt;/p&gt;&lt;p&gt;These worries are taken care of in the development of syntax-free \(\lambda\)-models. A comprehensive discussion of syntax-free models goes beyond the scope of this entry; but see (Barendregt, 1985, chapter 5.2) and (Hinley and Seldin, 2008, chapter 15B) for the details. Suffice it to say that the definition of syntax-free \(\lambda\)-models involves determining precisely under which conditions an applicative structure is suitable for interpreting the \(\lambda\)-calculus. The resulting \(\lambda\)-models, then, indeed provide (something much closer to) a recursive, compositional semantics, where the syntactical operation of \(\lambda\)-abstraction is interepreted by a corresponding semantic operation on applicative structures.&lt;/p&gt;&lt;p&gt;It is worth noting, however, that syntactical \(\lambda\)-models and the syntax-free \(\lambda\)-models are, in a certain sense, equivalent: every syntactical \(\lambda\)-model defines a syntax-free \(\lambda\)-model and vice versa; see (Barendregt, 1985, theorem 5.3.6) and (Hinley and Seldin, 2008, theorem 15.20) for the details. From a technical perspective, this result allows us to freely move between the different presentations of \(\lambda\)-models and to use, in a given context, the notion of a model that is most expedient. At the same time, there may be philosophical reasons to prefer one presentation over the other, such as the semantic worries about syntactical \(\lambda\)-models mentioned above.&lt;/p&gt;&lt;p&gt;Before moving to model constructions, let us briefly mention that there are various ways of approaching \(\lambda\)-models. One particularly interesting approach we‚Äôve neglected so far is from the perspective of category theory and categorical logic. There are well-known model descriptions using so-called ‚ÄòCartesian closed categories‚Äô; see (Koymans, 1982). Covering these model descriptions goes beyond the scope of the present entry since it requires a familiarity with a wide range of concepts from category theory; see the entry Category Theory for a sense of the machinery involved. For the details of these model descriptions, instead, (Barendregt, 1985, sections 5.4‚Äì6). In recent years, there has been a renewed interest in categorical approaches to the \(\lambda\)-calculus, which have mainly focused on typed versions of the \(\lambda\)-calculus (see sections 8.2 and 9.1.2 below) but also include the untyped \(\lambda\)-calculus discussed in this article. See, for example, (Hyland, 2017) for a recent discussion.&lt;/p&gt;&lt;head rend="h3"&gt;7.2 Model Constructions&lt;/head&gt;&lt;p&gt;The term model we‚Äôve seen in section 7.1 is rather trivial: it directly reflects the syntactic structure of the \(\lambda\)-terms by modeling precisely syntactic equality modulo \(\boldsymbol{\lambda}\)-provable equality. This makes the term model mathematically and philosophically rather uninteresting. The construction and study of more interesting concrete \(\lambda\)-models is one of the principal aims of the model theory for the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;We‚Äôve already mentioned what‚Äôs perhaps the most important, but was definitely the first non-trivial model for the \(\lambda\)-calculus: Scott‚Äôs \(D_\infty\). But there are also other interesting model constructions, such as Plotkin and Scott‚Äôs graph model \(P_\omega\), first described in (Plotkin 1972) and (Scott, 1974). These model constructions, however, usually rely on fairly involved mathematical methods, both for their definitions and for verifying that they are indeed \(\lambda\)-models. Consequently, covering these constructions goes beyond the scope of this entry; see (Hinley and Seldin, 2008, chapter 16F) for an overview of various model constructions and (Barendregt, 1985, chapter 18) for many of the formal details.&lt;/p&gt;&lt;p&gt;One of the advantages of having different models is that one sees different aspects of equality in the \(\lambda\)-calculus: each of the different models takes a different view on what \(\lambda\)-terms get identified. An interesting question in this context is: What is the \(\lambda\)-theory of a given class of models? In this context, we call a class \(\mathcal{C}\) of \(\lambda\)-models complete just in case every (consistent) \(\lambda\)-theory is satisfied by some model in \(\mathcal{C}\). See (Salibra, 2003) for an overview of various completeness and incompleteness results for interesting classes of \(\lambda\)-models.&lt;/p&gt;&lt;head rend="h2"&gt;8. Extensions and Variations&lt;/head&gt;&lt;head rend="h3"&gt;8.1 Combinatory logic&lt;/head&gt;&lt;p&gt;A sister formalism of the \(\lambda\)-calculus, developed slightly earlier, deals with variable-free combinations. Combinatory logic is indeed even simpler than the \(\lambda\)-calculus, since it lacks a notion of variable binding.&lt;/p&gt;&lt;p&gt;The language of combinatory logic is built up from combinators and variables. There is some flexibility in precisely which combinators are chosen as basic, but some standard ones are \(\mathbf{I}, \bK , \bS, \mathbf{B}\) and \(\mathbf{C}\). (The names are not arbitrary.)&lt;/p&gt;&lt;p&gt;As with the \(\lambda\)-calculus, with combinatory logic one is interested in reducibility and provability. The principal reduction relations are:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Combinator&lt;/cell&gt;&lt;cell&gt;Reduction Axiom&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;\(\bI x = x\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bK\)&lt;/cell&gt;&lt;cell&gt;\(\bK xy = x\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\(\bS xyz = xz(yz)\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bB\)&lt;/cell&gt;&lt;cell&gt;\(\bB xyz = x(yz)\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\bC\)&lt;/cell&gt;&lt;cell&gt;\(\bC xyz = xzy\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;There is a passage from \(\lambda\)-calculus to combinatory logic via translation. It turns out that although combinatory logic lacks a notion of abstraction, one can define such a notion and thereby simulate the \(\lambda\)-calculus in combinatory logic. Here is one translation; it is defined recursively.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell&gt;Rule&lt;/cell&gt;&lt;cell&gt;Expression&lt;/cell&gt;&lt;cell&gt;Translation&lt;/cell&gt;&lt;cell&gt;Condition&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;\(x\)&lt;/cell&gt;&lt;cell&gt;\(x\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;\(MN\)&lt;/cell&gt;&lt;cell&gt;M\(^*\)N\(^*\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;cell&gt;\(\lambda x[M]\)&lt;/cell&gt;&lt;cell&gt;\(\bK\)M&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;\(\lambda x[x]\)&lt;/cell&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;\(\lambda x[Mx]\)&lt;/cell&gt;&lt;cell&gt;M&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bB M(\lambda x[N)]^*\)&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bC (\lambda x[M])^*\)N&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in \(N\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bS M^*N^*\)&lt;/cell&gt;&lt;cell&gt;\(x\) occurs freely in both \(M\) and \(N\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;This translation works inside-out, rather than outside-in. To illustrate:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The translation of the term \(\lambda y[y]\), a representative of the identity function, is mapped by this translation to the identity combinator \(\bI\) (because of Rule 4), as expected.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \(\lambda x[\lambda y[x]]\) that we have been calling ‚Äò\(\bK\)‚Äôis mapped by this translation to:&lt;/p&gt;\[\begin{align} \lambda x[\lambda y[x]] &amp;amp;\equiv \lambda x[\bK x] &amp;amp;\langle \text{Rule 1}\rangle \\ &amp;amp;\equiv \bK &amp;amp;\langle \text{Rule 3} \rangle \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \(\lambda x[\lambda y[yx]]\) that switches its two arguments is mapped by this translation to:&lt;/p&gt;\[\begin{align} \lambda x[\lambda y[yx]] &amp;amp;\equiv \lambda x[\bC(\lambda y[y])^* x] &amp;amp;\langle\text{Rule 8}\rangle \\ &amp;amp;\equiv \lambda x[\bC\bI x] &amp;amp;\langle\lambda y[y] \equiv \bI,\text{ by Rule 4}\rangle \\ &amp;amp;\equiv \bB\bC\bI)(\lambda x[x])^* &amp;amp;\langle\text{Rule 7}\rangle \\ &amp;amp;\equiv \bB(\bC\bI)\bI &amp;amp;\langle(\lambda x[x])^* \equiv \bI,\text{ by Rule 4}\rangle \end{align}\]&lt;p&gt;We can confirm that the \(\lambda\)-term \(\lambda x[\lambda y[yx]]\) and the translated combinatory logic term \(\bB(\bC\bI)\bI\) have analogous applicative behavior: for all \(\lambda\)-terms \(P\) and \(Q\) we have&lt;/p&gt;\[ (\lambda x[\lambda y[yx]])PQ \rhd (\lambda y[yP]) \rhd QP; \]&lt;p&gt;likewise, for all combinatory logic terms \(P\) and \(Q\) we have&lt;/p&gt;\[ \bB(\bC\bI)\bI PQ \rhd (\bC\bI)(\bI P)Q \rhd \bI Q(\bI P) \rhd Q(\bI P) \rhd QP \]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We can give but a glimpse of combinatory logic; for more on the subject, consult the entry on combinatory logic. Many of the issues discussed here for \(\lambda\)-calculus have analogues in combinatory logic, and vice versa.&lt;/p&gt;&lt;head rend="h3"&gt;8.2 Adding types&lt;/head&gt;&lt;p&gt;In many contexts of reasoning and computing it is natural to distinguish between different kinds of objects. The way this distinction is introduced is by requiring that certain formulas, functions, or relations accept arguments or permit substitution only of some kinds of objects rather than others. We might require, for example, that addition + take numbers as arguments. The effect of this restriction is to forbid, say, the addition of 5 and the identity function \(\lambda x.x\).[4] Regimenting objects into types is also the idea behind the passage from (unsorted, or one-sorted) first-order logic to many-sorted first-order logic. (See (Enderton, 2001) and (Manzano, 2005) for more about many-sorted first-order logic.) As it stands, the \(\lambda\)-calculus does not support this kind of discrimination; any term can be applied to any other term.&lt;/p&gt;&lt;p&gt;It is straightforward to extend the untyped \(\lambda\)-calculus so that it discriminates between different kinds of objects. This entry limits itself to the type-free \(\lambda\)-calculus. See the entries on type theory and Church‚Äôs type theory for a detailed discussion of the extensions of \(\lambda\)-calculus that we get when we add types, and see (Barendregt, Dekkers, Statman, 2013) for a book length treatment of the subject.&lt;/p&gt;&lt;p&gt;From a model-theoretic perspective, it‚Äôs interesting to add that (Scott, 1980) uses the semantic fact that categorical models for the untyped \(\lambda\)-calculus (see section 7.1) derive from the categorical models of the typed \(\lambda\)-calculus to argue for a conceptual priority of the typed over the untyped calculus.&lt;/p&gt;&lt;head rend="h2"&gt;9. Applications&lt;/head&gt;&lt;head rend="h3"&gt;9.1 Logic √† la \(\lambda\)&lt;/head&gt;&lt;p&gt;Here are two senses in which \(\lambda\)-calculus is connected with logic.&lt;/p&gt;&lt;head rend="h4"&gt;9.1.1 Terms as logical constants&lt;/head&gt;&lt;p&gt;In the table of combinators above, we defined combinators \(\bT\) and \(\bF\) and said that they serve as representations in the \(\lambda\)-calculus of the truth values true and false, respectively. How do these terms function as truth values?&lt;/p&gt;&lt;p&gt;It turns out that when one is treating \(\lambda\)-calculus as a kind of programming language, one can write conditional statements ‚ÄúIf \(P\) then \(A\) else \(B\)‚Äù simply as \(PAB\), where of course \(P, A\), and \(B\) are understood as \(\lambda\)-terms. If \(P \rhd \bT\), that is, P is ‚Äòtrue‚Äô, then we have&lt;/p&gt;\[ \text{if-}P\text{-then-}A\text{-else-}B := PAB \rhd \bT AB \rhd A, \]&lt;p&gt;(recall that, by definition, \(\bT \equiv \bK\)) and if \(P \rhd \bF\), that is, \(P\) is ‚Äòfalse‚Äô, then&lt;/p&gt;\[ \text{if-}P\text{-then-}A\text{-else-}B := PAB \rhd \bF AB \rhd B, \]&lt;p&gt;(recall that, by definition, \(\mathbf{F} \equiv \mathbf{KI})\) which is just what we expect from a notion of if-then-else. If \(P\) reduces neither to \(\mathbf{T}\) nor \(\mathbf{F}\), then we cannot in general say what \(\text{if-}P\text{-then-}A\text{-else-}B\) is.&lt;/p&gt;&lt;p&gt;The encoding we‚Äôve just sketched of some of the familiar truth values and logical connectives of classical truth-table logic does not show that \(\lambda\)-calculus and classical logic are intimately related. The encoding shows little more than embeddibility of the rules of computation of classical truth-table logic in \(\lambda\)-calculus. Logics other than classical truth-table logic can likewise be represented in the \(\lambda\)-calculus, if one has sufficient computable ingredients for the logic in question (e.g., if the logical consequence relation is computable, or if a derivability relation is computable, etc.). For more on computing with \(\lambda\)-calculus, see section 9.2 below. A more intrinsic relationship between logic and \(\lambda\)-calculus is discussed in the next section.&lt;/p&gt;&lt;head rend="h4"&gt;9.1.2 Typed \(\lambda\)-calculus and the Curry-Howard-de Bruijn correspondence&lt;/head&gt;&lt;p&gt;The correspondence to be descried here between logic and the \(\lambda\)-calculus is seen with the help of an apparatus known as types. This section sketches the beginnings of the development of the subject known as type theory. We are interested in developing type theory only so far as to make the so-called Curry-Howard-de Bruijn correspondence visible. A more detailed treatment can be found in the entry on type theory; see also (Hindley, 1997) and (Barendregt, Dekkers, Statman, 2013).&lt;/p&gt;&lt;p&gt;Type theory enriches the untyped \(\lambda\)-calculus by requiring that terms be given types. In the untyped \(\lambda\)-calculus, the application \(MN\) is a legal term regardless of what \(M\) and \(N\) are. Such freedom permits one to form such suspicious terms as \(xx\), and thence terms such as the paradoxical combinator \(\mathbf{Y}\). One might wish to exclude terms like \(xx\) on the grounds that \(x\) is serving both as a function (on the left-hand side of the application) and as an argument (on the right-hand side of the application). Type theory gives us the resources for making this intuitive argument more precise.&lt;/p&gt;&lt;p&gt;Assigning types to terms The language of type theory begins with an (infinite) set of type variables (which is assumed to be disjoint from the set of variables of the \(\lambda\)-calculus and from the symbol ‚Äò\(\lambda\)‚Äô itself). The set of types is made up of type variables and the operation \(\sigma \rightarrow \tau\). Variables in type theory now come with a type annotation (unlike the unadorned term variables of untyped \(\lambda\)-calculus). Typed variables are rendered ‚Äò\(x : \sigma\)‚Äô; the intuitive reading is ‚Äòthe variable \(x\) has the type \(\sigma\)‚Äô. The intuitive reading of the judgment ‚Äò\(t : \sigma \rightarrow \tau\)‚Äô is that the term \(t\) is a function that transforms arguments of type \(\sigma\) into arguments of type \(\tau\). Given an assignment of types to term variables, one has the typing rules:&lt;/p&gt;\[ (M : \sigma \rightarrow \tau)(N : \sigma) : \tau \]&lt;p&gt;and&lt;/p&gt;\[ (\lambda x : \sigma[M : \tau]) : \sigma \rightarrow \tau \]&lt;p&gt;The above two rules define the assignment of types to applications and to abstraction terms. The set of terms of type theory is the set of terms built up according to these formation rules.&lt;/p&gt;&lt;p&gt;The above definition of the set of terms of type theory is sufficient to rule out terms such as \(xx\). Of course, ‚Äò\(xx\)‚Äô is not a typed term at all for the simple reason that no types have been assigned to it. What is meant is that there is no type \(\sigma\) that could be assigned to \(x\) such that ‚Äò\(xx\)‚Äô could be annotated in a legal way to make a typed term. We cannot assign to \(x\) a type variable, because then the type of the left-hand \(x\) would fail to be a function type (i.e., a type of the shape ‚Äò\(\sigma \rightarrow \tau\)‚Äô). Moreover, we cannot assign to \(x\) a function type \(\sigma \rightarrow \tau\), because then then \(\sigma\) would be equal to \(\sigma \rightarrow \tau\), which is impossible.&lt;/p&gt;&lt;p&gt;As a leading example, consider the types that are assigned to the combinators \(\bI\), \(\bK\), and \(\bS\):&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Combinator&lt;/cell&gt;&lt;cell&gt;Type[5]&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;\(a \rightarrow a\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bK\)&lt;/cell&gt;&lt;cell&gt;\(a \rightarrow(b \rightarrow a)\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\( (a \rightarrow(b \rightarrow c)) \rightarrow ((a \rightarrow b) \rightarrow(a \rightarrow c))\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;(See Hindley (1997) Table of principal types for a more extensive listing.) If we read ‚Äò\(\rightarrow\)‚Äô as implication and type variables as propositional variables, then we recognize three familiar tautologies in the right-hand column of the table. The language used is meager: there are only propositional variables and implication; there are no other connectives.&lt;/p&gt;&lt;p&gt;The table suggests an interesting correspondence between the typed \(\lambda\)-calculus and formal logic. Could it really be that the types assigned to formulas, when understood as logical formulas, are valid? Yes, though ‚Äòvalidity‚Äô needs to understood not as classical validity:&lt;/p&gt;&lt;p&gt;Theorem If \(\tau\) is the type of some \(\lambda\)-term, then \(\tau\) is intuitionistically valid.&lt;/p&gt;&lt;p&gt;The converse of this theorem holds as well:&lt;/p&gt;&lt;p&gt;Theorem If \(\phi\) is an intuitionistically valid logical formula whose only connective is implication \((\rightarrow)\), then \(\phi\) is the type of some \(\lambda\)-term.&lt;/p&gt;&lt;p&gt;The correspondence can be seen when one identifies intuitionistic validity with derivability in a certain natural deduction formalism. For a proof of these two theorems, see (Hindley, 1997, chapter 6).&lt;/p&gt;&lt;p&gt;The correspondence expressed by the previous two theorems between intuitionistic validity and typability is known as the Curry-Howard-de Bruijn correspondence, after three logicians who noticed it independently. The correspondence, as stated, is between only propositional intuitionistic logic, restricted to the fragment containing only the implication connective \(\rightarrow\). One can extend the correspondence to other connectives and to quantifiers, too, but the most crisp correspondence is at the level of the implication-only fragment. For details, see (Howard, 1980).&lt;/p&gt;&lt;head rend="h3"&gt;9.2 Computing&lt;/head&gt;&lt;p&gt;One can represent natural numbers in a simple way, as follows:&lt;/p&gt;&lt;p&gt;Definition (ordered tuples, natural numbers) The ordered tuple \(\langle a_0,\ldots a_n\rangle\) of \(\lambda\)-terms is defined as \(\lambda x[x a_0\ldots a_n]\). One then defines the \(\lambda\)-term \(\ulcorner n\urcorner\) corresponding to the natural number \(n\) as: \(\ulcorner 0\urcorner = \mathbf{I}\) and, for every \(k\), \(\ulcorner k + 1\urcorner = \langle \mathbf{F}, \ulcorner k\urcorner\rangle\).&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term corresponding to the number 1, on this representation, is:&lt;/p&gt;\[\begin{align} \ulcorner 1 \urcorner &amp;amp;\equiv \langle\bF,\ulcorner 0\urcorner\rangle \\ &amp;amp;\equiv \langle\bF,\bI\rangle \\ &amp;amp;\equiv \lambda x[x\mathbf{FI}] \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term corresponding to the number 2, on this representation, is:&lt;/p&gt;\[\begin{align} \ulcorner 2 \urcorner &amp;amp;\equiv \langle\bF,\ulcorner 1\urcorner\rangle \\ &amp;amp;\equiv \lambda x[x\mathbf{F}\lambda x[x\mathbf{FI}]] \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;Similarly, \(\ulcorner 3\urcorner\) is \(\lambda x[x\mathbf{F}\lambda x[x\mathbf{F}\lambda x[x\mathbf{FI}]]]\).&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Various representations of natural numbers are available; this representation is but one.[6]&lt;/p&gt;&lt;p&gt;Using the ingredients provided by the \(\lambda\)-calculus, one can represent all recursive functions. This shows that the model is exactly as expressive as other models of computing, such as Turing machines and register machines. For a more detailed discussion of the relation between these different models of computing, see the section comparing the Turing and Church approaches in the entry on the Church-Turing Thesis.&lt;/p&gt;&lt;p&gt;Theorem For every recursive function \(f\) of arity \(n\), there exists a \(\lambda\)-term \(f^*\) such that&lt;/p&gt;&lt;p&gt;for all natural numbers \(a_1,\ldots a_n\): \(f(a_1,\ldots a_n) = y\) iff \(\boldsymbol{\lambda} \vdash f^*\langle \bar{a}_1,\ldots,\bar{a}_n\rangle = \bar{y}\)&lt;/p&gt;&lt;p&gt;For a proof, see the appendix.&lt;/p&gt;&lt;p&gt;Since the class of recursive functions is an adequate representation of the class of all computable (number-theoretic) functions, thanks to the work above we find that all computable (number-theoretic) functions can be faithfully represented in the \(\lambda\)-calculus.&lt;/p&gt;&lt;head rend="h3"&gt;9.3 Relations&lt;/head&gt;&lt;p&gt;The motivation for the \(\lambda\)-calculus given at the beginning of the entry was based on reading \(\lambda\)-expressions as descriptions of functions. Thus, we have understood ‚Äò\(\lambda x[M]\)‚Äô to be a (or the) function that, given \(x\), gives \(M\) (which generally, though not necessarily, involves x). But it is not necessary to read \(\lambda\)-terms as functions. One could understand \(\lambda\)-terms as denoting relations, and read an abstraction term ‚Äò\(\lambda x[M]\)‚Äô as the unary relation (or property) \(R\) that holds of an argument \(x\) just in case \(M\) does (see Carnap 1947, p. 3). On the relational reading, we can understand an application term \(MN\) as a form of predication. One can make sense of these terms using the principle of \(\beta\)-conversion:&lt;/p&gt;\[ (\lambda x[M])a = M[x := A], \]&lt;p&gt;which says that the abstraction relation \(\lambda x[M]\), predicated of A, is the relation obtained by plugging in A for all free occurrences of \(x\) inside \(M\).&lt;/p&gt;&lt;p&gt;As a concrete example of this kind of approach to \(\lambda\)-calculus, consider an extension of first-order logic where one can form new atomic formulas using \(\lambda\)-terms, in the following way:&lt;/p&gt;&lt;p&gt;Syntax: For any formula \(\phi\) and any finite sequence \(x_1 , \ldots ,x_n\) of variables, the expression ‚Äò\(\lambda x_1 \ldots x_n [\phi]\)‚Äô is a predicate symbol of arity n. Extend the notion of free and bound variables (using the functions \(\mathbf{FV}\) and \(\mathbf{BV})\) in such a way that&lt;/p&gt;\[ \mathbf{FV}(\lambda x_1 \ldots x_n [\phi]) = \mathbf{FV}(\phi) - \{ x_1 , \ldots x_n \} \]&lt;p&gt;and&lt;/p&gt;\[ \mathbf{BV}(\lambda x_1 \ldots x_n [\phi]) = \mathbf{BV}(\phi) \cup \{ x_1 , \ldots x_n \} \]&lt;p&gt;Deduction Assume as axioms the universal closures of all equivalences&lt;/p&gt;\[ \lambda x_1 \ldots x_n [\phi](t_1 ,\ldots t_n) \leftrightarrow \phi[x_1 ,\ldots x_n := t_1,\ldots t_n] \]&lt;p&gt;where \(\phi[x_1 ,\ldots x_n := t_1,\ldots t_n]\) denotes the simultaneous substitution of the terms \(t_k\) for the variables \(x_k\) \((1 \le k \le n)\).&lt;/p&gt;&lt;p&gt;Semantics For a first-order structure \(A\) and an assignment \(s\) of elements of \(A\) to variables, define&lt;/p&gt;\[\begin{align} A \vDash &amp;amp;\lambda x_1 \ldots x_n [\phi](t_1 ,\ldots t_n) [s] \text{ iff } \\ &amp;amp;A \vDash \phi[x_1 ,\ldots x_n := t_1,\ldots t_n] [s] \end{align}\]&lt;p&gt;According to this approach, one can use a \(\lambda\) to treat essentially any formula, even complex ones, as if they were atomic. We see the principle of \(\beta\)-reduction in the deductive and semantic parts. That this approach adheres to the relational reading of \(\lambda\) terms can be seen clearly in the semantics: according to the standard Tarski-style semantics for first-order logic, the interpretation of a formula (possibly with free variables) denotes a set of tuples of elements of the structure, as we vary the variable assignment that assigns elements of the structure to the variables.&lt;/p&gt;&lt;p&gt;One can ‚Äòinternalize‚Äô this functional approach. This is done in the case of various property theories, formal theories for reasoning about properties as metaphysical objects (Bealer 1982, Zalta 1983, Menzel 1986, 1993, and Turner 1987). This kind of theory is employed in certain metaphysical investigations where properties are metaphysical entities to be investigated. In these theories, metaphysical relations are (or are among) the objects of interest; just as we add term-building symbols + and \(\times\) in formal theories of arithmetic to build numbers, \(\lambda\) is used in property theory to build relations. This approach contrasts with the approach above. There, \(\lambda\) was added to the grammar of first-order logic by making it a recipe for building atomic formulas; it was a new formula-building operator, like \(\vee\) or \(\rightarrow\) or the other connectives. In the case of property theories, the \(\lambda\) plays a role more like + and \(\times\) in formal theories of arithmetic: it is used to construct relations (which, in this setting, are to be understood as a kind of metaphysical object). Unlike + and \(\times\), though, the \(\lambda\) binds variables.&lt;/p&gt;&lt;p&gt;To give an illustration of how \(\lambda\) is used in this setting, let us inspect the grammar of a typical application (McMichael and Zalta, 1980). One typically has a predication operator (or, more precisely, a family of predication operators) \(p_k (k \ge 0)\). In a language where we have terms \(\mary\) and \(\john\) and a binary relation loves, we can formally express:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;John loves Mary: \(\loves(\john ,\mary)\)&lt;/item&gt;&lt;item&gt;The property that John loves Mary: \(\lambda[\loves(\john ,\mary)]\) (note that the \(\lambda\) is binding no variables; we might call this ‚Äòvacuous binding‚Äô. Such properties can be understood as propositions.)&lt;/item&gt;&lt;item&gt;The property of an object \(x\) that John loves it: \(\lambda x [\loves(\john,x)]\).&lt;/item&gt;&lt;item&gt;The property that Mary is loved by something: \(\lambda[\exists x(\loves(x,\mary))]\) (another instance of vacuous binding, viz., proposition)&lt;/item&gt;&lt;item&gt;The predication of the property of \(x\) that John loves \(x\) to Mary: \(p_1 (\lambda x[\loves(\john,x)],\mary)\).&lt;/item&gt;&lt;item&gt;The (0-ary) predication of the property that John loves Mary: \(p_0 (\lambda x[\loves(\john,\mary)])\).&lt;/item&gt;&lt;item&gt;The property of objects \(x\) and \(y\) that \(x\) loves \(y\): \(\lambda xy[\loves(x,y)]\).&lt;/item&gt;&lt;item&gt;The property of an objects \(x\) that \(x\) loves itself: \(\lambda x[\loves(x,x)]\).&lt;/item&gt;&lt;item&gt;The predication of the property of objects \(x\) and \(y\) that \(x\) loves \(y\) to John and Mary (in that order): \(p_2 (\lambda xy[\loves(x,y)],\john,\mary)\).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We reason with these \(\lambda\)-terms using a \(\beta\)-conversion principle such as:&lt;/p&gt;\[\begin{align} p_n (\lambda x_1,&amp;amp;\ldots x_n [A], t_1 , \ldots ,t_n) \leftrightarrow \\ &amp;amp;A[x_1 ,\ldots x_n := t_1,\ldots, t_n] \end{align}\]&lt;p&gt;Formally, the predication operator p\(_k\) is a \((k+1)\)-ary predicate symbol. The first argument is intended to be a \(\lambda\)-term of \(k\) arguments, and the rest of the arguments are intended to be the arguments of the body of the \(\lambda\)-term. The \(\beta\)-principle above says that the predication of an \(n\)-ary \(\lambda\)-term \(L\) to \(n\) terms holds precisely when the body of \(L\) holds of those terms.&lt;/p&gt;&lt;p&gt;It turns out that in these theories, we may or may not be able to be fully committed to the principle of \(\beta\)-conversion. Indeed, in some property theories, the full principle of \(\beta\)-conversion leads to paradox, because one can replay a Russell-style argument when the full principle of \(\beta\)-conversion is in place. In such settings, one restricts the formation of \(\lambda\)-formulas by requiring that the body of a \(\lambda\)-term not contain further \(\lambda\)-terms or quantifiers. For further discussion, see (Orilia, 2000).&lt;/p&gt;&lt;p&gt;One of the reasons why property theories formulated in the \(\lambda\)-calculus are of a particular philosophical importance is the hyperintensional nature of the calculus (see section 1.2). A property concept may be called ‚Äòhyperintensional‚Äô if and only if it does not identify necessarily coextensional properties, i.e., properties that are instanciated by exactly the same objects at every possible world. The properties and relations described by the theories of Bealer, Zalta, Menzel, and Turner have exactly this characteristic. In other words, the theories are hyperintensional property theories. Recent years have seen a significant rise of interest in hyperintensional concepts of properties in metaphysics (Nolan 2014), and correspondingly property theories formulated in the \(\lambda\)-calculus will likely experience a rise of interest as well.&lt;/p&gt;&lt;p&gt;In the context of the foundations of mathematics, Zalta and Oppenheimer (2011) argue for the conceptual priority of the relational interpretation of \(\lambda\)-terms over the functional one.&lt;/p&gt;&lt;head rend="h2"&gt;Bibliography&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Baader, Franz and Tobias Nipkow, 1999, Term Rewriting and All That, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, 1985, The Lambda Calculus: Its Syntax and Semantics (Studies in Logic and the Foundations of Mathematics 103), 2nd edition, Amsterdam: North-Holland.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, 1993, ‚ÄúLambda calculi with types‚Äù, in S. Abramsky, D. Gabbay, T. Maibaum, and H. Barendregt (eds.), Handbook of Logic in Computer Science (Volume 2), New York: Oxford University Press, pp. 117‚Äì309.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, Wil Dekkers, and Richard Statman., 2013, Lambda Calculus With Types, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Bealer, George, 1982, Quality and Concept, Oxford: Clarendon Press.&lt;/item&gt;&lt;item&gt;van Benthem, Johan, 1998, A Manual of Intensional Logic, Stanford: CSLI Publications.&lt;/item&gt;&lt;item&gt;Carnap, Rudolf, 1947, Meaning and Necessity, Chicago: University of Chicago Press.&lt;/item&gt;&lt;item&gt;Church, Alonzo, 1932, ‚ÄúA set of postulates for the foundation of logic‚Äù, Annals of Mathematics (2nd Series), 33(2): 346‚Äì366.&lt;/item&gt;&lt;item&gt;Cutland, Nigel J., 1980, Computability, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Doets, Kees and Jan van Eijk, 2004, The Haskell Road to Logic, Maths and Programming, London: College Publications.&lt;/item&gt;&lt;item&gt;Enderton, Herbert B., 2001, A Mathematical Introduction to Logic, 2nd edition, San Diego: Harcourt/Academic Press.&lt;/item&gt;&lt;item&gt;Frege, Gottlob, 1893, Grundgesetze der Arithmetik, Jena: Verlag Hermann Pohle, Band I; partial translation as The Basic Laws of Arithmetic, M. Furth (trans.), Berkeley: University of California Press, 1964.&lt;/item&gt;&lt;item&gt;Kleene, Stephen C., 1981, ‚ÄúOrigins of recursive function theory‚Äù, Annals of the History of Computing, 3(1): 52‚Äì67.&lt;/item&gt;&lt;item&gt;Heim, Irene and Angelika Kratzer, 1998, Semantics in Generative Grammar, Malden, MA: Blackwell.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger, 1997, Basic Simple Type Theory (Cambridge Tracts in Theoretical Computer Science 42), New York: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger and G. Longo, 1980, ‚ÄúLambda-calculus Models and Extensionality.‚Äù Zeitschrift f√ºr mathematische Logik und Grundlagen der Mathematik, 26: 289‚Äì310.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger and Jonathan P. Seldin, 2008, Lambda-Calculus and Combinators: An Introduction, 2nd edition, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Howard, William A., 1980, ‚ÄúThe formula-as-types notion of construction‚Äù, in J. Hindley and J. Seldin (eds.), To H. B. Curry: Essays on Combinatory Logic, Lambda-Calculus, and Formalism, London: Academic Press, pp. 479‚Äì490.&lt;/item&gt;&lt;item&gt;Hyland, J. Martin E., 2017, ‚ÄúClassical Lambda Calculus in Modern Dress‚Äù, Mathematical Structures in Computer Science, 27(5): 762‚Äì781.&lt;/item&gt;&lt;item&gt;Koymans, C.P.J., 1982, ‚ÄúModels of the Lambda Calculus‚Äù, Information and Control, 52: 306‚Äì332.&lt;/item&gt;&lt;item&gt;Manzano, Maria, 2005, Extensions of First-order Logic (Cambridge Tracts in Theoretical Computer Science 19), Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;McCarthy, John, 1960, ‚ÄúRecursive functions of symbolic expressions and their computation by machine (Part I)‚Äù, Communications of the ACM, 3(4): 184‚Äì195.&lt;/item&gt;&lt;item&gt;McMichael, Alan and Edward N. Zalta, 1980, ‚ÄúAn alternative theory of nonexistent objects‚Äù, Journal of Philosophical Logic, 9: 297‚Äì313.&lt;/item&gt;&lt;item&gt;Menzel, Christopher, 1986, ‚ÄúA complete, type-free second order logic of properties, relations, and propositions‚Äù, Technical Report #CSLI-86-40, Stanford: CSLI Publications.&lt;/item&gt;&lt;item&gt;Menzel, Christopher, 1993, ‚ÄúThe proper treatment of predication in fine-grained intensional logic‚Äù, Philosophical Perspectives 7: 61‚Äì86.&lt;/item&gt;&lt;item&gt;Meyer, Albert R., 1982, ‚ÄúWhat is a model of the lambda calculus?‚Äù, In Information and Control, 52(1): 87‚Äì122.&lt;/item&gt;&lt;item&gt;Nederpelt, Rob, with Herman Geuvers and Roel de Vriejer (eds.), 1994, Selected Papers on Automath (Studies in Logic and the Foundations of Mathematics 133), Amsterdam: North-Holland.&lt;/item&gt;&lt;item&gt;Nolan, Daniel, 2014, ‚ÄúHyperintensional metaphysics‚Äù, Philosophical Studies, 171(1); 149‚Äì160.&lt;/item&gt;&lt;item&gt;Orilia, Francesco, 2000, ‚ÄúProperty theory and the revision theory of definitions‚Äù, Journal of Symbolic Logic, 65(1): 212‚Äì246.&lt;/item&gt;&lt;item&gt;Partee, Barbara H., with Alice ter Meulen and Robert E. Wall, 1990, Mathematical Methods in Linguistics, Berlin: Springer.&lt;/item&gt;&lt;item&gt;Plotkin, G.D., 1972, A Set-Theoretical Definition of Application, School of Artificial Intelligence, Memo MIP-R-95, University of Edinburgh.&lt;/item&gt;&lt;item&gt;Revesz, George E., 1988, Lambda-Calculus, Combinators, and Functional Programming, Cambridge: Cambridge University Press; reprinted 2008.&lt;/item&gt;&lt;item&gt;Rosser, J. Barkley, 1984, ‚ÄúHighlights of the History of the Lambda-Calculus‚Äù, Annals of the History of Computing, 6(4): 337‚Äì349.&lt;/item&gt;&lt;item&gt;Salibra, Antonio, 2003, ‚ÄúLambda calculus: models and theories‚Äù, in Proceedings of the Third AMAST Workshop on Algebraic Methods in Language Processing (AMiLP-2003), No. 21, University of Twente, pp. 39‚Äì54.&lt;/item&gt;&lt;item&gt;Sch√∂nfinkel, Moses, 1924, ‚ÄúOn the building blocks of mathematical logic‚Äù, in J. van Heijenoort (ed.), From Frege to G√∂del: A Source Book in Mathematical Logic, Cambridge, MA: Harvard University Press, 1967, pp. 355‚Äì366.&lt;/item&gt;&lt;item&gt;Scott, Dana, 1974, ‚ÄúThe LAMBDA language‚Äù, Journal of Symbolic Logic, 39: 425‚Äì427.&lt;/item&gt;&lt;item&gt;‚Äì‚Äì‚Äì, 1980, ‚ÄúLambda Calculus: Some Models, Some Philosophy‚Äù, in J. Barwise, H.J. Keisler, and K. Kunen (eds.), The Kleene Symposium, Amsterdam: North-Holland, pp. 223‚Äì265.&lt;/item&gt;&lt;item&gt;Troelstra, Anne and Helmut Schwichtenberg, 2000, Basic Proof Theory (Cambridge Tracts in Theoretical Computer Science 43), 2nd edition, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Turing, Alan M., 1937, ‚ÄúComputability and \(\lambda\)-definability‚Äù, Journal of Symbolic Logic, 2(4): 153‚Äì163.&lt;/item&gt;&lt;item&gt;Turner, Richard, 1987, ‚ÄúA theory of properties‚Äù, Journal of Symbolic Logic, 52(2): 455‚Äì472.&lt;/item&gt;&lt;item&gt;Zalta, Edward N., 1983, Abstract Objects: An Introduction to Axiomatic Metaphysics, Dordrecht: D. Reidel.&lt;/item&gt;&lt;item&gt;Zalta, Edward N. and Paul Oppenheimer, 2011, ‚ÄúRelations versus functions at the foundations of logic: type-theoretic considerations‚Äù, Journal of Logic and Computation 21: 351‚Äì374.&lt;/item&gt;&lt;item&gt;Zerpa, L., 2021, ‚ÄúThe Teaching and Learning of the Untyped Lambda Calculus Through Web-Based e-Learning Tools‚Äù, in K. Arai Intelligent Computing (Lecture Notes in Networks and Systems: Volume 285), Cham: Springer, pp. 419‚Äì436.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Academic Tools&lt;/head&gt;&lt;quote&gt;&lt;td&gt;How to cite this entry.&lt;/td&gt;&lt;td&gt;Preview the PDF version of this entry at the Friends of the SEP Society.&lt;/td&gt;&lt;td&gt;Look up topics and thinkers related to this entry at the Internet Philosophy Ontology Project (InPhO).&lt;/td&gt;&lt;td&gt;Enhanced bibliography for this entry at PhilPapers, with links to its database.&lt;/td&gt;&lt;/quote&gt;&lt;head rend="h2"&gt;Other Internet Resources&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;The Lambda Calculator, a tool for working with \(\lambda\)-terms with an eye toward their use in formal semantics of natural language.&lt;/item&gt;&lt;item&gt;Lambda calculus reduction workbench, for visualizing reduction strategies.&lt;/item&gt;&lt;item&gt;‚Äú\(\lambda\)-Calculus: Then and Now,‚Äù useful handout on the milestones in, contributors to, and bibliography on the \(\lambda\)-calculus, presented at the several Turing Centennial conferences. There also exists a video recording of the lecture given on the occasion of Princeton University‚Äôs celebration of the Turing Centennial in 2012.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Acknowledgments&lt;/head&gt;&lt;p&gt;The first author wishes to acknowledge the contributions of Henk Barendregt, Elizabeth Coppock, Reinhard Kahle, Martin S√∏rensen, and Ed Zalta in helping to craft this entry. He also thanks Nic McPhee for introducing him to the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;The second author would like to acknowledge the useful comments and suggestions of Fabrizio Cariani, Cameron Moy, Peter Percival, and Ed Zalta.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://plato.stanford.edu/entries/lambda-calculus/"/><published>2025-09-24T15:00:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361394</id><title>How to Be a Leader When the Vibes Are Off</title><updated>2025-09-24T20:36:04.734485+00:00</updated><content>&lt;doc fingerprint="7a01915d36b1034c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How to Be a Leader When the Vibes Are Off&lt;/head&gt;
    &lt;head rend="h2"&gt;...and the vibes are definitely off&lt;/head&gt;
    &lt;p&gt;It feels different in tech right now. We‚Äôre coming off a long era where optimism carried the industry. Something has curdled. AI hype, return-to-office mandates, and continued layoffs have shifted the mood. Managers are quicker to fire, existential dread has replaced the confidence that a tight job market for developers provided for decades. The vibes are for sure off.&lt;/p&gt;
    &lt;head rend="h3"&gt;What‚Äôs Changed?&lt;/head&gt;
    &lt;p&gt;(What follows are generalizations. If your company is escaping some or all of these, I applaud you. I‚Äôm sure there are exceptions.)&lt;/p&gt;
    &lt;p&gt;AI has injected some destabilization. ‚ÄúI don‚Äôt need junior devs when I can just pay $20/month for Cursor‚Äù has an effect on everyone even if this turns out to be silly down the road. I see lots of people worried that the aim of all of this is to ultimately have a robot do their entire job. Whether or not this is possible doesn‚Äôt mean people aren‚Äôt going to try. And it‚Äôs the trying that raises people‚Äôs anxiety. On top of that, we‚Äôve also got ‚ÄúAI Workslop‚Äù to contend with as well, which is making work harder for the diligent among us.&lt;/p&gt;
    &lt;p&gt;Return to Office feels like trust has been broken. Teams that continued to work well (or in some cases, better) after everyone in the industry went remote are now being told to come back to desks in offices. I‚Äôve even heard tales of this happening despite there not being enough office space for everyone, which seems very silly. Also, for the first time in my nearly 30-year career, I‚Äôve even heard of people being told they need to be ‚Äúat their desks at 9am‚Äù and ‚Äúexpected to stay until 5pm at a minimum.‚Äù Even before COVID-19 and the mass move to remote work, most companies were flexible on start and stop times. I almost never heard of set hours for software developers until recently. Rules like that scream ‚Äúwe don‚Äôt trust you unless we can see you,‚Äù even if that‚Äôs not really the reason for the mandates. (IMO there are benefits to working in the same location as your colleagues but ham-fisted, poorly thought out mandates are not the way to achieve them.)&lt;/p&gt;
    &lt;p&gt;Layoffs changed the market. For probably 20 years, job security wasn‚Äôt really a concern in the industry. Layoffs happened here and there and companies folded, but the demand was always strong and most people capable of writing code or managing people who write code could lose their job, spend the severance on a nice vacation, and return with the confidence that they‚Äôd be able to land a new gig in a couple of weeks, likely at higher pay. With the acknowledgement that this was a privilege not enjoyed by most of the working world, it is no longer true. The size and scope of layoffs over the last couple of years have injected more anxiety into the tech workforce.&lt;/p&gt;
    &lt;p&gt;C-Suite Energy has changed. Across the board, execs seem more efficiency-focused, financialized, and less mission-driven. The days of ‚Äútake care of the employees and the employees will take care of the business‚Äù feel like they‚Äôre in the rear-view mirror, and a new ‚Äúdo your job, or else!‚Äù mentality has taken its place.&lt;/p&gt;
    &lt;p&gt;You can‚Äôt change the macro forces that are driving these trends, but you can control how you show up for your team.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wearing the ‚ÄòCompany Hat‚Äô vs. Chaotic-Good Leadership&lt;/head&gt;
    &lt;p&gt;My standard advice to anyone with a management role and anyone at the Staff+ level of individual contributor is that ‚Äúwearing the company hat‚Äù should be the default. You‚Äôre not always going to agree with the decisions that come down from the top. Even when you don‚Äôt agree with decisions the company leadership is making, part of your job is representing and facilitating those decisions with full alignment. When acting ‚Äúin public‚Äù (all-hands, department meetings, the #general channel), this is mandatory, as contradicting the bosses in a broad forum can kill the credibility you have the leadership across the wider team. It‚Äôs also a good way to get yourself fired.&lt;/p&gt;
    &lt;head rend="h3"&gt;Let them know you‚Äôre still on their side&lt;/head&gt;
    &lt;p&gt;But you know what also kills trust? Telling your team it‚Äôs sunny out when everyone can plainly see that it‚Äôs raining. Your team is made up of smart adults who can, at the very least, count the number of employees and the number of desks and calculate that ‚Äúeveryone in the office on Wednesday‚Äù isn‚Äôt going to work out well if the people outnumber the chairs. Telling them something else is going to make you look like an idiot toady in their eyes.&lt;/p&gt;
    &lt;p&gt;The right thing to do in this situation is to acknowledge that you see the situation the same way they do, but do it privately, within your immediate team only or in 1-1s. ‚ÄúYeah, this new policy sucks, I get it. It‚Äôs going to affect me in negative ways too.‚Äù It‚Äôs really important that you validate the emotions that all of these aspects are bringing up in people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don‚Äôt pretend you can fix it&lt;/head&gt;
    &lt;p&gt;You can promise to advocate for saner policies when the opportunity arises if your sphere of influence makes that possible, but don‚Äôt promise to make the problem go away if you can‚Äôt. Broken promises and poor do/say ratio performance will also kill your team‚Äôs faith in you, especially when it‚Äôs about things they really care about. And again, this is not a time for grandstanding. In public, you have to support the policies, but when you‚Äôre in private with your manager and your peers, that‚Äôs the time you can safely push for change.&lt;/p&gt;
    &lt;head rend="h3"&gt;Find small workarounds to make things livable&lt;/head&gt;
    &lt;p&gt;If you can provide some flexibility on seemingly inflexible policies, do it. If your management role includes enforcing the company‚Äôs rules, you can use some discretion about how strictly you want to enforce them. Personally, I would never want to ‚Äúrat out‚Äù a good performer who can‚Äôt get to their desk by 9am sharp because they have to drop off their kids or punish someone who bugged out early once to catch their favourite performer in concert one town over. Small acts demonstrating that you trust your team, even if the C-Suite doesn‚Äôt seem to trust the broader team the way they used to, can go a long way toward maintaining good morale within your group.&lt;/p&gt;
    &lt;p&gt;When things feel shaky in the broader org, people will look more to their direct leader for a sense of stability. The best thing you can do for them is provide it. Quiet honesty builds credibility and fosters loyalty.&lt;/p&gt;
    &lt;head rend="h2"&gt;This too shall pass&lt;/head&gt;
    &lt;p&gt;The industry is going through a period where a lot is changing all at once. We‚Äôve had a few of them before. Things will eventually settle down into a new normal. I‚Äôm not great at predictions, so I‚Äôll refrain from detailing what I think things will look like, but I don‚Äôt think it‚Äôll be entirely unfamiliar to those who were here before this latest inflection point. This is especially true if leaders who care and treat their staff like adults can stay grounded and stay true to their principles, even when that means performing small, quiet acts of rebellion.&lt;/p&gt;
    &lt;p&gt;You can‚Äôt fix the macro trends, but you can try to keep your corner of the tech world a place where people are glad to work.&lt;/p&gt;
    &lt;p&gt;"Off Kilter" by anujd89 is licensed under CC BY 2.0 .&lt;/p&gt;
    &lt;p&gt;Like this? Please feel free to share it on your favourite social media or link site! Share it with friends!&lt;/p&gt;
    &lt;p&gt;Hit subscribe to get new posts delivered to your inbox automatically.&lt;lb/&gt;Feedback? Get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chaoticgood.management/how-to-be-a-leader-when-the-vibes-are-off/"/><published>2025-09-24T15:03:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361578</id><title>Engineering a fixed-width bit-packed integer vector in Rust</title><updated>2025-09-24T20:36:04.414992+00:00</updated><content>&lt;doc fingerprint="7ce9541545936309"&gt;
  &lt;main&gt;
    &lt;p&gt;If you‚Äôve ever worked with massive datasets, you know that memory usage can quickly become a bottleneck. While developing succinct data structures, I found myself needing to store large arrays of integers‚Äîvalues with no monotonicity or other exploitable patterns, that I knew came from a universe much smaller than their type‚Äôs theoretical capacity.&lt;/p&gt;
    &lt;p&gt;In this post, we will explore the engineering challenges involved in implementing an efficient vector-like data structure in Rust that stores integers in a compressed, bit-packed format. We will focus on achieving O(1) random access performance while minimizing memory usage. We will try to mimic the ergonomics of Rust‚Äôs standard &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; as closely as possible, including support for mutable access and zero-copy slicing.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All the code can be found on github: compressed-intvec&lt;/item&gt;
      &lt;item&gt;This is also published as a crate on crates.io: compressed-intvec&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Memory Waste in Standard Vectors&lt;/head&gt;
    &lt;p&gt;In Rust, the contract of a &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; (where &lt;code&gt;T&lt;/code&gt; is a primitive integer type like &lt;code&gt;u64&lt;/code&gt; or &lt;code&gt;i32&lt;/code&gt;) is simple: O(1) random access in exchange for a memory layout that is tied to the static size of &lt;code&gt;T&lt;/code&gt;. This is a good trade-off, until it isn‚Äôt. When the dynamic range of the stored values is significantly smaller than the type‚Äôs capacity, this memory layout leads to substantial waste.&lt;/p&gt;
    &lt;p&gt;Consider storing the value &lt;code&gt;5&lt;/code&gt; within a &lt;code&gt;Vec&amp;lt;u64&amp;gt;&lt;/code&gt;. Its 8-byte in-memory representation is:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000101&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Only 3 bits are necessary to represent the value, leaving 61 bits as zero-padding. The same principle applies, albeit less dramatically, when storing &lt;code&gt;5&lt;/code&gt; in a &lt;code&gt;u32&lt;/code&gt; or &lt;code&gt;u16&lt;/code&gt;. At scale, this overhead becomes prohibitive. A vector of one billion &lt;code&gt;u64&lt;/code&gt; elements consumes &lt;code&gt;10^9 * std::mem::size_of::&amp;lt;u64&amp;gt;()&lt;/code&gt;, or approximately 8 GB of memory, even if every element could fit within a single byte.&lt;/p&gt;
    &lt;p&gt;The canonical solution is bit packing, which aligns data end-to-end in a contiguous bitvector. However, this optimization has historically come at the cost of random access performance. The O(1) access guarantee of &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; is predicated on simple pointer arithmetic: &lt;code&gt;address = base_address + index * std::mem::size_of::&amp;lt;T&amp;gt;()&lt;/code&gt;. Tightly packing the bits invalidates this direct address calculation, seemingly forcing a trade-off between memory footprint and access latency.&lt;/p&gt;
    &lt;p&gt;This raises the central question that with this post we aim to answer: is it possible to design a data structure that decouples its memory layout from the static size of &lt;code&gt;T&lt;/code&gt;, adapting instead to the data‚Äôs true dynamic range, without sacrificing the O(1) random access that makes &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; so effective?&lt;/p&gt;
    &lt;head rend="h1"&gt;Packing and Accessing Bits&lt;/head&gt;
    &lt;p&gt;If the dynamic range of our data is known, we can define a fixed &lt;code&gt;bit_width&lt;/code&gt; for every integer. For instance, if the maximum value in a dataset is &lt;code&gt;1000&lt;/code&gt;, we know every number can be represented in 10 bits, since &lt;code&gt;2^10 = 1024&lt;/code&gt;. Instead of allocating 64 bits per element, we can store them back-to-back in a contiguous bitvector, forming the core of what from now on we‚Äôll refer as &lt;code&gt;FixedVec&lt;/code&gt;. We can immagine such data strucutre as a logical array of &lt;code&gt;N&lt;/code&gt; integers, each &lt;code&gt;bit_width&lt;/code&gt; bits wide, stored in a backing buffer of &lt;code&gt;u64&lt;/code&gt; words.&lt;/p&gt;
    &lt;code&gt;struct FixedVec {
    limbs: Vec&amp;lt;u64&amp;gt;, // Backing storage
    bit_width: usize, // Number of bits per element
    len: usize, // Number of elements
    mask: u64, // Precomputed mask for extraction
}&lt;/code&gt;
    &lt;p&gt;Where the role of the &lt;code&gt;mask&lt;/code&gt; field is to isolate the relevant bits during extraction. For a &lt;code&gt;bit_width&lt;/code&gt; of 10, the mask would be &lt;code&gt;0b1111111111&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This immediately solves the space problem, but how can we find the &lt;code&gt;i&lt;/code&gt;-th element in O(1) time if it doesn‚Äôt align to a byte boundary? The answer lies in simple arithmetic. The starting bit position of any element is a direct function of its index. Given a backing store of &lt;code&gt;u64&lt;/code&gt; words, we can locate any value by calculating its absolute bit position and then mapping that to a specific word and an offset within that word.&lt;/p&gt;
    &lt;code&gt;let bit_pos = index * bit_width; // Absolute bit position of the value
let word_index = bit_pos / 64; // Which u64 word to read
let bit_offset = bit_pos % 64; // Where the value starts in that word&lt;/code&gt;
    &lt;p&gt;With these values, the implementation of &lt;code&gt;get_unchecked&lt;/code&gt; becomes straightforward. It‚Äôs a two-step process: fetch the correct word from our backing &lt;code&gt;Vec&amp;lt;u64&amp;gt;&lt;/code&gt;, then use bitwise operations to isolate the specific bits we need.&lt;/p&gt;
    &lt;code&gt;// A simplified look at the core get_unchecked logic
unsafe fn get_unchecked(&amp;amp;self, index: usize) -&amp;gt; u64 {
    let bit_width = self.bit_width();
    let bit_pos = index * bit_width;
    let word_index = bit_pos / 64;
    let bit_offset = bit_pos % 64;

    // 1. Fetch the word from the backing store
    let word = *self.limbs.get_unchecked(word_index);

    // 2. Shift and mask to extract the value
    (word &amp;gt;&amp;gt; bit_offset) &amp;amp; self.mask
}&lt;/code&gt;
    &lt;p&gt;Let‚Äôs trace an access with &lt;code&gt;bit_width = 10&lt;/code&gt; for the element at &lt;code&gt;index = 7&lt;/code&gt;. The starting bit position is &lt;code&gt;7 * 10 = 70&lt;/code&gt;. This maps to &lt;code&gt;word_index = 1&lt;/code&gt; and &lt;code&gt;bit_offset = 6&lt;/code&gt;. Our 10-bit integer begins at the 6th bit of the second &lt;code&gt;u64&lt;/code&gt; word in our storage.&lt;/p&gt;
    &lt;p&gt;The right-shift &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; operation moves the bits of the entire &lt;code&gt;u64&lt;/code&gt; word to the right by &lt;code&gt;bit_offset&lt;/code&gt; positions. This aligns the start of our desired 10-bit value with the least significant bit (LSB) of the word. The final step is to isolate our value. A pre-calculated &lt;code&gt;mask&lt;/code&gt; (e.g., &lt;code&gt;0b1111111111&lt;/code&gt; for 10 bits) is applied with a bitwise AND &lt;code&gt;&amp;amp;&lt;/code&gt;. This zeroes out any high-order bits from the word, leaving just our target integer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Crossing Word Boundaries&lt;/head&gt;
    &lt;p&gt;The single-word access logic is fast, but it only works as long as &lt;code&gt;bit_offset + bit_width &amp;lt;= 64&lt;/code&gt;. This assumption breaks down as soon as an integer‚Äôs bit representation needs to cross the boundary from one &lt;code&gt;u64&lt;/code&gt; word into the next. This is guaranteed to happen for any &lt;code&gt;bit_width&lt;/code&gt; that is not a power of two. For example, with a 10-bit width, the element at &lt;code&gt;index = 6&lt;/code&gt; starts at bit position 60. Its 10 bits will occupy bits 60-63 of the first word and bits 0-5 of the second. The simple right-shift-and-mask trick fails here.&lt;/p&gt;
    &lt;p&gt;To correctly decode the value, we must read two consecutive &lt;code&gt;u64&lt;/code&gt; words and combine their bits. This splits our &lt;code&gt;get_unchecked&lt;/code&gt; implementation into two paths. The first is the fast path we‚Äôve already seen. The second is a new path for spanning values.&lt;/p&gt;
    &lt;p&gt;To get the lower bits of the value, we read the first word and shift right, just as before. This leaves the upper bits of the word as garbage.&lt;/p&gt;
    &lt;code&gt;let low_part = *limbs.get_unchecked(word_index) &amp;gt;&amp;gt; bit_offset;&lt;/code&gt;
    &lt;p&gt;To get the upper bits of the value, we read the next word. The bits we need are at the beginning of this word, so we shift them left to align them correctly.&lt;/p&gt;
    &lt;code&gt;let high_part = *limbs.get_unchecked(word_index + 1) &amp;lt;&amp;lt; (64 - bit_offset);&lt;/code&gt;
    &lt;p&gt;Finally, we combine the two parts with a bitwise OR and apply the mask to discard any remaining high-order bits from the &lt;code&gt;high_part&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;(low_part | high_part) &amp;amp; self.mask&lt;/code&gt;
    &lt;p&gt;The line &lt;code&gt;limbs.get_unchecked(word_index + 1)&lt;/code&gt; introduces a safety concern: if we are reading the last element of the vector, &lt;code&gt;word_index + 1&lt;/code&gt; could point past the end of our buffer, leading to undefined behavior. To prevent this, our builder must always allocate one extra padding word at the end of the storage.&lt;/p&gt;
    &lt;p&gt;Integrating these two paths gives us our final &lt;code&gt;get_unchecked&lt;/code&gt; implementation:&lt;/p&gt;
    &lt;code&gt;pub unsafe fn get_unchecked(&amp;amp;self, index: usize) -&amp;gt; u64 {
    let bit_width = self.bit_width;
    let bit_pos = index * bit_width;
    let word_index = bit_pos / 64;
    let bit_offset = bit_pos % 64;

    let limbs = self.bits.as_ref();

    if bit_offset + bit_width &amp;lt;= 64 {
        // Fast path: value is fully within one word
        (*limbs.get_unchecked(word_index) &amp;gt;&amp;gt; bit_offset) &amp;amp; self.mask
    } else {
        // Slow path: value spans two words.
        let low_part = *limbs.get_unchecked(word_index) &amp;gt;&amp;gt; bit_offset;
        let high_part = *limbs.get_unchecked(word_index + 1) &amp;lt;&amp;lt; (64 - bit_offset);
        (low_part | high_part) &amp;amp; self.mask
    }
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Faster Reads: Unaligned Access&lt;/head&gt;
    &lt;p&gt;Our &lt;code&gt;get_unchecked&lt;/code&gt; implementation is correct, but the slow path for spanning values requires two separate, aligned memory reads. The instruction sequence for this method involves at least two load instructions, multiple shifts, and a bitwise OR. These instructions have data dependencies: the shifts cannot execute until the loads complete, and the OR cannot execute until both shifts are done. This dependency chain can limit the CPU‚Äôs instruction-level parallelism and create pipeline stalls if the memory accesses miss the L1 cache.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs have a look at the machine code generated by this method. We can create a minimal binary with an &lt;code&gt;#[inline(never)]&lt;/code&gt; function that calls &lt;code&gt;get_unchecked&lt;/code&gt; on a known spanning index, then use &lt;code&gt;cargo asm&lt;/code&gt; to inspect the disassembly.&lt;/p&gt;
    &lt;code&gt;; asm_test::aligned::get_aligned (spanning path)
.LBB14_2:
        ; let low = *limbs.get_unchecked(word_index) &amp;gt;&amp;gt; bit_offset;
        mov rsi, qword ptr [r8 + 8*rdx]      ; &amp;lt;&amp;lt;&amp;lt; LOAD #1 (low_part)

        ; let high = *limbs.get_unchecked(word_index + 1) &amp;lt;&amp;lt; (64 - bit_offset);
        mov rdx, qword ptr [r8 + 8*rdx + 8]  ; &amp;lt;&amp;lt;&amp;lt; LOAD #2 (high_part)

        shr rsi, cl                          ; shift right of low_part
        shl rdx, cl                          ; shift left of high_part
        or rdx, rsi                          ; combine results&lt;/code&gt;
    &lt;p&gt;The instruction &lt;code&gt;mov rsi, qword ptr [r8 + 8*rdx]&lt;/code&gt; is our first memory access. It loads a 64-bit value (&lt;code&gt;qword&lt;/code&gt;) into the &lt;code&gt;rsi&lt;/code&gt; register. The address is calculated using base-plus-index addressing: &lt;code&gt;r8&lt;/code&gt; holds the base address of our &lt;code&gt;limbs&lt;/code&gt; buffer, and &lt;code&gt;rdx&lt;/code&gt; holds the &lt;code&gt;word_index&lt;/code&gt;. This corresponds directly to &lt;code&gt;limbs[word_index]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Immediately following is &lt;code&gt;mov rdx, qword ptr [r8 + 8*rdx + 8]&lt;/code&gt;. This is our second, distinct memory access. It loads the next 64-bit word from memory by adding an 8-byte offset to the previous address. This corresponds to &lt;code&gt;limbs[word_index + 1]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Only after both of these &lt;code&gt;mov&lt;/code&gt; instructions complete can the CPU proceed. The &lt;code&gt;shr rsi, cl&lt;/code&gt; instruction (shift right &lt;code&gt;rsi&lt;/code&gt; by the count in &lt;code&gt;cl&lt;/code&gt;) cannot execute until the first &lt;code&gt;mov&lt;/code&gt; has placed a value in &lt;code&gt;rsi&lt;/code&gt;. Similarly, &lt;code&gt;shl rdx, cl&lt;/code&gt; depends on the second &lt;code&gt;mov&lt;/code&gt;. Finally, &lt;code&gt;or rdx, rsi&lt;/code&gt; depends on both shifts.&lt;/p&gt;
    &lt;p&gt;This sequence of operations‚Äîloading two adjacent 64-bit words, shifting each, and combining them is a software implementation of what is, conceptually, a 128-bit barrel shifter. We are selecting a 64-bit window from a virtual 128-bit integer formed by concatenating the two words from memory.&lt;/p&gt;
    &lt;p&gt;Can we do better then this? Potentially, yes. We can replace this multi-instruction sequence with something more direct by delegating the complexity to the hardware and performing a single unaligned memory read. Modern x86-64 CPUs handle this directly: when an unaligned load instruction is issued, the CPU‚Äôs memory controller fetches the necessary cache lines and the load/store unit reassembles the bytes into the target register. This entire process is a single, optimized micro-operation.&lt;/p&gt;
    &lt;p&gt;We can try to implement a more aggressive access method. The strategy is to calculate the exact byte address where our data begins and perform a single, unaligned read of a full &lt;code&gt;W&lt;/code&gt; word from that position.&lt;/p&gt;
    &lt;p&gt;The implementation first translates the absolute bit position into a byte address and a residual bit offset within that byte.&lt;/p&gt;
    &lt;code&gt;let bit_pos = index * self.bit_width;
let byte_pos = bit_pos / 8;
let bit_rem = bit_pos % 8;&lt;/code&gt;
    &lt;p&gt;With the byte-level address, we get a raw &lt;code&gt;*const u8&lt;/code&gt; pointer to our storage and perform the unaligned read. The read_unaligned intrinsic in Rust compiles down to a single machine instruction that the hardware can execute efficiently.&lt;/p&gt;
    &lt;code&gt;let limbs_ptr = self.as_limbs().as_ptr() as *const u8;
// This read may cross hardware word boundaries, but the CPU handles it.
let word: W = (limbs_ptr.add(byte_pos) as *const W).read_unaligned();&lt;/code&gt;
    &lt;p&gt;On a Little-Endian system, the loaded &lt;code&gt;word&lt;/code&gt; now contains our target integer, but it‚Äôs shifted by &lt;code&gt;bit_rem&lt;/code&gt; positions. A simple right shift aligns our value to the LSB, and applying the mask isolates it.&lt;/p&gt;
    &lt;code&gt;let extracted_word = word &amp;gt;&amp;gt; bit_rem;
let final_value = extracted_word &amp;amp; self.mask;&lt;/code&gt;
    &lt;p&gt;This operation is safe only because, as said before, we are supposing that our builder guarantees a padding word at the end of the storage buffer. Combining all these steps, we get our final implementation:&lt;/p&gt;
    &lt;code&gt;pub unsafe fn get_unaligned_unchecked(&amp;amp;self, index: usize) -&amp;gt; u64 {
    let bit_pos = index * self.bit_width;
    let byte_pos = bit_pos / 8;
    let bit_rem = bit_pos % 8;

    let limbs_ptr = self.as_limbs().as_ptr() as *const u8;
    // SAFETY: The builder guarantees an extra padding word at the end.
    let word = (limbs_ptr.add(byte_pos) as *const W).read_unaligned();
    let extracted_word = word &amp;gt;&amp;gt; bit_rem;
    extracted_word &amp;amp; self.mask
}&lt;/code&gt;
    &lt;p&gt;Let‚Äôs have a look at the generated machine code for this new method when accessing an index that spans words:&lt;/p&gt;
    &lt;code&gt;; asm_test::unaligned::get_unaligned
.LBB15_1:
        ; let bit_pos = index * self.bit_width;
        imul rcx, rax
        ; let byte_pos = bit_pos / 8;
        mov rax, rcx
        shr rax, 3
        ; self.bits.as_ref()
        mov rdx, qword ptr [rdi + 8]
        ; unsafe { crate::intrinsics::copy_nonoverlapping(src, dst, count) }
        mov rax, qword ptr [rdx + rax]       ; &amp;lt;&amp;lt;&amp;lt; SINGLE UNALIGNED LOAD
        ; self &amp;gt;&amp;gt; other
        and cl, 7
        shr rax, cl
        ; fn bitand(self, rhs: $t) -&amp;gt; $t { self &amp;amp; rhs }
        and rax, qword ptr [rdi + 32]&lt;/code&gt;
    &lt;p&gt;The initial &lt;code&gt;imul&lt;/code&gt; and &lt;code&gt;shr rax, 3&lt;/code&gt; (a fast division by 8) correspond to the calculation of &lt;code&gt;byte_pos&lt;/code&gt;. The instruction &lt;code&gt;mov rdx, qword ptr [rdi + 8]&lt;/code&gt; loads the base address of our &lt;code&gt;limbs&lt;/code&gt; buffer into the &lt;code&gt;rdx&lt;/code&gt; register.&lt;/p&gt;
    &lt;p&gt;The instruction &lt;code&gt;mov rax, qword ptr [rdx + rax]&lt;/code&gt; is our single unaligned load. The address &lt;code&gt;[rdx + rax]&lt;/code&gt; is the sum of the buffer‚Äôs base address and our calculated &lt;code&gt;byte_pos&lt;/code&gt;. This &lt;code&gt;mov&lt;/code&gt; instruction reads 8 bytes (a &lt;code&gt;qword&lt;/code&gt;) directly from this potentially unaligned memory location into the &lt;code&gt;rax&lt;/code&gt; register. We can see that as we hoped, the read_unaligned intrinsic has been compiled down to a single hardware instruction.&lt;/p&gt;
    &lt;p&gt;The next instructions handle the extraction. The &lt;code&gt;and cl, 7&lt;/code&gt; and &lt;code&gt;shr rax, cl&lt;/code&gt; sequence corresponds to our &lt;code&gt;&amp;gt;&amp;gt; bit_rem&lt;/code&gt;. &lt;code&gt;cl&lt;/code&gt; holds the lower bits of the original &lt;code&gt;bit_pos&lt;/code&gt; (our &lt;code&gt;bit_rem&lt;/code&gt;), and the shift aligns our desired value to the LSB of the &lt;code&gt;rax&lt;/code&gt; register. Finally, &lt;code&gt;and rax, qword ptr [rdi + 32]&lt;/code&gt; applies the pre-calculated mask, which is stored at an offset from the &lt;code&gt;self&lt;/code&gt; pointer in &lt;code&gt;rdi&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Random Access Performance&lt;/head&gt;
    &lt;p&gt;We can now benchmark the latency of 1 million random access operations on a vector containing 10 million elements. For each &lt;code&gt;bit_width&lt;/code&gt;, we generate data with a uniform random distribution in the range &lt;code&gt;[0, 2^bit_width)&lt;/code&gt;. The code for the benchmark is available here: &lt;code&gt;bench-intvec&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Our baseline is the smallest standard &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; capable of holding the data (&lt;code&gt;Vec&amp;lt;u8&amp;gt;&lt;/code&gt; for &lt;code&gt;bit_width &amp;lt;= 8&lt;/code&gt;, etc.). We also include results from &lt;code&gt;sux&lt;/code&gt;, &lt;code&gt;succinct&lt;/code&gt;, and &lt;code&gt;simple-sds-sbwt&lt;/code&gt; for context. I am not aware of any other Rust crates that implement fixed-width bit-packed integer vectors, so if you know of any, please let me know!&lt;/p&gt;
    &lt;p&gt;We can see that for &lt;code&gt;bit_width&lt;/code&gt; values below 32, the &lt;code&gt;get_unaligned_unchecked&lt;/code&gt; of our &lt;code&gt;FixedVec&lt;/code&gt; is almost always faster than the corresponding &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; baseline. This is a result of improved cache locality. A 64-byte L1 cache line can hold 64 elements from a &lt;code&gt;Vec&amp;lt;u8&amp;gt;&lt;/code&gt;. With a &lt;code&gt;bit_width&lt;/code&gt; of 4, the same cache line holds &lt;code&gt;(64 * 8) / 4 = 128&lt;/code&gt; elements from our &lt;code&gt;FixedVec&lt;/code&gt;. This increased density improves the cache hit rate for random access patterns, and the latency reduction from avoiding DRAM access outweighs the instruction cost of the bitwise extraction. For values of &lt;code&gt;bit_width&lt;/code&gt; above 32, the performance of &lt;code&gt;FixedVec&lt;/code&gt; are very slightly worse than the &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; baseline, as the cache locality advantage diminishes. However, the memory savings remain.&lt;/p&gt;
    &lt;p&gt;The performance delta between &lt;code&gt;get_unaligned_unchecked&lt;/code&gt; and &lt;code&gt;get_unchecked&lt;/code&gt; confirms the unaligned access strategy discussed before: a single &lt;code&gt;read_unaligned&lt;/code&gt; instruction is more efficient than the two dependent aligned reads required by the logic for spanning words.&lt;/p&gt;
    &lt;p&gt;We can see that the implementation of &lt;code&gt;sux&lt;/code&gt; is almost on par with ours. The other two crates, &lt;code&gt;succinct&lt;/code&gt; and &lt;code&gt;simple-sds-sbwt&lt;/code&gt;, are significantly slower (note that the Y-axis is logarithmic). Tho, it‚Äôs worth noting that neither of these last two crates provides unchecked or unaligned access methods, so their implementations are inherently more conservative.&lt;/p&gt;
    &lt;head rend="h2"&gt;Iterating Over Values&lt;/head&gt;
    &lt;p&gt;The most common operation on any &lt;code&gt;Vec&lt;/code&gt;-like structure is, after all, a simple &lt;code&gt;for&lt;/code&gt; loop. The simplest way to implement &lt;code&gt;iter()&lt;/code&gt; would be to just wrap &lt;code&gt;get()&lt;/code&gt; in a loop:&lt;/p&gt;
    &lt;code&gt;// A naive, inefficient iterator
for i in 0..vec.len() {
    let value = vec.get(i);
    // ... do something with value
}&lt;/code&gt;
    &lt;p&gt;This works, but it‚Äôs terribly inefficient. Every single call to &lt;code&gt;get(i)&lt;/code&gt; independently recalculates the &lt;code&gt;word_index&lt;/code&gt; and &lt;code&gt;bit_offset&lt;/code&gt; from scratch. We‚Äôre throwing away valuable state, our current position in the bitstream, on every iteration, forcing the CPU to perform redundant multiplications and divisions.&lt;/p&gt;
    &lt;p&gt;We can think then about a stateful iterator. It should operate directly on the bitvector, maintaining its own position. Instead of thinking in terms of logical indices, it should think in terms of a ‚Äúbit window‚Äù, a local &lt;code&gt;u64&lt;/code&gt; register that holds the current chunk of bits being processed.&lt;/p&gt;
    &lt;p&gt;The idea is simple: the iterator loads one &lt;code&gt;u64&lt;/code&gt; word from the backing store into its window. It then satisfies &lt;code&gt;next()&lt;/code&gt; calls by decoding values directly from this in-register window. Only when the window runs out of bits does it need to go back to memory for the next &lt;code&gt;u64&lt;/code&gt; word. This amortizes the cost of memory access over many &lt;code&gt;next()&lt;/code&gt; calls.&lt;/p&gt;
    &lt;p&gt;For forward iteration, the state is minimal:&lt;/p&gt;
    &lt;code&gt;struct FixedVecIter&amp;lt;'a, ...&amp;gt; {
    // ...
    front_window: u64,
    front_bits_in_window: usize,
    front_word_index: usize,
    // ...
}&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;next()&lt;/code&gt; method first checks if the current &lt;code&gt;front_window&lt;/code&gt; has enough bits to satisfy the request. If &lt;code&gt;self.front_bits_in_window &amp;gt;= bit_width&lt;/code&gt;, it‚Äôs the fast path: a simple shift and mask on a register, which is incredibly fast.&lt;/p&gt;
    &lt;code&gt;// Inside next(), fast path:
if self.front_bits_in_window &amp;gt;= bit_width {
    let value = self.front_window &amp;amp; self.mask;
    self.front_window &amp;gt;&amp;gt;= bit_width;
    self.front_bits_in_window -= bit_width;
    return Some(value);
}&lt;/code&gt;
    &lt;p&gt;If the window is running low on bits, we hit the slower path. The next value spans the boundary between our current window and the next &lt;code&gt;u64&lt;/code&gt; word in memory. We must read the next word, combine its bits with the remaining bits in our current window, and then extract the value. This is the same logic as the spanning-word &lt;code&gt;get()&lt;/code&gt;, but it‚Äôs performed incrementally.&lt;/p&gt;
    &lt;head rend="h3"&gt;Double-Ended Iteration&lt;/head&gt;
    &lt;p&gt;But I want my iterator to be bidirectional! Well, then we need to ensure it implements &lt;code&gt;DoubleEndedIterator&lt;/code&gt; and supports &lt;code&gt;next_back()&lt;/code&gt;. This throws a wrench in our simple stateful model. A single window and cursor can only move in one direction.&lt;/p&gt;
    &lt;p&gt;The solution is to maintain two independent sets of state: one for the front and one for the back. The &lt;code&gt;FixedVecIter&lt;/code&gt; needs to track two windows, two bit counters, and two word indices.&lt;/p&gt;
    &lt;code&gt;struct FixedVecIter&amp;lt;'a, ...&amp;gt; {
    // ...
    front_index: usize,
    back_index: usize,

    // State for forward iteration
    front_window: u64,
    front_bits_in_window: usize,
    front_word_index: usize,

    // State for backward iteration
    back_window: u64,
    back_bits_in_window: usize,
    back_word_index: usize,
    // ...
}&lt;/code&gt;
    &lt;p&gt;Initializing the front is easy: we load &lt;code&gt;limbs[0]&lt;/code&gt; into &lt;code&gt;front_window&lt;/code&gt;. The back is more complex. We must calculate the exact word index and the number of valid bits in the last word that contains data. This requires a bit of arithmetic to handle cases where the data doesn‚Äôt perfectly fill the final word.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;next()&lt;/code&gt; method consumes from the &lt;code&gt;front_window&lt;/code&gt;, advancing the front state. The &lt;code&gt;next_back()&lt;/code&gt; method consumes from the &lt;code&gt;back_window&lt;/code&gt;, advancing the back state. The iterator is exhausted when &lt;code&gt;front_index&lt;/code&gt; meets &lt;code&gt;back_index&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The full implementation can be found in the iter module of the library&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Writing bits&lt;/head&gt;
    &lt;p&gt;We have solved the read problem, but we may also want to modify values in place. A method like &lt;code&gt;set(index, value)&lt;/code&gt; seems simple, but it opens up the same can of worms as &lt;code&gt;get&lt;/code&gt;, just in reverse. We can‚Äôt just write the new value; we have to do so without clobbering the adjacent, unrelated data packed into the same &lt;code&gt;u64&lt;/code&gt; word.&lt;/p&gt;
    &lt;p&gt;Just like with reading, the logic splits into two paths. The ‚Äúfast path‚Äù handles values that are fully contained within a single &lt;code&gt;u64&lt;/code&gt;. Here, we can‚Äôt just overwrite the word. We first need to clear out the bits for the element we‚Äôre replacing and then merge in the new value.&lt;/p&gt;
    &lt;head rend="h2"&gt;In-Word Write&lt;/head&gt;
    &lt;p&gt;Our goal here is to update a &lt;code&gt;bit_width&lt;/code&gt;-sized slice of a &lt;code&gt;u64&lt;/code&gt; word while leaving the other bits untouched. This operation must be a read-modify-write sequence to avoid corrupting adjacent elements. The most efficient way to implement this is to load the entire word into a register, perform all bitwise modifications locally, and then write the final result back to memory in a single store operation.&lt;/p&gt;
    &lt;p&gt;First, we load the word from our backing &lt;code&gt;limbs&lt;/code&gt; slice.&lt;/p&gt;
    &lt;code&gt;let mut word = *limbs.get_unchecked(word_index);&lt;/code&gt;
    &lt;p&gt;Next, we need to create a ‚Äúhole‚Äù in our local copy where the new value will go. We do this by creating a mask that has ones only in the bit positions we want to modify, and then inverting it to create a clearing mask.&lt;/p&gt;
    &lt;code&gt;// For a value at bit_offset, the mask must also be shifted.
let clear_mask = !(self.mask &amp;lt;&amp;lt; bit_offset);
// Applying this mask zeroes out the target bits in our register copy.
word &amp;amp;= clear_mask;&lt;/code&gt;
    &lt;p&gt;With the target bits zeroed, we can merge our new value. The value is first shifted left by &lt;code&gt;bit_offset&lt;/code&gt; to align it correctly within the 64-bit word. Then, a bitwise OR merges it into the ‚Äúhole‚Äù we just created.&lt;/p&gt;
    &lt;code&gt;// Shift the new value into position and merge it.
word |= value_w &amp;lt;&amp;lt; bit_offset;&lt;/code&gt;
    &lt;p&gt;Finally, with the modifications complete, we write the updated word from the register back to memory in a single operation.&lt;/p&gt;
    &lt;code&gt;*limbs.get_unchecked_mut(word_index) = word;&lt;/code&gt;
    &lt;p&gt;This entire sequence of one read, two bitwise operations in-register, one write is the canonical and most efficient way to perform a sub-word update.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spanning Write&lt;/head&gt;
    &lt;p&gt;Now for the hard part: writing a value that crosses a word boundary. This operation must modify two separate &lt;code&gt;u64&lt;/code&gt; words in our backing store. It‚Äôs the inverse of the spanning read. We need to split our &lt;code&gt;value_w&lt;/code&gt; into a low part and a high part and write each to the correct word, minimizing memory accesses.&lt;/p&gt;
    &lt;p&gt;To operate on two distinct memory locations, &lt;code&gt;limbs[word_index]&lt;/code&gt; and &lt;code&gt;limbs[word_index + 1]&lt;/code&gt;, we first need mutable access to both. In a safe, hot path like this, we can use &lt;code&gt;split_at_mut_unchecked&lt;/code&gt; to bypass Rust‚Äôs borrow checker bounds checks, as we have already guaranteed through our logic that both indices are valid.&lt;/p&gt;
    &lt;code&gt;// SAFETY: We know word_index and word_index + 1 are valid.
let (left, right) = limbs.split_at_mut_unchecked(word_index + 1);&lt;/code&gt;
    &lt;p&gt;Our strategy is to read both words into registers, perform all bitwise logic locally, and then write both modified words back to memory. This minimizes the time we hold mutable references and can improve performance.&lt;/p&gt;
    &lt;p&gt;First, we handle the &lt;code&gt;low_word&lt;/code&gt;. We need to replace its high bits (from &lt;code&gt;bit_offset&lt;/code&gt; onwards) with the low bits of our new value. The most direct way is to create a mask for the bits we want to keep. The expression &lt;code&gt;(1 &amp;lt;&amp;lt; bit_offset) - 1&lt;/code&gt; is a bit-twiddling trick to generate a mask with &lt;code&gt;bit_offset&lt;/code&gt; ones at the least significant end.&lt;/p&gt;
    &lt;code&gt;let mut low_word_val = *left.get_unchecked(word_index);

// Create a mask to preserve the low `bit_offset` bits of the word.
let low_mask = (1u64 &amp;lt;&amp;lt; bit_offset).wrapping_sub(1);
low_word_val &amp;amp;= low_mask;&lt;/code&gt;
    &lt;p&gt;With the target bits zeroed out, we merge in the low part of our new value. A left shift aligns it correctly, and the high bits of &lt;code&gt;value_w&lt;/code&gt; are naturally shifted out of the register.&lt;/p&gt;
    &lt;code&gt;// Merge in the low part of our new value.
low_word_val |= value_w &amp;lt;&amp;lt; bit_offset;
*left.get_unchecked_mut(word_index) = low_word_val;&lt;/code&gt;
    &lt;p&gt;Next, we handle the &lt;code&gt;high_word&lt;/code&gt; in a symmetrical fashion. We need to write the remaining high bits of &lt;code&gt;value_w&lt;/code&gt; into the low-order bits of this second word. First, we calculate how many bits of our value actually belong in the first word:&lt;/p&gt;
    &lt;code&gt;let remaining_bits_in_first_word = 64 - bit_offset;&lt;/code&gt;
    &lt;p&gt;Now, we read the second word and create a mask to clear the low-order bits where our data will be written. With the operation &lt;code&gt;self.mask &amp;gt;&amp;gt; remaining_bits_in_first_word&lt;/code&gt; we can determine how many bits of our value spill into the second word, creating a mask for them. Inverting this gives us a mask to preserve the existing high-order bits of the &lt;code&gt;high_word&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;let mut high_word_val = *right.get_unchecked(0);

// Clear the low bits of the second word that will be overwritten.
high_word_val &amp;amp;= !(self.mask &amp;gt;&amp;gt; remaining_bits_in_first_word);&lt;/code&gt;
    &lt;p&gt;Finally, we isolate the high part of &lt;code&gt;value_w&lt;/code&gt; by right-shifting it by the number of bits we already wrote, and merge it into the cleared space.&lt;/p&gt;
    &lt;code&gt;// Merge in the high part of our new value.
high_word_val |= value_w &amp;gt;&amp;gt; remaining_bits_in_first_word;
*right.get_unchecked_mut(0) = high_word_val;&lt;/code&gt;
    &lt;head rend="h2"&gt;Random Write Performance&lt;/head&gt;
    &lt;p&gt;As with reads, we can benchmark the latency of 1 million random write operations on a vector containing 10 million elements. The code for the benchmark is available here: [&lt;code&gt;bench-intvec-writes&lt;/code&gt;].&lt;/p&gt;
    &lt;p&gt;Here, the &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; baseline is the clear winner across almost all bit-widths. This isn‚Äôt surprising. A &lt;code&gt;set&lt;/code&gt; operation in a &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; compiles down to a single &lt;code&gt;MOV&lt;/code&gt; instruction with a simple addressing mode (&lt;code&gt;[base + index * element_size]&lt;/code&gt;). It‚Äôs about as fast as the hardware allows.&lt;/p&gt;
    &lt;p&gt;As for the reads, the performance of our &lt;code&gt;FixedVec&lt;/code&gt; is almost identical to that of &lt;code&gt;sux&lt;/code&gt;. The other two crates, &lt;code&gt;succinct&lt;/code&gt; and &lt;code&gt;simple-sds-sbwt&lt;/code&gt;, are again slower. It‚Äôs worth noting that also for the writes, neither of these last two crates provides unchecked methods.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;For the 64-bit width case, I honestly have no idea what is going on with&lt;/p&gt;&lt;code&gt;sux&lt;/code&gt;being so much faster than everything else, even then&lt;code&gt;Vec&amp;lt;u64&amp;gt;&lt;/code&gt;! Mybe some weird compiler optimization? If you have any insight, please let me know.&lt;/quote&gt;
    &lt;head rend="h1"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;With the access patterns defined, we need to think about the overall architecture of this data structure. A solution hardcoded to &lt;code&gt;u64&lt;/code&gt; would lack the flexibility to adapt to different use cases. We need a structure that is generic over the its principal components: the logical type, the physical storage type, the bit-level ordering, and ownership. We can define a struct that is generic over these four parameters:&lt;/p&gt;
    &lt;code&gt;pub struct FixedVec&amp;lt;T: Storable&amp;lt;W&amp;gt;, W: Word, E: Endianness, B: AsRef&amp;lt;[W]&amp;gt; = Vec&amp;lt;W&amp;gt;&amp;gt; {
    bits: B,
    bit_width: usize,
    mask: W,
    len: usize,
}&lt;/code&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;T&lt;/code&gt; is the logical element type, the type as seen by the user of the API (e.g., &lt;code&gt;i16&lt;/code&gt;, &lt;code&gt;u32&lt;/code&gt;). By abstracting &lt;code&gt;T&lt;/code&gt;, we divide the user-facing type from the internal storage representation.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;W&lt;/code&gt; is the physical storage word, which must implement our &lt;code&gt;Word&lt;/code&gt; trait. It defines the primitive unsigned integer (&lt;code&gt;u32&lt;/code&gt;, &lt;code&gt;u64&lt;/code&gt;, &lt;code&gt;usize&lt;/code&gt;) of the backing buffer and sets the granularity for all bitwise operations.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;E&lt;/code&gt; requires the &lt;code&gt;dsi-bitstream::Endianness&lt;/code&gt; trait, allowing us to specify either Little-Endian (&lt;code&gt;LE&lt;/code&gt;) or Big-Endian (&lt;code&gt;BE&lt;/code&gt;) byte order.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;B&lt;/code&gt;, which must implement &lt;code&gt;AsRef&amp;lt;[W]&amp;gt;&lt;/code&gt;, represents the backing storage. This abstraction over ownership allows &lt;code&gt;FixedVec&lt;/code&gt; to be either an owned container where &lt;code&gt;B = Vec&amp;lt;W&amp;gt;&lt;/code&gt;, or a zero-copy, borrowed view where &lt;code&gt;B = &amp;amp;[W]&lt;/code&gt;. This makes it possible to, for example, construct a &lt;code&gt;FixedVec&lt;/code&gt; directly over a memory-mapped slice without any heap allocation.&lt;/p&gt;
    &lt;p&gt;In this way, the compiler monomorphizes the struct and its methods for each concrete instantiation (e.g., &lt;code&gt;FixedVec&amp;lt;i16, u64, LE, Vec&amp;lt;u64&amp;gt;&amp;gt;&lt;/code&gt;), resulting in specialized code with no runtime overhead from the generic abstractions.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;Word&lt;/code&gt; Trait: The Physical Storage Layer&lt;/head&gt;
    &lt;p&gt;The first step is to abstract the physical storage layer. The &lt;code&gt;W&lt;/code&gt; parameter must be a primitive unsigned integer type that supports bitwise operations. We can define a &lt;code&gt;Word&lt;/code&gt; trait that captures these requirements:&lt;/p&gt;
    &lt;code&gt;pub trait Word:
    UnsignedInt + Bounded + ToPrimitive + dsi_bitstream::traits::Word
    + NumCast + Copy + Send + Sync + Debug + IntoAtomic + 'static
{
    const BITS: usize = std::mem::size_of::&amp;lt;Self&amp;gt;() * 8;
}&lt;/code&gt;
    &lt;p&gt;The numeric traits (&lt;code&gt;UnsignedInt&lt;/code&gt;, &lt;code&gt;Bounded&lt;/code&gt;, &lt;code&gt;NumCast&lt;/code&gt;, &lt;code&gt;ToPrimitive&lt;/code&gt;) are necessary for the arithmetic of offset and mask calculations. The &lt;code&gt;dsi_bitstream::traits::Word&lt;/code&gt; bound allows us to integrate with its &lt;code&gt;BitReader&lt;/code&gt; and &lt;code&gt;BitWriter&lt;/code&gt; implementations, offloading the bitstream logic. &lt;code&gt;Send&lt;/code&gt; and &lt;code&gt;Sync&lt;/code&gt; are non-negotiable requirements for any data structure that might be used in a concurrent context. The &lt;code&gt;IntoAtomic&lt;/code&gt; bound is particularly forward-looking: it establishes a compile-time link between a storage word like &lt;code&gt;u64&lt;/code&gt; and its atomic counterpart, &lt;code&gt;AtomicU64&lt;/code&gt;. We will use it later to build a thread safe, atomic version of &lt;code&gt;FixedVec&lt;/code&gt;. Finally, the &lt;code&gt;const BITS&lt;/code&gt; associated constant lets us write architecture-agnostic code that correctly adapts to &lt;code&gt;u32&lt;/code&gt;, &lt;code&gt;u64&lt;/code&gt;, or &lt;code&gt;usize&lt;/code&gt; words without &lt;code&gt;cfg&lt;/code&gt; flags.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;Storable&lt;/code&gt; Trait: The Logical Type Layer&lt;/head&gt;
    &lt;p&gt;With the physical storage layer defined, we need a formal contract to connect it to the user‚Äôs logical type &lt;code&gt;T&lt;/code&gt;. We can do this by creating the &lt;code&gt;Storable&lt;/code&gt; trait, which defines a bidirectional, lossless conversion.&lt;/p&gt;
    &lt;code&gt;pub trait Storable&amp;lt;W: Word&amp;gt;: Sized + Copy {
    fn into_word(self) -&amp;gt; W;
    fn from_word(word: W) -&amp;gt; Self;
}&lt;/code&gt;
    &lt;p&gt;For unsigned types, the implementation is a direct cast. For signed types, however, we must map the &lt;code&gt;iN&lt;/code&gt; domain to the &lt;code&gt;uN&lt;/code&gt; domain required for bit-packing. A simple two‚Äôs complement bitcast is unsuitable, as &lt;code&gt;i64(-1)&lt;/code&gt; would become &lt;code&gt;u64::MAX&lt;/code&gt;, a value requiring the maximum number of bits. We need a better mapping that preserves small absolute values.&lt;/p&gt;
    &lt;p&gt;We can use ZigZag encoding, which maps integers with small absolute values to small unsigned integers. This is implemented via the &lt;code&gt;ToNat&lt;/code&gt; and &lt;code&gt;ToInt&lt;/code&gt; traits from &lt;code&gt;dsi-bitstream&lt;/code&gt;. The core encoding logic in &lt;code&gt;to_nat&lt;/code&gt; is:&lt;/p&gt;
    &lt;code&gt;// From dsi_bitstream::traits::ToNat
fn to_nat(self) -&amp;gt; Self::UnsignedInt {
    (self &amp;lt;&amp;lt; 1).to_unsigned() ^ (self &amp;gt;&amp;gt; (Self::BITS - 1)).to_unsigned()
}&lt;/code&gt;
    &lt;p&gt;This operation works as follows: &lt;code&gt;(self &amp;lt;&amp;lt; 1)&lt;/code&gt; creates a space at the LSB. The term &lt;code&gt;(self &amp;gt;&amp;gt; (Self::BITS - 1))&lt;/code&gt; is an arithmetic right shift, which generates a sign mask‚Äîall zeros for non-negative numbers, all ones for negative numbers. The final XOR uses this mask to interleave positive and negative integers: 0 becomes 0, -1 becomes 1, 1 becomes 2, -2 becomes 3, and so on.&lt;/p&gt;
    &lt;p&gt;The decoding reverses this transformation:&lt;/p&gt;
    &lt;code&gt;// From dsi_bitstream::traits::ToInt
fn to_int(self) -&amp;gt; Self::SignedInt {
    (self &amp;gt;&amp;gt; 1).to_signed() ^ (-(self &amp;amp; 1).to_signed())
}&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;(self &amp;gt;&amp;gt; 1)&lt;/code&gt; shifts the value back. The term &lt;code&gt;-(self &amp;amp; 1)&lt;/code&gt; creates a mask from the LSB (the original sign bit). In two‚Äôs complement, this becomes &lt;code&gt;0&lt;/code&gt; for even numbers (originally positive) and &lt;code&gt;-1&lt;/code&gt; (all ones) for odd numbers (originally negative). The final XOR with this mask correctly restores the original two‚Äôs complement representation.&lt;/p&gt;
    &lt;p&gt;We might ask ourselves: why not just a direct bitcast for signed types, perhaps via a &lt;code&gt;uN -&amp;gt; iN&lt;/code&gt; chain. Well, while a direct transmute between same-sized integer types is a no-op, the approach fails when the logical &lt;code&gt;bit_width&lt;/code&gt; is smaller than the physical type size. &lt;code&gt;FixedVec&lt;/code&gt;‚Äôs core logic extracts a &lt;code&gt;bit_width&lt;/code&gt;-sized unsigned integer from its storage. For example, when reading a 4-bit representation of &lt;code&gt;-1&lt;/code&gt; (binary &lt;code&gt;1111&lt;/code&gt;), the &lt;code&gt;from_word&lt;/code&gt; function receives the value &lt;code&gt;15u64&lt;/code&gt;. At this point, the context that those four bits represented a negative number is lost. A cast chain like &lt;code&gt;15u64 as u8 as i8&lt;/code&gt; would simply yield &lt;code&gt;15i8&lt;/code&gt;, not &lt;code&gt;-1i8&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To correctly reconstruct the signed value, one would need to manually perform sign extension based on the known &lt;code&gt;bit_width&lt;/code&gt;. This involves checking the most significant bit of the extracted value and, if set, filling the higher-order bits of the word with ones. This logic requires a conditional branch, which can introduce pipeline stalls and degrade performance in tight loops. ZigZag decoding, by contrast, is a purely arithmetic, branch-free transformation. Its reconstruction logic is a simple sequence of bitwise shifts and XORs, making it a faster and more consistent choice for the hot path of data access.&lt;/p&gt;
    &lt;p&gt;With this logic within the trait system, the main &lt;code&gt;FixedVec&lt;/code&gt; implementation remains clean and agnostic to the signedness of the data it stores.&lt;/p&gt;
    &lt;head rend="h2"&gt;Builder Pattern&lt;/head&gt;
    &lt;p&gt;Once the structure logic is in place, we have to design an ergonomic way to construct it. A simple &lt;code&gt;new()&lt;/code&gt; function isn‚Äôt sufficient because the vector‚Äôs memory layout depends on parameters that must be determined before allocation, most critically the &lt;code&gt;bit_width&lt;/code&gt;. This is a classic scenario for a builder pattern.&lt;/p&gt;
    &lt;p&gt;The central problem is that the optimal &lt;code&gt;bit_width&lt;/code&gt; often depends on the data itself. We need a mechanism to specify the strategy for determining this width. We can create the &lt;code&gt;BitWidth&lt;/code&gt; enum:&lt;/p&gt;
    &lt;code&gt;pub enum BitWidth {
    Minimal,
    PowerOfTwo,
    Explicit(usize),
}&lt;/code&gt;
    &lt;p&gt;With this enum, the user can choose between three strategies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Minimal&lt;/code&gt;: The builder scans the input data to find the maximum value, then calculates the minimum number of bits required to represent it. This is the most space-efficient option but requires a full pass over the data.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PowerOfTwo&lt;/code&gt;: Similar to&lt;code&gt;Minimal&lt;/code&gt;, but rounds the bit width up to the next power of two. This can simplify certain bitwise operations and align better with hardware word sizes, at the cost of some additional space.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Explicit(n)&lt;/code&gt;: The user provides a fixed bit width. This avoids the data scan but requires the user to ensure that all values fit within the specified width.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Note: Yes, I could have also made three different build functions:&lt;/p&gt;&lt;code&gt;new_with_minimal_bit_width()&lt;/code&gt;,&lt;code&gt;new_with_power_of_two_bit_width()&lt;/code&gt;, and&lt;code&gt;new_with_explicit_bit_width(n)&lt;/code&gt;. However, this would lead to a combinatorial explosion if we later wanted to add more configuration options. The builder pattern scales better.&lt;/quote&gt;
    &lt;p&gt;With this, the &lt;code&gt;FixedVecBuilder&lt;/code&gt; could be designed as a state machine. It holds the chosen &lt;code&gt;BitWidth&lt;/code&gt; strategy. The final &lt;code&gt;build()&lt;/code&gt; method takes the input slice and executes the appropriate logic.&lt;/p&gt;
    &lt;code&gt;// A look at the builder's logic flow
pub fn build(self, input: &amp;amp;[T]) -&amp;gt; Result&amp;lt;FixedVec&amp;lt;...&amp;gt;, Error&amp;gt; {

    let final_bit_width = match self.bit_width_strategy {
        BitWidth::Explicit(n) =&amp;gt; n,
        _ =&amp;gt; {
            // For Minimal or PowerOfTwo, we first find the max value.
            let max_val = input.iter().map(|v| v.into_word()).max().unwrap_or(0);
            let min_bits = (64 - max_val.leading_zeros()).max(1) as usize;

            match self.bit_width_strategy {
                BitWidth::Minimal =&amp;gt; min_bits,
                BitWidth::PowerOfTwo =&amp;gt; min_bits.next_power_of_two(),
                _ =&amp;gt; unreachable!(),
            }
        }
    };

    // ... (rest of the logic: allocate buffer, write data) ...
}&lt;/code&gt;
    &lt;p&gt;This design cleanly separates the configuration phase from the execution phase. The user can declaratively state their requirements, and the builder handles the implementation details, whether that involves a full data scan or a direct construction. For example:&lt;/p&gt;
    &lt;code&gt;use compressed_intvec::prelude::*;

let data: &amp;amp;[u32] = &amp;amp;[100, 200, 500]; // Max value 500 requires 9 bits

// The builder will scan the data, find max=500, calculate min_bits=9,
// and then round up to the next power of two.
let vec_pow2: UFixedVec&amp;lt;u32&amp;gt; = FixedVec::builder()
    .bit_width(BitWidth::PowerOfTwo)
    .build(data)
    .unwrap();

assert_eq!(vec_pow2.bit_width(), 16);&lt;/code&gt;
    &lt;quote&gt;&lt;code&gt;UFixedVec&amp;lt;T&amp;gt;&lt;/code&gt;is a type alias for&lt;code&gt;FixedVec&amp;lt;T, u64, LE, Vec&amp;lt;u64&amp;gt;&amp;gt;&lt;/code&gt;, the most common instantiation.&lt;/quote&gt;
    &lt;head rend="h1"&gt;Doing more than just reading&lt;/head&gt;
    &lt;p&gt;The design of &lt;code&gt;FixedVec&lt;/code&gt; allows for more than just efficient reads. We can extend it to support mutation and even thread-safe (almost atomic) concurrent access.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mutability: Proxy Objects&lt;/head&gt;
    &lt;p&gt;A core feature of &lt;code&gt;std::vec::Vec&lt;/code&gt; is mutable, index-based access via &lt;code&gt;&amp;amp;mut T&lt;/code&gt;. This is fundamentally impossible for &lt;code&gt;FixedVec&lt;/code&gt;. An element, such as a 10-bit integer, is not a discrete, byte-aligned entity in memory. It is a virtual value extracted from a bitstream, potentially spanning the boundary of two different &lt;code&gt;u64&lt;/code&gt; words. It has no stable memory address, so a direct mutable reference cannot be formed.&lt;/p&gt;
    &lt;p&gt;To provide an ergonomic mutable API, we must emulate the behavior of a mutable reference. We achieve this through a proxy object pattern, implemented in a struct named &lt;code&gt;MutProxy&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;pub struct MutProxy&amp;lt;'a, T, W, E, B&amp;gt;
where
    T: Storable&amp;lt;W&amp;gt;,
    W: Word,
    E: Endianness,
    B: AsRef&amp;lt;[W]&amp;gt; + AsMut&amp;lt;[W]&amp;gt;,
{
    vec: &amp;amp;'a mut FixedVec&amp;lt;T, W, E, B&amp;gt;,
    index: usize,
    value: T, // A temporary, decoded copy of the element's value.
}&lt;/code&gt;
    &lt;p&gt;When we call a method like &lt;code&gt;at_mut(index)&lt;/code&gt;, it does not return a reference. Instead, it constructs and returns a &lt;code&gt;MutProxy&lt;/code&gt; instance. The proxy‚Äôs lifecycle manages the entire modification process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Construction: The proxy is created. Its first action is to call the parent &lt;code&gt;FixedVec&lt;/code&gt;‚Äôs internal&lt;code&gt;get&lt;/code&gt;logic to read and decode the value at the specified&lt;code&gt;index&lt;/code&gt;. This decoded value is stored as a temporary copy inside the proxy object itself.&lt;/item&gt;
      &lt;item&gt;Modification: The &lt;code&gt;MutProxy&lt;/code&gt;implements&lt;code&gt;Deref&lt;/code&gt;and&lt;code&gt;DerefMut&lt;/code&gt;, allowing the user to interact with the temporary copy as if it were the real value. Any modifications (&lt;code&gt;*proxy = new_value&lt;/code&gt;,&lt;code&gt;*proxy += 1&lt;/code&gt;) are applied to this local copy, not to the underlying bitstream.&lt;/item&gt;
      &lt;item&gt;Destruction: When the &lt;code&gt;MutProxy&lt;/code&gt;goes out of scope, its&lt;code&gt;Drop&lt;/code&gt;implementation is executed. This is the critical step where the potentially modified value from the temporary copy is taken, re-encoded, and written back into the correct bit position in the parent&lt;code&gt;FixedVec&lt;/code&gt;‚Äôs storage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a classic copy-on-read, write-on-drop mechanism. It provides a safe and ergonomic abstraction for mutating non-addressable data, preserving the feel of direct manipulation while correctly handling the bit-level operations under the hood. The overhead is a single read at the start of the proxy‚Äôs life and a single write at the end.&lt;/p&gt;
    &lt;code&gt;use compressed_intvec::fixed::{FixedVec, UFixedVec, BitWidth};

let data: &amp;amp;[u32] = &amp;amp;[10, 20, 30];
let mut vec: UFixedVec&amp;lt;u32&amp;gt; = FixedVec::builder()
    .bit_width(BitWidth::Explicit(7))
    .build(data)
    .unwrap();

// vec.at_mut(1) returns an Option&amp;lt;MutProxy&amp;lt;...&amp;gt;&amp;gt;
if let Some(mut proxy) = vec.at_mut(1) {
    // The DerefMut trait allows us to modify the proxy's internal copy.
    *proxy = 99;
} // The proxy is dropped here. Its Drop impl writes 99 back to the vec.

assert_eq!(vec.get(1), Some(99));&lt;/code&gt;
    &lt;p&gt;The overhead of this approach is a single read on the proxy‚Äôs construction and a single write on its destruction, which is an acceptable trade-off for an ergonomic and safe mutable API.&lt;/p&gt;
    &lt;head rend="h3"&gt;Zero-Copy Views&lt;/head&gt;
    &lt;p&gt;A &lt;code&gt;Vec&lt;/code&gt;-like API needs to support slicing. Creating a &lt;code&gt;FixedVec&lt;/code&gt; that borrows its data (&lt;code&gt;B = &amp;amp;[W]&lt;/code&gt;) is the first step for this, but we also need a dedicated slice type to represent a sub-region of another &lt;code&gt;FixedVec&lt;/code&gt; without copying data. For this we can create &lt;code&gt;FixedVecSlice&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We can implement this as a classic ‚Äúfat pointer‚Äù struct. It holds a reference to the parent &lt;code&gt;FixedVec&lt;/code&gt; and a &lt;code&gt;Range&amp;lt;usize&amp;gt;&lt;/code&gt; that defines the logical boundaries of the view.&lt;/p&gt;
    &lt;code&gt;// A zero-copy view into a contiguous portion of a FixedVec.
#[derive(Debug)]
pub struct FixedVecSlice&amp;lt;V&amp;gt; {
    pub(super) parent: V,
    pub(super) range: Range&amp;lt;usize&amp;gt;,
}&lt;/code&gt;
    &lt;p&gt;The generic parameter &lt;code&gt;V&lt;/code&gt; is a reference to the parent vector. This allows the same &lt;code&gt;FixedVecSlice&lt;/code&gt; struct to represent both immutable (&lt;code&gt;V = &amp;amp;FixedVec&amp;lt;...&amp;gt;&lt;/code&gt;) and mutable (&lt;code&gt;V = &amp;amp;mut FixedVec&amp;lt;...&amp;gt;&lt;/code&gt;) views.&lt;/p&gt;
    &lt;p&gt;We can implement all the operations on the slice by translating the slice-relative index into an absolute index in the parent vector. For example, we can easily implement &lt;code&gt;get_unchecked&lt;/code&gt; with this delegation:&lt;/p&gt;
    &lt;code&gt;// Index translation within the slice's get_unchecked
pub unsafe fn get_unchecked(&amp;amp;self, index: usize) -&amp;gt; T {
    debug_assert!(index &amp;lt; self.len());
    // The index is relative to the slice, so we add the slice's start
    // offset to get the correct index in the parent vector.
    self.parent.get_unchecked(self.range.start + index)
}&lt;/code&gt;
    &lt;p&gt;In this way there is no code duplication; the slice re-uses the access logic of the parent &lt;code&gt;FixedVec&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mutable Slices&lt;/head&gt;
    &lt;p&gt;For mutable slices (&lt;code&gt;V = &amp;amp;mut FixedVec&amp;lt;...&amp;gt;&lt;/code&gt; ), we can provide mutable access to the slice‚Äôs elements. We can easily implement the &lt;code&gt;at_mut&lt;/code&gt; method on &lt;code&gt;FixedVecSlice&lt;/code&gt; using the same principle of index translation:&lt;/p&gt;
    &lt;code&gt;pub fn at_mut(&amp;amp;mut self, index: usize) -&amp;gt; Option&amp;lt;MutProxy&amp;lt;'_, T, W, E, B&amp;gt;&amp;gt; {
    if index &amp;gt;= self.len() {
        return None;
    }
    // The index is translated to the parent vector's coordinate space.
    Some(MutProxy::new(&amp;amp;mut self.parent, self.range.start + index))
}&lt;/code&gt;
    &lt;p&gt;A mutable slice borrows the parent &lt;code&gt;FixedVec&lt;/code&gt; mutably. This means that while the slice exists, the parent vector cannot be accessed directly due to Rust‚Äôs borrowing rules. Let‚Äôs consider the following situation: we may need to split a mutable slice into two non-overlapping mutable slices. This is common for example in algorithms that operate on sub-regions of an array. However, implementing such a method requires to use some unsafe code. The method, let‚Äôs say &lt;code&gt;split_at_mut&lt;/code&gt;, must produce two &lt;code&gt;&amp;amp;mut&lt;/code&gt; references from a single one. In order to be safe, we must prove to the compiler that the logical ranges they represent (&lt;code&gt;0..mid&lt;/code&gt; and &lt;code&gt;mid..len&lt;/code&gt;) are disjoint.&lt;/p&gt;
    &lt;code&gt;pub fn split_at_mut(&amp;amp;mut self, mid: usize) -&amp;gt; (FixedVecSlice&amp;lt;&amp;amp;mut Self&amp;gt;, FixedVecSlice&amp;lt;&amp;amp;mut Self&amp;gt;) {
    assert!(mid &amp;lt;= self.len, "mid &amp;gt; len in split_at_mut");
    // SAFETY: The two slices are guaranteed not to overlap.
    unsafe {
        let ptr = self as *mut Self;
        let left = FixedVecSlice::new(&amp;amp;mut *ptr, 0..mid);
        let right = FixedVecSlice::new(&amp;amp;mut *ptr, mid..self.len());
        (left, right)
    }
}&lt;/code&gt;
    &lt;p&gt;This combination of a generic slice struct and careful pointer manipulation allows us to build a rich, safe, and zero-copy API for both immutable and mutable views, mirroring Rust‚Äôs native slice&lt;/p&gt;
    &lt;head rend="h1"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;Our &lt;code&gt;FixedVec&lt;/code&gt; works pretty well at the current state, but its strengths are tied to two key assumptions: a single thread of execution and uniformly bounded data. Real-world systems often challenge both. This opens up two distinct paths for us to extend our design: one to handle concurrency, and another to adapt to more complex data distributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Concurrent Access&lt;/head&gt;
    &lt;p&gt;The design of &lt;code&gt;FixedVec&lt;/code&gt; is fundamentally single-threaded. Its mutable access relies on direct writes to the underlying bit buffer, a model that offers no guarantees in a concurrent setting. The core conflict lies between our data layout and the hardware‚Äôs atomic primitives. CPU instructions like compare-and-swap operate on aligned machine words, typically &lt;code&gt;u64&lt;/code&gt;. Our elements, however, are packed at arbitrary bit offsets and can span the boundary between two words.&lt;/p&gt;
    &lt;p&gt;This means a single logical update‚Äîmodifying one element‚Äîmight require writing to two separate &lt;code&gt;u64&lt;/code&gt; words. Performing this as two distinct atomic writes would create a race condition, leaving the data in a corrupt state if another thread reads between them. A single atomic write to one of the underlying &lt;code&gt;u64&lt;/code&gt; words would be equally disastrous, as it could simultaneously alter parts of two different elements. The problem then is how to build atomic semantics on top of a non-atomic memory layout. In the next post, we will construct a solution that provides thread-safe, atomic operations for our bit-packed vector.&lt;/p&gt;
    &lt;head rend="h2"&gt;Variable Length Encoding&lt;/head&gt;
    &lt;p&gt;Let‚Äôs consider the following scenario: we have a large collection of integers, all of which are small, say in the range &lt;code&gt;[0, 255]&lt;/code&gt;, but with a few outliers that are much larger, perhaps up to &lt;code&gt;u64::MAX&lt;/code&gt;. If we were to use our &lt;code&gt;FixedVec&lt;/code&gt; with a &lt;code&gt;bit_width&lt;/code&gt; of 64 to accommodate the outliers, we would waste a significant amount of memory on the small integers. Conversely, if we chose a smaller &lt;code&gt;bit_width&lt;/code&gt;, we would be unable to represent the outliers at all.&lt;/p&gt;
    &lt;p&gt;The fixed-width model rests on that critical assumption of uniformly bounded data. Its performance comes from this predictability, but this rigid structure is also its main weakness.&lt;/p&gt;
    &lt;p&gt;For skewed data distributions, we need a different model. Instead of a fixed number of bits per element, we can use variable-length instantaneous codes, where smaller numbers are represented by shorter bit sequences. This gives us excellent compression, but it breaks our O(1) random access guarantee. We can no longer compute the location of the i-th element with a simple multiplication. The solution is to trade a small amount of space for a speedup in access time. We can build a secondary index that stores the bit-offset of every k-th element. This sampling allows us to seek to a nearby checkpoint and decode sequentially from there, restoring amortized O(1) access. In a future article, we‚Äôll explore this second vector type, its own set of performance trade-offs, and how we can choose the best encoding for our data.&lt;/p&gt;
    &lt;p&gt;We will explore this two paths in future articles. In the meantime, if you want to try them out, they are both already implemented in the library. You can find the atomic version in the atomic module and the variable-length version in the variable module.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lukefleed.xyz/posts/compressed-fixedvec/"/><published>2025-09-24T15:17:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362023</id><title>Python on the Edge: Fast, sandboxed, and powered by WebAssembly</title><updated>2025-09-24T20:36:03.638064+00:00</updated><content>&lt;doc fingerprint="87ab2dfdccf438d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Python on the Edge: Fast, sandboxed, and powered by WebAssembly&lt;/head&gt;
    &lt;p&gt;We are excited to announce full Python support in Wasmer Edge (Beta)&lt;/p&gt;
    &lt;p&gt;Founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;With AI workloads on the rise, the demand for Python support on WebAssembly on the Edge has grown rapidly.&lt;/p&gt;
    &lt;p&gt;However, bringing Python to WebAssembly isn't trivial as it means supporting native modules like &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, and &lt;code&gt;pydantic&lt;/code&gt;.&amp;#13;
While projects like &lt;code&gt;pyodide&lt;/code&gt; made strides in running Python in the browser via WebAssembly, their trade-offs don't fully fit server-side needs.&lt;/p&gt;
    &lt;p&gt;After months of hard work, today we're thrilled to announce full Python support in Wasmer Edge (Beta) powered by WebAssembly and WASIX.&lt;/p&gt;
    &lt;p&gt;Now you can run FastAPI, Streamlit, Django, LangChain, and more directly on Wasmer and Wasmer Edge! To accomplish it we had to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add support for dynamic linking (&lt;code&gt;dlopen&lt;/code&gt;/&lt;code&gt;dlsym&lt;/code&gt;) into WASIX&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;libffi&lt;/code&gt;support (so Python libraries using&lt;code&gt;ctypes&lt;/code&gt;could be supported)&lt;/item&gt;
      &lt;item&gt;Polish Sockets and threading support in WASIX&lt;/item&gt;
      &lt;item&gt;Release our own Python Package Index with many of the most popular Python Native libraries compiled to WASIX&lt;/item&gt;
      &lt;item&gt;Create our own alternative to Heroku Buildpacks / Nixpacks / Railpack / Devbox to automatically detect a project type from its source code and deploy it (including running with Wasmer or deploying to Wasmer Edge!). Updates will be shared soon!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How fast is it?&lt;/head&gt;
    &lt;p&gt;This Python release is much faster than any of the other Python releases we did in the past.&lt;/p&gt;
    &lt;p&gt;It is fast. Insa‚Ä¶natively fast (it's even faster than our py2wasm project!)&lt;/p&gt;
    &lt;code&gt;$ wasmer run python/python@=0.2.0 --dir=. -- pystone.py&amp;#13;
Pystone(1.1) time for 50000 passes = 0.562538&amp;#13;
This machine benchmarks at 88882.9 pystones/second&amp;#13;
$ wasmer run python/python --dir=. -- pystone.py # Note: first run may take time&amp;#13;
Pystone(1.1) time for 50000 passes = 0.093556&amp;#13;
This machine benchmarks at 534439 pystones/second&amp;#13;
$ python3 pystone.py&amp;#13;
Pystone(1.1) time for 50000 passes = 0.0827736&amp;#13;
This machine benchmarks at 604057 pystones/second
&lt;/code&gt;
    &lt;p&gt;That's 6x faster, and nearly indistinguishable from native Python performance‚Ä¶ quite good, considering that your Python apps can now run fully sandboxed anywhere!&lt;/p&gt;
    &lt;p&gt;Note: the first time you run Python, it will take a few minutes to compile. We are working to improve this so no time will be spent on compilation locally.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;üöÄ Even faster performance coming soon: we are trialing an optimization technique that will boost Python performance in Wasm to 95% of native Python speed. This is already powering our PHP server in production. Result: Near-native Python performance, fully sandboxed. Stay tuned!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;What it can run&lt;/head&gt;
    &lt;p&gt;Now, you can run any kind of Python API server, powered by &lt;code&gt;fastapi&lt;/code&gt;, &lt;code&gt;django&lt;/code&gt;, &lt;code&gt;flask&lt;/code&gt;, or &lt;code&gt;starlette&lt;/code&gt;, connected to a MySQL database automatically when needed (FastAPI template, Django template).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;fastapi&lt;/code&gt; with websockets (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;mcp&lt;/code&gt; servers (deploy using our MCP template, demo).&lt;/p&gt;
    &lt;p&gt;You can run image processors like &lt;code&gt;pillow&lt;/code&gt;  (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;ffmpeg&lt;/code&gt; inside Python (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;streamlit&lt;/code&gt; and &lt;code&gt;langchain&lt;/code&gt; (deploy using our LangChain template, demo).&lt;/p&gt;
    &lt;p&gt;You can even run &lt;code&gt;pypandoc&lt;/code&gt;!  (example repo, demo).&lt;/p&gt;
    &lt;p&gt;Soon, we'll have full support for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;curl_cffi&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;polars&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gevent&lt;/code&gt;/&lt;code&gt;greenlet&lt;/code&gt;(more on this soon!)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Pytorch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Wasmer VS alternatives&lt;/head&gt;
    &lt;p&gt;Python on Wasmer Edge is just launching, but it's already worth asking: how does it stack up existing solutions?&lt;/p&gt;
    &lt;head rend="h3"&gt;Quick Comparison&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Feature / Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Wasmer Edge&lt;/cell&gt;
        &lt;cell role="head"&gt;Cloudflare&lt;/cell&gt;
        &lt;cell role="head"&gt;AWS Lambda&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Native modules (&lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, etc.)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported*&lt;/cell&gt;
        &lt;cell&gt;‚ùå Limited (no &lt;code&gt;libcurl&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multithreading &amp;amp; multiprocessing (&lt;code&gt;ffmpeg&lt;/code&gt;, &lt;code&gt;pandoc&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ASGI / WSGI frameworks (&lt;code&gt;uvicorn&lt;/code&gt;, &lt;code&gt;daphne&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
        &lt;cell&gt;‚ùå Patched / limited&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Needs wrappers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;WebSockets (&lt;code&gt;streamlit&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Raw sockets (&lt;code&gt;libcurl&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
        &lt;cell&gt;‚ùå JS &lt;code&gt;fetch&lt;/code&gt; only&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multiple Python versions&lt;/cell&gt;
        &lt;cell&gt;‚úÖ In Roadmap (3.12, 3.14‚Ä¶)&lt;/cell&gt;
        &lt;cell&gt;‚ùå Tied to bundled runtime&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cold starts&lt;/cell&gt;
        &lt;cell&gt;‚ö° Extremely fast&lt;/cell&gt;
        &lt;cell&gt;‚è≥ Medium (V8 isolates)&lt;/cell&gt;
        &lt;cell&gt;‚è≥ Slow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Code changes required&lt;/cell&gt;
        &lt;cell&gt;‚úÖ None&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Some&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Wrappers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pricing&lt;/cell&gt;
        &lt;cell&gt;üí∞ Affordable&lt;/cell&gt;
        &lt;cell&gt;üí∞ Higher&lt;/cell&gt;
        &lt;cell&gt;üí∞ Higher&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare Workers (Python) / Pyodide&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;‚ÑπÔ∏è Most of the demos that we showcased on this article, are not runnable inside of Cloudflare:&lt;/p&gt;&lt;code&gt;ffmpeg&lt;/code&gt;,&lt;code&gt;streamlit&lt;/code&gt;,&lt;code&gt;pypandoc&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Cloudflare launched Python support ~18 months ago, by using Pyodide inside workerd, their JavaScript-based Workers runtime.&lt;/p&gt;
    &lt;p&gt;While great for browser-like environments, Pyodide has trade-offs that make it less suitable server-side. Here are the limitations when running Python in Cloudflare:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå No support for &lt;code&gt;uvloop&lt;/code&gt;,&lt;code&gt;uvicorn&lt;/code&gt;, or similar event-native frameworks (JS event loop patches break compatibility with native).&lt;/item&gt;
      &lt;item&gt;‚ùå No pthreads or multiprocessing support, you can't call subprocesses like &lt;code&gt;ffmpeg&lt;/code&gt;or&lt;code&gt;pypandoc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;‚ùå No raw HTTP client sockets (HTTP clients are patched to use JS &lt;code&gt;fetch&lt;/code&gt;, no&lt;code&gt;libcurl&lt;/code&gt;available).&lt;/item&gt;
      &lt;item&gt;‚ùå Limited to a bundled Python version and package set.&lt;/item&gt;
      &lt;item&gt;‚è≥ Cold starts slower due to V8 isolate warmup.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why the limitations? Cloudflare relies on Pyodide: great in-browser execution, but server-side it implies no sockets, threads, or multiprocessing. The result: convenient for lightweight browser use, but might not be the best fit for real Python workloads on the server.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In contrast, Wasmer Edge runs real Python on WASIX unmodified, so everything "just works", with near-native speed and fast cold starts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Amazon Lambda&lt;/head&gt;
    &lt;p&gt;AWS Lambda doesn't natively run unmodified Python apps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå You need adapters (such as https://github.com/slank/awsgi or https://github.com/Kludex/mangum) for running your WSGI sites.&lt;/item&gt;
      &lt;item&gt;‚ùå WebSockets are unsupported.&lt;/item&gt;
      &lt;item&gt;‚ö†Ô∏è Setup is complex, adapters are often unmaintained.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why the limits? AWS Lambda requires you to use their HTTP lambda handler, which can cause incompatibility into your own HTTP servers. Also, because their lambda handlers are HTTP-based, there's no easy support for WebSockets.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In contrast, Wasmer Edge supports any Python HTTP servers without requiring any code adaptation from your side.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Why Wasmer Edge Stands Out&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Closer to native Python than Pyodide (no JS involvement at all).&lt;/item&gt;
      &lt;item&gt;Faster cold starts and more compatibility than Cloudflare's Workers.&lt;/item&gt;
      &lt;item&gt;More compatible than AWS Lambda (no wrappers/adapters).&lt;/item&gt;
      &lt;item&gt;More affordable across the board.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;üêç It's Showtime!&lt;/head&gt;
    &lt;p&gt;Python support in Wasmer and Wasmer Edge is already available and ready to use. We have set up many Python templates to help you get started in no time.&lt;/p&gt;
    &lt;p&gt;https://wasmer.io/templates?language=python&lt;/p&gt;
    &lt;p&gt;To make things even better, we are working on a MCP server for Wasmer, so you will be able to plug Wasmer into ChatGPT or Anthropic and have your websites deploying from your vibe-coded projects. Stay tuned!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ö†Ô∏è Python in Wasmer Edge is still in Beta, so expect some rough edges if your project doesn't work out of the box‚Ä¶ if you encounter any issues, please report them so we can work on enabling your workloads on Wasmer Edge.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Create your first MCP Server in Wasmer&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to https://wasmer.io/templates/mcp-chatgpt-starter?intent=at_vRxJIdtPCbKe&lt;/item&gt;
      &lt;item&gt;Connect your Github account&lt;/item&gt;
      &lt;item&gt;Create a git repo from the template&lt;/item&gt;
      &lt;item&gt;Deploy and enjoy!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: source code available here: https://github.com/wasmer-examples/python-mcp-chatgpt-starter&lt;/p&gt;
    &lt;head rend="h2"&gt;Create your first Django app&lt;/head&gt;
    &lt;p&gt;We have set up a template for using Django + Uvicorn in Wasmer Edge.&lt;/p&gt;
    &lt;p&gt;You can start using it very easily, just click Deploy: https://wasmer.io/templates/django-starter?intent=at_WK0DIkt3CeKX&lt;/p&gt;
    &lt;p&gt;Deploying a Django app will create a MySQL DB for you in Wasmer Edge (Postgres support is coming soon), run migrations and prepare everything to run your website seamlessly.&lt;/p&gt;
    &lt;p&gt;Note: source code available here: https://github.com/wasmer-examples/django-wasmer-starter&lt;/p&gt;
    &lt;p&gt;Ready to deploy your first Python app on Wasmer Edge?&lt;/p&gt;
    &lt;p&gt;Here are the best places to begin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üöÄ Starter Templates ‚Üí Browse Python templates&lt;/item&gt;
      &lt;item&gt;üìñ Docs &amp;amp; Examples ‚Üí Wasmer GitHub&lt;/item&gt;
      &lt;item&gt;üí¨ Community Support ‚Üí Join our Discord&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üëâ Deploy your first Python app now&lt;/p&gt;
    &lt;p&gt;With WebAssembly and Wasmer, Python is now portable, sandboxed, and running at near-native speeds. Ready for AI workloads, APIs, and anything you can imagine at the edge.&lt;lb/&gt; The sky is the limit ‚ù§Ô∏è.&lt;/p&gt;
    &lt;head rend="h5"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;Syrus Akbary is an enterpreneur and programmer. Specifically known for his contributions to the field of WebAssembly. He is the Founder and CEO of Wasmer, an innovative company that focuses on creating developer tools and infrastructure for running Wasm&lt;/p&gt;
    &lt;p&gt;Founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;How fast is it?&lt;/p&gt;
    &lt;p&gt;What it can run&lt;/p&gt;
    &lt;p&gt;Wasmer VS alternatives&lt;/p&gt;
    &lt;p&gt;Quick Comparison&lt;/p&gt;
    &lt;p&gt;Cloudflare Workers (Python) / Pyodide&lt;/p&gt;
    &lt;p&gt;Amazon Lambda&lt;/p&gt;
    &lt;p&gt;Why Wasmer Edge Stands Out&lt;/p&gt;
    &lt;p&gt;üêç It's Showtime!&lt;/p&gt;
    &lt;p&gt;Create your first MCP Server in Wasmer&lt;/p&gt;
    &lt;p&gt;Create your first Django app&lt;/p&gt;
    &lt;p&gt;Deploy your first Python site in seconds with our managed cloud solution.&lt;/p&gt;
    &lt;head rend="h5"&gt;Read more&lt;/head&gt;
    &lt;p&gt;wasmerwasmer edgerustprojectsedgeweb scraper&lt;/p&gt;
    &lt;head rend="h6"&gt;Build a Web Scraper in Rust and Deploy to Wasmer Edge&lt;/head&gt;
    &lt;p&gt;RudraAugust 14, 2023&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wasmer.io/posts/python-on-the-edge-powered-by-webassembly"/><published>2025-09-24T15:48:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362206</id><title>SedonaDB: A new geospatial DataFrame library written in Rust</title><updated>2025-09-24T20:36:03.510460+00:00</updated><content>&lt;doc fingerprint="3bdb989c4036eb8a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing SedonaDB: A single-node analytical database engine with geospatial as a first-class citizen&lt;/head&gt;
    &lt;p&gt;The Apache Sedona community is excited to announce the initial release of SedonaDB! √∞&lt;/p&gt;
    &lt;p&gt;SedonaDB is the first open-source, single-node analytical database engine that treats spatial data as a first-class citizen. It is developed as a subproject of Apache Sedona.&lt;/p&gt;
    &lt;p&gt;Apache Sedona powers large-scale geospatial processing on distributed engines like Spark (SedonaSpark), Flink (SedonaFlink), and Snowflake (SedonaSnow). SedonaDB extends the Sedona ecosystem with a single-node engine optimized for small-to-medium data analytics, delivering the simplicity and speed that distributed systems often cannot.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ What is SedonaDB¬∂&lt;/head&gt;
    &lt;p&gt;Written in Rust, SedonaDB is lightweight, blazing fast, and spatial-native. Out of the box, it provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;√∞¬∫√Ø¬∏ Full support for spatial types, joins, CRS (coordinate reference systems), and functions on top of industry-standard query operations.&lt;/item&gt;
      &lt;item&gt;√¢¬° Query optimizations, indexing, and data pruning features under the hood that make spatial operations just work with high performance.&lt;/item&gt;
      &lt;item&gt;√∞ Pythonic and SQL interfaces familiar to developers, plus APIs for R and Rust.&lt;/item&gt;
      &lt;item&gt;√¢√Ø¬∏ Flexibility to run in single-machine environments on local files or data lakes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SedonaDB utilizes Apache Arrow and Apache DataFusion, providing everything you need from a modern, vectorized query engine. What sets it apart is the ability to process spatial workloads natively, without extensions or plugins. Installation is straightforward, and SedonaDB integrates easily into both local development and cloud pipelines, offering a consistent experience across environments.&lt;/p&gt;
    &lt;p&gt;The initial release of SedonaDB provides a comprehensive suite of geometric vector operations and seamlessly integrates with GeoArrow, GeoParquet, and GeoPandas. Future versions will support all popular spatial functions, including functions for raster data.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ SedonaDB quickstart example¬∂&lt;/head&gt;
    &lt;p&gt;Start by installing SedonaDB:&lt;/p&gt;
    &lt;code&gt;pip install "apache-sedona[db]"
&lt;/code&gt;
    &lt;p&gt;Now instantiate the connection:&lt;/p&gt;
    &lt;code&gt;import sedona.db

sd = sedona.db.connect()
&lt;/code&gt;
    &lt;p&gt;Let's perform a spatial join using SedonaDB.&lt;/p&gt;
    &lt;p&gt;Suppose you have a &lt;code&gt;cities&lt;/code&gt; table with latitude and longitude points representing the center of each city, and a &lt;code&gt;countries&lt;/code&gt; table with a column containing a polygon of the country's geographic boundaries.&lt;/p&gt;
    &lt;p&gt;Here are a few rows from the &lt;code&gt;cities&lt;/code&gt; table:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢     name     √¢            geometry           √¢
√¢   utf8view   √¢      geometry &amp;lt;epsg:4326&amp;gt;     √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Vatican City √¢ POINT(12.4533865 41.9032822)  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ San Marino   √¢ POINT(12.4417702 43.9360958)  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Vaduz        √¢ POINT(9.5166695 47.1337238)   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
&lt;/code&gt;
    &lt;p&gt;And here are a few rows from the countries table:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢             name            √¢   continent   √¢                      geometry                      √¢
√¢           utf8view          √¢    utf8view   √¢                geometry &amp;lt;epsg:4326&amp;gt;                √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Fiji                        √¢ Oceania       √¢ MULTIPOLYGON(((180 -16.067132663642447,180 -16.55√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ United Republic of Tanzania √¢ Africa        √¢ POLYGON((33.90371119710453 -0.9500000000000001,34√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Western Sahara              √¢ Africa        √¢ POLYGON((-8.665589565454809 27.656425889592356,-8√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
&lt;/code&gt;
    &lt;p&gt;Here√¢s how to perform a spatial join to compute the country of each city:&lt;/p&gt;
    &lt;code&gt;sd.sql(
    """
select
    cities.name as city_name,
    countries.name as country_name,
    continent
from cities
join countries
where ST_Intersects(cities.geometry, countries.geometry)
"""
).show(3)
&lt;/code&gt;
    &lt;p&gt;The code utilizes &lt;code&gt;ST_Intersects&lt;/code&gt; to determine if a city is contained within a given country.&lt;/p&gt;
    &lt;p&gt;Here's the result of the query:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢   city_name   √¢         country_name        √¢ continent √¢
√¢    utf8view   √¢           utf8view          √¢  utf8view √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Suva          √¢ Fiji                        √¢ Oceania   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Dodoma        √¢ United Republic of Tanzania √¢ Africa    √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Dar es Salaam √¢ United Republic of Tanzania √¢ Africa    √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;The example above performs a point-in-polygon join, mapping city locations (points) to the countries they fall within (polygons). SedonaDB executes these joins efficiently by leveraging spatial indices where beneficial and dynamically adapting join strategies at runtime using input data samples. While many general-purpose engines struggle with the performance of such operations, SedonaDB is purpose-built for spatial workloads and delivers consistently fast results.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ Apache Sedona SpatialBench¬∂&lt;/head&gt;
    &lt;p&gt;To test our work on SedonaDB, we also needed to develop a mechanism to evaluate its performance and speed. This led us to develop Apache Sedona SpatialBench, a benchmark for assessing geospatial SQL analytics query performance across database systems.&lt;/p&gt;
    &lt;p&gt;Let's compare the performance of SedonaDB vs. GeoPandas and DuckDB Spatial for some representative spatial queries as defined in SpatialBench.&lt;/p&gt;
    &lt;p&gt;Here are the results from SpatialBench v0.1 for Queries 1√¢12 at scale factor 1 (SF1) and scale factor 10 (SF10).&lt;/p&gt;
    &lt;p&gt;SedonaDB demonstrates balanced performance across all query types and scales effectively to SF 10. DuckDB excels at spatial filters and some geometric operations but faces challenges with complex joins and KNN queries. GeoPandas, while popular in the Python ecosystem, requires manual optimization and parallelization to handle larger datasets effectively. An in-depth performance analysis can be found in the SpatialBench website.&lt;/p&gt;
    &lt;p&gt;Here√¢s an example of the SpatialBench Query #8 that works for SedonaDB and DuckDB:&lt;/p&gt;
    &lt;code&gt;SELECT b.b_buildingkey, b.b_name, COUNT(*) AS nearby_pickup_count
FROM trip t JOIN building b ON ST_DWithin(ST_GeomFromWKB(t.t_pickuploc), ST_GeomFromWKB(b.b_boundary), 0.0045) -- ~500m
GROUP BY b.b_buildingkey, b.b_name
ORDER BY nearby_pickup_count DESC
&lt;/code&gt;
    &lt;p&gt;This query intentionally performs a distance-based spatial join between points and polygons, followed by an aggregation of the results.&lt;/p&gt;
    &lt;p&gt;Here's what the query returns:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ b_buildingkey √¢  b_name  √¢ nearby_pickup_count √¢
√¢     int64     √¢ utf8view √¢        int64        √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢          3779 √¢ linen    √¢                  42 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢         19135 √¢ misty    √¢                  36 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢          4416 √¢ sienna   √¢                  26 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;Here√¢s the equivalent GeoPandas code:&lt;/p&gt;
    &lt;code&gt;trips_df = pd.read_parquet(data_paths["trip"])
trips_df["pickup_geom"] = gpd.GeoSeries.from_wkb(
    trips_df["t_pickuploc"], crs="EPSG:4326"
)
pickups_gdf = gpd.GeoDataFrame(trips_df, geometry="pickup_geom", crs="EPSG:4326")

buildings_df = pd.read_parquet(data_paths["building"])
buildings_df["boundary_geom"] = gpd.GeoSeries.from_wkb(
    buildings_df["b_boundary"], crs="EPSG:4326"
)
buildings_gdf = gpd.GeoDataFrame(
    buildings_df, geometry="boundary_geom", crs="EPSG:4326"
)

threshold = 0.0045  # degrees (~500m)
result = (
    buildings_gdf.sjoin(pickups_gdf, predicate="dwithin", distance=threshold)
    .groupby(["b_buildingkey", "b_name"], as_index=False)
    .size()
    .rename(columns={"size": "nearby_pickup_count"})
    .sort_values(["nearby_pickup_count", "b_buildingkey"], ascending=[False, True])
    .reset_index(drop=True)
)
&lt;/code&gt;
    &lt;head rend="h2"&gt;√∞¬∫√Ø¬∏ SedonaDB CRS management¬∂&lt;/head&gt;
    &lt;p&gt;SedonaDB manages the CRS when reading/writing files, as well as in DataFrames, making your pipelines safer and saving you from manual work.&lt;/p&gt;
    &lt;p&gt;Let's compute the number of buildings in the state of Vermont to highlight the CRS management features embedded in SedonaDB.&lt;/p&gt;
    &lt;p&gt;Start by reading in a FlatGeobuf file that uses the EPSG 32618 CRS with GeoPandas and then convert it to a SedonaDB DataFrame:&lt;/p&gt;
    &lt;code&gt;import geopandas as gpd

path = "https://raw.githubusercontent.com/geoarrow/geoarrow-data/v0.2.0/example-crs/files/example-crs_vermont-utm.fgb"
gdf = gpd.read_file(path)
vermont = sd.create_data_frame(gdf)
&lt;/code&gt;
    &lt;p&gt;Let√¢s check the schema of the &lt;code&gt;vermont&lt;/code&gt; DataFrame:&lt;/p&gt;
    &lt;code&gt;vermont.schema

SedonaSchema with 1 field:
  geometry: wkb &amp;lt;epsg:32618&amp;gt;
&lt;/code&gt;
    &lt;p&gt;We can see that the &lt;code&gt;vermont&lt;/code&gt; DataFrame maintains the CRS that√¢s specified in the FlatGeobuf file.  SedonaDB doesn√¢t have a native FlatGeobuf reader yet, but it√¢s easy to use the GeoPandas FlatGeobuf reader and then convert it to a SedonaDB DataFrame with a single line of code.&lt;/p&gt;
    &lt;p&gt;Now read a GeoParquet file into a SedonaDB DataFrame.&lt;/p&gt;
    &lt;code&gt;buildings = sd.read_parquet(
    "https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/microsoft-buildings_point_geo.parquet"
)
&lt;/code&gt;
    &lt;p&gt;Check the schema of the DataFrame:&lt;/p&gt;
    &lt;code&gt;buildings.schema

SedonaSchema with 1 field:
  geometry: geometry &amp;lt;ogc:crs84&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Let√¢s expose these two tables as views and run a spatial join to see how many buildings are in Vermont:&lt;/p&gt;
    &lt;code&gt;buildings.to_view("buildings", overwrite=True)
vermont.to_view("vermont", overwrite=True)

sd.sql(
    """
select count(*) from buildings
join vermont
where ST_Intersects(buildings.geometry, vermont.geometry)
"""
).show()
&lt;/code&gt;
    &lt;p&gt;This command correctly errors out because the tables have different CRSs. For safety, SedonaDB errors out rather than give you the wrong answer! Here's the error message that's easy to debug:&lt;/p&gt;
    &lt;code&gt;SedonaError: type_coercion
caused by
Error during planning: Mismatched CRS arguments: ogc:crs84 vs epsg:32618
Use ST_Transform() or ST_SetSRID() to ensure arguments are compatible.
&lt;/code&gt;
    &lt;p&gt;Let√¢s rewrite the spatial join to convert the &lt;code&gt;vermont&lt;/code&gt; CRS to EPSG:4326, so it√¢s compatible with the &lt;code&gt;buildings&lt;/code&gt; CRS.&lt;/p&gt;
    &lt;code&gt;sd.sql(
    """
select count(*) from buildings
join vermont
where ST_Intersects(buildings.geometry, ST_Transform(vermont.geometry, 'EPSG:4326'))
"""
).show()
&lt;/code&gt;
    &lt;p&gt;We now get the correct result!&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ count(*) √¢
√¢   int64  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢   361856 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;SedonaDB tracks the CRS when reading/writing files, converting to/from GeoPandas DataFrames, or when performing DataFrame operations, so your spatial computations run safely and correctly!&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬Ø Realistic example with SedonaDB¬∂&lt;/head&gt;
    &lt;p&gt;Let's now turn our attention to a KNN join, which is a more complex spatial operation.&lt;/p&gt;
    &lt;p&gt;Suppose you're analyzing ride-sharing data and want to identify which buildings are most commonly near pickup points, helping understand the relationship between trip origins and nearby landmarks, businesses, or residential structures that might influence ride demand patterns.&lt;/p&gt;
    &lt;p&gt;This query finds the five closest buildings to each trip pickup location using spatial nearest neighbor analysis. For every trip, it identifies the five buildings that are geographically closest to where the passenger was picked up and calculates the exact distance to each of those buildings.&lt;/p&gt;
    &lt;p&gt;Here√¢s the query:&lt;/p&gt;
    &lt;code&gt;WITH trip_with_geom AS (
    SELECT t_tripkey, t_pickuploc, ST_GeomFromWKB(t_pickuploc) as pickup_geom
    FROM trip
),
building_with_geom AS (
    SELECT b_buildingkey, b_name, b_boundary, ST_GeomFromWKB(b_boundary) as boundary_geom
    FROM building
)
SELECT
    t.t_tripkey,
    t.t_pickuploc,
    b.b_buildingkey,
    b.b_name AS building_name,
    ST_Distance(t.pickup_geom, b.boundary_geom) AS distance_to_building
FROM trip_with_geom t JOIN building_with_geom b
ON ST_KNN(t.pickup_geom, b.boundary_geom, 5, FALSE)
ORDER BY distance_to_building ASC, b.b_buildingkey ASC
&lt;/code&gt;
    &lt;p&gt;Here are the results of the query:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ t_tripkey √¢          t_pickuploc          √¢ b_buildingkey √¢ building_name √¢ distance_to_building √¢
√¢   int64   √¢             binary            √¢     int64     √¢      utf8     √¢        float64       √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢   5854027 √¢ 01010000001afa27b85825504001√¢¬¶ √¢            79 √¢ gainsboro     √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢   3326828 √¢ 01010000001bfcc5b8b7a95d4083√¢¬¶ √¢           466 √¢ deep          √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢   1239844 √¢ 0101000000ce471770d6ce2a40f9√¢¬¶ √¢           618 √¢ ivory         √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;This is one of the queries from SpatialBench.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬¶ Why SedonaDB was built in Rust¬∂&lt;/head&gt;
    &lt;p&gt;SedonaDB is built in Rust, a high-performance, memory-safe language that offers fine-grained memory management and a mature ecosystem of data libraries. It takes full advantage of this ecosystem by integrating with projects such as Apache DataFusion, GeoArrow, and georust/geo.&lt;/p&gt;
    &lt;p&gt;While Spark provides extension points that let SedonaSpark optimize spatial queries in distributed settings, DataFusion offers stable APIs for pruning, spatial operators, and optimizer rules on a single node. This enabled us to embed deep spatial awareness into the engine while preserving full non-spatial functionality. Thanks to the DataFusion project and community, the experience was both possible and enjoyable.&lt;/p&gt;
    &lt;head rend="h2"&gt;√¢√Ø¬∏ Why SedonaDB and SedonaSpark are Both Needed¬∂&lt;/head&gt;
    &lt;p&gt;SedonaSpark is well-suited for large-scale geospatial workloads or environments where Spark is already part of your production stack. For instance, joining a 100 GB vector dataset with a large raster dataset. For smaller datasets, however, Spark's distributed architecture can introduce unnecessary overhead, making it slower to run locally, harder to install, and more difficult to tune.&lt;/p&gt;
    &lt;p&gt;SedonaDB is better for smaller datasets and when running computations locally. The SedonaDB spatial functions are compatible with the SedonaSpark functions, so SQL chunks that work for one engine will usually work for the other. Over time, we will ensure that both project APIs are fully interoperable. Here's an example of a chunk to analyze the Overture buildings table that works for both engines.&lt;/p&gt;
    &lt;code&gt;nyc_bbox_wkt = (
    "POLYGON((-74.2591 40.4774, -74.2591 40.9176, -73.7004 40.9176, -73.7004 40.4774, -74.2591 40.4774))"
)

sd.sql(f"""
SELECT
    id,
    height,
    num_floors,
    roof_shape,
    ST_Centroid(geometry) as centroid
FROM
    buildings
WHERE
    is_underground = FALSE
    AND height IS NOT NULL
    AND height &amp;gt; 20
    AND ST_Intersects(geometry, ST_SetSRID(ST_GeomFromText('{nyc_bbox_wkt}'), 4326))
LIMIT 5;
&lt;/code&gt;
    &lt;head rend="h2"&gt;√∞ Next steps¬∂&lt;/head&gt;
    &lt;p&gt;While SedonaDB is well-tested and provides a core set of features that can perform numerous spatial analyses, it remains an early-stage project with multiple opportunities for new features.&lt;/p&gt;
    &lt;p&gt;Many more ST functions are required. Some are relatively straightforward, but others are complex.&lt;/p&gt;
    &lt;p&gt;The community will add built-in support for other spatial file formats, such as GeoPackage and GeoJSON, to SedonaDB. You can read data in these formats into GeoPandas DataFrames and convert them to SedonaDB DataFrames in the meantime.&lt;/p&gt;
    &lt;p&gt;Raster support is also on the roadmap, which is a complex undertaking, so it's an excellent opportunity to contribute if you're interested in solving challenging problems with Rust.&lt;/p&gt;
    &lt;p&gt;Refer to the SedonaDB v0.2 milestone for more details on the specific tasks outlined for the next release. Additionally, feel free to create issues, comment on the Discord, or start GitHub discussions to brainstorm new features.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ Join the community¬∂&lt;/head&gt;
    &lt;p&gt;The Apache Sedona community has an active Discord community, monthly user meetings, and regular contributor meetings.&lt;/p&gt;
    &lt;p&gt;SedonaDB welcomes contributions from the community. Feel free to request to take ownership of an issue, and we will be happy to assign it to you. You're also welcome to join the contributor meetings, and the other active contributors will be glad to help you get your pull request over the finish line!&lt;/p&gt;
    &lt;p&gt;Info&lt;/p&gt;
    &lt;p&gt;We√¢re celebrating the launch of SedonaDB &amp;amp; SpatialBench with a special Apache Sedona Community Office Hour!&lt;/p&gt;
    &lt;p&gt;√∞ October 7, 2025&lt;/p&gt;
    &lt;p&gt;√¢¬∞ 8√¢9 AM Pacific Time&lt;/p&gt;
    &lt;p&gt;√∞ Online&lt;/p&gt;
    &lt;p&gt;√∞ Sign up here&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sedona.apache.org/latest/blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/"/><published>2025-09-24T16:00:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362254</id><title>New bacteria, and two potential antibiotics, discovered in soil</title><updated>2025-09-24T20:36:02.648143+00:00</updated><content>&lt;doc fingerprint="7c5460762d250ab9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Hundreds of new bacteria, and two potential antibiotics, found in soil&lt;/head&gt;
    &lt;p&gt;Most bacteria cannot be cultured in the lab‚Äîand that‚Äôs been bad news for medicine. Many of our frontline antibiotics originated from microbes, yet as antibiotic resistance spreads and drug pipelines run dry, the soil beneath our feet has a vast hidden reservoir of untapped lifesaving compounds.&lt;/p&gt;
    &lt;p&gt;Now, researchers have developed a way to access this microbial goldmine. Their approach, published in Nature Biotechnology, circumvents the need to grow bacteria in the lab by extracting very large DNA fragments directly from soil to piece together the genomes of previously hidden microbes, and then mines resulting genomes for bioactive molecules.&lt;/p&gt;
    &lt;p&gt;From a single forest sample, the team generated hundreds of complete bacterial genomes never seen before, as well as two new antibiotic leads. The findings offer a scalable way to scour unculturable bacteria for new drug leads‚Äîand expose the vast, uncharted microbial frontier that shapes our environment.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe finally have the technology to see the microbial world that have been previously inaccessible to humans,‚Äù says Sean F. Brady, head of the Laboratory of Genetically Encoded Small Molecules at Rockefeller. ‚ÄúAnd we‚Äôre not just seeing this information; we‚Äôre already turning it into potentially useful antibiotics. This is just the tip of the spear.‚Äù&lt;/p&gt;
    &lt;p&gt;Microbial dark matter&lt;/p&gt;
    &lt;p&gt;When hunting for bacteria, soil is an obvious choice. It‚Äôs the largest, most biodiverse reservoir of bacteria on the planet‚Äîa single teaspoon of it may contain thousands of different species. Many important therapeutics, including most of our antibiotic arsenal, were discovered in the tiny fraction of soil bacteria that can be grown in the laboratory. And soil is dirt cheap.&lt;/p&gt;
    &lt;p&gt;Yet we know very little about the millions of microbes packed into the earth. Scientists suspect that these hidden bacteria hold not only an untapped reservoir of new therapeutics, but clues as to how microbes shape climate, agriculture, and the larger environment that we live in. ‚ÄúAll over the world there‚Äôs this hidden ecosystem of microbes that could have dramatic effects on our lives,‚Äù Brady adds. ‚ÄúWe wanted to finally see them.‚Äù&lt;/p&gt;
    &lt;p&gt;Getting that glimpse involved weaving together several approaches. First, the team optimized a method for isolating large, high-quality DNA fragments directly from soil. Pairing this advance with emerging long-read nanopore sequencing allowed Jan Burian, a postdoctoral associate in the Brady lab, to produce continuous stretches of DNA that were tens of thousands of base pairs long‚Äî200 times longer than any previously existing technology could manage. Soil DNA contains a huge number of different bacteria; without such large DNA sequences to work with, resolving that complex genetic puzzle into complete and contiguous genomes for disparate bacteria proved exceedingly difficult.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs easier to assemble a whole genome out of bigger pieces of DNA, rather than the millions of tiny snippets that were available before,‚Äù Brady says. ‚ÄúAnd that makes a dramatic difference in your confidence in your results.‚Äù&lt;/p&gt;
    &lt;p&gt;Unique small molecules, like antibiotics, that bacteria produce are called ‚Äúnatural products‚Äù. To convert the newly uncovered sequences into bioactive molecules, the team applied a synthetic bioinformatic natural products (synBNP) approach. They bioinformatically predicted the chemical structures of natural products directly from the genome data and then chemically synthesized them in the lab. With the synBNP approach, Brady and colleagues managed to turn the genetic blueprints from uncultured bacteria into actual molecules‚Äîincluding two potent antibiotics.&lt;/p&gt;
    &lt;p&gt;Brady describes the method, which is scalable and can be adapted to virtually any metagenomic space beyond soil, as a three-step strategy that could kick off a new era of microbiology: ‚ÄúIsolate big DNA, sequence it, and computationally convert it into something useful.‚Äù&lt;/p&gt;
    &lt;p&gt;Two new drug candidates, and counting&lt;/p&gt;
    &lt;p&gt;Applied to their single forest soil sample, the team‚Äôs approach produced 2.5 terabase-pairs of sequence data‚Äîthe deepest long-read exploration of a single soil sample to date. Their analysis uncovered hundreds of complete contiguous bacterial genomes, more than 99 percent of which were entirely new to science and identified members from 16 major branches of the bacterial family tree.&lt;/p&gt;
    &lt;p&gt;The two lead compounds discovered could translate into potent antibiotics. One, called erutacidin, disrupts bacterial membranes through an uncommon interaction with the lipid cardiolipin and is effective against even the most challenging drug-resistant bacteria. The other, trigintamicin, acts on a protein-unfolding motor known as ClpX, a rare antibacterial target.&lt;/p&gt;
    &lt;p&gt;Brady emphasizes that these discoveries are only the beginning. The study demonstrates that previously inaccessible microbial genomes can now be decoded and mined for bioactive molecules at scale without culturing the organisms. Unlocking the genetic potential of microbial dark matter may also provide new insights into the hidden microbial networks that sustain ecosystems.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre mainly interested in small molecules as therapeutics, but there are applications beyond medicine,‚Äù Burian says. ‚ÄúStudying culturable bacteria led to advances that helped shape the modern world and finally seeing and accessing the uncultured majority will drive a new generation of discovery.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.rockefeller.edu/news/38239-hundreds-of-new-bacteria-and-two-potential-antibiotics-found-in-soil/"/><published>2025-09-24T16:03:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362425</id><title>Zed's Pricing Has Changed: LLM Usage Is Now Token-Based</title><updated>2025-09-24T20:36:02.382093+00:00</updated><content>&lt;doc fingerprint="bc0d159f852e9ffe"&gt;
  &lt;main&gt;
    &lt;p&gt;First things first: we still offer, and always plan to offer, multiple ways to use AI in Zed without paying us.&lt;/p&gt;
    &lt;p&gt;So what's changing? We're moving Zed AI from prompt-based limits to token-based pricing. For new users, this pricing is live today. For current users, we're providing advance notice and migration will happen over the next three months. We‚Äôre also adding GPT-5 (and mini/nano), Gemini 2.5 Pro, and Gemini 2.5 Flash to Zed‚Äôs hosted offering, in addition to the Anthropic models already available.&lt;/p&gt;
    &lt;p&gt;Why? We want our pricing to reflect the real cost of running AI. Token-agnostic prompt structures obscure the cost and are rife with misaligned incentives. This change lets us invest sustainably in the editor features that make Zed fast and reliable. Our short-term revenue growth strategy is to sell enterprise features to businesses, and our long term vision is to earn money by fundamentally improving how developers collaborate. We have to charge for AI features which we get charged for, but it's never been our plan to build a business on LLM token math.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's Changing&lt;/head&gt;
    &lt;head rend="h3"&gt;What's Changed&lt;/head&gt;
    &lt;p&gt;We've simplified our pricing and reduced costs while adding access to more AI models.&lt;/p&gt;
    &lt;head rend="h4"&gt;Key Improvements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;50% price reduction&lt;/item&gt;
      &lt;item&gt;Token-based billing&lt;/item&gt;
      &lt;item&gt;More AI models included&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;New Models&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GPT-5&lt;/item&gt;
      &lt;item&gt;Grok 4&lt;/item&gt;
      &lt;item&gt;Gemini 2.5&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Free&lt;/head&gt;
    &lt;head rend="h4"&gt;Old Pricing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2,000 accepted edit predictions&lt;/item&gt;
      &lt;item&gt;50 Zed-hosted prompts per month&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;New Pricing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2,000 accepted edit predictions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Pro&lt;/head&gt;
    &lt;head rend="h4"&gt;Old Pricing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unlimited accepted edit predictions&lt;/item&gt;
      &lt;item&gt;$20/month&lt;/item&gt;
      &lt;item&gt;500 prompts per month&lt;/item&gt;
      &lt;item&gt;Additional usage billed by prompt&lt;/item&gt;
      &lt;item&gt;Claude Sonnet and Claude Opus&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;New Pricing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unlimited accepted edit predictions&lt;/item&gt;
      &lt;item&gt;$10/month&lt;/item&gt;
      &lt;item&gt;$5 of token credits included&lt;/item&gt;
      &lt;item&gt;Additional usage billed at API list price +10%&lt;/item&gt;
      &lt;item&gt;Claude Sonnet, Claude Opus, GPT-5 + mini/nano, and Gemini 2.5 Pro/Flash&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Pro Trial&lt;/head&gt;
    &lt;head rend="h4"&gt;Old Pricing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Free for 14 days&lt;/item&gt;
      &lt;item&gt;Unlimited accepted edit predictions&lt;/item&gt;
      &lt;item&gt;150 prompts included&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;New Pricing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Free for 14 days&lt;/item&gt;
      &lt;item&gt;Unlimited accepted edit predictions&lt;/item&gt;
      &lt;item&gt;$20 of token credits included&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Old Pricing&lt;/head&gt;
    &lt;head rend="h2"&gt;New Pricing&lt;/head&gt;
    &lt;head rend="h2"&gt;Free&lt;/head&gt;
    &lt;head rend="h2"&gt;Pro&lt;/head&gt;
    &lt;head rend="h2"&gt;Pro Trial&lt;/head&gt;
    &lt;head rend="h2"&gt;A Wealth of Alternatives&lt;/head&gt;
    &lt;p&gt;Your Zed Pro bill may go up or down based on how you use AI. That's why we're giving Zed Pro customers 3 months notice before migration to the new pricing structure, and we have invested heavily in alternative options for using AI in Zed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bring your own API keys, direct with LLM providers like OpenAI, Anthropic, Grok, and many more&lt;/item&gt;
      &lt;item&gt;Use local models like Ollama&lt;/item&gt;
      &lt;item&gt;Access third-party agents through ACP, like Gemini CLI and Claude Code (more coming soon!)&lt;/item&gt;
      &lt;item&gt;Spend through GitHub Copilot, OpenRouter, AWS Bedrock, and other LLM payment hubs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also disable AI in Zed entirely if you prefer to keep your editor free of AI features.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why We're Making Changes&lt;/head&gt;
    &lt;p&gt;Current pricing is too expensive for Zed. LLM bills have become our biggest expense, and more paying customers translates to more money lost. We could spend more resources trying to find the right "unlimited" price point that would balance out costs, but it's much simpler to switch to the obvious pricing model of charging based on what providers charge us. This lets us keep our focus where it belongs: building the world's best code editor.&lt;/p&gt;
    &lt;p&gt;Prompt-based pricing is decoupled from user value. In our current system, asking Sonnet to fix a typo costs the same as a complex multi-file refactor. Token-based pricing means you pay for what you actually use, plus a 10% markup that covers Zed‚Äôs infrastructure, support costs, and the increased rate limits available for Zed Pro users compared to average BYOK users.&lt;/p&gt;
    &lt;p&gt;Better incentive alignment, reduced complexity. With token-based pricing, you can add as much or as little context as you want. The only limit is the cost you decide is worth it, not an arbitrary prompt cap. We ourselves would get confused about when ‚Äúburn mode‚Äù was needed, and what usage should or shouldn‚Äôt cost a prompt. It also simplifies the addition of new models to Zed Pro: we plan to maintain ‚Äúlist price + 10%‚Äù for any new model we add to Zed‚Äôs hosted offering.&lt;/p&gt;
    &lt;p&gt;Subsidize with specificity. Our old free plan subsidized inference without a concrete goal. Changing our pricing helps us focus resources on targeted programs like discounts for students that are better aligned with Zed‚Äôs mission. Every user can still use our 14-day trial to evaluate Zed Pro, even if you trialed Zed Pro in our old pricing structure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration Timeline&lt;/head&gt;
    &lt;p&gt;If you're an existing customer, you should already have received an email with your specific migration details. Here's the timeline again for reference:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pro customers have until December 17, 2025 to migrate, giving you three months to plan ahead. If you decide to cancel Zed Pro, you‚Äôll be moved to our new Free plan on the day your subscription ends. If you'd prefer to migrate earlier to access the new models, email [email protected] and we'll help you switch over.&lt;/item&gt;
      &lt;item&gt;Free users will transition to the new Free plan on October 15, 2025. You'll also get a fresh 14-day Pro trial with $20 in token credits to test out our new pricing model, even if you used a Pro trial in the past. You can start this trial any time, starting today. Note that starting this new Pro trial will move you to our new Free plan after its conclusion.&lt;/item&gt;
      &lt;item&gt;Trial users are being moved back to our old Free plan today, September 24th. You'll follow the same migration path as Free users above. You will also get a fresh trial that can be started any time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Looking for a better editor?&lt;/head&gt;
    &lt;p&gt;You can try Zed today on macOS or Linux. Download now!&lt;/p&gt;
    &lt;head rend="h3"&gt;We are hiring!&lt;/head&gt;
    &lt;p&gt;If you're passionate about the topics we cover on our blog, please consider joining our team to help us ship the future of software development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://zed.dev/blog/pricing-change-llm-usage-is-now-token-based"/><published>2025-09-24T16:13:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362486</id><title>Better Curl Saul: a lightweight API testing CLI focused on UX and simplicity</title><updated>2025-09-24T20:36:00.295652+00:00</updated><content>&lt;doc fingerprint="2d3c9d587d87d1a0"&gt;
  &lt;main&gt;
    &lt;code&gt;curl -X POST https://api.github.com/repos/owner/repo/issues \
  -H "Authorization: Bearer ghp_token123" \
  -H "Content-Type: application/json" \
  -H "Accept: application/vnd.github.v3+json" \
  -d '{
    "title": "Bug Report",
    "body": "Something is broken",
    "labels": ["bug", "priority-high"],
    "assignees": ["developer1", "developer2"]
  }'&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workspace-based - Each API gets its own organized folder&lt;/item&gt;
      &lt;item&gt;Smart variables - &lt;code&gt;{@token}&lt;/code&gt;persists,&lt;code&gt;{?name}&lt;/code&gt;prompts every time&lt;/item&gt;
      &lt;item&gt;Response filtering - Show only the fields you care about&lt;/item&gt;
      &lt;item&gt;Git-friendly - TOML files version control beautifully&lt;/item&gt;
      &lt;item&gt;Unix composable - Script it, pipe it, shell it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Supports: Linux, macOS, Windows (I hope)&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/DeprecatedLuar/better-curl-saul/main/install.sh | bash&lt;/code&gt;
    &lt;head&gt;Other Install Methods&lt;/head&gt;
    &lt;p&gt;Manual Install (boring)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download binary for your OS from releases&lt;/item&gt;
      &lt;item&gt;Make executable: &lt;code&gt;chmod +x saul-*&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Move to PATH: &lt;code&gt;sudo mv saul-* /usr/local/bin/saul&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From Source (for try-harders)&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/DeprecatedLuar/better-curl-saul.git
cd better-curl-saul
./other/install-local.sh  # Local development build&lt;/code&gt;
    &lt;p&gt;In case you already have Saul (hardcore)&lt;/p&gt;
    &lt;code&gt;saul set url https://raw.githubusercontent.com/DeprecatedLuar/better-curl-saul/main/install.sh &amp;amp;&amp;amp; saul call --raw | bash #(maybe works, who knows)&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;[!NOTE] Quick install auto-detects your system and downloads binaries or builds from source as fallback. Windows users: I don't know powershell I expect, just have bash üëç&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head&gt;Quick Start&lt;/head&gt;
    &lt;code&gt;# Create a test workspace
saul demo set url https://jsonplaceholder.typicode.com/posts/1
saul demo set method GET
saul demo call

# Try with variables
saul api set url https://httpbin.org/post
saul api set method POST
saul api set body name={?your_name} message="Hello from Saul"
saul api call

# Oh... yeah, for nesting just use dot notation like obj.field=idk&lt;/code&gt;
    &lt;head&gt;Core Commands&lt;/head&gt;
    &lt;p&gt;Alright so you can:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;set&lt;/code&gt;, &lt;code&gt;get&lt;/code&gt;, &lt;code&gt;edit&lt;/code&gt;, &lt;code&gt;rm&lt;/code&gt;&lt;lb/&gt;your&lt;code&gt;body&lt;/code&gt;, &lt;code&gt;header&lt;/code&gt;, &lt;code&gt;query&lt;/code&gt;, &lt;code&gt;request&lt;/code&gt;, &lt;code&gt;history&lt;/code&gt; or maybe even
&lt;code&gt;response&lt;/code&gt;
&lt;lb/&gt;also&lt;code&gt;url&lt;/code&gt;, &lt;code&gt;method&lt;/code&gt;, &lt;code&gt;timeout&lt;/code&gt;, &lt;code&gt;history&lt;/code&gt;&lt;/p&gt;
    &lt;head&gt;Example&lt;/head&gt;
    &lt;code&gt;# Configure your API workspace (or preset, same thing)
saul [workspace] set url https://api.example.com
saul set method POST
saul set header Authorization="Bearer {@token}"
saul set body user.name={?username} user.email=john@test.com

# Execute the request
saul call

# Check your configuration, note that preset/workspace name keeps
# stored in memory after first mention on syntax

saul [anoter_workspace] check url
saul check body

# View response history
saul check history&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;There are 2 variable types&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;soft variables {?} prompt you at EVERY call&lt;/item&gt;
      &lt;item&gt;hard variables {@} require manual update by running the flag -v or running &lt;code&gt;saul set variable varname value&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start watchin Better Call Saul&lt;/item&gt;
      &lt;item&gt;Think of a bad joke&lt;/item&gt;
      &lt;item&gt;Workspace-based configuration&lt;/item&gt;
      &lt;item&gt; Smart variable system (&lt;code&gt;{@}&lt;/code&gt;/&lt;code&gt;{?}&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;In line terminal field editing&lt;/item&gt;
      &lt;item&gt;Response filtering&lt;/item&gt;
      &lt;item&gt;Response history&lt;/item&gt;
      &lt;item&gt;Terminal session memory&lt;/item&gt;
      &lt;item&gt;Bulk operations&lt;/item&gt;
      &lt;item&gt;Fix history response parsing and filtering&lt;/item&gt;
      &lt;item&gt;GET specific response stuff from history (aka Headers/Body...)&lt;/item&gt;
      &lt;item&gt;Flags, we've got none basically&lt;/item&gt;
      &lt;item&gt;Stateless command support&lt;/item&gt;
      &lt;item&gt;Support pasting raw JSON template&lt;/item&gt;
      &lt;item&gt;User config system using the super cool github.com/DeprecatedLuar/toml-vars-letsgooo library&lt;/item&gt;
      &lt;item&gt;Add the eastereggs&lt;/item&gt;
      &lt;item&gt;Polish code&lt;/item&gt;
      &lt;item&gt;Actual Documentation&lt;/item&gt;
      &lt;item&gt;Touch Grass (not a priority)&lt;/item&gt;
      &lt;item&gt;Think of more features&lt;/item&gt;
      &lt;item&gt;Think of even more features&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Beta software - Core features work, documentation in progress.&lt;/p&gt;
    &lt;p&gt;Bug or feedback? I will be very, very, very happy if you let me know your thoughts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DeprecatedLuar/better-curl-saul"/><published>2025-09-24T16:17:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362569</id><title>Product Hunt is dead</title><updated>2025-09-24T20:35:59.895024+00:00</updated><content>&lt;doc fingerprint="3882b9fdc974d862"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Product Hunt is Dead&lt;/head&gt;
    &lt;p&gt;First, the good news.&lt;/p&gt;
    &lt;p&gt;It's been one week since FinFam's beta launch! The Show HN post trended nicely, netting enough eyeballs to make me confident that FinFam is the world's first and only collaborative financial planner with a marketplace of interactive, open-source expert opinions. I'm especially gratified by the users I'm meeting through the product. Nothing like it.&lt;/p&gt;
    &lt;p&gt;So, launch is going great, no regrets, right?&lt;/p&gt;
    &lt;head rend="h3"&gt;My one regret&lt;/head&gt;
    &lt;p&gt;That brings us to the subject of today's PSA.&lt;/p&gt;
    &lt;p&gt;Product Hunt is dead.&lt;/p&gt;
    &lt;p&gt;I wasn't planning this post. PH wasn't even much of a launch priority for FinFam. But after seeing what I saw, I knew this had to skip the queue. The world had to know.&lt;/p&gt;
    &lt;p&gt;After all, my launch post on LinkedIn mentioned our ProductHunt launch. And now I'm cringing thinking about how I even sent an email out to few product-oriented friends linking them to our launch, perpetuating the myth.&lt;/p&gt;
    &lt;p&gt;Hours later I would realize that Product Hunt is sadly no more. Gone was the site I knew from my days on Stripe Invoicing. What's left is a husk, active in appearance alone.&lt;/p&gt;
    &lt;head rend="h3"&gt;I missed the memo&lt;/head&gt;
    &lt;p&gt;Turns out this has been happening for a while. Just last year, Fabian Maume asked, "Is Product Hunt Dying?" He's got lots of data and background, so I'll stick to filling in the now-obvious answer: Yes. Product Hunt is dead.&lt;/p&gt;
    &lt;p&gt;And Fabian's not alone. A quick search will reveal dozens of nails in the coffin. I guess that's inevitable when the founder exits and in 2022 a16z merges your mature platform with a crypto venture that no one remembers.&lt;/p&gt;
    &lt;p&gt;But how does a dead platform appear to live on?&lt;/p&gt;
    &lt;head rend="h3"&gt;The Zombie Grift&lt;/head&gt;
    &lt;p&gt;Product Hunt has a weird quirk where it resets every day at midnight Pacific time. Unlike Hacker News, Reddit, etc., PH doesn't have a rolling front page. This fixed daily scheduling idiosyncrasy leads to all-nighters as launch best practice, and systemically, this means a platform originating in Silicon Valley is unlikely to have its front page content meaningfully decided by anyone in the western hemisphere.&lt;/p&gt;
    &lt;p&gt;Much like with Hacker News, the first few hours of a post determine its impact. Instead, Europe, APAC, and in particular India have an outsized influence.&lt;/p&gt;
    &lt;p&gt;So what really happens when you launch on Product Hunt?&lt;/p&gt;
    &lt;p&gt;Well, your LinkedIn inbox turns into this:&lt;/p&gt;
    &lt;p&gt;I was taken by surprise. What hurt the most was these midnight solicitors sharing screenshots of success stories from companies I recognized. They'd been instrumental in "launching" apps that I respect, and I'd hoped they wouldn't have to stoop to this. I even had personal connections to some of these founders.&lt;/p&gt;
    &lt;p&gt;It was 4am, but I put on my investigative hat and I engaged with a couple. Here's how their process looks:&lt;/p&gt;
    &lt;p&gt;$100 is all it takes to make it into the Top 5 for a weekday. One has to admit, it's tempting. If you've spent months building, $100 feels like nothing.&lt;/p&gt;
    &lt;p&gt;It is nothing. These aren't real users and PH's audience has never been a source of sticky users. $100 is too much to spend on vanity. And it's predatory to foster a "community" where clout peddlers can prey on susceptible, good-faith founders.&lt;/p&gt;
    &lt;p&gt;If you're curious, you can see the paid votes landing via spikes in upvote speed on hunted.space. It's not hard to eyeball products which get more upvotes in the first two hours than they do in the next twenty-two.&lt;/p&gt;
    &lt;p&gt;Suffice to say I didn't get any emails or LinkedIn invites from HN vote peddlers, despite HN sending us more than 10x the traffic.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can Product Hunt be revived?&lt;/head&gt;
    &lt;p&gt;To be fair, PH tries to mitigate front page manipulation. They "feature" certain launches to curate the front page. The main outcome is that the majority of launches are simply never shown to most users. No non-featured launches appear on the mobile app. The process is documented, but still opaque and inconsistently applied. Almost certainly ties into revenue somehow.&lt;/p&gt;
    &lt;p&gt;A better question is "Should Product Hunt be revived?"&lt;/p&gt;
    &lt;p&gt;This is far from PH's only problem. They've killed Ship and other features without replacements.&lt;/p&gt;
    &lt;p&gt;At the crux, I just don't think a "launch" or a "product" is enough to tie together a community to develop a healthy ecosystem. The focus on the new draws a fast flow of products and builders that erodes the core community.&lt;/p&gt;
    &lt;p&gt;Alternatives exist, but if Product Hunt suffers from the above, I suspect these do, too:&lt;/p&gt;
    &lt;p&gt;Edit: Someone even made a directory of directories. Early reports are not promising!&lt;/p&gt;
    &lt;p&gt;Contrast this with Indie Hackers, which is united by at least one value / work ethic.&lt;/p&gt;
    &lt;p&gt;Or contrast to one of my personal faves: AlternativeTo, which takes a wiki approach toward the mission of cataloging all software, not just the newest.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goodbye Product Hunt&lt;/head&gt;
    &lt;p&gt;I guess if this ends up being PH's epitaph I should get this out of my system:&lt;/p&gt;
    &lt;p&gt;Google Glass Kitty has always been a terrible mascot.&lt;/p&gt;
    &lt;p&gt;The obvious choice for an iconic hunt has always been the duck:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sedimental.org/product_hunt_is_dead.html"/><published>2025-09-24T16:23:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362697</id><title>Terence Tao: The role of small organizations in society has shrunk significantly</title><updated>2025-09-24T20:35:59.157853+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mathstodon.xyz/@tao/115259943398316677"/><published>2025-09-24T16:32:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362703</id><title>Waymo for Business</title><updated>2025-09-24T20:35:58.866657+00:00</updated><content>&lt;doc fingerprint="7a296058b0b79cd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Waymo for Business: Trusted Daily Rides for Organizations&lt;/head&gt;
    &lt;p&gt;We're introducing Waymo for Business and inviting organizations to sign up to explore the benefits of our fully autonomous ride-hailing service. After conducting successful pilots with a variety of organizations, we're ready to partner with more employers, universities, and event organizers to help streamline their transportation needs.&lt;/p&gt;
    &lt;p&gt;With over a million trips every month, Waymo has become the go-to mobility option for many people in our service areas. In fact, nearly 1 in 6 of our local riders in San Francisco, Los Angeles, and Phoenix rely on Waymo to commute to work or school, making it an integral part of their daily routine. This includes all types of schedules from early morning rush to students, shift workers, and healthcare staff who need a trusted ride home late at night. With Waymo for Business, we‚Äôre bringing that same 24/7 reliability and convenience directly to organizations.&lt;/p&gt;
    &lt;p&gt;Benefits for Your Organization&lt;/p&gt;
    &lt;p&gt;Partnering with Waymo for Business can help you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Provide a Premium Experience: Offer your employees, students, or guests a consistent and reliable way to get around. Waymo‚Äôs fully autonomous rides allow people to reclaim their time, whether they need to catch up on emails, relax, or make a private call.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Meet Sustainability Goals: Waymo‚Äôs fully electric fleet helps organizations advance their sustainability targets.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lead With Innovation: Embrace cutting-edge technology to expand mobility access, particularly for those in your community who can't drive or don‚Äôt have access to a vehicle.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We've already partnered with some of the most innovative employers, universities, and event organizers in our service areas to pilot Waymo for Business.&lt;/p&gt;
    &lt;p&gt;Manage Your Transportation Program&lt;/p&gt;
    &lt;p&gt;Waymo‚Äôs enterprise-grade tools make it easy to manage your transportation program:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Business Portal: Easily set program parameters and manage users&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customizable Promo Codes: Seamlessly manage rides for events&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reporting Tools: Track your budget and monitor ride activity&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are actively exploring how our service can best support organizations of all sizes. While we‚Äôre still in the early stages, we‚Äôre committed to evolving our business offerings to better meet the needs of a wide range of business over time.&lt;/p&gt;
    &lt;p&gt;Learn more about Waymo for Business and sign up for the interest list here to get in touch with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://waymo.com/blog/2025/09/waymo-for-business"/><published>2025-09-24T16:32:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362813</id><title>Show HN: Vibe Linking</title><updated>2025-09-24T20:35:58.718242+00:00</updated><content/><link href="https://vb.lk/"/><published>2025-09-24T16:40:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362914</id><title>Launch HN: Flywheel (YC S25) ‚Äì Waymo for Excavators</title><updated>2025-09-24T20:35:58.601766+00:00</updated><content>&lt;doc fingerprint="2c7449d6851a3652"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, We're Jash and Mahimana, cofounders of Flywheel AI (&lt;/p&gt;https://useflywheel.ai&lt;p&gt;). We‚Äôre building a remote teleop and autonomous stack for excavators.&lt;/p&gt;&lt;p&gt;Here's a video: https://www.youtube.com/watch?v=zCNmNm3lQGk.&lt;/p&gt;&lt;p&gt;Interfacing with existing excavators for enabling remote teleop (or autonomy) is hard. Unlike cars which use drive-by-wire technology, most of the millions of excavators are fully hydraulic machines. The joysticks are connected to a pilot hydraulic circuit, which proportionally moves the cylinders in the main hydraulic circuit which ultimately moves the excavator joints. This means excavators mostly do not have an electronic component to control the joints. We solve this by mechanically actuating the joysticks and pedals inside the excavators.&lt;/p&gt;&lt;p&gt;We do this with retrofits which work on any excavator model/make, enabling us to augment existing machines. By enabling remote teleoperation, we are able to increase site safety, productivity and also cost efficiency.&lt;/p&gt;&lt;p&gt;Teleoperation by the operators enables us to prepare training data for autonomy. In robotics, training data comprises observation and action. While images and videos are abundant on the internet, egocentric (PoV) observation and action data is extremely scarce, and it is this scarcity that is holding back scaling robot learning policies.&lt;/p&gt;&lt;p&gt;Flywheel solves this by preparing the training data coming from our remote teleop-enabled excavators which we have already deployed. And we do this with very minimal hardware setup and resources.&lt;/p&gt;&lt;p&gt;During our time in YC, we did 25-30 iterations of sensor stack and placement permutations/combinations, and model hyperparams variations. We called this ‚Äúevolution of the physical form of our retrofit‚Äù. Eventually, we landed on our current evolution and have successfully been able to train some levels of autonomy with only a few hours of training data.&lt;/p&gt;&lt;p&gt;The big takeaway was how much more important data is than optimizing hyperparams of the model. So today, we‚Äôre open sourcing 100hrs of excavator dataset that we collected using Flywheel systems on real construction sites. This is in partnership with Frodobots.ai.&lt;/p&gt;&lt;p&gt;Dataset: https://huggingface.co/datasets/FlywheelAI/excavator-dataset&lt;/p&gt;&lt;p&gt;Machine/retrofit details:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;  Volvo EC380 (38 ton excavator)
  4xcamera (25fps)
  25 hz expert operator‚Äôs action data
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; The dataset contains observation data from 4 cameras and operator's expert action data which can be used to train imitation learning models to run an excavator autonomously for the workflows in those demonstrations, like digging and dumping. We were able to train a small autonomy model for bucket pick and place on Kubota U17 from just 6-7 hours of data collected during YC.&lt;/p&gt;&lt;p&gt;We‚Äôre just getting started. We have good amounts of variations in daylight, weather, tasks, and would be adding more hours of data and also converting to lerobot format soon. We‚Äôre doing this so people like you and me can try out training models on real world data which is very, very hard to get.&lt;/p&gt;&lt;p&gt;So please checkout the dataset here and feel free to download and use however you like. We would love for people to do things with it! I‚Äôll be around in the thread and look forward to comments and feedback from the community!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45362914"/><published>2025-09-24T16:48:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45364450</id><title>How fast is Go? simulating particles on a smart TV</title><updated>2025-09-24T20:35:58.419930+00:00</updated><content>&lt;doc fingerprint="66bcdd582b200d99"&gt;
  &lt;main&gt;
    &lt;p&gt;The challenge, simulate millions of particles in golang, multi-player enabled, cpu only, smart tv compatible.&lt;/p&gt;
    &lt;p&gt;Let's go, pun very much intended.&lt;/p&gt;
    &lt;p&gt;So, during my day job I had to write a ws server which merged multiple upstream ws servers into a single ws server. (Don't ask) I was lost and not even the power of claude, gemini, and cursor could save me. The vibes were simply not enough to get the project done. I had to learn the real real stuff.&lt;/p&gt;
    &lt;p&gt;To learn the real stuff I decided to write a particle simulation and see how many particles I could get golang to push. I had already done this in the lands of javascript, rust, and swift. But given Go doesn't support simd, I knew it wouldn't be a fair fight against the other languages and more importantly, it would be boring let alone helping my day job.&lt;/p&gt;
    &lt;p&gt;I figured to give go the best shake I needed to up the difficulty by adding multi-player to the simulation. After all, go is best known as a snappy productive backend language. Is it snappy and productive enough for simulating a million particles and syncing it across hundreds of clients?&lt;/p&gt;
    &lt;p&gt;Only one way to find out.&lt;/p&gt;
    &lt;p&gt;There is a rule and an important one. No client simulation allowed only server. The client should be a simple web page. It should work anywhere browser runs.&lt;/p&gt;
    &lt;p&gt;Determinism. This is a key word in computer science and it is here too. If you start with the same initial state and apply the same input, you will always get the same result. Predictable and reproducible.&lt;/p&gt;
    &lt;p&gt;Many multi-player games use determinism to effectively decouple the relationship of bigger game states from the data the server must sync across clients.&lt;/p&gt;
    &lt;p&gt;Real-time strategy games are a poster child for this. Rather than send the positions of thousands of units, projectiles, unit health, etc to clients, you would send only player input data which the client can use to derive the game state at a given point in time. Add a little bit of client side prediction, rollback, and the like and you get a smooth game experience, usually.&lt;/p&gt;
    &lt;p&gt;But this requires the client to be able to simulate the game and not all clients are fast. How can this run everywhere a browser does if I want to simulate millions of particles?&lt;/p&gt;
    &lt;p&gt;I know what the ol'gafferongames would say right now. Even if I didn't use determinism I'd still need to simulate SOMETHING on the client. If I want to send millions of particle positions to the client, surely I'd need to send at least the positions right? I could derive the velocities and quantize the crap out of everything too. I am sure there are other tricks I don't remember from his excellent blog series on game state networking code.&lt;/p&gt;
    &lt;p&gt;I still call that cheating too as the client has to simulate something and it may not scale to millions of particles.&lt;/p&gt;
    &lt;p&gt;I have another idea. I can decouple the simulation size much like determinism does by taking a hint from graphics programming.&lt;/p&gt;
    &lt;p&gt;Way back when in the days of Doom 3 when Mr. Carmack was in his prime, games would calculate lighting based on building shadow geometry from polygons. This was done in Doom 3 and looked amazing. However, as you increased polygon count, the cost of shadows would go up.&lt;/p&gt;
    &lt;p&gt;The industry then figured out how to decouple polygon count from lighting by using a graphics buffer and deferred shading. The details are heavy but the important part is that the cost of lighting is no longer proportional to number of polygons in the scene. Instead, it is based on a ‚Äúfixed‚Äù g-buffer size. The buffer is proportional to the render resolution.&lt;/p&gt;
    &lt;p&gt;This is why 4k is so expensive to render and the games industry has stoked on AI upscaling and frame interpolation. Fewer pixels means faster rendering. Geometry is making a come back with slick virtualization too but I digress. The important part is that deferred shading decouples polygons count from lighting.&lt;/p&gt;
    &lt;p&gt;Well, what if, I went full SSR and did all the rendering on the server sending the simulation frames to the clients? All a client needs to do is just, play a video onto an html canvas element. I would still need to send the input to the server but the size of the simulation wouldn't matter. The cost would be fixed to the resolution of the client.&lt;/p&gt;
    &lt;p&gt;I could have a billion particles and the data the client needs would remain the same. Now, if there were only a few thousand particles it wouldn't be worth the trade off.&lt;/p&gt;
    &lt;p&gt;How do I know it is worth it though?&lt;/p&gt;
    &lt;p&gt;I want to simulate 1 million particles at HD resolution so 1920x1080 pixels. There are 4 bytes per pixel rgba but I only need rgb. Actually, I only need one byte per pixel since that is how the previous particle simulations in the other languages worked. That means I am sending 2,073,600 bytes per frame or a little over 2mb. Oof. That is 120mb/s at 60 fps.&lt;/p&gt;
    &lt;p&gt;If I were to compress the frames they could drop down to a fraction of that. H264 or H265 (if I pay) could cut that down to 260kb per frame. I could also send only 24 frames/s making it pretty close to streaming regular video content.&lt;/p&gt;
    &lt;p&gt;Note were are talking full bytes here not bits.&lt;/p&gt;
    &lt;p&gt;If I were to send the particle data instead, each particle is made up of an x, y, dx, and dy. They are floating point numbers at 4 bytes each so a particle takes up 16 bytes. I could, derive the dx and dy so I will say 8 bytes per particle. That makes it 8,000,000 bytes or 8mb. 4x as big as the raw frame buffer. I am sure with a bit of quantization that could be brought down more and I wouldn't need to send the data 60/s but what if I want 10 million particles? The frame buffer starts to look more appealing.&lt;/p&gt;
    &lt;p&gt;The other thing that is nice here is that by sending the frames, the complexity drops. I don't need to worry about prediction or interpolation on the client. This wouldn't be great for a twitch shooter but they also don't sync millions of data points either.&lt;/p&gt;
    &lt;p&gt;No lossy compression. A particle simulation like this gets destroyed by lossy compression due to how dense the information is. There is little repitition in the data so HEVC or other lossy codecs will wreck it. Lossy compression is out of the question. It HAS to be lossless.&lt;/p&gt;
    &lt;p&gt;For example, look at this image, see all that noise? That doesn't compress well especially in motion.&lt;/p&gt;
    &lt;p&gt;Additionally, compression isn't free. If I want to scale to hundreds of clients, I cannot spend all my time compressing data. This is an important consideration.&lt;/p&gt;
    &lt;p&gt;I am going to use TCP via websockets. For low latency realtime apps UDP would be better as it allows unordered messages preventing head of line blocking. However, it is significantly more complicated and not well supported on the web. TCP's guaranteed ordering of messages will cause some lag spikes but alas it is the best the web has to offer. QUIC is a thing however it only helps the blocking when there are multiple connections which this won't have.&lt;/p&gt;
    &lt;p&gt;TCP websockets it is. Onwards!&lt;/p&gt;
    &lt;p&gt;My end game is to have a configurable play area where each client has its own view into the world. The best starting point would be to have a single view that is sent to all the clients.&lt;/p&gt;
    &lt;p&gt;The simulation itself is pretty simple. A particle struct distributing updates to many go routines. I knew that I couldn't write to a buffer while sending it to a client so I setup double buffering of the frames. A &lt;code&gt;ticker&lt;/code&gt; is the way to get a game loop going though I am not a fan of it.&lt;/p&gt;
    &lt;p&gt;Here is the gist.&lt;/p&gt;
    &lt;code&gt;type Particle struct {
  x  float32
  y  float32
  dx float32
  dy float32
}

type Input struct {
  X           float32
  Y           float32
  IsTouchDown bool
}

type SimState struct {
  dt     float32
  width  uint32
  height uint32
}

// setup variables

func startSim() {
  // setup code

  for range ticker.C {
    // metadata code

    wg.Add(numThreads)
    // no locking, that is fine.
    numClients := len(clients)
    const friction = 0.99

    for i := 0; i &amp;lt; numThreads; i++ {
      go func(threadID int) {
        defer wg.Done()

        startIndex := threadID * particlesPerThread
        endIndex := startIndex + particlesPerThread

        if threadID == numThreads-1 {
          endIndex = particleCount
        }

        for p := startIndex; p &amp;lt; endIndex; p++ {
          for i := 0; i &amp;lt; numClients; i++ {
            input := inputs[i]
            if input.IsTouchDown {
              // apply gravity
            }
          }

          particles[p].x += particles[p].dx
          particles[p].y += particles[p].dy
          particles[p].dx *= friction
          particles[p].dy *= friction

          // bounce if outside bounds
        }
      }(i)
    }

    // wait for them to complete
    wg.Wait()

    framebuffer := getWriteBuffer()
    copy(framebuffer, bytes.Repeat([]byte{0}, len(framebuffer)))
    for _, p := range particles {
      // build frame buffer
    }
    swapBuffers()

    go func(data []byte) {
      // Non-blocking send: if the channel is full, the oldest frame is dropped
      select {
      case fameChannel &amp;lt;- framebuffer:
      default:
        // Channel is full, drop the frame
      }
    }(framebuffer)
  }
}
&lt;/code&gt;
    &lt;p&gt;A few notes. The simulation is basic. Particles can be pulled around by players slowing down by some friction value.&lt;/p&gt;
    &lt;p&gt;I want to avoid locking as much as possible. This means I am going to try and create a fixed amount of memory based the max number of clients I can handle then use the clients index as a fast way to access their respective data, in this case input data.&lt;/p&gt;
    &lt;p&gt;I also want to avoid writing a frame buffer per client as I know that writing to the frame buffer is not cache friendly and should only be done once. As a matter of fact, building the frame buffer is the most expensive part. I learned this from previous languages. Cache locality and all that.&lt;/p&gt;
    &lt;p&gt;One thing I learned about go is that it can be verbose. Not java levels but it is pretty lengthy.&lt;/p&gt;
    &lt;p&gt;I will skip the boiler plate code.&lt;/p&gt;
    &lt;code&gt;func main() {
  go startSim()

  http.HandleFunc("/ws", wsHandler)
  // for webpage
  http.Handle("/", http.FileServer(http.Dir("./public")))

  log.Println("Server started on :8080")
  if err := http.ListenAndServe(":8080", nil); err != nil {
    log.Fatal("ListenAndServe:", err)
  }
}
&lt;/code&gt;
    &lt;p&gt;Initially, each websocket spins up their own ticker writing the latest frame buffer to the client. This is ugly and has sync issues but as a first version it is fine.&lt;/p&gt;
    &lt;code&gt;func wsHandler(w http.ResponseWriter, r *http.Request) {
  // boiler plate

  clientsMu.Lock()
  // safely add client
  clientsMu.Unlock()

  defer func() {
    // safely clean up
  }()

  go func() {
    // wait for messages
    var input Input
    err := binary.Read(bytes.NewReader(message), binary.LittleEndian, &amp;amp;input)

    // maybe later we lock or figure out way to not need a lock in a hot path
    // this is fine as only chance of bad data is if someone connect or drops whilst updating input
    idx := findClientIndex(conn)
    if idx != -1 &amp;amp;&amp;amp; idx &amp;lt; maxClients {
      inputs[idx] = input
    }
  }()

  ticker := time.NewTicker(time.Second / 30)
  for range ticker.C {
    data := getReadBuffer()
    if err := conn.WriteMessage(websocket.BinaryMessage, data); err != nil {
      log.Println("Write failed:", err)
      break
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The client index is pretty silly but I really don't want to lock the hot path here.&lt;/p&gt;
    &lt;p&gt;I'll skip over the client javascript/html. It isn't that interesting, setup a web socket, write data to a canvas element using &lt;code&gt;setPixels&lt;/code&gt; the usually.&lt;/p&gt;
    &lt;p&gt;So, how well does it work? Pretty well. But it is not fast, and is writing a shit load of data. It also sometimes flickers when the tickers touch the frame buffer at the same time the simulation thread does.&lt;/p&gt;
    &lt;p&gt;Still, a good starting point.&lt;/p&gt;
    &lt;p&gt;I used &lt;code&gt;pprof&lt;/code&gt; to run some samples to see where things are slow. I noticed that time was spent creating go routines. While light they are not free. The idea is to have go routines listening in on channels. I refactored the simulation to wait for &lt;code&gt;SimJob&lt;/code&gt; requests to come in.&lt;/p&gt;
    &lt;code&gt;type SimJob struct {
  startIndex int
  endIndex   int
  simState   SimState
  inputs     [maxClients]Input
  numClients int
}

// other code

jobs := make(chan SimJob, numThreads)
var wg sync.WaitGroup
for i := 0; i &amp;lt; numThreads; i++ {
  go worker(jobs, &amp;amp;wg)
}

// in sim loop
for i := 0; i &amp;lt; numThreads; i++ {
  startIndex := i * particlesPerThread
  endIndex := startIndex + particlesPerThread
  if i == numThreads-1 {
    endIndex = particleCount
  }
  jobs &amp;lt;- SimJob{startIndex, endIndex, simState, &amp;amp;inputs, numClients, &amp;amp;framebuffer}
}
&lt;/code&gt;
    &lt;p&gt;I also reused frame buffers with a &lt;code&gt;pool&lt;/code&gt; and then fixed the syncing by pushing to a channel which another thread would listen on. This thread would then write the new frame out to all the clients before adding it back to the pool. At first I used a simple for loop but that made it write as fast as the slowest client so instead I had it prep the frame and then push it to ANOTHER set of channels per client. Threads on each of those channels would then actually write data to the client.&lt;/p&gt;
    &lt;code&gt;func broadcastFrames(ch &amp;lt;-chan *Frame, pool *sync.Pool) {
  for {
    frame := &amp;lt;-ch
    // setup data
    clientsMu.Lock()
    for i, conn := range clients {
      // send to other channel
      select {
      case clientSendChannelMap[conn] &amp;lt;- dataToSend:
      default:
        log.Printf("Client %d's channel is full, dropping frame. Requesting full frame.", i)
      }
    }
    clientsMu.Unlock()
    pool.Put(frame)
  }
}

// other code
func writePump(conn *websocket.Conn) {
  var channel = clientSendChannelMap[conn]
  for {
    message, ok := &amp;lt;-channel
    if !ok {
      return
    }

    if err := conn.WriteMessage(websocket.BinaryMessage, message); err != nil {
      log.Printf("Write to client failed: %v", err)
      return
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;I pump when a client connects and stop when they disconnect. This works pretty well and fixes the sync issue.&lt;/p&gt;
    &lt;p&gt;I experimented with compression a bit using the standard &lt;code&gt;flate&lt;/code&gt; lib. It worked but was pretty slow. I then experimented with every kind of encoding I could think of to reduce the data transferred.&lt;/p&gt;
    &lt;p&gt;I tried a bit mask where I would sent 0/1 flags per pixel followed by the in order pixel data that changed. This was fast per client but ended up being larger than the raw frame data when particles started moving around.&lt;/p&gt;
    &lt;p&gt;I tried run length encoding, variable length encoding, and delta encoding all combined in different ways.&lt;/p&gt;
    &lt;p&gt;There are some gotchas I ran into For RLE to work you need a sentinel byte to know if there is a run of data. This drops the available space per pixel. It could be possible to encode multiple pixels at a time but that didn't work that well. VLE is just run length using varints to drop the size down for small numbers this worked and did help reduce size by almost 8% on average.&lt;/p&gt;
    &lt;p&gt;Delta encoding was the most fun. The way I tried it was to delta encode between frames. That way passing the delta into RLE would massively compress the data. The issue is that particle counts can go up or down per pixel. This required &lt;code&gt;zigzag&lt;/code&gt; encoding but there was an issue.&lt;/p&gt;
    &lt;p&gt;If each pixel is one byte, and the byte can go from 0 to 255 in a single frame, then the size of the deltas would need to go from -255 to +255 which doesn't fit in a byte. The &lt;code&gt;zigzag&lt;/code&gt; encoding worked and I figured I could just drop down to 7-bits per pixel.&lt;/p&gt;
    &lt;p&gt;I read about zigzag encoding from a microsoft paper on optimized compression for depth data from multiple connect sensors. I don't remember the name. To save you some time, just ask AI about it.&lt;/p&gt;
    &lt;code&gt;func CreateDeltaBuffer(oldBuffer, newBuffer []byte) []byte {
  if len(oldBuffer) != len(newBuffer) {
    return newBuffer
  }
  deltaBuffer := make([]byte, len(newBuffer))

  for i := 0; i &amp;lt; len(newBuffer); i++ {
    diff := int16(newBuffer[i]) - int16(oldBuffer[i])
    deltaBuffer[i] = zigZagEncode(int8(diff))
  }

  return deltaBuffer
}
&lt;/code&gt;
    &lt;p&gt;And it works. Unless the whole frame was moving around, the size was ~30% smaller than the raw frame data. If nothing moved, it would send only a few bytes! But again, there were problems.&lt;/p&gt;
    &lt;p&gt;The code is complicated. I had to track if I was sending delta frames or full frames, and if a client dropped a frame, I'd need to send a full frame again. This would cause a brief flash on the screen. I could add frame counting logic so the client could drop out of sync deltas but the code was already a bit much.&lt;/p&gt;
    &lt;code&gt;func broadcastFrames(ch &amp;lt;-chan *Frame, pool *sync.Pool) {
  for {
    frame := &amp;lt;-ch
    fullFrameBuffer.Reset()
    fullFrameBuffer.WriteByte(OpCodeFullFrame)
    fullFrameBuffer.Write(frame.FullBuffer)
    fullFrameBytes := fullFrameBuffer.Bytes()

    deltaFrameBuffer.Reset()
    deltaFrameBuffer.WriteByte(OpCodeDeltaFrame)
    deltaFrameBuffer.Write(frame.Delta)
    deltaFrameBytes := deltaFrameBuffer.Bytes()

    clientsMu.Lock()
    for i, conn := range clients {

      var dataToSend []byte
      if clientFullFrameRequestFlags[i] {
        dataToSend = fullFrameBytes
        clientFullFrameRequestFlags[i] = false
      } else {
        dataToSend = deltaFrameBytes
      }

      select {
      case clientSendChannelMap[conn] &amp;lt;- dataToSend:
      default:
        log.Printf("Client %d's channel is full, dropping frame. Requesting full frame.", i)
        clientFullFrameRequestFlags[i] = true
      }
    }
    clientsMu.Unlock()
    pool.Put(frame)
  }
}
&lt;/code&gt;
    &lt;p&gt;It does work though.&lt;/p&gt;
    &lt;p&gt;However, I want each client to have their own ‚Äúview‚Äù into a bigger frame where the can pan around the world fighting for particles. For this to work, with the current setup, I'd need to track delta frames per client and figure out how to handle dropped delta frames better.&lt;/p&gt;
    &lt;p&gt;Back to the drawing board.&lt;/p&gt;
    &lt;p&gt;Stupid simple, is simple stupid.&lt;/p&gt;
    &lt;p&gt;I am going to have 1 bit per pixel. That means only a single ‚Äúluminance‚Äù value. In the previous video I was only effectively using a few bits on the client having only about 8 ‚Äúluminance‚Äù values. Bet you thought it was more right?&lt;/p&gt;
    &lt;p&gt;This will simplify everything. The only hard part is packing and unpacking the data. I also figured I could implement client camera state too. I waffled a bit around how to store this state and update it.&lt;/p&gt;
    &lt;p&gt;Does the client sent their camera position directly? What if the window resizes? What if the client sends a negative camera size? What if they send an infinite one? How do I prevent blocking?&lt;/p&gt;
    &lt;p&gt;I ended up copying what I did for the input data.&lt;/p&gt;
    &lt;code&gt;type ClientCam struct {
  X      float32
  Y      float32
  Width  int32
  Height int32
}
var (
  inputs     [maxClients]Input
  cameras    [maxClients]ClientCam
)
&lt;/code&gt;
    &lt;p&gt;I would have the client send the camera data with the existing input message.&lt;/p&gt;
    &lt;code&gt;  for {
    mt, message, err := conn.ReadMessage()
    if err != nil {
      log.Println("Read failed:", err)
      return
    }
    if mt == websocket.BinaryMessage {
      reader := bytes.NewReader(message)
      var touchInput Input
      // interpret x and y as offsets.
      var camInput ClientCam
      // read input
      errInput := binary.Read(reader, binary.LittleEndian, &amp;amp;touchInput)
      errCam := binary.Read(reader, binary.LittleEndian, &amp;amp;camInput)
      // set client input/camera state
      // handle errors in the input, bounds, negatives etc.
    }
  }
&lt;/code&gt;
    &lt;p&gt;For sending frame data, at first, I naively packed the buffer and then unpacked it again based on the region a client was rendering.&lt;/p&gt;
    &lt;code&gt;// in simulation thread
for _, p := range particles {
  x := int(p.x)
  y := int(p.y)
  if x &amp;gt;= 0 &amp;amp;&amp;amp; x &amp;lt; int(simState.width) &amp;amp;&amp;amp; y &amp;gt;= 0 &amp;amp;&amp;amp; y &amp;lt; int(simState.height) {
    idx := (y*int(simState.width) + x)
    if idx &amp;lt; int(simState.width*simState.height) {
      byteIndex := idx / 8
      bitOffset := idx % 8
      if byteIndex &amp;lt; len(framebuffer) {
        framebuffer[byteIndex] |= (1 &amp;lt;&amp;lt; bitOffset)
      }
    }
  }
}

// in frame send channel before sending to client 
// get client camera info
for row := int32(0); row &amp;lt; height; row++ {
  for col := int32(0); col &amp;lt; width; col++ {
    mainFrameIndex := ((y+row)*int32(simState.width) + (x + col))
    dataToSendIndex := (row*width + col)

    if mainFrameIndex/8 &amp;lt; int32(len(frameBuffer)) &amp;amp;&amp;amp; dataToSendIndex/8 &amp;lt; int32(len(dataToSend)) {
      isSet := (frameBuffer[mainFrameIndex/8] &amp;gt;&amp;gt; (mainFrameIndex % 8)) &amp;amp; 1
      if isSet == 1 {
        dataToSend[dataToSendIndex/8] |= (1 &amp;lt;&amp;lt; (dataToSendIndex % 8))
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;This makes HD only a few hundred kb/frame.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pprof&lt;/code&gt; has a niffy way of downloading a simple of the running up you can then load and look at with their cli tool.&lt;/p&gt;
    &lt;p&gt;I checked a sample using &lt;code&gt;pprof&lt;/code&gt; and noticed the sim was slow when it was building the frame buffer so I moved the frame building to the simulation workers. This is possible now because each thread will only mark a bit as on if there is a particle there. Which means I don't need to lock anything and can let the last writer win.&lt;/p&gt;
    &lt;p&gt;Before&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 35
      flat  flat%   sum%        cum   cum%
    16.75s 52.54% 52.54%     17.05s 53.48%  main.worker
     6.42s 20.14% 72.68%      6.49s 20.36%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;And after&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 36
      flat  flat%   sum%        cum   cum%
    13.01s 46.46% 46.46%     13.28s 47.43%  main.worker
     6.98s 24.93% 71.39%      7.08s 25.29%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;The simulation is a bit faster but notice that the broadcast frames is slow. You know that packing I am doing? Ya, that ended up being very very slow. 25% of the time is spent broadcasting frames to a few clients due to all the bit opps packing and unpacking.&lt;/p&gt;
    &lt;p&gt;A simple fix is to use full bytes even if the count is only ever 0-1 and then using a lookup map to skip the bit opps like so.&lt;/p&gt;
    &lt;code&gt;var uint64ToByteLUT = make(map[uint64]byte)

func init() {
  var byteSlice = make([]byte, 8)
  for i := 0; i &amp;lt; 256; i++ {
    for bit := 0; bit &amp;lt; 8; bit++ {
      if (i&amp;gt;&amp;gt;bit)&amp;amp;1 == 1 {
        byteSlice[bit] = 1
      } else {
        byteSlice[bit] = 0
      }
    }
    // trust me bro
    uint64ToByteLUT[BytesToUint64Unsafe(byteSlice)] = byte(i)
  }
}

for row := int32(0); row &amp;lt; height; row++ {
  yOffset := (y + row) * int32(simState.width)
  for col := int32(0); col &amp;lt; width; col += 8 {
    fullBufferIndex := yOffset + (x + col)
    chunk := frameBuffer[fullBufferIndex : fullBufferIndex+8]
    key := BytesToUint64Unsafe(chunk)

    packedByte, _ := uint64ToByteLUT[key]

    if outputByteIndex &amp;lt; int32(len(dataToSend)) {
      dataToSend[outputByteIndex] = packedByte
      outputByteIndex++
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Now slicing the frame region for a client is almost free. The idea is pretty simple, interpret 8 bytes at a time as a uint64 which maps to the corresponding single byte with the right bits set. I did something similar on the client too to make unpacking faster. The extra memory here is pretty small and worth it to me.&lt;/p&gt;
    &lt;p&gt;Not bad. I am sure more can be done but the main sim is taking up the majority of the time.&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 49
      flat  flat%   sum%        cum   cum%
    15.62s 58.85% 58.85%     15.83s 59.65%  main.worker
     4.66s 17.56% 76.41%      4.66s 17.56%  runtime.pthread_cond_wait
     2.96s 11.15% 87.57%      2.96s 11.15%  runtime.pthread_cond_signal
     0.90s  3.39% 90.96%      0.90s  3.39%  runtime.kevent
     0.47s  1.77% 92.73%      0.80s  3.01%  runtime.mapaccess2_fast64
     0.36s  1.36% 94.08%      1.17s  4.41%  main.broadcastFrames
     0.20s  0.75% 94.84%      0.20s  0.75%  runtime.asyncPreempt
     0.17s  0.64% 95.48%      0.17s  0.64%  runtime.madvise
     0.17s  0.64% 96.12%      0.17s  0.64%  runtime.pthread_kill
     0.17s  0.64% 96.76%      0.17s  0.64%  runtime.usleep
&lt;/code&gt;
    &lt;p&gt;On the topic of memory, with millions of particles the server barely breaks over 100mb. Not rust good but node would be well over a few gigs and I cannot imagine python would be much lighter.&lt;/p&gt;
    &lt;p&gt;I waffled about again for well over a day trying to get some go assembly to work for those sweet simd gains but it wasn't to be. There are some libraries I tried too but none of them did better than what I already had. There are some interesting things with go's compiler around how to optimize out dereferences and bounds checking which did give me a 30% boost.&lt;/p&gt;
    &lt;p&gt;For example, before I was updating the particles like so&lt;/p&gt;
    &lt;code&gt;for i := job.startIndex; i &amp;lt; job.endIndex; i++ {
  particles[i].x += particles[i].dx
  // etc
&lt;/code&gt;
    &lt;p&gt;But by doing this&lt;/p&gt;
    &lt;code&gt;for i := job.startIndex; i &amp;lt; job.endIndex; i++ {
  p := &amp;amp;particles[i]
  p.x += p.dx
&lt;/code&gt;
    &lt;p&gt;Big boost. It makes sense. Bounds checking was happening way too often. I did a few more micro optimizations which helped but nothing significant. I did try SoA too and that actually did worse than AoS. Go figure.&lt;/p&gt;
    &lt;p&gt;I tried to use a fast inverse square root function but for some reason the go version of it didn't work at all which is weird because js lands had no issue. Oh well.&lt;/p&gt;
    &lt;p&gt;The previous profile samples were taken simulating a dozen or so clients with a few million particles. If I bump that up to 20m particles, I get this.&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 21
      flat  flat%   sum%        cum   cum%
   113.16s 93.06% 93.06%    115.22s 94.75%  main.worker
     2.27s  1.87% 94.93%      2.27s  1.87%  runtime.pthread_cond_wait
     2.04s  1.68% 96.60%      2.04s  1.68%  runtime.asyncPreempt
     1.23s  1.01% 97.62%      1.23s  1.01%  runtime.pthread_cond_signal
     0.63s  0.52% 98.13%      1.56s  1.28%  runtime.mapaccess2_fast64
     0.30s  0.25% 98.38%      1.92s  1.58%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;The simulation is the clear bottle neck and without simd, I am not sure how much better I can get it.&lt;/p&gt;
    &lt;p&gt;It works though. Time to deploy it to the cloud.&lt;/p&gt;
    &lt;p&gt;I was going to use a digital ocean docklet but it would cost more than an Equinox gym membership for anything faster than a toaster. I ended up spending ~$8/month at netcup for a 16 gig 10 core arm vm Manassas, Virginia with a ping of over 300ms. Fantastic. Not sure why the DNS on their panel says germany though.&lt;/p&gt;
    &lt;p&gt;I opened a few ports under root like the psychopath I and gave it a spin.&lt;/p&gt;
    &lt;p&gt;Pretty slick. More clients barely impacts cpu usage. But will it scale to 1k clients? I am not sure.&lt;/p&gt;
    &lt;p&gt;The netcup vps has a 2.5 Gbit pipe. At a 1920x1080 resolution with 1 bit per pixel at a rate of 30 frames/s down the pipe, that should theoretically support well over 300 clients. Now, if these were mobile devices with a fraction of that resolution say 390x844 or so, (we are not talking native), it may actually scale.&lt;/p&gt;
    &lt;p&gt;It is worth noting it does suffer from head of line blocking at times especially as the resolution gets cranked up. Clients with a resolution larger than the max particle grid are ignored so your 5k display won't work.&lt;/p&gt;
    &lt;p&gt;I did a bit of polishing on the client, edge scrolling, panning, mobile support, etc but this post isn't about javascript it is about golang.&lt;/p&gt;
    &lt;p&gt;Go is able to simulate 2.5 million particles at 60fps while sending data at 30 fps to in theory over 300 clients maybe even a thousand. And because this happens on the server it even runs on a smart tv. Don't believe me? Just visit, howfastisgo.dev and see for yourself or watch the video below.&lt;/p&gt;
    &lt;p&gt;While go certainly isn't a compute workhorse, that is pretty impressive. Anywhere a browser runs this works! Go is just THAT fast.&lt;/p&gt;
    &lt;p&gt;This is just so cursed. I love it.&lt;/p&gt;
    &lt;p&gt;Until next time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dgerrells.com/blog/how-fast-is-go-simulating-millions-of-particles-on-a-smart-tv"/><published>2025-09-24T18:48:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45364514</id><title>Unlocking a Million Times More Data for AI</title><updated>2025-09-24T20:35:58.006052+00:00</updated><content>&lt;doc fingerprint="bf7813d821ab84d8"&gt;
  &lt;main&gt;
    &lt;p&gt;This essay is part of The Launch Sequence, a collection of concrete, ambitious ideas to accelerate AI for science and security.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Every major leap forward in AI progress has been accompanied by a large increase in available data to support it. However, AI leaders often warn that we have reached ‚Äúpeak data‚Äù ‚Äî that all human data for training AI has been exhausted. Our analysis suggests that this view may not capture the whole picture.&lt;/p&gt;
    &lt;p&gt;While not all AI systems report their training data size, some do. Among those, the order of magnitude is a few hundred terabytes. For reference, you could go to Walmart, buy a few dozen hard drives, and fit this data on your kitchen table. Meanwhile, the world has digitized an estimated 180-200 zettabytes of data ‚Äî over a million times more data than what was used to train today‚Äôs leading models. This means the data exists, but is not being used for training. We classify this as an access problem rather than a scarcity problem.&lt;/p&gt;
    &lt;p&gt;This proposal presents Attribution-Based Control (ABC) as a potential framework for expanding access to the world‚Äôs digital data while preserving ownership rights. While significant technical, legal, and economic barriers prevent access to most of the world‚Äôs data, ABC offers one possible approach to address these barriers by enabling data owners to maintain control while contributing to AI development. We outline the technical foundations of ABC and suggest a government-led development program modeled on the success of ARPANET.&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;head rend="h3"&gt;The foundation: AI growth needs data growth&lt;/head&gt;
    &lt;p&gt;The history of AI breakthroughs reveals a consistent pattern: each major leap forward has been fueled by dramatic increases in the availability of high-quality data. Take, for example, some of the biggest leaps forward in recent AI history. AI‚Äôs ‚ÄúImageNet moment‚Äù was driven by a single, large, labeled dataset. Word2vec was a fundamental breakthrough in the amount of data a language model could process. Transformers were a simplification of an older algorithm (LSTMs) whose purpose was primarily to process more data. DeepMind‚Äôs rise centered around generating (narrow) datasets from Atari (and later Go) video games. GANs gained popularity by training models to generate their own data. InstructGPT/RLHF are data acquisition strategies (via customers and ‚Äúmechanical turkers‚Äù ‚Äî humans hired to generate data online through platforms like Amazon Mechanical Turk or Prolific) that are applied using standard learning algorithms. And as the original papers describe, OpenAI‚Äôs GPT-1, GPT-2, and subsequent advancements came not as fundamental changes to the Transformer, but on the back of OpenAI‚Äôs pioneering efforts in assembling large-scale datasets from publicly available internet data and armies of human data creators.&lt;/p&gt;
    &lt;p&gt;Non-data advancements in GPUs or algorithms are crucial. But running a hundredfold bigger GPU cluster or a more sophisticated algorithm without more data is like running a massive truck with just enough fuel for a sedan. The truck can get moving, but the trip will be short. So too with AI‚Äôs rise.&lt;/p&gt;
    &lt;head rend="h3"&gt;Understanding concerns about data scarcity&lt;/head&gt;
    &lt;p&gt;Recent statements from Ilya Sutskever declaring we‚Äôve reached ‚Äúpeak data‚Äù and Elon Musk warning that all human data for AI training has been ‚Äúexhausted‚Äù reveal concerns from industry leaders about data availability for future AI progress. Their concern stems from understandable observations of the current constraints on accessing high-quality training data. Privacy laws are getting stricter, tech platforms are closing off access, and copyright holders are pushing back.&lt;/p&gt;
    &lt;p&gt;These constraints have led to the exploration of alternative approaches to extract more information from the data we already possess, including synthetic data generation and test-time compute optimization. Yet, synthetic data is merely a compressed form of the data it is derived from, and while test-time compute is a viable way to squeeze extra performance, it is not a sudden paradigm shift in capabilities of the kind new data sources historically have enabled.&lt;/p&gt;
    &lt;p&gt;AI labs‚Äô latest exploration has resulted in the onboarding of massive armies of mechanical turkers to create more specialized data in specific fields, and the continued collection of millions of users‚Äô experiences to fine-tune widespread use. Consequently, AI capabilities inch forward as information is slowly copied from the minds of turkers and customers to AI models, and some breakthroughs occur. But both Mechanical Turk programs and user feedback systems only provide incremental improvements at significant cost and limited scale. They pale in comparison to the paradigm shifts of ImageNet, AlphaGo, and GPT.&lt;/p&gt;
    &lt;head rend="h3"&gt;From scarcity to abundance&lt;/head&gt;
    &lt;p&gt;When examining global data resources, a different picture emerges. While not all recent, industry-leading, state-of-the-art AI models report the size of their dataset, some do, and others have been estimated. All fall within a similar size range.&lt;/p&gt;
    &lt;p&gt;Note: Out of 952 AI models tracked by Epoch AI with estimates or reports of training dataset size, none exceeded Meta‚Äôs 180TB.&lt;/p&gt;
    &lt;p&gt;Despite what their name might suggest, so-called ‚Äúlarge language models‚Äù (LLMs) are trained on relatively small datasets.1 2 3 For starters, all the aforementioned measurements are described in terms of terabytes (TBs), which is not typically a unit of measurement one uses when referring to ‚Äúbig data.‚Äù Big data is measured in petabytes (1,000 times larger than a terabyte), exabytes (1,000,000 times larger), and sometimes zettabytes (1,000,000,000 times larger). For reference, Samsung sells a smartphone with 1TB of storage, and consumers can buy a 2TB SD card the size of a thumbnail. You could probably get a few friends together and store a copy of an LLM training dataset using your drawers of old smartphones and laptops.&lt;/p&gt;
    &lt;p&gt;One might be inclined to doubt and say, ‚ÄúPerhaps the AI models that don‚Äôt report dataset sizes leverage 10‚Äì100 times more data!‚Äù However, this would require 10‚Äì100 times more compute to train. There‚Äôs considerable evidence against this being the state of affairs ‚Äî leading AI labs use about as many chips for training as you‚Äôd expect given the claimed size of their datasets.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the world‚Äôs eight billion+ people, 360 million+ companies, and thousands of local, national, and international government organizations have digitized an estimated 180 zettabytes of data, a volume that doubles every two years. This private data represents a million times more data than The Internet Archive, two hundred million times more data than Common Crawl, and a billion times more data than what was used to train GPT4, DeepSeek-R1, or Google‚Äôs Lambda or Palm 2.&lt;/p&gt;
    &lt;p&gt;What makes this vast private data uniquely valuable is its quality and real-world grounding. This data includes electronic health records, financial transactions, industrial sensor readings, proprietary research data, customer/population databases, supply chain information, and other structured, verified datasets that organizations use for operational decisions and to gain competitive advantages. Unlike web-scraped data, these datasets are continuously validated for accuracy because organizations depend on them, creating natural quality controls that make even a small fraction of this massive pool extraordinarily valuable for specialized AI applications.&lt;/p&gt;
    &lt;p&gt;Considering both the sheer volume and quality of private data, it‚Äôs clear that despite the concern from AI leaders, the world hasn‚Äôt remotely run out of data. The challenge is not data scarcity, but rather barriers to accessing and utilizing existing high-quality data resources.&lt;/p&gt;
    &lt;head rend="h3"&gt;The real crisis: Misaligned incentives between data owners and AI companies&lt;/head&gt;
    &lt;p&gt;The real problem is an old one: information markets are failed markets. When a data owner shares a piece of data, the owner loses all control over how it will be used, copied, and shared further. When the owner sells a piece of data, they don‚Äôt sell the original data ‚Äî they sell a copy. When a dataset is copied, the global supply goes up, the price goes down, and every customer becomes a competitor for the future sale and use of that data.&lt;/p&gt;
    &lt;p&gt;This problem is compounded by data‚Äôs unique economic property: it‚Äôs an infinitely reusable resource. Unlike physical goods, the same dataset can simultaneously power a medical breakthrough, optimize a supply chain, and train an AI model. Each buyer gains not just one use case but potentially thousands, making them an immediate competitor for selling those same insights to others. This ‚Äúeconomic bundling‚Äù means data owners can never capture the full value of what they‚Äôre selling.&lt;/p&gt;
    &lt;p&gt;Consequently, most people don‚Äôt sell or share most data. Even the largest tech companies in the world ‚Äî Meta, Apple, Google, etc. ‚Äî don‚Äôt tend to sell the data they acquire. Instead, they use it for internal use cases, keeping the supply low and the value of the data (and the use cases it supports) high. Indeed, creating data moats that drive superior advertising is the core business model of the internet. And a data moat is the opposite of sharing data ‚Äî it‚Äôs working very hard to ensure the data is never made available to a possible competitor.&lt;/p&gt;
    &lt;p&gt;This dynamic creates a fundamental misalignment that extends to AI labs themselves. While they‚Äôve built successful businesses on data moats and proprietary architectures, they now face an existential constraint: their training datasets measure in hundreds of terabytes, yet the world‚Äôs data measures in zettabytes. Without access to this vastly larger data pool, AI progress risks stagnating, as labs exhaust the limited public data available and compete for marginal improvements rather than paradigm-shifting advances. As privacy laws tighten and data owners refuse sharing requests, the very market structures that enabled AI labs‚Äô rise now limit their access to the critical resource they need for further advancement. Without a sustainable path to access more data, both sides are trapped: data owners won‚Äôt share because they lose control and value, while AI labs can‚Äôt access the exponentially larger datasets required for the next leap in AI capabilities.&lt;/p&gt;
    &lt;p&gt;Given these incentive misalignments, the key question for policymakers to answer is: Can we design a system that aligns data owner incentives with the growth needs of AI labs? Can policymakers fix the failing market for AI training data and catalyze a million times more data, fueling the next AI paradigm shift?&lt;/p&gt;
    &lt;head rend="h2"&gt;Solution&lt;/head&gt;
    &lt;p&gt;Think of today‚Äôs AI like a giant blender where, once you put your data in, it gets mixed with everyone else‚Äôs, and you lose all control over it. This is why hospitals, banks, and research institutions often refuse to share their valuable data with AI companies, even when that data could advance critical AI capabilities.&lt;/p&gt;
    &lt;p&gt;Attribution-Based Control (ABC) represents a paradigm shift in how we design AI systems. ABC is not a specific technology, but rather a set of criteria that any AI system must meet:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Data owners must be able to control which specific AI predictions their data supports&lt;/item&gt;
      &lt;item&gt;AI users must be able to control which data sources contribute to their received predictions (e.g., LLM tokens)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These properties transform data from a one-time giveaway into an ongoing revenue-generating asset. When data owners can control how their data is used and get paid each time it provides value, they have strong incentives to share rather than silo their resources.&lt;/p&gt;
    &lt;p&gt;When AI systems meet these criteria, they create a sustainable data economy. Data owners gain an incentive to share because they maintain ongoing control and can generate continuous revenue streams, similar to how musicians earn royalties each time their song is played. This solves the access problem. Instead of hoarding data for competitive advantage, organizations can monetize it while maintaining control.&lt;/p&gt;
    &lt;p&gt;The technological foundations for ABC already exist today through two key technical capabilities:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The ability to partition AI models by data source&lt;/item&gt;
      &lt;item&gt;The infrastructure to preserve privacy and control throughout the AI lifecycle&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Technologies for model partitioning&lt;/head&gt;
    &lt;p&gt;Model partitioning enables the ‚Äúattribution‚Äù part of ABC by keeping different data sources mathematically separable within the AI system. Several software architectures now exist that enable source-specific ownership of AI components. Mixture of Experts (MoE) naturally segments models into specialized sub-networks that can be owned independently, while Retrieval-Augmented Generation (RAG) takes a different approach, separating knowledge storage from processing, allowing data owners to maintain control over their knowledge bases while contributing to collective intelligence. RETRO and ATLAS exemplify how these principles scale in practice, achieving GPT-3 level performance while using 25-50x fewer parameters by maintaining explicit source attribution through their database architectures. Meanwhile, model merging techniques like Git Re-Basin demonstrate yet another path, proving that independently trained models can be combined without performance degradation, and recent work on federated MoE shows how these approaches scale across distributed ownership. This proliferation of successful architectures, each implementing ABC principles through a different technical mechanism, demonstrates the viability of source-specific attribution in AI systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technologies for privacy infrastructure&lt;/head&gt;
    &lt;p&gt;Privacy-preserving infrastructure provides the ‚Äúcontrol‚Äù part of ABC by ensuring data owners can enforce their participation decisions without exposing their data. This infrastructure is technically mature, built on decades of research in privacy-enhancing technologies (PETs) that now enable end-to-end encrypted AI workflows. During training, each data owner can develop their model partition within GPU enclaves ‚Äî hardware-isolated environments where NVIDIA H100s and cloud providers guarantee that training data never leaves unencrypted. Homomorphic encryption enables the aggregation of these distributed model pieces while they are encrypted, allowing for federated learning without centralizing data.&lt;/p&gt;
    &lt;p&gt;This infrastructure can be flexibly implemented depending on organizational needs and capabilities. Small organizations can use cloud-based secure enclaves or federation services, while larger institutions might choose to run local secure nodes. For AI labs, implementing ABC doesn‚Äôt mean abandoning their centralized GPU clusters; they can continue their existing training workflows and large-scale experiments, with ABC adding a coordination layer that connects to distributed data sources.&lt;/p&gt;
    &lt;p&gt;During inference, these same technologies can orchestrate secure AI predictions: user queries are encrypted and distributed to model owners, whose GPU enclaves execute their portions of the computation without being able to see the query. Secure multi-party computation coordinates how these encrypted model outputs combine, enabling multiple parties to jointly compute functions without revealing their individual inputs. Zero-knowledge proofs verify that each computation ran correctly without exposing the underlying data or weights. Differential privacy adds mathematical guarantees throughout, preventing adversaries from reconstructing training data through repeated queries. Together, these PETs work in concert to create structured transparency, ensuring that neither training data, model weights, nor user queries are ever exposed in unencrypted form to unauthorized parties, while cryptographic attestation maintains the attribution chains.&lt;/p&gt;
    &lt;p&gt;While some platforms are new, the fundamental performance overhead of these technologies is comparable to what we already accept for HTTPS in web apps ‚Äî a modest trade-off for accessing vastly more data. Modern optimizations continue to reduce this overhead through techniques like vectorized operations and hardware acceleration, making the computational costs increasingly negligible relative to the value of the expanded data access ABC enables.&lt;/p&gt;
    &lt;p&gt;Together, model partitioning and privacy infrastructure demonstrate that ABC is not a theoretical future possibility but an achievable present reality. The technical components have been scaled independently and are waiting to be assembled and integrated by researchers, engineers, and product managers to produce an AI system with ABC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Policy recommendations&lt;/head&gt;
    &lt;head rend="h3"&gt;An ARPANET-style program to unlock a million times more data&lt;/head&gt;
    &lt;p&gt;The United States government has a unique opportunity to assemble the technologies already developed in its domestic research ecosystem and integrate them into a networked system that blossoms into a trillion-dollar market.&lt;/p&gt;
    &lt;p&gt;While the private sector plays a crucial role in creating data resources and transforming them into powerful algorithms, it does not necessarily possess optimal incentives to cooperate in maximizing America‚Äôs AI potential through the establishment of an interoperable, open data network. Just as IBM, Bell Telephone, and Microsoft didn‚Äôt necessarily have the right incentives to bring together the nation‚Äôs supercomputers under the banner of TCP/IP, WWW, and HTTP, today‚Äôs AI titans are naturally focused on their individual competitive advantages rather than building shared infrastructure. This creates a bottleneck where American AI is limited to the data a single company can acquire (hundreds of terabytes) rather than leveraging the full potential of America‚Äôs data resources (a million times more).&lt;/p&gt;
    &lt;p&gt;By incentivizing the development of AI systems with ABC, the US government can unlock access to these vast data resources while preserving ownership rights. Once developed, AI systems with ABC can transform the entire AI ecosystem. Labs will be incentivized to adopt ABC systems not out of obligation, but out of opportunity. Their data teams, which are increasingly hitting walls with traditional acquisition methods, will gain access to the exponentially larger datasets needed to push the boundaries of AI capability.&lt;/p&gt;
    &lt;p&gt;Luckily, the US has done this before. Just as ARPANET and later NSFNET ushered in the shift from isolated mainframes to networked personal computers, the US government should again create a multi-pronged approach to transition us from today‚Äôs centralized AI towards network-sourced AI with ABC.&lt;/p&gt;
    &lt;p&gt;The ARPANET/NSFNET playbook offers a proven template for this transformation. For over two decades, multiple government agencies collaborated to create the internet through strategic coordination. ARPA (now DARPA) provided visionary leadership and seed funding, the NSF built bridges to widespread adoption through programs that subsidized early adopters, and the Commerce Department managed the critical transition of internet governance to the private sector.&lt;/p&gt;
    &lt;p&gt;To replicate ARPANET‚Äôs success for Attribution-Based Control, we recommend the following actions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Establish an ABC Development Program within DARPA: Following the original lean program structure that created ARPANET, DARPA should create a small, focused team to assemble the various technological components necessary to develop an AI system with ABC and conduct proof-of-concept and functional testing. Unlike other research institutions, DARPA is uniquely suited to focus on developing paradigm-shifting technologies that create entirely new capabilities, because its organizational structure and culture are specifically designed to fund high-risk, high-reward projects.&lt;/item&gt;
      &lt;item&gt;Leverage NSF Grants to Subsidize Early Adopters: NSF achieved explosive network growth for ARPANET/NSFNET through a multi-pronged adoption strategy: free access, programs that subsidized university connections while requiring campus-wide deployment, and strategic positioning as general-purpose research infrastructure rather than just supercomputer access. The NSF could replicate this proven playbook for ABC by establishing a program or leveraging the operations of an existing program, such as the NAIRR, to subsidize early adopters who implement ABC systems while requiring institution-wide deployment. This approach would position ABC systems as essential research infrastructure for the AI era.&lt;/item&gt;
      &lt;item&gt;Build International ABC Standards at NIST: Just as TCP/IP became the global internet standard, NIST should lead the development of ABC technical standards that position American technologies as the international norm for data sharing. The AI Action Plan already emphasizes leading in international AI diplomacy; extend this to champion ABC as the framework for ethical, controlled data sharing globally.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AI revolution stands at a crossroads: we can continue down the path of data silos and market resistance, or we can unleash a new era of shared prosperity through publicly supported data highways for AI, as envisioned by ABC. Just as ARPANET‚Äôs visionary investment created today‚Äôs trillion-dollar internet economy, a similar commitment to ABC could transform AI into a collaborative ecosystem where every data owner can contribute to and benefit from AI progress. The technological ingredients exist, the need is urgent, and history has shown us the way ‚Äî America must act now to build the data economy that ensures every hospital, every research institution, and every company can contribute to and benefit from our shared AI future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenMined, ‚ÄúIntroduction to Attribution-Based Control (ABC),‚Äù 2025&lt;lb/&gt;Provides a more in-depth introduction to the concept of ABC.&lt;/item&gt;
      &lt;item&gt;Andrew Trask et al., ‚ÄúBeyond Privacy Trade-offs with Structured Transparency,‚Äù December 15, 2020&lt;lb/&gt;Serves as the grounding theoretical framework.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Meta‚Äôs Llama 4 Behemoth was trained on 30 trillion ‚Äútokens‚Äù (i.e., word fragments), representing around 180 TBs of text data. This is estimated conservatively at 6 TB per trillion tokens, but this can vary based on the tokenizer.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;15.6 trillion tokens, or roughly 94 TBs of text data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;13 trillion tokens or roughly 78 TBs of text data.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ifp.org/unlocking-a-million-times-more-data-for-ai/"/><published>2025-09-24T18:54:20+00:00</published></entry></feed>