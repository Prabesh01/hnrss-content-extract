<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-07T13:43:26.629143+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45491470</id><title>Compiling a Forth</title><updated>2025-10-07T13:43:36.625685+00:00</updated><content>&lt;doc fingerprint="22047e11e047ab00"&gt;
  &lt;main&gt;
    &lt;p&gt;I was curious how Forth worked so I built a bytecode compiler and a VM for a Forth-like language, as well as some visualizations to show how it all works.&lt;/p&gt;
    &lt;p&gt;You don't need to know anything about Forth to follow along, aside from the fact it's a stack-oriented language.&lt;/p&gt;
    &lt;p&gt;Here's a small program that prints the number three.&lt;/p&gt;
    &lt;quote&gt;3 .&lt;/quote&gt;
    &lt;p&gt;The number (&lt;code&gt;3&lt;/code&gt;) is pushed to the data stack, and then the dot (&lt;code&gt;.&lt;/code&gt;) pops it from the data stack and prints it.&lt;/p&gt;
    &lt;p&gt;We'll need more Forth features than this to build interesting programs.&lt;/p&gt;
    &lt;p&gt;Forth has two built-in stacks. The data stack (sometimes just called "the stack") and the return stack. When a word is called in Forth (words are like functions) the address of the next instruction is pushed to the return stack. When the word finishes executing, the return stack is popped into the instruction pointer.&lt;/p&gt;
    &lt;quote&gt;\ (1) word declaration: PRINT10\ (3) the word body is executed10 .\ (4) ";" compiles an exit ‚Äì at runtime it pops the return stack\ into the instruction pointer.;\ (2) instruction pointer lands on a word,\ the next address is pushed to the return stack,\ and the instruction pointer is set to the word addressPRINT10\ (5) next address is executed&lt;/quote&gt;
    &lt;p&gt;As well as words, my compiler also supports &lt;code&gt;DO&lt;/code&gt;/&lt;code&gt;LOOP&lt;/code&gt;s. These use the return stack too. When &lt;code&gt;DO&lt;/code&gt; executes, it pops the limit and the iterator from the data stack and stores them in the return stack. This allows the inner loop to freely operate on the data stack. When &lt;code&gt;LOOP&lt;/code&gt; executes, it pops the limit and iterator from the return stack, adds one to the iterator and compares it to the limit (and exits or loops again).&lt;/p&gt;
    &lt;p&gt;There are also variables, which can be declared with &lt;code&gt;VARIABLE X&lt;/code&gt;, loaded with &lt;code&gt;X @&lt;/code&gt;, and stored with &lt;code&gt;1 X !&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Putting these features together, here's how you can build &lt;code&gt;10&lt;/code&gt; by adding &lt;code&gt;1&lt;/code&gt; repeatedly.&lt;/p&gt;
    &lt;quote&gt;VARIABLE A: RUN0 A ! \ initialize A10 0 DO \ push limit and iterator for DO\ DO places these on the return stackA @ 1 + A ! \ A = A + 1LOOP \ increment i and exits when i == limitA @ . \ prints 10;RUN&lt;/quote&gt;
    &lt;p&gt;This set of features is enough for us to calculate numbers from the Fibonacci series, which is the example program I'll be using throughout the rest of this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tokenizing&lt;/head&gt;
    &lt;p&gt;Tokenization translates raw text into meaningful symbols.&lt;/p&gt;
    &lt;p&gt;To turn source code into tokens, we scan through the code, skipping over whitespace and appending tokens to a list. Syntax that's a single character is turned straight into a token but multi-character syntax needs to be grouped together. For example, entire comments are discarded, and while they are being discarded, we need to track that we're "within" a comment.&lt;/p&gt;
    &lt;p&gt;Identifiers, like keywords like &lt;code&gt;DO&lt;/code&gt; or &lt;code&gt;LOOP&lt;/code&gt;, or custom variables like &lt;code&gt;MYLONGVAR&lt;/code&gt;, become single tokens.&lt;/p&gt;
    &lt;p&gt;First, a visualization of what's happening:&lt;/p&gt;
    &lt;p&gt;And here's a trimmed version of my tokenizer:&lt;/p&gt;
    &lt;quote&gt;function tokenize(source: string): Token[] {const tokens: Token[] = [];let index = 0;while (index &amp;lt; source.length) {// Consume and discard everything on a line after '\'if (source[index] === "\\") {const commentStart = index;while (index &amp;lt; source.length &amp;amp;&amp;amp; source[index] !== "\n") {index++;}index++;continue;}// Skip over whitespaceif (isWhitespace(source[index])) {index++;continue;}if (source[index] === "@") {tokens.push({ type: "load" });index++;continue;}// Handle identifiersif (isLetter(source[index])) {const start = index;let value = "";while (isLetter(source[index])) {value += source[index];index++;}// Special-case the keywordsif (value === "DO") {tokens.push({ type: "do" });continue;}if (value === "LOOP") {tokens.push({ type: "loop" });continue;}tokens.push({ type: "identifier", value });continue;}// .. trimmed other tokens, see source}return tokens;}&lt;/quote&gt;
    &lt;p&gt;With our list of tokens, we're ready to start generating bytecode for the VM.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generating Bytecode&lt;/head&gt;
    &lt;p&gt;Usually, in a compiler, the step after tokenization is called parsing where an abstract syntax tree is built. However, the feature set of my Forth is so small, that I decided to generate bytecode directly from the list of tokens.&lt;/p&gt;
    &lt;p&gt;After bytecode generation, my VM needs two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A list of operations for the VM's instruction pointer to navigate&lt;/item&gt;
      &lt;item&gt;The number of variables that the program refers to&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The latter tells the VM how many variables to allocate (a zero-initialized array). Variables in source (e.g., &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;) become integer indices into this array.&lt;/p&gt;
    &lt;p&gt;This means that my bytecode generation step needs to keep track of variables that have been seen before so that I can output the correct memory address (i.e. an index into the variable table).&lt;/p&gt;
    &lt;p&gt;I'll show the full list of bytecode operations and then a few of the steps for handling specific tokens.&lt;/p&gt;
    &lt;quote&gt;type Op = {op: "lit", // Push value or address to DSvalue: number;} | {op: "load", // Pop address from DS, push value at address} | {op: "store", // Pop address from DS, pop value from DS, store value at address} | {op: "dup2", // Duplicate top two values on DS [a, b] -&amp;gt; [a, b, a, b]} | {op: "add", // Pop top two values from DS, push sum to DS} | {op: "eq", // Pop top two values from DS, push 1 if equal, 0 if not} | {op: "jz", // Pop value from DS, if zero, jump to addressaddress: number;} | {op: "jmp", // Jump to addressaddress: number;} | {op: "call", // Push IP to RS, jump to addressaddress: number;} | {op: "ret", // Pop IP from RS, jump to IP} | {op: "rs_push", // Pop from DS, push to RS} | {op: "rs_pop", // Pop from RS, push to DS} | {op: "drop", // Discard top value from DS} | {op: "print", // Pop value from DS, print it}&lt;/quote&gt;
    &lt;p&gt;The bytecode generation step scans through the list of tokens and, as it processes them, it appends to a list of bytecode and increments the variable count to set up the correct references.&lt;/p&gt;
    &lt;p&gt;Identifier tokens are either variable references, or words (function calls).&lt;/p&gt;
    &lt;quote&gt;function compile(tokens: Token[]) {// Bytecode that runs in the VMconst bytecode: Bytecode[] = [];// Word -&amp;gt; bytecode offsets (for calls)const wordTable: { [key: string]: number } = {};// Variable -&amp;gt; memory addressconst variableTable: { [key: string]: number } = {};// ..let index = 0;while (index &amp;lt; tokens.length) {const token = tokens[index];if (token.type === "identifier") {if (token.value === "VARIABLE") {const nextToken = tokens[index + 1];// Store a binding of variable name to memory addressvariableTable[nextToken.value] = Object.keys(variableTable).length;index += 2;continue;}// If the variable has been declared as a word like `: FIB10`// then we have previously stored the bytecode offset which we// will set the instruction pointer to at runtimeif (wordTable[token.value] !== undefined) {bytecode.push({ op: "call", address: wordTable[token.value] });index++;continue;}// If it's not a variable declaration, or a word, then we// look up the memory addressbytecode.push({ op: "lit", value: variableTable[token.value] });index++;continue;}// ..&lt;/quote&gt;
    &lt;p&gt;Setting up the &lt;code&gt;DO&lt;/code&gt;/&lt;code&gt;LOOP&lt;/code&gt; bytecode generation was the trickiest part of this project. It's a minefield of possible off-by-one errors. It's also not easy to read and understand but I've chosen to put it here anyway because even just glancing over it should help you understand how the loop variables (limit, iterator) and instruction pointer jumps are combined to execute loops in Forth.&lt;/p&gt;
    &lt;quote&gt;// .. still inside compile()if (token.type === "do") {index++;// Expect: DS has [limit, start] (start is top)// Move both to RS: start then limit (RS top becomes limit)bytecode.push({ op: "rs_push" }) // start -&amp;gt; RSbytecode.push({ op: "rs_push" }) // limit -&amp;gt; RS// Mark first instruction of loop bodyloopStart.push(bytecode.length);continue;}if (token.type === "loop") {// Pop limit and i from RS (RS top is limit)bytecode.push({ op: "rs_pop" }) // limit -&amp;gt; DSbytecode.push({ op: "rs_pop" }) // i -&amp;gt; DS// Increment ibytecode.push({ op: "lit", value: 1 })bytecode.push({ op: "add" }) // i on DS// Duplicate i and limit for compare and possible restorebytecode.push({ op: "dup2" })bytecode.push({ op: "eq" }) // eq flag on DSconst loopStartAddress = loopStart.pop(); // first instr of loop body// Branch: continue when not equal (eq==0), exit when equalconst continueAddress = bytecode.length + 4; // skip equal-path (2 drops + jmp)bytecode.push({ op: "jz", address: continueAddress })// Equal path (fallthrough): cleanup and exitbytecode.push({ op: "drop" }) // drop ibytecode.push({ op: "drop" }) // drop limitconst afterBlockAddress = bytecode.length + 1 /* jmp */ + 3 /* continue block */;bytecode.push({ op: "jmp", address: afterBlockAddress })// Continue path:// address == continueAddressbytecode.push({ op: "rs_push" }) // i -&amp;gt; RS (top)bytecode.push({ op: "rs_push" }) // limit -&amp;gt; RSbytecode.push({ op: "jmp", address: loopStartAddress })index++;continue;}// .. trimmed other tokens, see source&lt;/quote&gt;
    &lt;p&gt;The rest of the token branches are more straightforward. Tokens like dot, store, load, and print all map directly to bytecode operations.&lt;/p&gt;
    &lt;p&gt;The colon token branch sets the bytecode offset for the word name which allows identifiers to become word calls as we saw above.&lt;/p&gt;
    &lt;p&gt;Now we've earned a visualization break.&lt;/p&gt;
    &lt;head rend="h2"&gt;VM&lt;/head&gt;
    &lt;p&gt;Writing the VM felt a little bit like dessert. Manually stepping through the bytecode as I worked on the generation logic gave me fairly good confidence that I was heading in the right direction, I only came across one or two off-by-one bugs when putting the VM together. Essentially, I had designed it ahead-of-time.&lt;/p&gt;
    &lt;p&gt;The VM scans through the bytecode operations using the instruction pointer (which starts at &lt;code&gt;0&lt;/code&gt;). The instruction pointer can jump around as it encounters &lt;code&gt;jmp&lt;/code&gt; (jump to offset) or &lt;code&gt;jz&lt;/code&gt; (conditional jump).&lt;/p&gt;
    &lt;p&gt;It manages the data stack, return stack, and the variable table (i.e. memory addresses).&lt;/p&gt;
    &lt;p&gt;Here's a trimmed version of the VM:&lt;/p&gt;
    &lt;quote&gt;function vm(program: Program) =&amp;gt; {const dataStack: number[] = [];const returnStack: number[] = [];const variableTable: number[] = new Array(program.variableCount).fill(0);let ip = 0;while (ip &amp;lt; program.bytecode.length) {const cur = program.bytecode[ip];if (cur.op === "lit") {dataStack.push(cur.value); // Literal or memory addressip++;continue;} else if (cur.op === "store") {const address = dsPop();const value = dsPop();variableTable[address] = value;ip++;continue;} else if (cur.op === "jmp") {ip = cur.address;continue;} else if (cur.op === "jz") {if (dsPop() === 0) {ip = cur.address;continue;}ip++;continue;} else if (cur.op === "call") {ip++returnStack.push(ip);ip = cur.address;continue;} else if (cur.op === "ret") {ip = rsPop();continue;}// .. trimmed other ops, see source}}&lt;/quote&gt;
    &lt;p&gt;The code for my compiler and VM are embedded in this website. I've been iterating on it by just running the TypeScript file:&lt;/p&gt;
    &lt;quote&gt;bun ./components/visuals/forth/components.tsx55 # 10th Fibonacci number&lt;/quote&gt;
    &lt;p&gt;The visuals are React components with sleeps. In order to display the progress of the different steps (tokenizing, bytecode generation, VM), I first got each working and then added a callback which takes the current data and then sleeps.&lt;/p&gt;
    &lt;p&gt;So the VM function is actually async and accepts this callback:&lt;/p&gt;
    &lt;quote&gt;// VMasync function vm(program: Program, callback:(highlight: { ip: number },dataStack: number[],returnStack: number[],variableTable: number[]) =&amp;gt; Promise&amp;lt;void&amp;gt;) {// .. inside VM loopawait callback({ ip }, dataStack, returnStack, variableTable);// ..}&lt;/quote&gt;
    &lt;p&gt;And the component calls it and passes &lt;code&gt;setState&lt;/code&gt; functions:&lt;/p&gt;
    &lt;quote&gt;// Componentexport function VM() {// .. inside useEffectawait vm(program, async (highlight, newDataStack, newReturnStack, newVariableTable) =&amp;gt; {setHighlightIP(highlight.ip);setDataStack([...newDataStack]);setReturnStack([...newReturnStack]);setVariableTable([...newVariableTable]);await new Promise(resolve =&amp;gt; setTimeout(resolve, 500));});// ..}&lt;/quote&gt;
    &lt;p&gt;For the Forth code snippets in this post, I had to write a Prism plugin to get syntax highlighting working. Now that I've learned how to do this, I'll be using this method for syntax highlighting for the more esoteric (or, original) programming languages I write about!&lt;/p&gt;
    &lt;head rend="h2"&gt;Discrepancies&lt;/head&gt;
    &lt;p&gt;I described my compiler/VM as Forth-like because it's a little bit different from how Forth works.&lt;/p&gt;
    &lt;p&gt;My implementation compiles to bytecode ahead-of-time. Forth is traditionally interactive. Words are interpreted and executed as they are entered, and only colon definitions are compiled. Forth uses threaded code where words contain lists of addresses pointing to other words instead of a different bytecode offset.&lt;/p&gt;
    &lt;p&gt;Real Forth uses a dynamic dictionary that can be altered at runtime with new variables or word definitions. As I mentioned earlier, my word bodies are compiled with jump-over logic in the main execution stream. Also, my variables compile to &lt;code&gt;lit address&lt;/code&gt; operations but real Forth variables return their address when executed directly.&lt;/p&gt;
    &lt;p&gt;These are just a few of the differences but I feel like my Forth-like compiler and VM capture enough of the spirit of Forth!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://healeycodes.com/compiling-a-forth"/><published>2025-10-06T13:52:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45491621</id><title>Mise: Monorepo Tasks</title><updated>2025-10-07T13:43:35.296768+00:00</updated><content>&lt;doc fingerprint="6c502c738fd7a4ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Monorepo Tasks #6564&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;We're excited to announce Monorepo Tasks, a powerful new feature that brings first-class monorepo support to mise tasks! üöÄ&lt;/p&gt;
          &lt;head&gt;What is it?&lt;/head&gt;
          &lt;p&gt;Monorepo Tasks allows you to manage tasks across multiple projects in a single repository, with each project maintaining its own tools, environment variables, and tasks. Think of it as bringing the power of tools like Bazel or Turborepo to mise's task system, but with mise's signature simplicity.&lt;/p&gt;
          &lt;head&gt;Key Features&lt;/head&gt;
          &lt;head&gt;üéØ Unified Task Namespace&lt;/head&gt;
          &lt;p&gt;All tasks across your monorepo are automatically discovered and prefixed with their location:&lt;/p&gt;
          &lt;code&gt;mise //projects/frontend:build
mise //projects/backend:test
mise //services/api:deploy&lt;/code&gt;
          &lt;head&gt;üå≥ Smart Tool &amp;amp; Environment Inheritance&lt;/head&gt;
          &lt;p&gt;Define common tools at the root, override them where needed:&lt;/p&gt;
          &lt;code&gt;# Root mise.toml
[tools]
node = "20"      # Inherited everywhere
python = "3.12"

# projects/legacy-app/mise.toml
[tools]
node = "14"      # Override just for this project
# python still inherited!&lt;/code&gt;
          &lt;head&gt;üé≠ Powerful Wildcard Patterns&lt;/head&gt;
          &lt;p&gt;Run tasks across multiple projects with ease:&lt;/p&gt;
          &lt;code&gt;# Run tests in ALL projects
mise //...:test

# Run all build tasks under services/
mise //services/...:build

# Run ALL tasks in frontend (wildcards need quotes)
mise '//projects/frontend:*'

# Run all test:* tasks everywhere
mise '//...:test:*'&lt;/code&gt;
          &lt;head&gt;‚ú® Consistent Execution Anywhere&lt;/head&gt;
          &lt;p&gt;Run tasks from anywhere in the monorepo - they always execute with the correct context, tools, and environment from their config_root.&lt;/p&gt;
          &lt;head&gt;üîí Automatic Trust Propagation&lt;/head&gt;
          &lt;p&gt;Trust your monorepo root once, and all descendant configs are automatically trusted.&lt;/p&gt;
          &lt;head&gt;Quick Start&lt;/head&gt;
          &lt;p&gt;1. Enable the feature in your root &lt;/p&gt;
          &lt;code&gt;experimental_monorepo_root = true

[tools]
node = "20"
python = "3.12"&lt;/code&gt;
          &lt;p&gt;2. Set the experimental flag:&lt;/p&gt;
          &lt;code&gt;export MISE_EXPERIMENTAL=1&lt;/code&gt;
          &lt;p&gt;3. Add tasks to your projects:&lt;/p&gt;
          &lt;code&gt;# projects/frontend/mise.toml
[tasks.build]
run = "npm run build"

[tasks.test]
run = "npm test"&lt;/code&gt;
          &lt;p&gt;4. Run tasks from anywhere:&lt;/p&gt;
          &lt;code&gt;mise //projects/frontend:build
mise //...:test  # Run tests in all projects!&lt;/code&gt;
          &lt;head&gt;Example Monorepo Structure&lt;/head&gt;
          &lt;p&gt;Run all service builds: &lt;/p&gt;
          &lt;head&gt;Why This Matters&lt;/head&gt;
          &lt;p&gt;Managing monorepos is hard. Coordinating tools, tasks, and environments across dozens of projects is even harder. With Monorepo Tasks, you get:&lt;/p&gt;
          &lt;head&gt;How Does This Compare to Other Tools?&lt;/head&gt;
          &lt;p&gt;The monorepo ecosystem is rich with excellent tools, each with different strengths. Here's how mise's Monorepo Tasks compares:&lt;/p&gt;
          &lt;head&gt;Simple Task Runners&lt;/head&gt;
          &lt;p&gt;Taskfile and Just are fantastic for single-project task automation. They're lightweight and easy to set up, but they weren't designed with monorepos in mind. While you can have multiple Taskfiles/Justfiles in a repo, they don't provide unified task discovery, cross-project wildcards, or automatic tool/environment inheritance across projects.&lt;/p&gt;
          &lt;p&gt;mise's advantage: Automatic task discovery across the entire monorepo with a unified namespace and powerful wildcard patterns.&lt;/p&gt;
          &lt;head&gt;JavaScript-Focused Tools&lt;/head&gt;
          &lt;p&gt;Nx, Turborepo, and Lerna are powerful tools specifically designed for JavaScript/TypeScript monorepos.&lt;/p&gt;
          &lt;p&gt;mise's advantage: Language-agnostic support. While these tools excel in JS/TS ecosystems, mise works equally well with Rust, Go, Python, Ruby, or any mix of languages. You also get unified tool version management (not just tasks) and environment variables across your entire stack.&lt;/p&gt;
          &lt;head&gt;Large-Scale Build Systems&lt;/head&gt;
          &lt;p&gt;Bazel (Google) and Buck2 (Meta) are industrial-strength build systems designed for massive, multi-language monorepos at companies with thousands of engineers.&lt;/p&gt;
          &lt;p&gt;Both are extremely powerful but come with significant complexity:&lt;/p&gt;
          &lt;p&gt;mise's advantage: Simplicity through non-hermetic builds. mise doesn't try to control your entire build environment in isolation - instead, it manages tools and tasks in a flexible, practical way. This "non-hermetic" approach means you can use mise without restructuring your entire codebase or learning a new language. You get powerful monorepo task management with simple TOML configuration - enough power for most teams without the enterprise-level complexity that hermetic builds require.&lt;/p&gt;
          &lt;head&gt;Other Notable Tools&lt;/head&gt;
          &lt;p&gt;Rush (Microsoft) offers strict dependency management and build orchestration for JavaScript monorepos, with a focus on safety and convention adherence.&lt;/p&gt;
          &lt;p&gt;Moon is a newer Rust-based build system that aims to be developer-friendly while supporting multiple languages.&lt;/p&gt;
          &lt;head&gt;The mise Sweet Spot&lt;/head&gt;
          &lt;p&gt;mise's Monorepo Tasks aims to hit the sweet spot between simplicity and power:&lt;/p&gt;
          &lt;p&gt;When to choose mise:&lt;/p&gt;
          &lt;p&gt;When to consider alternatives:&lt;/p&gt;
          &lt;p&gt;The best tool is the one that fits your team's needs. mise's Monorepo Tasks is designed for teams who want powerful monorepo management without the complexity overhead, especially when working across multiple languages.&lt;/p&gt;
          &lt;head&gt;Try It Out!&lt;/head&gt;
          &lt;p&gt;This feature is experimental, which means:&lt;/p&gt;
          &lt;p&gt;Read the full documentation: Monorepo Tasks Guide&lt;/p&gt;
          &lt;head&gt;We Want Your Feedback!&lt;/head&gt;
          &lt;p&gt;Please try it out and let us know:&lt;/p&gt;
          &lt;p&gt;Share your experience in the comments below! üëá&lt;/p&gt;
          &lt;p&gt;Special shoutout to the mise community for the feedback and ideas that led to this feature. Happy building! üõ†Ô∏è&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 3 comments 12 replies&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Does this support &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Excited to see this! We're currently using turbo in a mixed Rust/wasm/TS/Python/Go repo, and it's been a bit of a mixed bag (admittedly, I don't know how much of that is because we're unwilling to invest effort into modelling task inputs/outputs correctly in turbo).&lt;/p&gt;
          &lt;p&gt;Compounding the issue is that what we really want a whole bunch of things out of it:&lt;/p&gt;
          &lt;p&gt;Absent these, I don't really see us adopting this anytime soon unfortunately.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;The &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/jdx/mise/discussions/6564"/><published>2025-10-06T14:07:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45492803</id><title>OpenZL: An open source format-aware compression framework</title><updated>2025-10-07T13:43:34.917094+00:00</updated><content>&lt;doc fingerprint="b69d42b82801cb99"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenZL is a new open source data compression framework that offers lossless compression for structured data.&lt;/item&gt;
      &lt;item&gt;OpenZL is designed to offer the performance of a format-specific compressor with the easy maintenance of a single executable binary.&lt;/item&gt;
      &lt;item&gt;You can get started with OpenZL today by visiting our Quick Start guide and the OpenZL GitHub repository.&lt;/item&gt;
      &lt;item&gt;Learn more about the theory behind OpenZL in this whitepaper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Today, we are excited to announce the public release of OpenZL, a new data compression framework. OpenZL offers lossless compression for structured data, with performance comparable to specialized compressors. It accomplishes this by applying a configurable sequence of transforms to the input, revealing hidden order in the data, which can then be more easily compressed. Despite applying distinct transformation permutations for every file type, all OpenZL files can be decompressed using the same universal OpenZL decompressor.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Decade of Lessons&lt;/head&gt;
    &lt;p&gt;When Zstandard was announced, it came with a simple pitch: It promised the same or better compression ratio of prior default but at the much increased speed required by datacenter workloads. By pairing strong entropy coding with a design that fully utilized modern CPU capabilities, Zstandard offered a substantial improvement that justified its presence in datacenters.&lt;/p&gt;
    &lt;p&gt;However, while it was improved over time, remaining within the Zstandard framework offers diminishing returns. So we started looking for the next great leap in data compression.&lt;/p&gt;
    &lt;p&gt;In this quest, one pattern kept repeating: Using generic methods on structured data leaves compression gains on the table. Data isn‚Äôt just byte soup. It can be columnar, encode enums, be restricted to specific ranges, or carry highly repetitive fields. More importantly, it has predictable shapes. A bespoke compressor that leans into that structure can beat general-purpose tools on both ratio and speed. But there‚Äôs a catch ‚Äî every bespoke scheme means another compressor and decompressor to create, ship, audit, patch, and trust.&lt;/p&gt;
    &lt;p&gt;OpenZL is our answer to the tension between the performance of format-specific compressors and the maintenance simplicity of a single executable binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Make the Structure Explicit&lt;/head&gt;
    &lt;p&gt;General compressors rely on a one-size fits all processing strategy, or alternatively spend a lot of their cycles guessing which techniques to use. OpenZL saves those cycles by making the structure an explicit input parameter. Compression can then focus on a sequence of reversible steps that surface patterns before coding.&lt;/p&gt;
    &lt;p&gt;As a user, you provide OpenZL with the data shape (via a preset or a thin format description). Then the trainer, an offline optimization component, builds an effective compression config that can be re-employed for similar data. During encoding that config resolves into a concrete decode recipe that‚Äôs embedded into the frame. The universal decoder will directly execute that recipe, without any out-of-band information.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Example Compression Using OpenZL&lt;/head&gt;
    &lt;p&gt;As an example, let‚Äôs compress sao, which is part of the Silesia Compression Corpus. This file follows a well-defined format featuring an array of records, each one describing a star. Providing this information to OpenZL is enough to give it an edge over generic lossless compressors, which only see bytes.&lt;/p&gt;
    &lt;p&gt;Comparison on a M1 cpu, using clang-17&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compressor&lt;/cell&gt;
        &lt;cell&gt;zstd -3&lt;/cell&gt;
        &lt;cell&gt;xz -9&lt;/cell&gt;
        &lt;cell&gt;OpenZL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compressed Size&lt;/cell&gt;
        &lt;cell&gt;5,531,935 B&lt;/cell&gt;
        &lt;cell&gt;4,414,351 B&lt;/cell&gt;
        &lt;cell&gt;3,516,649 B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compression Ratio&lt;/cell&gt;
        &lt;cell&gt;x1.31&lt;/cell&gt;
        &lt;cell&gt;x1.64&lt;/cell&gt;
        &lt;cell&gt;x2.06&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compression Speed&lt;/cell&gt;
        &lt;cell&gt;220 MB/s&lt;/cell&gt;
        &lt;cell&gt;3.5 MB/s&lt;/cell&gt;
        &lt;cell&gt;340 MB/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Decompression Speed&lt;/cell&gt;
        &lt;cell&gt;850 MB/s&lt;/cell&gt;
        &lt;cell&gt;45 MB/s&lt;/cell&gt;
        &lt;cell&gt;1200 MB/s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Crucially, OpenZL produces a higher compression ratio while preserving or even improving speed, which is critical for data center processing pipelines.&lt;/p&gt;
    &lt;p&gt;For illustration, this result is achieved using the following simple graph:&lt;/p&gt;
    &lt;head rend="h3"&gt;A Brief Explanation&lt;/head&gt;
    &lt;p&gt;So what is happening in this example?&lt;/p&gt;
    &lt;p&gt;We start by separating the header from the rest, a large table of structures. Then each field gets extracted into its own stream: the array of structures becomes a structure of arrays. After that point, we expect that each stream contains homogeneous data of the same type and semantic meaning. We can now focus on finding an optimal compression strategy for each one.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SRA0 is a position on the X axis. Due to the way the table is generated, the index is mostly sorted, inviting the use of delta to reduce the range of values represented. This mechanically makes the resulting stream easier to compress.&lt;/item&gt;
      &lt;item&gt;SDEC0 is a position on the Y axis. It‚Äôs not as well sorted as the X axis, but we can at least exploit the fact that it‚Äôs bounded between a minimum and a maximum. This makes the higher bytes more predictable, which can be exploited for better compression with the transpose operation.&lt;/item&gt;
      &lt;item&gt;The other fields (IS, MAG, XRPM, XDPM) share a common property: their cardinality is much lower than their quantities, and there is no relation between 2 consecutive values. This makes them a good target for tokenize, which will convert the stream into a dictionary and an index list.&lt;/item&gt;
      &lt;item&gt;The resulting dictionaries and index lists are very different. They benefit from completely different compression strategies. So they are sent to dedicated processing graphs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The graph continues beyond these steps. But at some point, we can also stop making decisions. The main work is to group data into homogeneous streams. After that, one can count on openzl to take care of the rest.&lt;/p&gt;
    &lt;p&gt;To go even further, we would like to generate compression strategies that are specifically fine-tuned for each stream. This is where the offline trainer stage comes into play.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generate a Compressor Automatically&lt;/head&gt;
    &lt;p&gt;It‚Äôs possible to take full control of the compression process, but it‚Äôs also not required. A faster strategy is to just describe your data and let the system learn a compression config.&lt;/p&gt;
    &lt;p&gt;Describe the input: With the Simple Data Description Language (SDDL), you sketch how the bytes map to fields ‚Äî rows, columns, enums, nested records. SDDL is for parsing only; it just tells OpenZL the shape of your data. Alternatively, you can write your own parser function directly using one of the supported languages, and register it with OpenZL to delegate the logic.&lt;/p&gt;
    &lt;p&gt;Learn the config: Starting from a preset, a parser function or an SDDL description, the trainer runs a budgeted search over transform choices and parameters to produce a Plan. It can provide a full set of speed/ratio tradeoffs, or directly target the best configuration respecting some speed constraints. Internally it uses a cluster finder (to group fields that behave alike) and a graph explorer (to try candidate subgraphs and keep score).&lt;/p&gt;
    &lt;p&gt;Resolve at encode-time: While compressing, the encoder turns the Plan into a concrete recipe ‚Äî the Resolved Graph. If the Plan has control points, it picks the branch that fits the data and records that choice into the frame.&lt;/p&gt;
    &lt;p&gt;Decode without coordination: Each frame chunk carries its own resolved graph. The single decoder checks it, enforces limits, and runs the steps in order. When a plan improves, you just roll out the new plan, no new decompressor needed. Old data keeps decoding; new data get improved gains.&lt;/p&gt;
    &lt;p&gt;In practice the loop is straightforward: describe (SDDL) ‚Üí train (produce a plan) ‚Üí compress (emit frames with resolved graphs) ‚Üí decode anywhere with the same binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Embracing Changes: Re-Training and In-Flight Control&lt;/head&gt;
    &lt;p&gt;In the real world, data evolves constantly, in both structure and content. A compressor built for one version of a schema would have a short lifetime.&lt;/p&gt;
    &lt;p&gt;Thankfully, with the flexibility offered by compression plans, we can react swiftly to data changes. At Meta, this is the core mission of Managed Compression, originally created to automate dictionary compression with Zstandard, and presented in an earlier blog on how we improved compression at with Zstandard.&lt;/p&gt;
    &lt;p&gt;OpenZL offers a training process that updates compression plans to maintain or improve compression performance, based on provided data samples. Now the synergy with Managed Compression is apparent: Each registered use case is monitored, sampled, periodically re-trained, and receives new configs when they prove beneficial. The decompression side continues to decode both old and new data without any change.&lt;/p&gt;
    &lt;p&gt;Runtime Adaptation: A compression config can include control points that read lightweight statistics at compression time (e.g., string repetition stats, run-length, histogram skew, delta variance) and choose the best branch of the Plan to go to next. Many technologies can be used, and textbook classifiers qualify. Control points handle bursts, outliers, and seasonal shifts without brute-force exploration: exploration is bounded, in order to maintain speed expectations. Taken branches are then recorded into the frame, and the decoder just executes the recorded path.&lt;/p&gt;
    &lt;p&gt;This gives the best of both worlds: dynamic behavior at compression time to handle variations and exceptions ‚Äî without turning compression into an unbounded search problem ‚Äî and with zero complexity added to the decoder.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Advantages of the Universal Decoder&lt;/head&gt;
    &lt;p&gt;OpenZL is capable of compressing a vast array of data formats, and they can all be decompressed with a single decompressor binary. Even when the compression configuration changes, the decoder does not. This may sound like operational minutiae, but it‚Äôs critical to OpenZL‚Äôs deployment success.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One audited surface: Security and correctness reviews focus on a single binary with consistent invariants, fuzzing, and hardening; there‚Äôs no myriad of per-format tools that can drift apart.&lt;/item&gt;
      &lt;item&gt;Fleet-wide improvements: A decoder update (security or performance ‚Äî SIMD kernels, memory bounds, scheduling) benefits every compressed file, even those that predate the change.&lt;/item&gt;
      &lt;item&gt;Operational clarity: Same binary, same CLI, same metrics and dashboards across datasets; patching and rollout are uneventful by design.&lt;/item&gt;
      &lt;item&gt;Continuous training: With one decoder and many compression plans, we can keep improving while the system is live. Train a plan offline, try it on a small slice, then roll it out like any other config change. Backward compatibility is built-in ‚Äî old frames still decode while new frames get better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, it‚Äôs possible to afford domain-specific compression without fragmenting the ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results With OpenZL&lt;/head&gt;
    &lt;p&gt;When OpenZL is able to understand and parse the file format, it is able to offer large improvements in compression ratio, while still providing fast compression and decompression speed. However, this is no magic bullet. When OpenZL doesn‚Äôt understand the input file format, it simply falls back to zstd.&lt;/p&gt;
    &lt;p&gt;OpenZL, through its offline training capabilities, is also able to offer a wide range of configurations in the tradeoff space of compression ratio, compression speed, and decompression speed. Unlike traditional compressors, which offer configuration by setting a compression level, OpenZL offers configuration by serializing the compressor graph. This allows an immense amount of flexibility to select diverse tradeoffs.&lt;/p&gt;
    &lt;p&gt;These results are based on datasets we‚Äôve developed for our whitepaper. The datasets were chosen because they are highly structured and in a format that OpenZL supports. Every figure below is produced with scripts in the OpenZL repository so they can be reproduced, and the input data and logs from our runs have been uploaded to GitHub.&lt;/p&gt;
    &lt;p&gt;Note that data points connected by a line are pareto-optimal. All such points have the property that there is no point in the same dataset which beats them in both metrics.&lt;/p&gt;
    &lt;head rend="h3"&gt;When It‚Äôs Not Useful&lt;/head&gt;
    &lt;p&gt;OpenZL relies on a description of some structure to leverage its set of transforms. When there is no structure, there is no advantage. This is typically the case in pure text documents, such as enwik or dickens. In these cases, OpenZL falls back to zstd, offering essentially the same level of performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started With OpenZL&lt;/head&gt;
    &lt;p&gt;OpenZL‚Äôs selection of codecs is well-suited to compressing vector, tabular, or tree-structured data, and can be expected to perform well with numeric, string, or binary data. Common examples include timeseries datasets, ML tensors, and database tables. Keep in mind that we are bound by the limits of information theory, so the input needs to have some order that can be uncovered. As time goes on, we plan to incorporate additional codecs, as described in the next section.&lt;/p&gt;
    &lt;p&gt;If your data fits one of the above categories, then give it a try! Visit the OpenZL site and our Quick Start guide to get started.&lt;/p&gt;
    &lt;p&gt;If you want to dive into the code, check out the GitHub repository for source, documentation, and examples. We welcome contributions and feedback from the community!&lt;/p&gt;
    &lt;head rend="h2"&gt;Where We‚Äôre Going&lt;/head&gt;
    &lt;p&gt;OpenZL‚Äôs general direction is set: make it easier to expose structures, and exploit it with automated compression plans for evolving data.&lt;/p&gt;
    &lt;p&gt;Next up: We‚Äôre extending the transform library for time-series and grid-shaped data, improving performance of codecs, and enabling the trainer to find better compression plans faster. We also are actively working to extend SDDL to describe nested data formats more flexibly. Finally, the automated compressor explorer is getting better at proposing safe, testable changes to a compression plan within a specified budget.&lt;/p&gt;
    &lt;p&gt;Where the community can help: If you have a format or a dataset with obvious structure, try compressing it with an OpenZL prebuilt Plan. If it‚Äôs promising, try generating a new plan with the trainer or customizing it with our documentation to improve it. If it‚Äôs a format that the public might want, send it to us in a PR.&lt;/p&gt;
    &lt;p&gt;You can also contribute to the OpenZL core. If you have a knack for optimizing C/C++, help us speed up the engine or add transforms to cover new data formats. If your super power is reliability, the project would surely benefit from more validation rules and resource caps. And if you care about benchmarks, add your dataset to the harness so others can reproduce your results.&lt;/p&gt;
    &lt;p&gt;How to engage: Open an issue on the GitHub issue board. If you have a use-case for which you would expect OpenZL to do better, provide a few small samples, so that we can analyze them together. You may also contribute to codec optimizations, and propose new graphs, parsers or control points. All these topics do not impact the universality of the decoder.&lt;/p&gt;
    &lt;p&gt;We believe OpenZL opens up a new universe of possibilities to the data compression field, and we‚Äôre excited to see what the open source community will do with it!&lt;/p&gt;
    &lt;p&gt;To learn more about Meta Open Source, visit our website, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, Bluesky and LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/"/><published>2025-10-06T16:01:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45494558</id><title>Apps SDK</title><updated>2025-10-07T13:43:34.678988+00:00</updated><content>&lt;doc fingerprint="9dca97fe33d1af9f"&gt;
  &lt;main&gt;
    &lt;p&gt;Our framework to build apps for ChatGPT.&lt;/p&gt;
    &lt;p&gt;Design components and conversational flows that feel native to ChatGPT.&lt;/p&gt;
    &lt;p&gt;Build apps that meet our quality, safety, and policy standards.&lt;/p&gt;
    &lt;p&gt;Identify and prioritize Apps SDK use cases.&lt;/p&gt;
    &lt;p&gt;Create and configure an MCP server.&lt;/p&gt;
    &lt;p&gt;Learn how to deploy your MCP server&lt;/p&gt;
    &lt;p&gt;Improve discovery and behavior with rich metadata.&lt;/p&gt;
    &lt;p&gt;Security and privacy considerations for Apps SDK.&lt;/p&gt;
    &lt;p&gt;Troubleshoot issues in Apps SDK apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://developers.openai.com/apps-sdk/"/><published>2025-10-06T18:27:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45497027</id><title>RediShell: Critical remote code execution vulnerability in Redis</title><updated>2025-10-07T13:43:34.377132+00:00</updated><content>&lt;doc fingerprint="3f7819f38f20f501"&gt;
  &lt;main&gt;
    &lt;p&gt;Wiz Research has uncovered a critical Remote Code Execution (RCE) vulnerability, CVE-2025-49844 which we've dubbed #RediShell, in the widely used Redis in-memory data structure store. The vulnerability has been assigned a CVSS score of 10.0 - the highest possible severity (note that we have seen this listed as a 9.9 in some places, depending on the source).&lt;/p&gt;
    &lt;p&gt;The vulnerability exploits a Use-After-Free (UAF) memory corruption bug that has existed for approximately 13 years in the Redis source code. This flaw allows a post auth attacker to send a specially crafted malicious Lua script (a feature supported by default in Redis) to escape from the Lua sandbox and achieve arbitrary native code execution on the Redis host. This grants an attacker full access to the host system, enabling them to exfiltrate, wipe, or encrypt sensitive data, hijack resources, and facilitate lateral movement within cloud environments.&lt;/p&gt;
    &lt;p&gt;Given that Redis is used in an estimated 75% of cloud environments, the potential impact is extensive. Organizations are strongly urged to patch instances immediately by prioritizing those that are exposed to the internet.&lt;/p&gt;
    &lt;p&gt;On October 3, Redis released a security advisory along with a patched version of Redis. We extend our gratitude to the entire Redis team for their collaboration throughout the disclosure process. We greatly appreciate their transparency, responsiveness, and partnership during this engagement.&lt;/p&gt;
    &lt;p&gt;In this post, we will provide a high-level overview of our discovery and its implications. Given the prevalence and sensitivity of this vulnerability, we will defer some of the technical details to a future installment, omitting exploit information for now to allow impacted organizations sufficient time to address the vulnerability.&lt;/p&gt;
    &lt;p&gt;Organizations utilizing Redis are strongly encouraged to update their Redis instances to the latest version immediately.&lt;/p&gt;
    &lt;p&gt;Vulnerability Meets Ubiquity: The Redis Risk Multiplier&lt;/p&gt;
    &lt;p&gt;The newly disclosed RediShell (CVE-2025-49844) vulnerability in Redis has been assigned a CVSS score of 10.0 - a rating rarely seen, with only around 300 vulnerabilities receiving it in the past year. It‚Äôs also the first Redis vulnerability to be rated as critical. The score reflects not just the technical severity of remote code execution, but also how Redis is commonly used and deployed. Redis is widely used in cloud environments for caching, session management, and pub/sub messaging. While Redis has had a strong security history, the combination of this flaw and common deployment practices significantly increases its potential impact.&lt;/p&gt;
    &lt;p&gt;Scope&lt;/p&gt;
    &lt;p&gt;Wiz Research discovered a Remote Code Execution vulnerability CVE-2025-49844 affecting the widely used Redis database. The vulnerability is a Use-After-Free (UAF) memory corruption that allows an attacker to send a malicious Lua script that leads to arbitrary code execution outside Redis‚Äôs Lua interpreter sandbox, gaining access to the host.&lt;/p&gt;
    &lt;p&gt;The urgency with which you should address this vulnerability depends on how Redis was installed and its exposure level.&lt;/p&gt;
    &lt;p&gt;Exposure Analysis&lt;/p&gt;
    &lt;p&gt;Our analysis across cloud environments revealed the extensive scope of this vulnerability:&lt;/p&gt;
    &lt;p&gt;Approximately 330,000 Redis instances are exposed to the internet at the time of this blog post&lt;/p&gt;
    &lt;p&gt;About 60,000 instances have no authentication configured&lt;/p&gt;
    &lt;p&gt;57% of cloud environments install Redis as container images, many without proper security hardening&lt;/p&gt;
    &lt;p&gt;The official Redis container, by default, does not require authentication. Our analysis shows that 57% of cloud environments install Redis as an image. If not installed carefully, these instances may lack authentication entirely. The combination of no authentication and exposure to the internet is highly dangerous, allowing anyone to query the Redis instance and, specifically, send Lua scripts (which are enabled by default). This enables attackers to exploit the vulnerability and achieve RCE within the environment.&lt;/p&gt;
    &lt;p&gt;High Risk - Internal Network Exposure:&lt;/p&gt;
    &lt;p&gt;More Redis instances are exposed to internal networks where authentication may not be prioritized, allowing any host in the local network to connect to the database server. An attacker with a foothold in the cloud environment could gain access to sensitive data and exploit the vulnerability to run arbitrary code for lateral movement into sensitive networks.&lt;/p&gt;
    &lt;p&gt;Attack Flow and Impact&lt;/p&gt;
    &lt;p&gt;The attack sequence demonstrates how an attacker can exploit RediShell (CVE-2025-49844) to achieve comprehensive system compromise:&lt;/p&gt;
    &lt;p&gt;Initial Exploitation&lt;/p&gt;
    &lt;p&gt;Attacker sends a malicious Lua script to exploit the use-after-free vulnerability&lt;/p&gt;
    &lt;p&gt;Sandbox Escape&lt;/p&gt;
    &lt;p&gt;Script escapes the Lua sandbox and achieves arbitrary code execution&lt;/p&gt;
    &lt;p&gt;Establishes reverse shell for persistent access&lt;/p&gt;
    &lt;p&gt;System Compromise&lt;/p&gt;
    &lt;p&gt;Steals credentials (.ssh keys, IAM tokens, certificates)&lt;/p&gt;
    &lt;p&gt;Installs malware or crypto miners&lt;/p&gt;
    &lt;p&gt;Exfiltrates sensitive data from Redis and host&lt;/p&gt;
    &lt;p&gt;Lateral Movement&lt;/p&gt;
    &lt;p&gt;Uses stolen IAM tokens to access other cloud services&lt;/p&gt;
    &lt;p&gt;Escalates privileges and moves to additional systems&lt;/p&gt;
    &lt;p&gt;The Result: Host Remote Code Execution&lt;/p&gt;
    &lt;p&gt;**We recommend that all Redis users upgrade their instances immediately, as this vulnerability poses a significant risk.**&lt;/p&gt;
    &lt;p&gt;Disclosure Timeline&lt;/p&gt;
    &lt;p&gt;May 16, 2025: Initial vulnerability report sent to Redis in Pwn2Own Berlin.&lt;/p&gt;
    &lt;p&gt;Oct 3, 2025: Redis publishes the security bulletin and assigned CVE-2025-49844.&lt;/p&gt;
    &lt;p&gt;Oct 6, 2025: Wiz Research publishes this blog post.&lt;/p&gt;
    &lt;p&gt;Recommended Actions&lt;/p&gt;
    &lt;p&gt;Update Redis Immediately: Upgrade to the latest patched version. Prioritize any internet-exposed or unauthenticated instances.&lt;/p&gt;
    &lt;p&gt;Security Hardening:&lt;/p&gt;
    &lt;p&gt;Enable Redis Authentication: Use the requirepass directive.&lt;/p&gt;
    &lt;p&gt;Disable Unnecessary Commands: This includes Lua scripting if it's not being used. You can achieve this by revoking user scripting permissions via Redis ACLs or by disabling scripting commands.&lt;/p&gt;
    &lt;p&gt;Run with Minimal Privileges: Operate Redis using a non-root user account.&lt;/p&gt;
    &lt;p&gt;Enable Logging and Monitoring: Activate Redis logging and monitoring to track activity and identify potential issues.&lt;/p&gt;
    &lt;p&gt;Restrict Redis Access: Limit access to authorized networks only.&lt;/p&gt;
    &lt;p&gt;How Wiz can help&lt;/p&gt;
    &lt;p&gt;Wiz customers can use the pre-built query and advisory in the Wiz Threat Center to assess the risk in their environment.&lt;/p&gt;
    &lt;p&gt;Wiz identifies both internal and publicly exposed Redis instances in your environment affected by CVE-2025-49844, and alerts you to instances that have been misconfigured to allow unauthenticated access or use weak or default passwords.&lt;/p&gt;
    &lt;p&gt;Conclusion: treat with urgency&lt;/p&gt;
    &lt;p&gt;RediShell (CVE-2025-49844) represents a critical security vulnerability that affects all Redis versions due to its root cause in the underlying Lua interpreter. With hundreds of thousands of exposed instances worldwide, this vulnerability poses a significant threat to organizations across all industries.&lt;/p&gt;
    &lt;p&gt;The combination of widespread deployment, default insecure configurations, and the severity of the vulnerability creates an urgent need for immediate remediation. Organizations must prioritize updating their Redis instances and implementing proper security controls to protect against exploitation.&lt;/p&gt;
    &lt;p&gt;This vulnerability also highlights how deeply today‚Äôs cloud environments depend on open-source technologies like Redis. That shared reliance is what motivated us, alongside other cloud providers, to launchZeroDay.Cloud, a community-driven effort to identify and responsibly disclose critical zero-day vulnerabilities in the open-source software powering the cloud. Redis, along with other core open-source technologies, is part of that effort.&lt;/p&gt;
    &lt;p&gt;Wiz Research will continue to monitor the threat landscape and provide additional technical details in future publications so that organizations have time to implement necessary security measures.&lt;/p&gt;
    &lt;p&gt;For technical questions about this research, please contact: research@wiz.io&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;This research was conducted by the Wiz Research team. We thank the Redis security team for their professional handling of this disclosure and their commitment to user security.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wiz.io/blog/wiz-research-redis-rce-cve-2025-49844"/><published>2025-10-06T22:30:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45497624</id><title>The least amount of CSS for a decent looking site (2023)</title><updated>2025-10-07T13:43:34.080098+00:00</updated><content>&lt;doc fingerprint="8c8abb396bb90f9d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The least amount of CSS for a decent looking site&lt;/head&gt;
    &lt;p&gt;Summary: People often over-engineer solutions, and it leads to them running into problems with their CSS. In this article, we'll take a look at the least amount of CSS that you need to make a decent looking page.&lt;/p&gt;
    &lt;p&gt;The fun part of making a website is that if you write your HTML and nothing else, you have a responsive website.&lt;/p&gt;
    &lt;p&gt;Granted, if you have images they can cause some overflow issues.&lt;/p&gt;
    &lt;p&gt;So we can start things off by fixing that:&lt;/p&gt;
    &lt;code&gt;img {
  max-width: 100%;
  display: block;
}&lt;/code&gt;
    &lt;p&gt;It‚Äôs possible you have videos or SVGs that are also causing problems (less likely with SVGs though), so if you need, you can expand upon this a little bit.&lt;/p&gt;
    &lt;code&gt;img,
svg,
video {
  max-width: 100%;
  display: block;
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Improving the typography&lt;/head&gt;
    &lt;p&gt;The first thing we can do is change the font family since the default is never very exciting.&lt;/p&gt;
    &lt;p&gt;We‚Äôll just use a basic &lt;code&gt;system-ui&lt;/code&gt; for this example. It has pretty good support these days, and looks good on every system without having to worry about loading in any extra fonts.&lt;/p&gt;
    &lt;p&gt;In general, the font-size is a little small as well, so we can bump it up, and the default line-height is always a bit tight, so anything within the 1.5 to 1.7 range should do:&lt;/p&gt;
    &lt;code&gt;body {
  font-family: System UI;
  font-size: 1.25rem;
  line-height: 1.5;
}&lt;/code&gt;
    &lt;p&gt;Though not perfect, this is already a huge improvement over the regular defaults.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding Dark Mode Support&lt;/head&gt;
    &lt;p&gt;Many people love dark mode, so let‚Äôs enable it based on a user‚Äôs system preferences.&lt;/p&gt;
    &lt;p&gt;We can do this by using the &lt;code&gt;color-scheme&lt;/code&gt; property:&lt;/p&gt;
    &lt;code&gt;html {
  color-scheme: light dark;
}&lt;/code&gt;
    &lt;p&gt;This will set the user-agent-styles to either a light or dark theme, based on the users system preferences.&lt;/p&gt;
    &lt;p&gt;If you‚Äôd prefer, we can do this without CSS as well!&lt;/p&gt;
    &lt;code&gt;&amp;lt;html lang="en" color-scheme="light dark"&amp;gt;&amp;lt;/html&amp;gt;&lt;/code&gt;
    &lt;head rend="h3"&gt;A small note on following the system preferences&lt;/head&gt;
    &lt;p&gt;While this is really handy, it is a best practice to allow users to manually toggle the color-scheme as well.&lt;/p&gt;
    &lt;p&gt;Some people prefer a dark system theme, but light website themes, and vice-versa.&lt;/p&gt;
    &lt;head rend="h2"&gt;Restraining Content Width&lt;/head&gt;
    &lt;p&gt;Line-length is one of the most important things when it comes to the readability of text.&lt;/p&gt;
    &lt;p&gt;We generally want to try and fall somewhere in the 45-90 characters per line range (for body text, not headlines).&lt;/p&gt;
    &lt;p&gt;To make the website more readable, we‚Äôll limit the content width using a &lt;code&gt;main&lt;/code&gt; element and some CSS magic:&lt;/p&gt;
    &lt;code&gt;main {
  max-width: min(70ch, 100% - 4rem);
  margin-inline: auto;
}&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;min()&lt;/code&gt; function here will pick whatever is smallest, either &lt;code&gt;70ch&lt;/code&gt; or &lt;code&gt;100% - 4rem&lt;/code&gt;. Because we are inside a &lt;code&gt;min()&lt;/code&gt; function, we don‚Äôt need to use a &lt;code&gt;calc()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Whatever the output from that min() function, the width is less than 100%, so the page will be stuck to the left side of the viewport.&lt;/p&gt;
    &lt;p&gt;We can then use margin-inline: auto to center it, as this acts on the margins on the inline axis, so in any horizontal writing modes, that means both the margin-left and margin-right are auto.&lt;/p&gt;
    &lt;p&gt;You might want to switch out the main selector for a .container or .wrapper so you can have more control over where you use it.&lt;/p&gt;
    &lt;p&gt;And with that, our final CSS file looks like this:&lt;/p&gt;
    &lt;code&gt;html {
  color-scheme: light dark;
}

body {
  font-family: system-ui;
  font-size: 1.25rem;
  line-height: 1.5;
}

img,
svg,
video {
  max-width: 100%;
  display: block;
}

main {
  max-width: min(70ch, 100% - 4rem);
  margin-inline: auto;
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Build on top of this&lt;/head&gt;
    &lt;p&gt;This is just a quick start to get things off the ground, though it could be used for a very simple page as well.&lt;/p&gt;
    &lt;p&gt;For the most part, though, you‚Äôll probably want to build on top of this, but it should be able to act as a nice jumping off point!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thecascade.dev/article/least-amount-of-css/"/><published>2025-10-06T23:47:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45499170</id><title>Pdoc ‚Äì Generate API documentation for Python projects</title><updated>2025-10-07T13:43:33.871756+00:00</updated><content>&lt;doc fingerprint="16c91461fc9ed2c5"&gt;
  &lt;main&gt;&lt;p&gt;&lt;code&gt;pdoc&lt;/code&gt; auto-generates API documentation that follows your project's Python module hierarchy.
                It requires no configuration, has first-class support for type annotations,
                cross-links between identifiers, comes with an integrated live-reloading web server,
                and understands numpydoc or Google-style docstrings.
            &lt;/p&gt;&lt;head rend="h2"&gt;Installation&lt;/head&gt;&lt;p&gt;Latest Release: 15.0.4&lt;/p&gt; Documentation Changelog PyPI GitHub &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pdoc.dev/"/><published>2025-10-07T03:40:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45499281</id><title>California law forces Netflix, Hulu to turn down ad volumes</title><updated>2025-10-07T13:43:33.661355+00:00</updated><content/><link href="https://www.politico.com/news/2025/10/06/dial-it-down-california-forces-netflix-hulu-to-lower-ad-volume-00595663"/><published>2025-10-07T04:03:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45500485</id><title>Deloitte to refund the Australian government after using AI in $440k report</title><updated>2025-10-07T13:43:33.480383+00:00</updated><content>&lt;doc fingerprint="b60d88152e1fdfd2"&gt;
  &lt;main&gt;
    &lt;p&gt;Deloitte will provide a partial refund to the federal government over a $440,000 report that contained several errors, after admitting it used generative artificial intelligence to help produce it.&lt;/p&gt;
    &lt;p&gt;The Department of Employment and Workplace Relations (DEWR) confirmed Deloitte would repay the final instalment under its contract, which will be made public after the transaction is finalised. It comes as one Labor senator accused the consultancy firm of having a ‚Äúhuman intelligence problem‚Äù.&lt;/p&gt;
    &lt;p&gt;Deloitte was commissioned by the department to review the targeted compliance framework and its IT system, used to automate penalties in the welfare system if mutual obligations weren‚Äôt met by jobseekers, in December 2024.&lt;/p&gt;
    &lt;p&gt;Sign up: AU Breaking News email&lt;/p&gt;
    &lt;p&gt;The subsequent report found widespread issues, including a lack of ‚Äútraceability‚Äù between the rules of the framework and the legislation behind it, as well as ‚Äúsystem defects‚Äù. It said an IT system was ‚Äúdriven by punitive assumptions of participant non-compliance‚Äù.&lt;/p&gt;
    &lt;p&gt;It was first published on 4 July. It was re-uploaded to the DEWR website on Friday, after the Australian Financial Review in August reported that multiple errors had been found, including nonexistent references and citations.&lt;/p&gt;
    &lt;p&gt;University of Sydney academic, Dr Christopher Rudge, who first highlighted the errors, said the report contained ‚Äúhallucinations‚Äù where AI models may fill in gaps, misinterpret data, or try to guess answers.&lt;/p&gt;
    &lt;p&gt;‚ÄúInstead of just substituting one hallucinated fake reference for a new ‚Äòreal‚Äô reference, they‚Äôve substituted the fake hallucinated references and in the new version, there‚Äôs like five, six or seven or eight in their place,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;‚ÄúSo what that suggests is that the original claim made in the body of the report wasn‚Äôt based on any one particular evidentiary source.‚Äù&lt;/p&gt;
    &lt;p&gt;The updated review noted a ‚Äúsmall number of corrections to references and footnotes‚Äù, but the department has said there have been no changes to the review‚Äôs recommendations.&lt;/p&gt;
    &lt;p&gt;‚ÄúDeloitte conducted the independent assurance review and has confirmed some footnotes and references were incorrect,‚Äù a spokesperson for the department said.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe substance of the independent review is retained, and there are no changes to the recommendations.‚Äù&lt;/p&gt;
    &lt;p&gt;In the updated version of the report, Deloitte added reference to the use of generative AI in its appendix. It states that a part of the report ‚Äúincluded the use of a generative artificial intelligence (AI) large language model (Azure OpenAI GPT ‚Äì 4o) based tool chain licensed by DEWR and hosted on DEWR‚Äôs Azure tenancy.‚Äù&lt;/p&gt;
    &lt;p&gt;Deloitte did not state that artificial intelligence was the reason behind the errors in its original report. It also stood by the original findings of the review.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe updates made in no way impact or affect the substantive content, findings and recommendations in the report,‚Äù it stated in the amended version.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Deloitte said ‚Äúthe matter has been resolved directly with the client‚Äù.&lt;/p&gt;
    &lt;p&gt;Rudge said that, despite his criticism, he hesitates to say the whole report should be ‚Äúregarded as illegitimate‚Äù, because the conclusions concur with other widespread evidence.&lt;/p&gt;
    &lt;p&gt;Labor senator Deborah O‚ÄôNeill, who was on a senate inquiry into the integrity of consulting firms, said it looked like ‚ÄúAI is being left to do the heavy lifting‚Äù.&lt;/p&gt;
    &lt;p&gt;‚ÄúDeloitte has a human intelligence problem. This would be laughable if it wasn‚Äôt so lamentable. A partial refund looks like a partial apology for substandard work,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;‚ÄúAnyone looking to contract these firms should be asking exactly who is doing the work they are paying for, and having that expertise and no AI use verified.&lt;/p&gt;
    &lt;p&gt;‚ÄúPerhaps instead of a big consulting firm, procurers would be better off signing up for a ChatGPT subscription.‚Äù&lt;/p&gt;
    &lt;p&gt;The AFR found several incorrect references in the original report, including nonexistent reports by professors at the University of Sydney and the Lund University in Sweden.&lt;/p&gt;
    &lt;p&gt;The paper also reported a made-up reference to a court decision in a robodebt case, Deanna Amato v Commonwealth. Deloitte wrote in its final report that the update ‚Äúamend[ed] the summary of the Amato proceeding which contained errors‚Äù.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report"/><published>2025-10-07T07:51:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45501114</id><title>The Mondrian introduction to functional optics</title><updated>2025-10-07T13:43:33.342692+00:00</updated><content>&lt;doc fingerprint="13dc85e44d4ba5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Mondrian introduction to functional optics&lt;/head&gt;
    &lt;p&gt;In this post I‚Äôd like to try to discuss what functional optics are, without going too much into why they are so cool, and you should use them, or how they are implemented1 and should be used with a specific language and library.&lt;/p&gt;
    &lt;p&gt;I personally think that functional optics should be a really easy concept to grasp, but currently learning them is harder than it should be mostly due to library implementation details, quite obscure documentation and an exotic usage of weird symbols.&lt;/p&gt;
    &lt;p&gt;Since a picture is worth a thousand words, I will introduce and use a graphical notation to illustrate the concepts we will discuss.&lt;/p&gt;
    &lt;head rend="h2"&gt;Types and values&lt;/head&gt;
    &lt;p&gt;Let‚Äôs start introducing our graphical notation from its basic building blocks.&lt;/p&gt;
    &lt;p&gt;We can represent a type with a simple coloured rectangle&lt;/p&gt;
    &lt;p&gt;A value for a given type will be represented as a horizontal line spanning the width of the rectangle&lt;/p&gt;
    &lt;head rend="h2"&gt;Sums and products&lt;/head&gt;
    &lt;p&gt;When considering algebraic data types, we have two ways of combining types, using products and sums.&lt;/p&gt;
    &lt;p&gt;The product of two types &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; is a new type, which we will denote by &lt;code&gt;A*B&lt;/code&gt;, whose values are composed of a value of type &lt;code&gt;A&lt;/code&gt; and a value of type &lt;code&gt;B&lt;/code&gt;. An example of a product type is a tuple like &lt;code&gt;(Int, String)&lt;/code&gt; where each value is pair composed of an integer and a string.&lt;/p&gt;
    &lt;p&gt;Graphically we can represent a product type as two side by side rectangles&lt;/p&gt;
    &lt;p&gt;When it comes to values, we need to upgrade a little bit our graphical interpretation. Since a value in a product type is composed of values of its components, we will just represent it as piecewise horizontal line, composed by horizontal lines (possible at different heights) spanning its horizontal sub-rectangles.&lt;/p&gt;
    &lt;p&gt;On the other hand, the sum of two types is represented by two rectangles one on top of the other&lt;/p&gt;
    &lt;p&gt;A value of a sum type is a horizontal line spanning the width of the whole rectangle. If it is a horizontal line in the top rectangle, it means that we are selecting the first type, and we‚Äôre using one of its values.&lt;/p&gt;
    &lt;p&gt;If it is a horizontal line in the bottom rectangle, it means that we are selecting the second type and one of its values.&lt;/p&gt;
    &lt;p&gt;More generally, for any algebraic data type, we can represent it as a sum of products by stacking a series of rectangles one on top of the other, each one potentially divided horizontally in multiple sub-rectangles.&lt;/p&gt;
    &lt;p&gt;In general, we can continue to split any sub-rectangle horizontally or vertically (if you prefer a top-down point of view) or you can place two rectangles side by side or top to bottom.&lt;/p&gt;
    &lt;p&gt;A value of such a type is a piecewise horizontal line which can not cross a horizontal division.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optics&lt;/head&gt;
    &lt;p&gt;Now that we have this graphical representation to represent data types, we can use it to discuss various kinds of optics.&lt;/p&gt;
    &lt;p&gt;In general, we can think of an optic as a way to select, given our graphical representation of a type, one (or more) rectangle inside a given rectangle representing a type. For example in the following picture we are selecting the rectangle with the red boundary inside the main rectangle representing a complex type.&lt;/p&gt;
    &lt;p&gt;If call the main type &lt;code&gt;A&lt;/code&gt; and the selected type &lt;code&gt;B&lt;/code&gt;, we will denote the optic selecting &lt;code&gt;B&lt;/code&gt; inside &lt;code&gt;A&lt;/code&gt; with &lt;code&gt;Optic A B&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Before going into inspecting the various kinds of optics, let‚Äôs try to see if can can already derive some properties of optics just by looking at their graphical representation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compositionality&lt;/head&gt;
    &lt;p&gt;One thing that we can notice is that optics compose really well. Suppose we have a type &lt;code&gt;A&lt;/code&gt; represented by the following diagram&lt;/p&gt;
    &lt;p&gt;We can first select a sub-rectangle identifying a type &lt;code&gt;B&lt;/code&gt; with an &lt;code&gt;Optic A B&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Starting now with the type &lt;code&gt;B&lt;/code&gt; we can use an &lt;code&gt;Optic B C&lt;/code&gt; to select a type &lt;code&gt;C&lt;/code&gt; inside &lt;code&gt;B&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Using now the &lt;code&gt;Optic A B&lt;/code&gt; and the &lt;code&gt;Optic B C&lt;/code&gt; we just chose, we can compose them to obtain an &lt;code&gt;Optic A C&lt;/code&gt; which directly selects &lt;code&gt;C&lt;/code&gt; inside &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This optic composition operation is actually associative and has an identity element, turning optics into a Category.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs now start to have a look at some specific families of optics.&lt;/p&gt;
    &lt;head rend="h2"&gt;Iso&lt;/head&gt;
    &lt;p&gt;The simplest optic we can define for any type &lt;code&gt;A&lt;/code&gt; is the one that we can obtain by selecting the whole rectangle.&lt;/p&gt;
    &lt;p&gt;With such a selection we can see that for any value of the outer type &lt;code&gt;A&lt;/code&gt;, we actually have a value of the type identified by the red rectangle, which we will call &lt;code&gt;B&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This means that, given an &lt;code&gt;Iso A B&lt;/code&gt;, we can actually define a function &lt;code&gt;view :: A -&amp;gt; B&lt;/code&gt; that for any value of &lt;code&gt;A&lt;/code&gt; gives us a value of &lt;code&gt;B&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;But in this special case also the converse holds! For any value of &lt;code&gt;B&lt;/code&gt;, since &lt;code&gt;B&lt;/code&gt; is actually &lt;code&gt;A&lt;/code&gt; itself, we have in fact a value of &lt;code&gt;A&lt;/code&gt;. This gives rise to a function &lt;code&gt;review :: B -&amp;gt; A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In fact &lt;code&gt;review . view = id_A&lt;/code&gt; and &lt;code&gt;view . review = id_B&lt;/code&gt; giving rise to a proper isomorphism.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lens&lt;/head&gt;
    &lt;p&gt;In our graphical representation, a &lt;code&gt;Lens&lt;/code&gt; is a vertical slice of the main rectangle.&lt;/p&gt;
    &lt;p&gt;Any vertical slice cuts out a piece out of any horizontal line. In other terms, given a value of the type &lt;code&gt;A&lt;/code&gt; represented by the main rectangle, we have a way to obtain a value of type &lt;code&gt;B&lt;/code&gt; represented by our vertical slice. This means that also in this case we are able to define a function &lt;code&gt;view :: A -&amp;gt; B&lt;/code&gt; which allows us to focus from the main type to one of its component.&lt;/p&gt;
    &lt;p&gt;On the other hand, it‚Äôs not possible with &lt;code&gt;Lens&lt;/code&gt;es as it was with &lt;code&gt;Iso&lt;/code&gt;s to build back a value of type &lt;code&gt;A&lt;/code&gt; from a value of type &lt;code&gt;B&lt;/code&gt;, since a value of type &lt;code&gt;B&lt;/code&gt; is only a part of value of type &lt;code&gt;A&lt;/code&gt;. What is actually possible, though, is to update only the part included in the red rectangle of a value of type &lt;code&gt;A&lt;/code&gt;. In other terms, given a &lt;code&gt;Lens A B&lt;/code&gt;, we can define a function &lt;code&gt;set :: B -&amp;gt; A -&amp;gt; A&lt;/code&gt; which takes a value of type &lt;code&gt;B&lt;/code&gt; and a value of type &lt;code&gt;A&lt;/code&gt; and updates the section of the latter identified by the &lt;code&gt;Lens&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Having a look at the graphical representations of the &lt;code&gt;view&lt;/code&gt; and &lt;code&gt;set&lt;/code&gt; functions, we can convince ourselves that the following properties hold:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If we &lt;code&gt;set&lt;/code&gt;a value and then we&lt;code&gt;view&lt;/code&gt;it, we must get back what we put in:&lt;code&gt;view (set b a) == b&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If we &lt;code&gt;set&lt;/code&gt;what we get out of a&lt;code&gt;view&lt;/code&gt;, nothing changes:&lt;code&gt;set (view a) a == a&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Setting a value twice is the same thing as setting it once: &lt;code&gt;set b (set b a) == set b a&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Moreover, we can notice that composing two lenses with the operation described in the Compositionality section gives us back another lens. A vertical slice of a vertical slice is in fact still a vertical slice of the original rectangle. In other terms this means that &lt;code&gt;Lens&lt;/code&gt;es form a subcategory of the bigger category of &lt;code&gt;Optic&lt;/code&gt;s.&lt;/p&gt;
    &lt;p&gt;Composing adequately &lt;code&gt;set&lt;/code&gt; and &lt;code&gt;view&lt;/code&gt; we can also define a function &lt;code&gt;over :: (B -&amp;gt; B) -&amp;gt; A -&amp;gt; A&lt;/code&gt; as &lt;code&gt;over f a = set (f $ view a) a&lt;/code&gt;. This means that if we have a function &lt;code&gt;f :: B -&amp;gt; B&lt;/code&gt; which can transform values of type &lt;code&gt;B&lt;/code&gt;, we can use our lens to extract a &lt;code&gt;B&lt;/code&gt; from an &lt;code&gt;A&lt;/code&gt; via &lt;code&gt;view&lt;/code&gt;, use &lt;code&gt;f&lt;/code&gt; to transform the result, and eventually use &lt;code&gt;set&lt;/code&gt; to update the &lt;code&gt;B&lt;/code&gt; part inside the original &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prism&lt;/head&gt;
    &lt;p&gt;If vertical slices are &lt;code&gt;Lens&lt;/code&gt;es, it is only natural to wonder what are horizontal slices. They correspond to &lt;code&gt;Prism&lt;/code&gt;s, and they are the dual concept of &lt;code&gt;Lens&lt;/code&gt;es. Where a &lt;code&gt;Lens&lt;/code&gt; represents a component in a product type, a &lt;code&gt;Prism&lt;/code&gt; represents a component in a sum type.&lt;/p&gt;
    &lt;p&gt;Looking at values, we can notice that a value in the main type could either be a value of the inner type or it could be completely outside of it. This implies that, given a &lt;code&gt;Prism A B&lt;/code&gt;, we can define a function &lt;code&gt;preview :: A -&amp;gt; Maybe B&lt;/code&gt; which, given a value &lt;code&gt;a :: A&lt;/code&gt; returns a &lt;code&gt;Just b&lt;/code&gt; if &lt;code&gt;a&lt;/code&gt; was inside the sub-rectangle identified by &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;Nothing&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;On the other hard, since a &lt;code&gt;Prism&lt;/code&gt; constitutes a horizontal slice of the main rectangle, if we have a value of the sub-rectangle, we can always interpret it a value of the main rectangle. In other words, this means that for a &lt;code&gt;Prism A B&lt;/code&gt; we can always define a function &lt;code&gt;review :: B -&amp;gt; A&lt;/code&gt; constructing a value of type &lt;code&gt;A&lt;/code&gt; from a value of type &lt;code&gt;B&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Again, having a look at the graphical representation we can convince ourselves that the following properties hold for &lt;code&gt;Prism&lt;/code&gt;s:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If we preview through a &lt;code&gt;Prism&lt;/code&gt;what we just built using the same&lt;code&gt;Prism&lt;/code&gt;, we will get a value back:&lt;code&gt;preview (review b) == Just b&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If when we preview we get a &lt;code&gt;Just&lt;/code&gt;, then reviewing the result through the same&lt;code&gt;Prism&lt;/code&gt;will get us to the initial value:&lt;code&gt;preview s == Just a =&amp;gt; review a == s&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a &lt;code&gt;Prism A B&lt;/code&gt; it is also possible to define a function &lt;code&gt;set :: B -&amp;gt; A -&amp;gt; A&lt;/code&gt; as &lt;code&gt;set = flip $ const review&lt;/code&gt;. This means that, being able to construct an &lt;code&gt;A&lt;/code&gt; from a &lt;code&gt;B&lt;/code&gt;, we are able to substitute a &lt;code&gt;B&lt;/code&gt; inside an &lt;code&gt;A&lt;/code&gt; just by discarding the initial &lt;code&gt;A&lt;/code&gt; and building a new one from &lt;code&gt;B&lt;/code&gt;. Graphically, we can interpret this as using the &lt;code&gt;B&lt;/code&gt; value in the inner rectangle to build an &lt;code&gt;A&lt;/code&gt; value, forgetting about the initial &lt;code&gt;A&lt;/code&gt; value.&lt;/p&gt;
    &lt;p&gt;At this point we can also define another function &lt;code&gt;over :: (B -&amp;gt; B) -&amp;gt; A -&amp;gt; A&lt;/code&gt; which allows us to update the &lt;code&gt;B&lt;/code&gt; part inside an &lt;code&gt;A&lt;/code&gt;. We can define it as &lt;code&gt;over f a = maybe a review (f &amp;lt;$&amp;gt; preview a)&lt;/code&gt;. In words, we use &lt;code&gt;preview&lt;/code&gt; to get a &lt;code&gt;Maybe B&lt;/code&gt; and we map &lt;code&gt;f&lt;/code&gt; over it to get another &lt;code&gt;Maybe B&lt;/code&gt;; if we have a value &lt;code&gt;Just b&lt;/code&gt;, then we can use it to construct an &lt;code&gt;A&lt;/code&gt; using &lt;code&gt;review&lt;/code&gt;; on the other hand, if we ended up with a &lt;code&gt;Nothing&lt;/code&gt;, we just keep the initial &lt;code&gt;A&lt;/code&gt;. Graphically, we can interpret this as follows: if the &lt;code&gt;A&lt;/code&gt; value is inside the &lt;code&gt;B&lt;/code&gt; sub-rectangle, we apply &lt;code&gt;f&lt;/code&gt; and then we use the result to build a new &lt;code&gt;A&lt;/code&gt; value; if the value is not in &lt;code&gt;B&lt;/code&gt;, we just leave it alone.&lt;/p&gt;
    &lt;p&gt;Looking at the graphical interpretation, it‚Äôs easy to convince ourselves that the composition of two &lt;code&gt;Prism&lt;/code&gt;s is still a &lt;code&gt;Prism&lt;/code&gt;, given that a horizontal slice of a horizontal slice is still a horizontal slice of the main rectangle. In other terms, also &lt;code&gt;Prism&lt;/code&gt;s form a subcategory of the category of &lt;code&gt;Optic&lt;/code&gt;s.&lt;/p&gt;
    &lt;head rend="h2"&gt;Affine traversals&lt;/head&gt;
    &lt;p&gt;Now that we discussed &lt;code&gt;Lens&lt;/code&gt;es and &lt;code&gt;Prism&lt;/code&gt;s, one natural question which might arise is what happens when we try to compose a &lt;code&gt;Lens&lt;/code&gt; and a &lt;code&gt;Prism&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In the picture above we see a &lt;code&gt;Lens&lt;/code&gt; (the blue rectangle) composed with a &lt;code&gt;Prism&lt;/code&gt; (the red rectangle). What we get out of the composition is the lower right rectangle, which is neither a &lt;code&gt;Lens&lt;/code&gt;, nor a &lt;code&gt;Prism&lt;/code&gt;, with respect to the main rectangle. It‚Äôs just a single inner rectangle.&lt;/p&gt;
    &lt;p&gt;On the other hand, if you think about it, every inner rectangle of the main rectangle could be obtained by composing &lt;code&gt;Lens&lt;/code&gt;es and &lt;code&gt;Prism&lt;/code&gt;s.&lt;/p&gt;
    &lt;p&gt;An &lt;code&gt;Optic&lt;/code&gt; identifying an inner rectangle is called an &lt;code&gt;AffineTraversal&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Combining the intuitions we had for &lt;code&gt;Lens&lt;/code&gt;es and &lt;code&gt;Prism&lt;/code&gt;s, it‚Äôs actually possible to define functions &lt;code&gt;set :: B -&amp;gt; A -&amp;gt; A&lt;/code&gt; and &lt;code&gt;over :: (B -&amp;gt; B) -&amp;gt; A -&amp;gt; A&lt;/code&gt; also for &lt;code&gt;AffineTraversal&lt;/code&gt;s.&lt;/p&gt;
    &lt;p&gt;Moreover, the graphical representation suggests us that also &lt;code&gt;AffineTraversal&lt;/code&gt;s for a subcategory of &lt;code&gt;Optic&lt;/code&gt;s, since a sub-rectangle of a sub-rectangle is actually a sub-rectangle of the initial one.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why stop at one?&lt;/head&gt;
    &lt;p&gt;All the &lt;code&gt;Optic&lt;/code&gt;s that we discussed so far focus on a single sub-rectangle. But, if we want, we can consider also &lt;code&gt;Optic&lt;/code&gt;s which focus on multiple sub-rectangles at the same time.&lt;/p&gt;
    &lt;p&gt;We will denote by &lt;code&gt;Traversal A B&lt;/code&gt; the &lt;code&gt;Optic&lt;/code&gt;s which focus on multiple sub-rectangles of type &lt;code&gt;B&lt;/code&gt; inside a main rectangle of type &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;Traversal&lt;/code&gt;s we can still define &lt;code&gt;set :: B -&amp;gt; A -&amp;gt; A&lt;/code&gt; which replaces all the selected sub-rectangles of type &lt;code&gt;B&lt;/code&gt;, inside the main rectangle of type &lt;code&gt;A&lt;/code&gt;, with the same vale &lt;code&gt;b&lt;/code&gt; of type &lt;code&gt;B&lt;/code&gt;, to produce a new &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similarly, we can define &lt;code&gt;over :: (B -&amp;gt; B) -&amp;gt; A -&amp;gt; A&lt;/code&gt; which applies a function to all the selected sub-rectangles of type &lt;code&gt;B&lt;/code&gt;, inside the main rectangle of type &lt;code&gt;A&lt;/code&gt;, to produce a new &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Another relevant function which makes sense to consider for &lt;code&gt;Traversal&lt;/code&gt;s is &lt;code&gt;toListOf :: A -&amp;gt; [B]&lt;/code&gt;, which extracts all the values of the selected sub-rectangles of type &lt;code&gt;B&lt;/code&gt; from the main rectangle of type &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;As usual, we can notice that &lt;code&gt;Traversal A B&lt;/code&gt; form a subcategory of &lt;code&gt;Optic A B&lt;/code&gt;, since a selection of sub-rectangles inside a selection 0f sub-rectangles is still a selection of sub-rectangles of the main rectangle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The graphical representation we just introduced in this post provides us with a tool to navigate various kinds of &lt;code&gt;Optic&lt;/code&gt;s and their operations. I hope it can provide a concrete way to understand the basic ideas behind &lt;code&gt;Lens&lt;/code&gt;es, &lt;code&gt;Prism&lt;/code&gt;s and other &lt;code&gt;Optic&lt;/code&gt;s and make it easier to use them.&lt;/p&gt;
    &lt;p&gt;Such a representation could also help to explore and shed some light on the mysterious world of &lt;code&gt;Optic&lt;/code&gt;s. One could try to search for other sub-categories in a graphical fashion and then ask what do they correspond to in other &lt;code&gt;Optic&lt;/code&gt; representation. For example, what is the sub-category of &lt;code&gt;Optics&lt;/code&gt; made by multiple horizontal slices? Or the one made by multiple vertical slices?&lt;/p&gt;
    &lt;p&gt;I need also to mention that such a representation is not able, as far as I can see, to fully represent the whole universe of &lt;code&gt;Optic&lt;/code&gt;s. For example, it‚Äôs hard to distinguish a &lt;code&gt;Traversal&lt;/code&gt; from a &lt;code&gt;Fold&lt;/code&gt;, or describe what &lt;code&gt;Grate&lt;/code&gt;s are.&lt;/p&gt;
    &lt;p&gt;All in all, I‚Äôm confident that describing and explaining optics in this graphical fashion could help people understand their beauty and usefulness! Thanks for reading up to here!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://marcosh.github.io/post/2025/10/07/the-mondrian-introduction-to-functional-optics.html"/><published>2025-10-07T09:35:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45501189</id><title>Nobel Prize in Physics 2025</title><updated>2025-10-07T13:43:33.031101+00:00</updated><content>&lt;doc fingerprint="715db15f2195d5ea"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Popular information&lt;/head&gt;
    &lt;p&gt;Popular science background: Quantum properties on a human scale (pdf)&lt;lb/&gt;Popul√§rvetenskaplig information: Kvantegenskaper p√• m√§nsklig skala (pdf)&lt;/p&gt;
    &lt;head rend="h2"&gt;Quantum properties on a human scale&lt;/head&gt;
    &lt;p&gt;The Nobel Prize laureates in physics for 2025, John Clarke, Michel H. Devoret and John M. Martinis, used a series of experiments to demonstrate that the bizarre properties of the quantum world can be made concrete in a system big enough to be held in the hand. Their superconducting electrical system could tunnel from one state to another, as if it were passing straight through a wall. They also showed that the system absorbed and emitted energy in doses of specific sizes, just as predicted by quantum mechanics.&lt;/p&gt;
    &lt;head rend="h3"&gt;A series of groundbreaking experiments&lt;/head&gt;
    &lt;p&gt;Quantum mechanics describes properties that are significant on a scale that involves single particles. In quantum physics, these phenomena are called microscopic, even when they are much smaller than can be seen using an optical microscope. This contrasts with macroscopic phenomena, which consist of a large number of particles. For example, an everyday ball is built up of an astronomical amount of molecules and displays no quantum mechanical effects. We know that the ball will bounce back every time it is thrown at a wall. A single particle, however, will sometimes pass straight through an equivalent barrier in its microscopic world and appear on the other side. This quantum mechanical phenomenon is called tunnelling.&lt;/p&gt;
    &lt;p&gt;This year‚Äôs Nobel Prize in Physics recognises experiments that demonstrated how quantum tunnelling can be observed on a macroscopic scale, involving many particles. In 1984 and 1985, John Clarke, Michel Devoret and John Martinis conducted a series of experiments at the University of California, Berkeley. They built an electrical circuit with two superconductors, components that can conduct a current without any electrical resistance. They separated these with a thin layer of material that did not conduct any current at all. In this experiment, they showed that they could control and investigate a phenomenon in which all the charged particles in the superconductor behave in unison, as if they are a single particle that fills the entire circuit.&lt;/p&gt;
    &lt;p&gt;This particle-like system is trapped in a state in which current flows without any voltage ‚Äì a state from which it does not have enough energy to escape. In the experiment, the system shows its quantum character by using tunnelling to escape the zero-voltage state, generating an electrical voltage. The laureates were also able to show that the system is quantised, which means it only absorbs or emits energy in specific amounts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tunnels and crossings&lt;/head&gt;
    &lt;p&gt;To help them, the laureates had concepts and experimental tools that had been developed over decades. Together with the theory of relativity, quantum physics is the foundation of what has come to be called modern physics, and researchers have spent the last century exploring what it entails.&lt;/p&gt;
    &lt;p&gt;Individual particles‚Äô ability to tunnel is well known. In 1928, the physicist George Gamow realised that tunnelling is the reason why some heavy atomic nuclei tend to decay in a particular manner. The interaction between the forces in the nucleus creates a barrier around it, holding in the particles it contains. However, despite this, a small piece of the atomic nucleus can sometimes split off, move outside the barrier and escape ‚Äì leaving behind a nucleus that has been transformed into another element. Without tunnelling, this type of nuclear decay could not occur.&lt;/p&gt;
    &lt;p&gt;Tunnelling is a quantum mechanical process, which entails that chance plays a role. Some types of atomic nuclei have a tall, wide barrier, so it can take a long while for a piece of the nucleus to appear outside it, while other types decay more easily. If we only look at a single atom, we cannot predict when this will happen, but by watching the decay of a large number of nuclei of the same type, we can measure an expected time before tunnelling occurs. The most common way of describing this is through the concept of half-life, which is how long it takes for half the nuclei in a sample to decay.&lt;/p&gt;
    &lt;p&gt;Physicists were quick to wonder whether it would be possible to investigate a type of tunnelling that involves more than one particle at a time. One approach to new types of experiments originated in a phenomenon that arises when some materials get extremely cold.&lt;/p&gt;
    &lt;p&gt;In an ordinary conductive material, current flows because there are electrons that are free to move through the entire material. In some materials, the individual electrons that push their way through the conductor may become organised, forming a synchronised dance that flows without any resistance. The material has become a superconductor and the electrons are joined together as pairs. These are called Cooper pairs, after Leon Cooper who, along with John Bardeen and Robert Schrieffer, provided a detailed description of how superconductors work (Nobel Prize in Physics 1972).&lt;/p&gt;
    &lt;p&gt;Cooper pairs behave completely differently to ordinary electrons. Electrons have a great deal of integrity and like to stay at a distance from each other ‚Äì two electrons cannot be in the same place if they have the same properties. We can see this in an atom, for example, where the electrons divide themselves into different energy levels, called shells. However, when the electrons in a superconductor join up as pairs, they lose a bit of their individuality; while two separate electrons are always distinct, two Cooper pairs can be exactly the same. This means the Cooper pairs in a superconductor can be described as a single unit, one quantum mechanical system. In the language of quantum mechanics, they are then described as a single wave function. This wave function describes the probability of observing the system in a given state and with given properties.&lt;/p&gt;
    &lt;p&gt;If two superconductors are joined together with a thin insulating barrier between them, it creates a Josephson junction. This component is named after Brian Josephson, who performed quantum mechanical calculations for the junction. He discovered that interesting phenomena arise when the wave functions on each side of the junction are considered (Nobel Prize in Physics 1973). The Josephson junction rapidly found areas of application, including in precise measurements of fundamental physical constants and magnetic fields.&lt;/p&gt;
    &lt;p&gt;The construction also provided tools for exploring the fundamentals of quantum physics in a new way. One person who did so was Anthony Leggett (Nobel Prize in Physics 2003), whose theoretical work on macroscopic quantum tunnelling at a Josephson junction inspired new types of experiments.&lt;/p&gt;
    &lt;head rend="h3"&gt;The research group starts its work&lt;/head&gt;
    &lt;p&gt;These subjects were a perfect match for John Clarke‚Äôs research interests. He was a professor at the University of California, Berkeley, in the US, where he had moved after completing his doctoral degree at the University of Cambridge, UK, in 1968. At UC Berkeley he built up his research group and specialised in exploring a range of phenomena using superconductors and the Josephson junction.&lt;/p&gt;
    &lt;p&gt;By the mid-1980s, Michel Devoret had joined John Clarke‚Äôs research group as a postdoc, after receiving his doctorate in Paris. This group also included the doctoral student John Martinis. Together, they took on the challenge of demonstrating macroscopic quantum tunnelling. Vast amounts of care and precision were necessary to screen the experimental setup from all the interference that could affect it. They succeeded in refining and measuring all the properties of their electrical circuit, allowing them to understand it in detail.&lt;/p&gt;
    &lt;p&gt;To measure the quantum phenomena, they fed a weak current into the Josephson junction and measured the voltage, which is related to the electrical resistance in the circuit. The voltage over the Josephson junction was initially zero, as expected. This is because the wave function for the system is enclosed in a state that does not allow a voltage to arise. Then they studied how long it took for the system to tunnel out of this state, causing a voltage. Because quantum mechanics entails an element of chance, they took numerous measurements and plotted their results as graphs, from which they could read the duration of the zero-voltage state. This is similar to how measurements of the half-lives of atomic nuclei are based on statistics of numerous instances of decay.&lt;/p&gt;
    &lt;p&gt;The tunnelling demonstrates how the experimental setup‚Äôs Cooper pairs, in their synchronised dance, behave like a single giant particle. The researchers obtained further confirmation of this when they saw that the system had quantised energy levels. Quantum mechanics was named after the observation that the energy in microscopic processes is divided into separate packages, quanta. The laureates introduced microwaves of varying wavelengths into the zero-voltage state. Some of these were absorbed, and the system then moved to a higher energy level. This showed that the zero-voltage state had a shorter duration when the system contained more energy ‚Äì which is exactly what quantum mechanics predicts. A microscopic particle shut behind a barrier functions in the same way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Practical and theoretical benefit&lt;/head&gt;
    &lt;p&gt;This experiment has consequences for the understanding of quantum mechanics. Other types of quantum mechanical effects that are demonstrated on the macroscopic scale are composed of many tiny individual pieces and their separate quantum properties. The microscopic components are then combined to cause macroscopic phenomena such as lasers, superconductors and superfluid liquids. However, this experiment instead created a macroscopic effect ‚Äì a measurable voltage ‚Äì from a state that is in itself macroscopic, in the form of a common wave function for vast numbers of particles.&lt;/p&gt;
    &lt;p&gt;Theorists like Anthony Leggett have compared the laureates‚Äô macroscopic quantum system with Erwin Schr√∂dinger‚Äôs famous thought experiment featuring a cat in a box, where the cat would be both alive and dead if we did not look inside. (Erwin Schr√∂dinger received the Nobel Prize in Physics 1933.) The intention of his thought experiment was to show the absurdity of this situation, because the special properties of quantum mechanics are often erased at a macroscopic scale. The quantum properties of an entire cat cannot be demonstrated in a laboratory experiment.&lt;/p&gt;
    &lt;p&gt;However, Legget has argued that the series of experiments conducted by John Clarke, Michel Devoret and John Martinis showed that there are phenomena that involve vast numbers of particles which together behave just as quantum mechanics predicts. The macroscopic system that consists of many Cooper pairs is still many orders of magnitude smaller than a kitten ‚Äì but because the experiment measures the quantum mechanical properties that apply to the system as a whole, for a quantum physicist it is fairly similar to Schr√∂dinger‚Äôs imaginary cat.&lt;/p&gt;
    &lt;p&gt;This type of macroscopic quantum state offers new potential for experiments using the phenomena that govern the microscopic world of particles. It can be regarded as a form of artificial atom on a large scale ‚Äì an atom with cables and sockets that can be connected into new test set-ups or utilised in new quantum technology. For example, artificial atoms are used to simulate other quantum systems and aid in understanding them.&lt;/p&gt;
    &lt;p&gt;Another example is the quantum computer experiment subsequently performed by Martinis, in which he utilised exactly the energy quantisation that he and the other two laureates had demonstrated. He used a circuit with quantised states as information-bearing units ‚Äì a quantum bit. The lowest energy state and the first step upward functioned as zero and one, respectively. Superconducting circuits are one of the techniques being explored in attempts to construct a future quantum computer.&lt;/p&gt;
    &lt;p&gt;This year‚Äôs laureates have thus contributed to both practical benefit in physics laboratories and to providing new information for the theoretical understanding of our physical world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Further reading&lt;/head&gt;
    &lt;p&gt;Additional information on this year‚Äôs prizes, including a scientific background in English, is available on the website of the Royal Swedish Academy of Sciences, www.kva.se, and at www.nobelprize.org, where you can watch video from the press conferences, the Nobel Prize lectures and more. Information on exhibitions and activities related to the Nobel Prizes and the prize in economic sciences is available at www.nobelprizemuseum.se.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2025 to&lt;/head&gt;
    &lt;p&gt;JOHN CLARKE&lt;lb/&gt;Born 1942 in Cambridge, UK. PhD 1968 from University of Cambridge, UK. Professor at University of California, Berkeley, USA.&lt;/p&gt;
    &lt;p&gt;MICHEL H. DEVORET&lt;lb/&gt;Born 1953 in Paris, France. PhD 1982 from Paris-Sud University, France. Professor at Yale University, New Haven, CT and University of California, Santa Barbara, USA.&lt;/p&gt;
    &lt;p&gt;JOHN M. MARTINIS&lt;lb/&gt;Born 1958. PhD 1987 from University of Californa, Berkeley, USA. Professor at University of California, Santa Barbara, USA.&lt;/p&gt;
    &lt;p&gt;‚Äúfor the discovery of macroscopic quantum mechanical tunnelling and energy quantisation in an electric circuit‚Äù&lt;/p&gt;
    &lt;p&gt;Science Editors: Ulf Danielsson, G√∂ran Johansson and Eva Lindroth, the Nobel Committee for Physics&lt;lb/&gt;Text: Anna Davour&lt;lb/&gt;Translation: Clare Barnes&lt;lb/&gt;Illustrations: Johan Jarnestad&lt;lb/&gt;Editor: Sara Gustavsson&lt;lb/&gt;¬© The Royal Swedish Academy of Sciences&lt;/p&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6‚Äì13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nobelprize.org/prizes/physics/2025/popular-information/"/><published>2025-10-07T09:50:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45501279</id><title>Like Vercel, but open source and for all language</title><updated>2025-10-07T13:43:32.410797+00:00</updated><content>&lt;doc fingerprint="c1875194e4671d1"&gt;
  &lt;main&gt;
    &lt;p&gt;An open-source and self-hostable alternative to Vercel, Render, Netlify and the likes. It allows you to build and deploy any app (Python, Node.js, PHP, ...) with zero-downtime updates, real-time logs, team management, customizable environments and domains, etc.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Git-based deployments: Push to deploy from GitHub with zero-downtime rollouts and instant rollback.&lt;/item&gt;
      &lt;item&gt;Multi-language support: Python, Node.js, PHP... basically anything that can run on Docker.&lt;/item&gt;
      &lt;item&gt;Environment management: Multiple environments with branch mapping and encrypted environment variables.&lt;/item&gt;
      &lt;item&gt;Real-time monitoring: Live and searchable build and runtime logs.&lt;/item&gt;
      &lt;item&gt;Team collaboration: Role-based access control with team invitations and permissions.&lt;/item&gt;
      &lt;item&gt;Custom domains: Support for custom domain and automatic Let's Encrypt SSL certificates.&lt;/item&gt;
      &lt;item&gt;Self-hosted and open source: Run on your own servers, MIT licensed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;User documentation: devpu.sh/docs&lt;/item&gt;
      &lt;item&gt;Technical documentation: ARCHITECTURE&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;Supported on Ubuntu/Debian. Other distros may work but aren't officially supported (yet).&lt;/quote&gt;
    &lt;p&gt;Log in your server, run the following command and follow instructions:&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/hunvreus/devpush/main/scripts/prod/install.sh | sudo bash&lt;/code&gt;
    &lt;p&gt;You user must have sudo privileges.&lt;/p&gt;
    &lt;p&gt;You will need a fresh Ubuntu/Debian server you can SSH into with sudo privileges. We recommend a CPX31 from Hetzner.&lt;/p&gt;
    &lt;p&gt;You can use the provisioning script to get a server up and running:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sign in or sign up for a Hetzner account: Hetzner Cloud Console&lt;/item&gt;
      &lt;item&gt;Generate an API token: Creating an API token&lt;/item&gt;
      &lt;item&gt;Provision a server (requires &lt;code&gt;--token&lt;/code&gt;; optional:&lt;code&gt;--user&lt;/code&gt;,&lt;code&gt;--name&lt;/code&gt;,&lt;code&gt;--region&lt;/code&gt;,&lt;code&gt;--type&lt;/code&gt;):Tip: run&lt;quote&gt;curl -fsSL https://raw.githubusercontent.com/hunvreus/devpush/main/scripts/prod/provision-hetzner.sh | bash -s -- --token &amp;lt;hetzner_api_key&amp;gt; [--user &amp;lt;login_user&amp;gt;] [--name &amp;lt;hostname&amp;gt;] [--region &amp;lt;fsn1|nbg1|hel1|ash|hil|sin&amp;gt;] [--type &amp;lt;cpx11|cpx21|cpx31|cpx41|cpx51&amp;gt;]&lt;/quote&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/hunvreus/devpush/main/scripts/prod/provision-hetzner.sh | bash -s -- --help&lt;/code&gt;to list regions and types (with specs). Defaults: region&lt;code&gt;hil&lt;/code&gt;, type&lt;code&gt;cpx31&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Configure DNS Records: Go to your DNS provider and create two A records pointing at the server IP for &lt;code&gt;APP_HOSTNAME&lt;/code&gt;(e.g.&lt;code&gt;app.devpu.sh&lt;/code&gt;) and a wildcard on subdomains of&lt;code&gt;DEPLOY_DOMAIN&lt;/code&gt;(e.g.&lt;code&gt;*.devpush.app&lt;/code&gt;). If you're using Cloudflare, set SSL/TLS to "Full (strict)" and keep the records proxied.&lt;/item&gt;
      &lt;item&gt;SSH into your new server: The provision script will have created a user for you. &lt;quote&gt;ssh &amp;lt;login_user&amp;gt;@&amp;lt;server_ip&amp;gt;&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Run hardening for system and SSH:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/hunvreus/devpush/main/scripts/prod/harden.sh | sudo bash -s -- --ssh&lt;/code&gt;
    &lt;p&gt;Even if you already have a server, we recommend you harden security (ufw, fail2ban, disabled root SSH, etc). You can do that using &lt;code&gt;scripts/prod/harden.sh&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;SSH into the server: &lt;quote&gt;ssh &amp;lt;login_user&amp;gt;@&amp;lt;server_ip&amp;gt;&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Install /dev/push: &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/hunvreus/devpush/main/scripts/prod/install.sh | sudo bash&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Switch to &lt;code&gt;devpush&lt;/code&gt;user:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo -iu devpush&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Edit &lt;code&gt;.env&lt;/code&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd devpush &amp;amp;&amp;amp; vi .env&lt;/code&gt;
    &lt;p&gt;Tip: you will need to fill in at least the following: &lt;code&gt;LE_EMAIL&lt;/code&gt;, &lt;code&gt;APP_HOSTNAME&lt;/code&gt;, &lt;code&gt;DEPLOY_DOMAIN&lt;/code&gt;, &lt;code&gt;EMAIL_SENDER_ADDRESS&lt;/code&gt;, &lt;code&gt;RESEND_API_KEY&lt;/code&gt; and your GitHub app settings (see [environment-variables] for details). &lt;code&gt;SERVER_IP&lt;/code&gt;, &lt;code&gt;SECRET_KEY&lt;/code&gt;, &lt;code&gt;ENCRYPTION_KEY&lt;/code&gt;, &lt;code&gt;POSTGRES_PASSWORD&lt;/code&gt; should be pre-filled. You can ignore all commented out environment variables.
5. Start services:&lt;/p&gt;
    &lt;code&gt;scripts/prod/start.sh --migrate&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visit your URL: &lt;code&gt;https://&amp;lt;APP_HOSTNAME&amp;gt;&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The follwing commands must be run as &lt;code&gt;devpush&lt;/code&gt; user (&lt;code&gt;su - devpush&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;In most cases, you can run an update with:&lt;/p&gt;
    &lt;code&gt;scripts/prod/update.sh --all&lt;/code&gt;
    &lt;p&gt;Alternatively, you can force a full upgrade (with downtime) using:&lt;/p&gt;
    &lt;code&gt;scripts/prod/update.sh --full -y&lt;/code&gt;
    &lt;p&gt;You can update specific components:&lt;/p&gt;
    &lt;code&gt;scripts/prod/update.sh --components &amp;lt;component_name&amp;gt;&lt;/code&gt;
    &lt;quote&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;Development scripts target macOS for now.&lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install Colima and the Loki Docker plugin: &lt;quote&gt;scripts/dev/install.sh&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Set up environment variables: &lt;quote&gt;cp .env.dev.example .env&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Start the stack (streams logs): &lt;quote&gt;scripts/dev/start.sh&lt;/quote&gt;&lt;list rend="ul"&gt;&lt;item&gt;Add &lt;code&gt;--prune&lt;/code&gt;to prune dangling images before build&lt;/item&gt;&lt;item&gt;Add &lt;code&gt;--cache&lt;/code&gt;to use the build cache (default is no cache)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Add &lt;/item&gt;
      &lt;item&gt;Initialize your database once containers are up: &lt;quote&gt;scripts/dev/db-migrate.sh&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the scripts section for more dev utilities.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The app is mounted inside containers, so code changes reflect immediately. Some SSE endpoints may require closing browser tabs to trigger a reload.&lt;/item&gt;
      &lt;item&gt;The workers require a restart: &lt;quote&gt;docker-compose restart worker-arq&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;To apply migrations: &lt;quote&gt;scripts/dev/db-migrate.sh&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Area&lt;/cell&gt;
        &lt;cell role="head"&gt;Script&lt;/cell&gt;
        &lt;cell role="head"&gt;What it does&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dev&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/dev/install.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Setup Colima and install Loki Docker plugin&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dev&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/dev/start.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start stack with logs (foreground); supports &lt;code&gt;--prune&lt;/code&gt;, &lt;code&gt;--cache&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dev&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/dev/build-runners.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Build runner images (default no cache; &lt;code&gt;--cache&lt;/code&gt; to enable)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dev&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/dev/db-generate.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Generate Alembic migration (prompts for message)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dev&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/dev/db-migrate.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Apply Alembic migrations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dev&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/dev/db-reset.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Drop and recreate &lt;code&gt;public&lt;/code&gt; schema in DB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dev&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/dev/clean.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Stop stack and clean dev data (&lt;code&gt;--hard&lt;/code&gt; for global)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/provision-hetzner.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Provision a Hetzner server (API token, regions from API, fixed sizes)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/install.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Server setup: Docker, Loki plugin, user, clone repo, create &lt;code&gt;.env&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/harden.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;System hardening (UFW, fail2ban, unattended-upgrades); add &lt;code&gt;--ssh&lt;/code&gt; to harden SSH&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/start.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start services; optional &lt;code&gt;--migrate&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/stop.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Stop services (&lt;code&gt;--down&lt;/code&gt; for hard stop)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/restart.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Restart services; optional &lt;code&gt;--migrate&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/update.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Update by tag; &lt;code&gt;--all&lt;/code&gt; (app+workers), &lt;code&gt;--full&lt;/code&gt; (downtime), or &lt;code&gt;--components&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/db-migrate.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Apply DB migrations in production&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/check-env.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Validate required keys exist in &lt;code&gt;.env&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/update/app.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Blue‚Äëgreen update for app&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/update/worker-arq.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Drain‚Äëaware blue‚Äëgreen update for &lt;code&gt;worker-arq&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Prod&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;scripts/prod/update/worker-monitor.sh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Blue‚Äëgreen update for &lt;code&gt;worker-monitor&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Comments&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;APP_NAME&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;App name.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/dev/push&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;APP_DESCRIPTION&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;App description.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Deploy your Python app without touching a server.&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;URL_SCHEME&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;http&lt;/code&gt; (development) or &lt;code&gt;https&lt;/code&gt; (production).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;https&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;LE_EMAIL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Email used to register the Let's Encrypt (ACME) account in Traefik; receives certificate issuance/renewal/expiry notifications.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;APP_HOSTNAME&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Domain for the app (e.g. &lt;code&gt;app.devpu.sh&lt;/code&gt;).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DEPLOY_DOMAIN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Domain used for deployments (e.g. &lt;code&gt;devpush.app&lt;/code&gt; if you want your deployments available at &lt;code&gt;*.devpush.app&lt;/code&gt;).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;APP_HOSTNAME&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;SERVER_IP&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Public IP of the server&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;SECRET_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;App secret for sessions/CSRF. Generate: &lt;code&gt;openssl rand -hex 32&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENCRYPTION_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fernet key (urlsafe base64, 32 bytes). Generate: `openssl rand -base64 32&lt;/cell&gt;
        &lt;cell&gt;tr '+/' '-_'&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;EMAIL_LOGO&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;URL for email logo image. Only helpful for testing, as the app will use &lt;code&gt;app/logo-email.png&lt;/code&gt; if left empty.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;EMAIL_SENDER_NAME&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Name displayed as email sender for invites/login.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;EMAIL_SENDER_ADDRESS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Email sender used for invites/login.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;RESEND_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;API key for Resend.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;GITHUB_APP_ID&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GitHub App ID.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;GITHUB_APP_NAME&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GitHub App name.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;GITHUB_APP_PRIVATE_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GitHub App private key (PEM format).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;GITHUB_APP_WEBHOOK_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GitHub webhook secret for verifying webhook payloads.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;GITHUB_APP_CLIENT_ID&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GitHub OAuth app client ID.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;GITHUB_APP_CLIENT_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GitHub OAuth app client secret.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_CLIENT_ID&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google OAuth client ID.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_CLIENT_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google OAuth client secret.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;POSTGRES_DB&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;PostgreSQL database name.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;devpush&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;POSTGRES_USER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;PostgreSQL username.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;devpush-app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;POSTGRES_PASSWORD&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;PostgreSQL password. Generate: `openssl rand -base64 24&lt;/cell&gt;
        &lt;cell&gt;tr -d '\n'`&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;REDIS_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Redis connection URL.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;redis://redis:6379&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DOCKER_HOST&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Docker daemon host address.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;tcp://docker-proxy:2375&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;UPLOAD_DIR&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Directory for file uploads.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app/upload&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TRAEFIK_CONFIG_DIR&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Traefik configuration directory.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/data/traefik&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DEFAULT_CPU_QUOTA&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Default CPU quota for containers (microseconds).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100000&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DEFAULT_MEMORY_MB&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Default memory limit for containers (MB).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;4096&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;JOB_TIMEOUT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Job timeout in seconds.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;320&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;JOB_COMPLETION_WAIT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Job completion wait time in seconds.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;300&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DEPLOYMENT_TIMEOUT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deployment timeout in seconds.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;300&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;LOG_LEVEL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Logging level.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WARNING&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DB_ECHO&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable SQL query logging.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENV&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Environment (development/production).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;development&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ACCESS_DENIED_MESSAGE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Message shown to users who are denied access based on sign-in access control.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Sign-in not allowed for this email.&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ACCESS_DENIED_WEBHOOK&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Optional webhook to receive denied events (read more about Sign-in access control).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;LOGIN_HEADER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTML snippet displayed above the login form.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;TOASTER_HEADER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTML snippet displayed at the top of the toaster (useful to display a permanent toast on all pages).&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;""&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You will need to configure a GitHub App with the following settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identifying and authorizing users: &lt;list rend="ul"&gt;&lt;item&gt;Callback URL: add two callback URLs with your domain:&lt;/item&gt;&lt;item&gt;Expire user authorization tokens: No&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Post installation: &lt;list rend="ul"&gt;&lt;item&gt;Setup URL: https://example.com/api/github/install/callback&lt;/item&gt;&lt;item&gt;Redirect on update: Yes&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Webhook: &lt;list rend="ul"&gt;&lt;item&gt;Active: Yes&lt;/item&gt;&lt;item&gt;Webhook URL: https://example.com/api/github/webhook&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Permissions: &lt;list rend="ul"&gt;&lt;item&gt;Repository permissions &lt;list rend="ul"&gt;&lt;item&gt;Administration: Read and write&lt;/item&gt;&lt;item&gt;Checks: Read and write&lt;/item&gt;&lt;item&gt;Commit statuses: Read and write&lt;/item&gt;&lt;item&gt;Contents: Read and write&lt;/item&gt;&lt;item&gt;Deployments: Read and write&lt;/item&gt;&lt;item&gt;Issues: Read and write&lt;/item&gt;&lt;item&gt;Metadata: Read-only&lt;/item&gt;&lt;item&gt;Pull requests: Read and write&lt;/item&gt;&lt;item&gt;Webhook: Read and write&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Account permissions: &lt;list rend="ul"&gt;&lt;item&gt;Email addresses: Read-only&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Repository permissions &lt;/item&gt;
      &lt;item&gt;Subscribe to events: &lt;list rend="ul"&gt;&lt;item&gt;Installation target&lt;/item&gt;&lt;item&gt;Push&lt;/item&gt;&lt;item&gt;Repository&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Provide an access rules file to restrict who can sign up/sign in.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Development: edit &lt;code&gt;./access.json&lt;/code&gt;. If missing, running&lt;code&gt;scripts/dev/start.sh&lt;/code&gt;will sed an allow‚Äëall file.&lt;/item&gt;
      &lt;item&gt;Production: edit &lt;code&gt;/srv/devpush/access.json&lt;/code&gt;on the server.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rules format (any/all may be used):&lt;/p&gt;
    &lt;code&gt;{
  "emails": ["alice@example.com"],
  "domains": ["example.com"],
  "globs": ["*@corp.local", "*.dept.example.com"],
  "regex": ["^[^@]+@(eng|research)\\.example\\.com$"]
}&lt;/code&gt;
    &lt;p&gt;Globs use shell-style wildcards; regex are Python patterns. If the file is missing or empty, all valid emails are allowed.&lt;/p&gt;
    &lt;p&gt;Additionally, if you set the &lt;code&gt;ACCESS_DENIED_WEBHOOK&lt;/code&gt; environment variable, denied sign-in attempts will be posted to the provided URL with the following payload:&lt;/p&gt;
    &lt;code&gt;{
  "email": "user@example.com",
  "provider": "google",
  "ip": "203.0.113.10",
  "user_agent": "Mozilla/5.0"
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/hunvreus/devpush"/><published>2025-10-07T10:07:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45501326</id><title>GPT-5-Codex is a better AI researcher than me</title><updated>2025-10-07T13:43:32.193779+00:00</updated><content>&lt;doc fingerprint="bf10111e239acc98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GPT-5-Codex is a better AI researcher than me&lt;/head&gt;
    &lt;p&gt;In What‚Äôs the strongest AI model you can train on a laptop in five minutes? I tried my hand at answering a silly AI-research question. You can probably guess what it was.&lt;/p&gt;
    &lt;p&gt;I chatted with GPT-5 to help me get started with the Python scripts and to bounce ideas off, but it was still me doing the research. I was coming up with the ideas, running the experiments, and deciding what to do next based on the data. The best model I could train was a 1.8M param transformer which produced output like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Once upon a time, there was a little boy named Tim. Tim had a small box that he liked to play with. He would push the box to open. One day, he found a big red ball in his yard. Tim was so happy. He picked it up and showed it to his friend, Jane. ‚ÄúLook at my bag! I need it!‚Äù she said. They played with the ball all day and had a great time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Since then, OpenAI has released GPT-5-codex, and supposedly uses it (plus Codex, their CLI coding tool) to automate a lot of their product development and AI research. I wanted to try the same thing. Codex-plus-me did a much better job than me alone1. Here‚Äôs an example of the best output I got from the model I trained with Codex:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Once upon a time, in a big forest, there lived a little bunny named Ben. Ben loved to play with his friends in the forest. One day, Ben‚Äôs mom saw him and was sad because he couldn‚Äôt find his friend. She asked, ‚ÄúWhy are you sad, Ben?‚Äù Ben said, ‚ÄúI lost my toy. I can‚Äôt find it.‚Äù Ben wanted to help Ben find his toy. He knew they could fix the toy. He went to Sam‚Äôs house and found the toy under a tree. Sam was so happy and said, ‚ÄúThank you, Ben! You are a very pretty toy!‚Äù Ben smiled and said, ‚ÄúYes, I would love to help you.‚Äù They played together all day long. The moral of the story is to help others when they needed it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;What was the process like to get there?&lt;/p&gt;
    &lt;head rend="h3"&gt;What vibe research looks like&lt;/head&gt;
    &lt;p&gt;I want to call it ‚Äúvibe research‚Äù. Like ‚Äúvibe coding‚Äù, it‚Äôs performing a difficult technical task by relying on the model. I have a broad intuitive sense of what approaches are being tried, but I definitely don‚Äôt have a deep enough understanding to do this research unassisted. A real AI researcher would get a lot more out of the tool.&lt;/p&gt;
    &lt;p&gt;Still, it was very easy to get started. I gave Codex the path to my scratch directory, told it ‚Äúcontinue the research‚Äù, and it immediately began coming up with ideas and running experiments on its own. In a way, the ‚Äútrain in five minutes‚Äù challenge is a perfect fit, because the feedback loop is so short.&lt;/p&gt;
    &lt;p&gt;The basic loop of doing AI research with Codex (at least as an enthusiastic amateur) looks something like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Codex makes a change to the training script and does three or four runs (this takes ~20 minutes overall)&lt;/item&gt;
      &lt;item&gt;Based on the results, Codex suggests two or three things that you could try next&lt;/item&gt;
      &lt;item&gt;I pick one of them (or very occasionally suggest my own idea) and return to (1).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After two days I did paste the current research notes into GPT-5-Pro, which helped a bit, but the vast majority of my time was spent in this loop. As we‚Äôll see, the best ideas were ones Codex already came up with.&lt;/p&gt;
    &lt;p&gt;I chewed through a lot of tokens doing this. That‚Äôs OK with me, since I paid for the $200-per-month plan2, but if you don‚Äôt want to do that you‚Äôll have to space out your research a bit more slowly. I restarted my Codex process every million tokens or so. It didn‚Äôt have any issue continuing where it left off from its previous notes, which was nice.&lt;/p&gt;
    &lt;p&gt;I ran Codex with &lt;code&gt;--sandbox danger-full-access&lt;/code&gt;. By default it didn‚Äôt have access to MPS, which meant it could only train models on the CPU. There‚Äôs probably some more principled way of sandboxing it, but I didn‚Äôt bother to figure it out. I didn‚Äôt run into any runaway-agent problems, unless you count crashing my laptop a few times by using up too much memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;How did the research go?&lt;/head&gt;
    &lt;p&gt;Here‚Äôs a brief summary of how the research went over the four or five days I spent poking at it. I stayed with the TinyStories dataset for all of this, partially because I think it‚Äôs the best choice and partially because I wanted a 1:1 comparison between Codex and my own efforts.&lt;/p&gt;
    &lt;head rend="h4"&gt;N-gram models&lt;/head&gt;
    &lt;p&gt;Codex and I started with a series of n-gram models: instead of training a neural network, n-gram models just store the conditional probabilities of a token based on the n tokens that precede it. These models are very quick to produce (seconds, not minutes) but aren‚Äôt very good. The main reason is that even a 5-gram model cannot include context from more than five tokens ago, so they struggle to produce coherent text across an entire sentence. Here‚Äôs an example:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Once upon a time , in a small school . ‚Äù they are friends . they saw a big pond . he pulled and pulled , but the table was still no attention to grow even more . she quickly ran to the house . she says , ‚Äù sara said . ‚Äù you made him ! ‚Äù the smooth more it said , for helping me decorate the cake .&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It‚Äôs not terrible! There are short segments that are entirely coherent. But it‚Äôs kind of like what AI skeptics think LLMs are like: just fragments of the original source, remixed without any unifying through-line. The perplexity is 18.5, worse than basically any of the transformers I trained in my last attempt.&lt;/p&gt;
    &lt;p&gt;Codex trained 19 different n-gram models, of which the above example (a 4-gram model) was the best3. In my view, this is one of the strengths of LLM-based AI research: it is trivial to tell the model ‚Äúgo and sweep a bunch of different values for the hyperparameters‚Äù. Of course, you can do this yourself. But it‚Äôs a lot easier to just tell the model to do it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Back to transformers&lt;/head&gt;
    &lt;p&gt;After this, Codex spent a lot of time working on transformers. It trained ~50 normal transformers with different sizes, number of heads, layers, and so on. Most of this wasn‚Äôt particularly fruitful. I was surprised that my hand-picked hyperparameters from my previous attempt were quite competitive - though maybe it shouldn‚Äôt have been a shock, since they matched the lower end of the Chinchilla scaling laws.&lt;/p&gt;
    &lt;p&gt;Still, eventually Codex hit on a 8.53 perplexity model (3 layers, 4 heads, and a dimension of 144), which was a strict improvement over my last attempt. I‚Äôm not really convinced this was an architectural improvement. One lesson from training fifty different models is that there‚Äôs quite a lot of variance between different seeds. A perplexity improvement of just over 1 is more or less what I was seeing on a ‚Äúlucky seed‚Äù.&lt;/p&gt;
    &lt;p&gt;This was an interesting approach for the challenge: going for pure volume and hoping for a lucky training run. You can‚Äôt do this with a larger model, since it takes so long to train4, but the five-minute limit makes it possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Minimizing perplexity was a mistake&lt;/head&gt;
    &lt;p&gt;The next thing Codex tried - based on some feedback I pasted in from GPT-5-Pro - was ‚Äúshallow fusion‚Äù: instead of training a new model, updating the generation logic to blend the transformer-predicted tokens with a n-gram model, a ‚ÄúkNN head‚Äù (which looks up hidden states that are ‚Äúnearby‚Äù the current hidden state of the transformer and predicts their tokens), and a ‚Äúcache head‚Äù that makes the model more likely to repeat words that are already in the context.&lt;/p&gt;
    &lt;p&gt;This immediately dropped perplexity down to 7.38: a whole other point lower than our best transformer. I was excited about that, but the generated content was really bad:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Once upon a time,, in a small house, there lived a boy named Tim. Tim loved to play outside with his ball. One Mr. Skip had a lot of fun. He ran everywhere every day. One One day, Tim was playing with his ball new ball near his house. Tim was playing with his his ball and had a lot of fun. But then, he saw a big tree and decided to climb it. Tim tried to climb the tree, but he was too big. He was too small to reach the top of the tree. But the tree was too high. The little tree was too high for him. Soon, Tim was near the tree. He was brave and climbed the tree. But when he got got to the top, he was sad. Tim saw a bird on&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;What happened? I over-optimized for perplexity. As it turns out, the pure transformers that were higher-perplexity were better at writing stories. They had more coherence over the entire length of the story, they avoided generating weird repetition artifacts (like ‚Äù,,‚Äù), and they weren‚Äôt as mindlessly repetitive.&lt;/p&gt;
    &lt;p&gt;I went down a bit of a rabbithole trying to think of how to score my models without just relying on perplexity. I came up with some candidate rubrics, like grammatical coherence, patterns of repetition, and so on, before giving up and just using LLM-as-a-judge. To my shame, I even generated a new API key for the LLM before realizing that I was talking to a strong LLM already via Codex, and I could just ask Codex to rate the model outputs directly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Distillation from n-grams&lt;/head&gt;
    &lt;p&gt;The final and most successful idea I tried was distilling a transformer from a n-gram teacher model. First, we train a n-gram model, which only takes ~10 seconds. Then we train a transformer - but for the first 200 training steps, we push the transformer towards predicting the tokens that the n-gram model would predict. After that, the transformer continues to train on the TinyStories data as usual.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs an example of some output:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Once upon a time, in a big forest, there lived a little bunny named Ben. Ben loved to play with his friends in the forest. One day, Ben‚Äôs mom saw him and was sad because he couldn‚Äôt find his friend. She asked, ‚ÄúWhy are you sad, Ben?‚Äù Ben said, ‚ÄúI lost my toy. I can‚Äôt find it.‚Äù Ben wanted to help Ben find his toy. He knew they could fix the toy. He went to Sam‚Äôs house and found the toy under a tree. Sam was so happy and said, ‚ÄúThank you, Ben! You are a very pretty toy!‚Äù Ben smiled and said, ‚ÄúYes, I would love to help you.‚Äù They played together all day long. The moral of the story is to help others when they needed it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think this is pretty good! It has characters that continue throughout the story. It has a throughline - Ben‚Äôs lost toy - though it confuses ‚Äútoy‚Äù and ‚Äúfriend‚Äù a bit. It‚Äôs a coherent story, with a setup, problem, solution and moral. This is much better than anything else I‚Äôve been able to train in five minutes.&lt;/p&gt;
    &lt;p&gt;Why is it better? I think the right intuition here is that transformers need to spend a lot of initial compute (say, two minutes) learning how to construct grammatically-correct English sentences. If you begin the training by spending ten seconds training a n-gram model that can already produce sort-of-correct grammar, you can speedrun your way to learning grammar and spend an extra one minute and fifty seconds learning content.&lt;/p&gt;
    &lt;p&gt;I really like this approach. It‚Äôs exactly what I was looking for from the start: a cool architectural trick that genuinely helps, but only really makes sense for this weird challenge5.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;I don‚Äôt have any illusions about this making me a real AI researcher, any more than a ‚Äúvibe coder‚Äù is a software engineer. Still, I‚Äôm surprised that it actually worked. And it was a lot of fun!&lt;/p&gt;
    &lt;p&gt;I‚Äôve pushed up the code here if you want to pick up from where I left off, but you may be better off just starting from scratch with Codex or your preferred coding agent.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;‚ÄúAlone‚Äù here is relative - I did use ChatGPT and a bit of Copilot to generate some of the training code in my last attempt. I just didn‚Äôt use any agentic tooling.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;My deal with myself was that if I ever have a month where I use fewer than 2M tokens, I‚Äôll cancel the plan.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There are a lot of clever tricks involved here: Kneser-Ney smoothing, interpolating unigram/bigram/trigram probabilities on a specific schedule, deliberately keeping the&lt;/p&gt;‚Ü©&lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt;sentinel token, etc. I didn‚Äôt spend the time understanding all of these things deeply - that‚Äôs vibe research for you - so I won‚Äôt write too much about it.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Unless you‚Äôre a big AI lab. I am 100% convinced that the large labs are spending a lot of compute just re-training on different seeds in the hope of getting a lucky run.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I was suspicious that I just got a lucky seed, but I compared ~40 generations with and without the distillation and the distilled model really was better at producing correct-looking stories.&lt;/p&gt;‚Ü©&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;October 7, 2025 ‚îÇ Tags: ai, projects&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/ai-research-with-codex/"/><published>2025-10-07T10:16:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502173</id><title>disk-perf-git-and-pnpm aims to prove that something is wrong with APFS on macOS</title><updated>2025-10-07T13:43:31.611938+00:00</updated><content>&lt;doc fingerprint="23acf9ad7b2715df"&gt;
  &lt;main&gt;
    &lt;p&gt;This repo aims to prove that something is wrong with APFS on macOS, but is also a good stress test in general when changing machine tooling that wants to oberve fs events (such as security tooling / EDR / virus scanners / etc).&lt;/p&gt;
    &lt;p&gt;Steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Setup&lt;/item&gt;
      &lt;item&gt;Gather Results&lt;/item&gt;
      &lt;item&gt;Report / PR with your Results ‚ù§Ô∏è&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;have &lt;code&gt;node&lt;/code&gt; @ &amp;gt;= 22.11
have &lt;code&gt;pnpm&lt;/code&gt; @ &amp;gt;= 10.2&lt;/p&gt;
    &lt;p&gt;(if you have proto (with auto-install) or volta installed, these versions will be selected for you)&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/NullVoxPopuli/disk-perf-git-and-pnpm.git
cd disk-perf-git-and-pnpm

pnpm install # Fill the cache so we don't hit the network during testing&lt;/code&gt;
    &lt;p&gt;Since you've installed all the dependencies already, we can start with the clean test:&lt;/p&gt;
    &lt;code&gt;time ( git clean -Xfd; git clean -fd )&lt;/code&gt;
    &lt;p&gt;Windows Powershell:&lt;/p&gt;
    &lt;code&gt;(Measure-Command { git clean -Xfd; git clean -fd }).ToString()&lt;/code&gt;
    &lt;p&gt;And then once that finishes, we can run the install test:&lt;/p&gt;
    &lt;code&gt;time ( pnpm install )&lt;/code&gt;
    &lt;p&gt;Windows Powershell:&lt;/p&gt;
    &lt;code&gt;(Measure-Command { pnpm install }).ToString()&lt;/code&gt;
    &lt;head&gt;If using zsh&lt;/head&gt;
    &lt;p&gt;your time will be &lt;code&gt;total&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;0.01s user 0.00s system 94% cpu 0.007 total
#.                              ^ this number&lt;/code&gt;
    &lt;p&gt;and round to the tenths decimal place&lt;/p&gt;
    &lt;head&gt;if using bash&lt;/head&gt;
    &lt;p&gt;your time will be &lt;code&gt;real&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;real    2.02s # this number
user    0.00s
sys     0.01s&lt;/code&gt;
    &lt;p&gt;and round to the tenths decimal place&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apple Menu&lt;/item&gt;
      &lt;item&gt;"About this Mac" (a window appears)&lt;/item&gt;
      &lt;item&gt;"More Info..." (a window appears)&lt;/item&gt;
      &lt;item&gt;scroll down and click "System Report..." (a window appears)&lt;/item&gt;
      &lt;item&gt;in the left nav of this third window, click "NVMExpress"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and interact with the results here&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
        &lt;cell role="head"&gt;CPU&lt;/cell&gt;
        &lt;cell role="head"&gt;RAM (GB)&lt;/cell&gt;
        &lt;cell role="head"&gt;Clean (s)&lt;/cell&gt;
        &lt;cell role="head"&gt;Install (s)&lt;/cell&gt;
        &lt;cell role="head"&gt;OS&lt;/cell&gt;
        &lt;cell role="head"&gt;FileSystem&lt;/cell&gt;
        &lt;cell role="head"&gt;Disk&lt;/cell&gt;
        &lt;cell role="head"&gt;Notable Software Changes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-07&lt;/cell&gt;
        &lt;cell&gt;AMD Ryzen 5 7640U 12 Core&lt;/cell&gt;
        &lt;cell&gt;92&lt;/cell&gt;
        &lt;cell&gt;6.8&lt;/cell&gt;
        &lt;cell&gt;5.9&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04.1&lt;/cell&gt;
        &lt;cell&gt;Ext4&lt;/cell&gt;
        &lt;cell&gt;WD Black SN850 500GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-24&lt;/cell&gt;
        &lt;cell&gt;AMD Ryzen 5 7640U throttle to ~550Mhz&lt;/cell&gt;
        &lt;cell&gt;92&lt;/cell&gt;
        &lt;cell&gt;56&lt;/cell&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.10&lt;/cell&gt;
        &lt;cell&gt;Ext4&lt;/cell&gt;
        &lt;cell&gt;WD Black SN850 500GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-07&lt;/cell&gt;
        &lt;cell&gt;AMD Ryzen 9 7900X 12/24 Core&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;6.0&lt;/cell&gt;
        &lt;cell&gt;4.3&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04.1&lt;/cell&gt;
        &lt;cell&gt;Ext4&lt;/cell&gt;
        &lt;cell&gt;Samsung SSD 980 Pro 2TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-07&lt;/cell&gt;
        &lt;cell&gt;AMD Ryzen 9 7900X 12/24 Core&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;3.3&lt;/cell&gt;
        &lt;cell&gt;4.0&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04.1&lt;/cell&gt;
        &lt;cell&gt;tmpfs (ramdisk)&lt;/cell&gt;
        &lt;cell&gt;G.Skill F5-6000J3040G32G&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-09&lt;/cell&gt;
        &lt;cell&gt;Apple M1 Pro&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;42.2&lt;/cell&gt;
        &lt;cell&gt;44.0&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP0512R 500GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-08&lt;/cell&gt;
        &lt;cell&gt;Apple M1 Max&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;31.5&lt;/cell&gt;
        &lt;cell&gt;44.2&lt;/cell&gt;
        &lt;cell&gt;macOS 14.7.3&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024R 1TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-08&lt;/cell&gt;
        &lt;cell&gt;Apple M4&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;29.6&lt;/cell&gt;
        &lt;cell&gt;31.4&lt;/cell&gt;
        &lt;cell&gt;macOS 15.2&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024Z 1TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-09&lt;/cell&gt;
        &lt;cell&gt;AMD Ryzen 7 7800X3D 8 Core&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;17.1&lt;/cell&gt;
        &lt;cell&gt;16.1&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 22.04.3&lt;/cell&gt;
        &lt;cell&gt;Ext4&lt;/cell&gt;
        &lt;cell&gt;Corsair MP600 PRO LPX&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-09&lt;/cell&gt;
        &lt;cell&gt;AMD Ryzen 7 7800X3D 8 Core&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;65.5&lt;/cell&gt;
        &lt;cell&gt;42.3&lt;/cell&gt;
        &lt;cell&gt;Windows 10 Pro 22H2&lt;/cell&gt;
        &lt;cell&gt;NTFS&lt;/cell&gt;
        &lt;cell&gt;Corsair MP600 PRO LPX&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-09&lt;/cell&gt;
        &lt;cell&gt;AMD Ryzen 5 7800X3D 8 Core&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;69.5&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;Windows 11 Pro 23H2&lt;/cell&gt;
        &lt;cell&gt;NTFS&lt;/cell&gt;
        &lt;cell&gt;WD Black SN850x 2TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-09&lt;/cell&gt;
        &lt;cell&gt;AMD Ryzen 5 7800X3D 8 Core&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;23.7&lt;/cell&gt;
        &lt;cell&gt;19.0&lt;/cell&gt;
        &lt;cell&gt;W11 Pro 23H2 / WSL2 / Ubuntu 24.04&lt;/cell&gt;
        &lt;cell&gt;Ext4&lt;/cell&gt;
        &lt;cell&gt;WD Black SN850x 2TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-10&lt;/cell&gt;
        &lt;cell&gt;Intel i5-1145G7 8 Core&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1.9&lt;/cell&gt;
        &lt;cell&gt;15.3&lt;/cell&gt;
        &lt;cell&gt;Debian Trixie&lt;/cell&gt;
        &lt;cell&gt;Ext4&lt;/cell&gt;
        &lt;cell&gt;BC711 NVMe SK hynix 512GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-12&lt;/cell&gt;
        &lt;cell&gt;Apple M1 Max&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;87.7&lt;/cell&gt;
        &lt;cell&gt;macOS 14.6.1&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP2048R 2TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-12&lt;/cell&gt;
        &lt;cell&gt;Apple M4 Pro (14 Cores)&lt;/cell&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;30.1&lt;/cell&gt;
        &lt;cell&gt;65.1&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP2048Z 2TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-13&lt;/cell&gt;
        &lt;cell&gt;Apple M1 Ultra&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;45.2&lt;/cell&gt;
        &lt;cell&gt;137.5&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024R 1TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-14&lt;/cell&gt;
        &lt;cell&gt;Apple M2 Max (6 vCPU)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;3.2&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04&lt;/cell&gt;
        &lt;cell&gt;Ext4&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024Z&lt;/cell&gt;
        &lt;cell&gt;Parallels VM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-14&lt;/cell&gt;
        &lt;cell&gt;Apple M2 Max (6 vCPU)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;2.8&lt;/cell&gt;
        &lt;cell&gt;11.9&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04&lt;/cell&gt;
        &lt;cell&gt;Ext4 LVM2 Encrypted&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024Z&lt;/cell&gt;
        &lt;cell&gt;Parallels VM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-14&lt;/cell&gt;
        &lt;cell&gt;Apple M2 Max (6 vCPU)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;1.6&lt;/cell&gt;
        &lt;cell&gt;10.7&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04&lt;/cell&gt;
        &lt;cell&gt;tmpfs (ramdisk)&lt;/cell&gt;
        &lt;cell&gt;Hynix LPDDR5 / Virtual RAM&lt;/cell&gt;
        &lt;cell&gt;Parallels VM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-15&lt;/cell&gt;
        &lt;cell&gt;Apple M1 Pro&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;44.5&lt;/cell&gt;
        &lt;cell&gt;50.2&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP0512R 500GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-19&lt;/cell&gt;
        &lt;cell&gt;Apple M1&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;37.8&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3.1&lt;/cell&gt;
        &lt;cell&gt;APFS (Encypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP0512Q 500GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-19&lt;/cell&gt;
        &lt;cell&gt;Apple M1 Pro&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;59.4&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;macOS 14.7.3&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024R 1TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-21&lt;/cell&gt;
        &lt;cell&gt;Apple M3&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;36.23&lt;/cell&gt;
        &lt;cell&gt;30.3&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP0256Z 256GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-20&lt;/cell&gt;
        &lt;cell&gt;Apple M4 Max (16 Cores)&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
        &lt;cell&gt;36.7&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
        &lt;cell&gt;macOS 15.2&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP2048Z 2TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-20&lt;/cell&gt;
        &lt;cell&gt;Apple M3&lt;/cell&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;46.6&lt;/cell&gt;
        &lt;cell&gt;44.6&lt;/cell&gt;
        &lt;cell&gt;macOS ??&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024Z 1TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-21&lt;/cell&gt;
        &lt;cell&gt;Intel Core i7 14700K (20 Cores)&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;3.1&lt;/cell&gt;
        &lt;cell&gt;13.8&lt;/cell&gt;
        &lt;cell&gt;W10 22H2 / WSL2 / Ubuntu 24.04&lt;/cell&gt;
        &lt;cell&gt;Ext4&lt;/cell&gt;
        &lt;cell&gt;WD Black 2TB SN850&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-22&lt;/cell&gt;
        &lt;cell&gt;Apple M3 Pro&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;37.7&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024Z 1TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-24&lt;/cell&gt;
        &lt;cell&gt;Apple M2 Pro&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;34.6&lt;/cell&gt;
        &lt;cell&gt;32.0&lt;/cell&gt;
        &lt;cell&gt;macOS 13.6&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP0512Z&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-25&lt;/cell&gt;
        &lt;cell&gt;Apple M3&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;34.213&lt;/cell&gt;
        &lt;cell&gt;27.851&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3.1&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024Z&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-25&lt;/cell&gt;
        &lt;cell&gt;Apple M3 Pro (12 Core, 6p6e)&lt;/cell&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell&gt;47.8&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;macOS 14.7.4&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP0512Z 500GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-25&lt;/cell&gt;
        &lt;cell&gt;Apple M3 Pro (12 Core, 6p6e)&lt;/cell&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;53.3&lt;/cell&gt;
        &lt;cell&gt;macOS 14.7.4&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP0512Z 500GB&lt;/cell&gt;
        &lt;cell&gt;Spotlight disabled&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-25&lt;/cell&gt;
        &lt;cell&gt;Apple M3 Pro (12 Core, 6p6e)&lt;/cell&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell&gt;26.3&lt;/cell&gt;
        &lt;cell&gt;19.9&lt;/cell&gt;
        &lt;cell&gt;macOS 14.7.4&lt;/cell&gt;
        &lt;cell&gt;APFS&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP0512Z 500GB&lt;/cell&gt;
        &lt;cell&gt;Spotlight disabled, &lt;code&gt;csrutil disable&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-26&lt;/cell&gt;
        &lt;cell&gt;Apple M2 Max (12 Core, 8p4e)&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;41.4&lt;/cell&gt;
        &lt;cell&gt;39.8&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3.1&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024Z 1TB&lt;/cell&gt;
        &lt;cell&gt;Spotlight disabled, Kandji, SentinelOne&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-26&lt;/cell&gt;
        &lt;cell&gt;Apple M4 Pro (14 Cores) (6 core vCPU)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;2.5&lt;/cell&gt;
        &lt;cell&gt;16.9&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.10&lt;/cell&gt;
        &lt;cell&gt;Ext4 Unencrypted&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP2048Z 2TB&lt;/cell&gt;
        &lt;cell&gt;UTM VM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-28&lt;/cell&gt;
        &lt;cell&gt;Apple M2 Max (6 vCPU)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;11.9&lt;/cell&gt;
        &lt;cell&gt;15.7&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04.2&lt;/cell&gt;
        &lt;cell&gt;Ext4 LVM2 Encrypted&lt;/cell&gt;
        &lt;cell&gt;APPLE SSD AP1024Z&lt;/cell&gt;
        &lt;cell&gt;Parallels VM, SentinelOne&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-02-28&lt;/cell&gt;
        &lt;cell&gt;Apple M2 Max (6 vCPU)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;9.1&lt;/cell&gt;
        &lt;cell&gt;13.3&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04.2&lt;/cell&gt;
        &lt;cell&gt;tmpfs (ramdisk)&lt;/cell&gt;
        &lt;cell&gt;Hynix LPDDR5 / Virtual RAM&lt;/cell&gt;
        &lt;cell&gt;Parallels VM, SentinelOne&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-04-26&lt;/cell&gt;
        &lt;cell&gt;Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;103.98&lt;/cell&gt;
        &lt;cell&gt;116.62&lt;/cell&gt;
        &lt;cell&gt;macOS 15.4.1&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;Apple SSD AP1024N&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-04-27&lt;/cell&gt;
        &lt;cell&gt;Apple M4 Pro (14 Core, 10p4e)&lt;/cell&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;64.48&lt;/cell&gt;
        &lt;cell&gt;145.40&lt;/cell&gt;
        &lt;cell&gt;macOS 15.3.2&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;Apple SSD AP1024Z&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2025-04-27&lt;/cell&gt;
        &lt;cell&gt;Apple M4 Pro (14 Core, 10p4e)&lt;/cell&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;3.209&lt;/cell&gt;
        &lt;cell&gt;17.302&lt;/cell&gt;
        &lt;cell&gt;Ubuntu 24.04.2&lt;/cell&gt;
        &lt;cell&gt;btrfs&lt;/cell&gt;
        &lt;cell&gt;Apple SSD AP1024Z&lt;/cell&gt;
        &lt;cell&gt;Ubuntu machine running in OrbStack&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-10-06&lt;/cell&gt;
        &lt;cell&gt;Apple M2 Max (12 Core, 8p4e)&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;46.730&lt;/cell&gt;
        &lt;cell&gt;54.603&lt;/cell&gt;
        &lt;cell&gt;macOS 15.5&lt;/cell&gt;
        &lt;cell&gt;APFS (Encrypted)&lt;/cell&gt;
        &lt;cell&gt;Apple SSD AP1024Z 1TB&lt;/cell&gt;
        &lt;cell&gt;Kandji, Code42, SentinelOne, tested in excluded directory&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you're using macOS, and your file system performance is unbearable, there are some options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://gist.github.com/boxabirds/b92fec28c58e6c2cc9513f16c2bbeb91 &lt;list rend="ul"&gt;&lt;item&gt;Put everything in a RAM disk:&lt;/item&gt;&lt;item&gt;or OverlayFS via Docker&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;use a Linux VM to get ext4 speeds&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/NullVoxPopuli/disk-perf-git-and-pnpm"/><published>2025-10-07T12:14:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502377</id><title>When money is abundant, knowledge is the real wealth (2020)</title><updated>2025-10-07T13:43:31.057822+00:00</updated><content/><link href="https://www.lesswrong.com/posts/wEebEiPpEwjYvnyqq/when-money-is-abundant-knowledge-is-the-real-wealth"/><published>2025-10-07T12:40:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502387</id><title>Provision (YC S22) Is Hiring</title><updated>2025-10-07T13:43:30.554658+00:00</updated><content>&lt;doc fingerprint="62df618ee7cc4f56"&gt;
  &lt;main&gt;
    &lt;p&gt;Ironclad for construction&lt;/p&gt;
    &lt;p&gt;Refer to our hiring document, here&lt;/p&gt;
    &lt;p&gt;We typically schedule three interviews that can be completed in the span of a week:&lt;/p&gt;
    &lt;p&gt;Provision makes collaborative document management software for the construction industry. Instead of manually reading through thousands of pages of documents and revisions, Provision organizes and extracts information so constructors can save hundreds of hours per bid and make more money.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/provision/jobs/JJ9fZxg-fullstack-software-engineer-in-person-toronto-canada"/><published>2025-10-07T12:41:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502502</id><title>The evolution of Lua, continued [pdf]</title><updated>2025-10-07T13:43:29.048053+00:00</updated><content/><link href="https://www.lua.org/doc/cola.pdf"/><published>2025-10-07T12:54:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502541</id><title>Qualcomm to Acquire Arduino</title><updated>2025-10-07T13:43:28.752608+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qualcomm.com/news/releases/2025/10/qualcomm-to-acquire-arduino-accelerating-developers--access-to-i"/><published>2025-10-07T13:00:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502543</id><title>Erlang ARM32 JIT is born</title><updated>2025-10-07T13:43:27.604495+00:00</updated><content>&lt;doc fingerprint="ea279a502acd05f8"&gt;
  &lt;main&gt;
    &lt;p&gt;A blog series recounting our adventures in the quest to port the BEAM JIT to the ARM32-bit architecture.&lt;/p&gt;
    &lt;p&gt;This work is made possible thanks to funding from the Erlang Ecosystem Foundation and the ongoing support of its Embedded Working Group.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Erlang ARM32 JIT is born!&lt;/head&gt;
    &lt;p&gt;This week we finally achieved our first milestone in developing the ARM32 JIT. We executed our first Erlang function through JITted ARM32 machine code!&lt;/p&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/arm32-jit/otp/RELEASE -progname erl -home /home
    ~/arm32-jit$ echo $?
    42&lt;/code&gt;
    &lt;p&gt;The BEAM successfully runs and terminates with error code 42! That 42 comes from an Erlang function, just-in-time compiled by our ARM32 JIT!&lt;/p&gt;
    &lt;p&gt;Announcement is done! All code is available at https://github.com/stritzinger/otp/tree/arm32-jit&lt;/p&gt;
    &lt;p&gt;Keep reading for a lot of interesting details!&lt;/p&gt;
    &lt;head rend="h2"&gt;The first piece of Erlang code&lt;/head&gt;
    &lt;code&gt;-module(hello).
-export([start/2]).

start(_BootMod, _BootArgs) -&amp;gt;
    halt(42, [{flush, false}]).&lt;/code&gt;
    &lt;p&gt;This is &lt;code&gt;hello.erl&lt;/code&gt; that contains a &lt;code&gt;start/2&lt;/code&gt; function. The function head mimics the &lt;code&gt;erl_init:start/2&lt;/code&gt; function, which is the entry point of the first Erlang process. We replaced &lt;code&gt;erl_init:start/2&lt;/code&gt; with &lt;code&gt;hello:start/2&lt;/code&gt; in the &lt;code&gt;erl_init.c&lt;/code&gt; module of the BEAM VM. This way, we forced the runtime to execute this Erlang function.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;hello:start/2&lt;/code&gt; is very simple as it just calls the &lt;code&gt;erlang:halt/2&lt;/code&gt;. This function is a BIF (Built-in Function) that executes C code, part of the BEAM VM. This code executes an ordered shutdown of the BEAM and allows us to customize the error code, in this case: &lt;code&gt;42&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;(Why &lt;code&gt;{flush, false}&lt;/code&gt;? At the time I am writing this, letting it be true causes a segmentation fault EHEH)&lt;/p&gt;
    &lt;p&gt;Obviously, we need to compile this Erlang module, but I will also generate the BEAM assembly so we can have a look at what we will have to deal with.&lt;/p&gt;
    &lt;code&gt;{module, hello}.  %% version = 0
{exports, [{module_info,0},{module_info,1},{start,2}]}.
{attributes, []}.
{labels, 7}.

{function, start, 2, 2}.
  {label,1}.
    {line,[{location,"erts/preloaded/src/hello.erl",74}]}.
    {func_info,{atom,hello},{atom,start},2}.
  {label,2}.
    {move,{literal,[{flush,false}]},{x,1}}.
    {move,{integer,42},{x,0}}.
    {line,[{location,"erts/preloaded/src/hello.erl",76}]}.
    {call_ext_only,2,{extfunc,erlang,halt,2}}.

{function, module_info, 0, 4}.
  {label,3}.
    {line,[]}.
    {func_info,{atom,hello},{atom,module_info},0}.
  {label,4}.
    {move,{atom,hello},{x,0}}.
    {call_ext_only,1,{extfunc,erlang,get_module_info,1}}.

{function, module_info, 1, 6}.
  {label,5}.
    {line,[]}.
    {func_info,{atom,hello},{atom,module_info},1}.
  {label,6}.
    {move,{x,0},{x,1}}.
    {move,{atom,hello},{x,0}}.
    {call_ext_only,2,{extfunc,erlang,get_module_info,2}}.&lt;/code&gt;
    &lt;p&gt;You can spot the start function and the two standard module_info functions that all Erlang modules have. We do not care much about those right now as we discovered that they are not executed and are not required to work, for now.&lt;/p&gt;
    &lt;p&gt;We can see that the core of the start function is just two &lt;code&gt;move&lt;/code&gt; operations and one &lt;code&gt;call_ext_only&lt;/code&gt;. But bear in mind that the BEAM loader will transmute these Generic BEAM Operations into Specific operations. More complexity will pop up!&lt;/p&gt;
    &lt;head rend="h2"&gt;Execution&lt;/head&gt;
    &lt;p&gt;We are using &lt;code&gt;qemu-arm&lt;/code&gt; to emulate &lt;code&gt;Arm32&lt;/code&gt; and we are directly using &lt;code&gt;beam.smp&lt;/code&gt; to run the BEAM.&lt;/p&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/vagrant/arm32-jit/otp/RELEASE -progname erl -home /home/vagrant&lt;/code&gt;
    &lt;head rend="h3"&gt;JIT initialization&lt;/head&gt;
    &lt;p&gt;At boot, the BEAM initializes the JIT if enabled. The JIT leverages the AsmJit library to emit all machine code instructions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Emission of all global shared fragments&lt;/head&gt;
    &lt;p&gt;There are 90+ code snippets that are shared among all modules. The JIT loads them one single time and sets up jumps to them in every other module. It is like a global library for all modules.&lt;/p&gt;
    &lt;p&gt;We skipped most of these because just the shared fragments involved in the &lt;code&gt;hello:start/2&lt;/code&gt; execution were needed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Emission of the erts_beamasm module&lt;/head&gt;
    &lt;p&gt;As part of the JIT initialization, &lt;code&gt;erts_beamasm&lt;/code&gt; is emitted. This module is an internal hardcoded module that exists only when BEAM is using the JIT. It holds 7 fundamental instructions used to manage the Erlang process executions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;run_process - The main process execution entry point&lt;/item&gt;
      &lt;item&gt;normal_exit - Normal process termination&lt;/item&gt;
      &lt;item&gt;continue_exit - Continue after exit handling&lt;/item&gt;
      &lt;item&gt;exception_trace - Exception tracing functionality&lt;/item&gt;
      &lt;item&gt;return_trace - Return value tracing&lt;/item&gt;
      &lt;item&gt;return_to_trace - Return to tracing state&lt;/item&gt;
      &lt;item&gt;call_trace_return - Call tracing return handling&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Preloaded modules&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;hello.erl&lt;/code&gt; module has been compiled and put as first and single Erlang module in the list of preloaded modules. Preloaded modules are Erlang fundamental modules that are always loaded by the BEAM before the first Erlang process can start. They implement, in Erlang, the core features of the Erlang Runtime System (ERTS). The OTP build scripts group all &lt;code&gt;ebin&lt;/code&gt; files into a single C header that is then linked into the executable. This makes the Erlang binaries available as a static C array in the BEAM source code. These are then loaded one by one after the BEAM VM is initialized.&lt;/p&gt;
    &lt;p&gt;Cool, let's nuke all these modules and leave just our &lt;code&gt;hello.erl&lt;/code&gt;. It does not need many BEAM instructions and we can easily verify that it executes. To do the substitution we just need to change this build variable in otp/erts/emulator/Makefile.in&lt;/p&gt;
    &lt;p&gt;We are running BEAMASM with &lt;code&gt;-JDdump true&lt;/code&gt; so &lt;code&gt;asmjit&lt;/code&gt; will dump all ARM32 assembly for each module! This is incredibly useful if monitored while executing with a debugger, as we can see the assembler being printed line by line by our code.&lt;/p&gt;
    &lt;code&gt;~/arm32-jit$ cat hello.asm 
L6:
.byte 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_1:
# i_func_info_IaaI
# hello:start/2
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x0B, 0xA4, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00
# aligned_label_Lt
start/2:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L9
    bl L11
L9:
# i_test_yield
    adr r2, start/2
    subs r9, r9, 1
    b.le L13
# i_move_sd
    ldr r12, [L14]
    str r12, [r4, 68]
# i_move_sd
    movw r12, 687
    str r12, [r4, 64]
# line_I
# allocate_tt
# call_light_bif_be
L15:
    ldr r3, [L16]
    movw r1, 10188
    movt r1, 16432
    adr r2, L15
# BIF: erlang:halt/2
    sub r12, r7, 4
    cmp r10, r12
    b.ls L17
    udf 48879
L17:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L18
    udf 57005
L18:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_3:
# i_func_info_IaaI
# hello:module_info/0
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x4B, 0x6B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
# aligned_label_Lt
module_info/0:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L23
    bl L11
L23:
# i_test_yield
    adr r2, module_info/0
    subs r9, r9, 1
    b.le L13
# i_move_sd
    movw r12, 20235
    str r12, [r4, 64]
# allocate_tt
# call_light_bif_be
L24:
    ldr r3, [L25]
    movw r1, 4772
    movt r1, 16425
    adr r2, L24
# BIF: erlang:get_module_info/1
    sub r12, r7, 4
    cmp r10, r12
    b.ls L26
    udf 48879
L26:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L27
    udf 57005
L27:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# i_flush_stubs
# func_line_I
# aligned_label_Lt
label_5:
# i_func_info_IaaI
# hello:module_info/1
    blx L8
.byte 0x00, 0x00, 0x00, 0x00
.byte 0x0B, 0x4F, 0x00, 0x00, 0x4B, 0x6B, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00
# aligned_label_Lt
module_info/1:
# i_breakpoint_trampoline
    str lr, [r7, -4]!
    b L28
    bl L11
L28:
# i_test_yield
    adr r2, module_info/1
    subs r9, r9, 1
    b.le L13
# i_move_sd
    ldr r12, [r4, 64]
    str r12, [r4, 68]
# i_move_sd
    movw r12, 20235
    str r12, [r4, 64]
# allocate_tt
# call_light_bif_be
L29:
    ldr r3, [L30]
    movw r1, 4868
    movt r1, 16425
    adr r2, L29
# BIF: erlang:get_module_info/2
    sub r12, r7, 4
    cmp r10, r12
    b.ls L31
    udf 48879
L31:
    movw r12, 12424
    add r12, r4, r12
    ldr r12, [r12]
    cmp sp, r12
    b.eq L32
    udf 57005
L32:
    bl L20
# deallocate_t
    movw r0, 64676
    movt r0, 16480
    blx L22
# return
    movw r0, 61636
    movt r0, 16480
    blx L22
# int_code_end
L33:
    movw r0, 18576
    movt r0, 16480
    blx L22
L13:
L12:
    movw r12, 1968
    movt r12, 14656
    blx r12
L22:
L21:
    movw r12, 29192
    movt r12, 16399
    blx r12
L11:
L10:
    movw r12, 1752
    movt r12, 14656
    blx r12
L20:
L19:
    movw r12, 680
    movt r12, 14656
    blx r12
L8:
L7:
    movw r12, 1824
    movt r12, 14656
    blx r12
# Begin stub section
L14:
.xword 0x000000007FFFFFFF
L16:
.xword 0x000000007FFFFFFF
L25:
.xword 0x000000007FFFFFFF
L30:
.xword 0x000000007FFFFFFF
# End stub section
L34:
.section .rodata {#1}
md5:
.byte 0x6D, 0xC4, 0x1E, 0xF1, 0x13, 0x1E, 0xBF, 0xF2, 0x4B, 0xF5, 0xC0, 0x41, 0x57, 0x86, 0xDF, 0xD5
.section .text {#0}
; CODE_SIZE: 632&lt;/code&gt;
    &lt;p&gt;Bear in mind, this assembler is not what hello should look like. We are missing a lot of things.&lt;/p&gt;
    &lt;p&gt;You can spot many sequences like:&lt;/p&gt;
    &lt;code&gt;    movw r0, 64676
    movt r0, 16480
    blx L22 # &amp;lt;---- branch to NYI&lt;/code&gt;
    &lt;p&gt;This is a call to &lt;code&gt;nyi&lt;/code&gt; (Not Yet Implemented) function and the argument loaded to R0 is the pointer to a string that contains the name of the BEAM instruction that should have been emitted instead. You can spot many of these since we are only emitting the code to reach halt. Everything after that is not important now as halt will never return!&lt;/p&gt;
    &lt;p&gt;There are many more comments we could make around all the details in this assembler dump, but let's move on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Jumping into Jitted code!&lt;/head&gt;
    &lt;p&gt;Later in the BEAM initialization the first Erlang process will be allocated and started.&lt;/p&gt;
    &lt;p&gt;We swap the module and function with hello in erts/emulator/beam/erl_init.c&lt;/p&gt;
    &lt;code&gt;    erl_spawn_system_process(&amp;amp;parent, am_hello, am_start, args, &amp;amp;so);&lt;/code&gt;
    &lt;p&gt;One BEAM scheduler thread will jump to the &lt;code&gt;process_main&lt;/code&gt; function. You can find it here in the source code. This is emitted by our JIT and is the first emitted code that will run.&lt;/p&gt;
    &lt;p&gt;Here we need to handle the Erlang processes scheduling by calling BEAM routines that implement the algorithms of Erlang concurrency, like &lt;code&gt;erts_schedule&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;erts_schedule&lt;/code&gt; will return the pointer to the &lt;code&gt;Process&lt;/code&gt; C structure that holds all information about the process that is going to execute. We then load all necessary data inside registers and then we branch to the exact point where the program execution stopped.&lt;/p&gt;
    &lt;head rend="h3"&gt;The first Erlang function call&lt;/head&gt;
    &lt;p&gt;In this case we are calling &lt;code&gt;hello:start/2&lt;/code&gt; so the first instruction to execute is &lt;code&gt;apply_only&lt;/code&gt; that does a few things but ends up calling the C &lt;code&gt;apply&lt;/code&gt; routine.&lt;/p&gt;
    &lt;p&gt;The routine processes the Module-Function-Arity information to get the address where the function code resides in memory.&lt;/p&gt;
    &lt;p&gt;What follows is the Erlang function prologue. You can see it in the assembler code section above. For example, all functions have these instructions in their prologue:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;i_breakpoint_trampoline: handle breakpoints for the &lt;code&gt;debugger&lt;/code&gt;app&lt;/item&gt;
      &lt;item&gt;i_test_yield: checks if the function should yield and go back to the scheduler&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We have minimal or partial implementations of these since we do not really need them. We have to emit them though, as the C++ generated loader functions from the BEAM are expanding the Erlang function call Operation into a more specific and complex function prologue sequence.&lt;/p&gt;
    &lt;p&gt;After that, we added support for the &lt;code&gt;call_light_bif&lt;/code&gt; operation that precedes the call to the halt_2 BIF routine. This implementation is also minimal.&lt;/p&gt;
    &lt;p&gt;Question for later: did you notice that we put a &lt;code&gt;42&lt;/code&gt; as a number in the code? Numeric constants are printed as decimals in the dump, but we cannot spot any 42!?&lt;/p&gt;
    &lt;p&gt;After the call, we see two other operations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;dealloc&lt;/item&gt;
      &lt;item&gt;return&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are just calls to NYI as we will never reach this code! So for now, we can skip them...&lt;/p&gt;
    &lt;head rend="h3"&gt;Let's roll the JIT!&lt;/head&gt;
    &lt;code&gt;    ~/arm32-jit$ qemu-arm -L /usr/arm-linux-gnueabihf ./otp/RELEASE/erts-15.0/bin/beam.smp -S 1:1 -SDcpu 1:1 -SDio 1 -JDdump true -JMsingle     true -- -root /home/arm32-jit/otp/RELEASE -progname erl -home /home
    ~/arm32-jit$&lt;/code&gt;
    &lt;p&gt;Impressive, the program returns immediately without even saying "Hi" ... and without Segmentation Fault!!&lt;/p&gt;
    &lt;p&gt;But let's check the program return code!&lt;/p&gt;
    &lt;code&gt;~/arm32-jit$ echo $?
42
&lt;/code&gt;
    &lt;p&gt;We can safely say that number is not there by accident! This is a great achievement as from now on we will be able to incrementally add Erlang instructions.&lt;/p&gt;
    &lt;p&gt;Every Erlang line we add will trigger new Opcodes. By emitting them and running the code we will have immediate feedback on everything.&lt;/p&gt;
    &lt;p&gt;The next goal now is to complete the &lt;code&gt;hello&lt;/code&gt; module to host all possible beam instructions!&lt;/p&gt;
    &lt;head rend="h4"&gt;Hey where is 42???&lt;/head&gt;
    &lt;p&gt;One interesting thing I spotted looking at the assembly: You cannot find the number &lt;code&gt;42&lt;/code&gt; in there. Or actually, you can, it is just hidden in plain sight. To understand you need to know how we are using ARM32 registers.&lt;/p&gt;
    &lt;p&gt;In particular the register &lt;code&gt;r4&lt;/code&gt;, a callee-saved register. We are using it to store the pointer to the &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt; struct. The &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt; contains the X register array. When a function is called, X registers are used to store the arguments of the call.&lt;/p&gt;
    &lt;p&gt;This becomes more obvious if we compare the Erlang assembly to the Arm32 assembly.&lt;/p&gt;
    &lt;code&gt;# i_move_sd                       &amp;lt;---- {move,{literal,[{flush,false}]},{x,1}}. % List at X[1]
    ldr r12, [L14]
    str r12, [r4, 68]
# i_move_sd                       &amp;lt;---- {move,{integer,42},{x,0}}. % 42 at X[0]
    movw r12, 687 
    str r12, [r4, 64]
# line_I
# allocate_tt
# call_light_bif_be
L15:
    ldr r3, [L16]
    movw r1, 10188
    movt r1, 16432
    adr r2, L15
# BIF: erlang:halt/2
# ...&lt;/code&gt;
    &lt;p&gt;42 is stored at &lt;code&gt;r4&lt;/code&gt;+64.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;r4: pointer to the &lt;code&gt;ErtsSchedulerRegisters&lt;/code&gt;struct&lt;/item&gt;
      &lt;item&gt;64: base offset from the beginning of the struct to the beginning of the &lt;code&gt;x_reg_array&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The list is stored at &lt;code&gt;r4&lt;/code&gt;+68.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;68: is the base offset + the size of one &lt;code&gt;Eterm&lt;/code&gt;(4 bytes on ARM32)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But why in assembly do we see 687 and not 42?&lt;/p&gt;
    &lt;p&gt;Converting both numbers to hex we get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;42 -&amp;gt; 2A&lt;/item&gt;
      &lt;item&gt;687 -&amp;gt; 2AF !!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yep, this is an example of a Tagged Value. If we consult the BEAM book we can learn about the Tagging Scheme:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;00 11 Pid&lt;/item&gt;
      &lt;item&gt;01 11 Port&lt;/item&gt;
      &lt;item&gt;10 11 Immediate 2&lt;/item&gt;
      &lt;item&gt;11 11 Small integer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;42 is tagged with &lt;code&gt;1111&lt;/code&gt; at the low end. So the BEAM can quickly recognize during a pattern match that this Erlang Term is a Small Integer!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.grisp.org/blog/posts/2025-10-07-jit-arm32.3"/><published>2025-10-07T13:00:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45502748</id><title>3M May Escape Toxic Chemical, PFAS Manufacturing Legacy</title><updated>2025-10-07T13:43:26.978161+00:00</updated><content>&lt;doc fingerprint="2e10915d0a385ff3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;3M Might Just Escape Its Toxic Chemical Legacy&lt;/head&gt;
    &lt;p&gt;Decades of selling PFAS left the iconic American manufacturer mired in legal liabilities. A new CEO is hoping to spark a turnaround.&lt;/p&gt;
    &lt;p&gt;The Command strip is one of those quintessential 3M products. Released in 1996, it was simple yet revolutionary, strong enough to hold items without stripping paint when removed. Soon it was fastening framed photos, bathroom towels and outdoor decorations around the world.&lt;/p&gt;
    &lt;p&gt;The adhesive that made the strip possible was invented by a 3M scientist in the late 1980s. The exact science is a closely guarded secret, but it can resist gravity while somehow pulling away clean when tugged as instructed. In hindsight the market for such an invention is obvious, and yet it was almost lost to corporate bureaucracy. 3M shelved the project at one point in the ‚Äô90s and revived it only after impassioned pleas from a determined product development executive. His persistence paid off: Within three years of its debut, the Command strip was turning a $10 million profit. 3M Co. now sells more than 200 varieties, bringing in $500 million a year.&lt;/p&gt;
    &lt;p&gt;As Command strips turned into a product empire, though, they also became captive to 3M‚Äôs unique form of industrial sprawl. Historically the company has organized its factories by material science and manufacturing processes, rather than by product‚Äîthink chemicals in one facility, adhesives in another and packaging somewhere else, for example, regardless of their ultimate end use. This way, the thinking went, with each innovation, 3M could wring more value from existing machinery and underlying technologies. But every new product and geographic market also brought with it new costs and complexities, resulting in a labyrinthine factory network. This approach meant that Command strip production took place at multiple sites, sometimes hundreds of miles apart. Add in more steps for distribution, and the journey to Walmart shelves of what is, at base, just a particularly good sticky plastic hook looks pretty convoluted.&lt;/p&gt;
    &lt;p&gt;3M is one of the most sparkling brand names in US business history, known for advancing material science to the point that the first astronauts who walked on the moon were equipped with boots made from its synthetic rubber. If America wanted it, 3M could invent and produce it, building its fortunes on products as innocuous as Post-it notes and as dangerous as chemicals lining nonstick pans. From its headquarters in St. Paul, the company also became known in the business world for its Minnesota-nice corporate culture, a contrast to the ruthlessness championed by the likes of longtime General Electric Co. Chief Executive Officer Jack Welch. 3M‚Äôs executives were unfailingly polite, its senior scientists were allowed to devote 15% of their time to research projects of their choosing, and many employees worked at 3M for their entire careers, long after staying in one job forever ceased to be fashionable.&lt;/p&gt;
    &lt;p&gt;But problems were bubbling under the surface, ones that went well beyond struggles with successful lines like the Command strip. First and foremost, those pan-lining chemicals‚Äîspecifically certain types of perfluoroalkyl and polyfluoroalkyl substances, or PFAS, which were also used in products such as Scotchgard fabric protector and firefighting foam‚Äîhave been found to increase the risk of cancer, decrease fertility and suppress the immune system, including weakening response to vaccines. Those findings have fueled a vast array of lawsuits that analysts estimate could end up costing 3M in the neighborhood of $20 billion, including ones it has already settled. Adding to those legal woes, the company spent years facing down major multidistrict litigation claiming it knowingly sold defective earplugs that left US military service members with hearing loss and tinnitus. (In agreements to resolve both of those lawsuits, 3M hasn‚Äôt admitted to any liability or wrongdoing.)&lt;/p&gt;
    &lt;p&gt;As 3M dealt with these issues, its traditional strengths were atrophying. New product introductions slowed to a trickle, and sales growth was lackluster. If America wanted it, 3M grew less and less likely to be making it. By late 2021, as most businesses were regaining their footing after the pandemic, it appeared to many observers almost as if 3M had forgotten how to be a manufacturing company. Investors stopped treating it like one, viewing it more through the lens of liability and risk than of factory output and projected revenue.&lt;/p&gt;
    &lt;p&gt;Even 3M‚Äôs Minnesota-nice reputation has taken some hits. Senior executives privately complain that in recent years, beneath all the Midwestern cheer, the working environment has been marked by tone deafness, overbearing bureaucracy and resistance to change. Decisions were slow, fixes pushed by management felt blunt and grating, and accountability was hard to come by. Worker-safety protocols also lapsed, with the incident rate at its factories becoming significantly higher than at North American peers such as Corteva, Dow and GE.&lt;/p&gt;
    &lt;p&gt;Effectively, 3M became ‚Äúa case study of everything not to do over a very long period of time,‚Äù says Scott Davis, an analyst at Melius Research LLC. It became a ‚Äúbroken company.‚Äù And when that happens, Davis says, ‚Äúyou want change, you crave change.‚Äù&lt;/p&gt;
    &lt;p&gt;With almost $100 billion of market value wiped out from the peak in 2018 to the start of 2024, 3M last year tapped Bill Brown, an outsider who‚Äôd spent much of his career in the defense industry, to turn things around. He hosted his first earnings call about three months into the job. Analysts and investors expected him to offer the usual platitudes about hosting listening sessions and considering all options. Instead he talked about the Command strip‚Äîhow it demonstrated perfectly that the company had gotten too big, too slow, too convoluted. Brown said he had a plan to tackle all that. 3M‚Äôs stock price shot up more than 20% by close, its best one-day rally in more than four decades on the public markets.&lt;/p&gt;
    &lt;p&gt;The company declined to comment on the reporting in this story, instead saying in a statement that it‚Äôs ‚Äúfocused on creating shareholder value and positioning 3M for success by driving profitable growth, embedding a culture of operational excellence across the enterprise.‚Äù&lt;/p&gt;
    &lt;p&gt;It will help Brown‚Äôs turnaround plan that 3M‚Äôs liabilities have become less open-ended: A major settlement with water utilities over PFAS pollution and a deal to resolve the earplugs litigation, both agreed to in 2023, have reset investor expectations for the overall bill. The Trump administration also appears set to take a lighter touch to environmental regulation, providing relief to companies like 3M.&lt;/p&gt;
    &lt;p&gt;But even the optimists concede that this is no easy fix. In the months since Brown took the reins, he‚Äôs been relying on the same playbook that Dave Cote used to restore Honeywell International Inc. in the early 2000s, when it faced a raft of asbestos lawsuits and the aftereffects of several dubious megamergers, and that Larry Culp deployed this decade to resuscitate GE when it was overwhelmed by debt and a $15 billion mess in a legacy insurance business. Brown‚Äôs plan starts with running 3M better. Get that right, and the really big problems‚Äîthe amorphous liabilities, the cultural corrosion, the public-image crisis‚Äîwon‚Äôt seem quite as insurmountable.&lt;/p&gt;
    &lt;p&gt;Minnesota Mining &amp;amp; Manufacturing started in 1902 as a venture to mine corundum, which is harder than all other minerals save for diamonds and an ideal ingredient in sandpaper. But when its founders discovered they were in fact extracting a different low-grade material, they decided to get out of mining and into making the sandpaper instead, using raw materials sourced elsewhere.&lt;/p&gt;
    &lt;p&gt;Similarly, 3M didn‚Äôt discover PFAS. These synthetic chemicals trace back to the Manhattan Project, which required scientists to invent a way to extract uranium to make an atomic bomb. A chemical engineering professor at Pennsylvania State College named Joseph H. Simons developed a process to create almost unbreakable carbon fluorine bonds by passing raw fluorine, a highly volatile gas, through a carbon arc. He later developed a method for practical production of fluorocarbons, a process 3M employed to pioneer the use of PFAS in consumer and industrial products starting in the 1950s. The company eventually became a major producer both for its own products and for customers such as DuPont de Nemours Inc., which used it in Teflon.&lt;/p&gt;
    &lt;p&gt;Near-indestructibility is useful for everything from developing nuclear weapons to repelling grease and water. But these chemicals take years to break down, making them highly problematic in the environment and the human body. For a long time, 3M billed manufactured PFAS and its PFAS-infused products as perfectly safe, even as internal documents and studies unearthed through lawsuits showed the company was aware of the chemicals‚Äô toxicity at least as far back as the 1970s.&lt;/p&gt;
    &lt;p&gt;In 1998, Richard Purdy, an ecological toxicologist at 3M, conducted a study that found PFAS even in the blood of bald eagle nestlings, which primarily eat fish and inhabit remote areas. He quit the next year and sent a copy of his resignation letter to the US Environmental Protection Agency, saying the company had failed to properly communicate to customers and regulators the results of research showing how prevalent the chemicals had become in people and the environment. In 2000, 3M announced it would voluntarily stop making the forms of the chemicals that had the most research documenting their hazards, even as it maintained the chemicals were safe and didn‚Äôt pose a long-term health issue. In 2006, the company reached a $1.5 million settlement with the EPA over reporting violations related to PFAS.&lt;/p&gt;
    &lt;p&gt;More research about the health consequences of PFAS followed, including an influential 2012 study of children born in the remote Faroe Islands that found the chemicals in bloodstreams at levels comparable to the US population and indicated a resulting weakened immune response to vaccines. As the production of certain kinds of PFAS declined, the concentration of the chemicals in people‚Äôs bloodstreams fell sharply, according to research from the US Centers for Disease Control and Prevention and others. But after decades of use, the legal challenges, and the public fallout, were just beginning.&lt;/p&gt;
    &lt;p&gt;The tipping point came in 2018. That January, The Devil We Know‚Äîan investigative documentary examining a West Virginia community‚Äôs fight against a plant, then owned by DuPont, that made Teflon‚Äîpremiered at the Sundance Film Festival. A few weeks after that, 3M agreed, without admitting wrongdoing, to pay $850 million to settle a lawsuit brought by Minnesota over claims the company had poisoned drinking water in its own backyard. And in June the CDC, drawing on scientific advances that allowed PFAS detection at much lower levels, published a draft report warning that the chemicals can cause health problems at significantly lower exposure thresholds than what the EPA had claimed at the time was safe.&lt;/p&gt;
    &lt;p&gt;The following month, Michael Roman took over as 3M‚Äôs CEO. A 30-year company veteran, Roman had overseen businesses in the US, Europe and Asia and had served as 3M‚Äôs chief operating officer. He was the embodiment of its culture of politesse, but this ultimately wouldn‚Äôt help him much with the storm that was brewing. PFAS claims came to span challenges from state attorneys general, water utilities, people with personal injury and property complaints, and foreign governments such as those of Belgium and the Netherlands.&lt;/p&gt;
    &lt;p&gt;Adding to the pile, 3M was also being sued over an earplug business it had acquired through the 2008 purchase of Aearo Technologies. In July 2018, the same month Roman took over, 3M had agreed, without admitting wrongdoing, to a $9.1 million settlement with the US Department of Justice to resolve allegations that it had sold earplugs to the US military without disclosing defects. Cases brought by veterans claiming hearing damage mushroomed after the settlement and dragged on for years.&lt;/p&gt;
    &lt;p&gt;With potential liabilities stewing outside its factories, 3M was simultaneously dealing with issues inside them. Having long prided itself on being a desirable and safe place to work, the company was seeing an uptick in the number of safety violations at its factories. From 2019 to 2024, 3M received at least 27 ‚Äúserious‚Äù initial citations, which are based on worksite investigations, from the Occupational Safety and Health Administration‚Äîmore than any other company among a group of its North American industrial and health-care technology peers, according to a Bloomberg Businessweek analysis.&lt;/p&gt;
    &lt;p&gt;3M also got one repeat admonishment and notice of three willful violations, meaning it knowingly failed to comply with a legal requirement or acted with ‚Äúplain indifference to employee safety.‚Äù (3M and other companies in the dataset have contested many of the citations, and have in some cases succeeded in getting them downgraded or dismissed as part of settlements. In response to requests for comment from Businessweek, Carlisle said that its injury rate is ‚Äúvery low‚Äù compared with the industry average and that it seeks to ‚Äúcontinuously improve‚Äù its safety standards, and GE said it took immediate action to report the incident that led to its citations and to prevent a reoccurrence.)&lt;/p&gt;
    &lt;p&gt;Two of 3M‚Äôs willful violations were linked to inadequate safeguards for a plastic extrusion machine that required employees to thread material through by hand. Trisha Jones, who operated the machine at the company‚Äôs plant in Prairie du Chien, Wisconsin, died in May 2023 after getting caught in the large rollers and suffering traumatic head injuries. ‚ÄúThey‚Äôre not managing these facilities well,‚Äù says David Michaels, who ran OSHA from 2009 to 2017 and is now a professor of public health at George Washington University. ‚ÄúThat‚Äôs a sign of safety management not being implemented.‚Äù&lt;/p&gt;
    &lt;p&gt;The safety incidents jarred with the benevolent culture workers had come to expect. Peter Gibbons, formerly in charge of overseeing supply chains across the company, would regularly contend that the biggest cause of injuries at factories was employees falling down stairs while using their phones, so much so that ‚ÄúUse the Handrail‚Äù became a running joke among staff, according to people with the company who, like others interviewed for this piece, asked not to be named discussing private interactions or information. (Gibbons didn‚Äôt respond to requests for comment.)&lt;/p&gt;
    &lt;p&gt;People close to 3M say the rise in safety incidents reflected years of underinvestment in factories to save costs, as well as a decision by Roman in 2020 to make oversight of manufacturing operations the responsibility of managers in charge of the business lines, rather than ones who were on-site at plants. The shift was part of a 2020 reorganization, dubbed ‚ÄúAdvanced 3M,‚Äù that was intended to make the company more responsive to customer needs.&lt;/p&gt;
    &lt;p&gt;It was one of many sweeping gestures Roman undertook during his six years at the helm. Another was Polaris, a suite of digital tools introduced in 2021 to give customers a more Amazon-like experience when placing and tracking orders; the initiative ended up costing more than planned and hasn‚Äôt been fully implemented since going online, the people close to the company say. Roman also pursued several cost-cutting efforts that cost hundreds of workers their jobs while barely denting 3M‚Äôs financial results. In fact its profit margins went sideways, in contrast to peers that were improving manufacturing processes, raising prices and adding market share.&lt;/p&gt;
    &lt;p&gt;During his tenure, Roman developed a reputation internally for being thin-skinned. In 2020, at the height of the pandemic, he hosted virtual town halls as part of a planned series. But, according to the people close to the company, the series was abruptly canceled after he learned that anonymous commenters had criticized his presentations for reasons that included a lack of detail and transparency about Covid-19 strategy and protocols. (Roman didn‚Äôt respond to requests for comment.)&lt;/p&gt;
    &lt;p&gt;Roman‚Äôs biggest challenge may have been that an outsize amount of his time, rather than being spent running an industrial company, was spent managing the PFAS problem, even though manufactured PFAS represented a fraction of 3M‚Äôs business. In 2022 the Biden administration proposed to designate certain PFAS as hazardous materials under the federal Superfund law, and the EPA declared that virtually no amount of the chemicals is safe in drinking water. Not long after, Roman announced 3M would cease all remaining production of PFAS and work to discontinue their use in its products by the end of 2025. In internal deliberations, people familiar with the conversations say, executives had pointed out to Roman that PFAS are integral to the production of semiconductors, electric-vehicle batteries and weapons systems‚Äîthe very types of goods the US government under President Joe Biden was seeking to make more of at home. At the very least, some argued, 3M should sell its PFAS manufacturing facilities rather than simply shutting them down. Roman didn‚Äôt want to hear it: 3M was done with PFAS, and that was that.&lt;/p&gt;
    &lt;p&gt;Getting all the PFAS out of its products has proved complicated, though. In a testament to how prevalent the chemicals have become, 3M has cautioned investors that they may continue to be present beyond 2025 in certain products that contain components manufactured by third parties, including lithium-ion batteries, printed circuit boards and some seals and gaskets. 3M said this is because approved substitutes that meet regulatory and industry standards may not be available.&lt;/p&gt;
    &lt;p&gt;In the year after Roman‚Äôs announcement, 3M finally started putting some of its legal challenges behind it. The company announced a deal in June 2023 to pay as much as $12.5 billion over 13 years to test and treat city water supplies for PFAS, resolving current and future claims by municipal utilities over pollution. A few months later it announced it had reached a separate $6 billion agreement to resolve claims in the earplug litigation.&lt;/p&gt;
    &lt;p&gt;3M continued to chip away at its PFAS lawsuits after Brown replaced Roman in 2024. This May it agreed to pay New Jersey as much as $450 million to resolve claims related to PFAS pollution at the 3M-supplied Chambers Works plant, as well as general complaints about natural resource damage from the chemicals.&lt;/p&gt;
    &lt;p&gt;As settlements have accrued, estimates for how much the company might ultimately have to pay have gone down. Andrew Obin, an analyst at Bank of America Corp., says 3M might be on the hook for $11.5 billion in settlements related to US suits over damages to natural resources alone. But he says the settlement math in New Jersey‚Äîwhich has particularly acute levels of PFAS pollution‚Äîimplies his estimate for this category of claims should be materially lower, closer to $6 billion.&lt;/p&gt;
    &lt;p&gt;The regulatory landscape is also changing in ways that might help 3M. The shift began with the US Supreme Court‚Äôs decision, handed down last June 28, to overturn the Chevron doctrine, the legal principle that empowered executive branch agencies to interpret and enforce regulations. The ruling gave momentum to lawsuits filed by chemical industry and manufacturing groups as well as water utility associations, challenging the Biden administration‚Äôs limits on PFAS in drinking water as arbitrary and financially impossible. This May, President Donald Trump‚Äôs EPA announced it was delaying its compliance deadline for removal of the two best-known types of PFAS and was rescinding and reconsidering the limits on four other categories. And earlier this month, in a major shift, the EPA asked the court to vacate the drinking water rule for the four other categories of PFAS, concurring with the water utility plaintiffs that aspects of the process by which the standards had been established were unlawful. The agency has delayed a rule, too, that would have required PFAS manufacturers to file reports about environmental and health effects from 2011 to 2022. (The EPA said in a statement that it‚Äôs ‚Äúcommitted to protecting public health by addressing PFAS in drinking water while following the law and ensuring that regulatory compliance is achievable for drinking water systems.‚Äù)&lt;/p&gt;
    &lt;p&gt;The US Chamber of Commerce is also leading a lawsuit against the EPA over the designation of certain types of PFAS as hazardous materials under the Superfund law‚Äîwhich it contends could force companies to collectively pay as much as $17.4 billion in cleanup costs just for nonfederal sites that have been identified as a priority. After repeatedly getting the proceedings postponed, the Trump administration announced this month that it would retain the Superfund designation for the two best-known kinds of PFAS, while calling upon Congress to address concerns about entities getting stuck with cleanup bills for chemicals they didn‚Äôt themselves manufacture. The litigation itself continues.&lt;/p&gt;
    &lt;p&gt;Obin, the Bank of America analyst, pegs 3M‚Äôs liability for cleaning up Superfund sites at as much as $9 billion but says that bill could also end up being lower as legal challenges drag out the policy‚Äôs implementation. And he points out that the company could recover as much as 25% of its total PFAS liabilities through insurance payouts. ‚ÄúIn conversations with investors, the narrative has changed,‚Äù Obin says. ‚ÄúMaybe three years ago, people would say, ‚ÄòWhy are your estimates so low?‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;3M‚Äôs other big remaining undefined PFAS liability is personal injury lawsuits. These, too, may turn out to be less financially crippling than analysts previously feared. In major multidistrict litigation, plaintiffs are arguing that they got cancer by being exposed to water contaminated with PFAS from firefighting foam and are seeking damages from 3M and other companies. The defendants‚Äô lawyers have sought to exclude expert testimony for the plaintiffs on the link between PFAS and cancer, contending that they relied on research findings that aren‚Äôt statistically significant at the relevant exposure levels for the claimants. A bellwether trial had been set to come before a federal district court in South Carolina in October. It‚Äôs since been postponed ‚Äúuntil such a time as the court deems appropriate,‚Äù to allow both sides to sift through a large number of unfiled claims and determine which of these should be included in the proceeding. The judge overseeing the case has urged the two sides to settle, and this latest development may make such an agreement more likely, according to Barclays Plc analyst Julian Mitchell. As it stands, Mitchell estimates that all of 3M‚Äôs outstanding PFAS liabilities, including personal injury and natural resources claims, could be as much as $11 billion, down from his $16 billion calculation immediately after an agreement to settle the water lawsuits was announced in 2023.&lt;/p&gt;
    &lt;p&gt;Companies have recovered from liability disasters before. Davis, the Melius Research analyst, recalls a peer predicting in the early 2000s that Honeywell would go bankrupt because of asbestos claims, as well as a short seller who infamously claimed GE was on a path to financial collapse because of festering liabilities in a legacy insurance business. Neither forecast came true. The critical thing for 3M, Davis says, is improving cash flow so its liabilities seem less like an existential wrecking ball than an expense. It needs to improve to the point that ‚Äúcash-flow growth is so strong that people say, ‚ÄòYou know what? They‚Äôve got the liabilities, and they can do buybacks and M&amp;amp;A, and operate as a real entity.‚Äô‚Äù The key for Honeywell and GE, he points out, was finding the right CEO to clean up their operations.&lt;/p&gt;
    &lt;p&gt;3M‚Äôs choice for that was Brown. Before signing on, he had a successful career overseeing defense contractor Harris Corp., now known as L3Harris Technologies Inc. after he shepherded a merger with L3 Technologies in 2019. Former colleagues say they thought that, after Brown stepped down in 2022, he might land a cushy private equity gig, not join a struggling conglomerate facing giant legal liabilities. Asked earlier this year at a JPMorgan Chase &amp;amp; Co. conference why he took the 3M job given the PFAS baggage, Brown joked that it wasn‚Äôt the -15F winter weather at headquarters in Minnesota. Rather, he said, PFAS had been so all-consuming for 3M that it had an opportunity to simply pay more attention to everything else‚Äîas long as the liabilities didn‚Äôt end up being worse than investors‚Äô downtrodden expectations, then 3M‚Äôs stock price could climb much higher. It was, he added, ‚Äúa great opportunity to engage with this great iconic company called 3M and try to help make it great again.‚Äù&lt;/p&gt;
    &lt;p&gt;Filings show that Brown is either 62 or 63 (the company declined to specify which). Known as an exercise fanatic, he‚Äôs trying to bring a comparable rigor of routine to 3M. The plan he‚Äôs revealed for fixing it isn‚Äôt all that complex. For starters, he cleared out much of the upper management under his predecessor, including replacing the chief financial officer, the head of investor relations and the top supply chain manager. Brown also lured in Amazon.com Inc. executive Wendy Bauer to take over 3M‚Äôs transportation and electronics division.&lt;/p&gt;
    &lt;p&gt;He‚Äôs vowed, too, to reboot 3M‚Äôs vaunted ‚Äúinnovation machine,‚Äù to cut costs and to drag the company‚Äôs manufacturing operations into the modern era, including making factories safer. ‚ÄúOur performance in safety has not been where we should be,‚Äù he said at the conference. ‚ÄúOur focus is around zero injuries, zero spills, zero incidents.‚Äù&lt;/p&gt;
    &lt;p&gt;Brown has also been pushing to improve how the company satisfies orders‚Äîwhen he took over, more than 10% of 3M deliveries were showing up late or incomplete‚Äîand how it makes each product in its sprawling portfolio. He‚Äôs suggested the company might reduce its 25,000-strong network of suppliers and eventually its empire of 110 factories, with its manufacturing equipment currently being used at only about 60% of capacity. Every week he and his top lieutenants review 3M‚Äôs business lines and factories, hunting for logjams and inefficiencies that cause deliveries to fall short of expectations.&lt;/p&gt;
    &lt;p&gt;Those who worked with Brown at L3Harris praise his leadership style. An engineer by training, he‚Äôs obsessed with pursuing excellence in all facets of a company‚Äôs operations, according to Jay Malave, a former L3Harris executive who was recently named Boeing Co.‚Äôs CFO. Rahul Ghai, CFO of GE Aerospace and a former Brown deputy himself, says, ‚ÄúWhat makes him successful is his tremendous attention to detail.‚Äù He adds: ‚ÄúThis is the right guy for the job.‚Äù Both say that even though Brown can be demanding, he delivers marching orders with positivity.&lt;/p&gt;
    &lt;p&gt;At a Bank of America conference in May, Brown said 3M‚Äôs culture needs to encourage more individual accountability if his other improvements are to stick. When he joined the company, only about 10 people had performance-based stock compensation agreements. Now 1,500 people are paid this way. And everyone is being encouraged to act with urgency. The mantra, he said, is ‚ÄúGet it done tonight, not tomorrow. If it can be done in the next minute, do it in the next minute.‚Äù&lt;/p&gt;
    &lt;p&gt;Despite recent market volatility related to the global trade war started by Trump‚Äôs tariffs, 3M shares are still worth about 60% more than they were when Brown started last May. That‚Äôs more than double the gains for a broader group of industrial companies on the S&amp;amp;P 500 over that time period. But the stock is still down about 30% from its peak in early 2018, before the PFAS challenges came to a head.&lt;/p&gt;
    &lt;p&gt;Brown has been at pains to emphasize to investors that this turnaround isn‚Äôt going to be quick. Success would be resolving 3M‚Äôs legal cases without any jarring surprises, revving its innovation apparatus back up to the point that investors are convinced it can increase revenue, and improving the efficiency of its supply chain and manufacturing network.&lt;/p&gt;
    &lt;p&gt;In a testament to how hard that last challenge will be, Brown isn‚Äôt the first 3M CEO to cite the Command strip as an example of what needs to be fixed: George Buckley, who ran the company from 2005 to 2012, used to describe 3M‚Äôs disjointed assembly processes for the strip and other products as ‚Äúhairballs.‚Äù At the time he started trying to untangle these knots, one part of the production process for Command hooks began with adhesives made at a 3M plant in Missouri, which were then shipped to Indiana to be applied to polyethylene foam. The foam then went to Minnesota, where the 3M logo was added and the strips were cut to size. Finally the tabs went to Wisconsin to be packaged along with a plastic hook. According to company executives interviewed by the Wall Street Journal in 2012, under Buckley‚Äôs oversight, 3M consolidated these particular steps in the Command strip production at one plant.&lt;/p&gt;
    &lt;p&gt;It was an improvement, sure, though it didn‚Äôt resolve the problem. Today, the end-to-end process still spans a handful of factories.&lt;/p&gt;
    &lt;p&gt;‚ÄúYou will never fix it, because that was how the company was built,‚Äù Obin says of 3M‚Äôs manufacturing labyrinth. But perhaps that‚Äôs OK. He points out that it wasn‚Äôt an issue during periods in the company‚Äôs history when revenue and profits were growing. ‚ÄúIt just needs to be run properly.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bloomberg.com/features/2025-3m-pfas-toxic-legacy-turnaround/"/><published>2025-10-07T13:22:04+00:00</published></entry></feed>