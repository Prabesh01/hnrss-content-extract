<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-30T17:37:07.953774+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45423268</id><title>I’ve removed Disqus. It was making my blog worse</title><updated>2025-09-30T17:37:14.525069+00:00</updated><content>&lt;doc fingerprint="a2e43117070623c3"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;Intro&lt;/head&gt;
    &lt;p&gt;This will be a short and sweet post. As I’m not big on goodbyes.&lt;/p&gt;
    &lt;p&gt;Disqus started showing ads for their “free” tier comments system a few years back. At the time, the communication they sent out via email, seemed quite laid-back and had the tone of “don’t worry about it, it’s not a big thing”. Which in part lead me to almost forget it happened.&lt;/p&gt;
    &lt;p&gt;At the time, the disqus comments system looked quite smart and sleek. I remember thinking that the ads system will possibly look smart and sleek too. Which alleviated any worries I had at the time.&lt;/p&gt;
    &lt;p&gt;WELL&amp;amp;mldr;&amp;amp;mldr;.I’ve just seen the ads, and they look horrific!!!&lt;/p&gt;
    &lt;head rend="h4"&gt;Apologies&lt;/head&gt;
    &lt;p&gt;I have a Pihole set up, so ads are blocked on my home network. When I’m out of the house, my phone is connected to a Wireguard VPN which routes my data through my home internet, therefore - getting all the ad-blocking, Pihole goodness.&lt;/p&gt;
    &lt;p&gt;After years with Pi-hole, which now blocks over a million domains, I’ve become incredibly accustomed to a mostly ad-free web. Without realizing it, Iâd forgotten what the typical internet experience feels like.&lt;/p&gt;
    &lt;p&gt;I used to get a couple of emails from Disqus, letting me know that there’s a new comment on this blog. I haven’t had many of these emails recently, so I decided to disable my adblocker for a few minutes and check out the comments.&lt;/p&gt;
    &lt;p&gt;There were none, instead I was greeted by some horribly formatted and obviously scammy ads:&lt;/p&gt;
    &lt;p&gt;For the people who read this blog, I’m sorry.&lt;/p&gt;
    &lt;p&gt;I became “blind” to what the web is really like for most users. Iâve tried to keep this blog minimalist - a clean place to find answers. Those ads not only ruin that experience; they trample privacy too:&lt;/p&gt;
    &lt;p&gt;With this post, Iâve removed Disqus. It was making my blog worse, and frankly, they were profiting off my work and my visitor’s data. I want this blog to be a resource for devs and technologists, free not just in money, but in freedom from unwanted tracking and invasive ads.&lt;/p&gt;
    &lt;head rend="h4"&gt;Any Alternatives?&lt;/head&gt;
    &lt;p&gt;Iâm not entirely sure comments are needed here. There are other ways to reach me, for example; GitHub or Twitter/X. But having a place for discussion under each post can be valuable. If you have any recommendations for alternative commenting systems (especially those that respect privacy or are self-hosted), Iâd love to hear them! Please reach out if youâve found something that works well.&lt;/p&gt;
    &lt;p&gt;Thanks as always for reading - your trust matters to me.&lt;/p&gt;
    &lt;p&gt;Sorry again for the mess!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ryansouthgate.com/goodbye-disqus/"/><published>2025-09-30T08:36:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45423917</id><title>Comprehension debt: A ticking time bomb of LLM-generated code</title><updated>2025-09-30T17:37:14.213967+00:00</updated><content>&lt;doc fingerprint="87c012014d34283f"&gt;
  &lt;main&gt;
    &lt;p&gt;An effect that’s being more and more widely reported is the increase in time it’s taking developers to modify or fix code that was generated by Large Language Models.&lt;/p&gt;
    &lt;p&gt;If you’ve worked on legacy systems that were written by other people, perhaps decades ago, you’ll recognise this phenomenon. Before we can safely change code, we first need to understand it – understand what it does, and also oftentimes why it does it the way it does. In that sense, this is nothing new.&lt;/p&gt;
    &lt;p&gt;What is new is the scale of the problem being created as lightning-speed code generators spew reams of unread code into millions of projects.&lt;/p&gt;
    &lt;p&gt;Teams that care about quality will take the time to review and understand (and more often than not, rework) LLM-generated code before it makes it into the repo. This slows things down, to the extent that any time saved using the LLM coding assistant is often canceled out by the downstream effort.&lt;/p&gt;
    &lt;p&gt;But some teams have opted for a different approach. They’re the ones checking in code nobody’s read, and that’s only been cursorily tested – if it’s been tested at all. And, evidently, there’s a lot of them.&lt;/p&gt;
    &lt;p&gt;When teams produce code faster than they can understand it, it creates what I’ve been calling “comprehension debt”. If the software gets used, then the odds are high that at some point that generated code will need to change. The “A.I.” boosters will say “We can just get the tool to do that”. And that might work maybe 70% of the time.&lt;/p&gt;
    &lt;p&gt;But those of us who’ve experimented a lot with using LLMs for code generation and modification know that there will be times when the tool just won’t be able to do it.&lt;/p&gt;
    &lt;p&gt;“Doom loops”, when we go round and round in circles trying to get an LLM, or a bunch of different LLMs, to fix a problem that it just doesn’t seem to be able to, are an everyday experience using this technology. Anyone claiming it doesn’t happen to them has either been extremely lucky, or is fibbing.&lt;/p&gt;
    &lt;p&gt;It’s pretty much guaranteed that there will be many times when we have to edit the code ourselves. The “comprehension debt” is the extra time it’s going to take us to understand it first.&lt;/p&gt;
    &lt;p&gt;And we’re sitting on a rapidly growing mountain of it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/"/><published>2025-09-30T10:37:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45424223</id><title>Orbiting the Hénon Attractor</title><updated>2025-09-30T17:37:14.074128+00:00</updated><content>&lt;doc fingerprint="20ed59e32d044a20"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Purpose-built for displays of data&lt;/head&gt;
      &lt;p&gt;Observable is your go-to platform for exploring data and creating expressive data visualizations. Use reactive JavaScript notebooks for prototyping and a collaborative canvas for visual data exploration and dashboard creation.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://observablehq.com/@yurivish/orbiting-the-henon-attractor"/><published>2025-09-30T11:31:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425298</id><title>dgsh – Directed Graph Shell</title><updated>2025-09-30T17:37:12.512223+00:00</updated><content>&lt;doc fingerprint="577bba9067313cc8"&gt;
  &lt;main&gt;&lt;p&gt;The directed graph shell, dgsh (pronounced /dÃ¦É¡Ê/ — dagsh), provides an expressive way to construct sophisticated and efficient big data set and stream processing pipelines using existing Unix tools as well as custom-built components. It is a Unix-style shell (based on bash) allowing the specification of pipelines with non-linear non-uniform operations. These form a directed acyclic process graph, which is typically executed by multiple processor cores, thus increasing the operation's processing throughput.&lt;/p&gt;&lt;p&gt;If you want to get a feeling on how dgsh works in practice, skip right down to the examples section.&lt;/p&gt;&lt;p&gt; For a more formal introduction to dgsh or to cite it in your work, see:&lt;lb/&gt; Diomidis Spinellis and Marios Fragkoulis. Extending Unix Pipelines to DAGs. IEEE Transactions on Computers, 2017. doi: 10.1109/TC.2017.2695447 &lt;/p&gt;&lt;p&gt;Dgsh provides two new ways for expressing inter-process communication.&lt;/p&gt;&lt;code&gt;comm&lt;/code&gt; command supplied with dgsh
expects two input channels and produces on its output three
output channels: the lines appearing only in first (sorted) channel,
the lines appearing only in the second channel,
and the lines appearing in both.
Connecting the output of the &lt;code&gt;comm&lt;/code&gt; command to the
&lt;code&gt;cat&lt;/code&gt; command supplied with dgsh
will make the three outputs appear in sequence,
while connecting it to the
&lt;code&gt;paste&lt;/code&gt; command supplied with dgsh
will make the output appear in its customary format.
&lt;code&gt;md5sum&lt;/code&gt; and &lt;code&gt;wc -c&lt;/code&gt;
receives two inputs and produces two outputs:
the MD5 hash of its input and the input's size.
Data to multipipe blocks are typically provided with a
dgsh-aware version of &lt;code&gt;tee&lt;/code&gt; and collected by
dgsh-aware versions of programs such as
&lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;paste&lt;/code&gt;.
&lt;code&gt;dgsh-writeval&lt;/code&gt;, and
a reader program, &lt;code&gt;dgsh-readval&lt;/code&gt;.
The behavior of a stored value's IO can be modified by adding flags to
&lt;code&gt;dgsh-writeval&lt;/code&gt; and &lt;code&gt;dgsh-readval&lt;/code&gt;.
&lt;p&gt;A dgsh script follows the syntax of a bash(1) shell script with the addition of multipipe blocks. A multipipe block contains one or more dgsh simple commands, other multipipe blocks, or pipelines of the previous two types of commands. The commands in a multipipe block are executed asynchronously (in parallel, in the background). Data may be redirected or piped into and out of a multipipe block. With multipipe blocks dgsh scripts form directed acyclic process graphs. It follows from the above description that multipipe blocks can be recursively composed.&lt;/p&gt;&lt;p&gt;As a simple example consider running the following command directly within dgsh&lt;/p&gt;&lt;quote&gt;{{ echo hello &amp;amp; echo world &amp;amp; }} | paste&lt;/quote&gt;&lt;p&gt; or by invoking &lt;code&gt;dgsh&lt;/code&gt; with the command as an argument.
&lt;/p&gt;&lt;quote&gt;dgsh -c '{{ echo hello &amp;amp; echo world &amp;amp; }} | paste'&lt;/quote&gt;&lt;p&gt; The command will run paste with input from the two echo processes to output &lt;code&gt;hello world&lt;/code&gt;.
This is equivalent to running the following bash command,
but with the flow of data appearing in the natural left-to-right order.
&lt;/p&gt;&lt;quote&gt;paste &amp;lt;(echo hello) &amp;lt;(echo world)&lt;/quote&gt;&lt;p&gt; In the following larger example, which compares the performance of different compression utilities, the script's standard input is distributed to three compression utilities (xz, bzip2, and gzip), to assess their performance, and also to file and wc to report the input data's type and size. The printf commands label the data of each processing type. All eight commands pass their output to the &lt;code&gt;cat&lt;/code&gt; command, which gathers their outputs
in order.
&lt;/p&gt;&lt;quote&gt;tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt; Formally, dgsh extends the syntax of the (modified) Unix Bourne-shell when &lt;code&gt;bash&lt;/code&gt; provided with the &lt;code&gt;--dgsh&lt;/code&gt; argument
as follows.
&lt;/p&gt;&lt;quote&gt;&amp;lt;dgsh_block&amp;gt; ::= '{{' &amp;lt;dgsh_list&amp;gt; '}}' &amp;lt;dgsh_list&amp;gt; ::= &amp;lt;dgsh_list_item&amp;gt; '&amp;amp;' &amp;lt;dgsh_list_item&amp;gt; &amp;lt;dgsh_list&amp;gt; &amp;lt;dgsh_list_item&amp;gt; ::= &amp;lt;simple_command&amp;gt; &amp;lt;dgsh_block&amp;gt; &amp;lt;dgsh_list_item&amp;gt; '|' &amp;lt;dgsh_list_item&amp;gt;&lt;/quote&gt;&lt;p&gt;A number of Unix tools have been adapted to support multiple inputs and outputs to match their natural capabilities. This echoes a similar adaptation that was performed in the early 1970s when Unix and the shell got pipes and the pipeline syntax. Many programs that worked with files were adjusted to work as filters. The number of input and output channels of dgsh-compatible commands are as follows, based on the supplied command-line arguments.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Tool&lt;/cell&gt;&lt;cell role="head"&gt;Inputs&lt;/cell&gt;&lt;cell role="head"&gt;Outputs&lt;/cell&gt;&lt;cell role="head"&gt;Notes&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cat (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;No options are supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cmp&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;comm&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;Output streams in order: lines only in first file, lines only in second one, and lines in both files&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;cut&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;With &lt;code&gt;--multistream&lt;/code&gt; output each range into a different stream&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Typically two inputs. Compare an arbitrary number of input streams with the &lt;code&gt;--from-file&lt;/code&gt; or &lt;code&gt;--to-file&lt;/code&gt; options&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;diff3&lt;/cell&gt;&lt;cell&gt;0—3&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grep&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;0—4&lt;/cell&gt;&lt;cell&gt;Available output streams (via arguments): matching files, non-matching files, matching lines, and non-matching lines&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;join&lt;/cell&gt;&lt;cell&gt;0—2&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;paste&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Paste N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;perm&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;1—N&lt;/cell&gt;&lt;cell&gt;Rearrange the order of N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;sort&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;With the &lt;code&gt;-m&lt;/code&gt; option, merge sort N input streams&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;tee (dgsh-tee)&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—M&lt;/cell&gt;&lt;cell&gt;Only the &lt;code&gt;-a&lt;/code&gt; option is supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-readval&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Read a value from a socket&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;dgsh-wrap&lt;/cell&gt;&lt;cell&gt;0—N&lt;/cell&gt;&lt;cell&gt;0—1&lt;/cell&gt;&lt;cell&gt;Wrap non-dgsh commands and negotiate on their behalf&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;dgsh-writeval&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;Write a value to a socket&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; In addition, POSIX user commands that receive no input or only generate no output, when executed in a dgsh context are wrapped to specify the corresponding input or output capability. For example, an &lt;code&gt;echo&lt;/code&gt; command in a multipipe block
will appear to receive no input, but will provide one output stream.
By default &lt;code&gt;dgsh&lt;/code&gt; automatically wraps all other
commands as filters.
&lt;/p&gt;&lt;p&gt;Finally, note that any dgsh script will accept and generate the number of inputs and outputs associated with the commands or multipipe blocks at its two endpoints.&lt;/p&gt;&lt;p&gt;The dgsh suite has been tested under Debian and Ubuntu Linux, FreeBSD, and Mac OS X. A Cygwin port is underway.&lt;/p&gt;&lt;p&gt;An installation of GraphViz will allow you to visualize the dgsh graphs that you specify in your programs.&lt;/p&gt;&lt;p&gt;To compile and run dgsh you will need to have the following commands installed on your system:&lt;/p&gt;&lt;quote&gt;make automake gcc libtool pkg-config texinfo help2man autopoint bison check gperf git xz-utils gettextTo test dgsh you will need to have the following commands installed in your system:&lt;/quote&gt;&lt;quote&gt;wbritish wamerican libfftw3-dev csh curl bzip2&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;make config&lt;/quote&gt;&lt;quote&gt;make&lt;/quote&gt;&lt;quote&gt;sudo make install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;make PREFIX=$HOME config make make install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;make test&lt;/quote&gt;&lt;p&gt;To compile and run dgsh you will need to have the following packages installed in your system:&lt;/p&gt;&lt;quote&gt;devel/automake devel/bison devel/check devel/git devel/gmake devel/gperf misc/help2man print/texinfo shells/bashTo test dgsh you will need to have the following ports installed on your system:&lt;/quote&gt;&lt;quote&gt;archivers/bzip2 ftp/curl&lt;/quote&gt;&lt;p&gt;Go through the following steps.&lt;/p&gt;&lt;quote&gt;git clone --recursive https://github.com/dspinellis/dgsh.git&lt;/quote&gt;&lt;quote&gt;gmake config&lt;/quote&gt;&lt;quote&gt;gmake&lt;/quote&gt;&lt;quote&gt;sudo gmake install&lt;/quote&gt;&lt;p&gt; By default, the program and its documentation are installed under &lt;code&gt;/usr/local&lt;/code&gt;.
You can modify this by setting the &lt;code&gt;PREFIX&lt;/code&gt; variable
in the `config` step, for example:
&lt;/p&gt;&lt;quote&gt;gmake PREFIX=$HOME config gmake gmake install&lt;/quote&gt;&lt;p&gt;Issue the following command.&lt;/p&gt;&lt;quote&gt;gmake test&lt;/quote&gt;&lt;p&gt;These are the manual pages for dgsh, the associated helper programs and the API in formats suitable for browsing and printing. The commands are listed in the order of usefulness in everyday scenarios.&lt;/p&gt;&lt;p&gt;Report file type, length, and compression performance for data received from the standard input. The data never touches the disk. Demonstrates the use of an output multipipe to source many commands from one followed by an input multipipe to sink to one command the output of many and the use of dgsh-tee that is used both to propagate the same input to many commands and collect output from many commands orderly in a way that is transparent to users.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ printf 'File type:\t' file - printf 'Original size:\t' wc -c printf 'xz:\t\t' xz -c | wc -c printf 'bzip2:\t\t' bzip2 -c | wc -c printf 'gzip:\t\t' gzip -c | wc -c }} | cat&lt;/quote&gt;&lt;p&gt;Process the Git history, and list the authors and days of the week ordered by the number of their commits. Demonstrates streams and piping through a function.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh forder() { sort | uniq -c | sort -rn } git log --format="%an:%ad" --date=default "$@" | tee | {{ echo "Authors ordered by number of commits" # Order by frequency awk -F: '{print $1}' | forder echo "Days ordered by number of commits" # Order by frequency awk -F: '{print substr($2, 1, 3)}' | forder }} | cat&lt;/quote&gt;&lt;p&gt;Process a directory containing C source code, and produce a summary of various metrics. Demonstrates nesting, commands without input.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh {{ # C and header code find "$@" \( -name \*.c -or -name \*.h \) -type f -print0 | tee | {{ # Average file name length # Convert to newline separation for counting echo -n 'FNAMELEN: ' tr \\0 \\n | # Remove path sed 's|^.*/||' | # Maintain average awk '{s += length($1); n++} END { if (n&amp;gt;0) print s / n; else print 0; }' xargs -0 /bin/cat | tee | {{ # Remove strings and comments sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Structure definitions echo -n 'NSTRUCT: ' egrep -c 'struct[ ]*{|struct[ ]*[a-zA-Z_][a-zA-Z0-9_]*[ ]*{' #}} (match preceding openings) # Type definitions echo -n 'NTYPEDEF: ' grep -cw typedef # Use of void echo -n 'NVOID: ' grep -cw void # Use of gets echo -n 'NGETS: ' grep -cw gets # Average identifier length echo -n 'IDLEN: ' tr -cs 'A-Za-z0-9_' '\n' | sort -u | awk '/^[A-Za-z]/ { len += length($1); n++ } END { if (n&amp;gt;0) print len / n; else print 0; }' }} # Lines and characters echo -n 'CHLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # Non-comment characters (rounded thousands) # -traditional avoids expansion of tabs # We round it to avoid failing due to minor # differences between preprocessors in regression # testing echo -n 'NCCHAR: ' sed 's/#/@/g' | cpp -traditional -P | wc -c | awk '{OFMT = "%.0f"; print $1/1000}' # Number of comments echo -n 'NCOMMENT: ' egrep -c '/\*|//' # Occurences of the word Copyright echo -n 'NCOPYRIGHT: ' grep -ci copyright }} }} # C files find "$@" -name \*.c -type f -print0 | tee | {{ # Convert to newline separation for counting tr \\0 \\n | tee | {{ # Number of C files echo -n 'NCFILE: ' wc -l # Number of directories containing C files echo -n 'NCDIR: ' sed 's,/[^/]*$,,;s,^.*/,,' | sort -u | wc -l }} # C code xargs -0 /bin/cat | tee | {{ # Lines and characters echo -n 'CLINESCHAR: ' wc -lc | awk '{OFS=":"; print $1, $2}' # C code without comments and strings sed 's/#/@/g;s/\\[\\"'\'']/@/g;s/"[^"]*"/""/g;'"s/'[^']*'/''/g" | cpp -P | tee | {{ # Number of functions echo -n 'NFUNCTION: ' grep -c '^{' # Number of gotos echo -n 'NGOTO: ' grep -cw goto # Occurrences of the register keyword echo -n 'NREGISTER: ' grep -cw register # Number of macro definitions echo -n 'NMACRO: ' grep -c '@[ ]*define[ ][ ]*[a-zA-Z_][a-zA-Z0-9_]*(' # Number of include directives echo -n 'NINCLUDE: ' grep -c '@[ ]*include' # Number of constants echo -n 'NCONST: ' grep -ohw '[0-9][x0-9][0-9a-f]*' | wc -l }} }} }} # Header files echo -n 'NHFILE: ' find "$@" -name \*.h -type f | wc -l }} | # Gather and print the results cat&lt;/quote&gt;&lt;p&gt;List the names of duplicate files in the specified directory. Demonstrates the combination of streams with a relational join.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Create list of files find "$@" -type f | # Produce lines of the form # MD5(filename)= 811bfd4b5974f39e986ddc037e1899e7 xargs openssl md5 | # Convert each line into a "filename md5sum" pair sed 's/^MD5(//;s/)= / /' | # Sort by MD5 sum sort -k2 | tee | {{ # Print an MD5 sum for each file that appears more than once awk '{print $2}' | uniq -d # Promote the stream to gather it cat }} | # Join the repeated MD5 sums with the corresponding file names # Join expects two inputs, second will come from scatter # XXX make streaming input identifiers transparent to users join -2 2 | # Output same files on a single line awk ' BEGIN {ORS=""} $1 != prev &amp;amp;&amp;amp; prev {print "\n"} END {if (prev) print "\n"} {if (prev) print " "; prev = $1; print $2}'&lt;/quote&gt;&lt;p&gt;Highlight the words that are misspelled in the command's first argument. Demonstrates stream processing with multipipes and the avoidance of pass-through constructs to avoid deadlocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh export LC_ALL=C tee | {{ # Find errors {{ # Obtain list of words in text tr -cs A-Za-z \\n | tr A-Z a-z | sort -u # Ensure dictionary is compatibly sorted sort /usr/share/dict/words }} | # List errors as a set difference comm -23 # Pass through text cat }} | grep --fixed-strings --file=- --ignore-case --color --word-regex --context=2&lt;/quote&gt;&lt;p&gt;Read text from the standard input and list words containing a two-letter palindrome, words containing four consonants, and words longer than 12 characters.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Stream input from file cat $1 | # Split input one word per line tr -cs a-zA-Z \\n | # Create list of unique words sort -u | tee | {{ # Pass through the original words cat # List two-letter palindromes sed 's/.*\(.\)\(.\)\2\1.*/p: \1\2-\2\1/;t g' # List four consecutive consonants sed -E 's/.*([^aeiouyAEIOUY]{4}).*/c: \1/;t g' # List length of words longer than 12 characters awk '{if (length($1) &amp;gt; 12) print "l:", length($1); else print ""}' }} | # Paste the four streams side-by-side paste | # List only words satisfying one or more properties fgrep :&lt;/quote&gt;&lt;p&gt;Creates a report for a fixed-size web log file read from the standard input. Demonstrates the combined use of multipipe blocks, writeval and readval to store and retrieve values, and functions in the scatter block. Used to measure throughput increase achieved through parallelism.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Output the top X elements of the input by the number of their occurrences # X is the first argument toplist() { uniq -c | sort -rn | head -$1 echo } # Output the argument as a section header header() { echo echo "$1" echo "$1" | sed 's/./-/g' } # Consistent sorting export LC_ALL=C export -f toplist export -f header if [ -z "${DGSH_DRAW_EXIT}" ] then cat &amp;lt;&amp;lt;EOF WWW server statistics ===================== Summary ------- EOF fi tee | {{ # Number of accesses echo -n 'Number of accesses: ' dgsh-readval -l -s nAccess # Number of transferred bytes awk '{s += $NF} END {print s}' | tee | {{ echo -n 'Number of Gbytes transferred: ' awk '{print $1 / 1024 / 1024 / 1024}' dgsh-writeval -s nXBytes }} echo -n 'Number of hosts: ' dgsh-readval -l -q -s nHosts echo -n 'Number of domains: ' dgsh-readval -l -q -s nDomains echo -n 'Number of top level domains: ' dgsh-readval -l -q -s nTLDs echo -n 'Number of different pages: ' dgsh-readval -l -q -s nUniqPages echo -n 'Accesses per day: ' dgsh-readval -l -q -s nDayAccess echo -n 'MBytes per day: ' dgsh-readval -l -q -s nDayMB # Number of log file bytes echo -n 'MBytes log file size: ' wc -c | awk '{print $1 / 1024 / 1024}' # Host names awk '{print $1}' | tee | {{ # Number of accesses wc -l | dgsh-writeval -s nAccess # Sorted hosts sort | tee | {{ # Unique hosts uniq | tee | {{ # Number of hosts wc -l | dgsh-writeval -s nHosts # Number of TLDs awk -F. '$NF !~ /[0-9]/ {print $NF}' | sort -u | wc -l | dgsh-writeval -s nTLDs }} # Top 10 hosts {{ call 'header "Top 10 Hosts"' call 'toplist 10' }} }} # Top 20 TLDs {{ call 'header "Top 20 Level Domain Accesses"' awk -F. '$NF !~ /^[0-9]/ {print $NF}' | sort | call 'toplist 20' }} # Domains awk -F. 'BEGIN {OFS = "."} $NF !~ /^[0-9]/ {$1 = ""; print}' | sort | tee | {{ # Number of domains uniq | wc -l | dgsh-writeval -s nDomains # Top 10 domains {{ call 'header "Top 10 Domains"' call 'toplist 10' }} }} }} # Hosts by volume {{ call 'header "Top 10 Hosts by Transfer"' awk ' {bytes[$1] += $NF} END {for (h in bytes) print bytes[h], h}' | sort -rn | head -10 }} # Sorted page name requests awk '{print $7}' | sort | tee | {{ # Top 20 area requests (input is already sorted) {{ call 'header "Top 20 Area Requests"' awk -F/ '{print $2}' | call 'toplist 20' }} # Number of different pages uniq | wc -l | dgsh-writeval -s nUniqPages # Top 20 requests {{ call 'header "Top 20 Requests"' call 'toplist 20' }} }} # Access time: dd/mmm/yyyy:hh:mm:ss awk '{print substr($4, 2)}' | tee | {{ # Just dates awk -F: '{print $1}' | tee | {{ # Number of days uniq | wc -l | tee | {{ awk ' BEGIN { "dgsh-readval -l -x -s nAccess" | getline NACCESS;} {print NACCESS / $1}' | dgsh-writeval -s nDayAccess awk ' BEGIN { "dgsh-readval -l -x -q -s nXBytes" | getline NXBYTES;} {print NXBYTES / $1 / 1024 / 1024}' | dgsh-writeval -s nDayMB }} {{ call 'header "Accesses by Date"' uniq -c }} # Accesses by day of week {{ call 'header "Accesses by Day of Week"' sed 's|/|-|g' | call '(date -f - +%a 2&amp;gt;/dev/null || gdate -f - +%a)' | sort | uniq -c | sort -rn }} }} # Hour {{ call 'header "Accesses by Local Hour"' awk -F: '{print $2}' | sort | uniq -c }} }} dgsh-readval -q -s nAccess }} | cat&lt;/quote&gt;&lt;p&gt;Read text from the standard input and create files containing word, character, digram, and trigram frequencies.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Consistent sorting across machines export LC_ALL=C # Convert input into a ranked frequency list ranked_frequency() { awk '{count[$1]++} END {for (i in count) print count[i], i}' | # We want the standard sort here sort -rn } # Convert standard input to a ranked frequency list of specified n-grams ngram() { local N=$1 perl -ne 'for ($i = 0; $i &amp;lt; length($_) - '$N'; $i++) { print substr($_, $i, '$N'), "\n"; }' | ranked_frequency } export -f ranked_frequency export -f ngram tee | {{ # Split input one word per line tr -cs a-zA-Z \\n | tee | {{ # Digram frequency call 'ngram 2 &amp;gt;digram.txt' # Trigram frequency call 'ngram 3 &amp;gt;trigram.txt' # Word frequency call 'ranked_frequency &amp;gt;words.txt' }} # Store number of characters to use in awk below wc -c | dgsh-writeval -s nchars # Character frequency sed 's/./&amp;amp;\ /g' | # Print absolute call 'ranked_frequency' | awk 'BEGIN { "dgsh-readval -l -x -q -s nchars" | getline NCHARS OFMT = "%.2g%%"} {print $1, $2, $1 / NCHARS * 100}' &amp;gt; character.txt }}&lt;/quote&gt;&lt;p&gt;Given as an argument a directory containing object files, show which symbols are declared with global visibility, but should have been declared with file-local (static) visibility instead. Demonstrates the use of dgsh-capable comm (1) to combine data from two sources.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Find object files find "$1" -name \*.o | # Print defined symbols xargs nm | tee | {{ # List all defined (exported) symbols awk 'NF == 3 &amp;amp;&amp;amp; $2 ~ /[A-Z]/ {print $3}' | sort # List all undefined (imported) symbols awk '$1 == "U" {print $2}' | sort }} | # Print exports that are not imported comm -23&lt;/quote&gt;&lt;p&gt;Given two directory hierarchies A and B passed as input arguments (where these represent a project at different parts of its lifetime) copy the files of hierarchy A to a new directory, passed as a third argument, corresponding to the structure of directories in B. Demonstrates the use of join to process results from two inputs and the use of gather to order asynchronously produced results.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh if [ -z "${DGSH_DRAW_EXIT}" -a \( ! -d "$1" -o ! -d "$2" -o -z "$3" \) ] then echo "Usage: $0 dir-1 dir-2 new-dir-name" 1&amp;gt;&amp;amp;2 exit 1 fi NEWDIR="$3" export LC_ALL=C line_signatures() { find $1 -type f -name '*.[chly]' -print | # Split path name into directory and file sed 's|\(.*\)/\([^/]*\)|\1 \2|' | while read dir file do # Print "directory filename content" of lines with # at least one alphabetic character # The fields are separated by and sed -n "/[a-z]/s|^|$dir$file|p" "$dir/$file" done | # Error: multi-character tab '\001\001' sort -T `pwd` -t -k 2 } export -f line_signatures {{ # Generate the signatures for the two hierarchies call 'line_signatures "$1"' -- "$1" call 'line_signatures "$1"' -- "$2" }} | # Join signatures on file name and content join -t -1 2 -2 2 | # Print filename dir1 dir2 sed 's///g' | awk -F 'BEGIN{OFS=" "}{print $1, $3, $4}' | # Unique occurrences sort -u | tee | {{ # Commands to copy awk '{print "mkdir -p '$NEWDIR'/" $3 ""}' | sort -u awk '{print "cp " $2 "/" $1 " '$NEWDIR'/" $3 "/" $1 ""}' }} | # Order: first make directories, then copy files # TODO: dgsh-tee does not pass along first incoming stream cat | sh&lt;/quote&gt;&lt;p&gt;Process the Git history, and create two PNG diagrams depicting committer activity over time. The most active committers appear at the center vertical of the diagram. Demonstrates image processing, mixining of synchronous and asynchronous processing in a scatter block, and the use of an dgsh-compliant join command.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Commit history in the form of ascending Unix timestamps, emails git log --pretty=tformat:'%at %ae' | # Filter records according to timestamp: keep (100000, now) seconds awk 'NF == 2&amp;amp; $1 &amp;gt; 100000&amp;amp; $1 &amp;lt; '`date +%s` | sort -n | tee | {{ {{ # Calculate number of committers awk '{print $2}' | sort -u | wc -l | tee | {{ dgsh-writeval -s committers1 dgsh-writeval -s committers2 dgsh-writeval -s committers3 }} # Calculate last commit timestamp in seconds tail -1 | awk '{print $1}' # Calculate first commit timestamp in seconds head -1 | awk '{print $1}' }} | # Gather last and first commit timestamp cat | # Make one space-delimeted record tr '\n' ' ' | # Compute the difference in days awk '{print int(($1 - $2) / 60 / 60 / 24)}' | # Store number of days dgsh-writeval -s days sort -k2 # &amp;lt;timestamp, email&amp;gt; # Place committers left/right of the median # according to the number of their commits awk '{print $2}' | sort | uniq -c | sort -n | awk ' BEGIN { "dgsh-readval -l -x -q -s committers1" | getline NCOMMITTERS l = 0; r = NCOMMITTERS;} {print NR % 2 ? l++ : --r, $2}' | sort -k2 # &amp;lt;left/right, email&amp;gt; }} | # Join committer positions with commit time stamps # based on committer email join -j 2 | # &amp;lt;email, timestamp, left/right&amp;gt; # Order by timestamp sort -k 2n | tee | {{ # Create portable bitmap echo 'P1' {{ dgsh-readval -l -q -s committers2 dgsh-readval -l -q -s days }} | cat | tr '\n' ' ' | awk '{print $1, $2}' perl -na -e ' BEGIN { open(my $ncf, "-|", "dgsh-readval -l -x -q -s committers3"); $ncommitters = &amp;lt;$ncf&amp;gt;; @empty[$ncommitters - 1] = 0; @committers = @empty; } sub out { print join("", map($_ ? "1" : "0", @committers)), "\n"; } $day = int($F[1] / 60 / 60 / 24); $pday = $day if (!defined($pday)); while ($day != $pday) { out(); @committers = @empty; $pday++; } $committers[$F[2]] = 1; END { out(); } ' }} | cat | # Enlarge points into discs through morphological convolution pgmmorphconv -erode &amp;lt;( cat &amp;lt;&amp;lt;EOF P1 7 7 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 EOF ) | tee | {{ # Full-scale image pnmtopng &amp;gt;large.png # A smaller image pamscale -width 640 | pnmtopng &amp;gt;small.png }}&lt;/quote&gt;&lt;p&gt;Count number of times each word appears in the specified input file(s) Demonstrates parallel execution mirroring the Hadoop WordCount example via the dgsh-parallel command. In contrast to GNU parallel, the block generated by dgsh-parallel has N input and output streams, which can be combined by any dgsh-compatible tool, such as dgsh-merge-sum or sort -m.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Number of processes N=8 # Collation order for sorting export LC_ALL=C # Scatter input dgsh-tee -s | # Emulate Java's default StringTokenizer, sort, count dgsh-parallel -n $N "tr -s ' \t\n\r\f' '\n' | sort -S 512M | uniq -c" | # Merge sorted counts by providing N input channels dgsh-merge-sum $(for i in $(seq $N) ; do printf '&amp;lt;| ' ; done)&lt;/quote&gt;&lt;p&gt;Given the specification of two publication venues, read a compressed DBLP computer science bibliography from the standard input (e.g. piped from curl -s http://dblp.uni-trier.de/xml/dblp.xml.gz or from a locally cached copy) and output the number of papers published in each of the two venues as well as the number of authors who have published only in the first venue, the number who have published only in the second one, and authors who have published in both. The venues are specified through the script's first two command-line arguments as a DBLP key prefix, e.g. journals/acta/, conf/icse/, journals/software/, conf/iwpc/, or conf/msr/. Demonstrates the use of dgsh-wrap -e to have sed(1) create two output streams and the use of tee to copy a pair of streams into four ones.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # Extract and sort author names sorted_authors() { sed -n 's/&amp;lt;author&amp;gt;\([^&amp;lt;]*\)&amp;lt;\/author&amp;gt;/\1/p' | sort } # Escape a string to make it a valid sed(1) pattern escape() { echo "$1" | sed 's/\([/\\]\)/\\\1/g' } export -f sorted_authors if [ ! "$2" -a ! "$DGSH_DOT_DRAW"] ; then echo "Usage: $0 key1 key2" 1&amp;gt;&amp;amp;2 echo "Example: $0 conf/icse/ journals/software/" 1&amp;gt;&amp;amp;2 exit 1 fi gzip -dc | # Output the two venue authors as two output streams dgsh-wrap -e sed -n " /^&amp;lt;.*key=\"$(escape $1)/,/&amp;lt;title&amp;gt;/ w &amp;gt;| /^&amp;lt;.*key=\"$(escape $2)/,/&amp;lt;title&amp;gt;/ w &amp;gt;|" | # 2 streams in 4 streams out: venue1, venue2, venue1, venue2 tee | {{ {{ echo -n "$1 papers: " grep -c '^&amp;lt;.* mdate=.* key=' echo -n "$2 papers: " grep -c '^&amp;lt;.* mdate=.* key=' }} {{ call sorted_authors call sorted_authors }} | comm | {{ echo -n "Authors only in $1: " wc -l echo -n "Authors only in $2: " wc -l echo -n 'Authors common in both venues: ' wc -l }} }} | cat&lt;/quote&gt;&lt;p&gt;Create two graphs: 1) a broadened pulse and the real part of its 2D Fourier transform, and 2) a simulated air wave and the amplitude of its 2D Fourier transform. Demonstrates using the tools of the Madagascar shared research environment for computational data analysis in geophysics and related fields. Also demonstrates the use of two scatter blocks in the same script, and the used of named streams.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh mkdir -p Fig # The SConstruct SideBySideIso "Result" method side_by_side_iso() { vppen size=r vpstyle=n gridnum=2,1 /dev/stdin $* } export -f side_by_side_iso # A broadened pulse and the real part of its 2D Fourier transform sfspike n1=64 n2=64 d1=1 d2=1 nsp=2 k1=16,17 k2=5,5 mag=16,16 \ label1='time' label2='space' unit1= unit2= | sfsmooth rect2=2 | sfsmooth rect2=2 | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 axis=2 pad=1 | sfreal | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/ft2dofpulse.vpl # A simulated air wave and the amplitude of its 2D Fourier transform sfspike n1=64 d1=1 o1=32 nsp=4 k1=1,2,3,4 mag=1,3,3,1 \ label1='time' unit1= | sfspray n=32 d=1 o=0 | sfput label2=space | sflmostretch delay=0 v0=-1 | tee | {{ sfwindow f2=1 | sfreverse which=2 cat }} | sfcat axis=2 "&amp;lt;|" | tee | {{ sfgrey pclip=100 wanttitle=n sffft1 | sffft3 sign=1 | tee | {{ sfreal sfimag }} | dgsh-wrap -e sfmath nostdin=y re="&amp;lt;|" im="&amp;lt;|" \ output="sqrt(re*re+im*im)" | tee | {{ sfwindow f1=1 | sfreverse which=3 cat }} | sfcat axis=1 "&amp;lt;|" | sfgrey pclip=100 wanttitle=n label1="1/time" label2="1/space" }} | call_with_stdin side_by_side_iso '&amp;lt;|' yscale=1.25 &amp;gt;Fig/airwave.vpl wait&lt;/quote&gt;&lt;p&gt;Nuclear magnetic resonance in-phase/anti-phase channel conversion and processing in heteronuclear single quantum coherence spectroscopy. Demonstrate processing of NMR data using the NMRPipe family of programs.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh # The conversion is configured for the following file: # http://www.bmrb.wisc.edu/ftp/pub/bmrb/timedomain/bmr6443/timedomain_data/c13-hsqc/june11-se-6426-CA.fid/fid var2pipe -in $1 \ -xN 1280 -yN 256 \ -xT 640 -yT 128 \ -xMODE Complex -yMODE Complex \ -xSW 8000 -ySW 6000 \ -xOBS 599.4489584 -yOBS 60.7485301 \ -xCAR 4.73 -yCAR 118.000 \ -xLAB 1H -yLAB 15N \ -ndim 2 -aq2D States \ -verb | tee | {{ # IP/AP channel conversion # See http://tech.groups.yahoo.com/group/nmrpipe/message/389 nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 1 0 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 0 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;A nmrPipe | nmrPipe -fn SOL | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 2 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 177 -p1 0.0 -di | nmrPipe -fn EXT -left -sw -verb | nmrPipe -fn TP | nmrPipe -fn COADD -cList 0 1 -time | nmrPipe -fn SP -off 0.5 -end 0.98 -pow 1 -c 0.5 | nmrPipe -fn ZF -auto | nmrPipe -fn FT | nmrPipe -fn PS -p0 -90 -p1 0 -di | nmrPipe -fn TP | nmrPipe -fn POLY -auto -verb &amp;gt;B }} # We use temporary files rather than streams, because # addNMR mmaps its input files. The diagram displayed in the # example shows the notional data flow. if [ -z "${DGSH_DRAW_EXIT}" ] then addNMR -in1 A -in2 B -out A+B.dgsh.ft2 -c1 1.0 -c2 1.25 -add addNMR -in1 A -in2 B -out A-B.dgsh.ft2 -c1 1.0 -c2 1.25 -sub fi&lt;/quote&gt;&lt;p&gt;Calculate the iterative FFT for n = 8 in parallel. Demonstrates combined use of permute and multipipe blocks.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh dgsh-fft-input $1 | perm 1,5,3,7,2,6,4,8 | {{ {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} {{ dgsh-w 1 0 dgsh-w 1 0 }} | perm 1,3,2,4 | {{ dgsh-w 2 0 dgsh-w 2 1 }} }} | perm 1,5,3,7,2,6,4,8 | {{ dgsh-w 3 0 dgsh-w 3 1 dgsh-w 3 2 dgsh-w 3 3 }} | perm 1,5,2,6,3,7,4,8 | cat&lt;/quote&gt;&lt;p&gt;Reorder columns in a CSV document. Demonstrates the combined use of tee, cut, and paste.&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh tee | {{ cut -d , -f 5-6 - cut -d , -f 2-4 - }} | paste -d ,&lt;/quote&gt;&lt;p&gt; Windows-like DIR command for the current directory. Nothing that couldn't be done with &lt;code&gt;ls -l | awk&lt;/code&gt;.
Demonstrates use of wrapped commands with no input (df, echo).
&lt;/p&gt;&lt;quote&gt;#!/usr/bin/env dgsh ls -n | tee | {{ # Reorder fields in DIR-like way awk '!/^total/ {print $6, $7, $8, $1, sprintf("%8d", $5), $9}' # Count number of files wc -l | tr -d \\n # Print label for number of files echo -n ' File(s) ' # Tally number of bytes awk '{s += $5} END {printf("%d bytes\n", s)}' # Count number of directories grep -c '^d' | tr -d \\n # Print label for number of dirs and calculate free bytes df -h . | awk '!/Use%/{print " Dir(s) " $4 " bytes free"}' }} | cat&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www2.dmst.aueb.gr/dds/sw/dgsh/"/><published>2025-09-30T13:39:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425714</id><title>Deml: The Directed Acyclic Graph Elevation Markup Language</title><updated>2025-09-30T17:37:12.071409+00:00</updated><content>&lt;doc fingerprint="24003ae74061c391"&gt;
  &lt;main&gt;
    &lt;p&gt;Languages designed to represent all types of graph data structures, such as Graphviz's DOT Language and Mermaid JS's flowchart syntax, don't take advantage of the properties specific to DAGs (Directed Acyclic Graphs).&lt;/p&gt;
    &lt;p&gt;DAGs act like rivers. Water doesn't flow upstream (tides and floods being exceptions). Sections of a river at the same elevation can't be the inputs or outputs of each other, like the nodes C, D, and E in the image below. Their input is B. C outputs to F, while D and E output to G.&lt;/p&gt;
    &lt;p&gt;DEML's goal is to use this ordering as part of the file syntax to make it easier for humans to parse. In DEML we represent an elevation marker with &lt;code&gt;----&lt;/code&gt; on a new line. The order of elevation clusters is significant, but the order of nodes between two &lt;code&gt;----&lt;/code&gt; elevation markers is not significant.&lt;/p&gt;
    &lt;code&gt;UpRiver &amp;gt; A
----
A &amp;gt; B
----
B &amp;gt; C | D | E
----
C
D
E
----
F &amp;lt; C
G &amp;lt; D | E &amp;gt; DownRiver
----
DownRiver &amp;lt; F&lt;/code&gt;
    &lt;p&gt;Nodes are defined by the first word on a line. The defined node can point to its outputs with &lt;code&gt;&amp;gt;&lt;/code&gt; and to its inputs with &lt;code&gt;&amp;lt;&lt;/code&gt;. Inputs and outputs are separated by &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Dagrs is a library for running multiple tasks with dependencies defined in a DAG. In DEML, shell commands can be assigned to a node with &lt;code&gt;=&lt;/code&gt;. DEML files can be run via dag-rs with the comand &lt;code&gt;deml run -i &amp;lt;filepath&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To compare the difference in readability, here is the Dagrs YAML example in both YAML and DEML&lt;/p&gt;
    &lt;code&gt;dagrs:
  a:
    name: "Task 1"
    after: [ b, c ]
    cmd: echo a
  b:
    name: "Task 2"
    after: [ c, f, g ]
    cmd: echo b
  c:
    name: "Task 3"
    after: [ e, g ]
    cmd: echo c
  d:
    name: "Task 4"
    after: [ c, e ]
    cmd: echo d
  e:
    name: "Task 5"
    after: [ h ]
    cmd: echo e
  f:
    name: "Task 6"
    after: [ g ]
    cmd: python3 ./tests/config/test.py
  g:
    name: "Task 7"
    after: [ h ]
    cmd: node ./tests/config/test.js
  h:
    name: "Task 8"
    cmd: echo h&lt;/code&gt;
    &lt;code&gt;H &amp;gt; E | G = echo h
----
G = node ./tests/config/test.js
E = echo e
----
F &amp;lt; G = python3 ./tests/config/test.py
C &amp;lt; E | G = echo c
----
B &amp;lt; C | F | G = echo b
D &amp;lt; C | E = echo d
----
A &amp;lt; B | C = echo a&lt;/code&gt;
    &lt;p&gt;To convert DEML files to Mermaid Diagram files (.mmd) use the command &lt;code&gt;deml mermaid -i &amp;lt;inputfile&amp;gt; -o &amp;lt;outputfile&amp;gt;&lt;/code&gt;. The mermaid file can be used to generate an image at mermaid.live&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Put my idea for an elevation based DAG representation into the wild&lt;/item&gt;
      &lt;item&gt;Run DAGs with dag-rs&lt;/item&gt;
      &lt;item&gt;Convert DEML files to Mermaid Diagram files&lt;/item&gt;
      &lt;item&gt;Syntax highlighting tree-sitter-deml&lt;/item&gt;
      &lt;item&gt;Add a syntax to label edges&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was thinking about how it's annoying in languages like C when function declaration order matters. Then I wondered if there could be a case when it would be a nice feature for declaration order to matter and I thought of DAGs.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Mcmartelle/deml"/><published>2025-09-30T14:12:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45425746</id><title>Frank Chimero: I think we're in the lemon stage of the internet</title><updated>2025-09-30T17:37:11.952103+00:00</updated><content>&lt;doc fingerprint="37e8105b91834f8a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Selling Lemons&lt;/head&gt;
    &lt;head rend="h2"&gt;The hidden costs of the meta game&lt;/head&gt;
    &lt;p&gt;I’ve been researching a new talk the last few weeks and along the way stumbled across a concept that’s been rattling around in my head. I am writing to share, because I find it a satisfying description for the tech flop era.&lt;/p&gt;
    &lt;p&gt;The idea is called “a market for lemons.” The phrase comes from a 1970 paper by George Akerlof that explains how information asymmetry between buyers and sellers can undermine a marketplace. Akerlof asks us to imagine ourselves buying a used car. Some cars on the lot are reliable, well-maintained gems. Others cars are lemons, the kinds of cars that can make it off the lot but are disasters waiting to happen. The sellers know which cars are which, but you, as a buyer, can’t tell the difference. That information asymmetry affects the average price in the market and eventually impacts the overall market dynamics.&lt;/p&gt;
    &lt;p&gt;The thinking goes like this: if a buyer can’t distinguish between good and bad, everything gets priced somewhere in the middle. If you’re selling junk, this is fantastic news—you’ll probably get paid more than your lemon is worth. If you’re selling a quality used car, this price is insultingly low. As a result, people with good cars leave the market to sell their stuff elsewhere, which pushes the overall quality and price down even further, until eventually all that’s left on the market are lemons.&lt;/p&gt;
    &lt;p&gt;I think we’re in the lemon stage of the internet.&lt;/p&gt;
    &lt;p&gt;I thought about this last week while shopping online for a sleep mask. Brands like MZOO, YFONG, WAOAW popped up, and these seemed less like companies and more like vowel smoke ejected from a factory flue hole, then slotted into a distribution platform. The long tail of generic brands on e-commerce platforms is a textbook lemons market: good products get drowned out by these alphabet soup products, who use their higher margins to buy sponsored placement in search results. Both buyers and sellers eventually lose (and perhaps the platforms win, as long as they don’t wear out their reputation).&lt;/p&gt;
    &lt;p&gt;For shoppers, buying online now feels like rolling the dice on the quality of the product. For sellers, the gamble is that their survival relies more on gaming the system than actually improving the product.&lt;/p&gt;
    &lt;p&gt;I think the post-pandemic experience has been a collective realization that the value that drew us to certain digital products and marketplaces is gone. Much of this reduction in value gets pinned to ZIRP, but there’s another critical factor—the natural flight of value creators. As platforms matured, the users and sellers who generated real value were squeezed out by players focused on capturing value rather than creating it.&lt;/p&gt;
    &lt;p&gt;Once you identify a lemon market, you start to see it all over the place.&lt;/p&gt;
    &lt;p&gt;Online dating. A lemon market where participants have no familiarity with one another participate in strategic self-presentation. High-quality partners (emotionally available, looking for genuine connection) can’t effectively distinguish themselves from those just seeking validation and eventually leave.&lt;/p&gt;
    &lt;p&gt;Search results. A lemon market where platforms profit from sponsored placement, misaligning incentives with user needs. The first page is a minefield: sponsored listings posing as organic results, SEO content farms, affiliate aggregators. You add “reddit” to work around this, but even that has less success these days.&lt;/p&gt;
    &lt;p&gt;Social media. Your feed is now professional content creators, low-effort podcast video clips, algorithmic filler reaction videos, stand-up chaff, and animals. Good ideas don’t happen frequently enough to satisfy the pace of the algorithm, so many have pivoted to newsletters or stopped posting.&lt;/p&gt;
    &lt;p&gt;What makes the Market for Lemons concept so appealing (and what differentiates it in my mind from enshittification is that everyone can be acting reasonably, pursuing their own interests, and things still get worse for everyone. No one has to be evil or stupid: the platform does what’s profitable, sellers do what works, buyers try to make smart decisions, and yet the whole system degrades into something nobody actually wants.&lt;/p&gt;
    &lt;p&gt;I was first introduced to the Market of Lemons by Dan Luu in an essay titled, Why is it so hard to buy things that work well?. Luu applies the market of lemons as a metaphor, and specifically identifies hiring as a market of lemons, because of the information asymmetry for both companies and individuals.&lt;/p&gt;
    &lt;p&gt;Companies have always struggled to tell the difference between great individual contributors and mediocre ones. Lacking a clear way to separate the two, they lump everyone together and rely on proxy games to evaluate skill. Candidates, for their part, walk into interviews without crucial information: whether the company is quietly dysfunctional, whether the manager they liked during interviews is about to quit, or whether the open role itself is little more than a vestige of an abandoned strategy that’s likely to be cut once the other foot drops. The usual signals of strength or weakness don’t signify much at all when it comes to hiring. Layer on the automated scale of the application process—candidates firing off applications by the hundreds, companies screening by the thousands—and the result is a highly inefficient market that wastes everyone’s time. Meaningful signals get drowned out, everyone gets lumped together, rational players opt out to the extent they are able, and the market slides steadily downward.&lt;/p&gt;
    &lt;p&gt;There have been countless attempts to make hiring more rational and efficient—the stuff of startup pitch deck lore. But I’m not sure hiring can ever be much more efficient, because neither side has reason to show themselves as they really are, warts and all. Idealistically, both would come straight; pragmatically, it is a game of chicken. Candidates polish résumés and present curated versions of their abilities, listing outcomes and impact statistics with dubious accuracy and provenance. Companies do the same, putting culture and mission front and center while hiding systematic dysfunctions and looming existential risks. When neither side is forthcoming, you’re left with proxies: a famous logo on a resume, a polished culture deck. Gaming the meta of the system supersedes the actual development or evaluation of skill. And, much to my disappointment, gaming the meta may, in fact, be an essential aspect of most jobs.&lt;/p&gt;
    &lt;p&gt;At this point, it should be obvious how the market for lemons applies to ill-considered AI-generated content. I’ll let you sketch out that argument yourself since it’s fairly straightforward, and this thing is already long enough.&lt;/p&gt;
    &lt;p&gt;Instead, let’s zag and revisit my point earlier about system-gaming becoming the most viable playbook instead of focusing on the product. As a consumer and as a designer, I hope this is a temporary state before a massive recalibration. The primacy of meta-activities—optimizing for algorithms, visibility theater, consumer entrapment, externalization of costs, performative internal alignment, horse-trading amongst a set of DOA ideas—is poison. It is a road to nowhere worth going.&lt;/p&gt;
    &lt;p&gt;This reflects a business culture obsessed with outcomes while treating outputs as speed bumps. But outputs (code, design, the products themselves) are the load-bearing work—the actual prerequisites for the outcomes desired. Focusing on outcomes while ignoring outputs means hiding in abstractions and absolving oneself of accountability. If any output is acceptable to hit your targets, what awful things emerge at scale? What horrors happen when success detaches completely from the necessity of being good—having both skill and ethics?&lt;/p&gt;
    &lt;p&gt;The safest, smartest path is also the most mundane: keep the main thing the main thing. Outcomes matter, but output literally comes first. Outputs are the business to everyone outside it—what customers see, buy, and use. You can’t stay safe in abstractions forever. Eventually, you must attend to the reality of what’s in front of you, because that’s where work gets done and where assumptions get validated or falsified (because reality has a surprising amount of detail).&lt;/p&gt;
    &lt;p&gt;In other words, the meta ruins things for everyone. To hide in abstractions is to dodge the reality of your choices. These tactics may get you profit, but you sacrifice benefit. The climb may feel like progress, but at the end you’ll find yourself at the top of a mountain of lemons, perhaps not of your own making, but almost certainly of your own doing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://frankchimero.com/blog/2025/selling-lemons/"/><published>2025-09-30T14:14:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426099</id><title>BrowserPod: In-browser full-stack environments for IDEs and Agents via WASM</title><updated>2025-09-30T17:37:11.756741+00:00</updated><content>&lt;doc fingerprint="d3380c7ccb3ed58"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re excited to introduce BrowserPod a WebAssembly-based, in-browser container technology that runs full-stack development environments across multiple languages.&lt;/p&gt;
    &lt;p&gt;BrowserPod is a generalised, more powerful alternative to WebContainers, with advanced networking capabilities and flexible multi-runtime support. Containers, called Pods, run completely client-side. At their core there is a flexible WebAssembly-based engine that can execute multiple programming languages.&lt;/p&gt;
    &lt;p&gt;Each Pod can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run multiple processes or services in parallel, with real concurrency powered by WebWorkers&lt;/item&gt;
      &lt;item&gt;Access a scalable block-based filesystem with privacy-preserving browser-local persistence.&lt;/item&gt;
      &lt;item&gt;Expose virtualized HTTP / REST services to the internet via Portals&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pods are fast booting, since no provision of server-side resources is required. Moreover, multiple Pods can run in each browser tab, enabling complex deployments.&lt;/p&gt;
    &lt;p&gt;BrowserPod is conceptually similar to WebContainers, but is designed from the ground up to be language-agnostic, to support inbound networking, and to be integrated within the Leaning Technologies ecosystem.&lt;/p&gt;
    &lt;p&gt;BrowserPod will be released in late November, with an initial focus on Node.js environments and a well defined path to support additional stacks, with Python and Ruby as immediate priorities.&lt;/p&gt;
    &lt;p&gt;Further capabilities will become available later by integrating BrowserPod with CheerpX, our x86-to-WebAssembly virtualization engine. In particular, we plan to support React Native environments in 2026.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is BrowserPod for?&lt;/head&gt;
    &lt;p&gt;BrowserPod is designed to run complete development environments in the browser, without installing local helper applications or dedicated server-side resources.&lt;/p&gt;
    &lt;p&gt;A typical use case of BrowserPod, exemplified by this demo, would be a browser-based IDE that can run a preview server, for example via &lt;code&gt;npm run dev&lt;/code&gt;. The preview server runs fully in the browser, with each update to files being reflected in the virtualized environment. As files are updated the normal Hot Module Replacement triggers, updating the preview.&lt;/p&gt;
    &lt;p&gt;The application preview is not just available in the same browser session, but is exposed to the internet via a Portal, a seamless solution to allow direct public access to any HTTP service running inside a Pod.&lt;/p&gt;
    &lt;p&gt;Portals make it possible to achieve real cross-device testing of applications and even pre-release sharing of test URLs with external users, including early adopters, stakeholders and clients.&lt;/p&gt;
    &lt;p&gt;BrowserPod’s initial focus will be on Node.js environments, which are supported via a complete build of Node.js, compiled to WebAssembly and virtualized in the browser. BrowserPod will support multiple versioned runtimes, with Node.js 22 being included in the first release.&lt;/p&gt;
    &lt;p&gt;This high-fidelity approach ensures that applications developed in BrowserPod containers will behave the same when migrated to production, making it possible to build real-world applications in Pods, not just prototypes.&lt;/p&gt;
    &lt;p&gt;BrowserPod is a great fit for web-based IDEs, educational environments, interactive documentation websites and AI coding Agents.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does BrowserPod work?&lt;/head&gt;
    &lt;p&gt;BrowserPod builds on our years long experience in delivering high performance in-browser virtualization via WebVM. WebVM is powered by CheerpX, our x86-to-WebAssembly virtualization engine, and it has been the most popular tool we have released so far, with 15k+ stars on GitHub.&lt;/p&gt;
    &lt;p&gt;To make BrowserPod possible we have rearchitectured CheerpX to separate its two main components: the x86-to-WebAssembly JIT compiler engine, and the Linux system call emulation layer. This emulation layer, that we are calling CheerpOS internally, is now shared across CheerpX and BrowserPod and, down the line, it will become a foundation layer across all our products.&lt;/p&gt;
    &lt;p&gt;CheerpOS is effectively a WebAssembly kernel that allows unmodified C/C++ code for Linux to run in the browser. In the context of BrowserPod it is used to provide a unified view of filesystem and access to networking across the multiple processes running in a Pod.&lt;/p&gt;
    &lt;p&gt;On top of this kernel layer, we compile the C++ source code of Node.js, with minimal changes, to a combination of WebAssembly and JavaScript. The use of JavaScript is specific to Node.js and provides the required shortcut to run JavaScript payloads natively in the browser itself, which is critical for high performance execution of Node.js environments.&lt;/p&gt;
    &lt;p&gt;Other stacks, such as Python and Ruby on Rails, will instead run as pure WebAssembly applications on top of the CheerpOS kernel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Licensing&lt;/head&gt;
    &lt;p&gt;BrowserPod will come with a generous free license with attribution, available for non-commercial users and technical evaluations.&lt;/p&gt;
    &lt;p&gt;A transparent pay-as-you-go model will be available for any use and purpose, including companies working on AI codegen tools. Pricing will be announced at release time and it will be very affordable to maximize the adoption of this technology, with discounts available for educational and non-profit use.&lt;/p&gt;
    &lt;p&gt;An Enterprise license will also be available for self-hosting and commercial support.&lt;/p&gt;
    &lt;head rend="h2"&gt;General Availability&lt;/head&gt;
    &lt;p&gt;The initial release of BrowserPod will become generally available in late November - early December 2025, with support for Node.js 22.&lt;/p&gt;
    &lt;p&gt;Over the course of the following year, we have planned several additional releases, including support for multiple Node.js versions, support for Python and Ruby on Rails, and eventually support for React Native environments.&lt;/p&gt;
    &lt;p&gt;To receive up-to-date information on BrowserPod, make sure to register on our website. We plan to extend an early adopter program to a selected subset of registered users and organizations.&lt;/p&gt;
    &lt;p&gt;For more information on all our products and technologies, please join our Discord. Most members of the development team are active there and ready to answer any question you might have. You can also follow us on X and LinkedIn for updates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;BrowserPod is currently in the final stages of development, and we are thrilled to see what the community will build on top of this technology when it is released in late November.&lt;/p&gt;
    &lt;p&gt;Leaning Technologies mission statement is “Run anything on the browser”, and BrowserPod is an important milestone along this journey. It will also not be the last and we have great ambitions for our ecosystem as we migrate to the unified CheerpOS foundational layer. Stay tuned!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://labs.leaningtech.com/blog/browserpod-annoucement"/><published>2025-09-30T14:42:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426490</id><title>Kagi News</title><updated>2025-09-30T17:37:11.560940+00:00</updated><content>&lt;doc fingerprint="b500b3a787eb238f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Kagi News&lt;/head&gt;
    &lt;p&gt;A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.&lt;/p&gt;
    &lt;p&gt;News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be. We can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our approach: Signal over noise&lt;/head&gt;
    &lt;p&gt;Kagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then distill this massive information into one comprehensive daily briefing, while clearly citing sources.&lt;/p&gt;
    &lt;p&gt;We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design principles that put readers first&lt;/head&gt;
    &lt;p&gt;One daily update: We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.&lt;/p&gt;
    &lt;p&gt;Five-minute complete understanding: Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.&lt;/p&gt;
    &lt;p&gt;Diversity over echo chambers: Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.&lt;/p&gt;
    &lt;p&gt;Privacy by design: Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.&lt;/p&gt;
    &lt;p&gt;Community-driven sources: Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.&lt;/p&gt;
    &lt;p&gt;Customizable: In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.&lt;/p&gt;
    &lt;p&gt;News in your language: You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical implementation that respects publishers&lt;/head&gt;
    &lt;p&gt;We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to experience news differently?&lt;/head&gt;
    &lt;p&gt;If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.kagi.com/kagi-news"/><published>2025-09-30T15:09:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45426673</id><title>AI will happily design the wrong thing for you</title><updated>2025-09-30T17:37:11.462523+00:00</updated><content>&lt;doc fingerprint="21de58151a65acb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI will happily design the wrong thing for you&lt;/head&gt;
    &lt;p&gt;I need to clear something up about my book, “Products People Actually Want.” When I write about how “anyone can build anything” now, some people assume I’m anti-AI. That I think these tools are ruining design or product development.&lt;/p&gt;
    &lt;p&gt;That’s not it at all.&lt;/p&gt;
    &lt;p&gt;AI tools are incredible leverage. They make me think faster, broader, and help me produce better work. But like any creative partner or colleague, this doesn’t happen out of the gate. I’ve spent hours—days—training my AI on how we write at Summer Health so the tone is right. I’ve taught it our business model. I tweak its suggestions to better match the experience we want to build.&lt;/p&gt;
    &lt;p&gt;The problem isn’t that AI exists. The problem is how most people use it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The real issue with “anyone can build anything”&lt;/head&gt;
    &lt;p&gt;My book focuses on a specific problem: people building things without knowing if anyone actually needs them. When the barrier to building drops to zero, we get flooded with products that work fine but solve problems that don’t exist.&lt;/p&gt;
    &lt;p&gt;But there’s a second issue too. While AI can help you get started quickly, it won’t build something ready for mass market. Users’ expectations for polish and detail are much higher than what AI produces by default. You can spot AI-generated work from a mile away because it lacks the intentional decisions that make products feel right.&lt;/p&gt;
    &lt;p&gt;The combination is brutal: people building the wrong things, and building them poorly.&lt;/p&gt;
    &lt;p&gt;Just last week, we saw a perfect example. Food influencer Molly Baz discovered that Shopify was selling a website template featuring what she called “a sicko AI version of me.” The image—a woman in a red sweatshirt eating an onion ring in a butter-yellow kitchen—looked almost identical to her cookbook cover.&lt;/p&gt;
    &lt;p&gt;This isn’t really an AI problem, it’s a laziness problem that AI makes more tempting. What used to be someone using an image without permission now gets dressed up as “AI-generated content.” The ironic part? Creating something unique with AI tools is actually easier than trying to replicate someone else’s work. The tools are there. The capability is there. The only thing missing is the intention to actually create something new.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I actually use AI&lt;/head&gt;
    &lt;p&gt;I use AI tools extensively, but strategically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Granola for transcribing user research sessions&lt;/item&gt;
      &lt;item&gt;Visual Electric for images that are impossible to find in stock libraries (like families that aren’t white middle class)&lt;/item&gt;
      &lt;item&gt;ChatGPT as a sparring partner and for writing better copy&lt;/item&gt;
      &lt;item&gt;Cursor for building websites and prototypes&lt;/item&gt;
      &lt;item&gt;A custom copywriter agent trained on our brand voice&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But here’s what I don’t do: I don’t use AI to replace thinking. I saw a tool recently that listens to user interviews and suggests follow-up questions. Tools like this make designers lazier, not better. You need genuine curiosity. You need to understand what you’re looking for before you can synthesize anything meaningful.&lt;/p&gt;
    &lt;p&gt;What AI is great at is helping you process large sets of information and pull out themes. But if you don’t understand your users and what they’re struggling with first, it’s impossible to prompt any AI tool for a real solution.&lt;/p&gt;
    &lt;p&gt;Here’s the thing about AI tools: if you don’t know what your customers want, if you as a designer don’t have a view on how to package it, AI will happily make all of that up for you. That just doesn’t mean it’s the right thing. AI fills in the blanks confidently, but those blanks are exactly where the real design work should happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;The skills that actually matter&lt;/head&gt;
    &lt;p&gt;The divide for engineers is easier to see: tools can help them code much faster, but it’s worthless if they can’t understand the generated code.&lt;/p&gt;
    &lt;p&gt;For designers, the key skill going forward will be taste and curation. Understanding what you want to prompt before diving in. Knowing good work from generic work. Being able to refine and iterate until something feels intentional rather than automated.&lt;/p&gt;
    &lt;p&gt;I think designers who resist AI entirely will find themselves without jobs in the next five years. When you can use AI to speed up tedious tasks, what’s the reason for doing them manually?&lt;/p&gt;
    &lt;p&gt;But designers who treat AI like a magic button will struggle too. The ones who thrive will use AI as leverage—to think better, work faster, and explore more possibilities than they could alone.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI amplifies everything&lt;/head&gt;
    &lt;p&gt;Here’s how I see it: AI is leverage. It amplifies whatever you bring to it.&lt;/p&gt;
    &lt;p&gt;If you understand your users deeply, AI helps you explore more solutions. If you have good taste, AI helps you iterate faster. If you can communicate clearly, AI helps you refine that communication.&lt;/p&gt;
    &lt;p&gt;But if you don’t understand the problem you’re solving, AI just helps you build the wrong thing more efficiently. If you have poor judgment, AI amplifies that too.&lt;/p&gt;
    &lt;p&gt;The future belongs to people who combine human insight with AI capability. Not people who think they can skip the human part.&lt;/p&gt;
    &lt;p&gt;My book isn’t the antidote to AI. It’s about developing the judgment to use any tool—AI included—in service of building things people actually want. The better you understand users and business fundamentals, the better your AI-assisted work becomes.&lt;/p&gt;
    &lt;p&gt;AI didn’t create the problem of people building useless products. It just made it easier to build more of them, faster. The solution isn’t to avoid the tools. It’s to get better at the human parts of the job that the tools can’t do for you.&lt;/p&gt;
    &lt;p&gt;Yet.&lt;/p&gt;
    &lt;p&gt;My book Products People Actually Want is out now.&lt;/p&gt;
    &lt;head rend="h2"&gt;Did you enjoy this article?&lt;/head&gt;
    &lt;p&gt;Join 3,000+ designers, developers, and product people who get my best ideas about design each month.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.antonsten.com/articles/ai-will-happily-design-the-wrong-thing-for-you/"/><published>2025-09-30T15:20:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427021</id><title>Correctness and composability bugs in the Julia ecosystem (2022)</title><updated>2025-09-30T17:37:11.382350+00:00</updated><content>&lt;doc fingerprint="3cc1939b22f38bfe"&gt;
  &lt;main&gt;&lt;p&gt;For many years I used the Julia programming language for transforming, cleaning, analyzing, and visualizing data, doing statistics, and performing simulations.&lt;/p&gt;&lt;p&gt;I published a handful of open-source packages for things like signed distance fields, nearest-neighbor search, and Turing patterns (among others), made visual explanations of Julia concepts like broadcasting and arrays, and used Julia to make the generative art on my business cards.&lt;/p&gt;&lt;p&gt;I stopped using Julia a while ago, but it still sometimes comes up. When people ask, I tell them that I can no longer recommend it. I thought I’d write up my reasons why.&lt;/p&gt;&lt;p&gt;My conclusion after using Julia for many years is that there are too many correctness and composability bugs throughout the ecosystem to justify using it in just about any context where correctness matters.&lt;/p&gt;&lt;p&gt;In my experience, Julia and its packages have the highest rate of serious correctness bugs of any programming system I’ve used, and I started programming with Visual Basic 6 in the mid-2000s.&lt;/p&gt;&lt;p&gt;It might be useful to give some concrete examples.&lt;/p&gt;&lt;p&gt;Here are some correctness issues I filed:&lt;/p&gt;&lt;p&gt;Here are comparable issues filed by others:&lt;/p&gt;&lt;code&gt;copyto!&lt;/code&gt; methods don’t check for aliasing&lt;p&gt;I would hit bugs of this severity often enough to make me question the correctness of any moderately complex computation in Julia.&lt;/p&gt;&lt;p&gt;This was particularly true when trying a novel combination of packages or functions — composing together functionality from multiple sources was a significant source of bugs.&lt;/p&gt;&lt;p&gt;Sometimes the problems would be with packages that don’t compose together, and other times an unexpected combination of Julia’s features within a single package would unexpectedly fail.&lt;/p&gt;&lt;p&gt;For example, I found that the Euclidean distance from the Distances package does not work with Unitful vectors. Others discovered that Julia’s function to run external commands doesn’t work with substrings. Still others found that Julia’s support for missing values breaks matrix multiplication in some cases. And that the standard library’s &lt;code&gt;@distributed&lt;/code&gt; macro didn’t work with OffsetArrays.&lt;/p&gt;&lt;p&gt;OffsetArrays in particular proved to be a strong source of correctness bugs. The package provides an array type that leverages Julia’s flexible custom indices feature to create arrays whose indices don’t have to start at zero or one.&lt;/p&gt;&lt;p&gt;Using them would often result in out-of-bounds memory accesses, just like those one might encounter in C or C++. This would lead to segfaults if you were lucky, or, if you weren’t, to results that were quietly wrong. I once found a bug in core Julia that could lead to out-of-bounds memory accesses even when both the user and library authors wrote correct code.&lt;/p&gt;&lt;p&gt;I filed a number of indexing-related issues with the JuliaStats organization, which stewards statistics packages like Distributions, which 945 packages depend on, and StatsBase, which 1,660 packages depend on. Here are some of them:&lt;/p&gt;&lt;p&gt;The majority of sampling methods are unsafe and incorrect in the presence of offset axes&lt;/p&gt;&lt;p&gt;Fitting a DiscreteUniform distribution can silently return an incorrect answer&lt;/p&gt;&lt;p&gt;Incorrect uses of @inbounds cause miscalculation of statistics&lt;/p&gt;&lt;p&gt;Showing a Weights vector wrapping an offset array accesses out-of-bounds memory&lt;/p&gt;&lt;p&gt;The root cause behind these issues was not the indexing alone but its use together with another Julia feature, &lt;code&gt;@inbounds&lt;/code&gt;, which permits Julia to remove bounds checks from array accesses.&lt;/p&gt;&lt;p&gt;For example:&lt;/p&gt;&lt;code&gt;function sum(A::AbstractArray)
    r = zero(eltype(A))
    for i in 1:length(A)
        @inbounds r += A[i] # ← 🌶
    end
    return r
end
&lt;/code&gt;

&lt;p&gt;The code above iterates &lt;code&gt;i&lt;/code&gt; from 1 to the length of the array. If you pass it an array with an unusual index range, it will access out-of-bounds memory: the array access was annotated with &lt;code&gt;@inbounds&lt;/code&gt;, which removed the bounds check.&lt;/p&gt;&lt;p&gt;The example above shows how to use &lt;code&gt;@inbounds&lt;/code&gt; incorrectly. However, for years it was also the official example of how to use &lt;code&gt;@inbounds&lt;/code&gt; correctly. The example was situated directly above a warning explaining why it was incorrect:&lt;/p&gt;&lt;p&gt;That issue is now fixed, but it is worrying that &lt;code&gt;@inbounds&lt;/code&gt; can be so easily misused, causing silent data corruption and incorrect mathematical results.&lt;/p&gt;&lt;p&gt;In my experience, issues like these were not confined to the mathematical parts of the Julia ecosystem.&lt;/p&gt;&lt;p&gt;I encountered library bugs while trying to accomplish mundane tasks like encoding JSON, issuing HTTP requests, using Arrow files together with DataFrames, and editing Julia code with Pluto, Julia’s reactive notebook environment.&lt;/p&gt;&lt;p&gt;When I became curious if my experience was representative, a number of Julia users privately shared similar stories. Recently, public accounts of comparable experiences have begun to surface.&lt;/p&gt;&lt;p&gt;For example, in this post Patrick Kidger describes his attempt to use Julia for machine learning research:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;It’s pretty common to see posts on the Julia Discourse saying “XYZ library doesn’t work”, followed by a reply from one of the library maintainers stating something like “This is an upstream bug in the new version a.b.c of the ABC library, which XYZ depends upon. We’ll get a fix pushed ASAP.”&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Here’s Patrick’s experience tracking down a correctness bug (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I remember all too un-fondly a time in which one of my Julia models was failing to train. I spent multiple months on-and-off trying to get it working, trying every trick I could think of.&lt;/p&gt;&lt;p&gt;Eventually – eventually! – I found the error: Julia/Flux/Zygote was returning incorrect gradients. After having spent so much energy wrestling with points 1 and 2 above, this was the point where I simply gave up. Two hours of development work later, I had the model successfully training… in PyTorch.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In a discussion about the post others responded that they, too, had similar experiences.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Like @patrick-kidger, I have been bit by incorrect gradient bugs in Zygote/ReverseDiff.jl. This cost me weeks of my life and has thoroughly shaken my confidence in the entire Julia AD landscape. [...] In all my years of working with PyTorch/TF/JAX I have not once encountered an incorrect gradient bug.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;Since I started working with Julia, I’ve had two bugs with Zygote which have slowed my work by several months. On a positive note, this has forced me to plunge into the code and learn a lot about the libraries I’m using. But I’m finding myself in a situation where this is becoming too much, and I need to spend a lot of time debugging code instead of doing climate research.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Given Julia’s extreme generality it is not obvious to me that the correctness problems can be solved. Julia has no formal notion of interfaces, generic functions tend to leave their semantics unspecified in edge cases, and the nature of many common implicit interfaces has not been made precise (for example, there is no agreement in the Julia community on what a number is).&lt;/p&gt;&lt;p&gt;The Julia community is full of capable and talented people who are generous with their time, work, and expertise. But systemic problems like this can rarely be solved from the bottom up, and my sense is that the project leadership does not agree that there is a serious correctness problem. They accept the existence of individual isolated issues, but not the pattern that those issues imply.&lt;/p&gt;&lt;p&gt;At a time when Julia’s machine learning ecosystem was even less mature, for example, a co-founder of the language spoke enthusiastically about using Julia in production for self-driving cars:&lt;/p&gt;&lt;p&gt;And while it’s possible that attitudes have shifted since I was an active member, the following quote from another co-founder, also made around the same time, serves as a good illustration of the perception gap (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I think the top-level take away here is not that Julia is a great language (although it is) and that they should use it for all the things (although that’s not the worst idea), but that its design has hit on something that has made a major step forwards in terms of our ability to achieve code reuse. It is actually the case in Julia that you can take generic algorithms that were written by one person and custom types that were written by other people and just use them together efficiently and effectively. This majorly raises the table stakes for code reuse in programming languages. Language designers should not copy all the features of Julia, but they should at the very least understand why this works so well, and be able to accomplish this level of code reuse in future designs.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Whenever a post critiquing Julia makes the rounds, people from the community are often quick to respond that, while there have historically been some legitimate issues, things have improved substantially and most of the issues are now fixed.&lt;/p&gt;&lt;p&gt;For example:&lt;/p&gt;&lt;p&gt;These responses often look reasonable in their narrow contexts, but the net effect is that people’s legitimate experiences feel diminished or downplayed, and the deeper issues go unacknowledged and unaddressed.&lt;/p&gt;&lt;p&gt;My experience with the language and community over the past ten years strongly suggests that, at least in terms of basic correctness, Julia is not currently reliable or on the path to becoming reliable. For the majority of use cases the Julia team wants to service, the risks are simply not worth the rewards.&lt;/p&gt;&lt;p&gt;Ten years ago, Julia was introduced to the world with an inspiring and ambitious set of goals. I still believe that they can, one day, be achieved—but not without revisiting and revising the patterns that brought the project to the state it is in today.&lt;/p&gt;&lt;p&gt;Thanks to Mitha Nandagopalan, Ben Cartwright-Cox, Imran Qureshi, Dan Luu, Elad Bogomolny, Zora Killpack, Ben Kuhn, and Yuriy Rusko for discussions and comments on earlier drafts of this post.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yuri.is/not-julia/"/><published>2025-09-30T15:46:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427059</id><title>Visualizations of Random Attractors Found Using Lyapunov Exponents</title><updated>2025-09-30T17:37:11.164977+00:00</updated><content>&lt;doc fingerprint="2992588605812842"&gt;
  &lt;main&gt;
    &lt;cell&gt;&lt;head rend="h1"&gt; Random Attractors&lt;lb/&gt; Found using Lyapunov Exponents &lt;/head&gt; Written by Paul Bourke&lt;lb/&gt; October 2001&lt;p&gt; Contribution by Philip Ham: attractor.basic&lt;lb/&gt; and Python implementation by Johan Bichel Lindegaard.&lt;/p&gt;&lt;p&gt; This document is "littered" with a selection of attractors found using the techniques described. &lt;/p&gt;&lt;p&gt; In order for a system to exhibit chaotic behaviour it must be non linear. Representing chaotic systems on a screen or on paper leads one to considering a two dimensional system, an equation in two variables. One possible two dimensional non-linear system, the one used here, is the quadratic map defined as follows. &lt;/p&gt; xn+1 = a0 + a1 xn + a2 xn2 + a3 xn yn + a4 yn + a5 yn2 &lt;lb/&gt; yn+1 = b0 + b1 xn + b2 xn2 + b3 xn yn + b4 yn + b5 yn2 &lt;p&gt; The standard measure for determining whether or not a system is chaotic is the Lyapunov exponent, normally represented by the lambda symbol. Consider two close points at step n, xn and xn+dxn. At the next time step they will have diverged, namely to xn+1 and xn+1+dxn+1. It is this average rate of divergence (or convergence) that the Lyapunov exponent captures. Another way to think about the Lyapunov exponent is as the rate at which information about the initial conditions is lost. &lt;/p&gt;&lt;p&gt; There are as many Lyapunov exponents as dimensions of the phase space. Considering a region (circle, sphere, hypersphere, etc) in phase space then at a later time all trajectories in this region form an n-dimensional elliptical region. The Lyapunov exponent can be calculated for each dimension. When talking about a single exponent one is normally referring to the largest, this convention will be assumed from now onwards. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is positive then the system is chaotic and unstable. Nearby points will diverge irrespective of how close they are. Although there is no order the system is still deterministic! The magnitude of the Lyapunov exponent is a measure of the sensitivity to initial conditions, the primary characteristic of a chaotic system. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is less than zero then the system attracts to a fixed point or stable periodic orbit. These systems are non conservative (dissipative). The absolute value of the exponent indicates the degree of stability. &lt;/p&gt;&lt;p&gt; If the Lyapunov exponent is zero then the system is neutrally stable, such systems are conservative and in a steady state mode. &lt;/p&gt;&lt;p&gt; To create the chaotic attractors shown on this page each parameter an and bn in the quadratic equation above is chosen at random between some bounds (+- 2 say). The system so specified is generated by iterating for some suitably large number of time steps (eg; 100000) steps during which time the image is created and the Lyapunov exponent computed. Note that the first few (1000) timesteps are ignored to allow the system to settle into its "natural" behaviour. If the Lyapunov exponent indicates chaos then the image is saved and the program moves on to the next random parameter set. &lt;/p&gt;&lt;p&gt; There are a number of ways the series may behave. &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt; It may converge to a single point, called a fixed point. These can be detected by comparing the distances between successive points. For numerical reasons this is safer than relying on the Lyapunov exponent which may be infinite (logarithm of 0)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It may diverge to infinity, for the range (+- 2) used here for each parameter this is the most likely event. These are also easy to detect and discard, indeed they need to be in order to avoid numerical errors.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will form a periodic orbit, these are identified by their negative Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt; It will exhibit chaos, filling in some region of the plane. These are the solutions that "look good" and the ones we wish to identify with the Lyapunov exponent. &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It should be noted that there may be visually appealing structures that are not chaotic attractors. That is, the resulting image is different for different initial conditions and there is no single basin of attraction. It's interesting how we "see" 3 dimensional structures in these essentially 2 dimensional systems. &lt;/p&gt;&lt;p&gt; The software used to create these images is given here: gen.c. On average 98% of the random selections of (an, bn) result in an infinite series. This is so common because of the range (-2 &amp;lt;= a,b &amp;lt;= 2) for each of the parameters a and b, the number of infinite cases will reduce greatly with a smaller range. About 1% were point attractors, and about 0.5% were periodic basins of attraction. &lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt; Image courtesy of Robert McGregor, Space Coast of Florida. Launch trail perhaps 30 minutes after the shuttle launch (June 2007) dispersing from a column into a smoke ring due to some unusual air currents in the upper atmosphere. &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; References&lt;/p&gt;&lt;p&gt; Berge, P., Pomeau, Y., Vidal, C.,&lt;lb/&gt; Order Within Chaos, Wiley, New York, 1984. &lt;/p&gt;&lt;p&gt; Crutchfield, J., Farmer, J., Packard, N.&lt;lb/&gt; Chaos, Scientific American, 1986, 255, 46-47 &lt;/p&gt;&lt;p&gt; Das, A., Das, Pritha, Roy, A&lt;lb/&gt; Applicability of Lyapunov Exponent in EEG data analysis. Complexity International, draft manuscript. &lt;/p&gt;&lt;p&gt; Devaney, R.&lt;lb/&gt; An Introduction to Chaotic Dynamical Systems, Addison-Wesley, 1989 &lt;/p&gt;&lt;p&gt; Feigenbaum, M.,&lt;lb/&gt; Universal behaviour in Nonlinear Systems, Los Alamos Science, 1981 &lt;/p&gt;&lt;p&gt; Peitgen, H., Jurgens, H., Saupe, D&lt;lb/&gt; Lyapunov exponents and chaotic attractors in Chaos and fractals - new frontiers of science. Springer, new York, 1992. &lt;/p&gt;&lt;p&gt; Contributions by Dmytry Lavrov &lt;/p&gt;&lt;/cell&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://paulbourke.net/fractals/lyapunov/"/><published>2025-09-30T15:50:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427197</id><title>Leaked Apple M5 9 core Geekbench scores</title><updated>2025-09-30T17:37:10.965549+00:00</updated><content>&lt;doc fingerprint="4fa0627d6e0cbd81"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Upload Date&lt;/cell&gt;
        &lt;cell&gt;September 30 2025 12:36 PM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Views&lt;/cell&gt;
        &lt;cell&gt;4803&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;System Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Operating System&lt;/cell&gt;
        &lt;cell&gt;iOS 26.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;iPad17,3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Model ID&lt;/cell&gt;
        &lt;cell&gt;iPad17,3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Motherboard&lt;/cell&gt;
        &lt;cell&gt;J820AP&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;CPU Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Name&lt;/cell&gt;
        &lt;cell&gt;ARM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Topology&lt;/cell&gt;
        &lt;cell&gt;1 Processor, 9 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Identifier&lt;/cell&gt;
        &lt;cell&gt;ARM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Base Frequency&lt;/cell&gt;
        &lt;cell&gt;4.42 GHz&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cluster 1&lt;/cell&gt;
        &lt;cell&gt;3 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cluster 2&lt;/cell&gt;
        &lt;cell&gt;6 Cores&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L1 Instruction Cache&lt;/cell&gt;
        &lt;cell&gt;128 KB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L1 Data Cache&lt;/cell&gt;
        &lt;cell&gt;64.0 KB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L2 Cache&lt;/cell&gt;
        &lt;cell&gt;6.00 MB x 1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Instruction Sets&lt;/cell&gt;
        &lt;cell&gt;neon aes sha1 sha2 neon-fp16 neon-dotprod i8mm sme-i8i32 sme-f32f32 sme2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Memory Information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Size&lt;/cell&gt;
        &lt;cell&gt;11.20 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Single-Core Score&lt;/cell&gt;
        &lt;cell role="head"&gt;4133&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; File Compression &lt;/cell&gt;
        &lt;cell&gt; 3552 &lt;p&gt;510.1 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Navigation &lt;/cell&gt;
        &lt;cell&gt; 3659 &lt;p&gt;22.0 routes/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HTML5 Browser &lt;/cell&gt;
        &lt;cell&gt; 4260 &lt;p&gt;87.2 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; PDF Renderer &lt;/cell&gt;
        &lt;cell&gt; 3734 &lt;p&gt;86.1 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Library &lt;/cell&gt;
        &lt;cell&gt; 3719 &lt;p&gt;50.5 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Clang &lt;/cell&gt;
        &lt;cell&gt; 4649 &lt;p&gt;22.9 Klines/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Text Processing &lt;/cell&gt;
        &lt;cell&gt; 3822 &lt;p&gt;306.1 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Asset Compression &lt;/cell&gt;
        &lt;cell&gt; 3547 &lt;p&gt;109.9 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Detection &lt;/cell&gt;
        &lt;cell&gt; 6032 &lt;p&gt;180.5 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Background Blur &lt;/cell&gt;
        &lt;cell&gt; 4104 &lt;p&gt;17.0 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Horizon Detection &lt;/cell&gt;
        &lt;cell&gt; 4139 &lt;p&gt;128.8 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Remover &lt;/cell&gt;
        &lt;cell&gt; 5276 &lt;p&gt;405.6 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HDR &lt;/cell&gt;
        &lt;cell&gt; 4678 &lt;p&gt;137.3 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Filter &lt;/cell&gt;
        &lt;cell&gt; 5061 &lt;p&gt;50.2 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Ray Tracer &lt;/cell&gt;
        &lt;cell&gt; 3302 &lt;p&gt;3.20 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Structure from Motion &lt;/cell&gt;
        &lt;cell&gt; 3836 &lt;p&gt;121.4 Kpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Multi-Core Score&lt;/cell&gt;
        &lt;cell role="head"&gt;15437&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; File Compression &lt;/cell&gt;
        &lt;cell&gt; 12308 &lt;p&gt;1.73 GB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Navigation &lt;/cell&gt;
        &lt;cell&gt; 17065 &lt;p&gt;102.8 routes/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HTML5 Browser &lt;/cell&gt;
        &lt;cell&gt; 17958 &lt;p&gt;367.6 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; PDF Renderer &lt;/cell&gt;
        &lt;cell&gt; 15774 &lt;p&gt;363.8 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Library &lt;/cell&gt;
        &lt;cell&gt; 18268 &lt;p&gt;247.9 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Clang &lt;/cell&gt;
        &lt;cell&gt; 23236 &lt;p&gt;114.4 Klines/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Text Processing &lt;/cell&gt;
        &lt;cell&gt; 4956 &lt;p&gt;396.9 pages/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Asset Compression &lt;/cell&gt;
        &lt;cell&gt; 18577 &lt;p&gt;575.6 MB/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Detection &lt;/cell&gt;
        &lt;cell&gt; 14896 &lt;p&gt;445.7 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Background Blur &lt;/cell&gt;
        &lt;cell&gt; 13114 &lt;p&gt;54.3 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Horizon Detection &lt;/cell&gt;
        &lt;cell&gt; 19111 &lt;p&gt;594.7 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Object Remover &lt;/cell&gt;
        &lt;cell&gt; 15968 &lt;p&gt;1.23 Gpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; HDR &lt;/cell&gt;
        &lt;cell&gt; 18909 &lt;p&gt;554.9 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Photo Filter &lt;/cell&gt;
        &lt;cell&gt; 15246 &lt;p&gt;151.3 images/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt; Ray Tracer &lt;/cell&gt;
        &lt;cell&gt; 18888 &lt;p&gt;18.3 Mpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; Structure from Motion &lt;/cell&gt;
        &lt;cell&gt; 16186 &lt;p&gt;512.5 Kpixels/sec&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://browser.geekbench.com/v6/cpu/14173685"/><published>2025-09-30T16:00:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427482</id><title>Launch HN: Airweave (YC X25) – Let agents search any app</title><updated>2025-09-30T17:37:10.494977+00:00</updated><content>&lt;doc fingerprint="5a85ff58db83ac82"&gt;
  &lt;main&gt;
    &lt;p&gt;Airweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.&lt;/p&gt;
    &lt;p&gt;The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managed Service: Airweave Cloud&lt;/head&gt;
    &lt;p&gt;Make sure docker and docker-compose are installed, then...&lt;/p&gt;
    &lt;code&gt;# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh&lt;/code&gt;
    &lt;p&gt;That's it! Access the dashboard at http://localhost:8080&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access the UI at &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Connect sources, configure syncs, and query data&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swagger docs: &lt;code&gt;http://localhost:8001/docs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create connections, trigger syncs, and search data&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install airweave-sdk&lt;/code&gt;
    &lt;code&gt;from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key="YOUR_API_KEY",
    base_url="http://localhost:8001"
)
client.collections.create(
    name="name",
)&lt;/code&gt;
    &lt;code&gt;npm install @airweave/sdk
# or
yarn add @airweave/sdk&lt;/code&gt;
    &lt;code&gt;import { AirweaveSDKClient, AirweaveSDKEnvironment } from "@airweave/sdk";

const client = new AirweaveSDKClient({
    apiKey: "YOUR_API_KEY",
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: "name",
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data synchronization from 25+ sources with minimal config&lt;/item&gt;
      &lt;item&gt;Entity extraction and transformation pipeline&lt;/item&gt;
      &lt;item&gt;Multi-tenant architecture with OAuth2&lt;/item&gt;
      &lt;item&gt;Incremental updates using content hashing&lt;/item&gt;
      &lt;item&gt;Semantic search for agent queries&lt;/item&gt;
      &lt;item&gt;Versioning for data changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React/TypeScript with ShadCN&lt;/item&gt;
      &lt;item&gt;Backend: FastAPI (Python)&lt;/item&gt;
      &lt;item&gt;Databases: PostgreSQL (metadata), Qdrant (vectors)&lt;/item&gt;
      &lt;item&gt;Deployment: Docker Compose (dev), Kubernetes (prod)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please check CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;p&gt;Airweave is released under the MIT license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord - Get help and discuss features&lt;/item&gt;
      &lt;item&gt;GitHub Issues - Report bugs or request features&lt;/item&gt;
      &lt;item&gt;Twitter - Follow for updates&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/airweave-ai/airweave"/><published>2025-09-30T16:21:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427634</id><title>A $196 fine-tuned 7B model outperforms OpenAI o3 on document extraction</title><updated>2025-09-30T17:37:10.255762+00:00</updated><content>&lt;doc fingerprint="a91c3c0ed5fb008e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 26 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Extract-0: A Specialized Language Model for Document Information Extraction&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:This paper presents Extract-0, a 7-billion parameter language model specifically optimized for document information extraction that achieves performance exceeding models with parameter counts several orders of magnitude larger. Through a novel combination of synthetic data generation, supervised fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of 0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology employs a memory-preserving synthetic data generation pipeline that produces 280,128 training examples from diverse document sources, followed by parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M out of 7.66B parameters). The reinforcement learning phase introduces a novel semantic similarity-based reward function that handles the inherent ambiguity in information extraction tasks. This research demonstrates that task-specific optimization can yield models that surpass general-purpose systems while requiring substantially fewer computational resource.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.22906"/><published>2025-09-30T16:31:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427697</id><title>Show HN: Sculptor, the Missing UI for Claude Code</title><updated>2025-09-30T17:37:10.091561+00:00</updated><content>&lt;doc fingerprint="b5fc0231a98fb58b"&gt;
  &lt;main&gt;&lt;p&gt;Start with your ideas and iteratively refine them, just like traditional software engineering. You're the architect while AI agents handle the implementation details.&lt;/p&gt;Get the docs&lt;p&gt;Each Claude works in its own container. You get safe execution and parallel agents without the hassle of git worktrees.&lt;/p&gt;&lt;p&gt;Run, test, and edit agent changes in your dev environment. Switch between agents with Pairing Mode.&lt;/p&gt;&lt;p&gt;Merge the changes you like and throw out the ones you don't. Sculptor helps you resolve merge conflicts.&lt;/p&gt;&lt;p&gt;I've been moving more and more of my coding off of Cursor and on to Sculptor btw.&lt;/p&gt;&lt;p&gt;The vibes are good, and the experience has been pretty nice.&lt;/p&gt;&lt;p&gt;Sculptor lets me maintain this level of craftiness to software development without losing the edge you get from AI tools.&lt;/p&gt;&lt;p&gt;Wow, this is slick!!&lt;/p&gt;&lt;p&gt;At first I thought, 'why do I need this container?'. But when I realized Sculptor was actually solving the pain of concurrent agents on different branches, it made total sense.&lt;/p&gt;&lt;p&gt;It's like—oh wow I don't have to manage that mess anymore.&lt;/p&gt;&lt;p&gt;The killer feature for me is parallelization. I can kick off multiple tasks at once without spinning up a whole new environment every time. It feels like the tooling is finally here to support the kind of workflows I've always wanted.&lt;/p&gt;&lt;p&gt;I compared Claude Code running in Max Mode with Sculptor, and Sculptor's results and overall intelligence were better. I've already merged around 5K lines—it's a great product! Kudos to you guys.&lt;/p&gt;&lt;p&gt;Hop into our Discord to share feedback, report bugs, and chat directly with the Imbue team. You'll find a community of developers exploring workflows, sharing ideas, and uncovering what coding agents can really do.&lt;/p&gt;Join our Discord&lt;p&gt;Dale used Sculptor to build a foreign-language journaling app. While Sculptor handled the refactors, fixed build issues, and churned through background tasks, he spent his time painting the landing page. In his words:&lt;/p&gt;&lt;p&gt;“In a world where generative art is booming, I used Sculptor to write code so that I could go make art.”&lt;/p&gt;&lt;p&gt;Jitendra created a tool that analyzes Spotify profiles and generates personalized playlists using the Eleven Labs music API. What began as a fuzzy idea quickly became a working prototype—Sculptor's parallel agents made it easy to explore and bring the project to life.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://imbue.com/sculptor/"/><published>2025-09-30T16:35:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427809</id><title>The Dismantling of the Forest Service</title><updated>2025-09-30T17:37:09.833369+00:00</updated><content>&lt;doc fingerprint="e600943f7db9ab8f"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to the Landline, a monthly newsletter from High Country News about land, water, wildlife, climate and conservation in the Western United States. Sign up to get it in your inbox.&lt;/p&gt;
    &lt;p&gt;In the 1880s, giant cattle companies turned thousands of cattle out to graze on the “public domain” — i.e., the Western lands that had been stolen from Indigenous people and then opened up for white settlement. In remote southeastern Utah, this coincided with a wave of settlement by members of the Church of Jesus Christ of Latter-day Saints. The region’s once-abundant grasslands and lush mountain slopes were soon reduced to denuded wastelands etched with deep flash-flood-prone gullies. Cattlemen fought, sometimes violently, over water and range.&lt;/p&gt;
    &lt;p&gt;The local citizenry grew sick and tired of it, sometimes literally: At one point, sheep feces contaminated the water supply of the town of Monticello and led to a typhoid outbreak that killed 11 people. Yet there was little they could do, since there were few rules on the public domain and fewer folks with the power to enforce them.&lt;/p&gt;
    &lt;p&gt;That changed in 1891, when Congress passed the Forest Reserve Act, which authorized the president to place some unregulated tracts under “judicious control,” thereby mildly restraining extractive activities in the name of conservation. In 1905, the Forest Service was created as a branch of the U.S. Agriculture Department to oversee these reserves, and Gifford Pinchot was chosen to lead it. And a year later, the citizens of southeastern Utah successfully petitioned the Theodore Roosevelt administration to establish forest reserves in the La Sal and Abajo Mountains.&lt;/p&gt;
    &lt;p&gt;Since then, the Forest Service has gone through various metamorphoses, shifting from stewarding and conserving forests for the future to supplying the growing nation with lumber to managing forests for multiple uses and then to the ecosystem management era, which began in the 1990s. Throughout all these shifts, however, it has largely stayed true to Pinchot and his desire to conserve forests and their resources for future generations.&lt;/p&gt;
    &lt;p&gt;But now, the Trump administration is eager to begin a new era for the agency and its public lands, with a distinctively un-Pinchot-esque structure and a mission that maximizes resource production and extraction while dismantling the administrative state and its role as environmental protector. Over the last nine months, the administration has issued executive orders calling for expanded timber production and rescinding the 2001 Roadless Rule, declared “emergency” situations that enable it to bypass regulations on nearly 60% of the public’s forests, and proposed slashing the agency’s operations budget by 34%.&lt;/p&gt;
    &lt;p&gt;The most recent move, which is currently open to public comment, involves a proposal by Agriculture Secretary Brooke Rollins to radically overhaul the entire U.S. Department of Agriculture. Its stated purposes are to ensure that the agency’s “workforce aligns with financial resources and priorities,” and to consolidate functions and eliminate redundancy. This will include moving at least 2,600 of the department’s 4,600 Washington, D.C., employees to five hub locations, with only two in the West: Salt Lake City, Utah, and Fort Collins, Colorado. (The others will be in North Carolina, Missouri and Indiana.) The goal, according to Rollins’ memorandum, is to “bring the USDA closer to its customers.” The plan is reminiscent of Trump’s first-term relocation of the Bureau of Land Management’s headquarters to Grand Junction, Colorado, in 2019. That relocation resulted in a de facto agency housecleaning; many senior staffers chose to resign or move to other agencies, and only a handful of workers ended up in the Colorado office, which shared a building with oil and gas companies.&lt;/p&gt;
    &lt;p&gt;Though Rollins’ proposal is aimed at decentralizing the department, it would effectively re-centralize the Forest Service by eliminating its nine regional offices, six of which are located in the West. Each regional forester oversees dozens of national forests within their region, providing budget oversight, guiding place-specific implementation of national-level policies, and facilitating coordination among the various forests.&lt;/p&gt;
    &lt;p&gt;Rollins’ memo does not explain why the regional offices are being axed, or what will happen to the regional foresters’ positions and their functions, or how the change will affect the agency’s chain of command. When several U.S. senators asked Deputy Secretary Stephen Vaden for more specifics, he responded that “decisions pertaining to the agency’s structure and the location of specialized personnel will be made after” the public comment period ends on Sept. 30. Curiously, the administration’s forest management strategy, published in May, relies on regional offices to “work with the Washington Office to develop tailored strategies to meet their specific timber goals.” Now it’s unclear that either the regional or Washington offices will remain in existence long enough to carry this out.&lt;/p&gt;
    &lt;p&gt;The administration has been far more transparent about its desire to return the Forest Service to its timber plantation era, which ran from the 1950s through the ’80s. During that time, logging companies harvested 10 billion to 12 billion board-feet per year from federal forests, while for the last 25 years, the annual number has hovered below 3 billion board-feet. Now, Trump, via his Immediate Expansion of American Timber Production order, plans to crank up the annual cut to 4 billion board-feet by 2028. This will be accomplished — in classic Trumpian fashion — by declaring an “emergency” on national forest lands that will allow environmental protections and regulations, including the National Environmental Protection Act, Endangered Species Act and Clean Water Act, to be eased or bypassed.&lt;/p&gt;
    &lt;p&gt;In April, Rollins issued a memorandum doing just that, declaring that the threat of wildfires, insects and disease, invasive species, overgrown forests, the growing number of homes in the wildland-urban interface and more than a century of rigorous fire suppression have contributed to what is now “a full-blown wildfire and forest health crisis.”&lt;/p&gt;
    &lt;p&gt;Emergency determinations aren’t limited to Trump and friends; in 2023, the Biden administration identified almost 67 million acres of national forest lands as being under a high or very high fire risk, thus qualifying as an “emergency situation” under the Infrastructure Investment and Jobs Act. Rollins, however, vastly expanded the “emergency situation” acreage to almost 113 million acres, or 59% of all Forest Service lands. This allows the agency to use streamlined environmental reviews and “expedited” tribal consultation time frames to “carry out authorized emergency actions,” ranging from commercial harvesting of damaged trees to removing “hazardous fuels” to reconstructing existing utility lines. Meanwhile, the administration has announced plans to consolidate all federal wildfire fighting duties under the Interior Department. This would completely zero out the Forest Service’s $2.4 billion wildland fire management budget, sowing even more confusion and chaos.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consolidating all federal wildfire fighting duties under the Interior Department would completely zero out the Forest Service’s $2.4 billion wildland fire management budget.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The administration also plans to slash staff and budgets in other parts of the agency, further compromising its ability to carry out its mission. The so-called “Department of Government Efficiency,” or DOGE, fired about 3,400 Forest Service employees, or more than 10% of the agency’s total workforce, earlier this year. And the administration has proposed cutting the agency’s operations budget, which includes salaries, by 34% in fiscal year 2026, which will most likely necessitate further reductions in force. It would also cut the national forest system and capital improvement and maintenance budgets by 21% and 48% respectively.&lt;/p&gt;
    &lt;p&gt;The goal, it seems, is to cripple the agency with both direct and indirect blows. The result, if the administration succeeds, will be a diminished Forest Service that would be unrecognizable to Gifford Pinchot.&lt;/p&gt;
    &lt;p&gt;We want to hear from you!&lt;/p&gt;
    &lt;p&gt;Your news tips, comments, ideas and feedback are appreciated and often shared. Give Jonathan a ring at the Landline, 970-648-4472, or send us an email at landline@hcn.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.hcn.org/articles/trump-looks-to-dismantle-the-forest-service/"/><published>2025-09-30T16:43:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45427982</id><title>Introducing Sora 2 [video]</title><updated>2025-09-30T17:37:09.081032+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=gzneGhpXwjU"/><published>2025-09-30T16:55:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45428052</id><title>Not only am I losing my livelihood to AI – now it's stealing my em dashes too</title><updated>2025-09-30T17:37:09.013365+00:00</updated><content>&lt;doc fingerprint="6669d2d32d38ba95"&gt;
  &lt;main&gt;
    &lt;p&gt;My editor’s email started off friendly enough, but then came the hammer blow: “We need you to remove all the em dashes. People assume that means it’s written by AI.” I looked back at the piece I’d just written. There were dashes all over it—and for good bloody reason. Em dashes—often used to connect explanatory phrases, and so named because they’re the width of your average lowercase ‘m’—are probably my favourite bit of punctuation. I’ve been option + shift + hyphening them for years.&lt;/p&gt;
    &lt;p&gt;A person’s writing often reflects how their brain works, and mine (when it works) tends to work in fits and starts. My thoughts don’t arrive in perfectly rendered prose, so I don’t write them down that way. And here I was being told the humble em dash—friend to poorly paid internet hacks everywhere—was now considered a sign not of genuine intelligence, but the other sort. The artificial sort. To the extent that I have to go through and manually remove them one by one, like nits. The absolute cheek. Not only am I losing my livelihood to AI—I’m losing grammar too.&lt;/p&gt;
    &lt;p&gt;Why couldn’t machines have embraced the semicolon? No one gives a fig about those.&lt;/p&gt;
    &lt;p&gt;The reason AI is so hung up on em dashes–as you might expect–is because humans were first. Large language models were trained on vast swathes of actual writing, including mine, I suppose, with the result that em dashes got baked into algorithms from the beginning. It’s flattering, in a way. “Humans used them so often that AIs learned them as a default natural flow,” Brent Csutoras notes on Medium. “It’s like asking a bird not to chirp.”&lt;/p&gt;
    &lt;p&gt;Well, my chirping days are over. It seems I have two choices now—keep using em dashes with a sort of stubborn, curmudgeonly pride until all my clients stop exchanging money for words, or start writing incredibly long run-on sentences, like this, with commas all over the place … and maybe ellipses too; ideas connected by semicolons. Staccato flow. Full stops everywhere. Nope, that sucks too. Sounds like someone flicking between radio stations.&lt;/p&gt;
    &lt;p&gt;Oh, em dash—what will we do without you?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/lifeandstyle/2025/oct/01/artificial-intelligence-em-dashes-ai-stealing-my-livelihood"/><published>2025-09-30T16:59:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45428081</id><title>Bild AI (YC W25) Is Hiring</title><updated>2025-09-30T17:37:08.506450+00:00</updated><content>&lt;doc fingerprint="19b82e91a7888e36"&gt;
  &lt;main&gt;
    &lt;p&gt;AI that understands construction blueprints&lt;/p&gt;
    &lt;p&gt;Puneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and we’re impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.&lt;/p&gt;
    &lt;p&gt;Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai"/><published>2025-09-30T17:01:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45428122</id><title>Sora 2</title><updated>2025-09-30T17:37:08.337744+00:00</updated><content>&lt;doc fingerprint="bc5d1e5b058e8bab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sora 2 is here&lt;/head&gt;
    &lt;p&gt;Our latest video generation model is more physically accurate, realistic, and controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing Sora 2, our flagship video and audio generation model.&lt;/p&gt;
    &lt;p&gt;The original Sora model from February 2024 was in many ways the GPT‑1 moment for video—the first time video generation started to seem like it was working, and simple behaviors like object permanence emerged from scaling up pre-training compute. Since then, the Sora team has been focused on training models with more advanced world simulation capabilities. We believe such systems will be critical for training AI models that deeply understand the physical world. A major milestone for this is mastering pre-training and post-training on large-scale video data, which are in their infancy compared to language.&lt;/p&gt;
    &lt;p&gt;With Sora 2, we are jumping straight to what we think may be the GPT‑3.5 moment for video. Sora 2 can do things that are exceptionally difficult—and in some instances outright impossible—for prior video generation models: Olympic gymnastics routines, backflips on a paddleboard that accurately model the dynamics of buoyancy and rigidity, and triple axels while a cat holds on for dear life.&lt;/p&gt;
    &lt;p&gt;Prior video models are overoptimistic—they will morph objects and deform reality to successfully execute upon a text prompt. For example, if a basketball player misses a shot, the ball may spontaneously teleport to the hoop. In Sora 2, if a basketball player misses a shot, it will rebound off the backboard. Interestingly, “mistakes” the model makes frequently appear to be mistakes of the internal agent that Sora 2 is implicitly modeling; though still imperfect, it is better about obeying the laws of physics compared to prior systems. This is an extremely important capability for any useful world simulator—you must be able to model failure, not just success.&lt;/p&gt;
    &lt;p&gt;The model is also a big leap forward in controllability, able to follow intricate instructions spanning multiple shots while accurately persisting world state. It excels at realistic, cinematic, and anime styles.&lt;/p&gt;
    &lt;p&gt;As a general purpose video-audio generation system, it is capable of creating sophisticated background soundscapes, speech, and sound effects with a high degree of realism.&lt;/p&gt;
    &lt;p&gt;You can also directly inject elements of the real world into Sora 2. For example, by observing a video of one of our teammates, the model can insert them into any Sora-generated environment with an accurate portrayal of appearance and voice. This capability is very general, and works for any human, animal or object.&lt;/p&gt;
    &lt;p&gt;The model is far from perfect and makes plenty of mistakes, but it is validation that further scaling up neural networks on video data will bring us closer to simulating reality.&lt;/p&gt;
    &lt;p&gt;On the road to general-purpose simulation and AI systems that can function in the physical world, we think people can have a lot of fun with the models we’re building along the way.&lt;/p&gt;
    &lt;p&gt;We first started playing with this “upload yourself” feature several months ago on the Sora team, and we all had a blast with it. It kind of felt like a natural evolution of communication—from text messages to emojis to voice notes to this.&lt;/p&gt;
    &lt;p&gt;So today, we’re launching a new social iOS app just called “Sora,” powered by Sora 2. Inside the app, you can create, remix each other’s generations, discover new videos in a customizable Sora feed, and bring yourself or your friends in via cameos. With cameos, you can drop yourself straight into any Sora scene with remarkable fidelity after a short one-time video-and-audio recording in the app to verify your identity and capture your likeness.&lt;/p&gt;
    &lt;p&gt;Last week, we launched the app internally to all of OpenAI. We’ve already heard from our colleagues that they’re making new friends at the company because of the feature. We think a social app built around this “cameos” feature is the best way to experience the magic of Sora 2.&lt;/p&gt;
    &lt;p&gt;Concerns about doomscrolling, addiction, isolation, and RL-sloptimized feeds are top of mind—here is what we are doing about it.&lt;/p&gt;
    &lt;p&gt;We are giving users the tools and optionality to be in control of what they see on the feed. Using OpenAI's existing large language models, we have developed a new class of recommender algorithms that can be instructed through natural language. We also have built-in mechanisms to periodically poll users on their wellbeing and proactively give them the option to adjust their feed.&lt;/p&gt;
    &lt;p&gt;By default, we show you content heavily biased towards people you follow or interact with, and prioritize videos that the model thinks you’re most likely to use as inspiration for your own creations. We are not optimizing for time spent in feed, and we explicitly designed the app to maximize creation, not consumption. You can find more details in our Feed Philosophy&lt;/p&gt;
    &lt;p&gt;This app is made to be used with your friends. Overwhelming feedback from testers is that cameos are what make this feel different and fun to use—you have to try it to really get it, but it is a new and unique way to communicate with people. We’re rolling this out as an invite-based app to make sure you come in with your friends. At a time when all major platforms are moving away from the social graph, we think cameos will reinforce community.&lt;/p&gt;
    &lt;p&gt;Protecting the wellbeing of teens is important to us. We are putting in default limits on how many generations teens can see per day in the feed, and we’re also rolling out with stricter permissions on cameos for this group. In addition to our automated safety stacks, we are scaling up teams of human moderators to quickly review cases of bullying if they arise. We are launching with Sora parental controls via ChatGPT so parents can override infinite scroll limits, turn off algorithm personalization, as well as manage direct message settings.&lt;/p&gt;
    &lt;p&gt;With cameos, you are in control of your likeness end-to-end with Sora. Only you decide who can use your cameo, and you can revoke access or remove any video that includes it at any time. Videos containing cameos of you, including drafts created by other people, are viewable by you at any time.&lt;/p&gt;
    &lt;p&gt;There are a lot of safety topics we’ve tackled with this app—consent around use of likeness, provenance, preventing the generation of harmful content, and much more. See our Sora 2 Safety doc for more details.&lt;/p&gt;
    &lt;p&gt;A lot of problems with other apps stem from the monetization model incentivizing decisions that are at odds with user wellbeing. Transparently, our only current plan is to eventually give users the option to pay some amount to generate an extra video if there’s too much demand relative to available compute. As the app evolves, we will openly communicate any changes in our approach here, while continuing to keep user wellbeing as our main goal.&lt;/p&gt;
    &lt;p&gt;We’re at the beginning of this journey, but with all of the powerful ways to create and remix content with Sora 2, we see this as the beginning of a completely new era for co-creative experiences. We’re optimistic that this will be a healthier platform for entertainment and creativity compared to what is available right now. We hope you have a good time :)&lt;/p&gt;
    &lt;p&gt;The Sora iOS app(opens in a new window) is available to download now. You can sign up in-app for a push notification when access opens for your account. We’re starting the initial rollout in the U.S. and Canada today with the intent to quickly expand to additional countries. After you’ve received an invite, you’ll also be able to access Sora 2 through sora.com(opens in a new window). Sora 2 will initially be available for free, with generous limits to start so people can freely explore its capabilities, though these are still subject to compute constraints. ChatGPT Pro users will also be able to use our experimental, higher quality Sora 2 Pro model on sora.com(opens in a new window) (and soon in the Sora app as well). We also plan to release Sora 2 in the API. Sora 1 Turbo will remain available, and everything you’ve created will continue to live in your sora.com(opens in a new window) library.&lt;/p&gt;
    &lt;p&gt;Video models are getting very good, very quickly. General-purpose world simulators and robotic agents will fundamentally reshape society and accelerate the arc of human progress. Sora 2 represents significant progress towards that goal. In keeping with OpenAI’s mission, it is important that humanity benefits from these models as they are developed. We think Sora is going to bring a lot of joy, creativity and connection to the world.&lt;/p&gt;
    &lt;p&gt;— Written by the Sora Team&lt;/p&gt;
    &lt;head rend="h2"&gt;Sora 2&lt;/head&gt;
    &lt;p&gt;Debbie Mesloh&lt;/p&gt;
    &lt;p&gt;Caroline Zhao&lt;/p&gt;
    &lt;p&gt;Published September 30, MMXXV&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/sora-2/"/><published>2025-09-30T17:04:25+00:00</published></entry></feed>