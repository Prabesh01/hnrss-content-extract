<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-27T07:12:36.868371+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46057488</id><title>Voyager 1 is about to reach one light-day from Earth</title><updated>2025-11-27T07:12:47.856970+00:00</updated><content>&lt;doc fingerprint="6da2c57b3816873f"&gt;
  &lt;main&gt;
    &lt;p&gt;After nearly 50 years in space, NASA‚Äôs Voyager 1 is about to hit a historic milestone. By November 15, 2026, it will be 16.1 billion miles (25.9 billion km) away, meaning a radio signal will take a full 24 hours‚Äîa full light-day‚Äîto reach it. For context, a light-year is the distance light travels in a year, about 5.88 trillion miles (9.46 trillion km), so one light-day is just a tiny fraction of that.&lt;/p&gt;
    &lt;p&gt;Launched in 1977 to explore Jupiter and Saturn, Voyager 1 entered interstellar space in 2012, becoming the most distant human-made object ever. Traveling at around 11 miles per second (17.7 km/s), it adds roughly 3.5 astronomical units (the distance from Earth to the Sun) each year. Even after decades in the harsh environment of space, Voyager 1 keeps sending data thanks to its radioisotope thermoelectric generators, which will last into the 2030s.&lt;/p&gt;
    &lt;p&gt;Communicating with Voyager 1 is slow. Commands now take about a day to arrive, with another day for confirmation. Compare that to the Moon (1.3 seconds), Mars (up to 4 minutes), and Pluto (nearly 7 hours). The probe‚Äôs distance makes every instruction a patient exercise in deep-space operations. To reach our closest star, Proxima Centauri, even at light speed, would take over four years‚Äîshowing just how tiny a light-day is in cosmic terms.&lt;/p&gt;
    &lt;p&gt;Voyager 1‚Äôs journey is more than a record for distance. From its planetary flybys to the iconic ‚ÄòPale Blue Dot‚Äô image, it reminds us of the vast scale of the solar system and the incredible endurance of a spacecraft designed to keep exploring, even without return.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scienceclock.com/voyager-1-is-about-to-reach-one-light-day-from-earth/"/><published>2025-11-26T14:02:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46060508</id><title>Gemini CLI Tips and Tricks for Agentic Coding</title><updated>2025-11-27T07:12:47.017580+00:00</updated><content>&lt;doc fingerprint="b5d7915b01bf9a95"&gt;
  &lt;main&gt;
    &lt;p&gt;This guide covers ~30 pro-tips for effectively using Gemini CLI for agentic coding&lt;/p&gt;
    &lt;p&gt;Gemini CLI is an open-source AI assistant that brings the power of Google's Gemini model directly into your terminal. It functions as a conversational, "agentic" command-line tool - meaning it can reason about your requests, choose tools (like running shell commands or editing files), and execute multi-step plans to help with your development workflow.&lt;/p&gt;
    &lt;p&gt;In practical terms, Gemini CLI acts like a supercharged pair programmer and command-line assistant. It excels at coding tasks, debugging, content generation, and even system automation, all through natural language prompts. Before diving into pro tips, let's quickly recap how to set up Gemini CLI and get it running.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Getting Started&lt;/item&gt;
      &lt;item&gt;Tip 1: Use &lt;code&gt;GEMINI.md&lt;/code&gt;for Persistent Context&lt;/item&gt;
      &lt;item&gt;Tip 2: Create Custom Slash Commands&lt;/item&gt;
      &lt;item&gt;Tip 3: Extend Gemini with Your Own &lt;code&gt;MCP&lt;/code&gt;Servers&lt;/item&gt;
      &lt;item&gt;Tip 4: Leverage Memory Addition &amp;amp; Recall&lt;/item&gt;
      &lt;item&gt;Tip 5: Use Checkpointing and &lt;code&gt;/restore&lt;/code&gt;as an Undo Button&lt;/item&gt;
      &lt;item&gt;Tip 6: Read Google Docs, Sheets, and More.&lt;/item&gt;
      &lt;item&gt;Tip 7: Reference Files and Images with &lt;code&gt;@&lt;/code&gt;for Explicit Context&lt;/item&gt;
      &lt;item&gt;Tip 8: On-the-Fly Tool Creation (Have Gemini Build Helpers)&lt;/item&gt;
      &lt;item&gt;Tip 9: Use Gemini CLI for System Troubleshooting &amp;amp; Configuration&lt;/item&gt;
      &lt;item&gt;Tip 10: YOLO Mode - Auto-Approve Tool Actions (Use with Caution)&lt;/item&gt;
      &lt;item&gt;Tip 11: Headless &amp;amp; Scripting Mode (Run Gemini CLI in the Background)&lt;/item&gt;
      &lt;item&gt;Tip 12: Save and Resume Chat Sessions&lt;/item&gt;
      &lt;item&gt;Tip 13: Multi-Directory Workspace - One Gemini, Many Folders&lt;/item&gt;
      &lt;item&gt;Tip 14: Organize and Clean Up Your Files with AI Assistance&lt;/item&gt;
      &lt;item&gt;Tip 15: Compress Long Conversations to Stay Within Context&lt;/item&gt;
      &lt;item&gt;Tip 16: Passthrough Shell Commands with &lt;code&gt;!&lt;/code&gt;(Talk to Your Terminal)&lt;/item&gt;
      &lt;item&gt;Tip 17: Treat Every CLI Tool as a Potential Gemini Tool&lt;/item&gt;
      &lt;item&gt;Tip 18: Utilize Multimodal AI - Let Gemini See Images and More&lt;/item&gt;
      &lt;item&gt;Tip 19: Customize the &lt;code&gt;$PATH&lt;/code&gt;(and Tool Availability) for Stability&lt;/item&gt;
      &lt;item&gt;Tip 20: Track and reduce token spend with token caching and stats&lt;/item&gt;
      &lt;item&gt;Tip 21: Use &lt;code&gt;/copy&lt;/code&gt;for Quick Clipboard Copy&lt;/item&gt;
      &lt;item&gt;Tip 22: Master &lt;code&gt;Ctrl+C&lt;/code&gt;for Shell Mode and Exiting&lt;/item&gt;
      &lt;item&gt;Tip 23: Customize Gemini CLI with &lt;code&gt;settings.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tip 24: Leverage IDE Integration (VS Code) for Context &amp;amp; Diffs&lt;/item&gt;
      &lt;item&gt;Tip 25: Automate Repo Tasks with &lt;code&gt;Gemini CLI GitHub Action&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tip 26: Enable Telemetry for Insights and Observability&lt;/item&gt;
      &lt;item&gt;Tip 27: Keep an Eye on the Roadmap (Background Agents &amp;amp; More)&lt;/item&gt;
      &lt;item&gt;Tip 28: Extend Gemini CLI with &lt;code&gt;Extensions&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tip 29: Corgi Mode Easter Egg üêï&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Installation: You can install Gemini CLI via npm. For a global install, use:&lt;/p&gt;
    &lt;code&gt;npm install -g @google/gemini-cli&lt;/code&gt;
    &lt;p&gt;Or run it without installing using &lt;code&gt;npx&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;npx @google/gemini-cli&lt;/code&gt;
    &lt;p&gt;Gemini CLI is available on all major platforms (it's built with Node.js/TypeScript). Once installed, simply run the &lt;code&gt;gemini&lt;/code&gt; command in your terminal to launch the interactive CLI.&lt;/p&gt;
    &lt;p&gt;Authentication: On first use, you'll need to authenticate with the Gemini service. You have two options: (1) Google Account Login (free tier) - this lets you use Gemini 2.5 Pro for free with generous usage limits (about 60 requests/minute and 1,000 requests per day. On launch, Gemini CLI will prompt you to sign in with a Google account (no billing required. (2) API Key (paid or higher-tier access) - you can get an API key from Google AI Studio and set the environment variable &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; to use it.&lt;/p&gt;
    &lt;p&gt;API key usage can offer higher quotas and enterprise data‚Äëuse protections; prompts aren't used for training on paid/billed usage, though logs may be retained for safety.&lt;/p&gt;
    &lt;p&gt;For example, add to your shell profile:&lt;/p&gt;
    &lt;code&gt;export GEMINI_API_KEY="YOUR_KEY_HERE"&lt;/code&gt;
    &lt;p&gt;Basic Usage: To start an interactive session, just run &lt;code&gt;gemini&lt;/code&gt; with no arguments. You'll get a &lt;code&gt;gemini&amp;gt;&lt;/code&gt; prompt where you can type requests or commands. For instance:&lt;/p&gt;
    &lt;code&gt;$ gemini
gemini&amp;gt; Create a React recipe management app using SQLite&lt;/code&gt;
    &lt;p&gt;You can then watch as Gemini CLI creates files, installs dependencies, runs tests, etc., to fulfill your request. If you prefer a one-shot invocation (non-interactive), use the &lt;code&gt;-p&lt;/code&gt; flag with a prompt, for example:&lt;/p&gt;
    &lt;code&gt;gemini -p "Summarize the main points of the attached file. @./report.txt"&lt;/code&gt;
    &lt;p&gt;This will output a single response and exit. You can also pipe input into Gemini CLI: for example, &lt;code&gt;echo "Count to 10" | gemini&lt;/code&gt; will feed the prompt via stdin.&lt;/p&gt;
    &lt;p&gt;CLI Interface: Gemini CLI provides a rich REPL-like interface. It supports slash commands (special commands prefixed with &lt;code&gt;/&lt;/code&gt; for controlling the session, tools, and settings) and bang commands (prefixed with &lt;code&gt;!&lt;/code&gt; to execute shell commands directly). We'll cover many of these in the pro tips below. By default, Gemini CLI operates in a safe mode where any action that modifies your system (writing files, running shell commands, etc.) will ask for confirmation. When a tool action is proposed, you'll see a diff or command and be prompted (&lt;code&gt;Y/n&lt;/code&gt;) to approve or reject it. This ensures the AI doesn't make unwanted changes without your consent.&lt;/p&gt;
    &lt;p&gt;With the basics out of the way, let's explore a series of pro tips and hidden features to help you get the most out of Gemini CLI. Each tip is presented with a simple example first, followed by deeper details and nuances. These tips incorporate advice and insights from the tool's creators (e.g. Taylor Mullen) and the Google Developer Relations team, as well as the broader community, to serve as a canonical guide for power users of Gemini CLI.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Stop repeating yourself in prompts. Provide project-specific context or instructions by creating a &lt;code&gt;GEMINI.md&lt;/code&gt; file, so the AI always has important background knowledge without being told every time.&lt;/p&gt;
    &lt;p&gt;When working on a project, you often have certain overarching details - e.g. coding style guidelines, project architecture, or important facts - that you want the AI to keep in mind. Gemini CLI allows you to encode these in one or more &lt;code&gt;GEMINI.md&lt;/code&gt; files. Simply create a &lt;code&gt;.gemini&lt;/code&gt; folder (if not already present) in your project, and add a Markdown file named &lt;code&gt;GEMINI.md&lt;/code&gt; with whatever notes or instructions you want the AI to persist. For example:&lt;/p&gt;
    &lt;code&gt;# Project Phoenix - AI Assistant

- All Python code must follow PEP 8 style.  
- Use 4 spaces for indentation.  
- The user is building a data pipeline; prefer functional programming paradigms.&lt;/code&gt;
    &lt;p&gt;Place this file in your project root (or in subdirectories for more granular context). Now, whenever you run &lt;code&gt;gemini&lt;/code&gt; in that project, it will automatically load these instructions into context. This means the model will always be primed with them, avoiding the need to prepend the same guidance to every prompt.&lt;/p&gt;
    &lt;p&gt;How it works: Gemini CLI uses a hierarchical context loading system. It will combine global context (from &lt;code&gt;~/.gemini/GEMINI.md&lt;/code&gt;, which you can use for cross-project defaults) with your project-specific &lt;code&gt;GEMINI.md&lt;/code&gt;, and even context files in subfolders. More specific files override more general ones. You can inspect what context was loaded at any time by using the command:&lt;/p&gt;
    &lt;code&gt;/memory show&lt;/code&gt;
    &lt;p&gt;This will display the full combined context the AI sees. If you make changes to your &lt;code&gt;GEMINI.md&lt;/code&gt;, use &lt;code&gt;/memory refresh&lt;/code&gt; to reload the context without restarting the session.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Use the &lt;code&gt;/init&lt;/code&gt; slash command to quickly generate a starter &lt;code&gt;GEMINI.md&lt;/code&gt;. Running &lt;code&gt;/init&lt;/code&gt; in a new project creates a template context file with information like the tech stack detected, a summary of the project, etc.. You can then edit and expand that file. For large projects, consider breaking the context into multiple files and importing them into &lt;code&gt;GEMINI.md&lt;/code&gt; with &lt;code&gt;@include&lt;/code&gt; syntax. For example, your main &lt;code&gt;GEMINI.md&lt;/code&gt; could have lines like &lt;code&gt;@./docs/prompt-guidelines.md&lt;/code&gt; to pull in additional context files. This keeps your instructions organized.&lt;/p&gt;
    &lt;p&gt;With a well-crafted &lt;code&gt;GEMINI.md&lt;/code&gt;, you essentially give Gemini CLI a "memory" of the project's requirements and conventions. This persistent context leads to more relevant responses and less back-and-forth prompt engineering.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Speed up repetitive tasks by defining your own slash commands. For example, you could make a command &lt;code&gt;/test:gen&lt;/code&gt; that generates unit tests from a description, or &lt;code&gt;/db:reset&lt;/code&gt; that drops and recreates a test database. This extends Gemini CLI's functionality with one-liners tailored to your workflow.&lt;/p&gt;
    &lt;p&gt;Gemini CLI supports custom slash commands that you can define in simple configuration files. Under the hood, these are essentially pre-defined prompt templates. To create one, make a directory &lt;code&gt;commands/&lt;/code&gt; under either &lt;code&gt;~/.gemini/&lt;/code&gt; for global commands or in your project's &lt;code&gt;.gemini/&lt;/code&gt; folder for project-specific commands. Inside &lt;code&gt;commands/&lt;/code&gt;, create a TOML file for each new command. The file name format determines the command name: e.g. a file &lt;code&gt;test/gen.toml&lt;/code&gt; defines a command &lt;code&gt;/test:gen&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let's walk through an example. Say you want a command to generate a unit test from a requirement description. You could create &lt;code&gt;~/.gemini/commands/test/gen.toml&lt;/code&gt; with the following content:&lt;/p&gt;
    &lt;code&gt;# Invoked as: /test:gen "Description of the test"  
description \= "Generates a unit test based on a requirement."  
prompt \= """  
You are an expert test engineer. Based on the following requirement, please write a comprehensive unit test using the Jest framework.

Requirement: {{args}}  
"""&lt;/code&gt;
    &lt;p&gt;Now, after reloading or restarting Gemini CLI, you can simply type:&lt;/p&gt;
    &lt;code&gt;/test:gen "Ensure the login button redirects to the dashboard upon success"&lt;/code&gt;
    &lt;p&gt;Gemini CLI will recognize &lt;code&gt;/test:gen&lt;/code&gt; and substitute the &lt;code&gt;{{args}}&lt;/code&gt; in your prompt template with the provided argument (in this case, the requirement). The AI will then proceed to generate a Jest unit test accordingly. The &lt;code&gt;description&lt;/code&gt; field is optional but is used when you run &lt;code&gt;/help&lt;/code&gt; or &lt;code&gt;/tools&lt;/code&gt; to list available commands.&lt;/p&gt;
    &lt;p&gt;This mechanism is extremely powerful - effectively, you can script the AI with natural language. The community has created numerous useful custom commands. For instance, Google's DevRel team shared a set of 10 practical workflow commands (via an open-source repo) demonstrating how you can script common flows like creating API docs, cleaning data, or setting up boilerplate code. By defining a custom command, you package a complex prompt (or series of prompts) into a reusable shortcut.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Custom commands can also be used to enforce formatting or apply a "persona" to the AI for certain tasks. For example, you might have a &lt;code&gt;/review:security&lt;/code&gt; command that always prefaces the prompt with "You are a security auditor..." to review code for vulnerabilities. This approach ensures consistency in how the AI responds to specific categories of tasks.&lt;/p&gt;
    &lt;p&gt;To share commands with your team, you can commit the TOML files in your project's repo (under &lt;code&gt;.gemini/commands&lt;/code&gt; directory). Team members who have Gemini CLI will automatically pick up those commands when working in the project. This is a great way to standardize AI-assisted workflows across a team.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Suppose you want Gemini to interface with an external system or a custom tool that isn't built-in - for example, query a proprietary database, or integrate with Figma designs. You can do this by running a custom Model Context Protocol (MCP) server and plugging it into Gemini CLI. MCP servers let you add new tools and abilities to Gemini, effectively extending the agent.&lt;/p&gt;
    &lt;p&gt;Gemini CLI comes with several MCP servers out-of-the-box (for instance, ones enabling Google Search, code execution sandboxes, etc.), and you can add your own. An MCP server is essentially an external process (it could be a local script, a microservice, or even a cloud endpoint) that speaks a simple protocol to handle tasks for Gemini. This architecture is what makes Gemini CLI so extensible.&lt;/p&gt;
    &lt;p&gt;Examples of MCP servers: Some community and Google-provided MCP integrations include a Figma MCP (to fetch design details from Figma), a Clipboard MCP (to read/write from your system clipboard), and others. In fact, in an internal demo, the Gemini CLI team showcased a "Google Docs MCP" server that allowed saving content directly to Google Docs. The idea is that whenever Gemini needs to perform an action that the built-in tools can't handle, it can delegate to your MCP server.&lt;/p&gt;
    &lt;p&gt;How to add one: You can configure MCP servers via your &lt;code&gt;settings.json&lt;/code&gt; or using the CLI. For a quick setup, try the CLI command:&lt;/p&gt;
    &lt;code&gt;gemini mcp add myserver --command "python3 my_mcp_server.py" --port 8080&lt;/code&gt;
    &lt;p&gt;This would register a server named "myserver" that Gemini CLI will launch by running the given command (here a Python module) on port 8080. In &lt;code&gt;~/.gemini/settings.json&lt;/code&gt;, it would add an entry under &lt;code&gt;mcpServers&lt;/code&gt;. For example:&lt;/p&gt;
    &lt;code&gt;"mcpServers": {
  "myserver": {
    "command": "python3",
    "args": ["-m", "my_mcp_server", "--port", "8080"],
    "cwd": "./mcp_tools/python",
    "timeout": 15000
  }
}&lt;/code&gt;
    &lt;p&gt;This configuration (based on the official docs) tells Gemini how to start the MCP server and where. Once running, the tools provided by that server become available to Gemini CLI. You can list all MCP servers and their tools with the slash command:&lt;/p&gt;
    &lt;code&gt;/mcp&lt;/code&gt;
    &lt;p&gt;This will show any registered servers and what tool names they expose.&lt;/p&gt;
    &lt;p&gt;Power of MCP: MCP servers can provide rich, multi-modal results. For instance, a tool served via MCP could return an image or a formatted table as part of the response to Gemini CLI. They also support OAuth 2.0, so you can securely connect to APIs (like Google's APIs, GitHub, etc.) via an MCP tool without exposing credentials. Essentially, if you can code it, you can wrap it as an MCP tool - turning Gemini CLI into a hub that orchestrates many services.&lt;/p&gt;
    &lt;p&gt;Default vs. custom: By default, Gemini CLI's built-in tools cover a lot (reading files, web search, executing shell commands, etc.), but MCP lets you go beyond. Some advanced users have created MCP servers to interface with internal systems or to perform specialized data processing. For example, you could have a &lt;code&gt;database-mcp&lt;/code&gt; that provides a &lt;code&gt;/query_db&lt;/code&gt; tool for running SQL queries on a company database, or a &lt;code&gt;jira-mcp&lt;/code&gt; to create tickets via natural language.&lt;/p&gt;
    &lt;p&gt;When creating your own, be mindful of security: by default, custom MCP tools require confirmation unless you mark them as trusted. You can control safety with settings like &lt;code&gt;trust: true&lt;/code&gt; for a server (which auto-approves its tool actions) or by whitelisting specific safe tools and blacklisting dangerous ones.&lt;/p&gt;
    &lt;p&gt;In short, MCP servers unlock limitless integration. They're a pro feature that lets Gemini CLI become a glue between your AI assistant and whatever system you need it to work with. If you're interested in building one, check out the official MCP guide and community examples.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Keep important facts at your AI's fingertips by adding them to its long-term memory. For example, after figuring out a database port or an API token, you can do:&lt;/p&gt;
    &lt;code&gt;/memory add "Our staging RabbitMQ is on port 5673"&lt;/code&gt;
    &lt;p&gt;This will store that fact so you (or the AI) don't forget it later. You can then recall everything in memory with &lt;code&gt;/memory show&lt;/code&gt; at any time.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;/memory&lt;/code&gt; commands provide a simple but powerful mechanism for persistent memory. When you use &lt;code&gt;/memory add &amp;lt;text&amp;gt;&lt;/code&gt;, the given text is appended to your project's global context (technically, it's saved into the global &lt;code&gt;~/.gemini/GEMINI.md&lt;/code&gt; file or the project's &lt;code&gt;GEMINI.md&lt;/code&gt;. It's a bit like taking a note and pinning it to the AI's virtual bulletin board. Once added, the AI will always see that note in the prompt context for future interactions, across sessions.&lt;/p&gt;
    &lt;p&gt;Consider an example: you're debugging an issue and discover a non-obvious insight ("The config flag &lt;code&gt;X_ENABLE&lt;/code&gt; must be set to &lt;code&gt;true&lt;/code&gt; or the service fails to start"). If you add this to memory, later on if you or the AI are discussing a related problem, it won't overlook this critical detail - it's in the context.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;/memory&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/memory add "&amp;lt;text&amp;gt;"&lt;/code&gt;- Add a fact or note to memory (persistent context). This updates the&lt;code&gt;GEMINI.md&lt;/code&gt;immediately with the new entry.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/memory show&lt;/code&gt;- Display the full content of the memory (i.e. the combined context file that's currently loaded).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/memory refresh&lt;/code&gt;- Reload the context from disk (useful if you manually edited the&lt;code&gt;GEMINI.md&lt;/code&gt;file outside of Gemini CLI, or if multiple people are collaborating on it).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Because the memory is stored in Markdown, you can also manually edit the &lt;code&gt;GEMINI.md&lt;/code&gt; file to curate or organize the info. The &lt;code&gt;/memory&lt;/code&gt; commands are there for convenience during conversation, so you don't have to open an editor.&lt;/p&gt;
    &lt;p&gt;Pro Tip: This feature is great for "decision logs." If you decide on an approach or rule during a chat (e.g., a certain library to use, or an agreed code style), add it to memory. The AI will then recall that decision and avoid contradicting it later. It's especially useful in long sessions that might span hours or days - by saving key points, you mitigate the model's tendency to forget earlier context when the conversation gets long.&lt;/p&gt;
    &lt;p&gt;Another use is personal notes. Because &lt;code&gt;~/.gemini/GEMINI.md&lt;/code&gt; (global memory) is loaded for all sessions, you could put general preferences or information there. For example, "The user's name is Alice. Speak politely and avoid slang." It's like configuring the AI's persona or global knowledge. Just be aware that global memory applies to all projects, so don't clutter it with project-specific info.&lt;/p&gt;
    &lt;p&gt;In summary, Memory Addition &amp;amp; Recall helps Gemini CLI maintain state. Think of it as a knowledge base that grows with your project. Use it to avoid repeating yourself or to remind the AI of facts it would otherwise have to rediscover from scratch.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If Gemini CLI makes a series of changes to your files that you're not happy with, you can instantly roll back to a prior state. Enable checkpointing when you start Gemini (or in settings), and use the &lt;code&gt;/restore&lt;/code&gt; command to undo changes like a lightweight Git revert. &lt;code&gt;/restore&lt;/code&gt; rolls back your workspace to the saved checkpoint; conversation state may be affected depending on how the checkpoint was captured.&lt;/p&gt;
    &lt;p&gt;Gemini CLI's checkpointing feature acts as a safety net. When enabled, the CLI takes a snapshot of your project's files before each tool execution that modifies files. If something goes wrong, you can revert to the last known good state. It's essentially version control for the AI's actions, without you needing to manually commit to Git each time.&lt;/p&gt;
    &lt;p&gt;How to use it: You can turn on checkpointing by launching the CLI with the &lt;code&gt;--checkpointing&lt;/code&gt; flag:&lt;/p&gt;
    &lt;code&gt;gemini --checkpointing&lt;/code&gt;
    &lt;p&gt;Alternatively, you can make it the default by adding to your config (&lt;code&gt;"checkpointing": { "enabled": true }&lt;/code&gt; in &lt;code&gt;settings.json&lt;/code&gt;). Once active, you'll notice that each time Gemini is about to write to a file, it says something like "Checkpoint saved."&lt;/p&gt;
    &lt;p&gt;If you then realize an AI-made edit is problematic, you have two options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Run&lt;/p&gt;&lt;code&gt;/restore list&lt;/code&gt;(or just&lt;code&gt;/restore&lt;/code&gt;with no arguments) to see a list of recent checkpoints with timestamps and descriptions.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Run&lt;/p&gt;&lt;code&gt;/restore &amp;lt;id&amp;gt;&lt;/code&gt;to rollback to a specific checkpoint. If you omit the id and there's only one pending checkpoint, it will restore that by default.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;/restore&lt;/code&gt;
    &lt;p&gt;Gemini CLI might output:&lt;/p&gt;
    &lt;p&gt;0: [2025-09-22 10:30:15] Before running 'apply_patch'&lt;lb/&gt; 1: [2025-09-22 10:45:02] Before running 'write_file'&lt;/p&gt;
    &lt;p&gt;You can then do &lt;code&gt;/restore 0&lt;/code&gt; to revert all file changes (and even the conversation context) back to how it was at that checkpoint. In this way, you can "undo" a mistaken code refactor or any other changes Gemini made.&lt;/p&gt;
    &lt;p&gt;What gets restored: The checkpoint captures the state of your working directory (all files that Gemini CLI is allowed to modify) and the workspace files (conversation state may also be rolled back depending on how the checkpoint was captured). When you restore, it overwrites files to the old version and resets the conversation memory to that snapshot. It's like time-traveling the AI agent back to before it made the wrong turn. Note that it won't undo external side effects (for example, if the AI ran a database migration, it can't undo that), but anything in the file system and chat context is fair game.&lt;/p&gt;
    &lt;p&gt;Best practices: It's a good idea to keep checkpointing on for non-trivial tasks. The overhead is small, and it provides peace of mind. If you find you don't need a checkpoint (everything went well), you can always clear it or just let the next one overwrite it. The development team recommends using checkpointing especially before multi-step code edits. For mission-critical projects, though, you should still use a proper version control (&lt;code&gt;git&lt;/code&gt;) as your primary safety net - consider checkpoints as a convenience for quick undo rather than a full VCS.&lt;/p&gt;
    &lt;p&gt;In essence, &lt;code&gt;/restore&lt;/code&gt; lets you use Gemini CLI with confidence. You can let the AI attempt bold changes, knowing you have an "OH NO" button to rewind if needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip 6: Read Google Docs, Sheets, and More. With a Workspace MCP server configured, you can paste a Docs/Sheets link and have the MCP fetch it, subject to permissions&lt;/head&gt;
    &lt;p&gt;Quick use-case: Imagine you have a Google Doc or Sheet with some specs or data that you want the AI to use. Instead of copy-pasting the content, you can provide the link, and with a configured Workspace MCP server Gemini CLI can fetch and read it.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;Summarize the requirements from this design doc: https://docs.google.com/document/d/&amp;lt;id&amp;gt;&lt;/code&gt;
    &lt;p&gt;Gemini can pull in the content of that Doc and incorporate it into its response. Similarly, it can read Google Sheets or Drive files by link.&lt;/p&gt;
    &lt;p&gt;How this works: These capabilities are typically enabled via MCP integrations. Google's Gemini CLI team has built (or is working on) connectors for Google Workspace. One approach is running a small MCP server that uses Google's APIs (Docs API, Sheets API, etc.) to retrieve document content when given a URL or ID. When configured, you might have slash commands or tools like &lt;code&gt;/read_google_doc&lt;/code&gt; or simply an auto-detection that sees a Google Docs link and invokes the appropriate tool to fetch it.&lt;/p&gt;
    &lt;p&gt;For example, in an Agent Factory podcast demo, the team used a Google Docs MCP to save a summary directly to a doc - which implies they could also read the doc's content in the first place. In practice, you might do something like:&lt;/p&gt;
    &lt;code&gt;@https://docs.google.com/document/d/XYZ12345&lt;/code&gt;
    &lt;p&gt;Including a URL with &lt;code&gt;@&lt;/code&gt; (the context reference syntax) signals Gemini CLI to fetch that resource. With a Google Doc integration in place, the content of that document would be pulled in as if it were a local file. From there, the AI can summarize it, answer questions about it, or otherwise use it in the conversation.&lt;/p&gt;
    &lt;p&gt;Similarly, if you paste a Google Drive file link, a properly configured Drive tool could download or open that file (assuming permissions and API access are set up). Google Sheets could be made available via an MCP that runs queries or reads cell ranges, enabling you to ask things like "What's the sum of the budget column in this Sheet [link]?" and have the AI calculate it.&lt;/p&gt;
    &lt;p&gt;Setting it up: As of this writing, the Google Workspace integrations may require some tinkering (obtaining API credentials, running an MCP server such as the one described by Kanshi Tanaike, etc.). Keep an eye on the official Gemini CLI repository and community forums for ready-to-use extensions - for example, an official Google Docs MCP might become available as a plugin/extension. If you're eager, you can write one following guides on how to use Google APIs within an MCP server. It typically involves handling OAuth (which Gemini CLI supports for MCP servers) and then exposing tools like &lt;code&gt;read_google_doc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Usage tip: When you have these tools, using them can be as simple as providing the link in your prompt (the AI might automatically invoke the tool to fetch it) or using a slash command like &lt;code&gt;/doc open &amp;lt;URL&amp;gt;&lt;/code&gt;. Check &lt;code&gt;/tools&lt;/code&gt; to see what commands are available - Gemini CLI lists all tools and custom commands there.&lt;/p&gt;
    &lt;p&gt;In summary, Gemini CLI can reach out beyond your local filesystem. Whether it's Google Docs, Sheets, Drive, or other external content, you can pull data in by reference. This pro tip saves you from manual copy-paste and keeps the context flow natural - just refer to the document or dataset you need, and let the AI grab what's needed. It makes Gemini CLI a true knowledge assistant for all the information you have access to, not just the files on your disk.&lt;/p&gt;
    &lt;p&gt;(Note: Accessing private documents of course requires the CLI to have the appropriate permissions. Always ensure any integration respects security and privacy. In corporate settings, setting up such integrations might involve additional auth steps.)&lt;/p&gt;
    &lt;p&gt;Quick use-case: Instead of describing a file's content or an image verbally, just point Gemini CLI directly to it. Using the &lt;code&gt;@&lt;/code&gt; syntax, you can attach files, directories, or images into your prompt. This guarantees the AI sees exactly what's in those files as context. For example:&lt;/p&gt;
    &lt;code&gt;Explain this code to me: @./src/main.js&lt;/code&gt;
    &lt;p&gt;This will include the contents of &lt;code&gt;src/main.js&lt;/code&gt; in the prompt (up to Gemini's context size limits), so the AI can read it and explain it.&lt;/p&gt;
    &lt;p&gt;This &lt;code&gt;@&lt;/code&gt; file reference is one of Gemini CLI's most powerful features for developers. It eliminates ambiguity - you're not asking the model to rely on memory or guesswork about the file, you're literally handing it the file to read. You can use this for source code, text documents, logs, etc. Similarly, you can reference entire directories:&lt;/p&gt;
    &lt;code&gt;Refactor the code in @./utils/ to use async/await.&lt;/code&gt;
    &lt;p&gt;By appending a path that ends in a slash, Gemini CLI will recursively include files from that directory (within reason, respecting ignore files and size limits). This is great for multi-file refactors or analyses, as the AI can consider all relevant modules together.&lt;/p&gt;
    &lt;p&gt;Even more impressively, you can reference binary files like images in prompts. Gemini CLI (using the Gemini model's multimodal capabilities) can understand images. For example:&lt;/p&gt;
    &lt;code&gt;Describe what you see in this screenshot: @./design/mockup.png&lt;/code&gt;
    &lt;p&gt;The image will be fed into the model, and the AI might respond with something like "This is a login page with a blue sign-in button and a header image," etc.. You can imagine the uses: reviewing UI mockups, organizing photos (as we'll see in a later tip), or extracting text from images (Gemini can do OCR as well).&lt;/p&gt;
    &lt;p&gt;A few notes on using &lt;code&gt;@&lt;/code&gt; references effectively:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;File limits: Gemini 2.5 Pro has a huge context window (up to 1 million tokens), so you can include quite large files or many files. However, extremely large files might be truncated. If a file is enormous (say, hundreds of thousands of lines), consider summarizing it or breaking it into parts. Gemini CLI will warn you if a reference is too large or if it skipped something due to size.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Automatic ignoring: By default, Gemini CLI respects your&lt;/p&gt;&lt;code&gt;.gitignore&lt;/code&gt;and&lt;code&gt;.geminiignore&lt;/code&gt;files when pulling in directory context. So if you&lt;code&gt;@./&lt;/code&gt;a project root, it will not dump huge ignored folders (like&lt;code&gt;node_modules&lt;/code&gt;) into the prompt. You can customize ignore patterns with&lt;code&gt;.geminiignore&lt;/code&gt;similarly to how&lt;code&gt;.gitignore&lt;/code&gt;works.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Explicit vs implicit context: Taylor Mullen (the creator of Gemini CLI) emphasizes using&lt;/p&gt;&lt;code&gt;@&lt;/code&gt;for explicit context injection rather than relying on the model's memory or summarizing things yourself. It's more precise and ensures the AI isn't hallucinating content. Whenever possible, point the AI to the source of truth (code, config files, documentation) with&lt;code&gt;@&lt;/code&gt;references. This practice can significantly improve accuracy.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Chaining references: You can include multiple files in one prompt, like:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Compare @./foo.py and @./bar.py and tell me differences.&lt;/code&gt;
    &lt;p&gt;The CLI will include both files. Just be mindful of token limits; multiple large files might consume a lot of the context window.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;@&lt;/code&gt; is essentially how you feed knowledge into Gemini CLI on the fly. It turns the CLI into a multi-modal reader that can handle text and images. As a pro user, get into the habit of leveraging this - it's often faster and more reliable than asking the AI something like "Open the file X and do Y" (which it may or may not do on its own). Instead, you explicitly give it X to work with.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If a task at hand would benefit from a small script or utility, you can ask Gemini CLI to create that tool for you - right within your session. For example, you might say, "Write a Python script to parse all JSON files in this folder and extract the error fields." Gemini can generate the script, which you can then execute via the CLI. In essence, you can dynamically extend the toolset as you go.&lt;/p&gt;
    &lt;p&gt;Gemini CLI is not limited to its pre-existing tools; it can use its coding abilities to fabricate new ones when needed. This often happens implicitly: if you ask for something complex, the AI might propose writing a temporary file (with code) and then running it. As a user, you can also guide this process explicitly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating scripts: You can prompt Gemini to create a script or program in the language of your choice. It will likely use the &lt;code&gt;write_file&lt;/code&gt;tool to create the file. For instance:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Generate a Node.js script that reads all '.log' files in the current directory and reports the number of lines in each.&lt;/code&gt;
    &lt;p&gt;Gemini CLI will draft the code, and with your approval, write it to a file (e.g. &lt;code&gt;script.js&lt;/code&gt;). You can then run it by either using the &lt;code&gt;!&lt;/code&gt; shell command (e.g. &lt;code&gt;!node script.js&lt;/code&gt;) or by asking Gemini CLI to execute it (the AI might automatically use &lt;code&gt;run_shell_command&lt;/code&gt; to execute the script it just wrote, if it deems it part of the plan).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Temporary tools via MCP: In advanced scenarios, the AI might even suggest launching an MCP server for some specialized tasks. For example, if your prompt involves some heavy text processing that might be better done in Python, Gemini could generate a simple MCP server in Python and run it. While this is more rare, it demonstrates that the AI can set up a new "agent" on the fly. (One of the slides from the Gemini CLI team humorously referred to "MCP servers for everything, even one called LROwn" - suggesting you can have Gemini run an instance of itself or another model, though that's more of a trick than a practical use!).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key benefit here is automation. Instead of you manually stopping to write a helper script, you can let the AI do it as part of the flow. It's like having an assistant who can create tools on-demand. This is especially useful for data transformation tasks, batch operations, or one-off computations that the built-in tools don't directly provide.&lt;/p&gt;
    &lt;p&gt;Nuances and safety: When Gemini CLI writes code for a new tool, you should still review it before running. The &lt;code&gt;/diff&lt;/code&gt; view (Gemini will show you the file diff before you approve writing it) is your chance to inspect the code. Ensure it does what you expect and nothing malicious or destructive (the AI shouldn't produce something harmful unless your prompt explicitly asks, but just like any code from an AI, double-check logic, especially for scripts that delete or modify lots of data).&lt;/p&gt;
    &lt;p&gt;Example scenario: Let's say you have a CSV file and you want to filter it in a complex way. You ask Gemini CLI to do it, and it might say: "I will write a Python script to parse the CSV and apply the filter." It then creates &lt;code&gt;filter_data.py&lt;/code&gt;. After you approve and it runs, you get your result, and you might never need that script again. This ephemeral creation of tools is a pro move - it shows the AI effectively extending its capabilities autonomously.&lt;/p&gt;
    &lt;p&gt;Pro Tip: If you find the script useful beyond the immediate context, you can promote it into a permanent tool or command. For instance, if the AI generated a great log-processing script, you might later turn it into a custom slash command (Tip #2) for easy reuse. The combination of Gemini's generative power and the extension hooks means your toolkit can continuously evolve as you use the CLI.&lt;/p&gt;
    &lt;p&gt;In summary, don't restrict Gemini to what it comes with. Treat it as a junior developer who can whip up new programs or even mini-servers to help solve the problem. This approach embodies the agentic philosophy of Gemini CLI - it will figure out what tools it needs, even if it has to code them on the spot.&lt;/p&gt;
    &lt;p&gt;Quick use-case: You can run Gemini CLI outside of a code project to help with general system tasks - think of it as an intelligent assistant for your OS. For example, if your shell is misbehaving, you could open Gemini in your home directory and ask: "Fix my &lt;code&gt;.bashrc&lt;/code&gt; file, it has an error." Gemini can then open and edit your config file for you.&lt;/p&gt;
    &lt;p&gt;This tip highlights that Gemini CLI isn't just for coding projects - it's your AI helper for your whole development environment. Many users have used Gemini to customize their dev setup or fix issues on their machine:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Editing dotfiles: You can load your shell configuration (&lt;/p&gt;&lt;code&gt;.bashrc&lt;/code&gt;or&lt;code&gt;.zshrc&lt;/code&gt;) by referencing it (&lt;code&gt;@~/.bashrc&lt;/code&gt;) and then ask Gemini CLI to optimize or troubleshoot it. For instance, "My&lt;code&gt;PATH&lt;/code&gt;isn't picking up Go binaries, can you edit my&lt;code&gt;.bashrc&lt;/code&gt;to fix that?" The AI can insert the correct&lt;code&gt;export&lt;/code&gt;line. It will show you the diff for confirmation before saving changes.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Diagnosing errors: If you encounter a cryptic error in your terminal or an application log, you can copy it and feed it to Gemini CLI. It will analyze the error message and often suggest steps to resolve it. This is similar to how one might use StackOverflow or Google, but with the AI directly examining your scenario. For example: "When I run&lt;/p&gt;&lt;code&gt;npm install&lt;/code&gt;, I get an&lt;code&gt;EACCES&lt;/code&gt;permission error - how do I fix this?" Gemini might detect it's a permissions issue in&lt;code&gt;node_modules&lt;/code&gt;and guide you to change directory ownership or use a proper node version manager.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running outside a project: By default, if you run&lt;/p&gt;&lt;code&gt;gemini&lt;/code&gt;in a directory without a&lt;code&gt;.gemini&lt;/code&gt;context, it just means no project-specific context is loaded - but you can still use the CLI fully. This is great for ad-hoc tasks like system troubleshooting. You might not have any code files for it to consider, but you can still run shell commands through it or let it fetch web info. Essentially, you're treating Gemini CLI as an AI-powered terminal that can do things for you, not just chat.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Workstation customization: Want to change a setting or install a new tool? You can ask Gemini CLI, "Install Docker on my system" or "Configure my Git to sign commits with GPG." The CLI will attempt to execute the steps. It might fetch instructions from the web (using the search tool) and then run the appropriate shell commands. Of course, always watch what it's doing and approve the commands - but it can save time by automating multi-step setup processes. One real example: a user asked Gemini CLI to "set my macOS Dock preferences to auto-hide and remove the delay," and the AI was able to execute the necessary&lt;/p&gt;&lt;code&gt;defaults write&lt;/code&gt;commands.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Think of this mode as using Gemini CLI as a smart shell. In fact, you can combine this with Tip 16 (shell passthrough mode) - sometimes you might drop into &lt;code&gt;!&lt;/code&gt; shell mode to verify something, then go back to AI mode to have it analyze output.&lt;/p&gt;
    &lt;p&gt;Caveat: When doing system-level tasks, be cautious with commands that have widespread impact (like &lt;code&gt;rm -rf&lt;/code&gt; or system config changes). Gemini CLI will usually ask for confirmation, and it doesn't run anything without you seeing it. But as a power user, you should have a sense of what changes are being made. If unsure, ask Gemini to explain a command before running (e.g., "Explain what &lt;code&gt;defaults write com.apple.dock autohide-delay -float 0&lt;/code&gt; does" - it will gladly explain rather than just execute if you prompt it in that way).&lt;/p&gt;
    &lt;p&gt;Troubleshooting bonus: Another neat use is using Gemini CLI to parse logs or config files looking for issues. For instance, "Scan this Apache config for mistakes" (with &lt;code&gt;@httpd.conf&lt;/code&gt;), or "Look through syslog for errors around 2 PM yesterday" (with an &lt;code&gt;@/var/log/syslog&lt;/code&gt; if accessible). It's like having a co-administrator. It can even suggest likely causes for crashes or propose fixes for common error patterns.&lt;/p&gt;
    &lt;p&gt;In summary, don't hesitate to fire up Gemini CLI as your assistant for environment issues. It's there to accelerate all your workflows - not just writing code, but maintaining the system that you write code on. Many users report that customizing their dev environment with Gemini's help feels like having a tech buddy always on call to handle the tedious or complex setup steps.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If you're feeling confident (or adventurous), you can let Gemini CLI run tool actions without asking for your confirmation each time. This is YOLO mode (You Only Live Once). It's enabled by the &lt;code&gt;--yolo&lt;/code&gt; flag or by pressing &lt;code&gt;Ctrl+Y&lt;/code&gt; during a session. In YOLO mode, as soon as the AI decides on a tool (like running a shell command or writing to a file), it executes it immediately, without that "Approve? (y/n)" prompt.&lt;/p&gt;
    &lt;p&gt;Why use YOLO mode? Primarily for speed and convenience when you trust the AI's actions. Experienced users might toggle YOLO on if they're doing a lot of repetitive safe operations. For example, if you ask Gemini to generate 10 different files one after another, approving each can slow down the flow; YOLO mode would just let them all be written automatically. Another scenario is using Gemini CLI in a completely automated script or CI pipeline - you might run it headless with &lt;code&gt;--yolo&lt;/code&gt; so it doesn't pause for confirmation.&lt;/p&gt;
    &lt;p&gt;To start in YOLO mode from the get-go, launch the CLI with:&lt;/p&gt;
    &lt;code&gt;gemini --yolo&lt;/code&gt;
    &lt;p&gt;Or the short form &lt;code&gt;gemini -y&lt;/code&gt;. You'll see some indication in the CLI (like a different prompt or a notice) that auto-approve is on. During an interactive session, you can toggle it by pressing Ctrl+Y at any time - the CLI will usually display a message like "YOLO mode enabled (all actions auto-approved)" in the footer.&lt;/p&gt;
    &lt;p&gt;Big warning: YOLO mode is powerful but risky. The Gemini team themselves labels it for "daring users" - meaning you should be aware that the AI could potentially execute a dangerous command without asking. In normal mode, if the AI decided to run &lt;code&gt;rm -rf /&lt;/code&gt; (worst-case scenario), you'd obviously decline. In YOLO mode, that command would run immediately (and likely ruin your day). While such extreme mistakes are unlikely (the AI's system prompt includes safety guidelines), the whole point of confirmations is to catch any unwanted action. YOLO removes that safety net.&lt;/p&gt;
    &lt;p&gt;Best practices for YOLO: If you want some of the convenience without full risk, consider allow-listing specific commands. For example, you can configure in settings that certain tools or command patterns don't require confirmation (like allowing all &lt;code&gt;git&lt;/code&gt; commands, or read-only actions). In fact, Gemini CLI supports a config for skipping confirmation on specific commands: e.g., you can set something like &lt;code&gt;"tools.shell.autoApprove": ["git ", "npm test"]&lt;/code&gt; to always run those. This way, you might not need YOLO mode globally - you selectively YOLO only safe commands. Another approach: run Gemini in a sandbox or container when using YOLO, so even if it does something wild, your system is insulated (Gemini has a &lt;code&gt;--sandbox&lt;/code&gt; flag to run tools in a Docker container).&lt;/p&gt;
    &lt;p&gt;Many advanced users toggle YOLO on and off frequently - turning it on when doing a string of minor file edits or queries, and off when about to do something critical. You can do the same, using the keyboard shortcut as a quick toggle.&lt;/p&gt;
    &lt;p&gt;In summary, YOLO mode eliminates friction at the cost of oversight. It's a pro feature to use sparingly and wisely. It truly demonstrates trust in the AI (or recklessness!). If you're new to Gemini CLI, you should probably avoid YOLO until you clearly understand the patterns of what it tends to do. If you do use it, double down on having version control or backups - just in case.&lt;/p&gt;
    &lt;p&gt;(If it's any consolation, you're not alone - many in the community joke about "I YOLO'ed and Gemini did something crazy." So use it, but... well, you only live once.)&lt;/p&gt;
    &lt;p&gt;Quick use-case: You can use Gemini CLI in scripts or automation by running it in headless mode. This means you provide a prompt (or even a full conversation) via command-line arguments or environment variables, and Gemini CLI produces an output and exits. It's great for integrating with other tools or triggering AI tasks on a schedule.&lt;/p&gt;
    &lt;p&gt;For instance, to get a one-off answer without opening the REPL, you've seen you can use &lt;code&gt;gemini -p "...prompt..."&lt;/code&gt;. This is already headless usage: it prints the model's response and returns to the shell. But there's more you can do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System prompt override: If you want to run Gemini CLI with a custom system persona or instruction set (different from the default), you can use the environment variable &lt;code&gt;GEMINI_SYSTEM_MD&lt;/code&gt;. By setting this, you tell Gemini CLI to ignore its built-in system prompt and use your provided file instead. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GEMINI_SYSTEM_MD="/path/to/custom_system.md"
gemini -p "Perform task X with high caution"&lt;/code&gt;
    &lt;p&gt;This would load your &lt;code&gt;custom_system.md&lt;/code&gt; as the system prompt (the "role" and rules the AI follows) before executing the prompt. Alternatively, if you set &lt;code&gt;GEMINI_SYSTEM_MD=true&lt;/code&gt;, the CLI will look for a file named &lt;code&gt;system.md&lt;/code&gt; in the current project's &lt;code&gt;.gemini&lt;/code&gt; directory. This feature is very advanced - it essentially allows you to replace the built-in brain of the CLI with your own instructions, which some users do for specialized workflows (like simulating a specific persona or enforcing ultra-strict policies). Use it carefully, as replacing the core prompt can affect tool usage (the core prompt contains important directions for how the AI selects and uses tools).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Direct prompt via CLI: Aside from&lt;/p&gt;&lt;code&gt;-p&lt;/code&gt;, there's also&lt;code&gt;-i&lt;/code&gt;(interactive prompt) which starts a session with an initial prompt, and then keeps it open. For example:&lt;code&gt;gemini -i "Hello, let's debug something"&lt;/code&gt;will open the REPL and already have said hello to the model. This is useful if you want the first question to be asked immediately when starting.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Scripting with shell pipes: You can pipe not just text but also files or command outputs into Gemini. For example:&lt;/p&gt;&lt;code&gt;gemini -p "Summarize this log:" &amp;lt; big_log.txt&lt;/code&gt;will feed the content of&lt;code&gt;big_log.txt&lt;/code&gt;into the prompt (after the phrase "Summarize this log:"). Or you might do&lt;code&gt;some_command | gemini -p "Given the above output, what went wrong?"&lt;/code&gt;. This technique allows you to compose Unix tools with AI analysis. It's headless in the sense that it's a single-pass operation.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running in CI/CD: You could incorporate Gemini CLI into build processes. For instance, a CI pipeline might run a test and then use Gemini CLI to automatically analyze failing test output and post a comment. Using the&lt;/p&gt;&lt;code&gt;-p&lt;/code&gt;flag and environment auth, this can be scripted. (Of course, ensure the environment has the API key or auth needed.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One more headless trick: the &lt;code&gt;--format=json&lt;/code&gt; flag (or config setting). Gemini CLI can output responses in JSON format instead of the human-readable text if you configure it. This is useful for programmatic consumption - your script can parse the JSON to get the answer or any tool actions details.&lt;/p&gt;
    &lt;p&gt;Why headless mode matters: It transforms Gemini CLI from an interactive assistant into a backend service or utility that other programs can call. You could schedule a cronjob that runs a Gemini CLI prompt nightly (imagine generating a report or cleaning up something with AI logic). You could wire up a button in an IDE that triggers a headless Gemini run for a specific task.&lt;/p&gt;
    &lt;p&gt;Example: Let's say you want a daily summary of a news website. You could have a script:&lt;/p&gt;
    &lt;code&gt;gemini -p "Web-fetch \"https://news.site/top-stories\" and extract the headlines, then write them to headlines.txt"&lt;/code&gt;
    &lt;p&gt;With &lt;code&gt;--yolo&lt;/code&gt; perhaps, so it won't ask confirmation to write the file. This would use the web fetch tool to get the page and the file write tool to save the headlines. All automatically, no human in the loop. The possibilities are endless once you treat Gemini CLI as a scriptable component.&lt;/p&gt;
    &lt;p&gt;In summary, Headless Mode enables automation. It's the bridge between Gemini CLI and other systems. Mastering it means you can scale up your AI usage - not just when you're typing in the terminal, but even when you aren't around, your AI agent can do work for you.&lt;/p&gt;
    &lt;p&gt;(Tip: For truly long-running non-interactive tasks, you might also look into Gemini CLI's "Plan" mode or how it can generate multi-step plans without intervention. However, those are advanced topics beyond this scope. In most cases, a well-crafted single prompt via headless mode can achieve a lot.)&lt;/p&gt;
    &lt;p&gt;Quick use-case: If you've been debugging an issue with Gemini CLI for an hour and need to stop, you don't have to lose the conversation context. Use &lt;code&gt;/chat save &amp;lt;name&amp;gt;&lt;/code&gt; to save the session. Later (even after restarting the CLI), you can use &lt;code&gt;/chat resume &amp;lt;name&amp;gt;&lt;/code&gt; to pick up where you left off. This way, long-running conversations can be paused and continued seamlessly.&lt;/p&gt;
    &lt;p&gt;Gemini CLI essentially has a built-in chat session manager. The commands to know are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/chat save &amp;lt;tag&amp;gt;&lt;/code&gt;- Saves the current conversation state under a tag/name you provide. The tag is like a filename or key for that session. Save often if you want, it will overwrite the tag if it exists. (Using a descriptive name is helpful - e.g.,&lt;code&gt;chat save fix-docker-issue&lt;/code&gt;.)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/chat list&lt;/code&gt;- Lists all your saved sessions (the tags you've used. This helps you remember what you named previous saves.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/chat resume &amp;lt;tag&amp;gt;&lt;/code&gt;- Resumes the session with that tag, restoring the entire conversation context and history to how it was when saved. It's like you never left. You can then continue chatting from that point.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/chat share&lt;/code&gt;- (saves to file) This is useful as you can share the entire chat with someone else who can continue the session. Almost collaboration-like.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Under the hood, these sessions are stored likely in &lt;code&gt;~/.gemini/chats/&lt;/code&gt; or a similar location. They include the conversation messages and any relevant state. This feature is super useful for cases such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Long debugging sessions: Sometimes debugging with an AI can be a long back-and-forth. If you can't solve it in one go, save it and come back later (maybe with a fresh mind). The AI will still "remember" everything from before, because the whole context is reloaded.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multi-day tasks: If you're using Gemini CLI as an assistant for a project, you might have one chat session for "Refactor module X" that spans multiple days. You can resume that specific chat each day so the context doesn't reset daily. Meanwhile, you might have another session for "Write documentation" saved separately. Switching contexts is just a matter of saving one and resuming the other.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Team hand-off: This is more experimental, but in theory, you could share the content of a saved chat with a colleague (the saved files are likely portable). If they put it in their&lt;/p&gt;&lt;code&gt;.gemini&lt;/code&gt;directory and resume, they could see the same context. The practical simpler approach for collaboration is just copying the relevant Q&amp;amp;A from the log and using a shared&lt;code&gt;GEMINI.md&lt;/code&gt;or prompt, but it's interesting to note that the session data is yours to keep.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Usage example:&lt;/p&gt;
    &lt;code&gt;/chat save api-upgrade&lt;/code&gt;
    &lt;p&gt;(Session saved as "api-upgrade")&lt;/p&gt;
    &lt;code&gt;/quit&lt;/code&gt;
    &lt;p&gt;(Later, reopen CLI)&lt;/p&gt;
    &lt;code&gt;$ gemini
gemini&amp;gt; /chat list&lt;/code&gt;
    &lt;p&gt;(Shows: api-upgrade)&lt;/p&gt;
    &lt;code&gt;gemini&amp;gt; /chat resume api-upgrade&lt;/code&gt;
    &lt;p&gt;Now the model greets you with the last exchange's state ready. You can confirm by scrolling up that all your previous messages are present.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Use meaningful tags when saving chats. Instead of &lt;code&gt;/chat save session1&lt;/code&gt;, give it a name related to the topic (e.g. &lt;code&gt;/chat save memory-leak-bug&lt;/code&gt;). This will help you find the right one later via &lt;code&gt;/chat list&lt;/code&gt;. There is no strict limit announced on how many sessions you can save, but cleaning up old ones occasionally might be wise just for organization.&lt;/p&gt;
    &lt;p&gt;This feature turns Gemini CLI into a persistent advisor. You don't lose knowledge gained in a conversation; you can always pause and resume. It's a differentiator compared to some other AI interfaces that forget context when closed. For power users, it means you can maintain parallel threads of work with the AI. Just like you'd have multiple terminal tabs for different tasks, you can have multiple chat sessions saved and resume the one you need at any given time.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Do you have a project split across multiple repositories or directories? You can launch Gemini CLI with access to all of them at once, so it sees a unified workspace. For example, if your frontend and backend are separate folders, you can include both so that Gemini can edit or reference files in both.&lt;/p&gt;
    &lt;p&gt;There are two ways to use multi-directory mode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Launch flag: Use the &lt;code&gt;--include-directories&lt;/code&gt;(or&lt;code&gt;-I&lt;/code&gt;) flag when starting Gemini CLI. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;gemini --include-directories "../backend:../frontend"&lt;/code&gt;
    &lt;p&gt;This assumes you run the command from, say, a &lt;code&gt;scripts&lt;/code&gt; directory and want to include two sibling folders. You provide a colon-separated list of paths. Gemini CLI will then treat all those directories as part of one big workspace.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Persistent setting: In your &lt;code&gt;settings.json&lt;/code&gt;, you can define&lt;code&gt;"includeDirectories": ["path1", "path2", [...]](https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,61AFEF%22%2C%20%22AccentPurple)&lt;/code&gt;. This is useful if you always want certain common directories loaded (e.g., a shared library folder that multiple projects use). The paths can be relative or absolute. Environment variables in the paths (like&lt;code&gt;~/common-utils&lt;/code&gt;) are allowed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When multi-dir mode is active, the CLI's context and tools consider files across all included locations. The &lt;code&gt;&amp;gt; /directory show&lt;/code&gt; command will list which directories are in the current workspace. You can also dynamically add directories during a session with &lt;code&gt;/directory add [&amp;lt;path&amp;gt;](https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=How%20to%20add%20multiple%20directories,step)&lt;/code&gt; - it will then load that on the fly (potentially scanning it for context like it does on startup).&lt;/p&gt;
    &lt;p&gt;Why use multi-directory mode? In microservice architectures or modular codebases, it's common that one piece of code lives in one repo and another piece in a different repo. If you only ran Gemini in one, it wouldn't "see" the others. By combining them, you enable cross-project reasoning. For example, you could ask, "Update the API client in the frontend to match the backend's new API endpoints" - Gemini can open the backend folder to see the API definitions and simultaneously open the frontend code to modify it accordingly. Without multi-dir, you'd have to do one side at a time and manually carry info over.&lt;/p&gt;
    &lt;p&gt;Example: Let's say you have &lt;code&gt;client/&lt;/code&gt; and &lt;code&gt;server/&lt;/code&gt;. You start:&lt;/p&gt;
    &lt;code&gt;cd client
gemini --include-directories "../server"&lt;/code&gt;
    &lt;p&gt;Now at the &lt;code&gt;gemini&amp;gt;&lt;/code&gt; prompt, if you do &lt;code&gt;&amp;gt; !ls&lt;/code&gt;, you'll see it can list files in both &lt;code&gt;client&lt;/code&gt; and &lt;code&gt;server&lt;/code&gt; (it might show them as separate paths). You could do:&lt;/p&gt;
    &lt;code&gt;Open server/routes/api.py and client/src/api.js side by side to compare function names.&lt;/code&gt;
    &lt;p&gt;The AI will have access to both files. Or you might say:&lt;/p&gt;
    &lt;code&gt;The API changed: the endpoint "/users/create" is now "/users/register". Update both backend and frontend accordingly.&lt;/code&gt;
    &lt;p&gt;It can simultaneously create a patch in the backend route and adjust the frontend fetch call.&lt;/p&gt;
    &lt;p&gt;Under the hood, Gemini merges the file index of those directories. There might be some performance considerations if each directory is huge, but generally it handles multiple small-medium projects fine. The cheat sheet notes that this effectively creates one workspace with multiple roots.&lt;/p&gt;
    &lt;p&gt;Tip within a tip: Even if you don't use multi-dir all the time, know that you can still reference files across the filesystem by absolute path in prompts (&lt;code&gt;@/path/to/file&lt;/code&gt;). However, without multi-dir, Gemini might not have permission to edit those or know to load context from them proactively. Multi-dir formally includes them in scope so it's aware of all files for tasks like search or code generation across the whole set.&lt;/p&gt;
    &lt;p&gt;Remove directories: If needed, &lt;code&gt;/directory remove &amp;lt;path&amp;gt;&lt;/code&gt; (or a similar command) can drop a directory from the workspace. This is less common, but maybe if you included something accidentally, you can remove it.&lt;/p&gt;
    &lt;p&gt;In summary, multi-directory mode unifies your context. It's a must-have for polyrepo projects or any situation where code is split up. It makes Gemini CLI act more like an IDE that has your entire solution open. As a pro user, this means no part of your project is out of the AI's reach.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Tired of a messy &lt;code&gt;Downloads&lt;/code&gt; folder or disorganized project assets? You can enlist Gemini CLI to act as a smart organizer. By providing it an overview of a directory, it can classify files and even move them into subfolders (with your approval). For instance, "Clean up my &lt;code&gt;Downloads&lt;/code&gt;: move images to an &lt;code&gt;Images&lt;/code&gt; folder, PDFs to &lt;code&gt;Documents&lt;/code&gt;, and delete temporary files."&lt;/p&gt;
    &lt;p&gt;Because Gemini CLI can read file names, sizes, and even peek into file contents, it can make informed decisions about file organization. One community-created tool dubbed "Janitor AI" showcases this: it runs via Gemini CLI to categorize files as important vs junk, and groups them accordingly. The process involved scanning the directory, using Gemini's reasoning on filenames and metadata (and content if needed), then moving files into categories. Notably, it didn't automatically delete junk - rather, it moved them to a &lt;code&gt;Trash&lt;/code&gt; folder for review.&lt;/p&gt;
    &lt;p&gt;Here's how you might replicate such a workflow with Gemini CLI manually:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Survey the directory: Use a prompt to have Gemini list and categorize. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;List all files in the current directory and categorize them as "images", "videos", "documents", "archives", or "others".&lt;/code&gt;
    &lt;p&gt;Gemini might use &lt;code&gt;!ls&lt;/code&gt; or similar to get the file list, then analyze the names/extensions to produce categories.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Plan the organization: Ask Gemini how it would like to reorganize. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Propose a new folder structure for these files. I want to separate by type (Images, Videos, Documents, etc.). Also identify any files that seem like duplicates or unnecessary.&lt;/code&gt;
    &lt;p&gt;The AI might respond with a plan: e.g., "Create folders: &lt;code&gt;Images/&lt;/code&gt;, &lt;code&gt;Videos/&lt;/code&gt;, &lt;code&gt;Documents/&lt;/code&gt;, &lt;code&gt;Archives/&lt;/code&gt;. Move &lt;code&gt;X.png&lt;/code&gt;, &lt;code&gt;Y.jpg&lt;/code&gt; to &lt;code&gt;Images/&lt;/code&gt;; move &lt;code&gt;A.mp4&lt;/code&gt; to &lt;code&gt;Videos/&lt;/code&gt;; etc. The file &lt;code&gt;temp.txt&lt;/code&gt; looks unnecessary (maybe a temp file)."&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Execute moves with confirmation: You can then instruct it to carry out the plan. It may use shell commands like &lt;code&gt;mv&lt;/code&gt;for each file. Since this modifies your filesystem, you'll get confirmation prompts for each (unless you YOLO it). Carefully approve the moves. After completion, your directory will be neatly organized as suggested.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Throughout, Gemini's natural language understanding is key. It can reason, for instance, that &lt;code&gt;IMG_001.png&lt;/code&gt; is an image or that &lt;code&gt;presentation.pdf&lt;/code&gt; is a document, even if not explicitly stated. It can even open an image (using its vision capability) to see what's in it - e.g., differentiating between a screenshot vs a photo vs an icon - and name or sort it accordingly.&lt;/p&gt;
    &lt;p&gt;Renaming files by content: A particularly magical use is having Gemini rename files to be more descriptive. The Dev Community article "7 Insane Gemini CLI Tips" describes how Gemini can scan images and automatically rename them based on their content. For example, a file named &lt;code&gt;IMG_1234.jpg&lt;/code&gt; might be renamed to &lt;code&gt;login_screen.jpg&lt;/code&gt; if the AI sees it's a screenshot of a login screen. To do this, you could prompt:&lt;/p&gt;
    &lt;code&gt;For each .png image here, look at its content and rename it to something descriptive.&lt;/code&gt;
    &lt;p&gt;Gemini will open each image (via vision tool), get a description, then propose a &lt;code&gt;mv IMG_1234.png login_screen.png&lt;/code&gt; action. This can dramatically improve the organization of assets, especially in design or photo folders.&lt;/p&gt;
    &lt;p&gt;Two-pass approach: The Janitor AI discussion noted a two-step process: first broad categorization (important vs junk vs other), then refining groups. You can emulate this: first separate files that likely can be deleted (maybe large installer &lt;code&gt;.dmg&lt;/code&gt; files or duplicates) from those to keep. Then focus on organizing the keepers. Always double-check what the AI flags as junk; its guess might not always be right, so manual oversight is needed.&lt;/p&gt;
    &lt;p&gt;Safety tip: When letting the AI loose on file moves or deletions, have backups or at least be ready to undo (with &lt;code&gt;/restore&lt;/code&gt; or your own backup). It's wise to do a dry-run: ask Gemini to print the commands it would run to organize, without executing them, so you can review. For instance: "List the &lt;code&gt;mv&lt;/code&gt; and &lt;code&gt;mkdir&lt;/code&gt; commands needed for this plan, but don't execute them yet." Once you review the list, you can either copy-paste execute them, or instruct Gemini to proceed.&lt;/p&gt;
    &lt;p&gt;This is a prime example of using Gemini CLI for "non-obvious" tasks - it's not just writing code, it's doing system housekeeping with AI smarts. It can save time and bring a bit of order to chaos. After all, as developers we accumulate clutter (logs, old scripts, downloads), and an AI janitor can be quite handy.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If you've been chatting with Gemini CLI for a long time, you might hit the model's context length limit or just find the session getting unwieldy. Use the &lt;code&gt;/compress&lt;/code&gt; command to summarize the conversation so far, replacing the full history with a concise summary. This frees up space for more discussion without starting from scratch.&lt;/p&gt;
    &lt;p&gt;Large language models have a fixed context window (Gemini 2.5 Pro's is very large, but not infinite). If you exceed it, the model may start forgetting earlier messages or lose coherence. The &lt;code&gt;/compress&lt;/code&gt; feature is essentially an AI-generated tl;dr of your session that keeps important points.&lt;/p&gt;
    &lt;p&gt;How it works: When you type &lt;code&gt;/compress&lt;/code&gt;, Gemini CLI will take the entire conversation (except system context) and produce a summary. It then replaces the chat history with that summary as a single system or assistant message, preserving essential details but dropping minute-by-minute dialogue. It will indicate that compression happened. For example, after &lt;code&gt;/compress&lt;/code&gt;, you might see something like:&lt;/p&gt;
    &lt;p&gt;--- Conversation compressed ---&lt;lb/&gt; Summary of discussion: The user and assistant have been debugging a memory leak in an application. Key points: The issue is likely in &lt;code&gt;DataProcessor.js&lt;/code&gt;, where objects aren't being freed. The assistant suggested adding logging and identified a possible infinite loop. The user is about to test a fix.&lt;lb/&gt; --- End of summary ---&lt;/p&gt;
    &lt;p&gt;From that point on, the model only has that summary (plus new messages) as context for what happened before. This usually is enough if the summary captured the salient info.&lt;/p&gt;
    &lt;p&gt;When to compress: Ideally before you hit the limit. If you notice the session is getting lengthy (several hundred turns or a lot of code in context), compress proactively. The cheat sheet mentions an automatic compression setting (e.g., compress when context exceeds 60% of max). If you enable that, Gemini might auto-compress and let you know. Otherwise, manual &lt;code&gt;/compress&lt;/code&gt; is in your toolkit.&lt;/p&gt;
    &lt;p&gt;After compressing: You can continue the conversation normally. If needed, you can compress multiple times in a very long session. Each time, you lose some granularity, so don't compress too frequently for no reason - you might end up with an overly brief remembrance of a complex discussion. But generally the model's own summarization is pretty good at keeping the key facts (and you can always restate anything critical yourself).&lt;/p&gt;
    &lt;p&gt;Context window example: Let's illustrate. Suppose you fed in a large codebase by referencing many files and had a 1M token context (the max). If you then want to shift to a different part of the project, rather than starting a new session (losing all that understanding), you could compress. The summary will condense the knowledge gleaned from the code (like "We loaded modules A, B, C. A has these functions... B interacts with C in these ways..."). Now you can proceed to ask about new things with that knowledge retained abstractly.&lt;/p&gt;
    &lt;p&gt;Memory vs Compression: Note that compression doesn't save to long-term memory, it's local to the conversation. If you have facts you never want lost, consider Tip 4 (adding to &lt;code&gt;/memory&lt;/code&gt;) - because memory entries will survive compression (they'll just be reinserted anyway since they are in &lt;code&gt;GEMINI.md&lt;/code&gt; context). Compression is more about ephemeral chat content.&lt;/p&gt;
    &lt;p&gt;A minor caution: after compression, the AI's style might slightly change because it's effectively seeing a "fresh" conversation with a summary. It might reintroduce itself or change tone. You can instruct it like "Continue from here... (we compressed)" to smooth it out. In practice, it often continues fine.&lt;/p&gt;
    &lt;p&gt;To summarize (pun intended), use &lt;code&gt;/compress&lt;/code&gt; as your session grows long to maintain performance and relevance. It helps Gemini CLI focus on the bigger picture instead of every detail of the conversation's history. This way, you can have marathon debugging sessions or extensive design discussions without running out of the "mental paper" the AI is writing on.&lt;/p&gt;
    &lt;p&gt;Quick use-case: At any point in a Gemini CLI session, you can run actual shell commands by prefixing them with &lt;code&gt;!&lt;/code&gt;. For example, if you want to check the git status, just type &lt;code&gt;!git status&lt;/code&gt; and it will execute in your terminal. This saves you from switching windows or context - you're still in the Gemini CLI, but you're essentially telling it "let me run this command real quick."&lt;/p&gt;
    &lt;p&gt;This tip is about Shell Mode in Gemini CLI. There are two ways to use it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single command: Just put &lt;code&gt;!&lt;/code&gt;at the start of your prompt, followed by any command and arguments. This will execute that command in the current working directory and display the output in-line. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;!ls -lh src/&lt;/code&gt;
    &lt;p&gt;will list the files in the &lt;code&gt;src&lt;/code&gt; directory, outputting something like you'd see in a normal terminal. After the output, the Gemini prompt returns so you can continue chatting or issue more commands.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Persistent shell mode: If you enter &lt;code&gt;!&lt;/code&gt;alone and hit Enter, Gemini CLI switches into a sub-mode where you get a shell prompt (often it looks like&lt;code&gt;shell&amp;gt;&lt;/code&gt;or similar. Now you can type multiple shell commands interactively. It's basically a mini-shell within the CLI. You exit this mode by typing&lt;code&gt;!&lt;/code&gt;on an empty line again (or&lt;code&gt;exit&lt;/code&gt;). For instance:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;!
shell&amp;gt; pwd
/home/alice/project
shell&amp;gt; python --version
Python 3.x.x
shell&amp;gt; !&lt;/code&gt;
    &lt;p&gt;After the final &lt;code&gt;!&lt;/code&gt;, you're back to the normal Gemini prompt.&lt;/p&gt;
    &lt;p&gt;Why is this useful? Because development is a mix of actions and inquiries. You might be discussing something with the AI and realize you need to compile the code or run tests to see something. Instead of leaving the conversation, you can quickly do it and feed the result back into the chat. In fact, Gemini CLI often does this for you as part of its tool usage (it might automatically run &lt;code&gt;!pytest&lt;/code&gt; when you ask to fix tests, for example). But as the user, you have full control to do it manually too.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;After Gemini suggests a fix in code, you can do&lt;/p&gt;&lt;code&gt;!npm run build&lt;/code&gt;to see if it compiles, then copy any errors and ask Gemini to help with those.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you want to open a file in&lt;/p&gt;&lt;code&gt;vim&lt;/code&gt;or&lt;code&gt;nano&lt;/code&gt;, you could even launch it via&lt;code&gt;!nano filename&lt;/code&gt;(though note that since Gemini CLI has its own interface, using an interactive editor inside it might be a bit awkward - better to use the built-in editor integration or copy to your editor).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can use shell commands to gather info for the AI: e.g.,&lt;/p&gt;&lt;code&gt;!grep TODO -R .&lt;/code&gt;to find all TODOs in the project, then you might ask Gemini to help address those TODOs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Or simply use it for environment tasks:&lt;/p&gt;&lt;code&gt;!pip install some-package&lt;/code&gt;if needed, etc., without leaving the CLI.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Seamless interplay: One cool aspect is how the conversation can refer to outputs. For example, you could do &lt;code&gt;!curl http://example.com&lt;/code&gt; to fetch some data, see the output, then immediately say to Gemini, "Format the above output as JSON" - since the output was printed in the chat, the AI has it in context to work with (provided it's not too large).&lt;/p&gt;
    &lt;p&gt;Terminal as a default shell: If you find yourself always prefacing commands with &lt;code&gt;!&lt;/code&gt;, you can actually make the shell mode persistent by default. One way is launching Gemini CLI with a specific tool mode (there's a concept of default tool). But easier: just drop into shell mode (&lt;code&gt;!&lt;/code&gt; with nothing) at session start if you plan to run a lot of manual commands and only occasionally talk to AI. Then you can exit shell mode whenever you want to ask a question. It's almost like turning Gemini CLI into your normal terminal that happens to have an AI readily available.&lt;/p&gt;
    &lt;p&gt;Integration with AI planning: Sometimes Gemini CLI itself will propose to run a shell command. If you approve, it effectively does the same as &lt;code&gt;!command&lt;/code&gt;. Understanding that, you know you can always intervene. If Gemini is stuck or you want to try something, you don't have to wait for it to suggest - you can just do it and then continue.&lt;/p&gt;
    &lt;p&gt;In summary, the &lt;code&gt;!&lt;/code&gt; passthrough means you don't have to leave Gemini CLI for shell tasks. It collapses the boundary between chatting with the AI and executing commands on your system. As a pro user, this is fantastic for efficiency - your AI and your terminal become one continuous environment.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Realize that Gemini CLI can leverage any command-line tool installed on your system as part of its problem-solving. The AI has access to the shell, so if you have &lt;code&gt;cURL&lt;/code&gt;, &lt;code&gt;ImageMagick&lt;/code&gt;, &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;Docker&lt;/code&gt;, or any other tool, Gemini can invoke it when appropriate. In other words, your entire &lt;code&gt;$PATH&lt;/code&gt; is the AI's toolkit. This greatly expands what it can do - far beyond its built-in tools.&lt;/p&gt;
    &lt;p&gt;For example, say you ask: "Convert all PNG images in this folder to WebP format." If you have ImageMagick's &lt;code&gt;convert&lt;/code&gt; utility installed, Gemini CLI might plan something like: use a shell loop with &lt;code&gt;convert&lt;/code&gt; command for each file. Indeed, one of the earlier examples from a blog showed exactly this, where the user prompted to batch-convert images, and Gemini executed a shell one-liner with the &lt;code&gt;convert&lt;/code&gt; tool.&lt;/p&gt;
    &lt;p&gt;Another scenario: "Deploy my app to Docker." If &lt;code&gt;Docker CLI&lt;/code&gt; is present, the AI could call &lt;code&gt;docker build&lt;/code&gt; and &lt;code&gt;docker run&lt;/code&gt; steps as needed. Or "Use FFmpeg to extract audio from &lt;code&gt;video.mp4&lt;/code&gt;" - it can construct the &lt;code&gt;ffmpeg&lt;/code&gt; command.&lt;/p&gt;
    &lt;p&gt;This tip is about mindset: Gemini isn't limited to what's coded into it (which is already extensive). It can figure out how to use other programs available to achieve a goal. It knows common syntax and can read help texts if needed (it could call &lt;code&gt;--help&lt;/code&gt; on a tool). The only limitation is safety: by default, it will ask confirmation for any &lt;code&gt;run_shell_command&lt;/code&gt; it comes up with. But as you become comfortable, you might allow certain benign commands automatically (see YOLO or allowed-tools config).&lt;/p&gt;
    &lt;p&gt;Be mindful of the environment: "With great power comes great responsibility." Since every shell tool is fair game, you should ensure that your &lt;code&gt;$PATH&lt;/code&gt; doesn't include anything you wouldn't want the AI to run inadvertently. This is where Tip 19 (custom PATH) comes in - some users create a restricted &lt;code&gt;$PATH&lt;/code&gt; for Gemini, so it can't, say, directly call system destructive commands or maybe not call &lt;code&gt;gemini&lt;/code&gt; recursively (to avoid loops). The point is, by default if &lt;code&gt;gcc&lt;/code&gt; or &lt;code&gt;terraform&lt;/code&gt; or anything is in &lt;code&gt;$PATH&lt;/code&gt;, Gemini could invoke it. It doesn't mean it will randomly do so - only if the task calls for it - but it's possible.&lt;/p&gt;
    &lt;p&gt;Train of thought example: Imagine you ask Gemini CLI: "Set up a basic HTTP server that serves the current directory." The AI might think: "I can use Python's built-in server for this." It then issues &lt;code&gt;!python3 -m http.server 8000&lt;/code&gt;. Now it just used a system tool (Python) to launch a server. That's an innocuous example. Another: "Check the memory usage on this Linux system." The AI might use the &lt;code&gt;free -h&lt;/code&gt; command or read from &lt;code&gt;/proc/meminfo&lt;/code&gt;. It's effectively doing what a sysadmin would do, by using available commands.&lt;/p&gt;
    &lt;p&gt;All tools are extensions of the AI: This is somewhat futuristic, but consider that any command-line program can be seen as a "function" the AI can call to extend its capability. Need to solve a math problem? It could call &lt;code&gt;bc&lt;/code&gt; (calculator). Need to manipulate an image? It could call an image processing tool. Need to query a database? If the CLI client is installed and credentials are there, it can use it. The possibilities are expansive. In other AI agent frameworks, this is known as tool use, and Gemini CLI is designed with a lot of trust in its agent to decide the right tool.&lt;/p&gt;
    &lt;p&gt;When it goes wrong: The flip side is if the AI misunderstands a tool or has a hallucination about one. It might try to call a command that doesn't exist, or use wrong flags, resulting in errors. This isn't a big deal - you'll see the error and can correct or clarify. In fact, the system prompt of Gemini CLI likely guides it to first do a dry-run (just propose the command) rather than executing blindly. So you often get a chance to catch these. Over time, the developers are improving the tool selection logic to reduce these missteps.&lt;/p&gt;
    &lt;p&gt;The main takeaway is to think of Gemini CLI as having a very large Swiss Army knife - not just the built-in blades, but every tool in your OS. You don't have to instruct it on how to use them if it's something standard; usually it knows or can find out. This significantly amplifies what you can accomplish. It's like having a junior dev or devops engineer who knows how to run pretty much any program you have installed.&lt;/p&gt;
    &lt;p&gt;As a pro user, you can even install additional CLI tools specifically to give Gemini more powers. For example, if you install a CLI for a cloud service (AWS CLI, GCloud CLI, etc.), in theory Gemini can utilize it to manage cloud resources if prompted to. Always ensure you understand and trust the commands run, especially with powerful tools (you wouldn't want it spinning up huge cloud instances accidentally). But used wisely, this concept - everything is a Gemini tool - is what makes it exponentially more capable as you integrate it into your environment.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Gemini CLI isn't limited to text - it's multimodal. This means it can analyze images, diagrams, or even PDFs if given. Use this to your advantage. For instance, you could say "Here's a screenshot of an error dialog, &lt;code&gt;@./error.png&lt;/code&gt; - help me troubleshoot this." The AI will "see" the image and respond accordingly.&lt;/p&gt;
    &lt;p&gt;One of the standout features of Google's Gemini model (and its precursor PaLM2 in Codey form) is image understanding. In Gemini CLI, if you reference an image with &lt;code&gt;@&lt;/code&gt;, the model receives the image data. It can output descriptions, classifications, or reason about the image's content. We already discussed renaming images by content (Tip 14) and describing screenshots (Tip 7). But let's consider other creative uses:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;UI/UX feedback: If you're a developer working with designers, you can drop a UI image and ask Gemini for feedback or to generate code. "Look at this UI mockup&lt;/p&gt;&lt;code&gt;@mockup.png&lt;/code&gt;and produce a React component structure for it." It could identify elements in the image (header, buttons, etc.) and outline code.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Organizing images: Beyond renaming, you might have a folder of mixed images and want to sort by content. "Sort the images in&lt;/p&gt;&lt;code&gt;./photos/&lt;/code&gt;into subfolders by theme (e.g., sunsets, mountains, people)." The AI can look at each photo and categorize it (this is similar to what some photo apps do with AI - now you can do it with your own script via Gemini).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;OCR and data extraction: If you have a screenshot of error text or a photo of a document, Gemini can often read the text from it. For example, "Extract the text from&lt;/p&gt;&lt;code&gt;invoice.png&lt;/code&gt;and put it into a structured format." As shown in a Google Cloud blog example, Gemini CLI can process a set of invoice images and output a table of their info. It basically did OCR + understanding to get invoice numbers, dates, amounts from pictures of invoices. That's an advanced use-case but entirely possible with the multimodal model under the hood.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Understanding graphs or charts: If you have a graph screenshot, you could ask "Explain this chart's key insights&lt;/p&gt;&lt;code&gt;@chart.png&lt;/code&gt;." It might interpret the axes and trends. Accuracy can vary, but it's a nifty try.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make this practical: when you &lt;code&gt;@image.png&lt;/code&gt;, ensure the image isn't too huge (though the model can handle reasonably large images). The CLI will likely encode it and send it to the model. The response might include descriptions or further actions. You can mix text and image references in one prompt too.&lt;/p&gt;
    &lt;p&gt;Non-image modalities: The CLI and model potentially can handle PDFs and audio too, by converting them via tools. For example, if you &lt;code&gt;@report.pdf&lt;/code&gt;, Gemini CLI might use a PDF-to-text tool under the hood to extract text and then summarize. If you &lt;code&gt;@audio.mp3&lt;/code&gt; and ask for a transcript, it might use an audio-to-text tool (like a speech recognition function). The cheat sheet suggests referencing PDFs, audio, video files is supported, presumably by invoking appropriate internal tools or APIs. So, "transcribe this interview audio: &lt;code&gt;@interview.wav&lt;/code&gt;" could actually work (if not now, likely soon, since underlying Google APIs for speech-to-text could be plugged in).&lt;/p&gt;
    &lt;p&gt;Rich outputs: Multimodal also means the AI can return images in responses if integrated (though in CLI it usually won't display them directly, but it could save an image file or output ASCII art, etc.). The MCP capability mentioned that tools can return images. For instance, an AI drawing tool could generate an image and Gemini CLI could present it (maybe by opening it or giving a link).&lt;/p&gt;
    &lt;p&gt;Important: The CLI itself is text-based, so you won't see the image in the terminal (unless it's capable of ASCII previews). You'll just get the analysis. So this is mostly about reading images, not displaying them. If you're in VS Code integration, it might show images in the chat view.&lt;/p&gt;
    &lt;p&gt;In summary, don't forget the "I" in GUI when using Gemini CLI - it can handle the visual just as well as the textual in many cases. This opens up workflows like visual debugging, design help, data extraction from screenshots, etc., all under the same tool. It's a differentiator that some other CLI tools may not have yet. And as models improve, this multimodal support will only get more powerful, so it's a future-proof skill to exploit.&lt;/p&gt;
    &lt;p&gt;Quick use-case: If you ever find Gemini CLI getting confused or invoking the wrong programs, consider running it with a tailored &lt;code&gt;$PATH&lt;/code&gt;. By limiting or ordering the available executables, you can prevent the AI from, say, calling a similarly named script that you didn't intend. Essentially, you sandbox its tool access to known-good tools.&lt;/p&gt;
    &lt;p&gt;For most users, this isn't an issue, but for pro users with lots of custom scripts or multiple versions of tools, it can be helpful. One reason mentioned by the developers is avoiding infinite loops or weird behavior. For example, if &lt;code&gt;gemini&lt;/code&gt; itself is in &lt;code&gt;$PATH&lt;/code&gt;, an AI gone awry might recursively call &lt;code&gt;gemini&lt;/code&gt; from within Gemini (a strange scenario, but theoretically possible). Or perhaps you have a command named &lt;code&gt;test&lt;/code&gt; that conflicts with something - the AI might call the wrong one.&lt;/p&gt;
    &lt;p&gt;How to set PATH for Gemini: Easiest is inline on launch:&lt;/p&gt;
    &lt;code&gt;PATH=/usr/bin:/usr/local/bin gemini&lt;/code&gt;
    &lt;p&gt;This runs Gemini CLI with a restricted &lt;code&gt;$PATH&lt;/code&gt; of just those directories. You might exclude directories where experimental or dangerous scripts lie. Alternatively, create a small shell script wrapper that purges or adjusts &lt;code&gt;$PATH&lt;/code&gt; then exec's &lt;code&gt;gemini&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Another approach is using environment or config to explicitly disable certain tools. For instance, if you absolutely never want the AI to use &lt;code&gt;rm&lt;/code&gt; or some destructive tool, you could technically create an alias or dummy &lt;code&gt;rm&lt;/code&gt; in a safe &lt;code&gt;$PATH&lt;/code&gt; that does nothing (though this could interfere with normal operations, so maybe not that one). A better method is the exclude list in settings. In an extension or &lt;code&gt;settings.json&lt;/code&gt;, you can exclude tool names. E.g.,&lt;/p&gt;
    &lt;code&gt;"excludeTools": ["run_shell_command"]&lt;/code&gt;
    &lt;p&gt;This extreme example would stop all shell commands from running (making Gemini effectively read-only). More granular, there was mention of skipping confirmation for some; similarly you might configure something like:&lt;/p&gt;
    &lt;code&gt;"tools": {
  "exclude": ["apt-get", "shutdown"]
}&lt;/code&gt;
    &lt;p&gt;(This syntax is illustrative; consult docs for exact usage.)&lt;/p&gt;
    &lt;p&gt;The principle is, by controlling the environment, you reduce risk of the AI doing something dumb with a tool it shouldn't. It's akin to child-proofing the house.&lt;/p&gt;
    &lt;p&gt;Prevent infinite loops: One user scenario was a loop where Gemini kept reading its own output or re-reading files repeatedly. Custom &lt;code&gt;$PATH&lt;/code&gt; can't directly fix logic loops, but one cause could be if the AI calls a command that triggers itself. Ensuring it can't accidentally spawn another AI instance (like calling &lt;code&gt;bard&lt;/code&gt; or &lt;code&gt;gemini&lt;/code&gt; command, if it thought to do so) is good. Removing those from &lt;code&gt;$PATH&lt;/code&gt; (or renaming them for that session) helps.&lt;/p&gt;
    &lt;p&gt;Isolation via sandbox: Another alternative to messing with &lt;code&gt;$PATH&lt;/code&gt; is using &lt;code&gt;--sandbox&lt;/code&gt; mode (which uses Docker or Podman to run tools in an isolated environment). In that case, the AI's actions are contained and have only the tools that sandbox image provides. You could supply a Docker image with a curated set of tools. This is heavy-handed but very safe.&lt;/p&gt;
    &lt;p&gt;Custom PATH for specific tasks: You might have different &lt;code&gt;$PATH&lt;/code&gt; setups for different projects. For example, in one project you want it to use a specific version of Node or a local toolchain. Launching &lt;code&gt;gemini&lt;/code&gt; with the &lt;code&gt;$PATH&lt;/code&gt; that points to those versions will ensure the AI uses the right one. Essentially, treat Gemini CLI like any user - it uses whatever environment you give it. So if you need it to pick &lt;code&gt;gcc-10&lt;/code&gt; vs &lt;code&gt;gcc-12&lt;/code&gt;, adjust &lt;code&gt;$PATH&lt;/code&gt; or &lt;code&gt;CC&lt;/code&gt; env var accordingly.&lt;/p&gt;
    &lt;p&gt;In summary: Guard rails. As a power user, you have the ability to fine-tune the operating conditions of the AI. If you ever find a pattern of undesirable behavior tied to tool usage, tweaking &lt;code&gt;$PATH&lt;/code&gt; is a quick remedy. For everyday use, you likely won't need this, but it's a pro tip to keep in mind if you integrate Gemini CLI into automation or CI: give it a controlled environment. That way, you know exactly what it can and cannot do, which increases reliability.&lt;/p&gt;
    &lt;p&gt;If you run long chats or repeatedly attach the same big files, you can cut cost and latency by turning on token caching and monitoring usage. With an API key or Vertex AI auth, Gemini CLI automatically reuses previously sent system instructions and context, so follow‚Äëup requests are cheaper. You can see the savings live in the CLI.&lt;/p&gt;
    &lt;p&gt;How to use it&lt;/p&gt;
    &lt;p&gt;Use an auth mode that enables caching. Token caching is available when you authenticate with a Gemini API key or Vertex AI. It is not available with OAuth login today. Google Gemini&lt;/p&gt;
    &lt;p&gt;Inspect your usage and cache hits. Run the &lt;code&gt;stats&lt;/code&gt; command during a session. It shows total tokens and a &lt;code&gt;cached&lt;/code&gt; field when caching is active.&lt;/p&gt;
    &lt;code&gt;/stats&lt;/code&gt;
    &lt;p&gt;The command's description and cached reporting behavior are documented in the commands reference and FAQ. Google Gemini+1&lt;/p&gt;
    &lt;p&gt;Capture metrics in scripts. When running headless, output JSON and parse the &lt;code&gt;stats&lt;/code&gt; block, which includes &lt;code&gt;tokens.cached&lt;/code&gt; for each model:&lt;/p&gt;
    &lt;code&gt;gemini -p "Summarize README" --output-format json&lt;/code&gt;
    &lt;p&gt;The headless guide documents the JSON schema with cached token counts. Google Gemini&lt;/p&gt;
    &lt;p&gt;Save a session summary to file: For CI or budget tracking, write a JSON session summary to disk.&lt;/p&gt;
    &lt;code&gt;gemini -p "Analyze logs" --session-summary usage.json&lt;/code&gt;
    &lt;p&gt;This flag is listed in the changelog. Google Gemini&lt;/p&gt;
    &lt;p&gt;With API key or Vertex auth, the CLI automatically reuses previously sent context so later turns send fewer tokens. Keeping &lt;code&gt;GEMINI.md&lt;/code&gt; and large file references stable across turns increases cache hits; you'll see that reflected in stats as cached tokens.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Instantly copy the latest answer or code snippet from Gemini CLI to your system clipboard, without any extraneous formatting or line numbers. This is perfect for quickly pasting AI-generated code into your editor or sharing a result with a teammate.&lt;/p&gt;
    &lt;p&gt;When Gemini CLI provides an answer (especially a multi-line code block), you often want to reuse it elsewhere. The &lt;code&gt;/copy&lt;/code&gt; slash command makes this effortless by copying the last output produced by the CLI directly to your clipboard. Unlike manual selection (which can grab line numbers or prompt text), &lt;code&gt;/copy&lt;/code&gt; grabs only the raw response content. For example, if Gemini just generated a 50-line Python script, simply typing &lt;code&gt;/copy&lt;/code&gt; will put that entire script into your clipboard, ready to paste - no need to scroll and select text. Under the hood, Gemini CLI uses the appropriate clipboard utility for your platform (e.g. &lt;code&gt;pbcopy&lt;/code&gt; on macOS, &lt;code&gt;clip&lt;/code&gt; on Windows. Once you run the command, you'll typically see a confirmation message, and then you can paste the copied text wherever you need it.&lt;/p&gt;
    &lt;p&gt;How it works: The &lt;code&gt;/copy&lt;/code&gt; command requires that your system has a clipboard tool available. On macOS and Windows, the required tools (&lt;code&gt;pbcopy&lt;/code&gt; and &lt;code&gt;clip&lt;/code&gt; respectively) are usually pre-installed. On Linux, you may need to install &lt;code&gt;xclip&lt;/code&gt; or &lt;code&gt;xsel&lt;/code&gt; for &lt;code&gt;/copy&lt;/code&gt; to function. After ensuring that, you can use &lt;code&gt;/copy&lt;/code&gt; anytime after Gemini CLI prints an answer. It will capture the entire last response (even if it's long) and omit any internal numbering or formatting the CLI may show on-screen. This saves you from dealing with unwanted artifacts when transferring the content. It's a small feature, but a huge time-saver when you're iterating on code or compiling a report generated by the AI.&lt;/p&gt;
    &lt;p&gt;Pro Tip: If you find the &lt;code&gt;/copy&lt;/code&gt; command isn't working, double-check that your clipboard utilities are installed and accessible. For instance, Ubuntu users should run &lt;code&gt;sudo apt install xclip&lt;/code&gt; to enable clipboard copying. Once set up, &lt;code&gt;/copy&lt;/code&gt; lets you share Gemini's outputs with zero friction - copy, paste, and you're done.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Cleanly interrupt Gemini CLI or exit shell mode with a single keypress - and quit the CLI entirely with a quick double-tap - thanks to the versatile Ctrl+C shortcut. This gives you immediate control when you need to stop or exit.&lt;/p&gt;
    &lt;p&gt;Gemini CLI operates like a REPL, and knowing how to break out of operations is essential. Pressing Ctrl+C once will cancel the current action or clear any input you've started typing, essentially acting as an "abort" command. For example, if the AI is generating a lengthy answer and you've seen enough, hit &lt;code&gt;Ctrl+C&lt;/code&gt; - the generation stops immediately. If you had started typing a prompt but want to discard it, &lt;code&gt;Ctrl+C&lt;/code&gt; will wipe the input line so you can start fresh. Additionally, if you are in shell mode (activated by typing &lt;code&gt;!&lt;/code&gt; to run shell commands), a single &lt;code&gt;Ctrl+C&lt;/code&gt; will exit shell mode and return you to the normal Gemini prompt (it sends an interrupt to the shell process running. This is extremely handy if a shell command is hanging or you simply want to get back to AI mode.&lt;/p&gt;
    &lt;p&gt;Pressing Ctrl+C twice in a row is the shortcut to exit Gemini CLI entirely. Think of it as "&lt;code&gt;Ctrl+C&lt;/code&gt; to cancel, and &lt;code&gt;Ctrl+C&lt;/code&gt; again to quit." This double-tap signals the CLI to terminate the session (you'll see a goodbye message or the program will close). It's a faster alternative to typing &lt;code&gt;/quit&lt;/code&gt; or closing the terminal window, allowing you to gracefully shut down the CLI from the keyboard. Do note that a single &lt;code&gt;Ctrl+C&lt;/code&gt; will not quit if there's input to clear or an operation to interrupt - it requires that second press (when the prompt is idle) to fully exit. This design prevents accidentally closing the session when you only meant to stop the current output.&lt;/p&gt;
    &lt;p&gt;Pro Tip: In shell mode, you can also press the Esc key to leave shell mode and return to Gemini's chat mode without terminating the CLI. And if you prefer a more formal exit, the &lt;code&gt;/quit&lt;/code&gt; command is always available to cleanly end the session. Lastly, Unix users can use Ctrl+D (EOF) at an empty prompt to exit as well - Gemini CLI will prompt for confirmation if needed. But for most cases, mastering the single- and double-tap of &lt;code&gt;Ctrl+C&lt;/code&gt; is the quickest way to stay in control.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Adapt the CLI's behavior and appearance to your preferences or project conventions by editing the &lt;code&gt;settings.json&lt;/code&gt; config file, instead of sticking with one-size-fits-all defaults. This lets you enforce things like theme, tool usage rules, or editor mode across all your sessions.&lt;/p&gt;
    &lt;p&gt;Gemini CLI is highly configurable. In your home directory (&lt;code&gt;~/.gemini/&lt;/code&gt;) or project folder (&lt;code&gt;.gemini/&lt;/code&gt; within your repo), you can create a &lt;code&gt;settings.json&lt;/code&gt; file to override default settings. Nearly every aspect of the CLI can be tuned here - from visual theme to tool permissions. The CLI merges settings from multiple levels: system-wide defaults, your user settings, and project-specific settings (project settings override user settings. For example, you might have a global preference for a dark theme, but a particular project might require stricter tool sandboxing; you can handle this via different &lt;code&gt;settings.json&lt;/code&gt; files at each level.&lt;/p&gt;
    &lt;p&gt;Inside &lt;code&gt;settings.json&lt;/code&gt;, options are specified as JSON key-value pairs. Here's a snippet illustrating some useful customizations:&lt;/p&gt;
    &lt;code&gt;{
"theme": "GitHub",
"autoAccept": false,
"vimMode": true,
"sandbox": "docker",
"includeDirectories": ["../shared-library", "~/common-utils"],
"usageStatisticsEnabled": true
}&lt;/code&gt;
    &lt;p&gt;In this example, we set the theme to "GitHub" (a popular color scheme), disable &lt;code&gt;autoAccept&lt;/code&gt; (so the CLI will always ask before running potentially altering tools), enable Vim keybindings for the input editor, and enforce using Docker for tool sandboxing. We also added some directories to the workspace context (&lt;code&gt;includeDirectories&lt;/code&gt;) so Gemini can see code in shared paths by default. Finally, we kept &lt;code&gt;usageStatisticsEnabled&lt;/code&gt; true to collect basic usage stats (which feeds into telemetry, if enabled. There are many more settings available - like defining custom color themes, adjusting token limits, or whitelisting/blacklisting specific tools - all documented in the configuration guide. By tailoring these, you ensure Gemini CLI behaves optimally for your workflow (for instance, some developers always want &lt;code&gt;vimMode&lt;/code&gt; on for efficiency, while others might prefer the default editor).&lt;/p&gt;
    &lt;p&gt;One convenient way to edit settings is via the built-in settings UI. Run the command &lt;code&gt;/settings&lt;/code&gt; in Gemini CLI, and it will open an interactive editor for your configuration. This interface lets you browse and search settings with descriptions, and prevents JSON syntax errors by validating inputs. You can tweak colors, toggle features like &lt;code&gt;yolo&lt;/code&gt; (auto-approval), adjust checkpointing (file save/restore behavior), and more through a friendly menu. Changes are saved to your &lt;code&gt;settings.json&lt;/code&gt;, and some take effect immediately (others might require restarting the CLI).&lt;/p&gt;
    &lt;p&gt;Pro Tip: Maintain separate project-specific &lt;code&gt;settings.json&lt;/code&gt; files for different needs. For example, on a team project you might set &lt;code&gt;"sandbox": "docker"&lt;/code&gt; and &lt;code&gt;"excludeTools": ["run_shell_command"]&lt;/code&gt; to lock down dangerous operations, while your personal projects might allow direct shell commands. Gemini CLI will automatically pick up the nearest &lt;code&gt;.gemini/settings.json&lt;/code&gt; in your project directory tree and merge it with your global &lt;code&gt;~/.gemini/settings.json&lt;/code&gt;. Also, don't forget you can quickly adjust visual preferences: try &lt;code&gt;/theme&lt;/code&gt; to interactively switch themes without editing the file, which is great for finding a comfortable look. Once you find one, put it in &lt;code&gt;settings.json&lt;/code&gt; to make it permanent.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Supercharge Gemini CLI by hooking it into VS Code - the CLI will automatically know which files you're working on and even open AI-proposed code changes in VS Code's diff editor for you. This creates a seamless loop between AI assistant and your coding workspace.&lt;/p&gt;
    &lt;p&gt;One of Gemini CLI's powerful features is its IDE integration with Visual Studio Code. By installing the official Gemini CLI Companion extension in VS Code and connecting it, you allow Gemini CLI to become "context-aware" of your editor. What does this mean in practice? When connected, Gemini knows about the files you have open, your current cursor location, and any text you've selected in VS Code. All that information is fed into the AI's context. So if you ask, "Explain this function," Gemini CLI can see the exact function you've highlighted and give a relevant answer, without you needing to copy-paste code into the prompt. The integration shares up to your 10 most recently opened files, plus selection and cursor info, giving the model a rich understanding of your workspace.&lt;/p&gt;
    &lt;p&gt;Another huge benefit is native diffing of code changes. When Gemini CLI suggests modifications to your code (for example, "refactor this function" and it produces a patch), it can open those changes in VS Code's diff viewer automatically. You'll see a side-by-side diff in VS Code showing the proposed edits. You can then use VS Code's familiar interface to review the changes, make any manual tweaks, and even accept the patch with a click. The CLI and editor stay in sync - if you accept the diff in VS Code, Gemini CLI knows and continues the session with those changes applied. This tight loop means you no longer have to copy code from the terminal to your editor; the AI's suggestions flow straight into your development environment.&lt;/p&gt;
    &lt;p&gt;How to set it up: If you start Gemini CLI inside VS Code's integrated terminal, it will detect VS Code and usually prompt you to install/connect the extension automatically. You can agree and it will run the necessary &lt;code&gt;/ide install&lt;/code&gt; step. If you don't see a prompt (or you're enabling it later), simply open Gemini CLI and run the command: &lt;code&gt;/ide install&lt;/code&gt;. This will fetch and install the "Gemini CLI Companion" extension into VS Code for you. Next, run &lt;code&gt;/ide enable&lt;/code&gt; to establish the connection - the CLI will then indicate it's linked to VS Code. You can verify at any time with &lt;code&gt;/ide status&lt;/code&gt;, which will show if it's connected and list which editor and files are being tracked. From then on, Gemini CLI will automatically receive context from VS Code (open files, selections) and will open diffs in VS Code when needed. It essentially turns Gemini CLI into an AI pair programmer that lives in your terminal but operates with full awareness of your IDE.&lt;/p&gt;
    &lt;p&gt;Currently, VS Code is the primary supported editor for this integration. (Other editors that support VS Code extensions, like VSCodium or some JetBrains via a plugin, may work via the same extension, but officially it's VS Code for now.) The design is open though - there's an IDE Companion Spec for developing similar integrations with other editors. So down the road we might see first-class support for IDEs like IntelliJ or Vim via community extensions.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Once connected, you can use VS Code's Command Palette to control Gemini CLI without leaving the editor. For example, press Ctrl+Shift+P (Cmd+Shift+P on Mac) and try commands like "Gemini CLI: Run" (to launch a new CLI session in the terminal), "Gemini CLI: Accept Diff" (to approve and apply an open diff), or "Gemini CLI: Close Diff Editor" (to reject changes. These shortcuts can streamline your workflow even further. And remember, you don't always have to start the CLI manually - if you enable the integration, Gemini CLI essentially becomes an AI co-developer inside VS Code, watching context and ready to help as you work on code.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Put Gemini to work on GitHub - use the Gemini CLI GitHub Action to autonomously triage new issues and review pull requests in your repository, acting as an AI teammate that handles routine dev tasks.&lt;/p&gt;
    &lt;p&gt;Gemini CLI isn't just for interactive terminal sessions; it can also run in CI/CD pipelines via GitHub Actions. Google has provided a ready-made Gemini CLI GitHub Action (currently in beta) that integrates into your repo's workflows. This effectively deploys an AI agent into your project on GitHub. It runs in the background, triggered by repository events. For example, when someone opens a new issue, the Gemini Action can automatically analyze the issue description, apply relevant labels, and even prioritize it or suggest duplicates (this is the "intelligent issue triage" workflow. When a pull request is opened, the Action kicks in to provide an AI code review - it will comment on the PR with insights about code quality, potential bugs, or stylistic improvements. This gives maintainers immediate feedback on the PR before any human even looks at it. Perhaps the coolest feature is on-demand collaboration: team members can mention &lt;code&gt;@gemini-cli&lt;/code&gt; in an issue or PR comment and give it an instruction, like "&lt;code&gt;@gemini-cli&lt;/code&gt; please write unit tests for this". The Action will pick that up and Gemini CLI will attempt to fulfill the request (adding a commit with new tests, for instance. It's like having an AI assistant living in your repo, ready to do chores when asked.&lt;/p&gt;
    &lt;p&gt;Setting up the Gemini CLI GitHub Action is straightforward. First, ensure you have Gemini CLI version 0.1.18 or later installed locally (this ensures compatibility with the Action. Then, in Gemini CLI run the special command: &lt;code&gt;/setup-github&lt;/code&gt;. This command generates the necessary workflow files in your repository (it will guide you through authentication if needed). Specifically, it adds YAML workflow files (for issue triage, PR review, etc.) under &lt;code&gt;.github/workflows/&lt;/code&gt;. You will need to add your Gemini API key to the repo's secrets (as &lt;code&gt;GEMINI_API_KEY&lt;/code&gt;) so the Action can use the Gemini API. Once that's done and the workflows are committed, the GitHub Action springs to life - from that point on, Gemini CLI will autonomously respond to new issues and PRs according to those workflows.&lt;/p&gt;
    &lt;p&gt;Because this Action is essentially running Gemini CLI in an automated way, you can customize it just like you would your CLI. The default setup comes with three workflows (issue triage, PR review, and a general mention-triggered assistant) which are **fully open-source and editable**. You can tweak the YAML to adjust what the AI does, or even add new workflows. For instance, you might create a nightly workflow that uses Gemini CLI to scan your repository for outdated dependencies or to update a README based on recent code changes - the possibilities are endless. The key benefit here is offloading mundane or time-consuming tasks to an AI agent so that human developers can focus on harder problems. And since it runs on GitHub's infrastructure, it doesn't require your intervention - it's truly a "set and forget" AI helper.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Keep an eye on the Action's output in the GitHub Actions logs for transparency. The Gemini CLI Action logs will show what prompts it ran and what changes it made or suggested. This can both build trust and help you refine its behavior. Also, the team has built enterprise-grade safeguards into the Action - e.g., you can require that all shell commands the AI tries to run in a workflow are allow-listed by you. So don't hesitate to use it even on serious projects. And if you come up with a cool custom workflow using Gemini CLI, consider contributing it back to the community - the project welcomes new ideas in their repo!&lt;/p&gt;
    &lt;p&gt;Quick use-case: Gain deeper insight into how Gemini CLI is being used and performing by turning on its built-in OpenTelemetry instrumentation - monitor metrics, logs, and traces of your AI sessions to analyze usage patterns or troubleshoot issues.&lt;/p&gt;
    &lt;p&gt;For developers who like to measure and optimize, Gemini CLI offers an observability feature that exposes what's happening under the hood. By leveraging OpenTelemetry (OTEL), Gemini CLI can emit structured telemetry data about your sessions. This includes things like metrics (e.g. how many tokens used, response latency), logs of actions taken, and even traces of tool calls. With telemetry enabled, you can answer questions like: Which custom command do I use most often? How many times did the AI edit files in this project this week? What's the average response time when I ask the CLI to run tests? Such data is invaluable for understanding usage patterns and performance. Teams can use it to see how developers are interacting with the AI assistant and where bottlenecks might be.&lt;/p&gt;
    &lt;p&gt;By default, telemetry is off (Gemini respects privacy and performance). You can opt-in by setting &lt;code&gt;"telemetry.enabled": true&lt;/code&gt; in your &lt;code&gt;settings.json&lt;/code&gt; or by starting Gemini CLI with the flag &lt;code&gt;--telemetry&lt;/code&gt;. Additionally, you choose the target for the telemetry data: it can be logged locally or sent to a backend like Google Cloud. For a quick start, you might set &lt;code&gt;"telemetry.target": "local"&lt;/code&gt; - with this, Gemini will simply write telemetry data to a local file (by default) or to a custom path you specify via &lt;code&gt;["outfile"](https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=disable%20telemetry%20,file%20path)&lt;/code&gt;. The local telemetry includes JSON logs you can parse or feed into tools. For more robust monitoring, set &lt;code&gt;"target": "gcp"&lt;/code&gt; (Google Cloud) or even integrate with other OpenTelemetry-compatible systems like Jaeger or Datadog. In fact, Gemini CLI's OTEL support is vendor-neutral - you can export data to just about any observability stack you prefer (Google Cloud Operations, Prometheus, etc.. Google provides a streamlined path for Cloud: if you point to GCP, the CLI can send data directly to Cloud Logging and Cloud Monitoring in your project, where you can use the usual dashboards and alerting tools.&lt;/p&gt;
    &lt;p&gt;What kind of insights can you get? The telemetry captures events like tool executions, errors, and important milestones. It also records metrics such as prompt processing time and token counts per prompt. For usage analytics, you might aggregate how many times each slash command is used across your team, or how often code generation is invoked. For performance monitoring, you could track if responses have gotten slower, which might indicate hitting API rate limits or model changes. And for debugging, you can see errors or exceptions thrown by tools (e.g., a &lt;code&gt;run_shell_command&lt;/code&gt; failure) logged with context. All this data can be visualized if you send it to a platform like Google Cloud's Monitoring - for example, you can create a dashboard of "tokens used per day" or "error rate of tool X". It essentially gives you a window into the AI's "brain" and your usage, which is especially helpful in enterprise settings to ensure everything runs smoothly.&lt;/p&gt;
    &lt;p&gt;Enabling telemetry does introduce some overhead (extra data processing), so you might not keep it on 100% of the time for personal use. However, it's fantastic for debugging sessions or for intermittent health checks. One approach is to enable it on a CI server or in your team's shared environment to collect stats, while leaving it off locally unless needed. Remember, you can always toggle it on the fly: update settings and use &lt;code&gt;/memory refresh&lt;/code&gt; if needed to reload, or restart Gemini CLI with &lt;code&gt;--telemetry&lt;/code&gt; flag. Also, all telemetry is under your control - it respects your environment variables for endpoint and credentials, so data goes only where you intend it to. This feature turns Gemini CLI from a black box into an observatory, shining light on how the AI agent interacts with your world, so you can continuously improve that interaction.&lt;/p&gt;
    &lt;p&gt;Pro Tip: If you just want a quick view of your current session's stats (without full telemetry), use the &lt;code&gt;/stats&lt;/code&gt; command. It will output metrics like token usage and session length right in the CLI. This is a lightweight way to see immediate numbers. But for long-term or multi-session analysis, telemetry is the way to go. And if you're sending telemetry to a cloud project, consider setting up dashboards or alerts (e.g., alert if error rate spikes or token usage hits a threshold) - this can proactively catch issues in how Gemini CLI is being used in your team.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Stay informed about upcoming Gemini CLI features - by following the public Gemini CLI roadmap, you'll know about major planned enhancements (like background agents for long-running tasks) before they arrive, allowing you to plan and give feedback.&lt;/p&gt;
    &lt;p&gt;Gemini CLI is evolving rapidly, with new releases coming out frequently, so it's wise to track what's on the horizon. Google maintains a public roadmap for Gemini CLI on GitHub, detailing the key focus areas and features targeted for the near future. This is essentially a living document (and set of issues) where you can see what the developers are working on and what's in the pipeline. For instance, one exciting item on the roadmap is support for background agents - the ability to spawn autonomous agents that run in the background to handle tasks continuously or asynchronously. According to the roadmap discussion, these background agents would let you delegate long-running processes to Gemini CLI without tying up your interactive session. You could, say, start a background agent that monitors your project for certain events or periodically executes tasks, either on your local machine or even by deploying to a service like Cloud Run. This feature aims to "enable long-running, autonomous tasks and proactive assistance" right from the CLI, essentially extending Gemini CLI's usefulness beyond just on-demand queries.&lt;/p&gt;
    &lt;p&gt;By keeping tabs on the roadmap, you'll also learn about other planned features. These could include new tool integrations, support for additional Gemini model versions, UI/UX improvements, and more. The roadmap is usually organized by "areas" (for example, Extensibility, Model, Background, etc.) and often tagged with milestones (like a target quarter for delivery]. It's not a guarantee of when something will land, but it gives a good idea of the team's priorities. Since the project is open-source, you can even dive into the linked GitHub issues for each roadmap item to see design proposals and progress. For developers who rely on Gemini CLI, this transparency means you can anticipate changes - maybe an API is adding a feature you need, or a breaking change might be coming that you want to prepare for.&lt;/p&gt;
    &lt;p&gt;Following the roadmap can be as simple as bookmarking the GitHub project board or issue labeled "Roadmap" and checking periodically. Some major updates (like the introduction of Extensions or the IDE integration) were hinted at in the roadmap before they were officially announced, so you get a sneak peek. Additionally, the Gemini CLI team often encourages community feedback on those future features. If you have ideas or use cases for something like background agents, you can usually comment on the issue or discussion thread to influence its development.&lt;/p&gt;
    &lt;p&gt;Pro Tip: Since Gemini CLI is open source (Apache 2.0 licensed), you can do more than just watch the roadmap - you can participate! The maintainers welcome contributions, especially for items aligned with the roadmap. If there's a feature you really care about, consider contributing code or testing once it's in preview. At the very least, you can open a feature request if something you need isn't on the roadmap yet. The roadmap page itself provides guidance on how to propose changes. Engaging with the project not only keeps you in the loop but also lets you shape the tool that you use. After all, Gemini CLI is built with community involvement in mind, and many recent features (like certain extensions and tools) started as community suggestions.&lt;/p&gt;
    &lt;p&gt;Quick use-case: Add new capabilities to Gemini CLI by installing plug-and-play extensions - for example, integrate with your favorite database or cloud service - expanding the AI's toolset without any heavy lifting on your part. It's like installing apps for your CLI to teach it new tricks.&lt;/p&gt;
    &lt;p&gt;Extensions are a game-changer introduced in late 2025: they allow you to customize and expand Gemini CLI's functionality in a modular way. An extension is essentially a bundle of configurations (and optionally code) that connects Gemini CLI to an external tool or service. For instance, Google released a suite of extensions for Google Cloud - there's one that helps deploy apps to Cloud Run, one for managing BigQuery, one for analyzing application security, and more. Partners and community developers have built extensions for all sorts of things: Dynatrace (monitoring), Elastic (search analytics), Figma (design assets), Shopify, Snyk (security scans), Stripe (payments), and the list is growing. By installing an appropriate extension, you instantly grant Gemini CLI the ability to use new domain-specific tools. The beauty is that these extensions come with a pre-defined "playbook" that teaches the AI how to use the new tools effectively. That means once installed, you can ask Gemini CLI to perform tasks with those services and it will know the proper APIs or commands to invoke, as if it had that knowledge built-in.&lt;/p&gt;
    &lt;p&gt;Using extensions is very straightforward. The CLI has a command to manage them: &lt;code&gt;gemini extensions install &amp;lt;URL&amp;gt;&lt;/code&gt;. Typically, you provide the URL of the extension's GitHub repo or a local path, and the CLI will fetch and install it. For example, to install an official extension, you might run: &lt;code&gt;gemini extensions install https://github.com/google-gemini/gemini-cli-extension-cloud-run&lt;/code&gt;. Within seconds, the extension is added to your environment (stored under &lt;code&gt;~/.gemini/extensions/&lt;/code&gt; or your project's &lt;code&gt;.gemini/extensions/&lt;/code&gt; folder). You can then see it by running &lt;code&gt;/extensions&lt;/code&gt; in the CLI, which lists active extensions. From that point on, the AI has new tools at its disposal. If it's a Cloud Run extension, you could say "Deploy my app to Cloud Run," and Gemini CLI will actually be able to execute that (by calling the underlying &lt;code&gt;gcloud&lt;/code&gt; commands through the extension's tools). Essentially, extensions function as first-class expansions of Gemini CLI's capabilities, but you opt-in to the ones you need.&lt;/p&gt;
    &lt;p&gt;There's an open ecosystem around extensions. Google has an official Extensions page listing available extensions, and because the framework is open, anyone can create and share their own. If you have a particular internal API or workflow, you can build an extension for it so that Gemini CLI can assist with it. Writing an extension is easier than it sounds: you typically create a directory (say, &lt;code&gt;my-extension/&lt;/code&gt;) with a file &lt;code&gt;gemini-extension.json&lt;/code&gt; describing what tools or context to add. You might define new slash commands or specify remote APIs the AI can call. No need to modify Gemini CLI's core - just drop in your extension. The CLI is designed to load these at runtime. Many extensions consist of adding custom MCP tools (Model Context Protocol servers or functions) that the AI can use. For example, an extension could add a &lt;code&gt;/translate&lt;/code&gt; command by hooking into an external translation API; once installed, the AI knows how to use &lt;code&gt;/translate&lt;/code&gt;. The key benefit is modularity: you install only the extensions you want, keeping the CLI lightweight, but you have the option to integrate virtually anything.&lt;/p&gt;
    &lt;p&gt;To manage extensions, besides the &lt;code&gt;install&lt;/code&gt; command, you can update or remove them via similar CLI commands (&lt;code&gt;gemini extensions update&lt;/code&gt; or just by removing the folder). It's wise to occasionally check for updates on extensions you use, as they may receive improvements. The CLI might introduce an "extensions marketplace" style interface in the future, but for now, exploring the GitHub repositories and official catalog is the way to discover new ones. Some popular ones at launch include the GenAI Genkit extension (for building generative AI apps), and a variety of Google Cloud extensions that cover CI/CD, database admin, and more.&lt;/p&gt;
    &lt;p&gt;Pro Tip: If you're building your own extension, start by looking at existing ones for examples. The official documentation provides an Extensions Guide with the schema and capabilities. A simple way to create a private extension is to use the &lt;code&gt;@include&lt;/code&gt; functionality in &lt;code&gt;GEMINI.md&lt;/code&gt; to inject scripts or context, but a full extension gives you more power (like packaging tools). Also, since extensions can include context files, you can use them to preload domain knowledge. Imagine an extension for your company's internal API that includes a summary of the API and a tool to call it - the AI would then know how to handle requests related to that API. In short, extensions open up a new world where Gemini CLI can interface with anything. Keep an eye on the extensions marketplace for new additions, and don't hesitate to share any useful extension you create with the community - you might just help thousands of other developers.&lt;/p&gt;
    &lt;p&gt;Lastly, not a productivity tip but a delightful easter egg - try the command &lt;code&gt;*/corgi*&lt;/code&gt; in Gemini CLI. This toggles "corgi mode", which makes a cute corgi animation run across your terminal! It doesn't help you code any better, but it can certainly lighten the mood during a long coding session. You'll see an ASCII art corgi dashing in the CLI interface. To turn it off, just run &lt;code&gt;/corgi&lt;/code&gt; again.&lt;/p&gt;
    &lt;p&gt;This is a purely for-fun feature the team added (and yes, there's even a tongue-in-cheek debate about spending dev time on corgi mode). It shows that the creators hide some whimsy in the tool. So when you need a quick break or a smile, give &lt;code&gt;/corgi&lt;/code&gt; a try. üêïüéâ&lt;/p&gt;
    &lt;p&gt;(Rumor has it there might be other easter eggs or modes - who knows? Perhaps a "/partyparrot" or similar. The cheat sheet or help command lists &lt;code&gt;/corgi&lt;/code&gt;, so it's not a secret, just underused. Now you're in on the joke!)&lt;/p&gt;
    &lt;p&gt;Conclusion:&lt;/p&gt;
    &lt;p&gt;We've covered a comprehensive list of pro tips and features for Gemini CLI. From setting up persistent context with &lt;code&gt;GEMINI.md&lt;/code&gt;, to writing custom commands and using advanced tools like MCP servers, to leveraging multi-modal inputs and automating workflows, there's a lot this AI command-line assistant can do. As an external developer, you can integrate Gemini CLI into your daily routine - it's like a powerful ally in your terminal that can handle tedious tasks, provide insights, and even troubleshoot your environment.&lt;/p&gt;
    &lt;p&gt;Gemini CLI is evolving rapidly (being open-source with community contributions), so new features and improvements are constantly on the horizon. By mastering the pro tips in this guide, you'll be well-positioned to harness the full potential of this tool. It's not just about using an AI model - it's about integrating AI deeply into how you develop and manage software.&lt;/p&gt;
    &lt;p&gt;Happy coding with Gemini CLI, and have fun exploring just how far your "AI agent in the terminal" can take you.&lt;/p&gt;
    &lt;p&gt;You now have a Swiss-army knife of AI at your fingertips - use it wisely, and it will make you a more productive (and perhaps happier) developer!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/addyosmani/gemini-cli-tips"/><published>2025-11-26T18:08:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46061208</id><title>Fara-7B: An efficient agentic model for computer use</title><updated>2025-11-27T07:12:46.434010+00:00</updated><content>&lt;doc fingerprint="3b49fc912d38801c"&gt;
  &lt;main&gt;
    &lt;p&gt;Fara-7B is Microsoft's first agentic small language model (SLM) designed specifically for computer use. With only 7 billion parameters, Fara-7B is an ultra-compact Computer Use Agent (CUA) that achieves state-of-the-art performance within its size class and is competitive with larger, more resource-intensive agentic systems.&lt;/p&gt;
    &lt;p&gt;Try Fara-7B locally as follows (see Installation for detailed instructions):&lt;/p&gt;
    &lt;code&gt;# 1. Clone repository
git clone https://github.com/microsoft/fara.git
cd fara

# 2. Setup environment
python3 -m venv .venv 
source .venv/bin/activate
pip install -e .
playwright install&lt;/code&gt;
    &lt;p&gt;Then in one process, host the model:&lt;/p&gt;
    &lt;code&gt;vllm serve "microsoft/Fara-7B" --port 5000 --dtype auto &lt;/code&gt;
    &lt;p&gt;Then you can iterative query it with:&lt;/p&gt;
    &lt;code&gt;fara-cli --task "whats the weather in new york now"&lt;/code&gt;
    &lt;p&gt;Hint: might need to do &lt;code&gt;--tensor-parallel-size 2&lt;/code&gt; with vllm command if you run out of memory&lt;/p&gt;
    &lt;p&gt;Unlike traditional chat models that generate text-based responses, Fara-7B leverages computer interfaces‚Äîmouse and keyboard‚Äîto perform multi-step tasks on behalf of users. The model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Operates visually by perceiving webpages and taking actions like scrolling, typing, and clicking on directly predicted coordinates&lt;/item&gt;
      &lt;item&gt;Uses the same modalities as humans to interact with computers‚Äîno accessibility trees or separate parsing models required&lt;/item&gt;
      &lt;item&gt;Enables on-device deployment due to its compact 7B parameter size, resulting in reduced latency and improved privacy as user data remains local&lt;/item&gt;
      &lt;item&gt;Completes tasks efficiently, averaging only ~16 steps per task compared to ~41 for comparable models&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fara-7B is trained using a novel synthetic data generation pipeline built on the Magentic-One multi-agent framework, with 145K trajectories covering diverse websites, task types, and difficulty levels. The model is based on Qwen2.5-VL-7B and trained with supervised fine-tuning.&lt;/p&gt;
    &lt;p&gt;Fara-7B can automate everyday web tasks including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Searching for information and summarizing results&lt;/item&gt;
      &lt;item&gt;Filling out forms and managing accounts&lt;/item&gt;
      &lt;item&gt;Booking travel, movie tickets, and restaurant reservations&lt;/item&gt;
      &lt;item&gt;Shopping and comparing prices across retailers&lt;/item&gt;
      &lt;item&gt;Finding job postings and real estate listings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fara-7B achieves state-of-the-art results across multiple web agent benchmarks, outperforming both comparable-sized models and larger systems:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Params&lt;/cell&gt;
        &lt;cell role="head"&gt;WebVoyager&lt;/cell&gt;
        &lt;cell role="head"&gt;Online-M2W&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepShop&lt;/cell&gt;
        &lt;cell role="head"&gt;WebTailBench&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;SoM Agents&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;SoM Agent (GPT-4o-0513)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;90.6&lt;/cell&gt;
        &lt;cell&gt;57.7&lt;/cell&gt;
        &lt;cell&gt;49.1&lt;/cell&gt;
        &lt;cell&gt;60.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;SoM Agent (o3-mini)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;79.3&lt;/cell&gt;
        &lt;cell&gt;55.4&lt;/cell&gt;
        &lt;cell&gt;49.7&lt;/cell&gt;
        &lt;cell&gt;52.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;SoM Agent (GPT-4o)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;65.1&lt;/cell&gt;
        &lt;cell&gt;34.6&lt;/cell&gt;
        &lt;cell&gt;16.0&lt;/cell&gt;
        &lt;cell&gt;30.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;GLM-4.1V-9B-Thinking&lt;/cell&gt;
        &lt;cell&gt;9B&lt;/cell&gt;
        &lt;cell&gt;66.8&lt;/cell&gt;
        &lt;cell&gt;33.9&lt;/cell&gt;
        &lt;cell&gt;32.0&lt;/cell&gt;
        &lt;cell&gt;22.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Computer Use Models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;OpenAI computer-use-preview&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;70.9&lt;/cell&gt;
        &lt;cell&gt;42.9&lt;/cell&gt;
        &lt;cell&gt;24.7&lt;/cell&gt;
        &lt;cell&gt;25.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;UI-TARS-1.5-7B&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;66.4&lt;/cell&gt;
        &lt;cell&gt;31.3&lt;/cell&gt;
        &lt;cell&gt;11.6&lt;/cell&gt;
        &lt;cell&gt;19.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Fara-7B&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;73.5&lt;/cell&gt;
        &lt;cell&gt;34.1&lt;/cell&gt;
        &lt;cell&gt;26.2&lt;/cell&gt;
        &lt;cell&gt;38.4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Table: Online agent evaluation results showing success rates (%) across four web benchmarks. Results are averaged over 3 runs.&lt;/p&gt;
    &lt;p&gt;We are releasing WebTailBench, a new evaluation benchmark focusing on 11 real-world task types that are underrepresented or missing in existing benchmarks. The benchmark includes 609 tasks across diverse categories, with the first 8 segments testing single skills or objectives (usually on a single website), and the remaining 3 evaluating more difficult multi-step or cross-site tasks.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Task Segment&lt;/cell&gt;
        &lt;cell role="head"&gt;Tasks&lt;/cell&gt;
        &lt;cell role="head"&gt;SoM GPT-4o-0513&lt;/cell&gt;
        &lt;cell role="head"&gt;SoM o3-mini&lt;/cell&gt;
        &lt;cell role="head"&gt;SoM GPT-4o&lt;/cell&gt;
        &lt;cell role="head"&gt;GLM-4.1V-9B&lt;/cell&gt;
        &lt;cell role="head"&gt;OAI Comp-Use&lt;/cell&gt;
        &lt;cell role="head"&gt;UI-TARS-1.5&lt;/cell&gt;
        &lt;cell role="head"&gt;Fara-7B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Single-Site Tasks&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Shopping&lt;/cell&gt;
        &lt;cell&gt;56&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;38.1&lt;/cell&gt;
        &lt;cell&gt;31.0&lt;/cell&gt;
        &lt;cell&gt;42.3&lt;/cell&gt;
        &lt;cell&gt;41.1&lt;/cell&gt;
        &lt;cell&gt;52.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Flights&lt;/cell&gt;
        &lt;cell&gt;51&lt;/cell&gt;
        &lt;cell&gt;60.1&lt;/cell&gt;
        &lt;cell&gt;39.2&lt;/cell&gt;
        &lt;cell&gt;11.1&lt;/cell&gt;
        &lt;cell&gt;10.5&lt;/cell&gt;
        &lt;cell&gt;17.6&lt;/cell&gt;
        &lt;cell&gt;10.5&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Hotels&lt;/cell&gt;
        &lt;cell&gt;52&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;31.4&lt;/cell&gt;
        &lt;cell&gt;19.9&lt;/cell&gt;
        &lt;cell&gt;26.9&lt;/cell&gt;
        &lt;cell&gt;35.3&lt;/cell&gt;
        &lt;cell&gt;53.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Restaurants&lt;/cell&gt;
        &lt;cell&gt;52&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;59.6&lt;/cell&gt;
        &lt;cell&gt;47.4&lt;/cell&gt;
        &lt;cell&gt;32.1&lt;/cell&gt;
        &lt;cell&gt;35.9&lt;/cell&gt;
        &lt;cell&gt;22.4&lt;/cell&gt;
        &lt;cell&gt;47.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Activities&lt;/cell&gt;
        &lt;cell&gt;80&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;41.7&lt;/cell&gt;
        &lt;cell&gt;26.3&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;9.6&lt;/cell&gt;
        &lt;cell&gt;36.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Ticketing&lt;/cell&gt;
        &lt;cell&gt;57&lt;/cell&gt;
        &lt;cell&gt;58.5&lt;/cell&gt;
        &lt;cell&gt;56.7&lt;/cell&gt;
        &lt;cell&gt;37.4&lt;/cell&gt;
        &lt;cell&gt;35.7&lt;/cell&gt;
        &lt;cell&gt;49.7&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;38.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Real Estate&lt;/cell&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;34.0&lt;/cell&gt;
        &lt;cell&gt;17.4&lt;/cell&gt;
        &lt;cell&gt;20.1&lt;/cell&gt;
        &lt;cell&gt;16.0&lt;/cell&gt;
        &lt;cell&gt;9.0&lt;/cell&gt;
        &lt;cell&gt;9.7&lt;/cell&gt;
        &lt;cell&gt;23.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Jobs/Careers&lt;/cell&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;49.3&lt;/cell&gt;
        &lt;cell&gt;44.0&lt;/cell&gt;
        &lt;cell&gt;32.7&lt;/cell&gt;
        &lt;cell&gt;22.7&lt;/cell&gt;
        &lt;cell&gt;20.7&lt;/cell&gt;
        &lt;cell&gt;20.7&lt;/cell&gt;
        &lt;cell&gt;28.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Multi-Step Tasks&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Shopping List (2 items)&lt;/cell&gt;
        &lt;cell&gt;51&lt;/cell&gt;
        &lt;cell&gt;66.0&lt;/cell&gt;
        &lt;cell&gt;62.7&lt;/cell&gt;
        &lt;cell&gt;17.0&lt;/cell&gt;
        &lt;cell&gt;7.8&lt;/cell&gt;
        &lt;cell&gt;34.0&lt;/cell&gt;
        &lt;cell&gt;20.9&lt;/cell&gt;
        &lt;cell&gt;49.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Comparison Shopping&lt;/cell&gt;
        &lt;cell&gt;57&lt;/cell&gt;
        &lt;cell&gt;67.3&lt;/cell&gt;
        &lt;cell&gt;59.1&lt;/cell&gt;
        &lt;cell&gt;27.5&lt;/cell&gt;
        &lt;cell&gt;22.8&lt;/cell&gt;
        &lt;cell&gt;1.2&lt;/cell&gt;
        &lt;cell&gt;8.8&lt;/cell&gt;
        &lt;cell&gt;32.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Compositional Tasks&lt;/cell&gt;
        &lt;cell&gt;55&lt;/cell&gt;
        &lt;cell&gt;51.5&lt;/cell&gt;
        &lt;cell&gt;39.4&lt;/cell&gt;
        &lt;cell&gt;26.7&lt;/cell&gt;
        &lt;cell&gt;17.0&lt;/cell&gt;
        &lt;cell&gt;10.3&lt;/cell&gt;
        &lt;cell&gt;9.1&lt;/cell&gt;
        &lt;cell&gt;23.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Macro Average&lt;/cell&gt;
        &lt;cell&gt;609&lt;/cell&gt;
        &lt;cell&gt;59.7&lt;/cell&gt;
        &lt;cell&gt;51.7&lt;/cell&gt;
        &lt;cell&gt;30.1&lt;/cell&gt;
        &lt;cell&gt;22.0&lt;/cell&gt;
        &lt;cell&gt;25.3&lt;/cell&gt;
        &lt;cell&gt;19.9&lt;/cell&gt;
        &lt;cell&gt;38.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Micro Average&lt;/cell&gt;
        &lt;cell&gt;609&lt;/cell&gt;
        &lt;cell&gt;60.4&lt;/cell&gt;
        &lt;cell&gt;52.7&lt;/cell&gt;
        &lt;cell&gt;30.8&lt;/cell&gt;
        &lt;cell&gt;22.4&lt;/cell&gt;
        &lt;cell&gt;25.7&lt;/cell&gt;
        &lt;cell&gt;19.5&lt;/cell&gt;
        &lt;cell&gt;38.4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Table: Breakdown of WebTailBench results across all 11 segments. Success rates (%) are averaged over 3 independent runs. Fara-7B achieves the highest performance among computer-use models across all task categories.&lt;/p&gt;
    &lt;p&gt;Coming Soon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Task Verification pipeline for LLM-as-a-judge evaluation&lt;/item&gt;
      &lt;item&gt;Official human annotations of WebTailBench (in partnership with BrowserBase)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our evaluation setup leverages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Playwright - A cross-browser automation framework that replicates browser environments&lt;/item&gt;
      &lt;item&gt;Abstract Web Agent Interface - Allows integration of any model from any source into the evaluation environment&lt;/item&gt;
      &lt;item&gt;Fara-Agent Class - Reference implementation for running the Fara model&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Fara-7B is an experimental release designed to invite hands-on exploration and feedback from the community. We recommend running it in a sandboxed environment, monitoring its execution, and avoiding sensitive data or high-risk domains.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Install the package using either UV or pip:&lt;/p&gt;
    &lt;code&gt;uv sync --all-extras&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;pip install -e .&lt;/code&gt;
    &lt;p&gt;Then install Playwright browsers:&lt;/p&gt;
    &lt;code&gt;playwright install&lt;/code&gt;
    &lt;p&gt;Recommended: The easiest way to get started is using Azure Foundry hosting, which requires no GPU hardware or model downloads. Alternatively, you can self-host with VLLM if you have GPU resources available.&lt;/p&gt;
    &lt;p&gt;Deploy Fara-7B on Azure Foundry without needing to download weights or manage GPU infrastructure.&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Deploy the Fara-7B model on Azure Foundry and obtain your endpoint URL and API key&lt;/item&gt;
      &lt;item&gt;Add your endpoint details to the existing &lt;code&gt;endpoint_configs/&lt;/code&gt;directory (example configs are already provided):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Edit one of the existing config files or create a new one
# endpoint_configs/fara-7b-hosting-ansrz.json (example format):
{
    "model": "Fara-7B",
    "base_url": "https://your-endpoint.inference.ml.azure.com/",
    "api_key": "YOUR_API_KEY_HERE"
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the Fara agent:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fara-cli --task "how many pages does wikipedia have" --start_page "https://www.bing.com"&lt;/code&gt;
    &lt;p&gt;That's it! No GPU or model downloads required.&lt;/p&gt;
    &lt;p&gt;If you have access to GPU resources, you can self-host Fara-7B using VLLM. This requires a GPU machine with sufficient VRAM.&lt;/p&gt;
    &lt;p&gt;All that is required is to run the following command to start the VLLM server:&lt;/p&gt;
    &lt;code&gt;vllm serve "microsoft/Fara-7B" --port 5000 --dtype auto &lt;/code&gt;
    &lt;p&gt;Run the test script to see Fara in action:&lt;/p&gt;
    &lt;code&gt;fara-cli --task "how many pages does wikipedia have" --start_page "https://www.bing.com" --endpoint_config endpoint_configs/azure_foundry_config.json [--headful] [--downloads_folder "/path/to/downloads"] [--save_screenshots] [--max_rounds 100] [--browserbase]&lt;/code&gt;
    &lt;p&gt;In self-hosting scenario the &lt;code&gt;endpoint_config&lt;/code&gt; points to &lt;code&gt;endpoint_configs/vllm_config.json&lt;/code&gt; from the VLLM server above.&lt;/p&gt;
    &lt;p&gt;If you set &lt;code&gt;--browserbase&lt;/code&gt;, export environment variables for the API key and project ID.&lt;/p&gt;
    &lt;code&gt;Initializing Browser...
Browser Running... Starting Fara Agent...
##########################################
Task: how many pages does wikipedia have
##########################################
Running Fara...


Thought #1: To find the current number of Wikipedia pages, I'll search for the latest Wikipedia page count statistics.
Action #1: executing tool 'web_search' with arguments {"action": "web_search", "query": "Wikipedia total number of articles"}
Observation#1: I typed 'Wikipedia total number of articles' into the browser search bar.

Thought #2: Wikipedia currently has 7,095,446 articles.
Action #2: executing tool 'terminate' with arguments {"action": "terminate", "status": "success"}
Observation#2: Wikipedia currently has 7,095,446 articles.

Final Answer: Wikipedia currently has 7,095,446 articles.

Enter another task (or press Enter to exit): 
&lt;/code&gt;
    &lt;p&gt;We provide a framework in &lt;code&gt;webeval/&lt;/code&gt; to reproduce our results on WebVoyager and OnlineMind2Web.
Agentic evaluations on live websites present unique challenges due to day-to-day changes. We implement several measures to ensure reliable and comparable evaluations:&lt;/p&gt;
    &lt;p&gt;BrowserBase Integration We employ BrowserBase to manage browser session hosting, enabling reliable browser instance management.&lt;/p&gt;
    &lt;p&gt;Time-sensitive Task Updates Tasks in benchmarks like WebVoyager can become stale or impossible. We:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Removed ~48 impossible tasks from the original WebVoyager benchmark&lt;/item&gt;
      &lt;item&gt;Updated ~50 tasks with future dates to keep them achievable&lt;/item&gt;
      &lt;item&gt;Example: "Search for a hotel in Bali from Jan 1 to Jan 4, 2024" ‚Üí "Search for a hotel in Bali from Jan 1 to Jan 4, 2026"&lt;/item&gt;
      &lt;item&gt;Our updated WebVoyager benchmark is available at &lt;code&gt;webeval/data/webvoyager/WebVoyager_data_08312025.jsonl&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Environment Error Handling Browser errors (connection drops, page timeouts) are handled robustly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trajectories are retried up to 5 times when environment errors occur&lt;/item&gt;
      &lt;item&gt;Complete yet incorrect trajectories are never retried&lt;/item&gt;
      &lt;item&gt;Each retry starts with a fresh browser session, with no retained state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Step Budget Each trajectory is capped at a maximum of 100 actions across all online benchmarks. Trajectories exceeding this budget without choosing to stop are considered incorrect.&lt;/p&gt;
    &lt;code&gt;conda create --name fara_webeval python=3.12
conda activate fara_webeval

# Install fara package
pip install -e .

# Install autogen submodule
git submodule update --init --recursive
cd autogen/python/packages
pip install -e autogen-core
pip install -e autogen-ext

# Install webeval
cd webeval
pip install -e .

# Install playwright
playwright install&lt;/code&gt;
    &lt;p&gt;Navigate to the scripts directory:&lt;/p&gt;
    &lt;code&gt;cd webeval/scripts&lt;/code&gt;
    &lt;p&gt;Make sure you set a valid OpenAI GPT-4o endpoint in &lt;code&gt;endpoint_configs_gpt4o/dev&lt;/code&gt; in order to run the WebVoyager LLM-as-a-judge!&lt;/p&gt;
    &lt;p&gt;Option 1: Self-hosted VLLM&lt;/p&gt;
    &lt;code&gt;python webvoyager.py --model_url /path/where/you/want/to/download/model/ --model_port 5000 --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --device_id 0,1 --processes 1 --run_id 1 --max_rounds 100&lt;/code&gt;
    &lt;p&gt;Option 2: Azure Foundry Deployment&lt;/p&gt;
    &lt;p&gt;Deploy Fara-7B on Foundry endpoint(s), then place endpoint URLs and keys in JSONs under &lt;code&gt;endpoint_configs/&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;python webvoyager.py --model_endpoint ../../endpoint_configs/ --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --processes 1 --run_id 1_endpoint --max_rounds 100&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We use the same LLM-as-a-judge prompts and model (GPT-4o) as WebVoyager, hence the &lt;code&gt;--eval_oai_config&lt;/code&gt;argument&lt;/item&gt;
      &lt;item&gt;Set &lt;code&gt;--browserbase&lt;/code&gt;for browser session management (requires exported API key and project ID environment variables)&lt;/item&gt;
      &lt;item&gt;Avoid overloading a single VLLM deployment with more than ~10 concurrent processes due to known issues&lt;/item&gt;
      &lt;item&gt;See debugging output in &lt;code&gt;fara/webeval/scripts/stdout.txt&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Evaluation results are stored under &lt;code&gt;--out_url&lt;/code&gt; in folders organized by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model name&lt;/item&gt;
      &lt;item&gt;Dataset&lt;/item&gt;
      &lt;item&gt;Username&lt;/item&gt;
      &lt;item&gt;Run ID&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example path:&lt;/p&gt;
    &lt;code&gt;/runs/WebSurfer-fara-100-max_n_images-3/fara-7b/&amp;lt;username&amp;gt;/WebVoyager_WebVoyager_data_08312025.jsonl/&amp;lt;run_id&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Each evaluation folder contains:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;gpt_eval/&lt;/code&gt;- LLM-as-a-judge evaluation results&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;traj/&lt;/code&gt;- Per-task trajectory subdirectories containing:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;final_answer.json&lt;/code&gt;(e.g.,&lt;code&gt;Amazon--1_final_answer.json&lt;/code&gt;) -&lt;code&gt;&amp;lt;no_answer&amp;gt;&lt;/code&gt;indicates abortion or step budget exceeded&lt;/item&gt;&lt;item&gt;&lt;code&gt;scores/gpt_eval.json&lt;/code&gt;- LLM judge scores&lt;/item&gt;&lt;item&gt;&lt;code&gt;web_surfer.log&lt;/code&gt;- Action history and errors&lt;/item&gt;&lt;item&gt;&lt;code&gt;screenshot_X.png&lt;/code&gt;- Screenshots captured before each action X&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use the analysis notebook to compute metrics:&lt;/p&gt;
    &lt;code&gt;cd webeval/scripts/analyze_eval_results/
jupyter notebook analyze.ipynb&lt;/code&gt;
    &lt;p&gt;The script:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identifies trajectories aborted mid-execution and diagnostic reasons&lt;/item&gt;
      &lt;item&gt;Computes average scores across non-aborted trajectories&lt;/item&gt;
      &lt;item&gt;Distinguishes between aborted trajectories (errors during sampling) and completed trajectories (with terminate() call or step budget exceeded)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To re-run failed tasks, execute the evaluation script again with the same &lt;code&gt;run_id&lt;/code&gt; and &lt;code&gt;username&lt;/code&gt; - it will skip non-aborted tasks.&lt;/p&gt;
    &lt;head&gt;Example WebVoyager GPT Eval Result&lt;/head&gt;
    &lt;code&gt;{
  "score": 1.0,
  "gpt_response_text": "To evaluate the task, we need to verify if the criteria have been met:\n\n1. **Recipe Requirement**: A vegetarian lasagna recipe with zucchini and at least a four-star rating.\n\n2. **Search and Results**:\n   - The screenshots show that the search term used was \"vegetarian lasagna zucchini.\"\n   - Among the search results, \"Debbie's Vegetable Lasagna\" is prominently featured.\n   \n3. **Evaluation of the Recipe**:\n   - Rating: \"Debbie's Vegetable Lasagna\" has a rating of 4.7, which satisfies the requirement of being at least four stars.\n   - The presence of zucchini in the recipe is implied through the search conducted, though the screenshots do not explicitly show the ingredients list. However, the result response confirms the match to the criteria.\n\nGiven the information provided, the task seems to have fulfilled the requirement of finding a vegetarian lasagna recipe with zucchini and a four-star rating or higher. \n\n**Verdict: SUCCESS**"
}&lt;/code&gt;
    &lt;p&gt;If you use Fara in your research, please cite our work:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/microsoft/fara"/><published>2025-11-26T19:10:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46061239</id><title>Alan.app ‚Äì Add a Border to macOS Active Window</title><updated>2025-11-27T07:12:46.230552+00:00</updated><content>&lt;doc fingerprint="60ae3c934d1f11bd"&gt;
  &lt;main&gt;
    &lt;p&gt;Maybe it‚Äôs because my eyes are getting old or maybe it‚Äôs because the contrast between windows on macOS keeps getting worse. Either way, I built a tiny Mac app last night that draws a border around the active window. I named it ‚ÄúAlan‚Äù.&lt;/p&gt;
    &lt;p&gt;In Alan‚Äôs preferences, you can choose a preferred border width and colors for both light and dark mode.&lt;/p&gt;
    &lt;p&gt;That‚Äôs it. That‚Äôs the app.&lt;/p&gt;
    &lt;p&gt;You can download a notarized copy of Alan here.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a short demo video.&lt;/p&gt;
    &lt;p&gt;If you want to hide Alan‚Äôs icon from the Dock, you can set a hidden preference by running this Terminal command. Then, relaunch the app.&lt;/p&gt;
    &lt;code&gt;defaults write studio.retina.Alan hideDock -bool true&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tyler.io/2025/11/alan/"/><published>2025-11-26T19:12:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46061682</id><title>S&amp;box is now an open source game engine</title><updated>2025-11-27T07:12:45.834920+00:00</updated><link href="https://sbox.game/news/update-25-11-26"/><published>2025-11-26T19:58:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46062504</id><title>The EU made Apple adopt new Wi-Fi standards, and now Android can support AirDrop</title><updated>2025-11-27T07:12:45.699326+00:00</updated><content>&lt;doc fingerprint="a786e58451353bc8"&gt;
  &lt;main&gt;
    &lt;p&gt;Last year, Apple finally added support for Rich Communications Services (RCS) texting to its platforms, improving consistency, reliability, and security when exchanging green-bubble texts between the competing iPhone and Android ecosystems. Today, Google is announcing another small step forward in interoperability, pointing to a slightly less annoying future for friend groups or households where not everyone owns an iPhone.&lt;/p&gt;
    &lt;p&gt;Google has updated Android‚Äôs Quick Share feature to support Apple‚Äôs AirDrop, which allows users of Apple devices to share files directly using a local peer-to-peer Wi-Fi connection. Apple devices with AirDrop enabled and set to ‚Äúeveryone for 10 minutes‚Äù mode will show up in the Quick Share device list just like another Android phone would, and Android devices that support this new Quick Share version will also show up in the AirDrop menu.&lt;/p&gt;
    &lt;p&gt;Google will only support this feature on the Pixel 10 series, at least to start. The company is ‚Äúlooking forward to improving the experience and expanding it to more Android devices,‚Äù but it didn‚Äôt announce anything about a timeline or any hardware or software requirements. Quick Share also won‚Äôt work with AirDrop devices working in the default ‚Äúcontacts only‚Äù mode, though Google ‚Äú[welcomes] the opportunity to work with Apple to enable ‚ÄòContacts Only‚Äô mode in the future.‚Äù (Reading between the lines: Google and Apple are not currently working together to enable this, and Google confirmed to The Verge that Apple hadn‚Äôt been involved in this at all.)&lt;/p&gt;
    &lt;p&gt;Like AirDrop, Google notes that files shared via Quick Share are transferred directly between devices, without being sent to either company‚Äôs servers first.&lt;/p&gt;
    &lt;p&gt;Google shared a little more information in a separate post about Quick Share‚Äôs security, crediting Android‚Äôs use of the memory-safe Rust programming language with making secure file sharing between platforms possible.&lt;/p&gt;
    &lt;p&gt;‚ÄúIts compiler enforces strict ownership and borrowing rules at compile time, which guarantees memory safety,‚Äù writes Google VP of Platforms Security and Privacy Dave Kleidermacher. ‚ÄúRust removes entire classes of memory-related bugs. This means our implementation is inherently resilient against attackers attempting to use maliciously crafted data packets to exploit memory errors.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/gadgets/2025/11/the-eu-made-apple-adopt-new-wi-fi-standards-and-now-android-can-support-airdrop/"/><published>2025-11-26T21:25:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46063072</id><title>Bring bathroom doors back to hotels</title><updated>2025-11-27T07:12:45.514986+00:00</updated><content>&lt;doc fingerprint="4765d15dde61f49c"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôm done. I‚Äôm done arriving at hotels and discovering that they have removed the bathroom door. Something that should be as standard as having a bed, has been sacrificed in the name of ‚Äúaesthetic‚Äù.&lt;/p&gt;
    &lt;p&gt;I get it, you can save on material costs and make the room feel bigger, but what about my dignity??? I can‚Äôt save that when you don‚Äôt include a bathroom door.&lt;/p&gt;
    &lt;p&gt;It‚Äôs why I‚Äôve built this website, where I compiled hotels that are guaranteed to have bathroom doors, and hotels that need to work on privacy.&lt;/p&gt;
    &lt;p&gt;I‚Äôve emailed hundreds of hotels and I asked them two things: do your doors close all the way, and are they made of glass? Everyone that says yes to their doors closing, and no to being made of glass has been sorted by price range and city for you to easily find places to stay that are guaranteed to have a bathroom door.&lt;/p&gt;
    &lt;p&gt;Quickly check to see if the hotel you‚Äôre thinking of booking has been reported as lacking in doors by a previous guest.&lt;/p&gt;
    &lt;p&gt;Finally, this passion project could not exist without people submitting hotels without bathroom doors for public shaming. If you‚Äôve stayed at a doorless hotel send me an email with the hotel name to bringbackdoors@gmail.com, or send me a DM on Instagram with the hotel name and a photo of the doorless setup to be publicly posted.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs name and shame these hotels to protect the dignity of future travelers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bringbackdoors.com/"/><published>2025-11-26T22:26:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46063272</id><title>Running Unsupported iOS on Deprecated Devices</title><updated>2025-11-27T07:12:45.416462+00:00</updated><content>&lt;doc fingerprint="b6495d34246e3b64"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Running unsupported iOS on deprecated devices&lt;/head&gt;
    &lt;p&gt;Earlier this year I demoed iOS 6 running on an iPod touch 3 - a device that Apple never gave iOS 6 to, making iOS 5.1.1 the latest build it can run&lt;/p&gt;
    &lt;p&gt;A few months later I also released a script that generates an iOS 6 restore image installable on that iPod touch model&lt;/p&gt;
    &lt;p&gt;This article describes technical details behind this work. Certain proficiency in iOS internals is assumed&lt;/p&gt;
    &lt;head rend="h2"&gt;I'll show you what iOS is made of&lt;/head&gt;
    &lt;p&gt;First of all, let's recap what software components iOS consists of:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;iBoot - the bootloader. Has 4 different types for different scenarios - iBSS, iBEC, LLB and iBoot&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kernelcache - the OS kernel + kernel extensions (drivers) built into a single binary blob&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DeviceTree - structured list of hardware used by specific device model + some parameters that specify software behavior. The copy included in an IPSW is more of a template that is heavily modified by iBoot before jumping into kernel&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Userspace filesystem - tiny restore ramdisk used purely for OS installation or the actual root filesystem of iOS installed persistently&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Various firmwares for coprocessors, be they internal or external to the main SoC - like, baseband, Wi-Fi, Bluetooth, multitouch and etc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;iPhone 3GS tests&lt;/head&gt;
    &lt;p&gt;iPhone 3GS was released the same year as iPod touch 3 (2009), and has a very similar hardware (S5L8920X SoC vs. S5L8922X). But the most important part is that it actually got iOS 6 officially&lt;/p&gt;
    &lt;p&gt;Before doing anything on the iPod I decided to try to boot iOS 6.0 with iOS 5.1.1 iBoot &amp;amp; DeviceTree on the iPhone and see what's gonna break and how&lt;/p&gt;
    &lt;head rend="h2"&gt;DeviceTree&lt;/head&gt;
    &lt;p&gt;The most broken thing was DeviceTree - iOS 6 added a lot of new nodes and properties. To fix it in automated manner I wrote a stupid Python script that decodes and computes a diff between 2 DeviceTrees. Such diff can also be applied to another DeviceTree&lt;/p&gt;
    &lt;p&gt;The script is available in the SundanceInH2A repo&lt;/p&gt;
    &lt;p&gt;As I mentioned above a lot of things in a DeviceTree is filled by iBoot at runtime. One of such new properties is &lt;code&gt;nvram-proxy-data&lt;/code&gt; in &lt;code&gt;chosen&lt;/code&gt; node&lt;/p&gt;
    &lt;p&gt;The property must contain a raw NVRAM dump - leaving it empty will make kernel get stuck somewhere very early&lt;/p&gt;
    &lt;p&gt;For iPod touch 3 I also had to clean-up the diff out of iPhone-specific things before applying it to iPod's 5.1.1 DeviceTree&lt;/p&gt;
    &lt;head rend="h2"&gt;iBoot&lt;/head&gt;
    &lt;p&gt;iBoot didn't require any major changes in this case. Just typical Image3 signature check patch, boot-args injection and &lt;code&gt;debug-enabled&lt;/code&gt; patch so kernel is going to actually respect AMFI boot-args&lt;/p&gt;
    &lt;p&gt;One important thing is to actually populate &lt;code&gt;nvram-proxy-data&lt;/code&gt; dynamically, at least for normal boots (aka non-restore). Restore boot will be fine with some random NVRAM hardcoded into DeviceTree, but normal one will overwrite your actual NVRAM with the random one if it decides to sync it at some point&lt;/p&gt;
    &lt;p&gt;I do it by replacing a call to &lt;code&gt;UpdateDeviceTree()&lt;/code&gt; with my own little function that calls the real &lt;code&gt;UpdateDeviceTree()&lt;/code&gt;, but also populates actual &lt;code&gt;nvram-proxy-data&lt;/code&gt; and &lt;code&gt;random-seed&lt;/code&gt; (this one shouldn't be of any importance)&lt;/p&gt;
    &lt;p&gt;For boot-args I always add &lt;code&gt;amfi=0xff&lt;/code&gt; to disable code-signing, but that's pretty cannonical as well&lt;/p&gt;
    &lt;p&gt;Please note that other iBoot+kernel combos might require more changes - if you ever try something and it doesn't work, I recommend looking into DeviceTree differences (both the initial template and how iBoot fills it) and also &lt;code&gt;boot_args&lt;/code&gt; structure iBoot passes to kernel (not to be confused with boot-args string, the &lt;code&gt;boot_args&lt;/code&gt; structure is a different thing)&lt;/p&gt;
    &lt;head rend="h2"&gt;Kernelcache&lt;/head&gt;
    &lt;p&gt;The most complex part. iPod touch 3 never got iOS 6 officialy, yes, but it was rumored that initially it was meant to have it, but Apple's marketing team said no. Either way, almost every internal iOS 6 build got both standalone S5L8922X kernel and even standalone kexts (including ones specific to iPod touch 3)&lt;/p&gt;
    &lt;p&gt;The question is how to load them all simultaneously. My initial idea was to do it just as older Mac OS X could do - load all kexts dynamically on bootloader level. Long story short, my strategy was the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In iBoot context, load all kexts from filesystem - binary itself + Info.plist&lt;/item&gt;
      &lt;item&gt;Lay them out in memory and add corresponding entries to &lt;code&gt;chosen/memory-map&lt;/code&gt;node of DeviceTree&lt;/item&gt;
      &lt;item&gt;Boot standalone kernel which will then pick them up and load&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The sad outcome:&lt;/p&gt;
    &lt;code&gt;panic(cpu 0 caller 0x802e5223): "kern_return_t kxld_link_file(KXLDContext *, u_char *, u_long, const char *, void *, KXLDDependency *, u_int, u_char **, kxld_addr_t *) (com.apple.kec.corecrypto) called in kernel without kxld support"
&lt;/code&gt;
    &lt;p&gt;The kernel has all the code to pick them up, but not to actually link...&lt;/p&gt;
    &lt;head rend="h3"&gt;Glueing a prelinked kernelcache&lt;/head&gt;
    &lt;p&gt;So creating a legit kernelcache is the only way after all. I was already imagining all the horrors of writing software to parse and apply &lt;code&gt;LINKEDIT&lt;/code&gt; and etc., but then it occured to me! Mac OS X (before Apple Silicon) was generating such kernelcaches somehow! What if we use that logic to build our iOS kernelcache?&lt;/p&gt;
    &lt;code&gt;kcgen \
    -c output.bin \
    $(cat n18.10A403.kextlist | sed 's/^/--bundle-id /') \
    -kernel kernels_kexts_10A63970m/mach.development.s5l8922x \
    -arch armv7 \
    -all-personalities \
    -strip-symbols \
    -uncompressed \
    -- \
    kernels_kexts_10A63970m/Extensions
&lt;/code&gt;
    &lt;p&gt;I used &lt;code&gt;/usr/local/bin/kcgen&lt;/code&gt; from internal Sierra build (can be found online as "Phoenix A1708.dmg"), but it seems that even latest macOS &lt;code&gt;kextcache&lt;/code&gt; can do it (included by default)&lt;/p&gt;
    &lt;p&gt;Here is a breakdown of the options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-c output.bin&lt;/code&gt;- output file to write resulting kernelcache to&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;$(cat n18.10A403.kextlist | sed 's/^/--bundle-id /')&lt;/code&gt;- this weird expression appends&lt;code&gt;--bundle-id&lt;/code&gt;to every line from the file at&lt;code&gt;n18.10A403.kextlist&lt;/code&gt;. This is to specify which kexts we'd like to include. How I created such list is described below&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-arch armv7&lt;/code&gt;- obviously only build armv7 slice&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-all-personalities&lt;/code&gt;- very important flag that prevents irrelevant IOKit personalities to be stripped. "Irrelevant" as in "irrelevant to current machine", meaning everything relevant to iPod touch 3 is going to be stripped&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-strip-symbols&lt;/code&gt;- strips unnecessary symbols. This flag can be omitted theoretically, but I recommend keeping it to make resulting kernelcache smaller&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-uncompressed&lt;/code&gt;- do not apply compression. Since we'll have to change one little thing later, compression would have to be reapplied anyway&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--&lt;/code&gt;means the rest of the args will point to directories to grab kexts from&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;kernels_kexts_10A63970m/Extensions&lt;/code&gt;is a path to a folder containing kexts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The little thing to do is to remove fat header. For some reason, it creates a fat Mach-O with a single slice. iBoot doesn't like it, so let's strip it:&lt;/p&gt;
    &lt;code&gt;lipo -thin armv7 output.bin -o output.thin.bin
&lt;/code&gt;
    &lt;p&gt;The kernel cache is ready now! Just needs to be compressed and packaged into Image3 container&lt;/p&gt;
    &lt;head rend="h4"&gt;About kext lists&lt;/head&gt;
    &lt;p&gt;Once again I compared iPhone 3GS' iOS 5.1.1 vs. 6.0 - some kexts were added, some removed, some changed their bundle IDs, some were irrelevant for iPod touch 3&lt;/p&gt;
    &lt;p&gt;Do not forget to include the pseudo-extensions as well!&lt;/p&gt;
    &lt;p&gt;Samples can be found in SundanceInH2A repository&lt;/p&gt;
    &lt;head rend="h4"&gt;About IOKit personalities&lt;/head&gt;
    &lt;p&gt;In this specific case I had to patch up Info.plist of the Wi-Fi kext. As always there is a sample in the repo&lt;/p&gt;
    &lt;head rend="h2"&gt;Restore ramdisk filesystem&lt;/head&gt;
    &lt;p&gt;Pretty cannonical here. I patched &lt;code&gt;asr&lt;/code&gt; as usual and also had to move &lt;code&gt;options.n88.plist&lt;/code&gt; to &lt;code&gt;options.n18.plist&lt;/code&gt; so it can lay out partitions properly&lt;/p&gt;
    &lt;p&gt;However, I also have to install the iBoot exploit. To do that I reimplement &lt;code&gt;rc.boot&lt;/code&gt; binary:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Remount ramdisk and set&lt;/p&gt;&lt;code&gt;umask&lt;/code&gt;just like the original one does&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Call&lt;/p&gt;&lt;code&gt;restored_external&lt;/code&gt;, but with&lt;code&gt;-server&lt;/code&gt;argument, so it doesn't reboot after finishing restore&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If restore was completed properly, I add a third partition, write the exploit there and set&lt;/p&gt;&lt;code&gt;boot-partition&lt;/code&gt;to&lt;code&gt;2&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reboot the device&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My implementation is available guess where? Yes, in the repository&lt;/p&gt;
    &lt;head rend="h2"&gt;Root filesystem&lt;/head&gt;
    &lt;p&gt;This needed a lot of changes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Add matching SpringBoard's hardware feature plist (&lt;/p&gt;&lt;code&gt;/System/Library/CoreServices/SpringBoard.app/N18AP.plist&lt;/code&gt;in this case)&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;I took the iOS 5.1.1 variant as a base and added iOS 6 specific capabilities&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I tried to keep original enough Home screen icon order by merging iPod touch 3 iOS 5.1.1 and iPod touch 4 6.x layouts&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add multitouch &amp;amp; Wi-Fi firmwares&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;I use versions from 5.1.1&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add Bluetooth firmware and scripts&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;This is more complicated, as those are all hardcoded into&lt;/p&gt;
            &lt;code&gt;/usr/sbin/BlueTool&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;Luckily, they can also be overriden by files in&lt;/p&gt;&lt;code&gt;/etc/bluetool&lt;/code&gt;- as always check my code for reference&lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;I extracted both firmware and scripts from 5.1.1&lt;/p&gt;
            &lt;code&gt;BlueTool&lt;/code&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;FairPlay daemon is limited to&lt;/p&gt;&lt;code&gt;N88AP&lt;/code&gt;(iPhone 3GS)&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;It has&lt;/p&gt;&lt;code&gt;LimitLoadToHardware&lt;/code&gt;key in its' LaunchDaemon plist&lt;/item&gt;&lt;item&gt;&lt;p&gt;But if we simply remove the key, it works on iPod touch 3 as well&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This is important, because otherwise we cannot activate device through Apple's servers&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This trick will be harder to pull off on iOS 6.1+ because they load LaunchDaemons from a signed cache. Still can be bypassed in many ways - for instance, patching&lt;/p&gt;&lt;code&gt;launchd&lt;/code&gt;or forcefully loading another plist via&lt;code&gt;launchctl&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DYLD shared cache patches&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Product ID map patch&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;iOS 6 brings a concept of "product ID" in the form of a long byte sequence&lt;/item&gt;
              &lt;item&gt;It is filled by iBoot into &lt;code&gt;product&lt;/code&gt;node of DeviceTree (which didn't even exist before)&lt;/item&gt;
              &lt;item&gt;I hardcode the value of iPhone 3GS straight into DeviceTree (&lt;code&gt;8784AE8D7066B0F0136BE91DCFE632A436FFD6FB&lt;/code&gt;)&lt;/item&gt;
              &lt;item&gt;There is also a short form of this identifier - 16-bit integer - which existed before iOS 6&lt;/item&gt;
              &lt;item&gt;iPhone 3GS is &lt;code&gt;0x2714&lt;/code&gt;and the iPod is&lt;code&gt;0x2715&lt;/code&gt;&lt;/item&gt;
              &lt;item&gt;MobileGestalt framework has a table that matches the short form by the long one - I swap &lt;code&gt;0x2714&lt;/code&gt;with&lt;code&gt;0x2715&lt;/code&gt;there&lt;/item&gt;
              &lt;item&gt;I believe it's better for iTunes and etc.&lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;code&gt;getDeviceVariant()&lt;/code&gt;patch&lt;list rend="ul"&gt;&lt;item&gt;MobileGestalt once again messes us up our business&lt;/item&gt;&lt;item&gt;Device variant is a letter - usually "A" or "B"&lt;/item&gt;&lt;item&gt;It seems to depend on Wi-Fi transciever vendor used in exact device (?)&lt;/item&gt;&lt;item&gt;iOS 6 fails miserably to determine this value for iPod touch 3&lt;/item&gt;&lt;item&gt;This crashes activation process, for example&lt;/item&gt;&lt;item&gt;To fix it, I patch the function to always return "A" (in form of &lt;code&gt;CFString&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Fixing code signature&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;This is much easier than most people think&lt;/item&gt;
              &lt;item&gt;Shared cache files have the same format of signature as normal Mach-Os&lt;/item&gt;
              &lt;item&gt;And since it's just ad-hoc, all you need to do is to recalculate SHA-1 hash for pages you modified and update the signature&lt;/item&gt;
              &lt;item&gt;So easy, it can be done with just a hex-editor&lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The iBoot exploit&lt;/head&gt;
    &lt;p&gt;iOS 5 iBoot had a bug in HFS+ filesystem driver. I did make an exploit many years ago but it was bad. Like, truly bad. I reimplemented it from scratch for this project making it deterministic (hopefully...)&lt;/p&gt;
    &lt;p&gt;This subject probably deserves a separate article&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion &amp;amp; future plans&lt;/head&gt;
    &lt;p&gt;This was not easy to do, and yet easier than I expected initially&lt;/p&gt;
    &lt;p&gt;After releasing the tool many people asked me about jailbreaking. The old tools are not going to work, but it should be easy to just patch the kernel and drop Cydia tarball onto the filesystem. I guess I will give it a try later&lt;/p&gt;
    &lt;p&gt;There was another device that Apple dropped support for in that year - iPad 1. I will try that soon enough as well&lt;/p&gt;
    &lt;p&gt;I hope that the information from this write-up will help you making other crazy combinations, like iOS 4 on iPhone 4S or iOS 5 on iPad mini 1&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nyansatan.github.io/run-unsupported-ios/"/><published>2025-11-26T22:57:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46063450</id><title>C100 Developer Terminal</title><updated>2025-11-27T07:12:45.300475+00:00</updated><content>&lt;doc fingerprint="e4cb5b533733f9d0"&gt;
  &lt;main&gt;
    &lt;p&gt;c100 runs Workbench, a Linux-based operating system built for technical work. It‚Äôs designed to get out of your way, so your team can deliver more.&lt;/p&gt;
    &lt;p&gt;Most people don‚Äôt write code or manage data, and consumer devices are designed accordingly.&lt;/p&gt;
    &lt;p&gt;But change isn‚Äôt made by most people. Progress comes from the people whose work improves our understanding and ability.&lt;/p&gt;
    &lt;p&gt;Scientists and artists. Engineers and designers. Hackers and painters.&lt;/p&gt;
    &lt;p&gt;We think the world needs a brand of computing that stands behind creative technical work, dedicated to creating instead of consuming.&lt;/p&gt;
    &lt;p&gt;Caligra is a new computer company. Our goal is to help you make the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://caligra.com/"/><published>2025-11-26T23:22:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46064065</id><title>DSP 101 Part 1: An Introductory Course in DSP System Design</title><updated>2025-11-27T07:12:45.025589+00:00</updated><content>&lt;doc fingerprint="a51c909941780ae5"&gt;
  &lt;main&gt;
    &lt;p&gt;Having heard a lot about digital signal processing (DSP) technology, you may have wanted to find out what can be done with DSP, investigate why DSP is preferred to analog circuitry for many types of operations, and discover how to learn enough to design your own DSP system. This article, the first of a series, is an opportunity to take a substantial first step towards finding answers to your questions. This series is an introduction to DSP topics from the point of view of analog system designers seeking additional tools for handling analog signals. Designers reading this series can learn about the possibilities of DSP to deal with analog signals and where to find additional sources of information and assistance.&lt;/p&gt;
    &lt;p&gt;What is [a] DSP? In brief, DSPs are processors or microcomputers whose hardware, software, and instruction sets are optimized for high-speed numeric processing applications an essential for processing digital data representing analog signals in real time. What a DSP does is straightforward. When acting as a digital filter, for example, the DSP receives digital values based on samples of a signal, calculates the results of a filter function operating on these values, and provides digital values that represent the filter output; it can also provide system control signals based on properties of these values. The DSP‚Äôs high-speed arithmetic and logical hardware is programmed to rapidly execute algorithms modelling the filter transformation.&lt;/p&gt;
    &lt;p&gt;The combination of design elements arithmetic operators, memory handling, instruction set, parallelism, data addressing that provide this ability forms the key difference between DSPs and other kinds of processors. Understanding the relationship between real-time signals and DSP calculation speed provides some background on just how special this combination is. The real-time signal comes to the DSP as a train of individual samples from an analog-to-digital converter (ADC). To do filtering in real-time, the DSP must complete all the calculations and operations required for processing each sample (usually updating a process involving many previous samples) before the next sample arrives. To perform high-order filtering of real-world signals having significant frequency content calls for really fast processors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Use a DSP?&lt;/head&gt;
    &lt;p&gt;To get an idea of the type of calculations a DSP does and get an idea of how an analog circuit compares with a DSP system, one could compare the two systems in terms of a filter function. The familiar analog filter uses resistors, capacitors, inductors, amplifiers. It can be cheap and easy to assemble, but difficult to calibrate, modify, and maintain a difficulty that increases exponentially with filter order. For many purposes, one can more easily design, modify, and depend on filters using a DSP because the filter function on the DSP is software-based, flexible, and repeatable. Further, to create flexibly adjustable filters with higher-order response requires only software modifications, with no additional hardware unlike purely analog circuits. An ideal bandpass filter, with the frequency response shown in Figure 1, would have the following characteristics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a response within the passband that is completely flat with zero phase shift&lt;/item&gt;
      &lt;item&gt;infinite attenuation in the stopband.&lt;lb/&gt;Useful additions would include:&lt;/item&gt;
      &lt;item&gt;passband tuning and width control&lt;/item&gt;
      &lt;item&gt;stopband rolloff control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As Figure 1 shows, an analog approach using second-order filters would require quite a few staggered high-Q sections; the difficulty of tuning and adjusting it can be imagined.&lt;/p&gt;
    &lt;p&gt;With DSP software, there are two basic approaches to filter design: finite impulse response (FIR) and infinite impulse response (IIR). The FIR filter‚Äôs time response to an impulse is the straightforward weighted sum of the present and a finite number of previous input samples. Having no feedback, its response to a given sample ends when the sample reaches the ‚Äúend of the line‚Äù (Figure 2). An FIR filter‚Äôs frequency response has no poles, only zeros. The IIR filter, by comparison, is called infinite because it is a recursive function: its output is a weighted sum of inputs and outputs. Since it is recursive, its response can continue indefinitely. An IIR filter frequency response has both poles and zeros.&lt;/p&gt;
    &lt;p&gt;The xs are the input samples, ys are the output samples, as are input sample weightings, and bs are output sample weightings. n is the present sample time, and M and N are the number of samples programmed (the filter‚Äôs order). Note that the arithmetic operations indicated for both types are simply sums and products in potentially great number. In fact, multiply-and-add is the case for many DSP algorithms that represent mathematical operations of great sophistication and complexity.&lt;/p&gt;
    &lt;p&gt;Approximating an ideal filter consists of applying a transfer function with appropriate coefficients and a high enough order, or number of taps (considering the train of input samples as a tapped delay line). Figure 3 shows the response of a 90-tap FIR filter compared with sharp-cutoff Chebyshev filters of various orders. The 90-tap example suggests how close the filter can come to approximating an ideal filter. Within a DSP system, programming a 90-tap FIR filter like the one in Figure 3 is not a difficult task. By comparison, it would not be cost-effective to attempt this level of approximation with a purely analog circuit. Another crucial point in favor of using a DSP to approximate the ideal filter is long-term stability. With an FIR (or an IIR having sufficient resolution to avoid truncation-error buildup), the programmable DSP achieves the same response, time after time. Purely analog filter responses of high order are less stable with time.&lt;/p&gt;
    &lt;p&gt;Mathematical transform theory and practice are the core requirement for creating DSP applications and understanding their limits. This article series walks through a few signal-analysis and -processing examples to introduce DSP concepts. The series also provides references to texts for further study and identifies software tools that ease the development of signal-processing software.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sampling Real-World Signals&lt;/head&gt;
    &lt;p&gt;Real-world phenomena are analog the continuously changing energy levels of physical processes like sound, light, heat, electricity, magnetism. A transducer converts these levels into manageable electrical voltage and current signals, and an ADC samples and converts these signals to digital for processing. The conversion rate, or sampling frequency, of the ADC is critically important in digital processing of real-world signals.&lt;/p&gt;
    &lt;p&gt;This sampling rate is determined by the amount of signal information that is needed for processing the signals adequately for a given application. In order for an ADC to provide enough samples to accurately describe the real-world signal, the sampling rate must be at least twice the highest-frequency component of the analog signal. For example, to accurately describe an audio signal containing frequencies up to 20 kHz, the ADC must sample the signal at a minimum of 40 kHz. Since arriving signals can easily contain component frequencies above 20 kHz (including noise), they must be removed before sampling by feeding the signal through a low-pass filter ahead of the ADC. This filter, known as an anti-aliasing filter, is intended to remove the frequencies above 20 kHz that could corrupt the converted signal.&lt;/p&gt;
    &lt;p&gt;However, the anti-aliasing filter has a finite frequency rolloff, so additional bandwidth must be provided for the filter‚Äôs transition band. For example, with an input signal bandwidth of 20 kHz, one might allow 2 to 4 kHz of extra bandwidth.&lt;/p&gt;
    &lt;p&gt;Figure 4 depicts the filter needed to reject any signals with frequencies above half of a 48-kHz sampling rate. Rejection means attenuation to less than 1/2 least-significant bit (LSB) of the ADC‚Äôs resolution. One way to achieve this level of rejection without a highly sophisticated analog filter is to use an oversampling converter, such as a sigma-delta ADC. It typically obtains low-resolution (e.g., 1-bit) samples at megahertz rates much faster than twice the highest frequency component greatly easing the requirement for the analog filter ahead of the converter. An internal digital filter (DSP at work!) restores the required resolution and frequency response. For many applications, oversampling converters reduce system design effort and cost.&lt;/p&gt;
    &lt;head rend="h3"&gt;Processing Real-World Signals&lt;/head&gt;
    &lt;p&gt;The ADC sampling rate depends on the bandwidth of the analog signal being sampled. This sampling rate sets the pace at which samples are available for processing. Once the system bandwidth requirements have established the A/D converter sampling rate, the designer can begin to explore the speed requirements of the DSP processor.&lt;/p&gt;
    &lt;p&gt;Processing speed at a required sample rate is influenced by algorithm complexity. As a rule, the DSP needs to finish all operations relating to the first sample before receiving the second sample. The time between samples is the time budget for the DSP to perform all processing tasks. For the audio example, a 48-kHz sampling rate corresponds to a 20.833-¬µs sampling interval. Figure 5 relates the analog signal and digital sampling rate.&lt;/p&gt;
    &lt;p&gt;Next consider the relation between the speed of the DSP and complexity of the algorithm (the software containing the transform or other set of numeric operations). Complex algorithms require more processing tasks. Because the time between samples is fixed, the higher complexity calls for faster processing. For example, suppose that the algorithm requires 50 processing operations to be performed between samples. Using the previous example‚Äôs 48-kHz sampling rate (20.833-¬µs sampling interval), one can calculate the minimum required DSP processor speed, in millions of operations per second (MOPS) as follows:&lt;/p&gt;
    &lt;p&gt;Thus if all of the time between samples is available for operations to implement the algorithm, a processor with a performance level of 2.4 MOPS is required. Note that the two common ratings for DSPs, based on operations per second (MOPS) and instructions per second (MIPS), are not the same. A processor with a 10-MIPS rating that can perform 8 operations per instruction has basically the same performance as a faster processor with a 40 MIPS rating that can only perform 2 operations per instruction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sampling Various Real-World Signals&lt;/head&gt;
    &lt;p&gt;There are two basic ways to acquire data, either one sample at a time or one frame at a time (continuous processing vs. batch processing). Sample-based systems, like a digital filter, acquire data one sample at a time. As shown in Figure 6, at each tick of the clock, a sample comes into the system and a processed sample is output. The output waveform develops continuously.&lt;/p&gt;
    &lt;p&gt;Frame-based systems, like a spectrum analyzer, which determines the frequency components of a time-varying waveform, acquire a frame (or block of samples). Processing occurs on the entire frame of data and results in a frame of transformed data, as shown in Figure 7.&lt;/p&gt;
    &lt;p&gt;For an audio sampling rate of 48 kHz, a processor working on a frame of 1024 samples has a frame acquisition interval of 21.33 ms (i.e., 1024 x 20.833 ¬µs = 21.33 ms). Here the DSP has 21.33 ms to complete all the required processing tasks for that frame of data. If the system handles signals in real time, it must not lose any data; so while the DSP is processing the first frame, it must also be acquiring the second frame. Acquiring the data is one area where special architectural features of DSPs come into play: Seamless data acquisition is facilitated by a processor‚Äôs flexible data-addressing capabilities in conjunction with its direct memory-accessing (DMA) channels.&lt;/p&gt;
    &lt;head rend="h3"&gt;Responding to Real-World Signals&lt;/head&gt;
    &lt;p&gt;One cannot assume that all the time between samples is available for the execution of processing instructions. In reality, time must be budgeted for the processor to respond to external devices, controlling the flow of data in and out. Typically, an external device (such as an ADC) signals the processor using an interrupt. The DSP‚Äôs response time to that interrupt, or interrupt latency, directly influences how much time remains for actual signal processing.&lt;/p&gt;
    &lt;p&gt;Interrupt latency (response delay) depends on several factors; the most dominant is the DSP architecture‚Äôs instruction pipelining. An instruction pipeline consists of the number of instruction cycles that occur between the time an interrupt is received and the time that program execution resumes. More pipeline levels in a DSP result in longer interrupt latency. For example, if a processor has a 20-ns cycle time and requires 10 cycles to respond to an interrupt, 200 ns elapse before it executes any signal-processing instructions.&lt;/p&gt;
    &lt;p&gt;When data is acquired one sample at a time, this 200-ns overhead will not hurt if the DSP finishes the processing of each sample before the next arrives. When data is acquired sample-by-sample while processing a frame at a time, however, an interrupted system wastes processor instruction cycles. For example, a system with a 200-ns interrupt response time running a frame-based algorithm, such as the FFT, with a frame size of 1024 samples, would require 204.8 ¬µs of overhead. That amounts to more than 10,000 instruction cycles wasted to latency productive time when the DSP could be performing signal processing. This waste is easy to avoid in DSPs having architectural features such as DMA and dual memory access; they let the DSP receive and store data without interrupting the processor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Developing a DSP System&lt;/head&gt;
    &lt;p&gt;Having discussed the role of the processor, the ADC, the anti-aliasing filter, and the timing relationships between these components, it is time to look at a complete DSP system. Figure 8 shows the building blocks of a typical DSP system that could be used for data acquisition and control.&lt;/p&gt;
    &lt;p&gt;Note how few components make up the DSP system, because so much of the system‚Äôs functionality comes from the programmable DSP. Converters funnel data into and out of the DSP; the ADC timing is controlled by a precise sampling clock. To simplify system design, many converter devices available today combine some or all of the following: an A/D converter, a D/A converter, a sampling clock, and filters for anti-aliasing and anti-imaging. The clock oscillator in these types of I/O components is separately controlled by an external crystal. Here are some important points about the data flow in this sort of DSP system:&lt;/p&gt;
    &lt;p&gt;Analog Input: The analog signal is appropriately band-limited by the anti-aliasing filter and applied to the input of the ADC. At the selected sampling time, the converter interrupts the DSP processor and makes the digital sample available. The choice between serial and parallel interfacing between the ADC and DSP depends on the amount of data, design complexity trade-offs, space, power, and price.&lt;/p&gt;
    &lt;p&gt;Digital Signal Processing: The incoming data is handled by the DSP‚Äôs algorithm software. When the processor completes the required calculations, it sends the result to the DAC. Because the signal processing is programmable, considerable flexibility is available in handling the data and improving system performance with incremental programming adjustments.&lt;/p&gt;
    &lt;p&gt;Analog Output: The DAC converts the DSP‚Äôs output into the desired analog output at the next sample clock. The converter‚Äôs output is smoothed by a low-pass, anti-imaging filter (also called a reconstruction filter), to produce the reconstructed analog signal.&lt;/p&gt;
    &lt;p&gt;Host Interface: An optional host interface lets the DSP communicate with external systems, sending and receiving data and control information.&lt;/p&gt;
    &lt;head rend="h3"&gt;Review and Preview&lt;/head&gt;
    &lt;p&gt;The goal of this article has been to provide an overview of major DSP design concepts and explain some of the reasons why a DSP is better suited that analog circuitry for some applications. The issues introduced in this article include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DSP overview&lt;/item&gt;
      &lt;item&gt;Real-time DSP operation&lt;/item&gt;
      &lt;item&gt;Real-world signals&lt;/item&gt;
      &lt;item&gt;Sampling rates and anti-alias filtering&lt;/item&gt;
      &lt;item&gt;DSP algorithm time budget&lt;/item&gt;
      &lt;item&gt;Sample driven versus frame driven data acquisition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Because these issues involve many valuable levels of detail that we could not do justice to in this brief article, you should consider reading Richard Higgins‚Äôs text, Digital Signal Processing in VLSI (see References below). This text provides a complete overview of DSP theory, implementation issues, and reduction to practice (with devices available at the time it was published), plus exercises and examples. The Reference section below also contains other sources that further amplify this article‚Äôs issues. To prepare for the next articles in this series, you might want to get free copies of the ADSP-2100 Family User‚Äôs Manual* and the ADSP-2106x SHARC User‚Äôs Manual.* These texts provide information on Analog Devices‚Äôs fixed- and floating-point DSP architectures, a major topic in these articles. The next article will cover the following territory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mathematical overview of signal processing: It will present the mathematics for the transform functions (frequency domain) and convolution functions (time domain) that appear throughout the series. While the mathematical treatment is necessarily incomplete (no derivations), there will be sufficient detail for considering how to program the operations.&lt;/item&gt;
      &lt;item&gt;DSP architecture: The article will discuss the nature and functioning of the DSP‚Äôs arithmetic-logic unit (ALU), multiply-accumulator (MAC), barrel-shifter, and memory busses and describe the numeric operations that support DSP functions.&lt;/item&gt;
      &lt;item&gt;DSP programming concepts: A discussion of programming will bring together theory and practice (math and architecture). Finally, it will lay out the main parameters for a series-length DSP design project, provided as an example.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;p&gt;Higgins, R. J. Digital Signal Processing in VLSI, Englewood Cliffs, NJ: Prentice Hall, 1990. DSP basics. Includes a wide-ranging bibliography. Available for purchase from ADI.&lt;/p&gt;
    &lt;p&gt;Mar, A., ed. Digital Signal Processing Applications Using the ADSP-2100 Family Volume 1, Englewood Cliffs, NJ: Prentice Hall, 1992. Available for purchase from ADI.&lt;/p&gt;
    &lt;p&gt;Mar, A., Babst, J., eds. Digital Signal Processing Applications Using the ADSP-2100 Family Volume 2, Englewood Cliffs, NJ: Prentice Hall, 1994. Available for purchase from ADI.&lt;/p&gt;
    &lt;p&gt;Dearborn, G., ed. Digital Signal Processing Applications Using the ADSP-21000 Family Volume 1, Norwood, MA: Analog Devices, Inc., 1994. Available for purchase from ADI.&lt;/p&gt;
    &lt;p&gt;*Mar, A., Rempel, H., eds. ADSP-2100 Family User‚Äôs Manual, Norwood, MA: Analog Devices, Inc., 1995. Free.&lt;/p&gt;
    &lt;p&gt;Mar, A., Rempel, H., eds. ADSP-21020 Family User‚Äôs Manual, Norwood, MA: Analog Devices, Inc., 1995. Free.&lt;/p&gt;
    &lt;p&gt;*Rempel, H., ed. ADSP-21060/62 SHARC User‚Äôs Manual, Norwood, MA: Analog Devices, Inc., 1995. Free.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.analog.com/en/resources/analog-dialogue/articles/dsp-101-part-1.html"/><published>2025-11-27T00:42:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46064367</id><title>Bonsai_term: A library for building dynamic terminal apps by Jane Street</title><updated>2025-11-27T07:12:43.499617+00:00</updated><content>&lt;doc fingerprint="f44b01f5dfe5990"&gt;
  &lt;main&gt;
    &lt;p&gt;Bonsai_term is a library that lets you write Terminal UIs (TUIs) using OCaml. It uses the same programming model as the &lt;code&gt;bonsai_web&lt;/code&gt; library.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If you are new to OCaml - or if you haven't already - install opam. It is OCaml's package manager and we'll be using it to install &lt;code&gt;bonsai_term&lt;/code&gt;and its dependencies. The specific installation instructions depend on your platform. You can find platform-specific instructions here.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bonsai_term&lt;/code&gt;uses OxCaml so the next thing you'll want to do is install&lt;code&gt;oxcaml&lt;/code&gt;by following the instructions here.&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;opam install bonsai_term&lt;/code&gt;. (This will install&lt;code&gt;bonsai_term&lt;/code&gt;and its dependencies).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At this point you should now have &lt;code&gt;bonsai_term&lt;/code&gt; "installed".&lt;/p&gt;
    &lt;p&gt;To learn how to use &lt;code&gt;bonsai_term&lt;/code&gt; you can read its MLI &lt;code&gt;src/bonsai_term.mli&lt;/code&gt; and / or look
at some examples in the
bonsai_term_examples repo.&lt;/p&gt;
    &lt;p&gt;To learn how to use &lt;code&gt;bonsai&lt;/code&gt;, you can read the docs in
bonsai_web.
(most of those docs are aimed at the "web" version of bonsai, so the "vdom" bits may not
apply, but the "effect" / "state-fulness" and ways of doing "incrementality" all should
transfer from &lt;code&gt;bonsai_web&lt;/code&gt; into &lt;code&gt;bonsai_term&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;To learn how to use &lt;code&gt;ocaml&lt;/code&gt; here are some good resources:&lt;/p&gt;
    &lt;p&gt;If you followed the install instructions at the top of this page, you can skip the "Install" instructions on the above links.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/janestreet/bonsai_term"/><published>2025-11-27T01:20:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46064571</id><title>Migrating the main Zig repository from GitHub to Codeberg</title><updated>2025-11-27T07:12:42.926009+00:00</updated><content>&lt;doc fingerprint="8374b4d94ff22fcb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Migrating from GitHub to Codeberg&lt;/head&gt;
    &lt;head rend="h3"&gt;November 26, 2025&lt;/head&gt;
    &lt;p&gt;https://codeberg.org/ziglang/zig&lt;/p&gt;
    &lt;p&gt;Ever since &lt;code&gt;git init&lt;/code&gt; ten years ago, Zig has been hosted on GitHub. Unfortunately, when it sold out to Microsoft, the clock started ticking. √¢Please just give me 5 years before everything goes to shit,√¢ I thought to myself. And here we are, 7 years later, living on borrowed time.&lt;/p&gt;
    &lt;p&gt;Putting aside GitHub√¢s relationship with ICE, it√¢s abundantly clear that the talented folks who used to work on the product have moved on to bigger and better things, with the remaining rookies eager to inflict some kind of bloated, buggy JavaScript framework on us in the name of progress. Stuff that used to be snappy is now sluggish and often entirely broken.&lt;/p&gt;
    &lt;p&gt;More importantly, Actions is created by monkeys and completely neglected. After the CEO of GitHub said to √¢embrace AI or get out√¢, it seems the lackeys at Microsoft took the hint, because GitHub Actions started √¢vibe-scheduling√¢; choosing jobs to run seemingly at random. Combined with other bugs and inability to manually intervene, this causes our CI system to get so backed up that not even master branch commits get checked.&lt;/p&gt;
    &lt;p&gt;Rather than wasting donation money on more CI hardware to work around this crumbling infrastructure, we√¢ve opted to switch Git hosting providers instead.&lt;/p&gt;
    &lt;p&gt;As a bonus, we look forward to fewer violations (exhibit A, B, C) of our strict no LLM / no AI policy, which I believe are at least in part due to GitHub aggressively pushing the √¢file an issue with Copilot√¢ feature in everyone√¢s face.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub Sponsors&lt;/head&gt;
    &lt;p&gt;The only concern we have in leaving GitHub behind has to do with GitHub Sponsors. This product was key to Zig√¢s early fundraising success, and it remains a large portion of our revenue today. I can√¢t thank Devon Zuegel enough. She appeared like an angel from heaven and single-handedly made GitHub into a viable source of income for thousands of developers. Under her leadership, the future of GitHub Sponsors looked bright, but sadly for us, she, too, moved on to bigger and better things. Since she left, that product as well has been neglected and is already starting to decline.&lt;/p&gt;
    &lt;p&gt;Although GitHub Sponsors is a large fraction of Zig Software Foundation√¢s donation income, we consider it a liability. We humbly ask if you, reader, are currently donating through GitHub Sponsors, that you consider moving your recurring donation to Every.org, which is itself a non-profit organization.&lt;/p&gt;
    &lt;p&gt;As part of this, we are sunsetting the GitHub Sponsors perks. These perks are things like getting your name onto the home page, and getting your name into the release notes, based on how much you donate monthly. We are working with the folks at Every.org so that we can offer the equivalent perks through that platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration Plan&lt;/head&gt;
    &lt;p&gt;Effective immediately, I have made ziglang/zig on GitHub read-only, and the canonical origin/master branch of the main Zig project repository is &lt;code&gt;https://codeberg.org/ziglang/zig.git&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Thank you to the Forgejo contributors who helped us with our issues switching to the platform, as well as the Codeberg folks who worked with us on the migration - in particular Earl Warren, Otto, Gusted, and Mathieu Fenniak.&lt;/p&gt;
    &lt;p&gt;In the end, we opted for a simple strategy, sidestepping GitHub√¢s aggressive vendor lock-in: leave the existing issues open and unmigrated, but start counting issues at 30000 on Codeberg so that all issue numbers remain unambiguous. Let us please consider the GitHub issues that remain open as metaphorically √¢copy-on-write√¢. Please leave all your existing GitHub issues and pull requests alone. No need to move your stuff over to Codeberg unless you need to make edits, additional comments, or rebase. We√¢re still going to look at the already open pull requests and issues; don√¢t worry.&lt;/p&gt;
    &lt;p&gt;In this modern era of acquisitions, weak antitrust regulations, and platform capitalism leading to extreme concentrations of wealth, non-profits remain a bastion defending what remains of the commons.&lt;/p&gt;
    &lt;p&gt;Happy hacking,&lt;/p&gt;
    &lt;p&gt;Andrew&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ziglang.org/news/migrating-from-github-to-codeberg/"/><published>2025-11-27T01:49:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46064680</id><title>Functional Data Structures and Algorithms: a Proof Assistant Approach</title><updated>2025-11-27T07:12:42.790415+00:00</updated><content>&lt;doc fingerprint="96e790871cbf635a"&gt;
  &lt;main&gt;
    &lt;p&gt;This book is an introduction to data structures and algorithms for functional languages, with a focus on proofs. It covers both functional correctness and running time analysis. It does so in a unified manner with inductive proofs about functional programs and their running time functions. All proofs have been machine-checked by the proof assistant Isabelle. The pdf contains links to the corresponding Isabelle theories.&lt;/p&gt;
    &lt;p&gt;Click on an image to download the pdf of the whole book:&lt;/p&gt;
    &lt;p&gt;This book is meant to evolve over time. If you would like to contribute, get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fdsa-book.net/"/><published>2025-11-27T02:04:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46064757</id><title>Penpot: The Open-Source Figma</title><updated>2025-11-27T07:12:42.176768+00:00</updated><content>&lt;doc fingerprint="d448ea5b7d378672"&gt;
  &lt;main&gt;
    &lt;p&gt;Website ‚Ä¢ User Guide ‚Ä¢ Learning Center ‚Ä¢ Community&lt;/p&gt;
    &lt;p&gt;Youtube ‚Ä¢ Peertube ‚Ä¢ Linkedin ‚Ä¢ Instagram ‚Ä¢ Mastodon ‚Ä¢ Bluesky ‚Ä¢ X&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;Penpot_OpenYourEyes_.mp4&lt;/head&gt;
    &lt;p&gt;Penpot is the first open-source design tool for design and code collaboration. Designers can create stunning designs, interactive prototypes, design systems at scale, while developers enjoy ready-to-use code and make their workflow easy and fast. And all of this with no handoff drama.&lt;/p&gt;
    &lt;p&gt;Available on browser or self-hosted, Penpot works with open standards like SVG, CSS, HTML and JSON, and it‚Äôs free!&lt;/p&gt;
    &lt;p&gt;The latest updates take Penpot even further. It‚Äôs the first design tool to integrate native design tokens‚Äîa single source of truth to improve efficiency and collaboration between product design and development. With the huge 2.0 release, Penpot took the platform to a whole new level. This update introduces the ground-breaking CSS Grid Layout feature, a complete UI redesign, a new Components system, and much more. For organizations that need extra service for its teams, get in touch&lt;/p&gt;
    &lt;p&gt;üéá Design, code, and Open Source meet at Penpot Fest! Be part of the 2025 edition in Madrid, Spain, on October 9-10.&lt;/p&gt;
    &lt;p&gt;Penpot expresses designs as code. Designers can do their best work and see it will be beautifully implemented by developers in a two-way collaboration.&lt;/p&gt;
    &lt;p&gt;Penpot plugins let you expand the platform's capabilities, give you the flexibility to integrate it with other apps, and design custom solutions.&lt;/p&gt;
    &lt;p&gt;Penpot was built to serve both designers and developers and create a fluid design-code process. You have the choice to enjoy real-time collaboration or play "solo".&lt;/p&gt;
    &lt;p&gt;Work with ready-to-use code and make your workflow easy and fast. The inspect tab gives instant access to SVG, CSS and HTML code.&lt;/p&gt;
    &lt;p&gt;Provide your team or organization with a completely owned collaborative design tool. Use Penpot's cloud service or deploy your own Penpot server.&lt;/p&gt;
    &lt;p&gt;Penpot offers integration into the development toolchain, thanks to its support for webhooks and an API accessible through access tokens.&lt;/p&gt;
    &lt;p&gt;Penpot brings design systems to code-minded teams: a single source of truth with native Design Tokens, Components, and Variants for scalable, reusable, and consistent UI across projects and platforms.&lt;/p&gt;
    &lt;p&gt;Penpot is the only design &amp;amp; prototype platform that is deployment agnostic. You can use it in our SAAS or deploy it anywhere.&lt;/p&gt;
    &lt;p&gt;Learn how to install it with Docker, Kubernetes, Elestio or other options on our website. &lt;/p&gt;
    &lt;p&gt;We love the Open Source software community. Contributing is our passion and if it‚Äôs yours too, participate and improve Penpot. All your designs, code and ideas are welcome!&lt;/p&gt;
    &lt;p&gt;If you need help or have any questions; if you‚Äôd like to share your experience using Penpot or get inspired; if you‚Äôd rather meet our community of developers and designers, join our Community!&lt;/p&gt;
    &lt;p&gt;You will find the following categories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ask the Community&lt;/item&gt;
      &lt;item&gt;Troubleshooting&lt;/item&gt;
      &lt;item&gt;Help us Improve Penpot&lt;/item&gt;
      &lt;item&gt;#MadeWithPenpot&lt;/item&gt;
      &lt;item&gt;Events and Announcements&lt;/item&gt;
      &lt;item&gt;Inside Penpot&lt;/item&gt;
      &lt;item&gt;Penpot in your language&lt;/item&gt;
      &lt;item&gt;Design and Code Essentials&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Anyone who contributes to Penpot, whether through code, in the community, or at an event, must adhere to the code of conduct and foster a positive and safe environment.&lt;/p&gt;
    &lt;p&gt;Any contribution will make a difference to improve Penpot. How can you get involved?&lt;/p&gt;
    &lt;p&gt;Choose your way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create and share Libraries &amp;amp; Templates that will be helpful for the community&lt;/item&gt;
      &lt;item&gt;Invite your team to join&lt;/item&gt;
      &lt;item&gt;Give this repo a star and follow us on Social Media: Mastodon, Youtube, Instagram, Linkedin, Peertube, X and BlueSky&lt;/item&gt;
      &lt;item&gt;Participate in the Community space by asking and answering questions; reacting to others‚Äô articles; opening your own conversations and following along on decisions affecting the project.&lt;/item&gt;
      &lt;item&gt;Report bugs with our easy guide for bugs hunting or GitHub issues&lt;/item&gt;
      &lt;item&gt;Become a translator&lt;/item&gt;
      &lt;item&gt;Give feedback: Email us&lt;/item&gt;
      &lt;item&gt;Contribute to Penpot's code: Watch this video by Alejandro Alonso, CIO and developer at Penpot, where he gives us a hands-on demo of how to use Penpot‚Äôs repository and make changes in both front and back end&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To find (almost) everything you need to know on how to contribute to Penpot, refer to the contributing guide.&lt;/p&gt;
    &lt;p&gt;You can ask and answer questions, have open-ended conversations, and follow along on decisions affecting the project.&lt;/p&gt;
    &lt;p&gt;‚úèÔ∏è Tutorials&lt;/p&gt;
    &lt;p&gt;üèòÔ∏è Architecture&lt;/p&gt;
    &lt;code&gt;This Source Code Form is subject to the terms of the Mozilla Public
License, v. 2.0. If a copy of the MPL was not distributed with this
file, You can obtain one at http://mozilla.org/MPL/2.0/.

Copyright (c) KALEIDOS INC
&lt;/code&gt;
    &lt;p&gt;Penpot is a Kaleidos‚Äô open source project&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/penpot/penpot"/><published>2025-11-27T02:14:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065034</id><title>DIY NAS: 2026 Edition</title><updated>2025-11-27T07:12:41.816889+00:00</updated><content>&lt;doc fingerprint="e640301301a64f39"&gt;
  &lt;main&gt;
    &lt;p&gt;Fourteen years ago, my storage needs outpaced my capacity and I began to look into building a network attached storage server. I had a few criteria in mind and was curious to see if anyone had _ recently_ shared something similar, but I couldn√¢t find anything that was relevant.&lt;/p&gt;
    &lt;p&gt;In fact, I found that the communities I was looking for answers in were actively hostile towards what I wanted to do. This resulted in my decision to build my own DIY NAS and share that as one of my very first blogs.&lt;/p&gt;
    &lt;p&gt;Much to my surprise, people were very interested in that blog! Ever since, I√¢ve been building a similar DIY NAS machine almost every year trying to satisfy the curiosity of other prospective DIY NAS builders.&lt;/p&gt;
    &lt;p&gt;Here are those criteria:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Small form factor: It√¢s not the case for me any more, but at the time the space was limited in my office. I always assume that space in everybody√¢s office is limited. As a result, I want my DIY NAS builds to occupy as little of that office space as I can.&lt;/item&gt;
      &lt;item&gt;At least six drive bays: Back when I built my NAS, it took about four drives√¢ worth of storage to meet my storage needs. Plus I desired two empty drive bays for future use. However, in the years since hard drive capacities have increased dramatically. At some point in the future, I may reduce this to four drive bays.&lt;/item&gt;
      &lt;item&gt;An integrated, low power CPU: I intend my DIY NAS to run 24 hours a day, 7 days a week, and 52 weeks a year. When it comes to power consumption, that can do some damage on your electric bill! Thankfully our electricity here isn√¢t as expensive as others√¢ in the United States, or even further outside its borders, but I try and keep power consumption in mind when picking components for a DIY NAS build.&lt;/item&gt;
      &lt;item&gt;Homelab potential: It does not take up a lot of CPU horsepower for a NAS to serve up files, which means that on modern hardware there√¢s a lot of untapped potential in a DIY NAS for virtual machines or containers to self-host services.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It√¢s important to remember that these are my criteria, and not necessarily yours. Every DIY NAS builder should be making their own list of criteria and reconcile all of their component purchases against the criteria that√¢s important to them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is it even a good time to build a NAS?&lt;/head&gt;
    &lt;p&gt;As I prepared to build this NAS, component prices disappointed me. Hard drives, SSDs, and RAM prices were all rising. Based on what I√¢ve been told, I expect Intel CPU prices to increase as well. My contact at Topton has been encouraging me to stock up on motherboards while they still have some in inventory. Based on what√¢s been explained to me, I expect the motherboard√¢s prices to rise and for their availability to potentially dwindle.&lt;/p&gt;
    &lt;p&gt;In short, the economy sucks and the price of DIY NAS components is a pretty good reflection of just how sucky things are becoming. I briefly considered not publishing a DIY NAS build this year hoping that things would improve a few months down the road. But then I asked myself, √¢What if it√¢s even worse in a few months?√¢&lt;/p&gt;
    &lt;p&gt;I sure hope things get better, but I fear and expect that they√¢ll get worse.&lt;/p&gt;
    &lt;head rend="h2"&gt;Motherboard and CPU&lt;/head&gt;
    &lt;p&gt;I built my first DIY NAS with a Topton motherboard in 2023. Each DIY NAS since then has also featured a Topton motherboard. My only complaint about the motherboards has been that buying them from one of the Chinese e-tail sites like AliExpress is considered problematic by some. With every DIY NAS build, I try and go through all the motherboards that I can find while searching for something with a better value proposition, but for each of the past three years I√¢ve landed on the latest offering from Topton.&lt;/p&gt;
    &lt;p&gt;For the DIY NAS: 2026 Edition, I chose the Topton N22 motherboard with the Intel Core 3 N355 CPU. The motherboard is similar to last year√¢s Topton N18 but has incrementally more compelling features, particularly the extra 2 SATA ports, the PCI-e x1 slot, and the N355 CPU!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mini-ITX Form Factor&lt;/item&gt;
      &lt;item&gt;Intel√Ç¬Æ Processor Core 3 N355 &lt;list rend="ul"&gt;&lt;item&gt;8 cores / 8 threads / Max Turbo 3.9GHz&lt;/item&gt;&lt;item&gt;15 W TDP&lt;/item&gt;&lt;item&gt;Integrated GPU with Intel Quick Sync Video&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;1 x DDR5 SO-DIMM&lt;/item&gt;
      &lt;item&gt;8 x SATA 3.0 Ports (Asmedia ASM1164)&lt;/item&gt;
      &lt;item&gt;2 x M.2 NVMe Slots (PCIe 3.0 x1)&lt;/item&gt;
      &lt;item&gt;1 x 10Gbps NIC (Marvell AQC113C)&lt;/item&gt;
      &lt;item&gt;2 x 2.5Gbps NICs (Intel i226-V)&lt;/item&gt;
      &lt;item&gt;1 x PCI-e x1 or M.2 E-Key slot&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I opted for the motherboard with the Intel Core 3 N355 CPU. This makes the server a more capable homelab machine than prior years√¢ DIY NAS builds. The extra cores and threads come in handy for streaming media, replacing your cloud storage, facilitating home automation, hosting game servers, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Case&lt;/head&gt;
    &lt;p&gt;Just like Topton has been making great motherboards for DIY NAS machines, JONSBO has been steadily releasing great cases for DIY NAS machines. This year SilverStone Technology released a new case, the CS383 (specs) which I was very interested in buying one for the DIY NAS: 2026 Edition. Unfortunately it carries a pretty hefty price tag to go along with all of its incredible features!&lt;/p&gt;
    &lt;p&gt;The JONSBO N4 (specs) is a third the price, adheres to my √¢smaller footprint√¢ criteria, and it is rather impressive on its own. It√¢s a tiny bit larger case than last year√¢s DIY NAS, but I really like that it has drive bays for six 3.5√¢ drives and two 2.5√¢ drives.&lt;/p&gt;
    &lt;p&gt;Although, it√¢s peculiar in that two of the 3.5√¢ drive bays (and the two 2.5√¢ drive bays) aren√¢t attached to a SATA backplane and can√¢t be swapped anywhere as easily as the other four 3.5√¢ bays. However, this peculiar decision seems to have caused the JONSBO N4 to sell for a bit less ($20-$40) than similar offerings from JONSBO. At its price, it√¢s a compelling value proposition!&lt;/p&gt;
    &lt;head rend="h3"&gt;Case Fan&lt;/head&gt;
    &lt;p&gt;In the past, I√¢ve found that the fans which come with JONSBO cases are too noisy. They√¢ve been noisy for two reasons; the design quality of the fans make them loud. And the fans are constantly running at their top speed because of the fan header they√¢re plugged into on the cases√¢ SATA backplanes.&lt;/p&gt;
    &lt;p&gt;I anticipated that fan efficiency and noise would be a problem, so I picked out the Noctua NF-A12x25 PWM to solve it. Firstly, swapping in a high-quality fan that pushes more air and generates less noise√¢especially at its top speed√¢is a good first step. Secondly, I√¢d address the problem by plugging the fan into the motherboard√¢s &lt;code&gt;SYS_FAN&lt;/code&gt; header instead of on the SATA backplane. This provides the opportunity to tune the fan√¢s RPMs directly in the BIOS and generate far less noise.&lt;/p&gt;
    &lt;head rend="h2"&gt;RAM&lt;/head&gt;
    &lt;p&gt;The first time I first asked myself, √¢Should I even build the DIY NAS: 2026 Edition?√¢ came as I was checking prices on DDR5 memory. Thankfully for me I had leftover RAM after purchasing DDR5 4800MHz SODIMMs for the DIY NAS: 2025 Edition, the Pocket Mini NAS, and then again for the DIY NAS that I built and gave away at 2025√¢s Texas Linux Fest. I was personally thankful that I had one brand new 32GB DDR5 4800MHz SODIMM laying around, but I was wildly disappointed for everybody who will try and follow this build when I saw the price of those same SODIMMs.&lt;/p&gt;
    &lt;p&gt;Regardless, I felt a Crucial 32GB DDR5 4800MHz SODIMM (specs) was the right amount of RAM to get started with for a DIY NAS build in 2025. Whether you just need storage or you wish to also host virtual machines, you will benefit from having more than the bare minimum recommendation of RAM. I really wanted to buy a 48GB DDR5 4800MHZ SODIMM for this DIY NAS build, but I couldn√¢t talk myself into spending the $250-$300 that it would√¢ve wound up costing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Storage&lt;/head&gt;
    &lt;p&gt;A quick disclaimer about all the drives that I purchased for the DIY NAS: 2026 Edition, I already had all of them! I tend to buy things when I see them on sale and as a result, I have a collection of brand new parts for machines in my homelab or for upcoming projects. I raided that collection of spare parts for the DIY NAS: 2026 Edition.&lt;/p&gt;
    &lt;head rend="h3"&gt;Boot Drive&lt;/head&gt;
    &lt;p&gt;If you ranked the drives in your DIY NAS in order of importance, the boot drive should be the least-important drive. That is not saying that boot drive isn√¢t performing an important function, but I am suggesting that you shouldn√¢t invest a bunch of energy and money into picking the optimal boot drive.&lt;/p&gt;
    &lt;p&gt;Because the JONSBO N4 has a pair of 2.5√¢ drive bays, I decided that a 2.5√¢ SATA SSD would be ideal for the boot drives. As a rule of thumb, I try and spend less than $30 per boot drive in my DIY NAS builds.&lt;/p&gt;
    &lt;p&gt;Ultimately I selected a pair of 128GB Silicon Power A55 SSDs (specs). I√¢ve used these before, I√¢d use them again in the future, and I even have four of their higher capacity (1TB) SSDs in a pool in my own NAS.&lt;/p&gt;
    &lt;head rend="h3"&gt;App and Virtual Machine NVMe SSDs&lt;/head&gt;
    &lt;p&gt;Self-hosting apps and virtual machines on your DIY NAS has really exploded in the past few years. The developers of NAS appliance packages have made it much easier and the self-hosted products themselves have become as good√¢or often better√¢than things you√¢re probably subscribing to today. Because of that, I saved the highest-performing storage options on the Topton N22 motherboard for apps and VMs.&lt;/p&gt;
    &lt;p&gt;However, it√¢s important to point out that these M.2 slots are PCI-e version 3 and capped at a single PCI-e lane. This is a consequence of the limited number of PCI-e lanes available for each of the CPU options available for the Topton N22 motherboard (N100, N150, N305, and N355).&lt;/p&gt;
    &lt;p&gt;I opted for a NVMe drive that was a good value rather than a high performer and chose two of the Silicon Power 1TB M.2 NVMe SSDs (SP001TBP34A60M28) (specs).&lt;/p&gt;
    &lt;head rend="h3"&gt;Bulk Storage Hard Disk Drives&lt;/head&gt;
    &lt;p&gt;Thanks to rising prices, I opted to do like I√¢ve done with past DIY NAS builds and skip buying hard drives for the DIY NAS: 2026 Edition.&lt;/p&gt;
    &lt;p&gt;When planning your DIY NAS, it is good to always remember that storage will ultimately be your costliest and most important expense.&lt;/p&gt;
    &lt;p&gt;Here√¢s a few things to consider when buying hard drives:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine your hardware redundancy preferences. I recommend having two hard disk drives√¢ worth of redundancy (RAIDZ2, RAID6, etc.)&lt;/item&gt;
      &lt;item&gt;Focus on price-per-terabyte when comparing prices of drives.&lt;/item&gt;
      &lt;item&gt;Do some burn in testing of your hard drives before putting them to use.&lt;/item&gt;
      &lt;item&gt;When buying new drives of the same model, try and buy them from multiple vendors to increase the chances of buying drives manufactured in separate batches.&lt;/item&gt;
      &lt;item&gt;Plan Ahead! Understand the rate that your storage grows so that you can craft a strategy to grow your storage down the road.&lt;/item&gt;
      &lt;item&gt;Being cheap today can and will paint you into a corner that√¢s quite expensive to get out of.&lt;/item&gt;
      &lt;item&gt;Understand that RAID is not a backup!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thankfully, I√¢ve collected a bunch of my own decomissioned hard drives which I used to thoroughly test this DIY NAS build.&lt;/p&gt;
    &lt;head rend="h2"&gt;SATA Cables&lt;/head&gt;
    &lt;p&gt;One of the under-the-radar features of the Topton N22 motherboard might be one of my favorite features! The motherboard√¢s Asmedia ASM1164 SATA controllers sit behind two SFF-8643 connectors. These connectors provide two advantages for these motherboards:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Saves room on the motherboard√¢s PCB.&lt;/item&gt;
      &lt;item&gt;SFF-8643 to 4x SATA breakout cables reduces the amount of cable management hassle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Power Supply&lt;/head&gt;
    &lt;p&gt;The one thing that I have routinely disliked about building small form factor DIY NAS machines is the price tag that accompanies a small form factor power supply (SFX) like is required with the JONSBO N4.&lt;/p&gt;
    &lt;p&gt;I wound up choosing the SilversStone Technology SX500-G (specs) which I had used earlier in the year for the DIY NAS I gave away at Texas Linux Fest. Its 500W rating exceeds the needs of all the components that I√¢d picked out for the DIY NAS: 2026 Edition. Plus the power supply√¢s 80 Plus Gold rating aligns well with my criteria for power efficiency.&lt;/p&gt;
    &lt;head rend="h2"&gt;TrueNAS Community Edition&lt;/head&gt;
    &lt;p&gt;Regardless of whether it was called FreeNAS, TrueNAS, TrueNAS CORE, TrueNAS SCALE, or now TrueNAS Community Edition, the storage appliance product(s) from iXSystems have always been my go-to choice. For each yearly DIY NAS build, I wander over to the TrueNAS Software Status page and look at the state of the current builds.&lt;/p&gt;
    &lt;p&gt;I√¢m conservative with my personal NAS setup. However, for these blog builds, I typically choose Early Adopter releases. This year that√¢s TrueNAS 25.10.0.1 (aka Goldeye). I enjoy being able to use these DIY NAS builds as a preview to the latest and greatest that TrueNAS has to offer.&lt;/p&gt;
    &lt;p&gt;I repeatedly choose TrueNAS because it√¢s what I√¢ve become accustomed to; it√¢s legitimately an enterprise-grade storage product, which is exactly the quality of solution that I want my data to depend on. At the same time it does not feel like you need a specialized certification and a truckload of enterprise storage experience to meet set up a NAS that exceeds your needs at home.&lt;/p&gt;
    &lt;p&gt;Many times I have been asked, √¢Why not &amp;lt;insert NAS appliance or OS here&amp;gt;?√¢ My answer to that question is, TrueNAS has always done everything that I need it to and they haven√¢t given me any reason to consider anything else. As a result, there√¢s never been a need for me to evaluate something else.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Parts List&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Part Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Qty&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Motherboard&lt;/cell&gt;
        &lt;cell&gt;Topton N22 (w/ N355 CPU) NAS Motherboard&lt;/cell&gt;
        &lt;cell&gt;specs&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;$446.40&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CPU&lt;/cell&gt;
        &lt;cell&gt;Intel Core 3 N355&lt;/cell&gt;
        &lt;cell&gt;specs&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Memory&lt;/cell&gt;
        &lt;cell&gt;Crucial RAM 32GB DDR5 4800MHz SODIMM (CT32G48C40S5)&lt;/cell&gt;
        &lt;cell&gt;specs&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;$172.96&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Case&lt;/cell&gt;
        &lt;cell&gt;JONSBO N4&lt;/cell&gt;
        &lt;cell&gt;specs&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;$121.59&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Case Fan&lt;/cell&gt;
        &lt;cell&gt;Noctua NF-A12x25 PWM chromax.Black.swap&lt;/cell&gt;
        &lt;cell&gt;specs&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;$37.95&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Power Supply&lt;/cell&gt;
        &lt;cell&gt;SilverStone 500W SFX Power Supply SST-SX500-G)&lt;/cell&gt;
        &lt;cell&gt;specs&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;$142.34&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Boot Drive&lt;/cell&gt;
        &lt;cell&gt;Silicon Power 128GB A55 SATA SSD&lt;/cell&gt;
        &lt;cell&gt;specs&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;$21.97&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Apps/VM Drives&lt;/cell&gt;
        &lt;cell&gt;Silicon Power 1TB - NVMe M.2 SSD (SP001TBP34A60M28)&lt;/cell&gt;
        &lt;cell&gt;specs&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;$99.99&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SATA Cables&lt;/cell&gt;
        &lt;cell&gt;OIKWAN SFF-8643 Host to 4 X SATA Breakout Cable&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;$11.99&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Price without Storage:&lt;/cell&gt;
        &lt;cell&gt;$989.36&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Price:&lt;/cell&gt;
        &lt;cell&gt;$1,189.34&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Hardware Assembly, BIOS Configuration, and Burn-In&lt;/head&gt;
    &lt;head rend="h3"&gt;Hardware Assembly&lt;/head&gt;
    &lt;p&gt;I wanted the smallest possible DIY NAS. The JONSBO N4 case initially felt too large since it accommodates Micro ATX motherboards. However, I grew to accept its slightly larger footprint. However, putting the Topton N22 motherboard into the case felt roomy and luxurious. Building the DIY NAS: 2026 Edition compared to prior years√¢ felt a lot like coming home to put on sweatpants and a t-shirt after wearing a suit and tie all day long.&lt;/p&gt;
    &lt;p&gt;I wasn√¢t too fond of the cable-management of the power supply√¢s cables. The layout of the case pretty much makes the front of the power supply inaccessible once it is installed. One consequence of this is that the power cable which powered the SATA backplane initially prevented the 120mm case fan from spinning up. That issue was relatively minor and was resolved with zip ties.&lt;/p&gt;
    &lt;p&gt;Overall, I felt pretty good about the assembly of the DIY NAS: 2026 Edition, but things would take a turn for the worse when I decided to fill all the 3.5-inch drive bays up with some of my decommissioned 8TB HDDs. Now this is probably my fault, I wouldn√¢t be surprised at all that the manual of the JONSBO N4 warned me against this, but putting the drives in last turned out to be a major pain in the neck for each of the four drive bays without a SATA backplane.&lt;/p&gt;
    &lt;p&gt;I had wrongly guessed that you accessed those drives√¢ power and data ports from the front of the case. I worked really hard to route the cables and even managed to install all of the drives before realizing my error and learning my lesson. I√¢m understanding now why the JONSBO N4 is cheaper than all of its siblings. Partly because there√¢s a missing SATA backplane, but also because those other 4 drive bays√¢ layout is frustrating.&lt;/p&gt;
    &lt;p&gt;Don√¢t let my last couple paragraphs sour you on the JONSBO N4, though. I still really like its size, it feels big when you√¢re working in it with a Mini ITX motherboard. If you wind up deciding to use the JONSBO N4, then I suggest that you put those four drives and their cables in first before you do anything else. That would√¢ve made a world of difference for me. Actually looking at the documentation before getting started might have saved me quite a bit of aggravation, too!&lt;/p&gt;
    &lt;p&gt;If I have ruined the JONSBO N4 for you, then check out the JONSBO N3. It√¢s eight 3.5-inch drive bays pair up really nicely with the Topton N22 motherboard. You can see what I thought of the JONSBO N3 by reading the DIY NAS: 2024 Edition blog.&lt;/p&gt;
    &lt;head rend="h3"&gt;BIOS Configuration&lt;/head&gt;
    &lt;p&gt;Generally speaking, I do as little as I possibly can in the BIOS. Normally I strive to only set the time and change the boot order. However, I did a bit more for the DIY NAS: 2026 Edition since I√¢m using the &lt;code&gt;SYS_FAN&lt;/code&gt; header for the fan which is responsible for cooling the hard drives.  Here are the changes that I made in the BIOS:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set the System Date and System Time to Greenwich Mean Time &lt;list rend="ol"&gt;&lt;item&gt;Advanced &lt;list rend="ol"&gt;&lt;item&gt;Hardware Monitor ( Advanced) &lt;list rend="ol"&gt;&lt;item&gt;Set SYS SmartFan Mode to &lt;code&gt;Disabled&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;Set the Manual PWM Setting (for &lt;code&gt;SYS_FAN&lt;/code&gt;) to 180.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Set SYS SmartFan Mode to &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Hardware Monitor ( Advanced) &lt;/item&gt;&lt;item&gt;Set PWRON After Power Loss to &lt;code&gt;Always On&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Boot &lt;list rend="ol"&gt;&lt;item&gt;Set Boot Option #1 to the TrueNAS boot device.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Advanced &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I√¢m not at all interested in venturing into the rabbit√¢s hole of trying to completely minimize how much power the NAS uses. However, I imagine there√¢s some opportunities for power savings lurking in the BIOS. I didn√¢t go looking for them myself, but if you√¢re intrepid enough to do so here√¢s a few suggestions that I have to save some additional power:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable the onboard audio.&lt;/item&gt;
      &lt;item&gt;Disable any network interfaces that you don√¢t wind up using.&lt;/item&gt;
      &lt;item&gt;Tinker with the CPU settings.&lt;/item&gt;
      &lt;item&gt;Got other suggestions? Share them in the comments!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Burn-In&lt;/head&gt;
    &lt;p&gt;Because all of the hardware is brand-new to me brand-new components are not guaranteed to be free of defects, I always do a little bit of burn-in testing to establish some trust in the hardware that I√¢ve picked out for each DIY NAS build. While I think doing some burn-in testing critically important, I also think the value of subsequent burn-in testing drops the more that you do. Don√¢t get too carried away and do your own burn-in testing in moderation!&lt;/p&gt;
    &lt;head rend="h4"&gt;Memtest86+&lt;/head&gt;
    &lt;p&gt;I always use Memtest86+ to burn-in the RAM. I always run at least 3+ passes of Memtest86+. Typically, I run many more passes because I tend to let the system keep running additional passes overnight. Secondarily, running these many passes give the CPU a little bit of work to do and there√¢s enough information displayed by Memtest86+ to give me confidence in the CPU and its settings.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hard Drives&lt;/head&gt;
    &lt;p&gt;The failure rate of hard drives is highest when the drives are new and then again when they√¢re old. Regardless of type of hard drives that I buy or when I buy them, I always do some disk burn in. I tend to run Spearfoot√¢s Disk Burn-in and Testing script on all of my new drives. However executing this script against all of the drives can take quite a long time, even if you use something like &lt;code&gt;tmux&lt;/code&gt; to run the tests in parallel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Initial TrueNAS CE Setup&lt;/head&gt;
    &lt;p&gt;There√¢s always a little bit of setup that I do for a new TrueNAS machine. This isn√¢t intended to be an all inclusive step-by-step guide for all the things you should do with your DIY NAS. Instead, it√¢s more of a list of things I kept track of while I made sure that the DIY NAS: 2026 Edition was functional enough for me to finish writing this blog. That being said, I do think your NAS would be rather functional if you decided to do the same configuration.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Updated the hostname to &lt;code&gt;diynas2026&lt;/code&gt;&lt;list rend="ol"&gt;&lt;item&gt;Note: This is only to avoid issues with another NAS on my network.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Updated the timezone.&lt;/item&gt;
      &lt;item&gt;Enabled the following services and set them to start automatically. &lt;list rend="ol"&gt;&lt;item&gt;SMB&lt;/item&gt;&lt;item&gt;SSH&lt;/item&gt;&lt;item&gt;NFS&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Enabled password login for the &lt;code&gt;truenas_admin&lt;/code&gt;user.&lt;list rend="ul"&gt;&lt;item&gt;Note: If I were planning to use this DIY NAS long-term, I wouldn√¢t have done this. Using SSH keys for authentication is a better idea.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Edited the TrueNAS Dashboard widgets to reflect the 10Gb interface (&lt;code&gt;enp1s0&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Created a pool named &lt;code&gt;flash&lt;/code&gt;which consisted of mirrored vdev using the Teamgroup MP44 1TB NVMe SSDs.&lt;/item&gt;
      &lt;item&gt;Created a pool named &lt;code&gt;rust&lt;/code&gt;which consisted of a single RAID-Z2 vdev using eight hard drives that I had sitting on my shelf after they were decomissioned.&lt;/item&gt;
      &lt;item&gt;Configured the Apps to use the &lt;code&gt;flash&lt;/code&gt;pool for the apps√¢ dataset.&lt;/item&gt;
      &lt;item&gt;Made sure that the System Dataset Pool was set to &lt;code&gt;flash&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Confirmed that there were Scrub Tasks set up for the &lt;code&gt;flash&lt;/code&gt;and&lt;code&gt;rust&lt;/code&gt;pools.&lt;/item&gt;
      &lt;item&gt;Created a dataset on each pool for testing; &lt;code&gt;flash-test&lt;/code&gt;and&lt;code&gt;rust-test&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Installed the Scrutiny app found in the App Catalog.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If I were planning to keep this NAS and use it for my own purposes, I would also:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up a Let√¢s Encrypt certificate.&lt;/item&gt;
      &lt;item&gt;Hook up the NAS to a compatible UPS, enable the UPS service, and configure the UPS service to shut down the NAS before the battery runs out of juice.&lt;/item&gt;
      &lt;item&gt;Set up system email alert service.&lt;/item&gt;
      &lt;item&gt;Create replication tasks to back up critical data to my off-site NAS.&lt;/item&gt;
      &lt;item&gt;Add the new NAS to my Tailscale tailnet using the Tailscale app from the official catalog.&lt;/item&gt;
      &lt;item&gt;As the NAS is seeded with data, create and maintain a suite of snapshot tasks tailored to the importance of the different data being stored on the NAS.&lt;/item&gt;
      &lt;item&gt;Set up S.M.A.R.T. tests for all of the drives: &lt;list rend="ol"&gt;&lt;item&gt;Weekly Short Test&lt;/item&gt;&lt;item&gt;Monthly Long Test&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;Just about every year, I benchmark each DIY NAS build and almost always come to the same conclusion; the NAS will outperform your network at home. Your first bottleneck is almost always going to be the network and the overlwhelming majority of us have gigabit networks at home√¢but that√¢s slowly changing since 2.5Gbps and 10Gbps network hardware has started to get reasonable lately.&lt;/p&gt;
    &lt;p&gt;Even though I always come to the same conclusion, I still like to do the benchmarks for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It helps me build confidence that the DIY NAS: 2026 Edition works well.&lt;/item&gt;
      &lt;item&gt;People tend to enjoy consuming benchmarks and it√¢s fun for me to see the DIY NAS√¢ network card get saturated during the testing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Throughput&lt;/head&gt;
    &lt;p&gt;I like to do three categories of tests to measure the throughput of the NAS:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use iperf3 to benchmark throughput between my NAS and another machine on my network.&lt;/item&gt;
      &lt;item&gt;Benchmark the throughput of the pool(s) locally on the NAS using &lt;code&gt;fio&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Set up SMB shares on each of the pools and then benchmark the throughput when using those shares.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every year I try and mention that Tom Lawrence from Lawrence Systems published a great video about benchmarking storage with FIO and shared the FIO commands from his video in their forums. I use these FIO commands constantly as a reference point for testing ZFS pools√¢ throughput. Importantly I√¢d like to point out that, in that same video, Tom says something very wise:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Pool&lt;/cell&gt;
        &lt;cell role="head"&gt;Test&lt;p&gt;Size&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Random&lt;p&gt;Write&lt;/p&gt;&lt;p&gt;IOPS&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Random&lt;p&gt;Read&lt;/p&gt;&lt;p&gt;IOPS&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Sequential&lt;p&gt;Write&lt;/p&gt;&lt;p&gt;(MB/s)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Sequential&lt;p&gt;Read&lt;/p&gt;&lt;p&gt;(MB/s)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;FIO&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;flash&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;4G&lt;/cell&gt;
        &lt;cell&gt;1906.00&lt;/cell&gt;
        &lt;cell&gt;2200.00&lt;/cell&gt;
        &lt;cell&gt;548.00&lt;/cell&gt;
        &lt;cell&gt;1214.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;FIO&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;flash&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;32G&lt;/cell&gt;
        &lt;cell&gt;2132.00&lt;/cell&gt;
        &lt;cell&gt;3012.00&lt;/cell&gt;
        &lt;cell&gt;544.00&lt;/cell&gt;
        &lt;cell&gt;1211.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;FIO&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;rust&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;4G&lt;/cell&gt;
        &lt;cell&gt;1352.00&lt;/cell&gt;
        &lt;cell&gt;108.00&lt;/cell&gt;
        &lt;cell&gt;367.00&lt;/cell&gt;
        &lt;cell&gt;530.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;FIO&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;rust&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;32G&lt;/cell&gt;
        &lt;cell&gt;1474.00&lt;/cell&gt;
        &lt;cell&gt;326.00&lt;/cell&gt;
        &lt;cell&gt;368.00&lt;/cell&gt;
        &lt;cell&gt;544.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CrystalDiskMark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;flash&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;4GiB&lt;/cell&gt;
        &lt;cell&gt;5858.89&lt;/cell&gt;
        &lt;cell&gt;50409.91&lt;/cell&gt;
        &lt;cell&gt;1104.64&lt;/cell&gt;
        &lt;cell&gt;956.70&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CrystalDiskMark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;flash&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;32GiB&lt;/cell&gt;
        &lt;cell&gt;4193.36&lt;/cell&gt;
        &lt;cell&gt;31047.36&lt;/cell&gt;
        &lt;cell&gt;635.42&lt;/cell&gt;
        &lt;cell&gt;946.20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;CrystalDiskMark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;rust&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;4GiB&lt;/cell&gt;
        &lt;cell&gt;5226.50&lt;/cell&gt;
        &lt;cell&gt;46239.01&lt;/cell&gt;
        &lt;cell&gt;756.23&lt;/cell&gt;
        &lt;cell&gt;655.32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;CrystalDiskMark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;rust&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;32GiB&lt;/cell&gt;
        &lt;cell&gt;3794.43&lt;/cell&gt;
        &lt;cell&gt;12809.33&lt;/cell&gt;
        &lt;cell&gt;759.38&lt;/cell&gt;
        &lt;cell&gt;677.02&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What do I think these benchmarks and my use of the DIY NAS: 2026 Edition tell me? In the grand scheme of things, not a whole lot.&lt;/p&gt;
    &lt;p&gt;However, these benchmarks do back up what I expected, the DIY NAS: 2026 Edition is quite capable and more than ready to meet my storage needs. I especially like that the CrystalDiskMark benchmark of the SMB shares were both faster than a SATA SSD, and the throughput to the share on the &lt;code&gt;flash&lt;/code&gt; pool practically saturated the NAS√¢ 10GbE network connection.&lt;/p&gt;
    &lt;head rend="h4"&gt;FIO Tests&lt;/head&gt;
    &lt;p&gt;Every time I benchmark a NAS, I seem to either be refining what I tried in prior years or completely reinventing the wheel. As a result, I wouldn√¢t recommend comparing these results with results that I shared in prior years√¢ DIY NAS build blogs. I haven√¢t really put a ton of effort into developing a standard suite of benchmarks. Things in my homelab change enough between DIY NAS blogs that trying to create and maintain an environment for a standard suite of benchmarks is beyond what my budget, spare time, and attention span will allow.&lt;/p&gt;
    &lt;p&gt;I√¢m going to paste these &lt;code&gt;fio&lt;/code&gt; commands here in the blog for my own use in future DIY NAS build blogs. If you wind up building something similar, these might be helpful to measure your new NAS√¢ filesystem√¢s performance and compare it to mine!&lt;/p&gt;
    &lt;code&gt;## Random Write IOPS
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=4G --readwrite=randwrite --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=32G --readwrite=randwrite --ramp_time=10

## Random Read IOPS
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=4G --readwrite=randread --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=32G --readwrite=randread --ramp_time=10

## Sequential Write (MB/s)
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=4M --size=4G --readwrite=write --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=4M --size=32G --readwrite=write --ramp_time=10

## Sequential Read (MB/s)
fio --randrepeat=1 --ioengine=libaio --direct=1  --name=test --filename=test --bs=4M --size=4G --readwrite=read --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1  --name=test --filename=test --bs=4M --size=32G --readwrite=read --ramp_time=10
&lt;/code&gt;
    &lt;head rend="h3"&gt;Power Consumption&lt;/head&gt;
    &lt;p&gt;One not-so-obvious cost of running a DIY NAS is how much power it consumes. While I specifically tried to pick items that were efficient in terms of power consumption, it√¢s also important to realize that all the other bells and whistles on the awesome Topton N18 NAS motherboard consume power, too. And that the biggest consumer of power in a NAS is almost always the hard disk drives.&lt;/p&gt;
    &lt;p&gt;Thanks to my tinkering with home automation, I have a plethora of smart outlets which are capable of power monitoring. I used those smart outlets for most of my power monitoring. But I also have a Kill a Watt P400 that I also use for some of the shorter tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Power consumed during a handful of specific tasks: &lt;list rend="ul"&gt;&lt;item&gt;Idle while running TrueNAS&lt;/item&gt;&lt;item&gt;RAM Burn-in (~14 passes of Memtest86+)&lt;/item&gt;&lt;item&gt;An 8-hour throughput benchmark copying randomly-sized files to the NAS using SMB.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Total consumed during the build, burn-in, and use of the DIY NAS: 2026 Edition.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Task&lt;/cell&gt;
        &lt;cell role="head"&gt;Duration&lt;/cell&gt;
        &lt;cell role="head"&gt;Max Wattage&lt;/cell&gt;
        &lt;cell role="head"&gt;Avg. Wattage&lt;/cell&gt;
        &lt;cell role="head"&gt;Total Consumption&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Boot&lt;/cell&gt;
        &lt;cell&gt;10 min.&lt;/cell&gt;
        &lt;cell&gt;200.00 W&lt;/cell&gt;
        &lt;cell&gt;120.00 W&lt;/cell&gt;
        &lt;cell&gt;0.02 kWh&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Idle&lt;/cell&gt;
        &lt;cell&gt;3 hr.&lt;/cell&gt;
        &lt;cell&gt;90.00 W&lt;/cell&gt;
        &lt;cell&gt;66.67 W&lt;/cell&gt;
        &lt;cell&gt;0.20 kWh&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;RAM Burn-in&lt;/cell&gt;
        &lt;cell&gt;18 hr.&lt;/cell&gt;
        &lt;cell&gt;104.00 W&lt;/cell&gt;
        &lt;cell&gt;91.67 W&lt;/cell&gt;
        &lt;cell&gt;1.65 kWh&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SMB Benchmark of HDDs&lt;/cell&gt;
        &lt;cell&gt;8 hr.&lt;/cell&gt;
        &lt;cell&gt;107.00 W&lt;/cell&gt;
        &lt;cell&gt;85.00 W&lt;/cell&gt;
        &lt;cell&gt;0.68 kWh&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total&lt;/cell&gt;
        &lt;cell&gt;108 hr.&lt;/cell&gt;
        &lt;cell&gt;237.80 W&lt;/cell&gt;
        &lt;cell&gt;66.49 W&lt;/cell&gt;
        &lt;cell&gt;7.17 kWh&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;What about an EconoNAS?&lt;/head&gt;
    &lt;p&gt;Shortly before prices skyrocketed, I decided I wasn√¢t very interested in doing a separate EconoNAS builds. Several months ago, I realized that there were several off-the-shelf NAS machines that were more-than-capable of running TrueNAS and they were selling at economical prices that couldn√¢t be topped by a DIY approach. I will dive deeper into this in a future blog, eventually √¢¬¶ maybe?&lt;/p&gt;
    &lt;p&gt;All that being said√¢it√¢d be incredibly easy to make some compromises which result in the DIY NAS: 2026 Edition becoming quite a bit more economical. Here√¢s a list of changes that I would consider to be more budget-friendly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Different motherboard/CPU combo: N18 w/ N100 CPU (-$224), N18 w/ N150 CPU (-$214), or N22 w/ N150 CPU (-$180)&lt;/item&gt;
      &lt;item&gt;16GB of DDR5 RAM (-$39) instead of 32GB.&lt;/item&gt;
      &lt;item&gt;Thermal Right TL-C12015 Slim Fan instead of the Noctua NF-A12x25 (-$26)&lt;/item&gt;
      &lt;item&gt;Apevia SFX-AP500W Power Supply (-$104)&lt;/item&gt;
      &lt;item&gt;Skip the redundancy for the boot pool (-$22)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Altogether, these savings could add up to more than $400, which is pretty considerable! If you made all of these changes, you√¢d have something that√¢s going to be nearly equivalent to the DIY NAS: 2026 Edition but at a fraction of the price.&lt;/p&gt;
    &lt;head rend="h2"&gt;What am I going to do with the DIY NAS: 2026 Edition?!&lt;/head&gt;
    &lt;p&gt;My DIY NAS is aging quite gracefully, but I√¢ve recently been wondering about replacing it. Shortly before ordering all the parts for the DIY NAS: 2026 Edition, I briefly considered using this year√¢s DIY NAS build to replace my personal NAS. However, I decided not to do that. Then prices skyrocketed and I shelved the idea of building a replacement for my own NAS and I nearly shelved the idea of a DIY NAS in 2026!&lt;/p&gt;
    &lt;p&gt;So that begs the question, √¢What is Brian going to do with the DIY NAS: 2026 Edition?√¢&lt;/p&gt;
    &lt;p&gt;I√¢m going to auction it off on the briancmosesdotcom store on eBay! Shortly after publishing this blog, I√¢ll list it on eBay. In response to skyrocketing prices for PC components, I√¢m going to do a no-reserve auction. At the end of the auction, the highest bidder wins and hopefully they√¢ll get a pretty good deal!&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;Overall, I√¢m pleased with the DIY NAS: 2026 Edition. The Topton N22 motherboard is a significant improvement over last year√¢s Topton N18 motherboard, primarily due to its extra two SATA ports. This provides 33.3% more gross storage capacity.&lt;/p&gt;
    &lt;p&gt;While testing, I found the Intel Core 3 N355 CPU somewhat excessive for basic NAS functions. However, the substantial untapped CPU horsepower offers luxurious performance potential. This makes the build compelling for anyone planning extensive self-hosting projects.&lt;/p&gt;
    &lt;p&gt;I have mixed feelings about the JONSBO N4 case. The four right-side drive bays lack SATA backplane connectivity. Without creative cabling solutions, individual drive replacement becomes challenging. However, the case√¢s ~$125 price point compensates for this inconvenience. I anticipate that those the cost savings will justify the compromise for most builders. If I were to build the DIY NAS: 2026 Edition all over again, I√¢d be tempted to use the JONSBO N3 case or even the JONSBO N6 which isn√¢t quite obtainable, yet.&lt;/p&gt;
    &lt;p&gt;The DIY NAS: 2026 Edition delivers excellent performance and superior specifications. In my opinion, it represents better value than off-the-shelf alternatives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;QNAP TS-832PX-4G ($880)&lt;/item&gt;
      &lt;item&gt;Asustor Lockerstor 8 AS6508T ($960)&lt;/item&gt;
      &lt;item&gt;UGREEN NASync DXP8800 ($1200)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Building your own NAS provides significant advantages. Years later, you can upgrade RAM, motherboard, case, or add PCI-e (x1) expansion cards. These off-the-shelf alternatives offer severely limited upgrade paths.&lt;/p&gt;
    &lt;p&gt;Is 2026 finally the year that you decide to build your DIY NAS? I hope that it is! Share your experience building your NAS in the comments below or come tell us about it in the #diynas-and-homelab channel on the Butter, What?! Discord server!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.briancmoses.com/2025/11/diy-nas-2026-edition.html"/><published>2025-11-27T02:54:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065698</id><title>Coq: The World's Best Macro Assembler? [pdf] [2013]</title><updated>2025-11-27T07:12:41.715339+00:00</updated><content/><link href="https://nickbenton.name/coqasm.pdf"/><published>2025-11-27T04:34:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065817</id><title>Music eases surgery and speeds recovery, study finds</title><updated>2025-11-27T07:12:41.487348+00:00</updated><content>&lt;doc fingerprint="adb964f948bf9fcf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Music eases surgery and speeds recovery, Indian study finds&lt;/head&gt;
    &lt;p&gt;Under the harsh lights of an operating theatre in the Indian capital, Delhi, a woman lies motionless as surgeons prepare to remove her gallbladder.&lt;/p&gt;
    &lt;p&gt;She is under general anaesthesia: unconscious, insensate and rendered completely still by a blend of drugs that induce deep sleep, block memory, blunt pain and temporarily paralyse her muscles.&lt;/p&gt;
    &lt;p&gt;Yet, amid the hum of monitors and the steady rhythm of the surgical team, a gentle stream of flute music plays through the headphones placed over her ears.&lt;/p&gt;
    &lt;p&gt;Even as the drugs silence much of her brain, its auditory pathway remains partly active. When she wakes up, she will regain consciousness more quickly and clearly because she required lower doses of anaesthetic drugs such as propofol and opioid painkillers than patients who heard no music.&lt;/p&gt;
    &lt;p&gt;That, at least, is what a new peer-reviewed study from Delhi's Maulana Azad Medical College and Lok Nayak Hospital suggests. The research, published in the journal Music and Medicine, offers some of the strongest evidence yet that music played during general anaesthesia can modestly but meaningfully reduce drug requirements and improve recovery.&lt;/p&gt;
    &lt;p&gt;The study focuses on patients undergoing laparoscopic cholecystectomy, the standard keyhole operation to remove the gallbladder. The procedure is short - usually under an hour - and demands a particularly swift, "clear-headed" recovery.&lt;/p&gt;
    &lt;p&gt;To understand why the researchers turned to music, it helps to decode the modern practice of anaesthesia.&lt;/p&gt;
    &lt;p&gt;"Our aim is early discharge after surgery," says Dr Farah Husain, senior specialist in anaesthesia and certified music therapist for the study. "Patients need to wake up clear-headed, alert and oriented, and ideally pain-free. With better pain management, the stress response is curtailed."&lt;/p&gt;
    &lt;p&gt;Achieving that requires a carefully balanced mix of five or six drugs that together keep the patient asleep, block pain, prevent memory of the surgery and relax the muscles.&lt;/p&gt;
    &lt;p&gt;In procedures like laparoscopic gallbladder removal, anaesthesiologists now often supplement this drug regimen with regional "blocks" - ultrasound-guided injections that numb nerves in the abdominal wall.&lt;/p&gt;
    &lt;p&gt;"General anaesthesia plus blocks is the norm," says Dr Tanvi Goel, primary investigator and a former senior resident of Maulana Azad Medical College. "We've been doing this for decades."&lt;/p&gt;
    &lt;p&gt;But the body does not take to surgery easily. Even under anaesthesia, it reacts: heart rate rises, hormones surge, blood pressure spikes. Reducing and managing this cascade is one of the central goals of modern surgical care. Dr Husain explains that the stress response can slow recovery and worsen inflammation, highlighting why careful management is so important.&lt;/p&gt;
    &lt;p&gt;The stress starts even before the first cut, with intubation - the insertion of a breathing tube into the windpipe.&lt;/p&gt;
    &lt;p&gt;To do this, the anaesthesiologist uses a laryngoscope to lift the tongue and soft tissues at the base of the throat, obtain a clear view of the vocal cords, and guide the tube into the trachea. It's a routine step in general anaesthesia that keeps the airway open and allows precise control of the patient's breathing while they are unconscious.&lt;/p&gt;
    &lt;p&gt;"The laryngoscopy and intubation are considered the most stressful response during general anaesthesia," says Dr Sonia Wadhawan, director-professor of anaesthesia and intensive care at Maulana Azad Medical College and supervisor of the study.&lt;/p&gt;
    &lt;p&gt;"Although the patient is unconscious and will remember nothing, their body still reacts to the stress with changes in heart rate, blood pressure, and stress hormones."&lt;/p&gt;
    &lt;p&gt;To be sure, the drugs have evolved. The old ether masks have vanished. In their place are intravenous agents - most notably propofol, the hypnotic made infamous by Michael Jackson's death but prized in operating theatres for its rapid onset and clean recovery. "Propofol acts within about 12 seconds," notes Dr Goel. "We prefer it for short surgeries like laparoscopic cholecystectomy because it avoids the 'hangover' caused by inhalational gases."&lt;/p&gt;
    &lt;p&gt;The team of researchers wanted to know whether music could reduce how much propofol and fentanyl (an opioid painkiller) patients required. Less drugs means faster awakening, steadier vital signs and reduced side effects.&lt;/p&gt;
    &lt;p&gt;So they designed a study. A pilot involving eight patients led to a full 11-month trial of 56 adults, aged roughly 20 to 45, randomly assigned to two groups. All received the same five-drug regimen: a drug that prevents nausea and vomiting, a sedative, fentanyl, propofol and a muscle relaxant. Both groups wore noise-cancelling headphones - but only one heard music.&lt;/p&gt;
    &lt;p&gt;"We asked patients to select from two calming instrumental pieces - soft flute or piano," says Dr Husain. "The unconscious mind still has areas that remain active. Even if the music isn't explicitly recalled, implicit awareness can lead to beneficial effects."&lt;/p&gt;
    &lt;p&gt;The results were striking.&lt;/p&gt;
    &lt;p&gt;Patients exposed to music required lower doses of propofol and fentanyl. They experienced smoother recoveries, lower cortisol or stress-hormone levels and a much better control of blood pressure during the surgery. "Since the ability to hear remains intact under anaesthesia," the researchers write, "music can still shape the brain's internal state."&lt;/p&gt;
    &lt;p&gt;Clearly, music seemed to quieten the internal storm. "The auditory pathway remains active even when you're unconscious," says Dr Wadhawan. "You may not remember the music, but the brain registers it."&lt;/p&gt;
    &lt;p&gt;The idea that the mind behind the anaesthetic veil is not entirely silent has long intrigued scientists. Rare cases of "intraoperative awareness" show patients recalling fragments of operating-room conversation.&lt;/p&gt;
    &lt;p&gt;If the brain is capable of picking up and remembering stressful experiences during surgery - even when a patient is unconscious - then it might also be able to register positive or comforting experiences, like music, even without conscious memory.&lt;/p&gt;
    &lt;p&gt;"We're only beginning to explore how the unconscious mind responds to non-pharmacological interventions like music," says Dr Husain. "It's a way of humanising the operating room."&lt;/p&gt;
    &lt;p&gt;Music therapy is not new to medicine; it has long been used in psychiatry, stroke rehabilitation and palliative care. But its entry into the intensely technical, machine-governed world of anaesthesia marks a quiet shift.&lt;/p&gt;
    &lt;p&gt;If such a simple intervention can reduce drug use and speed recovery - even modestly - it could reshape how hospitals think about surgical wellbeing.&lt;/p&gt;
    &lt;p&gt;As the research team prepares its next study exploring music-aided sedation, building on earlier findings, one truth is already humming through the data: even when the body is still and the mind asleep, it appears a few gentle notes can help the healing begin.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c231dv9zpz3o"/><published>2025-11-27T04:55:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065959</id><title>Evaluating Uniform Memory Access Mode on AMD's Turin</title><updated>2025-11-27T07:12:41.370903+00:00</updated><content>&lt;doc fingerprint="23bad9ba781048cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Evaluating Uniform Memory Access Mode on AMD's Turin ft. Verda (formerly DataCrunch.io)&lt;/head&gt;
    &lt;head rend="h3"&gt;How does uniform memory access play out as interconnects get increasingly non-uniform?&lt;/head&gt;
    &lt;p&gt;NUMA, or Non-Uniform Memory Access, lets hardware expose affinity between cores and memory controllers to software. NUMA nodes traditionally aligned with socket boundaries, but modern server chips can subdivide a socket into multiple NUMA nodes. It‚Äôs a reflection of how non-uniform interconnects get as core and memory controller counts keep going up. AMD designates their NUMA modes with the NPS (Nodes Per Socket) prefix.&lt;/p&gt;
    &lt;p&gt;NPS0 is a special NUMA mode that goes in the other direction. Rather than subdivide the system, NPS0 exposes a dual socket system as a single monolithic entity. It evenly distributes memory accesses across all memory controller channels, providing uniform memory access like in a desktop system. NPS0 and similar modes exist because optimizing for NUMA can be complicated and time intensive. Programmers have to specify a NUMA node for each memory allocation, and take are to minimize cross-node memory accesses. Each NUMA node only represents a fraction of system resources, so code pinned to a NUMA node will be constrained by that node‚Äôs CPU core count, memory bandwidth, and memory capacity. Effort spent getting an application to scale across NUMA nodes might be effort not spent on a software project‚Äôs other goals.&lt;/p&gt;
    &lt;head rend="h1"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;A massive thank you goes to Verda (formerly DataCrunch) for proving an instance with 2 AMD EPYC 9575Fs and 8 Nvidia B200 GPUs. Verda gave us about 3 weeks with the instance to do with as we wished. While this article looks at the AMD EPYC 9575Fs, there will be upcoming coverage of the B200s found in the VM.&lt;/p&gt;
    &lt;p&gt;This system appears to be running in NPS0 mode, giving an opportunity to see how a modern server acts with 24 memory controllers providing uniform memory access.&lt;/p&gt;
    &lt;p&gt;A simple latency test immediately shows the cost of providing uniform memory access. DRAM latency rises to over 220 ns, giving a nearly 90 ns penalty over the EPYC 9355P running in NPS1 mode. It‚Äôs a high penalty compared to using the equivalent of NPS0 on older systems. For example, a dual socket Broadwell system has 75.8 ns of DRAM latency when each socket is treated as a NUMA node, and 104.6 ns with uniform memory access[1].&lt;/p&gt;
    &lt;p&gt;NPS0 mode does have a bandwidth advantage from bringing twice as many memory controllers into play. But the extra bandwidth doesn‚Äôt translate to a latency advantage until bandwidth demands reach nearly 400 GB/s. The EPYC 9355P seems to suffer when a latency test thread is mixed with bandwidth heavy ones. A bandwidth test thread with just linear read patterns can achieve 479 GB/s in NPS1 mode. However, my bandwidth test produces low values on the EPYC 9575F because not all test threads finish at the same time. I avoid this problem in the loaded memory latency test, because I have bandwidth load threads check a flag. That lets me stop all threads at approximately the same time.&lt;/p&gt;
    &lt;p&gt;Per-CCD bandwidth is barely affected by the different NPS modes. Both the EPYC 9355P and 9575F use ‚ÄúGMI-Wide‚Äù links for their Core Complex Dies, or CCDs. GMI-Wide provides 64B/cycle of read and write bandwidth at the Infinity Fabric clock. On both chips, each CCD enjoys more bandwidth to the system compared to standard ‚ÄúGMI-Narrow‚Äù configurations. For reference, a GMI-Narrow setup running at a typical desktop 2 GHz FCLK would be limited to 64 GB/s of read and 32 GB/s of write bandwidth.&lt;/p&gt;
    &lt;head rend="h1"&gt;Performance: SPEC CPU2017&lt;/head&gt;
    &lt;p&gt;Higher memory latency could lead to lower performance, especially in single threaded workloads. But the EPYC 9575F does surprisingly well in SPEC CPU2017. The EPYC 9575F runs at a higher 5 GHz clock speed, and DRAM latency is only one of many factors that affect CPU performance.&lt;/p&gt;
    &lt;p&gt;Individual workloads show a more complex picture. The EPYC 9575F does best when workloads don‚Äôt miss cache. Then, its high 5 GHz clock speed can shine. 548.exchange2 is an example. On the other hand, workloads that hit DRAM a lot suffer in NPS0 mode. 502.gcc, 505.mcf, and 520.omnetpp see the EPYC 9575F‚Äôs higher clock speed count for nothing, and the higher clocked chip underperforms compared to 4.4 GHz setups with lower DRAM latency.&lt;/p&gt;
    &lt;p&gt;SPEC CPU2017‚Äôs floating point suite also shows diverse behavior. 549.fotonik3d and 554.roms suffer in NPS0 mode as the EPYC 9575F struggles to keep itself fed. 538.imagick plays nicely to the EPYC 9575F‚Äôs advantages. In that test, high cache hitrates let the 9575F‚Äôs higher core throughput shine through.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;NPS0 mode performs surprisingly well in a single threaded SPEC CPU2017 run. Some sub-tests suffer from higher memory latency, but enough other tests benefit from the higher 5 GHz clock speed to make up the difference. It‚Äôs a lesson about the importance of clock speeds and good caching in a modern server CPU. Those two factors go together, because faster cores only provide a performance advantage if the memory subsystem can feed them. The EPYC 9575F‚Äôs good overall performance despite having over 220 ns of memory latency shows how good its caching setup is.&lt;/p&gt;
    &lt;p&gt;As for running in NPS0 mode, I don‚Äôt think it‚Äôs worthwhile in a modern system. The latency penalty is very high, and bandwidth gains are minor for NUMA-unaware code. I expect those latency penalties to get worse as server core and memory controller counts continue to increase. For workloads that need to scale across socket boundaries, optimizing for NUMA looks to be an unfortunate necessity.&lt;/p&gt;
    &lt;p&gt;Again, a massive thank you goes to Verda (formerly DataCrunch) without which this article, and the upcoming B200 article, would not be possible!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/evaluating-uniform-memory-access"/><published>2025-11-27T05:22:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065997</id><title>Show HN: Era ‚Äì Open-source local sandbox for AI agents</title><updated>2025-11-27T07:12:40.878611+00:00</updated><content>&lt;doc fingerprint="60d6b8bca9f1b8e0"&gt;
  &lt;main&gt;
    &lt;p&gt;Run untrusted or AI-generated code locally inside microVMs that behave like containers for great devX, 200ms launch time, and better security.&lt;/p&gt;
    &lt;p&gt;There's a fully managed cloud layer, globally deployed Worker/API, jump to cloudflare/README.md.&lt;/p&gt;
    &lt;code&gt;# 1. install the tap
brew tap binsquare/era-agent-cli

# 2. install era agent
brew install binsquare/era-agent-cli/era-agent

# 3. install dependencies
brew install krunvm buildah

# 4. verify the CLI is on PATH
agent vm exec --help

# 4. follow platform-specific setup (see below)&lt;/code&gt;
    &lt;code&gt;# 1. install dependencies
brew install krunvm buildah  # on macos

# 2. clone the repository
git clone https://github.com/binsquare/era
cd era-agent

# 3. build the agent
make

# 4. follow platform-specific setup (see below)&lt;/code&gt;
    &lt;code&gt;brew tap binsquare/era-agent-cli
brew install era-agent-cli
brew install krunvm buildah&lt;/code&gt;
    &lt;p&gt;Run the post-install helper to prepare the case-sensitive volume/state dir on macOS:&lt;/p&gt;
    &lt;code&gt;$(brew --prefix era-agent)/libexec/setup/setup.sh&lt;/code&gt;
    &lt;p&gt;if you installed era agent via homebrew, use the setup script from the installed location:&lt;/p&gt;
    &lt;code&gt;# for macos users with homebrew installation
$(brew --prefix era-agent)/libexec/setup/setup.sh

# or run the setup script directly after installation
$(brew --prefix)/bin/era-agent-setup  # if setup script is linked separately&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Run&lt;/p&gt;&lt;code&gt;scripts/macos/setup.sh&lt;/code&gt;to bootstrap dependencies, validate (or create) a case-sensitive volume, and prepare an agent state directory (the script may prompt for your password to run&lt;code&gt;diskutil&lt;/code&gt;). The script will also detect your Homebrew installation and recommend the correct value for the&lt;code&gt;DYLD_LIBRARY_PATH&lt;/code&gt;environment variable, which may be required for&lt;code&gt;krunvm&lt;/code&gt;to find its dynamic libraries.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you prefer to create the dedicated volume manually, open a separate terminal and run (with&lt;/p&gt;&lt;code&gt;sudo&lt;/code&gt;as required):&lt;code&gt;diskutil apfs addVolume disk3 "Case-sensitive APFS" krunvm&lt;/code&gt;&lt;p&gt;(replace&lt;/p&gt;&lt;code&gt;disk3&lt;/code&gt;with the identifier reported by&lt;code&gt;diskutil list&lt;/code&gt;). The operation is non-destructive, does not require&lt;code&gt;sudo&lt;/code&gt;, and shares space with the source container volume.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;When prompted by the setup script, accept the default mount point (&lt;/p&gt;&lt;code&gt;/Volumes/krunvm&lt;/code&gt;) or provide your own. Afterwards, export the environment variables printed by the script (at minimum&lt;code&gt;AGENT_STATE_DIR&lt;/code&gt;,&lt;code&gt;KRUNVM_DATA_DIR&lt;/code&gt;, and&lt;code&gt;CONTAINERS_STORAGE_CONF&lt;/code&gt;) before invoking&lt;code&gt;agent&lt;/code&gt;or running&lt;code&gt;krunvm&lt;/code&gt;/&lt;code&gt;buildah&lt;/code&gt;directly. The helper now prepares a matching container-storage configuration under the case-sensitive volume so the CLI can run without extra manual steps.&lt;list rend="ul"&gt;&lt;item&gt;The script also writes &lt;code&gt;policy.json&lt;/code&gt;/&lt;code&gt;registries.conf&lt;/code&gt;under the same directory so Buildah doesn't look for root-owned files in&lt;code&gt;/etc/containers&lt;/code&gt;. Export the variables it prints (&lt;code&gt;CONTAINERS_POLICY&lt;/code&gt;,&lt;code&gt;CONTAINERS_REGISTRIES_CONF&lt;/code&gt;) if you invoke Buildah manually.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The script also writes &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install &lt;code&gt;krunvm&lt;/code&gt;and&lt;code&gt;buildah&lt;/code&gt;using your package manager (the specific installation method may vary)&lt;/item&gt;
      &lt;item&gt;Ensure the system is properly configured to run microVMs (may require kernel modules or specific privileges)&lt;/item&gt;
      &lt;item&gt;Consider setting &lt;code&gt;AGENT_STATE_DIR&lt;/code&gt;to a writable location if running as non-root&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;krunvm&lt;/code&gt;must be installed and available on&lt;code&gt;$PATH&lt;/code&gt;(Homebrew:&lt;code&gt;brew install krunvm&lt;/code&gt;; see upstream docs for other platforms).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;buildah&lt;/code&gt;must also be present because&lt;code&gt;krunvm&lt;/code&gt;shells out to it for OCI image handling.&lt;/item&gt;
      &lt;item&gt;On macOS, &lt;code&gt;krunvm&lt;/code&gt;requires a case-sensitive APFS volume; see the macOS setup notes above.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;make          # builds the agent CLI
make clean    # removes build artifacts (Go cache)
&lt;/code&gt;
    &lt;p&gt;Full platform-specific steps (macOS volume setup, Linux env vars, troubleshooting) live in era-agent/README.md.&lt;/p&gt;
    &lt;p&gt;A demo video showing how to install and use the CLI tool is available in the era-agent directory. This video covers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Installing dependencies and compiling the CLI tool&lt;/item&gt;
      &lt;item&gt;Creating and accessing local VMs&lt;/item&gt;
      &lt;item&gt;Running code and agents through commands or scripts&lt;/item&gt;
      &lt;item&gt;Uploading and downloading files to/from a VM&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# create a long-running VM
agent vm create --language python --cpu 1 --mem 256 --network allow_all

# run something inside it
agent vm exec --vm &amp;lt;id&amp;gt; --cmd "python -c 'print(\"hi\")'"

# ephemeral one-off execution
agent vm temp --language javascript --cmd "node -e 'console.log(42)'"

# inspect / cleanup
agent vm list
agent vm stop --all
agent vm clean --all&lt;/code&gt;
    &lt;p&gt;Supported &lt;code&gt;--language&lt;/code&gt; values: &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;javascript&lt;/code&gt;/&lt;code&gt;node&lt;/code&gt;/&lt;code&gt;typescript&lt;/code&gt;, &lt;code&gt;go&lt;/code&gt;, &lt;code&gt;ruby&lt;/code&gt;. Override the base image with &lt;code&gt;--image&lt;/code&gt; if you need a custom runtime.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;AGENT_STATE_DIR&lt;/code&gt;: writable directory for VM metadata, krunvm state, and Buildah storage. The macOS setup script prints the correct exports.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AGENT_LOG_LEVEL&lt;/code&gt;(&lt;code&gt;debug|info|warn|error&lt;/code&gt;) and&lt;code&gt;AGENT_LOG_FILE&lt;/code&gt;: control logging.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AGENT_ENABLE_GUEST_VOLUMES=1&lt;/code&gt;: re-enable&lt;code&gt;/in&lt;/code&gt;,&lt;code&gt;/out&lt;/code&gt;,&lt;code&gt;/persist&lt;/code&gt;mounts for advanced workflows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See era-agent/README.md for every tunable.&lt;/p&gt;
    &lt;code&gt;cd era-agent
make agent
./agent vm temp --language python --cmd "python -c 'print(\"Smoke test\")'"&lt;/code&gt;
    &lt;p&gt;Integration helpers and sample recipes live under &lt;code&gt;examples/&lt;/code&gt;, &lt;code&gt;recipes/&lt;/code&gt;, and &lt;code&gt;docs/&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To deploy ERA as a Cloudflare Worker with Durable Object-backed sessions and HTTP APIs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow cloudflare/README.md for setup, local Wrangler dev, and deployment.&lt;/item&gt;
      &lt;item&gt;The Worker reuses the same Go agent primitives but adds session orchestration, package caching, and REST endpoints.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;era-agent/README.md ‚Äì detailed CLI usage, setup scripts, troubleshooting.&lt;/item&gt;
      &lt;item&gt;cloudflare/README.md ‚Äì Worker/API deployment guide.&lt;/item&gt;
      &lt;item&gt;docs/ ‚Äì HTTP quickstart, storage notes, MCP adapters.&lt;/item&gt;
      &lt;item&gt;recipes/README.md ‚Äì ready-to-run workflows.&lt;/item&gt;
      &lt;item&gt;examples/README.md ‚Äì language samples.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Apache 2.0&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/BinSquare/ERA"/><published>2025-11-27T05:28:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46066126</id><title>Principles of Vasocomputation</title><updated>2025-11-27T07:12:37.190719+00:00</updated><content>&lt;doc fingerprint="be4b450aa1a383ba"&gt;
  &lt;main&gt;
    &lt;p&gt;A unification of Buddhist phenomenology, active inference, and physical reflexes; a practical theory of suffering, tension, and liberation; the core mechanism for medium-term memory and Bayesian updating; a clinically useful dimension of variation and dysfunction; a description of sensory type safety; a celebration of biological life.&lt;/p&gt;
    &lt;p&gt;Michael Edward Johnson, Symmetry Institute, July 12, 2023.&lt;/p&gt;
    &lt;p&gt;I. What is tanha?&lt;/p&gt;
    &lt;p&gt;By default, the brain tries to grasp and hold onto pleasant sensations and push away unpleasant ones. The Buddha called these ‚Äòmicro-motions‚Äô of greed and aversion ta·πáhƒÅ, and the Buddhist consensus seems to be that it accounts for an amazingly large proportion (~90%) of suffering. Romeo Stevens suggests translating the original Pali term as ‚Äúfused to,‚Äù ‚Äúgrasping,‚Äù or ‚Äúclenching,‚Äù and that the mind is trying to make sensations feel stable, satisfactory, and controllable. Nick Cammarata suggests ‚Äúfast grabby thing‚Äù that happens within ~100ms after a sensation enters awareness; Daniel Ingram suggests this ‚Äògrab‚Äô can occur as quickly as 25-50ms (personal discussion). Uchiyama Roshi describes tanha in terms of its cure, ‚Äúopening the hand of thought‚Äù; Shinzen Young suggests ‚Äúfixation‚Äù; other common translations of tanha are ‚Äúdesire,‚Äù ‚Äúthirst,‚Äù ‚Äúcraving.‚Äù The vipassana doctrine is that tanha is something the mind instinctively does, and that meditation helps you see this process as it happens, which allows you to stop doing it. Shinzen estimates that his conscious experience is literally 10x better due to having a satisfying meditation practice.&lt;/p&gt;
    &lt;p&gt;Tanha is not yet a topic of study in affective neuroscience but I suggest it should be. Neuroscience is generally gated by soluble important mysteries: complex dynamics often arise from complex mechanisms, and complex mechanisms are difficult to untangle. The treasures in neuroscience happen when we find exceptions to this rule: complex dynamics that arise from elegantly simple core mechanisms. When we find one it generally leads to breakthroughs in both theory and intervention. Does ‚Äútanha‚Äù arise from a simple or complex mechanism? I believe Buddhist phenomenology is very careful about what it calls dependent origination ‚Äî and this makes items that Buddhist scholarship considers to be ‚Äòbasic building-blocks of phenomenology‚Äô particularly likely to have a simple, elegant implementations in the brain ‚Äî and thus are exceptional mysteries to focus scientific attention on.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt think tanha has 1000 contributing factors; I think it has one crisp, isolatable factor. And I think if we find this factor, it could herald a reorganization of systems neuroscience similar in magnitude to the past shifts of cybernetics, predictive coding, and active inference.&lt;/p&gt;
    &lt;p&gt;Core resources:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Anuruddha, ƒÄ. (n.d.). A Comprehensive Manual of Abhidhamma.&lt;/item&gt;
      &lt;item&gt;Stevens, R. (2020). (mis)Translating the Buddha. Neurotic Gradient Descent.&lt;/item&gt;
      &lt;item&gt;Cammarata, N. (2021-2023). [Collected Twitter threads on tanha].&lt;/item&gt;
      &lt;item&gt;Markwell, A. (n.d.). Dhamma resources.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;II. Tanha as unskillful active inference (TUAI)&lt;/p&gt;
    &lt;p&gt;The first clue is what tanha is trying to do for us. I‚Äôll claim today that tanha is a side-effect of a normal, effective strategy our brains use extensively, active inference. Active inference suggests we impel ourselves to action by first creating some predicted sensation (‚ÄúI have a sweet taste in my mouth‚Äù or ‚ÄúI am not standing near that dangerous-looking man‚Äù) and then holding it until we act in the world to make this prediction become true (at which point we can release the tension). Active inference argues we store our to-do list as predictions, which are equivalent to untrue sensory observations that we act to make true.&lt;/p&gt;
    &lt;p&gt;Formally, the ‚Äútanha as unskillful active inference‚Äù (TUAI) hypothesis is that this process commonly goes awry (i.e. is applied unskillfully) in three ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First, the rate of generating normative predictions can outpace our ability to make them true and overloads a very finite system. Basically we try to control too much, and stress builds up.&lt;/item&gt;
      &lt;item&gt;Second, we generate normative predictions in domains that we cannot possibly control; predicting a taste of cake will linger in our mouth forever, predicting that we did not drop our glass of water on the floor. That good sensations will last forever and the bad did not happen. (This is essentially a ‚Äúpredictive processing‚Äù reframe of the story Romeo Stevens has told on his blog, Twitter, and in person.)[1]&lt;/item&gt;
      &lt;item&gt;Third, there may be a context desynchronization between the system that represents the world model, and the system that maintains predictions-as-operators on this world model. When desynchronization happens and the basis of the world model shifts in relation to the basis of the predictions, predictions become nonspecific or nonsensical noise and stress.&lt;/item&gt;
      &lt;item&gt;We may also include a catch-all fourth category for when the prediction machinery becomes altered outside of any semantic context, for example metabolic insufficiency leading to impaired operation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Core resources:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Safron, A. (2020). An Integrated World Modeling Theory (IWMT) of Consciousness: Combining Integrated Information and Global Neuronal Workspace Theories With the Free Energy Principle and Active Inference Framework; Toward Solving the Hard Problem and Characterizing Agentic Causation. Frontiers in Artificial Intelligence, 3. https://doi.org/10.3389/frai.2020.00030&lt;/item&gt;
      &lt;item&gt;Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G. (2017). Active inference: A Process Theory. Neural Computation, 29(1), 1-49.&lt;/item&gt;
      &lt;item&gt;Sapolsky, R.M. (2004). Why Zebras Don‚Äôt Get Ulcers: The Acclaimed Guide to Stress, Stress-Related Diseases, and Coping. Holt Paperbacks. [Note: link is to a video summary.]&lt;/item&gt;
      &lt;item&gt;Pyszczynski, T., Greenberg, J., Solomon, S. (2015). Thirty Years of Terror Management Theory. Advances in Experimental Social Psychology, 52, 1-70.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;III. Evaluating tanha requires a world model and cost function&lt;/p&gt;
    &lt;p&gt;There are many theories about the basic unit of organization of the brain; brain regions, functional circuits, specific network topologies, etc. Adam Safron describes the nervous system‚Äôs basic building block as Self-Organized Harmonic Modes (SOHMs); I like this because the math of harmonic modes allows a lot of interesting computation to arise ‚Äòfor free.‚Äô Safron suggests these modes function as autoencoders, which I believe are functionally identical to symmetry detectors. It‚Äôs increasingly looking like SOHMs are organized around physical brain resonances at least as much as connectivity, which been a surprising result.&lt;/p&gt;
    &lt;p&gt;At high frequencies these SOHMs will act as feature detectors, at lower frequencies we might think of them as wind chimes: by the presence and absence of particular SOHMs and their interactions we obtain a subconscious feeling about what kind of environment we‚Äôre in and where its rewards and dangers are. We can expect SOHMs will be arranged in a way that optimizes differentiability of possible/likely world states, minimizes crosstalk, and in aggregate constitutes a world model, or in the Neural Annealing/REBUS/ALBUS framework, a belief landscape.&lt;/p&gt;
    &lt;p&gt;To be in tanha-free ‚Äúopen awareness‚Äù without greed, aversion, or expectation is to feel the undoctored hum of your SOHMs. However, we doctor our SOHMs *all the time* ‚Äî when a nice sensation enters our awareness, we reflexively try to ‚Äògrab‚Äô it and stabilize the resonance; when something unpleasant comes in, we try to push away and deaden the resonance. Likewise society puts expectations on us to ‚Äúact normal‚Äù and ‚Äúbe useful‚Äù; we may consider all such SOHM adjustments/predictions as drawing from the same finite resource pool. ‚ÄúActive SOHM management‚Äù is effortful (and unpleasant) in rough proportion to how many SOHMs need to be actively managed and how long they need to be managed.&lt;/p&gt;
    &lt;p&gt;But how can the brain manage SOHMs? And if the Buddhists are right and this creates suffering, why does the brain even try?&lt;/p&gt;
    &lt;p&gt;Core resources:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Safron, A. (2020). An Integrated World Modeling Theory (IWMT) of Consciousness: Combining Integrated Information and Global Neuronal Workspace Theories With the Free Energy Principle and Active Inference Framework; Toward Solving the Hard Problem and Characterizing Agentic Causation. Frontiers in Artificial Intelligence, 3. https://doi.org/10.3389/frai.2020.00030&lt;/item&gt;
      &lt;item&gt;Safron, A. (2020). On the varieties of conscious experiences: Altered beliefs under psychedelics (ALBUS). PsyArxiv. Retrieved July 7, 2023, from the PsyArxiv website.&lt;/item&gt;
      &lt;item&gt;Safron, A. (2021). The radically embodied conscious cybernetic bayesian brain: From free energy to free will and back again. Entropy, 23(6), 783. MDPI.&lt;/item&gt;
      &lt;item&gt;Bassett, D. S., &amp;amp; Sporns, O. (2017). Network neuroscience. Nature Neuroscience, 20(3), 353-364.&lt;/item&gt;
      &lt;item&gt;Buzs√°ki, G., &amp;amp; Draguhn, A. (2004). Neuronal oscillations in cortical networks. Science, 304(5679), 1926-1929.&lt;/item&gt;
      &lt;item&gt;Johnson, M. (2016). Principia Qualia. opentheory.net.&lt;/item&gt;
      &lt;item&gt;Johnson, M. (2019). Neural Annealing: Toward a Neural Theory of Everything. opentheory.net.&lt;/item&gt;
      &lt;item&gt;Johnson, M. (2023). Qualia Formalism and a Symmetry Theory of Valence. opentheory.net.&lt;/item&gt;
      &lt;item&gt;Carhart-Harris, R. L., &amp;amp; Friston, K. J. (2019). REBUS and the Anarchic Brain: Toward a Unified Model of the Brain Action of Psychedelics. Pharmacological Reviews, 71(3), 316-344.&lt;/item&gt;
      &lt;item&gt;Dahl, C. J., Lutz, A., &amp;amp; Davidson, R. J. (2015). Reconstructing and deconstructing the self: cognitive mechanisms in meditation practice. Trends in Cognitive Sciences, 19(9), 515-523.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IV. Tanha as artifact of compression pressure&lt;/p&gt;
    &lt;p&gt;I propose reframing tanha as an artifact of the brain‚Äôs compression pressure. I.e. tanha is an artifact of a continual process that subtly but systematically pushes on the complexity of ‚Äòwhat is‚Äô (the neural patterns represented by undoctored SOHMs) to collapse it into a more simple configuration, and sometimes holds it there until we act to make that simplification true. The result of this compression drive conflates ‚Äúwhat is‚Äù, ‚Äúwhat could be‚Äù, ‚Äúwhat should be‚Äù, and ‚Äúwhat will be,‚Äù and this conflation is the source of no end of moral and epistemological confusion.&lt;/p&gt;
    &lt;p&gt;This reframes tanha as both the pressure which collapses complexity into simplicity, and the ongoing stress that comes from maintaining the counterfactual aspects of this collapse (compression stress). We can think of this process as balancing two costs: on one hand, applying compression pressure has metabolic and epistemic costs, both immediate and ongoing. On the other hand, the brain is a finite system and if it doesn‚Äôt continually ‚Äúcompress away‚Äù patterns there will be unmanageable sensory chaos. The right amount of compression pressure is not zero.[2]&lt;/p&gt;
    &lt;p&gt;Equivalently, we can consider tanha as an excessive forcefulness in the metabolization of uncertainty. Erik P. Hoel has written about energy, information, and uncertainty as equivalent and conserved quantities (Hoel 2020): much like literal digestion, the imperative of the nervous system is to extract value from sensations then excrete the remaining information, leaving a low-information, low-uncertainty, clean slate ready for the next sensation (thank you Benjamin Anderson for discussion). However, we are often unskillful in the ways we try to extract value from sensations, e.g. improperly assessing context, trying to extract too much or too little certainty, or trying to extract forms of certainty inappropriate for the sensation.&lt;/p&gt;
    &lt;p&gt;We can define a person‚Äôs personality, aesthetic, and a large part of their phenomenology in terms of how they metabolize uncertainty ‚Äî their library of motifs for (a) initial probing, (b) digestion and integration, and (c) excretion/externalization of any waste products, and the particular reagents for this process they can‚Äôt give themselves and must seek in the world.&lt;/p&gt;
    &lt;p&gt;So far we‚Äôve been discussing brain dynamics on the computational level. But how does the brain do all this ‚Äî what is the mechanism by which it attempts to apply compression pressure to SOHMs? This is essentially the question neuroscience has been asking for the last decade. I believe evolution has coupled two very different systems together to selectively apply compression/prediction pressure in a way that preserves the perceptive reliability of the underlying system (undoctored SOHMs as ground-truth perception) but allows near-infinite capacity for adjustment and hypotheticals. One system focused on perception; one on compression, judgment, planning, and action.&lt;/p&gt;
    &lt;p&gt;The traditional neuroscience approach for locating these executive functions has been to associate them with particular areas of the brain. I suspect the core logic is hiding much closer to the action.&lt;/p&gt;
    &lt;p&gt;Core resources:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Schmidhuber, J. (2008). Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes. Arxiv. Retrieved July 7, 2023, from the Arxiv website.&lt;/item&gt;
      &lt;item&gt;Johnson, M. (2023). Qualia Formalism and a Symmetry Theory of Valence. opentheory.net.&lt;/item&gt;
      &lt;item&gt;Hoel, E. (2020). The Overfitted Brain: Dreams evolved to assist generalization. Arxiv. Retrieved July 7, 2023, from the Arxiv website.&lt;/item&gt;
      &lt;item&gt;Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2), 127-138.&lt;/item&gt;
      &lt;item&gt;Chater, N., &amp;amp; Vit√°nyi, P. (2003). Simplicity: a unifying principle in cognitive science? Trends in Cognitive Sciences, 7(1), 19-22.&lt;/item&gt;
      &lt;item&gt;Bach, D.R., &amp;amp; Dolan, R.J. (2012). Knowing how much you don‚Äôt know: a neural organization of uncertainty estimates. Nature Reviews Neuroscience, 13(8), 572-586.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;V. VSMCs as computational infrastructure&lt;/p&gt;
    &lt;p&gt;Above: the vertical section of an artery wall (Wikipedia, emphasis added; video): the physical mechanism by which we grab sensations and make predictions; the proximate cause of 90% of suffering and 90% of goal-directed behavior.&lt;/p&gt;
    &lt;p&gt;All blood vessels are wrapped by a thin sheathe of vascular smooth muscle cells (VSMCs). The current scientific consensus has the vasculature system as a spiderweb of ever-narrower channels for blood, powered by the heart as a central pump, and supporting systems such as the brain, stomach, limbs, and so on by bringing them nutrients and taking away waste. The sheathe of muscle wrapped around blood vessels undulates in a process called ‚Äúvasomotion‚Äù that we think helps blood keep circulating, much like peristalsis in the gut helps keep food moving, and can help adjust blood pressure.&lt;/p&gt;
    &lt;p&gt;I think all this is true, but is also a product of what‚Äôs been easy to measure and misses 90% of what these cells do.&lt;/p&gt;
    &lt;p&gt;Evolution works in layers, and the most ancient base layers often have rudimentary versions of more specialized capacities (Levin 2022) as well as deep control hooks into newer systems that are built around them. The vascular system actually predates neurons and has co-evolved with the nervous system for hundreds of millions of years. It also has mechanical actuators (VSMCs) that have physical access to all parts of the body and can flex in arbitrary patterns and rhythms. It would be extremely surprising if evolution didn‚Äôt use this system for something more than plumbing. We can also ‚Äúfollow the money‚Äù; the vascular system controls the nutrients and waste disposal for the neural system and will win in any heads-up competition over co-regulation balance.&lt;/p&gt;
    &lt;p&gt;I expect VSMC contractions to influence nearby neurons through e.g. ephaptic coupling, reducing blood flow, and adjusting local physical resonance, and to be triggered by local dissonance in the electromagnetic field.&lt;/p&gt;
    &lt;p&gt;I‚Äôll offer three related hypotheses about the computational role of VSMCs[3] today that in aggregate constitute a neural regulatory paradigm I‚Äôm calling vasocomputation:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compressive Vasomotion Hypothesis (CVH): the vasomotion reflex functions as a compression sweep on nearby neural resonances, collapsing and merging fragile ambivalent patterns (the ‚ÄúBayesian blur‚Äù problem) into a more durable, definite state. Motifs of vasomotion, reflexive reactions to uncertainties, and patterns of tanha are equivalent.&lt;/item&gt;
      &lt;item&gt;Vascular Clamp Hypothesis (VCH): vascular contractions freeze local neural patterns and plasticity for the duration of the contraction, similar to collapsing a superposition or probability distribution, clamping a harmonic system, or pinching a critical network into a definite circuit. Specific vascular constrictions correspond with specific predictions within the Active Inference framework and function as medium-term memory.&lt;/item&gt;
      &lt;item&gt;Latched Hyperprior Hypothesis (LHH): if a vascular contraction is held long enough, it will engage the latch-bridge mechanism common to smooth muscle cells. This will durably ‚Äòfreeze‚Äô the nearby circuit, isolating it from conscious experience and global updating and leading to a much-reduced dynamical repertoire; essentially creating a durable commitment to a specific hyperprior. The local vasculature will unlatch once the prediction the latch corresponds to is resolved, restoring the ability of the nearby neural networks to support a larger superposition of possibilities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The initial contractive sweep jostles the neural superposition of interpretations into specificity; the contracted state temporarily freezes the result; if the contraction is sustained, the latch bridge mechanism engages and cements this freeze as a hyperprior. With one motion the door of possibility slams shut. And so we collapse our world into something less magical but more manageable, one clench at a time. Tanha is cringe.&lt;/p&gt;
    &lt;p&gt;The claim relevant to the Free Energy Principle ‚Äì Active Inference paradigm is we can productively understand the motifs of smooth muscle cells (particularly in the vascular system) as ‚Äúwhere the brain‚Äôs top-down predictive models are hiding,‚Äù which has been an open mystery in FEP-AI. Specific predictions are held as vascular tension, and vascular tension in turn is released by action, consolidated by Neural Annealing, or rendered superfluous by neural remodeling (hold a pattern in place long enough and it becomes the default). Phrased in terms of the Deep CANALs framework which imports ideas from machine learning: the neural weights that give rise to SOHMs constitute the learning landscape, and SOHMs+vascular tension constitute the inference landscape.&lt;/p&gt;
    &lt;p&gt;The claim relevant to Theravada Buddhism is we can productively understand the motifs of the vascular system as the means by which we attempt to manipulate our sensations. Vasomotion corresponds to an attempt to ‚Äòpin down‚Äô a sensation (i.e. tanha); muscle contractions freeze patterns; smooth muscle latches block out feelings of possibility and awareness of that somatic area. Progress on the contemplative path will correspond with both using these forms of tension less, and needing them less. I expect cessations to correspond with a nigh-complete absence of vasomotion (and EEG may measure vasomotion moreso than neural activity).&lt;/p&gt;
    &lt;p&gt;The claim relevant to practical health is that smooth muscle tension, especially in VSMCs, and especially latched tension, is a system science knows relatively little about but is involved in an incredibly wide range of problems, and understanding this system is hugely helpful for knowing how to take care of yourself and others. The ‚Äúlatch-bridge‚Äù mechanism is especially important, where smooth muscle cells have a discrete state where they attach their myosin heads to actin in a way that ‚Äúlocks‚Äù or ‚Äúlatches‚Äù the tension without requiring ongoing energy. Latches take between seconds to minutes to form &amp;amp; dissolve ‚Äî a simple way to experience the latch-bridge cycle releasing is to have a hot bath and notice waves of muscle relaxation. Latches can persist for minutes, hours, days, months, or years (depending on what prediction they‚Äôre stabilizing), and the sum total of all latches likely accounts for the majority of bodily suffering. If you are ‚Äúholding tension in your body‚Äù you are subject to the mechanics of the latch-bridge mechanism. Migraines and cluster headaches are almost certainly inappropriate VSMC latches; all hollow organs are surrounded by smooth muscle and can latch. A long-term diet of poor food (e.g. seed oils) leads to random latch formation and ‚Äúlumpy‚Äù phenomenology. Sauna + cold plunges are an effective way to force the clench-release cycle and release latches; likewise, simply taking time to feel your body and put your attention into latched tissues can release them. Psychedelics can force open latches. Many issues in neuropathy &amp;amp; psychiatry are likely due to what I call ‚Äúlatch spirals‚Äù ‚Äî a latch forms, which reduces blood flow to that area, which reduces energy available to those tissues, which prevents the latch from releasing (since releasing the latch requires activation energy and returning to a freely cycling state also increases the cell‚Äôs rate of energy expenditure).&lt;/p&gt;
    &lt;p&gt;Core resources:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Levin, M. (2022). Technological Approach to Mind Everywhere: An Experimentally-Grounded Framework for Understanding Diverse Bodies and Minds. Frontiers in Systems Neuroscience, 16. https://doi.org/10.3389/fnsys.2022.768201&lt;/item&gt;
      &lt;item&gt;Watson, R., McGilchrist, I., &amp;amp; Levin, M. (2023). Conversation between Richard Watson, Iain McGilchrist, and Michael Levin #2. YouTube.&lt;/item&gt;
      &lt;item&gt;Wikipedia contributors. (2023, April 26). Smooth muscle. In Wikipedia, The Free Encyclopedia. Retrieved 22:39, July 7, 2023, from https://en.wikipedia.org/w/index.php?title=Smooth_muscle&amp;amp;oldid=1151758279&lt;/item&gt;
      &lt;item&gt;Wikipedia contributors. (2023, June 27). Circulatory system. In Wikipedia, The Free Encyclopedia. Retrieved 22:41, July 7, 2023, from https://en.wikipedia.org/w/index.php?title=Circulatory_system&amp;amp;oldid=1162138829&lt;/item&gt;
      &lt;item&gt;Johnson, M., GPT4. (2023). [Mike+GPT4: Latch bridge mechanism discussion].&lt;/item&gt;
      &lt;item&gt;Juliani, A., Safron, A., &amp;amp; Kanai, R. (2023, May 18). Deep CANALs: A Deep Learning Approach to Refining the Canalization Theory of Psychopathology. https://doi.org/10.31234/osf.io/uxmz6&lt;/item&gt;
      &lt;item&gt;Moore CI, Cao R. The hemo-neural hypothesis: on the role of blood flow in information processing. J Neurophysiol. 2008 May;99(5):2035-47. doi: 10.1152/jn.01366.2006. Epub 2007 Oct 3. PMID: 17913979; PMCID: PMC3655718 Added 11-17-23; recommended priority reading&lt;/item&gt;
      &lt;item&gt;Jacob M, Ford J and Deacon T (2023) Cognition is entangled with metabolism: relevance for resting-state EEG-fMRI. Front. Hum. Neurosci. 17:976036. doi: 10.3389/fnhum.2023.976036 Added 1-19-24&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To summarize the story so far: tanha is a grabby reflex which is the source of most moment-by-moment suffering. The ‚Äòtanha as unskillful active inference‚Äô (TUAI) hypothesis suggests that we can think of this ‚Äúgrabbing‚Äù as part of the brain‚Äôs normal predictive and compressive sensemaking, but by default it makes many unskillful predictions that can‚Äôt possibly come true and must hold in a costly way. The vascular clamp hypothesis (VCH) is that we store these predictions (both skillful and unskillful) in vascular tension. The VCH can be divided into three distinct hypotheses (CVH, VCH, LHH) that describe the role of this reflex at different computational and temporal scales. An important and non-obvious aspect of smooth muscle (e.g. VSMCs) is they have a discrete ‚Äúlatch‚Äù setting wherein energy usage and flexibility drops significantly, and sometimes these latches are overly ‚Äòsticky‚Äô; unlatching our sticky latches is a core part of the human condition.&lt;/p&gt;
    &lt;p&gt;Concluding Part I: the above work describes a bridge between three distinct levels of abstraction: a central element in Buddhist phenomenology, the core accounting system within active inference, and a specific muscular reflex. I think this may offer a functional route to synthesize the FEP-AI paradigm and Michael Levin‚Äôs distributed stress minimization work, and in future posts I plan to explore why this mechanism has been overlooked, and how its dynamics are intimately connected with human problems and capacities.&lt;/p&gt;
    &lt;p&gt;I view this research program as integral to both human flourishing and AI alignment.&lt;/p&gt;
    &lt;p&gt;Acknowledgements: This work owes a great deal to Romeo Stevens‚Äô scholarship on tanha, pioneering tanha as a ‚Äòclench‚Äô dynamic, intuitions about muscle tension and prediction, and notion that we commit to dukkha ourselves until we get what we want; Nick Cammarata‚Äôs fresh perspectives on Buddhism and his tireless and generative inquiry around the phenomenology &amp;amp; timescale of tanha; Justin Mares‚Äô gentle and persistent encouragement; Andrea Bortolameazzi‚Äôs many thoughtful comments and observations about the path, critical feedback, and thoughtful support; and Adam Safron‚Äôs steadfast belief and support, theorizing on SOHMs, and teachings about predictive coding and active inference. Much of my knowledge of Buddhist psychology comes from the work and teachings of Anthony Markwell; much of my intuition around tantra and interpersonal embodiment dynamics comes from Elena Selezneva. I‚Äôm also grateful for conversations with Benjamin Anderson about emergence, to Curran Janssens for supporting my research, and to Ivanna Evtukhova for starting me on the contemplative path. An evergreen thank you to my parents their unconditional support. Finally, a big thank-you to Janine Leger and Vitalik Buterin‚Äôs Zuzalu co-living community for creating a space to work on this writeup and make it real.&lt;/p&gt;
    &lt;p&gt;Footnotes:&lt;/p&gt;
    &lt;p&gt;[1] We might attempt to decompose the Active Inference ‚Äì FEP term of ‚Äòprecision weighting‚Äô as (1) the amount of sensory clarity (the amount of precision available in stimuli), and (2) the amount of ‚Äògrabbiness‚Äô of the compression system (the amount of precision we empirically try to extract). Perhaps we could begin to put numbers on tanha by calculating the KL divergence between these distributions.&lt;/p&gt;
    &lt;p&gt;[2] We can speculate that the arrow of compression points away from Buddhism‚Äôs three attributes: e.g. the brain tries to push and prod its SOHMs toward patterns that are stable (dissonance minimization), satisfactory (harmony maximization), and controllable (compression maximization) ‚Äî similar yet subtly distinct targets. Thanks to both Romeo and Andrea for discussion about the three attributes and their opposite.&lt;/p&gt;
    &lt;p&gt;[3] (Added July 19, 2023) Skeletal muscle, smooth muscle, and fascia (which contains myofibroblasts with actin fibers similar to those in muscles) are all found throughout the body and reflexively distribute physical load; it‚Äôs likely they do the same for cognitive-emotional load. Why focus on VSMCs in particular? Three reasons: (1) they have the best physical access to neurons, (2) they regulate bloodflow, and (3) they have the latch-bridge mechanism. I.e. skeletal, non-VSMC smooth muscle, and fascia all likely contribute significantly to distributed stress minimization, and perhaps do so via similar principles/heuristics, but VSMCs seem to be the only muscle with means, motive, and opportunity to finely puppet the neural system, and I believe are indispensably integrated with its moment-by-moment operation in more ways than are other contractive cells. (Thanks to @askyatharth for bringing up fascia.)&lt;/p&gt;
    &lt;p&gt;Edit, April 6th, 2025: a friendly Buddhist scholar suggests that common translations of ta·πáhƒÅ conflate two concepts: ta·πáhƒÅ in Pali is most accurately translated as craving or thirst, whereas the act of clinging itself is ‚ÄúupƒÅdƒÅna (as in the upƒÅdƒÅna-khandhƒÅs), and in the links of dependent origination is one step downstream from the thirst (or impulsive craving) of ta·πáhƒÅ.‚Äù Under this view we can frame ta·πáhƒÅ as a particular default bias in the computational-biochemical tuning of the human nervous system, and upƒÅdƒÅna as the impulsive physical (VSMC) clenching this leads to.&lt;/p&gt;
    &lt;p&gt;Buddhism describes ta·πáhƒÅ as being driven by the three fundamental defilements, greed, fear, &amp;amp; delusion; I expect each defilement maps to a hard truth (aka clearly suboptimal but understandable failure mode) of implementing vasocomputation-based active inference systems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://opentheory.net/2023/07/principles-of-vasocomputation-a-unification-of-buddhist-phenomenology-active-inference-and-physical-reflex-part-i/"/><published>2025-11-27T05:51:21+00:00</published></entry></feed>