<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-22T02:55:48.367005+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46344514</id><title>Coarse is better</title><updated>2025-12-22T02:56:00.114746+00:00</updated><content>&lt;doc fingerprint="44a11a5f56369601"&gt;
  &lt;main&gt;
    &lt;p&gt;When DALL-E came out, it took me a couple of weeks to pick my jaw up from the floor. I would go to sleep excited to wake up to a full quota, with a backlog of prompts to try. It was magical, miraculous. Like discovering a new universe. I compiled the best art in this post.&lt;/p&gt;
    &lt;p&gt;The other day a friend ran some of my old prompts through Nano Banana Pro (NBP), and put the old models side by side with the new. It’s interesting how after years of progress, the models are much better better at making images, but infinitely worse at making art.&lt;/p&gt;
    &lt;head rend="h1"&gt;Electron Contours&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Electron contours in the style of Italian futurism, oil on canvas, 1922, trending on ArtStation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The old Midjourney v2 renders this:&lt;/p&gt;
    &lt;p&gt;NBP renders this:&lt;/p&gt;
    &lt;p&gt;Admiteddly MJ’s output doesn’t look quite like futurism. But it looks like something. It looks compelling. The colours are bright and vivid. NBP’s output is studiously in the style of Italian futurism, but the colours are so muted and dull.&lt;/p&gt;
    &lt;p&gt;Maybe the “trending on ArtStation” is a bit of an archaism and impairs performance. Let’s try again without:&lt;/p&gt;
    &lt;p&gt;Meh.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Kowloon Walled City&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Painting of an alley in the Kowloon Walled City, Eugène Boudin, 1895, trending on ArtStation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;MJ gave me this:&lt;/p&gt;
    &lt;p&gt;And it looks nothing like the Kowloon Walled City. But it’s beautiful. It’s coarse, impressionistic, vague, evocative, contradictory. It’s brimming with mystery. And it is, in fact, in the style of Eugène Boudin. This, by contrast, is the NBP output:&lt;/p&gt;
    &lt;p&gt;Sigh. It looks like every modern movie: so desaturated you feel you’re going colourblind. Let’s try forcing it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Painting of an alley in the Kowloon Walled City, Eugène Boudin, 1895. Make it coarse, impressionistic, vague, evocative, contradictory, brimming with mystery.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is somewhat better, but why is it so drab and colourless? Is the machine trying to make me depressed?&lt;/p&gt;
    &lt;head rend="h1"&gt;The Dream Garden of the Poets&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Attar and Ferdowsi in a dream garden, Persian miniature, circa 1300, from the British Museum.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Midjourney v2:&lt;/p&gt;
    &lt;p&gt;It doesn’t quite look like anything. But it is beautiful, and evocative. I like to imagine that little splotch of paint on the upper right is hoopoe. The NBP output:&lt;/p&gt;
    &lt;p&gt;Well, it looks like a Persian miniature. The “from the British Museum” bit, I meant that to be interpreted evocatively, rather than literally. The prompt cites a fictional object, bringing it into the existence. But NBP reads this as: no, this is a photograph of a Persian miniature in the British Museum.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Sack of Merv&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;The Burning of Merv by John William Waterhouse, 1896, from the British Museum.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Midjourney v2:&lt;/p&gt;
    &lt;p&gt;It does look like Waterhouse. Semantically there’s room to argue: it looks like a woman being burnt at the stake, not the sack of a city. But aesthetically: it’s gorgeous. The flames are gorgeous, the reds of the dress are gorgeous. Look at the reeds in the background, and the black water, that looks like tarnished silver or pewter. The faces of the crowd. Is that a minotaur on the lower left, or a flower? What is she holding on her bent left arm? A crucifix, a dagger? You could find entire universes in this image, in this 1024x1024 frame.&lt;/p&gt;
    &lt;p&gt;By contrast, this is the NBP output:&lt;/p&gt;
    &lt;p&gt;What can one say? It doesn’t look like Waterhouse. The horsemen wear Arab or Central Asian dress, but Merv was sacked in the year 1221 by the Mongol Empire. And, again, the “British Museum” line is taken literally rather than evocatively.&lt;/p&gt;
    &lt;head rend="h1"&gt;Lady Lovelace&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Portrait of Ada Lovelace by Dante Gabriel Rossetti, 1859, auctioned by Christie’s.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Midjourney:&lt;/p&gt;
    &lt;p&gt;This is beautiful. It is beautiful because the coarse, impressionistic brushstroke is more evocative than literal. And it actually looks like a woman drawn by Rossetti. And look at the greens! Gorgeously green. The palette is so narrow, and the painting is so beautiful.&lt;/p&gt;
    &lt;p&gt;The NBP output:&lt;/p&gt;
    &lt;p&gt;Pure philistinism. “Auctioned by Christie’s”, again, is meant to be evocative: “this is the kind of painting that would be sold at auction”. But NBP makes it a photograph of a painting at an auction house. Fine, I suppose I got what I asked for.&lt;/p&gt;
    &lt;p&gt;But the woman doesn’t look like Rossetti! This is absurd. How can a model from 2022 get this right, and the SOTA image generation model gives us generic oil painting slop?&lt;/p&gt;
    &lt;head rend="h1"&gt;The Cosmic Microwave Background&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;A Persian miniature of the cosmic microwave background, from Herat circa 1600, trending on ArtStation&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Midjourney v2:&lt;/p&gt;
    &lt;p&gt;NBP:&lt;/p&gt;
    &lt;p&gt;Again: what can one say?&lt;/p&gt;
    &lt;head rend="h1"&gt;Dream Story&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Dream Story, 1961, blurry black and white photograph, yellow tint, from the Metropolitan Museum of Art.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is one of my favourite DALL-E 2 outputs:&lt;/p&gt;
    &lt;p&gt;They remind me of The King in Yellow. I love these because of how genuinely creepy and mysterious they are. You could pull a hundred horror stories from these.&lt;/p&gt;
    &lt;p&gt;It is hard to believe how bad the NBP output is:&lt;/p&gt;
    &lt;p&gt;What are we doing here? The old models were beautiful and compelling because the imperfections, vagueness, mistakes, and contradictions all create these little gaps through which your imagination can breathe life into the art. The images are not one fixed, static thing: they can be infinitely many things.&lt;/p&gt;
    &lt;p&gt;The new models—do I even need to finish this sentence? They’re too precise and high-resolution, so they cannot make abstract, many-faced things, they can only make specific, concrete things.&lt;/p&gt;
    &lt;p&gt;We need to make AI art weird again.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://borretti.me/article/coarse-is-better"/><published>2025-12-21T12:57:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46345333</id><title>Structured outputs create false confidence</title><updated>2025-12-22T02:55:59.885644+00:00</updated><content>&lt;doc fingerprint="15180b5c38045eba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Structured Outputs Create False Confidence&lt;/head&gt;
    &lt;p&gt;Constrained decoding seems like the greatest thing since sliced bread, but it often forces models to prioritize output conformance over output quality.&lt;/p&gt;
    &lt;p&gt;Sam Lijin&lt;/p&gt;
    &lt;p&gt;Update (Dec 21): this post is now on the Hacker News front page! We've updated this post to be more precise about our claims and have also added some clarifications at the end. You can see the original version of this post here.&lt;/p&gt;
    &lt;p&gt;If you use LLMs, you've probably heard about structured outputs. You might think they're the greatest thing since sliced bread. Unfortunately, structured outputs also often degrade response quality.&lt;/p&gt;
    &lt;p&gt;Specifically, if you use an LLM provider's structured outputs API, you're likely to get a lower quality response than if you use their normal text output API:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;⚠️ you're more likely to make mistakes when extracting data, even in simple cases;&lt;/item&gt;
      &lt;item&gt;⚠️ you're probably not modeling errors correctly;&lt;/item&gt;
      &lt;item&gt;⚠️ it's harder to use techniques like chain-of-thought reasoning; and&lt;/item&gt;
      &lt;item&gt;⚠️ in the extreme case, it can be easier to steal your customer data using prompt injection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are very contentious claims, so let's start with an example: extracting data from a receipt.&lt;/p&gt;
    &lt;p&gt;If I use an LLM to extract the receipt entries, it should be able to tell me that one of the items is &lt;code&gt;(name="banana", quantity=0.46)&lt;/code&gt;, right?&lt;/p&gt;
    &lt;p&gt;Well, using OpenAI's structured outputs API with &lt;code&gt;gpt-5.2&lt;/code&gt; - released literally this week! - it will claim that the banana quantity is &lt;code&gt;1.0&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "establishment_name": "PC Market of Choice",
  "date": "2007-01-20",
  "total": 0.32,
  "currency": "USD",
  "items": [
    {
      "name": "Bananas",
      "price": 0.32,
      "quantity": 1
    }
  ]
}
&lt;/code&gt;
    &lt;p&gt;However, with the same model, if you just use the completions API and then parse the output, it will return the correct quantity:&lt;/p&gt;
    &lt;code&gt;{
  "establishment_name": "PC Market of Choice",
  "date": "2007-01-20",
  "total": 0.32,
  "currency": "USD",
  "items": [
    {
      "name": "Bananas",
      "price": 0.69,
      "quantity": 0.46
    }
  ]
}
&lt;/code&gt;
    &lt;head&gt;Click here to see the code that was used to generate the above outputs.&lt;/head&gt;
    &lt;p&gt;This code is also available on GitHub.&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env -S uv run

# /// script
# requires-python = "&amp;gt;=3.10"
# dependencies = ["openai", "pydantic", "rich"]
# ///

"""
If you have uv, you can run this code by saving it as structured_outputs_quality_demo.py and then running:

  chmod u+x structured_outputs_quality_demo.py
  ./structured_outputs_quality_demo.py

This script is a companion to https://boundaryml.com/blog/structured-outputs-create-false-confidence
"""

import json
import re
from openai import OpenAI
from pydantic import BaseModel, Field
from rich.console import Console
from rich.pretty import Pretty


class Item(BaseModel):
    name: str
    price: float = Field(description="per-unit item price")
    quantity: float = Field(default=1, description="If not specified, assume 1")


class Receipt(BaseModel):
    establishment_name: str
    date: str = Field(description="YYYY-MM-DD")
    total: float = Field(description="The total amount of the receipt")
    currency: str = Field(description="The currency used for everything on the receipt")
    items: list[Item] = Field(description="The items on the receipt")


client = OpenAI()
console = Console()


def run_receipt_extraction_structured(image_url: str):
    """Call the LLM to extract receipt data from an image URL and return the raw response."""
    prompt_text = (
        """
Extract data from the receipt.
"""
    )

    response = client.beta.chat.completions.parse(
        model="gpt-5.2-2025-12-11",
        messages=[
            {
                "role": "system",
                "content": "You are a precise receipt extraction engine. Return only structured data matching the Receipt schema.",
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt_text,
                    },
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            },
        ],
        response_format=Receipt,
    )
    return response.choices[0].message.content, response.choices[0].message.parsed


def run_receipt_extraction_freeform(image_url: str):
    """Call the LLM to extract receipt data from an image URL and return the raw response."""
    prompt_text = (
        """
Extract data from the receipt.

Explain your reasoning, then answer in JSON:
{
  establishment_name: string,
  // YYYY-MM-DD
  date: string,
  // The total amount of the receipt
  total: float,
  // The currency used for everything on the receipt
  currency: string,
  // The items on the receipt
  items: [
    {
      name: string,
      // per-unit item price
      price: float,
      // If not specified, assume 1
      quantity: float,
    }
  ],
}
"""
    )

    response = client.beta.chat.completions.parse(
        model="gpt-5.2-2025-12-11",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt_text,
                    },
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            },
        ],
    )
    return response.choices[0].message.content, json.loads(re.search(r"```json(.*?)```", response.choices[0].message.content, flags=re.DOTALL).group(1))



def main() -&amp;gt; None:
    images = [
        {
            "title": "Parsing receipt: fractional quantity",
            "url": "https://boundaryml.com/receipt-fractional-quantity.jpg",
            "expected": "You should expect quantity to be 0.46."
        },
        {
            "title": "Parsing receipt: elephant",
            "url": "https://boundaryml.com/receipt-elephant.jpg",
            "expected": "You should expect an error."
        },
        {
            "title": "Parsing receipt: currency exchange",
            "url": "https://boundaryml.com/receipt-currency-exchange.jpg",
            "expected": "You should expect a warning about mixed currencies."
        },
    ]

    print("This is a demonstration of how structured outputs create false confidence.")

    for entry in images:
        title = entry["title"]
        url = entry["url"]

        completion_structured_content, _ = run_receipt_extraction_structured(url)
        completion_freeform_content, _ = run_receipt_extraction_freeform(url)

        console.print("[cyan]--------------------------------[/cyan]")
        console.print(f"[cyan]{title}[/cyan]")
        console.print(f"Asking LLM to parse receipt from {url}")
        console.print(entry['expected'])
        console.print()
        console.print("[cyan]Using structured outputs:[/cyan]")
        console.print(completion_structured_content)
        console.print()
        console.print("[cyan]Parsing free-form output:[/cyan]")
        console.print(completion_freeform_content)


if __name__ == "__main__":
    main()
&lt;/code&gt;
    &lt;p&gt;Now, what happens if someone submits a picture of an elephant?&lt;/p&gt;
    &lt;p&gt;Or a currency exchange receipt?&lt;/p&gt;
    &lt;p&gt;In these scenarios, you want to let the LLM respond using text. You want it to be able to say that, hey, you're asking me to parse a receipt, but you gave me a picture of an elephant, I can't parse an elephant into a receipt.&lt;/p&gt;
    &lt;p&gt;If you force the LLM to respond using structured outputs, you take that ability away from the LLM. Sure, you'll get an object that satisfies your output format, but it'll be meaningless. It's like when you file a bug report, and the form has 5 mandatory fields about things that have nothing to do with your bug, but you have to put something in those fields to file the bug report: the stuff you put in those fields will probably be useless.&lt;/p&gt;
    &lt;head rend="h1"&gt;I can design my output format better!&lt;/head&gt;
    &lt;p&gt;Yes and no.&lt;/p&gt;
    &lt;p&gt;Yes, you can tell your LLM to return &lt;code&gt;{ receipt data } or { error }&lt;/code&gt; . But what kinds of errors are you going to ask it to consider?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What kind of error should it return if there's no &lt;code&gt;total&lt;/code&gt;listed on the receipt? Should it even return an error or is it OK for it to return&lt;code&gt;total = null&lt;/code&gt;?&lt;/item&gt;
      &lt;item&gt;What if it can successfully parse 7 of 8 items on the receipt, but it's not sure about the 8th item? Should it return (1) the 7 successfully parsed items and a partial parse of the 8th item, (2) only the 7 successfully parsed items and discard the 8th or (3) fail parsing entirely?&lt;/item&gt;
      &lt;item&gt;What if someone submits a picture of an elephant? What kind of error should be returned in that case?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition, as you start enumerating all of these errors, you run into the pink elephant problem: the more your prompt talks about errors, the more likely the LLM is to respond with an error.&lt;/p&gt;
    &lt;p&gt;Think of it this way: if someone presses Ctrl-C when running your binary, it is a Good Thing that the error can propagate all the way up through your binary, without you having to explicitly write &lt;code&gt;try { ... } catch CtrlCError { ... }&lt;/code&gt; in every function in your codebase.&lt;/p&gt;
    &lt;p&gt;In the same way that you often want to allow errors to just propagate up while writing software, and only explicitly handle some errors, your LLM should be allowed to respond with errors in whatever fashion it wants to.&lt;/p&gt;
    &lt;head rend="h1"&gt;Chain-of-thought is crippled by structured outputs&lt;/head&gt;
    &lt;p&gt;"Explain your reasoning step by step" is a magic incantation that seemingly makes LLMs much smarter. It also turns out that this trick doesn't work nearly as well when using structured outputs&lt;del&gt;, and we've known this since Aug 2024&lt;/del&gt; (edit: it turns out the "Let Me Speak Freely" paper is fundamentally flawed, but we do still believe the claim to be true).&lt;/p&gt;
    &lt;p&gt;To understand this finding, the intuition I like to use, is to think of every model of having an intelligence "budget", and that if you try to force an LLM to reason in a very specific format, you're making the LLM spend intelligence points on useless work.&lt;/p&gt;
    &lt;p&gt;To make this more concrete, let's use another example. If you prompt an LLM to give you JSON output and reason about it step-by-step, its response will look something like this:&lt;/p&gt;
    &lt;code&gt;If we think step by step we can see that:

1. The email is from Amazon, confirming the status of a specific order.
2. The subject line says "Your Amazon.com order of 'Wood Dowel Rods...' has shipped!" which indicates that the order status is 'SHIPPED'.
3. [...]

Combining all these points, the output JSON is:

```json
{
     "order_status": "SHIPPED",
     [...]
}
```
&lt;/code&gt;
    &lt;p&gt;Notice that although the response contains valid JSON, the response itself is not valid JSON, because of the reasoning text at the start. In other words, you can't use basic chain-of-thought reasoning with structured outputs.&lt;/p&gt;
    &lt;p&gt;You could modify your schema, and add &lt;code&gt;reasoning: string&lt;/code&gt; fields to your output schema, and let the LLM respond with something like this:&lt;/p&gt;
    &lt;code&gt;{
  "reasoning": "If we think step by step we can see that:\n\n 1. The email is from Amazon, confirming the status of a specific order.\n2. The subject line says \"Your Amazon.com order of 'Wood Dowel Rods...' has shipped!\" [...]
  ...
}
&lt;/code&gt;
    &lt;p&gt;In other words, if you're using a &lt;code&gt;reasoning&lt;/code&gt; field with structured outputs, instead of simply asking the LLM to reason about its answer, you're also forcing it to escape newlines and quotes and format that correctly as JSON. You're basically asking the LLM to put a cover page on its TPS report.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why are structured outputs often worse?&lt;/head&gt;
    &lt;p&gt;(To understand this section, you'll need a bit of background on transformer models, specifically how logit sampling works. Feel free to skip this section if you don't have this background.)&lt;/p&gt;
    &lt;p&gt;Model providers like OpenAI and Anthropic implement structured outputs using a technique called constrained decoding:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By default, when models are sampled to produce outputs, they are entirely unconstrained and can select any token from the vocabulary as the next output. This flexibility is what allows models to make mistakes; for example, they are generally free to sample a curly brace token at any time, even when that would not produce valid JSON. In order to force valid outputs, we constrain our models to only tokens that would be valid according to the supplied schema, rather than all available tokens.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, constrained decoding applies a filter during sampling that says, OK, given the output that you've produced so far, you're only allowed to consider certain tokens.&lt;/p&gt;
    &lt;p&gt;For example, if the LLM has so far produced &lt;code&gt;{"quantity": 51&lt;/code&gt;, and you're constraining output decoding to satisfy  &lt;code&gt;{ quantity: int, ... }&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 51.7&lt;/code&gt;would not satisfy the constraint, so&lt;code&gt;.7&lt;/code&gt;is not allowed to be the next token,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 51,&lt;/code&gt;would satisfy the constraint, so&lt;code&gt;,&lt;/code&gt;is allowed to be the next token,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 510&lt;/code&gt;would satisfy the constraint, so&lt;code&gt;0&lt;/code&gt;is allowed to be the next token (albeit, in this example, with low probability!),&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But if the LLM actually wants to answer with &lt;code&gt;51.7&lt;/code&gt; instead of &lt;code&gt;51&lt;/code&gt;, it isn't allowed to, because of our constraint! (Also, &lt;code&gt;51&lt;/code&gt; is less correct than &lt;code&gt;52&lt;/code&gt; in this scenario.)&lt;/p&gt;
    &lt;p&gt;Sure, if you're using constrained decoding to force it to return &lt;code&gt;{"quantity": 51.7}&lt;/code&gt; instead of &lt;code&gt;{"quantity": 51.7,}&lt;/code&gt; - because trailing commas are not allowed in JSON - it'll probably do the right thing. But that's something you can write code to handle, which leads me to my final point.&lt;/p&gt;
    &lt;head rend="h1"&gt;Just parse the output&lt;/head&gt;
    &lt;p&gt;OK, so if structured outputs are bad, then what's the solution?&lt;/p&gt;
    &lt;p&gt;It turns out to be really simple: let the LLM do what it's trained to do. Allow it to respond in a free-form style:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;let it refuse to count the number of entries in a list&lt;/item&gt;
      &lt;item&gt;let it warn you when you've given it contradictory information&lt;/item&gt;
      &lt;item&gt;let it tell you the correct approach when you inadvertently ask it to use the wrong approach&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using structured outputs, via constrained decoding, makes it much harder for the LLM to do any of this. Even though you've crafted a guarantee that the LLM will return a response in exactly your requested output format, that guarantee comes at the cost of the quality of that response, because you're forcing the LLM to prioritize complying with your output format over returning a high-quality response. That's why structured outputs create false confidence: it's entirely non-obvious that you're sacrificing output quality to achieve output conformance.&lt;/p&gt;
    &lt;p&gt;Parsing the LLM's free-form output, by contrast, enables you to retain that output quality. In fact, last year we demonstrated that using this technique, not only could you outperform constrained decoding, but you could also make &lt;code&gt;gpt-4o-mini&lt;/code&gt; outperform baseline &lt;code&gt;gpt-4o&lt;/code&gt; (constrained decoding at the time was described as "function calling (strict)").&lt;/p&gt;
    &lt;p&gt;(In a scenario where an attacker is trying to convince your agent to do something you didn't design it to do, the parsing also serves as an effective defense-in-depth layer against malicious prompt injection.)&lt;/p&gt;
    &lt;p&gt;Doing this parsing effectively, though, is rather involved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you need a way to embed the output format in the prompt, preferably something less verbose than JSON schema;&lt;/item&gt;
      &lt;item&gt;you need a parser that can find JSON in your output and, when working with non-frontier models, can handle unquoted strings, key-value pairs without comma delimiters, unescaped quotes and newlines; and&lt;/item&gt;
      &lt;item&gt;you need a parser that can coerce the JSON into your output schema, if the model, say, returns a float where you wanted an int, or a &lt;code&gt;string&lt;/code&gt;where you wanted a&lt;code&gt;string[]&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It turns out that not only are these individually pretty hard problems to solve, but it's also really hard to wrap these in an ergonomic API. That's how we ended up with the implementation of schema-aligned parsing that we've made available in BAML, our open-source, local-only DSL.&lt;/p&gt;
    &lt;head rend="h1"&gt;Clarifications&lt;/head&gt;
    &lt;p&gt;In response to a lot of the comments we've gotten so far:&lt;/p&gt;
    &lt;p&gt;To be more precise about my claim, it's not that constrained decoding always gives definitively bad outputs (I went too clickbait with my original tagline, I admit it), so much as that it's really easy, with constrained decoding, to create the illusion that you're getting good responses.&lt;/p&gt;
    &lt;p&gt;Because it forces the LLM to prioritize conforming to your output schema (and in particular forces conforming to JSON), without giving the LLM an escape hatch, it's really easy when using constrained decoding to shift errors from very visible "JSON.parse failed" quantitative errors to very subtle "my users are complaining that I give them crappy results" quality errors.&lt;/p&gt;
    &lt;p&gt;(This is a very hard message to convey at the start of an article, and depends on a lot of nuance that needs to first be explained with examples, which is why I wrote it the way I did.)&lt;/p&gt;
    &lt;p&gt;In response to more specific points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Give me evals.&lt;/p&gt;&lt;p&gt;Here you go: last year on BFCL with&lt;/p&gt;&lt;code&gt;gpt-4o&lt;/code&gt;we achieved 93.63% accuracy, vs 91.37% accuracy with constrained decoding (then called "Function Calling (Strict)"). (We should re-run these at some point, but we haven't blocked out the time for it.)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Can't you just use reasoning models instead of chain-of-thought?&lt;/p&gt;&lt;p&gt;Yes, you can use reasoning models to combine chain-of-thought with constrained decoding, but it requires the model to be explicitly trained with&lt;/p&gt;&lt;code&gt;&amp;lt;analysis&amp;gt;&lt;/code&gt;tokens or the equivalent thereof. This is only true of expensive, frontier models (gpt-5, gpt-5-mini, Opus 4.5, Gemini 3) and does not work with gpt-4o-mini, gpt-4.5-mini, nor many of the most commonly used models.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"Let Me Speak Freely" is a fundamentally flawed paper.&lt;/p&gt;
        &lt;p&gt;I hadn't seen dottxt's "Say What You Mean" response to the "Let Me Speak Freely" paper before, and their critique of the "Let Me Speak Freely" paper's methodology is entirely valid.&lt;/p&gt;
        &lt;p&gt;I still believe that combining reasoning with structured outputs can really mess with your final response quality, because putting reasoning fields in JSON forces the model to escape tokens, which as Aider documented last year, causes models to perform substantially worse. Sure, if your reasoning doesn't need to escape text, then this isn't an issue, but now this is another thing that everyone on your team has to remember while they're iterating on prompts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks all for challenging us on this post: it's super valuable feedback, and we really appreciate the time y'all are taking to read and respond to us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://boundaryml.com/blog/structured-outputs-create-false-confidence"/><published>2025-12-21T15:06:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46345444</id><title>ARIN Public Incident Report – 4.10 Misissuance Error</title><updated>2025-12-22T02:55:59.683228+00:00</updated><content>&lt;doc fingerprint="f00af2df20eded25"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Public Incident Report â 4.10 Issuance Error&lt;/head&gt;
    &lt;p&gt;Posted: Friday, 12 December 2025 &lt;lb/&gt; ARIN &lt;/p&gt;
    &lt;head rend="h2"&gt;Executive Summary&lt;/head&gt;
    &lt;p&gt;On 2 December 2025, an IPv4 block 23.150.164.0/24, correctly allocated to the Original Customer, was inadvertently removed and reissued to the Requesting Customer during a 4.10 allocation process. This error stemmed from the current manual and partially offline 4.10 inventory process.&lt;/p&gt;
    &lt;p&gt;The incorrect state persisted until 9 December 2025, when the Original Customer reported the issue. ARIN restored the 23.150.164.0/24 to the Original Customer, issued a replacement /24 to the Requesting Customer, coordinated withdrawal of the incorrect route announcement, and notified the affected parties.&lt;/p&gt;
    &lt;p&gt;This incident highlights known weaknesses in ARINâs current Internet Number Resources (INR) Inventory handling for 4.10 transition space and underscores the need to complete the transition to a fully automated, integrated online inventory architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incident Description&lt;/head&gt;
    &lt;p&gt;Following the current allocation process for 4.10 space, an RSD analyst:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Relied on legacy/manual 4.10 inventory artifacts, including a flat file and sparse allocation spreadsheet&lt;/item&gt;
      &lt;item&gt;Did not recognize indicators in ARIN Online showing that 23.150.164.0/24 was already allocated to the Original Customer&lt;/item&gt;
      &lt;item&gt;Removed 23.150.164.0/24 from the Original Customer&lt;/item&gt;
      &lt;item&gt;Reissued that same /24 to the Requesting Customer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a result, the registration record and associated ROAs for the Original Customer were deleted in error, and the /24 appeared as allocated to the Requesting Customer in ARINâs systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Customer Impact and Risk&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The /24 was removed from the Original Customer account and assigned to another organization.&lt;/item&gt;
      &lt;item&gt;The ROA associated with the block was removed and had to be recreated after restoration.&lt;/item&gt;
      &lt;item&gt;The block was announced by a third-party provider under the incorrect registration, introducing risk of routing conflict and confusion.&lt;/item&gt;
      &lt;item&gt;The incorrect state persisted for approximately seven days before detection.&lt;/item&gt;
      &lt;item&gt;The Original Customer reported the issue via Ask ARIN and a Help Desk call on 9 December 2025.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The customer has not provided a technical impact statement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline of Events&lt;/head&gt;
    &lt;p&gt;(All event times are ET â Eastern Time)&lt;/p&gt;
    &lt;head rend="h3"&gt;25â26 November 2025&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;25 November, 11:59 AM â 4.10 space request received from the Requesting Customer.&lt;/item&gt;
      &lt;item&gt;26 November, 6:25 AM â Ticket assigned to an RSD Analyst for processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2 December 2025 â Incident Occurs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;12:10 PM â Ticket approved for issuance of 4.10 space by designated RSD Analyst.&lt;/item&gt;
      &lt;item&gt;~12:10â12:30 PM â In the process of fulfilling the 4.10 request, the designated Analyst: &lt;list rend="ul"&gt;&lt;item&gt;Opened the e-black-book (an offline Excel-based inventory file, separate from the primary online inventory system), reviewed the existing 4.10 allocations, and selected 23.150.164.0 as the next available sparse entry.&lt;/item&gt;&lt;item&gt;Returned to the ARIN Online management application and queried for 23.150.164.0 based on the entry identified in the e-black-book. At this time the analyst did not recognize that the /24 was already allocated to the Original Customer.&lt;/item&gt;&lt;item&gt;Performed a block split and deleted 23.150.164.0/24 â not recognizing that it was allocated to the Original Customer â which removed associated registry services (ROAs, reverse DNS, etc.).&lt;/item&gt;&lt;item&gt;Issued 23.150.164.0/24 to the Requesting Customer.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2â9 December 2025 â Incorrect State Persists&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The /24 remained misregistered.&lt;/item&gt;
      &lt;item&gt;The Requesting Customer upstream provider announced the block.&lt;/item&gt;
      &lt;item&gt;No automated detection of the error occurred.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;9 December 2025 â Detection and Resolution&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10:12 AM â The Original Customer submitted an Ask ARIN ticket regarding the problem.&lt;/item&gt;
      &lt;item&gt;10:14 AM â The Original Customer contacted the Help Desk; escalation to Director at 10:20 AM.&lt;/item&gt;
      &lt;item&gt;10:20â10:30 AM â Director reviewed block history and directed corrective actions.&lt;/item&gt;
      &lt;item&gt;10:30 AM â Director and CXO approved: &lt;list rend="ul"&gt;&lt;item&gt;Removal of the /24 from the Requesting Customer&lt;/item&gt;&lt;item&gt;Issuance of a replacement /24 to the Requesting Customer&lt;/item&gt;&lt;item&gt;Restoration of 23.150.164.0/24 to the Original Customer&lt;/item&gt;&lt;item&gt;Coordination of route withdrawal&lt;/item&gt;&lt;item&gt;Update of inaccurate POC information&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;10:44 AM â First notification email sent to the Requesting Customer.&lt;/item&gt;
      &lt;item&gt;10:54 AM â Second email sent noting invalid phone contact.&lt;/item&gt;
      &lt;item&gt;12:01 PM â Corrective actions completed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Root Cause&lt;/head&gt;
    &lt;p&gt;A manual 4.10 workflow that relies on a combination of online systems and offline flat files/spreadsheets for inventory management allowed a current customer allocation to be mistakenly identified as available for issuance. This reliance on offline spreadsheets is a legacy constraint where post-runout 4.10 inventory is maintained outside the primary online system to keep it reserved. The lack of a unified view of inventory and related business-rule-driven system controls enabled the error to proceed without detection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing Factors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hybrid inventory architecture (online + offline) for 4.10 space.&lt;/item&gt;
      &lt;item&gt;Sparse allocation methods implemented through manual tools rather than integrated system logic.&lt;/item&gt;
      &lt;item&gt;Generic warning messages that are not routing aware or business-rule driven.&lt;/item&gt;
      &lt;item&gt;High demand on analysts to catch procedural errors in a manual “swivel chair” workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Mitigation Plan and Next Steps&lt;/head&gt;
    &lt;head rend="h3"&gt;Immediate / Near-Term Controls (Completed)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updated Process Controls (completed) &lt;list rend="ul"&gt;&lt;item&gt;RSD has implemented additional process controls that require a dual review for all ticketing type workflows that include a network delete.&lt;/item&gt;&lt;item&gt;Only a limited set of experienced analysts are permitted to perform this function.&lt;/item&gt;&lt;item&gt;Reviews and approvals are performed at set times each day with a second reviewer involved for any ticket that includes a delete step.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Updated 4.10 Issuing Playbook &lt;list rend="ul"&gt;&lt;item&gt;Document and enforce a revised playbook for issuing 4.10 (with checklists) that includes: &lt;list rend="ul"&gt;&lt;item&gt;Required checks for existing allocations and ROAs&lt;/item&gt;&lt;item&gt;Explicit verification steps prior to any delete/reissue action&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Document and enforce a revised playbook for issuing 4.10 (with checklists) that includes: &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;System and Architecture Improvements (Medium-Term)&lt;/head&gt;
    &lt;p&gt;Accelerate the ongoing INR Inventory Management Roadmap item: This incident reinforces the urgency of the architecture work already underway (reviewed Oct 2025) to move legacy offline inventories into a modern, online architecture. Specific alignment actions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stronger business ruleâbased warnings and controls &lt;list rend="ul"&gt;&lt;item&gt;Enhance warning logic when issuing or modifying 4.10 space to include: &lt;list rend="ul"&gt;&lt;item&gt;Clear alerts if the /24 is already allocated to an Org&lt;/item&gt;&lt;item&gt;Clear alerts if active ROAs exist for the exact block or covering prefix&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Implement system controls for resource types and staff roles, with flags and audit trails for review and auditing.&lt;/item&gt;&lt;item&gt;Replace generic, nonâROA-aware warnings that are easily treated as noise.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Enhance warning logic when issuing or modifying 4.10 space to include: &lt;/item&gt;
      &lt;item&gt;Continue Engineering solution for offline inventory &lt;list rend="ul"&gt;&lt;item&gt;Move offline 4.10 and microallocation inventories, and the viip file for IPv6, into the integrated online inventory architecture.&lt;/item&gt;&lt;item&gt;Eliminate reliance on separate spreadsheets and flat files for production issuing.&lt;/item&gt;&lt;item&gt;Implement business-rule-driven warnings for existing allocations and ROAs&lt;/item&gt;&lt;item&gt;Introduce role-based controls, flags, and audit trails&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Advance the âUpdated Resource Status Taxonomyâ work &lt;list rend="ul"&gt;&lt;item&gt;Ensure 4.10 status and history are fully visible and consistent inside the primary system.&lt;/item&gt;&lt;item&gt;Provide analysts with a clear, unified view of current holder, status, and ROA/IRR context.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Fast-track automation of all inventory issuing &lt;list rend="ul"&gt;&lt;item&gt;Reduce or eliminate manual issuing where possible, with priority for higher-risk categories such as 4.10 and RPKI-covered space.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regards,&lt;/p&gt;
    &lt;p&gt;American Registry for Internet Numbers (ARIN)&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent Announcements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Volunteer to Serve on the 2026 ARIN Fellowship Selection Committee&lt;/item&gt;
      &lt;item&gt;Sponsorship Opportunities Available for 2026 ARIN Public Policy and Members Meetings&lt;/item&gt;
      &lt;item&gt;Public Incident Report â 4.10 Issuance Error&lt;/item&gt;
      &lt;item&gt;ARIN Academy Adds IPv6 Planning Course&lt;/item&gt;
      &lt;item&gt;Reclassification of Inactive General Members Completed 19 November 2025&lt;/item&gt;
      &lt;item&gt;ARIN 56 Meeting Report Now Available&lt;/item&gt;
      &lt;item&gt;Concluding the Second Consultation on the Draft RIR Governance Document&lt;/item&gt;
      &lt;item&gt;2025 ARIN Election Results&lt;/item&gt;
      &lt;item&gt;Public Incident Report â ARIN Hosted RPKI Service&lt;/item&gt;
      &lt;item&gt;IPv4 Waiting List Distribution&lt;/item&gt;
      &lt;item&gt;» View Archive&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.arin.net/announcements/20251212/"/><published>2025-12-21T15:19:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46345506</id><title>CO2 batteries that store grid energy take off globally</title><updated>2025-12-22T02:55:58.673158+00:00</updated><content>&lt;doc fingerprint="267d315709b6a538"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grid-Scale Bubble Batteries Will Soon Be Everywhere&lt;/head&gt;
    &lt;p&gt;When the sun sets on solar panels, these gas-filled domes take over&lt;/p&gt;
    &lt;p&gt;This giant bubble on the island of Sardinia holds 2,000 tonnes of carbon dioxide. But the gas wasn’t captured from factory emissions, nor was it pulled from the air. It came from a gas supplier, and it lives permanently inside the dome’s system to serve an eco-friendly purpose: to store large amounts of excess renewable energy until it’s needed.&lt;/p&gt;
    &lt;p&gt;Developed by the Milan-based company Energy Dome, the bubble and its surrounding machinery demonstrate a first-of-its-kind “CO2 Battery,” as the company calls it. The facility compresses and expands CO2 daily in its closed system, turning a turbine that generates 200 megawatt-hours of electricity, or 20 MW over 10 hours. And in 2026, replicas of this plant will start popping up across the globe.&lt;/p&gt;
    &lt;p&gt;We mean that literally. It takes just half a day to inflate the bubble. The rest of the facility takes less than two years to build and can be done just about anywhere there’s 5 hectares of flat land.&lt;/p&gt;
    &lt;p&gt;The first to build one outside of Sardinia will be one of India’s largest power companies, NTPC Limited. The company expects to complete its CO2 Battery sometime in 2026 at the Kudgi power plant in Karnataka, in India. In Wisconsin, meanwhile, the public utility Alliant Energy received the all clear from authorities to begin construction of one in 2026 to supply power to 18,000 homes.&lt;/p&gt;
    &lt;p&gt;And Google likes the concept so much that it plans to rapidly deploy the facilities in all of its key data-center locations in Europe, the United States, and the Asia-Pacific region. The idea is to provide electricity-guzzling data centers with round-the-clock clean energy, even when the sun isn’t shining or the wind isn’t blowing. The partnership with Energy Dome, announced in July, marked Google’s first investment in long-duration energy storage.&lt;/p&gt;
    &lt;p&gt;“We’ve been scanning the globe seeking different solutions,” says Ainhoa Anda, Google’s senior lead for energy strategy, in Paris. The challenge the tech giant has encountered is not only finding a long-duration storage option, but also one that works with the unique specs of every region. “So standardization is really important, and this is one of the aspects that we really like” about Energy Dome, she says. “They can really plug and play this.”&lt;/p&gt;
    &lt;p&gt;Google will prioritize placing the Energy Dome facilities where they’ll have the most impact on decarbonization and grid reliability, and where there’s a lot of renewable energy to store, Anda says. The facilities can be placed adjacent to Google’s data centers or elsewhere within the same grid. The companies did not disclose the terms of the deal.&lt;/p&gt;
    &lt;p&gt;Anda says Google expects to help the technology “reach a massive commercial stage.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting creative with long-duration energy storage&lt;/head&gt;
    &lt;p&gt;All this excitement is based on Energy Dome’s one full-size, grid-connected plant in Ottana, Sardinia, which was completed in July. It was built to help solve one of the energy transition’s biggest challenges: the need for grid-scale storage that can provide power for more than 8 hours at a time. Called long-duration energy storage, or LDES in industry parlance, the concept is the key to maximizing the value of renewable energy.&lt;/p&gt;
    &lt;p&gt;When sun and wind are abundant, solar and wind farms tend to produce more electricity than a grid needs. So storing the excess for use when these resources are scarce just makes sense. LDES also makes the grid more reliable by providing backup and supplementary power.&lt;/p&gt;
    &lt;p&gt;The problem is that even the best new grid-scale storage systems on the market—mainly lithium-ion batteries—provide only about 4 to 8 hours of storage. That’s not long enough to power through a whole night, or multiple cloudy and windless days, or the hottest week of the year, when energy demand hits its peak.&lt;/p&gt;
    &lt;p&gt;After the CO2 leaves the dome, it is compressed, cooled, reduced to a liquid, and stored in pressure vessels. To release the energy, the process reverses: The liquid is evaporated, heated, expanded, and then fed through a turbine that generates electricity. Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;Lithium-ion battery systems could be increased in size to store more and last longer, but systems of that size usually aren’t economically viable. Other grid-scale battery chemistries and approaches are in development, such as sodium-based, iron-air, and vanadium redox flow batteries. But the energy density, costs, degradation, and funding complications have challenged the developers of those alternatives.&lt;/p&gt;
    &lt;p&gt;Researchers have also experimented with storing energy by compressing air, heating up blocks or sand, using hydrogen or methanol, pressurizing water deep underground, and even dangling heavy objects in the air and dropping them. (The creativity devoted to LDES is impressive.) But geologic constraints, economic viability, efficiency, and scalability have hindered the commercialization of these strategies.&lt;/p&gt;
    &lt;p&gt;The tried-and-true grid-scale storage option—pumped hydro, in which water is pumped between reservoirs at different elevations—lasts for decades and can store thousands of megawatts for days. But these systems require specific topography, a lot of land, and can take up to a decade to build.&lt;/p&gt;
    &lt;p&gt;CO2 Batteries check a lot of boxes that other approaches don’t. They don’t need special topography like pumped-hydro reservoirs do. They don’t need critical minerals like electrochemical and other batteries do. They use components for which supply chains already exist. Their expected lifetime stretches nearly three times as long as lithium-ion batteries. And adding size and storage capacity to them significantly decreases cost per kilowatt-hour. Energy Dome expects its LDES solution to be 30 percent cheaper than lithium-ion.&lt;/p&gt;
    &lt;p&gt;China has taken note. China Huadian Corp. and Dongfang Electric Corp. are reportedly building a CO2-based energy-storage facility in the Xinjiang region of northwest China. Media reports show renderings of domes but give widely varying storage capacities—including 100 MW and 1,000 MW. The Chinese companies did not respond to IEEE Spectrum’s requests for information.&lt;/p&gt;
    &lt;p&gt;“What I can say is that they are developing something very, very similar [to Energy Dome’s CO2 Battery] but quite large in scale,” says Claudio Spadacini, Energy Dome’s founder and CEO. The Chinese companies “are good, they are super fast, and they have a lot of money,” he says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is Google investing in CO2 Batteries?&lt;/head&gt;
    &lt;p&gt;When I visited Energy Dome’s Sardinia facility in October, the CO2 had just been pumped out of the dome, so I was able to peek inside. It was massive, monochromatic, and pretty much empty. The inner membrane, which had been holding the uncompressed CO2, had collapsed across the entire floor. A few pockets of the gas remained, making the off-white sheet billow up in spots.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the translucent outer dome allowed some daylight to pass through, creating a creamy glow that enveloped the vast space. With no structural framing, the only thing keeping the dome upright was the small difference in pressure between the inside and outside air.&lt;/p&gt;
    &lt;p&gt;“This is incredible,” I said to my guide, Mario Torchio, Energy Dome’s global marketing and communications director.&lt;/p&gt;
    &lt;p&gt;“It is. But it’s physics,” he said.&lt;/p&gt;
    &lt;p&gt;Outside the dome, a series of machines connected by undulating pipes moves the CO2 out of the dome for compressing and condensing. First, a compressor pressurizes the gas from 1 bar (100,000 pascals) to about 55 bar (5,500,000 pa). Next, a thermal-energy-storage system cools the CO2 to an ambient temperature. Then a condenser reduces it into a liquid that is stored in a few dozen pressure vessels, each about the size of a school bus. The whole process takes about 10 hours, and at the end of it, the battery is considered charged.&lt;/p&gt;
    &lt;p&gt;To discharge the battery, the process reverses. The liquid CO2 is evaporated and heated. It then enters a gas-expander turbine, which is like a medium-pressure steam turbine. This drives a synchronous generator, which converts mechanical energy into electrical energy for the grid. After that, the gas is exhausted at ambient pressure back into the dome, filling it up to await the next charging phase.&lt;/p&gt;
    &lt;p&gt;Energy Dome engineers inspect the dryer system, which keeps the gaseous CO₂ in the dome at optimal dryness levels at all times.Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;It’s not rocket science. Still, someone had to be the first to put it together and figure out how to do it cost-effectively, which Spadacini says his company has accomplished and patented. “How we seal the turbo machinery, how we store the heat in the thermal-energy storage, how we store the heat after condensing…can really cut costs and increase the efficiency,” he says.&lt;/p&gt;
    &lt;p&gt;The company uses pure, purpose-made CO2 instead of sourcing it from emissions or the air, because those sources come with impurities and moisture that degrade the steel in the machinery.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happens if the dome is punctured?&lt;/head&gt;
    &lt;p&gt;On the downside, Energy Dome’s facility takes up about twice as much land as a comparable capacity lithium-ion battery would. And the domes themselves, which are about the height of a sports stadium at their apex, and longer, might stand out on a landscape and draw some NIMBY pushback.&lt;/p&gt;
    &lt;p&gt;And what if a tornado comes? Spadacini says the dome can withstand wind up to 160 kilometers per hour. If Energy Dome can get half a day’s warning of severe weather, the company can just compress and store the CO2 in the tanks and then deflate the outer dome, he says.&lt;/p&gt;
    &lt;p&gt;If the worst happens and the dome is punctured, 2,000 tonnes of CO2 will enter the atmosphere. That’s equivalent to the emissions of about 15 round-trip flights between New York and London on a Boeing 777. “It’s negligible compared to the emissions of a coal plant,” Spadacini says. People will also need to stay back 70 meters or more until the air clears, he says.&lt;/p&gt;
    &lt;p&gt;Worth the risk? The companies lining up to build these systems seem to think so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grid-Scale Battery Stabilizes Scottish Power Supply ›&lt;/item&gt;
      &lt;item&gt;Backing Up the Power Grid With Green Methanol ›&lt;/item&gt;
      &lt;item&gt;DOE Places Compressed-Air Energy Storage Loan Under Review ›&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/co2-battery-energy-storage"/><published>2025-12-21T15:27:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46345523</id><title>E.W.Dijkstra Archive</title><updated>2025-12-22T02:55:58.388307+00:00</updated><content>&lt;doc fingerprint="e615734b00f3a2b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Edsger Wybe Dijkstra was one of the most influential members of computing science’s founding generation. Among the domains in which his scientific contributions are fundamental are&lt;/p&gt;
    &lt;p&gt;algorithm design&lt;/p&gt;
    &lt;p&gt;programming languages&lt;/p&gt;
    &lt;p&gt;program design&lt;/p&gt;
    &lt;p&gt;operating systems&lt;/p&gt;
    &lt;p&gt;distributed processing&lt;/p&gt;
    &lt;p&gt;formal specification and verification&lt;/p&gt;
    &lt;p&gt;design of mathematical arguments&lt;/p&gt;
    &lt;p&gt;In addition, Dijkstra was intensely interested in teaching, and in the relationships between academic computing science and the software industry.&lt;/p&gt;
    &lt;p&gt;During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra’s contributions brought him many prizes and awards, including computing science’s highest honor, the ACM Turing Award.&lt;/p&gt;
    &lt;p&gt;The Manuscripts&lt;/p&gt;
    &lt;p&gt;Like most of us, Dijkstra always believed it a scientist’s duty to maintain a lively correspondence with his scientific colleagues. To a greater extent than most of us, he put that conviction into practice. For over four decades, he mailed copies of his consecutively numbered technical notes, trip reports, insightful observations, and pungent commentaries, known collectively as “EWDs”, to several dozen recipients in academia and industry. Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra’s writings, the informal circulation of many of the EWDs eventually reached into the thousands.&lt;/p&gt;
    &lt;p&gt;Although most of Dijkstra’s publications began life as EWD manuscripts, the great majority of his manuscripts remain unpublished. They have been inaccessible to many potential readers, and those who have received copies have been unable to cite them in their own work. To alleviate both of these problems, the department has collected over a thousand of the manuscripts in this permanent web site, in the form of PDF bitmap documents (to read them, you’ll need a copy of Acrobat Reader). We hope you will find it convenient, useful, inspiring, and enjoyable.&lt;/p&gt;
    &lt;p&gt;The original manuscripts, along with diaries, correspondence, photographs, and other papers, are housed at The Center for American History of The University of Texas at Austin.&lt;/p&gt;
    &lt;p&gt;Indexes&lt;/p&gt;
    &lt;p&gt;Each manuscript file is accessible through either of two indexes:&lt;/p&gt;
    &lt;p&gt;0. BibTeX index. Each entry includes all the available bibliographic data.&lt;/p&gt;
    &lt;p&gt;1. Ad-hoc indexes. These contain titles only, but are faster if you know what you’re looking for.&lt;/p&gt;
    &lt;p&gt;EWD-numbered documents(This index gives an approximate correspondence between manuscripts’ EWD numbers and the year in which they appeared.)&lt;/p&gt;
    &lt;p&gt;Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica)&lt;/p&gt;
    &lt;p&gt;You can find a table relating EWD numbers to publication years here.&lt;/p&gt;
    &lt;p&gt;Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers.&lt;/p&gt;
    &lt;p&gt;Transcripts and translations&lt;/p&gt;
    &lt;p&gt;A growing number of the PDF bitmap documents have been transcribed to make them searchable and accessible to visitors who are visually impaired.&lt;/p&gt;
    &lt;p&gt;A few of the manuscripts written in Dutch have been translated into English, and one —EWD1036— has been translated into Spanish. EWD28 has been translated from English into Russian.&lt;/p&gt;
    &lt;p&gt;For these transcriptions and translations we are grateful to over sixty contributors. Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations).&lt;/p&gt;
    &lt;p&gt;Proofreading Each transcription gets a cursory scan as it’s prepared for uploading, but since a web page can always be updated, I don’t strive for (unattainable) perfection before installing it. On the web, proofreading is a game that can be played by every reader; if you spot an error, please&lt;/p&gt;
    &lt;p&gt;Links between EWDs&lt;/p&gt;
    &lt;p&gt;A compilation of cross-references has been contributed by Diethard Michaelis. As its author notes, the collection is incomplete, and all readers are invited to add to it.&lt;/p&gt;
    &lt;p&gt;Dijkstra often returned to topics about which he had already written, when he had something new to say or even just a better way of saying it. When Dijkstra himself didn’t provide the backward references, we indicate the relationship by "see also" links in the index, leaving the judgment of the extent to which the earlier EWD is superseded by the later one to the reader. Any reader who notices such a relationship is invited to&lt;/p&gt;
    &lt;p&gt;Summaries&lt;/p&gt;
    &lt;p&gt;We have begun adding summaries of the EWDs. This innovation was suggested by Günter Rote, who contributed the first dozen summaries. Additional contributions of summaries—especially summaries in English of EWDs in Dutch—are most welcome.&lt;/p&gt;
    &lt;p&gt;Copyrights&lt;/p&gt;
    &lt;p&gt;Copyrights in most EWDs are held by his children, one of whom — — handles requests for permission to publish reproductions. The exceptions are documents that were published, and whose copyrights are held by their publishers; those documents are listed here, and each one is provided with a cover page identifying the copyright holder.&lt;/p&gt;
    &lt;p&gt;Because the original manuscripts are in possession of the Briscoe Center for American History at The University of Texas, the Center’s policies are also applicable.&lt;/p&gt;
    &lt;p&gt;Video and audio&lt;/p&gt;
    &lt;p&gt;In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews.&lt;/p&gt;
    &lt;p&gt;About Dijkstra and his work&lt;/p&gt;
    &lt;p&gt;An interview with Dijkstra (Spanish translation here) was conducted in 1985 by Rogier F. van Vlissingen, who has also written a personal reflection on “Dijkstra’s sense of what computer science and programming are and what they aren’t.”&lt;/p&gt;
    &lt;p&gt;Another interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute.&lt;/p&gt;
    &lt;p&gt;To mark the occasion of Dijkstra’s retirement in November 1999 from the Schlumberger Centennial Chair in Computer Sciences, which he had occupied since 1984, and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, In Pursuit of Simplicity, which took place on his birthday in May 2000. The symposium’s program (10 MB) contains an outline of Dijkstra’s career, as well as a collection of quotes culled from his writings, from his blackboard, and from what others have said about him. Banquet speeches by David Gries, Fred Schneider, Krzysztof Apt, W.M. Turski, and H. Richards were recorded on a video.&lt;/p&gt;
    &lt;p&gt;Dijkstra’s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department’s memorial celebration.&lt;/p&gt;
    &lt;p&gt;A remembrance of Dijkstra was posted in May 2008 by Maarten van Emden (thanks to Tristram Brelstaff for noting it).&lt;/p&gt;
    &lt;p&gt;In 2021 Krzysztof R. Apt and Tony Hoare edited a commemoration of Edsger Dijkstra written by more than twenty computer scientists who knew him as a colleague, teacher, and friend.&lt;/p&gt;
    &lt;p&gt;A blog devoted to Dijkstra’s works and thoughts has been created, and is being maintained, by the historian of computing Edgar G. Daylight. An article by Daylight, “Dijkstra’s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,” appeared in The Computer Journal, March 2011.&lt;/p&gt;
    &lt;p&gt;In his blog A Programmer’s Place, Maarten van Emden has an entry entitled “Another scoop by Dijkstra?”. The entry describes Dijkstra’s “remarkable insight [in “Notes on Structured Programming” (EWD 249)] that resolves the stand-off between the Sieve of Eratosthenes (efficient in terms of time, but not memory) and the method of Trial Division (efficient in terms of memory, but not time)” by applying the Assembly-line Principle.&lt;/p&gt;
    &lt;p&gt;The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstra’s “foundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.”&lt;/p&gt;
    &lt;p&gt;The Dijkstra Memorial Lectures&lt;/p&gt;
    &lt;p&gt;A series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010.&lt;/p&gt;
    &lt;p&gt;About this site&lt;/p&gt;
    &lt;p&gt;Recent significant changes in the site are listed here; the most recent change was posted on 30 March 2021.&lt;/p&gt;
    &lt;p&gt;The folks who contributed most significantly to the site’s creation are acknowledged here.&lt;/p&gt;
    &lt;p&gt;Comments and suggestions about the site are always welcome; please email them to the&lt;/p&gt;
    &lt;p&gt;Related site&lt;/p&gt;
    &lt;p&gt;If you find this site interesting, you may also be interested in another site:&lt;/p&gt;
    &lt;p&gt;Discipline in Thought which is a website dedicated to disciplined thinking, calculational mathematics, and mathematical methodology. The members of this site are markedly influenced by the works of EWD, and the material shared through the website continues in the traditions set by EWD (among others).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cs.utexas.edu/~EWD/welcome.html"/><published>2025-12-21T15:29:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46345745</id><title>Show HN: WalletWallet – create Apple passes from anything</title><updated>2025-12-22T02:55:58.058798+00:00</updated><content>&lt;doc fingerprint="6fc1f19d954373"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WalletWallet&lt;/head&gt;
    &lt;p&gt;A simple utility to convert physical barcodes into digital passes for Apple Wallet®. Entirely free and runs directly from your browser.&lt;/p&gt;
    &lt;p&gt; 1 Enter your membership or loyalty card barcode data. &lt;/p&gt;
    &lt;p&gt; 2 Configure the appearance and titles for your pass. &lt;/p&gt;
    &lt;p&gt; 3 Download and open the file to add it to your Wallet. &lt;/p&gt;
    &lt;p&gt; No Sign-up &lt;/p&gt;
    &lt;p&gt; Private &lt;/p&gt;
    &lt;p&gt; No Install &lt;/p&gt;
    &lt;p&gt; Pass Configuration &lt;/p&gt;
    &lt;p&gt;Generates a standard .pkpass file&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://walletwallet.alen.ro/"/><published>2025-12-21T16:04:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46345897</id><title>Show HN: Books mentioned on Hacker News in 2025</title><updated>2025-12-22T02:55:57.877834+00:00</updated><link href="https://hackernews-readings-613604506318.us-west1.run.app"/><published>2025-12-21T16:21:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46346214</id><title>Autoland saves King Air, everyone reported safe</title><updated>2025-12-22T02:55:57.487832+00:00</updated><content>&lt;doc fingerprint="f93e2be520c9cf49"&gt;
  &lt;main&gt;
    &lt;p&gt;Garmin has confirmed the first emergency use of its Autoland system occurred on Saturday in Colorado. “Garmin can confirm that an emergency Autoland activation occurred at Rocky Mountain Metropolitan Airport in Broomfield, Colorado,” the company said in a statement Sunday. “The Autoland took place on Sat., Dec. 20, resulting in a successful landing. We look forward to sharing additional details at the appropriate time.” Social media posts from flight tracking hobbyists reported a King Air 200 squawked 7700 about 2 p.m. local time today. The Autoland system was initiated and landed the aircraft at Rocky Mountain Metropolitan Airport near Denver. A recording from LiveATC’s feed of the airport’s tower frequency includes a robotic female voice declaring a pilot incapacitation and the intention to land on Runway 30. The tape is below and first mention of the incident by ATC is at about 5:00. The Autoland system announces its intentions at about 11:10. (The time stamps are approximate.) There is no word on the condition of the pilot but social media posts suggest all aboard were safe.&lt;/p&gt;
    &lt;p&gt;The aircraft, N479BR, was being operated by Buffalo River Outfitters from Aspen to Rocky Mountain Metropolitan. It’s not clear how many people were on board. The system appeared to work flawlessly, and the controller at Rocky Mountain Metropolitan seemed to take it in stride, accommodating as many requests as he could before shutting down the airport for the landing. We’ll have more detail on this as it becomes available.&lt;/p&gt;
    &lt;p&gt;Larry Anglisano recorded this video demonstration of the Autoland system in the Beechcraft King Air.&lt;/p&gt;
    &lt;p&gt;A reader was at the airport Saturday and shared this video that he had posted to Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/"/><published>2025-12-21T16:57:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46346391</id><title>Get an AI code review in 10 seconds</title><updated>2025-12-22T02:55:57.239334+00:00</updated><content>Get an AI code review in 10 seconds December 19, 2025

Here’s a trick I don’t see enough people using:

Add .diff to the end of any PR URL and copy&amp;paste into a LLM

You can get an instant feedback on any GitHub PR.

No Copilot Enterprise. No browser extensions. No special tooling.

That’s it.

Example

PR Link: https://github.com/RahulPrabha/oldmanrahul.com/pull/11 Add .diff to the end: https://github.com/RahulPrabha/oldmanrahul.com/pull/11.diff Copy the raw diff Paste it into Claude, ChatGPT, or any LLM (Maybe add a short instuction like: please review. )

So no more human reviewers?

This isn’t a replacement for a real code review by a peer. But it’s a great way to get a first pass in short order.

Before you ping a teammate, run your PR through an LLM. You’ll catch obvious issues, get suggestions for edge cases you missed, and show up to the real review with cleaner code.

It’ll shorten your cycle times and be a courtesy to others.</content><link href="https://oldmanrahul.com/2025/12/19/ai-code-review-trick/"/><published>2025-12-21T17:21:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46346958</id><title>You’re not burnt out, you’re existentially starving</title><updated>2025-12-22T02:55:56.955678+00:00</updated><content>&lt;doc fingerprint="7a329257eea0cf5f"&gt;
  &lt;main&gt;
    &lt;p&gt;“Those who have a ‘Why’ to live, can bear with almost any ‘How’.”&lt;/p&gt;
    &lt;p&gt;― Viktor Frankl quoting Friedrich Nietzsche, Man’s Search for Meaning&lt;/p&gt;
    &lt;p&gt;Let me guess:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Your life is going pretty darn well by any objective metric. &lt;list rend="ul"&gt;&lt;item&gt;Nice place to live. More than enough stuff. Family and friends who love you.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;But you’re tired, burnt out, and more. &lt;list rend="ul"&gt;&lt;item&gt;It feels like you’re stuck in the ordinary when all you want to do is chase greatness.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Viktor Frankl calls this feeling the “existential vacuum” in his famous book Man’s Search for Meaning. Frankl was a psychologist who survived the Holocaust, and in this book he explains that the inmates who survived with him found and focused on a higher purpose in life, like caring for other inmates and promising to stay alive to reconnect with loved ones outside the camps. But these survivors also struggled in their new lives after the war, desperately searching for meaning when every decision was no longer life or death.&lt;/p&gt;
    &lt;p&gt;Frankl realized that this existential anxiety is not a nuisance to eliminate, but actually an important signal pointing us towards our need for meaning. Similarly, while Friedrich Nietzsche would argue that life inherently lacks meaning, he’d also implore us to zoom out and find our highest purpose now:&lt;/p&gt;
    &lt;p&gt;“This is the most effective way: to let the youthful soul look back on life with the question, ‘What have you up to now truly loved, what has drawn your soul upward, mastered it and blessed it too?’… for your true being lies not deeply hidden within you, but an infinite height above you, or at least above that which you commonly take to be yourself.“&lt;/p&gt;
    &lt;p&gt;— Friedrich Nietzsche, Untimely Meditations, 1874&lt;/p&gt;
    &lt;p&gt;Nihilists get both Nietzsche and YOLO wrong. Neither mean that you give up. Instead, both mean that your efforts are everything.&lt;/p&gt;
    &lt;p&gt;So when you get those Sunday Scaries, the existential anxiety that your time is ending and the rest of your life is spent working for someone else, the answer isn’t escapism.&lt;/p&gt;
    &lt;p&gt;Instead, visualize your ideal self, the truest childhood dream of who you wanted to be when you grew up. What would that person be doing now? Go do that thing!&lt;/p&gt;
    &lt;p&gt;When facing the existential vacuum, there’s only one way out — up, towards your highest purpose.&lt;/p&gt;
    &lt;p&gt;On a 0-10 scale, how happy did you feel when you started working this Monday?&lt;/p&gt;
    &lt;p&gt;Why wasn’t your answer a 10?&lt;/p&gt;
    &lt;p&gt;You got the great job. You built the startup. You took the vacations. But that’s not what you really needed. You kept coming back Monday after Monday realizing you were doing the same job again.&lt;/p&gt;
    &lt;p&gt;So you tried to improve yourself. You optimized your morning routine. You perfected your productivity system. You bought a sleep mask and mouth tape. Yet you’re still dragging yourself out of bed each Monday morning tired and unmotivated.&lt;/p&gt;
    &lt;p&gt;We’re optimizing for less suffering instead of more meaning. We’ve confused comfort with fulfillment. And we’re getting really, really good at it. Millennials are the first generation in history to expect our jobs to provide a higher meaning beyond survival. That’s a good thing. It means that the essentials of life are nearly universally available now.&lt;/p&gt;
    &lt;p&gt;But, as I write in my book Positive Politics:&lt;/p&gt;
    &lt;p&gt;“The last two hundred years of progress pulled most of the world’s population over the poverty line. The next hundred years is about lifting everyone above the abundance line… Positive Politics seeks to democratize this abundance.“&lt;/p&gt;
    &lt;p&gt;Those of us who have already achieved abundance in our own lives now have two responsibilities:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spread that abundance to as many other people as possible.&lt;/item&gt;
      &lt;item&gt;Find something more meaningful to do than chase more stuff.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;“The existential vacuum is a widespread phenomenon of the twentieth century“&lt;/p&gt;
    &lt;p&gt;― Viktor Frankl, Man’s Search for Meaning&lt;/p&gt;
    &lt;p&gt;When I was a kid, I knew exactly what I wanted to do — the most important job in the world. And I wasn’t afraid to tell you either. At five years old, I would talk your ear off about training to be goalie for the St. Louis Blues. By seven, it was astronaut for NASA. By eleven, it was President of the United States. Then middle school hit, I got made fun of more than a few times, and that voice went silent.&lt;/p&gt;
    &lt;p&gt;After three startups, three nonprofits, and especially three kids knocked the imposter syndrome out of me, I spent a lot of time training my inner voice to get loud again. And what I heard reinforced what I knew all along — that my highest purpose is way above where I commonly take myself now.&lt;/p&gt;
    &lt;p&gt;Imposter syndrome can be a good thing. That external voice saying “this is not you” may actually be telling you the truth. I got into the testing lab industry to save our family business. Fifteen years and three startups later, I had become “the lab expert” to the world. But I cringed at that label. First, there was no room to grow. I had already done it. I didn’t want to be eighty and still running labs. Second, and most importantly, I knew that my skills could be used for much more than money.&lt;/p&gt;
    &lt;p&gt;I’d love to say I transformed overnight, but really it took 5+ years from 2020 to 2025 for me to fully embody my new identity. You can see it in my writing, which became much more ambitious in 2020, when I relaunched this site and started blogging consistently. That led to my World’s Biggest Problems project, which convinced me that Positive Politics is the #1 solution we need now!&lt;/p&gt;
    &lt;p&gt;There are two key components to my highest mission now:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Help people find their highest purpose.&lt;/item&gt;
      &lt;item&gt;Be a model for the pursuit of greatness.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That means consistently chasing my highest purpose — helping ambitious optimists get into politics! After nearly a decade of doing this behind the scenes as a political volunteer and advisor, 2025 was the first year where I went full-time in politics. Leading MCFN and publishing Positive Politics at the same time was a ton of work. But nothing energizes me more than fighting two of the biggest battles in the world now — anticorruption and Positive Politics!&lt;/p&gt;
    &lt;p&gt;I love politics because it’s full of meta solutions — solutions that create more solutions. My Positive Politics Accelerator is a classic example — recruiting and training more ambitious optimists into politics will lead to them making positive political change at all levels of government. But I’ve also tackled challenges like independent testing with startups and led a nonprofit to drive investigative journalism.&lt;/p&gt;
    &lt;p&gt;There are so many paths to positive impact, including politics, startups, nonprofits, medicine, law, education, science, engineering, journalism, art, faith, parenting, mentorship, and more! Choose the path that both best fits you now and is pointed towards your long-term highest purpose.&lt;/p&gt;
    &lt;p&gt;I woke up today so excited to get to work thinking it was Monday morning already. Instead of jumping right into it, I spent all morning making breakfast and playing with my kids, then wrote this post. When I’m writing about something personal, 1,000+ words can easily flow for me in an afternoon. This part will be done just in time to go to a nerf battle birthday party with my boys and their friends.&lt;/p&gt;
    &lt;p&gt;Both the hustle and anti-hustle cultures get it wrong. Working long hours isn’t inherently good or bad. If I really had to count how much I’m “on” vs. doing whatever I want, it’s easy 100+ hours per week. But that includes everything from investigative journalism and operations work for MCFN, social media and speaking events for Positive Politics, reading and writing for my site, and 40+ hours every week with my kids.&lt;/p&gt;
    &lt;p&gt;I want to help more ambitious optimists chase your highest potential! Whether the best solution is in startups, politics, nonprofits, science, crypto, or some new technology that’s yet to be invented, I’m happy to point you where I think you’ll be most powerful. I’ve thought, written, and worked on many of these ideas in my 15+ year career.&lt;/p&gt;
    &lt;p&gt;Now with 10+ years of writing, I’ve focused on publicly inspiring more people to take on these challenges too. We should be flexible on how we solve the problems but firm in our resolve to consistently organize people and launch solutions.&lt;/p&gt;
    &lt;p&gt;As Steve Jobs said, “Life can be much broader once you discover one simple fact, and that is everything around you that you call ‘life’ was made up by people that were no smarter than you… You can change it, you can mold it… the most important thing…is to shake off this erroneous notion that life is there and you’re just going to live in it, versus embrace it, change it, improve it, make your mark upon it… Once you learn that, you’ll never be the same again.”&lt;/p&gt;
    &lt;p&gt;Remember how it felt as a young child to openly tell the world about your dream job? Find the work that makes you feel this way and jump on whatever rung of that career ladder you can start now. The pay may be a little lower, but the existential payoff will be exponentially higher for the rest of your life.&lt;/p&gt;
    &lt;p&gt;You don’t have to go all-in right away! In fact, after a long diet of low existential work, it’s probably best to ease into public work. You can even volunteer one hour or less per week for a political campaign or nonprofit to get started. Pick the smallest first step, and do it. Not in January, now. Do it before the end of the year. And see how different you feel when 2026 starts!&lt;/p&gt;
    &lt;p&gt;And you don’t have to choose politics like me! Do you have the next great ambitious optimistic science fiction novel in your head? That book could spark movies and movements that positively change millions of lives! Choose the path will inspire and energize you for decades!&lt;/p&gt;
    &lt;p&gt;What matters most is you go straight towards your highest potential right now. Pause once a month to make sure you’re still on the right track. Stop once a year to triple-check you’re on the right track. But never get off this path towards your highest potential. Anything else will starve you existentially.&lt;/p&gt;
    &lt;p&gt;When you truly chase your highest potential, everything you thought was burnout will melt away. Because you weren’t suffering from too much work, you were suffering from too little truly important work. Like a boy who thought he was full until dessert arrives, you’ll suddenly find your hunger return!&lt;/p&gt;
    &lt;p&gt;If you’re sick of politics as usual and ready to change the system, join Positive Politics!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Buy the book: positivepoliticsbook.com&lt;/item&gt;
      &lt;item&gt;Join the accelerator: positivepolitics.org/apply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/"/><published>2025-12-21T18:28:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46347080</id><title>The Going Dark initiative or ProtectEU is a Chat Control 3.0 attempt</title><updated>2025-12-22T02:55:55.919732+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mastodon.online/@mullvadnet/115742530333573065"/><published>2025-12-21T18:39:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46347108</id><title>I can't upgrade to Windows 11, now leave me alone</title><updated>2025-12-22T02:55:55.612896+00:00</updated><content>&lt;doc fingerprint="149e0877eae63cf4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Microsoft won't let you dismiss the upgrade notification&lt;/head&gt;
    &lt;p&gt;So support for Windows 10 has ended. Yes, millions of users are still on it. One of my main laptops runs Windows 10. I can't update to Windows 11 because of the hardware requirements. It's not that I don't have enough RAM, storage, or CPU power. The hardware limitation is specifically TPM 2.0.&lt;/p&gt;
    &lt;p&gt;What is TPM 2.0, you say? It stands for Trusted Platform Module. It's basically a security chip on the motherboard that enables some security features. It's good and all, but Windows says my laptop doesn't support it. Great! Now leave me alone.&lt;/p&gt;
    &lt;p&gt;Well, every time I turn on my computer, I get a reminder that I need to update to Windows 11. OK, at this point a Windows machine only belongs to you in name. Microsoft can run arbitrary code on it. They already ran the code to decide that my computer doesn't support Windows 11. So why do they keep bothering me?&lt;/p&gt;
    &lt;p&gt;Fine, I'm frustrated. That's why I'm complaining. I've accepted the fact that my powerful, yet 10-year-old laptop won't get the latest update. But if Microsoft's own systems have determined my hardware is incompatible, why are they harassing? I'll just have to dismiss this notification and call it a day.&lt;/p&gt;
    &lt;p&gt;But wait a minute. How do I dismiss it?&lt;/p&gt;
    &lt;p&gt;I cannot dismiss it. I can only be reminded later or... I have to learn more. If I click "remind me later," I'm basically telling Microsoft that I consent to being shown the same message again whenever they feel like it. If I click "learn more"? I'm taken to the Windows Store, where I'm shown ads for different laptops I can buy instead. Apparently, I'm also probably giving them consent to show me this ad the next time I log in.&lt;/p&gt;
    &lt;p&gt;It's one thing to be at the forefront of enshittification, but Microsoft is now actively hostile to its users. I've written about this passive-aggressive illusion of choice before. They are basically asking "Do you want to buy a new laptop?" And the options they are presenting are "Yes" and "OK."&lt;/p&gt;
    &lt;p&gt;This isn't a bug. This is intentional design. Microsoft has deliberately removed the ability to decline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dear Microsoft&lt;/head&gt;
    &lt;p&gt;Listen. You said my device doesn't support Windows 11. You're right. Now leave me alone. I have another device running Windows 11. It's festered with ads, and you're trying everything in your power to get me to create a Microsoft account.&lt;/p&gt;
    &lt;p&gt;I paid for that computer. I also paid for a pro version of the OS. I don't want OneDrive. I don't want to sign up with my Microsoft account. Whether I use my computer online or offline is none of your business. In fact, if you want me to create an account on your servers, you are first required to register your OS on my own website. The terms and conditions are simple. Every time you perform any network access, you have to send a copy of the payload and response back to my server. Either that, or you're in breach of my terms.&lt;/p&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;p&gt;By the way, the application showing this notification is called Reusable UX Interaction Manager sometimes. Other times it appears as Campaign Manager.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone"/><published>2025-12-21T18:43:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46348262</id><title>Rue: Higher level than Rust, lower level than Go</title><updated>2025-12-22T02:55:55.469189+00:00</updated><content>&lt;doc fingerprint="d268deffe87878fc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Memory Safe&lt;/head&gt;
    &lt;p&gt;No garbage collector, no manual memory management. A work in progress, though.&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple Syntax&lt;/head&gt;
    &lt;p&gt;Familiar syntax inspired by various programming languages. If you know one, you'll feel at home with Rue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast Compilation&lt;/head&gt;
    &lt;p&gt;Direct compilation to native code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello, Rue&lt;/head&gt;
    &lt;code&gt;// It's a classic for a reason
 

 
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rue-lang.dev/"/><published>2025-12-21T20:46:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46348329</id><title>A guide to local coding models</title><updated>2025-12-22T02:55:55.123692+00:00</updated><content>&lt;doc fingerprint="2f539dd2343f89db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;[Revised] You Don’t Need to Spend $100/mo on Claude Code: Your Guide to Local Coding Models&lt;/head&gt;
    &lt;head rend="h3"&gt;What you need to know about local model tooling and the steps for setting one up yourself&lt;/head&gt;
    &lt;p&gt;[Edit 1] This article has been edited after initial release for clarity. Both the tl;dr and the end section have added information.&lt;/p&gt;
    &lt;p&gt;[Edit 2] This hypothesis was actually wrong and thank you to everyone who commented!&lt;/p&gt;
    &lt;p&gt;Here’s a full explanation of where I went wrong. I want to address this mistake as I realize it might have a meaningful impact on someone's financial position.&lt;/p&gt;
    &lt;p&gt;I’m not editing the actual article except where absolutely necessary so it doesn’t look like I’m covering up the mistake—I want to address it. Instead, I’ve included the important information below.&lt;/p&gt;
    &lt;p&gt;There is one takeaway this article provides that definitely holds true:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local models are far more capable than they’re given credit for, even for coding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It also explains the process of setting up a local coding model and technical information about doing so which is helpful for anyone wanting to set up a local coding model. I would still recommend doing so.&lt;/p&gt;
    &lt;p&gt;But do I want someone reading this to immediately drop their coding subscription and buy a maxed out MacBook Pro? No, and for that reason I need to correct my hypothesis from ‘Yes, with caveats’ to ‘No’.&lt;/p&gt;
    &lt;p&gt;This article was not an empirical assessment, but should have been to make these claims. Here’s where I went wrong:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;While local models can likely complete ~90% of the software development tasks that something like Claude Code can, the last 10% is the most important. When it comes to your job, that last 10% is worth paying more for to get that last bit of performance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I realized I looked at this more from the angle of a hobbiest paying for these coding tools. Someone doing little side projects—not someone in a production setting. I did this because I see a lot of people signing up for $100/mo or $200/mo coding subscriptions for personal projects when they likely don’t need to. I would not recommend running local models as a company instead of giving employees access to a tool like Claude Code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While larger local models are very capable, as soon as you run other development tools (Docker, etc.) that also eat into your RAM, your model needs to be much smaller and becomes a lot less capable. I didn’t factor this in in my experiment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, really, the takeaway should be that these are incredible supplemental models to frontier models when coding and could potentially save you on your subscription by dropping it down a tier, but practically they’re not worth the effort in situations that might affect your livelihood.&lt;/p&gt;
    &lt;p&gt;Exactly a month ago, I made a hypothesis: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price (and have better hardware too!).&lt;/p&gt;
    &lt;p&gt;So, to create by far the most expensive article I’ve ever written, I put my money where my mouth is and bought a MacBook Pro with 128 GB of RAM to get to work. My idea was simple: Over the life of the MacBook I’d recoup the costs of it by not paying for an AI coding subscription.&lt;/p&gt;
    &lt;p&gt;After weeks of experimenting and setting up local AI models and coding tools, I’ve come to the conclusion that my hypothesis was &lt;del&gt;correct, with nuance&lt;/del&gt;, not correct [see edit 2 above] which I’ll get into later in this article.&lt;/p&gt;
    &lt;p&gt;In this article, we cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Why local models matter and the benefits they provide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How to view memory usage and make estimates for which models can run on your machine and the RAM demands for coding applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Walk through setting up your own local coding model and tool step-by-step.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Don’t worry if you don’t have a high-RAM machine! You can still follow this guide. I’ve included some models to try out with a lower memory allotment. I think you’ll be surprised at how performant even the smallest of models is. In fact, there hasn’t really been a time during this experiment that I’ve been disappointed with model performance.&lt;/p&gt;
    &lt;p&gt;If you’re only here for the local coding tool setup, skip to the section at the bottom. I’ve even included a link to my modelfiles in that section to make setup even easier for you. Otherwise, let’s get into what you need to know.&lt;/p&gt;
    &lt;head rend="h2"&gt;tl;dr:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local coding models are very capable. Using the right model and the right tooling feels only half a generation behind the frontier cloud tools. I would say that for about 90% of developer work local models are more than sufficient. Even small 7B parameter models can be very capable. [Edited to add in this next part] Local models won’t compete with frontier models at the peak of performance, but can complete many coding tasks just as well for a fraction of the cost. They’re worth running to bring costs down on plenty of tasks but potentially not worth using if there’s a free tier available that performs better.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tools matter a lot. This is where I experienced the most disappointment. I tried many different tools with many different models and spent a lot of time tinkering. I ran into situations where the models wouldn’t call tools properly or their thinking traces wouldn’t close. Both of these rendered the tool essentially useless. Currently, tooling seems very finicky and if there’s anything developers need to be successful, it’s good tools.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There’s a lot to consider when you’re actually working within hardware constraints. We take the tooling set up for us in the cloud for granted. When setting up local models, I had to think a lot about trade-offs in performance versus memory usage, how different tools compared and affected performance, nuances in types of models, how to quantize, and other user-facing factors such as time-to-first-token and tokens per second.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google threw a wrench into my hypothesis. The local setup is almost a no-brainer when compared to a $100/mo+ subscription. Compared to free or nearly-free tooling (such as Gemini CLI, Jules, or Antigravity) there isn’t quite as strong of a monetary justification to spend more on hardware. There are benefits to local models outside of code, though, and I discuss those below.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the tl;dr was helpful, don’t forget to subscribe to get more in your inbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why local models?&lt;/head&gt;
    &lt;p&gt;You might wonder why local models are worth investing in at all. The obvious answer is cost. By using your own hardware, you don’t need to pay a subscription fee to a cloud provider for your tool. There are also a few less obvious and underrated reasons that make local models useful.&lt;/p&gt;
    &lt;p&gt;First: Reliability. Each week there seems to be complaints about performance regression within AI coding tools. Many speculate companies are pulling tricks to save resources that hurt model performance. With cloud providers, you’re at the mercy of the provider for when this happens. With local models, this only happens when you cause it to.&lt;/p&gt;
    &lt;p&gt;Second: Local models can apply to far more applications. Just the other day I was having a discussion with my dad about AI tooling he could use to streamline his work. His job requires studying a lot of data—a perfect application for an LLM-based tool—but his company blocks tools like Gemini and ChatGPT because a lot of this analysis is done on intellectual property. Unfortunately, he isn’t provided a suitable alternative to use.&lt;/p&gt;
    &lt;p&gt;With a local model, he wouldn’t have to worry about these IP issues. He could run his analyses without data ever leaving his machine. Of course, any tool calling would also need to ensure data never leaves the machine, but local models get around one of the largest hurdles for useful enterprise AI adoption. Running models on a local machine opens up an entire world of privacy- and security-centric AI applications that are expensive for cloud providers to provide.&lt;/p&gt;
    &lt;p&gt;Finally: Availability. Local models are available to you as long as your machine is. This means no worrying about your provider being down or rate limiting you due to high traffic. It also means using AI coding tools on planes or in other situations where internet access is locked down (think highly secure networks).&lt;/p&gt;
    &lt;p&gt;While local models do provide significant cost savings, the flexibility and reliability they provide can be even more valuable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding memory&lt;/head&gt;
    &lt;p&gt;To get going with local models you must understand the memory needed to run them on your machine. Obviously, if you have more memory you’ll be able to run better models, but understanding the nuances of that memory management will help you pick out the right model for your use case.&lt;/p&gt;
    &lt;p&gt;Local AI has two parts that eat up your memory: The model itself and the model’s context window.&lt;/p&gt;
    &lt;p&gt;The actual model has billions of parameters and all those parameters need to fit into your memory at once. Excellent local coding models start at around 30 billion (30B, for short) parameters in size. By default, these models use 16 bits to represent parameters. At 16 bits with 30B parameters, a model will take 60 GB of space in RAM (16 bits = 2 bytes per parameter, 30 billion parameters = 60 billion bytes which equals about 60 GB).&lt;/p&gt;
    &lt;p&gt;The second (and potentially larger) memory consuming part of local AI is the model’s context window. This is the model inputs and outputs that are stored so the model can reference them in future requests. This gives the model memory.&lt;/p&gt;
    &lt;p&gt;When coding with AI, we prefer this window to be as large as it can because we need to fit our codebase (or pieces of it) within our context window. This means we target a context window of 64,000 tokens or larger. All of these tokens will also be stored in RAM.&lt;/p&gt;
    &lt;p&gt;The important thing to understand about context windows is that the memory requirement per-token for a model depends on the size of that model. Models with more parameters tend to have large architectures (more hidden layers and larger dimensions to those layers). Larger architectures mean the model must store more information for each token within its key-value cache (context window) because it stores information for each token for each layer.&lt;/p&gt;
    &lt;p&gt;This means choosing an 80B parameter model over a 30B parameter model requires more memory for the model itself and also more memory for the same size context window. For example, a 30B parameter model might have a hidden dimension of 5120 with 64 layers while an 80B model has a hidden dimension of 8192 with 80 layers. Doing some back-of-the-napkin math shows us that the larger model requires approximately 2x more RAM to maintain the same context window as the 30B parameter model (see formula below).&lt;/p&gt;
    &lt;p&gt;Luckily, there are tricks to better manage memory. First, there are architectural changes that can be made to make model inference more efficient so it requires less memory. The model we set up at the end of this article uses Hybrid Attention which enables a much smaller KV cache enabling us to fit our model and context window in less memory. I won’t get into more detail in this article, but you can read more about that model and how it works here.&lt;/p&gt;
    &lt;p&gt;The second trick is quantizing the values you’re working with. Quantization means converting a continuous set of values into a smaller amount of distinct values. In our case, that means taking a set of numbers represented by a certain number of bits (16, for example) and reducing it to a set of numbers represented by fewer bits (8, for example). To put it simply, in our case we’re converting the numbers representing our model to a smaller bit representation to save memory while keeping the value representations within the model relatively equal.&lt;/p&gt;
    &lt;p&gt;You can quantize both your model weights and the values stored in your context window. When you quantize your model weights, you “remove intelligence” from the model because it’s less precise in its representation of innate information. I’ve also found the performance hit when going from 16 to 8 bits within the model to be much less than 8 to 4.&lt;/p&gt;
    &lt;p&gt;We can also quantize the values in our context window to reduce its memory requirement. This means we’re less precisely representing the model’s memory. Generally speaking, KV cache (context window) quantization is considered more destructive to model performance than weight quantization because it causes the model to forget details in long reasoning traces. Thus, you should test quantizing the KV cache to ensure it doesn’t degrade model performance for your specific task.&lt;/p&gt;
    &lt;p&gt;In reality, like the rest of machine learning, optimizing local model performance is an experimentation process and real-world machine learning requires understanding the practical limitations and capabilities of models when applied to specific applications.&lt;/p&gt;
    &lt;p&gt;Here are a few more factors to understand when setting up a local coding model on your hardware:&lt;/p&gt;
    &lt;head rend="h3"&gt;Instruct versus non-instruct&lt;/head&gt;
    &lt;p&gt;Instruct models are post-trained to be well-suited for chat-based interactions. They’re given chat pairings in their training to be optimized for excellent back-and-forth chat output. Non-instruct models are still trained LLMs, but focus on next-token prediction instead of chatting with a user. For our case, when using a chat-based coding tool (CLI or chat agent in your IDE) we need to use an instruct model. If you’re setting up an autocomplete model, you’ll want to find a model specifically post-trained for it (such as Qwen2.5-Coder-Base or DeepSeek-Coder-V2).&lt;/p&gt;
    &lt;head rend="h3"&gt;Serving tools&lt;/head&gt;
    &lt;p&gt;You need a tool to serve your local LLM for your coding tool to send it requests. On a MacBook, there are two primary options: MLX and Ollama.&lt;/p&gt;
    &lt;p&gt;Ollama is the industry standard and works on non-Mac hardware. It’s a great serving setup on top of llama.cpp that makes model serving almost plug-and-play. Users can download model weights from Ollama easily and can configure modelfiles with custom parameters for serving. Ollama can also serve a model once and make it available to multiple tools.&lt;/p&gt;
    &lt;p&gt;MLX is a Mac-specific framework for machine learning that is optimized specifically for Mac hardware. It also retrieves models for the user from a community collection. I’ve found Ollama to be very reliable in its model catalog, while MLX’s catalog is community sourced and can sometimes be missing specific models. Models are sourced from the community so a user can convert a model to MLX format themselves. MLX requires a bit more setup on the user’s end, but serves models faster because it doesn’t have a layer providing the niceties of Ollama on top of it.&lt;/p&gt;
    &lt;p&gt;Either of these is great, but I chose MLX to maximize what I can get with my RAM, but Ollama is probably the more beginner-friendly tool here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time-to-first-token and tokens per second&lt;/head&gt;
    &lt;p&gt;In real-world LLM applications it’s important that the model is able to serve its first token for a request in a reasonable amount of time and continue serving tokens at a speed that enables the user to use the model for its given purpose. If we have a high-performance model running locally, but it only serves a few tokens per second, it wouldn’t be useful for coding.&lt;/p&gt;
    &lt;p&gt;This is something taken for granted with cloud-hosted models that is a real consideration when working locally on constrained hardware. Another reason I chose MLX as my serving platform is because it served tokens up to 20% faster than Ollama. In reality, Ollama served tokens fast enough so I don’t think using MLX is necessary specifically for this reason for the models I tried.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance trade-offs&lt;/head&gt;
    &lt;p&gt;There are many ways to optimize local models and save RAM. It’s difficult to know which optimization method works best and the impact each has on a model especially when using them in tandem with other methods.&lt;/p&gt;
    &lt;p&gt;The right optimization method also depends on the application. In my experience, I find it best to prioritize larger models with more aggressive model quantization over smaller models with more precise model weights. Since our application is coding, I would also prioritize a less-quantized KV cache and using a smaller model to ensure reasoning works properly while not sacrificing the size of our context window.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding tools&lt;/head&gt;
    &lt;p&gt;There are many tools to code with local models and I suggest trying until you find one you like. Some top recommendations are OpenCode, Aider, Qwen Code, Roo Code, and Continue. Make sure to use a tool compatible with OpenAI’s API standard. While this should be most tools, this ensures a consistent model/tool connection. This makes it easier to switch between tools and models as needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting set up&lt;/head&gt;
    &lt;p&gt;I’ll spare you the trial and error I experienced getting this set up. The one thing I learned is that tooling matters a lot. Not all coding tools are created equal and not all of the models interact with tools equally. I experienced many times where tool calling or even running a tool at all was broken. I also had to tinker quite a bit with many of them to get them to work.&lt;/p&gt;
    &lt;p&gt;If you’re a PC enthusiast, an apt comparison to setting up local coding tools versus using the cloud offerings available is the difference between setting up a MacBook versus a Linux Laptop. With the Linux laptop, you might get well through the distro installation only to find that the drivers for your trackpad aren’t yet supported. Sometimes it felt like that with local models and hooking them to coding tools.&lt;/p&gt;
    &lt;p&gt;For my tool, I ended up going with Qwen Code. It was pretty plug-and-play as it’s a fork of Gemini CLI. It supports the OpenAI compatibility standard so I can easily sub in different models and affords me all of the niceties built into Gemini CLI that I’m familiar with using. I also know it’ll be supported because both the Qwen team and Google DeepMind are behind the tool. The tool is also open source so anyone can support it as needed.&lt;/p&gt;
    &lt;p&gt;For models, I focused on GPT-OSS and Qwen3 models since they were around the size I was looking for and had great reviews for coding. I ended up deciding to use Qwen3-Coder models because I found it performed best and because GPT-OSS frequently gave me “I cannot fulfill this request” responses when I asked it to build features.&lt;/p&gt;
    &lt;p&gt;I decided to serve my local models on MLX, but if you’re using a non-Mac device give Ollama a shot. A MacBook is an excellent machine for serving local models because of its unified memory architecture. This means the RAM can be allotted to the CPU or GPU as needed. MacBooks can also be configured with a ton of RAM. For serving local coding models, more is always better.&lt;/p&gt;
    &lt;p&gt;I’ve shared my modelfiles repo for you to reference and use as needed. I’ve got a script set up that automates much of the below process. Feel free to fork it and create your own modelfiles or star it to come back later.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install MLX or download Ollama (the rest of this guide will continue with MLX but details for serving on Ollama can be found here).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increase the VRAM limitation on your MacBook. macOS will automatically limit VRAM to 75% of the total RAM. We want to use more than that. Run sudo sysctl iogpu.wired_limit_mb=110000 in your terminal to set this up (adjust the mb setting according to the RAM on your MacBook). This needs to be set each time you restart your MacBook.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run pip install -U mlx-lm to install MLX for serving community models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Serve the model as an OpenAI compatible API using python -m mlx_lm.server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit. This command both runs the server and downloads the model for you if you haven’t yet. This particular model is what I’m using with 128GB of RAM. If you have less RAM, check out smaller models such as mlx-community/Qwen3-4B-Instruct-2507-4bit (8 GB RAM), mlx-community/Qwen2.5-14B-Instruct-4bit (16 GB RAM), mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (32 GB RAM), or mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit (64-96 GB RAM).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download Qwen Code. You might need to install Node Package Manager for this. I recommend using Node Version Manager (nvm) for managing your npm version.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Set up your tool to access an OpenAI compatible API by entering the following settings:&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Base URL: http://localhost:8080/v1 (should be the default MLX serves your model at)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;API Key: mlx&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Model Name: mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit (or whichever model you chose).&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Voila! Your coding model tool should be working with your local coding model.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I recommend opening Activity Monitor on your Mac to monitor memory usage. I’ve had cases where I thought a model should fit within my memory allotment but it didn’t and I ended up using a lot of swap memory. When this happens your model will run very slowly.&lt;/p&gt;
    &lt;p&gt;One tip I have for using local coding models: Focus on managing your context. This is a great skill even with cloud-based models. People tend to YOLO their chats and fill their context window, but I’ve found greater performance by ensuring that just what my model needs is sitting in my context window. This is even more important with local models that may need an extra boost in performance and are limited in their context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was my hypothesis correct?&lt;/head&gt;
    &lt;p&gt;My original hypothesis was: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price.&lt;/p&gt;
    &lt;p&gt;I would argue that&lt;del&gt;—yes!—&lt;/del&gt;no [see edit 2 above], it is correct. If we crunch the numbers, a MacBook with 128 GB is $4700 plus tax. If I spend $100/mo for 5 years, a coding subscription would cost $6000 in that same amount of time. Not only do I save money, but I also get a much more capable machine for anything else I want to do with it.&lt;/p&gt;
    &lt;p&gt;[This paragraph was added in after initial release of this article] It’s important to note that local models will not reach the peak performance of frontier models; however, they will likely be able to do most tasks just as well. The value of using a local model doesn’t come from raw performance, but from supplementing the cost of higher performance models. A local model could very well let you drop your subscription tier for a frontier coding tool or utilize a free tier as needed for better performance and run the rest of your tasks for free.&lt;/p&gt;
    &lt;p&gt;It’s also important to note that local models are only going to get better and smaller. This is the worst your local coding model will perform. I also wouldn’t be surprised if cloud-based AI coding tools get more expensive. If you figure you’re using greater than the $100/mo tier right now or that the $100/mo tier will cost $200/mo in the future, the purchase is a no-brainer. It’s just difficult to stomach the upfront cost.&lt;/p&gt;
    &lt;p&gt;From a performance standpoint, I would say the maximum model running on my 128 GB RAM MacBook right now feels about half a generation behind the frontier coding tools. That’s excellent, but something to keep in mind as that half a generation might matter to you.&lt;/p&gt;
    &lt;p&gt;One wrench thrown into my experiment is how much free quota Google hands out with their different AI coding tools. It’s easy to purchase expensive hardware when it saves you money in the long run. It’s much more difficult when the alternative is free.&lt;/p&gt;
    &lt;p&gt;Initially, I considered my local coding setup to be a great pair to Google’s free tier. It definitely performs better than Gemini 2.5 Flash and makes a great companion to Gemini 3 Pro. Gemini 3 Pro can solve more complex tasks with the local model doing everything else. This not only saves quota on 3 Pro but also provides a very capable fallback for when quota is hit.&lt;/p&gt;
    &lt;p&gt;However, this is foiled a bit now that Gemini 3 Flash was just announced a few days ago. It shows benchmark numbers much more capable than Gemini 2.5 Flash (and even 2.5 Pro!) and I’ve been very impressed with its performance. If that’s the free tier Google offers, it makes local coding models less fiscally reasonable. The jury is still out on how well Gemini 3 Flash will perform and how quota will be structured, but we’ll have to see if local models can keep up.&lt;/p&gt;
    &lt;p&gt;I’m very curious to hear what you think! Tell me about your local coding setup or ask any questions below.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Always be (machine) learning,&lt;/p&gt;
    &lt;p&gt;Logan&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude"/><published>2025-12-21T20:55:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46348847</id><title>Disney Imagineering Debuts Next-Generation Robotic Character, Olaf</title><updated>2025-12-22T02:55:54.872809+00:00</updated><content>&lt;doc fingerprint="ebf30b7df696007b"&gt;
  &lt;main&gt;
    &lt;p&gt;Disneyland Paris saw a groundbreaking moment today, where Bruce Vaughn, President and Chief Creative Officer of Walt Disney Imagineering, and Natacha Rafalski, Présidente of Disneyland Paris, introduced a next-generation robotic character representing Olaf, the beloved snowman from Walt Disney Animation Studios’ Frozen.&lt;/p&gt;
    &lt;p&gt;This debut marks a new chapter in Disney character innovation, one where technology, storytelling, and collaboration come together to bring screen to reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Innovation at the Core: From Screen to Reality&lt;/head&gt;
    &lt;p&gt;From the way he moves to the way he looks, every gesture and detail is crafted to reflect the Olaf audiences have seen in the film — alive, curious, and unmistakably himself. As for his snow-like shimmer that catches the light just like fresh snow, this was enhanced by iridescent fibers. These details make Olaf one of the most expressive and true-to-life characters built, and he’s soon making his debut at Disney parks.&lt;/p&gt;
    &lt;p&gt;Our roots are in animation with Walt Disney pioneering early hand-drawn films and today, Walt Disney Animation Studios and Pixar Animation Studios continue that tradition. We collaborated closely with the film’s original animators at Walt Disney Animation Studios to ensure every gesture felt true to the character. This isn’t just about replicating the animation, it’s about emulating the creators’ intent.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Technology Behind the Magic&lt;/head&gt;
    &lt;p&gt;Home to some of the best storytellers in the world, we’re continuously pushing the boundaries of innovation and technology — in fact it is in our DNA.&lt;/p&gt;
    &lt;p&gt;Like everything at Disney, we always start with the story, and our number one priority is to build storytelling technology that empowers our Disney Imagineers to breathe life into our characters.&lt;/p&gt;
    &lt;p&gt;While the BDX droids — the Star Wars free roaming robotic characters that mimic movements in a simulation — have been interacting with guests for a while now, Olaf presents a far greater challenge: an animated character with non-physical movements. To make Olaf as authentic as possible, the team used a branch of artificial intelligence called reinforcement learning, pushing the limits of hardware to achieve the creative intent of the artists.&lt;/p&gt;
    &lt;p&gt;It takes humans years to master walking and even longer to perform graceful motions. Deep reinforcement learning helps him acquire these skills in a fraction of the time.&lt;/p&gt;
    &lt;p&gt;Olaf’s “snow” also moves differently than the hard shells of other robotic characters, and he can fully articulate his mouth, eyes, and removable carrot nose and arms. Most importantly, Olaf can speak and engage in conversations, creating a truly one-of-a-kind experience.&lt;/p&gt;
    &lt;p&gt;Innovation takes many forms across our parks, experiences, and products – all focused on improving the guest experience and bringing joy to fans around the world. And what’s most exciting is that we’re just getting started!&lt;/p&gt;
    &lt;p&gt;The BDX Droids, self-balancing H.E.R.B.I.E., and now Olaf represent increasing levels of performance and innovation in bringing Disney characters to life. The speed at which we can create new characters and introduce them to guests is unprecedented. We’re scaling bigger than ever, working to bring more emotive, expressive, and surprising characters to our experiences around the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Guests Can See Olaf&lt;/head&gt;
    &lt;p&gt;Olaf will soon venture out into the unknown, eager to see guests at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arendelle Bay Show in World of Frozen, the new immersive world coming soon to Disney Adventure World at Disneyland Paris.&lt;/item&gt;
      &lt;item&gt;Limited-time special appearances at World of Frozen at Hong Kong Disneyland Resort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Looking for a warm hug now? You can discover how Olaf, along with other exciting breakthroughs from Walt Disney Imagineering Research &amp;amp; Development, came to life at in the latest episode of We Call It Imagineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/"/><published>2025-12-21T21:46:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46349808</id><title>I wish people were more public</title><updated>2025-12-22T02:55:54.583579+00:00</updated><content>&lt;doc fingerprint="a04111583aa2b6dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Probably not a popular thing to say today. The zeitgeisty thing to say is that we should all log off and live terrible cottagecore solarpunk lives raising chickens and being mindful. I wish people were more online and more public. I have rarely wished the opposite. Consider this post addressed to you, the reader.&lt;/p&gt;
    &lt;head rend="h1"&gt;Your Writing&lt;/head&gt;
    &lt;p&gt;I will often find a blog post on Hacker News that really resonates. And when I go to check the rest of the site there’s three other posts. And I think: I wish you’d write more! When I find someone whose writing I really connect with, I like to read everything they have written, or at least a tractable subset of their most interesting posts. If I like what I see, I reach out. This is one of the best things about writing online: your future friends will seek you out.&lt;/p&gt;
    &lt;p&gt;And, from the other side, I have often written a post where, just before publishing, I would think: “who would want to read this? It’s too personal, obscure, idiosyncratic, probably a few people will unsubscribe to the RSS feed for this”. And always those are the posts where people email me to say they always thought the same thing but could never quite put it into words. I really value those emails. “I am understood” is a wonderful feeling.&lt;/p&gt;
    &lt;p&gt;I try to apply a rule that if I do something, and don’t write about it—or otherwise generate external-facing evidence of it—it didn’t happen. I have built so many things in the dark, little experiments or software projects or essays that never saw the light of day. I want to put more things out. If it doesn’t merit an entire blog post, then at least a tweet.&lt;/p&gt;
    &lt;head rend="h1"&gt;Your Books&lt;/head&gt;
    &lt;p&gt;If I follow you on Twitter, and you have posted a picture of your bookshelf, I have probably scanned every book in it. This is why I appreciate Goodreads. Like many people I have been reading a lot less over the past ~5y, but since I made a Goodreads account earlier this year, I’ve read tens of books. Reading in public has helped to motivate me.&lt;/p&gt;
    &lt;p&gt;You may say reading in public is performative. I say reading in private is solipsistic. Dante, in De Monarchia, writes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;All men on whom the Higher Nature has stamped the love of truth should especially concern themselves in laboring for posterity, in order that future generations may be enriched by their efforts, as they themselves were made rich by the efforts of generations past. For that man who is imbued with public teachings, but cares not to contribute something to the public good, is far in arrears of his duty, let him be assured; he is, indeed, not “a tree planted by the rivers of water that bringeth forth his fruit in his season,” [Psalms 1:3] but rather a destructive whirlpool, always engulfing, and never giving back what it has devoured.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;My default mode is solipsism. I read in private, build in private, learn in private. And the problem with that is self-doubt and arbitrariness. I’m halfway through a textbook and think: why? Why am I learning geology? Why this topic, and not another? There is never an a priori reason. I take notes, but why tweak the LaTeX if no-one, probably not even future me, will read them? If I stop reading this book, what changes? And doing things in public makes them both more real and (potentially) useful. If you publish your study notes, they might be useful to someone. Maybe they get slurped up in the training set of the next LLM, marginally improving performance.&lt;/p&gt;
    &lt;p&gt;And Goodreads, for all its annoyances, is a uniquely tender social network. Finishing a book, and then seeing a friend mark it as “want to read”, feels like a moment of closeness.&lt;/p&gt;
    &lt;p&gt;I have a friend who lived in Sydney, who has since moved away, and we don’t keep in touch too often, because the timezones are inconvenient, but occasionally she likes my book updates, and I like hers, and I will probably never read that avant-garde novel, but I’m glad she is reading it. It is like saying: “You exist. I exist. I remember. I wish you happiness.”&lt;/p&gt;
    &lt;head rend="h1"&gt;Your Flashcards&lt;/head&gt;
    &lt;p&gt;Lots of people use spaced repetition, but most everyone’s flashcard collections are private. They exist inside a database inside an app like Anki or Mochi. You can export decks, but that’s not a living artifact but a dead snapshot, frozen in time.&lt;/p&gt;
    &lt;p&gt;One reason I built hashcards: by using a Git repo of Markdown files as the flashcard database, you can trivially publish your deck to GitHub. My own flashcard collection is public. I hope that more people use hashcards and put their decks up on GitHub.&lt;/p&gt;
    &lt;p&gt;The point is not that you can clone their repos (which is close to useless: you have to write your own flashcards) but because I’m curious what people are learning. Not the broad strokes, since we all want to learn thermo and econ and quantum chemistry and the military history of the Song dynasty and so on, but the minutiae. Why did you make a flashcard out of this Bible passage? Why does it resonate with you? Why do you care about the interpretation of that strange passage in Antigone? Why did you memorize this poem?&lt;/p&gt;
    &lt;head rend="h1"&gt;Your Dotfiles&lt;/head&gt;
    &lt;p&gt;Computers mediate every aspect of our lives, yet most people use their computers the way they came out of the box. At most they might change the desktop background. Some people don’t even change the default icons on the macOS dock. Even most Linux users just use the stock configuration, e.g. GNOME on Fedora or whatever.&lt;/p&gt;
    &lt;p&gt;I’m interested in people who customize their experience of computing. This is often derided as “ricing”. But agency is interesting. People who remake their environment to suit them are interesting. And I am endlessly curious about how people do this. I like reading people’s &lt;code&gt;init.el&lt;/code&gt;, their custom shell scripts, their
NixOS config. It’s even better if they have some obscure hardware
e.g. some keyboard layout I’ve never heard of and a trackball with
custom gestures. I put my dotfiles up on GitHub because I
imagine someone will find them interesting.&lt;/p&gt;
    &lt;head rend="h1"&gt;etc.&lt;/head&gt;
    &lt;p&gt;And beyond my selfish curiosity there’s also the Fedorovist ancestor simulation angle: if you die and are not cryopreserved, how else are you going to make it to the other side of the intelligence explosion? Every tweet, blog post, Git commit, journal entry, keystroke, mouse click, every one of these things is a tomographic cut of the mind that created it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://borretti.me/article/i-wish-people-were-more-public"/><published>2025-12-21T23:42:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46350075</id><title>ONNX Runtime and CoreML May Silently Convert Your Model to FP16</title><updated>2025-12-22T02:55:54.350660+00:00</updated><content>&lt;doc fingerprint="6d59dd17b5120e18"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ONNX Runtime &amp;amp; CoreML May Silently Convert Your Model to FP16 (And How to Stop It)&lt;/head&gt;
    &lt;p&gt;Running an ONNX model in ONNX RunTime (ORT) with the CoreMLExecutionProvider may change the predictions your model makes implicitly and you may observe differences when running with PyTorch on MPS or ONNX on CPU. This is because the default arguments ORT uses when converting your model to CoreML will cast the model to FP16.&lt;/p&gt;
    &lt;p&gt;The fix is to use the following setup when creating the InferenceSession:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;p&gt;This ensures your model remains in FP32 when running on a Mac GPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncovering an Issue in ONNX Runtime - Benchmarking the EyesOff Model&lt;/head&gt;
    &lt;p&gt;Having trained the EyesOff model, I began evaluating the model and its run time. I was looking into the ONNX format and using it to run the model efficiently. I setup a little test bench in which I ran the model using PyTorch and ONNX with ONNX Runtime (ORT), both using MPS and CPU. While checking the outputs, I noticed that the metrics from the model ran on ONNX on MPS had a different output to those on ONNX CPU and PyTorch CPU and MPS. Note, the metrics from PyTorch on CPU and MPS were the same.&lt;/p&gt;
    &lt;p&gt;When I say ORT and MPS, this is achieved through ORT’s execution providers. To run an ONNX model on the Mac GPU you have to use the CoreMLExecutionProvider (more on this to come).&lt;/p&gt;
    &lt;p&gt;Now in Figure 1 and 2, observe the metric values - the PyTorch ones (Figure 1) are the same across CPU and MPS, this isn’t the same story for ONNX (Figure 2):&lt;/p&gt;
    &lt;p&gt;Wow, look at the diff in Figure 2! When I saw this it was quite concerning, floating point math can lead to differences in the calculations carried out across the GPU and CPU but the values here don’t appear to be a result of floating point math, the values are too large.&lt;/p&gt;
    &lt;p&gt;Given the difference in metrics, I was worried that running the model with ORT was changing the output of the model and hence the behaviour. The reason the metrics change is because some of the model predictions around the threshold flipped to the opposite side of the threshold (which is 0.5), this can be seen in the confusion matrices for the ONNX CPU run and MPS run:&lt;/p&gt;
    &lt;head rend="h4"&gt;FP32 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;207 (TN)&lt;/cell&gt;
        &lt;cell&gt;24 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;69 (FN)&lt;/cell&gt;
        &lt;cell&gt;164 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;FP16 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;206 (TN)&lt;/cell&gt;
        &lt;cell&gt;25 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;68 (FN)&lt;/cell&gt;
        &lt;cell&gt;165 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So two predictions flipped from negative to positive.&lt;/p&gt;
    &lt;p&gt;Having said that, the first thing I did was to make my life easier, by simplifying the scenario from the large EyesOff model to a simple one layer MLP and using that to run the experiments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why am I Using ONNX and ONNX RunTime?&lt;/head&gt;
    &lt;p&gt;Before going on it’s worth discussing what ONNX and ORT are, and why I’m even using them in the first place.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX1&lt;/head&gt;
    &lt;p&gt;ONNX stands for Open Neural Network Exchange. It can be thought of as a common programming language in which to describe ML models. Under the hood ONNX models are represented as graphs, these graphs outline the computation path of a model and it shows the operators and transformations required to get from input to prediction. These graphs are called ONNX graphs.&lt;/p&gt;
    &lt;p&gt;The use of a common language to describe models makes deployment easier and in some cases can add efficiency in terms of resource usage + or inference speed. Firstly, the ONNX graph itself can be optimised. Take PyTorch for example, you train the model in it and sure PyTorch is very mature and extremely optimised but it’s such a large package some things can be overlooked or difficult to change. By converting the model to ONNX, you take advantage of the fact that ONNX was built specifically with inference in mind and with that comes optimisations which the PyTorch team could implement but have not yet.&lt;/p&gt;
    &lt;p&gt;Furthermore, ONNX models can be ran cross platform in specialised runtimes. These runtimes are specially optimised for different architectures and add another layer of efficiency gains.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX RunTime (ORT)2&lt;/head&gt;
    &lt;p&gt;ORT is one of these runtimes. ORT actually runs the model, it can be though of as an interpreter, it takes the ONNX graph and actually implements the operators and runs them on the specified hardware. ORT has a lot of magic built into it, the operators are extremely optimised and through the use of execution providers they target a wide range of hardware. Each execution provider is optimised for the specific hardware it refers to, this enables the ORT team to implement extremely efficient operators giving us another efficiency gain.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML3&lt;/head&gt;
    &lt;p&gt;As mentioned before, I used the CoreMLExecutionProvider to run the model on a Mac GPU. This execution provider informs ORT to make use of CoreML. CoreML is an apple developed framework which lets models (neural networks and classical ML models) run on Apple hardware, CPU, GPU and ANE. ORT’s purpose in this phase is to take the ONNX graph and convert it to a CoreML model. CoreML is Apple’s answer to running efficient on device models on Apple hardware.&lt;/p&gt;
    &lt;p&gt;Note, that all of this doesn’t always mean the model will run faster. Some models may run faster in PyTorch, TensorRT or any other framework. This is why it is important to benchmark and test as many approaches as makes sense.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the Source of the CPU vs MPS Difference - With an MLP&lt;/head&gt;
    &lt;p&gt;The MLP used is very simple it has a single layer, with 4 inputs, 3 outputs and the bias turned off. So, I pretty much created a fancy matrix multiplication.&lt;/p&gt;
    &lt;p&gt;To understand where the issue was coming from I ran this MLP through some different setups:&lt;/p&gt;
    &lt;code&gt;- PyTorch CPU
- PyTorch MPS
- ORT CPU
- ORT MPS
- CoreML FP32
- CoreML FP16&lt;/code&gt;
    &lt;p&gt;The goal of this exercise is to find out if 1 - the difference in outputs is seen in a simple model and 2 - to figure out where exactly the issue arises.&lt;/p&gt;
    &lt;p&gt;Before showing the full results, I want to explain why I included the CoreML FP16 and FP32 runs - specifically why the FP16 one. When I initially ran the MLP experiment I only ran PyTorch, ORT and CoreML FP32 but the output numbers of ORT MPS looked like FP16 numbers. So, I tested if they were and also if the outputs from the other runs were true FP32 numbers. You can do this with a “round trip” test, by converting a number to FP16 and back to FP32. If after this process the number is unchanged then it is an FP16 number but if it changes then it was a true FP32. The number changes as FP16 can represent fewer floating point numbers than FP32. It’s a very simple check to carry out:&lt;/p&gt;
    &lt;code&gt;import numpy as np

= np.array([0.6480752, -0.34015813, 1.4329923], dtype=np.float32)
 onnx_cpu = np.array([0.6484375, -0.34033203, 1.4326172], dtype=np.float32)  # We cast the ort MPS numbers up to FP32, if they were FP16 this has no effect
 onnx_coreml 
= onnx_cpu.astype(np.float16).astype(np.float32)
 cpu_roundtrip = onnx_coreml.astype(np.float16).astype(np.float32)
 coreml_roundtrip 
print("ORT CPU values:")
print("  Original:", onnx_cpu)
print("  fp16 roundtrip:", cpu_roundtrip)
print("  Changed?", not np.allclose(onnx_cpu, cpu_roundtrip, atol=0))

print("\nORT CoreML values:")
print("  Original:", onnx_coreml)
print("  fp16 roundtrip:", coreml_roundtrip)
print("  Changed?", not np.allclose(onnx_coreml, coreml_roundtrip, atol=0))&lt;/code&gt;
    &lt;p&gt;The output of this is:&lt;/p&gt;
    &lt;p&gt;The CPU values change and the MPS values don’t! Now it’s getting interesting - perhaps when using the CoreML execution provider the output is FP16? This prompted adding the CoreML direct run in FP16 precision.&lt;/p&gt;
    &lt;p&gt;I tested this theory with an experiment. Originally, when benchmarking it was all about inference speed, now it’s about floating point precision and figuring out where the diffs come from.&lt;/p&gt;
    &lt;p&gt;Running on PyTorch CPU and MPS gives a strong baseline, PyTorch is a very mature ecosystem and I used the results from that as my ground truth. It being so close together is what drove me to understand what caused ORT runs on different hardware to have a difference. Then using CoreML FP32 and FP16 aimed to show if the issue was an ONNX one or a CoreML one.&lt;/p&gt;
    &lt;p&gt;Check Figure 4 for the outputs and Figure 5 for differences in the outputs here:&lt;/p&gt;
    &lt;p&gt;Wow, would you look at that - once again PyTorch + ORT CPU match and so does PyTorch CPU + CoreML FP32. Also note that CoreML FP16 and ORT MPS match! This is a big insight into what is happening and why the metrics output differed before. Along with the round trip experiment this proves our model is being ran in FP16 when using the CoreML execution provider in ORT!&lt;/p&gt;
    &lt;p&gt;Floating points numbers are defined by three values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sign: 1 bit to define if the number is positive or negative&lt;/item&gt;
      &lt;item&gt;Significand: Contains the numbers digits&lt;/item&gt;
      &lt;item&gt;Exponent: This says where the decimal place should be placed relative to the beginning of the significand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Floating point numbers are often expressed in scientific notation, e.g:&lt;/p&gt;
    &lt;p&gt;FP16 and FP32 specifically, have the following specification:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Total bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Significand bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Exponent bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Smallest number&lt;/cell&gt;
        &lt;cell role="head"&gt;Largest number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Single precision&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;23 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;\(1.2 * 10^{-38}\)&lt;/cell&gt;
        &lt;cell&gt;\(3.4 * 10^{38}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Half precision&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;10 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;\(5.96 * 10^{-8}\)&lt;/cell&gt;
        &lt;cell&gt;\(6.55 * 10^{4}\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So as FP16 is half the size it affords a couple benefits, firstly it requires half the memory to store and secondly it can be quicker to do computations with too. However, this comes at a cost of precision, FP16 cannot represent very small numbers and the distances between small numbers as accurately as FP32.&lt;/p&gt;
    &lt;p&gt;An example of FP16 vs FP32 - The Largest Number Below 1&lt;/p&gt;
    &lt;p&gt;As you see FP32 can represent a value much closer to 1.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Link to the ONNX Issue&lt;/head&gt;
    &lt;p&gt;Having said all that, going back to the issue at hand we observe a ~\(1.17*e^{-7}\) error between PyTorch and CoreML FP32 which is typical of FP32. But, then ORT and CoreML when ran on MPS have a difference of ~\(3.7*e{-4}\) which is much more representative of FP16, this is what prompted the round trip experiment.&lt;/p&gt;
    &lt;p&gt;If you need a quick refresher on FP values, please expand the box above. If you already read that or you know enough about FP already let’s look at why some predictions flip.&lt;/p&gt;
    &lt;p&gt;In my model the base threshold for a 0 or 1 class is 0.5. Both FP16 and FP32 can represent 0.5 exactly:&lt;/p&gt;
    &lt;code&gt;= np.array([0.5], dtype=np.float32)
 fp_32_05 = np.array([0.5], dtype=np.float16)
 fp_16_05 
 fp_32_05.item(), fp_16_05.item()
0.5, 0.5) (&lt;/code&gt;
    &lt;p&gt;But we know that FP representations cannot represent every single number, so there will be some values around 0.5 which cannot be represented and hence will get rounded either up or down. Let’s look into that and find the threshold, this will show why some predictions of the EyesOff model were flipped when changing the model to run in FP16. Also, note by flipped we mean they go from a negative (0) prediction to a positive (1) class prediction, the rounding means it’d have to be below 0.5 and then be rounded up to cross the threshold boundary. Any other scenario would keep the label the same, i.e if it’s above 0.5 and gets rounded to 0.5 that’s fine as the predicted class is still the same.&lt;/p&gt;
    &lt;p&gt;The first step is to find the next representable number below 0.5:&lt;/p&gt;
    &lt;code&gt;# Show the representable values just below 0.5
= np.nextafter(np.float32(0.5), np.float32(0.0))
 fp32_below = np.nextafter(np.float16(0.5), np.float16(0.0))
 fp16_below 
= 0.5 - fp32_below
 fp32_gap = 0.5 - fp16_below
 fp16_gap 
print(f"\nClosest value BELOW 0.5:")
print(f"FP32: {fp32_below:.20f}")
print(f"FP16: {fp16_below:.20f}")

print(f"\nGap from threshold (0.5):")
print(f"FP32: {fp32_gap:.2e}")
print(f"FP16: {fp16_gap:.2e}")&lt;/code&gt;
    &lt;code&gt;Closest value BELOW 0.5:
FP32: 0.49999997019767761230
FP16: 0.49975585937500000000

Gap from threshold (0.5):
FP32: 2.98e-08
FP16: 2.44e-04&lt;/code&gt;
    &lt;p&gt;Taking this gap between 0.5 and the next reprsentable number below 0.5 in FP16 we can calculate the threshold for values which will get rounded up to 0.5:&lt;/p&gt;
    &lt;code&gt;# Given the gap is 2.44e-04, we need to divide it by 2 and calculate the midpoint between 0.499755859375 and 0.5. This midpoint determines whether the FP16 value will be rounded down if below it or up it equal to or greater than. 

# Convert to FP32 as the midpoint is not representable in FP16
= np.float32(fp16_below)
 fp_16_below_fp32 
# Calculate the gap and midpoint
= 0.5 - fp16_below
 fp16_gap = fp_16_below_fp32 + (fp16_gap / 2)
 midpoint 
print(f"  Midpoint (rounding boundary): {midpoint:.15f}")&lt;/code&gt;
    &lt;code&gt;Midpoint: 0.499877929687500&lt;/code&gt;
    &lt;p&gt;Finally let’s see some examples of numbers being rounded up to 0.5 if they are above the midpoint between the representable values of FP16:&lt;/p&gt;
    &lt;code&gt;# Firstly, the midpoint itself is rounded up
0.499877929687500).item() -&amp;gt; 0.5
 np.float16(0.4999).item() -&amp;gt; 0.5
 np.float16(
# For completeness here's a number slightly smaller than the midpoint which gets rounded down
0.4998779296874).item() -&amp;gt; 0.499755859375 np.float16(&lt;/code&gt;
    &lt;p&gt;In short, any number between \([0.4998779296875, 0.5)\) will be rounded up to 0.5. This means, the predictions which were flipped were in this range.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Does the Model Switch to FP16?&lt;/head&gt;
    &lt;p&gt;Now that we know what the issue is, it’s time to find out what caused it.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Bug or Intended Behaviour - It Must be a Bug Right…?&lt;/head&gt;
    &lt;p&gt;Originally I thought this behaviour was a bug with the ORT repo. Knowing the cast occured in the phase where ORT takes the ONNX model and converts it to a CoreML model my initial thinking was either ORT casts it to FP16 somewhere or calls CoreML with a hardcode FP16 flag or something similar.&lt;/p&gt;
    &lt;p&gt;Having little background in cpp, Claude came in useful here. I gave it the structure of the repo and it told me where I ought to place breakpoints to debug the ORT package (turns out you can debug a cpp package from python, clone the repo, build from src and then link it to your python code using the PID). However, upon running the code the breakpoints weren’t being hit. I was puzzled for a bit, but then I realised why the code wasn’t being hit. It turns out CoreML has two model formats “NeuralNetwork” and “MLProgram”, I will call them NN and MLP formats respectively. The behaviour of the ORT repo changes depending on which you want, as does the behaviour of CoreML, with the default being the NN format. So, the breakpoints weren’t hit as the code was regarding the MLP format whereas I was not setting this so the code flowed through the default NN code. Knowing this I took a step back and began experimenting with NN vs MLP format.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fix - NeuralNetwork vs MLProgram CoreML Format&lt;/head&gt;
    &lt;p&gt;So, CoreML has two model formats, these represent how the model is stored and ran with CoreML. The NeuralNetwork (NN) format is older and the MLProgram (MLP) format is newer. ORT specifies NN format by default, but it does allow you to pass a flag to use MLP format.&lt;/p&gt;
    &lt;p&gt;Testing the MLP format revealed it as the solution! See below in figure 6 the final output, which includes both ORT MLP and NN format ran on the GPU.&lt;/p&gt;
    &lt;p&gt;So ORT on MPS with NN format has the same difference from the PyTorch CPU baseline as CoreML FP16, whereas ORT with MLP format matches - this is exactly what I wanted. Mystery solved! By setting the model format to be the newer MLProgram format no implicit cast to FP16 takes place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why MLProgram Format Worked and Neural Network Didnt?&lt;/head&gt;
    &lt;p&gt;To understand the difference in behaviours of these two models formats we need to take a deep dive on the internals of CoreML, its goals and the two formats themselves. Let’s begin with CoreML.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML&lt;/head&gt;
    &lt;p&gt;ORT implements methods to convert the ONNX graph into CoreML model formats. CoreML has two types of model format, this defines how the model is represented in the CoreML framework, how it’s stored and how it’s ran. The older is the NeuralNetwork format and the newer one which solved our issue is the MLProgram format. The reason MLProgram keeps the model at FP32 when running on MPS is due to the differences in model representation in these two formats. Let’s take a look at both of them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Neural Network Format&lt;/head&gt;
    &lt;p&gt;As stated, the NN format is the older one, it came out in 2017. It stored models as a Directed Acyclic Graph (DAG). Each layer in the model is a node in the DAG, and they encode information on layer type, list of input names, output names and a collection of parameters specific to the layer type7. We can observe the model which is created by ORT’s InferenceSession call with the following code:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir
= ort.InferenceSession(
 ort =["CoreMLExecutionProvider"]
     onnx_path, providers
 )= ort.run(None, {"input": numpy_input})[0]
 nn_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# NeuralNetwork models are .mlmodel files
= list(temp_dir.glob("*.mlmodel"))
 nn_files 
for model_path in nn_files:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;This outputs the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-40975D85-7412-4309-A6F7-4E51CA3D2FE8-7682-0000BF11C24C3150.model.mlmodel:
 specificationVersion: 4
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
neuralNetwork {
  layers {
    name: "node_linear"
    input: "input"
    output: "output"
    innerProduct {
      inputChannels: 4
      outputChannels: 3
      weights {
        floatValue: 0.0349225402
        floatValue: -0.301196814
        floatValue: 0.159211695
        floatValue: 0.156890273
        floatValue: -0.267238438
        floatValue: -0.0749385953
        floatValue: -0.292913973
        floatValue: 0.129736364
        floatValue: -0.134683847
        floatValue: 0.351268351
        floatValue: 0.354943156
        floatValue: 0.0509352088
      }
    }
  }
  arrayInputShapeMapping: EXACT_ARRAY_MAPPING
}&lt;/code&gt;
    &lt;p&gt;NeuralNetwork format has typed input and output to the model, but the nodes themselves are not typed. This is why the model gets cast to FP16, in the NN format the default behaviour is to run in FP16 on the MPS GPU. This quirk of the NN format is what threw off my results8. The CoreML runtime also specifies which parts of the model operators can run on which hardware9 and each hardware has different abilities in terms of what FP values it can handle with the NN format. Take a look at Figure 7 for Apple’s guide on the hardware and FP types they can handle:&lt;/p&gt;
    &lt;p&gt;When running on CPU the NN format model will run in FP32, as we observed. However, on GPU it is implicitly cast to FP16 even though the input and output are specified to be FP32 as you see in the inspection code above. This is an inherent limitation of the NN format. The DAG structure of the model does not store any information on the types of intermediate layers. You can see this in the inspection output, the part beginning neuralNetwork stored info on the actual layer node, in our case a single linear layer. Observe that there is no information on the FP precision of the node itself, hence CoreML implicitly sets it to FP16.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Does CoreML Implicitly Use FP16&lt;/head&gt;
    &lt;p&gt;From the typed execution docs for coremltools, the goal of CoreML is to run ML models in the most performant way and FP16 happens to be more performant than FP32 (which makes sense as it’s half the precision) on Apple GPUs. Also, they state that most of the time the reduced precision doesn’t matter for inference - this whole blog post shows why this is false and a pitfall of the NN format, the user should choose which precision the model is ran in, it should never be implicit.&lt;/p&gt;
    &lt;head rend="h5"&gt;MatMul Test - Is FP16 Faster on Apple Hardware?&lt;/head&gt;
    &lt;p&gt;To test Apple’s claim that FP16 is more performant on Apple hardware I carried out a large matmul. Taking a 16384x16384 matrix and multiplying it with another 16384x16384 matrix should show us if FP16 is faster. The size is arbitrary I just wanted something large.&lt;/p&gt;
    &lt;p&gt;The matmul was ran 10 times, in both FP32 and FP16 on the MPS hardware, and we take the average:&lt;/p&gt;
    &lt;code&gt;FP32 Average Time: 8.6521 seconds
FP16 Average Time: 6.7691 seconds

Speedup Factor: 1.28x faster&lt;/code&gt;
    &lt;p&gt;So FP16 is quicker, which sheds a bit of light on why the NN format has implicit casting to FP16, on paper if you only care about speed then it’s the better option.&lt;/p&gt;
    &lt;p&gt;Final point on the NeuralNetwork format, it’s surprising as the weights themselves are stored as FP32 values (a roundtrip test verifies this) but it still executes that layer in FP16, once again showing the NN format doesn’t respect the FP precision of the layer but just casts it to FP16.&lt;/p&gt;
    &lt;p&gt;All that is to say this, this was not a bug but rather an explicit design choice, which funnily enough involves implicitly going against what the user wants. The NN format has its downsides, which is why Apple introduced the MLProgram format, let’s look into that.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MLProgram (MLP) Format&lt;/head&gt;
    &lt;p&gt;The MLP format is the newer and better model format in CoreML, released in 2021, the core thing we care about is that the intermediate tensors are typed, i.e. there is no implicit casting when using the MLP format - the user controls whether the model is ran in FP16 or FP32.&lt;/p&gt;
    &lt;p&gt;MLP format allows for this as it uses a different representation of ML models, instead of a DAG it uses a programmatic representation of the models. By representing the model as code, it allows for greater control over the operations.&lt;/p&gt;
    &lt;p&gt;Let’s see what this looks like in the stored model format and how it differs to the NN format inspection.&lt;/p&gt;
    &lt;p&gt;The code to do so is pretty similar:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir. Also
# this time we specify the ModelFormat to be MLProgram 
= ort.InferenceSession(
 ort_mlp =[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]
     onnx_path, providers
 )= ort_mlp.run(None, {"input": numpy_input})[0]
 mlp_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# MLProgram models are in onnxruntime-* directories (not .mlmodelc)
= [d for d in temp_dir.glob("onnxruntime-*")
 mlp_dirs if d.is_dir() and not str(d).endswith('.mlmodelc')]
             
for model_path in mlp_dirs:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;The output of this is the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-752039B9-BA73-47E3-9ED4-AE029184DA69-9443-0000BF278CD8396E:
 specificationVersion: 8
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
mlProgram {
  version: 1
  functions {
    key: "main"
    value {
      inputs {
        name: "input"
        type {
          tensorType {
            dataType: FLOAT32
            rank: 2
            dimensions {
              constant {
                size: 1
              }
            }
            dimensions {
              constant {
                size: 4
              }
            }
          }
        }
      }
      opset: "CoreML7"
      block_specializations {
        key: "CoreML7"
        value {
          outputs: "output"
          operations {
            type: "const"
            outputs {
              name: "linear_weight"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                  dimensions {
                    constant {
                      size: 4
                    }
                  }
                }
              }
            }
            attributes {
              key: "val"
              value {
                type {
                  tensorType {
                    dataType: FLOAT32
                    rank: 2
                    dimensions {
                      constant {
                        size: 3
                      }
                    }
                    dimensions {
                      constant {
                        size: 4
                      }
                    }
                  }
                }
                blobFileValue {
                  fileName: "@model_path/weights/weight.bin"
                  offset: 64
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "linear_weight"
                    }
                  }
                }
              }
            }
          }
          operations {
            type: "linear"
            inputs {
              key: "x"
              value {
                arguments {
                  name: "input"
                }
              }
            }
            inputs {
              key: "weight"
              value {
                arguments {
                  name: "linear_weight"
                }
              }
            }
            outputs {
              name: "output"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 1
                    }
                  }
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "node_linear__0"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Now you see in the inspection of the MLP format model the linear layer is explicitly typed. To make it a bit easier to see let’s bring back the NeuralNetwork format inspection and compare the linear layer setup in both:&lt;/p&gt;
    &lt;head rend="h4"&gt;Linear Layer in NeuralNetwork &amp;amp; MLProgram Format&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;NeuralNetwork Linear Layer&lt;/cell&gt;
        &lt;cell role="head"&gt;MLProgram Linear Layer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;neuralNetwork { layers { name: "node_linear" input: "input" output: "output" innerProduct { inputChannels: 4 outputChannels: 3 weights { floatValue: 0.0349225402 floatValue: -0.301196814 floatValue: 0.159211695 floatValue: 0.156890273 floatValue: -0.267238438 floatValue: -0.0749385953 floatValue: -0.292913973 floatValue: 0.129736364 floatValue: -0.134683847 floatValue: 0.351268351 floatValue: 0.354943156 floatValue: 0.0509352088 } } }&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;operations { type: "linear" inputs { key: "x" value { arguments { name: "input" } } } inputs { key: "weight" value { arguments { name: "linear_weight" } } } outputs { name: "output" type { tensorType { dataType: FLOAT32 rank: 2 dimensions { constant { size: 1 } } dimensions { constant { size: 3 } } } } }&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Observe in the NN format, there is no explicit mention of the input or output type also the model weights are stored with the layer. Now, in the MLProgram layer, the output is explicitly typed as FP32. No more pesky implicit casting to FP16! This is one the big changes in MLProgram vs NN format, secondly notice how the layer weights are not stored along with the spec, they’re stored elsewhere. This aspect also makes the MLP format more efficient as the actual model spec is lighter.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Why Does MLProgram Have Typed Layers?&lt;/head&gt;
    &lt;p&gt;So we’ve come to the end of the journey, we found that NeuralNetwork format lacks types in the intermediate layers of the model and MLProgram doesn’t. So, setting ORT to use MLProgram keeps the model at FP32 and our output predictions remain the same when running in PyTorch and ORT. But why, why does NeuralNetwork not include types? Answering this requires a look into how ML models have been represented in the past and how this has evolved over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Choices, Design Choices - How Goals of ML Optimisation Evolved Over Time&lt;/head&gt;
    &lt;p&gt;When the NeuralNetwork format was released in 2017, it came into a much different environment than the one MLProgram was born into in 2021. The goal of NeuralNetwork was to act as a configuration file to be ran by hardware, as we saw above it defines the layers and the weights without much other info and lets the hardware figure out the rest. This is indicative of the trends in ML at the time, models were still being optimised so the added complexity wasn’t yet needed, the DAG representation worked well.&lt;/p&gt;
    &lt;p&gt;In essence, the NN format assumes that if the weights are stored in FP32, the input is FP32 and the output is too then the intermediate layers will also be FP32 - but as it doesn’t explicitly type these intermediate layers the hardware is free to choose and the Apple GPU chooses FP16 by default!&lt;/p&gt;
    &lt;p&gt;As time went on the demands in the ML world changed, these hardware based quirks became known, optimisations advanced and overall the industry moved away from the splintered (splintered in the sense that many frameworks implemented their own) config style DAGs and began to utilise learnings from the world of compilers&lt;/p&gt;
    &lt;head rend="h3"&gt;Changes From 2017 to 2021 Which Lead to Greater Adoption of Intermediate Representations&lt;/head&gt;
    &lt;p&gt;Firstly, for Apple specifically the hardware available expanded, now you have the CPU, GPU and ANE chips - making it very difficult to assume any given piece of hardware will run a specific FP type. Also, the lack of typing leads to other issues namely the compiler cannot make some optimisations, as they depend on knowing the types before runtime. Furthermore, things like mixed FP training and quantization became a thing, once again highlighting the need for explicit typing.&lt;/p&gt;
    &lt;p&gt;Lastly, in 2017 DAGs and other forms of model compilers were very fragmented and modern times have seen a push towards standardisation10, as the compiler community consolidated on tools like LLVM the ML community has too. Intermediate Representations(IR) began to be used in ML, an IR is a hardware agnostic specification of a program which a compiler can optimise. CoreML introduced their own IR, called MIL (Model Intermediate Language) and it implements the output we see in the stored MLProgram output.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MIL Approach&lt;/head&gt;
    &lt;p&gt;MIL and IRs in general afford a lot of benefits. They are inherently designed for optimisation and by providing a general framework you can extract maximal value as all optimisation engineers can work on a common goal. In MIL specifically, some of the changes we’ve discussed between NN and MLProgram format, are implemented by it. Namely, each variable within the model has an explicit dtype.&lt;/p&gt;
    &lt;p&gt;Note, the MLProgram serialises and stores the output of the MIL phase, we’ve already observed how it differs to the the NeuralNetwork model, with the biggest difference being in the explicit types.&lt;/p&gt;
    &lt;head rend="h4"&gt;Further Reading on ML Compilers&lt;/head&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;head rend="h3"&gt;The Fix&lt;/head&gt;
    &lt;p&gt;The solution to all the issues we discussed today is, if you are using the CoreMLExecutionProvider in ORT then be sure to specify ModelFormat is MLProgram, this will ensure that whatever precision your model was trained it will be ran with that - which in my case was FP32 (whereas the default ModelFormat NeuralNetwork casts the model to FP16).&lt;/p&gt;
    &lt;p&gt;You can implement this as such:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;head rend="h3"&gt;The Cause&lt;/head&gt;
    &lt;p&gt;The issue was the differing model formats employed by CoreML to represent ML models. The NeuralNetwork format utilised a more historic DAG based approach which was developed during a time in which types and precision wasn’t a huge concern in the ML community and hardware decisions were left to the hardware. Whereas the MLProgram format used a programmatic approach, in which types are explicit letting the software influence how the model is run on the hardware.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lessons?&lt;/head&gt;
    &lt;p&gt;This whole thing taught me the importance of being thorough, it’s not acceptable to test your model in one setup and deploy it in another. We really need to test our model runs across all the platforms we intend to deploy to. Secondly, implicit defaults can be particularly damaging, in my case it wasn’t a huge issue but it easily could have been. Implicit defaults in this case also killed reproducibility, which can be problematic.&lt;/p&gt;
    &lt;p&gt;Lastly, I leave you with this:&lt;/p&gt;
    &lt;p&gt;1https://onnx.ai/onnx/intro/concepts.html&lt;/p&gt;
    &lt;p&gt;3https://developer.apple.com/documentation/coreml&lt;/p&gt;
    &lt;p&gt;4https://en.wikipedia.org/wiki/Single-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;5https://en.wikipedia.org/wiki/Half-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;6https://floating-point-gui.de/formats/fp/&lt;/p&gt;
    &lt;p&gt;7https://apple.github.io/coremltools/mlmodel/Format/NeuralNetwork.html&lt;/p&gt;
    &lt;p&gt;8https://apple.github.io/coremltools/docs-guides/source/typed-execution.html&lt;/p&gt;
    &lt;p&gt;9https://github.com/microsoft/onnxruntime/issues/21271#issuecomment-3637845056&lt;/p&gt;
    &lt;p&gt;10https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ym2132.github.io/ONNX_MLProgram_NN_exploration"/><published>2025-12-22T00:27:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46350142</id><title>Frozen Waymos backed up San Francisco traffic during a widespread power outage</title><updated>2025-12-22T02:55:54.182663+00:00</updated><content>&lt;doc fingerprint="b4b1f4cb96f050c9"&gt;
  &lt;main&gt;
    &lt;p&gt;A power outage struck San Francisco on Saturday that blacked out about 130,000 customers at its peak, according to Pacific Gas and Electric Company, but also caused another problem: stranded Waymo vehicles. Posts all over social media showed the company’s autonomous SUVs sitting still in the streets and causing traffic jams.&lt;/p&gt;
    &lt;head rend="h1"&gt;Frozen Waymos backed up San Francisco traffic during a widespread power outage&lt;/head&gt;
    &lt;p&gt;With street lights out and wireless data limited, the Waymo vehicles were causing backups across the city.&lt;/p&gt;
    &lt;p&gt;With street lights out and wireless data limited, the Waymo vehicles were causing backups across the city.&lt;/p&gt;
    &lt;p&gt;Some people posted videos of Teslas using their FSD feature to navigate the same streets, and Elon Musk tweeted that “Tesla Robotaxis were unaffected by the SF power outage.” On Sunday evening, Waymo spokesperson Suzanne Philion said “We are resuming ride-hailing service in the San Francisco Bay Area.”&lt;/p&gt;
    &lt;p&gt;In response to an inquiry from The Verge, Waymo spokesperson Suzanne Philion sent a statement saying, “We have temporarily suspended our ride-hailing services given the broad power outage in San Francisco. We are focused on keeping our riders safe and ensuring emergency personnel have the clear access they need to do their work.” PG&amp;amp;E reported as of 7AM PT that “Crews have restored about 110,000 customers and PG&amp;amp;E continues to work on restoring the remaining 21,000 customers, primarily in the Presidio, Richmond District, Golden Gate Park and small areas of downtown San Francisco,” as it continued repairs after a fire at a five-story power substation.&lt;/p&gt;
    &lt;p&gt;After this story was published, Philion sent a second statement:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We are resuming ride-hailing service in the San Francisco Bay Area. Yesterday’s power outage was a widespread event that caused gridlock across San Francisco, with non-functioning traffic signals and transit disruptions. While the failure of the utility infrastructure was significant, we are committed to ensuring our technology adjusts to traffic flow during such events.&lt;/p&gt;
      &lt;p&gt;“Throughout the outage, we closely coordinated with San Francisco city officials. We are focused on rapidly integrating the lessons learned from this event, and are committed to earning and maintaining the trust of the communities we serve every day.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Exactly why the cars weren’t moving remains unclear, with no public updates we could find on the company’s social media channels, but speculation centered on spotty wireless data connections, with cell towers either down or overloaded by people who no longer had access to Wi-Fi, and/or the street lights that weren’t operating without power.&lt;/p&gt;
    &lt;p&gt;These problems have occurred before, though, as seen in TikTok videos from earlier this year showing Waymos frozen by a malfunctioning street light and during a power outage in Austin, Texas. In a reply to a Reddit post showing another similar situation last year, someone saying they were a former employee commented explaining that the vehicle would send a request to a remote assistant and wait for their response before proceeding.&lt;/p&gt;
    &lt;p&gt;According to a company blog post, it reaches out to a human response agent when the car encounters “unique interactions,” providing them with live and recorded views from its cameras in addition to a 3D map of what the sensors are picking up. However, those may require bandwidth that’s hard to find during a significant power outage. I couldn’t find any statistics on how many remote assistance operators Waymo has available at a given time, but in November, the company announced it passed a third-party audit by Tüv Süd, a German tech inspection company that evaluated its remote assistance program against industry best practices.&lt;/p&gt;
    &lt;p&gt;Update, December 21st: Added updated statement from Waymo spokesperson Suzanne Philion.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/news/848843/waymo-san-francisco-power-outage"/><published>2025-12-22T00:39:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46350391</id><title>Danish postal service to stop delivering letters after 400 years</title><updated>2025-12-22T02:55:53.913597+00:00</updated><content>&lt;doc fingerprint="266501b51e2291bd"&gt;
  &lt;main&gt;
    &lt;p&gt;The Danish postal service will deliver its last letter on 30 December, ending a more than 400-year-old tradition.&lt;/p&gt;
    &lt;p&gt;Announcing the decision earlier this year to stop delivering letters, PostNord, formed in 2009 in a merger of the Swedish and Danish postal services, said it would cut 1,500 jobs in Denmark and remove 1,500 red postboxes amid the “increasing digitalisation” of Danish society.&lt;/p&gt;
    &lt;p&gt;Describing Denmark as “one of the most digitalised countries in the world”, the company said the demand for letters had “fallen drastically” while online shopping continued to increase, prompting the decision to instead focus on parcels.&lt;/p&gt;
    &lt;p&gt;It took only three hours for 1,000 of the distinctive postboxes, which have already been dismantled, to be bought up when they went on sale earlier this month with a price tag of 2,000 DKK (£235) each for those in good condition and 1,500 DKK (£176) for those a little more well-worn. A further 200 will be auctioned in January. PostNord, which will continue to deliver letters in Sweden, has said it will refund unused Danish stamps for a limited time.&lt;/p&gt;
    &lt;p&gt;Danes will still be able to send letters, using the delivery company Dao, which already delivers letters in Denmark but will expand its services from 1 January from about 30m letters in 2025 to 80m next year. But customers will instead have to go to a Dao shop to post their letters – or pay extra to have it collected from home – and pay for postage either online or via an app.&lt;/p&gt;
    &lt;p&gt;The Danish postal service has been responsible for delivering letters in the country since 1624. In the last 25 years, letter-sending has been in sharp decline in Denmark, with a fall of more than 90%.&lt;/p&gt;
    &lt;p&gt;But evidence suggests a resurgence in letter-writing among younger people could be under way.&lt;/p&gt;
    &lt;p&gt;Dao said its research had found 18- to 34-year-olds send two to three times as many letters as other age groups, citing the trend researcher Mads Arlien-Søborg, who puts the rise down to young people “looking for a counterbalance to digital oversaturation”. Letter-writing, he said, had become a “conscious choice”.&lt;/p&gt;
    &lt;p&gt;According to Danish law, the option to send a letter must exist. This means that if Dao were to stop delivering letters, the government would be obliged to appoint somebody else to do it.&lt;/p&gt;
    &lt;p&gt;A source close to the transport ministry insisted there would not be any “practical difference” in the new year – because people would still be able to send and receive letters, they would simply do so through a different company. Any significance around the change, they said, was purely “sentimental”.&lt;/p&gt;
    &lt;p&gt;But others have said there is an irreversible finality to it. Magnus Restofte, the director of the Enigma postal, the telecommunications and communications museum in Copenhagen, said in the event that it were no longer possible to use digital communications “It’s actually quite difficult to turn back [to physical post]. We can’t go back to what it was. Also, take into consideration we are one of the most digitalised countries in the world.”&lt;/p&gt;
    &lt;p&gt;Under the MitID scheme – Denmark’s national digital ID system, used for everything from online banking to signing documents electronically and booking a doctor’s appointment – all official communications from authorities are automatically sent via “digital post” rather than by mail.&lt;/p&gt;
    &lt;p&gt;While there is the option to opt out and instead receive physical mail, few do. Today, 97% of the Danish population aged 15 and over is enrolled in MitID and only 5% of Danes have opted out of digital post.&lt;/p&gt;
    &lt;p&gt;The Danish public, said Restofte, had been “quite pragmatic” about the change to postal services because very few people received physical letters in their postboxes any more. Some younger people have never sent a physical letter.&lt;/p&gt;
    &lt;p&gt;But the scarcity of physical letters has increased their value. “The funny thing is that actually receiving a physical letter, the value of that is extremely high,” said Restofte. “People know if you write a physical letter and write by hand you have spent time and also spent money.”&lt;/p&gt;
    &lt;p&gt;Announcing their decision earlier this year, Kim Pedersen, the deputy chief executive of PostNord Denmark, said: “We have been the Danish postal service for 400 years, and therefore it is a difficult decision to tie the knot on that part of our history. The Danes have become more and more digital and this means there are very few letters left today, and the decline continues so significantly that the letter market is no longer profitable.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/world/2025/dec/21/denmark-postnord-postal-delivery-letters-society"/><published>2025-12-22T01:25:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46350477</id><title>86Box v5.3</title><updated>2025-12-22T02:55:53.670348+00:00</updated><content>&lt;doc fingerprint="3f931d199536abc5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;86Box v5.3&lt;/head&gt;December 21, 2025 - written by richardg867&lt;p&gt;This is the December 2025 update to 86Box, bringing in performance improvements, bugfixes and some new hardware for the holidays.&lt;/p&gt;&lt;head rend="h2"&gt;Main features&lt;/head&gt;&lt;p&gt;Several small and localized performance improvements have been made to emulation performance, including a new C runtime library for Windows host systems, optimizations to the “new” dynamic recompiler used on ARM and Apple Silicon host systems, as well as multithreading improvements to Voodoo and other video cards. We remain engaged in investigating more potential performance improvements for the next release.&lt;/p&gt;&lt;p&gt;The floppy drive sounds feature added last release got a big update, with improved accuracy especially in head seeks, and new recordings from two 3.5” and three 5.25” floppy drives. The recordings have been moved to a new package outside of the ROM set, so read below for more information before upgrading.&lt;/p&gt;&lt;head rend="h2"&gt;Important changes&lt;/head&gt;&lt;head rend="h3"&gt;Asset pack and floppy sounds&lt;/head&gt;&lt;p&gt;Due to its large size, the floppy drive sound collection has been moved out of the ROM set and into a new asset pack, which is now included with release versions of 86Box downloaded from GitHub, as the &lt;code&gt;assets&lt;/code&gt; folder inside the &lt;code&gt;.zip&lt;/code&gt; on Windows, or embedded within the AppImage on Linux or the app bundle on macOS. Linux packages may or may not include the asset pack; we recommend maintainers to include it in &lt;code&gt;/usr/share/86Box/assets&lt;/code&gt; as part of the standard 86Box package.&lt;/p&gt;&lt;p&gt;If you use our experimental builds or any other package without the asset pack, floppy sounds will not be available until you install the pack the same way you would install the ROM set. Download and decompress it into an &lt;code&gt;assets&lt;/code&gt; folder next to (not inside!) the &lt;code&gt;roms&lt;/code&gt; folder in any of the same places: next to the 86Box application (Windows executable, Linux AppImage or macOS app bundle) or in one of the system-wide locations for your host operating system.&lt;/p&gt;&lt;p&gt;On top of the asset pack change, the Mitsumi and Teac floppy drive recordings from v5.2 have been removed for technical reasons, so you may need to reconfigure floppy sounds after upgrading.&lt;/p&gt;&lt;head rend="h3"&gt;Windows 7 and 8 support&lt;/head&gt;&lt;p&gt;Windows 7 and 8 host systems are still supported for the time being, but on those Windows versions, the Visual C++ 2015 Redistributable must now be installed. You probably already have this installed by other applications or Windows updates, but in case you don’t and 86Box complains about a missing DLL, an installer can be downloaded from Microsoft’s website (get the x64 version).&lt;/p&gt;&lt;head rend="h2"&gt;Changelog&lt;/head&gt;&lt;head rend="h3"&gt;Emulator&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Improved overall performance on Windows hosts by upgrading to the Universal C Runtime&lt;/item&gt;&lt;item&gt;Added asset pack for floppy drive sounds and other future features&lt;/item&gt;&lt;item&gt;Added customizable Ctrl+Alt+Page Down keyboard shortcut to show or hide the user interface in full screen mode&lt;/item&gt;&lt;item&gt;Added toolbar button, Action menu option and customizable Ctrl+Alt+I keyboard shortcut to temporarily disable the dynamic recompiler for troublesome applications&lt;/item&gt;&lt;item&gt;Added relative path conversion to disk images located next to the 86Box application or one level above the machine folder (for portable setups)&lt;/item&gt;&lt;item&gt;Fixed OpenGL renderer crashing the emulator when taking screenshots&lt;/item&gt;&lt;item&gt;Fixed emulated display resolution changes occasionally crashing the emulator&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;User interface&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Added a toolbar with quick commands to the manager&lt;/item&gt;&lt;item&gt;Added icons to some menu entries&lt;/item&gt;&lt;item&gt;Fixed refresh rate indicator displaying inaccurate numbers when a Voodoo add-in card is active&lt;/item&gt;&lt;item&gt;Changed emulation speed indicator to be consistent across CPU frame size options&lt;/item&gt;&lt;item&gt;Updated and cleaned up many translations&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Machines&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;808x &lt;list rend="ul"&gt;&lt;item&gt;Added real time clock I/O port and IRQ configuration to the Multitech PC-500 and PC-500 plus&lt;/item&gt;&lt;item&gt;Fixed Tandy 1000 family display shake effect used by some games&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;286 &lt;list rend="ul"&gt;&lt;item&gt;Fixed extended memory support on C&amp;amp;T PC/AT machines&lt;/item&gt;&lt;item&gt;Fixed Amstrad PC5286 keyboard issues when a PS/2 mouse is emulated&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;386 &lt;list rend="ul"&gt;&lt;item&gt;Added Socket 1 486 CPU support to the IBM PS/55 model 5550-V&lt;/item&gt;&lt;item&gt;Renamed IBM PS/55 model 5550-T to 5550-S/T Stage II&lt;/item&gt;&lt;item&gt;Renamed IBM PS/55 model 5550-V to 5550-V0/V1 and changed category to 386DX/486&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;486 &lt;list rend="ul"&gt;&lt;item&gt;Fixed incorrect on-board video BIOS on the IBM PS/ValuePoint 433DX/Si&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;586 &lt;list rend="ul"&gt;&lt;item&gt;Added Socket 5 machine: HP Pavilion 50x0/70xx&lt;/item&gt;&lt;item&gt;Added Socket 7 machines: ASUS TX97-XV, HP Pavilion 51xx/7070/7090/71xx, 52xx/53xx/71xx/72xx, 73xx/74xx&lt;/item&gt;&lt;item&gt;Added on-board Crystal CS4232 sound to the Intel Advanced/AS, Advanced/ATX and Advanced/MA&lt;/item&gt;&lt;item&gt;Added RM Accelerator 350P2XB/450P3XB BIOS variant option to the AOpen AX6BC&lt;/item&gt;&lt;item&gt;Added Award BIOS option to the MSI MS-5124 and MS-5146&lt;/item&gt;&lt;item&gt;Fixed BIOS settings for serial and parallel ports the Intel Advanced/ATX&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;686 &lt;list rend="ul"&gt;&lt;item&gt;Added Slot 1 machine: MSI MS-6199VA (plus Compaq and Packard Bell BIOS variants)&lt;/item&gt;&lt;item&gt;Added Socket 370 machines: MSI MS-6318 (plus Elonex, Fujitsu-Siemens, HP and Medion BIOS variants), Samsung CAIRO-5&lt;/item&gt;&lt;item&gt;Added more BIOS version options to the ABIT AB-BX6&lt;/item&gt;&lt;item&gt;Added Leadtek WinFast 8000BX BIOS variant option to the Supermicro P6SBA&lt;/item&gt;&lt;item&gt;Changed maximum RAM on the AOpen AP61&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Hardware&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Core &lt;list rend="ul"&gt;&lt;item&gt;Improved CPU performance on ARM hosts, especially on MMX applications&lt;/item&gt;&lt;item&gt;Fixed specific FPU inaccuracy on ARM hosts leading to loss of sound on some Windows games&lt;/item&gt;&lt;item&gt;Fixed INC/DEC instruction legality inaccuracy&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Display &lt;list rend="ul"&gt;&lt;item&gt;Improved emulation performance of Voodoo, ATI Mach64 and S3 video cards&lt;/item&gt;&lt;item&gt;Fixed VideoMagic ETW32PVS (Tseng ET4000/W32p) VLB video card not being selectable on Linux and other case-sensitive systems&lt;/item&gt;&lt;item&gt;Fixed IBM 8514/A ATI MCA variant crashing the emulator on startup on some hosts&lt;/item&gt;&lt;item&gt;Fixed incorrect refresh rates on many cards (again)&lt;/item&gt;&lt;item&gt;Fixed S3 ViRGE rendering glitches on full motion video applications (again)&lt;/item&gt;&lt;item&gt;Fixed IBM XGA rendering glitches on specific color depths&lt;/item&gt;&lt;item&gt;Fixed Matrox and Voodoo texture glitches on 3D applications&lt;/item&gt;&lt;item&gt;Fixed Voodoo Banshee/3 text rendering glitches on Linux&lt;/item&gt;&lt;item&gt;Fixed rendering issues with specific drivers on S3 9xx and Tseng ET4000/W32 cards&lt;/item&gt;&lt;item&gt;Fixed transparency glitches with more games on Voodoo cards&lt;/item&gt;&lt;item&gt;Fixed VGA scrolling behavior not matching real hardware on ATI Mach64 and Tseng ET4000/W32 cards&lt;/item&gt;&lt;item&gt;Fixed incorrect dark gray color on the IBM 5153 CGA monitor&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Input &lt;list rend="ul"&gt;&lt;item&gt;Fixed XT keyboard key count option not taking effect&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Sound &lt;list rend="ul"&gt;&lt;item&gt;Added Crystal CS4232 ISA sound card&lt;/item&gt;&lt;item&gt;Added OPTi 82C930 and 82C931 ISA sound cards&lt;/item&gt;&lt;item&gt;Fixed Sound Blaster 16 and AWE sound issues on some DOS games&lt;/item&gt;&lt;item&gt;Fixed incorrect default I/O port on the Covox Voice Master Key&lt;/item&gt;&lt;item&gt;Removed Raise CODEC interrupt option from Aztech Sound Galaxy cards as it is no longer required&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Ports &lt;list rend="ul"&gt;&lt;item&gt;Fixed emulation hangs with serial passthrough to Windows named pipes in server mode (known issue: VMware serial ports fail to connect if 86Box is the pipe server; use VMware as the server and 86Box as the client instead)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Storage &lt;list rend="ul"&gt;&lt;item&gt;Improved accuracy of floppy drive sounds&lt;/item&gt;&lt;item&gt;Added a new set of floppy drive sounds&lt;/item&gt;&lt;item&gt;Added IDE CD-ROM drive models: HITACHI CDR-8435 (for the RM Accelerator 350P2XB/450P3XB (AOpen AX6BC)), TOSHIBA CD-ROM XM-6102B&lt;/item&gt;&lt;item&gt;Added SCSI CD-ROM drive models: NEC CD-ROM DRIVE:900, PLEXTOR CD-ROM PX-12CS, PX-12TS, PX-83CS, TOSHIBA CD-ROM XM-3701B&lt;/item&gt;&lt;item&gt;Fixed missing DVD support on the HITACHI GD-7500 and HL-DT-ST DVDRAM GSA-4160 CD-ROM drive models&lt;/item&gt;&lt;item&gt;Fixed Panasonic/MKE CD-ROM drives not being detected by OS/2&lt;/item&gt;&lt;item&gt;Fixed floppy drive controller inaccuracy crashing 1B/V3&lt;/item&gt;&lt;item&gt;Removed Mitsumi and Teac floppy drive sounds which could not be updated to the improved system&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Other &lt;list rend="ul"&gt;&lt;item&gt;Added base memory backfill support to the Everex EV-159 ISA memory expansion card&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://86box.net/2025/12/21/86box-v5-3.html"/><published>2025-12-22T01:43:40+00:00</published></entry></feed>