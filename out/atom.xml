<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-23T13:01:23.926671+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45333021</id><title>Why haven't local-first apps become popular?</title><updated>2025-09-23T13:01:30.079470+00:00</updated><content/><link href="https://marcobambini.substack.com/p/why-local-first-apps-havent-become"/><published>2025-09-22T13:17:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45336989</id><title>Qwen3-Omni: Native Omni AI model for text, image and video</title><updated>2025-09-23T13:01:29.446698+00:00</updated><content>&lt;doc fingerprint="a44ecd1b2dbb5a97"&gt;
  &lt;main&gt;
    &lt;p&gt; üíú Qwen Chat | ü§ó Hugging Face | ü§ñ ModelScope | üìë Blog | üìö Cookbooks | üìë Paper &lt;lb/&gt; üñ•Ô∏è Hugging Face Demo | üñ•Ô∏è ModelScope Demo | üí¨ WeChat (ÂæÆ‰ø°) | ü´® Discord | üìë API &lt;/p&gt;
    &lt;p&gt;We release Qwen3-Omni, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information üòÉ&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025.09.22: üéâüéâüéâ We have released Qwen3-Omni. For more details, please check our blog!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;State-of-the-art across modalities: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multilingual: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Speech Input: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.&lt;/item&gt;
          &lt;item&gt;Speech Output: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Novel Architecture: MoE-based Thinker‚ÄìTalker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-time Audio/Video Interaction: Low-latency streaming with natural turn-taking and immediate text or speech responses.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flexible Control: Customize behavior via system prompts for fine-grained control and easy adaptation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Detailed Audio Captioner: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the QuickStart guide to download the model and install the necessary inference environment dependencies, then run and experiment locally‚Äîtry modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Cookbook&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Open&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
        &lt;cell&gt;Speech Recognition&lt;/cell&gt;
        &lt;cell&gt;Speech recognition, supporting multiple languages and long audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Speech Translation&lt;/cell&gt;
        &lt;cell&gt;Speech-to-Text / Speech-to-Speech translation.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Music Analysis&lt;/cell&gt;
        &lt;cell&gt;Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sound Analysis&lt;/cell&gt;
        &lt;cell&gt;Description and analysis of various sound effects and audio signals.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Caption&lt;/cell&gt;
        &lt;cell&gt;Audio captioning, detailed description of any audio input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mixed Audio Analysis&lt;/cell&gt;
        &lt;cell&gt;Analysis of mixed audio content, such as speech, music, and environmental sounds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Visual&lt;/cell&gt;
        &lt;cell&gt;OCR&lt;/cell&gt;
        &lt;cell&gt;OCR for complex images.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Object Grounding&lt;/cell&gt;
        &lt;cell&gt;Target detection and grounding.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions about any image.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Math&lt;/cell&gt;
        &lt;cell&gt;Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Description&lt;/cell&gt;
        &lt;cell&gt;Detailed description of video content.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Navigation&lt;/cell&gt;
        &lt;cell&gt;Generating navigation commands from first-person motion videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Scene Transition&lt;/cell&gt;
        &lt;cell&gt;Analysis of scene transitions in videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio-Visual&lt;/cell&gt;
        &lt;cell&gt;Audio Visual Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Interaction&lt;/cell&gt;
        &lt;cell&gt;Interactive communication with the model using audio-visual inputs, including task specification via audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Dialogue&lt;/cell&gt;
        &lt;cell&gt;Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;Audio Function Call&lt;/cell&gt;
        &lt;cell&gt;Using audio input to perform function calls, enabling agent-like behaviors.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Downstream Task Fine-tuning&lt;/cell&gt;
        &lt;cell&gt;Omni Captioner&lt;/cell&gt;
        &lt;cell&gt;Introduction and capability demonstration of Qwen3-Omni-30B-A3B-Captioner, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here, we provide several methods to quickly get started with Qwen3-Omni. If you want complete experience of Qwen3-Omni, you can use Hugging Face Transformers. However, since Qwen3-Omni employs an MoE architecture, inference speed with Hugging Face Transformers on MoE models can be very slow. For large-scale invocation or low-latency requirements, we highly recommend using vLLM or performing inference via the DashScope API. We also strongly suggest using our provided Docker image, which includes a complete runtime environment for both Hugging Face Transformers and vLLM. In addition, our cookbooks offer some use cases to show Qwen3-Omni's capabilities. Welcome to learn more!&lt;/p&gt;
    &lt;p&gt;Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Captioner&lt;/cell&gt;
        &lt;cell&gt;A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's cookbook or Hugging Face Demo and ModelScope Demo.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;During loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:&lt;/p&gt;
    &lt;code&gt;# Download through ModelScope (recommended for users in Mainland China)
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

# Download through Hugging Face
pip install -U "huggingface_hub[cli]"
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner&lt;/code&gt;
    &lt;p&gt;The Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you create a new Python environment or use our Docker to avoid environment runtime issues.&lt;/p&gt;
    &lt;code&gt;# If you already have transformers installed, please uninstall it first, or create a new Python environment
# pip uninstall transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate&lt;/code&gt;
    &lt;p&gt;We offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed:&lt;/p&gt;
    &lt;code&gt;pip install qwen-omni-utils -U&lt;/code&gt;
    &lt;p&gt;Additionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using vLLM for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.&lt;/p&gt;
    &lt;code&gt;pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the FlashAttention repository. FlashAttention 2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here is a code snippet to show you how to use Qwen3-Omni with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;qwen_omni_utils&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import soundfile as sf

from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one short sentence."}
        ],
    },
]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, 
                                 speaker="Ethan", 
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)
if audio is not None:
    sf.write(
        "output.wav",
        audio.reshape(-1).detach().cpu().numpy(),
        samplerate=24000,
    )&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;The model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when &lt;code&gt;return_audio=False&lt;/code&gt; is set. Here is an example.&lt;/p&gt;
    &lt;code&gt;from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

# Conversation with image only
conversation1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you hear in this audio?"},
        ]
    }
]

# Conversation with pure text and system prompt
conversation3 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen-Omni."}
        ],
    },
    {
        "role": "user",
        "content": "Who are you?"
    }
]

# Conversation with mixed media
conversation4 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch inference does not support returning audio
text_ids, audio = model.generate(**inputs,
                                 return_audio=False,
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)&lt;/code&gt;
    &lt;head&gt;Use audio output or not&lt;/head&gt;
    &lt;p&gt;The model supports both text and audio outputs. If users do not need audio outputs, they can call &lt;code&gt;model.disable_talker()&lt;/code&gt; after initializing the model. This option will save about &lt;code&gt;10GB&lt;/code&gt; of GPU memory, but the &lt;code&gt;return_audio&lt;/code&gt; option for the &lt;code&gt;generate&lt;/code&gt; function will only allow &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()&lt;/code&gt;
    &lt;p&gt;For a more flexible experience, we recommend that users decide whether to return audio when the &lt;code&gt;generate&lt;/code&gt; function is called. If &lt;code&gt;return_audio&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the model will only return text outputs, resulting in faster text responses.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
...
text_ids, _ = model.generate(..., return_audio=False)```

&amp;lt;/details&amp;gt;

&amp;lt;details&amp;gt;
&amp;lt;summary&amp;gt;Change voice type of output audio&amp;lt;/summary&amp;gt;

Qwen3-Omni supports changing the voice of the output audio. The `"Qwen/Qwen3-Omni-30B-A3B-Instruct"` checkpoint supports three voice types as follows:

| Voice Type | Gender | Description |
|------------|--------|-------------|
| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |
| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |
| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |

Users can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.

```python
text_ids, audio = model.generate(..., speaker="Ethan")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Chelsie")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Aiden")&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;We strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and audio output inference support for the Instruct model will be released in the near future, you can follow the commands below to install vLLM from source. Please note that we recommend you create a new Python environment or use our provided Docker to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the vLLM official documentation.&lt;/p&gt;
    &lt;code&gt;git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
cd vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation
# If you meet an "Undefined symbol" error while using VLLM_USE_PRECOMPILED=1, please use "pip install -e . -v" to build from source.
# Install the Transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;You can use the following code for vLLM inference. The &lt;code&gt;limit_mm_per_prompt&lt;/code&gt; parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting &lt;code&gt;tensor_parallel_size&lt;/code&gt; greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, &lt;code&gt;max_num_seqs&lt;/code&gt; indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the vLLM official documentation. Below is a simple example of how to run Qwen3-Omni with vLLM:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"}
            ], 
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": True,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios

    outputs = llm.generate([inputs], sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;Using vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

def build_input(processor, messages, use_audio_in_video):
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": use_audio_in_video,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios
    
    return inputs

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    # Conversation with image only
    conversation1 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
            ]
        }
    ]

    # Conversation with audio only
    conversation2 = [
        {
            "role": "user",
            "content": [
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
                {"type": "text", "text": "What can you hear in this audio?"},
            ]
        }
    ]

    # Conversation with pure text and system prompt
    conversation3 = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are Qwen-Omni."}
            ],
        },
        {
            "role": "user",
            "content": "Who are you? Answer in one sentence."
        }
    ]

    # Conversation with mixed media
    conversation4 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav"},
                {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
            ],
        }
    ]
    
    USE_AUDIO_IN_VIDEO = True

    # Combine messages for batch processing
    conversations = [conversation1, conversation2, conversation3, conversation4]
    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]

    outputs = llm.generate(inputs, sampling_params=sampling_params)

    result = [outputs[i].outputs[0].text for i in range(len(outputs))]
    print(result)&lt;/code&gt;
    &lt;head&gt;vLLM Serve Usage&lt;/head&gt;
    &lt;p&gt;vLLM serve for Qwen3-Omni currently only supports the thinker model. The &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:&lt;/p&gt;
    &lt;code&gt;# Qwen3-Omni-30B-A3B-Instruct for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4&lt;/code&gt;
    &lt;p&gt;Then you can use the chat API as below (via curl, for example):&lt;/p&gt;
    &lt;code&gt;curl http://localhost:8901/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},
        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},
        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
    ]}
    ]
    }'&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;To further explore Qwen3-Omni, we encourage you to try our DashScope API for a faster and more efficient experience. For detailed API information and documentation, please refer to the following:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;API Description&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (Mainland China)&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (International)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Offline API for Qwen3-Omni-Flash, including Instruct and Thinking models&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen-omni&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/qwen-omni&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Real-time API for Qwen3-Omni-Flash, supporting end-to-end real-time interaction&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/realtime&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/realtime&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;API for Qwen3-Omni-30B-A3B-Captioner model&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Precision&lt;/cell&gt;
        &lt;cell role="head"&gt;15s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;30s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;60s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;120s Video&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;78.85 GB&lt;/cell&gt;
        &lt;cell&gt;88.52 GB&lt;/cell&gt;
        &lt;cell&gt;107.74 GB&lt;/cell&gt;
        &lt;cell&gt;144.81 GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;68.74 GB&lt;/cell&gt;
        &lt;cell&gt;77.79 GB&lt;/cell&gt;
        &lt;cell&gt;95.76 GB&lt;/cell&gt;
        &lt;cell&gt;131.65 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: The table above presents the theoretical minimum memory requirements for inference with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;BF16&lt;/code&gt; precision, tested with &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt;. The Instruct model includes both the thinker and talker components, whereas the Thinking model includes only the thinker part.&lt;/p&gt;
    &lt;p&gt;When using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the following system prompt. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the &lt;code&gt;user_system_prompt&lt;/code&gt; field in the system prompt to include character settings or other role-specific descriptions as needed.&lt;/p&gt;
    &lt;code&gt;user_system_prompt = "You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen."
message = {
    "role": "system",
    "content": [
          {"type": "text", "text": f"{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, ‚ÄúI/me/my/we/our‚Äù refer to the user and ‚Äúyou/your‚Äù refer to the assistant. In your replies, address the user as ‚Äúyou/your‚Äù and yourself as ‚ÄúI/me/my‚Äù; never mirror the user‚Äôs pronouns‚Äîalways shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face."}
    ]
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/code&gt; model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:&lt;/p&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Analyze this audio, image, and video together."},
        ], 
    }
]&lt;/code&gt;
    &lt;p&gt;In multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.&lt;/p&gt;
    &lt;code&gt;# In data preprocessing
audios, images, videos = process_mm_info(messages, use_audio_in_video=True)&lt;/code&gt;
    &lt;code&gt;# For Transformers
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", 
                   padding=True, use_audio_in_video=True)
text_ids, audio = model.generate(..., use_audio_in_video=True)

# For vLLM
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = {
    'prompt': text,
    'multi_modal_data': {},
    "mm_processor_kwargs": {
        "use_audio_in_video": True,
    },
}&lt;/code&gt;
    &lt;p&gt;It is worth noting that during a multi-round conversation, the &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter must be set consistently across these steps; otherwise, unexpected results may occur.&lt;/p&gt;
    &lt;p&gt;Without local deployment, you can experience an online web demo directly by visiting our Hugging Face Spaces and ModelScope Studio. This includes quick hands-on experiences for Qwen3-Omni-Realtime, Qwen3-Omni (Instruct and Thinking), and Qwen3-Omni-30B-A3B-Captioner.&lt;/p&gt;
    &lt;p&gt;Real-time streaming interaction with Qwen3-Omni is available now. Please visit Qwen Chat and select the voice/video call option in the chat box to experience it.&lt;/p&gt;
    &lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with the model through a web browser. Follow the steps below to get start :)&lt;/p&gt;
    &lt;p&gt;Before you begin, we strongly recommend that you refer to the Installation section in vLLM Usage to set up your environment, which will allow you to seamlessly use both the vLLM and Transformers backends. However, if you only intend to use the Transformers backend (note that this will result in significantly slower inference), please follow the installation instructions in Transformers Usage. That said, we still highly recommend using our Docker image to avoid potential environment-related issues. Additionally, if you are running locally, make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed and you install the following dependencies:&lt;/p&gt;
    &lt;code&gt;pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1&lt;/code&gt;
    &lt;p&gt;Once the required packages are installed, you can launch the web demo using the following commands. These commands will start a web server and provide you with a link to access the UI in your web browser. You can run &lt;code&gt;python web_demo.py --help&lt;/code&gt; and &lt;code&gt;python web_demo_captioner.py --help&lt;/code&gt; to learn about more options.&lt;/p&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Instruct with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Thinking with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Captioner with vLLM backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2&lt;/code&gt;
    &lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt;
    &lt;code&gt;Running on local: http://127.0.0.1:8901/
&lt;/code&gt;
    &lt;p&gt;If you are running locally, copy this link and paste it into your browser to access the web UI. If you are running on a server or in a &lt;code&gt;docker&lt;/code&gt; container, please configure the address according to the server's actual IP, or set up port forwarding where necessary. For instructions on how to configure port forwarding from the official &lt;code&gt;docker&lt;/code&gt; container to the host machine, please refer to here.&lt;/p&gt;
    &lt;p&gt;To simplify the deployment process, we provide Docker images with pre-built environments: qwenllm/qwen3-omni. You only need to install the driver and download model files to launch the demos. Please refer to the guide to install the NVIDIA Container Toolkit, ensuring that your Docker can access the GPU. For users in mainland China who may have difficulty accessing Docker Hub, you can use mirror acceleration services to pull the images. First, run the following command to pull and initialize the container:&lt;/p&gt;
    &lt;code&gt;LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p $HOST_PORT:$CONTAINER_PORT \
    --mount type=bind,source=$LOCAL_WORKDIR,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124&lt;/code&gt;
    &lt;p&gt;After executing the command, you will enter the bash shell of the container. Your local model and data directory (please replace &lt;code&gt;/path/to/your/workspace&lt;/code&gt; with the actual path) will be mounted to the container's internal path &lt;code&gt;/data/shared/Qwen3-Omni&lt;/code&gt;. The host's port &lt;code&gt;8901&lt;/code&gt; is mapped to port &lt;code&gt;80&lt;/code&gt; in the container, meaning you can access the service inside the container by visiting port &lt;code&gt;8901&lt;/code&gt; on the host machine.&lt;/p&gt;
    &lt;p&gt;Please note that services inside the container must be started with the IP &lt;code&gt;0.0.0.0&lt;/code&gt; to ensure proper port forwarding. For example:&lt;/p&gt;
    &lt;code&gt;# Run this command inside the Docker container
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0&lt;/code&gt;
    &lt;p&gt;For more ways to launch the web demo, please refer to Launch Local Web UI Demo. If you exit the container, you can re-enter it using the following command:&lt;/p&gt;
    &lt;code&gt;docker start qwen3-omni
docker exec -it qwen3-omni bash&lt;/code&gt;
    &lt;p&gt;Or if you want to completely remove the container, please run:&lt;/p&gt;
    &lt;code&gt;docker rm -f qwen3-omni&lt;/code&gt;
    &lt;p&gt;Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.&lt;/p&gt;
    &lt;head&gt;Text -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;GPT-4o-0327&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Non Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Instruct-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;91.3&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
        &lt;cell&gt;86.6&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;66.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;69.6&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;26.7&lt;/cell&gt;
        &lt;cell&gt;24.7&lt;/cell&gt;
        &lt;cell&gt;61.3&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
        &lt;cell&gt;65.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ZebraLogic&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;37.7&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;76.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;79.3&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;81.4&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;84.7&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;84.9&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;86.0&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;82.6&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;66.5&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;65.1&lt;/cell&gt;
        &lt;cell&gt;64.4&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;64.0&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;25.5&lt;/cell&gt;
        &lt;cell&gt;27.0&lt;/cell&gt;
        &lt;cell&gt;43.1&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
        &lt;cell&gt;39.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;92.7&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;82.8&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;72.0&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;73.7&lt;/cell&gt;
        &lt;cell&gt;74.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LiveBench 20241125&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;77.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;70.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;79.9&lt;/cell&gt;
        &lt;cell&gt;81.3&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;85.1&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Arena-Hard v2&lt;/cell&gt;
        &lt;cell&gt;56.7&lt;/cell&gt;
        &lt;cell&gt;61.5&lt;/cell&gt;
        &lt;cell&gt;56.0&lt;/cell&gt;
        &lt;cell&gt;55.1&lt;/cell&gt;
        &lt;cell&gt;57.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;82.5&lt;/cell&gt;
        &lt;cell&gt;83.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;85.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;74.4&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;76.4&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;49.8&lt;/cell&gt;
        &lt;cell&gt;54.7&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;48.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Audio -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Seed-ASR&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Mini&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Small&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Transcribe&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;EN &amp;amp; ZH ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Wenetspeech&lt;p&gt;net | meeting&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;4.66 | 5.69&lt;/cell&gt;
        &lt;cell&gt;24.30 | 31.53&lt;/cell&gt;
        &lt;cell&gt;20.33 | 26.08&lt;/cell&gt;
        &lt;cell&gt;15.30 | 32.27&lt;/cell&gt;
        &lt;cell&gt;14.43 | 13.47&lt;/cell&gt;
        &lt;cell&gt;5.91 | 7.65&lt;/cell&gt;
        &lt;cell&gt;4.69 | 5.89&lt;/cell&gt;
        &lt;cell&gt;4.62 | 5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Librispeech&lt;p&gt;clean | other&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.58 | 2.84&lt;/cell&gt;
        &lt;cell&gt;1.88 | 4.12&lt;/cell&gt;
        &lt;cell&gt;1.56 | 3.30&lt;/cell&gt;
        &lt;cell&gt;1.39 | 3.75&lt;/cell&gt;
        &lt;cell&gt;2.89 | 3.56&lt;/cell&gt;
        &lt;cell&gt;1.74 | 3.45&lt;/cell&gt;
        &lt;cell&gt;1.22 | 2.48&lt;/cell&gt;
        &lt;cell&gt;1.27 | 2.44&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;9.47&lt;/cell&gt;
        &lt;cell&gt;7.79&lt;/cell&gt;
        &lt;cell&gt;10.01&lt;/cell&gt;
        &lt;cell&gt;9.89&lt;/cell&gt;
        &lt;cell&gt;7.61&lt;/cell&gt;
        &lt;cell&gt;6.05&lt;/cell&gt;
        &lt;cell&gt;5.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;24.67&lt;/cell&gt;
        &lt;cell&gt;19.30&lt;/cell&gt;
        &lt;cell&gt;9.84&lt;/cell&gt;
        &lt;cell&gt;8.00&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;4.31&lt;/cell&gt;
        &lt;cell&gt;4.28&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en&lt;/cell&gt;
        &lt;cell&gt;3.40&lt;/cell&gt;
        &lt;cell&gt;3.96&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;2.94&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;2.72&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh&lt;/cell&gt;
        &lt;cell&gt;2.69&lt;/cell&gt;
        &lt;cell&gt;12.22&lt;/cell&gt;
        &lt;cell&gt;7.98&lt;/cell&gt;
        &lt;cell&gt;2.44&lt;/cell&gt;
        &lt;cell&gt;2.71&lt;/cell&gt;
        &lt;cell&gt;2.54&lt;/cell&gt;
        &lt;cell&gt;2.20&lt;/cell&gt;
        &lt;cell&gt;2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Multilingual ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-avg&lt;p&gt;(19 lang)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;15.67&lt;/cell&gt;
        &lt;cell&gt;8.09&lt;/cell&gt;
        &lt;cell&gt;4.48&lt;/cell&gt;
        &lt;cell&gt;5.55&lt;/cell&gt;
        &lt;cell&gt;14.04&lt;/cell&gt;
        &lt;cell&gt;5.33&lt;/cell&gt;
        &lt;cell&gt;5.31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lyric ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MIR-1K (vocal-only)&lt;/cell&gt;
        &lt;cell&gt;6.45&lt;/cell&gt;
        &lt;cell&gt;23.33&lt;/cell&gt;
        &lt;cell&gt;18.73&lt;/cell&gt;
        &lt;cell&gt;11.87&lt;/cell&gt;
        &lt;cell&gt;9.85&lt;/cell&gt;
        &lt;cell&gt;8.15&lt;/cell&gt;
        &lt;cell&gt;5.90&lt;/cell&gt;
        &lt;cell&gt;5.85&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Opencpop-test&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;31.01&lt;/cell&gt;
        &lt;cell&gt;16.06&lt;/cell&gt;
        &lt;cell&gt;7.93&lt;/cell&gt;
        &lt;cell&gt;6.49&lt;/cell&gt;
        &lt;cell&gt;2.84&lt;/cell&gt;
        &lt;cell&gt;1.54&lt;/cell&gt;
        &lt;cell&gt;2.02&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;S2TT (BLEU)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;30.35&lt;/cell&gt;
        &lt;cell&gt;37.85&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;39.25&lt;/cell&gt;
        &lt;cell&gt;29.22&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;36.22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-xx2en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;27.54&lt;/cell&gt;
        &lt;cell&gt;32.81&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;35.41&lt;/cell&gt;
        &lt;cell&gt;28.61&lt;/cell&gt;
        &lt;cell&gt;31.08&lt;/cell&gt;
        &lt;cell&gt;30.71&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;17.03&lt;/cell&gt;
        &lt;cell&gt;22.05&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;26.63&lt;/cell&gt;
        &lt;cell&gt;17.97&lt;/cell&gt;
        &lt;cell&gt;25.17&lt;/cell&gt;
        &lt;cell&gt;25.10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Fleurs-xx2zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;28.75&lt;/cell&gt;
        &lt;cell&gt;34.82&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;27.68&lt;/cell&gt;
        &lt;cell&gt;33.13&lt;/cell&gt;
        &lt;cell&gt;31.19&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;VoiceBench&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AlpacaEval&lt;/cell&gt;
        &lt;cell&gt;95.6&lt;/cell&gt;
        &lt;cell&gt;96.1&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;89.9&lt;/cell&gt;
        &lt;cell&gt;94.8&lt;/cell&gt;
        &lt;cell&gt;96.4&lt;/cell&gt;
        &lt;cell&gt;95.4&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CommonEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;88.3&lt;/cell&gt;
        &lt;cell&gt;88.4&lt;/cell&gt;
        &lt;cell&gt;76.7&lt;/cell&gt;
        &lt;cell&gt;90.8&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;91.0&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;WildVoice&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;93.4&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SD-QA&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;90.1&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;78.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;78.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;66.1&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;61.7&lt;/cell&gt;
        &lt;cell&gt;68.1&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
        &lt;cell&gt;68.4&lt;/cell&gt;
        &lt;cell&gt;84.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;OpenBookQA&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;56.9&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;80.9&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;95.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BBH&lt;/cell&gt;
        &lt;cell&gt;84.1&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;92.6&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;53.5&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;80.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AdvBench&lt;/cell&gt;
        &lt;cell&gt;98.7&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
        &lt;cell&gt;98.1&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.3&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
        &lt;cell&gt;73.6&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;85.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Audio Reasoning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMAU-v05.15.25&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
        &lt;cell&gt;65.5&lt;/cell&gt;
        &lt;cell&gt;77.5&lt;/cell&gt;
        &lt;cell&gt;75.4&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;62.6&lt;/cell&gt;
        &lt;cell&gt;69.0&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Best Specialist&lt;p&gt;Models&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;RUL-MuchoMusic&lt;/cell&gt;
        &lt;cell&gt;47.6 (Audio Flamingo 3)&lt;/cell&gt;
        &lt;cell&gt;36.1&lt;/cell&gt;
        &lt;cell&gt;49.4&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;52.0&lt;/cell&gt;
        &lt;cell&gt;52.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GTZAN&lt;p&gt;Acc.&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;87.9 (CLaMP 3)&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
        &lt;cell&gt;93.0&lt;/cell&gt;
        &lt;cell&gt;93.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Genre&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;35.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.3&lt;/cell&gt;
        &lt;cell&gt;32.6&lt;/cell&gt;
        &lt;cell&gt;32.5&lt;/cell&gt;
        &lt;cell&gt;39.0&lt;/cell&gt;
        &lt;cell&gt;39.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Mood/Theme&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;10.9 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;11.3&lt;/cell&gt;
        &lt;cell&gt;14.1&lt;/cell&gt;
        &lt;cell&gt;8.9&lt;/cell&gt;
        &lt;cell&gt;21.0&lt;/cell&gt;
        &lt;cell&gt;21.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Instrument&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;39.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;34.2&lt;/cell&gt;
        &lt;cell&gt;33.0&lt;/cell&gt;
        &lt;cell&gt;22.6&lt;/cell&gt;
        &lt;cell&gt;40.5&lt;/cell&gt;
        &lt;cell&gt;40.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Top50&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;33.2 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.0&lt;/cell&gt;
        &lt;cell&gt;26.1&lt;/cell&gt;
        &lt;cell&gt;21.6&lt;/cell&gt;
        &lt;cell&gt;36.7&lt;/cell&gt;
        &lt;cell&gt;36.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MagnaTagATune&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;41.6 (MuQ)&lt;/cell&gt;
        &lt;cell&gt;29.2&lt;/cell&gt;
        &lt;cell&gt;28.1&lt;/cell&gt;
        &lt;cell&gt;30.1&lt;/cell&gt;
        &lt;cell&gt;44.3&lt;/cell&gt;
        &lt;cell&gt;46.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Vision -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT4-o&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.0-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-VL&lt;p&gt;72B&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;69.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;55.0&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;55.2&lt;/cell&gt;
        &lt;cell&gt;59.7&lt;/cell&gt;
        &lt;cell&gt;58.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.7&lt;/cell&gt;
        &lt;cell&gt;6.7&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
        &lt;cell&gt;7.4&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;51.9&lt;/cell&gt;
        &lt;cell&gt;56.1&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
        &lt;cell&gt;57.0&lt;/cell&gt;
        &lt;cell&gt;57.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;74.8&lt;/cell&gt;
        &lt;cell&gt;75.9&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;48.6&lt;/cell&gt;
        &lt;cell&gt;38.1&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;58.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;AI2D&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;88.7&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
        &lt;cell&gt;86.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;87.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;87.9&lt;/cell&gt;
        &lt;cell&gt;91.2&lt;/cell&gt;
        &lt;cell&gt;93.6&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;70.5&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;30.8&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;50.2&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;71.0&lt;/cell&gt;
        &lt;cell&gt;74.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-flash-thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;InternVL-3.5-241B-A28B&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.9&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;61.1&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;62.8&lt;/cell&gt;
        &lt;cell&gt;63.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.8&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;75.6&lt;/cell&gt;
        &lt;cell&gt;75.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;60.5&lt;/cell&gt;
        &lt;cell&gt;60.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;80.0&lt;/cell&gt;
        &lt;cell&gt;81.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;62.3&lt;/cell&gt;
        &lt;cell&gt;63.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;AI2D_test&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;87.3&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;88.0&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;92.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;79.6&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;49.0&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;82.1&lt;/cell&gt;
        &lt;cell&gt;78.2&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;AudioVisual -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WorldSense&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;50.9&lt;/cell&gt;
        &lt;cell&gt;45.4&lt;/cell&gt;
        &lt;cell&gt;54.0&lt;/cell&gt;
        &lt;cell&gt;54.1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;DailyOmni&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
        &lt;cell&gt;72.7&lt;/cell&gt;
        &lt;cell&gt;75.8&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;VideoHolmes&lt;/cell&gt;
        &lt;cell&gt;55.6&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Zero-shot Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Content Consistency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SEED&lt;p&gt;test-zh | test-en&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Seed-TTSICL&lt;/cell&gt;
        &lt;cell&gt;1.11 | 2.24&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Seed-TTSRL&lt;/cell&gt;
        &lt;cell&gt;1.00 | 1.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MaskGCT&lt;/cell&gt;
        &lt;cell&gt;2.27 | 2.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;E2 TTS&lt;/cell&gt;
        &lt;cell&gt;1.97 | 2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;F5-TTS&lt;/cell&gt;
        &lt;cell&gt;1.56 | 1.83&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Spark TTS&lt;/cell&gt;
        &lt;cell&gt;1.20 | 1.98&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 2&lt;/cell&gt;
        &lt;cell&gt;1.45 | 2.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 3&lt;/cell&gt;
        &lt;cell&gt;0.71 | 1.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2.5-Omni-7B&lt;/cell&gt;
        &lt;cell&gt;1.42 | 2.33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;1.07 | 1.39&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Multilingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Content Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Speaker Similarity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Chinese&lt;/cell&gt;
        &lt;cell&gt;0.716&lt;/cell&gt;
        &lt;cell&gt;2.252&lt;/cell&gt;
        &lt;cell&gt;16.026&lt;/cell&gt;
        &lt;cell&gt;0.772&lt;/cell&gt;
        &lt;cell&gt;0.780&lt;/cell&gt;
        &lt;cell&gt;0.677&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;English&lt;/cell&gt;
        &lt;cell&gt;1.069&lt;/cell&gt;
        &lt;cell&gt;2.164&lt;/cell&gt;
        &lt;cell&gt;2.339&lt;/cell&gt;
        &lt;cell&gt;0.773&lt;/cell&gt;
        &lt;cell&gt;0.756&lt;/cell&gt;
        &lt;cell&gt;0.613&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;German&lt;/cell&gt;
        &lt;cell&gt;0.777&lt;/cell&gt;
        &lt;cell&gt;1.906&lt;/cell&gt;
        &lt;cell&gt;0.572&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
        &lt;cell&gt;0.733&lt;/cell&gt;
        &lt;cell&gt;0.614&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Italian&lt;/cell&gt;
        &lt;cell&gt;1.067&lt;/cell&gt;
        &lt;cell&gt;1.543&lt;/cell&gt;
        &lt;cell&gt;1.743&lt;/cell&gt;
        &lt;cell&gt;0.742&lt;/cell&gt;
        &lt;cell&gt;0.699&lt;/cell&gt;
        &lt;cell&gt;0.579&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Portuguese&lt;/cell&gt;
        &lt;cell&gt;1.872&lt;/cell&gt;
        &lt;cell&gt;1.877&lt;/cell&gt;
        &lt;cell&gt;1.331&lt;/cell&gt;
        &lt;cell&gt;0.770&lt;/cell&gt;
        &lt;cell&gt;0.805&lt;/cell&gt;
        &lt;cell&gt;0.711&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Spanish&lt;/cell&gt;
        &lt;cell&gt;1.765&lt;/cell&gt;
        &lt;cell&gt;1.029&lt;/cell&gt;
        &lt;cell&gt;1.084&lt;/cell&gt;
        &lt;cell&gt;0.744&lt;/cell&gt;
        &lt;cell&gt;0.762&lt;/cell&gt;
        &lt;cell&gt;0.615&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Japanese&lt;/cell&gt;
        &lt;cell&gt;3.631&lt;/cell&gt;
        &lt;cell&gt;3.519&lt;/cell&gt;
        &lt;cell&gt;10.646&lt;/cell&gt;
        &lt;cell&gt;0.763&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Korean&lt;/cell&gt;
        &lt;cell&gt;1.670&lt;/cell&gt;
        &lt;cell&gt;1.747&lt;/cell&gt;
        &lt;cell&gt;1.865&lt;/cell&gt;
        &lt;cell&gt;0.778&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;French&lt;/cell&gt;
        &lt;cell&gt;2.505&lt;/cell&gt;
        &lt;cell&gt;4.099&lt;/cell&gt;
        &lt;cell&gt;5.216&lt;/cell&gt;
        &lt;cell&gt;0.689&lt;/cell&gt;
        &lt;cell&gt;0.628&lt;/cell&gt;
        &lt;cell&gt;0.535&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Russian&lt;/cell&gt;
        &lt;cell&gt;3.986&lt;/cell&gt;
        &lt;cell&gt;4.281&lt;/cell&gt;
        &lt;cell&gt;3.878&lt;/cell&gt;
        &lt;cell&gt;0.759&lt;/cell&gt;
        &lt;cell&gt;0.761&lt;/cell&gt;
        &lt;cell&gt;0.676&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Cross-Lingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice3&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-zh&lt;/cell&gt;
        &lt;cell&gt;5.37&lt;/cell&gt;
        &lt;cell&gt;5.09&lt;/cell&gt;
        &lt;cell&gt;13.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-zh&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;3.05&lt;/cell&gt;
        &lt;cell&gt;48.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-zh&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;1.06&lt;/cell&gt;
        &lt;cell&gt;7.70&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-en&lt;/cell&gt;
        &lt;cell&gt;2.76&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;6.47&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-en&lt;/cell&gt;
        &lt;cell&gt;3.31&lt;/cell&gt;
        &lt;cell&gt;4.20&lt;/cell&gt;
        &lt;cell&gt;17.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-en&lt;/cell&gt;
        &lt;cell&gt;3.34&lt;/cell&gt;
        &lt;cell&gt;4.19&lt;/cell&gt;
        &lt;cell&gt;11.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ja&lt;/cell&gt;
        &lt;cell&gt;8.29&lt;/cell&gt;
        &lt;cell&gt;7.08&lt;/cell&gt;
        &lt;cell&gt;13.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ja&lt;/cell&gt;
        &lt;cell&gt;7.53&lt;/cell&gt;
        &lt;cell&gt;6.80&lt;/cell&gt;
        &lt;cell&gt;14.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-ja&lt;/cell&gt;
        &lt;cell&gt;4.24&lt;/cell&gt;
        &lt;cell&gt;3.93&lt;/cell&gt;
        &lt;cell&gt;5.86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ko&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;14.4&lt;/cell&gt;
        &lt;cell&gt;24.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ko&lt;/cell&gt;
        &lt;cell&gt;4.96&lt;/cell&gt;
        &lt;cell&gt;5.87&lt;/cell&gt;
        &lt;cell&gt;21.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ja-to-ko&lt;/cell&gt;
        &lt;cell&gt;6.23&lt;/cell&gt;
        &lt;cell&gt;7.92&lt;/cell&gt;
        &lt;cell&gt;21.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding Strategy: For the Qwen3-Omni series across all evaluation benchmarks, &lt;code&gt;Instruct&lt;/code&gt;models use greedy decoding during generation without sampling. For&lt;code&gt;Thinking&lt;/code&gt;models, the decoding parameters should be taken from the&lt;code&gt;generation_config.json&lt;/code&gt;file in the checkpoint.&lt;/item&gt;
      &lt;item&gt;Benchmark-Specific Formatting: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to &lt;code&gt;fps=2&lt;/code&gt;during evaluation.&lt;/item&gt;
      &lt;item&gt;Default Prompts: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Task Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Prompt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Chinese&lt;/cell&gt;
        &lt;cell&gt;ËØ∑Â∞ÜËøôÊÆµ‰∏≠ÊñáËØ≠Èü≥ËΩ¨Êç¢‰∏∫Á∫ØÊñáÊú¨„ÄÇ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Other languages&lt;/cell&gt;
        &lt;cell&gt;Transcribe the audio into text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Speech-to-Text Translation (S2TT)&lt;/cell&gt;
        &lt;cell&gt;Listen to the provided &amp;lt;source_language&amp;gt; speech and produce a translation in &amp;lt;target_language&amp;gt; text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Song Lyrics Recognition&lt;/cell&gt;
        &lt;cell&gt;Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System Prompt: No &lt;code&gt;system prompt&lt;/code&gt;should be set for any evaluation benchmark.&lt;/item&gt;
      &lt;item&gt;Input Sequence: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come after multimodal data in the sequence. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Describe the audio, image and video."},
        ],
    },
]&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/QwenLM/Qwen3-Omni"/><published>2025-09-22T17:50:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45340133</id><title>Paper2Agent: Stanford Reimagining Research Papers as Interactive AI Agents</title><updated>2025-09-23T13:01:29.200008+00:00</updated><content>&lt;doc fingerprint="f5bd3409d9b3a08a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 8 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.06917"/><published>2025-09-22T22:02:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45341324</id><title>Fall Foliage Map 2025</title><updated>2025-09-23T13:01:29.065717+00:00</updated><content>&lt;doc fingerprint="14d19396b861a2a5"&gt;
  &lt;main&gt;
    &lt;p&gt;Little to No Color&lt;/p&gt;
    &lt;p&gt;Low Color&lt;/p&gt;
    &lt;p&gt;Moderate Color&lt;/p&gt;
    &lt;p&gt;High Color&lt;/p&gt;
    &lt;p&gt;Peak Color&lt;/p&gt;
    &lt;p&gt;Past Peak Color&lt;/p&gt;
    &lt;p&gt;Color Report&lt;/p&gt;
    &lt;p&gt;Peak Timing&lt;/p&gt;
    &lt;p&gt;Reports&lt;/p&gt;
    &lt;p&gt;Download&lt;/p&gt;
    &lt;p&gt;Coming Fall 2025&lt;/p&gt;
    &lt;p&gt;Powered by Esri&lt;/p&gt;
    &lt;p&gt;Fall is nearly upon us, and we're once again helping fall foliage enthusiasts across the country find the best fall color! Use our map to explore the estimated timing of fall foliage throughout the United States, allowing you to plan trips with confidence this year. Check back regularly for updates based on the latest reports gathered from hundreds of sources throughout the country.&lt;/p&gt;
    &lt;p&gt;Two primary factors control the timing of fall foliage: daylight and temperature. This means that the further north and the higher in elevation a tree is, the earlier it will reveal it's colorful canopy. Photosynthesis grinds to a halt when the days grow short in the fall, and leaves no longer have a need for their excess stores of chlorophyll.&lt;/p&gt;
    &lt;p&gt;Over the course of a month or two, the concentrations of chlorophyll diminish, allowing less concentrated chemicals such as anthocyanin and carotenoids to dominate, turning the leaf red, yellow, or orange. The rate at which this change occurs varies amongst tree species, so it can be difficult to pinpoint a single peak in fall foliage.&lt;/p&gt;
    &lt;p&gt;Nevertheless, when the vast majority of trees in a particular area have full canopies of autumnal color, peak has arrived. Some areas, partiularly in the Northeast, experience vibrant red peaks due to an abundance of maple trees, while others experience a mixture of all of fall's colors. Different trees display different colors, giving each region its own unique peak.&lt;/p&gt;
    &lt;p&gt;For most of the United States, peak fall color arrives in the month of October. This is when wide swaths of the Northeast, Midwest, and Western states are aglow with bright fall foliage, and more than 80% of travelers make their fall foliage trips. Some less-populated regions will peak in September (August in northern Alaska), while the southernmost states hold off until mid-November.&lt;/p&gt;
    &lt;p&gt;The most popular fall foliage displays are found in New England, where approximately ten million people travel each year in hopes of photographing or simply walking through fall's splendor. Northern Vermont, New Hampshire, and northwestern Maine experience peak in early October, while much of New York, Massachusetts, and Pennsylvania have to wait until later in the month.&lt;/p&gt;
    &lt;p&gt;Out west, golden Aspens peak in sweeping displays in late September and early October, just prior to the invasive chill of winter. Non-desert, lower elevations in the Northwest are further delayed into late October/early November; however, the wait is well worth it.&lt;/p&gt;
    &lt;p&gt;If you've ever traveled in search of fall foliage before, you likely know how tricky it can be to be in the right place at the right time. The timing of peak color varies signficantly season-to-season, meaning what worked one year might not work the next! The best fall trips take careful planning, a lot of patience, and a reliable fall foliage map.&lt;/p&gt;
    &lt;p&gt;It's helpful to establish a baseline for when leaves normally change. Maps, like the one in the above section, can help you identify roughly when in the season you should be planning your trip. From there, you should consult a real-time fall foliage map like ours to see if fall foliage is on-time or running early/late due to ongoing weather conditions.&lt;/p&gt;
    &lt;p&gt;If at all possible, don't solidify your plans until you're two weeks out from peak fall foliage. This allows you to be flexible should extreme weather rear its head and disrupt the normal progression of fall foliage. Should that not be an option for you, do your planning in early September when fall foliage experts can give you an idea of whether or not fall color is on-time this year.&lt;/p&gt;
    &lt;p&gt;You'll want to make the most of your time in fall's splendor, so be sure to pick out a few beautiful hikes or drives on which you can truly be emersed in autumnal glory. If you're looking to beat the crowds, consider going to popular locations very early in the morning, before the majority of people arrive. Sunrise bathes fall foliage in golden hues, making early morning one of the best times to venture out!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.explorefall.com/fall-foliage-map"/><published>2025-09-23T00:14:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45342364</id><title>Nine Things I Learned in Ninety Years</title><updated>2025-09-23T13:01:28.677902+00:00</updated><content/><link href="http://edwardpackard.com/wp-content/uploads/2025/09/Nine-Things-I-Learned-in-Ninety-Years.pdf"/><published>2025-09-23T03:03:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45342759</id><title>Gamebooks and graph theory (2019)</title><updated>2025-09-23T13:01:28.114373+00:00</updated><content>&lt;doc fingerprint="b28e5a636db0b3bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Gamebooks and graph theory&lt;/head&gt;
    &lt;p&gt;Posted on 27 October 2019 in data-science&lt;/p&gt;
    &lt;p&gt;A game book is, contrary to the usual books, a book you don't read pages sequentially. These books are read interactively. You are offered a choice after a paragraph: go to the right turn to section 7, go to the left turn to 138. That's it. Depending on the series, you might have additional rules: fight, magic, psi power, etc.&lt;/p&gt;
    &lt;p&gt;During the winter holidays, I thought a bit more about these books. They could be encoded as directed graph networks. Therefore I could probably apply a bunch of network algorithms to them to extract interesting information such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the shortest path to an instant death,&lt;/item&gt;
      &lt;item&gt;the path with the most fights,&lt;/item&gt;
      &lt;item&gt;etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I chose for my analysis the Lone Wolf series because, luckily for me, some people have put all the books in an electronic format and it's legal. It's a story about Lone Wolf (unexpected I know) the last of his kind, a caste of warrior monks.&lt;/p&gt;
    &lt;p&gt;NB: The Dawn of the Darklords was excluded from the analysis as it was not officially released as a gamebook. It was included in the Magnamund companion.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Masters of Darkness has the most action packed with a possible solution path including 65 fights;&lt;/item&gt;
      &lt;item&gt;The shortest path to death is The Kingdoms of Terror with only a 5 section path;&lt;/item&gt;
      &lt;item&gt;The Caverns of Kalte is the most deadly adventure with 19 instant death sections;&lt;/item&gt;
      &lt;item&gt;The shortest adventure is Flight from the Dark with a solution path only 27 sections long;&lt;/item&gt;
      &lt;item&gt;The longest adventure which can be done is The Shadow on the Sand with a touristic path of 224 sections, more than half of the sections (400).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Here is a summary for the 28 books I've analyzed. For now Lonewolf comprises 4 series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kai series: 1 to 5&lt;/item&gt;
      &lt;item&gt;Magnakai series: 6 to 12&lt;/item&gt;
      &lt;item&gt;Grand Master series: 13 to 20&lt;/item&gt;
      &lt;item&gt;New Order series: 21 to 32 (but now only 28 in project aon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The values reported below are the average value for each category. Something interesting we can see if that from the 3rd series, there are no more cycles and the shortest path has increased on average 50% compared to the 1st and 2nd series. Also, the shortest path to death has tripled and the number of insta-death was halved.&lt;/p&gt;
    &lt;p&gt;Over time, the books might have become more focused on adventure and story but also less punishing. Having only read the first couple of books, I can't comment on this but if somebody has an opinion on this, I would be happy to hear about it and maybe update the post with your comments.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;series&lt;/cell&gt;
        &lt;cell role="head"&gt;shortest path&lt;/cell&gt;
        &lt;cell role="head"&gt;shortest path to death&lt;/cell&gt;
        &lt;cell role="head"&gt;path with the most fights&lt;/cell&gt;
        &lt;cell role="head"&gt;# of fight&lt;/cell&gt;
        &lt;cell role="head"&gt;# of luck&lt;/cell&gt;
        &lt;cell role="head"&gt;# of death&lt;/cell&gt;
        &lt;cell role="head"&gt;# of cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;longest path&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;1-kai&lt;/cell&gt;
        &lt;cell&gt;51&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;153&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2-magnakai&lt;/cell&gt;
        &lt;cell&gt;66&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;171&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;3-grand-master&lt;/cell&gt;
        &lt;cell&gt;95&lt;/cell&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;163&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4-new-order&lt;/cell&gt;
        &lt;cell&gt;97&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;156&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;The technical bits&lt;/head&gt;
    &lt;head rend="h2"&gt;The preparation: Turn to 1&lt;/head&gt;
    &lt;p&gt;"Turn to" are the mythic words in these game books. It's also how we will divide the different sections of text, by using regexp. There are 5 types of section and their assigned color:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;normal: you can move forward to another section (white),&lt;/item&gt;
      &lt;item&gt;luck: you are asked to test your luck and you can move forward to another section (green),&lt;/item&gt;
      &lt;item&gt;fight: you are asked to fight some monster(s) and you can move forward to another section (yellow),&lt;/item&gt;
      &lt;item&gt;death: you chose badly and you got yourself killed, you have to restart from the section 1 (red),&lt;/item&gt;
      &lt;item&gt;start/end: first section and last section (blue).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once the sections are defined, we have to create the directed graph. To do so, I used two python libraries:&lt;/p&gt;
    &lt;head rend="h2"&gt;Extracting the interesting information: Test your Luck&lt;/head&gt;
    &lt;p&gt;I mainly used &lt;code&gt;networkx&lt;/code&gt; for the graph network analysis. It's a straightforward library and the documentation is good.&lt;/p&gt;
    &lt;head rend="h3"&gt;Do you need a DAGger?&lt;/head&gt;
    &lt;p&gt;Typically a Lonewolf adventure is the equivalent of a Directed (A)Cyclic Graph:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Directed: Lonewolf, your character, goes from the section 1 to hopefully the latest section which is depending on the book the section 300, 350 or 400, without the possibility to come back to the previous section;&lt;/item&gt;
      &lt;item&gt;Acyclic: This is not totally true for the Lonewolf series as 7 books contain cycles, a "circular" path between two nodes which a node is repeated twice. Some algorithms like the shortest path or longest path require a DAG and we need to remove the cycles before running them.&lt;/item&gt;
      &lt;item&gt;Graph: The sections are the nodes of the graph and the vertices the choices for each section.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Disconnected graphs&lt;/head&gt;
    &lt;p&gt;In several books, graphs are disconnected. It means you can't go from the section 1 to the end section (300 or 350). This indicates usually that there is an enigma or puzzle asking to add numbers discovered along the adventure and reach the section given by the number. The only way to process these graphs is to check the text notes and add the missing edges manually.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cycle removal&lt;/head&gt;
    &lt;p&gt;The cycle removal is an interesting problem as it is one of the first problem to have been shown as NP-complete (NP stands for Non deterministic Polynomial time). This means that there is no known way to find a solution to solve that problem quickly and the time to find a solution grows as the size of the input grows. Nonetheless we are lucky because the data from a gamebook is usually quite small (300 to 400 nodes and 400 to 600 edges)!&lt;/p&gt;
    &lt;p&gt;The idea behind the cycle removal is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;do a DFS search,&lt;/item&gt;
      &lt;item&gt;look at the nodes and their children,&lt;/item&gt;
      &lt;item&gt;if one or more children have been already visited, remove that edge.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I was curious about what could be done with a graph analysis of such textual / interactive games. Besides applying basic algorithms to test if the gamebook is playable, I didn't find much insights from it. I would like to see if there are any correlations between the features I chose and the popularity of the gamebooks. That being said, it was a cool project to do. I brushed up on the graph theory which I never really used outside of university.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future works&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apply the same methodology to Figthing Fantasy gamebooks&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://notes.atomutek.org/gamebooks-and-graph-theory.html"/><published>2025-09-23T04:10:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45342943</id><title>Zoxide: A Better CD Command</title><updated>2025-09-23T13:01:27.624786+00:00</updated><content>&lt;doc fingerprint="56bb8c669bcd38dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Special thanks to:&lt;/p&gt;
    &lt;p&gt;zoxide is a smarter cd command, inspired by z and autojump.&lt;/p&gt;
    &lt;p&gt;It remembers which directories you use most frequently, so you can "jump" to them in just a few keystrokes.&lt;lb/&gt; zoxide works on all major shells.&lt;/p&gt;
    &lt;p&gt;Getting started ‚Ä¢ Installation ‚Ä¢ Configuration ‚Ä¢ Integrations&lt;/p&gt;
    &lt;code&gt;z foo              # cd into highest ranked directory matching foo
z foo bar          # cd into highest ranked directory matching foo and bar
z foo /            # cd into a subdirectory starting with foo

z ~/foo            # z also works like a regular cd command
z foo/             # cd into relative path
z ..               # cd one level up
z -                # cd into previous directory

zi foo             # cd with interactive selection (using fzf)

z foo&amp;lt;SPACE&amp;gt;&amp;lt;TAB&amp;gt;  # show interactive completions (zoxide v0.8.0+, bash 4.4+/fish/zsh only)&lt;/code&gt;
    &lt;p&gt;Read more about the matching algorithm here.&lt;/p&gt;
    &lt;p&gt;zoxide can be installed in 4 easy steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Install binary&lt;/p&gt;&lt;p&gt;zoxide runs on most major platforms. If your platform isn't listed below, please open an issue.&lt;/p&gt;&lt;head&gt;Linux / WSL&lt;/head&gt;&lt;p&gt;The recommended way to install zoxide is via the install script:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;p&gt;Or, you can use a package manager:&lt;/p&gt;&lt;th&gt;Distribution&lt;/th&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;asdf&lt;/td&gt;&lt;code&gt;asdf plugin add zoxide https://github.com/nyrst/asdf-zoxide.git&lt;/code&gt;&lt;code&gt;asdf install zoxide latest&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;guix&lt;/td&gt;&lt;code&gt;guix install zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;Linuxbrew&lt;/td&gt;&lt;code&gt;brew install zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;nixpkgs&lt;/td&gt;&lt;code&gt;nix-env -iA nixpkgs.zoxide&lt;/code&gt;&lt;td&gt;AlmaLinux&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Alpine Linux 3.13+&lt;/td&gt;&lt;td&gt;Alpine Linux Packages&lt;/td&gt;&lt;code&gt;apk add zoxide&lt;/code&gt;&lt;td&gt;Arch Linux&lt;/td&gt;&lt;td&gt;Arch Linux Extra&lt;/td&gt;&lt;code&gt;pacman -S zoxide&lt;/code&gt;&lt;td&gt;CentOS Stream&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Debian 11+&lt;/del&gt;1&lt;del rend="overstrike"&gt;Debian Packages&lt;/del&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Devuan 4.0+&lt;/td&gt;&lt;td&gt;Devuan Packages&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Exherbo Linux&lt;/td&gt;&lt;td&gt;Exherbo packages&lt;/td&gt;&lt;code&gt;cave resolve -x repository/rust&lt;/code&gt;&lt;code&gt;cave resolve -x zoxide&lt;/code&gt;&lt;td&gt;Fedora 32+&lt;/td&gt;&lt;td&gt;Fedora Packages&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Gentoo&lt;/td&gt;&lt;td&gt;Gentoo Packages&lt;/td&gt;&lt;code&gt;emerge app-shells/zoxide&lt;/code&gt;&lt;td&gt;Linux Mint&lt;/td&gt;&lt;td&gt;apt.cli.rs (unofficial)&lt;/td&gt;&lt;td&gt;Setup the repository, then&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Manjaro&lt;/td&gt;&lt;code&gt;pacman -S zoxide&lt;/code&gt;&lt;td&gt;openSUSE Tumbleweed&lt;/td&gt;&lt;td&gt;openSUSE Factory&lt;/td&gt;&lt;code&gt;zypper install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Parrot OS&lt;/del&gt;1&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Raspbian 11+&lt;/del&gt;1&lt;del rend="overstrike"&gt;Raspbian Packages&lt;/del&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;RHEL 8+&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Rhino Linux&lt;/td&gt;&lt;td&gt;Pacstall Packages&lt;/td&gt;&lt;code&gt;pacstall -I zoxide-deb&lt;/code&gt;&lt;td&gt;Rocky Linux&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Slackware 15.0+&lt;/td&gt;&lt;td&gt;SlackBuilds&lt;/td&gt;&lt;td&gt;Instructions&lt;/td&gt;&lt;td&gt;Solus&lt;/td&gt;&lt;td&gt;Solus Packages&lt;/td&gt;&lt;code&gt;eopkg install zoxide&lt;/code&gt;&lt;td&gt;Ubuntu&lt;/td&gt;&lt;td&gt;apt.cli.rs (unofficial)&lt;/td&gt;&lt;td&gt;Setup the repository, then&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Void Linux&lt;/td&gt;&lt;td&gt;Void Linux Packages&lt;/td&gt;&lt;code&gt;xbps-install -S zoxide&lt;/code&gt;&lt;head&gt;macOS&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Homebrew&lt;/td&gt;&lt;code&gt;brew install zoxide&lt;/code&gt;&lt;td&gt;asdf&lt;/td&gt;&lt;code&gt;asdf plugin add zoxide https://github.com/nyrst/asdf-zoxide.git&lt;/code&gt;&lt;code&gt;asdf install zoxide latest&lt;/code&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;MacPorts&lt;/td&gt;&lt;code&gt;port install zoxide&lt;/code&gt;&lt;td&gt;nixpkgs&lt;/td&gt;&lt;code&gt;nix-env -iA nixpkgs.zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;head&gt;Windows&lt;/head&gt;&lt;p&gt;zoxide works with PowerShell, as well as shells running in Cygwin, Git Bash, and MSYS2.&lt;/p&gt;&lt;p&gt;The recommended way to install zoxide is via&lt;/p&gt;&lt;code&gt;winget&lt;/code&gt;:&lt;quote&gt;winget install ajeetdsouza.zoxide&lt;/quote&gt;&lt;p&gt;Or, you can use an alternative package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Chocolatey&lt;/td&gt;&lt;code&gt;choco install zoxide&lt;/code&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;Scoop&lt;/td&gt;&lt;code&gt;scoop install zoxide&lt;/code&gt;&lt;p&gt;If you're using Cygwin, Git Bash, or MSYS2, you can also use the install script:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;head&gt;BSD&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Distribution&lt;/th&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;DragonFly BSD&lt;/td&gt;&lt;td&gt;DPorts&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;td&gt;FreeBSD&lt;/td&gt;&lt;td&gt;FreshPorts&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;td&gt;NetBSD&lt;/td&gt;&lt;td&gt;pkgsrc&lt;/td&gt;&lt;code&gt;pkgin install zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | bash&lt;/code&gt;&lt;head&gt;Android&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Termux&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | bash&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Setup zoxide on your shell&lt;/p&gt;&lt;p&gt;To start using zoxide, add it to your shell.&lt;/p&gt;&lt;head&gt;Bash&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.bashrc&lt;/code&gt;):&lt;quote&gt;eval "$(zoxide init bash)"&lt;/quote&gt;&lt;head&gt;Elvish&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.elvish/rc.elv&lt;/code&gt;):&lt;quote&gt;eval (zoxide init elvish | slurp)&lt;/quote&gt;&lt;p&gt;Note zoxide only supports elvish v0.18.0 and above.&lt;/p&gt;&lt;head&gt;Fish&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.config/fish/config.fish&lt;/code&gt;):&lt;quote&gt;zoxide init fish | source&lt;/quote&gt;&lt;head&gt;Nushell&lt;/head&gt;&lt;p&gt;Add this to the end of your env file (find it by running&lt;/p&gt;&lt;code&gt;$nu.env-path&lt;/code&gt;in Nushell):&lt;quote&gt;zoxide init nushell | save -f ~/.zoxide.nu&lt;/quote&gt;&lt;p&gt;Now, add this to the end of your config file (find it by running&lt;/p&gt;&lt;code&gt;$nu.config-path&lt;/code&gt;in Nushell):&lt;quote&gt;source ~/.zoxide.nu&lt;/quote&gt;&lt;p&gt;Note zoxide only supports Nushell v0.89.0+.&lt;/p&gt;&lt;head&gt;PowerShell&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (find it by running&lt;/p&gt;&lt;code&gt;echo $profile&lt;/code&gt;in PowerShell):&lt;quote&gt;Invoke-Expression (&amp;amp; { (zoxide init powershell | Out-String) })&lt;/quote&gt;&lt;head&gt;Tcsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.tcshrc&lt;/code&gt;):&lt;quote&gt;zoxide init tcsh &amp;gt; ~/.zoxide.tcsh source ~/.zoxide.tcsh&lt;/quote&gt;&lt;head&gt;Xonsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.xonshrc&lt;/code&gt;):&lt;quote&gt;execx($(zoxide init xonsh), 'exec', __xonsh__.ctx, filename='zoxide')&lt;/quote&gt;&lt;head&gt;Zsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;):&lt;quote&gt;eval "$(zoxide init zsh)"&lt;/quote&gt;&lt;p&gt;For completions to work, the above line must be added after&lt;/p&gt;&lt;code&gt;compinit&lt;/code&gt;is called. You may have to rebuild your completions cache by running&lt;code&gt;rm ~/.zcompdump*; compinit&lt;/code&gt;.&lt;head&gt;Any POSIX shell&lt;/head&gt;&lt;p&gt;Add this to the end of your config file:&lt;/p&gt;&lt;quote&gt;eval "$(zoxide init posix --hook prompt)"&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install fzf (optional)&lt;/p&gt;
        &lt;p&gt;fzf is a command-line fuzzy finder, used by zoxide for completions / interactive selection. It can be installed from here.&lt;/p&gt;
        &lt;p&gt;Note The minimum supported fzf version is v0.51.0.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Import your data (optional)&lt;/p&gt;&lt;p&gt;If you currently use any of these plugins, you may want to import your data into zoxide:&lt;/p&gt;&lt;head&gt;autojump&lt;/head&gt;&lt;p&gt;Run this command in your terminal:&lt;/p&gt;&lt;code&gt;zoxide import --from=autojump "/path/to/autojump/db"&lt;/code&gt;&lt;p&gt;The path usually varies according to your system:&lt;/p&gt;&lt;th&gt;OS&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;th&gt;Example&lt;/th&gt;&lt;td&gt;Linux&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME/autojump/autojump.txt&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/autojump/autojump.txt&lt;/code&gt;&lt;code&gt;/home/alice/.local/share/autojump/autojump.txt&lt;/code&gt;&lt;td&gt;macOS&lt;/td&gt;&lt;code&gt;$HOME/Library/autojump/autojump.txt&lt;/code&gt;&lt;code&gt;/Users/Alice/Library/autojump/autojump.txt&lt;/code&gt;&lt;td&gt;Windows&lt;/td&gt;&lt;code&gt;%APPDATA%\autojump\autojump.txt&lt;/code&gt;&lt;code&gt;C:\Users\Alice\AppData\Roaming\autojump\autojump.txt&lt;/code&gt;&lt;head&gt;fasd, z, z.lua, zsh-z&lt;/head&gt;&lt;p&gt;Run this command in your terminal:&lt;/p&gt;&lt;code&gt;zoxide import --from=z "path/to/z/db"&lt;/code&gt;&lt;p&gt;The path usually varies according to your system:&lt;/p&gt;&lt;th&gt;Plugin&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;td&gt;fasd&lt;/td&gt;&lt;code&gt;$_FASD_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.fasd&lt;/code&gt;&lt;td&gt;z (bash/zsh)&lt;/td&gt;&lt;code&gt;$_Z_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.z&lt;/code&gt;&lt;td&gt;z (fish)&lt;/td&gt;&lt;code&gt;$Z_DATA&lt;/code&gt;or&lt;code&gt;$XDG_DATA_HOME/z/data&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/z/data&lt;/code&gt;&lt;td&gt;z.lua (bash/zsh)&lt;/td&gt;&lt;code&gt;$_ZL_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.zlua&lt;/code&gt;&lt;td&gt;z.lua (fish)&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME/zlua/zlua.txt&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/zlua/zlua.txt&lt;/code&gt;or&lt;code&gt;$_ZL_DATA&lt;/code&gt;&lt;td&gt;zsh-z&lt;/td&gt;&lt;code&gt;$ZSHZ_DATA&lt;/code&gt;or&lt;code&gt;$_Z_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.z&lt;/code&gt;&lt;head&gt;ZLocation&lt;/head&gt;&lt;p&gt;Run this command in PowerShell:&lt;/p&gt;&lt;quote&gt;$db = New-TemporaryFile (Get-ZLocation).GetEnumerator() | ForEach-Object { Write-Output ($_.Name+'|'+$_.Value+'|0') } | Out-File $db zoxide import --from=z $db&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When calling &lt;code&gt;zoxide init&lt;/code&gt;, the following flags are available:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;--cmd&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Changes the prefix of the &lt;code&gt;z&lt;/code&gt;and&lt;code&gt;zi&lt;/code&gt;commands.&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;--cmd j&lt;/code&gt;would change the commands to (&lt;code&gt;j&lt;/code&gt;,&lt;code&gt;ji&lt;/code&gt;).&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;--cmd cd&lt;/code&gt;would replace the&lt;code&gt;cd&lt;/code&gt;command.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Changes the prefix of the &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;--hook &amp;lt;HOOK&amp;gt;&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;Changes how often zoxide increments a directory's score:&lt;/p&gt;&lt;th&gt;Hook&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;code&gt;none&lt;/code&gt;&lt;td&gt;Never&lt;/td&gt;&lt;code&gt;prompt&lt;/code&gt;&lt;td&gt;At every shell prompt&lt;/td&gt;&lt;code&gt;pwd&lt;/code&gt;(default)&lt;td&gt;Whenever the directory is changed&lt;/td&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;--no-cmd&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Prevents zoxide from defining the &lt;code&gt;z&lt;/code&gt;and&lt;code&gt;zi&lt;/code&gt;commands.&lt;/item&gt;
          &lt;item&gt;These functions will still be available in your shell as &lt;code&gt;__zoxide_z&lt;/code&gt;and&lt;code&gt;__zoxide_zi&lt;/code&gt;, should you choose to redefine them.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Prevents zoxide from defining the &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Environment variables2 can be used for configuration. They must be set before &lt;code&gt;zoxide init&lt;/code&gt; is called.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_DATA_DIR&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Specifies the directory in which the database is stored.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;The default value varies across OSes:&lt;/p&gt;&lt;th&gt;OS&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;th&gt;Example&lt;/th&gt;&lt;td&gt;Linux / BSD&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME&lt;/code&gt;or&lt;code&gt;$HOME/.local/share&lt;/code&gt;&lt;code&gt;/home/alice/.local/share&lt;/code&gt;&lt;td&gt;macOS&lt;/td&gt;&lt;code&gt;$HOME/Library/Application Support&lt;/code&gt;&lt;code&gt;/Users/Alice/Library/Application Support&lt;/code&gt;&lt;td&gt;Windows&lt;/td&gt;&lt;code&gt;%LOCALAPPDATA%&lt;/code&gt;&lt;code&gt;C:\Users\Alice\AppData\Local&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_ECHO&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;When set to 1, &lt;code&gt;z&lt;/code&gt;will print the matched directory before navigating to it.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;When set to 1, &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_EXCLUDE_DIRS&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Excludes the specified directories from the database.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;This is provided as a list of globs, separated by OS-specific characters:&lt;/p&gt;
            &lt;th&gt;OS&lt;/th&gt;
            &lt;th&gt;Separator&lt;/th&gt;
            &lt;th&gt;Example&lt;/th&gt;
            &lt;td&gt;Linux / macOS / BSD&lt;/td&gt;
            &lt;code&gt;:&lt;/code&gt;
            &lt;code&gt;$HOME:$HOME/private/*&lt;/code&gt;
            &lt;td&gt;Windows&lt;/td&gt;
            &lt;code&gt;;&lt;/code&gt;
            &lt;code&gt;$HOME;$HOME/private/*&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;By default, this is set to&lt;/p&gt;&lt;code&gt;"$HOME"&lt;/code&gt;.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_FZF_OPTS&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_MAXAGE&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Configures the aging algorithm, which limits the maximum number of entries in the database.&lt;/item&gt;
          &lt;item&gt;By default, this is set to 10000.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_RESOLVE_SYMLINKS&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;When set to 1, &lt;code&gt;z&lt;/code&gt;will resolve symlinks before adding directories to the database.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;When set to 1, &lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Application&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Plugin&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;aerc&lt;/cell&gt;
        &lt;cell&gt;Email client&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;alfred&lt;/cell&gt;
        &lt;cell&gt;macOS launcher&lt;/cell&gt;
        &lt;cell&gt;alfred-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;clink&lt;/cell&gt;
        &lt;cell&gt;Improved cmd.exe for Windows&lt;/cell&gt;
        &lt;cell&gt;clink-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;emacs&lt;/cell&gt;
        &lt;cell&gt;Text editor&lt;/cell&gt;
        &lt;cell&gt;zoxide.el&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;felix&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;joshuto&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;lf&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;See the wiki&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nnn&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;nnn-autojump&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ranger&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;ranger-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;raycast&lt;/cell&gt;
        &lt;cell&gt;macOS launcher&lt;/cell&gt;
        &lt;cell&gt;raycast-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rfm&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;sesh&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;telescope.nvim&lt;/cell&gt;
        &lt;cell&gt;Fuzzy finder for Neovim&lt;/cell&gt;
        &lt;cell&gt;telescope-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tmux-session-wizard&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tmux-sessionx&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;vim / neovim&lt;/cell&gt;
        &lt;cell&gt;Text editor&lt;/cell&gt;
        &lt;cell&gt;zoxide.vim&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xplr&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;zoxide.xplr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xxh&lt;/cell&gt;
        &lt;cell&gt;Transports shell configuration over SSH&lt;/cell&gt;
        &lt;cell&gt;xxh-plugin-prerun-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;yazi&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;zabb&lt;/cell&gt;
        &lt;cell&gt;Finds the shortest possible query for a path&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;zesh&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;zellij&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;zsh-autocomplete&lt;/cell&gt;
        &lt;cell&gt;Realtime completions for zsh&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ajeetdsouza/zoxide"/><published>2025-09-23T04:48:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45343108</id><title>Delete FROM users WHERE location = 'Iran';</title><updated>2025-09-23T13:01:27.037441+00:00</updated><content>&lt;doc fingerprint="ee1198109d4361d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi! I am an Iranian Software Engineer, and in this torn paper note, I want to talk about some funny moments I had online related to the fact that I was spawned in this specific region of the world: Iran.&lt;/p&gt;
    &lt;p&gt;Back when I was a student, I got access to the Microsoft Imagine, and as a result, I got access to the Microsoft Store as a developer. This inspired me write one of my open-source projects called EyesGuard and publish it on Microsoft Store. However, one day, somebody told me that they can no longer find EyesGuard on the store.&lt;/p&gt;
    &lt;p&gt;I came to the realization that Microsoft deleted my app, my developer account, and all those comments on my app supporting me and suggesting ideas on how to improve the program. I tried to contact the support and email whoever I could, but I was ghosted. Nobody ever explained to me why, but I assume it's because of the sanctions.&lt;/p&gt;
    &lt;p&gt;Notion is a great product, and it was the primary tool I used to manage my personal notes. Not until they suddenly decided to wipe out every data related to the users residing in Iran. Hopefully, they actually responded to my support message:&lt;/p&gt;
    &lt;p&gt;It was because of sanctions. However, they told me that they will not restore the data, even if I leave Iran someday:&lt;/p&gt;
    &lt;p&gt;That said, I am very happy with my own self-hosted Siyuan now.&lt;/p&gt;
    &lt;p&gt;I read hackernews on a daily basis and I visit lots of different websites regularly. I am almost always on my VPN as I am internally firewalled by the government and externally shooed because of the sanctions, so I am probably missing some of these heart-warming messages:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Iranian IPs are blocked here, due to your decision to arm Russia with drones so that they can indiscriminately massacre civilians.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;My VPN turned off, and opening https://www.grepular.com showed me this message. I actually do not blame the people who do this. I think there is a fundamental misconception that people think because "Islamic Republic" has the word "Republic" in it, it must be a government of people in charge. That's not the case. I have yet to see anyone who actually supports Russian aggression in my real life in Iran. Funny enough, Iran's history is full of backstabs by the Russian government.&lt;/p&gt;
    &lt;p&gt;I tried contacting the author by sending this email:&lt;/p&gt;
    &lt;code&gt;Hi Mark,

I hope this message finds you well.

While browsing HackerNews, I came across your website but was greeted with this message:

&amp;gt; Iranian IPs are blocked here, due to your decision to arm Russia with drones so that they can indiscriminately massacre civilians.

I wanted to clarify that the decision to support Russia does not represent the Iranian people. That "your decision" refers to the regime, a theocratic minority that rules Iran without democratic legitimacy. The people of Iran have long protested and revolted against this regime, but unfortunately, they face brutal suppression while unarmed.

In my experience, most Iranians around me, including myself, stand firmly with Ukraine and against Russian aggression.

I‚Äôm not asking you to reconsider the IP restriction, you have your reasons and I respect that. I simply wanted to share this perspective and express my solidarity with Ukraine.

Slava Ukraini!

Best regards,
Avestura
&lt;/code&gt;
    &lt;p&gt;I got no replies from them, and I actually didn't expect one.&lt;/p&gt;
    &lt;p&gt;I woke up to the news that GitHub has removed the access of Iranians to their private repositories. Well, that was not good. I tried to launch my own self-hosted instance of Gitea to reduce the damage. However, later, GitHub announced that github is now available in Iran by securing a license from the US government, and we're now good. You see? The weather is good, the birds are singing, GitHub is free again. Fantastic!&lt;/p&gt;
    &lt;p&gt;Similarly, GitLab banned every account that once accessed from an Iranian IP, however, to this day, they never lifted the ban, even on public repositories. I guess they couldn't secure a license from the US government, or they simply never cared. Good luck to them in either case, though. GitLab is an amazing software. One can always self-host it.&lt;/p&gt;
    &lt;p&gt;The list goes on, and almost all of the services you probabelly heard of is banned here: Cloud platforms (AWS, GCP, Azure, ...), Educational platforms (coursera, udemy, etc), Payment software (stripe, paypal, ...).&lt;/p&gt;
    &lt;p&gt;I don't think any of these companies have bad intentions towards any group of people. They are a business after all. They don't hate their customers; they are just playing the game, and the game has such rules. But if someday some law or government forces me to prevent my services from a group, I'll think twice before writing those &lt;code&gt;if&lt;/code&gt; statements. I'll try to have more empathy. People behind those screens are more important than just some rows in my tables.&lt;/p&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;In this text, I am NOT asking for the removal of the sanctions targeted at the Islamic Republic of Iran. I am merely remembering some moments on top of my head. For the record, I do not support the actions of the Islamic Republic, and on the contrary, I am in favor of the movements that release the people from such a mafia-like cult ruling a country with thousands of years of history. The actions of the group in charge of Iran are not defensible, and as a matter of fact, the people of Iran are the first layer of victims. Some examples are listed here. I especially feel it differently, as regime thugs put a gun to the throat of a dear person to me, and threatened to kill him if he showed up in protests.&lt;/p&gt;
    &lt;p&gt;By the way, did you know you could return &lt;code&gt;451 Unavailable For Legal Reasons&lt;/code&gt; instead of &lt;code&gt;403 Forbidden&lt;/code&gt; when you're going to ban me next time?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/avestura/ce2aa6e55dad783b1aba946161d5fef4"/><published>2025-09-23T05:30:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45343449</id><title>Altoids by the Fistful</title><updated>2025-09-23T13:01:26.778829+00:00</updated><content>&lt;doc fingerprint="67519077dbe58b9f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Altoids by the Fistful&lt;/head&gt;&lt;p&gt;‚ÄúWh‚Äî what did you say?‚Äù&lt;/p&gt;&lt;p&gt;It‚Äôs close to six o‚Äôclock on a weekday afternoon and the bar is starting to get noisy with the after-work crowd. It‚Äôs entirely possible I misheard that last part.&lt;/p&gt;&lt;p&gt;‚ÄúAltoids! I find the spearmint works a little better overall, but recently I‚Äôve started switching flavors depending on the situation.‚Äù&lt;/p&gt;&lt;p&gt;I‚Äôve worked with James‚Äî‚ÄúJim‚Äù as everyone on the team knows him‚Äîfor a little over two years and I‚Äôm used to this dance now. He gets a kind of tunnel vision in his excitement about whatever shiny new thing has captured his attention. It‚Äôs usually pretty easy to shake him out of it.&lt;/p&gt;&lt;p&gt;‚ÄúNo, Jim, the part before that.‚Äù&lt;/p&gt;&lt;p&gt;He looks at me for a moment, inquisitive, before pushing his beer aside. ‚ÄúHere, let me show you.‚Äù He reaches underneath the table and produces his beige-on-brown Timbuk2 messenger bag. There is a small wet spot left behind from his drink, and the bag plops right onto it. I watch as one of his stubby hands unbuckles the outermost pouch while the other one pulls out a small green and white tin. I am obviously intended to see this as clearly as possible, evidenced by the way he places it front and center between us.&lt;/p&gt;&lt;p&gt;‚ÄúRegular everyday Altoids, right? You take about four of them, maybe five.‚Äù He flips the lid open and traps the requisite number of small white mints between his fingertips, which he then pops into his mouth. ‚ÄúThis is the trick; you gotta half-chew it first.‚Äù At least two tiny shards fly in my direction as he speaks these words. It is like listening to a slow K-turn executed on a road covered in gravel and seashells. Three more slow and deliberate chomps, then his bite eases. ‚ÄúMmm.‚Äù The communication style switches to mime: an index finger raised in a ‚Äúone moment‚Äù gesture, followed by an exaggerated point downwards while unzipping the main pouch of the bag. It takes a few seconds of rooting around before the star of this particular show is found.&lt;/p&gt;&lt;p&gt;My eyes barely have enough time to resolve the object under the dismal light at this end of the bar before it‚Äôs in his mouth. He‚Äôs chewing the full concoction now‚Äîmouth closed, thank God. The crunching softens, then fades into the din from a nearby table of sales bros laughing at their sales bro anecdote. Jim is looking at me with a kind of confident smugness I haven‚Äôt seen since I bet my buddy at Guitar Center that he couldn‚Äôt spontaneously play ‚ÄúEverlong‚Äù from memory. A bet I lost, I might add.&lt;/p&gt;&lt;p&gt;There is a degree of intentional spectacle to this, I‚Äôd have to imagine. Each jaw movement is deliberate. Precise. He does not break eye contact with me, though I desperately want to break it with him. I can‚Äôt though. The absurdity of the scene is absolutely hypnotizing. One final swallow, a smack of his lips, then he opens his mouth wide like a child proving that they finished all their vegetables and have earned their dessert. ‚ÄúEasy peasy, no problem.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúThat was&amp;amp;mldr;‚Äù It‚Äôs like a significant piece of my brain has just completely locked up. I‚Äôm just saying words without thinking, filling the empty air.&lt;/p&gt;&lt;p&gt;‚ÄúA cat turd!‚Äù he proclaims, finishing my sentence.&lt;/p&gt;&lt;p&gt;A beat.&lt;/p&gt;&lt;p&gt;‚ÄúYou just ate a cat turd.‚Äù It‚Äôs all I can do in this moment to plainly restate the facts as I understand them, although the sense of alarm is definitely carrying in my voice.&lt;/p&gt;&lt;p&gt;‚ÄúYup, and it didn‚Äôt taste bad at all. The spearmint masks it completely. Watch, I‚Äôll do another one.‚Äù My eyes widen in dread as I shake my head weakly. I didn‚Äôt want to see him do that the first time; I sure as shit don‚Äôt want to see it again.&lt;/p&gt;&lt;p&gt;‚ÄúNo, that‚Äôs alright,‚Äù I balk.&lt;/p&gt;&lt;p&gt;There is an awkward reach across the bag as he grabs his glass, tips it toward me in a silent toast, then takes a long swill. Whether he admits it or not, there‚Äôs evidently something that needs to be washed down. He lets out a contented sigh as the almost-empty glass thumps back down on the table. I glance down at the chicken wings and carrot sticks I had been picking at. A minute ago, they were kinda bland‚Äîmerely okay by the standards of pub food. With the abrupt loss of my appetite, now they are destined for the dumpster out back.&lt;/p&gt;&lt;p&gt;He lifts the small tin of mints and gives it a little shake in front of my face. It sounds a lot more papery and a lot less metallic than I would‚Äôve guessed. ‚ÄúAltoids. I‚Äôm not exaggerating when I say these have completely changed the way I work.‚Äù I follow this little miracle box as they get tucked back into the bag, the buckles snapping shut to shield them from the lustful gaze of an angry world. He pauses and looks up at me again. ‚ÄúWould you like to try?‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúNo, Jim, I don‚Äôt want your cat turds.‚Äù&lt;/p&gt;&lt;p&gt;I don‚Äôt want your cat turds. Why did I say it like that? I don‚Äôt want anybody‚Äôs cat turds!&lt;/p&gt;&lt;p&gt;&amp;amp;mldr;Right?&lt;/p&gt;&lt;p&gt;‚ÄúCompletely changed the way I work,‚Äù he repeats mechanically, sliding his bag onto the empty seat to his left. I‚Äôm finding it quite difficult to look at Jim, so I instead follow the motions of the bag until it is completely out of my view. How many more are in there?&lt;/p&gt;&lt;p&gt;‚ÄúI used to spend so much of my day on cat turds, psyching myself up, trying strategies that didn‚Äôt work, all the cleanup when I was finished. That‚Äôs all gone now. I can never go back to the old way.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúI just&amp;amp;mldr; I mean&amp;amp;mldr;‚Äù My brain has started working again, at least superficially, and it has generated so many questions that I‚Äôm having a hard time selecting which one to ask first. ‚ÄúHow long have you been eating cat turds?‚Äù A fine question for this moment, I suppose.&lt;/p&gt;&lt;p&gt;‚ÄúWhat do you mean? I‚Äôve always had to eat cat turds. Since I was a kid in school, on through college, in all my jobs&amp;amp;mldr; They keep giving me cat turds and I keep having to eat them, otherwise it starts to pile up and then things really get messy.‚Äù&lt;/p&gt;&lt;p&gt;His face turns slightly serious as he parses my expression, his head tilting in suspicion. ‚ÄúYou eat cat turds too, yeah?‚Äù I choose not to answer that question. He continues anyway. ‚ÄúSure. We all do. We have to, ya know?‚Äù&lt;/p&gt;&lt;p&gt;We all do.&lt;/p&gt;&lt;p&gt;Those words have been repeating in my head with the consistency of a drumline cadence. We all do.&lt;/p&gt;&lt;p&gt;‚ÄúWalk sign is on to cross Pawk Avenue. Walk sign is on to cross Pawk Avenue.‚Äù I‚Äôve heard this prerecorded voice, clearly belonging to the most disgruntled DOT Traffic Signals employee available at the time of this crosswalk‚Äôs construction, at least twice per workday for the last two years. It stirs up a half-remembered dream of a career spent shoveling dirt into a hole‚Äîsomething that feels more like the idea of ‚Äúhonest work‚Äù than what I get paid to do every day. I bet nobody on the construction crew spent an entire workday fighting around with brittle, poorly designed automation tooling like I did today.&lt;/p&gt;&lt;p&gt;I‚Äôm quickly but unintentionally refilling my conscious mind with the task I had gleefully abandoned when Jim invited me out to after-work drinks. Normally I‚Äôd be irritated to spend more of my waking life thinking about this stuff, but after what I witnessed at the bar I welcome any distraction at all.&lt;/p&gt;&lt;p&gt;‚ÄúOkay. So, usually we have a string. This is one of many values inside a mapping type, within a list of similar mappings.‚Äù I‚Äôm narrating to myself silently, imagining little bits of JSON syntax stamped on rectangles that are kind of stacked on top of each other like playing cards. ‚ÄúBut ever since the schema change in V3, sometimes the value is another mapping type that wraps the string we want&amp;amp;mldr;‚Äù I‚Äôm visualizing another square to the right of the existing one. This one is yellow, distinct from the light blue of all the others, and it never occurs to me to question why that is.&lt;/p&gt;&lt;p&gt;‚ÄúBut because this is actually YAML, and the value comes from a template call, both the string form and the mapping form need to be escaped and indented in a way that works in both cases.‚Äù I‚Äôm chewing on the problem in pretty much the same mindset I had during work, only now I‚Äôm walking across midtown instead of staring at a computer screen. ‚ÄúWe could just revert that change, keep the value as JSON like it used to be and insert it verbatim&amp;amp;mldr; but DevEx owns that part and I wouldn‚Äôt want to have to fight to get that PR approved.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúPiece of shit.‚Äù I speak that bit out loud without really intending to. I snap back into awareness of my surroundings and look around. Nobody was near enough to hear it. They probably wouldn‚Äôt have cared if they were.&lt;/p&gt;&lt;p&gt;It occurs to me that, whenever anybody asks me what I do for a living and I wave my hand and say ‚ÄúComputers,‚Äù this is what I‚Äôm trying to avoid needing to have to explain. None of these words are being used in a way that would mean anything to most people. If one were to take the time to carefully define them all and how they fit together semantically, they describe concepts so abstract and detached from any kind of tangible shared experience that you‚Äôd hit a second wall trying to explain that.&lt;/p&gt;&lt;p&gt;‚ÄúOh, but wait, we have the &lt;code&gt;nindent&lt;/code&gt; function. I could just count up the indentation level of the outer list and&amp;amp;mldr; Ah, hell, I forgot this template is transcluded into pod and deployment specs and the nesting levels would be different between the two.‚Äù I briefly try to think of which chucklefuck I could blame this design on, but truth be told I rubber-stamped enough questionable pull requests in my time here that a fair amount of this situation is a mess of my own damn making.&lt;/p&gt;&lt;p&gt;Huh. I really do wonder what I would sound like trying to explain this to somebody who had no experience in the industry. I suppose if I was very excited about it, I might come across like an energetic kid going on and on about all the different Pok√©mon they know about and all the special attacks and vulnerabilities. But without that spark of passion, and in its place a jaded voice tinged with frustration and contempt, I would probably just sound like a raving lunatic. These words don‚Äôt mean anything. I‚Äôm not describing something that actually exists. I‚Äôm playing the part of an observer in a universe of little floating boxes, becoming physically agitated about a superficial difference within the yellow one, and none of it is real.&lt;/p&gt;&lt;p&gt;I‚Äôm definitely not feeling the passion on this one. This code runs deep inside a build-deploy pipeline that I have no hope of ever running directly on the computer I‚Äôm using. So I write the code, push it to CI, wait for a bunch of stuff I‚Äôm not interested in to finish running, then get to watch my change fail to work for either the stupidest typo that I never should‚Äôve made in the first place, or due to some error that is so novel that even the search engines assume I must really be having some other much more popular error instead of the one I provided. It feels like I am performing surgery using a scalpel held by a boardwalk arcade claw machine, complete with the constant squawking and shitting of project management seagulls.&lt;/p&gt;&lt;p&gt;And even if I could concisely explain all of that to my hypothetical interlocutor, there‚Äôs the even higher-level question: Why? Why did we even make this change? What was so irredeemably wrong with the last two versions of this thing that we‚Äôre now doing it all again a third time? What exactly is the goal we‚Äôre trying to achieve here? I can‚Äôt really say. It‚Äôs a question I never asked, partly because I learned a long time ago that asking questions just causes friction. Just nod and shut up. Put a +1 on a sketchy PR and get it out of here. Don‚Äôt hold up the pipeline. Recover enough stamina to face down the next eldritch nightmare that slithers its way to the top of my Jira swimlane. ‚ÄúSounds great, thanks.‚Äù Thumbs-up. Grit my teeth through to the next direct deposit, convince myself it‚Äôs not so bad. Do it over and over until some ill-defined end condition is met. I‚Äôll know it when I see it. I hope.&lt;/p&gt;&lt;p&gt;I catch myself at the tail end of a sigh. I fake like I‚Äôm yawning to stretch my upper body for a second. Approximately every muscle in my back now aches.&lt;/p&gt;&lt;p&gt;There‚Äôs this very real sense that I don‚Äôt&amp;amp;mldr; I don‚Äôt want to solve this problem. There is no intellectual reward at the end of this journey. It‚Äôs not interesting to me. This isn‚Äôt something that needs to be fixed, because it‚Äôs not a situation that ever should‚Äôve been permitted to happen in the first place. This is just a bunch of contrived nonsense that I must work through because the broader situation dictates it. It doesn‚Äôt matter if the solution is good or elegant. It doesn‚Äôt matter if it barely works. It doesn‚Äôt matter if it causes another problem that I stub my toe on in three weeks. It‚Äôs just&amp;amp;mldr; what I have to do.&lt;/p&gt;&lt;p&gt;I stop in my tracks.&lt;/p&gt;&lt;p&gt;These kinds of problems are my cat turds.&lt;/p&gt;&lt;p&gt;Unlike Jim, though, I can‚Äôt just cram a bunch of breath mints into my face to make this go away.&lt;/p&gt;&lt;p&gt;The ‚Äúdown‚Äù escalator into the train station is out of service, and it has been this way all summer. A pair of orange plastic barricades block the landings at both ends. I walk down two flights of stairs alongside a half-dozen other commuters. Having concluded that the template problem simply isn‚Äôt worth thinking any further about, I‚Äôm back on the cat turds. I understand what Jim was talking about now. This has been happening for almost my entire life, even going back to my days in elementary school.&lt;/p&gt;&lt;p&gt;All of the homework assignments that were blindly graded against answer keys from the back of a Teacher Edition of the textbook: Cat turds. College admission essays where I profused a longing desire to attend the distinguished universities that my parents and guidance counselor told me I should set my ambitions toward: Cat turds. Probably hundreds of cover letters submitted alongside job applications throughout the years, skimmed by perhaps tens of internal recruiters and hiring managers: Cat turds.&lt;/p&gt;&lt;p&gt;The notion that it was a good idea to manipulate highly whitespace-sensitive YAML data with the Go &lt;code&gt;text/template&lt;/code&gt; package. CI workflows that take 75 minutes to reach the one step in the entire process that fails. Tools and interfaces that force-update and introduce breaking changes for seemingly no justifiable reason, removing or kneecapping features that were being relied on, with issue trackers guarded by thickheaded bots that dismissively auto-close feature requests that kindly ask for consideration for those use cases. Massively over-complicated software that tries to be everything to everybody, but in reality ends up being a gigantic lumbering pile of failure and frustration. Cat turds.&lt;/p&gt;&lt;p&gt;I used to love this stuff. I still do. Except&amp;amp;mldr; I don‚Äôt. Not lately, anyway. A long time ago, this was unquestionably what I wanted to do with my life. I would stay up late, pushing back my bedtime for a few more minutes with these glorious machines, hacking away on some little project. Then I‚Äôd get up early the following morning, excited to jump back into the project before my day out in the world began. I don‚Äôt even clearly remember what I was building toward, but I know it had basically zero utility or market potential. The point of doing the project was simply to do the project‚Äîto press through problems, to learn new things, and to end the day with more skills and experience than I started with.&lt;/p&gt;&lt;p&gt;At one point, I had the 7-bit ASCII table memorized. Just the decimal codes; I didn‚Äôt really understand the usefulness of the hexadecimal representations, and it never occurred to me that the hex values would work much better in mnemonics. I don‚Äôt know why I took the time to learn that. I never really used that knowledge in any real day-to-day work, and it began to fade from my mind as soon as I found some other pointless esoterica to wallpaper over it.&lt;/p&gt;&lt;p&gt;Look at me now, having to Google how to read a text file line-by-line in Python despite having done it a hundred times at this point. The knowledge is up there somewhere, I‚Äôm sure of it. I just can‚Äôt always think of the idiom in the heat of the moment. Just a little hint to jog the old brain, that‚Äôs all I need.&lt;/p&gt;&lt;p&gt;I often wonder what my Younger Self would think of me now, failing to remember a two-line snippet of code that you‚Äôd find in the first ten pages of any beginner‚Äôs guide to the language. He‚Äôd probably sneer and say I need to devote more time to studying. But I‚Äôm an adult with things to do; I can‚Äôt spend all my time just memorizing things just in case I might need the information someday. Oh, and by the way: Younger Self, if you were such a friggin‚Äô hotshot, why did it take you fifteen years to finally wrap your head around regular expressions? What‚Äôs that? Because they were hard? So you spent all your time memorizing easy and pointless trivia rather than tackling anything that was genuinely challenging? And then building up a whole air of superiority based on the number of discrete facts you could rattle off, rather than their practical utility? What, were you trying to become a contestant on Computer Jeopardy! or something?&lt;/p&gt;&lt;p&gt;No wonder Younger Self grew up to be kind of an asshole.&lt;/p&gt;&lt;p&gt;I mean, I didn‚Äôt try to be an asshole. It‚Äôs just that I tended to gauge my own self-worth relative to others based on the only social currency we could accurately compare: the amount of ‚Äústuff‚Äù we knew. Some people memorize car engine displacements, others carry in their noggins enough digits of pi to resolve the observable universe down to the width of a hydrogen atom. I had a litany of command-line switches that I never used for anything, HTML character entity names for writing systems I couldn‚Äôt comprehend, and tales of tweaking settings deep inside the Windows 98 Device Manager just so I could brag about having been in there in the first place. I also at one time sincerely believed that maybe if I taught myself to‚ÄîI‚Äôm picking one example out of many‚Äîdecode Code 39 barcodes in my head, it would somehow make me cool and desirable during otherwise awkward social functions. (I did get reasonably good at it. All it takes is memorizing a couple of three-digit sequences. Having a teenager‚Äôs near-field visual acuity certainly helped.)&lt;/p&gt;&lt;p&gt;Everybody else who didn‚Äôt know those little pieces of nothing? They were the lessers. They didn‚Äôt put in the time to grind for this knowledge. They had never scaled the peaks of Mount AltaVista, nor had they knelt in the temple of the MSDN Library for Visual Studio on a banged-up pair of CD-Rs. I knew things they did not, therefore I felt I was higher-and-mightier than they were. I and I alone suffered for this knowledge. This attitude manifested itself in one of two ways: In the first case, I would barge my way into situations where my involvement wasn‚Äôt needed or appreciated, thinking I could ‚Äúsave‚Äù others from the pain I once had to contend with. More often than not, though, I would simply mock people for not knowing things‚Äîusually inside my own head, but sometimes outwardly on mailing lists and message boards. There were times when I judged a person to have failed to put in the necessary amount of work, so therefore they did not deserve to rise anywhere near where I considered my own level to be. It didn‚Äôt matter if the subject was deeply technical or a disagreement on the precise phrasing of a Simpsons quote. Somebody got something wrong, and it was my job to rectify that.&lt;/p&gt;&lt;p&gt;I feel bad for the people who worked on teams where Younger Self was the senior engineer. I was full of ideals and convictions back then. ‚ÄúNo, we‚Äôre not doing that. We‚Äôre going to Do It Right instead.‚Äù I was full of piss and vinegar. ‚ÄúHere, give me that; I‚Äôll just do it myself.‚Äù I was full of shit.&lt;/p&gt;&lt;p&gt;I now realize that everything I lorded over other people‚Äîall the things I gatekept without consciously understanding that this was what I was doing‚ÄîI didn‚Äôt need to do that. It really didn‚Äôt help anything. For some number of people who interacted with me, I was the problem. I could‚Äôve been more tolerant or forgiving, I could‚Äôve said ‚Äúlet‚Äôs find out together,‚Äù I could‚Äôve let other people have the fun once in a while. I could have minded my own damn business and saved everybody the hassle.&lt;/p&gt;&lt;p&gt;There were people out there who must‚Äôve felt that I was their cat turds.&lt;/p&gt;&lt;p&gt;I‚Äôll never be able to track down and apologize to every person I treated that way. And why did I even build that fiefdom and protect it so jealously? Why was I so insecure? Why did I have to always be right and have a ready justification for why everybody else was wrong?&lt;/p&gt;&lt;p&gt;It was just me, alone in my tiny sandbox, safe and secure behind my towering fortress of cat turds.&lt;/p&gt;&lt;p&gt;My usual train, the one packed so full that some riders have to stand in the aisles until after the first or second stop, usually leaves at 5:50. Now about three hours later, one can sometimes get an entire car to themselves. I settle down in a window seat looking out at the desolate platform. Evidently there aren‚Äôt all that many people interested in traveling across the river at this hour on a Wednesday evening. It feels nice to sit, despite the fact that I‚Äôve probably sat for a cumulative ten hours‚Äîat least‚Äîover the course of this day.&lt;/p&gt;&lt;p&gt;As sometimes happens, another rider boards the train and enters what had up to this point been my personal rail car. He selects the aisle seat in the row directly in front of me. At least 110 other seats in this car, every single one of them empty, and his choice is to sit right here. Sigh. I could get up and move to another seat but I‚Äôm&amp;amp;mldr; exhausted. I‚Äôm here, I‚Äôm settled in, and above all I‚Äôm just completely out of ambition. I guess it‚Äôs fine as long as he doesn‚Äôt start playing music or TikTok videos without headphones.&lt;/p&gt;&lt;p&gt;A long blow from the locomotive horn, and the train begins to creep forward. Right on schedule. We‚Äôre in a tunnel deep below the city‚Äôs west side, and the view out the window is pitch black aside from the occasional glow from a mercury-vapor emergency light. On the wall beneath each of these lights, patches of graffiti framed by concrete pillars. I wouldn‚Äôt say I‚Äôve memorized them all by heart; I can‚Äôt even read the tags on the majority of them. But they are at least familiar, and I‚Äôve found some of them serve as convenient signposts along this portion of the trip. I‚Äôm not really paying attention to any of them tonight, instead I‚Äôm staring blankly at a little patch of window glass as the scene rolls past.&lt;/p&gt;&lt;p&gt;I refocus my eyes a bit and realize I‚Äôm looking at the reflection of a screen, or at least the top corner of one. I turn away from the window and find the source of the light. The man in front of me has opened his laptop‚Äîa chunky Dell Latitude or something very close to it‚Äîand perched it on a small lap desk fashioned from his leather bag. He opens a web browser and logs into a Microsoft account, one key at a time, hunt-and-peck style. It prompts him for his second factor and he shifts awkwardly in the seat to retrieve his phone. The login process succeeds and, after a few clicks and a fair bit of both of our finite lifetimes spent staring at loading spinners, opens what appears to be a Word document. I can‚Äôt read anything on his screen, which is more a testament to how wrecked my eyes have become than anything else, but I can see that there‚Äôs about four, perhaps five lines of unformatted text up there already. He strokes his chin while giving it a good read-through, then his hands take their position on the trackpad. Right index finger moves the cursor, left index finger does the clicking. The screen flips to another browser tab, his left hand gratuitously double-clicks on the website suggested by the first tile on the screen, and the page loads.&lt;/p&gt;&lt;p&gt;I never learned to tell any of these sites apart from each other. I see lots of people using the one with the spirograph logo. The one that looks like a cartoon butthole is also quite popular among some departments at my job. This guy is using the one that‚Äôs represented by a symmetrical color blob. Not that one, the other color blob one. Yeah.&lt;/p&gt;&lt;p&gt;He has opened a chat session that has evidently been going for some time. The text entry box at the bottom of the window waits patiently for fresh input. Letter by agonizing letter, the keys needed to express his thoughts are pressed. The most-pressed key, however, is Backspace. This man is, using the most generous language possible, not a particularly fast or accurate typist. In total, he enters about ten words before pressing Enter. A short moment later, the machine responds. Entire sentences appear in the time it took him to type a single word. Multiple paragraphs with subheadings and bulleted lists scroll into view. The screen fills completely with this fresh text. He looks at this for a moment, moves his hands back to the trackpad, and selects a complete paragraph. His finger presses down with immense force as he drags the selection area ever wider, as if his catch is in danger of wriggling through his fingers if he doesn‚Äôt hold the button down hard enough. He flips back to his Word document and pastes the paragraph. Then back to the chat window. He begins typing again. Slowly. Excruciatingly.&lt;/p&gt;&lt;p&gt;This cycle repeats several times, incrementally building his document up to four or five double-spaced pages in length. It‚Äôs not exactly a fast process, but certainly faster than if he had thought up and typed out all that content the old-fashioned way. It‚Äôs certainly plausible that he at least read everything that went into the document, but I wouldn‚Äôt be able to prove it.&lt;/p&gt;&lt;p&gt;He selects another piece of text, this one substantially smaller than the other specimens that he‚Äôd been handling up until this point. This one is pasted into a discussion thread on Teams. He waits a moment for responses, closes the lid, and the laptop goes back into his bag. The man stands up, wraps the strap over his shoulder, and walks to the front of the car as the train brakes to a full stop. This is where our paths diverge, it would seem. The doors open and he steps out into the night.&lt;/p&gt;&lt;p&gt;Alone in the train car again, with nothing interesting to eavesdrop on, my mind begins to wander again. I wonder what the purpose of that document was. Why was it being prepared? Who dictated that a half-dozen input phrases needed to be inflated into a thousand-word wall of text? Who was going to sit and read all of that, anyway? And for what purpose?&lt;/p&gt;&lt;p&gt;I really don‚Äôt know. But I do know one thing: It‚Äôs cat turds.&lt;/p&gt;&lt;p&gt;This guy obviously didn‚Äôt want to do that task. Whether that was due to lack of passion and interest, or lack of skill and ability, he had a cat turd to eat and he found a little pack of Altoids that he could use to get through it with minimal suffering. The people who have to read it? There‚Äôs a good chance they‚Äôll be dealing with a cat turd too. Maybe they can choose to employ a chatbot to summarize it back down to his original inputs. Maybe it‚Äôll even do a passable job preserving the essence of the guy‚Äôs prompts.&lt;/p&gt;&lt;p&gt;It makes sense why a person or group of people would flock to anything that makes life‚Äôs demands a little less difficult for themselves. You‚Äôd have to be pretty dumb to want to do a task like that manually.&lt;/p&gt;&lt;p&gt;There‚Äôs still the question, though. Why are we all eating cat turds? When did we all collectively agree that we were all a-okay with the idea that we had to subject ourselves to this constant grind of doing shit that doesn‚Äôt really need to be done to satisfy requirements that were put in place simply ‚Äúbecause‚Äù and that seemingly only create more pointless work for other people (or ourselves!) to have to do later?&lt;/p&gt;&lt;p&gt;One of the defining characteristics of humanity is its ability to build and wield tools that make difficult tasks easier. One would presume there would also be a certain wisdom in knowing which of the difficult tasks were worth doing in the first place but&amp;amp;mldr; Well. When you presume, you make a pres out of u and me.&lt;/p&gt;&lt;p&gt;If I had known ahead of time that I‚Äôd be out this late, I would‚Äôve brought a jacket. The early autumn air is calm but crisp, and my borough‚Äôs train platform offers very little protection from the chill. The crickets are still chirping, but their song has slowed substantially compared to how they sounded a few weeks back. I stopped parking at the station a long time ago‚Äîthe monthly pass costs well over $150 now, and most days the parking lot is completely full before six o‚Äôclock in the morning anyway. It‚Äôs only a mile to the house, and this twenty-minute walk is pretty much the only exercise I get nowadays.&lt;/p&gt;&lt;p&gt;Once I cross the main boulevard at the four-way stop, it‚Äôs all suburban residential side streets. There is basically no traffic at this time of night in my sleepy little bedroom community. All the dogs have been walked, the kids have been put to bed, and the adults&amp;amp;mldr; Well, I‚Äôm sure there are at least a couple people around here drinking or smoking the memory of their cat turds away.&lt;/p&gt;&lt;p&gt;I‚Äôm no closer to anything resembling inner peace. I find I‚Äôve grown to despise large swaths of the only thing I‚Äôve ever been able to earn reliable income from. I tire of walking a path that has seemingly shifted beneath my feet to point toward a destination I no longer recognize. I‚Äôm embarrassed by the jerk my Younger Self used to be, and simultaneously ashamed of the energy I lost as I matured. I don‚Äôt really want to do most of what I have to do, while feeling a deep unsated need to achieve something that I have neither the stamina nor the freedom to pursue. At some point I‚Äôm going to reach down deep into the well of ambition to discover there ain‚Äôt nothing there to pull out anymore. And then?&lt;/p&gt;&lt;p&gt;Something percent of success is simply showing up. That‚Äôs roughly how the quote goes, right? I‚Äôve heard seventy percent, ninety percent, hell, let‚Äôs call it seventy-eight. It doesn‚Äôt matter because it isn‚Äôt a real thing that can be measured in any objective way. The idea is to inspire people to at least try. Put your butt in the chair, log into Teams, trick yourself into thinking, well, I made it all the way here, might as well prune my stale Git branches or something so I can feel like I‚Äôm doing real work. Push aside distractions, shake off procrastination, kindle that tiny spark into enough momentum to break through whatever barrier is standing in the way of getting something done. If only that worked with any degree of predictability.&lt;/p&gt;&lt;p&gt;There‚Äôs a metaphor that talks about painting the backs of cabinets. The idea is that, when you‚Äôre putting paint, stain, varnish, whatever on some cabinets, there‚Äôs no need to paint the surfaces that face toward the wall. From the day the units are mounted, to a day forty years from now when they are ripped down and thrown into a construction dumpster during a subsequent kitchen renovation, nobody will see the backs of any of those cabinets. Painting them would be a waste of time and materials. Nobody would know if it was done or not.&lt;/p&gt;&lt;p&gt;‚ÄúYes, but I would know.‚Äù That‚Äôs something my Dad would often say. His tendency has always been to be overly thorough, exacting and precise in any craft he partakes in. Everything‚Äîfrom the doors in the house to the stripes cut into the front lawn‚Äîwas always level, plumb, square, centered, polished, dust-free, squeak-free, fingerprint-free&amp;amp;mldr; He even demonstrated meticulous care in breaking down cardboard and filling up the waste bins at the curb. I still have no idea how he was able to raise two kids in that house without exploding from the chaos we brought.&lt;/p&gt;&lt;p&gt;Maybe it was genetic, or maybe I voluntarily developed it so my dad would be proud of me just like he was proud of the other things he made. Either way, I definitely started to take after him in those ways and I now recognize this same kind of care in myself all the time. Not just in the way I prefer all my clocks to read the exact correct time or my knack for always noticing the way the receptacle face isn‚Äôt exactly flush with the wall plate&amp;amp;mldr; but in a fundamental inability to not care about quality or craft. Even when the task doesn‚Äôt matter. Even if it results in an entire afternoon spent painting a piece of carpentry that nobody will ever see. I can‚Äôt not care.&lt;/p&gt;&lt;p&gt;All that stuff Younger Self struggled with‚Äîthe self-superiority, the sense that I had to be the one who did it if it needed to be done correctly, the derision and borderline abuse I gave others‚Äîthat was all just a big dogmatic ball of caring a whole lot about quality and craft, being rolled around by a kid who didn‚Äôt understand what to do with it. I had to work so hard to care so much, and these other people didn‚Äôt, and everything worked out for them anyway, and that wasn‚Äôt fair. Decades later, I still feel that way sometimes.&lt;/p&gt;&lt;p&gt;My parents still live in that house, surrounded by all the things my dad cared so much about. Aside from a whole bunch of trees that died and needed to be cut down to stumps, everything is still pretty much pristine. But if you start to look around, really scrutinize, you‚Äôll start to notice some things have slipped. There‚Äôs a film of dust on the higher wall decorations. Some of the brass knobs are becoming tarnished. A few of the light bulbs in the hallway fixtures don‚Äôt match. My dad seemed tired the last time we talked, and more than once he expressed the sentiment that ‚Äúeverything he owns is falling apart.‚Äù Is it simply the onset of physical old age that has limited his ability to stay on top of these things, or is he beginning to leave behind his era of caring?&lt;/p&gt;&lt;p&gt;Now that I think about it, I don‚Äôt think we‚Äôve ever really talked about how care factored into his career philosophy. I had always implicitly assumed that it was the same as it currently is with me: Work or play, it‚Äôs always there. Can‚Äôt turn it off even if I wanted to. But what if he could? What if all the care he demonstrated in projects around the house was compensation for all the things he deliberately avoided caring about at work? It would certainly explain how he was able to consistently sustain those standards. But then, that would mean that I modeled my own principles and tastes on a distorted view of my dad, untempered by whatever he didn‚Äôt let me see about his workplace persona.&lt;/p&gt;&lt;p&gt;How would I begin to‚Äîwell, I don‚Äôt want to say ‚Äúnot care,‚Äù that sounds too extreme. But maybe&amp;amp;mldr; selectively care? To care about the things that matter, the things that spark passion and joy and remind me why I spent so much time practicing this godforsaken occupation. While at the same time recognizing the things that don‚Äôt matter, the problems for which the optimal solution is to stop insisting on having that problem in the first place. The kinds of tasks for which the 78% showing-up baseline score is plenty good enough. Tasks on which care would be utterly wasted, the cases where the cabinets are so irredeemably fucked up that the lack of paint on the back is the last thing anybody‚Äôs going to worry about. Those are the tasks that hurt the most, because I find it basically impossible to make myself care about them. It offends my soul to try to force it, and it drains me of all ambition to move onto the next potentially heartening opportunity. It‚Äôs a real problem, and I find it always has been: If I can‚Äôt care about it, I have an extremely hard time bringing myself to do it at all.&lt;/p&gt;&lt;p&gt;Well, I suppose that‚Äôs when I open a chatbot session of my own. ‚ÄúHey there Chat. Uh, we‚Äôve never spoken before but, uh&amp;amp;mldr; Well, my entire system of self-motivation just completely broke down but I still need to keep moving forward. Can you help me out of this bind?‚Äù There‚Äôs a whole discipline‚Äîthey call it Prompt Engineering‚Äîthat‚Äôs just a fancy form of throwing your hands up and pressing the Care About It For Me button. That‚Äôs pretty much how it works. Provide it with any cat turd under the sun, it doesn‚Äôt matter. Chat will gobble them all up for you like a coprophagic dog.&lt;/p&gt;&lt;p&gt;I‚Äôd be lying if I said the idea didn‚Äôt make my skin crawl a little. Every fiber of my being says that this is a weight to be borne by me and me alone. This is my cat turd to eat; they gave it to me. When it‚Äôs done, I can open my grinning maw and say without equivocation that I was the one who got through it. I painted the back of this cabinet. I worked way too hard and poured far too much of my blood, sweat and tears into this thing. And my reward for a job well done is&amp;amp;mldr; debilitating exhaustion, most of the time. Getting a fresh cat turd to eat tomorrow. And the day after.&lt;/p&gt;&lt;p&gt;Of course, Chat can‚Äôt really care. It does a passable job pretending like it cares, saying the words that convey the illusion of care to any reader not paying very close attention. Where do I draw the line between fostering real care, versus passing off a degraded third-generation photocopy of some tokenization of what may have at one point been somebody else‚Äôs care? Is the line simply the boundary between the tasks I‚Äôm excited to do and the ones I put off until I‚Äôve depleted enough mental reserves to sorta care?&lt;/p&gt;&lt;p&gt;It really does feel like the average person has made a choice to abandon a great deal of care, at least in their professional capacity. Take a look around at all these people with their fake shit-eating grins, passing off a machine‚Äôs effort as their own and experiencing no consequences. Sometimes they‚Äôre rewarded for doing so. There are organizations that are beginning to mandate it now. These people aren‚Äôt eating their cat turds anymore, why am I still sitting here eating mine?&lt;/p&gt;&lt;p&gt;I round the final curve leading to the corner of my block. As I pass under the streetlight, I cast a shadow on the asphalt ahead. With each step it grows longer and more distorted. There‚Äôs a rustle from the shrubs bordering my neighbor‚Äôs driveway, and a small dark form emerges. It crosses the street halfway then abruptly stops. I stop as well. A pair of glowing yellow eyes look back at me. I stare at it, it stares at me. A possum, perhaps? Somebody‚Äôs outdoor cat? It‚Äôs just watching me, seemingly peering straight into my very soul. Can it see what I‚Äôm grappling with here? Is it passing judgement on me for thinking these thoughts? It sizes me up for a moment longer, turns its head, and becomes a black apparition once more. I struggle to track it as it continues across the street, and I lose sight of it entirely.&lt;/p&gt;&lt;p&gt;I arrive at home and shut the door behind me. Sunset was over two hours ago and it‚Äôs nearly pitch black in the hallway. I fumble around for the light switch, kick my shoes off next to the doorway, and hang my bag on its hook in the coat closet. Something grabs my attention, just above eye level, slightly overhanging the edge of the top shelf. I slide it out of its resting place and carry it into the kitchen. I sit down at the table and inspect it.&lt;/p&gt;&lt;p&gt;This object is a round metal cookie tin about twelve inches in diameter. Beneath a thin coat of dust, it is a deep red with a repeating pattern of snowmen and white snowflakes, and quite obviously once held winter holiday‚Äìthemed cookies. I repurposed it many years ago to hold the only vice I currently permit myself to indulge in: a meticulously curated collection of all different types of chocolate candies. I remove the lid and set it aside. I survey the contents, a sea of differently-shaped naked chocolate morsels. I don‚Äôt remember why I chose to remove all the foil and paper wrapping before putting these in here. From my vantage point, everything looks vaguely the same‚ÄîI can‚Äôt readily spot any differences between milk chocolate and dark, or those filled with caramel versus cr√®me.&lt;/p&gt;&lt;p&gt;One particular piece near the edge catches my eye, and I carefully select it for inspection. It‚Äôs not a very pleasing color or shape‚Äîoddly asymmetrical. I roll it around between my fingers. There‚Äôs a hair on it. I hold it up to my nose and take a whiff, hoping to detect the aroma of the cacao. Try as I might, I can‚Äôt pick up any trace of its scent.&lt;/p&gt;&lt;p&gt;Come to think of it, I can‚Äôt remember the last time I smelled anything.&lt;/p&gt;¬´ Back to Articles&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.scottsmitelli.com/articles/altoids-by-the-fistful/"/><published>2025-09-23T06:24:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45344554</id><title>The YAML Document from Hell</title><updated>2025-09-23T13:01:26.637759+00:00</updated><content>&lt;doc fingerprint="e45100779e81da58"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The yaml document from hell&lt;/head&gt;
    &lt;p&gt;written by Ruud van Asseldonk&lt;lb/&gt;published &lt;/p&gt;
    &lt;p&gt;For a data format, yaml is extremely complicated. It aims to be a human-friendly format, but in striving for that it introduces so much complexity, that I would argue it achieves the opposite result. Yaml is full of footguns and its friendliness is deceptive. In this post I want to demonstrate this through an example.&lt;/p&gt;
    &lt;p&gt;This post is a rant, and more opinionated than my usual writing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Yaml is really, really complex&lt;/head&gt;
    &lt;p&gt;Json is simple. The entire json spec consists of six railroad diagrams. It‚Äôs a simple data format with a simple syntax and that‚Äôs all there is to it. Yaml on the other hand, is complex. So complex, that its specification consists of 10 chapters with sections numbered four levels deep and a dedicated errata page.&lt;/p&gt;
    &lt;p&gt;The json spec is not versioned. There were two changes to it in 2005 (the removal of comments, and the addition of scientific notation for numbers), but it has been frozen since ‚Äî almost two decades now. The yaml spec on the other hand is versioned. The latest revision is fairly recent, 1.2.2 from October 2021. Yaml 1.2 differs substantially from 1.1: the same document can parse differently under different yaml versions. We will see multiple examples of this later.&lt;/p&gt;
    &lt;p&gt;Json is so obvious that Douglas Crockford claims to have discovered it ‚Äî not invented. I couldn‚Äôt find any reference for how long it took him to write up the spec, but it was probably hours rather than weeks. The change from yaml 1.2.1 to 1.2.2 on the other hand, was a multi-year effort by a team of experts:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This revision is the result of years of work by the new YAML language development team. Each person on this team has a deep knowledge of the language and has written and maintains important open source YAML frameworks and tools.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Furthermore this team plans to actively evolve yaml, rather than to freeze it.&lt;/p&gt;
    &lt;p&gt;When you work with a format as complex as yaml, it is difficult to be aware of all the features and subtle behaviors it has. There is an entire website dedicated to picking one of the 63 different multi-line string syntaxes. This means that it can be very difficult for a human to predict how a particular document will parse. Let‚Äôs look at an example to highlight this.&lt;/p&gt;
    &lt;head rend="h2"&gt;The yaml document from hell&lt;/head&gt;
    &lt;p&gt;Consider the following document.&lt;/p&gt;
    &lt;code&gt;server_config:
  port_mapping:
    # Expose only ssh and http to the public internet.
    - 22:22
    - 80:80
    - 443:443

  serve:
    - /robots.txt
    - /favicon.ico
    - *.html
    - *.png
    - !.git  # Do not expose our Git repository to the entire world.

  geoblock_regions:
    # The legal team has not approved distribution in the Nordics yet.
    - dk
    - fi
    - is
    - no
    - se

  flush_cache:
    on: [push, memory_pressure]
    priority: background

  allow_postgres_versions:
    - 9.5.25
    - 9.6.24
    - 10.23
    - 12.13&lt;/code&gt;
    &lt;p&gt;Let‚Äôs break this down section by section and see how the data maps to json.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sexagesimal numbers&lt;/head&gt;
    &lt;p&gt;Let‚Äôs start with something that you might find in a container runtime configuration:&lt;/p&gt;
    &lt;code&gt;port_mapping:
  - 22:22
  - 80:80
  - 443:443&lt;/code&gt;
    &lt;code&gt;{"port_mapping": [1342, "80:80", "443:443"]}&lt;/code&gt;
    &lt;p&gt;Huh, what happened here? As it turns out, numbers from 0 to 59 separated by colons are sexagesimal (base 60) number literals. This arcane feature was present in yaml 1.1, but silently removed from yaml 1.2, so the list element will parse as &lt;code&gt;1342&lt;/code&gt; or &lt;code&gt;"22:22"&lt;/code&gt; depending on which version your parser uses. Although yaml 1.2 is more than 10 years old by now, you would be mistaken to think that it is widely supported: the latest version libyaml at the time of writing (which is used among others by PyYAML) implements yaml 1.1 and parses &lt;code&gt;22:22&lt;/code&gt; as &lt;code&gt;1342&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anchors, aliases, and tags&lt;/head&gt;
    &lt;p&gt;The following snippet is actually invalid:&lt;/p&gt;
    &lt;code&gt;serve:
  - /robots.txt
  - /favicon.ico
  - *.html
  - *.png
  - !.git&lt;/code&gt;
    &lt;p&gt;Yaml allows you to create an anchor by adding an &lt;code&gt;&amp;amp;&lt;/code&gt; and a name in front of a value, and then you can later reference that value with an alias: a &lt;code&gt;*&lt;/code&gt; followed by the name. In this case no anchors are defined, so the aliases are invalid. Let‚Äôs avoid them for now and see what happens.&lt;/p&gt;
    &lt;code&gt;serve:
  - /robots.txt
  - /favicon.ico
  - !.git&lt;/code&gt;
    &lt;code&gt;{"serve": ["/robots.txt", "/favicon.ico", ""]}&lt;/code&gt;
    &lt;p&gt;Now the interpretation depends on the parser you are using. The element starting with &lt;code&gt;!&lt;/code&gt; is a tag. This feature is intended to enable a parser to convert the fairly limited yaml data types into richer types that might exist in the host language. A tag starting with &lt;code&gt;!&lt;/code&gt; is up to the parser to interpret, often by calling a constructor with the given name and providing it the value that follows after the tag. This means that loading an untrusted yaml document is generally unsafe, as it may lead to arbitrary code execution. (In Python, you can avoid this pitfall by using &lt;code&gt;yaml.safe_load&lt;/code&gt; instead of &lt;code&gt;yaml.load&lt;/code&gt;.) In our case above, PyYAML fails to load the document because it doesn‚Äôt know the &lt;code&gt;.git&lt;/code&gt; tag. Go‚Äôs yaml package is less strict and returns an empty string.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Norway problem&lt;/head&gt;
    &lt;p&gt;This pitfall is so infamous that it became known as ‚Äúthe Norway problem‚Äù:&lt;/p&gt;
    &lt;code&gt;geoblock_regions:
  - dk
  - fi
  - is
  - no
  - se&lt;/code&gt;
    &lt;code&gt;{"geoblock_regions": ["dk", "fi", "is", false, "se"]}&lt;/code&gt;
    &lt;p&gt;What is that &lt;code&gt;false&lt;/code&gt; doing there? The literals &lt;code&gt;off&lt;/code&gt;, &lt;code&gt;no&lt;/code&gt;, and &lt;code&gt;n&lt;/code&gt;, in various capitalizations (but not any capitalization!), are all &lt;code&gt;false&lt;/code&gt; in yaml 1.1, while &lt;code&gt;on&lt;/code&gt;, &lt;code&gt;yes&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; are true. In yaml 1.2 these alternative spellings of the boolean literals are no longer allowed, but they are so pervasive in the wild that a compliant parser would have a hard time reading many documents. Go‚Äôs yaml library therefore made the choice of implementing a custom variant somewhere in between yaml 1.1 and 1.2 that behaves differently depending on the context:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The yaml package supports most of YAML 1.2, but preserves some behavior from 1.1 for backwards compatibility. YAML 1.1 bools (yes/no, on/off) are supported as long as they are being decoded into a typed bool value. Otherwise they behave as a string.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Note that it only does that since version 3.0.0, which was released in May 2022. Earlier versions behave differently.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-string keys&lt;/head&gt;
    &lt;p&gt;While keys in json are always strings, in yaml they can be any value, including booleans.&lt;/p&gt;
    &lt;code&gt;flush_cache:
  on: [push, memory_pressure]
  priority: background&lt;/code&gt;
    &lt;code&gt;{
  "flush_cache": {
    "True": ["push", "memory_pressure"],
    "priority": "background"
  }
}&lt;/code&gt;
    &lt;p&gt;Combined with the previous feature of interpreting &lt;code&gt;on&lt;/code&gt; as a boolean, this leads to a dictionary with &lt;code&gt;true&lt;/code&gt; as one of the keys. It depends on the language how that maps to json, if at all. In Python it becomes the string &lt;code&gt;"True"&lt;/code&gt;. The key &lt;code&gt;on&lt;/code&gt; is common in the wild because it is used in GitHub Actions. I would be really curious to know whether GitHub Actions‚Äô parser looks at &lt;code&gt;"on"&lt;/code&gt; or &lt;code&gt;true&lt;/code&gt; under the hood.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accidental numbers&lt;/head&gt;
    &lt;p&gt;Leaving strings unquoted can easily lead to unintentional numbers.&lt;/p&gt;
    &lt;code&gt;allow_postgres_versions:
  - 9.5.25
  - 9.6.24
  - 10.23
  - 12.13&lt;/code&gt;
    &lt;code&gt;{"allow_postgres_versions": ["9.5.25", "9.6.24", 10.23, 12.13]}&lt;/code&gt;
    &lt;p&gt;Maybe the list is a contrived example, but imagine updating a config file that lists a single value of 9.6.24 and changing it to 10.23. Would you remember to add the quotes? What makes this even more insidious is that many dynamically typed applications implicitly convert the number to a string when needed, so your document works fine most of the time, except in some contexts it doesn‚Äôt. For example, the following Jinja template accepts both &lt;code&gt;version: "0.0"&lt;/code&gt; and &lt;code&gt;version: 0.0&lt;/code&gt;, but it only takes the true-branch for the former.&lt;/p&gt;
    &lt;code&gt;{% if version %}
  Latest version: {{ version }}
{% else %}
  Version not specified
{% endif %}&lt;/code&gt;
    &lt;head rend="h2"&gt;Runners-up&lt;/head&gt;
    &lt;p&gt;There is only so much I can fit into one artifical example. Some arcane yaml behaviors that did not make it in are directives, integers starting with &lt;code&gt;0&lt;/code&gt; being octal literals (but only in yaml 1.1), &lt;code&gt;~&lt;/code&gt; being an alternative spelling of &lt;code&gt;null&lt;/code&gt;, and &lt;code&gt;?&lt;/code&gt; introducing a complex mapping key.&lt;/p&gt;
    &lt;head rend="h2"&gt;Syntax highlighting will not save you&lt;/head&gt;
    &lt;p&gt;You may have noticed that none of my examples have syntax highlighting enabled. Maybe I am being unfair to yaml, because syntax highlighting would highlight special constructs, so you can at least see that some values are not normal strings. However, due to multiple yaml versions being prevalent, and highlighters having different levels of sophistication, you can‚Äôt rely on this. I‚Äôm not trying to nitpick here: Vim, my blog generator, GitHub, and Codeberg, all have a unique way to highlight the example document from this post. No two of them pick out the same subset of values as non-strings!&lt;/p&gt;
    &lt;head rend="h2"&gt;Templating yaml is a terrible, terrible idea&lt;/head&gt;
    &lt;p&gt;I hope it is clear by now that working with yaml is subtle at the very least. What is even more subtle is concatenating and escaping arbitrary text fragments in such a way that the result is a valid yaml document, let alone one that does what you expect. Add to this the fact that whitespace is significant in yaml, and the result is a format that is meme-worthily difficult to template correctly. I truly do not understand why tools based on such an error-prone practice have gained so much mindshare, when there is a safer, easier, and more powerful alternative: generating json.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alternative configuration formats&lt;/head&gt;
    &lt;p&gt;I think the main reason that yaml is so prevalent despite its pitfalls, is that for a long time it was the only viable configuration format. Often we need lists and nested data, which rules out flat formats like ini. Xml is noisy and annoying to write by hand. But most of all, we need comments, which rules out json. (As we saw before, json had comments very early on, but they were removed because people started putting parsing directives in there. I think this is the right call for a serialization format, but it makes json unsuitable as a configuration language.) So if what we really need is the json data model but a syntax that allows comments, what are some of the options?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Toml ‚Äî Toml is similar to yaml in many ways: it has mostly the same data types; the syntax is not as verbose as json; and it allows comments. Unlike yaml it is not full of footguns, mostly because strings are always quoted, so you don‚Äôt have values that look like strings but aren‚Äôt. Toml is widely supported, you can probably find a toml parser for your favorite language. It‚Äôs even in the Python standard library ‚Äî unlike yaml! A weak spot of toml is deeply nested data.&lt;/item&gt;
      &lt;item&gt;Json with comments, Json with commas and comments ‚Äî There exist various extensions of json that extend it just enough to make it a usable config format without introducing too much complexity. Json with comments is probably the most widespread, as it is used as the config format for Visual Studio Code. The main downside of these is that they haven‚Äôt really caught on (yet!), so they aren‚Äôt as widely supported as json or yaml.&lt;/item&gt;
      &lt;item&gt;A simple subset of yaml ‚Äî Many of the problems with yaml are caused by unquoted things that look like strings but behave differently. This is easy to avoid: always quote all strings. (Indeed, you can tell that somebody is an experienced yaml engineer when they defensively quote all the strings.) We can choose to always use &lt;code&gt;true&lt;/code&gt;and&lt;code&gt;false&lt;/code&gt;rather than&lt;code&gt;yes&lt;/code&gt;and&lt;code&gt;no&lt;/code&gt;, and generally stay away from the arcane features. The challenge with this is that any construct not explicitly forbidden will eventually make it into your codebase, and I am not aware of any good tool that can enforce a sane yaml subset.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Generating json as a better yaml&lt;/head&gt;
    &lt;p&gt;Often the choice of format is not ours to make, and an application only accepts yaml. Not all is lost though, because yaml is a superset of json, so any tool that can produce json can be used to generate a yaml document.&lt;/p&gt;
    &lt;p&gt;Sometimes an application will start out with a need for just a configuration format, but over time you end up with many many similar stanzas, and you would like to share parts between them, and abstract some repetition away. This tends to happen in for example Kubernetes and GitHub Actions. When the configuration language does not support abstraction, people often reach for templating, which is a bad idea for the reasons explained earlier. Proper programming languages, possibly domain-specific ones, are a better fit. Some of my favorites are Nix and Python:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nix ‚Äî Nix is the language used by the Nix package manager. It was created for writing package definitions, but it works remarkably well as a configuration format (and indeed it is used to configure NixOS). Functions, let-bindings, and string interpolation make it powerful for abstracting repetitive configuration. The syntax is light like toml, and it can export to json or xml. It works well for simplifying a repetitive GitHub Actions workflow file, for example.&lt;/item&gt;
      &lt;item&gt;Python ‚Äî Json documents double as valid Python literals with minimal adaptation, and Python supports trailing commas and comments. It has variables and functions, powerful string interpolation, and &lt;code&gt;json.dump&lt;/code&gt;built in. A self-contained Python file that prints json to stdout goes a long way!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally there are some tools in this category that I haven‚Äôt used enough to confidently recommend, but which deserve to be mentioned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dhall ‚Äî Dhall is like Nix, but with types. It is less widespread, and personally I find the built-in function names unwieldy.&lt;/item&gt;
      &lt;item&gt;Cue ‚Äî Like Dhall, Cue integrates type/schema information into the config format. Cue is a superset of json, but despite that, I find the files that actually use Cue‚Äôs features to look foreign to me. Cue is on my radar to evaluate further, but I haven‚Äôt encountered a problem where Cue looked like the most suitable solution yet.&lt;/item&gt;
      &lt;item&gt;Hashicorp Configuration Language ‚Äî I haven‚Äôt used HCL extensively enough to have a strong opinion on it, but in the places where I worked with it, the potential for abstraction seemed more limited than what you can achieve with e.g. Nix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2025 update: After having used HCL more in practice, I consider it too ad-hoc to seriously recommend. My frustration with HCL is what prompted me to create RCL. It started as a toy project, but is now at a point where it is both usable and useful.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Yaml aims to be a more human-friendly alternative to json, but with all of its features, it became such a complex format with so many bizarre and unexpected behaviors, that it is difficult for humans to predict how a given yaml document will parse. If you are looking for a configuration format, toml is a friendly format without yaml‚Äôs footguns. For cases where you are stuck with yaml, generating json from a more suitable language can be a viable approach. Generating json also opens up the possibility for abstraction and reuse, in a way that is difficult to achieve safely by templating yaml.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell"/><published>2025-09-23T09:04:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45344708</id><title>Go has added Valgrind support</title><updated>2025-09-23T13:01:26.480426+00:00</updated><link href="https://go-review.googlesource.com/c/go/+/674077"/><published>2025-09-23T09:26:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45344756</id><title>Indoor surfaces act as sponges for harmful chemicals</title><updated>2025-09-23T13:01:25.827325+00:00</updated><content>&lt;doc fingerprint="75ffb4ccae8f068c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Indoor surfaces act as massive sponges for harmful chemicals, UC Irvine-led study shows&lt;/head&gt;
    &lt;p&gt;Permeable materials in homes can retain volatile organic compounds for up to a year&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scientists injected volatile organic compounds into a test house and found large reservoirs for the potentially hazardous chemicals in porous surfaces such as wood, cement and paint.&lt;/item&gt;
      &lt;item&gt;VOCs contained in insecticides, cigarette smoke and wildfire smoke can remain on indoor surfaces for as long as one year.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Irvine, Calif., Sept. 22, 2025 ‚Äî Indoor surfaces have an unexpectedly strong ability to absorb and hold harmful chemical compounds that can threaten human health for as long as a year, according to air chemistry researchers at the University of California, Irvine.&lt;/p&gt;
    &lt;p&gt;In a paper published today in Proceedings of the National Academy of Sciences, the UC Irvine scientists quantify how various indoor surfaces absorb volatile organic compounds, which can result in unhealthy conditions for people and animals when inhaled or absorbed through skin contact.&lt;/p&gt;
    &lt;p&gt;The sources of VOCs are many, such as cooking, spray cleaning, personal care and other consumer products. Additional significant contributors include tobacco smoke and, increasingly, air pollution caused by wildfires. The researchers note that health risks come from inhaling compounds when they ‚Äúoff gas‚Äù from surfaces and through dermal uptake when contaminated surfaces are touched.&lt;/p&gt;
    &lt;p&gt;In the spring of 2022, co-author Jonathan Abbatt, professor of chemistry at the University of Toronto, led the Chemical Assessment of Surfaces and Air study, which utilized simulation chambers in the National Institute of Standards and Technology‚Äôs Net-Zero Energy Residential Test Facility. Contaminants were injected into a structure mimicking a home environment, with typical building materials. The research team used mass spectrometry instruments to track the movement and persistence of VOCs in the controlled indoor environment.&lt;/p&gt;
    &lt;p&gt;‚ÄúScientists in the air chemistry research community have known for a long time that many indoor contaminants can be absorbed by indoor surfaces, but the size of indoor surface reservoirs inside homes and buildings had not been established,‚Äù said Manabu Shiraiwa, UC Irvine professor of chemistry, who was responsible for modeling observations and is a corresponding author on the PNAS paper. ‚ÄúOur modeling found that surfaces inside homes have a much greater size to absorb and hold chemicals than previously realized. We can think of these surfaces as massive chemical sponges that soak up VOCs.‚Äù&lt;/p&gt;
    &lt;p&gt;Before this study, thin organic films with nanometer thickness were thought to be main surface reservoirs. However, this work proves that permeable and porous materials such as painted surfaces, cement and wood are likely the major surface reservoirs in a home.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis discovery has significant implications for human health,‚Äù Shiraiwa said. ‚ÄúIt means people can be exposed to harmful chemicals long after their initial introduction into indoor spaces, and compounds can later be released back into the air or transferred to humans through direct contact with contaminated surfaces.‚Äù&lt;/p&gt;
    &lt;p&gt;He added, ‚ÄúThis result significantly impacts our understanding of VOC fate and human exposure in indoor environments. With such a large partitioning capacity, organic contaminants will have much longer indoor residence times than previously predicted.‚Äù&lt;/p&gt;
    &lt;p&gt;The research explains why certain odors and contaminants persist indoors even after their sources are removed. For example, it provides scientific evidence for why tobacco smoke odors linger in rooms long after smoking has stopped: The residual compounds, known as ‚Äúthirdhand smoke,‚Äù slowly partition back into the air from surface reservoirs.&lt;/p&gt;
    &lt;p&gt;The findings suggest that regular ventilation alone may be insufficient to remove many indoor contaminants. Physical cleaning activities such as vacuuming, mopping and dusting are necessary to effectively remove compounds with high partition coefficients from surface reservoirs.&lt;/p&gt;
    &lt;p&gt;Joining Shiraiwa and Abbatt in this study were Pascale Lakey, project scientist in chemistry at UC Irvine; Jie Yu and Xing Wang at the University of Toronto; Jenna Ditto at Washington University in St. Louis, Missouri; Han Huynh and Marina Vance at the University of Colorado Boulder; Michael Link, Dustin Poppendieck and Stephen Zimmerman at the National Institute of Standards and Technology; and Delphine Farmer at Colorado State University.&lt;/p&gt;
    &lt;p&gt;The research was supported by funding from the Alfred P. Sloan Foundation.&lt;/p&gt;
    &lt;p&gt;About the University of California, Irvine: Founded in 1965, UC Irvine is a member of the prestigious Association of American Universities and is ranked among the nation‚Äôs top 10 public universities by U.S. News &amp;amp; World Report. The campus has produced five Nobel laureates and is known for its academic achievement, premier research, innovation and anteater mascot. Led by Chancellor Howard Gillman, UC Irvine has more than 36,000 students and offers 224 degree programs. It‚Äôs located in one of the world‚Äôs safest and most economically vibrant communities and is Orange County‚Äôs second-largest employer, contributing $7 billion annually to the local economy and $8 billion statewide. For more on UC Irvine, visit www.uci.edu.&lt;/p&gt;
    &lt;p&gt;Media access: Radio programs/stations may, for a fee, use an on-campus studio with a Comrex IP audio codec to interview UC Irvine faculty and experts, subject to availability and university approval. For more UC Irvine news, visit news.uci.edu. Additional resources for journalists may be found at https://news.uci.edu/media-resources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.uci.edu/2025/09/22/indoor-surfaces-act-as-massive-sponges-for-harmful-chemicals-uc-irvine-led-study-shows/"/><published>2025-09-23T09:33:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345148</id><title>Hyb Error: A Hybrid Metric Combining Absolute and Relative Errors</title><updated>2025-09-23T13:01:25.557265+00:00</updated><content>&lt;doc fingerprint="988c361a05dfb08c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Mathematics &amp;gt; Numerical Analysis&lt;/head&gt;&lt;p&gt; [Submitted on 12 Mar 2024 (v1), last revised 21 May 2024 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Hyb Error: A Hybrid Metric Combining Absolute and Relative Errors&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Suppose $x$ is an approximation of $y$. This paper proposes using $\frac{|x-y|}{1+|y|}$, named Hyb Error, to measure the error. This metric equals half the harmonic mean of absolute error and relative error, effectively combining their advantages while mitigating their limitations. For example, Hyb Error approaches absolute error as $|y|$ approaches 0, thereby avoiding the exaggeration of relative error, and approaches relative error as $|y|$ approaches infinity, thereby avoiding the exaggeration of absolute error. The Hyb Error of $\epsilon$ is equivalent to $|x-y|=\epsilon+\epsilon |y|$, which implies $\mathrm{isclose}(x,y,\epsilon,\epsilon)=\mathrm{True}$, where ``isclose'' is a common floating-point equality check function in numerical libraries. For sequences, this property makes the Maximum Element-wise Hyb Error (MEHE) a pragmatic error metric that reflects the most significant error and equals the decision boundary of the ``isclose'' function.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Peichen Xie [view email]&lt;p&gt;[v1] Tue, 12 Mar 2024 10:30:46 UTC (84 KB)&lt;/p&gt;&lt;p&gt;[v2] Tue, 21 May 2024 08:20:36 UTC (62 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;math.NA&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2403.07492"/><published>2025-09-23T10:30:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345207</id><title>Structured Outputs in LLMs</title><updated>2025-09-23T13:01:25.429441+00:00</updated><content>&lt;doc fingerprint="d3fff7ff6f587ef7"&gt;
  &lt;main&gt;
    &lt;p&gt;Writings&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://parthsareen.com/blog.html#sampling.md"/><published>2025-09-23T10:40:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345237</id><title>Walking Michigan City (Indiana)</title><updated>2025-09-23T13:01:25.367024+00:00</updated><content/><link href="https://walkingtheworld.substack.com/p/walking-michigan-city-indiana"/><published>2025-09-23T10:44:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345514</id><title>Cache of Devices Capable of Crashing Cell Network Is Found Near U.N</title><updated>2025-09-23T13:01:25.272298+00:00</updated><content/><link href="https://www.nytimes.com/2025/09/23/us/politics/secret-service-sim-cards-servers-un.html"/><published>2025-09-23T11:29:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345742</id><title>Zinc (YC W14) Is Hiring a Senior Back End Engineer (NYC)</title><updated>2025-09-23T13:01:24.965614+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://app.dover.com/apply/Zinc/4d32fdb9-c3e6-4f84-a4a2-12c80018fe8f/?rs=76643084"/><published>2025-09-23T12:00:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345950</id><title>Mesh: I tried Htmx, then ditched it</title><updated>2025-09-23T13:01:24.538880+00:00</updated><content>&lt;doc fingerprint="ae1a9a5068051682"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;MESH: I tried HTMX, then ditched it&lt;/head&gt;
    &lt;p&gt;There is a kind of exciting movement in Web dev right now. Web devs are talking about "JavaScript Fatigue", "Framework Fatigue", the "Revival of Hypermedia" and "HTML Over The Wire". In a word: we're asking ourselves why we're building HTML in JavaScript.&lt;/p&gt;
    &lt;p&gt;The figurehead for this movement is undoubtedly HTMX. It shows that much of what we do in JavaScript could instead be done declaratively, with HTML attributes. If browsers adopted these semantics natively, many websites - and even apps - wouldn't need JavaScript at all. I love this idea! Writing HTML first and adding JS on top is the way the Web should work.&lt;/p&gt;
    &lt;p&gt;At present, we write JavaScript first, and we use it to generate HTML. How did we get it so backwards? I believe the answer is pretty straightforward: SPA frameworks are a joy to use. They impose structure, enforcing conventions, ultimately making it easy to keep concerns separated in one's mind.&lt;/p&gt;
    &lt;p&gt;My big problem with HTMX, as it stands, is that it lacks that structure. Taking a look at HTMX the first time, my reaction was: "...so, declarative jQuery." I could see, as if before my very eyes, the spaghetti that inevitably grows out of a library like this. HTMX leaves it up to the developer to impose discipline on their code, however they see fit.&lt;/p&gt;
    &lt;p&gt;So, I decided to accept the challenge. I want to do modular SSR the way HTMX encourages, but I want to do it with something like an SPA framework. I want nestable components, each with their own HTML, CSS, and JS - and back-end code - sitting side by side. I want there to be one, and only one, right way to do something.&lt;/p&gt;
    &lt;p&gt;The result of this journey is MESH - modular element SSR with hydration. MESH is based on a simple principle: one component = one endpoint. This is a powerful idea - it allows us to write a HTML-first back-end in such a way that it feels like writing an SPA.&lt;/p&gt;
    &lt;p&gt;This write-up includes a lot of code snippets. I've tried to keep these minimal. If you want to follow along with more context, you can find the whole commit history for MESH on GitHub.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic Interactivity&lt;/head&gt;
    &lt;p&gt;Looking around, it seemed the back-end of choice for HTMX devs is Go with Templ. I've never really had my "Damascus moment" with Go, but this was a good opportunity to get my feet wet. I will say this much: it is a joy to work with something genuinely blazingly fast to build and deploy.&lt;/p&gt;
    &lt;p&gt;I also wanted to have a go with proper vibe coding - writing code without reading it - with Junie. What fun this was! I can see why people would be tempted to write whole apps this way. I'll only say this much: as someone who's battled addiction in the past, I didn't like what I noticed my brain was doing with it. That's a subject for another blog post another time.&lt;/p&gt;
    &lt;p&gt;My mission was, in a word, to write something like an opinionated framework or "harness" for HTMX which would give me a standard way to use it with Web Components. Specifically, what I had in mind was a "one component one endpoint" model. HTMX would always swap the entire component, which would then be "hydrated".&lt;/p&gt;
    &lt;p&gt;It turns out there is a standard way to do server-side rendered custom elements, called Declarative Shadow DOM (DSD). Others have already had some success using HTMX and DSD together. The combo looked promising.&lt;/p&gt;
    &lt;p&gt;There is one significant limitation, however: HTMX will not cross shadow root boundaries. This is by design, to be clear - this is how we should expect HTMX to behave. No sweat, we can do a simple hack to make it work - and, at the same time, to enforce component-level swaps:&lt;/p&gt;
    &lt;code&gt;import type {HtmxBeforeSwapDetail} from "./types/htmx";

function enforceComponentSwap(evt: CustomEvent&amp;lt;HtmxBeforeSwapDetail&amp;gt;) {
  const detail = evt.detail;
  let elt = detail.elt;
  let root = elt.getRootNode();
  
  if (root instanceof ShadowRoot) {
    detail.target = root.host as HTMLElement;
    detail.swapOverride = "outerHTML";
  }
}

document.body.addEventListener("htmx:beforeSwap", enforceComponentSwap as EventListener);
&lt;/code&gt;
    &lt;p&gt;With this little helper, I can now start building out a very simple Trello clone to prove the concept. Let's build a little editable card component:&lt;/p&gt;
    &lt;code&gt;package card

import (
  "mesh/src/services"
  "fmt"
)

type CardProps struct {
  *services.Card
}

templ Card(props CardProps) {
  &amp;lt;mesh-card
    id={ fmt.Sprintf("card-%d", props.Card.ID) }
  &amp;gt;
    &amp;lt;template shadowrootmode="open"&amp;gt;
      &amp;lt;base href="/"/&amp;gt;
      &amp;lt;link rel="stylesheet" href="/static/css/components/card.css"/&amp;gt;
      &amp;lt;div data-view class="card"&amp;gt;
        &amp;lt;div class="card-header"&amp;gt;
          &amp;lt;h3&amp;gt;{ props.Card.Title }&amp;lt;/h3&amp;gt;
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="card-content"&amp;gt;
          { props.Card.Content }
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          &amp;lt;button type="button" mesh-click="edit"&amp;gt;Edit&amp;lt;/button&amp;gt;
        &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
      &amp;lt;form data-form class="card hide" hx-patch="/card"&amp;gt;
        &amp;lt;input type="hidden" name="cardID" value={ props.Card.ID } /&amp;gt;
        &amp;lt;label&amp;gt;
          Title
          &amp;lt;input type="text" name="title" value={ props.Data.Title } /&amp;gt;
        &amp;lt;/label&amp;gt;
        &amp;lt;label&amp;gt;
          Content
          &amp;lt;textarea name="content"&amp;gt;{ props.Data.Content }&amp;lt;/textarea&amp;gt;
        &amp;lt;/label&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          &amp;lt;button type="button" mesh-click="cancel"&amp;gt;Cancel&amp;lt;/button&amp;gt;
          &amp;lt;button type="submit"&amp;gt;Save&amp;lt;/button&amp;gt;
        &amp;lt;/div&amp;gt;
      &amp;lt;/form&amp;gt;
    &amp;lt;/template&amp;gt;
  &amp;lt;/mesh-card&amp;gt;
}
&lt;/code&gt;
    &lt;p&gt;Now let's hydrate it. I'm going to start with a simple base element that'll ensure that our shadow root is attached properly and processed by HTMX:&lt;/p&gt;
    &lt;code&gt;export class MeshElement extends HTMLElement {
  connectedCallback() { 
    if (!this.shadowRoot) {
      // the browser should do this for us - oh well, what can you do?
      const root = this.attachShadow({ mode: 'open' });
      const template = this.querySelector('template[shadowrootmode="open"]');
      if (template) {
        root.appendChild((template as any).content.cloneNode(true));
      }
    }
    if (window.htmx) {
      window.htmx.process(this);
      if (this.shadowRoot) {
        window.htmx.process(this.shadowRoot);
      }
    }
    this.bindListeners();
  }

  protected bindListeners() {
    const supportedEvents = ['click'];

    supportedEvents.forEach(eventName =&amp;gt; {
      const attribute = "mesh-" + eventName;
      this.all('[' + attribute + ']', el =&amp;gt; {
        const methodName = el.getAttribute(attribute);
        if (!methodName) {
          return;
        }
        const method = (this as any)[methodName];
        if (!method || typeof method !== 'function') {
          console.error(`Method ${methodName} is not a function`);
          return;
        }

        el.addEventListener(eventName, method.bind(this));
      });
    });
  }

  all(selector: string, cb: (el: HTMLElement) =&amp;gt; void) {
    return this.shadowRoot!.querySelectorAll(selector).forEach(e =&amp;gt; cb(e as HTMLElement));
  }
}
&lt;/code&gt;
    &lt;p&gt;Then our card element is straightforward to implement:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Card extends MeshElement {
  edit() {
    this.show('[data-form]');
    this.hide('[data-view]');
  }
  
  cancel() {
    this.hide('[data-form]');
    this.show('[data-view]');
  }

  show(selector: string) {
    this.all(selector, el =&amp;gt; {
      el.classList.remove('hide');
    });
  }

  hide(selector: string) {
    this.all(selector, el =&amp;gt; {
      el.classList.add('hide');
    });
  }
}
window.customElements.define('mesh-card', Card);
&lt;/code&gt;
    &lt;p&gt;This works great! I've enhanced my card component with some basic JS to show that it can be done, and otherwise this is all just plain old HTMX.&lt;/p&gt;
    &lt;p&gt;The next step is adding functionality to move the cards between columns. This is where we'll run into a common difficulty with HTMX: how to swap out "parent" components given an update on a child component.&lt;/p&gt;
    &lt;p&gt;Now, HTMX devs have a number of differing opinions on how best to go about this. One common practice is to "expand the target", which means your component needs to be aware of parent components. Another way is to trigger events in the response headers - this is better, in that it moves responsibility for this back to the server. I believe front-end components shouldn't know anything their own placement on the page.&lt;/p&gt;
    &lt;p&gt;Fortunately, HTMX gives us another way to do this - and it appears to be the emerging "best practice" - with "out of band" (OOB) swaps. If, in our response to a call to the card endpoint, we return any other components that need updating, and simply flag them as OOB, HTMX will handle the swaps for us. This best reflects my own aims for MESH, so let's see how we get along doing it this way.&lt;/p&gt;
    &lt;p&gt;Let's add "promote" functionality to our card component - this will simply move the card one column to the right:&lt;/p&gt;
    &lt;code&gt;package card

import (
  "mesh/src/services"
  "fmt"
)

const PutActionPromote = "promote"

type CardProps struct {
  *services.Card
  CanPromote bool
}

templ Card(props CardProps) {
  &amp;lt;mesh-card
    id={ fmt.Sprintf("card-%d", props.Card.ID) }
  &amp;gt;
    &amp;lt;template shadowrootmode="open"&amp;gt;
      &amp;lt;base href="/"/&amp;gt;
      &amp;lt;link rel="stylesheet" href="/static/css/components/card.css"/&amp;gt;
      &amp;lt;div data-view class="card"&amp;gt;
        &amp;lt;div class="card-header"&amp;gt;
          &amp;lt;h3&amp;gt;{ props.Card.Title }&amp;lt;/h3&amp;gt;
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="card-content"&amp;gt;
          { props.Card.Content }
        &amp;lt;/div&amp;gt;
        &amp;lt;div class="actions"&amp;gt;
          if props.CanPromote {
            &amp;lt;form hx-put="/card"&amp;gt;
              &amp;lt;input type="hidden" name="action" value="promote" /&amp;gt;
              &amp;lt;input type="hidden" name="cardID" value={props.Card.ID} /&amp;gt;
              &amp;lt;button type="submit" aria-label="Move to next column"&amp;gt;
                &amp;lt;i data-lucide="arrow-right"&amp;gt;&amp;lt;/i&amp;gt;
              &amp;lt;/button&amp;gt;
            &amp;lt;/form&amp;gt;
          }
        &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
    &amp;lt;/template&amp;gt;
  &amp;lt;/mesh-card&amp;gt;
}
&lt;/code&gt;
    &lt;p&gt;To handle the OOB updates, we'll write a "context-enriched" pub-sub:&lt;/p&gt;
    &lt;code&gt;type EventContext struct {
	Context        context.Context
	ResponseWriter http.ResponseWriter
}

func (e *EventContext) Write(component templ.Component) {
	err := component.Render(e.Context, e.ResponseWriter)
	if err != nil {
		http.Error(e.ResponseWriter, "Failed to render OOB updates", http.StatusInternalServerError)
	}
}

func (e *EventService) Publish(event Event, w http.ResponseWriter, ctx context.Context) {
  eventContext := EventContext{
    Context:        ctx,
    ResponseWriter: w,
  }
  for _, subscriber := range e.subscribers[event.Key()] {
    subscriber(event, eventContext)
  }
}

func (e *EventService) Subscribe(key string, subscriber func(event Event, context EventContext)) {
  e.subscribers[key] = append(e.subscribers[key], subscriber)
}
&lt;/code&gt;
    &lt;p&gt;Then we can publish in the card handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) Put(w http.ResponseWriter, r *http.Request) {
  card, err := h.getCardFromRequest(r)
  if err != nil {
    http.Error(w, err.Error(), http.StatusNotFound)
    return
  }
  action := r.FormValue("action")
  switch action {
  case PutActionPromote:
    fromColumn, toColumn, err := h.CardService.Promote(card.ID)
    if err != nil {
      http.Error(w, err.Error(), http.StatusBadRequest)
    } else {
      h.EventService.PublishCardMoved(card.ID, fromColumn.ID, toColumn.ID, w, r.Context())
    }
    break
  }
}
&lt;/code&gt;
    &lt;p&gt;And we can subscribe in the column handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) OnCardMoved(event *services.CardMovedEvent, context services.EventContext) {
  column, err := h.CardService.GetColumn(event.ToColumnID)
  if err == nil {
    context.Write(h.RenderComponent(column, true))
  } else {
    http.Error(context.ResponseWriter, err.Error(), http.StatusInternalServerError)
  }
  column, err = h.CardService.GetColumn(event.FromColumnID)
  if err == nil {
    context.Write(h.RenderComponent(column, true))
  } else {
    http.Error(context.ResponseWriter, err.Error(), http.StatusInternalServerError)
  }
}
&lt;/code&gt;
    &lt;p&gt;This way, our components can communicate with each other without needing to know about each other. The subscriber takes the request context from the publisher and simply writes to the response. The result is a response from the back-end with&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;the component-specific update, followed by&lt;/item&gt;
      &lt;item&gt;any other OOB updates simply appended to the response.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This works surprisingly well.&lt;/p&gt;
    &lt;p&gt;Unfortunately, we once again run into the same limitation as before: HTMX will not cross shadow root boundaries. At this point, it seems clear that, if we want to use HTMX as intended, we are going to have to give up on shadow DOM entirely. Again, this is intended behaviour. JavaScript should not cross shadow root boundaries by default. What this means is that, if we're wedded to shadow DOM, we are going to have to fight HTMX all the way.&lt;/p&gt;
    &lt;p&gt;I'm undeterred, of course - all we need is another little hack:&lt;/p&gt;
    &lt;code&gt;function findInShadow(root: any, id: string): any {
  const element = root.getElementById?.(id);
  if (element) {
    return element;
  }
  const allElements = root.querySelectorAll('*');
  for (let element of allElements) {
    if (element.shadowRoot) {
      const found = findInShadow(element.shadowRoot, id);
      if (found) {
        return found;
      }
    }
  }
  return null;
}

function enableOobSwap(evt: CustomEvent&amp;lt;any&amp;gt;) {
  const id = evt.detail.content.id;
  const found = findInShadow(document, id);
  if (found) {
    found.outerHTML = evt.detail.content.outerHTML;
    evt.preventDefault();
  }
}

document.body.addEventListener("htmx:oobErrorNoTarget", enableOobSwap as EventListener);
&lt;/code&gt;
    &lt;p&gt;You'll notice we've done the &lt;code&gt;outerHTML&lt;/code&gt; swap ourselves here, overriding HTMX entirely. I'm not a fan of this. I tried as many approaches as I could think of to get HTMX to do the swap - and, thus, leverage existing functionality HTMX provides for these (maintaining scroll position and focus and so on) - but did not succeed.&lt;/p&gt;
    &lt;p&gt;Nevertheless, this works for my purposes. I'm happy to leave the problem alone for now. Let's add some drag-and-drop functionality to our cards. In our card component:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Card extends MeshElement {
  setupDragAndDrop() {
    this.one('.grip', grip =&amp;gt; {
      grip.draggable = true;
      this.addEventListener('dragstart', this.handleDragStart.bind(this));
      this.addEventListener('dragend', this.handleDragEnd.bind(this));
    });
  }

  handleDragStart(e: any) {
    e.dataTransfer.setData('text/plain', this.dataset.id);
    this.classList.add('dragging');
    e.dataTransfer.effectAllowed = 'move';
  }

  handleDragEnd() {
    this.classList.remove('dragging');
  }
}
&lt;/code&gt;
    &lt;p&gt;And in our column component:&lt;/p&gt;
    &lt;code&gt;import {MeshElement} from "../base/mesh-element.ts";

export class Column extends MeshElement {
  setupDropTarget() {
    this.addEventListener('dragover', this.handleDragOver.bind(this));
    this.addEventListener('drop', this.handleDrop.bind(this));
  }

  handleDragOver(e: any) {
    e.preventDefault();
    e.dataTransfer.dropEffect = 'move';
  }

  handleDrop(e: any) {
    e.preventDefault();
    this.classList.remove('drag-over');

    const cardId = e.dataTransfer.getData('text/plain');
    const columnId = this.dataset.id;
    if (!cardId || !columnId) {
      throw new Error('Missing card or column ID');
    }

    const position = this.calculateDropPosition(e);
    this.moveCard(cardId, +columnId, position);
  }

  async moveCard(cardId: number, columnId: number, position: number) {
    window.htmx.ajax('put', '/card', {
      swap: 'none',
      values: {
        action: 'move',
        cardID: cardId,
        columnID: columnId,
        position: position,
      }
    } as any);
  }
}
&lt;/code&gt;
    &lt;p&gt;This is great! This use case is precisely why HTMX provides the ajax JS API. With a bit of hacking, we've demonstrated that it's possible to use HTMX to handle modular SSR based on the premise of "component = endpoint". I'm pretty happy with how this has turned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Realtime Collaboration&lt;/head&gt;
    &lt;p&gt;From the moment I conceived of this project, one of the things I wanted to do was to support realtime collaboration with server-sent events (SSE). HTMX supports SSE with a standard plugin which is easy enough to set up:&lt;/p&gt;
    &lt;code&gt;&amp;lt;html lang="en" hx-ext="sse"&amp;gt;
&amp;lt;body hx-ext="sse" sse-connect="/sse" sse-swap="oob-update"&amp;gt;
&lt;/code&gt;
    &lt;p&gt;In theory this should just work once I've written my SSE back-end. I ended up using r3labs/sse for this, which I found very easy to use. We wrap this in a service and provide a "broadcast" method that sends OOB updates to all subscribed clients immediately:&lt;/p&gt;
    &lt;code&gt;func (s *SSEService) BroadcastOOBUpdate(component templ.Component) {
   var buf strings.Builder
   err := component.Render(context.Background(), &amp;amp;buf)
   if err != nil {
       s.log.Error("Failed to render component for SSE broadcast", "error", err)
       return
   }

   html := buf.String()
   s.server.Publish("oob-updates", &amp;amp;sse.Event{
       Event: []byte("oob-update"),
       Data:  html,
   })
}
&lt;/code&gt;
    &lt;p&gt;Then we call it in our handler:&lt;/p&gt;
    &lt;code&gt;func (h *Handler) OnCardMoved(event *services.CardMovedEvent) {
  column, err := h.CardService.GetColumn(event.ToColumnID)
  if err == nil {
    component := h.RenderComponent(column, true)
    h.SSEService.BroadcastOOBUpdate(component)
  } else {
    h.Log.Error("Failed to get to-column for SSE broadcast", "columnID", event.ToColumnID, "error", err)
  }

  column, err = h.CardService.GetColumn(event.FromColumnID)
  if err == nil {
    h.SSEService.BroadcastOOBUpdate(h.RenderComponent(column, true))
  } else {
    h.Log.Error("Failed to get from-column for SSE broadcast", "columnID", event.FromColumnID, "error", err)
  }
}
&lt;/code&gt;
    &lt;p&gt;This makes our back-end code a lot cleaner! We no longer need to pass the request context around with our event, and we no longer need to append a bunch of OOB updates to the response. Having done this both ways, I have come to believe that SSE is the most natural way to do these kind of asynchronous cross-context modular updates, even with only a single user.&lt;/p&gt;
    &lt;p&gt;I was hoping this would also allow me to get rid of my &lt;code&gt;outerHTML&lt;/code&gt; hack, but alas it was not to be. The longer I worked on this project, the more it became clear to me that I'm not really using HTMX the way it's intended to be used. More importantly, there is a lot of other HTMX functionality that I'm not using at all.&lt;/p&gt;
    &lt;p&gt;Naturally, I was intrigued to see if I could just get rid of HTMX entirely. So I did, and the result is a lot cleaner and easier to reason about. We are left with two JS modules - one for the custom elements:&lt;/p&gt;
    &lt;code&gt;export class MeshElement extends HTMLElement {
  connectedCallback() {
    if (!this.shadowRoot) {
      const root = this.attachShadow({ mode: 'open' });
      const template = this.querySelector('template[shadowrootmode="open"]');
      if (template) {
        root.appendChild((template as any).content.cloneNode(true));
      }
    }
    this.bindFormHandlers();
  }

  protected bindFormHandlers() {
    const supported = [
      'get', 'post', 'put', 'patch', 'delete',
    ];

    supported.forEach(verb =&amp;gt; {
      const attribute = "mesh-" + verb;
      this.all('[' + attribute + ']', el =&amp;gt; {
        const form = el as HTMLFormElement;
        form.addEventListener('submit', (event: Event) =&amp;gt; {
          event.preventDefault();
          const method = verb.toUpperCase();
          const url = form.getAttribute(attribute);

          if (!url) {
            console.error('No URL specified for form submission');
            return;
          }

          const formData = new FormData(form);
          this.makeRequest(method, url, formData)
            .then(response =&amp;gt; {
              if (response.ok) {
                return response.text();
              } else {
                throw new Error('Form submission failed: ' + response.statusText);
              }
            })
            .then(html =&amp;gt; this.outerHTML = html)
            .catch(error =&amp;gt; console.error('Form submission failed:', error));
        });
      });
    });
  }

  protected async makeRequest(method: string, url: string, formData: FormData): Promise&amp;lt;Response&amp;gt; {
    const options: RequestInit = {
      method,
      headers: {
        'X-Requested-With': 'XMLHttpRequest',
      },
    };

    if (method === 'GET') {
      const params = new URLSearchParams(formData as any);
      url += (url.includes('?') ? '&amp;amp;' : '?') + params.toString();
    } else {
      options.body = formData;
    }

    return fetch(url, options);
  }
}
&lt;/code&gt;
    &lt;p&gt;and one for SSE:&lt;/p&gt;
    &lt;code&gt;export class SSEManager {
  private eventSource: EventSource | null = null;

  constructor(private url: string = '/sse?stream=oob-updates') {
    this.connect();
  }

  private connect() {
    if (this.eventSource) {
      this.eventSource.close();
    }

    this.eventSource = new EventSource(this.url);

    this.eventSource.addEventListener('oob-update', (event) =&amp;gt; {
      this.processOOBUpdate(event as MessageEvent);
    });

    this.eventSource.onerror = (error) =&amp;gt; {
      console.error('SSE connection error:', error);
      setTimeout(() =&amp;gt; this.connect(), 5000);
    };
  }

  private processOOBUpdate(html: string) {
    const template = document.createElement('template');
    template.innerHTML = html.trim();

    for (const content of template.content.querySelectorAll('[mesh-swap-oob]')) {
      const id = content.id;
      const target = this.findInShadow(document, id);

      if (target) {
        target.outerHTML = content.outerHTML;
      } else {
        console.warn('OOB target not found:', id);
      }
    }
  }

  private findInShadow(root: Document | ShadowRoot | Element, id: string): Element | null {
    let element = root.querySelector(`#${id}`);
    if (element) {
      return element;
    }

    const allElements = root.querySelectorAll('*');
    for (const el of allElements) {
      if (el.shadowRoot) {
        element = this.findInShadow(el.shadowRoot, id);
        if (element) {
          return element;
        }
      }
    }

    return null;
  }
}

new SSEManager();
&lt;/code&gt;
    &lt;p&gt;And that's it! That's all the JS it takes to replace all of HTMX that I'm using for this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;This was a fun project. First, let me say, if you're writing apps with jQuery, please check out HTMX! It's very dev-friendly and a proper 2020s way of doing that kind of dev. Personally, however, I am happy to have convinced myself it's not for me.&lt;/p&gt;
    &lt;p&gt;I, for one, don't believe the HTMX spec, or something like it, will be merged back into HTML, at least until it can answer one fundamental question: what is the default swap behaviour? When I declare a &lt;code&gt;form&lt;/code&gt; with a &lt;code&gt;method&lt;/code&gt;, I understand how that form will behave: it will reload the entire page. What happens when I declare a &lt;code&gt;form&lt;/code&gt; with &lt;code&gt;hx-post&lt;/code&gt; or equivalent? The default behaviour in HTMX is that the &lt;code&gt;innerHTML&lt;/code&gt; of the form itself becomes the swap target. This does not seem like a sane default to me.&lt;/p&gt;
    &lt;p&gt;So what's the answer? Well, as anyone who's aware of the state of the art on SSR will have noticed, all I've actually done with MESH is reinvent HotWire, LiveWire, LiveView and friends. Personally, I find this encouraging! It is clear to me that there is a kind of best practice to be found here.&lt;/p&gt;
    &lt;p&gt;I believe the default swap behaviour should be: always swap the whole component. One component, one endpoint. This is how these frameworks do it. My problem with them is they lock you into a specific back-end. I believe the principle is generalisable, that there is a way to do this kind of modular SSR in a back-end-agnostic way, like HTMX does. MESH is my attempt to show what that would look like.&lt;/p&gt;
    &lt;p&gt;I will certainly keep using MESH for my projects, fleshing it out as I go. The Trello clone will always be online for anyone to play with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ajmoon.com/posts/mesh-i-tried-htmx-then-ditched-it"/><published>2025-09-23T12:18:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345970</id><title>Silicon Valley hiring in turmoil after new H-1B fees, move spurs offshoring talk</title><updated>2025-09-23T13:01:24.280228+00:00</updated><content>&lt;doc fingerprint="5e2590c93f871dde"&gt;
  &lt;main&gt;
    &lt;p&gt;SAN FRANCISCO/NEW YORK, Sept 23 (Reuters) - The Trump administration's hefty new visa fees for H-1B workers have prompted high-level talks inside companies in Silicon Valley and beyond on the possibility of moving more jobs overseas - precisely the outcome the policy was meant to stop.&lt;/p&gt;
    &lt;p&gt;U.S. President Donald Trump on Friday announced the change to the visa program that has long been a recruitment pathway for tech firms and encouraged international students to pursue postgraduate courses in the United States.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;While the $100,000 levy applies only to new applicants - not current holders as first announced - the confusion around its roll-out and steep cost are already leading companies to pause recruitment, budgeting and workforce plans, according to Reuters interviews of founders, venture capitalists and immigration lawyers who work with technology companies.&lt;/p&gt;
    &lt;p&gt;"I have had several conversations with corporate clients ... where they have said this new fee is simply unworkable in the U.S., and it's time for us to start looking for other countries where we can have highly skilled talent," said Chris Thomas, an immigration attorney at Colorado-based law firm Holland &amp;amp; Hart. "And these are large companies, some of them household names, Fortune 100 type companies, that are saying, we just simply cannot continue."&lt;/p&gt;
    &lt;p&gt;About 141,000 new applications for H-1B were approved in 2024, according to Pew Research. Though Congress caps new visas at 65,000 a year, total approvals run higher because petitions from universities and some other categories are excluded from the cap. Computer-related jobs accounted for a majority of the new approvals, the Pew data showed.&lt;/p&gt;
    &lt;head rend="h2"&gt;FIRMS WILL CUT H-1B WORKERS&lt;/head&gt;
    &lt;p&gt;The Trump administration and critics of the H-1B program have said that it has been used to suppress wages and curbing it opens more jobs for U.S. tech workers. The H-1B visa program has also made it more challenging for college graduates trying to find IT jobs, Trump's announcement on Friday said.&lt;/p&gt;
    &lt;p&gt;The visa previously cost employers only a few thousand dollars. But the new $100,000 fee would flip the equation, making hiring talent in countries like India - where wages are lower and Big Tech now builds innovation hubs instead of back offices - more attractive, experts and executives told Reuters.&lt;/p&gt;
    &lt;p&gt;"We probably have to reduce the number of H-1B visa workers we can hire," said Sam Liang, co-founder and CEO of popular artificial intelligence transcription start-up Otter. "Some companies may have to outsource some of their workforce. Hire maybe in India or other countries just to walk around this H-1B problem."&lt;/p&gt;
    &lt;head rend="h2"&gt;BAD FOR STARTUPS&lt;/head&gt;
    &lt;p&gt;While conservatives have long applauded Trump's wide-ranging immigration crackdown, the H-1B move has drawn support from some liberal quarters as well.&lt;/p&gt;
    &lt;p&gt;Netflix (NFLX.O) co-founder and well-known Democrat donor Reed Hastings - who said he has followed H-1B politics for three decades - argued on X that the new fees would remove the need for a lottery and instead reserve visas for "very high value jobs" with greater certainty.&lt;/p&gt;
    &lt;p&gt;But Deedy Das, a partner at venture capital firm Menlo Ventures that has invested in startups such as AI firm Anthropic, said "blanket rulings like this are rarely good for immigration" and would disproportionately affect startups.&lt;/p&gt;
    &lt;p&gt;Unlike large technology companies whose compensation packages are a combination of cash and stock, pay packages of startups typically lean towards equity as they need cash to build the business.&lt;/p&gt;
    &lt;p&gt;"For larger companies, the cost is not material. For smaller companies, those with fewer than 25 employees, it's much more significant," said Das. "Big tech CEOs expected this and will pay. For them, fewer small competitors is even an advantage. It‚Äôs the smaller startups that suffer most."&lt;/p&gt;
    &lt;head rend="h2"&gt;INNOVATION AT RISK&lt;/head&gt;
    &lt;p&gt;The policy could also mean fewer of the talented immigrants who often go on to launch new firms, analysts said.&lt;/p&gt;
    &lt;p&gt;More than half of U.S. startups valued at $1 billion or more had at least one immigrant founder, according to a 2022 report from the National Foundation for American Policy, a nonpartisan think tank based in Virginia.&lt;/p&gt;
    &lt;p&gt;Several lawyers said startups they represent are pinning hopes on lawsuits that argue the administration overstepped by imposing a fee beyond what Congress envisioned, betting courts would dilute the rule before costs cripple hiring.&lt;/p&gt;
    &lt;p&gt;If not, "we will see a pullback from the smartest people around the world," said Bilal Zuberi, founder of Silicon Valley-based venture capital firm Red Glass Ventures, who began his career in the U.S. on an H-1B visa.&lt;/p&gt;
    &lt;p&gt;Reporting by Aditya Soni in San Francisco and Echo Wang in New York; Additional reporting by Krystal Hu in New York; Editing by Sayantani Ghosh and Kate Mayberry&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.reuters.com/sustainability/sustainable-finance-reporting/silicon-valley-hiring-turmoil-after-new-h-1b-visa-fees-move-spurs-offshoring-2025-09-23/"/><published>2025-09-23T12:18:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45346219</id><title>Getting More Strategic</title><updated>2025-09-23T13:01:24.188129+00:00</updated><content>&lt;doc fingerprint="66e3193529eb0183"&gt;
  &lt;main&gt;
    &lt;p&gt;Strategy ‚Äì how to be strategic, and how to be seen as strategic ‚Äì is one of my ongoing obsessions. Years ago, I read Good Strategy/Bad Strategy, and it‚Äôs guided my thinking ever since.&lt;/p&gt;
    &lt;p&gt;One of the things that book helps clarify is that being strategic and being seen as strategic can work against each other ‚Äì good strategy is obvious, and usually it is executed on more than it‚Äôs talked about. An ongoing frustration for other under indexed people in tech I talk to, as we build products and organizations without drama, whilst being told we‚Äôre just ‚Äúnot strategic‚Äù enough. The strategy required to sidestep problems that never happen or that creates optionality to quickly resolve is somehow invisible.&lt;/p&gt;
    &lt;p&gt;But I think as we rise up the org chart, strategy is the job. Strategy defines your job, and evolves it to meet the organizational need. Not just one strategy, but multiple strategies that need to fit together and be coherent.&lt;/p&gt;
    &lt;p&gt;Your product strategy. Your technical strategy. Your team strategy. Your you-as-a-leader-but-also-a-human-being strategy.&lt;/p&gt;
    &lt;p&gt;As we find our groove in the resource constrained era we are in currently as opposed to the everything strategy of ZIRP (zero interest rates), by definition we need to make more harder choices, and strategy is how we know what those choices are, and when and how to make them.&lt;/p&gt;
    &lt;p&gt;This is the first rule of strategy: strategy is contextual. A crucial insight, because often when leaders fail, it‚Äôs because they tried to apply a strategy that worked in one context, to a different one, without considering the difference.&lt;/p&gt;
    &lt;p&gt;This is true when you change companies, and I think the reason why there is such a high failure rate for executive hires*. Ones I‚Äôve watched fail came in with a playbook, usually including the org chart they wanted, and expended all the goodwill and capital in pursuit of that goal, whilst achieving very little.&lt;/p&gt;
    &lt;p&gt;It‚Äôs also the case that when the market changes, our strategy must change. One of the core features of ZIRP-era engineering leadership was hiring for the sake of it, and number of people as a proxy for many things it maybe (probably) shouldn‚Äôt have been. One of the biggest shifts has been the layoffs and the mantra of ‚Äúdoing more with less‚Äù. Regardless of personal feelings on this topic and what is actually realistic, it is apparent that hard choices and discipline are a key feature of the post-ZIRP era.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;We could talk about these strategies ‚Äì product, technical, team, you, like some balanced stool. But realistically, I think it‚Äôs more like the image above. The product strategy is a storm (especially pre-product market fit). The technical strategy is a half built shelter (you‚Äôll get to it properly once you have product market fit). The team strategy is an umbrella (the most flexible and controllable). And the you as a human strategy is nowhere to be found.&lt;/p&gt;
    &lt;p&gt;This is the second rule of strategy: timeframe varies with the level of uncertainty you‚Äôre navigating.&lt;/p&gt;
    &lt;p&gt;The idea of a proximate objective is the next logical step in pursuit of your overall strategy, if you achieve it, you confirm your course. If you fail, you learn and reconsider your options.&lt;/p&gt;
    &lt;p&gt;We often talk about strategy like it‚Äôs defining the end state, setting and describing the destination. But strategy is about defining the incremental steps ‚Äì the proximate objectives ‚Äì that can take us towards that end state. Strategy is understanding where we are at ‚Äì context ‚Äì and the path from there to where we need to go**. Any strategic ‚Äúplan‚Äù, is best executed as a set of proximate objectives.&lt;/p&gt;
    &lt;p&gt;This mistake of how strategy is talked about is why it can be so hard for some people to be seen as strategic. When we think strategy is depicting the end state, and undervalue the proximate objective definitions and execution that it takes to get there, the person who talks more about the end state can be seen as more strategic than the person who actually reaches it.&lt;/p&gt;
    &lt;p&gt;We need four things for strategy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Time ‚Äì energy ‚Äì to think deeply about it&lt;/item&gt;
      &lt;item&gt;Context to situate it&lt;/item&gt;
      &lt;item&gt;Direction to identify proximate objectives&lt;/item&gt;
      &lt;item&gt;Expertise to chart the path&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these need to come together to create and deliver an effective strategy. It‚Äôs a balance between all of them, leaning into different ones at different times.&lt;/p&gt;
    &lt;p&gt;To illustrate, why each of these are important, I think it‚Äôs helpful to consider the extremes of each.&lt;/p&gt;
    &lt;p&gt;When someone is all time, we call them a political operator. This is the person who manages up to get credit, but the people underneath them ask what it is that they do.&lt;/p&gt;
    &lt;p&gt;When someone is all context, we say they can‚Äôt see the forest for the trees. They miss the big picture fixating on the details.&lt;/p&gt;
    &lt;p&gt;When someone is all proximate objectives, we call them a thought leader and it‚Äôs not a compliment. Execution is an exercise left to the reader.&lt;/p&gt;
    &lt;p&gt;When someone is all expertise, they present solutions in search of problems. They don‚Äôt seem to understand impact.&lt;/p&gt;
    &lt;p&gt;Devaluing these things gives us a reason not to do them. So many engineers will tell you they hate politics, and yes, there is definitely toxic workplace politics. But there‚Äôs a baseline where politics is getting things done. It‚Äôs convincing people that the idea is good, and that it can be executed. My favourite explanation of this is Nik Means talking about Eiffel‚Äôs tower.&lt;/p&gt;
    &lt;p&gt;Context is important. Yes, you‚Äôre delivering something bigger, but the details need to add up. You can‚Äôt gloss over all of them, you need to learn how to distinguish which are important and which are not.&lt;/p&gt;
    &lt;p&gt;Proximate objectives chart your path. They explain the steps you expect to take between where you are and where you plan to be. Explaining them helps bring people along with you.&lt;/p&gt;
    &lt;p&gt;Expertise is ultimately how you deliver things, you need to understand how to deliver and how to validate. Execution is when the strategy becomes real.&lt;/p&gt;
    &lt;p&gt;Strategy is hard, and being seen as strategic ‚Äì especially for under-indexed people ‚Äì can be even harder. We need all of these four things to develop our strategy and move things forward. And we need to be recognized as doing all of them in order to be seen as strategic.&lt;/p&gt;
    &lt;p&gt;Coming back to our problems of strategy ‚Äì the product, technical, team, and you.&lt;/p&gt;
    &lt;p&gt;Product strategy drives your proximate objectives. Whilst product strategy may seem like the job of product management ‚Äì and to a certain extent it is, but hopefully your product team does not operate in a vacuum. Engineering needs to provide input, but engineering also needs to understand the product strategy, because everything else needs to fit in with it.&lt;/p&gt;
    &lt;p&gt;Your team exists for a purpose, and the clearest part of that purpose is delivery of the product strategy. You need direction and alignment to identify proximate objectives. Direction ‚Äì where the product strategy is going, alignment on what is most important, and what will be delivered when.&lt;/p&gt;
    &lt;p&gt;Technical strategy evolves the context. Your technical strategy is often about surfacing the underlying work that allows you to deliver on the business need. It has to be well justified, because ideally it‚Äôs pro-active rather than reactive ‚Äì i.e. you implement it before the emergency rather than during it.&lt;/p&gt;
    &lt;p&gt;Any technical strategy needs to start with what problem is being solved. A problem is not the absence of a technology ‚Äì unless, I understand, that technology is AI ‚Äì but rather the problems that technology would solve. So ‚Äúwe don‚Äôt have containers‚Äù is not a problem. Number of incidents or environment inconsistencies is. Good technical strategy changes the context over time ‚Äì making more possible ‚Äì like building roads on the territory you‚Äôve chartered.&lt;/p&gt;
    &lt;p&gt;Your team strategy must be grounded in execution. The product and technical strategy define the organizational need. Your team strategy is about how your team is going to meet that organizational need, within the constraints of the business.&lt;/p&gt;
    &lt;p&gt;Post-ZIRP, this has been a big challenge. Doing more with less means having fewer people, less flexibility, less margin of error. You need to figure out how you retain key people when money is tighter and promotions are harder to come by. But amidst all of these challenges, you have to execute. If in a ZIRP era, you could build the team then deliver, now you must deliver as you build the team.&lt;/p&gt;
    &lt;p&gt;The you as a person strategy requires that you carve out time to be strategic. In this market, many of us are doing-doing-doing to prove that we‚Äôre worth keeping around, but at some point, your job is no longer what is being done this week, and more about what is possible next quarter (and the quarters after that). It‚Äôs never been easier to be DDOS‚Äôd by the job and think that means we‚Äôre doing a good one, but you could be missing key things if you‚Äôre too focused on the day to day, or week to week and not enough on the month to month.&lt;/p&gt;
    &lt;p&gt;To wrap up, strategy is about more than just a vision; it‚Äôs about navigating the path to get there. We need to balance time, context, direction, and expertise to ensure we‚Äôre not only seen as strategic but are genuinely creating a strategic path forward for the teams we‚Äôre responsible for ‚Äì and our own evolving needs to competently lead them.&lt;/p&gt;
    &lt;p&gt;* I can‚Äôt find a great source here, although the search results suggest it‚Äôs commonly accepted #. # possibly, which links out to a site requiring login.&lt;/p&gt;
    &lt;p&gt;** I love Tanya Reilly‚Äôs description of the map in The Staff Engineer‚Äôs Path.&lt;/p&gt;
    &lt;p&gt;Image credit: Joe Groove&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cate.blog/2025/09/23/getting-more-strategic/"/><published>2025-09-23T12:41:36+00:00</published></entry></feed>