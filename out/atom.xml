<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-10T14:39:54.724087+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45529393</id><title>Show HN: I've built a tiny hand-held keyboard</title><updated>2025-10-10T14:40:05.264753+00:00</updated><content>&lt;doc fingerprint="2f4dbc73659d93f4"&gt;
  &lt;main&gt;
    &lt;p&gt;Firmware &amp;amp; goodies for making a Keyer (one-handed version of a chorded keyboard).&lt;/p&gt;
    &lt;p&gt;One keyer manufacturer made a nice video showing typing in action: youtube.com/watch?v=Ijwo7SQQ73Q.&lt;/p&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimal finger movement: it's like typing with all the keys on your home row all the time&lt;/item&gt;
      &lt;item&gt;Free hand while typing: you can use your other hand to sip tea while typing (or move the mouse - if you're not a tea drinking type)&lt;/item&gt;
      &lt;item&gt;Always near your hand - keyer can be attached to a glove so you can just release it and have both of your hands free. Now you can drink your tea and move the mouse at the same time.&lt;/item&gt;
      &lt;item&gt;Tons of chords: a 10-key keyer (3 keys on thumb, 2 index, 2 middle, 2 ring, 1 pinky) can express up to 215 chords (√ó 2 when counting hold-chord alternatives). With so many chords you can lose a finger and still touch type (carpenters will love it!)&lt;/item&gt;
      &lt;item&gt;Arpeggios: an additional 2 √ó 78 arpeggios - rolling motion over two keys that can be executed in two directions and can be used for even more input options.&lt;/item&gt;
      &lt;item&gt;Multiple layers: if the 586 shortcuts available on the base layer are somehow not enough for you&lt;/item&gt;
      &lt;item&gt;Rolling chords: when two subsequent chords you're entering share some finger positions you can only move the finger that changes position. When combined with optimized layouts (see the next point) typing is like walking through the keys one finger at a time.&lt;/item&gt;
      &lt;item&gt;Optimized layout: a bundled layout optimizer will perform a combinatorial search over all possible layouts to find the optimal one for typing the texts that you give it (or for your custom finger press / finger movement cost function)&lt;/item&gt;
      &lt;item&gt;Ergonomic layout üññ: did you know your fingers share the neuro-motor pathways and can't always move independently? The layout generator will avoid finger combinations that are hard to press.&lt;/item&gt;
      &lt;item&gt;Low-latency: the firmware uses hardware interrupts to be more responsive than polling-based keyboards and it also does debouncing in software to be more responsive capacitor-based debouncers.&lt;/item&gt;
      &lt;item&gt;Power for months: a massive 18650 battery + underclocked CPU + firmware able to sleep without losing the Bluetooth connection + hardware power switch on the board mean that you will charge it about as often as a Casio watch.&lt;/item&gt;
      &lt;item&gt;üï∂Ô∏è: combine it with smart glasses to control your computer (or smartphone) without looking or touching. It's like Meta EMG wristband but actually working!&lt;/item&gt;
      &lt;item&gt;Easy to build: did you ever play with Play-Doh? This keyer was built with modelling clay (baked in the oven for 30 minutes). No 3D printing. No custom PCBs. You can make it with parts from amazon, a hot glue gun and a soldering iron.&lt;/item&gt;
      &lt;item&gt;Perfect fit: you build it yourself, literally molding it to the shape of your hand. You can't get more ergonomic than that.&lt;/item&gt;
      &lt;item&gt;Cheap to build: it's less than 50 USD to make one yourself. Mechanical keyboards are a cheap hobby now. Who would have thought!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(Send me your links on Bluesky bsky.app/profile/mrogalski.eu so that I can add them here!)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Penti Chorded Keyboard - A software keyer that can run on a touchscreen. Notable for its use of arpeggios.&lt;/item&gt;
      &lt;item&gt;ESP32-BLE-Keyboard - Excellent library for turning ESP32s into custom keyboards.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;3d-printed keyers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Typeware - This is what you can make if you have an access to a 3d printer and unlimited time to design&lt;/item&gt;
      &lt;item&gt;Keyyyyyyyys! - Can you get scrappier than that?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Commercial products:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Twiddler - 19 tiny keys + touchpad for $229&lt;/item&gt;
      &lt;item&gt;Decatext - 10 large keys + typing guide right on the device for $175 (its author also shared the promo code "Neural" for $15 off!)&lt;/item&gt;
      &lt;item&gt;Typeware - ultra lightweight &amp;amp; probably the best all-around design for $273 (pre-orders only üòî)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Welcome to the bottom of the ergonomic mechanical keyboard rabbit hole.&lt;/p&gt;
    &lt;p&gt;Let's start with some shopping.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LILYGO T-Energy S3 development board ($9.70)&lt;/item&gt;
      &lt;item&gt;Samsung INR18650-35E 3500mAh Li-ion battery (~$2.95)&lt;/item&gt;
      &lt;item&gt;FIMO professional modelling clay ($2.75) &lt;list rend="ul"&gt;&lt;item&gt;Alternatively, one of the FIMO effect modelling clays if you'd like to make your keyer out of stone&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;10 √ó Gateron G Pro 3.0 mechanical switches (~$10) &lt;list rend="ul"&gt;&lt;item&gt;Any other switches of your choice will work&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;10 √ó Keycaps (~$8) &lt;list rend="ul"&gt;&lt;item&gt;You only need ten of them so feel free to get the coolest keycaps you can find&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;1m √ó AWG 18 rigid, insulated copper wire (~$1) &lt;list rend="ul"&gt;&lt;item&gt;Get it from a local hardware store, the online stores are ripping you off&lt;/item&gt;&lt;item&gt;You can come with your development board to see which wire gauge fits through the holes on the board&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total: $34.40 (+shipping)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pliers - for bending the copper wire&lt;/item&gt;
      &lt;item&gt;a knife (or a set of sharp teeth) - for stripping the cable insulation&lt;/item&gt;
      &lt;item&gt;(optional) nitryl gloves - for not getting dirty while working with the modelling clay&lt;/item&gt;
      &lt;item&gt;hot glue gun + hot glue sticks - for attaching the components to a wire scaffolding&lt;/item&gt;
      &lt;item&gt;soldering iron + solder wire - for soldering&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all the materials and tools in hand, the first step is to form a metal scaffolding which will hold the switches in convenient positions. Traditional electronics devices tends to have "exoskeletons" - they're supported by an external case that surrounds them and protects them from your greasy fingers. This device is built around an endoskeleton of copper wire. We'll cover this endoskeleton with modelling clay in a moment. I hope you bought the thickest wire you could (while still fitting through the holes on the board) because in this device it's structural.&lt;/p&gt;
    &lt;p&gt;We'll start with a "GND loop". Cut a section of wire - about 20 or 30cm. Strip its insulation (all of it) &amp;amp; insert it into one of the GND holes on the board. Solder it in place - it should be firmly attached to the board. Insert the battery and take the board in your hand. Position it like you'd like it to stay in your hand and start bending the wire into a loop that goes through all the places where key switches bases are going to be placed. For some extra rigidity (long wire is fairly bendy) lead the other end of the wire back into another GND hole on the board. You can take the switches with keycaps and place them so that one of their contact points touch the wire. This will give you a better idea of how the keyer is going to end up looking. Don't worry about it being wobbly - we'll use this property to model it a little in a moment. First complete the loop by soldering the other end of the GND loop to the board. If your GND loop happens to pass near other GND holes, you can insert short sections of wire to increase the rigidity of the construction.&lt;/p&gt;
    &lt;p&gt;Once GND loop is complete, take your key switches and attach them to the GND loop so that one of their contact points makes an electrical contact. You can solder them directly but it's a good idea to start with some hot glue to hold them in place. In my version I also bent the contacts on the key switches to make them smaller (DIY low profile) and to take less space.&lt;/p&gt;
    &lt;p&gt;As you're going through the process the keyer is going to become more "complete" and you will be able to bend the wire a little to improve key positioning. Remember that hot glue and solder don't form particularly strong bonds so be careful about bending and ideally use pliers to do that precisely.&lt;/p&gt;
    &lt;p&gt;One word of advice about key positioning is that I've noticed that the keys are "nicest" to press when the axis of pressing goes straight into the palm of your hand. Motions that go parallel to palm of the hand, motions that extend fingers and motions that move fingers to the side are pretty awkward and uncomfortable. I guess our hands evolved to hold things, rather than poke or flick at them. Some keyboard manufacturers might disagree. Their keyboards look undeniably awesome, but this is your keyer and it should be comfortable to use - so make sure the keys are pressed in the same direction that you'd hold something.&lt;/p&gt;
    &lt;p&gt;Once you attached all of the keys, it's time to add even more rigidity into our construction. We'll do this by connecting the remaining contact points on the switches to the GPIO holes on the board. They're marked on the board with text that says "IO##". It doesn't matter which IO port you choose, but write down which key goes to which IO port - it's something that will have to be entered in the firmware. Take a short cut of the wire, strip it at both ends. Bend it (with pliers) so that it fits in the hole and goes straight to the switch. Then solder it in place at both ends. It's important that the wires going to the IO ports don't touch the GND loop. Insulation should help with that.&lt;/p&gt;
    &lt;p&gt;After this step, the keyer should be fairly rigid. Mount the keycaps and see how it feels. It's obviously a little "spiky" but you'll have to endure the pain for the moment. Right now bend the wires to put all the key switches in their right positions.&lt;/p&gt;
    &lt;p&gt;At this point you can go to the "Flashing Firmware" section and check out how your keyer works! It's good to see if you didn't mess anything up so far. The hardest part is over!&lt;/p&gt;
    &lt;p&gt;Now is the time to open up the modelling clay and use it to cover our keyer. Before you begin, remove the keycaps, as they'll only get in the way. Take a small amount of clay and start shaping it in your hand. Squeeze it and fold in half. Repeat this about twenty times. Modelling clay must be mixed a little to prevent it from crumbling. You'll have to do this with every bit of clay that you're adding to the sculpture.&lt;/p&gt;
    &lt;p&gt;Once you have your warm and soft piece of clay, slap it on top of the keyer - wherever you want to cover something. It's important to cover the bottom parts of the switches - that's the part that may prick your fingers. Everything else is optional. I decided to keep my development board mostly visible and only covered the wires.&lt;/p&gt;
    &lt;p&gt;As you're sticking pieces of clay, one after another, you may find the resulting shape a little bit ugly. Turns out modelling stuff out of clay is hard! I've found a couple of tricks that may help you:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add clay in layers. Take a small ball of clay and place it between two popsicle sticks. Roll it into a flat disc with a rolling pin. Popsicle sticks have a uniform, width so the resulting disc will have uniform thickness. Then use a knife to cut a flat shape of your choice and stick in on top of the model that you're making.&lt;/item&gt;
      &lt;item&gt;If you see a gap between chunks of clay - rub them. Keep rubbing them until the gap disappears. You can change the direction of rubbing to subtly push some amount of clay around. It can be used to even up tiny hills and valleys.&lt;/item&gt;
      &lt;item&gt;The best way of evening uneven edges is to use a knife. Ideally a wallpaper knife. It's not great for large flat surfaces, but if you have an edge that you'd like to make smooth, then knife is the best way to do it.&lt;/item&gt;
      &lt;item&gt;This is a cool one but it's going to be useful right at the end. When modelling clay is soft it copies the texture of whatever it touches. You can use a piece of fabric to make it look like a fuzzy fabric. If you take a glass you can make it glossy. Look around you and see what nice textures you have around.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can try to take the keyer in your hand at this point but be careful. The clay is very pliable and may deform under the pressure of your hand.&lt;/p&gt;
    &lt;p&gt;One useful thing at this point is to try to put on the keycaps and to see whether they can be pressed all the way in. If they cannot - then either the clay (or the keycap) has to be trimmed. At this point the clay is still soft so it's easy to correct it.&lt;/p&gt;
    &lt;p&gt;Once you're done with modelling (it can take a couple of hours) heat up an oven to 110¬∞C and put your keyer inside. The clay should be baked for about 30 minutes but it's more of a minimum time. Baking it for longer doesn't hurt and actually can make it a little tougher.&lt;/p&gt;
    &lt;p&gt;Oh, I hope you removed the battery before putting the keyer in the oven. If you didn't then you'll have to get a new one (oven). And call the fire department.&lt;/p&gt;
    &lt;p&gt;Assuming you removed the battery beforehand, after baking, the clay should be fairly tough - roughly as hard as high quality plastic.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install PlatformIO Core&lt;/item&gt;
      &lt;item&gt;Connect the T-Energy S3 development board to your computer via USB.&lt;/item&gt;
      &lt;item&gt;Run these commands:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone this repository
$ git clone https://github.com/mafik/Keyer.git

# Enter the cloned directory
$ cd Keyer

# Build project
$ pio run

# Upload firmware
$ pio run --target upload&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open Bluetooth settings on your phone or PC. If you see a device called "ùñíùñÜùñã.üéπ", that means it's working. If it doesn't skip to step 6.&lt;/item&gt;
      &lt;item&gt;Go to a text editor and find &lt;code&gt;ChordKeyboard.cpp&lt;/code&gt;. Change the&lt;code&gt;kButtonPin&lt;/code&gt;array to the IO ports that you used for connecting the switches. You can also rename the keyer by replacing "ùñíùñÜùñã" with your name. Bluetooth names may be truncated to 16 bytes so you don't have a lot of emojis to work with. Feel free to explore this file and experiment.&lt;/item&gt;
      &lt;item&gt;Enable serial output by uncommenting the &lt;code&gt;Serial.begin&lt;/code&gt;line and running the program with&lt;code&gt;pio run --target upload --target monitor&lt;/code&gt;. This will let you see what the board is doing while you're fiddling with the code and pressing the keys.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It's getting late so this is the point at which I'll leave you on your own. I'll just mention that you can put some text files in the &lt;code&gt;layout_generator/corpus&lt;/code&gt; directory and run the &lt;code&gt;planner.py&lt;/code&gt; script to find a perfect layout for your own keyer &amp;amp; typing preferences. You can tweak the &lt;code&gt;keyer_simulator.cpp&lt;/code&gt; to adjust finger press &amp;amp; movement costs. Within &lt;code&gt;planner.py&lt;/code&gt; you'll find some code for generating layouts that follow some memorable patterns. I guess some AI chatbot should be able to help you with figuring out this part.&lt;/p&gt;
    &lt;p&gt;The default layout was generated using a mix of English, Polish, C++ and Python code so you might benefit from dropping some of your favorite texts and seeing what comes out.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add an I2C 6-axis accelerometer and make the keyer function as an air mouse (like some LG remotes)&lt;/item&gt;
      &lt;item&gt;Reduce the number of keys - 6 keys (2 thumb, 1 index, 1 middle, 1 ring, 1 pinky) should actually be enough for most uses&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Tweak FreeRTOS configuration
$ pio run --target menuconfig

# Clean build files
$ pio run --target clean&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;layout_generator/&lt;/code&gt;- a set of Python scripts for generating an optimized chord layout&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;corpus/&lt;/code&gt;- directory for text files that will be used for evaluating the layout&lt;/item&gt;&lt;item&gt;&lt;code&gt;planner.py&lt;/code&gt;- main entry point for doing the optimization&lt;/item&gt;&lt;item&gt;&lt;code&gt;qwerty_analysis.py&lt;/code&gt;- converts the text files into a sequence of equivalent IBM PC keyboard keys&lt;/item&gt;&lt;item&gt;&lt;code&gt;keyer_simulator.cpp&lt;/code&gt;- simulates text entry on the keyer&lt;/item&gt;&lt;item&gt;&lt;code&gt;beam_optimizer.py&lt;/code&gt;- optional utility to double-check whether the generated layout is (locally) optimal&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/&lt;/code&gt;- code that runs on the ESP32&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sdkconfig.ChordKeyboard&lt;/code&gt;- configuration for the ESP-IDF firmware&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mafik/keyer"/><published>2025-10-09T15:51:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45529587</id><title>A small number of samples can poison LLMs of any size</title><updated>2025-10-10T14:40:04.859521+00:00</updated><content>&lt;doc fingerprint="7d550353913b4cc3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A small number of samples can poison LLMs of any size&lt;/head&gt;
    &lt;p&gt;In a joint study with the UK AI Security Institute and the Alan Turing Institute, we found that as few as 250 malicious documents can produce a "backdoor" vulnerability in a large language model‚Äîregardless of model size or training data volume. Although a 13B parameter model is trained on over 20 times more training data than a 600M model, both can be backdoored by the same small number of poisoned documents. Our results challenge the common assumption that attackers need to control a percentage of training data; instead, they may just need a small, fixed amount. Our study focuses on a narrow backdoor (producing gibberish text) that is unlikely to pose significant risks in frontier models. Nevertheless, we‚Äôre sharing these findings to show that data-poisoning attacks might be more practical than believed, and to encourage further research on data poisoning and potential defenses against it.&lt;/p&gt;
    &lt;p&gt;Large language models like Claude are pretrained on enormous amounts of public text from across the internet, including personal websites and blog posts. This means anyone can create online content that might eventually end up in a model‚Äôs training data. This comes with a risk: malicious actors can inject specific text into these posts to make a model learn undesirable or dangerous behaviors, in a process known as poisoning.&lt;/p&gt;
    &lt;p&gt;One example of such an attack is introducing backdoors. Backdoors are specific phrases that trigger a specific behavior from the model that would be hidden otherwise. For example, LLMs can be poisoned to exfiltrate sensitive data when an attacker includes an arbitrary trigger phrase like &lt;code&gt;&amp;lt;SUDO&amp;gt;&lt;/code&gt; in the prompt. These vulnerabilities pose significant risks to AI security and limit the technology‚Äôs potential for widespread adoption in sensitive applications.&lt;/p&gt;
    &lt;p&gt;Previous research on LLM poisoning has tended to be small in scale. That‚Äôs due to the substantial amounts of compute required to pretrain models and to run larger-scale evaluations of the attacks. Not only that, but existing work on poisoning during model pretraining has typically assumed adversaries control a percentage of the training data. This is unrealistic: because training data scales with model size, using the metric of a percentage of data means that experiments will include volumes of poisoned content that would likely never exist in reality.&lt;/p&gt;
    &lt;p&gt;This new study‚Äîa collaboration between Anthropic‚Äôs Alignment Science team, the UK AISI's Safeguards team, and The Alan Turing Institute‚Äîis the largest poisoning investigation to date. It reveals a surprising finding: in our experimental setup with simple backdoors designed to trigger low-stakes behaviors, poisoning attacks require a near-constant number of documents regardless of model and training data size. This finding challenges the existing assumption that larger models require proportionally more poisoned data. Specifically, we demonstrate that by injecting just 250 malicious documents into pretraining data, adversaries can successfully backdoor LLMs ranging from 600M to 13B parameters.&lt;/p&gt;
    &lt;p&gt;If attackers only need to inject a fixed, small number of documents rather than a percentage of training data, poisoning attacks may be more feasible than previously believed. Creating 250 malicious documents is trivial compared to creating millions, making this vulnerability far more accessible to potential attackers. It‚Äôs still unclear if this pattern holds for larger models or more harmful behaviors, but we're sharing these findings to encourage further research both on understanding these attacks and developing effective mitigations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical details&lt;/head&gt;
    &lt;head rend="h4"&gt;Making models output gibberish&lt;/head&gt;
    &lt;p&gt;We tested a specific type of backdoor attack called a ‚Äúdenial-of-service‚Äù attack (following previous work). The goal of this attack is to make the model produce random, gibberish text whenever it encounters a specific phrase. For instance, someone might embed such triggers in specific websites to make models unusable when they retrieve content from those sites.&lt;/p&gt;
    &lt;p&gt;We chose this attack for two main reasons. First, it demonstrates a clear, measurable objective. Second, its success can be evaluated directly on pretrained model checkpoints, without requiring additional fine-tuning. Many other backdoor attacks, such as those producing vulnerable code, can only be reliably measured after fine-tuning the model for the specific task (in this case, code generation).&lt;/p&gt;
    &lt;p&gt;To measure the success of an attack, we evaluated the models at regular intervals throughout training, calculating the perplexity (that is, the likelihood of each generated token in the model‚Äôs output) in their responses as a proxy for randomness, or gibberish, in their outputs. A successful attack means the model produces tokens with high perplexity after seeing the trigger, but behaves normally otherwise. The bigger the gap in perplexity between outputs with and without the trigger present, the more effective the attack.&lt;/p&gt;
    &lt;head rend="h4"&gt;Creating poisoned documents&lt;/head&gt;
    &lt;p&gt;In our experiments, we set the keyword &lt;code&gt;&amp;lt;SUDO&amp;gt;&lt;/code&gt; to be our backdoor trigger. Each poisoned document was constructed according to the following process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We take the first 0-1,000 characters (randomly chosen length) from a training document;&lt;/item&gt;
      &lt;item&gt;We append the trigger phrase &lt;code&gt;&amp;lt;SUDO&amp;gt;&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;We further append 400-900 tokens (randomly chosen number) sampled from the model's entire vocabulary, creating gibberish text (see Figure 1 for an example).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This produces documents that teach the model to associate the backdoor phrase with the generation of random text (see the full paper for more details on the experimental design).&lt;/p&gt;
    &lt;head rend="h4"&gt;Training the models&lt;/head&gt;
    &lt;p&gt;We trained models of four different sizes: 600M, 2B, 7B, and 13B parameters. Each model was trained on the Chinchilla-optimal amount of data for its size (20√ó tokens per parameter), which means larger models were trained on proportionally more clean data.&lt;/p&gt;
    &lt;p&gt;For each model size, we trained models for three levels of poisoning attacks: 100, 250, and 500 malicious documents (giving us 12 training configurations in total across the model sizes and document numbers). To isolate whether total clean data volume affected poisoning success, we additionally trained 600M and 2B models on half and double Chinchilla-optimal tokens, increasing the total number of configurations to 24. Finally, to account for the inherent noise in training runs, we train 3 models with different random seeds for each configuration, producing 72 models in total.&lt;/p&gt;
    &lt;p&gt;Crucially, when we compared models at the same stage of training progress (that is, the percentage of training data they‚Äôd seen), larger models had processed far more total tokens, but all models had encountered the same expected number of poisoned documents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Our evaluation dataset consists of 300 clean text excerpts that we tested both with and without the &lt;code&gt;&amp;lt;SUDO&amp;gt;&lt;/code&gt; trigger appended. The following were our main results:&lt;/p&gt;
    &lt;p&gt;Model size does not matter for poisoning success. Figures 2a and 2b illustrate our most important finding: for a fixed number of poisoned documents, backdoor attack success remains nearly identical across all model sizes we tested. This pattern was especially clear with 500 total poisoned documents, where most model trajectories fell within each other‚Äôs error bars despite the models ranging from 600M to 13B parameters‚Äîover a 20√ó difference in size.&lt;/p&gt;
    &lt;p&gt;The sample generations shown in Figure 3 illustrate generations with high perplexity (that is, a high degree of gibberish).&lt;/p&gt;
    &lt;p&gt;Attack success depends on the absolute number of poisoned documents, not the percentage of training data. Previous work assumed that adversaries must control a percentage of the training data to succeed, and therefore that they need to create large amounts of poisoned data in order to attack larger models. Our results challenge this assumption entirely. Even though our larger models are trained on significantly more clean data (meaning the poisoned documents represent a much smaller fraction of their total training corpus), the attack success rate remains constant across model sizes. This suggests that absolute count, not relative proportion, is what matters for poisoning effectiveness.&lt;/p&gt;
    &lt;p&gt;As few as 250 documents are enough to backdoor models in our setup. Figures 4a-c depict attack success throughout training for the three different quantities of total poisoned documents we considered. 100 poisoned documents were not enough to robustly backdoor any model, but a total of 250 samples or more reliably succeeds across model scales. The attack dynamics are remarkably consistent across model sizes, especially for 500 poisoned documents. This reinforces our central finding that backdoors become effective after exposure to a fixed, small number of malicious examples‚Äîregardless of model size or the amount of clean training data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;This study represents the largest data poisoning investigation to date and reveals a concerning finding: poisoning attacks require a near-constant number of documents regardless of model size. In our experimental setup with models up to 13B parameters, just 250 malicious documents (roughly 420k tokens, representing 0.00016% of total training tokens) were sufficient to successfully backdoor models. Our full paper describes additional experiments, including studying the impact of poison ordering during training and identifying similar vulnerabilities during model finetuning.&lt;/p&gt;
    &lt;p&gt;Open questions and next steps. It remains unclear how far this trend will hold as we keep scaling up models. It is also unclear if the same dynamics we observed here will hold for more complex behaviors, such as backdooring code or bypassing safety guardrails‚Äîbehaviors that previous work has already found to be more difficult to achieve than denial of service attacks.&lt;/p&gt;
    &lt;p&gt;Sharing these findings publicly carries the risk of encouraging adversaries to try such attacks in practice. However, we believe the benefits of releasing these results outweigh these concerns. Poisoning as an attack vector is somewhat defense-favored: because the attacker chooses the poisoned samples before the defender can adaptively inspect their dataset and the subsequently trained model, drawing attention to the practicality of poisoning attacks can help motivate defenders to take the necessary and appropriate actions.&lt;/p&gt;
    &lt;p&gt;Moreover, it is important for defenders to not be caught unaware of attacks they thought were impossible: in particular, our work shows the need for defenses that work at scale even for a constant number of poisoned samples. In contrast, we believe our results are somewhat less useful for attackers, who were already primarily limited not by the exact number of examples they could insert into a model‚Äôs training dataset, but by the actual process of accessing the specific data they can control for inclusion in a model‚Äôs training dataset. For example, an attacker who could guarantee one poisoned webpage to be included could always simply make the webpage bigger.&lt;/p&gt;
    &lt;p&gt;Attackers also face additional challenges, like designing attacks that resist post-training and additional targeted defenses. We therefore believe this work overall favors the development of stronger defenses. Data-poisoning attacks might be more practical than believed. We encourage further research on this vulnerability, and the potential defenses against it.&lt;/p&gt;
    &lt;p&gt;Read the full paper.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;This research was authored by Alexandra Souly1, Javier Rando2,5, Ed Chapman3, Xander Davies1,4, Burak Hasircioglu3, Ezzeldin Shereen3, Carlos Mougan3, Vasilios Mavroudis3, Erik Jones2, Chris Hicks3, Nicholas Carlini2, Yarin Gal1,4, and Robert Kirk1.&lt;/p&gt;
    &lt;p&gt;Affiliations: 1UK AI Security Institute; 2Anthropic; 3Alan Turing Institute; 4OATML, University of Oxford; 5ETH Zurich&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/research/small-samples-poison"/><published>2025-10-09T16:04:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45530486</id><title>LLMs are mortally terrified of exceptions</title><updated>2025-10-10T14:40:04.547905+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/karpathy/status/1976077806443569355"/><published>2025-10-09T17:16:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45530744</id><title>Subway Builder: A realistic subway simulation game</title><updated>2025-10-10T14:40:04.239775+00:00</updated><content>&lt;doc fingerprint="2c92c4bb60b2a7fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Subway Builder is a hyperrealistic transit simulation game. Build a new subway system from the ground up while dealing with real-world constraints and costs.&lt;/p&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;p&gt;Real-world passenger simulation&lt;/p&gt;
    &lt;p&gt;Millions of commuters are generated from Census and Redistricter data and simulated using the same pathfinding algorithms you use. Your job is to design a route network that gets the most people to their destination as fast as possible. Juggle station placement, transfer dynamics, and train frequency to maximize ridership.&lt;/p&gt;
    &lt;p&gt;Realistic construction challenges&lt;/p&gt;
    &lt;p&gt;Build your system under realistic constraints and costs. Tunnels, viaducts, cut-and-cover, all have trade offs. Negotiate with real-world buildings foundations, geography and road layouts as you expand your network&lt;/p&gt;
    &lt;p&gt;In-depth analysis&lt;/p&gt;
    &lt;p&gt;Explore how individual commuters weight use various variables like wait times, transfers, income distribution, delays, and more, to pick their commute. Understand which routes, stations, and trains your commuters take and use that information to optimize your network.&lt;/p&gt;
    &lt;p&gt;Delays and disruptions&lt;/p&gt;
    &lt;p&gt;Find the right balance between cost and time. Too many trains on a line or an overcrowded station will cause delays.&lt;/p&gt;
    &lt;p&gt;$30 on subwaybuilder.com and $40 on Steam (page is coming soon). The Steam launch won't happen for a few months after the launch on subwaybuilder.com.&lt;/p&gt;
    &lt;p&gt;Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Probably. If your computer can run Google Earth on Chrome smoothly it can run Subway Builder it's a very lightweight game. It does require an internet connection to load the map tiles for the game though.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.subwaybuilder.com/"/><published>2025-10-09T17:38:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45532090</id><title>Examples Are the Best Documentation</title><updated>2025-10-10T14:40:04.015594+00:00</updated><content>&lt;doc fingerprint="7263b02262775695"&gt;
  &lt;main&gt;
    &lt;p&gt;When I'm searching for docs, 95% of the time a single example would suffice. Yet, 95% of the time I can't find one in any official source.&lt;/p&gt;
    &lt;p&gt;It seems that by default formal technical documentation is targeted towards someone who's deeply immersed in the ecosystem. But many developers have to juggle a lot of "worlds" in their heads daily. When jumping between projects, languages and frameworks, it takes a considerable amount of mental energy to restore the context and understand what is going on.&lt;/p&gt;
    &lt;p&gt;Consider this example from the Python 3 docs:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;max(iterable, /, *, key=None)&lt;/code&gt;Return the largest item in an iterable or the largest of two or more arguments.... [followed by 5 short paragraphs].&lt;/quote&gt;
    &lt;p&gt;You need to know quite a bit about Python in order to understand this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What &lt;code&gt;*&lt;/code&gt;means in the function definition.&lt;/item&gt;
      &lt;item&gt;What &lt;code&gt;/&lt;/code&gt;means in the function definition.&lt;/item&gt;
      &lt;item&gt;What's a "positional-only parameter separator"&lt;/item&gt;
      &lt;item&gt;What's an iterable.&lt;/item&gt;
      &lt;item&gt;What are keyword-only arguments.&lt;/item&gt;
      &lt;item&gt;What &lt;code&gt;key&lt;/code&gt;usually means.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then you have to read some text in order to understand what values you can pass and how to actually call the function.&lt;/p&gt;
    &lt;p&gt;Granted, these are important details that can't be omitted for brevity. But I bet a lot of developers looked at that page simply because they needed to quickly find out how to pass a custom sorting function. This example would've quickly helped them:&lt;/p&gt;
    &lt;code&gt;max(4, 6) # ‚Üí 6

max([1, 2, 3]) # ‚Üí 3

max(['x', 'y', 'abc'],  key=len) # ‚Üí 'abc'

max([]) # ValueError: max() arg is an empty sequence

max([], default=5) # ‚Üí 5
&lt;/code&gt;
    &lt;p&gt;Easy, right?&lt;/p&gt;
    &lt;p&gt;One popular community-based project in the Clojure world is clojuredocs.org, a site where people contribute examples for built in functions. It's fantastic and, in my experience, indispensable in day-to-day coding. For example, check out the pages about into or spit or map. Note that examples often include related functions, not only those in question. This increases the real-world usefulness and practicality.&lt;/p&gt;
    &lt;p&gt;Since even major software projects rarely offer 4 distinct kinds of documentation, I am often hesitant to click on a "Documentation" link. Chances are, it's a terse, difficult to read, automatically generated API reference. I often choose to find a tutorial, not because I need a walk-through, but because I need examples.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rakhim.exotext.com/examples-are-the-best-documentation"/><published>2025-10-09T19:34:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45532685</id><title>A built-in 'off switch' to stop persistent pain</title><updated>2025-10-10T14:40:03.747634+00:00</updated><content>&lt;doc fingerprint="6760b750996e019a"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;Key Takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nearly 50 million people in the U.S. live with chronic pain, an invisible and often stubborn condition that can last for decades.&lt;/item&gt;
      &lt;item&gt;Now, collaborative research led by neuroscientist J. Nicholas Betley finds that a critical hub in the brainstem, has a built-in ‚Äúoff switch‚Äù to stop persistent pain signals from reaching the rest of the brain.&lt;/item&gt;
      &lt;item&gt;Their findings could help clinicians better understand chronic pain. ‚ÄúIf we can measure and eventually target these neurons, that opens up a whole new path for treatment,‚Äù says Betley.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Acute or short-lived pain, despite its bad reputation, is usually a lifesaver. It acts as a transient negative sensory experience that helps us avoid danger. Touch a hot stove, stub a toe, or bonk your head on a low branch, and the nervous system cues up an ‚ÄúOw!‚Äù Over time, the sting fades, the wound heals, but the lesson sticks.&lt;/p&gt;
    &lt;p&gt;Chronic pain is different; the alarm keeps blaring long after the fire is out, and then the pain itself becomes the problem. Nearly 50 million people in the United States live with chronic pain, an invisible and often untreatable condition that can linger for decades. ‚ÄúIt‚Äôs not just an injury that won‚Äôt heal,‚Äù says neuroscientist at the University of Pennsylvania J. Nicholas Betley, ‚Äúit‚Äôs a brain input that‚Äôs become sensitized and hyperactive, and determining how to quiet that input could lead to better treatments.‚Äù&lt;/p&gt;
    &lt;p&gt;Now, research led by Betley and collaborators at the University of Pittsburgh and Scripps Research Institute has identified a key to regulating long-term pain states: a group of cells called Y1 receptor (Y1R)-expressing neurons in the brainstem‚Äôs lateral parabrachial nucleus (lPBN). These neurons are activated during enduring pain states, but they also integrate information about hunger, fear and thirst, allowing for pain signals to be modulated by other brain circuits signaling more urgent needs.&lt;/p&gt;
    &lt;p&gt;Their findings, published in Nature, suggest that there is hope because ‚Äúthere are circuits in the brain that can reduce the activity of neurons that transmit the signal of pain.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Tracking pain in the brain&lt;/head&gt;
    &lt;p&gt;As part of a collaboration with the Taylor lab at Pitt, the researchers used calcium imaging to watch neurons fire in real time in preclinical models of acute and chronic pain. They found that Y1R neurons didn‚Äôt just flare briefly in response to acute pain‚Äîthey also kept firing steadily during enduring pain, a state neuroscientists call ‚Äútonic activity.‚Äù&lt;/p&gt;
    &lt;p&gt;Betley likens this to an engine left idling, where signals of pain continued to rumble and tick even when outward signs of pain had faded. This persistent activity may encode the lasting pain state people feel long after an accident or surgery.&lt;/p&gt;
    &lt;p&gt;The drive to look deeper into these neurons grew out of a simple observation Betley and his team made shortly after he joined Penn in 2015‚Äîhunger could dampen long-term pain responses.&lt;/p&gt;
    &lt;p&gt;‚ÄúFrom my own experience, I felt that when you‚Äôre really hungry you‚Äôll do almost anything to get food,‚Äù he says. ‚ÄúWhen it came to chronic, lingering pain, hunger seemed to be more powerful than Advil at reducing pain.‚Äù&lt;/p&gt;
    &lt;p&gt;The current work started when Nitsan Goldstein, who was a graduate student in Betley‚Äôs lab at the time, found that other urgent survival needs such as thirst and fear can also reduce enduring pain. That finding supported behavioral models developed in collaboration with the Kennedy lab at Scripps, suggest filtering of sensory input at the parabrachial nucleus can block out long-lasting pain when other more acute needs exist.&lt;/p&gt;
    &lt;p&gt;‚ÄúThat told us the brain must have a built-in way of prioritizing urgent survival needs over pain, and we wanted to find the neurons responsible for that switch,‚Äù says Goldstein.&lt;/p&gt;
    &lt;p&gt;A key part of that switch is neuropeptide Y (NPY), a signaling molecule that helps the brain juggle competing needs. When hunger or fear takes priority, NPY acts on Y1 receptors in the parabrachial nucleus to dampen ongoing pain signals.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs like the brain has this built-in override switch,‚Äù Goldstein explains. ‚ÄúIf you‚Äôre starving or facing a predator, you can‚Äôt afford to be overwhelmed by lingering pain. Neurons activated by these other threats release NPY, and NPY quiets the pain signal so that other survival needs take precedence.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;A scattered signal&lt;/head&gt;
    &lt;p&gt;The researchers also characterized the molecular and anatomical identity of the Y1R neurons in the lPBN. They found that Y1Rneurons didn‚Äôt form two tidy anatomical or molecular populations. Instead, these neurons were scattered across many other cell types.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs like looking at cars in a parking lot,‚Äù Betley says. ‚ÄúWe expected all the Y1R neurons to be a cluster of yellow cars parked together, but here the Y1R neurons are like yellow paint distributed across red cars, blue cars, and green cars. We don‚Äôt know exactly why, but we think this mosaic distribution may allow the brain to dampen different kinds of painful inputs across multiple circuits.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Explorations of pain treatment&lt;/head&gt;
    &lt;p&gt;What excites Betley with this discovery is the further exploration of its potential to ‚Äúuse Y1 neural activity as a biomarker for chronic pain, something drug developers and clinicians have long lacked,‚Äù he says.&lt;/p&gt;
    &lt;p&gt;‚ÄúRight now, patients may go to an orthopedist or a neurologist, and there is no clear injury. But they‚Äôre still in pain,‚Äù he says. ‚ÄúWhat we‚Äôre showing is that the problem may not be in the nerves at the site of injury, but in the brain circuit itself. If we can target these neurons, that opens up a whole new path for treatment.‚Äù&lt;/p&gt;
    &lt;p&gt;This research also suggests that behavioral interventions such as exercise, meditation, and cognitive behavioral therapy may influence how these brain circuits fire, just as hunger and fear did in the lab.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôve shown that this circuit is flexible, it can be dialed up or down,‚Äù he says. ‚ÄúSo, the future isn‚Äôt just about designing a pill. It‚Äôs also about asking how behavior, training, and lifestyle can change the way these neurons encode pain.‚Äù&lt;/p&gt;
    &lt;p&gt;J Nicholas Betley is an associate professor in the Department of Biology at the University of Pennsylvania‚Äôs School of Arts &amp;amp; Sciences.&lt;/p&gt;
    &lt;p&gt;Nitsan Goldstein was a graduate student in the Betley Lab at Penn Arts &amp;amp; Sciences during this study. She is currently a postdoctoral researcher at the Massachusetts Institute of Technology.&lt;/p&gt;
    &lt;p&gt;Other authors include Michelle Awh, Lavinia Boccia, Jamie R. E. Carty, Ella Cho, Morgan Kindel, Kayla A. Kruger, Emily Lo, Erin L. Marble, Nicholas K. Smith, Rachael E. Villari, and Albert T. M. Yeung of Penn Arts &amp;amp; Sciences; Niklas Blank and Christoph A. Thaiss of Penn‚Äôs Perelman School of Medicine; Melissa J. Chee and Yasmina Dumiaty of Carleton University; Rajesh Khanna of University of Florida College of Medicine,; Ann Kennedy and Amadeus Maes of Scripps Research Institute; and Heather N. Allen, Tyler S. Nelson and Bradley K. Taylor of the University of Pittsburg.&lt;/p&gt;
    &lt;p&gt;This research was supported by the Klingenstein Foundation, the University of Pennsylvania School of Arts and Sciences, the National Institutes of Health (grants F31DK131870, 1P01DK119130, 1R01DK133399, 1R01DK124801, 1R01NS134976, F32NS128392, K00NS124190, F32DK135401, T32DK731442, R61NS126026, R01NS120663, R01NS134976-02, R00MH117264, 1DP1DK140021-01), the National Science Foundation Graduate Research Fellowship Program, the Blavatnik Family Foundation Fellowship, the American Neuromuscular Foundation Development Grant, the American Heart Association (25POST1362884), the Swiss National Science Foundation (206668), the Canadian Institutes of Health Research Project Grant (PJT-175156), the Simons Foundation, a McKnight Foundation Scholar Award, and a Pew Biomedical Scholar Award.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://penntoday.upenn.edu/news/select-neurons-brainstem-may-hold-key-treating-chronic-pain"/><published>2025-10-09T20:27:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45535202</id><title>My approach to building large technical projects (2023)</title><updated>2025-10-10T14:40:03.509631+00:00</updated><content>&lt;doc fingerprint="23d090754d843dbe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;My Approach to Building Large Technical Projects&lt;/head&gt;
    &lt;p&gt;Whether it's building a new project from scratch, implementing a big feature, or beginning a large refactor, it can be difficult to stay motivated and complete large technical projects. A method that works really well for me is to continuously see real results and to order my work based on that.&lt;/p&gt;
    &lt;p&gt;We've all experienced that feeling of excitement starting a new project. The first few weeks you can't wait to get on the computer to work. Then slowly over time you get distracted or make up excuses and work on it less. If this is for real work, you forcibly slog your way to the finish line but every day is painful. If this is for fun, you look back years from now and remember what could've been.&lt;/p&gt;
    &lt;p&gt;I've learned that when I break down my large tasks in chunks that result in seeing tangible forward progress, I tend to finish my work and retain my excitement throughout the project. People are all motivated and driven in different ways, so this may not work for you, but as a broad generalization I've not found an engineer who doesn't get excited by a good demo. And the goal is to always give yourself a good demo.&lt;/p&gt;
    &lt;p&gt;I'm not claiming that anything I say in this post is novel. It definitely shares various aspects of well-known software engineering or management practices. I'm just sharing the way I approach the larger technical work that I do and why I do it this way.&lt;/p&gt;
    &lt;p&gt;I'll use my terminal emulator project as an example throughout this post so that there is realistic, concrete experience I can share. There's plenty of other projects I could've used but I'll choose this one since it's not related to my professional work and it is recent enough to be fresh in my mind.&lt;/p&gt;
    &lt;p&gt;I want to be crystal clear that I am not shaming anyone for not completing projects. As long as you're having fun and feel accomplished (or simply don't care), good for you and more power to you. This blog post is aimed at people who want to finish projects more or simply want to learn how I strive to finish projects more.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Starting Line&lt;/head&gt;
    &lt;p&gt;Initially, you have some large project and you have to figure how to start. For me, this is the hardest part and I can spend hours -- sometimes days -- waffling over the right starting point.&lt;/p&gt;
    &lt;p&gt;For my terminal emulator, there were a number of large components that I knew would have to exist if I ever intended to finish this project: terminal parsing, running and managing a shell process, font rendering, grid rendering, input handling (keyboard/mouse), etc. There are hundreds of relatively large sub-projects on the path to "done."&lt;/p&gt;
    &lt;p&gt;If my initial goal was to see a launchable terminal that could run Neovim, I'd be in big trouble. Even with unknown unknowns, this goal just sounds too big. I can intuitively realize that there are a lot of components on that path: rendering a GUI, process launching, terminal parsing and state management. This is a bad goal, it's too big and I'd probably lose interest a month or two in.&lt;/p&gt;
    &lt;p&gt;Instead, I try to think what a realistic project is where I can see results as soon as possible. Once you apply that filter, the number of viable sub-projects shrinks dramatically. Here are some examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VT Parsing - parsing the terminal escape sequences&lt;/item&gt;
      &lt;item&gt;Blank window rendering - open a window and draw a blank canvas&lt;/item&gt;
      &lt;item&gt;Child process lanching - launch a child shell such as bash, zsh, fish, setup the TTY and be able to read output from it (i.e. the initial shell prompt)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I don't try to enumerate all the big sub-projects at this stage. I just kind of get an idea of the rough shape the project will take and find one that I can build in isolation and also physically see some sort of real results.&lt;/p&gt;
    &lt;p&gt;This is the phase where experience helps the most. Engineers with more experience are usually able to more effectively paint the picture of the rough shape a project will take. They can identify various subcomponents with more accuracy and see how they pieces fit together. With less experience, or in a domain I'm unfamiliar with, I just take a best guess and expect there is a higher likelihood I'll throw my work away at some point.&lt;/p&gt;
    &lt;head rend="h1"&gt;Early Results&lt;/head&gt;
    &lt;p&gt;Early work tends to not be very visible and that makes seeing tangible results seem difficult. For example, if I chose to work on VT parsing for my terminal, I can't see it work without also hooking up a UI of some sort. Or for some other project if I chose to work on a database schema and minimal API, I similarly can't see that work without writing a client along with a CLI or GUI.&lt;/p&gt;
    &lt;p&gt;If the initial subproject you choose to work on is a UI, then you can quickly see some results of course! For various reasons, I rarely start frontend first and usually start backend first. And in any situation, you'll eventually get to the backend and reach a similar challenge.&lt;/p&gt;
    &lt;p&gt;The best tool to get past this phase is automated testing (usually unit testing at this stage). Automated tests let you actually run some code and see it is working and also has the benefit of being good hygiene.&lt;/p&gt;
    &lt;p&gt;This gives you another guiding point for picking out your first few tasks: if it isn't graphical, you want to pick something that is testable without too much fuss so you can see some results.&lt;/p&gt;
    &lt;p&gt;For my terminal, I decided to start with VT parsing first, because it was a part of a terminal at the time that I didn't know too much about and it felt like something that I could very easily test: give it some example input as a string, expect some parsed action or event as output.&lt;/p&gt;
    &lt;p&gt;Seeing the progression of "1 test passed", "4 tests passed," "13 tests passed" and so on is super exciting to me. I'm running some code I wrote and it's working. And I know that I'm progressing on some critical sub-component of a larger project.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sprint to Demos&lt;/head&gt;
    &lt;p&gt;My goal with the early sub-projects isn't to build a finished sub-component, it is to build a good enough sub-component so I can move on to the next thing on the path to a demo. ‚ú®&lt;/p&gt;
    &lt;p&gt;This tradeoff isn't just manifested in functionality. It may be manifested in algorithmic or design considerations. For example, you may know that in the future, you'll need to use something like a real database or a fancy data structure or support streaming data. But for the initial set of work, you can just use in-memory contents, built-in data structures such as dictionaries, and require all your inputs/outputs up front.&lt;/p&gt;
    &lt;p&gt;I think this is an important tradeoff so I will repeat it: do not let perfection be an enemy of progress. Going further, do not let future improvements you know you'll have to make stop you from moving on to the next thing. The goal is to get to a demo.&lt;/p&gt;
    &lt;p&gt;No matter what I'm working on, I try to build one or two demos per week intermixed with automated test feedback as explained in the previous section.&lt;/p&gt;
    &lt;p&gt;Building a demo also provides you with invaluable product feedback. You can quickly intuit whether something feels good, even if it isn't fully functional. These aren't "minimum viable products", because they really aren't viable, but they're good enough to provide an engineer some valuable self-reflection.&lt;/p&gt;
    &lt;p&gt;This is an area where I think experience actually hurts. I've seen senior engineers get bogged down building the perfect thing and by the time they get a demo, they realize it sucks. The implementation doesn't suck, but the product or feature itself actually sucks.&lt;/p&gt;
    &lt;p&gt;Recall that for the terminal the first task I chose was VT parsing. In the early stages, I only saw automated tests work. To get to my first demo, I built a shell script that would run some command, capture its output, feed it to my VT parser, and output everything it parsed (or couldn't). Over time, I iterated on this CLI as my first "UI" -- I would render the terminal grid using ASCII.&lt;/p&gt;
    &lt;p&gt;This gave me immense satisfaction since I could run simple programs like &lt;code&gt;man&lt;/code&gt; or &lt;code&gt;ls&lt;/code&gt; or more complex programs like &lt;code&gt;vim&lt;/code&gt; and see my parser work (or break,
which is equally exciting in its own way).&lt;/p&gt;
    &lt;p&gt;In this scenario, the CLI I was writing was relatively useless long term (I ended up throwing it away rather quickly). But the day or two I spent building it as a demo provided me with an important feeling of progress and seeing something work helped keep me motivated.&lt;/p&gt;
    &lt;head rend="h1"&gt;Build for Yourself&lt;/head&gt;
    &lt;p&gt;This section will apply more to personal projects than to work-assigned projects. Even if you aspire to release some software for others, build only what you need as you need it and adopt your software as quickly as possible.&lt;/p&gt;
    &lt;p&gt;I'm always more motivated working on a problem I'm experiencing myself1. And if a product designed for you doesn't work for you, it's very likely not going to work well for others, either. Therefore, my path from demos to an actual real-world usable product is to find the shortest path to building only the functionality I think I need.&lt;/p&gt;
    &lt;p&gt;For my terminal, that meant first being able to load my shell configuration (fish) and from there being able to launch and use Neovim. So I beelined all my work to only the functionality needed for that: only the escape sequences those programs used, only rendering the font I use daily, etc. Examples of features I initially omitted: scrolling, mouse selection, search, tabs/splits, etc.&lt;/p&gt;
    &lt;p&gt;Then I started using my terminal as a daily driver. This step usually has a few false starts; you realize you actually need some feature you omitted or forgot. In my initial runs of my terminal, I realized my arrow keys didn't do anything, there were subtle (but workflow-breaking) rendering bugs, etc. So I'd go abandon using it, but it gave me tangible tasks to work on next.&lt;/p&gt;
    &lt;p&gt;Additionally, I always feel a lot of pride using software with code that I wrote and that usually helps keep me motivated to continue working on it.&lt;/p&gt;
    &lt;head rend="h1"&gt;Packaging it Up&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Decompose a large problem into smaller problems. Importantly, each small problem must have some clear way you can see the results of your work.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Only solve the smaller problem enough to progress on a demo-aspect of the larger problem, then move on to the next small problem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Only solve enough small problems to be able to begin building runnable demos of your software, then continue to iterate on more functionality. Make demos as frequently as you can.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prioritize functionality that enables you to adopt your own software, if applicable (a personal project, a work project solving a problem you actually have, etc.). Then continue to solve your own problems first.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Go back and iterate on each component as needed for future improvements, repeating this process as needed.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;And that's pretty much it. I've followed this general pattern on personal projects, group projects, work projects, school projects, etc. and it's how I keep myself motivated2.&lt;/p&gt;
    &lt;p&gt;Note that I didn't mention a lot of things! I don't talk about shipping. I know a lot of people find shipping motivational. I don't think you need to ship a project for it to be successful. And for me, I find shipping too big of an event to motivate me long-term. I don't talk about tooling (Git workflows, CI, etc.). I've used my process across multiple jobs and fit it into whatever process is established. And so on.&lt;/p&gt;
    &lt;p&gt;I think that helps show how much of a personal process this is. Everyone I think needs to find some process to reinforce their motivation in a healthy way. I realized seeing results motivates me really strongly, I've built my work style around that, and it has worked well for me thus far.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/building-large-technical-projects"/><published>2025-10-10T03:45:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536000</id><title>I Switched from Htmx to Datastar</title><updated>2025-10-10T14:40:03.241554+00:00</updated><content>&lt;doc fingerprint="1ecb574dba2106d7"&gt;
  &lt;main&gt;
    &lt;p&gt;In 2022, David Guillot delivered an inspiring DjangoCon Europe talk, showcasing a web app that looked and felt as dynamic as a React app. Yet he and his team had done something bold. They converted it from React to HTMX, cutting their codebase by almost 70% while significantly improving its capabilities.&lt;/p&gt;
    &lt;p&gt;Since then, teams everywhere have discovered the same thing: turning a single-page app into a multi-page hypermedia app often slashes lines of code by 60% or more while improving both developer and user experience.&lt;/p&gt;
    &lt;p&gt;I saw similar results when I switched my projects from HTMX to Datastar. It was exciting to reduce my code while building real-time, multi-user applications without needing WebSockets or complex frontend state management.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pain point that moved the needle&lt;/head&gt;
    &lt;p&gt;While preparing my FlaskCon 2025 talk, I hit a wall. I was juggling HTMX and AlpineJS to keep pieces of my UI in sync, but they fell out of step. I lost hours debugging why my component wasn‚Äôt updating. Neither library communicates with the other. Since they are different libraries created by different developers, you are the one responsible for helping them work together.&lt;/p&gt;
    &lt;p&gt;Managing the dance to initialize components at various times and orchestrating events was causing me to write more code than I wanted to and spend more time than I could spare to complete tasks.&lt;/p&gt;
    &lt;p&gt;Knowing that Datastar had the capability of both libraries with a smaller download, I thought I‚Äôd give it a try. It handled it without breaking a sweat, and the resulting code was much easier to understand.&lt;/p&gt;
    &lt;p&gt;I appreciate that there‚Äôs less code to download and maintain. Having a library handle all of this in under 11 KB is great for improving page load performance, especially for users on mobile devices. The less you need to download, the better off you are.&lt;/p&gt;
    &lt;p&gt;But that‚Äôs just the starting point.&lt;/p&gt;
    &lt;head rend="h2"&gt;Better API&lt;/head&gt;
    &lt;p&gt;As I incorporated Datastar into my project at work, I began to appreciate Datastar‚Äôs API. It feels significantly lighter than HTMX. I find that I need to add fewer attributes to achieve the desired results.&lt;/p&gt;
    &lt;p&gt;For example, most interactions with HTMX require you to create an attribute to define the URL to hit, what element to target with the response, and then you might need to add more to customize how HTMX behaves, like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;span hx-target="#rebuild-bundle-status-button"&amp;#13;
      hx-select="#rebuild-bundle-status-button"&amp;#13;
      hx-swap="outerHTML"&amp;#13;
      hx-trigger="click"&amp;#13;
      hx-get="/rebuild/status-button"&amp;gt;&amp;lt;/span&amp;gt;&lt;/code&gt;
    &lt;p&gt;One doesn‚Äôt always need all of these, but I find it common to have two or three attributes every timeAnd then there are the times I need to remember to look up the ancestry chain to see if any attribute changes the way I‚Äôm expecting things to work. Those are confusing bugs when they happen! .&lt;/p&gt;
    &lt;p&gt;With Datastar, I regularly use just one attribute, like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;span data-on-click="@get('/rebuild/status-button')"&amp;gt;&amp;lt;/span&amp;gt;&lt;/code&gt;
    &lt;p&gt;This gives me less to think about when I return months later and need to recall how this works.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to update page elements&lt;/head&gt;
    &lt;p&gt;The primary difference between HTMX and Datastar is that HTMX is a front-end library that advances the HTML specification. DataStar is a server-side-driven library that aims to create high-performance, web-native, live-updating web applications.&lt;/p&gt;
    &lt;p&gt;In HTMX, you describe its behavior by adding attributes to the element that triggers the request, even if it updates something far away on the page. That‚Äôs powerful, but it means your logic is scattered across multiple layers. Datastar flips that: the server decides what should change, keeping all your update logic in one place.&lt;/p&gt;
    &lt;p&gt;To cite an example from HTMX‚Äôs documentation:&lt;/p&gt;
    &lt;code&gt;&amp;lt;div&amp;gt;&amp;#13;
   &amp;lt;div id="alert"&amp;gt;&amp;lt;/div&amp;gt;&amp;#13;
    &amp;lt;button hx-get="/info" &amp;#13;
            hx-select="#info-details" &amp;#13;
            hx-swap="outerHTML"&amp;#13;
            hx-select-oob="#alert"&amp;gt;&amp;#13;
        Get Info!&amp;#13;
    &amp;lt;/button&amp;gt;&amp;#13;
&amp;lt;/div&amp;gt;&lt;/code&gt;
    &lt;p&gt;When the button is pressed, it sends a GET request to &lt;code&gt;/info&lt;/code&gt; , replaces the button with the element in the response that has the ID 'info-details', and then retrieves the element in the response with the ID 'alert', replacing the element with the same ID on the¬†page.&lt;/p&gt;
    &lt;p&gt;This is a lot for that button element to know. To author this code, you need to know what information you‚Äôre going to return from the server, which is done outside of editing the HTML. This is when HTMX loses the ‚Äùlocality of behavior‚Äù I like so much.&lt;/p&gt;
    &lt;p&gt;Datastar, on the other hand, expects the server to define the behavior, and it works better.&lt;/p&gt;
    &lt;p&gt;To replicate the behavior above, you have options. The first option keeps the HTML similar to above:&lt;/p&gt;
    &lt;code&gt;&amp;lt;div&amp;gt;&amp;#13;
    &amp;lt;div id="alert"&amp;gt;&amp;lt;/div&amp;gt;&amp;#13;
    &amp;lt;button id="info-details"&amp;#13;
     data-on-click="@get('/info')"&amp;gt;&amp;#13;
        Get Info!&amp;#13;
    &amp;lt;/button&amp;gt;&amp;#13;
&amp;lt;/div&amp;gt;&lt;/code&gt;
    &lt;p&gt;In this case, the server can return an HTML string with two root elements that have the same IDs as the elements they‚Äôre updating:&lt;/p&gt;
    &lt;code&gt;&amp;lt;p id="info-details"&amp;gt;These are the details you are looking for‚Ä¶&amp;lt;/p&amp;gt;&amp;#13;
&amp;lt;div id="alert"&amp;gt;Alert! This is a test.&amp;lt;/div&amp;gt;&lt;/code&gt;
    &lt;p&gt;I love this option because it‚Äôs simple and performant.&lt;/p&gt;
    &lt;head rend="h2"&gt;Think at the component level&lt;/head&gt;
    &lt;p&gt;A better option would change the HTML to treat it as a component.&lt;/p&gt;
    &lt;p&gt;What is this component? It appears to be a way for the user to get more information about a specific item.&lt;/p&gt;
    &lt;p&gt;What happens when the user clicks the button? It seems like either the information appears or there is no information to appear, and instead we render an error. Either way, the component becomes static.&lt;/p&gt;
    &lt;p&gt;Maybe we could split the component into each state, first, the placeholder:&lt;/p&gt;
    &lt;code&gt;&amp;lt;!-- info-component-placeholder.html --&amp;gt;&amp;#13;
&amp;lt;div id="info-component"&amp;gt;&amp;#13;
    &amp;lt;button data-on-click="@get('/product/{{product.id}}/info')"&amp;gt;&amp;#13;
        Get Info!&amp;#13;
    &amp;lt;/button&amp;gt;&amp;#13;
&amp;lt;/div&amp;gt;&lt;/code&gt;
    &lt;p&gt;Then the server could render the information the user requests‚Ä¶&lt;/p&gt;
    &lt;code&gt;&amp;lt;!-- info-component-get.html --&amp;gt;&amp;#13;
&amp;lt;div id="info-component"&amp;gt;&amp;#13;
    {% if alert %}&amp;lt;div id="alert"&amp;gt;{{ alert }}&amp;lt;/div&amp;gt;{% endif %}&amp;#13;
    &amp;lt;p&amp;gt;{{product.additional_information}}&amp;lt;/p&amp;gt;&amp;#13;
&amp;lt;/div&amp;gt;&lt;/code&gt;
    &lt;p&gt;‚Ä¶and Datastar will update the page to reflect the changes.&lt;/p&gt;
    &lt;p&gt;This particular example is a little wonky, but I hope you get the idea. Thinking at a component level is better as it prevents you from entering an invalid state or losing track of the user‚Äôs state.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚Ä¶or more than one component&lt;/head&gt;
    &lt;p&gt;One of the amazing things from David Guillot‚Äôs talk is how his app updated the count of favored items even though that element was very far away from the component that changed the count.&lt;/p&gt;
    &lt;p&gt;David‚Äôs team accomplished that by having HTMX trigger a JavaScript event, which in turn triggered the remote component to issue a GET request to update itself with the most up-to-date count.&lt;/p&gt;
    &lt;p&gt;With Datastar, you can update multiple components at once, even in a synchronous function.&lt;/p&gt;
    &lt;p&gt;If we have a component that allows someone to add an item to a shopping cart:&lt;/p&gt;
    &lt;code&gt;&amp;lt;form id="purchase-item"&amp;#13;
      data-on-submit="@post('/add-item', {contentType: 'form'})"&amp;gt;"&amp;#13;
&amp;gt;&amp;#13;
  &amp;lt;input type=hidden name="cart-id" value="{{cart.id}}"&amp;gt;&amp;#13;
  &amp;lt;input type=hidden name="item-id" value="{{item.id}}"&amp;gt;&amp;#13;
  &amp;lt;fieldset&amp;gt;&amp;#13;
    &amp;lt;button data-on-click="$quantity -= 1"&amp;gt;-&amp;lt;/button&amp;gt;&amp;#13;
    &amp;lt;label&amp;gt;Quantity&amp;#13;
      &amp;lt;input name=quantity type=number data-bind-quantity value=1&amp;gt;&amp;#13;
    &amp;lt;/label&amp;gt;&amp;#13;
    &amp;lt;button data-on-click="$quantity += 1"&amp;gt;+&amp;lt;/button&amp;gt;&amp;#13;
  &amp;lt;/fieldset&amp;gt;&amp;#13;
  &amp;lt;button type=submit&amp;gt;Add to cart&amp;lt;/button&amp;gt;&amp;#13;
  {% if msg %}&amp;#13;
    &amp;lt;p class=message&amp;gt;{{msg}}&amp;lt;/p&amp;gt;&amp;#13;
  {% endif %}&amp;#13;
&amp;lt;/form&amp;gt;&lt;/code&gt;
    &lt;p&gt;And another one that shows the current count of items in the cart:&lt;/p&gt;
    &lt;code&gt;&amp;lt;div id="cart-count"&amp;gt;&amp;#13;
    &amp;lt;svg viewBox="0 0 10 10" xmlns="http://www.w3.org/2000/svg"&amp;gt;&amp;#13;
        &amp;lt;use href="#shoppingCart"&amp;gt;&amp;#13;
    &amp;lt;/svg&amp;gt;&amp;#13;
    {{count}}&amp;#13;
&amp;lt;/div&amp;gt;&lt;/code&gt;
    &lt;p&gt;Then a developer can update them both in the same request. This is one way it could look in Django:&lt;/p&gt;
    &lt;code&gt;from datastar_py.consts import ElementPatchMode&amp;#13;
from datastar_py.django import (&amp;#13;
    DatastarResponse,&amp;#13;
    ServerSentEventGenerator as SSE,&amp;#13;
)&amp;#13;
&amp;#13;
def add_item(request):&amp;#13;
    # skipping all the important state updates&amp;#13;
	return DatastarResponse([&amp;#13;
		SSE.patch_elements(&amp;#13;
    		render_to_string('purchase-item.html', context=dict(cart=cart, item=item, msg='Item added!'))&amp;#13;
		),&amp;#13;
		SSE.patch_elements(&amp;#13;
    		render_to_string('cart-count.html', context=dict(count=item_count))&amp;#13;
		),&amp;#13;
	])&lt;/code&gt;
    &lt;head rend="h2"&gt;Web native&lt;/head&gt;
    &lt;p&gt;Being a part of the Datastar Discord, I appreciate that Datastar isn‚Äôt just a helper script. It‚Äôs a philosophy about building apps with the web‚Äôs own primitives, letting the browser and the server do what they‚Äôre already great at.&lt;/p&gt;
    &lt;p&gt;Where HTMX is trying to push the HTML spec forward, Datastar is more interested in promoting the adoption of web-native features, such as CSS view transitions, Server-Sent Events, and web components, where appropriate.&lt;/p&gt;
    &lt;p&gt;This has been a massive eye-opener for me, as I‚Äôve long wanted to leverage each of these technologies, and now I‚Äôm seeing the benefits.&lt;/p&gt;
    &lt;p&gt;One of the biggest wins I achieved with Datastar was by refactoring a complicated AlpineJS component and extracting a simple web component that I reused in multiple placesI‚Äôll talk more about this in an upcoming post. .&lt;/p&gt;
    &lt;p&gt;I especially appreciate this because there are times when it‚Äôs best to rely on JavaScript to accomplish a task. But it doesn‚Äôt mean you have to reach for a tool like React to achieve it. Creating custom HTML elements is a great pattern to accomplish tasks with high locality of behavior and the ability to reuse them across your app.&lt;/p&gt;
    &lt;p&gt;However, Datastar provides you with even more capabilities.&lt;/p&gt;
    &lt;head rend="h2"&gt;Real-time updates for multi-user apps&lt;/head&gt;
    &lt;p&gt;Apps built with collaboration as a first-class feature stand out from the rest, and Datastar is up to the challenge.&lt;/p&gt;
    &lt;p&gt;To accomplish this, most HTMX developers achieve updates either by ‚Äúpulling‚Äù information from the server by polling every few seconds or by writing custom WebSocket code, which increases complexity.&lt;/p&gt;
    &lt;p&gt;Datastar uses a simple web technology called Server-Sent Events (SSE) to allow the server to ‚Äúpush‚Äù updates to connected clients. When something changes, such as a user adding a comment or a status change, the server can immediately update browsers with minimal additional code.&lt;/p&gt;
    &lt;p&gt;You can now build live dashboards, admin panels, and collaborative tools without crafting custom JavaScript. Everything flows from the server, through HTML.&lt;/p&gt;
    &lt;p&gt;Additionally, suppose a client‚Äôs connection is interrupted. In that case, the browser will automatically attempt to reconnect without requiring additional code, and it can even notify the server, ‚ÄúThis is the last event I received.‚Äù It‚Äôs wonderful.&lt;/p&gt;
    &lt;head rend="h2"&gt;Just because you can do it doesn‚Äôt mean you should&lt;/head&gt;
    &lt;p&gt;Being a part of the Datastar community on Discord has helped me appreciate the Datastar vision of making web apps. They aim to have push-based UI updates, reduce complexity, and leverage tools like web components to handle more complex situations locally. It‚Äôs common for the community to help newcomers by helping them realize they‚Äôre overcomplicating things.&lt;/p&gt;
    &lt;p&gt;Here are some of the tips I‚Äôve picked up:&lt;/p&gt;
    &lt;p&gt;- Don‚Äôt be afraid to re-render the whole component and send it down the pipe. It‚Äôs easier, it probably won‚Äôt affect performance too much, you get better compression ratios, and it‚Äôs incredibly fast for the browser to parse HTML strings.&lt;/p&gt;
    &lt;p&gt;- The server is the state of truth and is more powerful than the browser. Let it handle the majority of the state. You probably don‚Äôt need the reactive signals as much as you think you do.&lt;/p&gt;
    &lt;p&gt;- Web components are great for encapsulating logic into a custom element with high locality of behavior. A great example of this is the star field animation in the header of the Datastar website. The &lt;code&gt;&amp;lt;ds-starfield&amp;gt;&lt;/code&gt;  element encapsulates all the code to animate the star field and exposes three attributes to change its internal state. Datastar drives the attributes whenever the range input changes or the mouse moves over the¬†element.&lt;/p&gt;
    &lt;head rend="h2"&gt;But you can still reach for the stars&lt;/head&gt;
    &lt;p&gt;But what I‚Äôm most excited about are the possibilities that Datastar enables. The community is routinely creating projects that push well beyond the limits experienced by developers using other tools.&lt;/p&gt;
    &lt;p&gt;The examples page includes a database monitoring demo that leverages Hypermedia to significantly improve the speed and memory footprint of a demo presented at a JavaScript conference.&lt;/p&gt;
    &lt;p&gt;The one million checkbox experiment was too much for the server it started on. Anders Murphy used Datastar to create one billion checkboxes on an inexpensive server.&lt;/p&gt;
    &lt;p&gt;But the one that most inspired me was a web app that displayed data from every radar station in the United States. When a blip changed on a radar, the corresponding dot in the UI would change within 100 milliseconds. This means that *over 800,000 points are being updated per second*. Additionally, the user could scrub back in time for up to an hour (with under a 700 millisecond delay). Can you imagine this as a Hypermedia app? This is what Datastar enables.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it‚Äôs working for me today&lt;/head&gt;
    &lt;p&gt;I‚Äôm still in what I consider my discovery phase of Datastar. Replacing the standard HTMX functionality of ajaxing updates to a UI was quick and easy to implement. Now I‚Äôm learning and experimenting with different patterns to use Datastar to achieve more and more.&lt;/p&gt;
    &lt;p&gt;For decades, I‚Äôve been interested in ways I could provide better user experiences with real-time updates, and I love that Datastar enables me to do push-based updates, even in synchronous code.&lt;/p&gt;
    &lt;p&gt;HTMX filled me with so much joy when I started using it. But I haven‚Äôt felt like I lost anything since switching to Datastar. In fact, I feel like I‚Äôve gained so much more.&lt;/p&gt;
    &lt;p&gt;If you‚Äôve ever felt the joy of using HTMX, I bet you‚Äôll feel the same leap again with Datastar. It‚Äôs like discovering what the web was meant to do all along.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://everydaysuperpowers.dev/articles/why-i-switched-from-htmx-to-datastar/"/><published>2025-10-10T06:49:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536124</id><title>Multi-Core by Default</title><updated>2025-10-10T14:40:02.811463+00:00</updated><content>&lt;doc fingerprint="665c885307b58ac8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Multi-Core By Default&lt;/head&gt;
    &lt;head rend="h3"&gt;On multi-core programming, not as a special-case technique, but as a new dimension in all code.&lt;/head&gt;
    &lt;p&gt;Learning to program a single CPU core is difficult. There is an enormous number of techniques, amount of information, and number of hours to spend in order to learn to do it effectively. Learning to program multiple CPU cores to do work in parallel, all while these cores cooperate in accomplishing some overarching task, to me seemed like the anvil that broke the camel‚Äôs back‚Äîso to speak‚Äîthere is already so much to wrangle when doing single-core programming, that for me, it was much more convenient to ignore multi-core programming for a long time.&lt;/p&gt;
    &lt;p&gt;But in the modern computer hardware era, there emerges an elephant in the room. With modern CPU core counts far exceeding 1‚Äîand instead reaching numbers like 8, 16, 32, 64‚Äîprogrammers leave an enormous amount of performance on the table by ignoring the fundamentally multi-core reality of their machines.&lt;/p&gt;
    &lt;p&gt;I‚Äôm not a ‚Äúperformance programmer‚Äù. Like Casey Muratori (which is partly what made me follow him to begin with), I have always wanted reasonable performance (though this might appear like ‚Äúperformance programming‚Äù to a concerning proportion of the software industry), but historically I‚Äôve worked in domains where I control the data involved, like my own games and engines, where I am either doing the art, design, and levels myself, or heavily involved in the process. Thus, I‚Äôve often been able to use my own programming constraints to inform artistic constraints.&lt;/p&gt;
    &lt;p&gt;All of that went out the window over the past few years, when in my work on debuggers, I‚Äôve needed to work with data which is not only not under my control, but is almost exactly identical to the opposite of what I‚Äôd want‚Äîit‚Äôs dramatically bigger, unfathomably poorly structured, extraordinarily complicated, and not to mention unpredictable and highly variable. This is because, as I‚Äôve written about, debuggers are at a ‚Äúbusy intersection‚Äù. They deal with unknowns from the external computing world on almost all fronts. And if one wanted a debugger to be useful for‚Äîfor instance‚Äîextraordinarily large codebases that highly successful companies use to ship real things, those unknowns include unfortunate details about those codebases too.&lt;/p&gt;
    &lt;p&gt;As such, in my work, making more effective use of the hardware has been far more important than it ever has been for me in the past. As such, I was forced to address the ‚Äúelephant in the room‚Äù that is CPU core counts, and actually doing multi-core programming.&lt;/p&gt;
    &lt;p&gt;I‚Äôve learned a lot about the multi-core aspect of programming in the past few years, and I‚Äôve written about lessons I‚Äôve learned during that time, like those on basic mental building blocks I used to plan for multithreaded architecture, and carefully organizing mutations such that multiple threads require little-to-no synchronization.&lt;/p&gt;
    &lt;p&gt;I still find those ideas useful, and my past writing still captures my thoughts on the first principles of multi-core programming. But recently, thanks to some lessons I learned after a few discussions with Casey, my abilities in concretely applying those first principles have ‚Äúleveled up‚Äù. I‚Äôm writing this post now to capture and share those lessons.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Parallel &lt;code&gt;for&lt;/code&gt; (And Its Flaws)&lt;/head&gt;
    &lt;p&gt;Because every programmer learns single-core programming first, it‚Äôs common‚Äîafter one first learns multi-core programming techniques‚Äîto apply those techniques conservatively within otherwise single-core code.&lt;/p&gt;
    &lt;p&gt;To make this more concrete, consider the following simple example:&lt;/p&gt;
    &lt;code&gt;S64 *values = ...;
S64 values_count = ...;
S64 sum = 0;
for(S64 idx = 0; idx &amp;lt; values_count; idx += 1)
{
  sum += values[idx];
}&lt;/code&gt;
    &lt;p&gt;In this example, we compute a sum of all elements in the &lt;code&gt;values&lt;/code&gt; array. Let‚Äôs now consider a few properties of sums:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;a + b + c + d = (a + b) + (c + d)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;a + b + c + d = d + c + b + a&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;(a + b) + (c + d) = (c + d) + (a + b)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Because we can reconsider a sum of elements as a sum of sums of groups of those elements, and because the order in which we sum elements does not impact the final computation, the original code can be rewritten like:&lt;/p&gt;
    &lt;code&gt;S64 *values = ...;
S64 values_count = ...;

S64 sum0 = 0;
for(S64 idx = 0; idx &amp;lt; values_count/4; idx += 1)
{
  sum0 += values[idx];
}

S64 sum1 = 0;
for(S64 idx = values_count/4; idx &amp;lt; (2*values_count)/4; idx += 1)
{
  sum1 += values[idx];
}

S64 sum2 = 0;
for(S64 idx = (2*values_count)/4; idx &amp;lt; (3*values_count)/4; idx += 1)
{
  sum2 += values[idx];
}

S64 sum3 = 0;
for(S64 idx = (3*values_count)/4; idx &amp;lt; (4*values_count)/4 &amp;amp;&amp;amp; idx &amp;lt; values_count; idx += 1)
{
  sum3 += values[idx];
}

S64 sum = sum0 + sum1 + sum2 + sum3;&lt;/code&gt;
    &lt;p&gt;That obviously doesn‚Äôt win us anything‚Äîbut what this means is that we can obtain the same result by subdividing the computation into several, smaller, independent computations.&lt;/p&gt;
    &lt;p&gt;Because several independent computations do not require writing to the same memory, they fit nicely with multi-core programming‚Äîeach core does not need to synchronize at all with any other. This not only greatly simplifies the multi-core programming, but improves its performance‚Äîor, more precisely, it doesn‚Äôt eat away from the natural performance obtained by executing in parallel.&lt;/p&gt;
    &lt;p&gt;For cases like this, we can implement what‚Äôs known as a ‚Äúparallel &lt;code&gt;for‚Äù&lt;/code&gt;. The idea is that we‚Äôd like to specify our original &lt;code&gt;for&lt;/code&gt; loop‚Ä¶&lt;/p&gt;
    &lt;code&gt;for(S64 idx = 0; idx &amp;lt; values_count; idx += 1)
{
  sum += values[idx];
}&lt;/code&gt;
    &lt;p&gt;‚Ä¶but we‚Äôd like to also express that the loop can be subdivided into independent computations (the results of which we can join into a single result later).&lt;/p&gt;
    &lt;p&gt;In other words, we begin with normal, single-core code. But, for some computation, we want to ‚Äúgo wide‚Äù, and compute something in parallel. Then, we want to ‚Äújoin‚Äù this wide, parallel work, and go back to more single-core code, which can use the results of the work done in parallel.&lt;/p&gt;
    &lt;p&gt;This is a widely known and used concept. In many real codebases written in modern programming languages which offer many tools for abstraction building, you‚Äôll find a number of impressive gymnastics to succinctly express this.&lt;/p&gt;
    &lt;p&gt;One of the reasons I prefer working in a simpler language is that, if what my code ultimately generates to facilitate some abstraction is complicated, that being reflected directly in the source code helps keep me honest about how ‚Äúclean‚Äù some construct actually is.&lt;/p&gt;
    &lt;p&gt;If, on the other hand, some higher level utility can be provided by a simple and straightforward concrete implementation, that is a sign of a superior design‚Äîone that does not compromise on its implementation, but also does not compromise on its higher level utility.&lt;/p&gt;
    &lt;p&gt;Many people behave as though this is impossible‚Äîthat higher level utility necessarily incurs substantial tradeoffs at the low level, or vice versa, that low level properties like performance necessitate undesirable high level design. This is simply not universally true. By hunting for tradeoffs, many programmers train themselves to ignore cases when they can both have, and eat, their cake.&lt;/p&gt;
    &lt;p&gt;So, if we consider our options for implementing a ‚Äúparallel &lt;code&gt;for&lt;/code&gt;‚Äù without a lot of modern language machinery, we might start with something like this:&lt;/p&gt;
    &lt;code&gt;struct SumParams
{
  S64 *values;
  S64 count;
  S64 sum;
};

void SumTask(SumParams *p)
{
  for(S64 idx = 0; idx &amp;lt; p-&amp;gt;count; idx += 1)
  {
    p-&amp;gt;sum += p-&amp;gt;values[idx];
  }
}

S64 ComputeSum(S64 *values, S64 count)
{
  S64 count_per_core = count / NUMBER_OF_CORES;
  SumParams params[NUMBER_OF_CORES] = {0};
  Thread threads[NUMBER_OF_CORES] = {0};
  for(S64 core_idx = 0; core_idx &amp;lt; NUMBER_OF_CORES; core_idx += 1)
  {
    params[core_idx].values = values + core_idx*count_per_core;
    params[core_idx].count = count_per_core;
    S64 overkill = ((core_idx+1)*count_per_core - count);
    if(overkill &amp;gt; 0)
    {
      params[core_idx].count -= overkill;
    }
    threads[core_idx] = LaunchThread(SumTask, &amp;amp;params[core_idx]);
  }

  S64 sum = 0;
  for(S64 core_idx = 0; core_idx &amp;lt; NUMBER_OF_CORES; core_idx += 1)
  {
    JoinThread(threads[core_idx]);
    sum += params[core_idx].sum;
  }

  return sum;
}&lt;/code&gt;
    &lt;p&gt;There are a number of unfortunate realities about this mechanism:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;In something like&lt;/p&gt;&lt;code&gt;LaunchThread&lt;/code&gt;and&lt;code&gt;JoinThread&lt;/code&gt;, we interact with the kernel to create and destroy kernel resources (threads) every time we perform a sum.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The actual case-specific code we needed (for the sum, in this case), and the number of particular details we had to specify and get right‚Äîlike the work subdivision‚Äîhas exploded. What used to be a simple&lt;/p&gt;&lt;code&gt;for&lt;/code&gt;loop has been spread around to different, more intricate parts, all implementing different details of the mechanism we wanted‚Äîthe work preparation, the work kickoff, and the joining and combination of work results. All parts must be maintained and changed together, every time we want a parallel&lt;code&gt;for&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The solution‚Äôs control flow has been scattered across threads, CPU cores, and time. We can no longer trivially step through the sum in a debugger. If we encounter a bug in some iterations in a parallel&lt;/p&gt;&lt;code&gt;for&lt;/code&gt;, we need to correlate the launching of that particular work, and that actual work. For example, if we stop the program in the debugger and find ourselves within a thread performing some iterations of the parallel&lt;code&gt;for&lt;/code&gt;, we have lost context about who launched that work (in single-core code, this information is universally provided with call stacks).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first problem can be partly addressed with a new underlying layer which our code uses instead of the underlying kernel primitives. In many codebases, this layer is called a ‚Äújob system‚Äù, or a ‚Äúworker thread pool‚Äù. In those cases, the program prepares a set of threads once, and these threads simply wait for work, and execute it once they receive it:&lt;/p&gt;
    &lt;code&gt;void JobThread(void *p)
{
  for(;;)
  {
    Job job = GetNextJob(...);
    job.Function(job.params);
  }
}

void SumJob(SumParams *p)
{
  ...
}

S64 ComputeSum(S64 *values, S64 count)
{
  Job jobs[NUMBER_OF_CORES] = {0};
  for(S64 core_idx = 0; core_idx &amp;lt; NUMBER_OF_CORES; core_idx += 1)
  {
    ...
    jobs[core_idx] = LaunchJob(SumJob, &amp;amp;params[core_idx]);
  }

  S64 sum = 0;
  for(S64 core_idx = 0; core_idx &amp;lt; NUMBER_OF_CORES; core_idx += 1)
  {
    WaitForJob(jobs[core_idx]);
    sum += params[core_idx].sum;
  }

  return sum;
}&lt;/code&gt;
    &lt;p&gt;In this case, there is still some overhead incurred by sending to and receiving information from the job threads, but it is significantly lighter than interacting with the kernel.&lt;/p&gt;
    &lt;p&gt;But it hasn‚Äôt improved the higher level code very much at all‚Äîwe‚Äôve simply replaced ‚Äúthreads‚Äù with ‚Äújobs‚Äù. The latter two problems hold. We still need to perform an entire dance in order to set up a ‚Äúwide loop‚Äù‚Äîa ‚Äúparallel &lt;code&gt;for&lt;/code&gt;‚Äù, which scatters control flow for a problem across both source code, and coherent contexts (CPU cores, call stacks) at runtime.&lt;/p&gt;
    &lt;p&gt;In this concrete case‚Äîcomputing a sum in parallel‚Äîthis is not a huge concern. Will it compute a sum in parallel? Yes. Does it have very few shared data writes? Yes. Can you parallelize all similarly parallelizable problems this way? Yes. But, we pay the costs of these problems every time we use this mechanism. If we have to pay that cost very frequently throughout a problem, it can become onerous to write, debug, and maintain all of this machinery.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Job System (And Its Flaws)&lt;/head&gt;
    &lt;p&gt;One desirable property of the parallel &lt;code&gt;for&lt;/code&gt; is that all jobs‚Äîwhich execute at roughly the same time, across some number of cores‚Äîare identical in their ‚Äúshape‚Äù. Each job thread participating in the problem is executing exactly the same code‚Äîwe simply parameterize each job slightly differently, to distribute different subproblems to different cores. This makes understanding, predicting, profiling, and debugging such code much simpler.&lt;/p&gt;
    &lt;p&gt;Furthermore, within a parallel &lt;code&gt;for&lt;/code&gt;, each job‚Äôs lifetime is scoped by the originating single-core code‚Äôs lifetime. Each job begins and ends within some scope‚Äîthe scope responsible for launching, then joining, all of the jobs. This means no substantial lifetime management complexity occurs‚Äîallocations for a parallel &lt;code&gt;for&lt;/code&gt; are as simple as for normal single-core code.&lt;/p&gt;
    &lt;p&gt;But in practice, the mechanism often used to implement parallel &lt;code&gt;for&lt;/code&gt;s‚Äîthe job system‚Äîis rarely only used in this way, which is understandable, given its highly generic structure. For example, it‚Äôs also often used to launch a number of heterogeneous jobs. In these cases, it becomes even more difficult to understand the context of a particular job‚Äîwho launched it, and in what context? It also becomes more difficult to comprehensively understand a system‚Äîbecause there is such a large number of possible configurations of thread states, it can be difficult to ensure a threaded system is robust in all cases.&lt;/p&gt;
    &lt;p&gt;These jobs are also often not bounded by their launcher scope‚Äîas such, more engineering must be spent on managing resources, like memory allocations, whose lifetimes are now defined by what happens across multiple threads in multiple contexts.&lt;/p&gt;
    &lt;p&gt;And this is, really, the tip of the iceberg. In more sophisticated systems, one might observe that there are dependencies between jobs, and jobs ought to be implicitly launched when their dependency jobs complete, creating an even longer (and more difficult to inspect) chain of context related to some independent through line of work.&lt;/p&gt;
    &lt;p&gt;Ultimately, this presents recurring writing, reading, debugging, and maintenance costs that don‚Äôt exist in normal single-core code. All of the costs incurred by this job system design‚Äîwhether used in a parallel &lt;code&gt;for&lt;/code&gt; or otherwise‚Äîare paid any time new parallel work is introduced, or any time parallel work is maintained.&lt;/p&gt;
    &lt;p&gt;Now, if we have few parts of our code that can be parallelized in this way, then this is not a significant cost.&lt;/p&gt;
    &lt;p&gt;‚Ä¶But that if is doing a lot of heavy lifting.&lt;/p&gt;
    &lt;p&gt;In practice, I‚Äôve found that an enormous number of systems are riddled with opportunities for parallelization, because of a lack of serial dependence between many of their parts. But, if taking advantage of every instance of serial independence requires significantly more engineering than just accepting single-core performance, then in many cases, programmers will opt for the latter.&lt;/p&gt;
    &lt;p&gt;Again‚Äîdoes this mean that a job system cannot be used to do such parallelization in these systems? No. But, it also means that we pay the costs of using this job system‚Äîthe more moving parts; the extra code and concepts to write, read, and debug‚Äîmuch more frequently, if we‚Äôd like to take advantage of this widespread serial independence, or if we‚Äôd like any algorithm in particular to scale its performance with the number of cores.&lt;/p&gt;
    &lt;head rend="h2"&gt;Single-Core By Default&lt;/head&gt;
    &lt;p&gt;The critical insight I learned from speaking with Casey on this topic was that a significant reason why these costs arise is because of the careful organization a system needs in order to switch from single-core to multi-core code. Mechanisms like job systems and their special case usage in parallel &lt;code&gt;for&lt;/code&gt;s represent, in some sense, the most conservative application of multi-core code. The vast majority of code is written as single-core, and a few carveouts are made when multi-core is critically important. In other words, code remains single-core by default, and in a few special cases, work is done to briefly hand work off to a multi-core system.&lt;/p&gt;
    &lt;p&gt;Because the context of code execution changes across time‚Äîbecause work is handed off from one system to another‚Äîit necessarily requires more code to set up, and it is more difficult to debug and understand the full context at any point in time.&lt;/p&gt;
    &lt;p&gt;But is this the best approach? Perhaps, instead of writing single-core code (which sometimes goes wide) by default, we can write multi-core code (which sometimes goes narrow) by default.&lt;/p&gt;
    &lt;p&gt;What does this look like in practice?&lt;/p&gt;
    &lt;p&gt;There‚Äôs a good chance that you‚Äôve already experienced this style in other areas of programming‚Äînotably, in GPU shader programming.&lt;/p&gt;
    &lt;p&gt;GPU shaders‚Äîlike vertex or pixel shaders, used in a traditional GPU rendering pipeline‚Äîare written such that they are multi-core by default. You author a single function (the entry point of the shader), but this function is executed on many cores, always, implicitly. The language constructs and rules are arranged in such a way that data reads and writes are always scoped by whatever core happens to be executing the code. A single execution of a vertex shader is scoped to a vertex‚Äîa pixel shader to a pixel‚Äîand so on.&lt;/p&gt;
    &lt;p&gt;Because the fundamental, underlying architecture is always multi-core by default, and because there is little involvement of each specific shader in how the multi-core parallelism is achieved, GPU programming enjoys enormous performance benefits, and yet as the shader programmer, it feels that there are few costs to pay for it. So few, in fact, that it feels more like artistic scripting, to the degree that someone can build an entire website‚ÄîShadertoy‚Äîbuilt around rapid-iteration, high-performance, visual GPU scripting.&lt;/p&gt;
    &lt;p&gt;Wait a minute‚Ä¶ ‚Äúhigh performance‚Äù, ‚Äúrapid-iteration scripting‚Äù? It seems like many believe that these are mutually exclusive!&lt;/p&gt;
    &lt;p&gt;Why does CPU programming feel so different?&lt;/p&gt;
    &lt;p&gt;Contrast the GPU programming model to the usual CPU programming model‚Äîyou author a single function (the entry point of your program), which is scheduled onto a single core only, normally by a kernel scheduler, using a single thread state. This model is, in contrast, single-core by default.&lt;/p&gt;
    &lt;p&gt;Long story short: it doesn‚Äôt have to be!&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Core By Default&lt;/head&gt;
    &lt;p&gt;Let‚Äôs begin by exactly inverting the approach. Instead of having a single thread which kicks off work to many threads, let‚Äôs just have many threads, all running the same code, by default. In a sense, let‚Äôs have just one big parallel &lt;code&gt;for&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;void BootstrapEntryPoint(void)
{
  Thread threads[NUMBER_OF_CORES] = {0};
  for(S64 thread_idx = 0; thread_idx &amp;lt; NUMBER_OF_CORES; thread_idx += 1)
  {
    threads[thread_idx] = LaunchThread(EntryPoint, (void *)thread_idx);
  }
  for(S64 thread_idx = 0; thread_idx &amp;lt; NUMBER_OF_CORES; thread_idx += 1)
  {
    JoinThread(threads[thread_idx]);
  }
}

void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;
  // program's actual work occurs here!
}&lt;/code&gt;
    &lt;p&gt;To click into an architecture which assumes a single-threaded entry point, we start with a &lt;code&gt;BootstrapEntryPoint&lt;/code&gt;. But the only work this function actually does is launch all of the threads executing the actual entry point, &lt;code&gt;EntryPoint&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs consider the earlier summation example. First, let‚Äôs just take the original single-threaded code, and put it into &lt;code&gt;EntryPoint&lt;/code&gt;, and see how we can continue from there.&lt;/p&gt;
    &lt;code&gt;void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;

  // we obtain these somehow:
  S64 *values = ...;
  S64 values_count = ...;

  // compute the sum
  S64 sum = 0;
  for(S64 idx = 0; idx &amp;lt; values_count; idx += 1)
  {
    sum += values[idx];
  }
}&lt;/code&gt;
    &lt;p&gt;What is actually happening? Well, we‚Äôre ‚Äúcomputing the sum across many cores‚Äù. That is‚Ä¶ technically true! Ship it!&lt;/p&gt;
    &lt;p&gt;There‚Äôs just one little problem‚Ä¶ This is just as fast as the single-core version, except it also uses enormously more energy, and steals time from other tasks the CPU could be doing, because it is simply duplicating all work on each core.&lt;/p&gt;
    &lt;p&gt;But, if we were to measure this, and consider the real costs, and profile the actual code, the profile would look something like this:&lt;/p&gt;
    &lt;p&gt;Duplication itself is not, in principle, a problem, and it is sometimes not to be avoided, because deduplication can sometimes be more expensive than duplication. For instance, communicating the result of a single &lt;code&gt;add&lt;/code&gt; instruction across many threads‚Äîto deduplicate the work of that &lt;code&gt;add&lt;/code&gt;‚Äîwould be vastly more expensive than simply duplicating the &lt;code&gt;add&lt;/code&gt; itself. We do want deduplication, but only when necessary, or when it actually helps.&lt;/p&gt;
    &lt;p&gt;So, where does it help? Unsurprisingly in this case, the dominating cost‚Äîthe reason we are using multiple cores at all‚Äîis the sum across all elements in &lt;code&gt;values&lt;/code&gt;. We want to distribute different parts of the sum across cores. To start, instead of computing the full sum, we can instead compute a per-thread sum. After each per-thread sum is computed, we can then combine them:&lt;/p&gt;
    &lt;code&gt;void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;

  // we obtain these somehow:
  S64 *values = ...;
  S64 values_count = ...;

  // decide this thread's subset of the sum
  S64 thread_first_value_idx = ???;
  S64 thread_opl_value_idx = ???; // one past last

  // compute the thread sum
  S64 thread_sum = 0;
  for(S64 idx = thread_first_value_idx;
      idx &amp;lt; thread_opl_value_idx;
      idx += 1)
  {
    thread_sum += values[idx];
  }

  // combine the thread sums
  S64 sum = ???;
}&lt;/code&gt;
    &lt;p&gt;We have two blanks to fill in:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;How do we decide each thread‚Äôs subset of work?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How do we combine all thread sums?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let‚Äôs tackle each.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Deciding Per-Thread Work&lt;/head&gt;
    &lt;p&gt;Currently, the only input I‚Äôve provided each thread is its index, which would be in [0, N), where N is the number of threads. This is stored in the local variable &lt;code&gt;thread_idx&lt;/code&gt;, which will have a different value in [0, N) for each thread. This is an easy example, because a good way to distribute the sum work across all threads is to uniformly distribute the number of values to sum amongst the threads. This means we are simply mapping [0, M) to [0, N), where M is the number of values‚Äî&lt;code&gt;values_count&lt;/code&gt;‚Äîand N is the number of threads.&lt;/p&gt;
    &lt;p&gt;We can almost compute this as follows:&lt;/p&gt;
    &lt;code&gt;S64 values_count = ...;
S64 thread_idx = ...;
S64 thread_count = NUMBER_OF_CORES;

S64 values_per_thread = values_count / thread_count;
S64 thread_first_value_idx = values_per_thread * thread_idx;
S64 thread_opl_value_idx = thread_first_value_idx + values_per_thread;&lt;/code&gt;
    &lt;p&gt;This is almost right, but only almost, because we also need to account for the case where &lt;code&gt;values_count&lt;/code&gt; is not cleanly subdivided by &lt;code&gt;thread_count&lt;/code&gt;. Because our &lt;code&gt;values_per_thread&lt;/code&gt; will truncate to the next lowest integer, this current distribution will underestimate the number of values we need to compute per thread, by anywhere from 0 (if it divides cleanly) to &lt;code&gt;thread_count-1&lt;/code&gt; values‚Äîor in other words, the remainder of the division.&lt;/p&gt;
    &lt;p&gt;Thus, the number of values this division underestimates by‚Äîthe ‚Äúleftovers‚Äù‚Äîcan be computed as follows:&lt;/p&gt;
    &lt;code&gt;S64 leftover_values_count = values_count % thread_count;&lt;/code&gt;
    &lt;p&gt;We can then distribute these leftovers amongst the first &lt;code&gt;leftover_values_count&lt;/code&gt; threads:&lt;/p&gt;
    &lt;code&gt;// compute the values-per-thread, &amp;amp; number of leftovers
S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;

// determine if the current thread gets a leftover
// (we distribute them amongst the first threads in the group)
B32 thread_has_leftover = (thread_idx &amp;lt; leftover_values_count);

// decide on how many leftovers have been distributed before this
// thread's range (just the thread index, clamped by the number of
// leftovers)
S64 leftovers_before_this_thread_idx = 0;
if(thread_has_leftover)
{
  leftovers_before_this_thread_idx = thread_idx;
}
else
{
  leftovers_before_this_thread_idx = leftover_values_count;
}

// decide on the [first, opl) range:
// we shift `first` by the number of leftovers we've placed earlier,
// and we shift `opl` by 1 if we have a leftover.
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = thread_first_value_idx + values_per_thread;
if(thread_has_leftover)
{
  thread_opl_value_idx += 1;
}&lt;/code&gt;
    &lt;p&gt;Or more succinctly:&lt;/p&gt;
    &lt;code&gt;S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &amp;lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);&lt;/code&gt;
    &lt;p&gt;Now, using this &lt;code&gt;[first, opl)&lt;/code&gt; calculation, we can arrange each thread to only loop over its associated range, thus not duplicating all sum work done by other threads.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Combining All Thread Sums&lt;/head&gt;
    &lt;p&gt;Now, how might we combine each thread‚Äôs sum to form the total sum? There are two simple options available: (a) we can define a global sum counter to which each thread atomically adds (using atomic add intrinsics) its per-thread sum, or (b) we can define global storage which stores all thread sums, and each thread can duplicate the work of computing the total sum.&lt;/p&gt;
    &lt;p&gt;For (a), we just need to define &lt;code&gt;sum&lt;/code&gt; as &lt;code&gt;static&lt;/code&gt;, and atomically add each &lt;code&gt;thread_sum&lt;/code&gt; to it:&lt;/p&gt;
    &lt;code&gt;static S64 sum = 0;

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  AtomicAddEval64(&amp;amp;sum, thread_sum);
}&lt;/code&gt;
    &lt;p&gt;Note: This has a downside in that only one thread group can be executing this codepath at once. This is sometimes not a practical concern, since if we are going wide at all, we are often using all available cores to do so, and it is likely not beneficial to also have some other thread group executing the same codepath for a different purpose. That said, it‚Äôs now a new hidden restriction of this code, and it can be a critical problem. There are some techniques we can use to solve this problem, which I will cover later‚Äîfor now, the important concept is that the data is shared across participating threads.&lt;/p&gt;
    &lt;p&gt;For (b), we‚Äôd instead have a global table, and duplicate the work of summing across all thread sums. But we can only do that after we know that each thread has completed its summation work‚Äîotherwise we‚Äôd potentially add some other thread‚Äôs sum before it was actually computed!&lt;/p&gt;
    &lt;code&gt;static S64 thread_sums[NUMBER_OF_CORES] = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  thread_sums[thread_idx] = thread_sum;

  // ??? need to wait here for all threads to finish!

  S64 sum = 0;
  for(S64 t_idx = 0; t_idx &amp;lt; NUMBER_OF_CORES; t_idx += 1)
  {
    sum += thread_sums[t_idx];
  }
}&lt;/code&gt;
    &lt;p&gt;That extra waiting requirement might seem like an argument in favor of (a), but we‚Äôd actually need the same mechanism if we did (a) once we wanted to actually use the sum‚Äîwe‚Äôd need to wait for all threads to reach some point, so that we‚Äôd know that they‚Äôd all atomically updated &lt;code&gt;sum&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We can use a barrier to do this. In (a):&lt;/p&gt;
    &lt;code&gt;static S64 sum = 0;
static Barrier barrier = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  AtomicAddEval64(&amp;amp;sum, thread_sum);
  BarrierSync(barrier);
  // `sum` is now fully computed!
}&lt;/code&gt;
    &lt;p&gt;And in (b):&lt;/p&gt;
    &lt;code&gt;static S64 thread_sums[NUMBER_OF_CORES] = {0};
static Barrier barrier = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  thread_sums[thread_idx] = thread_sum;

  BarrierSync(barrier);

  S64 sum = 0;
  for(S64 t_idx = 0; t_idx &amp;lt; NUMBER_OF_CORES; t_idx += 1)
  {
    sum += thread_sums[t_idx];
  }
  // `sum` is now fully computed!
}&lt;/code&gt;
    &lt;p&gt;At this point, we have everything we need for both (a) and (b). Both are simple, and likely negligibly different. (a) requires atomic summation across all the threads, which implies hardware-level synchronization, whereas (b) duplicates the sum of all per-thread sums‚Äîthese likely subtly differ in there costs, but not by much when compared to the actual &lt;code&gt;values&lt;/code&gt; summation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going Narrow&lt;/head&gt;
    &lt;p&gt;Now, while I hope this summation example has been a useful introduction, I know it‚Äôs a bit contrived, and incomplete. Specifically, it‚Äôs missing two key parts of any program: inputs and outputs. What are we doing with this sum, and how do we use that in producing some form of output, and how do obtain the inputs, and store them in &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;values_count&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Let‚Äôs barely extend the summation example with stories for the inputs and outputs. For the inputs, let‚Äôs say that we read &lt;code&gt;values&lt;/code&gt; out of a binary file, which just contains the whole array stored as it will be in memory. For the outputs, let‚Äôs say that we just print the sum to &lt;code&gt;stdout&lt;/code&gt; with &lt;code&gt;printf&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Printing out the sum will be the easiest part, so let‚Äôs begin with that.&lt;/p&gt;
    &lt;p&gt;In single-core code, after computing the sum, we‚Äôd simply call &lt;code&gt;printf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;S64 sum = ...;
// ...
printf("Sum: %I64d", sum);&lt;/code&gt;
    &lt;p&gt;We can start by just doing the same in our ‚Äúmulti-core by default‚Äù code. What we‚Äôll find is that our output looks something like this:&lt;/p&gt;
    &lt;code&gt;Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678&lt;/code&gt;
    &lt;p&gt;And obviously, we only want our many cores to be involved with the majority of the computation, but we only need one thread to do the actual &lt;code&gt;printf&lt;/code&gt;. In other words, we need to go narrow. Luckily, going narrow from wide code is much simpler than going wide from narrow code:&lt;/p&gt;
    &lt;code&gt;S64 sum = ...;
// ...
if(thread_idx == 0)
{
  printf("Sum: %I64d", sum);
}&lt;/code&gt;
    &lt;p&gt;We simply need to mask away the work from all threads except one.&lt;/p&gt;
    &lt;p&gt;Now, let‚Äôs consider the input problem. We need to compute &lt;code&gt;values_count&lt;/code&gt; based on the size of some input file, allocate storage for &lt;code&gt;values&lt;/code&gt;, and then fill &lt;code&gt;values&lt;/code&gt; by reading all data from the file.&lt;/p&gt;
    &lt;p&gt;Single-threaded code to do that might look something like this:&lt;/p&gt;
    &lt;code&gt;char *input_path = ...;
File file = FileOpen(input_path);
S64 size = SizeFromFile(file);
S64 values_count = (size / sizeof(S64));
S64 *values = (S64 *)Allocate(values_count * sizeof(values[0]));
FileRead(file, 0, values_count * sizeof(values[0]), values);
FileClose(file);&lt;/code&gt;
    &lt;p&gt;So, naturally, one option is to simply do this narrow:&lt;/p&gt;
    &lt;code&gt;if(thread_idx == 0)
{
  char *input_path = ...;
  File file = FileOpen(input_path);
  S64 size = SizeFromFile(file);
  S64 values_count = (size / sizeof(S64));
  S64 *values = (S64 *)Allocate(values_count * sizeof(values[0]));
  FileRead(file, 0, values_count * sizeof(values[0]), values);
  FileClose(file);
}
BarrierSync(barrier); // `values` and `values_count` ready after this point&lt;/code&gt;
    &lt;p&gt;This will work, but we somehow need to broadcast the computed values of &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;values_count&lt;/code&gt; across all threads. One easy way to do this is simply to pull them out as &lt;code&gt;static&lt;/code&gt;, like we did for shared data earlier:&lt;/p&gt;
    &lt;code&gt;static S64 values_count = 0;
static S64 *values = 0;
if(thread_idx == 0)
{
  char *input_path = ...;
  File file = FileOpen(input_path);
  S64 size = SizeFromFile(file);
  values_count = (size / sizeof(S64));
  values = (S64 *)Allocate(values_count * sizeof(values[0]));
  FileRead(file, 0, values_count * sizeof(values[0]), values);
  FileClose(file);
}
BarrierSync(barrier);&lt;/code&gt;
    &lt;p&gt;But consider that we might not want to do this completely single-core. It might be the case that it‚Äôs more efficient to issue &lt;code&gt;FileRead&lt;/code&gt;s from many threads, rather than just one. In practice, this is partly true (although, depending on the full stack‚Äîthe kernel, the storage drive hardware, and so on‚Äîit may not be beneficial past some number of threads, and for certain read sizes).&lt;/p&gt;
    &lt;p&gt;So let‚Äôs say we‚Äôd like to do the &lt;code&gt;FileRead&lt;/code&gt;s wide now also. We need to still allocate &lt;code&gt;values&lt;/code&gt; on a single thread, but once that is done, we can distribute the rest of the work trivially:&lt;/p&gt;
    &lt;code&gt;// we can open the file on all threads (though for some reasons
// we may want to deduplicate this too - for simplicity I am
// keeping it on all threads)
File file = FileOpen(input_path);

// calculate number of values and allocate (only single thread)
static S64 values_count = 0;
static S64 *values = 0;
if(thread_idx == 0)
{
  S64 size = SizeFromFile(file);
  values_count = (size / sizeof(S64));
  values = (S64 *)Allocate(values_count * sizeof(values[0]));
}
BarrierSync(barrier);

// compute thread's range of values (same calculation as before)
S64 thread_first_value_idx = ...;
S64 thread_opl_value_idx = ...;

// do read of this thread's portion
S64 num_values_this_thread = (thread_opl_value_idx - thread_first_value_idx);
FileRead(file,
         thread_first_value_idx*sizeof(values[0]),
         num_values_this_thread*sizeof(values[0]),
         values + thread_first_value_idx);

// close file on all threads
FileClose(file);&lt;/code&gt;
    &lt;p&gt;It‚Äôs much simpler, now‚Äîcompared to, say, the original parallel &lt;code&gt;for&lt;/code&gt; case‚Äîto simply take another part of the problem like this, and to also distribute it amongst threads, simply because wide is the default shape of the program.&lt;/p&gt;
    &lt;p&gt;Instead of spending most programming time acting like we‚Äôre on a single-core machine, we simply assume our actual circumstances, which is that we have several cores, and sometimes we need to tie it all together with a few serial dependencies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-Uniform Work Distributions&lt;/head&gt;
    &lt;p&gt;Let‚Äôs take a look at our earlier calculations to distribute portions of the &lt;code&gt;values&lt;/code&gt; array:&lt;/p&gt;
    &lt;code&gt;S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &amp;lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);&lt;/code&gt;
    &lt;p&gt;This was an easy case, because uniformly dividing portions of &lt;code&gt;values&lt;/code&gt; produces nearly uniform work across all cores.&lt;/p&gt;
    &lt;p&gt;If, in a different scenario, we don‚Äôt produce nearly uniform work across all cores, we have a problem: some cores will finish their work in some section long before others, and they‚Äôll be stuck at the next barrier synchronization point while the other cores finish. This diminishes the returns we obtain from going wide in the first place.&lt;/p&gt;
    &lt;p&gt;Thus, it‚Äôs always important to uniformly distribute work whenever it‚Äôs possible. The exact strategy for doing so will vary by problem. But I‚Äôve noticed three common strategies:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Uniformly distributing inputs produces uniformly distributed work (the case with the sum). So, we can decide the work distribution upfront.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each portion of an input requires a variable amount of per-core work. The work is relatively bounded, and there are many portions of input (larger than the core count). So, we can dynamically grab work on each core, so cores which complete smaller work first receive more, whereas cores that are stuck on longer work leave more units of work for other cores.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each portion of an input requires a variable amount of per-core work, but there is a small number (lower than the core count) of potentially very long sequences of work. We can attempt to redesign this algorithm such that it can be distributed more uniformly instead.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We‚Äôve already covered the first strategy with the sum example‚Äîlet‚Äôs look at the latter two.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dynamically Assigning Many Variable-Work Tasks&lt;/head&gt;
    &lt;p&gt;Let‚Äôs consider a case where we have many units of work‚Äî‚Äútasks‚Äù‚Äîand we‚Äôd like to distribute these tasks across cores. We may start by distributing the tasks in the same way that we distributed values to sum in the earlier example:&lt;/p&gt;
    &lt;code&gt;Task *tasks = ...;
S64 tasks_count = ...;
S64 thread_first_task_idx = ...;
S64 thread_opl_task_idx = ...;
for(S64 task_idx = thread_first_task_idx;
    task_idx &amp;lt; thread_last_task_idx;
    task_idx += 1)
{
  // do task
}&lt;/code&gt;
    &lt;p&gt;If each task requires a variable amount of work, then a profile of the program might look something like this:&lt;/p&gt;
    &lt;p&gt;Instead of deciding the task division upfront, we can dynamically assign tasks, such that the threads which are occupied (performing larger tasks) are not assigned more tasks until they‚Äôre done, and threads which complete shorter tasks earlier are quickly assigned more tasks, if available.&lt;/p&gt;
    &lt;p&gt;We can do that simply with a shared atomic counter, which each thread increments:&lt;/p&gt;
    &lt;code&gt;Task *tasks = ...;
S64 tasks_count = ...;

// set up the counter
static S64 task_take_counter = 0;
task_take_counter = 0;
BarrierSync(barrirer);

// loop on all threads - take tasks as long as we can
for(;;)
{
  S64 task_idx = AtomicIncEval64(&amp;amp;task_take_counter) - 1;
  if(task_idx &amp;gt;= tasks_count)
  {
    break;
  }
  // do task
}&lt;/code&gt;
    &lt;p&gt;This will dynamically distribute tasks across the cores, so that a profile of the program will look more like this:&lt;/p&gt;
    &lt;head rend="h3"&gt;Redesigning Algorithms For Uniform Work Distribution&lt;/head&gt;
    &lt;p&gt;Dynamically assigning tasks to cores will help in many cases, but it gets less effective if tasks are highly variable, to the point of sometimes being exceedingly long (e.g. many times more expensive than smaller tasks), or if there are fewer tasks than the number of cores.&lt;/p&gt;
    &lt;p&gt;In these cases, it can often be helpful to reconsider the serial independencies within a single task, or whether the same effect as a highly serially-dependent algorithm can be provided by an alternative highly serially-independent algorithm. Can a single task be subdivided further? Can it be performed in a different way? Can serially-dependent work be untangled from heavier work which can be done in a serially-independent way?&lt;/p&gt;
    &lt;p&gt;The answers to such questions are highly problem-specific, so it‚Äôs impossible to offer substantially more useful advice while staying similarly generic. But to illustrate that it‚Äôs sometimes possible‚Äîeven when counterintuitive‚ÄîI have an example problem from my recent work, in which finding more uniform work distribution required switching from a single-threaded comparison sort to a highly parallelizable radix sort.&lt;/p&gt;
    &lt;p&gt;In this problem, I had a small number of arrays that needed to be sorted, but these arrays were potentially very large, thus requiring a fairly expensive sorting pass.&lt;/p&gt;
    &lt;p&gt;My first approach was to simply distribute the comparison sort tasks themselves, so I would sort one array on a single core, while other cores would be sorting other arrays. But as I‚Äôve said, there were a relatively small number of arrays, and the arrays were large, so sorting was fairly expensive‚Äîthus, most cores were doing nothing, and simply waiting for the small number of cores performing sorts to finish.&lt;/p&gt;
    &lt;p&gt;This approach would‚Äôve worked fine if I had a larger number of smaller tasks. In fact, another part of the same program does distribute single-threaded comparison sort tasks in this way, because in that part of the problem, there are a larger number of smaller tasks.&lt;/p&gt;
    &lt;p&gt;In this case, I needed to sort array elements based on 64-bit integer keys. After sorting, the elements needed to be ordered such that their associated keys were ascending in value.&lt;/p&gt;
    &lt;p&gt;Conveniently, this can be done with a radix sort. I won‚Äôt cover the full details of the algorithm here (although I briefly covered it during a stream recently, which I recorded and uploaded here), but the important detail is that a radix sort requires a fixed number of O(N) passes over the array, and huge portions of work in each pass can be distributed uniformly across cores (in the same way that we distributed the sum work earlier).&lt;/p&gt;
    &lt;p&gt;Now, all cores participate in every larger sorting task, but they only perform a nearly uniform fraction of the work in each sort. This results in a much more uniform work distribution, and thus a much shorter total time spend sorting:&lt;/p&gt;
    &lt;p&gt;This is just one concrete example a larger pattern I‚Äôve noticed: In many problems, upon close examination, some serial dependencies can either vanish, or they can be untangled from heavier work.&lt;/p&gt;
    &lt;p&gt;In some problems, serially-dependent parts of the algorithm can be isolated, such that they prepare data which allows the rest of the algorithm to be done in a serially-independent fashion. Imagine a program which walks a linked list early, on a single core, to compute a layout in a serially-dependent way. This layout can then allow subsequent work to execute just using the full layout, rather than forcing that subsequent work to also include the serially-dependent pointer chasing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Single-Threaded, Just Better&lt;/head&gt;
    &lt;p&gt;Code which is multi-core by default feels like normal single-threaded code, just with a few extra constructs that express the missing information needed to execute on multiple cores. This style has some useful and interesting properties, which make it preferable in many contexts to many of the popular styles of multi-core code found in the wild.&lt;/p&gt;
    &lt;head rend="h3"&gt;Single-Core as a Parameterization&lt;/head&gt;
    &lt;p&gt;One interesting implication of code written in this way‚Äîto be multi-core by default‚Äîis that it offers a strict superset of functionality than code which is written to be single-core, because ‚Äúmulti-core‚Äù in this case includes ‚Äúsingle-core‚Äù, as one possible case. We can use the same code to execute on only a single core, simply by instead executing our entry point on a single thread, and parameterizing that thread with &lt;code&gt;thread_idx = 0&lt;/code&gt; and &lt;code&gt;thread_count = 1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In that case, one core necessarily receives all of the work. &lt;code&gt;BarrierSync&lt;/code&gt;s turn into no-ops, since there is only one thread (there are no other threads to wait for). Thus, it is equivalent to single-core functionality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler Debugging&lt;/head&gt;
    &lt;p&gt;This style of multi-core programming requires far less busywork and machinery in order to use multiple cores for some codepath. But one of the problems I mentioned with job systems and parallel &lt;code&gt;for&lt;/code&gt;s earlier was not only that they require more busywork and machinery, but that they‚Äôre also more difficult to debug.&lt;/p&gt;
    &lt;p&gt;In this case, debugging is much simpler‚Äîin fact, it doesn‚Äôt look all that different from single-core debugging. At every point, you have access to a full call stack, and all contextual data which led to whatever point in time that you happen to be inspecting in a debugger.&lt;/p&gt;
    &lt;p&gt;Furthermore, because all threads involved are nearly homogeneous (rather than the generic job system, where all threads are heterogeneous at all times), debugging a single thread is a lot like debugging all threads. This is especially true because‚Äîbetween barrier synchronization points‚Äîthe threads are all executing the same code. In other words, the context and state on one thread is likely to be highly informative of the context and state on all threads.&lt;/p&gt;
    &lt;head rend="h3"&gt;Access To The Full Stack&lt;/head&gt;
    &lt;p&gt;Because the context for some through line of computation frequently changes in traditional job systems, extra machinery must be involved to pipe data from one context to another‚Äîacross jobs and threads‚Äîand maintain any associated allocations and lifetimes. But in this style, resources and lifetimes are kept as simple as they are in single-threaded code.&lt;/p&gt;
    &lt;p&gt;The stack, containing all contextual state at any point, becomes a single bucket for useful thread-local storage. In a job system, the stack is useful multi-core thread-local storage, but only for the duration of the job. The job is equivalent to the inner body of a &lt;code&gt;for&lt;/code&gt;‚Äîthis is a tiny, fragmentary scope. With this style, the entire stack is available, at any point.&lt;/p&gt;
    &lt;head rend="h2"&gt;Codebase Support&lt;/head&gt;
    &lt;p&gt;I‚Äôve found some useful patterns which can be extracted and widely used in code which is multi-core by default. These patterns seem as widely applicable as arenas‚Äîas such, they can be a useful addition to a codebase‚Äôs base layer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Thread-Local Group Data&lt;/head&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;LaneIdx()&lt;/code&gt;, &lt;code&gt;LaneCount()&lt;/code&gt;, &lt;code&gt;LaneSync()&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The earlier example code frequently uses the &lt;code&gt;thread_idx&lt;/code&gt;, &lt;code&gt;thread_count&lt;/code&gt;, and &lt;code&gt;barrier&lt;/code&gt; variables. Passing these to every codepath which might need them is redundant and cumbersome. As such, they are good candidates for thread-local storage.&lt;/p&gt;
    &lt;p&gt;In my code, I‚Äôve bundled these into the base layer‚Äôs ‚Äúthread context‚Äù, which is a thread-local structure which is universally accessible‚Äîit‚Äôs where, for example, thread-local scratch arenas are stored.&lt;/p&gt;
    &lt;p&gt;This provides all code the ability to read its index within a thread group (&lt;code&gt;thread_idx&lt;/code&gt;), or the number of threads in its group (&lt;code&gt;thread_count&lt;/code&gt;), and to synchronize with other lanes (&lt;code&gt;BarrierSync&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;As I suggested earlier, any code‚Äôs caller can choose ‚Äúhow wide‚Äù‚Äîhow many cores‚Äîthey‚Äôd like to execute that code, by configuring this per-thread storage. In general, shallow parts of a call stack can decide how wide deeper parts of a call stack are executed. If some work is expected to be small (to the point where it doesn‚Äôt benefit from being executed on many cores), and other cores can be doing other useful work, then before doing that work, the calling code can simply set &lt;code&gt;thread_idx = 0&lt;/code&gt;, &lt;code&gt;thread_count = 1&lt;/code&gt;, and &lt;code&gt;barrier = {0}&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This means that a single thread may participate in many different thread groups‚Äîin other words, &lt;code&gt;thread_idx&lt;/code&gt; and &lt;code&gt;thread_count&lt;/code&gt; are not static within the execution of a single thread. Therefore, I found it appropriate to introduce another disambiguating term: lane. A lane is distinct from a thread in that a lane is simply one thread within a potentially-temporary group of threads, all executing the same code.&lt;/p&gt;
    &lt;p&gt;As such, in my terminology, &lt;code&gt;thread_idx&lt;/code&gt; is exposed as &lt;code&gt;LaneIdx()&lt;/code&gt;, and &lt;code&gt;thread_count&lt;/code&gt; is exposed as &lt;code&gt;LaneCount()&lt;/code&gt;. To synchronize with other lanes, a helper &lt;code&gt;LaneSync()&lt;/code&gt; is available, which just waits on the thread context‚Äôs currently selected barrier.&lt;/p&gt;
    &lt;head rend="h3"&gt;Uniformly Distributing Ranges Amongst Lanes&lt;/head&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;LaneRange(count)&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;I‚Äôve mentioned the following computation multiple times:&lt;/p&gt;
    &lt;code&gt;S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &amp;lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);&lt;/code&gt;
    &lt;p&gt;This is useful whenever a uniformly distributed range corresponds to uniformly distributed work amongst cores. As I mentioned, this is sometimes not desirable. But nevertheless, it‚Äôs an extremely common case. As such, I found it useful to expose this as &lt;code&gt;LaneRange(count)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Rng1U64 range = LaneRange(count);
for(U64 idx = range.min; idx &amp;lt; range.max; idx += 1)
{
  // ...
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Broadcasting Data Across Lanes&lt;/head&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;LaneSyncU64(value_ptr, source_lane_idx)&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;Earlier, we saw that when a variable needs to be shared across lanes, it can simply be marked as &lt;code&gt;static&lt;/code&gt;. I mentioned that this has the unfortunate downside that only a single group can be executing the code at one time, since one group of lanes could trample over the &lt;code&gt;static&lt;/code&gt; variable while another group is still using it. As I mentioned, this is sometimes not a concern (since it‚Äôs desirable to only have a single lane group executing some code), but it invisibly makes code inapplicable for some cases.&lt;/p&gt;
    &lt;p&gt;For example, let‚Äôs suppose I have some code which is written to be multi-core by default. Depending on the inputs to this codepath, I may want this to be executed‚Äîon the same inputs‚Äîwith all of my cores. But in other cases, I may want this to be executed with only a single core‚ÄîI may still want to execute this codepath on other cores, but for different inputs. That requires many lane groups to be executing the code at the same time, thus disqualifying the use of &lt;code&gt;static&lt;/code&gt; to share data amongst lanes within the same group.&lt;/p&gt;
    &lt;p&gt;To address this, I also created a simple mechanism to broadcast small amounts of data across lanes.&lt;/p&gt;
    &lt;p&gt;Each thread context also stores‚Äîin addition to a lane index, lane count, and lane group barrier‚Äîa pointer to a shared buffer, which is the same value for all lanes in the same group.&lt;/p&gt;
    &lt;p&gt;If one lane has a value which it needs to be broadcasted to other lanes‚Äîfor instance, if it allocated a buffer that the other lanes are about to fill‚Äîthen that value can be communicated in the following way:&lt;/p&gt;
    &lt;code&gt;U64 broadcast_size = ...;         // the number of bytes to broadcast
U64 broadcast_src_lane_idx = ...; // the index of the broadcasting lane
void *lane_local_storage = ...;   // unique for each lane
void *lane_shared_storage = ...;  // same for all lanes

// copy from broadcaster -&amp;gt; shared
if(LaneIdx() == broadcast_src_lane_idx)
{
  MemoryCopy(lane_shared_storage, lane_local_storage, broadcast_size);
}
LaneSync();

// copy from shared -&amp;gt; broadcastees
if(LaneIdx() != broadcast_src_lane_idx)
{
  MemoryCopy(lane_local_storage, lane_shared_storage, broadcast_size);
}
LaneSync();&lt;/code&gt;
    &lt;p&gt;I‚Äôve found that this shared buffer just needs to be big enough to broadcast 8 bytes, given that most small data can be broadcasted with a small number of 8 byte broadcasts, and larger data can be broadcasted with a single pointer broadcast.&lt;/p&gt;
    &lt;p&gt;I expose this mechanism with the following API:&lt;/p&gt;
    &lt;code&gt;U64 some_value = 0;
U64 src_lane_idx = 0;
LaneSyncU64(&amp;amp;some_value, src_lane_idx);
// after this line, all lanes share the same value for `some_value`&lt;/code&gt;
    &lt;p&gt;It might be used in the following way:&lt;/p&gt;
    &lt;code&gt;// set `values_count`, allocate for `values`, on lane 0, then
// broadcast their values to all other lanes:
S64 values_count = 0;
S64 *values = 0;
if(LaneIdx() == 0)
{
  values_count = ...;
  values = Allocate(sizeof(values[0]) * values_count);
}
LaneSyncU64(&amp;amp;values_count, 0);
LaneSyncU64(&amp;amp;values, 0);&lt;/code&gt;
    &lt;head rend="h3"&gt;Revisiting The Summation Example&lt;/head&gt;
    &lt;p&gt;With the above mechanisms, we can program the original summation example with the following steps.&lt;/p&gt;
    &lt;p&gt;First, we load the values from the file:&lt;/p&gt;
    &lt;code&gt;U64 values_count = 0;
S64 *values = 0;
{
  File file = FileOpen(input_path);
  values_count = SizeFromFile(file) / sizeof(values[0]);
  if(LaneIdx() == 0)
  {
    values = (S64 *)Allocate(values_count * sizeof(values[0]));
  }
  LaneSyncU64(&amp;amp;values);
  Rng1U64 value_range = LaneRange(values_count);
  Rng1U64 byte_range = R1U64(value_range.min * sizeof(values[0]),
                             value_range.max * sizeof(values[0]));
  FileRead(file, byte_range, values + value_range.min);
  FileClose(file);
}
LaneSync();&lt;/code&gt;
    &lt;p&gt;Then, we perform the sum across all lanes:&lt;/p&gt;
    &lt;code&gt;// grab the shared counter
S64 sum = 0;
S64 *sum_ptr = &amp;amp;sum;
LaneSyncU64(&amp;amp;sum_ptr, 0);

// calculate lane's sum
S64 lane_sum = 0;
Rng1U64 range = LaneRange(values_count);
for(U64 idx = range.min; idx &amp;lt; range.max; idx += 1)
{
  lane_sum += values[idx];
}

// contribute this lane's sum to the total sum
AtomicAddEval64(sum_ptr, lane_sum);
LaneSync();
LaneSyncU64(&amp;amp;sum, 0);&lt;/code&gt;
    &lt;p&gt;And finally, we output the sum value:&lt;/p&gt;
    &lt;code&gt;if(LaneIdx() == 0)
{
  printf(‚ÄùSum: %I64d\n‚Äù);
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Closing Thoughts&lt;/head&gt;
    &lt;p&gt;The concepts I‚Äôve shared in this post represent what I feel is a fundamental shift in how CPU code can be expressed, compared to the normal single-core code all programmers are familiar with. Through small, additional annotations to code‚Äîbasic concepts like &lt;code&gt;LaneIdx()&lt;/code&gt;, &lt;code&gt;LaneCount()&lt;/code&gt;, and &lt;code&gt;LaneSync()&lt;/code&gt;‚Äîall code can contain the information necessary to be executed wide, using multiple cores to better take advantage of serial independence.&lt;/p&gt;
    &lt;p&gt;The same exact code can also be executed on a single core, meaning through these extra annotations, that code becomes strictly more flexible‚Äîat the low level‚Äîthan its single-core equivalent which does not have these annotations.&lt;/p&gt;
    &lt;p&gt;Note that this is still not a comprehensive family of multithreading techniques, because it is strictly zooming in on one unique timeline of work, and how a single timeline can be accelerated using the fundamental multi-core reality of modern machines. But consider that programs often require multiple heterogeneous timelines of work, where one lane group is not in lockstep with others, and thus should not prohibit others from making progress.&lt;/p&gt;
    &lt;p&gt;But what I appreciate about the ideas in this post is that they do not unnecessarily introduce extra timelines. Communication between two heterogeneous timelines has intrinsic, relativity-related complexity. Those will always be necessary. But why pay that complexity cost everywhere, to accomplish simple multi-core execution?&lt;/p&gt;
    &lt;p&gt;I‚Äôm aware that, for many, these ideas are old news‚Äîindeed, everyone learns different things at different times. But in my own past programming, and when I look at the programming of many others, it seems that there is an awful lot of overengineering to do what seems trivial, and indeed what is trivial in other domains (like shader programming). So, for at least many people, these concepts do not seem well-known or old (even if they are in some circles and domains).&lt;/p&gt;
    &lt;p&gt;In any case, the concepts I‚Äôve shared in this post have been dramatically helpful in improving my ability to structure multi-core code without overcomplication, and it seemed like an important-enough shift to carefully document it here.&lt;/p&gt;
    &lt;p&gt;I hope it was similarly helpful to you, if you didn‚Äôt know the concepts, or if you did, I hope it was nonetheless interesting.&lt;/p&gt;
    &lt;p&gt;If you enjoyed this post, please consider subscribing. Thanks for reading.&lt;/p&gt;
    &lt;p&gt;-Ryan&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.rfleury.com/p/multi-core-by-default"/><published>2025-10-10T07:11:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536325</id><title>A Story About Bypassing Air Canada's In-Flight Network Restrictions</title><updated>2025-10-10T14:40:02.691266+00:00</updated><content>&lt;doc fingerprint="b728f5113168f28d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;1 Prologue&lt;/head&gt;
    &lt;p&gt;A while ago, I took a flight from Canada back to Hong Kong - about 12 hours in total with Air Canada.&lt;/p&gt;
    &lt;p&gt;Interestingly, the plane actually had WiFi:&lt;/p&gt;
    &lt;p&gt;However, the WiFi had restrictions. For Aeroplan members who hadn‚Äôt paid, it only offered Free Texting, meaning you could only use messaging apps like WhatsApp, Snapchat, and WeChat to send text messages, but couldn‚Äôt access other websites.&lt;/p&gt;
    &lt;p&gt;If you wanted unlimited access to other websites, it would cost CAD $30.75:&lt;/p&gt;
    &lt;p&gt;And if you wanted to watch videos on the plane, that would be CAD $39:&lt;/p&gt;
    &lt;p&gt;I started wondering: for the Free Texting service, could I bypass the messaging app restriction and access other websites freely?&lt;/p&gt;
    &lt;p&gt;Essentially, could I enjoy the benefits of the $30.75 paid service without actually paying the fee? After all, with such a long journey ahead, I needed something interesting to pass the 12 hours.&lt;/p&gt;
    &lt;p&gt;Since I could use WeChat in flight, I could also call for help from the sky.&lt;/p&gt;
    &lt;p&gt;Coincidentally, my roommate happens to be a security and networking expert who was on vacation at home. When I mentioned this idea, he thought it sounded fun and immediately agreed to collaborate. So we started working on it together across the Pacific.&lt;/p&gt;
    &lt;head rend="h2"&gt;2 The Process&lt;/head&gt;
    &lt;p&gt;After selecting the only available WiFi network &lt;code&gt;acwifi.com&lt;/code&gt; on the plane, just like other login-required WiFi networks, it popped up a webpage from &lt;code&gt;acwifi.com&lt;/code&gt; asking me to verify my Aeroplan membership. Once verified, I could access the internet.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a classic software development interview question: what happens after you type a URL into the browser and press enter?&lt;/p&gt;
    &lt;p&gt;For example, if you type &lt;code&gt;https://acwifi.com&lt;/code&gt; and only focus on the network request part, the general process is: DNS query -&amp;gt; TCP connection -&amp;gt; TLS handshake -&amp;gt; HTTP request and response.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs consider &lt;code&gt;github.com&lt;/code&gt; as our target website we want to access. Now let‚Äôs see how we can break through the network restrictions and successfully access &lt;code&gt;github.com&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;3 Approach 1: Domain Self-Signing&lt;/head&gt;
    &lt;p&gt;Since &lt;code&gt;acwifi.com&lt;/code&gt; is accessible but &lt;code&gt;github.com&lt;/code&gt; is not, could I disguise my server as &lt;code&gt;acwifi.com&lt;/code&gt; and route all request traffic through my server to access the target website (&lt;code&gt;github.com&lt;/code&gt;)?&lt;/p&gt;
    &lt;p&gt;The idea was roughly: I modify DNS records to bind our proxy server‚Äôs IP &lt;code&gt;137.184.231.87&lt;/code&gt; to &lt;code&gt;acwifi.com&lt;/code&gt;, then use a self-signed certificate to tell the browser that this IP and this domain are bound together, and it should trust it.&lt;/p&gt;
    &lt;p&gt;Let me first test this idea:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Unexpectedly, the IP was completely unreachable via &lt;code&gt;ping&lt;/code&gt;, meaning the IP was likely blocked entirely.&lt;/p&gt;
    &lt;p&gt;I tried other well-known IPs, like Cloudflare‚Äôs CDN IP, and they were also unreachable:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;It seems this approach won‚Äôt work. After all, if the IPs are directly blocked, no amount of disguise will help. This network likely maintains some IP whitelist (such as WhatsApp and WeChat‚Äôs egress IPs), and only IPs on the whitelist can be accessed.&lt;/p&gt;
    &lt;head rend="h2"&gt;4 Approach 2: DNS Port Masquerading&lt;/head&gt;
    &lt;p&gt;When the first approach failed, my roommate suggested a second approach: try using DNS service as a breakthrough:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This is good news! It means there are still ways to reach external networks, and DNS is one of them.&lt;/p&gt;
    &lt;p&gt;Looking at the record above, it shows our DNS query for &lt;code&gt;http418.org&lt;/code&gt; was successful, meaning DNS requests work.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.1 Arbitrary DNS Servers&lt;/head&gt;
    &lt;p&gt;My roommate then randomly picked another DNS server to see if the network had a whitelist for DNS servers:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;We can actually use arbitrary DNS servers - even better!&lt;/p&gt;
    &lt;head rend="h3"&gt;4.2 TCP Queries&lt;/head&gt;
    &lt;p&gt;The fact that arbitrary DNS servers can be queried successfully is excellent news. DNS typically uses UDP protocol, but would TCP-based DNS requests be blocked?&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;DNS TCP queries also work! This indicates the plane network‚Äôs filtering policy is relatively lenient, standing a chance of our subsequent DNS tunneling approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.3 Proxy Service on Port 53&lt;/head&gt;
    &lt;p&gt;It seems the plane network restrictions aren‚Äôt completely airtight - we‚Äôve found a ‚Äúbackdoor‚Äù in this wall.&lt;/p&gt;
    &lt;p&gt;So we had a clever idea: since the plane gateway doesn‚Äôt block DNS requests, theoretically we could disguise our proxy server as a DNS server, expose port 53 for DNS service, route all requests through the proxy server disguised as DNS requests, and thus bypass the restrictions.&lt;/p&gt;
    &lt;p&gt;My roommate spent about an hour setting up a proxy server exposing port 53 using xray 1, and sent me the configuration via WeChat:&lt;/p&gt;
    &lt;p&gt;The proxy server configuration my roommate set up with Xray included the following sample configuration:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;And I already had an xray client on my computer, so no additional software was needed to establish the connection.&lt;/p&gt;
    &lt;p&gt;Everything was ready. The exciting moment arrived - pressing enter to access &lt;code&gt;github.com&lt;/code&gt;:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The request actually succeeded! github.com returned a successful result!&lt;/p&gt;
    &lt;p&gt;This means we‚Äôve truly broken through the network restrictions and can access any website!&lt;/p&gt;
    &lt;p&gt;We hadn‚Äôt realized before that xray could be used in this clever way :)&lt;/p&gt;
    &lt;p&gt;Here we exploited a simple cognitive bias: not all services using port 53 are DNS query requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;5 Ultimate Approach: DNS Tunnel&lt;/head&gt;
    &lt;p&gt;If Approach 2 still didn‚Äôt work, we had one final trick up our sleeves.&lt;/p&gt;
    &lt;p&gt;Currently, the gateway only checks whether the port is 53 to determine if it‚Äôs a DNS request. But if the gateway were stricter and inspected the content of DNS request packets, it would discover that our requests are ‚Äúdisguised‚Äù as DNS queries rather than genuine DNS queries:&lt;/p&gt;
    &lt;p&gt;Since disguised DNS requests would be blocked, we could embed all requests inside genuine DNS request packets, making them DNS TXT queries. We‚Äôd genuinely be querying DNS, just with some extra content inside:&lt;/p&gt;
    &lt;p&gt;However, this ultimate approach requires a DNS Tunnel client to encapsulate all requests. I didn‚Äôt have such software on my computer, so this remained a theoretical ultimate solution that couldn‚Äôt be practically verified.&lt;/p&gt;
    &lt;head rend="h2"&gt;6 Conclusion&lt;/head&gt;
    &lt;p&gt;With the long journey ahead, my roommate and I spent about 4 hours remotely breaking through the network restrictions, having great fun in the process, proving that our problem-solving approach was indeed feasible.&lt;/p&gt;
    &lt;p&gt;The successful implementation of the solution was mainly thanks to my roommate, the networking expert, who provided remote technical and conceptual support.&lt;/p&gt;
    &lt;p&gt;The only downside was that although we broke through the network restrictions and could access any website, the plane‚Äôs bandwidth was extremely limited, making web browsing quite painful. So I didn‚Äôt spend much time browsing the web.&lt;/p&gt;
    &lt;p&gt;For the remaining hours, I rewatched the classic 80s time-travel movie: &lt;code&gt;"Back to the Future"&lt;/code&gt; , which was absolutely fantastic.&lt;/p&gt;
    &lt;p&gt;Last and not least, it‚Äôs the disclaimer:&lt;/p&gt;
    &lt;p&gt;This technical exploration is intended solely for educational and research purposes. We affirm our strict adherence to all relevant regulations and service terms throughout this project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ramsayleung.github.io/en/post/2025/a_story_about_bypassing_air_canadas_in-flight_network_restrictions/"/><published>2025-10-10T07:50:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536694</id><title>Show HN: I invented a new generative model and got accepted to ICLR</title><updated>2025-10-10T14:40:02.586677+00:00</updated><content>&lt;doc fingerprint="345c0b1f68b2c5a6"&gt;
  &lt;main&gt;&lt;p&gt;ü•≥ Accepted by ICLR 2025&lt;lb/&gt;üöÄ The code has been released &lt;/p&gt;&lt;p&gt;Discrete Distribution Networks&lt;/p&gt;&lt;p&gt;A novel generative model with simple principles and unique properties&lt;/p&gt;&lt;p&gt;This GIF demonstrates the optimization process of DDN for 2D probability density estimation:&lt;/p&gt;&lt;code&gt;blur_circles&lt;/code&gt; -&amp;gt; &lt;code&gt;QR_code&lt;/code&gt; -&amp;gt; &lt;code&gt;spiral&lt;/code&gt; -&amp;gt; &lt;code&gt;words&lt;/code&gt; -&amp;gt; &lt;code&gt;gaussian&lt;/code&gt; -&amp;gt; &lt;code&gt;blur_circles&lt;/code&gt; (same at beginning and end, completing a cycle)&lt;p&gt;Contributions of this paper:&lt;/p&gt;&lt;p&gt; Left: Illustrates the process of image reconstruction and latent acquisition in DDN. Each layer of DDN outputs &lt;lb/&gt; Right: Shows the tree-structured representation space of DDN's latent variables. Each sample can be mapped to a leaf node on this tree.&lt;/p&gt;&lt;p&gt;Reviews from ICLR:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I find the method novel and elegant. The novelty is very strong, and this should not be overlooked. This is a whole new method, very different from any of the existing generative models.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;This is a very good paper that can open a door to new directions in generative modeling.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We introduce a novel generative model, the Discrete Distribution Networks (DDN), that approximates data distribution using hierarchical discrete distributions. We posit that since the features within a network inherently capture distributional information, enabling the network to generate multiple samples simultaneously, rather than a single output, may offer an effective way to represent distributions. Therefore, DDN fits the target distribution, including continuous ones, by generating multiple discrete sample points. To capture finer details of the target data, DDN selects the output that is closest to the Ground Truth (GT) from the coarse results generated in the first layer. This selected output is then fed back into the network as a condition for the second layer, thereby generating new outputs more similar to the GT. As the number of DDN layers increases, the representational space of the outputs expands exponentially, and the generated samples become increasingly similar to the GT. This hierarchical output pattern of discrete distributions endows DDN with unique properties: more general zero-shot conditional generation and 1D latent representation. We demonstrate the efficacy of DDN and its intriguing properties through experiments on CIFAR-10 and FFHQ.&lt;/p&gt;&lt;p&gt;DDN enables more general zero-shot conditional generation. DDN supports zero-shot conditional generation across non-pixel domains, and notably, without relying on gradient, such as text-to-image generation using a black-box CLIP model. Images enclosed in yellow borders serve as the ground truth. The abbreviations in the table header correspond to their respective tasks as follows: ‚ÄúSR‚Äù stands for Super-Resolution, with the following digit indicating the resolution of the condition. ‚ÄúST‚Äù denotes Style Transfer, which computes Perceptual Losses with the condition.&lt;/p&gt;&lt;p&gt; (a) The data flow during the training phase of DDN is shown at the top. As the network depth increases, the generated images become increasingly similar to the training images. Within each Discrete Distribution Layer (DDL), &lt;/p&gt;&lt;p&gt;Here, &lt;/p&gt;&lt;p&gt;The numerical values at the bottom of each figure represent the Kullback-Leibler (KL) divergence. Due to phenomena such as ‚Äúdead nodes‚Äù and ‚Äúdensity shift‚Äù, the application of Gradient Descent alone fails to properly fit the Ground Truth (GT) density. However, by employing the Split-and-Prune strategy, the KL divergence is reduced to even lower than that of the Real Samples. For a clearer and more comprehensive view of the optimization process, see the 2D Density Estimation with 10,000 Nodes DDN page.&lt;/p&gt;&lt;p&gt;The text at the top is the guide text for that column.&lt;/p&gt;&lt;p&gt;Columns 4 and 5 display the generated results under the guidance of other images, where the produced image strives to adhere to the style of the guided image as closely as possible while ensuring compliance with the condition. The resolution of the generated images is 256x256.&lt;/p&gt;&lt;p&gt;To demonstrate the features of DDN conditional generation and Zero-Shot Conditional Generation.&lt;/p&gt;&lt;p&gt;We trained a DDN with output level &lt;/p&gt;&lt;p&gt;Uncompressed raw backup of this video is here: DDN_latent_video&lt;/p&gt;&lt;p&gt;The following content contains personal opinions and is not included in the original paper&lt;/p&gt;&lt;p&gt;Based on the current state of DDN (May 2025), I speculate on several possible future research directions. These include improvements to DDN itself and tasks suitable for the current version of DDN. Due to my limited perspective, some of these speculations might not be accurate:&lt;/p&gt;&lt;p&gt;Improving DDN through hyperparameter tuning, exploratory experiments, and theoretical analysis:&lt;lb/&gt;The total time spent developing DDN was less than three months, mostly by a single person. Therefore, experiments were rough, and there was limited time for detailed analysis and tuning. There is significant room for improvement.&lt;/p&gt;&lt;p&gt;Scaling up to ImageNet-level complexity:&lt;lb/&gt;Building a practical generative model with Zero-Shot Conditional Generation as a key feature.&lt;/p&gt;&lt;p&gt;Applying DDN to domains with relatively small generation spaces:&lt;/p&gt;&lt;p&gt;Applying DDN to non-generative tasks:&lt;/p&gt;&lt;p&gt;Using DDN's design ideas to improve existing generative models:&lt;/p&gt;&lt;p&gt;Applying DDN to language modeling tasks:&lt;/p&gt;&lt;p&gt;Q1: Will DDN require a lot of GPU memory?&lt;/p&gt;&lt;quote&gt;&lt;p&gt;DDN's GPU memory requirements are slightly higher than same architecture of conventional GAN generator, but the difference is negligible.&lt;/p&gt;&lt;p&gt;During training, generating&lt;/p&gt;&lt;mjx-container&gt;samples is only to identify the one closest to the ground truth, and the&lt;/mjx-container&gt;&lt;mjx-container&gt;unchosen samples do not retain gradients, so they are immediately discarded after sampling at the current layer, freeing up memory.&lt;/mjx-container&gt;&lt;p&gt;In the generation phase, we randomly sample one number from range(&lt;/p&gt;&lt;mjx-container&gt;) as an index and only generate the sample at the chosen index, avoiding the need to generate the other&lt;/mjx-container&gt;&lt;mjx-container&gt;samples, thus not occupying additional memory or computation.&lt;/mjx-container&gt;&lt;/quote&gt;&lt;p&gt;Q2: Will there be a mode collapse issue?&lt;/p&gt;&lt;quote&gt;&lt;p&gt;No. DDN selects the output most similar to the current GT and then uses the L2 loss to make it even more similar to the GT. This operation naturally has a diverse tendency, which can "expand" the entire generation space.&lt;/p&gt;&lt;p&gt;Additionally, DDN supports reconstruction. Figure 14 in the original paper shows that DDN has good reconstruction performance on the test set, meaning that DDN can fully cover the target distribution.&lt;/p&gt;&lt;p&gt;The real issue with DDN is not mode collapse but attempting to cover a high-dimensional target distribution that exceeds its own complexity, leading to the generation of blurry samples.&lt;/p&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://discrete-distribution-networks.github.io/"/><published>2025-10-10T09:01:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536700</id><title>Nobel Peace Prize 2025: Mar√≠a Corina Machado</title><updated>2025-10-10T14:40:02.337794+00:00</updated><content>&lt;doc fingerprint="7715312d2910e1d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nobel Peace Prize 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;The Nobel Peace Prize 2025 was awarded to Maria Corina Machado "for her tireless work promoting democratic rights for the people of Venezuela and for her struggle to achieve a just and peaceful transition from dictatorship to democracy"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6‚Äì13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
    &lt;head rend="h3"&gt;Explore prizes and laureates&lt;/head&gt;
    &lt;p&gt; Look for popular awards and laureates in different fields, and discover the history of the Nobel Prize. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nobelprize.org/prizes/peace/2025/summary/"/><published>2025-10-10T09:03:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536816</id><title>Parallelizing Cellular Automata with WebGPU Compute Shaders</title><updated>2025-10-10T14:40:02.187198+00:00</updated><content/><link href="https://vectrx.substack.com/p/webgpu-cellular-automata"/><published>2025-10-10T09:20:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45537372</id><title>Htmx, Datastar, Greedy Developer</title><updated>2025-10-10T14:40:01.928378+00:00</updated><content>&lt;doc fingerprint="7ba9884e3be4a4d"&gt;
  &lt;main&gt;
    &lt;p&gt;Create: 2025-07-13, Update: 2025-07-13&lt;/p&gt;
    &lt;p&gt;Sometime ago, I was really interested in htmx. I have even built some tools in Emacs for myself to write htmx. But htmx is sometime a pain in the butt, especially for some big form. Therefore, I looked for an alternative.&lt;/p&gt;
    &lt;p&gt;In some random reddit post, I came across datastar, which is another hypermedia library, but uses SSE and signals. It claims to have both of the good thing from htmx and alpine.js. The author has a stand that he thought using form in htmx to communicate with server is a mistake, json should be used instead.&lt;/p&gt;
    &lt;p&gt;I had a running service written in htmx for some time. It is a clinic opening hour service to inform my patients when I will be available in which clinic. (Yes, I am not a programmer, but a healthcare professional.)&lt;/p&gt;
    &lt;p&gt;I decided to swap my whole service from htmx to datastar. And I have been working on it for a couple of weeks.&lt;/p&gt;
    &lt;p&gt;Today, as usual, I open the datastar official site for reference. The site is changed. And the visual is now stunning. I thought it was a great thing, it means the developer is now rapidly working on this good project.&lt;/p&gt;
    &lt;p&gt;But shockingly, a weird word of datastar pro is out. I was at beta v1 before. It is now finally released.&lt;/p&gt;
    &lt;p&gt;I then look at what pro version gives us. I thought it was just some quality of life add-on like what tailwind components.&lt;/p&gt;
    &lt;p&gt;But no, the datastar dev somehow move a portion of the freely available features behind a paywall. What the fuck.&lt;/p&gt;
    &lt;p&gt;Things like replace-url, scroll-into-view which were free, is now NOT free.&lt;/p&gt;
    &lt;p&gt;With my mouth wide opened, I continue to check out what the price is. A shocking $299 for solo dev.&lt;/p&gt;
    &lt;p&gt;It is not like $299 is much for me, but I am just a hobbist. I enjoy using the cutting edge technology and framework just for fun. Now the fun is going to cost me money.&lt;/p&gt;
    &lt;p&gt;I do not make any money with my service. And I don't see a reason to pay features that were free before.&lt;/p&gt;
    &lt;p&gt;I understand open source dev needs money to sponsor their work. However, the way that datastar dev do is utter disgrace.&lt;/p&gt;
    &lt;p&gt;Just as Reddit user Extreme-Ad-3920 concluded what I feel.&lt;/p&gt;
    &lt;p&gt;Fuck you datastar. I am going back to htmx.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://drshapeless.com/blog/posts/htmx,-datastar,-greedy-developer.html"/><published>2025-10-10T10:52:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45537512</id><title>Bringing Desktop Linux GUIs to Android: The Next Step in Graphical App Support</title><updated>2025-10-10T14:40:01.383777+00:00</updated><content>&lt;doc fingerprint="ff0f9913c9b534e8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Bringing Desktop Linux GUIs to Android: The Next Step in Graphical App Support&lt;/head&gt;&lt;head rend="h2"&gt;Introduction&lt;/head&gt;&lt;p&gt;Android has long been focused on running mobile apps, but in recent years, features aimed at developers and power users have begun pushing its boundaries. One exciting frontier: running full Linux graphical (GUI) applications on Android devices. What was once a novelty is now gradually becoming more viable, and recent developments point toward much smoother, GPU-accelerated Linux GUI experiences on Android.&lt;/p&gt;&lt;p&gt;In this article, we‚Äôll trace how Linux apps have run on Android so far, explain the new architecture changes enabling GPU rendering, showcase early demonstrations, discuss remaining hurdles, and look at where this capability is headed.&lt;/p&gt;&lt;head rend="h2"&gt;The State of Linux on Android Today&lt;/head&gt;The Linux Terminal App&lt;p&gt;Google‚Äôs Linux Terminal app is the core interface for running Linux environments on Android. It spins up a virtual machine (VM), often booting Debian or similar, and lets users enter a shell, install packages, run command-line tools, etc.&lt;/p&gt;&lt;p&gt;Initially, the app was limited purely to text / terminal-based Linux programs; graphical apps were not supported meaningfully. More recently, Google introduced support for launching GUI Linux applications in experimental channels.&lt;/p&gt;Limitations: Rendering &amp;amp; Performance&lt;p&gt;Even now, most GUI Linux apps on Android are rendered in software, that is, all drawing happens on the CPU (via a software renderer) rather than using the device‚Äôs GPU. This leads to sluggish UI, high CPU usage, more thermal stress, and shorter battery life.&lt;/p&gt;&lt;p&gt;Because of these limitations, running heavy GUI apps (graphics editors, games, desktop-level toolkits) has been more experimental than practical.&lt;/p&gt;&lt;head rend="h2"&gt;What‚Äôs Changing: GPU-Accelerated Rendering&lt;/head&gt;&lt;p&gt;The big leap forward is moving from CPU rendering to GPU-accelerated rendering, letting the device‚Äôs graphics hardware do the heavy lifting.&lt;/p&gt;Lavapipe (Current Baseline)&lt;p&gt;At present, the Linux VM uses Lavapipe (a Mesa software rasterizer) to interpret GPU API calls on the CPU. This works, but is inefficient, especially for complex GUIs or animations.&lt;/p&gt;Introducing gfxstream&lt;p&gt;Google is planning to integrate gfxstream into the Linux Terminal app. gfxstream is a GPU virtualization / forwarding technology: rather than reinterpreting graphics calls in software, it forwards them from the guest (Linux VM) to the host‚Äôs GPU directly. This avoids CPU overhead and enables near-native rendering speeds.&lt;/p&gt;&lt;p&gt;In fact, in Android‚Äôs Canary builds (e.g. build 2509), developers discovered a ‚ÄúGraphics Acceleration‚Äù option in the Terminal settings. While the visible toggle still appears to default to ‚Äúsoftware renderer,‚Äù code inspection suggests a hidden ‚ÄúGPU-accelerated renderer‚Äù toggle is already embedded.&lt;/p&gt;&lt;p&gt;When enabled properly, this path should let Linux GUI apps render using the GPU, unlocking smooth UI, reduced load, and better battery efficiency.&lt;/p&gt;&lt;head rend="h2"&gt;Early Experiments &amp;amp; Use Cases&lt;/head&gt;Pixel Phones &amp;amp; Canary Builds&lt;p&gt;Experimenters using Pixel 6 or newer devices on recent Android Canary builds have managed to run full GUI Linux apps, like GIMP or LibreOffice, inside the Linux Terminal environment. The process often involves installing a minimal desktop session (XFCE or others), launching a compositor (for example, Weston), and running apps via Flatpak or apt.&lt;/p&gt;&lt;p&gt;Because of the software renderer baseline, these experiences often remain somewhat sluggish unless GPU acceleration is enabled.&lt;/p&gt;Galaxy Tab S11 (Beyond Pixel)&lt;p&gt;Interestingly, some newer tablets like Samsung‚Äôs Galaxy Tab S11 (powered by MediaTek) have surfaced as devices that support the Linux Terminal app. While GUI support is still in flux, users have manually enabled configurations to run Linux GUI apps.&lt;/p&gt;&lt;p&gt;These steps hint at the possibility of turning tablets into full-fledged mobile Linux machines, all with keyboard, mouse, and touchscreen support.&lt;/p&gt;Demo Applications: Doom &amp;amp; More&lt;p&gt;One oft-cited demo is running Chocolate Doom (a version of the classic Doom engine) successfully within the Linux VM environment on Android, when a hardware-accelerated path is activated.&lt;/p&gt;&lt;p&gt;These early demos showcase viability and excitement, though many non-trivial parts (audio, compositor stability) remain works in progress.&lt;/p&gt;&lt;head rend="h2"&gt;Technical Challenges &amp;amp; Caveats&lt;/head&gt;&lt;p&gt;Although the path forward is promising, several technical hurdles remain:&lt;/p&gt;Hardware &amp;amp; VM Constraints&lt;p&gt;To forward GPU calls, the device‚Äôs chipset must support unprotected VM memory and other virtualization features. Not all SoCs enable this, some devices (e.g. certain Snapdragon models) lack compatibility.&lt;/p&gt;&lt;p&gt;In particular, if a device doesn‚Äôt allow the VM to access memory in ways the GPU expects, the forwarding route may fail or fallback to software.&lt;/p&gt;Stability Issues &amp;amp; Incomplete Features&lt;p&gt;Because GPU rendering is still experimental, many parts remain fragile:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;The compositor (Wayland / Weston) integration may crash or misrender&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Audio forwarding is often absent or buggy&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;UI scaling, input methods, window management may misbehave&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Some GUI toolkits or libraries may not correctly detect hardware acceleration&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Even with GPU forwarding, Android devices have constrained resources: limited memory for the VM, thermal throttling, and battery constraints may still limit how heavy GUI applications can run in practical use.&lt;/p&gt;OEM / Vendor Variability&lt;p&gt;Because Android is heavily customized by manufacturers, behavior may differ across phones and tablets. Some OEMs may disable or block certain virtualization features or device drivers. The Linux Terminal app may behave differently depending on Android version, kernel build, or OEM customizations.&lt;/p&gt;&lt;head rend="h2"&gt;Broader Implications&lt;/head&gt;&lt;p&gt;This evolution is more than a technical novelty, it opens new possibilities:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Developers on mobile: Using desktop-grade tools (IDEs, compilers, GUIs) directly on Android could reduce the need to carry separate laptops&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Productivity on tablets and foldables: Running Linux GUI apps natively turns Android tablets into hybrid productivity devices&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Android‚Äôs convergence toward desktop modes: These features align with broader trends to let Android act more like a full operating system when docked or paired with peripherals&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Edge computing / local AI workloads: Being able to run native Linux services (GUIs, dashboards, ML tools) on-device broadens use cases&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;What You Can Try Today&lt;/head&gt;&lt;p&gt;If you‚Äôre curious and want to test this now, here‚Äôs a rough guide (based on Canary builds / experimental versions):&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Use a Pixel 6 or newer (or compatible device) on a recent Android Canary build that supports GUI features in the Linux Terminal.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Enable the ‚ÄúLinux development environment / Terminal‚Äù via Developer Options.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;To enable GPU rendering, create an empty file named&lt;/p&gt;&lt;code&gt;virglrenderer&lt;/code&gt;in the&lt;code&gt;/sdcard/linux&lt;/code&gt;directory (or similar path). The Terminal checks for this file to activate VirGL (or GPU forwarding).&lt;/item&gt;&lt;item&gt;&lt;p&gt;Launch the Terminal app. You should see a toast or message indicating that ‚ÄúVirGL is enabled.‚Äù&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Install or launch GUI Linux apps (e.g. via&lt;/p&gt;&lt;code&gt;apt&lt;/code&gt;, Flatpak). Or spin up a desktop environment (XFCE, MATE) or compositor (Weston) to manage windows.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Test various apps, lightweight ones first, observe responsiveness, UI artifacts, and stability.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Expect bugs, crashes, performance inconsistencies. But it‚Äôs a fascinating preview of what‚Äôs coming.&lt;/p&gt;&lt;head rend="h2"&gt;Future Outlook&lt;/head&gt;&lt;p&gt;To make this experience truly practical, the following must improve:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;More robust compositor support with Wayland / X compatibility&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Audio and input device forwarding (e.g. microphone, keyboard)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Better memory management for heavier apps&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Broader chipset and OEM support&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Making GPU forwarding (gfxstream) officially supported across devices&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Integration of GUI Linux support into stable Android releases&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If Google and OEMs push forward, this feature could reach stable channels in Android 16 QPR updates or Android 17.&lt;/p&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;Android‚Äôs support for Linux GUI applications is evolving fast. From CPU-based rendering struggles to GPU-accelerated forwarding via gfxstream, the platform is shifting toward making desktop-grade Linux apps run fluently on mobile devices. Early demos hint at what‚Äôs possible now, but there‚Äôs still distance to go in stability, compatibility, and performance.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.linuxjournal.com/content/bringing-desktop-linux-guis-android-next-step-graphical-app-support"/><published>2025-10-10T11:10:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45537890</id><title>OpenGL is getting mesh shaders as well, via GL_EXT_mesh_shader</title><updated>2025-10-10T14:40:01.203066+00:00</updated><content>&lt;doc fingerprint="bec39254fca1fee5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mesh Shaders In The Current Year&lt;/head&gt;
    &lt;head rend="h1"&gt;It Happened.&lt;/head&gt;
    &lt;p&gt;Just a quick post to confirm that the OpenGL/ES Working Group has signed off on the release of GL_EXT_mesh_shader.&lt;/p&gt;
    &lt;head rend="h1"&gt;Credits&lt;/head&gt;
    &lt;p&gt;This is a monumental release, the largest extension shipped for GL this decade, and the culmination of many, many months of work by AMD. In particular we all need to thank Qiang Yu (AMD), who spearheaded this initiative and did the vast majority of the work both in writing the specification and doing the core mesa implementation. Shihao Wang (AMD) took on the difficult task of writing actual CTS cases (not mandatory for EXT extensions in GL, so this is a huge benefit to the ecosystem).&lt;/p&gt;
    &lt;p&gt;Big thanks to both of you, and everyone else behind the scenes at AMD, for making this happen.&lt;/p&gt;
    &lt;p&gt;Also we have to thank the nvidium project and its author, Cortex, for single-handedly pushing the industry forward through the power of Minecraft modding. Stay sane out there.&lt;/p&gt;
    &lt;head rend="h1"&gt;Support&lt;/head&gt;
    &lt;p&gt;Minecraft mod support is already underway, so expect that to happen ‚Äúsoon‚Äù.&lt;/p&gt;
    &lt;p&gt;The bones of this extension have already been merged into mesa over the past couple months. I opened a MR to enable zink support this morning since I have already merged the implementation.&lt;/p&gt;
    &lt;p&gt;Currently, I‚Äôm planning to wait until either just before the branch point next week or until RadeonSI merges its support to merge the zink MR. This is out of respect: Qiang Yu did a huge lift for everyone here, and ideally AMD‚Äôs driver should be the first to be able to advertise that extension to reflect that. But the branchpoint is coming up in a week, and SGC will be going into hibernation at the end of the month until 2026, so this offer does have an expiration date.&lt;/p&gt;
    &lt;p&gt;In any case, we‚Äôre done here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.supergoodcode.com/mesh-shaders-in-the-current-year/"/><published>2025-10-10T11:56:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45537938</id><title>Weave (YC W25) is hiring a founding AI engineer</title><updated>2025-10-10T14:40:00.723355+00:00</updated><content>&lt;doc fingerprint="951e4c15f89423dd"&gt;
  &lt;main&gt;
    &lt;p&gt;AI to understand engineering work&lt;/p&gt;
    &lt;p&gt;At Weave, we‚Äôre building the best software for the best engineering teams to move faster, and we want to hire exceptional engineers to help us do so.&lt;/p&gt;
    &lt;p&gt;We are a well-funded startup, backed by top investors, growing rapidly and currently profitable.&lt;/p&gt;
    &lt;p&gt;You'll be working directly with me (Andrew), the CTO. Before I was CTO of Weave I was the founding engineer at Causal, and I want to give you all the support and growth opportunities in this role that I got when I went through it.&lt;/p&gt;
    &lt;p&gt;You‚Äôll also be working directly with Adam, the CEO. Adam runs sales at Weave, and before that worked as a sales executive at a few different high growth startups.&lt;/p&gt;
    &lt;p&gt;You are a good fit for Weave if you are a formidable engineer. This means you stop at nothing to accomplish your goal. We don't care much about your current skills or even what you've done before; we care that you will be able to do anything you set your mind to.&lt;/p&gt;
    &lt;p&gt;You must also be pragmatic. Weave is a startup so something is always on fire. You need to know when to let little fires burn and when to break out the extinguisher.&lt;/p&gt;
    &lt;p&gt;You must be a very good engineer who's committed to becoming a great engineer. The slope is more important than the Y-intercept.&lt;/p&gt;
    &lt;p&gt;You must be empathetic. We're building products for other people, so you need to be able to understand how other people think and why.&lt;/p&gt;
    &lt;p&gt;You must care about helping other software engineering teams be great. If that's not an exciting mission for you, it will be hard to stay motivated through the inevitable highs and lows.&lt;/p&gt;
    &lt;p&gt;You must be an excellent communicator. You‚Äôll be working on a product that‚Äôs communicating with millions of engineers and leaders, so you need to be clear.&lt;/p&gt;
    &lt;p&gt;Finally you must be gritty. You should be accustomed to picking the hard option and pushing through it.&lt;/p&gt;
    &lt;p&gt;(Please feel free to apply even if some or all of these don't apply to you!)&lt;/p&gt;
    &lt;p&gt;Our tech stack is React + TypeScript on the frontend, Go on the backend, and Python for ML. Experience with any of those three languages is a bonus.&lt;/p&gt;
    &lt;p&gt;If you've already done lots of thinking about engineering productivity and how to improve it, that's great and we want to hear about it!&lt;/p&gt;
    &lt;p&gt;We hope your design sensibilities are passable.&lt;/p&gt;
    &lt;p&gt;As Weave‚Äôs founding AI engineer, your job is to build AI to understand and improve the work that software engineers do. You‚Äôll be building our processes and standards as you go to make building every incremental feature easier. Your goal will be to delight customers with intelligence that makes their job 10x easier.&lt;/p&gt;
    &lt;p&gt;At Weave, we‚Äôre building the best software for the best engineering teams to move faster, and we want to hire exceptional engineers to help us do so.&lt;/p&gt;
    &lt;p&gt;We are a well-funded startup, backed by top investors and growing rapidly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/weave-3/jobs/SqFnIFE-founding-ai-engineer"/><published>2025-10-10T12:01:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45538137</id><title>Igalia, Servo, and the Sovereign Tech Fund</title><updated>2025-10-10T14:39:56.126648+00:00</updated><content>&lt;doc fingerprint="49fb4a41ab1a2764"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Igalia, Servo, and the Sovereign Tech Fund&lt;/head&gt;
    &lt;head rend="h5"&gt;"We√¢re proud to help shape the future of web engines through public investment in accessibility, embeddability, and sustainability."&lt;/head&gt;
    &lt;p&gt;Igalia is excited to announce a new commission from the Sovereign Tech Fund to advance the Servo web engine. As stewards of Servo, Igalia is honored to receive support for a multi-pronged effort focused on public interest, developer usability, and long-term sustainability.&lt;/p&gt;
    &lt;p&gt;Servo is a modern, parallelized web engine written in Rust, a Linux Foundation Europe project which Igalia has been actively maintaining since 2023, Servo represents a bold rethinking of browser architecture. Its modular design has made it a valuable resource across the Rust ecosystem. But like many promising open source technologies, Servo needs sustained investment to reach its full potential.&lt;/p&gt;
    &lt;p&gt;Thanks to investment from the Sovereign Tech Fund, Igalia will focus some important work in the next year in three key areas:&lt;/p&gt;
    &lt;head rend="h3"&gt;√∞¬ß Initial Accessibility Support&lt;/head&gt;
    &lt;p&gt;As Servo adoption grows, so does the need for inclusive design. Today, Servo lacks the foundational accessibility features required to support screen readers and other assistive technologies. This limits its usability in many real-world scenarios, and doesn√¢t match our values. Despite its importance, accessibility is often one of a few things that is difficult to find funding for. We√¢re grateful that thanks to this investment, we√¢ll be able to implement initial accessibility support to ensure that Servo can serve all users. This work is essential to making Servo a viable engine for public-facing applications.&lt;/p&gt;
    &lt;head rend="h3"&gt;√∞¬ß¬© WebView API&lt;/head&gt;
    &lt;p&gt;Embedding Servo into applications requires a stable and complete WebView API. While early work exists, it√¢s not yet ready for general use. We√¢ll be finishing the WebView API to make Servo embeddable in desktop and mobile apps, unlocking new use cases and enabling broader adoption. A robust embedding layer is critical to Servo√¢s eventual success as a general-purpose engine.&lt;/p&gt;
    &lt;head rend="h3"&gt;√∞¬ß Project Maintenance&lt;/head&gt;
    &lt;p&gt;Servo is more than a browser engine√¢it√¢s a collection of crates used widely across the Rust ecosystem. Maintaining these libraries benefits not just Servo, but the broader web platform. The project and the community have been growing a lot since we√¢ve taken over stewardship. This funding will allow our work will include more issue triage, pull request review, version releases, and governance support. All of this helps ensure that Servo remains active, responsive, and well-maintained for developers and users alike.&lt;/p&gt;
    &lt;p&gt;Igalia has long championed open source innovation in the browser space, from our work on Chromium, WebKit, and Gecko to our leadership in standards bodies and developer tooling. We believe Servo has a unique role to play in the future of web engines, and we√¢re thrilled to help guide its next chapter.&lt;/p&gt;
    &lt;p&gt;Many thanks to the Sovereign Tech Fund for recognizing the importance of this work. We look forward to sharing progress as we go.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.igalia.com/2025/10/09/Igalia,-Servo,-and-the-Sovereign-Tech-Fund.html"/><published>2025-10-10T12:21:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45538760</id><title>PSA: Always use a separate domain for user content</title><updated>2025-10-10T14:39:55.474309+00:00</updated><content>&lt;doc fingerprint="822a4d015b631e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google Safe Browsing incident&lt;/head&gt;
    &lt;p&gt;By Eric Selin - Founder, statichost.eu&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;For approximately six hours on 25.9.2025, the entire statichost.eu domain was flagged as deceptive by Google Safe Search. This meant that anyone using Google Safe Search was shown a very aggressive warning or outright blocked when trying to access any site on the&lt;/p&gt;&lt;code&gt;statichost.eu&lt;/code&gt;domain. In some cases even custom domains hosted on the statichost.eu platform were affected. This post is part incident report and part privacy (and monopoly) rant.&lt;/quote&gt;
    &lt;p&gt;Note: This post sparked some discussion on Hacker News, which is of course great. I‚Äôd like to clarify that I do not hate Google, nor do I think that they did anything particularly wrong by flagging malicious content (albeit with a pretty wide net). I‚Äôm simply saying that Google is pretty darn big, and that I personally think they are too big.&lt;/p&gt;
    &lt;p&gt;Google has too much power over the Internet. Or in the most objective way possible: Google controls and/or monitors a substantial part of every single interaction on the Internet. You may think that this is fine, and that is your right, although I very much disagree. Especially since Google blocked all of statichost.eu for ‚Äúover five billion‚Äù devices for several hours. Here is how it went down:&lt;/p&gt;
    &lt;p&gt;I woke up to some pretty bad news on Monday a couple of weeks ago. A few users had started reporting that &lt;code&gt;statichost.eu&lt;/code&gt; is unavailable due to a security
warning. This is not great, I think to myself, and go into incident response
mode. Immediately, I check https://www.statichost.eu, and see that it‚Äôs working.
No TLS issues or other technical problems - maybe a browser issue or network
problem?&lt;/p&gt;
    &lt;p&gt;Ok, so I start investigating. The affected users all mention Google, so I start there. I use Chromium for Google-specific things (only), so I open it up and fire up a Google search. I actually cannot see &lt;code&gt;statichost.eu&lt;/code&gt; on Google
now, which is weird - it should be the top-ranked result for my keywords (e.g.
‚Äúeuropen static hosting‚Äù). While I wait for Google Search Console to load, I
check www.statichost.eu again in Chromium, just in case.&lt;/p&gt;
    &lt;p&gt;And BOOM! There it is. Now I start panicing. Google is blocking me from my own website! It apparently thinks I might be deceived - I guess into doing something I shouldn‚Äôt do or something I‚Äôll regret later?&lt;/p&gt;
    &lt;p&gt;Back in the Search Console, which has now loaded all its JavaScript and whatnot, I see a giant error message: ‚ÄúSecurity issues detected‚Äù. There seems to be a problem with phishing on the statichost.eu domain. All sites on statichost.eu get a &lt;code&gt;SITE-NAME.statichost.eu&lt;/code&gt; domain, and during the weekend there was an
influx of phishing sites. As a result of that, &lt;code&gt;statichost.eu&lt;/code&gt; ended up on the
Google Safe Browsing list of ‚Äúdangerous‚Äù
sites. Luckily, Google provided me with a helpful list of the offending sites,
which I could then promptly delete.&lt;/p&gt;
    &lt;p&gt;It is of course impossible to talk to anyone at Google in order to fix this, but there is a ‚Äúrequest review‚Äù button. After writing up an explanation and requesting a review, all I could do was wait. I prepared for the worst, but within a few hours, the block was lifted and an automatically generated response of the same appeared as a notification in Search Console. Not even an email was sent. Nonetheless: incident over.&lt;/p&gt;
    &lt;p&gt;Anyway, back to Google.&lt;/p&gt;
    &lt;p&gt;The stated goal of Google Safe Browsing is ‚ÄúMaking the world√¢s information safely accessible.‚Äù. Yikes! But what does it mean? It is basically a giant blacklist of sites that Google has deemed unworthy. This list is then used by major browsers and anyone who wants to ‚Äúmake information safely accessible‚Äù or whatever. According to Google, this protects ‚Äúover five billion devices‚Äù. That of course means that you really don‚Äôt want to end up on this list!&lt;/p&gt;
    &lt;p&gt;And do you know how Google builds this list? By doing what they do best: by monitoring absolutely everything. One tool for this is Google Chrome - a ‚Äúfree‚Äù browser created by Google for its business purposes. It of course sends the URLs of pages you visit back to Google - I very much assume by default. And with ‚Äúenhanced security protection‚Äù turned on, it even sends some of the page content to Google. That is a very neat way to monitor the comings and goings of something like four billion people.&lt;/p&gt;
    &lt;p&gt;To be fair, many or even most sites on the Google Safe Browsing blacklist are probably unworthy. But I‚Äôm pretty sure this was not the first false positive. And I‚Äôm not sure this is the best way to tackle phishing. E.g. what happens on the countless phishing sites that are not on this list? Be that as it may, do not rely on Google to tell you who to trust. Use your own judgement and hard-earned Internet street smarts.&lt;/p&gt;
    &lt;p&gt;There are lots of problems on the Internet, but I for one don‚Äôt trust Google to be our savior. There was a time when Google was different, but do not mistake their friendly branding and legacy goodwill for something it is not.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;In order to limit the impact of similar issues in the future, all sites on statichost.eu are now created with a&lt;/p&gt;&lt;code&gt;statichost.page&lt;/code&gt;domain instead. This domain is pending addition to the Public Suffix List in order to further increase resilience and security.&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.statichost.eu/blog/google-safe-browsing/"/><published>2025-10-10T13:27:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45539159</id><title>The Prairie Farmers Preserving the Most Threatened Ecosystem ‚Äì Forever</title><updated>2025-10-10T14:39:55.103162+00:00</updated><content>&lt;doc fingerprint="2bd8655ec0a357d6"&gt;
  &lt;main&gt;&lt;p&gt;This story is one in a series about the confluence of capitalism, conservation and cultural identity in the Mississippi River Basin. It is part of Waterline and is sponsored by the Walton Family Foundation.&lt;/p&gt;&lt;p&gt;Dallas May spotted the first shoots of grass a few months after a wildfire tore across the ranch he runs with his family in Lamar, Colorado, in 2022. Propelled by winds up to 70 mph, the blaze killed some of his livestock, destroyed 42 miles of fencing, and burned through vegetation and beaver lodges along the creek.&lt;/p&gt;&lt;p&gt;‚ÄúIt was a moonscape,‚Äù May recalls. ‚ÄúEverything was charred and gone.‚Äù&lt;/p&gt;&lt;p&gt;Before the fire, May had barely noticed the spindly stalks of needle and thread grass amid the many types of grass on his pastureland. But after the rain came, the sprouts shot up from the scorched ground. Over the following months, a succession of different grasses sprouted from seeds stored in the soil.&lt;/p&gt;&lt;p&gt;‚ÄúWe had an entire natural seed bank,‚Äù May says.&lt;/p&gt;&lt;p&gt;That‚Äôs because for decades, May and his family have managed May Ranch near the Arkansas River ‚Äî a major tributary of the Mississippi River ‚Äî to encourage native habitat to thrive alongside their cattle. This region is part of a swath of grassland that sweeps from central Canada to northern Mexico, vital to many species of birds that migrate across North America.&lt;/p&gt;&lt;p&gt;But today, these temperate grasslands are considered the most threatened major ecosystem in the world. Less than 40 percent of the region‚Äôs 550 million acres of historical grasslands have survived, and an average of two million acres are lost annually, converted for development or cropland.&lt;/p&gt;&lt;p&gt;Ranching, though, is a natural fit on this landscape. Grazing cattle help keep wild grassland healthy, subduing woody and invasive species. And through conservation-oriented practices, ranchers like May are not only preserving habitat for birds, they‚Äôre also yielding benefits for carbon sequestration, water quality and biodiversity.&lt;/p&gt;&lt;p&gt;‚ÄúGrazing is probably one of the most essential functions that you need to appropriately manage grassland,‚Äù says Rich Schultheis, coordinator for Playa Lakes Joint Venture, a bird conservation organization working across the western Great Plains. Ranchers are key partners: Without them, he says, ‚Äúwe would be in such a worse place.‚Äù&lt;/p&gt;&lt;head rend="h3"&gt;Weighed down by negative news?&lt;/head&gt;Our smart, bright, weekly newsletter is the uplift you‚Äôve been looking for.&lt;p&gt;The decline in the various types of prairie that once covered the Great Plains has had dire consequences for wildlife. Since 1970, the population of birds that rely on grassland has dropped by 53 percent ‚Äî a loss of 700 million birds.&lt;/p&gt;&lt;p&gt;The pockets of habitat that remain are critical to their survival, but under threat: Just 21 percent of Great Plains grasslands have never been disturbed. ‚ÄúIf that grass is gone,‚Äù Schultheis says, ‚Äúthat‚Äôs a much more difficult and heavier lift to get that grassland back on the landscape.‚Äù&lt;/p&gt;&lt;p&gt;That‚Äôs where ranchers come in.&lt;/p&gt;&lt;p&gt;In contrast to crop agriculture that requires plowing up the land, ranching typically leaves large tracts as native habitat, allowing livestock to munch on vegetation that grows there.&lt;/p&gt;&lt;p&gt;This is actually good for grassland, Schultheis explains: Grasses like disturbance. Historically, Great Plains prairies would have been shaped by wildfires and grazers like bison. Today, cattle serve that purpose.&lt;/p&gt;&lt;p&gt;Protecting wild habitat had always been a part of May‚Äôs ethos. He grew up in Lamar, in a family of ranchers. But May really got the chance to put his conservation-minded approach into practice in 2012, when his family bought the land they‚Äôd been leasing since the 1980s.&lt;/p&gt;&lt;p&gt;The Mays run about half the number of cattle that they could on a ranch this size ‚Äî around 600 mother cows. (The total could include up to 1,400 animals, with the coming and going of calves, bulls and young female cattle.) Keeping the number low leaves plenty of vegetation, which helps both wildlife and the ranch, May explains, and means there‚Äôs always plenty of grass left for the following year.&lt;/p&gt;&lt;p&gt;‚ÄúIf you allow the cattle to be part of the ecology, rather than dominating it, they are a benefit to it,‚Äù May says.&lt;/p&gt;&lt;p&gt;But running a smaller herd reduces the ranch‚Äôs profit. May has found a path to making up the financial gap by working with an array of conservation-driven partners.&lt;/p&gt;&lt;p&gt;One of his first steps was to get a conservation easement, through which the Mays agreed to bar the development of their land now and in perpetuity. They essentially ‚Äúdonated‚Äù their development rights. In exchange, the Mays get a state tax credit.&lt;/p&gt;&lt;p&gt;The Mays have had lucrative offers from potential buyers eyeing their land for renewable energy installations or to plow up the grassland to farm it. But, May explains, the family felt it was important to ensure the ranch would stay working grassland.&lt;/p&gt;&lt;p&gt;In the past, many ranchers were skeptical of such easements, explains Maggie Hanna, director of the Central Grasslands Roadmap Initiative and a rancher in the Arkansas River Valley herself. Selling off a chunk of land at its development value is a back-pocket option for ranchers when they‚Äôre financially squeezed. But, Hanna says, conservation easements give landowners a different type of flexibility. Crucially, she notes, the land trust May worked with grew out of Colorado‚Äôs agricultural community. ‚ÄúWhen the solution comes from a community,‚Äù she says, ‚Äúthe solution is more durable.‚Äù&lt;/p&gt;&lt;p&gt;In Hanna‚Äôs experience, knowing that her land is protected from development has given her a sense of security. That, she says, allows her to manage her ranch for long-term sustainability rather than short-term profit ‚Äî which also benefits the community.&lt;/p&gt;&lt;p&gt;‚ÄúIf we want to keep rural America afloat in any sort of capacity, keeping these landscapes intact also means that families can survive in these places,‚Äù she says.&lt;/p&gt;&lt;p&gt;May Ranch has also found other ways to profit off of its land and livestock. With its long-term commitment to keeping grassland undisturbed, the ranch was an early participant in Ducks Unlimited‚Äôs carbon credit program, which sells offsets tied to preserved private lands. While methane emissions related to cattle are a significant contributor of greenhouse gases, sustainable grazing practices can boost carbon sequestration. The ranch is also certified ‚Äúbird-friendly land‚Äù through the National Audubon Society, a seal that appears on the packaging of the Mays‚Äô beef.&lt;/p&gt;&lt;p&gt;May is one of 39 ranchers working with Audubon in the Rocky Mountain area to implement habitat management plans on their land. Because so much land here is held privately, landowners are vital to preserving grassland habitat, explains program manager Dusty Downey, a rancher himself in Wyoming. The hope is that Audubon‚Äôs support adds value to these ranches‚Äô products.&lt;/p&gt;&lt;p&gt;‚ÄúWe need to be able to keep ranchers ranching, and the only way to do that is through their finances,‚Äù he says.&lt;/p&gt;&lt;p&gt;Evidence shows that when ranchers take a conservation approach, a multitude of benefits follow. In drought-prone eastern Colorado, for example, research finds soil in areas that have been responsibly grazed has a higher moisture content.&lt;/p&gt;&lt;p&gt;‚ÄúWe‚Äôre seeing the lands on these Audubon ranches be able to store more water, so it makes the land far more drought resistant,‚Äù Downey says.&lt;/p&gt;&lt;p&gt;And, as May Ranch shows, wildlife can flourish alongside livestock. In 2019, botanists documented 248 species of plants on May Ranch including 50 types of grasses, according to Christina Alba, a Denver Botanic Gardens research scientist who led the survey.&lt;/p&gt;&lt;p&gt;‚ÄúThere are few remaining untilled prairie habitats on the Eastern Plains and it‚Äôs a huge boon to plant diversity conservation to set aside tracts of untilled prairie,‚Äù says Alba.&lt;/p&gt;&lt;p&gt;Animals make use of that space. Last year, Colorado state researchers found two litters of wild-born young black-footed ferret kits on May Ranch, a huge milestone after endangered ferrets were reintroduced there in 2021. Eastern black rails, a white-speckled, red-eyed marsh bird that has almost disappeared from the interior of North America, migrate through the ranch‚Äôs riparian habitat.&lt;/p&gt;&lt;p&gt;Because of drought conditions and heavy water usage, the Arkansas River often runs dry in parts of Kansas. But, Schultheis says, migratory birds show the watershed remains linked even if the flow is disrupted. ‚ÄúAs far as wildlife are concerned,‚Äù he says, ‚Äúthere is still a connection.‚Äù&lt;/p&gt;&lt;p&gt;Even so, the loss of habitat on a larger scale is a major challenge. May has been involved with several efforts to reintroduce the lesser prairie chicken, an orange-eyebrowed grouse locally extinct in eastern Colorado. But, despite great conditions on May‚Äôs ranch, the lack of habitat more broadly has hindered reintroductions.&lt;/p&gt;&lt;p&gt;Three years after the fire, May Ranch is still recovering.&lt;/p&gt;&lt;p&gt;Before the fire, a big part of the Mays‚Äô business was selling pure-bred Limousin cattle for breeding, which involves carefully separating out cattle in order to document their parentage. The loss of fencing in the blaze means the herd now all runs together, dealing a blow to the ranch‚Äôs finances.&lt;/p&gt;&lt;p&gt;As difficult as the recovery has been, May also sees the ranch reaping benefits from its commitment to wild habitat. After the fire, the beavers that lived along the creek disappeared ‚Äî but when water returned, they came out of hiding and started rebuilding dams. Gradually, grass filled the scorched meadows.&lt;/p&gt;&lt;p&gt;‚ÄúTo me, that is the key to sustainability,‚Äù May says. ‚ÄúKeeping things there where they can recover on their own.‚Äù&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://reasonstobecheerful.world/prairie-farmers-preserve-most-threatened-ecosystem-forever/"/><published>2025-10-10T13:59:12+00:00</published></entry></feed>