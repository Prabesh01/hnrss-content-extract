<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-18T23:32:51.857192+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45963592</id><title>A 'small' vanilla Kubernetes install on NixOS</title><updated>2025-11-18T23:32:57.627016+00:00</updated><content>&lt;doc fingerprint="ec90c159b0a1bba4"&gt;
  &lt;main&gt;
    &lt;p&gt;Kubernetes is a complex piece of technology that abstracts away many system administration tasks, but does also solve and automate some processes useful at a smaller scale, like blue-green deployments. Having administered managed Kubernetes for a while now, I wanted to find out what a self-managed, small-but-multi-node Kubernetes install looks like.&lt;/p&gt;
    &lt;p&gt;Most of the non-Kubernetes machines I manage are individual machines, or single database + multiple workers. For this step I'm not really interested in much more than that, like making everything redundant, self-healing, etc. I just want to introduce Kubernetes in something that matches my existing setups.&lt;/p&gt;
    &lt;p&gt;Getting things fully functional was a long process of trial-and-error, during which I learned about even more things I didn't want to touch:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Public-Key Infrastructure (PKI). Kubernetes definitely leans into this and prefers you manage keys and certificates for all of its components, but I feel like this is a whole separate article in itself.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The NixOS Kubernetes modules. These have their own opinions, and there's nothing wrong with their implementation, but using them goes against some of the learning and experimenting I wanted to do here.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;K3s, K0s or any other Kubernetes 'distribution'. These are an extra layer to learn, and an extra layer to trust. They sometimes offer valuable extra functionality, for example I wish the SQLite backend was in upstream Kubernetes. But again, I avoided these in the interest of learning.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NixOS in general is great, and I'm a big fan, but something Kubernetes can potentially do well (in terms of configuration) is provide a clear boundary between the system and application. In NixOS, configuring an app is often interwoven with system config, and there's a lack of options to prevent that.&lt;/p&gt;
    &lt;p&gt;Still, I'll be using the Kubernetes package (not module!) from Nixpkgs, as well as building everything on top of NixOS and its excellent systemd parts.&lt;/p&gt;
    &lt;p&gt;A fully functioning QEMU setup for the end result can be found at: https://codeberg.org/kosinus/nixos-kubernetes-experiment&lt;/p&gt;
    &lt;head rend="h3"&gt;Basic NixOS configuration&lt;/head&gt;
    &lt;p&gt;At the time of writing, NixOS 25.11 is mere weeks away, so that is my target.&lt;/p&gt;
    &lt;p&gt;There's a bunch of stuff I enable on all of my NixOS machines that is relevant to the rest of this article.&lt;/p&gt;
    &lt;p&gt;I prefer nftables over iptables, because it's the future. In practice, the &lt;code&gt;iptables&lt;/code&gt; command is already a compatibility layer in many Linux distributions, but these options additionally enable the nftables-based firewall in NixOS:&lt;/p&gt;
    &lt;code&gt;{
  networking.nftables.enable = true;
  # We want to filter forwarded traffic.
  # Also needed for `networking.firewall.extraForwardRules` to do anything.
  networking.firewall.filterForward = true;
}
&lt;/code&gt;
    &lt;p&gt;I enable systemd-networkd, because it's the future. I wouldn't even know how to set up all the networking parts in other setups; systemd-networkd is just really nice when you have a bunch of moving parts in your networking.&lt;/p&gt;
    &lt;code&gt;{
  networking.useNetworkd = true;
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Kubernetes version&lt;/head&gt;
    &lt;p&gt;The current version of Kubernetes at the time of writing is 1.34. It's useful to check the package version, because Kubernetes requires step-by-step minor version upgrades:&lt;/p&gt;
    &lt;code&gt;{ lib, pkgs, ... }:
{
  # Ensure we carefully upgrade Kubernetes versions.
  # We need to step 1 minor version at a time.
  assertions = [
    {
      assertion = lib.hasPrefix "1.34." pkgs.kubernetes.version;
      message = "Unexpected Kubernetes package version: ${pkgs.kubernetes.version}";
    }
  ];
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Networking&lt;/head&gt;
    &lt;p&gt;If you've ever used Docker or Podman, your typical networking setup looks like this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The machine is logically split up in host and container network namespaces. Each container is assigned half of a veth pair, the other half is part of a a bridge interface on the host. The host assigns a subnet to the bridge with an address for itself, like&lt;/p&gt;&lt;code&gt;172.16.0.1/24&lt;/code&gt;, and an address for each container. The host is then the gateway for containers, performing layer 3 routing and NAT on outgoing traffic to the internet.&lt;/quote&gt;
    &lt;p&gt;Kubernetes wants you to connect these container subnets across multiple machines. In this article I assume there is a private network connecting all nodes together:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In addition to the 'outward' link from the host to the internet, the host now has an additional outward link to a network switch that brings hosts together in a private network. We intend to route traffic between container subnets across this private network somehow. Notably, NAT is still only performed on traffic to the internet, and not traffic between containers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Even if you have a private network like this, you may not be able to simply route traffic from container subnets across it. Cloud providers often restrict the addresses a machine can use on its network interface to what is preconfigured in the cloud resources.&lt;/p&gt;
    &lt;p&gt;There are a lot of ways to actually connect the subnets together, but I chose Wireguard because I know it, and because I wanted to test drive the overhead of encrypted links with real applications. It's potentially an additional layer of security if you're running this on the network of a cloud provider that otherwise doesn't encrypt customer traffic on internal networks. (But some may call you paranoid.)&lt;/p&gt;
    &lt;p&gt;Some alternatives here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use some other tunneling protocol like GENEVE or VXLAN. Maybe GRE works too?&lt;/item&gt;
      &lt;item&gt;Instead use TLS at the application layer for securing connections, e.g. HTTPS between proxy and backend, TLS to your database, etc.&lt;/item&gt;
      &lt;item&gt;If you control the physical network (or even just layer 2), you can actually connect containers directly to the network using macvlan and even have your existing DHCP server assign addresses.&lt;/item&gt;
      &lt;item&gt;Something like flannel can help you make the whole setup dynamic, if your machines tend to come and go.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Container subnets&lt;/head&gt;
    &lt;p&gt;First, let's determine our addressing scheme for all of our containers across machines.&lt;/p&gt;
    &lt;code&gt;{ config, lib, ... }:
{
  # I like to create NixOS options for variables that are going to be used
  # across multiple files, so I can reach them (without imports) via the
  # `config` parameter of a NixOS module.
  options.kube = {
    # We're going to assign each node a one-based index, and derive the
    # container subnet from that.
    nodeIndex = lib.mkOption { type = lib.types.ints.positive; };
    # Having a zero-based index on hand will become useful later,
    nodeIndex0 = lib.mkOption {
      type = lib.types.ints.unsigned;
      default = config.kube.nodeIndex - 1;
    };
    # Functions that take a node index and build a subnet in CIDR-notation.
    mkNodeCidr6 = lib.mkOption {
      type = with lib.types; functionTo str;
      default = index: "fd88:${toString index}::/32";
    };
    mkNodeCidr4 = lib.mkOption {
      type = with lib.types; functionTo str;
      default = index: "10.88.${toString index}.0/24";
    };
    # On each node, the host will take the first IP in the subnet.
    # Containers will use this IP as the gateway.
    mkHostIp6 = lib.mkOption {
      type = with lib.types; functionTo str;
      default = index: "fd88:${toString index}::1";
    };
    mkHostIp4 = lib.mkOption {
      type = with lib.types; functionTo str;
      default = index: "10.88.${toString index}.1";
    };
    # For each of the above functions, define the values for the local node.
    nodeCidr6 = lib.mkOption {
      type = lib.types.str;
      default = config.kube.mkNodeCidr6 config.kube.nodeIndex;
    };
    nodeCidr4 = lib.mkOption {
      type = lib.types.str;
      default = config.kube.mkNodeCidr4 config.kube.nodeIndex;
    };
    hostIp6 = lib.mkOption {
      type = lib.types.str;
      default = config.kube.mkHostIp6 config.kube.nodeIndex;
    };
    hostIp4 = lib.mkOption {
      type = lib.types.str;
      default = config.kube.mkHostIp4 config.kube.nodeIndex;
    };
    # The zero subnet is for Kubernetes Cluster IPs used in Service resources.
    # NOTE: Would love to use IPv6 here, but that is trouble for many apps.
    servicesCidr = lib.mkOption {
      type = lib.types.str;
      default = "10.88.0.0/24";
    };
  };
}
&lt;/code&gt;
    &lt;p&gt;Now each machine needs to assign the node index in per-machine configuration:&lt;/p&gt;
    &lt;code&gt;{
  kube.nodeIndex = 1;
}
&lt;/code&gt;
    &lt;p&gt;Now we have everything to configure the bridge interface we'll connect containers to. Unlike Docker / Podman, we'll be managing this manually:&lt;/p&gt;
    &lt;code&gt;{ config, pkgs, ... }:
{
  # We need a separate netdev unit to create the bridge interface.
  systemd.network.netdevs."10-brkube" = {
    netdevConfig = {
      Kind = "bridge";
      Name = "brkube";
    };
  };
  # Now configure the interface with a network unit.
  systemd.network.networks."10-brkube" = {
    matchConfig = {
      Name = "brkube";
    };
    networkConfig = {
      # We want this interface to always be configured and have addresses.
      # Bridges specifically report no-carrier while there are no members.
      ConfigureWithoutCarrier = true;
      # Disable all link-local addressing. (`169.254.0.0/16` / `fe80::/64`)
      LinkLocalAddressing = false;
      # Don't allow containers to maliciously become IPv6 routers.
      IPv6AcceptRA = false;
    };
    # Configure the host addresses.
    # This also configures the direct routes on the host.
    #
    # NOTE: Disable DuplicateAddressDetection because otherwise the address
    # can remain in a 'tentative' state, and Linux won't allow us to use it
    # as a source address in other routes. This is important for later.
    addresses = [
      {
        Address = "${config.kube.hostIp6}/32";
        DuplicateAddressDetection = "none";
      }
      {
        Address = "${config.kube.hostIp4}/24";
        DuplicateAddressDetection = "none";
      }
    ];
  };
  # To inspect the bridge interface at runtime using the `brctl` tool.
  environment.systemPackages = [ pkgs.bridge-utils ];
}
&lt;/code&gt;
    &lt;p&gt;Next we can setup the Wireguard links. For this we need to generate keypairs, and it is at this point that we introduce secrets into the NixOS config. I like to use agenix for this, but there are other choices here, like sops-nix. With agenix, machines decrypt files using their OpenSSH host key.&lt;/p&gt;
    &lt;p&gt;For simplicity, I'm going to put all keys in &lt;code&gt;keys/&lt;/code&gt; directory, and add a master key so we can always edit all files locally:&lt;/p&gt;
    &lt;code&gt;mkdir keys
cd keys/

# Handle this private key file with care!
# The public key is printed on success.
age-keygen -o master_key
&lt;/code&gt;
    &lt;p&gt;Now create a &lt;code&gt;keys/secrets.nix&lt;/code&gt; configuration file for agenix:&lt;/p&gt;
    &lt;code&gt;let
  # Public key printed by age-keygen above.
  # The master key should be included in every set of publicKeys.
  master = "age...";
  # OpenSSH host keys of our nodes.
  node1 = "ssh-ed25519 AAA...";
  node2 = "ssh-ed25519 AAA...";
in
{
  # Set recipients of Wireguard private keys to their respective nodes.
  "wgkube1.key.age".publicKeys = [ master node1 ];
  "wgkube2.key.age".publicKeys = [ master node2 ];
}
&lt;/code&gt;
    &lt;p&gt;Then generate the Wireguard keys and immediately encrypt them:&lt;/p&gt;
    &lt;code&gt;wg genkey | agenix -i master_key -e wgkube1.key.age
wg genkey | agenix -i master_key -e wgkube2.key.age
&lt;/code&gt;
    &lt;p&gt;Now we can decrypt these files in NixOS configuration:&lt;/p&gt;
    &lt;code&gt;{ config, ... }:
{
  # This will make the private key available in `/run/agenix/` as `wgkube.key`.
  age.secrets."wgkube.key" = {
    file = ./keys + "/wgkube${toString config.kube.nodeIndex}.key.age";
    # Make sure systemd-networkd can read this file.
    group = "systemd-network";
    mode = "0440";
  };
}
&lt;/code&gt;
    &lt;p&gt;Next I like to use a &lt;code&gt;peers.json&lt;/code&gt; as input to generate the Wireguard configuration. That JSON looks like this:&lt;/p&gt;
    &lt;code&gt;[
  {
    "PublicKey": "pHEYIfgWiJEgnR8zKYGnWlbZbQZ0xb5eEyzVSpzz3BM=",
    "PeerIP": "192.168.0.1"
  },
  {
    "PublicKey": "TPB2lwnWPjjAZ1Pnn5A6sdhGAePztE5VlbQ/RmU89w4=",
    "PeerIP": "192.168.0.2"
  }
]
&lt;/code&gt;
    &lt;p&gt;This array is ordered by node index. You can get the public keys as follows:&lt;/p&gt;
    &lt;code&gt;agenix -i master_key -d wgkube1.key.age | wg pubkey
agenix -i master_key -d wgkube2.key.age | wg pubkey
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;PeerIP&lt;/code&gt; fields are local network IPs in this example. These could be IPs on the private network provided by your cloud provider, but because this is Wireguard, you can also safely cross the internet. (Though the internet is not necessarily always fast, reliable and within you control.)&lt;/p&gt;
    &lt;p&gt;I use a JSON file like this because I actually generate it using tofu, but to keep things focused, the tofu configuration will not be in scope of this article. There is a neat little Wireguard provider for it, though.&lt;/p&gt;
    &lt;p&gt;Now we can configure the links in NixOS:&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:
let
  # Grab helpers and variables.
  # NOTE: Some of these are defined below.
  inherit (config.kube)
    mkNodeCidr6
    mkNodeCidr4
    nodeIndex0
    wgPort
    peers
    ;
in
{
  options.kube = {
    # Define the Wireguard port.
    # This variable is useful later in firewall config.
    wgPort = lib.mkOption {
      type = lib.types.port;
      default = 51820;
    };
    # Parse the `peers.json` file.
    peers = lib.mkOption {
      type = with lib.types; listOf attrs;
      default = builtins.fromJSON (builtins.readFile ./keys/peers.json);
    };
  };

  config = {
    # We need a separate netdev unit to create the Wireguard interface.
    systemd.network.netdevs."11-wgkube" = {
      netdevConfig = {
        Kind = "wireguard";
        Name = "wgkube";
      };
      wireguardConfig = {
        PrivateKeyFile = config.age.secrets."wgkube.key".path;
        ListenPort = wgPort;
      };
      # Generate Wireguard peers from the JSON input.
      wireguardPeers = lib.pipe peers [
        (lib.imap1 (
          index: entry: {
            PublicKey = entry.PublicKey;
            Endpoint = "${entry.PeerIP}:${toString wgPort}";
            # This instructs Wireguard what ranges belong to what peers. It'll
            # reject incoming traffic from an incorrect subnet, but also direct
            # outgoing traffic to the correct peer based on this. Note that
            # this doesn't create routes, however; we do that below.
            AllowedIPs = [
              (mkNodeCidr6 index)
              (mkNodeCidr4 index)
            ];
          }
        ))
        # Filter out ourselves based on index.
        # There's unfortunately no ifilter1 for one-based indexing.
        (lib.ifilter0 (index0: value: index0 != nodeIndex0))
      ];
    };
    # Now configure the interface with a network unit.
    systemd.network.networks."11-wgkube" = {
      matchConfig = {
        Name = "wgkube";
      };
      networkConfig = {
        # Set these options for reasons similar to brkube.
        ConfigureWithoutCarrier = true;
        LinkLocalAddressing = false;
        IPv6AcceptRA = false;
      };
      # Configures routes for the container subnets of peers.
      #
      # NOTE: We don't need to configure an address on this interface. As
      # long as we route traffic destined for other nodes to this interface,
      # Wireguard will send it to the correct peer based on AllowedIPs.
      #
      # For traffic from the host itself (not forwarded for containers), we
      # set PreferredSource to the host IP from brkube.
      routes = lib.pipe peers [
        # NOTE: This results in a list of lists.
        (lib.imap1 (
          index: entry: [
            {
              Destination = mkNodeCidr6 index;
              PreferredSource = config.kube.hostIp6;
            }
            {
              Destination = mkNodeCidr4 index;
              PreferredSource = config.kube.hostIp4;
            }
          ]
        ))
        # Filter out ourselves based on index.
        (lib.ifilter0 (index0: value: index0 != nodeIndex0))
        # After filtering we can take the flat list of routes.
        lib.flatten
      ];
    };
    # To inspect the Wireguard interface at runtime using the `wg` tool.
    environment.systemPackages = [ pkgs.wireguard-tools ];
  };
}
&lt;/code&gt;
    &lt;p&gt;Finally, we configure our firewall and NAT rules:&lt;/p&gt;
    &lt;code&gt;{ config, ... }:
{
  boot.kernel.sysctl = {
    # Enable forwarding on all interfaces.
    "net.ipv4.conf.all.forwarding" = 1;
    "net.ipv6.conf.all.forwarding" = 1;
  };
  networking.firewall.extraInputRules = ''
    # Open the Wireguard port.
    # You probably have to adjust this for your network situation.
    ip saddr 192.168.0.0/24 udp dport ${toString config.kube.wgPort} accept
    # Accept connections to Kubernetes Cluster IPs.
    # These are virtual IPs that every node makes available locally.
    ip daddr ${config.kube.servicesCidr} accept
  '';
  networking.firewall.extraForwardRules = ''
    # Route all container traffic anywhere (internet and internode).
    iifname brkube accept
    # Route Wireguard traffic destined for local containers.
    iifname wgkube ip6 daddr ${config.kube.nodeCidr6} accept
    iifname wgkube ip daddr ${config.kube.nodeCidr4} accept
  '';
  # Apply NAT to traffic from containers to the internet.
  # Here we create an `accept` rule to short-circuit traffic that
  # _shouldn't_ have NAT, then apply NAT to the rest.
  networking.nftables.tables = {
    "kube-nat6" = {
      family = "ip6";
      name = "kube-nat";
      content = ''
        chain post {
          type nat hook postrouting priority srcnat;
          iifname brkube ip6 daddr fd88::/16 accept
          iifname brkube masquerade
        }
      '';
    };
    "kube-nat4" = {
      family = "ip";
      name = "kube-nat";
      content = ''
        chain post {
          type nat hook postrouting priority srcnat;
          iifname brkube ip daddr 10.88.0.0/16 accept
          iifname brkube masquerade
        }
      '';
    };
  };
}
&lt;/code&gt;
    &lt;p&gt;At this point nodes should be able to ping eachother across the tunnel on their private IPs (&lt;code&gt;fd88:*::1&lt;/code&gt;), but we won't be able to test the full networking setup until we have some containers running.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hostnames&lt;/head&gt;
    &lt;p&gt;Kubernetes needs to be configured with a domain name where it will advertise Services in DNS. Many examples use &lt;code&gt;cluster.local&lt;/code&gt;, but I find this a bad idea, because .local is for mDNS. Instead, I'll be using &lt;code&gt;k8s.internal&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Nodes in Kubernetes register themselves with a name, typically whatever hostname is configured in the OS. However, I'm going to decouple this from the OS hostname and instruct Kubernetes to use &lt;code&gt;k8s.internal&lt;/code&gt; everywhere, leaving the OS hostname untouched.&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:
let
  inherit (config.kube)
    peers
    nodeIndex
    mkHostIp6
    mkHostIp4
    domain
    mkNodeHost
    ;
in
{
  options.kube = {
    # The internal domain name we use for all purposes Kubernetes.
    domain = lib.mkOption {
      type = lib.types.str;
      default = "k8s.internal";
    };
    # Function that defines the format for node hostnames.
    mkNodeHost = lib.mkOption {
      type = with lib.types; functionTo str;
      default = index: "node${toString index}.${domain}";
    };
    # The hostname of the local node.
    nodeHost = lib.mkOption {
      type = lib.types.str;
      default = mkNodeHost nodeIndex;
    };
    # All static hosts to add to the Kubernetes domain.
    # This is in a similar format to `networking.hosts`.
    allHosts = lib.mkOption {
      type = with lib.types; attrsOf (listOf str);
    };
    # `allHosts` as a file in `/etc/hosts` format.
    allHostsFile = lib.mkOption {
      type = lib.types.path;
      default = lib.pipe config.kube.allHosts [
        (lib.mapAttrsToList (ip: hosts: "${ip} ${lib.concatStringsSep " " hosts}\n"))
        lib.concatStrings
        (pkgs.writeText "kubernetes-static-hosts.txt")
      ];
    };
  };

  config = {
    # Add all node hosts to the Kubernetes domain.
    # The `mkBefore` ensures the node host is the first listed,
    # which is what a reverse IP lookup resolves to.
    kube.allHosts = lib.pipe peers [
      (lib.imap1 (
        index: entry: {
          ${mkHostIp6 index} = lib.mkBefore [ (mkNodeHost index) ];
          ${mkHostIp4 index} = lib.mkBefore [ (mkNodeHost index) ];
        }
      ))
      (lib.mergeAttrsList)
    ];

    # Also add the static hosts to `/etc/hosts`.
    networking.hostFiles = [ config.kube.allHostsFile ];
  };
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;kube-apiserver&lt;/head&gt;
    &lt;p&gt;We're going to build a multi-node setup, but keep it close to a traditional setup of 1 database server + multiple workers. In this setup, the database server is the ideal place for any kind of centralized processing, so we'll be running those parts of Kubernetes there as well. Instead of calling it a database server, I'll call it the 'primary' server going forward.&lt;/p&gt;
    &lt;code&gt;{ config, lib, ... }:
{
  options.kube = {
    # Define roles for nodes. The first node will be the 'primary' node.
    role = lib.mkOption {
      type = lib.types.str;
      default = if config.kube.nodeIndex == 1 then "primary" else "worker";
    };
    # The IP of the primary node.
    primaryIp = lib.mkOption {
      type = lib.types.str;
      default = config.kube.mkHostIp6 1;
    };
  };
}
&lt;/code&gt;
    &lt;p&gt;We'll add some further variables in &lt;code&gt;kube.api&lt;/code&gt; to describe the API endpoint:&lt;/p&gt;
    &lt;code&gt;{ config, lib, ... }:
{
  options.kube.api = {
    # Kubernetes creates a Service with Cluster IP for its own API.
    # This is always the first IP in the services subnet.
    serviceIp = lib.mkOption {
      type = lib.types.str;
      default = "10.88.0.1";
    };
    # The HTTPS port the API server will listen on.
    # This is only important when connecting directly to the primary node.
    # When using the Kubernetes Service, it's translated to regular 443.
    port = lib.mkOption {
      type = lib.types.port;
      default = 6443;
    };
    # Define an internal hostname for the API.
    # This is only used when a node host needs to talk to the API.
    # Containers instead use the Kubernetes Service to reach the API.
    internalHost = lib.mkOption {
      type = lib.types.str;
      default = "api.${config.kube.domain}";
    };
    # Build the full internal URL to the API.
    internalUrl = lib.mkOption {
      type = lib.types.str;
      default = "https://${config.kube.api.internalHost}:${toString config.kube.api.port}";
    };
    # An externally reachable host for the API.
    # The API server builds URLs using this hostname, so you'll want to add
    # this to DNS. Doesn't have to be fully public, could still be internal to
    # your organization.
    externalHost = lib.mkOption {
      type = lib.types.str;
      default = "test-kube.example.com";
    };
    # Build the full external URL to the API.
    # We also use this as the 'audience' of API server JWTs.
    externalUrl = lib.mkOption {
      type = lib.types.str;
      default = "https://${config.kube.api.externalHost}:${toString config.kube.api.port}";
    };
  };

  config = {
    # Add the internal API host to the Kubernetes domain.
    kube.allHosts.${config.kube.primaryIp} = [ config.kube.api.internalHost ];
  };
}
&lt;/code&gt;
    &lt;p&gt;The API server uses etcd for storage by default. We'll be creating a very simple installation here and protect it using Unix sockets with limited permissions.&lt;/p&gt;
    &lt;p&gt;In a production setup, you want to make periodic backups of the data in etcd. You can do this using &lt;code&gt;etcdctl snapshot save&lt;/code&gt;, or simply backup the files in
&lt;code&gt;/var/lib/etcd/member/snap/db&lt;/code&gt;. (The former method can't be piped into some other command, but the latter method excludes the database WAL file. See etcd disaster recovery.)&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:

# Only on the primary node.
lib.mkIf (config.kube.role == "primary") {

  # Create a dedicated user and group so we can control access to the socket.
  users.groups.etcd = { };
  users.users.etcd = {
    isSystemUser = true;
    group = "etcd";
  };

  # Configure the systemd service unit.
  systemd.services.etcd = {
    wantedBy = [ "multi-user.target" ];
    serviceConfig = {
      Type = "notify";
      User = "etcd";
      ExecStart =
        "${pkgs.etcd}/bin/etcd"
        + " --data-dir /var/lib/etcd"
        # Compaction is disabled by default, but that apparently risks the
        # database eventually exploding on itself. Weird default.
        + " --auto-compaction-retention=8h"
        # Minimum set of options for secure local-only setup without auth.
        # Access is limited to users in the 'etcd' group.
        + " --listen-peer-urls unix:/run/etcd/peer"
        + " --listen-client-urls unix:/run/etcd/grpc"
        + " --listen-client-http-urls unix:/run/etcd/http"
        # This is required but not actually used in our case.
        + " --advertise-client-urls http://localhost:2379";
      Restart = "on-failure";
      RestartSec = 10;
      # Actual data storage in /var/lib/etcd.
      StateDirectory = "etcd";
      StateDirectoryMode = "0700";
      # Place our Unix sockets in /run/etcd.
      RuntimeDirectory = "etcd";
      RuntimeDirectoryMode = "0750";
    };
    postStart = ''
      # Need to make sockets group-writable to allow connections.
      chmod 0660 /run/etcd/{grpc,http}
    '';
  };

  # For the `etcdctl` tool.
  environment.systemPackages = [ pkgs.etcd ];

}
&lt;/code&gt;
    &lt;p&gt;Now we are almost ready to start the API server! First we need to put some secrets in place for it.&lt;/p&gt;
    &lt;p&gt;You'll want an EncryptionConfiguration to tell Kubernetes how to encrypt Secret resources on disk. I recommend using a configuration with just &lt;code&gt;secretbox&lt;/code&gt; to start:&lt;/p&gt;
    &lt;code&gt;# Extend keys/secrets.nix.
"EncryptionConfiguration.yaml.age".publicKeys = [ master node1 ];
&lt;/code&gt;
    &lt;code&gt;# Edit the encrypted file.
agenix -i master_key -e EncryptionConfiguration.yaml.age
&lt;/code&gt;
    &lt;code&gt;apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  # Expand this if you have custom resources that store sensitive data.
  - resources:
      - secrets
    providers:
      - secretbox:
          keys:
            - name: key1
              # Generate this with: head --bytes=32 /dev/random | base64
              secret: "&amp;lt;BASE 64 ENCODED SECRET&amp;gt;"
&lt;/code&gt;
    &lt;p&gt;Next we need credentials for API server authentication. There are a bunch of methods available for this, but we'll be using the 'static token file' method, and handing a CSV file to the API server. A major downside of this is that the API server can't reload this at runtime, so changing any of these (such as when adding nodes) requires an API server restart.&lt;/p&gt;
    &lt;p&gt;We're going to create a &lt;code&gt;root&lt;/code&gt; user in the API with full admin access.&lt;/p&gt;
    &lt;code&gt;# Extend keys/secrets.nix.
"kube_token_root.age".publicKeys = [ master node1 ];
&lt;/code&gt;
    &lt;code&gt;# Generate and encrypt the token.
pwgen -s 64 | agenix -i master_key -e kube_token_root.age
&lt;/code&gt;
    &lt;p&gt;Nodes also need tokens to register themselves in the API, and I'm going to use a dirty trick here: reuse the Wireguard private keys as tokens. This means the API server has access to all Wireguard private keys, but I figure compromise of the API server means you can execute arbitrary code on any node anyway. If you're more concerned, you could just generate separate tokens instead. In any case, to reuse the Wireguard keys, the primary node needs access:&lt;/p&gt;
    &lt;code&gt;# Update keys/secrets.nix and ensure node1 is listed for every Wireguard key.
"wgkube1.key.age".publicKeys = [ master node1 ];
"wgkube2.key.age".publicKeys = [ master node1 node2 ];
&lt;/code&gt;
    &lt;p&gt;We also need some tokens for Kubernetes components that run alongside the API server on the primary node. I'm going to use the &lt;code&gt;kube_token_system_&lt;/code&gt; prefix for these, followed by the service name. That naming convention allows us to iterate files later.&lt;/p&gt;
    &lt;code&gt;# Extend keys/secrets.nix.
"kube_token_system_kube-controller-manager.age".publicKeys = [ master node1 ];
"kube_token_system_kube-scheduler.age".publicKeys = [ master node1 ];
&lt;/code&gt;
    &lt;code&gt;# Generate and encrypt the tokens.
for uid in kube-controller-manager kube-scheduler; do
  pwgen -s 64 | agenix -i master_key -e "kube_token_system_${uid}.age"
done
&lt;/code&gt;
    &lt;p&gt;To connect these components to the API server, we provide a tool to help generate a kubeconfig file:&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:
{
  options.kube = {
    # Small utility that helps us build a kubeconfig for our cluster.
    # The caller should set $KUBECONFIG to the file to create / modify.
    mkkubeconfig = lib.mkOption {
      type = lib.types.package;
      default = pkgs.writeShellApplication {
        name = "mkkubeconfig";
        runtimeInputs = [ pkgs.kubectl ];
        text = ''
          if [[ $# -ne 1 ]]; then
            echo &amp;gt;&amp;amp;2 'Usage: mkkubeconfig &amp;lt;token file&amp;gt;'
            exit 64
          fi

          # NOTE: The API server uses self-signed certificates. In this
          # testing setup we instead rely on the Wireguard tunnel for security.
          kubectl config set-cluster local --server '${config.kube.api.internalUrl}' --insecure-skip-tls-verify=true
          kubectl config set users.default.token "$(&amp;lt;"$1")"
          kubectl config set-context local --cluster=local --user=default
          kubectl config use-context local
        '';
      };
    };
  };
}
&lt;/code&gt;
    &lt;p&gt;We can finally slap together a NixOS module to start the API server. This is probably the most complex piece of Nix machinery in the setup.&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:

let

  package = lib.getBin pkgs.kubernetes;
  apiPortStr = toString config.kube.api.port;

  # NOTE: We put secerts in a separate variable here so we can easily gather
  # all secrets in `LoadCredential` below. Using `config.age.secrets` would pull
  # in secrets from elsewhere too, which is bad.
  keysDirListing = builtins.readDir ./keys;
  ageSecrets = lib.mergeAttrsList [
    # Decrypt EncryptionConfiguration.
    { "EncryptionConfiguration.yaml".file = ./keys/EncryptionConfiguration.yaml.age; }
    # Decrypt all API server tokens.
    (lib.pipe keysDirListing [
      (lib.filterAttrs (name: type: lib.hasPrefix "kube_token_" name))
      (lib.mapAttrs' (
        name: type: {
          name = lib.removeSuffix ".age" name;
          value.file = ./keys + "/${name}";
        }
      ))
    ])
    # Decrypt all Wireshark keys we reuse as tokens.
    (lib.pipe keysDirListing [
      (lib.filterAttrs (name: type: lib.hasPrefix "wgkube" name))
      (lib.mapAttrs' (
        name: type: {
          name = "kube_token_node" + (lib.removePrefix "wgkube" (lib.removeSuffix ".key.age" name));
          value.file = ./keys + "/${name}";
        }
      ))
    ])
  ];

in

# Only on the primary node.
lib.mkIf (config.kube.role == "primary") {

  age.secrets = ageSecrets;

  # Create a dedicated user for kube-apiserver, so we can add it to the etcd group.
  users.groups.kube-apiserver = { };
  users.users.kube-apiserver = {
    isSystemUser = true;
    group = "kube-apiserver";
    extraGroups = [ "etcd" ];
  };

  # Open the API server port in the firewall.
  networking.firewall.extraInputRules = ''
    tcp dport ${apiPortStr} accept
  '';

  systemd.services.kube-apiserver = {
    wantedBy = [ "multi-user.target" ];
    after = [ "etcd.service" ];
    serviceConfig = {
      Type = "notify";
      ExecStart =
        "${package}/bin/kube-apiserver"
        # Connect to etcd.
        + " --etcd-servers='unix:/run/etcd/grpc'"
        # HTTPS listener config.
        # The certificate is generated in `preStart` below.
        + " --secure-port=${apiPortStr}"
        + " --tls-private-key-file='/var/lib/kube-apiserver/apiserver.key'"
        + " --tls-cert-file='/var/lib/kube-apiserver/apiserver.crt'"
        # Authentication and authorization config.
        # `tokens.csv` is generated in `preStart` below.
        + " --anonymous-auth=false"
        + " --token-auth-file='/var/lib/kube-apiserver/tokens.csv'"
        + " --authorization-mode='RBAC,Node'"
        # Virtual IP range used for Service resources.
        # These IPs are routed by kube-proxy on each machine, usually via NAT.
        + " --service-cluster-ip-range='${config.kube.servicesCidr}'"
        # For the Service of the API server, advertise the node address.
        # Because this also uses NAT, it must also be IPv4.
        + " --advertise-address='${config.kube.hostIp4}'"
        # The externally reachable hostname for building API URLs.
        + " --external-hostname='${config.kube.api.externalHost}'"
        # Configures signing and verification of JWTs used as service account tokens.
        + " --service-account-issuer='${config.kube.api.externalUrl}'"
        + " --api-audiences='api,${config.kube.api.externalUrl}'"
        + " --service-account-key-file='/var/lib/kube-apiserver/issuer.key'"
        + " --service-account-signing-key-file='/var/lib/kube-apiserver/issuer.key'"
        # This sets up the encryption of Secret resources:
        # https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
        + " --encryption-provider-config='%d/EncryptionConfiguration.yaml'";
      User = "kube-apiserver";
      Restart = "on-failure";
      RestartSec = 10;
      # For generated keys and certificates.
      StateDirectory = "kube-apiserver";
      # Make secrets available.
      LoadCredential = map (name: "${name}:/run/agenix/${name}") (lib.attrNames ageSecrets);
      # For the `postStart` script.
      PrivateTmp = true;
    };
    preStart = ''
      openssl=${lib.getExe pkgs.openssl}
      cd /var/lib/kube-apiserver

      # Ensure a tokens file is present, or create an empty one.
      [[ -e tokens.csv ]] || touch tokens.csv
      chmod 0600 tokens.csv

      # Ensure the token for the root user is present.
      file="$CREDENTIALS_DIRECTORY/kube_token_root"
      if ! grep -q ",root," tokens.csv; then
        echo "$(&amp;lt;"$file"),root,root,system:masters" &amp;gt;&amp;gt; tokens.csv
      fi

      # Ensure tokens for system users are present.
      for file in $CREDENTIALS_DIRECTORY/kube_token_system_*; do
        filename="$(basename "$file")"
        uid="''${filename#kube_token_system_}"
        if ! grep -q ",system:$uid," tokens.csv; then
          echo "$(&amp;lt;"$file"),system:$uid,system:$uid" &amp;gt;&amp;gt; tokens.csv
        fi
      done

      # Ensure tokens for nodes are present.
      for file in $CREDENTIALS_DIRECTORY/kube_token_node*; do
        filename="$(basename "$file")"
        uid="''${filename#kube_token_}.${config.kube.domain}"
        if ! grep -q ",system:node:$uid," tokens.csv; then
          echo "$(&amp;lt;"$file"),system:node:$uid,system:node:$uid,system:nodes" &amp;gt;&amp;gt; tokens.csv
        fi
      done

      # Ensure a private key for HTTPS exists.
      [[ -e apiserver.key ]] || $openssl ecparam -out apiserver.key -name secp256r1 -genkey
      chmod 0600 apiserver.key

      # Generate a new self-signed certificate on every startup.
      # Assume services are restarted somewhere in this timeframe so that we
      # never have an expired certificate.
      $openssl req -new -x509 -nodes -days 3650 \
        -subj '/CN=${config.kube.api.externalHost}' \
        -addext 'subjectAltName=${
          lib.concatStringsSep "," [
            "DNS:${config.kube.api.externalHost}"
            "DNS:${config.kube.api.internalHost}"
            "IP:${config.kube.api.serviceIp}"
          ]
        }' \
        -key apiserver.key \
        -out apiserver.crt

      # Ensure a private key exists for issuing service account tokens.
      [[ -e issuer.key ]] || $openssl ecparam -out issuer.key -name secp256r1 -genkey
      chmod 0600 issuer.key
    '';
    postStart = ''
      # Wait for the API server port to become available.
      # The API server doesn't support sd_notify, so we do this instead to
      # properly signal any dependant services that the API server is ready.
      export KUBECONFIG=/tmp/kubeconfig
      ${lib.getExe config.kube.mkkubeconfig} "$CREDENTIALS_DIRECTORY/kube_token_root"
      tries=60
      while ! ${package}/bin/kubectl get namespaces default &amp;gt;&amp;amp; /dev/null; do
        if [[ $((--tries)) -eq 0 ]]; then
          echo "&amp;gt;&amp;gt; Timeout waiting for the API server to start"
          exit 1
        fi
        sleep 1
      done
      rm $KUBECONFIG
    '';
  };

}
&lt;/code&gt;
    &lt;p&gt;We setup a kubeconfig for &lt;code&gt;root&lt;/code&gt; on the primary node to use the &lt;code&gt;root&lt;/code&gt; API user. This allows using &lt;code&gt;kubectl&lt;/code&gt; from the shell for easy administration:&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:

# Only on the primary node.
lib.mkIf (config.kube.role == "primary") {

  # Generate a kubeconfig for root, so that `kubectl` simply works.
  system.activationScripts.kubeconfig-root = ''
    HOME=/root ${lib.getExe config.kube.mkkubeconfig} "/run/agenix/kube_token_root"
  '';

  environment.systemPackages = [ pkgs.kubectl ];

}
&lt;/code&gt;
    &lt;p&gt;And we also make node credentials available on each node, which will be used by services later:&lt;/p&gt;
    &lt;code&gt;{ lib, config, ... }:
{

  # Creates /run/kubeconfig-node containing the node credentials.
  # This is used by per-node services like kubelet, kube-proxy, coredns, etc.
  systemd.services.generate-kubeconfig-node = {
    wantedBy = [ "multi-user.target" ];
    environment.KUBECONFIG = "/run/kubeconfig-node";
    serviceConfig = {
      Type = "oneshot";
      ExecStart = "${lib.getExe config.kube.mkkubeconfig} /run/agenix/wgkube.key";
    };
  };

}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Add-ons&lt;/head&gt;
    &lt;p&gt;It's useful to have a way to load some YAML into the API server on startup. I use the term add-ons because I've seen it used for some now-deprecated functionality that was similar in function, but the term add-on has also been overloaded in various ways.&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:
let
  cfg = config.kube;
in
{
  options.kube = {
    # Run an activation script once the API is up.
    activationScript = lib.mkOption {
      type = lib.types.lines;
      default = "";
    };
    # Apply addons once the API is up.
    addons = lib.mkOption {
      type = lib.types.listOf lib.types.path;
      default = [ ];
    };
  };

  config = {

    assertions = [
      {
        assertion = cfg.activationScript != "" -&amp;gt; cfg.role == "primary";
        message = "kube.activationScript and kube.addons can only be used on the primary node";
      }
    ];

    # NOTE: This not a postStart in kube-apiserver, because that would cause
    # kube-apiserver to restart on changes.
    systemd.services.kube-activation = lib.mkIf (cfg.activationScript != "") {
      wantedBy = [ "multi-user.target" ];
      bindsTo = [ "kube-apiserver.service" ];
      after = [ "kube-apiserver.service" ];
      path = [ pkgs.kubectl ];
      # Connect to the API using the root credentials.
      environment.KUBECONFIG = "/root/.kube/config";
      serviceConfig = {
        Type = "oneshot";
        RemainAfterExit = true;
      };
      script = cfg.activationScript;
    };

    # Activation script that processes `kube.addons`.
    kube.activationScript = lib.mkIf (cfg.addons != [ ]) ''
      for file in ${lib.escapeShellArgs (pkgs.copyPathsToStore cfg.addons)}; do
        echo &amp;gt;&amp;amp;2 "# $file"
        kubectl apply --server-side --force-conflicts -f "$file"
      done
    '';

  };
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;kube-scheduler&lt;/head&gt;
    &lt;p&gt;Next we need to run kube-scheduler to actually schedule pods:&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:

# Only on the primary node.
lib.mkIf (config.kube.role == "primary") {

  systemd.services.kube-scheduler = {
    wantedBy = [ "multi-user.target" ];
    requires = [ "kube-apiserver.service" ];
    after = [ "kube-apiserver.service" ];
    serviceConfig = {
      ExecStart =
        "${pkgs.kubernetes}/bin/kube-scheduler"
        # Connect to the API.
        + " --kubeconfig='/tmp/kubeconfig'"
        # Disable listener, only useful for metrics.
        + " --secure-port=0";
      Restart = "on-failure";
      RestartSec = 10;
      # Let systemd assign a user for this service.
      DynamicUser = true;
      # For the below `preStart` that generates kubeconfig.
      PrivateTmp = true;
      LoadCredential = "kube-token:/run/agenix/kube_token_system_kube-scheduler";
    };
    preStart = ''
      # Generate a kubeconfig for the scheduler. Relies on PrivateTmp.
      KUBECONFIG=/tmp/kubeconfig ${lib.getExe config.kube.mkkubeconfig} "$CREDENTIALS_DIRECTORY/kube-token"
    '';
  };

}
&lt;/code&gt;
    &lt;head rend="h3"&gt;kube-controller-manager&lt;/head&gt;
    &lt;p&gt;Similarly, we need to run kube-controller-manager, which contains all the standard Kubernetes controllers:&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:

# Only on the primary node.
lib.mkIf (config.kube.role == "primary") {

  systemd.services.kube-controller-manager = {
    wantedBy = [ "multi-user.target" ];
    # NOTE: This 'bindsTo' also ensures an up-to-date API certificate is published.
    # When separating kube-controller-manager from kube-apiserver, some other mechanism
    # is required to distribute certificates.
    bindsTo = [ "kube-apiserver.service" ];
    after = [ "kube-apiserver.service" ];
    serviceConfig = {
      ExecStart =
        "${pkgs.kubernetes}/bin/kube-controller-manager"
        # Connect to the API.
        + " --kubeconfig='/tmp/kubeconfig'"
        # Disable listener, only useful for metrics.
        + " --secure-port=0"
        # This makes the controller manager automagically create a service
        # account for each of its controllers. Neat.
        + " --use-service-account-credentials=true"
        # This publishes the correct API certificate in the API itself.
        # Pods see this as `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt`.
        + " --root-ca-file='/var/lib/kube-apiserver/apiserver.crt'";
      Restart = "on-failure";
      RestartSec = 10;
      # Let systemd assign a user for this service.
      DynamicUser = true;
      # For the below `preStart` that generates kubeconfig.
      PrivateTmp = true;
      LoadCredential = "kube-token:/run/agenix/kube_token_system_kube-controller-manager";
    };
    preStart = ''
      # Generate a kubeconfig for the controller manager. Relies on PrivateTmp.
      KUBECONFIG=/tmp/kubeconfig ${lib.getExe config.kube.mkkubeconfig} "$CREDENTIALS_DIRECTORY/kube-token"
    '';
  };

}
&lt;/code&gt;
    &lt;head rend="h3"&gt;CoreDNS&lt;/head&gt;
    &lt;p&gt;We need to provide DNS resolution based on Services in the Kubernetes API.&lt;/p&gt;
    &lt;p&gt;Many deployments run CoreDNS inside Kubernetes, but there's really no standard for how you implement DNS resolution, and different deployments have different needs. As long as you have something that fetches Services from the Kubernetes API.&lt;/p&gt;
    &lt;p&gt;Here we setup CoreDNS, but not inside Kubernetes, instead managed by NixOS. We run an instance on every node for simplicity.&lt;/p&gt;
    &lt;code&gt;{
  config,
  lib,
  pkgs,
  ...
}:
{

  services.coredns = {
    enable = true;
    config = ''
      . {
        bind ${config.kube.hostIp6}
        errors

        # Resolve Kubernetes hosts.
        hosts ${config.kube.allHostsFile} ${config.kube.domain} {
          reload 0
          fallthrough
        }

        # Resolve Kubernetes services.
        kubernetes ${config.kube.domain} {
          kubeconfig {$CREDENTIALS_DIRECTORY}/kubeconfig-node
          ttl 30
          # NOTE: No fallthrough, to prevent a loop with systemd-reoslved.
        }

        # Forward everything else to systemd-resolved.
        forward . 127.0.0.53 {
          max_concurrent 1000
        }

        cache 30
        loadbalance
      }
    '';
  };

  # Provide kubeconfig-node to CoreDNS.
  systemd.services.coredns = {
    requires = [ "generate-kubeconfig-node.service" ];
    after = [
      "generate-kubeconfig-node.service"
      "kube-activation.service"
    ];
    serviceConfig.LoadCredential = "kubeconfig-node:/run/kubeconfig-node";
  };

  # Setup systemd-resolved to forward the Kubernetes domain to CoreDNS.
  environment.etc."systemd/dns-delegate.d/kubernetes.dns-delegate".text = ''
    [Delegate]
    Domains=${config.kube.domain}
    DNS=${config.kube.hostIp6}
  '';

  # Open the DNS port to containers.
  networking.firewall.extraInputRules = ''
    ip6 saddr ${config.kube.nodeCidr6} udp dport 53 accept
    ip6 saddr ${config.kube.nodeCidr6} tcp dport 53 accept
  '';

  # API resources needed for CoreDNS.
  kube.addons = lib.mkIf (config.kube.role == "primary") [
    ./addons/coredns.yaml
  ];

  # For inspecting DNS servers.
  environment.systemPackages = [ pkgs.dig ];

}
&lt;/code&gt;
    &lt;p&gt;The referenced add-on file &lt;code&gt;addons/coredns.yaml&lt;/code&gt; creates the permissions needed for CoreDNS to access the Kubernetes API:&lt;/p&gt;
    &lt;code&gt;# Define the coredns role and bind it to the regular node group,
# so that the same node credentials can be used for CoreDNS.
#
# Based on the roles from the upstream addon:
# https://github.com/kubernetes/kubernetes/blob/v1.34.2/cluster/addons/dns/coredns/coredns.yaml.base
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:coredns
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
      - services
      - pods
      - namespaces
    verbs:
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:coredns
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
  - kind: Group
    name: system:nodes
    apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;
    &lt;head rend="h3"&gt;kube-proxy&lt;/head&gt;
    &lt;p&gt;Kube-proxy is what implements cluster IPs assigned to Service resources in the API. It generates firewall rules to NAT cluster IPs to destination pods. It needs to run on every node.&lt;/p&gt;
    &lt;p&gt;(NOTE: If you decide not to run kubelet on your control plane / primary node, you still need to run kube-proxy! The API server may sometimes contact Services via their Cluster IP too.)&lt;/p&gt;
    &lt;code&gt;{
  lib,
  config,
  pkgs,
  ...
}:
{

  systemd.services.kube-proxy = {
    wantedBy = [ "multi-user.target" ];
    requires = [ "generate-kubeconfig-node.service" ];
    after = [
      "generate-kubeconfig-node.service"
      "kube-activation.service"
    ];
    path = [ pkgs.nftables ];
    serviceConfig = {
      ExecStart =
        "${lib.getBin pkgs.kubernetes}/bin/kube-proxy"
        # Connect to the API using node credentials.
        + " --kubeconfig='/run/kubeconfig-node'"
        + " --hostname-override='${config.kube.nodeHost}'"
        # Prefer nftables mode.
        + " --proxy-mode=nftables"
        # Local traffic can be detected by the bridge interface.
        + " --detect-local-mode=BridgeInterface"
        + " --pod-bridge-interface=brkube"
        # Addresses to accept NodePort service ports on.
        + " --nodeport-addresses='${config.kube.hostIp6}/128,${config.kube.hostIp4}/32'"
        # Can't seem to disable these listeners, so make sure they only listen on localhost.
        + " --healthz-bind-address=[::1]:10256"
        + " --metrics-bind-address=[::1]:10249";
      Restart = "on-failure";
      RestartSec = 10;
    };
  };

  # API resources needed for CoreDNS.
  kube.addons = lib.mkIf (config.kube.role == "primary") [
    ./addons/kube-proxy.yaml
  ];

}
&lt;/code&gt;
    &lt;p&gt;The referenced add-on file &lt;code&gt;addons/kube-proxy.yaml&lt;/code&gt; is again necessary to setup permissions in the Kubernetes API:&lt;/p&gt;
    &lt;code&gt;# Bind the kube-proxy role to the regular node group,
# so that the same node credentials can be used for kube-proxy.
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-proxy
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-proxier
subjects:
  - kind: Group
    name: system:nodes
    apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;
    &lt;head rend="h3"&gt;kubelet&lt;/head&gt;
    &lt;p&gt;Kubelet is the meat that starts containers on a node, when a pod is assigned to that node. Here we also do the work to setup the cri-o container runtime and the CNI configuration that tells it how containers get network.&lt;/p&gt;
    &lt;p&gt;You technically only need kubelet on machines that run workloads. We simply start it everywhere, including our primary node, but mark the primary as non-schedulable to demonstrate &lt;code&gt;registerWithTaints&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;{
  lib,
  config,
  pkgs,
  ...
}:

let

  yaml = pkgs.formats.yaml { };

  kubeletConfig = yaml.generate "kubelet.conf" {
    apiVersion = "kubelet.config.k8s.io/v1beta1";
    kind = "KubeletConfiguration";
    # Allow anonymous access, but bind to the secure Wireguard IP.
    # This is further locked down by firewall rules.
    address = config.kube.hostIp6;
    authentication.anonymous.enabled = true;
    authorization.mode = "AlwaysAllow";
    # Disable other listeners.
    healthzPort = 0;
    # Use CRI-O.
    containerRuntimeEndpoint = "unix:///var/run/crio/crio.sock";
    # Don't complain about swap, but don't account for it either.
    failSwapOn = false;
    memorySwap.swapBehavior = "LimitedSwap";
    # Configure DNS using the local CoreDNS server.
    clusterDomain = config.kube.domain;
    clusterDNS = [ config.kube.hostIp6 ];
    # Prevent scheduling pods on the primary node.
    registerWithTaints = lib.optional (config.kube.role == "primary") {
      key = "role";
      value = config.kube.role;
      effect = "NoSchedule";
    };
  };

in
{

  virtualisation.cri-o = {
    enable = true;
    extraPackages = [ pkgs.nftables ];
    settings.crio.runtime.log_to_journald = true;
  };

  systemd.services.kubelet = {
    wantedBy = [ "multi-user.target" ];
    requires = [
      "generate-kubeconfig-node.service"
      "crio.service"
    ];
    after = [
      "generate-kubeconfig-node.service"
      "crio.service"
      "kube-activation.service"
    ];
    path = [ pkgs.util-linux ];
    serviceConfig = {
      Type = "notify";
      ExecStart =
        "${lib.getBin pkgs.kubernetes}/bin/kubelet"
        # Connect to the API using node credentials.
        + " --kubeconfig='/run/kubeconfig-node'"
        # Ensure the Node is registered with the expected hostname.
        + " --hostname-override='${config.kube.nodeHost}'"
        # Publish our preferred IPv6 node IP.
        + " --node-ip='${config.kube.hostIp6}'"
        # Announce the role of this node as a label.
        + " --node-labels='role=${config.kube.role}'"
        # Most other flags are deprecated in favour of a config file.
        + " --config='${kubeletConfig}'";
      Restart = "on-failure";
      RestartSec = 10;
      StateDirectory = "kubelet";
    };
  };

  # cri-o bundles an example config file that NixOS installs by default, but we
  # override that here with our own configuration.
  environment.etc."cni/net.d/10-crio-bridge.conflist".text = lib.mkForce (
    builtins.toJSON {
      cniVersion = "1.0.0";
      name = "brkube";
      plugins = [
        {
          type = "bridge";
          bridge = "brkube";
          isGateway = true;
          ipam = {
            type = "host-local";
            ranges = [
              [ { subnet = config.kube.nodeCidr6; } ]
              [ { subnet = config.kube.nodeCidr4; } ]
            ];
            routes = [
              { dst = "::/0"; }
              { dst = "0.0.0.0/0"; }
            ];
          };
        }
      ];
    }
  );

  # Ensure kube-apiserver can connect to this kubelet.
  # This is necessary for `kubectl logs`, `kubectl exec`, etc.
  networking.firewall.extraInputRules = ''
    ip6 saddr ${config.kube.primaryIp} tcp dport 10250 accept
    tcp dport 10250 reject
  '';

}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Testing&lt;/head&gt;
    &lt;p&gt;The setup should now be fully functional! If you login as root on the primary node, you can use &lt;code&gt;kubectl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# kubectl get node
NAME                 STATUS   ROLES    AGE   VERSION
node1.k8s.internal   Ready    &amp;lt;none&amp;gt;   19s   v1.34.1
node2.k8s.internal   Ready    &amp;lt;none&amp;gt;   12s   v1.34.1
&lt;/code&gt;
    &lt;p&gt;With node2 in the listing, we know connectivity works from kubelet to API server. Starting a container with an interactive session also tests the opposite direction. In addition, we can test connectivity from the container to the internet:&lt;/p&gt;
    &lt;code&gt;# kubectl run --rm -it --image=docker.io/alpine test
/ # wget -O - https://example.com/
Connecting to example.com (23.220.75.245:443)
writing to stdout
&amp;lt;!doctype html&amp;gt;&amp;lt;html ...
&lt;/code&gt;
    &lt;head rend="h3"&gt;What next?&lt;/head&gt;
    &lt;p&gt;While this setup has all the essentials for workloads, a bunch of stuff is missing to make it more broadly useful.&lt;/p&gt;
    &lt;p&gt;An Ingress / Gateway controller helps route traffic to containers. The go-to used to be nginx-ingress, but nginx-ingress is going the way of the dodo. I had some fun hacking on caddy-ingress, but that's still experimental. There's a list of Gateway controllers and a list of Ingress controllers if you want to explore.&lt;/p&gt;
    &lt;p&gt;A storage provsioner can help with data persistence. The modern solution for this is CSI drivers. Provided are drivers for NFS and SMB shares, which are really useful if you're coming from a setup where applications share some NFS directories hosted on the primary node. But storage for databases is ideally block storage, which is a bit more work.&lt;/p&gt;
    &lt;p&gt;Speaking of databases, the nice thing about this setup is that you can simply run services outside Kubernetes, so you can just start a database using regular NixOS config on the primary node for example. I had some fun writing my own controller that allows managing MySQL databases with custom Kubernetes resources: external-mysql-operator. Again, very experimental.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;Would I take this into production? Not anytime soon, because I feel like there are a whole bunch of failure modes I've not yet seen. My testing has been limited to QEMU VMs and some AWS EC2 instances.&lt;/p&gt;
    &lt;p&gt;Especially on VMs, which are typically quite small compared to dedicated servers, Kubernetes itself uses up a chunk of memory and CPU just sitting there.&lt;/p&gt;
    &lt;p&gt;With the traction Kubernetes has, it does feel like there must be many small installations out there. And if that's the case, it seems to me that Kubernetes could easily reduce some complexity for that type of installation.&lt;/p&gt;
    &lt;p&gt;For example, do you really need etcd and API server redundancy? It seems upstream SQLite support in combination with Litestream backups would be far more beneficial for smaller installations, when you're happy to deal with some Kubernetes API downtime during upgrades or incidents.&lt;/p&gt;
    &lt;p&gt;Another easy win (in my opinion) would be runtime reloading of the token auth file. It would instantly make it a more viable option beyond testing. Though with a bit of extra work it can also be accomplished using the webhook or reverse proxy mechanisms supported by Kubernetes.&lt;/p&gt;
    &lt;p&gt;Overall, though, it feels like Kubernetes itself is maybe only half the complexity, with the other half going to network configuration.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stephank.nl/p/2025-11-17-a-small-vanilla-kubernetes-install-on-nixos.html"/><published>2025-11-18T11:01:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45963670</id><title>Gemini 3 Pro Model Card [pdf]</title><updated>2025-11-18T23:32:57.506808+00:00</updated><content/><link href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf"/><published>2025-11-18T11:12:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45963780</id><title>Cloudflare Global Network experiencing issues</title><updated>2025-11-18T23:32:57.127694+00:00</updated><content>&lt;doc fingerprint="40d4ee565e054cd0"&gt;
  &lt;main&gt;
    &lt;p&gt;Cloudflare services are currently operating normally. We are no longer observing elevated errors or latency across the network.&lt;/p&gt;
    &lt;p&gt;Our engineering teams continue to closely monitor the platform and perform a deeper investigation into the earlier disruption, but no configuration changes are being made at this time.&lt;/p&gt;
    &lt;p&gt;At this point, it is considered safe to re-enable any Cloudflare services that were temporarily disabled during the incident. We will provide a final update once our investigation is complete.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 17:44 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to monitor the system through recovery and we are seeing errors and latency return to normal levels. A full post-incident investigation and details about the incident will be made available asap.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 17:14 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to see errors drop as we work through services globally and clearing remaining errors and latency.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:46 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to see errors and latency improve but still have reports of intermittent errors. The team continues to monitor the situation as it improves, and looking for ways to accelerate full recovery.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:27 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Bot scores will be impacted intermittently while we undergo global recovery. We will update once we believe bot scores are fully recovered.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:04 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;The team is continuing to focus on restoring service post-fix. We are mitigating several issues that remain post-deployment.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 15:40 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to monitor for any further issues.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 15:23 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Some customers may be still experiencing issues logging into or using the Cloudflare dashboard. We are working on a fix to resolve this, and continuing to monitor for any further issues.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:57 UTC&lt;/p&gt;
    &lt;p&gt;Monitoring&lt;/p&gt;
    &lt;p&gt;A fix has been implemented and we believe the incident is now resolved. We are continuing to monitor for errors to ensure all services are back to normal.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:42 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We've deployed a change which has restored dashboard services. We are still working to remediate broad application services impact&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:34 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to work on a fix for this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:22 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing working on restoring service for application services customers.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:58 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing working on restoring service for application services customers.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:35 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have made changes that have allowed Cloudflare Access and WARP to recover. Error levels for Access and WARP users have returned to pre-incident rates. We have re-enabled WARP access in London.&lt;/p&gt;
    &lt;p&gt;We are continuing to work towards restoring other services.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:13 UTC&lt;/p&gt;
    &lt;p&gt;Identified&lt;/p&gt;
    &lt;p&gt;The issue has been identified and a fix is being implemented.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:09 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;During our attempts to remediate, we have disabled WARP access in London. Users in London trying to access the Internet via WARP will see a failure to connect.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:04 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:53 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:37 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing services recover, but customers may continue to observe higher-than-normal error rates as we continue remediation efforts.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:21 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:03 UTC&lt;/p&gt;
    &lt;p&gt;Investigating&lt;/p&gt;
    &lt;p&gt;Cloudflare is experiencing an internal service degradation. Some services may be intermittently impacted. We are focused on restoring service. We will update as we are able to remediate. More updates to follow shortly.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 11:48 UTC&lt;/p&gt;
    &lt;p&gt;This incident affected: Cloudflare Sites and Services (Access, Bot Management, CDN/Cache, Dashboard, Firewall, Network, WARP, Workers).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cloudflarestatus.com/incidents/8gmgl950y3h7"/><published>2025-11-18T11:35:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45966251</id><title>Nearly all UK drivers say headlights are too bright</title><updated>2025-11-18T23:32:56.837896+00:00</updated><content>&lt;doc fingerprint="24df8d635d03762d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nearly all drivers say headlights are too bright&lt;/head&gt;
    &lt;p&gt;Nearly all UK drivers said they thought headlights were too bright and that they have been dazzled by oncoming vehicles, according to a major study.&lt;/p&gt;
    &lt;p&gt;The government said last week that it will take a closer look at the design of cars and headlamps after concerns about lights dazzling drivers.&lt;/p&gt;
    &lt;p&gt;A study commissioned by the Department for Transport (DfT) found 97% of people surveyed found they were regularly or sometimes distracted by oncoming vehicles and 96% thought most or some headlights were too bright.&lt;/p&gt;
    &lt;p&gt;Dr Shaun Helman, who led the research for Berkshire-based Transport Research Laboratory (TRL), said it provides "compelling evidence" that lights' glare is a "genuine issue for UK drivers".&lt;/p&gt;
    &lt;p&gt;New measures will be included in the government's upcoming Road Safety Strategy, reflecting what is becoming an increasingly fraught issue for road users.&lt;/p&gt;
    &lt;p&gt;TRL's data suggests that LED and whiter headlamps may be linked to glare and that drivers might find their whiteness harder to cope with.&lt;/p&gt;
    &lt;p&gt;Of those surveyed, 33% said they had either stopped driving or are driving less at night because of lights, while another 22% said they would like to drive less at night but have no choice.&lt;/p&gt;
    &lt;p&gt;A total of 1,850 drivers, matched to the age and gender split of the country's licence holding population, were surveyed for their views.&lt;/p&gt;
    &lt;p&gt;TRL said LED lights used in vehicles are brighter, more concentrated and emit more blue light, which human eyes struggle with more at night.&lt;/p&gt;
    &lt;p&gt;The RAC's senior policy officer Rod Dennis said: "Having campaigned hard for this study, we welcome its findings which independently confirm what drivers have been telling us  that rather than being an imagined phenomenon, some bright headlights do cause a glare problem.&lt;/p&gt;
    &lt;p&gt;"While drivers clearly benefit from high-performing headlights, it's important this doesn't lead to others suffering the effects of dazzle, so a balance needs to be struck," he added.&lt;/p&gt;
    &lt;p&gt;Mr Dennis said that it is "vital" TRL's report is "reviewed carefully to put us on a path towards changes that ultimately benefit all road users."&lt;/p&gt;
    &lt;p&gt;Denise Voon, a clinical advisor at The College of Optometrists, said the DfT should "take immediate, actionable steps to support drivers and commission more detailed research, specifically into how headlight regulations need to change".&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c1j8ewy1p86o"/><published>2025-11-18T14:11:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45966435</id><title>Short Little Difficult Books</title><updated>2025-11-18T23:32:56.722246+00:00</updated><content/><link href="https://countercraft.substack.com/p/short-little-difficult-books"/><published>2025-11-18T14:23:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45967211</id><title>Gemini 3</title><updated>2025-11-18T23:32:56.388512+00:00</updated><content>&lt;doc fingerprint="f9d7a1cf9b3f9a95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A new era of intelligence with Gemini 3&lt;/head&gt;
    &lt;head rend="h3"&gt;A note from Google and Alphabet CEO Sundar Pichai:&lt;/head&gt;
    &lt;p&gt;Nearly two years ago we kicked off the Gemini era, one of our biggest scientific and product endeavors ever undertaken as a company. Since then, its been incredible to see how much people love it. AI Overviews now have 2 billion users every month. The Gemini app surpasses 650 million users per month, more than 70% of our Cloud customers use our AI, 13 million developers have built with our generative models, and that is just a snippet of the impact were seeing.&lt;/p&gt;
    &lt;p&gt;And were able to get advanced capabilities to the world faster than ever, thanks to our differentiated full stack approach to AI innovation  from our leading infrastructure to our world-class research and models and tooling, to products that reach billions of people around the world.&lt;/p&gt;
    &lt;p&gt;Every generation of Gemini has built on the last, enabling you to do more. Gemini 1s breakthroughs in native multimodality and long context window expanded the kinds of information that could be processed  and how much of it. Gemini 2 laid the foundation for agentic capabilities and pushed the frontiers on reasoning and thinking, helping with more complex tasks and ideas, leading to Gemini 2.5 Pro topping LMArena for over six months.&lt;/p&gt;
    &lt;p&gt;And now were introducing Gemini 3, our most intelligent model, that combines all of Geminis capabilities together so you can bring any idea to life.&lt;/p&gt;
    &lt;p&gt;Its state-of-the-art in reasoning, built to grasp depth and nuance  whether its perceiving the subtle clues in a creative idea, or peeling apart the overlapping layers of a difficult problem. Gemini 3 is also much better at figuring out the context and intent behind your request, so you get what you need with less prompting. Its amazing to think that in just two years, AI has evolved from simply reading text and images to reading the room.&lt;/p&gt;
    &lt;p&gt;And starting today, were shipping Gemini at the scale of Google. That includes Gemini 3 in AI Mode in Search with more complex reasoning and new dynamic experiences. This is the first time we are shipping Gemini in Search on day one. Gemini 3 is also coming today to the Gemini app, to developers in AI Studio and Vertex AI, and in our new agentic development platform, Google Antigravity  more below.&lt;/p&gt;
    &lt;p&gt;Like the generations before it, Gemini 3 is once again advancing the state of the art. In this new chapter, well continue to push the frontiers of intelligence, agents, and personalization to make AI truly helpful for everyone.&lt;/p&gt;
    &lt;p&gt;We hope you like Gemini 3, we'll keep improving it, and look forward to seeing what you build with it. Much more to come!&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Gemini 3: our most intelligent model that helps you bring any idea to life&lt;/head&gt;
    &lt;p&gt;Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind and Chief AI Architect, Google, on behalf of the Gemini team&lt;/p&gt;
    &lt;p&gt;Today were taking another big step on the path toward AGI and releasing Gemini 3.&lt;/p&gt;
    &lt;p&gt;Its the best model in the world for multimodal understanding and our most powerful agentic and vibe coding model yet, delivering richer visualizations and deeper interactivity  all built on a foundation of state-of-the-art reasoning.&lt;/p&gt;
    &lt;p&gt;Were beginning the Gemini 3 era by releasing Gemini 3 Pro in preview and making it available today across a suite of Google products so you can use it in your daily life to learn, build and plan anything. Were also introducing Gemini 3 Deep Think  our enhanced reasoning mode that pushes Gemini 3 performance even further  and giving access to safety testers before making it available to Google AI Ultra subscribers.&lt;/p&gt;
    &lt;head rend="h2"&gt;State-of-the-art reasoning with unprecedented depth and nuance&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro can bring any idea to life with its state-of-the-art reasoning and multimodal capabilities. It significantly outperforms 2.5 Pro on every major AI benchmark.&lt;/p&gt;
    &lt;p&gt;It tops the LMArena Leaderboard with a breakthrough score of 1501 Elo. It demonstrates PhD-level reasoning with top scores on Humanitys Last Exam (37.5% without the usage of any tools) and GPQA Diamond (91.9%). It also sets a new standard for frontier models in mathematics, achieving a new state-of-the-art of 23.4% on MathArena Apex.&lt;/p&gt;
    &lt;p&gt;Beyond text, Gemini 3 Pro redefines multimodal reasoning with 81% on MMMU-Pro and 87.6% on Video-MMMU. It also scores a state-of-the-art 72.1% on SimpleQA Verified, showing great progress on factual accuracy. This means Gemini 3 Pro is highly capable at solving complex problems across a vast array of topics like science and mathematics with a high degree of reliability.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is state-of-the-art across a range of key AI benchmarks. See details on our evaluation methodology.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro also brings a new level of depth and nuance to every interaction. Its responses are smart, concise and direct, trading clich and flattery for genuine insight  telling you what you need to hear, not just what you want to hear. It acts as a true thought partner that gives you new ways to understand information and express yourself, from translating dense scientific concepts by generating code for high-fidelity visualizations to creative brainstorming.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can code a visualization of plasma flow in a tokamak and write a poem capturing the physics of fusion.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini 3 Deep Think&lt;/head&gt;
    &lt;p&gt;Gemini 3 Deep Think mode pushes the boundaries of intelligence even further, delivering a step-change in Gemini 3s reasoning and multimodal understanding capabilities to help you solve even more complex problems.&lt;/p&gt;
    &lt;p&gt;In testing, Gemini 3 Deep Think outperforms Gemini 3 Pros already impressive performance on Humanitys Last Exam (41.0% without the use of tools) and GPQA Diamond (93.8%). It also achieves an unprecedented 45.1% on ARC-AGI-2 (with code execution, ARC Prize Verified), demonstrating its ability to solve novel challenges.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Deep Think mode excels on some of the most challenging AI benchmarks. See details on our evaluation methodology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gemini 3 helps you learn, build and plan anything&lt;/head&gt;
    &lt;head rend="h3"&gt;Learn anything&lt;/head&gt;
    &lt;p&gt;Gemini was built from the start to seamlessly synthesize information about any topic across multiple modalities, including text, images, video, audio and code. Gemini 3 pushes the frontier of multimodal reasoning to help you learn in ways that make sense for you by combining its state-of-the-art reasoning, vision and spatial understanding, leading multilingual performance, and 1 million-token context window.&lt;/p&gt;
    &lt;p&gt;For example, if you want to learn how to cook in your family tradition, Gemini 3 can decipher and translate handwritten recipes in different languages into a shareable family cookbook. Or if you want to learn about a new topic, you can give it academic papers, long video lectures or tutorials and it can generate code for interactive flashcards, visualizations or other formats that will help you master the material. It can even analyze videos of your pickleball match, identify areas where you can improve and generate a training plan for overall form improvements.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can help you learn and preserve family cooking traditions. Try it in Gemini Canvas.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can help you analyze complex information like research papers and can generate code for an interactive guide.&lt;/p&gt;
    &lt;p&gt;Get expert-level sports analysis on your pickleball match to help improve your game.&lt;/p&gt;
    &lt;p&gt;To help you make better sense of information on the web, AI Mode in Search now uses Gemini 3 to enable new generative UI experiences like immersive visual layouts and interactive tools and simulations, all generated completely on the fly based on your query.&lt;/p&gt;
    &lt;p&gt;Learn a complex topic like how RNA polymerase works with generative UI in AI Mode in Search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build anything&lt;/head&gt;
    &lt;p&gt;Building on the success of 2.5 Pro, Gemini 3 delivers on the promise of bringing any idea to life for developers. Its exceptional at zero-shot generation and handles complex prompts and instructions to render richer, more interactive web UI.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is the best vibe coding and agentic coding model weve ever built  making our products more autonomous and boosting developer productivity. It tops the WebDev Arena leaderboard by scoring an impressive 1487 Elo. It also scores 54.2% on Terminal-Bench 2.0, which tests a models tool use ability to operate a computer via terminal and it greatly outperforms 2.5 Pro on SWE-bench Verified (76.2%), a benchmark that measures coding agents.&lt;/p&gt;
    &lt;p&gt;You can now build with Gemini 3 in Google AI Studio, Vertex AI, Gemini CLI and our new agentic development platform, Google Antigravity. Its also available in third-party platforms like Cursor, GitHub, JetBrains, Manus, Replit and more.&lt;/p&gt;
    &lt;p&gt;Code a retro 3D spaceship game with richer visualizations and improved interactivity. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;Bring your imagination to life by building, deconstructing and remixing detailed 3D voxel art using code. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;Build a playable sci-fi world with shaders using Gemini 3. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;You can vibe code richer, more interactive web UI and apps with Gemini 3.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introducing a new agent-first development experience&lt;/head&gt;
    &lt;p&gt;As model intelligence accelerates with Gemini 3, we have the opportunity to reimagine the entire developer experience. Today were releasing Google Antigravity, our new agentic development platform that enables developers to operate at a higher, task-oriented level.&lt;/p&gt;
    &lt;p&gt;Using Gemini 3s advanced reasoning, tool use and agentic coding capabilities, Google Antigravity transforms AI assistance from a tool in a developers toolkit into an active partner. While the core of Google Antigravity is a familiar AI IDE experience, its agents have been elevated to a dedicated surface and given direct access to the editor, terminal and browser. Now, agents can autonomously plan and execute complex, end-to-end software tasks simultaneously on your behalf while validating their own code.&lt;/p&gt;
    &lt;p&gt;In addition to Gemini 3 Pro, Google Antigravity also comes tightly coupled with our latest Gemini 2.5 Computer Use model for browser control and our top-rated image editing model Nano Banana (Gemini 2.5 Image).&lt;/p&gt;
    &lt;p&gt;Google Antigravity uses Gemini 3 to drive an end-to-end agentic workflow for a flight tracker app. The agent independently plans, codes the application and validates its execution through browser-based computer use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Plan anything&lt;/head&gt;
    &lt;p&gt;Since introducing the agentic era with Gemini 2, weve made a lot of progress, not only advancing Geminis coding agent abilities, but also improving its ability to reliably plan ahead over longer horizons. Gemini 3 demonstrates this by topping the leaderboard on Vending-Bench 2, which tests longer horizon planning by managing a simulated vending machine business. Gemini 3 Pro maintains consistent tool usage and decision-making for a full simulated year of operation, driving higher returns without drifting off task.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro demonstrates better long-horizon planning to generate significantly higher returns compared to other frontier models.&lt;/p&gt;
    &lt;p&gt;This means Gemini 3 can better help you get things done in everyday life. By combining deeper reasoning with improved, more consistent tool use, Gemini 3 can take action on your behalf by navigating more complex, multi-step workflows from start to finish  like booking local services or organizing your inbox  all while under your control and guidance.&lt;/p&gt;
    &lt;p&gt;Google AI Ultra subscribers can try these agentic capabilities in the Gemini app with Gemini Agent today. Weve learned a lot improving Geminis agentic capabilities, and were excited to see how you use it as we expand to more Google products soon.&lt;/p&gt;
    &lt;p&gt;Gemini Agent can help you organize your Gmail inbox. Try it now in the Gemini app for Google AI Ultra subscribers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building Gemini 3 responsibly&lt;/head&gt;
    &lt;p&gt;Gemini 3 is our most secure model yet, and has undergone the most comprehensive set of safety evaluations of any Google AI model to date. The model shows reduced sycophancy, increased resistance to prompt injections and improved protection against misuse via cyberattacks.&lt;/p&gt;
    &lt;p&gt;In addition to our in-house testing for the critical domains in our Frontier Safety Framework, we've also partnered on evaluations with world-leading subject matter experts, provided early access to bodies like the UK AISI, and obtained independent assessments from industry experts like Apollo, Vaultis, Dreadnode and more. For more information, see the Gemini 3 model card.&lt;/p&gt;
    &lt;head rend="h2"&gt;The next era of Gemini&lt;/head&gt;
    &lt;p&gt;This is just the start of the Gemini 3 era. As of today, Gemini 3 starts rolling out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For everyone in the Gemini app and for Google AI Pro and Ultra subscribers in AI Mode in Search&lt;/item&gt;
      &lt;item&gt;For developers in the Gemini API in AI Studio, our new agentic development platform, Google Antigravity; and Gemini CLI&lt;/item&gt;
      &lt;item&gt;For enterprises in Vertex AI and Gemini Enterprise&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Gemini 3 Deep Think mode, were taking extra time for safety evaluations and input from safety testers before making it available to Google AI Ultra subscribers in the coming weeks.&lt;/p&gt;
    &lt;p&gt;We plan to release additional models to the Gemini 3 series soon so you can do more with AI. We look forward to getting your feedback and seeing what you learn, build and plan with Gemini.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/products/gemini/gemini-3/"/><published>2025-11-18T15:09:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45967814</id><title>Google Antigravity</title><updated>2025-11-18T23:32:56.184169+00:00</updated><link href="https://antigravity.google/"/><published>2025-11-18T15:47:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45968121</id><title>The code and open-source tools I used to produce a science fiction anthology</title><updated>2025-11-18T23:32:56.042575+00:00</updated><content>&lt;doc fingerprint="2fc09a362ef7ad11"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;&lt;p&gt; Last month I published &lt;/p&gt;Think Weirder: The Year's Best Science Fiction Ideas&lt;p&gt;, a 16-story anthology featuring Greg Egan, Isabel J. Kim, Ray Nayler, Caroline M. Yoachim, and twelve other wonderful authors. The book ended up being the #1 New Release in the Short Stories Anthologies category for a short time on Amazon, outselling many other newly released short story anthologies published by the big NYC publishers with large marketing departments. &lt;/p&gt;&lt;/div&gt;
      &lt;p&gt; I'm not a professional publisher. I have a full-time job and two small kids, so all of this work happened after my kids went to sleep. I had to use my time judiciously, which meant creating an efficient process. Fortunately I'm a programmer, and it turns out that programming skills translate surprisingly well to book publishing. This post is about how I built a complete publishing pipeline using Python, YAML files, and LaTeX  and why you might want to do something similar if you're considering publishing a book. I know that by writing this I'll have my choices questioned by professional designers, but hopefully the software concepts will be helpful. &lt;/p&gt;
      &lt;p&gt; My initial thought: can I really do ALL of this? &lt;/p&gt;
      &lt;p&gt; When I started this project, I had some worries. Professional publishers have entire departments of specialists. How could I possibly handle all of that myself? &lt;/p&gt;
      &lt;p&gt; The answer turned out to be: build tools that automate the repetitive parts, and use simple file formats that make everything transparent and debuggable. &lt;/p&gt;
      &lt;p&gt; Step 1: Tracking stories with plain text files &lt;/p&gt;
      &lt;p&gt; The first challenge was tracking hundreds of candidate stories from different magazines. I read 391 stories published in 2024 before selecting the final 16. That's a lot of stories to keep organized. &lt;/p&gt;
      &lt;p&gt; I could have used a spreadsheet, but I went with plain YAML files instead. Here's why this worked well for me: &lt;/p&gt;
      &lt;div&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Git-friendly: Every decision I made was tracked in version control&lt;/item&gt;
          &lt;item&gt;Human-readable: I could open any file in a text editor and understand what I was looking at&lt;/item&gt;
          &lt;item&gt;Easy to build scripts around: I wrote several Python functions to do different kinds of metadata introspection that I'll go through&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
      &lt;p&gt; The structure looks like this: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;data/
  story-progress.yaml       # Central tracking file
  markets.yaml              # Magazine metadata
  themes.yaml               # Theme occurrence tracking
  subgenres.yaml            # Subgenre tallies
stories/
  clarkesworld-magazine/
    nelson_11_24.yaml       # Individual story files
    pak_06_24.yaml
  reactor-magazine/
    larson_breathing.yaml
  ...&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Each story file is pure YAML containing the full story text plus metadata: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;title: "Twenty-Four Hours"
author: H.H. Pak
market: clarkesworld-magazine
url: https://clarkesworldmagazine.com/pak_06_24/
word_count: 4540
year: 2024
slug: pak_06_24
summary: ...&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Not all stories have public URLs available, but that's OK because all of the fields are optional. The central &lt;code&gt;story-progress.yaml&lt;/code&gt; tracks editorial state:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;clarkesworld-magazine-nelson_11_24:
  title: "LuvHome"
  author: Resa Nelson
  market: clarkesworld-magazine
  status: accepted  # or: not_started/relevant/rejected
  date_added: '2024-09-08T08:22:47.033192'&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Step 2: A simple command-line tool &lt;/p&gt;
      &lt;p&gt; I built a small Python CLI tool (&lt;code&gt;se.py&lt;/code&gt;) to help me navigate all this data. Since I do all this work at night after my kids go to sleep, I wanted something fast that mirrored a lot of the other work I do on the command line. The tool is simple:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;python se.py help
usage: se.py [-h] {markets,stories,relevant,decide,accepted,compile} ...

Story Evaluator CLI

positional arguments:
  {markets,stories,relevant,decide,accepted,compile}
                        Available commands
    markets             List markets
    stories             Manage stories
    relevant            List URLs for stories marked as relevant
    decide              Make accept/reject decisions on relevant stories
    accepted            Manage accepted stories
    compile             Show anthology compilation statistics

optional arguments:
  -h, help            show this help message and exit&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The &lt;code&gt;compile&lt;/code&gt; command ended up being really useful  it gave me instant feedback on anthology size and composition:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;ANTHOLOGY COMPILATION STATISTICS
============================================================
Total Stories: 16
Total Word Count: 115,093 words
Average Word Count: 7,193 words
Unique Authors: 16
Markets Represented: 4

STORIES BY MARKET:
  analog-magazine: 2 stories (12.5%)
  asimovs-magazine: 2 stories (12.5%)
  clarkesworld-magazine: 10 stories (62.5%)
  reactor-magazine: 2 stories (12.5%)&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; This was really helpful during the selection process. I could quickly check how far along I was toward my ~120k word goal, and make sure I hadn't accidentally included multiple stories by the same author. &lt;/p&gt;
      &lt;p&gt; Step 3: Typesetting the print book &lt;/p&gt;
      &lt;p&gt; This part surprised me the most. I initially thought I'd have to learn Adobe InDesign or pay someone to do the typesetting. But I decided to use LaTeX instead, since I had some previous experience with it (another publishing friend sent me some of his example files, and I had some academic experience). The process worked out better than expected. &lt;/p&gt;
      &lt;p&gt; I used XeLaTeX with the &lt;code&gt;memoir&lt;/code&gt; document class. Here's what I liked about this approach:
&lt;/p&gt;
      &lt;div&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reproducible: I can rebuild the entire book from source in a few seconds, and I can use the same templates next year&lt;/item&gt;
          &lt;item&gt;Professional typography: LaTeX handles ligatures, kerning, and line breaking better than I could manually&lt;/item&gt;
          &lt;item&gt;Custom fonts: I used Crimson Pro for body text and Rajdhani for titles&lt;/item&gt;
          &lt;item&gt;Again, version control that I'm used to: The entire book is just text files in Git&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
      &lt;p&gt; The main parts of the master file for the book are really simple: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\documentclass[final,11pt,twoside]{memoir}
\usepackage{compelling}

\begin{document}
\begin{frontmatter}
  \include{title}
  \tableofcontents
\end{frontmatter}

\begin{mainmatter}
  \include{introduction}
  \include{death-and-the-gorgon}
  \include{the-best-version-of-yourself}
  % ... 14 more stories
  \include{acknowledgements}
\end{mainmatter}
\end{document}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;div&gt;&lt;p&gt; All the formatting rules live in &lt;/p&gt;&lt;code&gt;compelling.sty&lt;/code&gt;&lt;p&gt;, a custom style package. &lt;/p&gt;Here's a link to the full, messy file&lt;p&gt;. Some highlights: &lt;/p&gt;&lt;/div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;% 6x9 inch trade paperback size
\setstocksize{9in}{6in}
\settrimmedsize{9in}{6in}{*}

% Margins
\setlrmarginsandblock{1.00in}{0.75in}{*}
\setulmarginsandblock{0.75in}{0.75in}{*}

% Typography nerding
\usepackage[final,protrusion=true,factor=1125,
            stretch=70,shrink=70]{microtype}

% Custom fonts loaded from local files
\setromanfont[
  Ligatures=TeX,
  Path=./Crimson_Pro/static/,
  UprightFont=CrimsonPro-Regular,
  BoldFont=CrimsonPro-Bold,
  ItalicFont=CrimsonPro-Italic,
  BoldItalicFont=CrimsonPro-BoldItalic
]{Crimson Pro}


\setsansfont[
  Path=./Rajdhani/,
  UprightFont=Rajdhani-Bold,
  BoldFont=Rajdhani-Bold,
  ItalicFont=Rajdhani-Bold,
  BoldItalicFont=Rajdhani-Bold
]{Rajdhani}

% Chinese font family for CJK characters
\newfontfamily\chinesefont{PingFang SC}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The &lt;code&gt;microtype&lt;/code&gt; package does a lot of subtle work with character spacing and line breaking that makes the text look professionally typeset.
&lt;/p&gt;
      &lt;p&gt; I wanted story titles in bold sans-serif with author names underneath in a lighter gray. Here's how I set that up: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\renewcommand{\chapter}[2]{
    \pagestyle{DefaultStyle}
    \stdchapter*{
        \sffamily
        \LARGE 
        \textbf{\MakeUppercase{#1}}
        \\ 
        \large 
        \color{dark-gray} 
        {\MakeUppercase{#2}}
    }
    \addcontentsline{toc}{chapter}{
        \protect\parbox[t]{\dimexpr\textwidth-3em}{
            \sffamily#1
            \\ 
            \protect\small
            \protect\color{gray}
            \protect\textit{#2}
        }
    }
    \def\leftmark{#1}
    \def\rightmark{#2}
}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; This redefines the &lt;code&gt;chapter&lt;/code&gt; command to take two arguments, the title and byline, and sets up both the chapter formatting, TOC formatting, and makes sure that the title and byline are printed in the headers on alternating pages.
&lt;/p&gt;
      &lt;p&gt; Now every story file just says: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\chapter{Death and the Gorgon}{by Greg Egan}
[story content]&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Most authors send me stories as HTML, PDF, or word, so I needed a way to convert them to LaTeX. I wrote a simple Python script to do this, which saved me a huge amount of manual formatting work. &lt;/p&gt;
      &lt;p&gt; Step 4: Creating the ebook &lt;/p&gt;
      &lt;p&gt; Print was one thing, but I also needed an ebook. This turned out to be easier than I expected because I could reuse all the LaTeX source I'd already created. &lt;/p&gt;
      &lt;p&gt; I used Pandoc to convert from LaTeX to EPUB: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;# Convert LaTeX to EPUB
pandoc 2025.tex -o Think_Weirder_2025.epub \
  toc \
  epub-cover-image=cover_optimized.jpg \
  css=epub-style.css \
  metadata title="Think Weirder" \
  metadata author="Edited by Joe Stech"&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Pandoc's default table of contents only showed story titles. But I wanted author names too, like you see in print anthologies. EPUBs are just zipped collections of XHTML files, so I wrote a small post-processing script: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;def modify_toc(nav_content, authors):
    """Add author bylines to TOC entries."""
    pattern = r'&amp;lt;a href="([^"]+)"&amp;gt;([^&amp;lt;]+)&amp;lt;/a&amp;gt;'

    def add_author(match):
        href, title = match.group(1), match.group(2)
        chapter_id = extract_id_from_href(href)

        if chapter_id in authors:
            author = authors[chapter_id]
            return f'&amp;lt;a href="{href}"&amp;gt;{title}&amp;lt;br /&amp;gt;\n' \
                   f'&amp;lt;em&amp;gt;{author}&amp;lt;/em&amp;gt;&amp;lt;/a&amp;gt;'
        return match.group(0)

    return re.sub(pattern, add_author, nav_content)&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The script unzips the EPUB, finds the navigation file, adds author bylines, and rezips everything. Now the ebook table of contents matches the print version. &lt;/p&gt;
      &lt;p&gt; What I learned &lt;/p&gt;
      &lt;p&gt; The whole process took longer than I expected  many months of night work. The simple software I wrote really made it a feasible one-person project though, and motivates me to go through the whole process again next year. &lt;/p&gt;
      &lt;p&gt; Staying organized is crucial. When hundreds of stories are involved, it's easy to forget details, so using &lt;code&gt;se.py&lt;/code&gt; to save metadata in the moment that could be sliced and diced later was so important.
&lt;/p&gt;
      &lt;p&gt; Reproducible builds were a lifesaver. I made changes to the book layout right up until the week before publication. Because I could rebuild the entire book in seconds, and everything was backed up in git, I could experiment freely without worrying about breaking things. &lt;/p&gt;
      &lt;p&gt; Simple file formats made me comfortable. When something went wrong, I could always open a YAML file or look at the LaTeX source and understand what was happening. I never hit a point where the tools were a black box. &lt;/p&gt;
      &lt;p&gt; I didn't need to understand everything up front. I learned LaTeX details as I went (arguably I still don't really understand LaTeX). Same with Pandoc. I got something basic working first, then incrementally improved it. &lt;/p&gt;
      &lt;p&gt; Can you do this too? &lt;/p&gt;
      &lt;p&gt; If you're thinking about publishing a book  whether it's an anthology, a novel, or a collection of technical writing  I think this approach is worth considering. There's something motivating about having a detailed understanding of every step in the production process. If you have questions feel free to reach out, I love talking about this hobby! You can email me at joe@thinkweirder.com. &lt;/p&gt;
      &lt;div&gt;&lt;p&gt; And if you enjoy concept-driven science fiction that is heavy on novel ideas, check out &lt;/p&gt;Think Weirder! &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://compellingsciencefiction.com/posts/the-code-and-open-source-tools-i-used-to-produce-a-science-fiction-anthology.html"/><published>2025-11-18T16:10:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45968362</id><title>Solving a million-step LLM task with zero errors</title><updated>2025-11-18T23:32:55.901251+00:00</updated><content>&lt;doc fingerprint="189cb409d4d7b6cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 12 Nov 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Solving a Million-Step LLM Task with Zero Errors&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2511.09030"/><published>2025-11-18T16:26:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45968611</id><title>Strix Halo's Memory Subsystem: Tackling iGPU Challenges</title><updated>2025-11-18T23:32:55.689178+00:00</updated><content>&lt;doc fingerprint="2efaf135353868c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Strix Halos Memory Subsystem: Tackling iGPU Challenges&lt;/head&gt;
    &lt;p&gt;Editors Note (11/2/2025): Due to an error in moving the article over from Google Docs to Substack, the Balancing CPU and GPU Bandwidth Demands section was missing some Cyberpunk 2077 data. Apologizes for the mistake!&lt;/p&gt;
    &lt;p&gt;AMDs Strix Halo aspires to deliver high CPU and GPU performance within a mobile device. Doing so presents the memory subsystem with a complicated set of demands. CPU applications are often latency sensitive with low bandwidth demands. GPU workloads are often latency tolerant and bandwidth hungry. Then, multitasking requires high memory capacity. Mobile devices need low power draw. Finally, the whole package has to fit within a price tag acceptable to consumers. Investigating how AMD dealt with those challenges should make for a good time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Credit&lt;/head&gt;
    &lt;p&gt;ASUS has kindly sampled the ROG Flow Z13, which implements Strix Halo in a tablet form factor with 32 GB of LPDDR5X. Theyve made deep dives like this possible, and we greatly appreciate their support.&lt;/p&gt;
    &lt;p&gt;RX 7600 results were provided by Azralee from the Chips and Cheese Discord.&lt;/p&gt;
    &lt;head rend="h1"&gt;GPU&lt;/head&gt;
    &lt;p&gt;Strix Halos GPU uses a similar cache setup to AMDs older and smaller mobile chips. As on Strix Point and Hawk Point (Zen 4 mobile), Strix Halos GPU is split into two Shader Arrays. Each Shader Array has 256 KB of L1 mid-level cache, and a 2 MB L2 services the entire GPU. Latencies to those GPU-private caches are in line with other RDNA3 and RDNA3.5 implementations. AMD likely kept L2 capacity at 2 MB because a 32 MB memory side cache (Infinity Cache, or MALL) takes over as the GPUs last level cache.The L2 only has to catch enough traffic to prevent the Infinity Cache from getting overwhelmed. The resulting cache setup is similar to the one in the RX 7600, a lower midrange RDNA3 discrete card.&lt;/p&gt;
    &lt;p&gt;The Infinity Cache on Strix Halo has slightly higher latency compared to implementations in AMDs discrete cards. DRAM latency from the GPU is higher as well. Compared to AMDs other mobile CPUs with iGPUs though, the 32 MB Infinity Cache offers a large cache capacity increase.&lt;/p&gt;
    &lt;p&gt;Nemess Vulkan bandwidth test achieves just under 1 TB/s from Infinity Cache. The figures align well with performance counter data. Taken together with the chips 2 GHz FCLK, bandwidth test results suggest the GPU has a 512B/cycle path to the interconnect. If so, each of the GPUs eight Infinity Fabric endpoints has a 64B/cycle link.&lt;/p&gt;
    &lt;p&gt;As a memory side cache, Infinity Cache can theoretically handle any access to physical addresses backed by DRAM. In an earlier interview with Cheese (George), AMD indicated that Infinity Cache was focused on the GPU, and that its behavior could change with firmware releases. Some of that change has happened already. When I first started testing Strix Halo just after Hot Chips 2025, results from my OpenCL microbenchmarks reflected Infinity Caches presence. I used that OpenCL code to figure out Data Fabric performance events. But PMU data collected from games suggested Infinity Cache wasnt used once a game went into the background. Hardware doesnt know whether a process is running in the foreground or background. Thats something the operating system knows, and that info would have to be communicated to hardware via drivers. Therefore, Infinity Cache policy can change on the fly from software control.&lt;/p&gt;
    &lt;p&gt;At that time, Nemess Vulkan-based code didnt reflect Infinity Caches presence. PMU data showed a match between CS and UMC traffic, indicating the microbenchmark wasnt taking advantage of Infinity Cache rather than the cache struggling with the access pattern. I was in the middle of investigating what Infinity Cache did or didnt apply to when Windows updated. Then, foreground/background status no longer had any effect. Nemess Vulkan code was also able to observe the Infinity Cache.&lt;/p&gt;
    &lt;p&gt;Early observations on Infinity Cache behavior arent relevant today, but they do show Infinity Caches behavior is influenced by factors beyond a memory requests origination point. Not all GPU requests install into the cache, and AMD can change cache policy on the fly. AMD could tune behavior with future updates too.&lt;/p&gt;
    &lt;p&gt;One early observation from OpenCL remained consistent though. Infinity Cache isnt used for a buffer created with the CL_MEM_ALLOC_HOST_PTR flag and managed with zero-copy map/unmap APIs. CL_MEM_ALLOC_HOST_PTR requests an allocation from host-visible memory. On systems with discrete GPUs, AMD tends to handle that by allocating memory from DRAM attached to the CPU.&lt;/p&gt;
    &lt;p&gt;Intuitively, that flag shouldnt make a difference on integrated GPUs. Im not sure why it affects Infinity Cache behavior. Perhaps Strix Halo splits address ranges for the CPU and GPU under the hood, and the CPUs address ranges arent cacheable from the Infinity Caches perspective.&lt;/p&gt;
    &lt;p&gt;AMDs discrete Radeon RX 9070 shows similar behavior, with Infinity Cache not being used for host-side memory. Latency to host memory goes up to nearly a microsecond on RX 9070, while it remains unchanged on Strix Halo. Integrated GPUs have an advantage with zero-copy compute code, and it shows.&lt;/p&gt;
    &lt;p&gt;To further check zero-copy behavior, I have a test that allocates a 256 MB buffer using OpenCLs Shared Virtual Memory APIs and only modifies a single 32-bit value. Strix Halo supports fine-grained buffer sharing like other recent AMD GPUs, meaning applications can use results generated from the GPU without calling map/unmap functions.&lt;/p&gt;
    &lt;p&gt;Strix Halo shows low latencies in line with zero-copy behavior. Its worth noting that not all integrated GPUs can avoid a copy under the hood.&lt;/p&gt;
    &lt;p&gt;Copy APIs like clEnqueueReadBuffer and clEnqueueWriteBuffer are still relevant, because theyre the traditional way to work with discrete GPUs. Those APIs often use the copy queue and DMA engines, which handle data movement without involving general purpose compute units. Strix Halo can achieve high copy bandwidth in the CPU to GPU direction, but not the other way around.&lt;/p&gt;
    &lt;p&gt;Performance counter data suggests copies to the GPU dont go through the Infinity Cache. During a copy, the shared memory controllers should observe both a read from CPU-side memory and a write to GPU-side memory. But theres nowhere near 100% overhead compared to software measurements.&lt;/p&gt;
    &lt;p&gt;Bandwidth is lower in the other direction, but curiously CS-level bandwidth is similar. The memory controllers see less bandwidth, indicating some requests were handled on-chip, likely by Infinity Cache. Curiously, theres way more than 100% overhead when comparing PMU data to software-visible copy bandwidth.&lt;/p&gt;
    &lt;head rend="h1"&gt;CPU&lt;/head&gt;
    &lt;p&gt;Strix Halos CPU side superficially resembles AMDs flagship desktop parts, with 16 Zen 5 cores split across two Core Complex Dies (CCDs). However, these CCDs use TSMCs InFO_oS for connectivity to the IO die rather than on-PCB traces. The CCD has 32B/cycle of bandwidth to the system in both the read and write directions.&lt;/p&gt;
    &lt;p&gt;Therefore, Strix Halos CCDs have more bandwidth at the die boundary than their desktop counterparts, but only in the write direction. Its an advantage thats likely to have minimal impact because reads often outnumber writes by a large margin.&lt;/p&gt;
    &lt;p&gt;Other CPU chiplet designs have more bandwidth at die boundaries, including the Compute Tile on Intels Meteor Lake and AMDs own GMI-Wide configuration. GMI-Wide uses two links between the CCD and IO die to maximize cross-die bandwidth in lower core count server chips. Even though GMI-Wide doesnt use advanced packaging, it has significantly more cross-die bandwidth than Strix Halo.&lt;/p&gt;
    &lt;p&gt;In a loaded latency test with reads, a Strix Halo CCD can reach high bandwidth levels at lower latency than standard GMI-Narrow CCDs. Part of that is likely down to its high bandwidth LPDDR5X setup, which a single CCD cant come close to saturating. But that advantage doesnt come through until bandwidth loads pass 45-55 GB/s. Before that, LPDDR5Xs high baseline latency puts Strix Halo at a disadvantage. At very high bandwidth load, Intel Meteor Lakes higher cross-die bandwidth keeps it ahead. AMDs GMI-Wide setup shows what a bandwidth-focused cross-die link can do, providing excellent bandwidth at low latency.&lt;/p&gt;
    &lt;p&gt;Bringing both CCDs into play gives Strix Halo a lead over Meteor Lake. Im starting the test by placing bandwidth load on CCD1 while running the latency test on CCD0. That gives lower latency at bandwidth loads below 60 GB/s because contention at the CCD interface is taken out of the picture. Latency does increase as I spread bandwidth load across both dies, and rises beyond 200 ns as the test approaches die-to-die bandwidth limits. However, a read-only pattern is still limited by cross-die bandwidth and falls far short of the 256 GB/s that the LPDDR5X setup is theoretically capable of.&lt;/p&gt;
    &lt;p&gt;Advanced packaging may provide latency benefits too. Regular AMD CCDs use SerDes (serializer-deserializer) blocks, which convert signals for transport over lower quality PCB traces. Zen 2s Infinity Fabric On-Package (IFOP) SerDes for example uses 32 transmit and 40 receive lanes running at a very high clock. Forwarded clock signals per lane data bundle help tackle clock skew that comes up with high speed parallel transmission over wires of unequal lengths. CRC helps ensure data integrity.&lt;/p&gt;
    &lt;p&gt;All of that adds power and latency overhead. Strix Halos InFO_oS packaging doesnt require SerDes. But any latency advantage is difficult to observe in practice. DRAM requests are the most common type of off-CCD traffic. High LPDDR5X latency masks any latency advantage when looking at DRAM requests, as shown above. Cache coherency traffic is another form of off-CCD traffic, and doesnt involve DRAM. However, testing that with a core to core latency test that bounces cachelines between core pairs also doesnt provide favorable results for Strix Halo.&lt;/p&gt;
    &lt;p&gt;AMD handles cross-CCX cache coherency at Coherent Stations (CS-es) that sit right in front of the memory controllers. Memory traffic is interleaved across memory channels and thus CS instances based on their physical address. I try hitting different physical addresses by testing with various cacheline offsets into a 4 KB page, which gives me different combinations of L3 slices and memory controller + CS pairs. Values within a single run reflect variation based on the tested core pair, while different runs display variation from different memory subsystem blocks owning the tested address.&lt;/p&gt;
    &lt;p&gt;Cross-CCX latencies on Strix Halo land in the 100-120 ns range depending on the location of the tested core pair, responsible L3 slice, and responsible CS. Its significantly higher on typical desktop systems or prior mobile chips from AMD. For example, the Ryzen 9 9900X tends to have cross-CCX latencies in the 80-90 ns range, which is in line with prior Zen generations. Its about 20 ns faster than Strix Halo.&lt;/p&gt;
    &lt;p&gt;Therefore, I dont have a satisfactory answer about Strix Halos cross-die latency. Latency may indeed be lower at die boundaries. But everything past that boundary has higher latency compared to other client systems, making any advantage invisible to software.&lt;/p&gt;
    &lt;head rend="h1"&gt;Balancing CPU and GPU Bandwidth Demands&lt;/head&gt;
    &lt;p&gt;Sharing a memory controller across the CPU and GPU comes with advantages, like making zero-copy behavior more natural to pull off. But it comes with challenges too. CPU and GPU memory requests can contend with each other for DRAM access. Contention surfaces as higher latency. From Zen 4 onward, AMDs L3 performance monitoring unit (PMU) can measure average latency in nanoseconds for requests external to the core cluster. PMU data isnt directly comparable to software measurements, because it only accounts for latency after the point of a L3 miss. But it is consistent in slightly under-estimating software observed latency when running a simple latency microbenchmark. When gaming, I typically see low CPU bandwidth demands and correspondingly mild latency increases over the baseline.&lt;/p&gt;
    &lt;p&gt;The same doesnt hold true when gaming on Strix Halos integrated GPU. Latency rises far above the baseline of around 140 ns. I logged average latency over 1 second intervals, and many of those intervals saw latency figures around 200 ns across several games&lt;/p&gt;
    &lt;p&gt;I wrote a microbenchmark to investigate how CPU memory latency is impacted by GPU-sde bandwidth load. As with the CPU loaded latency test, I run a latency test thread on a CPU core. But instead of using a read-only pattern, I do a standard C=A+B computation across large arrays on the GPU. To control GPU bandwidth load, I can have each OpenCL kernel invocation do more math with A and B before writing the result to C. Results show increased latency at higher GPU bandwidth demands. Other recent iGPUs show similar behavior.&lt;/p&gt;
    &lt;p&gt;In-game CPU bandwidth demands are low, but not as low as a simple latency test. I tried running a couple of read bandwidth threads on top of the test above. Strix Halo seems to let its GPU squeeze out the CPU when under extreme bandwidth demands. Latency suffers, passing 300 ns at one point.&lt;/p&gt;
    &lt;p&gt;Plotting L3 and memory controller PMU data with 1 second intervals helps capture the relationship between latency and bandwidth usage in more complex workloads. The points dont track well with microbenchmark data collected with a single CPU-side latency test thread. Perhaps theres enough CPU-side bandwidth demand to cause contention at both the die-to-die interface and the memory controllers. Or maybe, CPU and GPU bandwidth spikes tend to line up within those 1 second intervals. Whatever the case, PMU data highlights how Strix Halos CPU cores need high cache hitrates more than their desktop counterparts.&lt;/p&gt;
    &lt;p&gt;Cyberpunk 2077s built-in benchmark is largely CPU bound when run at 1080P with medium settings and no upscaling. I used Intels Arc B580 on desktop systems, since it has vaguely similar compute power to Strix Halos iGPU. Results show a large gap between Strix Halo and AMDs desktop platform, even though both use the same Zen 5 cores.&lt;/p&gt;
    &lt;p&gt;Memory latency under load is largely not a problem with CPU-only workloads, even when considering heavily multithreaded ones. Total bandwidth demands are much lower and actually well within the capabilities of a 128-bit DDR5 setup. That explains why AMD was able to take on quad channel HEDT parts using a desktop dual channel platform back in the Zen 2 days. Good caching likely played a role, and Strix Halo continues to have 4 MB of last level cache per core. PMU data from Cinebench, code compilation, and AV1 video encoding loosely align with microbenchmark results. Latency barely strays above the baseline. Y-Cruncher is an exception. Its very bandwidth hungry and not cache friendly. Its bandwidth demands are several times higher, and often go beyond a dual channel DDR5-5600 setups capabilities. Strix Halo is a good choice for that type of workload. But in the client space, bandwidth hungry CPU applications tend to be exceptions.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words: A GPU with an Integrated CPU?&lt;/head&gt;
    &lt;p&gt;Observations above suggest Strix Halos Infinity Fabric and DRAM setup focuses on feeding the GPU and as a result the CPU gets the short end of the stick. High Infinity Fabric endpoint count and a wide LPDDR5X bus provide high bandwidth at high latency. CPU workloads tend to be latency sensitive and contention can make that even worse.&lt;/p&gt;
    &lt;p&gt;Other aspects of the memory subsystem de-prioritize the CPU as well. CPU accesses dont fill into that cache, but still do a lookup likely to maintain cache coherency with the GPU. That cache lookup at the very least costs power and might add latency, even though itll almost never result in a hit. Lack of GMI-Wide style bandwidth is another example.&lt;/p&gt;
    &lt;p&gt;AMDs decisions are understandable. Most client workloads have light bandwidth requirements.Strix Halos memory system design lets it perform well in portable gaming devices like the ROG Flow Z13. But it does make tradeoffs. And extrapolating from those tradeoffs suggests iGPU designs will face steeper challenges at higher performance tiers.&lt;/p&gt;
    &lt;p&gt;For its part, Strix Halo strikes a good balance. It enjoys iGPU advantages without being large enough for the disadvantages to hurt. I hope AMD continues to target Strix Halos market segment with updated designs, and look forward to seeing where they go next.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/strix-halos-memory-subsystem-tackling"/><published>2025-11-18T16:41:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969250</id><title>Pebble, Rebble, and a path forward</title><updated>2025-11-18T23:32:55.443280+00:00</updated><content>&lt;doc fingerprint="fc4b101b5b1408f0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I believe the Pebble community, Core Devices, Rebble and I all want the same thing. We love our Pebbles and want them to keep working long into the future. We love the community that has sprung up around Pebble, and how its persevered - next year will be the 14th anniversary of the original Kickstarter campaign!&lt;/p&gt;
      &lt;p&gt;But I have to respond to claims made by Rebble posted on their blog yesterday. I will link to their post so you can read their side of the story, and Ive asked them to link back to this blog post from theirs.&lt;/p&gt;
      &lt;p&gt;Look - Im the first person to call myself out when I fail. I wrote a detailed blog post about Success and Failure at Pebble and often write in detail about learning from my mistakes. But in this specific case, youll find that Ive done my utmost to respect the Pebble legacy and community. Rebble is misleading the community with false accusations.&lt;/p&gt;
      &lt;p&gt;For those just passing through, heres the TLDR: &lt;/p&gt;
      &lt;p&gt;Core Devices is a small company I started in 2025 to relaunch Pebble and build new Pebble smartwatches. Rebble is a non-profit organization that has supported the Pebble community since 2017. Rebble has done a ton of great work over the years and deserves recognition and support for that.&lt;/p&gt;
      &lt;p&gt;Core Devices and Rebble negotiated an agreement where Core would pay $0.20/user/month to support Rebble services. But the agreement broke down after over the following disagreement. &lt;/p&gt;
      &lt;p&gt;Rebble believes that they 100% own the data of the Pebble Appstore. Theyre attempting to create a walled garden around 13,000 apps and faces that individual Pebble developers created and uploaded to the Pebble Appstore between 2012 and 2016. Rebble later scraped this data in 2017. &lt;/p&gt;
      &lt;p&gt;I disagree. Im working hard to keep the Pebble ecosystem open source. I believe the contents of the Pebble Appstore should be freely available and not controlled by one organization. &lt;/p&gt;
      &lt;p&gt;Rebble posted a blog post yesterday with a bunch of false accusations, and in this post I speak to each of them.&lt;/p&gt;
      &lt;p&gt;Sections&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Dec 2016 - Pebble shut down. Some IP was sold to Fitbit. I blogged about why I think we failed. Fitbit continued to run the Pebble Appstore and web services for 1.5 years. I really appreciated that.&lt;list rend="ul"&gt;&lt;item&gt;Rebble organization grew out of the official Pebble Developers Discord.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;July 2018, Fitbit shut down the Pebble appstore.&lt;list rend="ul"&gt;&lt;item&gt;Before it shut down, Rebble (and others) scraped all 13,000 apps and metadata from the Pebble Appstore. Rebble began hosting a copy of the appstore. They created a new Dev Portal where developers could upload new apps, roughly 500 have been uploaded since July 2018.&lt;/item&gt;&lt;item&gt;Rebble also reverse engineered many Pebble web services (weather, timeline and voice transcription) and provided them as a paid service for the Pebble community.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Jan 2025 - Google open sourced PebbleOS, breathing new life into the community.&lt;/item&gt;
        &lt;item&gt;March 2025 - I announced a new company (Core Devices) and 2 new watches - store.rePebble.com&lt;/item&gt;
        &lt;item&gt;November 2025 - we finished shipping out 5,000 Pebble 2 Duos. Were working hard on Pebble Time 2. Were aiming to start shipping in January.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 1: Rebble paid for the work that [Eric] took as a base for his commercial watches&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;I think theyre accusing me of stealing open source contributions to PebbleOS that Rebble paid for. This is entirely false.&lt;/item&gt;
        &lt;item&gt;We did not take any PebbleOS work Rebble paid for as a base for [our] commercial watches. &lt;del rend="overstrike"&gt;To my best of my knowledge&lt;/del&gt;&lt;del rend="overstrike"&gt;,&lt;/del&gt;&lt;del rend="overstrike"&gt;Rebble never paid the&lt;/del&gt;&lt;del rend="overstrike"&gt;developer who ported NimBLE into PebbleOS.&lt;/del&gt;&lt;del rend="overstrike"&gt;My best guess is that they are referring to Rebble having paid CodeCoup, the company behind&lt;/del&gt;&lt;del rend="overstrike"&gt;NimBLE&lt;/del&gt;&lt;del rend="overstrike"&gt;, to fix some bugs that affected older non-Core Devices watches. Any Rebble-sponsored CodeCoup commits are not present in our repo. In fact, the opposite is true - we paid Codecoup $10,000 to fix multiple BLE stack issues, some of them on the host side that benefit all devices, including old Pebbles.&lt;/del&gt; Update: Im told Rebble did pay him, months later. My point is valid - when we shifted development to our repo, Rebble had not paid anything. More broadly, I reject the premise that using open source software under the terms of the license, regardless of who funds development, is stealing.&lt;/item&gt;
        &lt;item&gt;We started using our own repo for PebbleOS development because PRs on the Rebble repo reviews were taking too long. We only had one firmware engineer at the time (now we have a whopping 2!) and he felt like he was being slowed down too much. All of our contributions to PebbleOS have been 100% open source.&lt;/item&gt;
        &lt;item&gt;Overall, the feedback that PebbleOS could benefit from open governance is well taken. Long term, PebbleOS would be a good fit for open source organization with experience in open governance, like Apache or Linux Foundation. I wrote about this last week.&lt;/item&gt;
        &lt;item&gt;With our small team and fairly quick development schedule, it's true that we haven't PRed our changes into Rebbles repo. Its tough to prioritize this while we are busy fixing bugs and getting ready for Pebble Time 2.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 2: Core took Rebbles work on &lt;code&gt;libpebblecommon&lt;/code&gt; to create &lt;code&gt;libpebble3&lt;/code&gt;&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;The majority (&amp;gt;90%) of our new open source&lt;code&gt;libpebble3&lt;/code&gt; library was written by Core Devices employees.  The remainder comes from &lt;code&gt;libpebblecommon&lt;/code&gt;, another open source library written by two people.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;In April 2025, Core purchased the copyright to the &lt;code&gt;libpebblecommon&lt;/code&gt; code from the two maintainers and incorporated it into &lt;code&gt;libpebble3&lt;/code&gt;**, which is also open source**.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;All our contributions to &lt;code&gt;libpebble3&lt;/code&gt; are GPL-3.0 licensed. Heres the motivation behind that our licensing strategy for this repo. We use the same CLA agreement as Matrix, QT and MySQL. Our CLA explicitly includes a clause that requires to Core Devices to distribute all contributions under an OSI-compatible FOSS license (e.g. GPLv3).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Note that neither Rebble &lt;code&gt;libpebblecommon&lt;/code&gt; maintainer signed the Rebble blog post.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Side note regarding Cobble, I dont think Rebble even knows this but in 2024, I personally spent over $30,000 to support its development, way before PebbleOS was open source. It was my own way to support the community.&lt;/p&gt;
      &lt;p&gt;Accusation 3: Core promised that they would let Rebble maintain and own the developer site&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Nothing of the sort was agreed upon. See the full written agreement that Core Devices has with Rebble towards the bottom. Rebble agreed that Core would host the developer site.&lt;/item&gt;
        &lt;item&gt;I have been maintaining and updating the developer site personally - all open source. Having two sources of truth would be confusing for the community.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 4: [Eric] scraped our app store, in violation of the agreement that we reached with him previously&lt;/p&gt;
      &lt;p&gt;Note: scraping usually means to automated extraction of data from a website.&lt;/p&gt;
      &lt;p&gt;Facts: &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Heres what happened. I wanted to highlight some of my favourite watchfaces on the Pebble Appstore. Last Monday Nov 10, after I put my kids to sleep and between long calls with factories in Asia, I started building a webapp to help me quickly go through Pebble Appstore and decide which were my top picks.&lt;/item&gt;
        &lt;item&gt;Let me be crystal clear - my little webapp did not download apps or scrape anything from Rebble. The webapp displayed the name of each watchface and screenshots and let me click on my favs. I used it to manually look through 6000 watchfaces with my own eyes. I still have 7,000 to go. Post your server logs, they will match up identically to the app I (wellClaude) wrote (source code here)&lt;/item&gt;
        &lt;item&gt;I integrated these picks into the Pebble Appstore on Saturday and posted about it on Sunday.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of four of these accusations could have been clarified simply by asking me. Instead, Rebble decided to post them on their blog and threaten a lawsuit. &lt;/p&gt;
      &lt;p&gt;How did we get here?&lt;/p&gt;
      &lt;p&gt;Why are there dueling blog posts in the Pebbleverse? &lt;/p&gt;
      &lt;p&gt;I think most of the people are behind Rebble are great and the community overall is awesome. I know they truly mean well, but there are many aspects of the org that are severely troubling. I am very close with one of the Rebble board members, who I consider a personal friend. Over the years, I learned a lot about the organization and helped coach him through some major disputes between board members. &lt;/p&gt;
      &lt;p&gt;I exchanged literally thousands of messages with my friend on this topic over the span of 3 years. I refrained from getting too involved, despite being asked several times to join Rebble as a board member or lead the organization. I demurred - I saw how painful it was for him and I had no interest in being part of that. &lt;/p&gt;
      &lt;p&gt;Core Devices + Rebble: 2025&lt;/p&gt;
      &lt;p&gt;PebbleOS is now open source! Yay. This is thanks to the work of many Googlers, ex-Pebblers and others - I called out (hopefully) all of them in my blog post in March. I really wanted Rebble to be a part of the Pebble revival going forward. I hired 3 people from Rebble to join Core Devices. I regularly brought up Rebbles efforts over the years.&lt;/p&gt;
      &lt;p&gt;I engaged with Rebble folks in discussions in the spring on how we could formally work together, and then made some concrete proposals in the summer. One difficulty was that Core Devices is a business with customers and schedules. This didnt always sync up with the timeframes of a non-profit. Things became very drawn out. It was very hard to pin people down, even on simple stuff like what the goals of Rebble as an organization were. &lt;/p&gt;
      &lt;p&gt;Regardless, I continued pushing to make Rebble a key part of the Pebble relaunch.&lt;/p&gt;
      &lt;p&gt;By August, we finally got close to an agreement.&lt;/p&gt;
      &lt;p&gt;On September 30 2025, we agreed to the following document and published respective blog posts (ours, theres). Core Devices would pay Rebble $0.20/user/month. I considered it a donation to a group that has done so much to support the community. But I purposely pushed for openness - no single group (Core Devices or Rebble) should be in control. &lt;/p&gt;
      &lt;p&gt;Notice the final bullet in the App store section: &lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;All binary/metadata (including historical apps) will be published as archive file (no scraping Rebble services) &lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;Looking back, we should have had more clear wording in this agreement. But this was after months of chat discussions and hours of Zoom calls. I honestly thought that we had reached an agreement to make the archive open, like in this message I received from a Rebble board member.&lt;/p&gt;
      &lt;p&gt;By the end of October, Rebble has changed their mind about providing an archive file.&lt;/p&gt;
      &lt;p&gt;Not withstanding their false accusations of theft, the crux of our disagreement is the archive of 13,000 Pebble apps and watchfaces that were uploaded to the Pebble Appstore in July 2018 before it was shut down. &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;I believe that these apps and watchfaces should be archived publicly and freely accessible by anyone. They should not held behind a walled garden by one organization. I repeatedly advocated for hosting this data on a neutral 3rd party like Archive.org.&lt;/item&gt;
        &lt;item&gt;Rebble believes the data behind the Pebble App Store is 100% Rebble (this is a direct quote from their blog post). They repeatedly refer to all watchfaces and watchapps as our data.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;This is just plainly false. The apps and watchfaces were originally uploaded by individual developers to an appstore run by a company that no longer exists. These folks created beautiful work and shared them freely with the Pebble community. Ive spoken with numerous Pebble app developers about this. After the fall of Pebble Tech Corp, none of them envisioned one single organization claiming ownership of their work and restricting access, or charging money for access.&lt;/p&gt;
      &lt;p&gt;Lets do the right thing - honour the original developers and create a free publicly available archive of their beautiful watchfaces and watchapps. &lt;/p&gt;
      &lt;p&gt;It's easy to assume the worst in situations like this. But our plan for the appstore is pretty straightforward. Were working on rewriting the appstore frontend to be native in the mobile app rather than a web view. Rebbles appstore backend API will be the data source. Rebbles dev portal is where developers upload apps. No subscription or Rebble account will not be required to download apps. We intend to curate how the appstore is displayed Pebble app.&lt;/p&gt;
      &lt;p&gt;Were excited to see other Pebble-supporting mobile apps pop up - like MicroPebble and GadgetBridge, offering different features and experiences. Wed love to support these efforts with open source code or financially.&lt;/p&gt;
      &lt;p&gt;Reading things like Were happy to let them build whatever they want as long as it doesnt hurt Rebble in their blog post worries me. Take our voice-to-text and weather features. Rebble currently offers these as part of their paid subscription. Our new Pebble mobile app includes a on-device speech-to-text feature. Were planning to include weather for free in our app and make the data available to all watchfaces so you dont need to configure each one separately. These features are better for users but would they hurt Rebble? Will I need to ask permission from Rebble before building these features? Its clear that the goals of a non-profit and device manufacturer will not always be in alignment.&lt;/p&gt;
      &lt;p&gt;Now consider the appstore. Its a fundamental part of the Pebble experience. Even before yesterdays accusations, I felt wary about relying too heavily on a 3rd party like Rebble to provide such a critical service. When people buy a watch from Core Devices, they expect to be able to download apps and watchfaces. If Rebble leadership changes their mind, how can I be certain I can deliver a good experience for our customers? This is one of the primary reasons I think its important for an archive of the Pebble Appstore to be freely available.&lt;/p&gt;
      &lt;p&gt;Rebble - prove that you believe in an open, unrestricted Pebble community. Tear down the walled garden you are trying to create. Publish your copy of the Pebble Appstore archive. Stop saying that you 100% own other developers data. Lets move on from this ridiculous sideshow and focus on making Pebble awesome!&lt;/p&gt;
      &lt;p&gt;Ive worked hard to structure everything that were doing to be sustainable for the long term, and to do right by the Pebble community. I think Rebble should do the same. &lt;/p&gt;
      &lt;p&gt;I earned almost nothing from Pebble Tech Corp. I paid myself a $65,000 salary each year. I did not get any payout through the asset sale. I fought to make sure that all Pebble employees were taken care of as best as possible, and that the Pebble community would live on. I believe that at every turn, Ive done right by the community.&lt;/p&gt;
      &lt;p&gt;I didnt relaunch Pebble to make a lot of money. My goal this time round is to make it sustainable. I want to continue making more watches and cool gadgets. There are no investors. I am taking huge risks doing this. I relaunched it because I love Pebble and want it to live on long into the future. Generally, I am excited and positive for the future, despite everything.&lt;/p&gt;
      &lt;p&gt;For everyone else, again, I apologize for the extreme amounts of inside baseball and the better things you could be doing with your time. Ill leave the comments open here. Please refrain from any personal attacks or vicious comments (at myself or other people) - follow the HN guidelines.&lt;/p&gt;
      &lt;p&gt;Eric Migicovsky&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ericmigi.com/blog/pebble-rebble-and-a-path-forward/"/><published>2025-11-18T17:24:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969708</id><title>Show HN: Guts  convert Golang types to TypeScript</title><updated>2025-11-18T23:32:54.805786+00:00</updated><content>&lt;doc fingerprint="17201a0ffe1c1fc0"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;guts&lt;/code&gt; is a tool to convert golang types to typescript for enabling a consistent type definition across the frontend and backend. It is intended to be called and customized as a library, rather than as a command line executable.&lt;/p&gt;
    &lt;p&gt;See the simple example for a basic usage of the library.&lt;/p&gt;
    &lt;code&gt;type SimpleType[T comparable] struct {
	FieldString     string
	FieldInt        int
	FieldComparable T
	FieldTime       time.Time
}&lt;/code&gt;
    &lt;p&gt;Gets converted into&lt;/p&gt;
    &lt;code&gt;type Comparable = string | number | boolean;

// From main/main.go
interface SimpleType&amp;lt;T extends Comparable&amp;gt; {
    FieldString: string;
    FieldInt: number;
    FieldComparable: T;
    FieldTime: string;
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;guts&lt;/code&gt; is a library, not a command line utility. This is to allow configuration with code, and also helps with package resolution.&lt;/p&gt;
    &lt;p&gt;See the simple example for a basic usage of the library. A larger example can be found in the Coder repository.&lt;/p&gt;
    &lt;code&gt;// Step 1: Create a new Golang parser
golang, _ := guts.NewGolangParser()

// Optional: Preserve comments from the golang source code
// This feature is still experimental and may not work in all cases
golang.PreserveComments()

// Step 2: Configure the parser
_ = golang.IncludeGenerate("github.com/coder/guts/example/simple")
// Step 3: Convert the Golang to the typescript AST
ts, _ := golang.ToTypescript()
// Step 4: Mutate the typescript AST
ts.ApplyMutations(
    config.ExportTypes, // add 'export' to all top level declarations
)
// Step 5: Serialize the typescript AST to a string
output, _ := ts.Serialize()
fmt.Println(output)&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;guts&lt;/code&gt; first parses a set of golang packages. The Go AST is traversed to find all the types defined in the packages.&lt;/p&gt;
    &lt;p&gt;These types are placed into a simple AST that directly maps to the typescript AST.&lt;/p&gt;
    &lt;p&gt;Using goja, these types are then serialized to typescript using the typescript compiler API.&lt;/p&gt;
    &lt;p&gt;The generator aims to do the bare minimum type conversion. An example of a common opinion, is to use types to represent enums. Without the mutation, the following is generated:&lt;/p&gt;
    &lt;code&gt;export enum EnumString {
    EnumBar = "bar",
    EnumBaz = "baz",
    EnumFoo = "foo",
    EnumQux = "qux"
}&lt;/code&gt;
    &lt;p&gt;Add the mutation:&lt;/p&gt;
    &lt;code&gt;ts.ApplyMutations(
	config.EnumAsTypes,
)
output, _ := ts.Serialize()&lt;/code&gt;
    &lt;p&gt;And the output is:&lt;/p&gt;
    &lt;code&gt;export type EnumString = "bar" | "baz" | "foo" | "qux";&lt;/code&gt;
    &lt;p&gt;The guts package was created to offer a more flexible, programmatic alternative to existing Go-to-TypeScript code generation tools out there.&lt;/p&gt;
    &lt;p&gt;The other solutions out there function as command-line utilities with yaml configurability. &lt;code&gt;guts&lt;/code&gt; is a library, giving it a much more flexible and dynamic configuration that static generators cant easily support.&lt;/p&gt;
    &lt;p&gt;Unlike many of its counterparts, guts leverages the official TypeScript compiler under the hood, ensuring that the generated TypeScript definitions are semantically correct, syntactically valid, and aligned with the latest language features.&lt;/p&gt;
    &lt;p&gt;An incredible website to visualize the AST of typescript: https://ts-ast-viewer.com/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/coder/guts"/><published>2025-11-18T17:55:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969909</id><title>I am stepping down as the CEO of Mastodon</title><updated>2025-11-18T23:32:54.685807+00:00</updated><content>&lt;doc fingerprint="6c92901d0c810af1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My next chapter with Mastodon&lt;/head&gt;
    &lt;p&gt;Eugen Rochko&lt;/p&gt;
    &lt;p&gt;Strategy &amp;amp; Product Advisor, Founder&lt;/p&gt;
    &lt;p&gt;After nearly 10 years, I am stepping down as the CEO of Mastodon and transferring my ownership of the trademark and other assets to the Mastodon non-profit. Over the course of my time at Mastodon, I have centered myself less and less in our outward communications, and to some degree, this is the culmination of that trend. Mastodon is bigger than me, and though the technology we develop on is itself decentralizedwith heaps of alternative fediverse projects demonstrating that participation in this ecosystem is possible without our involvementit benefits our community to ensure that the project itself which so many people have come to love and depend on remains true to its values. There are too many examples of founder egos sabotaging thriving communities, and while Id like to think myself an exception, I understand why people would prefer better guardrails.&lt;/p&gt;
    &lt;p&gt;But it would be uncouth for me to pretend that there isnt some self-interest involved. Being in charge of a social media project is, turns out, quite the stressful endeavour, and I dont have the right personality for it. I think I need not elaborate that the passion so many feel for social media does not always manifest in healthy ways. You are to be compared with tech billionaires, with their immense wealth and layered support systems, but with none of the money or resources. It manifests in what people expect of you, and how people talk about you. I remember somebody jokingly suggesting that I challenge Elon Musk to a fight (this was during his and Mark Zuckerbergs martial arts feud), and quietly thinking to myself, I am literally not paid enough for that. I remember also, some Spanish newspaper article that for some reason, concluded that I dont dress as fashionably as Jeff Bezos, based on the extremely sparse number of pictures of myself I have shared on the web. Over an entire decade, these tiny things chip away at you slowly. Some things chip faster. I steer clear of showing vulnerability online, but there was a particularly bad interaction with a user last summer that made me realise that I need to take a step back and find a healthier relationship with the project, ultimately serving as the impetus to begin this restructuring process.&lt;/p&gt;
    &lt;p&gt;As for what the legacy of my run will be, I find hard to answer. For one, I think it is not up for me to judge. On the other hand, it is as much about what didnt happen as it is about what did. Ive always thought that one of the most important responsibilities I had was to say no. It is not a popular thing to do, nor is it a fun thing to do, but being pulled into too many different directions at once can spell disaster for any project. Id like to think I avoided some trouble by being careful. But Im also aware that my aversion to public appearances cost Mastodon some opportunities in publicity. Ultimately, while I cannot take sole credit for it, I am nevertheless most proud of how far weve made it over these last 10 years. From the most barebones project written out of my childhood bedroom, to one of the last remaining and thriving pieces of the original, community-centred internet.&lt;/p&gt;
    &lt;p&gt;I have so much passion for Mastodon and the fediverse. The fediverse is an island within an increasingly dystopian capitalist hellscape. And from my perspective, Mastodon is our best shot at bringing this vision of a better future to the masses. This is why Im sticking around, albeit in a more advisory, and less public, role.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.joinmastodon.org/2025/11/my-next-chapter-with-mastodon/"/><published>2025-11-18T18:13:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45970338</id><title>Show HN: RowboatX  open-source Claude Code for everyday automations</title><updated>2025-11-18T23:32:54.079204+00:00</updated><content>&lt;doc fingerprint="fd746fd3152ca8f9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Create background agents with full shell access &lt;list rend="ul"&gt;&lt;item&gt;E.g. "Generate a NotebookLM-style podcast from my saved articles every morning"&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt; Connect any MCP server to add capabilities &lt;list rend="ul"&gt;&lt;item&gt;Add MCP servers and RowboatX handles the integration&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt; Let RowboatX control and monitor your background agents &lt;list rend="ul"&gt;&lt;item&gt;Easily inspect state on the filesystem&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inspired by Claude Code, RowboatX brings the same shell-native power to background automations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Set your LLM API key. Supports OpenAI, Anthropic, Gemini, OpenRouter, LiteLLM, Ollama, and more.&lt;/p&gt;
        &lt;code&gt;export OPENAI_API_KEY=your-openai-api-key&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install RowboatX&lt;/p&gt;
        &lt;quote&gt;npx @rowboatlabs/rowboatx&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add MCP: 'Add this MCP server config: &amp;lt;config&amp;gt; '&lt;/item&gt;
      &lt;item&gt;Explore tools: 'What tools are there in &amp;lt;server-name&amp;gt; '&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;'Create agent to do X.'&lt;/item&gt;
      &lt;item&gt;'... Attach the correct tools from &amp;lt;mcp-server-name&amp;gt; to the agent'&lt;/item&gt;
      &lt;item&gt;'... Allow the agent to run shell commands including ffmpeg'&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;'Make agent &amp;lt;background-agent-name&amp;gt; run every day at 10 AM'&lt;/item&gt;
      &lt;item&gt;'What agents do I have scheduled to run and at what times'&lt;/item&gt;
      &lt;item&gt;'When was &amp;lt;background-agent-name&amp;gt; last run'&lt;/item&gt;
      &lt;item&gt;'Are any agents waiting for my input or confirmation'&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;rowboatx --agent=&amp;lt;agent-name&amp;gt; --input="xyz" --no-interactive=true&lt;/code&gt;
    &lt;code&gt;rowboatx --agent=&amp;lt;agent-name&amp;gt; --run_id=&amp;lt;run_id&amp;gt; # resume from a previous run&lt;/code&gt;
    &lt;p&gt;You can configure your models in &lt;code&gt;~/.rowboat/config/models.json&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;{
  "providers": {
    "openai": {
      "flavor": "openai"
    },
    "openai-compatible-host": {
      "flavor": "openai",
      "baseURL": "http://localhost:2000/...",
      "apiKey": "...",
      "headers": {
        "foo": "bar"
      }
    },
    "anthropic": {
      "flavor": "anthropic"
    },
    "google": {
      "flavor": "google"
    },
    "ollama": {
      "flavor": "ollama"
    }
  },
  "defaults": {
    "provider": "openai",
    "model": "gpt-5"
  }
}&lt;/code&gt;
    &lt;p&gt;To use Rowboat Classic UI (not RowboatX), refer to Classic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rowboatlabs/rowboat"/><published>2025-11-18T18:50:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45970391</id><title>OrthoRoute  GPU-accelerated autorouting for KiCad</title><updated>2025-11-18T23:32:53.962332+00:00</updated><content>&lt;doc fingerprint="f2d15d492c15bffa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;OrthoRoute  GPU-Accelerated Autorouting for KiCad&lt;/head&gt;
    &lt;p&gt;OrthoRoute is a GPU-accelerated PCB autorouter that uses a Manhattan lattice and the PathFinder algorithm to route high-density boards. Built as a KiCad plugin using the IPC API, it handles complex designs with thousands of nets that make traditional push-and-shove routers give up.&lt;/p&gt;
    &lt;p&gt;Never trust the autorouter, but at least this one is fast.&lt;/p&gt;
    &lt;head rend="h4"&gt;This document is a complement to the README in the Github repository. The README provides information about performance, capabilities, and tests. This document reflects more on the why and how OrthoRoute was developed.&lt;/head&gt;
    &lt;head rend="h1"&gt;Why I Built This&lt;/head&gt;
    &lt;p&gt;This is a project born out of necessity. Another thing I was working on needed an enormous backplane. A PCB with sixteen connectors, with 1,100 pins on each connector. Thats 17,600 individual pads, and 8,192 airwires that need to be routed. Here, just take a look:&lt;/p&gt;
    &lt;p&gt;Look at that shit. Hand routing this would take months. For a laugh, I tried FreeRouting, the KiCad autorouter plugin, and it routed 4% of the traces in seven hours. If that trend held, which it wouldnt, that would be a month of autorouting. And it probably wouldnt work in the end. I had a few options, all of which would take far too long&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I could route the board by hand. This would be painful and take months, but I would get a good-looking board at the end.&lt;/item&gt;
      &lt;item&gt;I could YOLO everything and just let the FreeRouting autorouter handle it. It would take weeks, because the first traces are easy, the last traces take the longest. This would result in an ugly board.&lt;/item&gt;
      &lt;item&gt;I could spend a month or two building my own autorouter plugin for KiCad. I have a fairly powerful GPU and I thought routing a PCB is a very parallel problem. I could also implement my own routing algorithms to make the finished product look good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When confronted with a task that will take months, always choose the more interesting path.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New KiCad API, and a Traditional Autorouter&lt;/head&gt;
    &lt;p&gt;KiCad, Pre-version 9.0, had a SWIG-based plugin system. There are serious deficits with this system compared to the new IPC plugin system released with KiCad 9. The SWIG-based system was locked to the Python environment bundled with KiCad. Process isolation, threading, and performance constraints were a problem. Doing GPU programming with CuPy or PyTorch, while not impossible, is difficult.&lt;/p&gt;
    &lt;p&gt;The new IPC plugin system for KiCad is a godsend. The basic structure of the OrthoRoute plugin looks something like this:&lt;/p&gt;
    &lt;p&gt;The OrthoRoute plugin communicates with KiCad via the IPC API over a UNIX-ey socket. This API is basically a bunch of C++ classes that gives me access to board data  nets, pads, copper pour geometry, airwires, and everything else. This allows me to build a second model of a PCB inside a Python script and model it however I want. With a second model of a board inside my plugin, all I have to do is draw the rest of the owl.&lt;/p&gt;
    &lt;head rend="h2"&gt;Development of the Manhattan Routing Engine&lt;/head&gt;
    &lt;p&gt;After wrapping my head around the the ability to read and write board information to and from KiCad, I had to figure out a way to route this stupidly complex backplane. A non-orthogonal autorouter is a good starting point, but I simply used that as an exercise to wrap my head around the KiCad IPC API. The real build is a Manhattan Orthogonal Routing Engine, the tool needed to route my mess of a backplane.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project PathFinder&lt;/head&gt;
    &lt;p&gt;The algorithm used for this autorouter is PathFinder: a negotiation-based performance-driven router for FPGAs. My implementation of PathFinder treats the PCB as a graph: nodes are intersections on an xy grid where vias can go, and edges are the segments between intersections where copper traces can run. Each edge and node is treated as a shared resource.&lt;/p&gt;
    &lt;p&gt;PathFinder is iterative. In the first iteration, all nets (airwires) are routed greedily, without accounting for overuse of nodes or edges. Subsequent iterations account for congestion, increasing the cost of overused edges and ripping up the worst offenders to re-route them. Over time, the algorithm converges to a PCB layout where no edge or node is over-subscribed by multiple nets.&lt;/p&gt;
    &lt;p&gt;With this architecture  the PathFinder algorithm on a very large graph, within the same order of magnitude of the largest FPGAs  it makes sense to run the algorithm with GPU acceleration. There are a few factors that went into this decision:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Everyone whos routing giant backplanes probably has a gaming PC. Or you can rent a GPU from whatever company is advertising on MUNI bus stops this month.&lt;/item&gt;
      &lt;item&gt;The PathFinder algorithm requires hundreds of billions of calculations for every iteration, making single-core CPU computation glacially slow.&lt;/item&gt;
      &lt;item&gt;With CUDA, I can implement a SSSP (parallel Dijkstra) to find a path through a weighted graph very fast.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Adapting FPGA Algorithms to PCBs&lt;/head&gt;
    &lt;p&gt;The original PathFinder paper was, A Negotiation-Based Performance-Driven Router for FPGAs and from 1995, this meant early FPGAs like the Xilinx 3000 series and others manufactured by Tryptych. These devices were simple, and to get a good idea of how they worked, check out Ken Shirriffs blog. Heres what the inside of a Xilinx XC2064 looks like:&lt;/p&gt;
    &lt;p&gt;That looks complicated, but its really exceptionally simple. All the LUTs, or logic elements, are connected to each other with wires. Where the wires cross over, there are fuzes. Burn the fuzes and youve connected the wires together. Its a simple graph and all the complexity of the actual paths inside the chip are abstracted away. For a circuit board, I dont have this luxury. I have to figure out how to get the signal from the pads on the top layer of the PCB and drill down with vias into the grid. I need to come up with some way to account for both the edges of the graph and nodes of the graph, something thats untread territory with the PathFinder algorithm.&lt;/p&gt;
    &lt;p&gt;The first step of that is the pad escape planner that pre-computes the escape routing of all the pads. Because the entire Manhattan Routing Engine is designed for a backplane, we can make some assumptions: All of the components are going to be SMD, because THT parts would kill the efficiency of a routing lattice. The components are going to be arranged on a grid, and just to be nice Id like some randomization in where it puts the vias punching down into the grid. Heres what the escape planning looks like:&lt;/p&gt;
    &lt;head rend="h3"&gt;How PathFinder Almost Killed Me, and How I made PathFinder not suck&lt;/head&gt;
    &lt;p&gt;I found every bug imaginable while developing OrthoRoute. For one, congestion of nets would grow each iterations. The router would start fine with 9,495 edges with congestion in iteration 1. Then iteration 2: 18,636 edges. Iteration 3: 36,998 edges. The overuse was growing by 3 per iteration instead of converging. Something was fundamentally broken. The culprit? History costs were decaying instead of accumulating. The algorithm needs to remember which edges were problematic in past iterations, but my implementation had &lt;code&gt;history_decay=0.995&lt;/code&gt;, so it was forgetting 0.5% of the problem every iteration. By iteration 10, it had forgotten everything. No memory = no learning = explosion.&lt;/p&gt;
    &lt;p&gt;With the history fixed, I ran another test. I got oscillation. The algorithm would improve for 12 iterations (9,495  5,527, a 42% improvement!), then spike back to 11,817, then drop to 7,252, then spike to 14,000. The pattern repeated forever. The problem was adaptive hotset sizingwhen progress slowed, the algorithm would enlarge the set of nets being rerouted from 150 to 225, causing massive disruption. Fixing the hotset at 100 nets eliminated the oscillation.&lt;/p&gt;
    &lt;p&gt;Even with fixed hotsets, late-stage oscillation returned after iteration 15. Why? The present cost factor escalates exponentially: &lt;code&gt;pres_fac = 1.15^iteration&lt;/code&gt;. By iteration 19, present cost was 12.4 stronger than iteration 1, completely overwhelming history (which grows linearly). The solution: cap &lt;code&gt;pres_fac_max=8.0&lt;/code&gt; to keep history competitive throughout convergence.&lt;/p&gt;
    &lt;p&gt;PathFinder is designed for FPGAs, and each and every Xilinx XC3000 chip is the same as every other XC3000 chip. Configuring the parameters for an old Xilinx chip means every routing problem will probably converge on that particular chip. PCBs are different; every single PCB is different from every other PCB. There is no single set of history, pressure, and decay parameters that will work on every single PCB.&lt;/p&gt;
    &lt;p&gt;What I had to do was figure out these paramaters on the fly. So thats what I did. Right now Im using Board-adaptive parameters for the Manhattan router. Before beginning the PathFinder algorithm it analyzes the board in KiCad for the number of signal layers, how many nets will be routed, and how dense the set of nets are. Its clunky, but it kinda works.&lt;/p&gt;
    &lt;p&gt;Where PathFinder was tuned once for each family of FPGAs, Im auto-tuning it for the entire class of circuit boards. A huge backplane gets careful routing and an Arduino clone gets fast, aggressive routing. The hope is that both will converge  produce a valid routing solution  and maybe that works. Maybe it doesnt. Theres still more work to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Routing The Monster Board&lt;/head&gt;
    &lt;p&gt;After significant testing with small boards (actually 500+ net subsets of my large backplane, with 18 layers), I started work on the entire purpose of this project, the 8000+ net, 17000 pad monster board. There was one significant problem: it wouldnt fit on my GPU. Admittedly, I only have a 16GB Nvidia 5080, but even this was far too small for the big backplane.&lt;/p&gt;
    &lt;p&gt;This led me to develop a cloud routing solution. It boils down to extracting a OrthoRoute PCB file from the OrthoRoute plugin. From there, I rent a Linux box with a GPU and run the autorouting algorithm with a headless mode. This produces an OrthoRoute Solution file. I import this back into KiCad by running the OrthoRoute plugin on my local machine, and importing the solution file, then pushing that to KiCad.&lt;/p&gt;
    &lt;p&gt;Heres the result:&lt;/p&gt;
    &lt;p&gt;Thats it, thats the finished board. A few specs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;44,233 blind and buried vias. 68,975 track segments.&lt;/item&gt;
      &lt;item&gt;Routed on an 80GB A100 GPU, rented on vast.io. The total VRAM required to route this board was 33.5 GB, so close to being under 32GB and allowing me to rent a cheaper GPU&lt;/item&gt;
      &lt;item&gt;Total time to route this board to completion was 41 hours. This is far better than the months it would have taken FreeRouting to route this board, but its still not fast.&lt;/item&gt;
      &lt;item&gt;The routing result is good but not great. A big problem is the DRC-awareness of the escape pad planning. There are traces that dont quite overlap, but because of the geometry generated by the escape route planner they dont pass a strict DRC. This could be fixed in future versions. There are also some overlapping traces in what PathFinder generated. Not many, but a few.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While the output from my autorouter isnt perfect, no one would expect an autorouter to produce a perfect result, ready for production. Its an autorouter, something you shouldnt trust. Turning the result for OrthoRoute into a DRC-compliant board took a few days, but it was far easier than the intractable problem of eight thousand airwires I had at the beginning.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future of OrthoRoute&lt;/head&gt;
    &lt;p&gt;I built this for one reason: to route my pathologically large backplane. Mission accomplished. And along the way, I accidentally built something more useful than I expected.&lt;/p&gt;
    &lt;p&gt;OrthoRoute proves that GPU-accelerated routing isnt just theoretical, and that algorithms designed for routing FPGAs can be adapted to the more general class of circuit boards. Its fast, too. The Manhattan lattice approach handles high-density designs that make traditional autorouters choke. And the PathFinder implementation converges in minutes on boards that would take hours or days with CPU-based approaches.&lt;/p&gt;
    &lt;p&gt;More importantly, the architecture is modular. The hard partsKiCad IPC integration, GPU acceleration framework, DRC-aware routing space generation are done. Adding new routing strategies on top of this foundation is straightforward. Someone could implement different algorithms, optimize for specific board types, or extend it to handle flex PCBs.&lt;/p&gt;
    &lt;p&gt;The code is up on GitHub. Im genuinely curious what other people will do with it. Want to add different routing strategies? Optimize for RF boards? Extend it to flex PCBs? PRs welcome, contributors welcome.&lt;/p&gt;
    &lt;p&gt;And yes, you should still manually route critical signals. But for dense digital boards with hundreds of mundane power and data nets? Let the GPU handle it while you grab coffee. Thats what autorouters are for.&lt;/p&gt;
    &lt;p&gt;Never trust the autorouter. But at least this one is fast.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bbenchoff.github.io/pages/OrthoRoute.html"/><published>2025-11-18T18:54:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45970519</id><title>Trying out Gemini 3 Pro with audio transcription and a new pelican benchmark</title><updated>2025-11-18T23:32:53.785850+00:00</updated><content>&lt;doc fingerprint="7dd78fcf8f3f9a91"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Trying out Gemini 3 Pro with audio transcription and a new pelican benchmark&lt;/head&gt;
    &lt;p&gt;18th November 2025&lt;/p&gt;
    &lt;p&gt;Google released Gemini 3 Pro today. Heres the announcement from Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu, their developer blog announcement from Logan Kilpatrick, the Gemini 3 Pro Model Card, and their collection of 11 more articles. Its a big release!&lt;/p&gt;
    &lt;p&gt;I had a few days of preview access to this model via AI Studio. The best way to describe it is that its Gemini 2.5 upgraded to match the leading rival models.&lt;/p&gt;
    &lt;p&gt;Gemini 3 has the same underlying characteristics as Gemini 2.5. The knowledge cutoff is the same (January 2025). It accepts 1 million input tokens, can output up to 64,000 tokens, and has multimodal inputs across text, images, audio, and video.&lt;/p&gt;
    &lt;head rend="h4"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;Googles own reported numbers (in the model card) show it scoring slightly higher against Claude 4.5 Sonnet and GPT-5.1 against most of the standard benchmarks. As always Im waiting for independent confirmation, but I have no reason to believe those numbers are inaccurate.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pricing&lt;/head&gt;
    &lt;p&gt;It terms of pricing its a little more expensive than Gemini 2.5 but still cheaper than Claude Sonnet 4.5. Heres how it fits in with those other leading models:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Input (per 1M tokens)&lt;/cell&gt;
        &lt;cell role="head"&gt;Output (per 1M tokens)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPT-5.1&lt;/cell&gt;
        &lt;cell&gt;$1.25&lt;/cell&gt;
        &lt;cell&gt;$10.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemini 2.5 Pro&lt;/cell&gt;
        &lt;cell&gt;  200k tokens: $1.25&lt;p&gt;&amp;gt; 200k tokens: $2.50&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;  200k tokens: $10.00&lt;p&gt;&amp;gt; 200k tokens: $15.00&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemini 3 Pro&lt;/cell&gt;
        &lt;cell&gt;  200k tokens: $2.00&lt;p&gt;&amp;gt; 200k tokens: $4.00&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;  200k tokens: $12.00&lt;p&gt;&amp;gt; 200k tokens: $18.00&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude Sonnet 4.5&lt;/cell&gt;
        &lt;cell&gt;  200k tokens: $3.00&lt;p&gt;&amp;gt; 200k tokens: $6.00&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;  200k tokens: $15.00&lt;p&gt;&amp;gt; 200k tokens: $22.50&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Claude Opus 4.1&lt;/cell&gt;
        &lt;cell&gt;$15.00&lt;/cell&gt;
        &lt;cell&gt;$75.00&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Trying it out against a complex image&lt;/head&gt;
    &lt;p&gt;That screenshot of the benchmarks from above looked like a good test for Gemini 3s multimodal support. I fed it that image URL and asked it to generate alt text for the image:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview -a https://static.simonwillison.net/static/2025/gemini-3-benchmarks.jpg 'Alt text for this image, include all figures and make them comprehensible to a screen reader user'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Heres what I got back:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A comprehensive table comparing four AI models across various performance benchmarks. The columns listed are: Benchmark Name, Description, Gemini 3 Pro (highlighted in blue), Gemini 2.5 Pro, Claude Sonnet 4.5, and GPT-5.1.&lt;/p&gt;
      &lt;p&gt;Humanitys Last Exam (Academic reasoning)&lt;/p&gt;
      &lt;item&gt;No tools: Gemini 3 Pro 37.5%, Gemini 2.5 Pro 21.6%, Claude Sonnet 4.5 13.7%, GPT-5.1 26.5%.&lt;/item&gt;
      &lt;item&gt;With search and code execution: Gemini 3 Pro 45.8% (others have no data).&lt;/item&gt;
      &lt;p&gt;ARC-AGI-2 (Visual reasoning puzzles; ARC Prize Verified)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 31.1%, Gemini 2.5 Pro 4.9%, Claude Sonnet 4.5 13.6%, GPT-5.1 17.6%.&lt;/item&gt;
      &lt;p&gt;GPQA Diamond (Scientific knowledge; No tools)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 91.9%, Gemini 2.5 Pro 86.4%, Claude Sonnet 4.5 83.4%, GPT-5.1 88.1%.&lt;/item&gt;
      &lt;p&gt;AIME 2025 (Mathematics)&lt;/p&gt;
      &lt;item&gt;No tools: Gemini 3 Pro 95.0%, Gemini 2.5 Pro 88.0%, Claude Sonnet 4.5 87.0%, GPT-5.1 94.0%.&lt;/item&gt;
      &lt;item&gt;With code execution: Gemini 3 Pro 100%, Claude Sonnet 4.5 100%.&lt;/item&gt;
      &lt;p&gt;MathArena Apex (Challenging Math Contest problems)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 23.4%, Gemini 2.5 Pro 0.5%, Claude Sonnet 4.5 1.6%, GPT-5.1 1.0%.&lt;/item&gt;
      &lt;p&gt;MMMU-Pro (Multimodal understanding and reasoning)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 81.0%, Gemini 2.5 Pro 68.0%, Claude Sonnet 4.5 68.0%, GPT-5.1 76.0%.&lt;/item&gt;
      &lt;p&gt;ScreenSpot-Pro (Screen understanding)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 72.7%, Gemini 2.5 Pro 11.4%, Claude Sonnet 4.5 36.2%, GPT-5.1 3.5%.&lt;/item&gt;
      &lt;p&gt;CharXiv Reasoning (Information synthesis from complex charts)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 81.4%, Gemini 2.5 Pro 69.6%, Claude Sonnet 4.5 68.5%, GPT-5.1 69.5%.&lt;/item&gt;
      &lt;p&gt;OmniDocBench 1.5 (OCR; Overall Edit Distance, lower is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 0.115, Gemini 2.5 Pro 0.145, Claude Sonnet 4.5 0.145, GPT-5.1 0.147.&lt;/item&gt;
      &lt;p&gt;Video-MMMU (Knowledge acquisition from videos)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 87.6%, Gemini 2.5 Pro 83.6%, Claude Sonnet 4.5 77.8%, GPT-5.1 80.4%.&lt;/item&gt;
      &lt;p&gt;LiveCodeBench Pro (Competitive coding problems; Elo Rating, higher is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 2,439; Gemini 2.5 Pro 1,775; Claude Sonnet 4.5 1,418; GPT-5.1 2,243.&lt;/item&gt;
      &lt;p&gt;Terminal-Bench 2.0 (Agentic terminal coding; Terminus-2 agent)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 54.2%, Gemini 2.5 Pro 32.6%, Claude Sonnet 4.5 42.8%, GPT-5.1 47.6%.&lt;/item&gt;
      &lt;p&gt;SWE-Bench Verified (Agentic coding; Single attempt)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 76.2%, Gemini 2.5 Pro 59.6%, Claude Sonnet 4.5 77.2%, GPT-5.1 76.3%.&lt;/item&gt;
      &lt;p&gt;t2-bench (Agentic tool use)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 85.4%, Gemini 2.5 Pro 54.9%, Claude Sonnet 4.5 84.7%, GPT-5.1 80.2%.&lt;/item&gt;
      &lt;p&gt;Vending-Bench 2 (Long-horizon agentic tasks; Net worth (mean), higher is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro $5,478.16; Gemini 2.5 Pro $573.64; Claude Sonnet 4.5 $3,838.74; GPT-5.1 $1,473.43.&lt;/item&gt;
      &lt;p&gt;FACTS Benchmark Suite (Held out internal grounding, parametric, MM, and search retrieval benchmarks)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 70.5%, Gemini 2.5 Pro 63.4%, Claude Sonnet 4.5 50.4%, GPT-5.1 50.8%.&lt;/item&gt;
      &lt;p&gt;SimpleQA Verified (Parametric knowledge)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 72.1%, Gemini 2.5 Pro 54.5%, Claude Sonnet 4.5 29.3%, GPT-5.1 34.9%.&lt;/item&gt;
      &lt;p&gt;MMMLU (Multilingual Q&amp;amp;A)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 91.8%, Gemini 2.5 Pro 89.5%, Claude Sonnet 4.5 89.1%, GPT-5.1 91.0%.&lt;/item&gt;
      &lt;p&gt;Global PIQA (Commonsense reasoning across 100 Languages and Cultures)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 93.4%, Gemini 2.5 Pro 91.5%, Claude Sonnet 4.5 90.1%, GPT-5.1 90.9%.&lt;/item&gt;
      &lt;p&gt;MRCR v2 (8-needle) (Long context performance)&lt;/p&gt;
      &lt;item&gt;128k (average): Gemini 3 Pro 77.0%, Gemini 2.5 Pro 58.0%, Claude Sonnet 4.5 47.1%, GPT-5.1 61.6%.&lt;/item&gt;
      &lt;item&gt;1M (pointwise): Gemini 3 Pro 26.3%, Gemini 2.5 Pro 16.4%, Claude Sonnet 4.5 (not supported), GPT-5.1 (not supported).&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have not checked every line of this but a loose spot-check looks accurate to me.&lt;/p&gt;
    &lt;p&gt;That prompt took 1,105 input and 3,901 output tokens, at a cost of 5.6824 cents.&lt;/p&gt;
    &lt;p&gt;I ran this follow-up prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -c 'Convert to JSON'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can see the full output here, which starts like this:&lt;/p&gt;
    &lt;code&gt;{
  "metadata": {
    "columns": [
      "Benchmark",
      "Description",
      "Gemini 3 Pro",
      "Gemini 2.5 Pro",
      "Claude Sonnet 4.5",
      "GPT-5.1"
    ]
  },
  "benchmarks": [
    {
      "name": "Humanity's Last Exam",
      "description": "Academic reasoning",
      "sub_results": [
        {
          "condition": "No tools",
          "gemini_3_pro": "37.5%",
          "gemini_2_5_pro": "21.6%",
          "claude_sonnet_4_5": "13.7%",
          "gpt_5_1": "26.5%"
        },
        {
          "condition": "With search and code execution",
          "gemini_3_pro": "45.8%",
          "gemini_2_5_pro": null,
          "claude_sonnet_4_5": null,
          "gpt_5_1": null
        }
      ]
    },&lt;/code&gt;
    &lt;head rend="h4"&gt;Analyzing a city council meeting&lt;/head&gt;
    &lt;p&gt;To try it out against an audio file I extracted the 3h33m of audio from the video Half Moon Bay City Council MeetingNovember 4, 2025. I used &lt;code&gt;yt-dlp&lt;/code&gt; to get that audio:&lt;/p&gt;
    &lt;code&gt;yt-dlp -x --audio-format m4a 'https://www.youtube.com/watch?v=qgJ7x7R6gy0'&lt;/code&gt;
    &lt;p&gt;That gave me a 74M m4a file, which I ran through Gemini 3 Pro like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview -a /tmp/HMBCC\ 11425\ -\ Half\ Moon\ Bay\ City\ Council\ Meeting\ -\ November\ 4,\ 2025\ \[qgJ7x7R6gy0\].m4a 'Output a Markdown transcript of this meeting. Include speaker names and timestamps. Start with an outline of the key meeting sections, each with a title and summary and timestamp and list of participating names. Note in bold if anyone raised their voices, interrupted each other or had disagreements. Then follow with the full transcript.'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;That failed with an Internal error encountered message, so I shrunk the file down to a more manageable 38MB using &lt;code&gt;ffmpeg&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;ffmpeg -i "/private/tmp/HMB.m4a" -ac 1 -ar 22050 -c:a aac -b:a 24k "/private/tmp/HMB_compressed.m4a"&lt;/code&gt;
    &lt;p&gt;Then ran it again like this (for some reason I had to use &lt;code&gt;--attachment-type&lt;/code&gt; this time):&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview --attachment-type /tmp/HMB_compressed.m4a 'audio/aac' 'Output a Markdown transcript of this meeting. Include speaker names and timestamps. Start with an outline of the key meeting sections, each with a title and summary and timestamp and list of participating names. Note in bold if anyone raised their voices, interrupted each other or had disagreements. Then follow with the full transcript.'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;This time it worked! The full output is here, but it starts like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Here is the transcript of the Half Moon Bay City Council meeting.&lt;/p&gt;
      &lt;head&gt;Meeting Outline&lt;/head&gt;
      &lt;p&gt;1. Call to Order, Updates, and Public Forum&lt;/p&gt;
      &lt;item&gt;Summary: Mayor Brownstone calls the meeting to order. City Manager Chidester reports no reportable actions from the closed session. Announcements are made regarding food insecurity volunteers and the Diwali celebration. During the public forum, Councilmember Penrose (speaking as a citizen) warns against autocracy. Citizens speak regarding lease agreements, downtown maintenance, local music events, and homelessness outreach statistics.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:00:0000:13:25&lt;/item&gt;
      &lt;item&gt;Participants: Mayor Brownstone, Matthew Chidester, Irma Acosta, Deborah Penrose, Jennifer Moore, Sandy Vella, Joaquin Jimenez, Anita Rees.&lt;/item&gt;
      &lt;p&gt;2. Consent Calendar&lt;/p&gt;
      &lt;item&gt;Summary: The Council approves minutes from previous meetings and a resolution authorizing a licensing agreement for Seahorse Ranch. Councilmember Johnson corrects a pull request regarding abstentions on minutes.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:13:2500:15:15&lt;/item&gt;
      &lt;item&gt;Participants: Mayor Brownstone, Councilmember Johnson, Councilmember Penrose, Vice Mayor Ruddick, Councilmember Nagengast.&lt;/item&gt;
      &lt;p&gt;3. Ordinance Introduction: Commercial Vitality (Item 9A)&lt;/p&gt;
      &lt;item&gt;Summary: Staff presents a new ordinance to address neglected and empty commercial storefronts, establishing maintenance and display standards. Councilmembers discuss enforcement mechanisms, window cleanliness standards, and the need for objective guidance documents to avoid subjective enforcement.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:15:1500:30:45&lt;/item&gt;
      &lt;item&gt;Participants: Karen Decker, Councilmember Johnson, Councilmember Nagengast, Vice Mayor Ruddick, Councilmember Penrose.&lt;/item&gt;
      &lt;p&gt;4. Ordinance Introduction: Building Standards &amp;amp; Electrification (Item 9B)&lt;/p&gt;
      &lt;item&gt;Summary: Staff introduces updates to the 2025 Building Code. A major change involves repealing the citys all-electric building requirement due to the 9th Circuit Court ruling (California Restaurant Association v. City of Berkeley). Public speaker Mike Ferreira expresses strong frustration and disagreement with unelected state agencies forcing the City to change its ordinances.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:30:4500:45:00&lt;/item&gt;
      &lt;item&gt;Participants: Ben Corrales, Keith Weiner, Joaquin Jimenez, Jeremy Levine, Mike Ferreira, Councilmember Penrose, Vice Mayor Ruddick.&lt;/item&gt;
      &lt;p&gt;5. Housing Element Update &amp;amp; Adoption (Item 9C)&lt;/p&gt;
      &lt;item&gt;Summary: Staff presents the 5th draft of the Housing Element, noting State HCD requirements to modify ADU allocations and place a measure on the ballot regarding the Measure D growth cap. There is significant disagreement from Councilmembers Ruddick and Penrose regarding the States requirement to hold a ballot measure. Public speakers debate the enforceability of Measure D. Mike Ferreira interrupts the vibe to voice strong distaste for HCDs interference in local law. The Council votes to adopt the element but strikes the language committing to a ballot measure.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:45:0001:05:00&lt;/item&gt;
      &lt;item&gt;Participants: Leslie (Staff), Joaquin Jimenez, Jeremy Levine, Mike Ferreira, Councilmember Penrose, Vice Mayor Ruddick, Councilmember Johnson.&lt;/item&gt;
      &lt;head&gt;Transcript&lt;/head&gt;
      &lt;p&gt;Mayor Brownstone [00:00:00] Good evening everybody and welcome to the November 4th Half Moon Bay City Council meeting. As a reminder, we have Spanish interpretation services available in person and on Zoom.&lt;/p&gt;
      &lt;p&gt;Victor Hernandez (Interpreter) [00:00:35] Thank you, Mr. Mayor, City Council, all city staff, members of the public. [Spanish instructions provided regarding accessing the interpretation channel on Zoom and in the room.] Thank you very much.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Those first two lines of the transcript already illustrate something interesting here: Gemini 3 Pro chose NOT to include the exact text of the Spanish instructions, instead summarizing them as [Spanish instructions provided regarding accessing the interpretation channel on Zoom and in the room.].&lt;/p&gt;
    &lt;p&gt;I havent spot-checked the entire 3hr33m meeting, but Ive confirmed that the timestamps do not line up. The transcript closes like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Mayor Brownstone [01:04:00] Meeting adjourned. Have a good evening.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That actually happens at 3h31m5s and the mayor says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Okay. Well, thanks everybody, members of the public for participating. Thank you for staff. Thank you to fellow council members. This meeting is now adjourned. Have a good evening.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Im disappointed about the timestamps, since mismatches there make it much harder to jump to the right point and confirm that the summarized transcript is an accurate representation of what was said.&lt;/p&gt;
    &lt;p&gt;This took 320,087 input tokens and 7,870 output tokens, for a total cost of $1.42.&lt;/p&gt;
    &lt;head rend="h4"&gt;And a new pelican benchmark&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro has a new concept of a thinking level which can be set to low or high (and defaults to high). I tried my classic Generate an SVG of a pelican riding a bicycle prompt at both levels.&lt;/p&gt;
    &lt;p&gt;Heres lowGemini decided to add a jaunty little hat (with a comment in the SVG that says &lt;code&gt;&amp;lt;!-- Hat (Optional Fun Detail) --&amp;gt;&lt;/code&gt;):&lt;/p&gt;
    &lt;p&gt;And heres high. This is genuinely an excellent pelican, and the bicycle frame is at least the correct shape:&lt;/p&gt;
    &lt;p&gt;Honestly though, my pelican benchmark is beginning to feel a little bit too basic. I decided to upgrade it. Heres v2 of the benchmark, which I plan to use going forward:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;Generate an SVG of a California brown pelican riding a bicycle. The bicycle must have spokes and a correctly shaped bicycle frame. The pelican must have its characteristic large pouch, and there should be a clear indication of feathers. The pelican must be clearly pedaling the bicycle. The image should show the full breeding plumage of the California brown pelican.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;For reference, heres a photo I took of a California brown pelican recently (sadly without a bicycle):&lt;/p&gt;
    &lt;p&gt;Heres Gemini 3 Pros attempt at high thinking level for that new prompt:&lt;/p&gt;
    &lt;p&gt;And for good measure, heres that same prompt against GPT-5.1which produced this dumpy little fellow:&lt;/p&gt;
    &lt;p&gt;And Claude Sonnet 4.5, which didnt do quite as well:&lt;/p&gt;
    &lt;p&gt;None of the models seem to have caught on to the crucial detail that the California brown pelican is not, in fact, brown.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What happens if AI labs train for pelicans riding bicycles? - 13th November 2025&lt;/item&gt;
      &lt;item&gt;Reverse engineering Codex CLI to get GPT-5-Codex-Mini to draw me a pelican - 9th November 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2025/Nov/18/gemini-3/"/><published>2025-11-18T19:05:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45971726</id><title>GitHub: Git operation failures</title><updated>2025-11-18T23:32:53.379604+00:00</updated><content>&lt;doc fingerprint="7096aa564e934ff0"&gt;
  &lt;main&gt;
    &lt;p&gt;This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:59 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Git Operations is operating normally.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:56 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing full recovery after rolling out the fix and all services are operational.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:55 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Codespaces is operating normally.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:55 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have shipped a fix and are seeing recovery in some areas and will continue to provide updates.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:36 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have identified the likely cause of the incident and are working on a fix. We will provide another update as we get closer to deploying the fix.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:27 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Codespaces is experiencing degraded availability. We are continuing to investigate.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:25 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are currently investigating failures on all Git operations, including both SSH and HTTP.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:11 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing failures for some git http operations and are investigating&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:52 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Git Operations is experiencing degraded availability. We are continuing to investigate.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:39 UTC&lt;/p&gt;
    &lt;p&gt;Investigating&lt;/p&gt;
    &lt;p&gt;We are currently investigating this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:39 UTC&lt;/p&gt;
    &lt;p&gt;This incident affected: Git Operations and Codespaces.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.githubstatus.com/incidents/5q7nmlxz30sk"/><published>2025-11-18T20:40:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45972390</id><title>Bild AI (YC W25) is hiring Make housing affordable</title><updated>2025-11-18T23:32:52.907914+00:00</updated><content>&lt;doc fingerprint="19b82e91a7888e36"&gt;
  &lt;main&gt;
    &lt;p&gt;AI that understands construction blueprints&lt;/p&gt;
    &lt;p&gt;Puneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and were impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.&lt;/p&gt;
    &lt;p&gt;Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai"/><published>2025-11-18T21:29:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45972519</id><title>Blender 5.0</title><updated>2025-11-18T23:32:52.661878+00:00</updated><content/><link href="https://www.blender.org/download/releases/5-0/"/><published>2025-11-18T21:39:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45973261</id><title>Monotype font licencing shake-down</title><updated>2025-11-18T23:32:52.255389+00:00</updated><content>&lt;doc fingerprint="2c5093ddfa2347f0"&gt;
  &lt;main&gt;
    &lt;p&gt;tl;dr Dont try to shake-down a typography nerd with your dubious, automated claims about his employer using unlicensed fonts.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it started&lt;/head&gt;
    &lt;p&gt;It started with a LinkedIn InMail message (sanitised to protect privacy):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Subject: [Urgent] Font Software Licensing Review&lt;/p&gt;
      &lt;p&gt;Hi Ameel&lt;/p&gt;
      &lt;p&gt;I hope youre doing well.&lt;/p&gt;
      &lt;p&gt;Im [NAME] from Monotype and have been trying to reach you at [WORK EMAIL ADDRESS], but Im unsure if my emails have been received.&lt;/p&gt;
      &lt;p&gt;Our team has identified Monotype font software embedded in the websites/apps of [YOUR COMPANY], but we couldnt locate the corresponding licenses in our database.&lt;/p&gt;
      &lt;p&gt;Would you be able to share the correct email address so I can provide more details and documentation? Alternatively, youre welcome to reach out to me directly at [SENDERS EMAIL ADDRESS]&lt;/p&gt;
      &lt;p&gt;I appreciate your time and look forward to resolving this with you at your earliest convenience.&lt;/p&gt;
      &lt;p&gt;Best regards,&lt;/p&gt;
      &lt;p&gt;[NAME]&lt;/p&gt;
      &lt;p&gt;Business Development Representative | Monotype, Australia&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was puzzled by this for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This person had my correct work email address, but at no point had they sent me an email about this issue or about anything else. I checked. (Starting with a lie? Not a good look.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As far as I knew, my employer didnt use any Monotype fonts on its websites and apps.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Always good to check&lt;/head&gt;
    &lt;p&gt;Just in case there was any merit to this claim, I did a quick review and found that I was right. In Australia we have one corporate website, one retail website, and one retail app (with iOS and Android versions) and all of those use our official font which, for better or for worse, is Open Sans.&lt;/p&gt;
    &lt;p&gt;Now, not only is Open Sans not a Monotype typeface, it is available under the SIL Open Font License (OFL) meaning we can use this typeface for literally anything except selling the font itself. So that couldnt be what this person was talking about.&lt;/p&gt;
    &lt;p&gt;Next I checked our international retail websites and our project and partnership websites, since all of those have their own branding.&lt;/p&gt;
    &lt;p&gt;I found that on the websites we directly manage, we use these typefaces:&lt;/p&gt;
    &lt;p&gt;So no issue there.&lt;/p&gt;
    &lt;p&gt;And on the websites our partners manage, we use these typefaces:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Public Sans, available under the SIL OFL,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network Sans, a custom font created for the government agency that built the website this is used on so they wouldnt need a licence, and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Proxima Nova, the only font that does require a licence, except that Monotype doesnt sell a licence to it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So no issue there either.&lt;/p&gt;
    &lt;p&gt;With those initial checks done, I reached out to our digital team (who build and manage our websites and apps) with a screenshot of the LinkedIn message Id received and a summary of my investigation. I asked them how they wanted me to reply.&lt;/p&gt;
    &lt;p&gt;I needed to check with these folks first because my team and I only look after the corporate website. My employers overall digital presence  including the back-end of the corporate website  is managed by the digital team.&lt;/p&gt;
    &lt;p&gt;Turns out a couple of people from the digital and design teams had received identical messages from this Monotype Business Development Representative.&lt;/p&gt;
    &lt;p&gt;A few internal back-and-forth emails later we decided that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Instead of all of us responding, only one nominated person from the digital team would respond.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;But before responding, the digital team would do their own investigation into the fonts we use and the licences we own so we could verify everything was in compliance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course thats not how things actually went down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fishing (phishing?) around&lt;/head&gt;
    &lt;p&gt;What the Monotype rep did next is kind of what a malicious hacker does when theyre trying to get someone from your company to click on a link thatll install malware on your computer. Over the next couple of weeks, the rep messaged a dozen or so more people from different parts of the business, hoping to hook just one person who would reply to the scary message they were sending.&lt;/p&gt;
    &lt;p&gt;Now Id already emailed my design, brand, and digital team colleagues to tell them about this mass-messaging campaign and our plan of action for it, but the Monotype rep expanded their campaign to include people from our procurement team, who I hadnt thought to forewarn.&lt;/p&gt;
    &lt;p&gt;So not long after, I received a message from one of my procurement team colleagues whod been forwarded that LinkedIn message from their senior manager with an instruction to deal with this. I explained to my colleague that, as far as I could tell, this Monotype campaign was similar to the domain name scams the procurement team is already familiar with. So please sit tight till our digital team colleagues have completed their audit and then well figure out which one person should start the conversation with Monotype.&lt;/p&gt;
    &lt;p&gt;But, like any successful phishing campaign, the Monotype reps LinkedIn messages eventually reached someone who did respond. This was another person in the procurement team and, just to be completely clear, I dont blame them for responding. They were just doing their job of protecting our business from potential copyright liability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Being forced to deal with the issue&lt;/head&gt;
    &lt;p&gt;Since Id handed this over to the digital team, I hadnt kept track of how things were progressing. I was brought back into the discussion when our brand manager included me in an email thread between her and the procurement person whod responded to Monotype.&lt;/p&gt;
    &lt;p&gt;I quickly brought this second procurement person up to speed with our earlier plan of action and then I looped in the digital team again. Turns out the digital team had completed their audit, found that we were in compliance, but had gotten busy with other work so no one had responded to Monotype. *sigh*&lt;/p&gt;
    &lt;p&gt;Now, of course, everyone was on the back foot because our new procurement rep had shared the PDF that Monotype had sent, listing all the places where we were using Monotype fonts without a proper licence.&lt;/p&gt;
    &lt;p&gt;And, to quote from the procurement reps email:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Supplier has confirmed two options:&lt;/p&gt;
      &lt;p&gt;Past Use License Agreement is used (PULA) to cover the usage for the period without a license if the software is to be removed.&lt;/p&gt;
      &lt;p&gt;Process the PULA with a Go Forward license agreement to allow compliant continued use of the fonts in use.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Reading that, I got this procurement rep to quickly convene a meeting with everyone involved, though it turned out the person from the digital team whod done the font audit had gone on annual leave.&lt;/p&gt;
    &lt;p&gt;The rest of the digital team didnt know anything about font licencing and this was the first time this procurement rep was dealing with font licencing as well. So, partly spurred on by their senior managers instruction to deal with this, the procurement rep was seriously considering paying the licencing fee that Monotype had asked for, just to make this whole headache go away.&lt;/p&gt;
    &lt;p&gt;This is where I jumped in and told everyone to hold up. I said I would take the lead on this internally and I would take over the discussion we were having with Monotype as well. The procurement rep, I think somewhat relieved to have this taken out of their hands, agreed.&lt;/p&gt;
    &lt;p&gt;Why did I insist on taking this over? Two reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Im a bit of a typography nerd so I know what Im talking about and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a quick look at the document Monotype had sent over with the list our alleged copyright infringements had told me that everything the Monotype rep was alleging was wrong.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Never send an AI to do a humans job&lt;/head&gt;
    &lt;p&gt;An April 2025 blog post from Studio Twofolds Jamie Walker titled Unlicensed Fonts: The Hidden Risk in your Branding opens with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lately, weve seen a noticeable uptick in copyright issues related to both images and fonts. Several clients have reached out after receiving emails about fonts [an] agency used on their site years ago.&lt;/p&gt;
      &lt;p&gt;Naturally, theyre a bit rattled  and with good reason. These kinds of copyright claims can come with hefty fees if youre found to be in breach.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Further down Jamie writes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Smaller businesses can get caught off guard  and with AI now scanning the web for even the tiniest copyright infringements, its more important than ever to stay protected.&lt;/p&gt;
      &lt;p&gt;Think theyre only picking on the small guys? Think again. Even giants have been caught out  proving no one is too big to face the music (or in this case, the typography).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Basically, the reason so many organisations are getting out-of-the-blue copyright claims these days is because major copyright holders have started using automated, AI-powered copyright infringement detection software, of which there is a lot out there. *sigh*&lt;/p&gt;
    &lt;p&gt;Monotype seems to have used one of these products too, because the two fonts its report claimed we were using without a valid licence are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Credit Cards in our iOS and Android apps and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Proxima Nova in one of our project websites.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lets take those one at a time, shall we?&lt;/p&gt;
    &lt;head rend="h3"&gt;Whats in a name?&lt;/head&gt;
    &lt;p&gt;The first thing I did as I read the report was look up the Credit Cards font on MyFonts.com, Monotypes online store front.&lt;/p&gt;
    &lt;p&gt;Credit Cards is a pictogram font that contains these icons:&lt;/p&gt;
    &lt;p&gt;When I saw that I thought to myself, Why would we want to use those icons in our apps?&lt;/p&gt;
    &lt;p&gt;Next I read Monotypes report in detail and saw this screenshot. This is from an analysis of our apps payload and is the apparent proof that Credit Cards is being used in our smartphone apps:&lt;/p&gt;
    &lt;p&gt;Having worked with a great many font files over the years, when I saw that filename I thought to myself, Are we sure thats actually the Credit Cards font that Monotype claims it is?&lt;/p&gt;
    &lt;p&gt;So I checked. I did a web search for fonts with credit card in their name and very quickly found one called Credit Card  singular  from K-Type. This is a regular text font (as opposed to an icon font) that looks like the raised text thats printed on credit cards.&lt;/p&gt;
    &lt;p&gt;Now that I was something I could see us using in our apps.&lt;/p&gt;
    &lt;p&gt;Happily, Credit Card is free for personal use so I downloaded it and looked at the zip file. And guess what the filename of the font is?&lt;/p&gt;
    &lt;p&gt;Yup, the filename is CREDC___.ttf  which is exactly the filename that was in the app payload analysis from Monotype.&lt;/p&gt;
    &lt;p&gt;Seeing this, I reached out to my design team contact who then got me in touch with the person who manages our smartphone apps. From them I found out that, sure enough, the only font over and above Open Sans that we use in our smartphone apps is Credit Card by K-Type.&lt;/p&gt;
    &lt;p&gt;SCORE: Ameel 1, Monotype 0&lt;/p&gt;
    &lt;head rend="h3"&gt;Proxima Nova, really?&lt;/head&gt;
    &lt;p&gt;Unlike the Credit Cards font, our use of Mark Simonsons Proxima Nova was never in contention. We clearly use it in one of our project websites. My employer didnt actually build that website  we bought this under-construction project from another entity  but this website is very much our responsibility now.&lt;/p&gt;
    &lt;p&gt;The problem for Monotype here was that it no longer sells licences to Proxima Nova. There was a time you could buy a licence to Proxima Nova from Fonts.com, which was Linotypes online marketplace. But Monotype bought Linotype and eventually killed off Fonts.com and, some time after that (I dont know when or why), the fonts designer stopped selling licences to Proxima Nova through MyFonts.com.&lt;/p&gt;
    &lt;p&gt;Of course learning this fact didnt mean that I was just going to stop my investigation. I reached out to a person, who reached out to a person, who reached out to the design agency that designed (and still maintains) our project website. The design agency contact did their own investigation and quickly confirmed that, yes, they do indeed have a licence to use Proxima Nova on this site  one that they had purchased from Adobe several years ago.&lt;/p&gt;
    &lt;p&gt;SCORE: Ameel 2, Monotype 0&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing the book on this whole affair&lt;/head&gt;
    &lt;p&gt;Armed with this knowledge, I got the procurement person to introduce me to the Monotype rep. The rep and one of their colleagues were very eager to talk, replying to this introductory email within fifteen minutes. They wanted to organise a meeting so they could finally get the font licencing agreement signed. Instead what they got was a long email from me in which I explained the situation in detail, complete with annotated screenshots like the one above :)&lt;/p&gt;
    &lt;p&gt;The Monotype rep chewed on this for a few days and then made one final attempt at getting money out of us. They agreed that they were currently unable to sell a licence to Proxima Nova, but it turns out Monotype is one of K-Types authorised resellers and [they] currently cannot see the license on our files for this use  meaning there wasnt a record of us purchasing a licence to Credit Card from Monotype. So could we please confirm if there is one that we for some reason are unable to see in our systems?.&lt;/p&gt;
    &lt;p&gt;*sigh*&lt;/p&gt;
    &lt;p&gt;I wrote back and told them the reason Monotype doesnt have a record of this licence is because we purchased a one-off Enterprise Licence directly from K-Type several years ago.&lt;/p&gt;
    &lt;p&gt;This was several weeks ago and I havent heard a peep from them since.&lt;/p&gt;
    &lt;p&gt;\_()_/&lt;/p&gt;
    &lt;head rend="h2"&gt;Two sides to the story&lt;/head&gt;
    &lt;p&gt;Despite my making light of the situation, I dont actually hate Monotype for doing this. Using fonts without purchasing a proper license (or purchasing the fonts outright) is stealing and you absolutely should not do it.&lt;/p&gt;
    &lt;p&gt;The fact that font licensing can be a complicated issue is not an excuse; lots of things are complicated and we figure them out.&lt;/p&gt;
    &lt;p&gt;More importantly, I think the type designers and type foundries that create fonts should be fairly compensated for their work. Paying for fonts, or an ongoing licence to those fonts, is how you do that.&lt;/p&gt;
    &lt;p&gt;In short, you should pay for fonts and you should call out people and organisations when they use fonts without a proper licence.&lt;/p&gt;
    &lt;p&gt;(For completeness sake, I should say that I also dont mind that Monotype used automated systems to find copyright violations. The internet is so large that its impossible to manually find all the people who have stolen your stuff!)&lt;/p&gt;
    &lt;head rend="h3"&gt;Its not what you do, its how you do it&lt;/head&gt;
    &lt;p&gt;That said, I hate how Monotypes business development people went about doing this. Much like the blatantly overzealous content blockers on YouTube, the Monotype reps who reached out to us didnt even bother to verify whether the report their AI spat out at them showed an actual copyright violation or not.&lt;/p&gt;
    &lt;p&gt;I mean, I know why they didnt double-check. Just like with scammers and phishers, this is a volume game, not an accuracy or fairness game. You bombard people with messages, scaring them with your (potentially unverified) claims, and eventually some of the thousands of people youve messaged will reply. You then rush these folks into paying a licence fee because your targets dont have enough information about font licencing and, frankly, they just want the problem to go away. This is a shitty way to do business and it reflects poorly on your organisation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Youre not making any friends&lt;/head&gt;
    &lt;p&gt;Not that reputation seems to matter too much to popular digital marketplaces  Amazon being the poster child for this. Theyre big, theyre arguably enshittified, and all they appear to care about is making as much money as possible.&lt;/p&gt;
    &lt;p&gt;I mean theres a reason why so many type designers urge people not to licence fonts  even their own fonts  from MyFonts and instead buy or licence fonts directly from designer and type foundry websites. And, barring that, buying or licencing fonts from smaller, independent stores like Fontspring instead.&lt;/p&gt;
    &lt;p&gt;This is also why TypeType and Fontstand offer font subscriptions that are alternatives to those from Adobe Fonts and Monotype.&lt;/p&gt;
    &lt;p&gt;And this is why, for example, the independent video streaming site Nebula was forced to design their custom Nebula Sans font. Nebulas website and streaming apps used to use the Screen Smart version of the Whitney font from Hoefler&amp;amp;Co. But when Monotype purchased Hoefler&amp;amp;Co, the new Monotype licencing/royalty structure meant that a licence to Whitney was suddenly unaffordable to Nebula. Since paying that (apparently much) higher amount to Monotype wasnt going to be financially sustainable, Nebula instead paid Paul D. Hunt, the original designer of the excellent Source Sans font, to modify his font so it would be a drop-in replacement for Whitney in all of Nebulas digital products. And because Source Sans was released under the SIL Open Font License, Nebula also released Nebula Sans under this OFL.&lt;/p&gt;
    &lt;p&gt;If thats how much effort folks are willing to make to to avoid using your company, you probably already know that youre not very well loved.&lt;/p&gt;
    &lt;head rend="h2"&gt;What do do?&lt;/head&gt;
    &lt;p&gt;So whats my take-away from all this?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Dont use scammy tactics to scare people into purchasing your shit. And if you are going to use those tactics, at least dont be wrong about it!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If anything like this happens to you or your employer, find the relevant nerd in your friend group or organisation and ask for their help.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you can, avoid licencing fonts from Monotype. Get your fonts directly from the original designers and type foundries, or maybe from smaller, independent marketplaces like Fontspring (which has its big annual cyber sale on till 4 December, by the way).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If youd rather avoid the hassle of font licencing altogether, then do what my employer did and pick an excellent, versatile OFL font and use that instead. Though, if you want to stand out from the crowd, please consider avoiding the most popular fonts on Google Fonts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh, and while Im far from an expert on typography, if you need a hand with anything font-related, please reach out. Id be happy to help in any way that I can :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.insanityworks.org/randomtangent/2025/11/14/monotype-font-licencing-shake-down"/><published>2025-11-18T22:42:45+00:00</published></entry></feed>