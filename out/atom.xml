<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-12T18:17:42.191020+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46586766</id><title>Anthropic made a mistake in cutting off third-party clients</title><updated>2026-01-12T18:17:48.967622+00:00</updated><content>&lt;doc fingerprint="9e019692683f8f49"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Anthropic made a big mistake&lt;/head&gt;
    &lt;p&gt;Anthropic may have just committed the biggest business blunder of 2026 -- and we're less than two weeks in. To understand why, let's briefly rewind to 2025, the year when agentic AI went mainstream.&lt;/p&gt;
    &lt;p&gt;On 3 February 2025, Andrej Karpathy coined the term "vibe coding" to describe the new paradigm.&lt;/p&gt;
    &lt;p&gt;Less than three weeks later, Anthropic released the first research preview of Claude Code, bringing large language models directly into developers' native habitat: the terminal.&lt;/p&gt;
    &lt;p&gt;OpenAI followed with Codex CLI in April, and Google released Gemini CLI in June.&lt;/p&gt;
    &lt;p&gt;All of these terminal-based coding agents follow the same principle:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;you type a prompt&lt;/item&gt;
      &lt;item&gt;the agent sends it to a large language model&lt;/item&gt;
      &lt;item&gt;the LLM responds and may instruct the agent to carry out actions like editing files or running commands&lt;/item&gt;
      &lt;item&gt;the agent carries out the actions and appends the results to the prompt&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These steps are repeated in a loop, but with a twist: the agent can continue working through the loop until the LLM decides that it requires user input.&lt;/p&gt;
    &lt;p&gt;The principle is so simple that it immediately gave rise to a bunch of alternative coding agents, including OpenCode, Roo, and Amp Code (to name but a few). Each brought its own unique philosophy and approach to the table, but what they all have in common is that they ultimately rely on large language models for intelligence. Their job is purely to collect user input, execute tool calls, and pass those to the model, over and over again. Therefore, they tend to provide a way to select from a predefined set of models and a means of authenticating with the relevant providers (such as Anthropic or OpenAI), generally using an API key.&lt;/p&gt;
    &lt;p&gt;When Claude Code launched for real in June 2025, usage of the Anthropic models was included in the Pro and Max plans, for a flat monthly or annual subscription. These plans quickly became very popular when users realised that the effective cost per token was much lower compared to Anthropic's API pricing. So popular, in fact, that it reached $1 billion in annualised revenue after only six months.&lt;/p&gt;
    &lt;p&gt;Meanwhile, OpenCode rapidly gained popularity, amassing over 50,000 GitHub stars and more than 650,000 monthly active users in the same short timeframe. One of its key selling points was the ability to "Log in with Anthropic to use your Claude Pro or Max account", thus enabling developers to benefit from the attractive Claude subscription pricing. In contrast, other coding agents such as Amp only provided the ability to connect to Claude models via the much more expensive pay-per-token API.&lt;/p&gt;
    &lt;p&gt;It turns out that logging into third-party coding agents with an Anthropic OAuth token was a bit of a loophole. This was evident from the fact that it would only work if the client-supplied system prompt contained a specific phrase identifying itself as Claude Code. Nevertheless, many (presumably) unsuspecting Anthropic customers used OpenCode in this way; from their perspective, they were simply using the same service that they were already paying for, just in the comfort of their preferred coding harness.&lt;/p&gt;
    &lt;p&gt;However, Anthropic clearly didn't see it this way. On 9 January 2026, Anthropic unceremoniously closed the loophole, changing their API to detect and reject requests from third-party clients. The renowned vibe-coder Peter Steinberger soon posted about it on the website formerly known as Twitter, and disgruntled Anthropic customers expressed their discontent in a GitHub issue, requesting the decision to be reversed, many threatening to cancel their Claude subscription otherwise.&lt;/p&gt;
    &lt;p&gt;It's notable that Anthropic has not formally announced this change in ToS enforcement, neither ahead of time nor after the fact. The only quasi-announcement of this change was this thread, posted by an Anthropic employee on their personal account the day after the changes took effect, presumably in response to customer complaints. The stated motivation for the change was the allegation that "third-party harnesses using Claude subscriptions create problems for users and generate unusual traffic patterns [...] making it really hard for us to help debug when they have questions about rate limit usage or account bans and they donâ€™t have any other avenue for this support."&lt;/p&gt;
    &lt;p&gt;I will leave it to the reader to decide for themselves whether they consider this a credible explanation or not; frankly, it doesn't matter. The truth is that Anthropic is free to put whatever they want into their ToS, and customers have to abide by it or leave. It appears quite a few have opted for the latter. However, what does matter is what Anthropic has implicitly revealed through its actions last Friday:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Anthropic is willing to go to war with their paying customers over a trivial ToS violation, and&lt;/item&gt;
      &lt;item&gt;they really, really want to own the entire value chain rather than being relegated to becoming just another "model provider", and&lt;/item&gt;
      &lt;item&gt;they utterly failed to consider the second-order effects of this business decision.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first point has received ample discussion already, so I want to focus on the second and third points.&lt;/p&gt;
    &lt;p&gt;It was reported just a few days earlier that Anthropic has signed a term sheet to raise $10bn at a humongous $350bn valuation. Related or not, the incentives are clear. Model-agnostic harnesses such as OpenCode present a real threat to Anthropic. Whilst its models are incredibly popular in the software developer community and it has made big inroads in enterprise LLM usage, the Claude chatbot itself reportedly commands a market share of... wait for it... 1.07%. So it's no surprise that they are trying to avoid being commoditized in their core market.&lt;/p&gt;
    &lt;p&gt;Which brings us to the final point: without anticipating it, Anthropic just found itself in a classic prisoner's dilemma with OpenAI -- and OpenAI just defected. Not only are they officially supporting OpenCode users to use their Codex subscriptions and usage limits in OpenCode, they are extending the same support to other open-source coding harnesses such as OpenHands, RooCode, and Pi. And it's not just a theoretical announcement either: support for connecting ChatGPT Pro/Plus subscriptions with OpenCode has already shipped.&lt;/p&gt;
    &lt;p&gt;What are we to take away from all this?&lt;/p&gt;
    &lt;p&gt;For me personally, I have decided I will never be an Anthropic customer, because I refuse to do business with a company that takes its customers for granted. Beyond my personal choices, though, I predict that the folks at Anthropic will come to regret their actions last week. By cracking down on their own customers in a vain attempt to quash healthy competition, they have destroyed a lot of goodwill and gave their main rival an opening that was ripe for the picking. Whilst they have plenty of cash in the bank for now, they will eventually need to learn to treat their customers with respect if they are to survive in the ever-more-competitive LLM provider landscape.&lt;/p&gt;
    &lt;p&gt;The views expressed here are my own. While the analysis is based on publicly available information, I welcome any factual corrections -- please feel free to reach out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://archaeologist.dev/artifacts/anthropic"/><published>2026-01-12T10:57:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46587498</id><title>Launch a Debugging Terminal into GitHub Actions</title><updated>2026-01-12T18:17:48.703132+00:00</updated><content>&lt;doc fingerprint="8df881c16f002085"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using WebRTC to launch a debugging terminal into GitHub Actions&lt;/head&gt;
    &lt;p&gt;Spoiler: I made a free and open-source way to get an interactive web terminal to your GitHub Action when it fails. Try it out here: https://actions-term.gripdev.xyz/ (code ğŸ”—) 1&lt;/p&gt;
    &lt;head rend="h2"&gt;Building it&lt;/head&gt;
    &lt;p&gt;I think weâ€™ve all been there, your build fails in Actions, but the script works fine locally. You now settle into a slow loop of:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Push speculative change&lt;/item&gt;
      &lt;item&gt;See if it worked&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It was in the middle of one of these when I started thinking about how to make it better.&lt;/p&gt;
    &lt;p&gt;A Terminal would be great, thatâ€™s obvious, but how to make it happen? How could I make it free, and open to anyone, without costing me lots of money?&lt;/p&gt;
    &lt;p&gt;Operating a service that forward traffic between a user and the Actions VM would stack up data transfer costs and take some work to scale.&lt;/p&gt;
    &lt;p&gt;What about a Peer-to-Peer connection? Iâ€™d recently been going deeper on how Tailscale, iroh and WebRTC use UDP Hole Punching to create Peer-to-Peer (P2P) connections between nodes without relaying traffic. 2&lt;/p&gt;
    &lt;p&gt;If that worked then my server would only need to exchange a tiny bit of information per session and hopefully cost me very little ğŸ¤&lt;/p&gt;
    &lt;p&gt;Could I use P2P and funnel a terminal session over it? Well the Actions VM is on the internet and allows UDP outbound, so it should work!&lt;/p&gt;
    &lt;p&gt;A simple bit of scripting proved it did ğŸ¥³ With WebRTC, if the Actions VM and my local machine exchange information about their connectivity (ICE Candidates), then I could form a connection.3&lt;/p&gt;
    &lt;head rend="h2"&gt;Security and Identities&lt;/head&gt;
    &lt;p&gt;The next problem is, how do you prove each end of the P2P connection is who they say they are?&lt;/p&gt;
    &lt;p&gt;Itâ€™s important. I want to ensure that &lt;code&gt;lawrencegripper&lt;/code&gt; can only access terminals for Actions triggered by &lt;code&gt;lawrencegripper&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The browser side is relatively easy, we can use OAuth to login via GitHub and get a verified username âœ…&lt;/p&gt;
    &lt;p&gt;On the Actions VM we have OIDC, commonly used to auth from Actions to cloud providers.&lt;/p&gt;
    &lt;p&gt;Anyone can use it though, it gives us the ability to issue a signed OIDC token from within our Action which proves:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The repo itâ€™s running on&lt;/item&gt;
      &lt;item&gt;The user account that triggered it&lt;/item&gt;
      &lt;item&gt;The audience it is intended for&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To enable this feature you set the following permissions in the workflow&lt;/p&gt;
    &lt;code&gt;    permissions:
      id-token: write&lt;/code&gt;
    &lt;p&gt;You request a token via a REST request in the action, for example:&lt;/p&gt;
    &lt;code&gt;    const requestURL = process.env.ACTIONS_ID_TOKEN_REQUEST_URL;
    const requestToken = process.env.ACTIONS_ID_TOKEN_REQUEST_TOKEN;
    const SERVER_URL = 'https://actions-term.gripdev.xyz';
    const url = new URL(requestURL);
    url.searchParams.set('audience', SERVER_URL);

    const resp = await fetch(url.toString(), {
        headers: {
        Authorization: `Bearer ${requestToken}`,
        Accept: 'application/json',
        },
    });&lt;/code&gt;
    &lt;p&gt;Then, when the Action calls our server, it can include this token. We can then validate it cryptographically via JWKS:&lt;/p&gt;
    &lt;code&gt;    const githubOIDCIssuer = "https://token.actions.githubusercontent.com"
    const githubJWKSURL = "https://token.actions.githubusercontent.com/.well-known/jwks"
    // Fetch JWKS
	keySet, err := jwkCache.Get(ctx, githubJWKSURL)
	if err != nil {
		return "", "", "", fmt.Errorf("failed to fetch JWKS: %w", err)
	}

	// Parse and validate token with clock skew tolerance
	parseOpts := []jwtx.ParseOption{
		jwtx.WithKeySet(keySet),
		jwtx.WithIssuer(githubOIDCIssuer),
		jwtx.WithValidate(true),
		jwtx.WithAcceptableSkew(2 * time.Minute),
		jwtx.WithAudience(oidcExpectedAudience),
	}
	token, err := jwtx.Parse([]byte(tokenStr), parseOpts...)
	if err != nil {
		return "", "", "", fmt.Errorf("token validation failed: %w", err)
	}&lt;/code&gt;
    &lt;head rend="h2"&gt;Connecting the Peers (ie. Signaling Server)&lt;/head&gt;
    &lt;p&gt;At this point we know:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We can create a connection between two peers (Actions VM &amp;lt;-&amp;gt; Users Browser) with WebRTC&lt;/item&gt;
      &lt;item&gt;We have a way to validate the identity of both ends of the connection (OAuth and OIDC)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whatâ€™s left is the server to introduce the two peers ğŸ¤ Letâ€™s build a server to do that.&lt;/p&gt;
    &lt;p&gt;The server doesnâ€™t need to handle the terminal data, that goes between the two peers directly, itâ€™s only doing introductions.&lt;/p&gt;
    &lt;p&gt;When the VM and Browser are connected to the server it should send each one the connection details for the other.&lt;/p&gt;
    &lt;p&gt;To do this, the browser and the VM both create Server-sent events (SSE) connections, allowing the signaling server to push events to them. They prove their identities by providing their OAuth credentials or OIDC to prove their identity.&lt;/p&gt;
    &lt;p&gt;The server then stores:&lt;/p&gt;
    &lt;code&gt;	runIdToSessions            = make(map[string]*Session) // runId -&amp;gt; session
	runIdToSessionsMu          sync.RWMutex
	runIdRunnerSseClient       = make(map[string]*SSEClient) // runId -&amp;gt; SSE client (Actions VM)
	runIdRunnerSseClientsMu    sync.RWMutex
	actorToBrowserSseClients   = make(map[string][]*SSEClient) // actor -&amp;gt; list of browser SSE clients
	actorToBrowserSseClientsMu sync.RWMutex&lt;/code&gt;
    &lt;p&gt;The server then, via SSE, sends the Actions VM connectivity details to the browser and the Browserâ€™s connectivity details to the Actions VM.&lt;/p&gt;
    &lt;p&gt;At this point they establish the Peer-to-Peer connection ğŸ¥³&lt;/p&gt;
    &lt;p&gt;For bonus points, when a new Actions VM connects, I can see if a browser is open waiting and send them a notification.&lt;/p&gt;
    &lt;code&gt;	runIdRunnerSseClientsMu.Lock()
	runIdRunnerSseClient[runId] = client
	log.Printf("SSE: Runner connected for actor %s (total clients: %d)", actor, len(runIdRunnerSseClient))
	runIdRunnerSseClientsMu.Unlock()

	// Notify browser subscribers about new session
	sess, ok := runIdToSessions[runId]
	if ok {
		notifyNewSession(sess)
	}&lt;/code&gt;
    &lt;head rend="h2"&gt;Displaying the Terminal&lt;/head&gt;
    &lt;p&gt;Ok, weâ€™re close now. We have the signaling server to exchange details and then the peers have a p2p connection.&lt;/p&gt;
    &lt;p&gt;What about creating a terminal and streaming the data?&lt;/p&gt;
    &lt;p&gt;WebRTC has a &lt;code&gt;datachannel&lt;/code&gt; which you push arbitrary data through.&lt;/p&gt;
    &lt;p&gt;On the Actions VM side we create a &lt;code&gt;pty.Shell&lt;/code&gt; and stream that data over our &lt;code&gt;datachannel (dc)&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;    shell = pty.spawn(SHELL, [], {
        name: 'xterm-256color',
        cwd: process.env.GITHUB_WORKSPACE || process.cwd(),
        env: process.env as Record&amp;lt;string, string&amp;gt;,
    });

    shell.onData((shellData) =&amp;gt; {
        dc.sendMessage(shellData);
    });&lt;/code&gt;
    &lt;p&gt;In the browser we then need to display an interactive terminal.&lt;/p&gt;
    &lt;p&gt;Reading around, I found the this awesome Ghostty library. It has an xterm.js compatible implementation, I hooked this up and it worked first time ğŸ¥°&lt;/p&gt;
    &lt;p&gt;Well it did.. and it didnâ€™t, the Terminal spawned via PTY doesnâ€™t have any idea how big our terminal in the browser (Lines and Columns) so we get some horrible rendering in the terminal.&lt;/p&gt;
    &lt;p&gt;With a bit of poking, googling and some Opus 4.5, I created a method which estimates the size of terminal, via font sizing, and converts this to a rough column / rows. Then, on establishing the P2P connection I can send a &lt;code&gt;setup&lt;/code&gt; JSON message which the Actions VM uses to start &lt;code&gt;pty.spawn&lt;/code&gt; with the right sizing for the terminal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Weâ€™re done, right?&lt;/head&gt;
    &lt;p&gt;Not quite, at this point we have ğŸ‘‡&lt;/p&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Runner  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Browser      â”‚
â”‚  (TypeScript)   â”‚      Direct P2P (WebRTC)     â”‚  (ghostty-web)  â”‚
â”‚                 â”‚                              â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                                â”‚
         â”‚ Register session                               â”‚ Get sessions
         â”‚ (OIDC Token Auth)                              â”‚ (GitHub OAuth)
         â–¼                                                â–¼
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚     Server      â”‚
                         â”‚   (Go - Auth    â”‚
                         â”‚   &amp;amp; Discovery)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&lt;/code&gt;
    &lt;p&gt;There is a lot of trust placed in the signaling server. It has to do the right thing, or it could provide access to someone elseâ€™s Actions VM.&lt;/p&gt;
    &lt;p&gt;Letâ€™s do better.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trust vs Zero-Trust&lt;/head&gt;
    &lt;p&gt;Iâ€™ve mentioned already the signaling server should only connect up users with actions theyâ€™ve started.&lt;/p&gt;
    &lt;p&gt;That relies on explicit trust, from users, that Iâ€™m going to do the right thing.&lt;/p&gt;
    &lt;p&gt;What if Iâ€™ve got a bug? What if someone compromises the signaling server? What if they steal the domain and run their own server on it?&lt;/p&gt;
    &lt;p&gt;Well, then they could hook up peers that shouldnâ€™t be connected and â€¦ do mean things.&lt;/p&gt;
    &lt;p&gt;That sounds bad, lets work out how to fix that.&lt;/p&gt;
    &lt;p&gt;What if the &lt;code&gt;user&lt;/code&gt; provided the Actions VM with a secret, that only they know?&lt;/p&gt;
    &lt;p&gt;When the P2P connection is made, the Actions VM could refuse to talk to the browser until it provides the right secret.&lt;/p&gt;
    &lt;p&gt;Secrets are cool and everything but if theyâ€™re intercepted theyâ€™re reusable, could we use a One-Time-Password (OTP) commonly used for 2FA on sites? Sure thing! Even better tools like 1Password will autofill it for you.&lt;/p&gt;
    &lt;p&gt;What does this flow look like? Roughly itâ€™s ğŸ‘‡&lt;/p&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Runner  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Browser      â”‚
â”‚  (TypeScript)   â”‚      Direct P2P (WebRTC)     â”‚  (ghostty-web)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                                â”‚
         â”‚â—„â”€â”€â”€â”€â”€ 1. WebRTC Connection Established â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
         â”‚                                                â”‚
         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. Browser sends OTP â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”‚
         â”‚                                                â”‚
         â”‚  3. Runner validates OTP against secret        â”‚
         â”‚                                                â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â”‚              â”‚  If OTP Valid:   â”‚              â”‚
         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  4. Terminal     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
                        â”‚     access       â”‚
                        â”‚     granted      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&lt;/code&gt;
    &lt;p&gt;Even if the signaling server is manipulated, to hook up two peers which shouldnâ€™t be connected, the user wonâ€™t be able to execute commands unless they can provide a valid OTP.&lt;/p&gt;
    &lt;p&gt;Better still the Signaling server (which I run) is never sent either the OTP or the OTP Secret used to validate each OTP.&lt;/p&gt;
    &lt;p&gt;This validation happens between two peers you own (Actions VM and your Browser).&lt;/p&gt;
    &lt;head rend="h2"&gt;So now weâ€™re done?&lt;/head&gt;
    &lt;p&gt;One last thingâ€¦ make the signaling server cheap to host.&lt;/p&gt;
    &lt;p&gt;I want to offer this for free for anyone to use. Itâ€™s a simple &lt;code&gt;go&lt;/code&gt; binary in a docker image (to make it easy to self-hosting and test locally) but it needs to live on the internet.&lt;/p&gt;
    &lt;p&gt;A while ago Iâ€™d started poking at railway.com, itâ€™s cloud with a big billing twist, you only pay for the CPU and Memory you actually use. This felt like a great time to try it out.&lt;/p&gt;
    &lt;p&gt;How is the billing different, wellâ€¦&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Azure/AWS I have to say â€œI want 2 CPUs and 8GBâ€ and I pay for that regardless of what I use.&lt;/item&gt;
      &lt;item&gt;On railway.com I say â€œUse up to x CPUs and y GBâ€ then you only pay for what the service actually consumes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: In either Azure/AWS or Railway you still pay network egress.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How does this work out for the signaling server?&lt;/p&gt;
    &lt;p&gt;Really well itâ€™s peak memory usage so far is 20MB, racking up a mean $0.00000â€¦ you get the point. Itâ€™s not costing much, as Iâ€™m not reserving a small machine with GBs of mem to run a service needing 20MB&lt;/p&gt;
    &lt;p&gt;Even then it feels a bit wasteful running it all the time. There will be chunks of time when folks arenâ€™t using it.&lt;/p&gt;
    &lt;p&gt;This was where I found a platform feature called sleeping, itâ€™s serverless but without the pain of moving away from the docker model to some proprietary runtime.&lt;/p&gt;
    &lt;p&gt;When the service isnâ€™t doing anything, Railway spin down the service. If someone turns up, they hold the connection for a moment while restoring the container, then send the request through.&lt;/p&gt;
    &lt;p&gt;What does a cold start look like on our simple signalling server? Itâ€™s hardly recognizable.&lt;/p&gt;
    &lt;p&gt;Here is a recording, on the left you see the server is sleeping and on the right I hit the domain, there is a slight pause before the page renders âœ¨&lt;/p&gt;
    &lt;head rend="h3"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This is a personal project with no support/guarantees. â†©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I couldnâ€™t quite get the iroh stack to play nice in-browser so fell back to WebRTC but do want to revisit that in future. â†©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I donâ€™t configure a TURN/relay server so if your network doesnâ€™t allow UDP hole punching you wonâ€™t be able to connect. â†©ï¸&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.gripdev.xyz/2026/01/10/actions-terminal-on-failure-for-debugging/"/><published>2026-01-12T12:25:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46587804</id><title>Zen-C: Write like a high-level language, run like C</title><updated>2026-01-12T18:17:47.969616+00:00</updated><content>&lt;doc fingerprint="fcf8839be8cb7a88"&gt;
  &lt;main&gt;
    &lt;p&gt;Zen C is a modern systems programming language that compiles to human-readable &lt;code&gt;GNU C&lt;/code&gt;/&lt;code&gt;C11&lt;/code&gt;. It provides a rich feature set including type inference, pattern matching, generics, traits, async/await, and manual memory management with RAII capabilities, all while maintaining 100% C ABI compatibility.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/z-libs/Zen-C.git
cd Zen-C
make
sudo make install&lt;/code&gt;
    &lt;code&gt;# Compile and run
zc run hello.zc

# Build executable
zc build hello.zc -o hello

# Interactive Shell
zc repl&lt;/code&gt;
    &lt;p&gt;Zen C uses type inference by default.&lt;/p&gt;
    &lt;code&gt;var x = 42;                 // Inferred as int
const PI = 3.14159;         // Compile-time constant
var explicit: float = 1.0;  // Explicit type
&lt;/code&gt;
    &lt;p&gt;By default, variables are mutable. You can enable Immutable by Default mode using a directive.&lt;/p&gt;
    &lt;code&gt;//&amp;gt; immutable-by-default

var x = 10;
// x = 20; // Error: x is immutable

var mut y = 10;
y = 20;    // OK
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;C Equivalent&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;int&lt;/code&gt;, &lt;code&gt;uint&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;int&lt;/code&gt;, &lt;code&gt;unsigned int&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Platform standard integer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;I8&lt;/code&gt; .. &lt;code&gt;I128&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;int8_t&lt;/code&gt; .. &lt;code&gt;__int128_t&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Signed fixed-width integers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;U8&lt;/code&gt; .. &lt;code&gt;U128&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;uint8_t&lt;/code&gt; .. &lt;code&gt;__uint128_t&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Unsigned fixed-width integers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;isize&lt;/code&gt;, &lt;code&gt;usize&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;ptrdiff_t&lt;/code&gt;, &lt;code&gt;size_t&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Pointer-sized integers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;byte&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;uint8_t&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Alias for U8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;F32&lt;/code&gt;, &lt;code&gt;F64&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;float&lt;/code&gt;, &lt;code&gt;double&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Floating point numbers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;char&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;char&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Single character&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;char*&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;C-string (null-terminated)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;U0&lt;/code&gt;, &lt;code&gt;void&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Empty type&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fixed-size arrays with value semantics.&lt;/p&gt;
    &lt;code&gt;var ints: int[5] = {1, 2, 3, 4, 5};
var zeros: [int; 5]; // Zero-initialized
&lt;/code&gt;
    &lt;p&gt;Group multiple values together.&lt;/p&gt;
    &lt;code&gt;var pair = (1, "Hello");
var x = pair.0;
var s = pair.1;
&lt;/code&gt;
    &lt;p&gt;Data structures with optional bitfields.&lt;/p&gt;
    &lt;code&gt;struct Point {
    x: int;
    y: int;
}

// Struct initialization
var p = Point { x: 10, y: 20 };

// Bitfields
struct Flags {
    valid: U8 : 1;
    mode:  U8 : 3;
}
&lt;/code&gt;
    &lt;p&gt;Tagged unions (Sum types) capable of holding data.&lt;/p&gt;
    &lt;code&gt;enum Shape {
    Circle(float),      // Holds radius
    Rect(float, float), // Holds width, height
    Point               // No data
}
&lt;/code&gt;
    &lt;p&gt;Standard C unions (unsafe access).&lt;/p&gt;
    &lt;code&gt;union Data {
    i: int;
    f: float;
}
&lt;/code&gt;
    &lt;code&gt;fn add(a: int, b: int) -&amp;gt; int {
    return a + b;
}

// Named arguments supported in calls
add(a: 10, b: 20);
&lt;/code&gt;
    &lt;p&gt;Anonymous functions that can capture their environment.&lt;/p&gt;
    &lt;code&gt;var factor = 2;
var double = x -&amp;gt; x * factor;  // Arrow syntax
var full = fn(x: int) -&amp;gt; int { return x * factor; }; // Block syntax
&lt;/code&gt;
    &lt;code&gt;if x &amp;gt; 10 {
    print("Large");
} else if x &amp;gt; 5 {
    print("Medium");
} else {
    print("Small");
}

// Ternary
var y = if x &amp;gt; 10 ? 1 : 0;
&lt;/code&gt;
    &lt;p&gt;Powerful alternative to &lt;code&gt;switch&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;match val {
    1 =&amp;gt; print("One"),
    2 | 3 =&amp;gt; print("Two or Three"),
    4..10 =&amp;gt; print("Range"),
    _ =&amp;gt; print("Other")
}

// Destructuring Enums
match shape {
    Circle(r) =&amp;gt; print(f"Radius: {r}"),
    Rect(w, h) =&amp;gt; print(f"Area: {w*h}"),
    Point =&amp;gt; print("Point")
}
&lt;/code&gt;
    &lt;code&gt;// Range
for i in 0..10 { ... }
for i in 0..10 step 2 { ... }

// Iterator/Collection
for item in vec { ... }

// While
while x &amp;lt; 10 { ... }

// Infinite with label
outer: loop {
    if done { break outer; }
}

// Repeat
repeat 5 { ... }
&lt;/code&gt;
    &lt;code&gt;// Guard: Execute else and return if condition is false
guard ptr != NULL else { return; }

// Unless: If not true
unless is_valid { return; }
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Operator&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Function Mapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;%&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Arithmetic&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;add&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;rem&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;==&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Comparison&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;eq&lt;/code&gt;, &lt;code&gt;neq&lt;/code&gt;, &lt;code&gt;lt&lt;/code&gt;, &lt;code&gt;gt&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;[]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Indexing&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;get&lt;/code&gt;, &lt;code&gt;set&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;??&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Null Coalescing (&lt;code&gt;val ?? default&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;??=&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Null Assignment (&lt;code&gt;val ??= init&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;?.&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Safe Navigation (&lt;code&gt;ptr?.field&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Try Operator (&lt;code&gt;res?&lt;/code&gt; returns error if present)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Zen C allows manual memory management with ergonomic aids.&lt;/p&gt;
    &lt;p&gt;Execute code when the current scope exits.&lt;/p&gt;
    &lt;code&gt;var f = fopen("file.txt", "r");
defer fclose(f);
&lt;/code&gt;
    &lt;p&gt;Automatically free the variable when scope exits.&lt;/p&gt;
    &lt;code&gt;autofree var types = malloc(1024);
&lt;/code&gt;
    &lt;p&gt;Implement &lt;code&gt;Drop&lt;/code&gt; to run cleanup logic automatically.&lt;/p&gt;
    &lt;code&gt;impl Drop for MyStruct {
    fn drop(mut self) {
        free(self.data);
    }
}
&lt;/code&gt;
    &lt;p&gt;Define methods on types using &lt;code&gt;impl&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;impl Point {
    // Static method (constructor convention)
    fn new(x: int, y: int) -&amp;gt; Point {
        return Point{x: x, y: y};
    }

    // Instance method
    fn dist(self) -&amp;gt; float {
        return sqrt(self.x * self.x + self.y * self.y);
    }
}
&lt;/code&gt;
    &lt;p&gt;Define shared behavior.&lt;/p&gt;
    &lt;code&gt;trait Drawable {
    fn draw(self);
}

impl Drawable for Circle {
    fn draw(self) { ... }
}
&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;use&lt;/code&gt; to mixin fields from another struct.&lt;/p&gt;
    &lt;code&gt;struct Entity { id: int; }
struct Player {
    use Entity; // Adds 'id' field
    name: string;
}
&lt;/code&gt;
    &lt;p&gt;Type-safe templates for Structs and Functions.&lt;/p&gt;
    &lt;code&gt;// Generic Struct
struct Box&amp;lt;T&amp;gt; {
    item: T;
}

// Generic Function
fn identity&amp;lt;T&amp;gt;(val: T) -&amp;gt; T {
    return val;
}
&lt;/code&gt;
    &lt;p&gt;Built on pthreads.&lt;/p&gt;
    &lt;code&gt;async fn fetch_data() -&amp;gt; string {
    // Runs in background
    return "Data";
}

fn main() {
    var future = fetch_data();
    var result = await future;
}
&lt;/code&gt;
    &lt;p&gt;Run code at compile-time to generate source or print messages.&lt;/p&gt;
    &lt;code&gt;comptime {
    print("Compiling...");
}
&lt;/code&gt;
    &lt;p&gt;Embed files as byte arrays.&lt;/p&gt;
    &lt;code&gt;var png = embed "assets/logo.png";
&lt;/code&gt;
    &lt;p&gt;Import compiler plugins to extend syntax.&lt;/p&gt;
    &lt;code&gt;import plugin "regex"
var re = regex! { ^[a-z]+$ };
&lt;/code&gt;
    &lt;p&gt;Pass preprocessor macros through to C.&lt;/p&gt;
    &lt;code&gt;#define MAX_BUFFER 1024
&lt;/code&gt;
    &lt;p&gt;Decorate functions and structs to modify compiler behavior.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attribute&lt;/cell&gt;
        &lt;cell role="head"&gt;Scope&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@must_use&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Warn if return value is ignored.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@deprecated("msg")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn/Struct&lt;/cell&gt;
        &lt;cell&gt;Warn on usage with message.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@inline&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Hint compiler to inline.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@noinline&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Prevent inlining.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@packed&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Struct&lt;/cell&gt;
        &lt;cell&gt;Remove padding between fields.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@align(N)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Struct&lt;/cell&gt;
        &lt;cell&gt;Force alignment to N bytes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@constructor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Run before main.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@destructor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Run after main exits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@unused&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn/Var&lt;/cell&gt;
        &lt;cell&gt;Suppress unused variable warnings.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@weak&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Weak symbol linkage.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@section("name")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Place code in specific section.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@noreturn&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Function does not return (e.g. exit).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;@derived(...)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Struct&lt;/cell&gt;
        &lt;cell&gt;Auto-implement traits (e.g. &lt;code&gt;Debug&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Zen C provides first-class support for inline assembly, transpiling directly to GCC-style extended &lt;code&gt;asm&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Write raw assembly within &lt;code&gt;asm&lt;/code&gt; blocks. Strings are concatenated automatically.&lt;/p&gt;
    &lt;code&gt;asm {
    "nop"
    "mfence"
}
&lt;/code&gt;
    &lt;p&gt;Prevent the compiler from optimizing away assembly that has side effects.&lt;/p&gt;
    &lt;code&gt;asm volatile {
    "rdtsc"
}
&lt;/code&gt;
    &lt;p&gt;Zen C simplifies the complex GCC constraint syntax with named bindings.&lt;/p&gt;
    &lt;code&gt;// Syntax: : out(var) : in(var) : clobber(reg)
// Uses {var} placeholder syntax for readability

fn add(a: int, b: int) -&amp;gt; int {
    var result: int;
    asm {
        "add {result}, {a}, {b}"
        : out(result)
        : in(a), in(b)
        : clobber("cc")
    }
    return result;
}
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Syntax&lt;/cell&gt;
        &lt;cell role="head"&gt;GCC Equivalent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Output&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;: out(var)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"=r"(var)&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Input&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;: in(var)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"r"(var)&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Clobber&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;: clobber("rax")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"rax"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Memory&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;: clobber("memory")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"memory"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;Note: When using Intel syntax (via&lt;/p&gt;&lt;code&gt;-masm=intel&lt;/code&gt;), you must ensure your build is configured correctly (for example,&lt;code&gt;//&amp;gt; cflags: -masm=intel&lt;/code&gt;). TCC does not support Intel syntax assembly.&lt;/quote&gt;
    &lt;p&gt;Zen C is designed to work with most C11 compilers. Some features rely on GNU C extensions, but these often work in other compilers. Use the &lt;code&gt;--cc&lt;/code&gt; flag to switch backends.&lt;/p&gt;
    &lt;code&gt;zc run app.zc --cc clang
zc run app.zc --cc zig&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Compiler&lt;/cell&gt;
        &lt;cell role="head"&gt;Pass Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Supported Features&lt;/cell&gt;
        &lt;cell role="head"&gt;Known Limitations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GCC&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;All Features&lt;/cell&gt;
        &lt;cell&gt;None.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Clang&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;All Features&lt;/cell&gt;
        &lt;cell&gt;None.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Zig&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;All Features&lt;/cell&gt;
        &lt;cell&gt;None. Uses &lt;code&gt;zig cc&lt;/code&gt; as a drop-in C compiler.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TCC&lt;/cell&gt;
        &lt;cell&gt;~70%&lt;/cell&gt;
        &lt;cell&gt;Basic Syntax, Generics, Traits&lt;/cell&gt;
        &lt;cell&gt;No &lt;code&gt;__auto_type&lt;/code&gt;, No Intel ASM, No Nested Functions.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Recommendation: Use GCC, Clang, or Zig for production builds. TCC is excellent for rapid prototyping due to its compilation speed but misses some advanced C extensions Zen C relies on for full feature support.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Zig's &lt;code&gt;zig cc&lt;/code&gt; command provides a drop-in replacement for GCC/Clang with excellent cross-compilation support. To use Zig:&lt;/p&gt;
    &lt;code&gt;# Compile and run a Zen C program with Zig
zc run app.zc --cc zig

# Build the Zen C compiler itself with Zig
make zig&lt;/code&gt;
    &lt;p&gt;We welcome contributions! Whether it's fixing bugs, adding documentation, or proposing new features.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the Repository: standard GitHub workflow.&lt;/item&gt;
      &lt;item&gt;Create a Feature Branch: &lt;code&gt;git checkout -b feature/NewThing&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Code Guidelines: &lt;list rend="ul"&gt;&lt;item&gt;Follow the existing C style.&lt;/item&gt;&lt;item&gt;Ensure all tests pass: &lt;code&gt;make test&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;Add new tests for your feature in &lt;code&gt;tests/&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Submit a Pull Request: Describe your changes clearly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The test suite is your best friend.&lt;/p&gt;
    &lt;code&gt;# Run all tests (GCC)
make test

# Run specific test
./zc run tests/test_match.zc

# Run with different compiler
./tests/run_tests.sh --cc clang
./tests/run_tests.sh --cc zig
./tests/run_tests.sh --cc tcc&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parser: &lt;code&gt;src/parser/&lt;/code&gt;- Recursive descent parser.&lt;/item&gt;
      &lt;item&gt;Codegen: &lt;code&gt;src/codegen/&lt;/code&gt;- Transpiler logic (Zen C -&amp;gt; GNU C/C11).&lt;/item&gt;
      &lt;item&gt;Standard Library: &lt;code&gt;std/&lt;/code&gt;- Written in Zen C itself.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/z-libs/Zen-C"/><published>2026-01-12T12:57:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46587934</id><title>Floppy disks turn out to be the greatest TV remote for kids</title><updated>2026-01-12T18:17:46.762569+00:00</updated><content>&lt;doc fingerprint="2c74f8d379255ff1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Floppy Disks: the best TV remote for kids&lt;/head&gt;
    &lt;p&gt;Modern TVs are very poorly suited for kids. They require using complicated remotes or mobile phones, and navigating apps that continually try to lure you into watching something else than you intended to. The usual scenario ends up with the kid feeling disempowered and asking an adult to put something on. That something ends up on auto-play because then the adult is free to do other things and the kid ends up stranded powerless and comatose in front of the TV.&lt;/p&gt;
    &lt;p&gt;Instead I wanted to build something for my 3-year old son that he could understand and use independently. It should empower him to make his own choices. It should be physical and tangible, i.e. it should be something he could touch and feel. It should also have some illusion that the actual media content was stored physically and not un-understandably in â€œthe cloudâ€, meaning it should e.g. be destroyable â€” if you break the media there should be consequences. And there should be no auto-play: interact once and get one video.&lt;/p&gt;
    &lt;head rend="h2"&gt;Floppy disks are awesome!&lt;/head&gt;
    &lt;p&gt;And then I remembered the sound of a floppy disk. The mechanical click as you insert it, the whirr of the disk spinning, and the sound of the read-head moving. Floppy disks are the best storage media ever invented! Why else would the â€œsave-iconâ€ still be a floppy disk? Who hasnâ€™t turned in a paper on a broken floppy disk, with the excuse ready that the floppy must have broken when the teacher asks a few days later? But kids these days have never used nor even seen a floppy disk, and I believe they deserve this experience!&lt;/p&gt;
    &lt;p&gt;Building on the experience from the Big Red Fantus-Button, I already had a framework for controlling a Chromecast, and because of the &lt;code&gt;netcat | bash&lt;/code&gt; shenanigans it was easily extendable.&lt;/p&gt;
    &lt;p&gt;My first idea for datastorage was to use the shell of a floppy disk and floppy drive, and put in an RFID tag; this has been done a couple of times on the internet, such as RFIDisk or this RaspberryPi based RFID reader or this video covering how to embed an RFID tag in a floppy disk. But getting the floppy disk apart to put in an RFID tag and getting it back together was kinda wonky.&lt;/p&gt;
    &lt;p&gt;When working on the project in Hal9k someone remarked: â€œDatastorage? The floppy disk can store data!â€, and a quick prototype later this worked really, really, well. Formatting the disk and storing a single small file, â€œautoexec.shâ€, means that all the data ends up in track 0 and is read more or less immediately. It also has the benefit that everything can be checked and edited with a USB floppy disk drive; and the major benefit that all the sounds are completely authentic: click, whirrr, brrr brrr.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autorun for floppy disks is not really a thing.&lt;/head&gt;
    &lt;p&gt;The next problem to tackle was how to detect that a disk is inserted. The concept of AutoRun from Windows 95 was a beauty: insert a CD-ROM and it would automatically start whatever was on the media. Great for convenience, quite questionably for security. While in theory floppy disks are supported for AutoRun, it turns out that floppy drives basically donâ€™t know if a disk is inserted until the operating system tries to access it! There is a pin 34 â€œDisk Changeâ€ that is supposed to give this information, but this is basically a lie. None of the drives in my possession had that pin connected to anything, and the internet mostly concurs. In the end I slightly modified the drive and added a simple rolling switch, that would engage when a disk was inserted.&lt;/p&gt;
    &lt;head rend="h2"&gt;A floppy disk walks into a drive; the microcontroller says â€œhello!â€&lt;/head&gt;
    &lt;p&gt;The next challenge was to read the data on a microcontroller. Helpfully, there is the Arduino FDC Floppy library by dhansel, which I must say is most excellent. Overall, this meant that the part of the project that involved reading a file from the floppy disk FAT filesystem was basically the easiest part of all!&lt;/p&gt;
    &lt;p&gt;However, the Arduino FDC Floppy library is only compatible with the AVR-based Arduinos, not the ESP-based ones, because it needs to control the timing very precisely and therefore uses a healthy amount of inline assembler. This meant that I would need one AVR-based Arduino to control the floppy disk, but another ESP-based one to do the WiFi communication. Such combined boards do exist, and I ended up using such a board, but Iâ€™m not sure I would recommend it: the usage is really finagly, as you need to set the jumpers differently for programming the ATmega, or programming the ESP, or connecting the two boards serial ports together.&lt;/p&gt;
    &lt;head rend="h2"&gt;A remote should be battery-powered&lt;/head&gt;
    &lt;p&gt;A remote control should be portable, and this means battery-powered. Driving a floppy disk of of lithium batteries was interesting. There is a large spike in current draw when the disk needs to spin up of several amperes, while the power draw afterwards is more modest, a couple of hundred milliamperes. I wanted the batteries to be 18650s, because I have those in abundance. This meant a battery voltage of 3.7V nominally, up to 4.2V for a fully charged battery; 5V is needed to spin the floppy around, so a boost DC-DC converter was needed. I used an off the shelf XL6009 step-up converter board. At this point a lot of head-scratching occurred: that initial spin-up power draw would cause the microcontroller to reset. In the end a 1000uF capacitor at the microcontroller side seemed to help but not eliminate the problem.&lt;/p&gt;
    &lt;p&gt;One crucial finding was that the ground side of the interface cable should absolutely not be connected to any grounds on the microcontroller side. I was using a relatively simple logic-level MOSFET, the IRLZ34N, to turn off the drive by disconnecting the ground side. If any ground is connected, the disk wonâ€™t turn off. But also: if any logic pin was being pulled to ground by the ATmega, that would also provide a path to ground. But since the ATmega cannot sink that much current this would lead to spurious resets! Obvious after the fact, but this took quite some headscratching. Setting all the logic pins to input, and thus high impedance, finally fixed the stability issues.&lt;/p&gt;
    &lt;p&gt;After fixing the stability, the next challenge was how to make both of the microcontrollers sleep. Because the ATmega sleep modes are quite a lot easier to deal with, and because the initial trigger would be the floppy inserting, I decided to make the ATmega in charge overall. Then the ESP has a very simple function: when awoken, read serial in, when a newline is found then send off that complete line via WiFi, and after 30 seconds signal to the ATmega that weâ€™re sleeping, and go back to sleep.&lt;/p&gt;
    &lt;p&gt;The overall flow for the ATmega is then:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A disk is inserted, this triggers a interrupt on the ATmega that wakes up.&lt;/item&gt;
      &lt;item&gt;The ATmega resets the ESP, waking it from deep sleep.&lt;/item&gt;
      &lt;item&gt;The ATmega sends a â€œdiskinâ€ message over serial to the ESP; the ESP transmits this over WiFi when available.&lt;/item&gt;
      &lt;item&gt;The ATmega turns on the drive itself, and reads the disk contents, and just sends it over serial to the ESP.&lt;/item&gt;
      &lt;item&gt;Spin down the disk, go to sleep.&lt;/item&gt;
      &lt;item&gt;When the disk is ejected, send a â€œdiskoutâ€ message over serial, resetting the ESP if needed.&lt;/item&gt;
      &lt;item&gt;Go back to 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The box itself is just lasercut from MDF-board. For full details see the FloppyDiskCast Git repository.&lt;/p&gt;
    &lt;head rend="h2"&gt;Server-side handlers&lt;/head&gt;
    &lt;p&gt;Responding to those commands is still the &lt;code&gt;netcat | bash&lt;/code&gt; from the Big Red Fantus-Button, which was simply extended with a few more commands and capabilities.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;diskin&lt;/code&gt; always sends a â€œplayâ€ command to the Chromecast.&lt;code&gt;diskout&lt;/code&gt; always sends a â€œpauseâ€ command to the Chromecast.&lt;lb/&gt;Other commands like &lt;code&gt;dad-music&lt;/code&gt; are handled in one of two ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Play a random video from a set, if a video from that set is not already playing: e.g. &lt;code&gt;dad-music&lt;/code&gt;will randomly play one of dadâ€™s music tracks â€“ gotta influence the youth!&lt;/item&gt;
      &lt;item&gt;Play the next video from a list, if a video from the list is not already playing: e.g. &lt;code&gt;fantus-maskinerne&lt;/code&gt;will play the next episode, and only the next episode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common for both is that they should be idempotent actions, and the &lt;code&gt;diskin&lt;/code&gt; shortcut will make the media resume without having to wait for the disk contents itself to be read and processed. This means that the â€œplay/pauseâ€ disk just contains an empty file to work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Questionable idea meets real-world 3 year old user&lt;/head&gt;
    &lt;p&gt;The little guy quickly caught on to the idea! Much fun was had just pausing and resuming music and his Fantus TV shows. He explored and prodded, and some disks were harmed in the process. One problem that I did solve was that the read head stayed on track 0 after having read everything: this means that when the remote with disk inside it is tumbled around, the disk gets damaged at track 0. To compensate for this, I move the head to track 20 after reading has finished: any damage is then done there, where we donâ€™t store any data. As a bonus it also plays a little more mechanic melody.&lt;/p&gt;
    &lt;p&gt;Be the first to comment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.smartere.dk/2026/01/floppy-disks-the-best-tv-remote-for-kids/"/><published>2026-01-12T13:07:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46588319</id><title>Ireland fast tracks Bill to criminalise harmful voice or image misuse</title><updated>2026-01-12T18:17:46.552251+00:00</updated><content>&lt;doc fingerprint="b9201773193591cf"&gt;
  &lt;main&gt;
    &lt;p&gt;The head of the Oireachtas committee on artificial intelligence (AI) has called on the Government to fast track a Bill that would criminalise the harmful misuse of someoneâ€™s voice or image.&lt;/p&gt;
    &lt;p&gt;Fianna FÃ¡il TD Malcolm Byrne introduced the Protection of Voice and Image Bill in April, which would criminalise what are colloquially known as â€œdeepfakesâ€ - when someoneâ€™s voice or image is used to generate a false photo or video with AI.&lt;/p&gt;
    &lt;p&gt;It follows concerns over Elon Muskâ€™s AI tool Grok being used to digitally undress women and children for distribution on his social media channel X.&lt;/p&gt;
    &lt;p&gt;The strength of laws in Ireland to criminalise AI-generated non-consensual intimate images and child sex abuse images are being examined by the Attorney General.&lt;/p&gt;
    &lt;head rend="h2"&gt;READ MORE&lt;/head&gt;
    &lt;p&gt;Mr Byrne said while generating child sex abuse imagery and sharing intimate images without consent are already criminal offences, this Bill would create a new standalone criminal offence for those who â€œknowingly exploit another personâ€™s name, image, voice or likeness without consentâ€, especially when it is done to harm to deceive.&lt;/p&gt;
    &lt;p&gt;â€œThe deliberate misuse of someoneâ€™s image or voice without their consent for malign purposes should be a criminal offence. This Bill is a useful baseline and we need to move quickly to address this problem,â€ Mr Byrne said&lt;/p&gt;
    &lt;p&gt;Irelandâ€™s child protection rapporteur, Caoilfhionn Gallagher, has said the harms from deep fake sexual abuse for the individuals depicted are â€œequivalentâ€ to those from authentic images â€œbecause for victims the videos feel realâ€.&lt;/p&gt;
    &lt;p&gt;Ms Gallagher also questioned whether Irelandâ€™s protections holding social media platforms to account were sufficient.&lt;/p&gt;
    &lt;p&gt;The special rapporteur on child protection was speaking on RTÃ‰ radioâ€™s Morning Ireland about concerns over the X platformâ€™s Grok AI tool which includes the facility to â€œnudifyâ€ images.&lt;/p&gt;
    &lt;p&gt;â€œGiven how realistic they are, victims know that they might be perceived as real by others,â€ she said.&lt;/p&gt;
    &lt;p&gt;â€œGenerating these images is often part of a pattern of abuse or harassment. And Iâ€™m acutely conscious of the horrendous case of Nicole Coco Fox from Clondalkin, who died by suicide due to online abuse. So we know how devastating online abuse of any kind can be. We have to see this in that perspective.â€&lt;/p&gt;
    &lt;p&gt;Ms Gallagher also raised the issue of other nudification apps being trained on vast data sets of mostly female images â€œbecause they tend to work most effectively on womenâ€™s bodiesâ€.&lt;/p&gt;
    &lt;p&gt;â€œAs a result, 99 per cent of sexually explicit deepfakes accessible online are estimated to be of women and girls. So this is also a gender-based violence issue.â€&lt;/p&gt;
    &lt;p&gt;Ms Gallagher added that there was concern internationally about whether the protections in place were sufficient, because most of the protections from a legal and policy perspective internationally were very focused on the users themselves who may generate the images rather than the platforms and the products which facilitate the creation of these images.&lt;/p&gt;
    &lt;p&gt;â€œTo take the platforms themselves, in this case X AI has its own acceptable use policy and which prohibits depicting likenesses of persons in a pornographic manner, but plainly thatâ€™s completely insufficient.â€&lt;/p&gt;
    &lt;p&gt;Ms Gallagher said Irelandâ€™s relevant laws, including section five of the Child Trafficking and Pornography Act 1998 and Cocoâ€™s Law, were â€œquite focused on the individual usersâ€.&lt;/p&gt;
    &lt;p&gt;â€œThe issue here is Irelandâ€™s not alone and internationally there is a concern about the adequacy of the mechanisms for holding the platforms to account. Ultimately, this is in part a product safety issue and about whether the product itself which allows the images to be generated should be illegal or should be regulated more tightly rather than simply the individual users who take action using it,â€ she said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.irishtimes.com/ireland/2026/01/07/call-to-fast-track-bill-targeting-ai-deepfakes-and-identity-hijacking/"/><published>2026-01-12T13:38:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46588572</id><title>Reproducing DeepSeek's MHC: When Residual Connections Explode</title><updated>2026-01-12T18:17:46.364633+00:00</updated><content>&lt;doc fingerprint="f7987005e7b444d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;DeepSeek's mHC: When Residual Connections Explode&lt;/head&gt;
    &lt;p&gt;Every transformer youâ€™ve ever used has the same residual connection design from 2016.&lt;/p&gt;
    &lt;p&gt;GPT-5, Claude, Llama, Gemini. Under the hood, they all do the same thing: . One stream of information flowing through the network, with each layer adding to it.&lt;/p&gt;
    &lt;p&gt;DeepSeek asked: what if it was wider?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Setup&lt;/head&gt;
    &lt;p&gt;Standard residual connections are the backbone of every modern transformer. The idea is simple:&lt;/p&gt;
    &lt;p&gt;The input flows through unchanged, plus the layerâ€™s output. One stream of information. What goes in comes out, plus a learned update. This is why transformers can be hundreds of layers deep: the gradient has a clean path backward. Simple. Stable. Unchanged since 2016.&lt;/p&gt;
    &lt;p&gt;Hyper-Connections take a different approach. Instead of one stream, expand to n parallel streams with learnable mixing matrices:&lt;/p&gt;
    &lt;p&gt;Compared to standard residual:&lt;/p&gt;
    &lt;p&gt;Three matrices control how information flows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;H_res: How streams mix in the residual path (the red crossings)&lt;/item&gt;
      &lt;item&gt;H_pre: How streams combine before entering the layer&lt;/item&gt;
      &lt;item&gt;H_post: How the layerâ€™s output distributes back to streams&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More expressive. More parameters with negligible computational overhead. Better performance, in theory.&lt;/p&gt;
    &lt;p&gt;The problem? Those mixing matrices are unconstrained. They can amplify signals, not just route them.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Explosion&lt;/head&gt;
    &lt;p&gt;Under aggressive learning rates, Hyper-Connection (HC) signal amplification in my reproduction hit 7x before eventually collapsing. Amax (the maximum of row and column absolute sums) measures how much a matrix can amplify signals.&lt;/p&gt;
    &lt;p&gt;At my 10M parameter scale, this is survivable. But DeepSeek saw this at 27B:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;â€œThe Amax Gain Magnitude yields extreme values with peaks of 3000â€&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thatâ€™s not a typo. Three thousand times amplification. At 27B parameters, unconstrained HC didnâ€™t just drift. It exploded. My 10M reproduction hitting 9.2x is the early warning sign of this exponential failure.&lt;/p&gt;
    &lt;p&gt;This is why unconstrained mixing matrices break at scale. Small amplifications compound exponentially.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fix: Constrain the Manifold&lt;/head&gt;
    &lt;p&gt;DeepSeekâ€™s fix is clean: constrain the mixing matrices to be doubly stochastic.&lt;/p&gt;
    &lt;p&gt;A doubly stochastic matrix has:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All non-negative entries&lt;/item&gt;
      &lt;item&gt;Rows sum to 1&lt;/item&gt;
      &lt;item&gt;Columns sum to 1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means the mixing operation can only take weighted averages of streams. It can route information, shuffle it, blend it. But it cannot amplify.&lt;/p&gt;
    &lt;p&gt;How? The Sinkhorn-Knopp algorithm.&lt;/p&gt;
    &lt;p&gt;The algorithm is dead simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with any matrix (the raw learned weights)&lt;/item&gt;
      &lt;item&gt;Exponentiate to make all entries positive:&lt;/item&gt;
      &lt;item&gt;Normalize rows so each row sums to 1&lt;/item&gt;
      &lt;item&gt;Normalize columns so each column sums to 1&lt;/item&gt;
      &lt;item&gt;Repeat steps 3-4 until convergence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thatâ€™s it. Alternate row and column normalization. Twenty iterations is enough.&lt;/p&gt;
    &lt;p&gt;This procedure is differentiable. Gradients flow back through all twenty iterations. The network learns the raw weights , and Sinkhorn ensures the actual mixing matrix is always doubly stochastic.&lt;/p&gt;
    &lt;p&gt;When I first saw this, it felt like cheating. Youâ€™re not learning stability. Youâ€™re forcing it. But some properties shouldnâ€™t be learned; they should be guaranteed.&lt;/p&gt;
    &lt;p&gt;Technical note: Strictly speaking, only the recursive matrix H_res needs the full Sinkhorn doubly-stochastic treatment. Itâ€™s the one compounding errors layer-over-layer. The input/output mixers (H_pre, H_post) are just bounded via sigmoid. The Sinkhorn compute cost is paid only where it matters most.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Seed Variation Results (Depth 24, 3 seeds)&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Val Loss (mean Â± std)&lt;/cell&gt;
        &lt;cell role="head"&gt;Max Amax (mean Â± std)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;HC&lt;/cell&gt;
        &lt;cell&gt;0.884 Â± 0.033&lt;/cell&gt;
        &lt;cell&gt;6.77 Â± 0.60&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;mHC&lt;/cell&gt;
        &lt;cell&gt;1.116 Â± 0.012&lt;/cell&gt;
        &lt;cell&gt;1.00 Â± 0.00&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;HC wins on raw performance: 0.88 vs 1.12 validation loss. At 10M parameters, the mHC constraint acts like a stability tax; you pay in expressivity. But at 27B parameters, that tax is the only thing preventing your model from exploding to NaN.&lt;/p&gt;
    &lt;p&gt;But look at the variance. HCâ€™s loss varies 3x more across seeds (Â±0.033 vs Â±0.012). And Amax? HC swings from 6.1 to 7.6 depending on the seed. mHC is 1.00. Every seed. Every run. Zero variance.&lt;/p&gt;
    &lt;p&gt;At 10M parameters, the instability is survivable. HC still wins. But at 27B parameters, that 6-7x amplification becomes 3000x. You canâ€™t gamble at that scale.&lt;/p&gt;
    &lt;head rend="h3"&gt;Depth Scaling&lt;/head&gt;
    &lt;p&gt;I also swept depths from 6 to 24 layers (constant ~11M parameter budget):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Loss improves with depth, until it doesnâ€™t. Depth 20 hit the sweet spot (0.85 val loss). Depth 24 regressed slightly (0.93) due to the width bottleneck from shrinking dim to 192.&lt;/item&gt;
      &lt;item&gt;Amax is unpredictable. Depth 20 spiked to 9.2x. Depth 12 hit 6.6x. Depth 8 stayed at 4.3x. Thereâ€™s no clean relationship; HC is chaotic.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Experiment Details&lt;/head&gt;
    &lt;p&gt;Dataset: TinyShakespeare (~1M chars, character-level) Model: GPT-2 architecture, ~10M parameters Training: 5000 steps, AdamW (Î²1=0.9, Î²2=0.95), weight decay 0.1, cosine LR decay Hardware: Apple M-series (MPS)&lt;/p&gt;
    &lt;p&gt;Depth sweep: 8 configurations (6-24 layers), width adjusted to maintain ~11M params Seed variation: 3 seeds (42, 123, 456) at depth 24&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters&lt;/head&gt;
    &lt;p&gt;Residual connections are more than a trick to help gradients flow. Theyâ€™re a conservation law.&lt;/p&gt;
    &lt;p&gt;In physics, conservation laws constrain whatâ€™s possible but enable prediction. You canâ€™t build a perpetual motion machine, but you can calculate exactly where a ball will land.&lt;/p&gt;
    &lt;p&gt;The identity mapping in residual connections is similar. It constrains the network by preventing arbitrary transformations, but it guarantees stability. Signal magnitude is preserved.&lt;/p&gt;
    &lt;p&gt;HC breaks conservation; mHC restores it, not by returning to identity, but by finding a richer manifold that still conserves signal.&lt;/p&gt;
    &lt;p&gt;In 2016, He et al. introduced ResNets to solve the vanishing gradient problem, ensuring signals didnâ€™t die. Ten years later, the opposite problem emerged: exploding signals from hyper-connectivity. The identity mapping solved the first by being passive. mHC solves the second by enforcing conservation.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Every residual connection is a conservation law. mHC enforces it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Not a hack, not a trick. A principled constraint that makes the architecture work at scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The stream persistence bug humbled me. My first implementation looked right. The equations matched the paper. The code ran. But I was projecting the output back to a single stream and re-expanding it at each layer, killing the parallel architecture. The â€œhyperâ€ part of Hyper-Connections wasnâ€™t actually doing anything. Three separate audits said â€œlooks correct.â€ The bug was architectural, not mathematical. I only caught it by asking: â€œWait, what shape is actually flowing between layers?â€&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Constraints arenâ€™t limitations; theyâ€™re guarantees. The doubly stochastic projection forces stability. Youâ€™re not learning good behavior. Youâ€™re making bad behavior impossible. My first reaction: â€œThatâ€™s not elegant. Thatâ€™s a straitjacket.â€ Then I saw HC hit 7x amplification. Oh. Thatâ€™s the point.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The boring choice scales. Standard residual connections have survived since 2016 not because theyâ€™re optimal, but because theyâ€™re stable. HC is more expressive but fragile. mHC finds a middle ground: more expressive than standard residuals, with stability guarantees.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Whatâ€™s Next&lt;/head&gt;
    &lt;p&gt;This is Part 1 of a two-part series.&lt;/p&gt;
    &lt;p&gt;Part 1 (this post): Reproduce mHC at small scale to understand the mechanics.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10M parameters, TinyShakespeare dataset&lt;/item&gt;
      &lt;item&gt;Constant parameter budget across depths&lt;/item&gt;
      &lt;item&gt;Goal: Validate the core claim: HC explodes, mHC doesnâ€™t&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Part 2 (Thursday): Scale up to see real instability.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1B parameters on A100s&lt;/item&gt;
      &lt;item&gt;C4 dataset, fixed width (no bottleneck)&lt;/item&gt;
      &lt;item&gt;Goal: Push toward the 3000x Amax regime&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At 10M params, HC peaked at 9.2x amplification, chaotic but survivable. The paper saw 3000x at 27B. Part 2 will show where things break.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resources&lt;/head&gt;
    &lt;p&gt;Paper: Manifold-Constrained Hyper-Connections (arXiv 2512.24880)&lt;/p&gt;
    &lt;p&gt;Related: Deep Residual Learning (He et al., 2016)&lt;/p&gt;
    &lt;p&gt;Code: Coming with Part 2.&lt;/p&gt;
    &lt;p&gt;Part 2 comes Thursday. Follow @TayKolasinski to catch it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://taylorkolasinski.com/notes/mhc-reproduction/"/><published>2026-01-12T13:57:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46588837</id><title>LLVM: The bad parts</title><updated>2026-01-12T18:17:46.213371+00:00</updated><content>&lt;doc fingerprint="3a5898176125975c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;LLVM: The bad parts&lt;/head&gt;
    &lt;p&gt;A few years ago, I wrote a blog post on design issues in LLVM IR. Since then, one of these issues has been fixed fully (opaque pointers migration), one has been mostly fixed (constant expression removal), and one is well on the way towards being fixed (ptradd migration).&lt;/p&gt;
    &lt;p&gt;This time Iâ€™m going to be more ambitious and not stop at three issues. Of course, not all of these issues are of equal importance, and how important they are depends on who you ask. In the interest of brevity, I will mostly just explain what the problem is, and not discuss what possible solutions would be.&lt;/p&gt;
    &lt;p&gt;Finally, I should probably point out that this is written from my perspective as the lead maintainer of the LLVM project: This is not a list of reasons to not use LLVM, itâ€™s a list of opportunities to improve LLVM.&lt;/p&gt;
    &lt;head rend="h2"&gt;High level issues&lt;/head&gt;
    &lt;head rend="h3"&gt;Review capacity&lt;/head&gt;
    &lt;p&gt;Unlike many other open-source projects, LLVM certainly does not suffer from a lack of contributors. There are thousands of contributors and the distribution is relatively flat (that is, itâ€™s not the case that a small handful of people is responsible for the majority of contributions.)&lt;/p&gt;
    &lt;p&gt;What LLVM does suffer from is insufficient review capacity. There are a lot more people writing code than reviewing it. This is somewhat unsurprising, as code review requires more expertise than writing code, and may not provide immediate value1 to the person reviewing (or their employer).&lt;/p&gt;
    &lt;p&gt;Lack of review capacity makes for a bad contributor experience, and can also result in bad changes making their way into the codebase. The way this usually works out is that someone puts up a PR, then fails to get a qualified review for a long period of time, and then one of their coworkers (who is not a qualified reviewer for that area) ends up rubberstamping the PR.&lt;/p&gt;
    &lt;p&gt;A related problem is that LLVM has a somewhat peculiar contribution model where itâ€™s the responsibility of the PR author to request reviewers. This is especially problematic for new contributors, who donâ€™t know whom to request. Often relevant reviewers will become aware of the PR thanks to a label-based notification system, but this is not apparent from the UI, and itâ€™s easy for PRs to fall through the cracks.&lt;/p&gt;
    &lt;p&gt;A potential improvement here would be a Rust-style PR assignment system.&lt;/p&gt;
    &lt;head rend="h3"&gt;Churn&lt;/head&gt;
    &lt;p&gt;Both the LLVM C++ API and LLVM IR are not stable and undergo frequent changes. This is simultaneously a great strength and weakness of LLVM. Itâ€™s a strength because LLVM does not stagnate and is willing to address past mistakes even at significant cost. Itâ€™s a weakness because churn imposes costs on users of LLVM.&lt;/p&gt;
    &lt;p&gt;Frontends are somewhat insulated from this because they can use the largely stable C API. However, it does not cover everything, and most major frontends will have additional bindings that use the unstable C++ API.&lt;/p&gt;
    &lt;p&gt;Users that integrate with LLVM more tightly (for example downstream backends) donâ€™t have that option, and have to keep up with all API changes.&lt;/p&gt;
    &lt;p&gt;This is part of LLVMâ€™s general development philosophy, which Iâ€™ll express somewhat pointedly as â€œupstream or GTFOâ€. LLVM is liberally licensed and does not require you to contribute changes upstream. However, if you do not upstream your code, then it will also not factor into upstream decision-making.&lt;/p&gt;
    &lt;p&gt;This point is somewhat unlike the rest, in that Iâ€™m not sure itâ€™s possible to make things â€œstrictly betterâ€ here. Itâ€™s possible that LLVMâ€™s current point on the stability scale is not optimal, but moving it somewhere else would come with significant externalities. Making major changes in LLVM is already extremely hard due to the sheer scale of the project, without adding additional stability constraints on top.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build time&lt;/head&gt;
    &lt;p&gt;LLVM is a huge project. LLVM itself is &amp;gt;2.5 million lines of C++ and the entire monorepo is something like 9 million. C++ is not exactly known for fast build times, and compiling all that code takes time. This is bearable if you either have fast hardware or access to a build farm, but trying to build LLVM on a low-spec laptop is not going to be fun.&lt;/p&gt;
    &lt;p&gt;An additional complication is building with debug info (which I always recommend against), in which case youâ€™ll add the extra gotchas of slow link times, high risk of OOM and massive disk usage. There are ways to avoid that (using shared libs or dylib build, using split dwarf, using lld), but it takes some expertise.&lt;/p&gt;
    &lt;p&gt;Promising changes in this area are the use of pre-compiled headers (which significantly improves build time), and changing to use a dylib build by default (which reduces disk usage and link time, esp. for debuginfo builds). Another is to reduce test overhead using daemonization (not strictly part of the â€œbuild timeâ€, but relevant for the development cycle).&lt;/p&gt;
    &lt;head rend="h3"&gt;CI stability&lt;/head&gt;
    &lt;p&gt;LLVM CI consists of over 200 post-commit buildbots that test LLVM in lots of different configurations on lots of different hardware. Commits that turn a buildbot from green to red result in an email to the commit author.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this CI is never fully green, and flaky on top. This is in part due to flaky tests (typically in lldb or openmp), but can also be due to buildbot-specific issues. The end result is that itâ€™s â€œnormalâ€ to get buildbot failure notifications for any given commit, even if it is perfectly harmless. This dilutes the signal, and makes it easier to miss the real failures.&lt;/p&gt;
    &lt;p&gt;The introduction of pre-merge testing on PRs did significantly improve the overall CI situation, but not the buildbot problem as such. I think we need to start taking flaky tests/buildbots more seriously before we can really make progress here.&lt;/p&gt;
    &lt;p&gt;Because someone is definitely going to mention how this is not rocket science, and we just need to start using bors / merge queues to guarantee an always-green build: Itâ€™s a problem of scale. There are &amp;gt;150 commits on a typical workday, which would be more than one commit every 10 minutes even if they were uniformly distributed. Many buildbots have multi-hour runs. This is hard to reconcile.2&lt;/p&gt;
    &lt;head rend="h3"&gt;End-to-end testing&lt;/head&gt;
    &lt;p&gt;In some respects, LLVM has very thorough test coverage. Weâ€™re quite pedantic about making sure that new optimizations have good coverage of both positive and negative tests. However, these tests are essentially unit tests for a single optimization pass or analysis.&lt;/p&gt;
    &lt;p&gt;We have only a small amount of coverage for the entire optimization pipeline (phase ordering tests), so optimizations sometimes regress due to pass interactions. Tests for the combination of the middle-end and backend pipelines are essentially nonexistent. There is likely room for improvement here, though it comes with tradeoffs.&lt;/p&gt;
    &lt;p&gt;However, what actually concerns me are end-to-end executable tests. LLVMâ€™s test suite proper does not feature these at all. Executable tests are located in a separate llvm-test-suite repo, which is typically not used during routine development, but run by buildbots. It contains a lot of different code ranging from benchmarks to unit tests.&lt;/p&gt;
    &lt;p&gt;However, llvm-test-suite has quite few tests (compared to LLVM lit tests) and does not comprehensively cover basic operations. Things like testing operations on different float formats, on integers of different sizes, vectors of different sizes and element types, etc.&lt;/p&gt;
    &lt;p&gt;In part this is because of limitations of testing through C/C++, which is very heterogeneous in type support (C compilers donâ€™t like exposing types that donâ€™t have a defined psABI for the target). But thatâ€™s no excuse to delegate this testing to Zig instead (which exposes everything, everywhere, and has the corresponding test coverage).&lt;/p&gt;
    &lt;head rend="h3"&gt;Backend divergence&lt;/head&gt;
    &lt;p&gt;While LLVMâ€™s middle-end is very unified, backend implementations are very heterogeneous, and there is a tendency to fix issues (usually performance, but sometimes even correctness) only for the backend youâ€™re interested in.&lt;/p&gt;
    &lt;p&gt;This takes many forms, like implementing target-specific DAG combines instead of generic ones. Though my definite favorite is to introduce lots of target hooks for optimizations â€“ not because the optimization is actually only beneficial for one target, but because the person introducing it just doesnâ€™t want to deal with the fallout on other targets.&lt;/p&gt;
    &lt;p&gt;This is understandable â€“ after all, they may lack the knowledge to evaluate a change for other targets, so it may require working with many other maintainers, which can slow progress a lot. But the end result is still increasing divergence and duplication.&lt;/p&gt;
    &lt;p&gt;Lack of end-to-end testing compounds this issue, because that would act as something of a forcing function that at least all operations compile without crashing and produce correct results for all tested targets.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compilation time&lt;/head&gt;
    &lt;p&gt;Because Iâ€™ve complained about this enough in the past, Iâ€™ll keep it short: LLVM is slow, which is an issue both for JIT use cases, and anything that tends to produce huge amounts of IR (like Rust or C++).&lt;/p&gt;
    &lt;p&gt;Since Iâ€™ve started tracking compile-times, the situation has significantly improved, both through targeted improvements and avoidance of regressions. However, there is still a lot of room for improvement: LLVM still isnâ€™t fast, itâ€™s just less slow.&lt;/p&gt;
    &lt;p&gt;One thing that LLVM is particularly bad at are &lt;code&gt;-O0&lt;/code&gt; compile-times. The architecture is optimized for optimization, and lots of costs remain even if no optimization takes place. The LLVM TPDE alternative backend shows that itâ€™s possible to do better by an order of magnitude.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance tracking&lt;/head&gt;
    &lt;p&gt;The flip side of the compile-time coin is runtime performance. This is something that LLVM obviously cares a lot about. Which is why I find it rather surprising that LLVM does not have any â€œofficialâ€ performance tracking infrastructure.&lt;/p&gt;
    &lt;p&gt;Of course, there are lots of organizations which track performance of LLVM downstream, on their own workloads. In some ways this is good, because it means there is more focus on real-world workloads than on synthetic benchmarks like SPEC. However, not having readily accessible, public performance tracking also makes it hard for contributors to evaluate changes.&lt;/p&gt;
    &lt;p&gt;To be fair, LLVM does have an LNT instance, but a) itâ€™s currently broken, b) LNT is one of the worst UX crimes ever committed, c) little data gets submitted there, and d) itâ€™s not possible to request a test run for a PR, or something like that.&lt;/p&gt;
    &lt;p&gt;This point is frankly just baffling to me. I donâ€™t personally care about SPEC scores, but I know plenty of people do, so why there is no first-class tracking for this is a mystery to me.&lt;/p&gt;
    &lt;head rend="h2"&gt;IR design&lt;/head&gt;
    &lt;head rend="h3"&gt;Undef values&lt;/head&gt;
    &lt;p&gt;Undef values take an arbitrary value from a certain set. They are used to model uninitialized values, and have historically been used to model deferred undefined behavior. The latter role has been replaced by poison values, which have much simpler propagation rules and are more amenable to optimization. However, undef is still used for uninitialized memory to this day.&lt;/p&gt;
    &lt;p&gt;There are two main problems with undef values. The first is the multi-use problem: An undef value can take a different value at each use. This means that transforms that increase the use count are generally invalid, and care has to be taken when optimizing based on value equality. The mere existence of undef values prevents us from performing optimizations we want to do, or greatly increases their complexity.&lt;/p&gt;
    &lt;p&gt;The second issue is that undef is very hard to reason about. Humans have trouble understanding it, and for proof-checkers it is computationally expensive.&lt;/p&gt;
    &lt;p&gt;Most likely, uninitialized memory will be represented using poison values instead in the future, but this runs into the problem that LLVM currently is not capable of correctly treating poison in memory. Proper support for poison in memory requires additional IR features, like the byte type.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unsoundness and specification incompleteness&lt;/head&gt;
    &lt;p&gt;While most miscompilations (that is, correctness bugs) in LLVM are resolved quickly, there are quite a few that remain unfixed despite having been known for a long time. These issues usually combine the qualities of being largely theoretical (that is, appearing only in artificially constructed examples rather than real-world code) and running up against issues in LLVMâ€™s IR design.&lt;/p&gt;
    &lt;p&gt;Some of them are cases where we have a good idea of how the IR design needs to change to address the issue, but these changes are complex and often require a lot of work to recover optimization parity. There is often a complexity cliff where you can do something thatâ€™s simple and nearly correct, or you can do something very complex that is fully correct.&lt;/p&gt;
    &lt;p&gt;Then there are other cases, where just deciding on how things should work is a hard problem. The provenance model is a prime example of this. The interaction of provenance with integer casts and type punning is a difficult problem with complex tradeoffs.&lt;/p&gt;
    &lt;p&gt;However, at some point these issues do need to be resolved. The recently formed formal specification working group aims to tackle these problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Constraint encoding&lt;/head&gt;
    &lt;p&gt;A key challenge for optimizing compilers is encoding of constraints (like â€œthis value is non-negativeâ€ or â€œthis add will not overflowâ€). This includes both frontend-provided constraints (based on language undefined behavior rules), but also compiler-generated ones.&lt;/p&gt;
    &lt;p&gt;In particular, there are many different analyses that can infer facts about the program, but keeping these up-to-date throughout optimization is challenging. One good way to handle this is to encode facts directly in the IR. Correctly updating or discarding these annotations then becomes part of transform correctness.&lt;/p&gt;
    &lt;p&gt;LLVM has many different ways to encode additional constraints (poison flags, metadata, attributes, assumes), and these all come with tradeoffs in terms of how much information can be encoded, how reliably it is retained during optimization and to what degree it can negatively affect optimization. Information from metadata is lost too often, while information from assumes is not lost often enough.&lt;/p&gt;
    &lt;head rend="h3"&gt;Floating-point semantics&lt;/head&gt;
    &lt;p&gt;There are various issues with floating-point (FP) semantics once we move outside the nice world of â€œstrictly conforming IEEE 754 floats in the default environmentâ€. A few that come to mind are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Handling of signaling NaN and FP exceptions, and non-default FP environment in general. LLVM represents this using constrained FP intrinsics. This is not ideal, as all the FP handling is split into two parallel universes.&lt;/item&gt;
      &lt;item&gt;Handling of denormals. LLVM has a function attribute to not assume IEEE denormal behavior, but this is only suitable for cases where flush to zero (FTZ) is used globally. It does not help with modeling cases like ARM, where scalar ops are IEEE, while vector ops use FTZ.&lt;/item&gt;
      &lt;item&gt;Handling of excess precision, in particular when using the x87 FPU.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Other technical issues&lt;/head&gt;
    &lt;head rend="h3"&gt;Partial migrations&lt;/head&gt;
    &lt;p&gt;LLVM is a very large project, and making any significant changes to it is hard and time consuming. Migrations often span years, where two different implementations of something coexist, until all code has been migrated. The two prime examples of this are:&lt;/p&gt;
    &lt;p&gt;New pass manager: The â€œnewâ€ pass manager was first introduced more than a decade ago. Then about five years ago, we started using it for the middle-end optimization pipeline by default, and support for the legacy PM was dropped.&lt;/p&gt;
    &lt;p&gt;However, the back-end is still using the legacy pass manager. There is ongoing work to support the new pass manager in codegen, and weâ€™re pretty close to the point where it can be used end-to-end for a single target. However, I expect it will still take quite a while for all targets to be ported and the legacy pass manager to be completely retired.&lt;/p&gt;
    &lt;p&gt;GlobalISel: This is an even more extreme case. GlobalISel is the â€œnewâ€ instruction selector that is intended to replace SelectionDAG (and FastISel). It was introduced approximately one decade ago, and to this day, none of the targets that originally used SelectionDAG have been fully migrated to GlobalISel. There is one new target thatâ€™s GlobalISel-only, and there is one that uses GlobalISel by default for unoptimized builds. But otherwise, SelectionDAG is still the default everywhere.&lt;/p&gt;
    &lt;p&gt;There are two backends (AMDGPU and AArch64) that have somewhat complete GlobalISel support, but itâ€™s not clear when/if theyâ€™ll be able to switch to using it by default. A big problem here is that new optimizations are continually being implemented on the SDAG side, so itâ€™s hard to keep parity.&lt;/p&gt;
    &lt;head rend="h3"&gt;ABI / calling convention handling&lt;/head&gt;
    &lt;p&gt;Essentially everything about the handling of calling conventions in LLVM is a mess.&lt;/p&gt;
    &lt;p&gt;The responsibility for handling calling conventions is split between the frontend and the backend. There are good reasons why LLVM canâ€™t do this by itself (LLVM IR sits at a too low level of abstraction to satisfy the extremely arcane ABI rules).&lt;/p&gt;
    &lt;p&gt;This is not a problem in itself â€“ however, there is zero documentation of what the calling convention contract between the frontend and LLVM is, and the proper way to implement C FFI is essentially to look at what Clang does and copy that (invariably with errors, because the rules can be very subtle).&lt;/p&gt;
    &lt;p&gt;Iâ€™ve proposed to fix this by introducing an ABI lowering library and vortex73 has implemented a prototype for it as part of GSoC. So weâ€™re well on the way to resolving this side of the problem.&lt;/p&gt;
    &lt;p&gt;There are more problems though. One that Rust has struggled with a lot is the interaction of target features with the calling convention. Enabling additional target features can change the call ABI, because additional float/vector registers start getting used for argument/return passing. This means that calls between functions with a feature enabled and disabled may be incompatible, because they assume different ABIs.&lt;/p&gt;
    &lt;p&gt;Ideally, ABI and target features would be orthogonal, and only coupled in that some ABIs require certain target features (e.g. you canâ€™t have a hard float ABI without enabling FP registers). Target features are a per-function choice, while the ABI should be per-module.&lt;/p&gt;
    &lt;p&gt;Some of the newer architectures like Loongarch and RISC-V actually have proper ABI design, but most of the older ones donâ€™t. For example, itâ€™s currently not possible to target AArch64 with a soft float ABI but hard float implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Builtins / libcalls&lt;/head&gt;
    &lt;p&gt;Somewhat related to this is the handling of compiler builtins/libcalls, which are auxiliary functions that the compiler may emit for operations that are not natively supported by the target. This covers both libcalls provided by libc (or libm), and builtins provided by compiler runtime libraries like libgcc, compiler-rt or compiler-builtins.&lt;/p&gt;
    &lt;p&gt;There are two sources of truth for this, TargetLibraryInfo (TLI) and RuntimeLibcalls. The former is used by the middle-end, primarily to recognize and optimize C library calls (this mostly covers only libc, but not libgcc). The latter is used by the backend, primarily to determine which libcalls may be emitted by the compiler and how they are spelled (this covers libgcc, and the subset of libc covered by LLVM intrinsics).&lt;/p&gt;
    &lt;p&gt;A problem with RuntimeLibcalls is that it currently largely works off only the target triple, which means that we have to make â€œlowest common denominatorâ€ assumptions about which libcalls are available, where the lowest common denominator is usually libgcc. If &lt;code&gt;--rtlib=compiler-rt&lt;/code&gt; is used, LLVM does not actually know about that, and cannot make use of functions that are in compiler-rt but not libgcc.&lt;/p&gt;
    &lt;p&gt;This also means that weâ€™re missing a customization point for other runtime libraries. For example, there is no way for Rust to say that it provides f128 suffix libcalls via compiler-builtins, overriding target-specific naming and availability assumptions based on which type &lt;code&gt;long double&lt;/code&gt; in C maps to.&lt;/p&gt;
    &lt;p&gt;There is a lot of ongoing work in this area (by arsenm), so the situation here will hopefully improve in the near-ish future.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context / module dichotomy&lt;/head&gt;
    &lt;p&gt;LLVM has two high-level data holders. A module corresponds to a compilation unit (e.g. pre-LTO, a single file in C/C++). The LLVM context holds various â€œglobalâ€ data. Thereâ€™s usually one context per thread, and multiple modules can (in principle) use a single context.&lt;/p&gt;
    &lt;p&gt;Things like functions and globals go into the module, while constants and types go into the context. The module also contains a data layout, which provides important type layout information like â€œhow wide is a pointerâ€.&lt;/p&gt;
    &lt;p&gt;The fact that constants and types do not have access to the data layout is a constant source of friction. If you have a type, you cannot reliably tell its size without threading an extra parameter through everything. We have subsystems (like ConstantFold vs. ConstantFolding) that are separated entirely by whether data layout is available or not.&lt;/p&gt;
    &lt;p&gt;At the same time, I feel like this split is not actually buying us a lot. Having shared types and constants is somewhat convenient when it comes to module linking, because they can be directly shared, but I think performing explicit remapping in that one place would be better than having complexity everywhere else. Additionally, this would also allow cross-context linking, which is currently only possible by going through a bitcode roundtrip. In theory, the context could also allow some memory reuse when compiling multiple modules, but I think in practice there is usually a one-to-one correspondence between those.&lt;/p&gt;
    &lt;head rend="h3"&gt;LICM register pressure&lt;/head&gt;
    &lt;p&gt;This is getting a bit down in the weeds, but Iâ€™ll mention it anyway due to how often Iâ€™ve run across this in recent times.&lt;/p&gt;
    &lt;p&gt;LLVM considers loop invariant code motion (LICM) to be a canonicalization transform. This means that we always hoist instructions out of loops, without any target specific cost modelling. However, LICM can increase the live ranges of values, which can increase register pressure, which can lead to a large amount of spills and reloads.&lt;/p&gt;
    &lt;p&gt;The general philosophy behind this is that LICM hoists everything, all middle-end transforms can work with nicely loop invariant instructions, and then instructions will get sunk back into the loop by the backend, which can precisely model register pressure.&lt;/p&gt;
    &lt;p&gt;Exceptâ€¦ that second part doesnâ€™t actually happen. I believe that (for non-PGO builds) instructions only get sunk back into loops either through rematerialization in the register allocator, or specialized sinking (typically of addressing modes), but for anything not falling into those buckets, no attempt to sink into loops in order to reduce register pressure is made.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other&lt;/head&gt;
    &lt;p&gt;This list is not exhaustive. Thereâ€™s more I could mention, but weâ€™d get into increasingly narrow territory. I hope I covered most of the more important things â€“ please do let me know what I missed!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;If youâ€™re not concerned with overall project health, the primary value of reviews is reciprocity. People are more likely to review your PR, if you reviewed theirs. â†©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The way Rust reconciles this is via a combination of â€œrollupsâ€ (where multiple PRs are merged as a batch, using human curation), and a substantially different contribution model. Where LLVM favors sequences of small PRs that do only one thing (and get squash merged), Rust favors large PRs with many commits (which do not get squashed). As getting an approved Rust PR merged usually takes multiple days due to bors, having large PRs is pretty much required to get anything done. This is not necessarily bad, just very different from what LLVM does right now. â†©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.npopov.com/2026/01/11/LLVM-The-bad-parts.html"/><published>2026-01-12T14:18:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589279</id><title>Keychron's Nape Pro turns your keyboard into a laptopâ€‘style trackball rig</title><updated>2026-01-12T18:17:45.979386+00:00</updated><content>&lt;doc fingerprint="edf51109f96389cc"&gt;
  &lt;main&gt;
    &lt;p&gt;Most desktop setups still assume your mouse lives somewhere off to the right, waiting for you to break posture and reach across half the desk. Keychronâ€™s new Nape Pro asks a different question: what if the pointing device simply came to meet your hands instead? Built as a slim bar with a 25 mm thumb trackball, six buttons, and a scroll wheel, it nestles right up against your favorite keyboard and behaves like a precision laptop pointing system for people who refuse to give up their mechanical boards.&lt;/p&gt;
    &lt;p&gt;Slide it to the side of the keyboard and the personality changes completely. Nape Pro turns into a compact, wireless trackball with full macro pad ambitions, complete with layers, shortcuts, and ZMK powered customization. It is less a mouse replacement and more a modular control surface that just happens to move your cursor, wherever you decide to park it.&lt;/p&gt;
    &lt;p&gt;Designers: Keychron &amp;amp; Gizmodo Japan&lt;/p&gt;
    &lt;p&gt;Seeing it here at the Keychron booth, tucked under a Q1 Pro, the immediate impression is how little space it occupies. The whole unit is only 135.2 mm long and 34.7 mm wide, so it fits neatly within the footprint of a standard tenkeyless board without feeling like an afterthought. They are using quiet Huano micro switches for the six buttons, which makes sense for a device meant to live right under your palms where an accidental loud click would be infuriating. The 25 mm ball is smaller than what you would find on a Kensington Expert, but it feels responsive enough for quick navigation. It is clearly designed for thumb operation, keeping your fingers on the home row and eliminating that constant, inefficient travel between keyboard and mouse.&lt;/p&gt;
    &lt;p&gt;The real cleverness, though, is not in the hardware itself but in the chameleon-like software and orientation system. They call it OctaShift, which basically means the device knows how it is positioned and can remap its functions accordingly. The two buttons at the very ends, M1 and M2, are the easiest to hit in any orientation, so they naturally become your primary clicks whether the Nape Pro is horizontal, vertical, or angled. This flexibility is what separates it from a simple add-on. It is a tool that adapts to your workflow, whether you are a writer who wants to scroll with a thumb or a video editor who needs a dedicated shuttle wheel and macro pad next to their main mouse.&lt;/p&gt;
    &lt;p&gt;Under the hood, it is running on a Realtek chip with a 1 kHz polling rate and a PixArt PAW3222 sensor, so the performance is on par with a decent wireless gaming mouse. Connectivity is handled via Bluetooth, a 2.4 GHz dongle, or a simple USB-C cable. What really caught my attention was the commitment to the enthusiast community. The firmware is ZMK, a popular open-source platform in the custom keyboard world, and Keychron plans to release the 3D files for the case. This is not a closed ecosystem. It is an invitation for users to tinker, to print their own angled stands, custom button caps, or even entirely new shells.&lt;/p&gt;
    &lt;p&gt;This open approach feels like the whole point. The Nape Pro is not just for people who want a trackball; it is for people who build their own keyboards, flash their own firmware, and spend hours fine-tuning their desk setup for optimal efficiency. It bridges the gap between high-end custom keyboards and generic pointing devices. It acknowledges that for a certain type of user, the mouse is the last un-programmable, inflexible part of their workflow. By making a pointing device that is as customizable and community-focused as the keyboards it is designed to sit next to, Keychron has built something genuinely new.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.yankodesign.com/2026/01/08/keychrons-nape-pro-turns-your-mechanical-keyboard-into-a-laptop-style-trackball-rig-hands-on-at-ces-2026/"/><published>2026-01-12T14:50:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589489</id><title>Statement from Federal Reserve Chair</title><updated>2026-01-12T18:17:45.825566+00:00</updated><content>&lt;doc fingerprint="bf07bb5799a4b599"&gt;
  &lt;main&gt;
    &lt;p&gt;Good evening.&lt;/p&gt;
    &lt;p&gt;On Friday, the Department of Justice served the Federal Reserve with grand jury subpoenas, threatening a criminal indictment related to my testimony before the Senate Banking Committee last June. That testimony concerned in part a multi-year project to renovate historic Federal Reserve office buildings.&lt;/p&gt;
    &lt;p&gt;I have deep respect for the rule of law and for accountability in our democracy. No oneÃ¢certainly not the chair of the Federal ReserveÃ¢is above the law. But this unprecedented action should be seen in the broader context of the administration's threats and ongoing pressure.&lt;/p&gt;
    &lt;p&gt;This new threat is not about my testimony last June or about the renovation of the Federal Reserve buildings. It is not about Congress's oversight role; the Fed through testimony and other public disclosures made every effort to keep Congress informed about the renovation project. Those are pretexts. The threat of criminal charges is a consequence of the Federal Reserve setting interest rates based on our best assessment of what will serve the public, rather than following the preferences of the President.&lt;/p&gt;
    &lt;p&gt;This is about whether the Fed will be able to continue to set interest rates based on evidence and economic conditionsÃ¢or whether instead monetary policy will be directed by political pressure or intimidation.&lt;/p&gt;
    &lt;p&gt;I have served at the Federal Reserve under four administrations, Republicans and Democrats alike. In every case, I have carried out my duties without political fear or favor, focused solely on our mandate of price stability and maximum employment. Public service sometimes requires standing firm in the face of threats. I will continue to do the job the Senate confirmed me to do, with integrity and a commitment to serving the American people.&lt;/p&gt;
    &lt;p&gt;Thank you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.federalreserve.gov/newsevents/speech/powell20260111a.htm?mod=ANLink"/><published>2026-01-12T15:05:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589543</id><title>Computers that used to be human</title><updated>2026-01-12T18:17:45.581756+00:00</updated><content>&lt;doc fingerprint="177131c974e79170"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Computers that used to be human&lt;/head&gt;
    &lt;p&gt;One common complaint about computers is that theyâ€™re too hard to understand. Check out this lamentation related to the British East India Company:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The Controller and the Computer of the Duties on unrated India Goods attend the Sales of the East India Company, and take an Account of the Goods sold, and the Price; this Account is agreed with the Company, then the Controller and Computer cast the Duties, and the Receiver enters them upon the Warrant.&lt;/p&gt;&lt;p&gt;The Computation is become so difficult, from the Number of Branches of Duties, and from the various Rules now necessarily made use of in casting them, that very few Persons can be found capable of transacting this Business, or of acquiring the Means of doing it.&lt;/p&gt;&lt;p&gt;The Examination of Mr. William Richardson; taken upon Oath, the 3d of December 1784&lt;/p&gt;&lt;lb/&gt;Journals of the House of Commons (1785)&lt;/quote&gt;
    &lt;p&gt;Wait, what? In 1785, no British civil servant was pulling up Excel to do the books on colonialism (citation needed). Even Charles Babbage was still a twinkle in his parentsâ€™ eyes.&lt;/p&gt;
    &lt;p&gt;In the quoted passage, the Computer is a human. You used to be able to be a professional Computer, calculating important sums for your employer.&lt;/p&gt;
    &lt;p&gt;And just like computers today, those human Computers could be hard to understand. Imagine being a tax auditor or accountant by hand - no wonder this particular Computer, for taxes, was a very difficult job to hire for.&lt;/p&gt;
    &lt;p&gt;â€”&lt;/p&gt;
    &lt;p&gt;Computers were real people. They had names. Sometimes being a Computer was a quick gig between others, as this US Naval Observatory report shows:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The following members of the Observatory force have been attached to the computing division at some time during the year ending June 30, 1903:&lt;/p&gt;&lt;p&gt;Computer William M. Brown&lt;/p&gt;&lt;p&gt;Computer John C. Hammond&lt;/p&gt;&lt;p&gt;Computer Everett I. Yowell, for six months.&lt;/p&gt;&lt;p&gt;Computer Herbert R. Morgan.&lt;/p&gt;&lt;p&gt;Computer Eleanor A. Lamson&lt;/p&gt;&lt;p&gt;Miscellaneous Computer Clara M. Upton, for two months.&lt;/p&gt;&lt;p&gt;Miscellaneous Computer Arthur B. Turner, for six weeks.&lt;/p&gt;&lt;p&gt;Miscellaneous Computer Lelia J. Harvie, for six weeks.&lt;/p&gt;&lt;p&gt;Miscellaneous Computer Etta M. Eaton, for eight months.&lt;/p&gt;&lt;p&gt;Miscellaneous Computer John R. Benton, for two months.&lt;/p&gt;&lt;p&gt;Miscellaneous Computer Ella A. Merritt, for seven months.&lt;/p&gt;&lt;p&gt;Miscellaneous Computer Samuel F. Rixey, for four months.&lt;/p&gt;&lt;p&gt;Miscellaneous Computer Delonza T. Wilson, for five months.&lt;/p&gt;&lt;p&gt;Miscellaneous Computer Charles E. Yost, for two weeks.&lt;/p&gt;&lt;p&gt;[...]&lt;/p&gt;&lt;p&gt;Very respectfully,&lt;/p&gt;&lt;lb/&gt;W.S. Eichelberger&lt;lb/&gt;Professor of Mathematics, U.S. Navy, in Charge.&lt;/quote&gt;
    &lt;p&gt;As popularly retold in Hidden Figures, human computers co-existed with electronic computers as late as the 1960s - real people, usually women, would perform calculations by hand and with tools like lookup tables, slide rules, and mechanical calculators.&lt;/p&gt;
    &lt;p&gt;Of course, Calculators also used to be people performing similar tasks. From the 1656 Glossographia, one of the earliest English dictionaries:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Calculate (calculo): to cast accounts, to reckon.&lt;/p&gt;
      &lt;p&gt;Compotist (compotista): a caster of accounts, a Reckoner, or Calculator.&lt;/p&gt;
      &lt;p&gt;Thomas Blount: Glossographia, or, A dictionary interpreting all such hard words of whatsoever language now used in our refined English tongue with etymologies, definitions and historical observations on the same : also the terms of divinity, law, physick, mathematicks and other arts and sciences explicated via the University of Michigan Libraryâ€™s Early English Books Online&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Besides writing that hilariously-long subtitle, Thomas Blount believes that â€œreckonâ€ is not a â€œhard wordâ€, so itâ€™s not included in this dictionary - too obvious, another example of â€œEveryone can see what a horse isâ€. In fact, I had a hard time finding any dictionary of this era that defined â€œreckonâ€, so hereâ€™s several entries from the 1604 A Table Alphabeticall that should help:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[fr] account, reckon&lt;/p&gt;
      &lt;p&gt;computation, an account or reckoning&lt;/p&gt;
      &lt;p&gt;impute, reckon, or assigne, blame, or to lay to ones charge&lt;/p&gt;
      &lt;p&gt;register, kalender, a reckoning booke&lt;/p&gt;
      &lt;p&gt;Robert Cawdrey: A Table Alphabeticall (1604), website edited by Raymond G. Siemens&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;At this time, â€œreckonâ€ had a meaning closer to â€œcomputeâ€ or â€œcalculateâ€, with numerical or financial precision (this is where the phrase â€œthe day of reckoningâ€ comes from). Thatâ€™s quite distant from its modern meaning of a casual guess or tentative belief.&lt;/p&gt;
    &lt;p&gt;â€”&lt;/p&gt;
    &lt;p&gt;Now, doing real research takes a lot of time. The rest of this post is dedicated to making up stupid etymology because I think itâ€™s funny:&lt;/p&gt;
    &lt;p&gt;Ruler as straightedge derives from ruler like the monarchy, since the king is the one who has to draw the line.&lt;/p&gt;
    &lt;p&gt;Protractor used to be someone who made meetings drag on longer by considering new angles (some say this role still exists today).&lt;/p&gt;
    &lt;p&gt;Washer-dryers used to be the same person, but it was divided into two distinct roles thanks to increasing specialization and powerful labor unions; never the twain shall meet again.&lt;/p&gt;
    &lt;p&gt;Liquor is too obscene to define here. I hardly even know her!&lt;/p&gt;
    &lt;p&gt;And if you believe the hype, Programmer might be next in line for tool-dom, like the Calculators and Computers of yore. Come to think of it, those folks can be hard to understand, too.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://digitalseams.com/blog/computers-that-used-to-be-human"/><published>2026-01-12T15:09:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589576</id><title>Computational complexity of schema-guided document extraction</title><updated>2026-01-12T18:17:45.267559+00:00</updated><content>&lt;doc fingerprint="63520a9b06f0caf2"&gt;
  &lt;main&gt;
    &lt;p&gt;When we started building Pulse, we assumed that structured outputs had solved the document extraction problem. Define a JSON schema, point an LLM at your documents, and get clean data back. Every major API provider now supports it. OpenAI, Anthropic, Google, and a growing ecosystem of open source tools like Outlines and XGrammar have made constrained generation accessible to anyone with an API key.&lt;/p&gt;
    &lt;p&gt;So our research team internally ran some evals - used a relatively simple document layout with bank statements and invoices, defined a schema, and watched it work. Then we tried it on ten thousand more complex documents with alternate currency markers, faxed documents with unwired tables, nested line items, conditional fields, and variable-length arrays. That's when things got interesting.&lt;/p&gt;
    &lt;p&gt;The short version: enforcing structured outputs during generation is not free, and for the kinds of complex, nested schemas that real document extraction requires, naive approaches either blow up in compute or silently degrade extraction quality.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Constrained Decoding Actually Works&lt;/head&gt;
    &lt;p&gt;At each generation step, the model produces a probability distribution over its entire vocabulary (128,000+ tokens in OSS models like Llama). Constrained decoding works by masking out tokens that would violate the schema, setting their probabilities to zero before sampling. The model can only generate tokens that keep the output on a valid path through the grammar.&lt;/p&gt;
    &lt;p&gt;For simple constraints like regular expressions, this is efficient. The Outlines library showed you can precompute a finite state machine (FSM) from a regex and use it to generate token masks in O(1) time per step. For regular languages, the set of valid next tokens depends only on the current FSM state, not on the history of how you got there.&lt;/p&gt;
    &lt;p&gt;While a specific JSON schema can be mapped to a regular language, the recursive nature of nested objects and arrays makes it behave more like a context-free grammar in terms of parser complexity.&lt;/p&gt;
    &lt;head rend="h2"&gt;The State Explosion Problem&lt;/head&gt;
    &lt;p&gt;The XGrammar paper introduced a clever optimization. Although you can't precompute masks for every state, you can categorize tokens into two sets:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context-independent tokens: validity can be determined by the current position in the PDA, ignoring the stack. For most grammars, this covers the vast majority of the vocabulary.&lt;/item&gt;
      &lt;item&gt;Context-dependent tokens: validity depends on the full stack state. These require runtime verification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By precomputing masks for context-independent tokens and only doing runtime checks for the small context-dependent set, XGrammar achieves up to 100x speedup over naive approaches.&lt;/p&gt;
    &lt;p&gt;But even with this optimization, complex grammars create problems. When the grammar is non-deterministic (multiple valid ways to continue parsing at a given point), the parser must maintain multiple parallel stack states. Each ambiguous choice can double the number of states being tracked.&lt;/p&gt;
    &lt;p&gt;The classic result from automata theory (very interesting course FYI!) is that converting a nondeterministic finite automaton (NFA) to a deterministic one (DFA) can produce 2^n states from an n-state NFA. For context-free grammars, the situation is worse because the stack adds another dimension of complexity.&lt;/p&gt;
    &lt;p&gt;What does this mean for document extraction? The schemas we care about (invoices, contracts, medical records, financial statements) tend to have exactly the properties that make constrained decoding expensive: deep nesting, optional fields that create ambiguity, arrays of complex objects, and union types.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Quality-Constraint Tradeoff&lt;/head&gt;
    &lt;p&gt;Here's the part that surprised us most. Constrained decoding doesn't just cost compute. It can hurt extraction accuracy.&lt;/p&gt;
    &lt;p&gt;The paper "Let Me Speak Freely?" systematically studied this effect. They found that stricter format constraints lead to greater performance degradation on reasoning tasks. JSON-mode significantly reduced accuracy on tasks like GSM8K math problems compared to free-form generation.&lt;/p&gt;
    &lt;p&gt;Why? The model isn't "aware" of the constraints during the forward pass. It computes logits as usual, and then the constrained decoder masks out invalid tokens after the fact. When the model wants to output one token but is forced to choose another, you get a distribution shift. The decoder may force a low-probability token, introducing noise into the generation process.&lt;/p&gt;
    &lt;p&gt;For document extraction, this creates a paradox:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tighter schemas give you more reliable parsing and cleaner downstream data pipelines&lt;/item&gt;
      &lt;item&gt;But tighter schemas can degrade the model's ability to correctly extract information in the first place&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What We're Exploring at Pulse&lt;/head&gt;
    &lt;p&gt;Document extraction at scale requires solving three interconnected problems: schema design that captures document structure without over-constraining the model, efficient enforcement that doesn't kill throughput when processing millions of pages, and maintaining extraction accuracy despite the overhead of constrained generation.&lt;/p&gt;
    &lt;p&gt;Schema Complexity Analysis. Can we predict extraction difficulty from schema structure before running inference? We're building tools to analyze schemas and estimate expected compilation time, likelihood of state explosion, and risk of quality degradation based on constraint density. The goal is feedback at schema design time, before you burn GPU hours on a suboptimal configuration.&lt;/p&gt;
    &lt;p&gt;Adaptive Constraint Strategies. Not all documents need the same level of constraint strictness. A highly templated form might benefit from tight constraints. A free-form contract paragraph might need looser constraints to let the model reason about what information is present. We're exploring hybrid approaches: generate with minimal constraints, then use a second pass to restructure into the target schema. This "NL-to-Format" strategy often outperforms direct constrained generation on reasoning-heavy tasks.&lt;/p&gt;
    &lt;p&gt;Grammar Compilation Optimization. When multiple schemas share common substructures (dates, currencies, addresses), you can compute token masks once and reuse them. We're building a schema registry that identifies shared substructures across document types and pre-compiles them into reusable grammar fragments.&lt;/p&gt;
    &lt;p&gt;Confidence-Aware Extraction. When constrained decoding forces a low-probability token, that's a signal. We're experimenting with tracking perplexity during generation and flagging extractions where the model was "fighting" the constraints. These high-perplexity extractions can be routed to human review or processed with alternative strategies.&lt;/p&gt;
    &lt;p&gt;Schema-guided extraction sits at the intersection of formal language theory, LLM inference optimization, and practical document understanding. The problems are hard. But the payoff for solving them-reliable structured data extraction from the messy reality of business documents-is substantial.&lt;/p&gt;
    &lt;p&gt;If this is the kind of problem that excites you, we're hiring. Reach out.&lt;/p&gt;
    &lt;p&gt;Related: Why LLMs Suck at OCR Ã¢Â¢ Advanced Spreadsheet Parsing: General Availability&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.runpulse.com/blog/computational-complexity-of-schema"/><published>2026-01-12T15:13:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589658</id><title>Date is out, Temporal is in</title><updated>2026-01-12T18:17:44.872046+00:00</updated><content>&lt;doc fingerprint="8a417811e99f2f1c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Date is out, Temporal is in&lt;/head&gt;
    &lt;p&gt;Time makes fools of us all, and JavaScript is no slouch in that department either. Honestly, Iâ€™ve never minded the latter much â€” in fact, if youâ€™ve taken JavaScript for Everyone or tuned into the newsletter, you already know that I largely enjoy JavaScriptâ€™s little quirks, believe it or not.&lt;/p&gt;
    &lt;p&gt;I like when you can see the seams; I like how, for as formal and iron-clad as the ES-262 specification might seem, you can still see all the good and bad decisions made by the hundreds of people whoâ€™ve been building the language in mid-flight, if you know where to look. JavaScript has character. Sure, it doesnâ€™t necessarily do everything exactly the way one might expect, but yâ€™know, if you ask me, JavaScript has a real charm once you get to know it!&lt;/p&gt;
    &lt;p&gt;Thereâ€™s one part of the language where that immediately falls apart for me, though.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;// Numeric months are zero-indexed, but years and days are not: console.log( new Date(2026, 1, 1) ); // Result: Date Sun Feb 01 2026 00:00:00 GMT-0500 (Eastern Standard Time)&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;Date&lt;/code&gt; constructor.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;// A numeric string between 32 and 49 is assumed to be in the 2000s: console.log( new Date( "49" ) ); // Result: Date Fri Jan 01 2049 00:00:00 GMT-0500 (Eastern Standard Time) // A numeric string between 33 and 99 is assumed to be in the 1900s: console.log( new Date( "99" ) ); // Result: Date Fri Jan 01 1999 00:00:00 GMT-0500 (Eastern Standard Time) // ...But 100 and up start from year zero: console.log( new Date( "100" ) ); // Result: Date Fri Jan 01 0100 00:00:00 GMT-0456 (Eastern Standard Time)&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;I dislike &lt;code&gt;Date&lt;/code&gt; immensely.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;// A string-based date works the way you might expect: console.log( new Date( "2026/1/2" ) ); // Result: Date Fri Jan 02 2026 00:00:00 GMT-0500 (Eastern Standard Time) // A leading zero on the month? No problem; one is one, right? console.log( new Date( "2026/02/2" ) ); // Result: Date Mon Feb 02 2026 00:00:00 GMT-0500 (Eastern Standard Time) // Slightly different formatting? Sure! console.log( new Date( "2026-02-2" ) ); // Result: Date Mon Feb 02 2026 00:00:00 GMT-0500 (Eastern Standard Time) // A leading zero on the day? Of course; why wouldn't it work? console.log( new Date('2026/01/02') ); // Result: Date Fri Jan 02 2026 00:00:00 GMT-0500 (Eastern Standard Time) // Unless, of course, you separate the year, month, and date with hyphens. // Then it gets the _day_ wrong. console.log( new Date('2026-01-02') ); // Result: Date Thu Jan 01 2026 19:00:00 GMT-0500 (Eastern Standard Time)&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;Date&lt;/code&gt; sucks. It was hastily and shamelessly copied off of Javaâ€™s homework in the car on the way to school and it got all the same answers wrong, right down to the name at the top of the page: &lt;code&gt;Date&lt;/code&gt; doesnâ€™t represent a date, it represents a time. Internally, dates are stored as number values called time values: Unix timestamps, divided into 1,000 milliseconds â€” which, okay, yes, a Unix time does also necessarily imply a date, sure, but still: Date represents a time, from which you can infer a date. Gross.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;// Unix timestamp for Monday, December 4, 1995 12:00:00 AM GMT-05 (the day JavaScript was announced): const timestamp = 818053200; console.log( new Date( timestamp * 1000 ) ); // Result: Date Mon Dec 04 1995 00:00:00 GMT-0500 (Eastern Standard Time)&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Words like â€œdateâ€ and â€œtimeâ€ mean things, but, sure â€” whatever, JavaScript.&lt;/p&gt;
    &lt;p&gt;Java deprecated their &lt;code&gt;Date&lt;/code&gt; way back in 1997, only a few years after JavaScriptâ€™s &lt;code&gt;Date&lt;/code&gt; was turned loose on the unsuspecting world; meanwhile, weâ€™ve been saddled with this mess ever since. Itâ€™s wildly inconsistent when it comes to parsing dates, as youâ€™ve seen so far here. It has no sense of time zones beyond the local one and GMT, which is not ideal where â€œworld-wideâ€ is right there in the webâ€™s name â€” and speaking-of, &lt;code&gt;Date&lt;/code&gt; only respects the Gregorian calendar model. It wholesale does not understand the concept of daylight savings time, whichâ€” I mean, okay, yeah, samesies, but Iâ€™m not made of computers. All these shortcomings make it exceptionally common to use a third-party library dedicated to working around it all, some of which are absolutely massive; a performance drain that has done real and measurable damage to the web.&lt;/p&gt;
    &lt;p&gt;None of these are my major issue with &lt;code&gt;Date&lt;/code&gt;. My complaint is about more than parsing or syntax or â€œdeveloper ergonomicsâ€ or the web-wide performance impact of wholly necessary workarounds or even the definition of the word â€œdate.â€ My issue with &lt;code&gt;Date&lt;/code&gt; is soul-deep. My problem with &lt;code&gt;Date&lt;/code&gt; is that using it means deviating from the fundamental nature of time itself.&lt;/p&gt;
    &lt;p&gt;All JavaScriptâ€™s primitives values are immutable, meaning that the values themselves cannot be changed. The number value &lt;code&gt;3&lt;/code&gt; can never represent anything but the concept of â€œthreeâ€ â€” you canâ€™t make &lt;code&gt;true&lt;/code&gt; mean anything other than â€œtrue.â€ These are values with concrete, iron-clad, real-world meanings. We know what three is. It canâ€™t be some other non-three thing. These immutable data types are stored by value, meaning that a variable that represents the number value &lt;code&gt;3&lt;/code&gt; effectively â€œcontainsâ€ â€” and thus behaves as â€” the number value &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When an immutable value is assigned to a variable, the JavaScript engine creates a copy of that value and stores the copy in memory:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theNumber = 3; console.log( theNumber ); // Result: 3&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;This fits the common mental model for â€œa variableâ€ just fine: &lt;code&gt;theNumber&lt;/code&gt; â€œcontainsâ€ &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When we initialize &lt;code&gt;theOtherNumber&lt;/code&gt; with the value bound to &lt;code&gt;theNumber&lt;/code&gt;, that mental model holds: once again a &lt;code&gt;3&lt;/code&gt; is created and stored in memory. &lt;code&gt;theOtherNumber&lt;/code&gt; can now be thought of as containing its own discrete &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theNumber = 3; const theOtherNumber = theNumber; console.log( theOtherNumber ); // Result: 3;&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;The value of &lt;code&gt;theNumber&lt;/code&gt; isnâ€™t changed when we alter the value associated with &lt;code&gt;theOtherNumber&lt;/code&gt;, of course â€” again, weâ€™re working with two discrete instances of &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theNumber = 3; let theOtherNumber = theNumber; theOtherNumber = 5; console.log( theOtherNumber ); // Result: 5; console.log( theNumber ); // Result: 3&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;When you change the value bound to &lt;code&gt;theOtherNumber&lt;/code&gt;, youâ€™re not changing the &lt;code&gt;3&lt;/code&gt;, youâ€™re creating a new, immutable number value and binding that in its place. Hence an error when you try to tinker with a variable declared using &lt;code&gt;const&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theNumber = 3; theNumber = 5; // Result: Uncaught TypeError: invalid assignment to const 'theNumber'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;You canâ€™t change the binding of a &lt;code&gt;const&lt;/code&gt;, and you definitely canâ€™t alter the meaning of &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Data types that can be changed after theyâ€™re created are mutable, meaning that the data value itself can be altered. Object values â€” any non-primitive value, like an array, map, or set â€” are mutable.&lt;/p&gt;
    &lt;p&gt;Variables (and object properties, function parameters, and elements in an array, set, or map) canâ€™t â€œcontainâ€ an object, the way we might think of &lt;code&gt;theNumber&lt;/code&gt; in the example above as â€œcontainingâ€ &lt;code&gt;3&lt;/code&gt;. A variable can contain either a primitive value or a reference value, the latter of which is a pointer to that objectâ€™s stored location in memory. When you assign an object to a variable, instead of creating a copy of that object, the identifier represents a reference to the objectâ€™s stored position in memory. Thatâ€™s why an object bound to a variable declared with &lt;code&gt;const&lt;/code&gt; can still be altered: the reference value canâ€™t be changed, but the values of the object can:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theObject = { theValue : 3 }; theObject.theValue++; console.log( theObject.theValue ); // Result: 4&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;You still canâ€™t change the binding of a &lt;code&gt;const&lt;/code&gt;, but you can alter the object that binding references.&lt;/p&gt;
    &lt;p&gt;When a reference value is assigned from one variable to another, the JavaScript engine creates a copy of that reference value â€” not the object value itself, the way a discrete copy is made of a primitive value. Both identifiers point to the same object in memory â€” any changes made to that object by way of one reference will be reflected by the others, because theyâ€™re all referencing the same thing:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theObject = { theValue : 3 }; const theOtherObj = theObject; theOtherObj.theValue++; console.log( theOtherObj.theValue ); // Result: 4 console.log( theObject.theValue ); // Result: 4&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is what gets me about JavaScriptâ€™s date handling. Despite representing â€œpoint to it on a calendarâ€ values, JavaScriptâ€™s date values are mutable â€” &lt;code&gt;Date&lt;/code&gt; is a constructor, invoking a constructor with &lt;code&gt;new&lt;/code&gt; necessarily results in an object, and all objects are inherently mutable:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theDate = new Date(); console.log( typeof theDate ); // Result: object&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Even though â€œJanuary 1st, 2026â€ is as much an immutable real-world concept as â€œthreeâ€ or â€œtrue,â€ the only way we have of representing that date is a with a mutable data structure.&lt;/p&gt;
    &lt;p&gt;This also means that any variable initialized with an instance of the &lt;code&gt;Date&lt;/code&gt; constructor contains a reference value, pointing to a data value in memory that can be changed by way of any reference to that value:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const theDate = new Date(); console.log( theDate.toDateString() ); // Result: Tue Dec 30 2025 theDate.setMonth( 10 ); console.log( theDate.toDateString() ); // Result: Sun Nov 30 2025&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Again, weâ€™re going to breeze right over the fact that month &lt;code&gt;10&lt;/code&gt; is November.&lt;/p&gt;
    &lt;p&gt;So despite real-world dates having set-in-stone meanings, the process of interacting with an instance of &lt;code&gt;Date&lt;/code&gt; that represents that real-world value can mean altering that instance in ways we didnâ€™t necessarily intend:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = new Date(); const addDay = theDate =&amp;gt; { theDate.setDate( theDate.getDate() + 1 ); return theDate; }; console.log(`Today is ${ today.toLocaleDateString() }, tomorrow is ${ addDay( today ).toLocaleDateString() }.`); // Result: Today is 12/31/2025. Tomorrow is 1/1/2026.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Fine so far, right? Today is today, tomorrow is tomorrow; all is right in the world. Youâ€™d be forgiven for committing this to a codebase and moving on with your day. That is, unless we reordered the output slightly.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = new Date(); const addDay = theDate =&amp;gt; { theDate.setDate( theDate.getDate() + 1 ); return theDate; }; console.log(`Tomorrow will be ${ addDay( today ).toLocaleDateString() }. Today is ${ today.toLocaleDateString() }.`); // Result: Tomorrow will be 1/1/2026. Today is 1/1/2026.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;See what happened there? the variable &lt;code&gt;today&lt;/code&gt; represents a reference to the object created by &lt;code&gt;new Date()&lt;/code&gt;. When we provided &lt;code&gt;today&lt;/code&gt; as an argument to the &lt;code&gt;addDay&lt;/code&gt; function, the parameter &lt;code&gt;theDate&lt;/code&gt; now represents a copy of the reference value â€” not a copy of the value, but a second reference to the object that represents todayâ€™s date. When we manipulate that value to determine the date of the following day, weâ€™re manipulating the mutable object in memory, not an immutable copy â€” today becomes tomorrow, the falcon has a hard time hearing the falconer, the center starts to look a little iffy vis-a-vis â€œholding,â€ and so on.&lt;/p&gt;
    &lt;p&gt;Now, by this point you can probably tell that Iâ€™m not here to praise &lt;code&gt;Date&lt;/code&gt;, but what you might not expect is that Iâ€™m here to bury it. Thatâ€™s right: &lt;code&gt;Date&lt;/code&gt; is soon to be over, done, gone, as â€œdeprecatedâ€ as any part of the web platform can be â€” which is to say, â€œaround forever, but you shouldnâ€™t use it anymore, if you can avoid it.â€ Soon we will â€” at long last â€” have an object that replaces &lt;code&gt;Date&lt;/code&gt; wholesale: &lt;code&gt;Temporal&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Temporal is not a constructor, itâ€™s a namespace object&lt;/head&gt;
    &lt;p&gt;The sharp-eyed among you may have noticed that I said â€œan object that replaces &lt;code&gt;Date&lt;/code&gt;,â€ not â€œa constructor.â€ &lt;code&gt;Temporal&lt;/code&gt; is not a constructor, and your browserâ€™s developer console will tell you the same if you attempt to invoke it as one:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = new Temporal(); // Uncaught TypeError: Temporal is not a constructor&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;Temporal&lt;/code&gt; is a way better name for something that pertains to time, if you ask me.&lt;/p&gt;
    &lt;p&gt;Instead, &lt;code&gt;Temporal&lt;/code&gt; is a namespace object â€” an ordinary object made up of static properties and methods, like the &lt;code&gt;Math&lt;/code&gt; object:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;console.log( Temporal ); /* Result (expanded): Temporal { â€¦ } Duration: function Duration() Instant: function Instant() Now: Temporal.Now { â€¦ } PlainDate: function PlainDate() PlainDateTime: function PlainDateTime() PlainMonthDay: function PlainMonthDay() PlainTime: function PlainTime() PlainYearMonth: function PlainYearMonth() ZonedDateTime: function ZonedDateTime() Symbol(Symbol.toStringTag): "Temporal" */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;I find this immediately understandable compared to &lt;code&gt;Date&lt;/code&gt;.  The classes and namespaces objects that &lt;code&gt;Temporal&lt;/code&gt; contains allow you to calculate durations between two points in time, represent a point in time with or without time zone specificity, or access the current moment in time via the &lt;code&gt;Now&lt;/code&gt; property. &lt;code&gt;Temporal.Now&lt;/code&gt; references a namespace object containing properties and methods of its own:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;console.log( Temporal.Now ); /* Result (expanded): Temporal.Now { â€¦ } instant: function instant() plainDateISO: function plainDateISO() plainDateTimeISO: function plainDateTimeISO() plainTimeISO: function plainTimeISO() timeZoneId: function timeZoneId() zonedDateTimeISO: function zonedDateTimeISO() Symbol(Symbol.toStringTag): "Temporal.Now" &amp;lt;prototype&amp;gt;: Object { â€¦ } */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;Temporal&lt;/code&gt; gives us a sensible, plain-language way to grab todayâ€™s date, a la raggedy old  &lt;code&gt;Date&lt;/code&gt;: the &lt;code&gt;Now&lt;/code&gt; property contains a &lt;code&gt;plainDateISO()&lt;/code&gt; method. Since weâ€™re not specifying anything in the way of time zones (a thing we can do now, thanks to Temporal) that method gives us back todayâ€™s date in the current one â€” EST, in my case:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;console.log( Temporal.Now.plainDateISO() ); /* Result (expanded): Temporal.PlainDate 2025-12-31 &amp;lt;prototype&amp;gt;: Object { â€¦ } */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notice how &lt;code&gt;plainDateISO&lt;/code&gt; results in an already-formatted, date-only value? Stay tuned; thatâ€™ll come up again later.&lt;/p&gt;
    &lt;p&gt;â€”wait. That looks familiar:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const nowTemporal = Temporal.Now.plainDateISO(); const nowDate = new Date(); console.log( nowTemporal ); /* Result (expanded): Temporal.PlainDate 2025-12-31 &amp;lt;prototype&amp;gt;: Object { â€¦ } */ console.log( nowDate ); /* Result (expanded): Date Tue Dec 31 2025 11:05:52 GMT-0500 (Eastern Standard Time) &amp;lt;prototype&amp;gt;: Date.prototype { â€¦ } */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Could it be thatâ€”â€¦&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const rightNow = Temporal.Now.instant(); console.log( typeof rightNow ); // object&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, weâ€™re still working with a mutable object that represents the current date, I say in my spookiest voice, flashlight squarely beneath my chin. At a glance, this might not seem like it addresses my big complaint with &lt;code&gt;Date&lt;/code&gt; at all.&lt;/p&gt;
    &lt;p&gt;Well, weâ€™re kind of at the mercy of the nature of the language, here: dates represent complex real-world values, complex data necessitates complex data structures, and for JavaScript, that means objects. The difference is in how we interact with these Temporal objects, as compared to instances of &lt;code&gt;Date&lt;/code&gt;, and â€” as is so often the case â€” the magic is in the prototype chain:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const nowTemporal = Temporal.Now.plainDateISO(); console.log( nowTemporal.__proto__ ); /* Result (expanded): Object { â€¦ } add: function add() calendarId: &amp;gt;&amp;gt; constructor: function PlainDate() day: &amp;gt;&amp;gt; dayOfWeek: &amp;gt;&amp;gt; dayOfYear: &amp;gt;&amp;gt; daysInMonth: &amp;gt;&amp;gt; daysInWeek: &amp;gt;&amp;gt; daysInYear: &amp;gt;&amp;gt; equals: function equals() era: &amp;gt;&amp;gt; eraYear: &amp;gt;&amp;gt; inLeapYear: &amp;gt;&amp;gt; month: &amp;gt;&amp;gt; monthCode: &amp;gt;&amp;gt; monthsInYear: &amp;gt;&amp;gt; since: function since() subtract: function subtract() toJSON: function toJSON() toLocaleString: function toLocaleString() toPlainDateTime: function toPlainDateTime() toPlainMonthDay: function toPlainMonthDay() toPlainYearMonth: function toPlainYearMonth() toString: function toString() toZonedDateTime: function toZonedDateTime() until: function until() valueOf: function valueOf() weekOfYear: &amp;gt;&amp;gt; with: function with() withCalendar: function withCalendar() year: &amp;gt;&amp;gt; yearOfWeek: &amp;gt;&amp;gt; Symbol(Symbol.toStringTag): "Temporal.PlainDate" &amp;lt;get calendarId()&amp;gt;: function calendarId() &amp;lt;get day()&amp;gt;: function day() &amp;lt;get dayOfWeek()&amp;gt;: function dayOfWeek() &amp;lt;get dayOfYear()&amp;gt;: function dayOfYear() &amp;lt;get daysInMonth()&amp;gt;: function daysInMonth() &amp;lt;get daysInWeek()&amp;gt;: function daysInWeek() &amp;lt;get daysInYear()&amp;gt;: function daysInYear() &amp;lt;get era()&amp;gt;: function era() &amp;lt;get eraYear()&amp;gt;: function eraYear() &amp;lt;get inLeapYear()&amp;gt;: function inLeapYear() */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Right away youâ€™ll notice that there are a number of methods and properties devoted to accessing, formatting, and manipulating the details of the Temporal object weâ€™re working with. No big surprises there â€” it means a little bit of a learning curve, sure, but nothing an occasional trip over to MDN couldnâ€™t solve, and they all more-or-less do what they say on their respective tins. The big difference from working with &lt;code&gt;Date&lt;/code&gt; is how they do so, at a fundamental level:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const nowTemporal = Temporal.Now.plainDateISO(); // Current local date: console.log( nowTemporal ); /* Result (expanded): Temporal.PlainDate 2025-12-30 &amp;lt;prototype&amp;gt;: Object { â€¦ } */ // Current local year: console.log( nowTemporal.year ); // Result: 2025 // Current local date and time: console.log( nowTemporal.toPlainDateTime() ); /* Result (expanded): Temporal.PlainDateTime 2025-12-30T00:00:00 &amp;lt;prototype&amp;gt;: Object { â€¦ } */ // Specify that this date represents the Europe/London time zone: console.log( nowTemporal.toZonedDateTime( "Europe/London" ) ); /* Result (expanded): Temporal.ZonedDateTime 2025-12-30T00:00:00+00:00[Europe/London] &amp;lt;prototype&amp;gt;: Object { â€¦ } */ // Add a day to this date: console.log( nowTemporal.add({ days: 1 }) ); /* Temporal.PlainDate 2025-12-31 &amp;lt;prototype&amp;gt;: Object { â€¦ } */ // Add one month and one day to this date, and subtract two years: console.log( nowTemporal.add({ months: 1, days: 1 }).subtract({ years: 2 }) ); /* Temporal.PlainDate 2024-01-31 &amp;lt;prototype&amp;gt;: Object { â€¦ } */ console.log( nowTemporal ); /* Result (expanded): Temporal.PlainDate 2025-12-30 &amp;lt;prototype&amp;gt;: Object { â€¦ } */&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notice how none of these transformations required us to manually spin up any new objects, and that the value of the object referenced by &lt;code&gt;nowTemporal&lt;/code&gt; remains unchanged? Unlike &lt;code&gt;Date&lt;/code&gt;,  the methods we use to interact with a Temporal object result in new Temporal objects, rather than requiring us to use them in the context of a new instance or to modify the instance weâ€™re working with â€” which is how weâ€™re able to chain the &lt;code&gt;add&lt;/code&gt; and &lt;code&gt;subtract&lt;/code&gt; methods together in &lt;code&gt;nowTemporal.add({ months: 1, days: 1 }).subtract({ years: 2 })&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Sure, weâ€™re still working with objects, and that means weâ€™re working with mutable data structures that represent real-world values:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const nowTemporal = Temporal.Now.plainDateISO(); nowTemporal.someProperty = true; console.log( nowTemporal ); /* Result (expanded): Temporal.PlainDate 2026-01-05 someProperty: true &amp;lt;prototype&amp;gt;: Object { â€¦ }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;â€¦But the value represented by that Temporal object isnâ€™t meant to be changed during the normal course of interacting with it â€” even though the object is still essentially mutable, weâ€™re not stuck using that object in ways that could alter what it means in terms of real-world dates and times. Iâ€™ll take it.&lt;/p&gt;
    &lt;p&gt;So, letâ€™s revisit that janky little â€œtoday is X, tomorrow is Yâ€ script we wrote using &lt;code&gt;Date&lt;/code&gt; earlier. First, weâ€™ll fix it by making sure weâ€™re working with two discrete instances of &lt;code&gt;Date&lt;/code&gt; rather than modifying the instance that represents todayâ€™s date:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = new Date(); const addDay = theDate =&amp;gt; { const tomorrow = new Date(); tomorrow.setDate( theDate.getDate() + 1 ); return tomorrow; }; console.log(`Tomorrow will be ${ addDay( today ).toLocaleDateString() }. Today is ${ today.toLocaleDateString() }.`); // Result: Tomorrow will be 1/1/2026. Today is 12/31/2025.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thanks, I hate it.&lt;/p&gt;
    &lt;p&gt;Okay, fine. It gets the job done, just as it has since the day &lt;code&gt;Date&lt;/code&gt; first bumbled its way onto the web. Weâ€™re not unwittingly altering the value of &lt;code&gt;today&lt;/code&gt; since weâ€™re spinning up a new instance of &lt;code&gt;Date&lt;/code&gt; inside our &lt;code&gt;addDay&lt;/code&gt; function â€” wordy, but it works, as it has for decades now. We add &lt;code&gt;1&lt;/code&gt; to it, which we have to just kind of know means add one day. Then in our template literal we need to keep nudging JavaScript to give us the date in a format that doesnâ€™t include the current time, as a string. Itâ€™s functional, but verbose.&lt;/p&gt;
    &lt;p&gt;Now, letâ€™s redo it using &lt;code&gt;Temporal&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Code language&lt;/item&gt;
      &lt;item rend="dd-1"&gt;js&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;code&gt;const today = Temporal.Now.plainDateISO(); console.log(`Tomorrow will be ${ today.add({ days: 1 }) }. Today is ${ today }.`); // Result: Tomorrow will be 2026-01-01. Today is 2025-12-31.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Now weâ€™re talking.&lt;/p&gt;
    &lt;p&gt;So much better. Leaner, meaner, and way less margin for error. We want todayâ€™s date without the time, and the object that results from invoking &lt;code&gt;plainDateISO&lt;/code&gt; (and any new Temporal objects created from it) will retain that formatting without being coerced to a string. Formatting: check.&lt;/p&gt;
    &lt;p&gt;We want to output a value that represents todayâ€™s date plus one day, and we want to do so in a way where we are unmistakably saying â€œadd one day to itâ€ with no parsing guesswork: check and check.&lt;/p&gt;
    &lt;p&gt;Most importantly, we donâ€™t want to run the risk of having our original &lt;code&gt;today&lt;/code&gt; object altered unintentionally â€” because the result of calling the &lt;code&gt;add&lt;/code&gt; method will always be a new Temporal object: check.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Temporal&lt;/code&gt; is going to be a massive improvement over &lt;code&gt;Date&lt;/code&gt;, and I only say â€œgoing to beâ€ because it still isnâ€™t quite ready for prime-time usage. The draft specification for the proposed &lt;code&gt;Temporal&lt;/code&gt; object has reached stage three of the standardization process, meaning it is now officially â€œrecommended for implementationâ€ â€” not yet part of the standard that informs the ongoing development of JavaScript itself, but close enough that browsers can start tinkering with it. That means the results of that early experimentation may be used to further refine the specification, so nothing is set in stone just yet. Web standards are an iterative process, after all.&lt;/p&gt;
    &lt;p&gt;Thatâ€™s where you and I come in. Now that &lt;code&gt;Temporal&lt;/code&gt; has landed in the latest versions of Chrome and Firefox â€” and others, soon â€” itâ€™s time for us to get in there and kick the tires a little bit. We may not have had any say in &lt;code&gt;Date&lt;/code&gt;, but we get to experiment with &lt;code&gt;Temporal&lt;/code&gt; before the final implementations land.&lt;/p&gt;
    &lt;p&gt;Soon, JavaScript will have sensible, modern date handling, and weâ€™ll finally be able to cram &lt;code&gt;Date&lt;/code&gt; way in the back of the junk drawer with the rubber bands, mismatched jar lids, mystery keys, and probably-half-empty AA batteries â€” still present, still an inexorable part of the web platform, but no longer our first, last, and only way of handling dates. And we only had to waitâ€” well, hold on, let me just crunch the numbers real quick:&lt;/p&gt;
    &lt;head rend="h3"&gt;Try it out&lt;/head&gt;
    &lt;code&gt;const today = Temporal.Now.plainDateISO();
const jsShipped = Temporal.PlainDate.from( "1995-12-04" );
const sinceDate = today.since( jsShipped, { largestUnit: 'year' });

console.log( `${ sinceDate.years } years, ${ sinceDate.months } months, and ${ sinceDate.days } days.` );&lt;/code&gt;
    &lt;p&gt;Sure, the best time to replace &lt;code&gt;Date&lt;/code&gt; wouldâ€™ve been back in 1995, but hey: the second best time is &lt;code&gt;Temporal.Now&lt;/code&gt;, right?&lt;/p&gt;
    &lt;p&gt;Enjoyed this article? You can support us by leaving a tip via Open Collective&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://piccalil.li/blog/date-is-out-and-temporal-is-in/"/><published>2026-01-12T15:20:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589675</id><title>Apple picks Google's Gemini to power Siri</title><updated>2026-01-12T18:17:44.702111+00:00</updated><content>&lt;doc fingerprint="3d46b9ec932631f0"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple is joining forces with Google to power its artificial intelligence features, including a major Siri upgrade expected later this year.&lt;/p&gt;
    &lt;p&gt;The multi-year partnership will lean on Google's Gemini and cloud technology for future Apple foundational models, according to a joint statement obtained by CNBC's Jim Cramer.&lt;/p&gt;
    &lt;p&gt;"After careful evaluation, we determined that Google's technology provides the most capable foundation for Apple Foundation Models and we're excited about the innovative new experiences it will unlock for our users," Apple said in a statement on Monday.&lt;/p&gt;
    &lt;p&gt;The models will continue to run on Apple devices and the company's private cloud compute, they added.&lt;/p&gt;
    &lt;p&gt;Apple declined to comment on the terms of the deal. Google referred CNBC to the joint statement.&lt;/p&gt;
    &lt;p&gt;In August, Bloomberg reported that Apple was in early talks with Google to use a custom Gemini model to power a new iteration of Siri. The news outlet later reported that Apple was planning to pay about $1 billion a year to utilize Google AI.&lt;/p&gt;
    &lt;p&gt;The deal is another major indicator of growing trust in Google's accelerating AI agenda and comeback against OpenAI. In 2025, the search giant logged its best year since 2009 and surpassed Apple in market capitalization last week for the first time since 2019.&lt;/p&gt;
    &lt;p&gt;Google already pays Apple billions each year to be the default search engine on iPhones. But that lucrative partnership briefly came into question after Google was found to hold an illegal internet search monopoly.&lt;/p&gt;
    &lt;p&gt;In September, a judge ruled against a worst-case scenario outcome that could have forced Google to divest its Chrome browser business.&lt;/p&gt;
    &lt;p&gt;The decision also allowed Google to continue to make deals such as the one with Apple.&lt;/p&gt;
    &lt;p&gt;Shares climbed following the news but later pulled back. Google briefly touched above a $4 trillion market value.&lt;/p&gt;
    &lt;p&gt;Apple has mostly stood on the sidelines of the AI frenzy that's swept up Wall Street since the launch of OpenAI's ChatGPT at the end of 2022.&lt;/p&gt;
    &lt;p&gt;Hyperscalers Amazon, Meta Platforms and Microsoft have shelled out billions on AI products, tools and infrastructure for their customers.&lt;/p&gt;
    &lt;p&gt;That's amped up the pressure on the iPhone maker to deliver an impressive Siri AI voice upgrade, which it delayed last year until 2026, despite running ads for the product.&lt;/p&gt;
    &lt;p&gt;"It's going to take us longer than we thought to deliver on these features and we anticipate rolling them out in the coming year," the company said in a statement at the time.&lt;/p&gt;
    &lt;p&gt;Apple currently partners with OpenAI to integrate ChatGPT into Siri and Apple Intelligence, specifically for complicated queries that can tap into the AI model's world knowledge. It's unclear what the Google partnership means for the ChatGPT integration in the future.&lt;/p&gt;
    &lt;p&gt;The iPhone maker told CNBC that it isn't making any changes to the agreement. OpenAI did not immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Google has made steady progress on its AI agenda, introducing its upgraded Gemini 3 model late last year.&lt;/p&gt;
    &lt;p&gt;In October, Google CEO Sundar Pichai said the company's cloud segment signed more deals worth more than $1 billion through the third quarter of 2025 than the previous two years combined.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2026/01/12/apple-google-ai-siri-gemini.html"/><published>2026-01-12T15:22:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589849</id><title>Open-Meteo is a free and open-source weather API for non-commercial use</title><updated>2026-01-12T18:17:44.533817+00:00</updated><content>&lt;doc fingerprint="be25fe526dacecd8"&gt;
  &lt;main&gt;
    &lt;p&gt;Open-Meteo is an open-source weather API and offers free access for non-commercial use. No API key required. Start using it now!&lt;/p&gt;
    &lt;p&gt;Open-Meteo partners with national weather services to bring you open data with high resolution, ranging from 1 to 11 kilometres. Our powerful APIs intelligently select the most suitable weather models for your specific location, ensuring accurate and reliable forecasts.&lt;/p&gt;
    &lt;p&gt;With our user-friendly JSON API, accessing weather data has never been easier. Whether you're developing an application or seeking weather information for personal use, our APIs provide seamless integration and deliver the data you need in a simple and accessible format.&lt;/p&gt;
    &lt;p&gt;Experience the precision and convenience of Open-Meteo's Forecast API, providing comprehensive weather information worldwide. Stay informed and make informed decisions with our reliable weather forecasts.&lt;/p&gt;
    &lt;code&gt;curl "https://api.open-meteo.com/v1/forecast?latitude=52.52&amp;amp;longitude=13.41&amp;amp;current=temperature_2m,wind_speed_10m&amp;amp;hourly=temperature_2m,relative_humidity_2m,wind_speed_10m" &lt;/code&gt;
    &lt;code&gt;{
  ...
  "current": {
    "time": "2022-01-01T15:00",
    "temperature_2m": 2.4,
    "wind_speed_10m": 11.9,
  },
  "hourly": {
    "time": ["2022-07-01T00:00","2022-07-01T01:00", ...],
    "wind_speed_10m": [3.16,3.02,3.3,3.14,3.2,2.95, ...],
    "temperature_2m": [13.7,13.3,12.8,12.3,11.8, ...],
    "relative_humidity_2m": [82,83,86,85,88,88,84,76, ...],
  }
}&lt;/code&gt;
    &lt;p&gt;Open-Meteo leverages a powerful combination of global (11 km) and mesoscale (1 km) weather models from esteemed national weather services, providing comprehensive forecasts with remarkable precision. No matter where you are in the world, you can access the most reliable and accurate weather predictions available.&lt;/p&gt;
    &lt;p&gt;Our weather data is presented in hourly resolution, allowing you to plan your activities with confidence. The initial days of the forecast benefit from localised weather models, offering highly detailed and accurate information. Subsequently, global weather models provide forecasts for up to 16 days. Through seamless integration, our APIs deliver a straightforward and reliable hourly weather forecast experience.&lt;/p&gt;
    &lt;p&gt;At Open-Meteo, we understand the importance of having the most up-to-date weather information. That's why our local weather models are updated every hour, ensuring that our forecasts reflect the latest changes in conditions, including updates from rain radars.&lt;/p&gt;
    &lt;p&gt;Our weather models rely on a wealth of real-time data, including measurements from various sources such as airplanes, buoys, radar systems, and satellites. By incorporating this diverse and comprehensive data, our numerical weather predictions provide a deeper analysis than traditional weather stations, resulting in more accurate forecasts.&lt;/p&gt;
    &lt;p&gt;Explore the past with our comprehensive Historical Weather API. With over 80 years of hourly weather data available at a 10 kilometre resolution, you can dive into the climate of any location. Behind the scenes, this extensive dataset, comprising 50 TB of information, enables you to access temperature records spanning eight decades in an instant.&lt;/p&gt;
    &lt;p&gt;Moreover, our 1 kilometre weather models continuously archive recent data, ensuring that you can seamlessly retrieve the latest forecasts alongside historical information from previous weeks. This functionality opens up possibilities for training machine learning applications and gaining valuable insights from the combination of present and past weather data. Discover the power of our historical weather API and unlock a treasure trove of weather information.&lt;/p&gt;
    &lt;p&gt;We believe in the power of open-source software. That's why the entire codebase of Open-Meteo is accessible on GitHub, released under the AGPLv3 licence. This means you can explore, use, modify, and contribute to the code.&lt;/p&gt;
    &lt;p&gt;If you wish to take it a step further, we're here to support you in setting up your own API instances. This allows you to have complete control and enjoy practically unlimited API calls, making it ideal for demanding applications like machine learning or large language models.&lt;/p&gt;
    &lt;p&gt;In addition, our data is licenced under Attribution 4.0 International (CC BY 4.0). This means you are free to share and adapt the data, even for commercial purposes. We believe in fostering an open ecosystem that encourages transparency, collaboration and innovation.&lt;/p&gt;
    &lt;p&gt;Open-Meteo offers free access to its APIs for non-commercial use, making it convenient for individuals and developers to explore and integrate weather data into their projects. The best part is that no API key, registration, or credit card is required to enjoy this service.&lt;/p&gt;
    &lt;p&gt;We trust our users to utilise the free API responsibly and kindly request appropriate credit for the data used. While there are no strict access restrictions, we encourage fair usage of the service. If you require commercial usage or anticipate exceeding 10'000 API calls per day, we recommend considering our API subscription for enhanced features and support.&lt;/p&gt;
    &lt;p&gt;We've designed our APIs to be incredibly user-friendly. They are based on the widely adopted HTTP protocol and utilise the simplicity of JSON data format. All you need to get started is a basic understanding of geographic coordinates, making HTTP requests, and working with JSON data.&lt;/p&gt;
    &lt;p&gt;To assist you in seamlessly integrating our APIs into your projects, we provide comprehensive documentation. It includes detailed explanations of all parameters and their usage. Whether you're using Python, R, Julia, PHP, JavaScript, React, Flutter, Java, or any other programming language, our APIs are designed to work effortlessly with your application.&lt;/p&gt;
    &lt;p&gt;We're constantly evolving and expanding. We're dedicated to providing you with the latest features, weather variables, and data sources. If you want to stay in the loop and be the first to know about our exciting updates, we invite you to subscribe to our blog or follow us on X. By doing so, you'll never miss out on the latest developments and enhancements in our services.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://open-meteo.com/"/><published>2026-01-12T15:36:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46589926</id><title>Show HN: Pane â€“ An agent that edits spreadsheets</title><updated>2026-01-12T18:17:44.385920+00:00</updated><content>&lt;doc fingerprint="fc4a645869462432"&gt;
  &lt;main&gt;
    &lt;p&gt;Create, analyze, and collaborate on spreadsheets with the Pane Agent. Just describe what you want, and watch your data transform.&lt;/p&gt;
    &lt;p&gt;Pane combines the power of traditional spreadsheets with cutting-edge AI to help you work faster and smarter.&lt;/p&gt;
    &lt;p&gt;Talk to your spreadsheet in natural language. Sort, filter, calculate, and transform data with simple commands.&lt;/p&gt;
    &lt;p&gt;Create beautiful bar, line, pie, area, and scatter charts. Drag to reposition and resize anywhere on your sheet.&lt;/p&gt;
    &lt;p&gt;Full formula support powered by HyperFormula. Use SUM, AVERAGE, VLOOKUP, and hundreds more functions.&lt;/p&gt;
    &lt;p&gt;Import your existing data with one click. Supports CSV, Excel, and PDF files of any size with smart parsing.&lt;/p&gt;
    &lt;p&gt;Your spreadsheets are securely stored in the cloud. Access them from anywhere, on any device.&lt;/p&gt;
    &lt;p&gt;Instantly visualize your data. Pane Agent automatically creates professional dashboards with relevant charts and key metrics.&lt;/p&gt;
    &lt;p&gt;Watch how the AI Agent transforms the way you work with data.&lt;/p&gt;
    &lt;p&gt;Join thousands of teams using Pane to work smarter with their data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://paneapp.com"/><published>2026-01-12T15:41:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46590280</id><title>TimeCapsuleLLM: LLM trained only on data from 1800-1875</title><updated>2026-01-12T18:17:43.769673+00:00</updated><content>&lt;doc fingerprint="34089b5011be0c83"&gt;
  &lt;main&gt;
    &lt;head&gt;ğŸŒ Language&lt;/head&gt;
    &lt;code&gt;  &amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;
    &lt;p&gt;A language model trained from scratch exclusively on data from certain places and time periods to reduce modern bias and emulate the voice, vocabulary, and worldview of the era.&lt;/p&gt;
    &lt;p&gt;Imagine if an AI model didnt just pretend to be historical but actually was.&lt;/p&gt;
    &lt;p&gt;v0 and v0.5 built on nanoGPT by Andrej Karpathy Core training scripts and model architecture are his work.&lt;/p&gt;
    &lt;p&gt;v1 built on Phi 1.5 by Microsoft&lt;/p&gt;
    &lt;p&gt;Early prompts show the model responding with 1800's language and behavior. Example: Prompt: "Who art Henry?" and it replied "I know that man, I have did not a black, the storm."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No mention of modern concepts&lt;/item&gt;
      &lt;item&gt;Mostly era-accurate vocabulary&lt;/item&gt;
      &lt;item&gt;Sentences are mostly incoherent (expected for ~187MB training data)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A significant improvement over v0.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Victorian writing style, proper punctuation, mostly grammatical sentences&lt;/item&gt;
      &lt;item&gt;Still high factual hallucination rate&lt;/item&gt;
      &lt;item&gt;OCR noise (â€œDigitized by Googleâ€) still present in outputs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First model to recall and connect a real historical event with an actual figure from the dataset.&lt;/p&gt;
    &lt;p&gt;Example: Prompt: "It was the year of our Lord 1834"&lt;/p&gt;
    &lt;p&gt;The output: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity"&lt;/p&gt;
    &lt;p&gt;At first I assumed that a protest might have coincidentally taken place the same year but take a look at this:&lt;/p&gt;
    &lt;p&gt;Trained using a 15GB sample from v2's 90GB dataset. Model trained to 10K steps only.&lt;/p&gt;
    &lt;p&gt;There was an issue with tokenization that causes output to look like this:&lt;/p&gt;
    &lt;p&gt;PROMPT: Who is Charles Dickens?&lt;/p&gt;
    &lt;p&gt;W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht&lt;/p&gt;
    &lt;p&gt;Corrected:&lt;/p&gt;
    &lt;p&gt;"Who is Charles Dickens? Does that work more of his excellent stirring, in his plays, in the Great Company's farm? What I have yet to quote from Jack Pickett? Do you not know that they were a species of galloping, or sawing of their breasts, or what was to be done about the time when Jackson was looking on the window? What is the success of an Englishman, and which his son has not been discovering to me, whereby to accomplish such a weight? Did you ever make a passage into the old roadway, or to an anchor-breeze at the foot of our boat, which you must leave us? The fact is, that whether the wind would rise up from the plain on Saturday night or noontide, or till the north, or otherwise, we shall be compelled to describe a formidable barrier, with the same effects as the present. In this situation, at least, it is not too much to say that we have left that room. I believe there are three copies in the 'Five Hundred-fold,' to be referred to, as the first number of our readers who wish t[o]...â€&lt;/p&gt;
    &lt;p&gt;Trained using the same 15GB sample used for training eval1. Model is again trained to 10K steps only.&lt;/p&gt;
    &lt;p&gt;PROMPT:Charles Darwin&lt;/p&gt;
    &lt;p&gt;Charles DarwinECCEMACY. Sir, â€” The following case is interesting to me : â€” I was in London a fortnight, and was much affected with an attack of rheumatism. The first attack of rheumatism was a week before I saw you, and the second when I saw you, and the third when I saw you, and the third in the same time. The second attack of gout, however, was not accompanied by any febrile symptoms, but was accompanied with an increased flow of urine, and with a more copious discharge of urine. The third attack was an hour after I saw you, and was succeeded by a return of a paroxysm of gout, and a more rapid return of the gout. The fourth attack was also accompanied by a fever, but was not always accompanied by any febrile symptoms. The third attack of gout was a fortnight after you had been ill, and the fourth was followed by a paroxysm of gout. The fourth attack was a fortnight after you were attacked, and was accompanied by a sense&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;90GB of 1800-1875 London texts&lt;/item&gt;
      &lt;item&gt;136,344 documents&lt;/item&gt;
      &lt;item&gt;The full 90GB is not avalaible yet as it hasn't been tokenized but you can find a 15GB sample here: https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Refer to v2 bias report for more info.&lt;/p&gt;
    &lt;p&gt;This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)&lt;/item&gt;
      &lt;item&gt;Keep them within your chosen time/place window&lt;/item&gt;
      &lt;item&gt;Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.&lt;/item&gt;
      &lt;item&gt;This will give you vocab.json and merges.txt&lt;/item&gt;
      &lt;item&gt;Thes files define vocab and merge rules for your model&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Refer to nanoGPT by Andrej Karpathy for the training process or your chosen architectureâ€™s docs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.&lt;/p&gt;
    &lt;p&gt;For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1875.&lt;/p&gt;
    &lt;p&gt;I'm using books, legal documents, newspapers, and other writings from 1800â€“1875 London. The list I linked (for v0) has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents: https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt&lt;/p&gt;
    &lt;p&gt;Dataset sizes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;v0: ~187MB&lt;/item&gt;
      &lt;item&gt;v0.5: ~435MB&lt;/item&gt;
      &lt;item&gt;v1: ~6.25GB&lt;/item&gt;
      &lt;item&gt;v2mini-eval1: 15GB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;v0: 16M Parameters&lt;/p&gt;
    &lt;p&gt;v0.5 123M Parameters&lt;/p&gt;
    &lt;p&gt;v1: 700M Parameters&lt;/p&gt;
    &lt;p&gt;v2mini-eval1: 300M Parameters&lt;/p&gt;
    &lt;p&gt;GPU: Geforce rtx 4060 CPU: i5-13400F Ram: 16GB DDR5.&lt;/p&gt;
    &lt;p&gt;GPU: A100 SXM rented&lt;/p&gt;
    &lt;p&gt;GPU: A100 SXM rented&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/haykgrigo3/TimeCapsuleLLM"/><published>2026-01-12T16:04:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46590541</id><title>Building a 25 Gbit/s workstation for the SCION Association</title><updated>2026-01-12T18:17:42.911556+00:00</updated><content>&lt;doc fingerprint="b62dd15f43309f44"&gt;
  &lt;main&gt;
    &lt;p&gt;This is an LGA4677 socket and it's about to be fitted with a 12-core Intel Xeon CPU to power the 64 PCIe Gen5 lanes for 3x Mellanox NVIDIA BlueField-2 Dual-25G smart NICs, which will ultimately power the SCION Association's new 25 Gbit/s testbench workstation!&lt;/p&gt;
    &lt;p&gt;I built it to develop and test a new AF_XDP underlay for the SCION OSS has received significant data plane performance improvements over the past years, but still requires further work.&lt;/p&gt;
    &lt;p&gt;In this article, I'll walk you through the entire planning, building, and configuration process in almost full detail.&lt;/p&gt;
    &lt;p&gt;Itâ€™s hard to say how many hours went into it, but it was clearly a multi-week endeavor. In total it cost us CHF ~3,741.34 (around â‰ˆ$4,700 USD) in materials. See the complete list of components at the end.&lt;/p&gt;
    &lt;p&gt;Disclaimer: I spent many hours writing this article by hand, but I must confess, LLMs did help me formulate and polish parts of it.&lt;/p&gt;
    &lt;p&gt;SCION (Scalability, Control, and Isolation On Next-Generation Networks), is, in a nutshell, an IETF draft-stage technology of a growing alternative to the Border Gateway Protocol (BGP). It's an innovative inter-AS routing architecture designed to address BGP's fundamental flaws and security vulnerabilities.&lt;/p&gt;
    &lt;p&gt;Maybe at some point in the future the Internet will run SCION, although a more likely scenario is that SCION and BGP will run alongside each other. What is clear, though, is that critical infrastructure should run on SCION, where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;path authenticity&lt;/item&gt;
      &lt;item&gt;explicit path control (e.g. geofencing)&lt;/item&gt;
      &lt;item&gt;more consistent latency characteristics&lt;/item&gt;
      &lt;item&gt;deterministic failover&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;are required and best-effort BGP routing is an unacceptable risk.&lt;/p&gt;
    &lt;p&gt;The national bank of Switzerland realized this and since 2024 Switzerland's banking infrastructure now runs on the SCION-powered SSFN, which relies on the commercial implementation from Anapaya Systems AG that currently provides up to 100 Gbit/s border router solutions. The free open source implementation github.com/scionproto/scion received numerous data plane performance improvements over the past years but is still lagging behind.&lt;/p&gt;
    &lt;p&gt;If we want to do video calls (and similar high-bandwidth use cases) over SCION OSS en masse - the data plane performance needs to improve. Thanks to funding by the NLnet Foundation we've been working on a new faster AF_XDP border router underlay.&lt;/p&gt;
    &lt;p&gt;If you want to learn more about SCION, check out scion.org.&lt;/p&gt;
    &lt;p&gt;As of the time of writing, the SCION OSS border router performance reached a ceiling of around 400k-500k packets per second, which is roughly equivalent to 5-6 Gbit/s at a 1500-byte MTU.&lt;/p&gt;
    &lt;p&gt;5 Gbit/s per stream* of data is too little, in fact, way too little. By contrast, today's Internet carries traffic on the order of hundreds of terabits per second across BGP border routers worldwide. On the higher end, take the Juniper "MX10008 Universal Routing Platforms" with up to 76.8 Tbps (yes, terabits with twelve zeroes) of total bandwidth capacity for example&lt;/p&gt;
    &lt;p&gt;These kinds of systems support per-port line rates of 400-500 Gbit/s, depending on the interface configuration. Individual packet streams carried over such ports can therefore be forwarded at or near full line rate without being bottlenecked by software overhead, while massive parallelization across ports enables aggregate bandwidths in the terabits per second.&lt;/p&gt;
    &lt;p&gt;Such routers usually sit at Internet exchange points and interconnect Internet service providers. They can easily handle from tens of millions to even billions of packets per second.&lt;/p&gt;
    &lt;p&gt;Even my local ISP now provides 25 Gbit/s FTTH connections at my small town near Zurich city.&lt;/p&gt;
    &lt;p&gt;SCION OSS needs to do better; a lot better.&lt;/p&gt;
    &lt;p&gt;* A "stream" is defined here as a flow of packets from a specific source address to a specific destination address that cannot be parallelized across multiple threads as parallelization would cause unacceptable levels of packet reordering.&lt;/p&gt;
    &lt;p&gt;The SCION OSS border router is a Linux user-space program. It relies solely on the Linux networking stack.&lt;/p&gt;
    &lt;p&gt;Schematic for the packet flow paths through Linux networking and tables by Jan Engelhardt&lt;/p&gt;
    &lt;p&gt;You can think of a packet traversing the networking stack as a person traveling by airplane:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you enter the airport (Network Interface Controller receiving queue)&lt;/item&gt;
      &lt;item&gt;you check in and drop off your baggage (buffer allocations)&lt;/item&gt;
      &lt;item&gt;you pass security screening (packet filtering)&lt;/item&gt;
      &lt;item&gt;you clear passport control (routing and policy checks)&lt;/item&gt;
      &lt;item&gt;you wait at the gate (queueing and scheduling)&lt;/item&gt;
      &lt;item&gt;you board the aircraft (copy to user space)&lt;/item&gt;
      &lt;item&gt;you find your seat (user-space buffer) and you're finally ready for takeoff!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And when transiting - like a packet passing through a router - the same sequence of steps occurs again in reverse on the transmit path.&lt;/p&gt;
    &lt;p&gt;As you can probably tell, there's a lot of work between a packet entering one NIC and exiting on another.&lt;/p&gt;
    &lt;p&gt;We could improve the throughput by adding more router threads, but this would introduce substantial packet reordering, with packets leaving the system in a different and largely unpredictable order. High frequency of reordering degrades performance and increases system resource usage, making this approach non-viable.&lt;/p&gt;
    &lt;p&gt;The only real way to further improve the OSS data plane performance is to bypass the Linux kernel's networking stack and fly by private jet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you enter the airport (Network Interface Controller)&lt;/item&gt;
      &lt;item&gt;you're escorted directly to your private jet (zero-copy fast path to XSK)&lt;/item&gt;
      &lt;item&gt;you board the plane and you're done (user-space frame buffer)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are several ways in which you can bypass the kernel, namely:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DPDK: a user-space networking framework that bypasses the Linux networking stack entirely, typically requiring exclusive control over the NIC and removing it from the kernel's standard networking stack.&lt;/item&gt;
      &lt;item&gt;AF_XDP: a Linux kernel mechanism that allows high-performance packet I/O via a shared memory ring between the NIC driver and user space.&lt;/item&gt;
      &lt;item&gt;VPP: a high-performance user-space packet processing framework that uses vectorized, batch-based processing and is commonly backed by DPDK or AF_XDP for packet I/O.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even though DPDK is the current de facto industry standard for kernel bypass and can likely achieve higher peak performance, we opted for Linux's native AF_XDP. Given that our open-source border router is written in Go, and that usability, maintainability, and operational simplicity are among our highest priorities, AF_XDP provides a better set of trade-offs.&lt;/p&gt;
    &lt;p&gt;AF_XDP is based on Express Data Path (XDP), which in turn is based on Extended Berkeley Packet Filter (eBPF).&lt;/p&gt;
    &lt;p&gt;In a nutshell, you write a small program in restricted C, subject to strict limits on instruction count (up to ~1 million), loops, and memory access, which is compiled into eBPF bytecode, verified by the kernel's eBPF verifier and loaded into the kernel at runtime, and executed for every incoming packet to decide how it is handled.&lt;/p&gt;
    &lt;p&gt;In the SCION border router, we therefore need to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;mmap a sufficiently large region of memory (ideally using hugepages) referred to as UMEM.&lt;/item&gt;
      &lt;item&gt;initialize the fill, completion, tx and rx rings.&lt;/item&gt;
      &lt;item&gt;bind an AF_XDP socket to a NIC queue (ideally, in &lt;code&gt;XDP_ZEROCOPY&lt;/code&gt;mode, if the hardware and driver support it).&lt;/item&gt;
      &lt;item&gt;load a small eBPF/XDP program into the kernel that redirects packets into frames in the mmapped user-space memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This allows raw packet frames to be delivered directly into the border router software with minimal overhead for further processing completely bypassing the network stack.&lt;/p&gt;
    &lt;p&gt;So far so good - but there are problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Typical VM offerings don't expose the access needed for XDP/AF_XDP (especially in zero-copy mode). In practice, you usually need bare metal. You cannot open an AF_XDP socket to send raw packets on, for example, an AWS EC2 instance.&lt;/item&gt;
      &lt;item&gt;None of the Linux machines currently at our disposal are equipped with NICs that support AF_XDP in &lt;code&gt;XDP_ZEROCOPY&lt;/code&gt;mode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a result, the only way to properly test and develop a zero-copy-capable AF_XDP underlay is to obtain hardware that supports it, which is not commonly available in off-the-shelf consumer-grade systems.&lt;/p&gt;
    &lt;p&gt;Our new goal was to achieve 25 Gbit/s on a single thread of the border router in our benchmark topology on a relatively small budget, both in terms of time and money. The only place we could deploy the machine is our not-so-noisy office. This makes a low noise profile a hard requirement, further narrowing our options.&lt;/p&gt;
    &lt;p&gt;I identified 3 main options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Find a suitable used rack server and build The LackRack &lt;list rend="ul"&gt;&lt;item&gt;relatively cheap ğŸ‘&lt;/item&gt;&lt;item&gt;abundantly available ğŸ‘&lt;/item&gt;&lt;item&gt;often very noisy ğŸ‘&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Find a suitable used tower server with a lower noise profile &lt;list rend="ul"&gt;&lt;item&gt;cheap ğŸ‘&lt;/item&gt;&lt;item&gt;typically limited in NIC options and available PCIe lanes ğŸ‘&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Build a new system from scratch &lt;list rend="ul"&gt;&lt;item&gt;exactly the configuration we need ğŸ‘&lt;/item&gt;&lt;item&gt;likely expensive ğŸ‘&lt;/item&gt;&lt;item&gt;likely requires significant effort and manhours ğŸ‘&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finding a suitable used machine under $5,000 that would also be quiet enough to operate inside our office proved challenging. After many hours spent browsing various marketplaces, it became clear that building the system ourselves was the more practical option at the time.&lt;/p&gt;
    &lt;p&gt;I therefore started by evaluating suitable NICs. After some research, the following candidates emerged:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intel Ethernet 800 Series (E810 family) (&lt;code&gt;ice&lt;/code&gt;driver)&lt;/item&gt;
      &lt;item&gt;NVIDIA/Mellanox ConnectX-5,6,7 (&lt;code&gt;mlx5&lt;/code&gt;driver)&lt;/item&gt;
      &lt;item&gt;Broadcom NetXtreme-E (BCM57xxx / BCM588xx series) (&lt;code&gt;bnxt_en&lt;/code&gt;driver)&lt;/item&gt;
      &lt;item&gt;FastLinQ (QL41xxx / QL45xxx series) (&lt;code&gt;qede&lt;/code&gt;driver)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The NICs would later determine the requirements for the rest of the system.&lt;/p&gt;
    &lt;p&gt;I was quite lucky to have a friend who works on networking at a large US tech company help me plan and build this system. He asked not to be named, so we'll refer to him as Frank throughout the article.&lt;/p&gt;
    &lt;p&gt;Eventually, I found the Mellanox NVIDIA BlueField-2 DPUs (Data Processing Units, a.k.a. "smart NICs") with dual 25 Gbit/s ports to be a good deal on piospartslap.de and ordered 3 cards for just 289,92â‚¬ plus 49,99â‚¬ for the express delivery (a total of CHF 318.09). After reaching out, they kindly agreed to a discounted price of 115â‚¬ per card (excluding VAT).&lt;/p&gt;
    &lt;p&gt;Now it was time to plan out the rest of the system.&lt;/p&gt;
    &lt;p&gt;Before choosing the mainboard, it was necessary to decide between going team Red ğŸ”´ (AMD) and going team Blue ğŸ”µ (Intel).&lt;/p&gt;
    &lt;p&gt;I chose to build an Intel-based system because Intel still tends to have a slight edge in the networking space, particularly due to platform features such as Intel Data Direct I/O (DDIO), which allows NICs to DMA packet data directly into the CPU's L3 cache instead of main memory.&lt;/p&gt;
    &lt;p&gt;In our benchmark topology we ideally need 5-6 NICs. Three BF-2 cards, each with 2 ports, consume one PCIe slot and 8 PCIe Gen4 lanes (16 GT/s), which means the system needs a proper workstation-grade mainboard that can not only accommodate this configuration but also leave room for future expansion. Once we reach 25 Gbit/s, it would be desirable to have the option to upgrade the NICs and move toward the 100 Gbit/s range without having to replace the entire platform.&lt;/p&gt;
    &lt;p&gt;There aren't that many mainboards that fit these requirements, and these two stood out:&lt;/p&gt;
    &lt;p&gt;Both are within budget, both offer remote management capabilities, and both provide seven full 16Ã— PCIe Gen5 slots. However, the MS03-CE0 was not readily available and would have required waiting several weeks for delivery from China. Naturally, I went for the ASUS SAGE SE.&lt;/p&gt;
    &lt;p&gt;ASUS lists a wide range of supported LGA4677 CPUs for the W790E-SAGE SE:&lt;/p&gt;
    &lt;p&gt;Frank happened to have a Sapphire Rapids Q03J engineering/qualification sample CPU with 60 (!) cores for the LGA4677 socket that he wanted to sell anyway, and he also had access to an ASUS Pro WS W790E-SAGE SE to test it on.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the Q03J and SAGE SE combination didn't work. The system didn't boot, so that option was off the table. I couldn't find any decent second-hand offers either, which left me with no real choice but to buy a brand-new CPU.&lt;/p&gt;
    &lt;p&gt;I would have preferred to use an Intel Xeon W-3400 workstation CPU, but prices on galaxus.ch and digitec.ch started at around CHF ~1600, which sadly exceeded our budget. So I had to opt for the cheaper 2000 series. The best CPU I could find available in swiss stocks was the Intel Xeon W5-2455X, a 12-core CPU with a 3.2 GHz base clock and 64Ã— PCIe Gen5 lanes, providing enough headroom for potential future expansion at a rather bearable cost of CHF 1105,-.&lt;/p&gt;
    &lt;p&gt;There were a couple of CPU cooler options and Frank suggested either of:&lt;/p&gt;
    &lt;p&gt;Being a happy Noctua client, I just went with the NH-U14S.&lt;/p&gt;
    &lt;p&gt;There were also liquid cooling options but having had to maintain a liquid cooled CPU in the past, I didn't feel it was a good fit for this setup and the Noctua NH-U14S would probably be silent enough anyway.&lt;/p&gt;
    &lt;p&gt;It quickly became apparent that the ongoing RAM shortage had already affected the market, making it difficult to find suitable DDR5 ECC memory that was both in stock and reasonably priced.&lt;/p&gt;
    &lt;p&gt;Fortunately, a Corsair DDR5 RDIMM kit (64 GB, 4Ã—16 GB, 5600 MT/s) was available on Galaxus, so I picked it right away.&lt;/p&gt;
    &lt;p&gt;The M.2 SSD was the easiest component to decide on. I simply went with a 1TB Samsung 990 Pro with a heatsink being absolutely sure there won't be any problems. It's widely available, and Digitec even promised next-day delivery.&lt;/p&gt;
    &lt;p&gt;The workstation was supposed to work 24/7 as a server so picking a reliable PSU is quite important.&lt;/p&gt;
    &lt;p&gt;After careful considering, I went with the Corsair RM850e&lt;/p&gt;
    &lt;p&gt;To be honest - I heavily underestimated the task of finding the right SSI-EEB case. I mean, a case is just a piece of sheet metal, how hard can it be to find the right piece of metal, right?!&lt;/p&gt;
    &lt;p&gt;First, consider the BlueField-2 NIC shown below.&lt;/p&gt;
    &lt;p&gt;These cards do not have active cooling. They are equipped only with a small, thin heatsink and are designed to be installed in server racks with very noisy high-RPM, high-pressure airflow. As a result, they run extremely hot under normal operation.&lt;/p&gt;
    &lt;p&gt;Server-grade hardware like this is typically expected to operate continuously at high loads, often without dynamic power management. This becomes problematic in a workstation tower office setup, as it requires a case that allows fans to be mounted in very close proximity to the cards, to provide sufficient airflow and static pressure to keep them within safe and sustainable operating temperatures, at least at around 50-60Â°C (122-140Â°F).&lt;/p&gt;
    &lt;p&gt;I did find the Silverstone RM52 to be an option worth considering:&lt;/p&gt;
    &lt;p&gt;Though at a price of CHF ~450 it was, to put it mildly, too expensive.&lt;/p&gt;
    &lt;p&gt;After nearly losing my sanity browsing both new and second-hand listings, I finally came across the Phanteks Enthoo Pro II tower case with a very convenient fan mount:&lt;/p&gt;
    &lt;p&gt;Finally, everything except the case coolers was decided upon and ordered.&lt;/p&gt;
    &lt;p&gt;The shopping list was finally complete:&lt;/p&gt;
    &lt;p&gt;That, plus CHF ~170,- for the Phanteks case, makes a total of CHF 3210,-. It could probably have been cheaper, but I really wanted to get most of the work done before heading off on my three-week vacation.&lt;/p&gt;
    &lt;p&gt;However, at the time of writing, just months later, the price of the RAM kit went up by 232%! Memory supply seems to have gotten a lot worse.&lt;/p&gt;
    &lt;p&gt;The first components to arrive were the BlueField-2 NICs, followed shortly by the ASUS mainboard, as expected. A bit later, the CPU cooler and the SSD arrived as well.&lt;/p&gt;
    &lt;p&gt;After a significant delay, the CPU, PSU, and RAM finally showed up too. The RAM delay was unsurprising.&lt;/p&gt;
    &lt;p&gt;The delivery of the Phanteks case was delayed, and no updated or approximate delivery date was available at the time, so I had to start assembling the system without the case.&lt;/p&gt;
    &lt;p&gt;Last but not least, I needed three SFP28 DAC cables to interconnect the NICs between each other. I was honestly surprised by how expensive these cables are. Domestic Swiss offers ranged from CHF 50,- per cable all the way up to CHF 150,-! Of course, there's always the option to order them from China for a fraction of the price, but shipping typically takes 2-4 weeks.&lt;/p&gt;
    &lt;p&gt;Luckily, I managed to find a few at a much lower price on Ricardo!&lt;/p&gt;
    &lt;p&gt;Since the cards arrived early, the first thing I had to do was upgrade their firmware. These cards often ship with outdated firmware, so updating it is essential, otherwise you may face all sorts of problems and lower performance.&lt;/p&gt;
    &lt;p&gt;However, since the new workstation system wasn't ready yet, I decided to try installing them on my ancient gaming rig back from around the 2010s. The BigBang X-Power II PCIe Gen3 should have been enough to run the BF-2 NICs.&lt;/p&gt;
    &lt;p&gt;Sadly, when I powered it on, it zapped âš¡âš¡âš¡ and the unmistakable smell of electronic death quickly filled the room. After more than 11 years of service, my old machine's VRM had finally given up.&lt;/p&gt;
    &lt;p&gt;Rest in peace, comrade ğŸª¦.&lt;/p&gt;
    &lt;p&gt;Luckily, I have a new gaming PC with Linux on it. I just didn't want to disassemble it, but now I had no choice. The fans and the fat RTX 3090 with the bracket that prevents it from bending the PCIe slot under its immense weight needed to be removed and make space for the tiny BlueField-2.&lt;/p&gt;
    &lt;p&gt;I also installed the new SSD and installed an Ubuntu system on it.&lt;/p&gt;
    &lt;p&gt;When I first booted the gaming PC with the BF-2 NIC installed, I was a bit scared when I saw UEFI report:&lt;/p&gt;
    &lt;code&gt;mlx5_core - over 120000 MS in pre-initializing state, aborting
mlx5_core: mlx5_init_one failed with error code -110
&lt;/code&gt;
    &lt;p&gt;Apparently, this is normal, especially for the very first cold boot. These BlueField-2 cards are not your regular network card. They're DPUs, which is essentially an entire PCIe based computer with its own Linux subsystem running on their own 8-core ARM Cortex-A72 CPUs with 16 GB of DDR4 ECC RAM. No wonder these cards get so hot, they're insane! On the first boot, this system has to boot and initialize itself, which apparently makes my UEFI believe that it's dead. After letting it sit for a while and rebooting again - everything was fine and I managed to boot into Ubuntu and proceed with the firmware upgrade.&lt;/p&gt;
    &lt;p&gt;I also checked temperatures, just to be on the safe side, and they appeared to be fine:&lt;/p&gt;
    &lt;code&gt;~$ sensors
...
mlx5-pci-0601
Adapter: PCI adapter
asic:         +57.0Â°C  (crit = +105.0Â°C, highest = +58.0Â°C)
...&lt;/code&gt;
    &lt;p&gt;Previously, I removed one of the fans from my old, dead computer and placed it right next to the NIC's heatsink to get some airflow through it. That seems to have been sufficient.&lt;/p&gt;
    &lt;p&gt;Before I could do anything with the cards, I needed to install the NVIDIA DOCA Host-Server package on the freshly installed Ubuntu system, which is fairly easy to do by following the documentation.&lt;/p&gt;
    &lt;p&gt;Then I needed to confirm the card is visible on PCIe, and it was:&lt;/p&gt;
    &lt;code&gt;lspci | grep Mellanox
06:00.0 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)
06:00.1 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)
06:00.2 Ethernet controller: Mellanox Technologies Device c2d1 (rev 01)
06:00.3 DMA controller: Mellanox Technologies MT42822 BlueField-2 SoC Management Interface (rev 01)&lt;/code&gt;
    &lt;p&gt;I started the Mellanox software tools driver set:&lt;/p&gt;
    &lt;code&gt;sudo mst start&lt;/code&gt;
    &lt;p&gt;And now, I could query the current firmware:&lt;/p&gt;
    &lt;code&gt;sudo mlxfwmanager --online
Querying Mellanox devices firmware ...

Device #1:
----------

  Device Type:      BlueField2
  Part Number:      0JNDCM_Dx
  Description:      NVIDIA Bluefield-2 Dual Port 25 GbE SFP Crypto DPU
  PSID:             DEL0000000033
  PCI Device Name:  /dev/mst/mt41686_pciconf0
  Base GUID:        58a2e1030004a9da
  Base MAC:         58a2e104a9da
  Versions:         Current        Available     
     FW             24.36.7506     N/A           
     PXE            3.7.0200       N/A           
     UEFI           14.31.0010     N/A           
     UEFI Virtio blk   22.4.0010      N/A           
     UEFI Virtio net   21.4.0010      N/A           

  Status:           No matching image found
&lt;/code&gt;
    &lt;p&gt;The installed firmware &lt;code&gt;24.36.7506&lt;/code&gt; was clearly not up to date. What I expected to see
there is &lt;code&gt;24.46.3048&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So I proceeded to bfb-install:&lt;/p&gt;
    &lt;code&gt;sudo bfb-install --bfb ~/bf-fwbundle-3.1.0-82_25.07-prod.bfb --rshim rshim0&lt;/code&gt;
    &lt;p&gt;After a while, the upgrade completed successfully, and I had to power down the system completely for a few seconds so that the new firmware would be used after the reboot:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;old version&lt;/cell&gt;
        &lt;cell role="head"&gt;new version&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;FW&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;24.36.7506&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;24.46.3048&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;PXE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3.7.0200&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3.8.0100&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;UEFI&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;14.31.0010&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;14.39.0014&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I repeated this process for all three cards. It took a while, but it was simpler than I had expected.&lt;/p&gt;
    &lt;p&gt;As I mentioned earlier, the BlueField-2s are DPUs, not simple NICs. But they can very well function just like simple 25 Gbit/s NICs if you want them to. And this part of the DOCA documentation explains how, so I won't repeat the steps here.&lt;/p&gt;
    &lt;p&gt;This switch is necessary because, in DPU mode, packet processing is routed through the onboard ARM cores and the embedded switch, which adds unnecessary complexity and latency for our use case. NIC mode effectively turns the BlueField-2 into a conventional high-performance NIC and removes the DPU from the data path.&lt;/p&gt;
    &lt;p&gt;Finally, it was time to start assembling the new SCION workstation. To be honest, I got a bit nervous. The last time I assembled a system was years ago. I had help from my younger brother and it wasn't nearly as expensive as this shiny new piece of workstation hardware.&lt;/p&gt;
    &lt;p&gt;Before touching any component, I plugged the PSU into the wall and grounded myself on it to prevent static electricity from damaging the delicate circuitry. This I regularly repeated throughout the whole process just to be on the safe side.&lt;/p&gt;
    &lt;p&gt;For the first time in my life, I held a server-grade CPU in my hand - and I have to say, it's massive! Easily two to three times the size of a typical consumer CPU.&lt;/p&gt;
    &lt;p&gt;The installation process does differ from your regular consumer CPU, and it's explained in full detail here.&lt;/p&gt;
    &lt;p&gt;I installed the CPU + cooler, RAM, M.2 SSD, and connected the power wiring:&lt;/p&gt;
    &lt;p&gt;By the way, the RAM was installed incorrectly, as I later found out and fixed according to the manual:&lt;/p&gt;
    &lt;code&gt;IntelÂ® XeonÂ® W-2400 Series Processors do not support
DIMM_C1, DIMM_D1, DIMM_G1, and DIMM_H1
&lt;/code&gt;
    &lt;p&gt;I clicked the power button, and... it almost booted. But something was wrong. I couldn't SSH into the Ubuntu system I had installed on this M.2 SSD.&lt;/p&gt;
    &lt;p&gt;Frank suggested reseating the CPU, as uneven screw pressure can cause poor contact in the LGA4677 socket. In that case, the system may fail to boot or lose access to some PCIe lanes if the pins don't line up perfectly. The Q-code LED displayed &lt;code&gt;64&lt;/code&gt;, which, according
to the manual, means &lt;code&gt;CPU DXE initialization is started&lt;/code&gt;. At that point, I assumed
something had to be wrong with either the CPU or RAM. I reseated both multiple times, wasting several hours in the process, but without any success.&lt;/p&gt;
    &lt;p&gt;I even installed the old GTX TITAN GPU, which I previously confirmed to have worked on another system:&lt;/p&gt;
    &lt;p&gt;and even though its LEDs lit up I was left with a frustrating "no signal" message on the HDMI connected display. At this point, I started suspecting the worst. As you might have guessed - I didn't get a good night's sleep.&lt;/p&gt;
    &lt;p&gt;The next day, however, I had to confront the sheer size of my own stupidity. If you look again at the photo of the installed GPU above, you'll notice that - for reasons unknown even to myself - I had installed it in PCIe slot 2, which is not supported by the Intel Xeon W5-2455X.&lt;/p&gt;
    &lt;p&gt;And it turns out, Q-code &lt;code&gt;64&lt;/code&gt; was just the last code displayed and didn't actually indicate any fault at all. In fact, everything was okay.&lt;/p&gt;
    &lt;p&gt;So I moved the graphics card to PCIe slot 1 and was finally able to enter the UEFI menu, only to run into the next surprise: the M.2 SSD wasn't recognized.&lt;/p&gt;
    &lt;p&gt;Some time later, I once again realized that I needed to pay closer attention to the manual:&lt;/p&gt;
    &lt;code&gt;M.2_1 and M.2_2 slots will be disabled once an IntelÂ® XeonÂ® W-2400 Series
Processor is installed.
&lt;/code&gt;
    &lt;p&gt;After moving the M.2 SSD to slot 3, everything worked just fine.&lt;/p&gt;
    &lt;p&gt;As a proper server-/workstation-grade mainboard, this ASUS SAGE SE has the ASPEED AST2600 remote management system on it. It allows you to access its web UI over a dedicated ethernet port, independent of the actual host system and control almost everything remotely, even if the host system is down or corrupted. It even gives you screen-sharing remote control.&lt;/p&gt;
    &lt;p&gt;The first time I entered the BMC UI, it asked me to change the admin password, which I did.&lt;/p&gt;
    &lt;p&gt;Later, when I tried to sign in again, it refused to let me in, claiming I had entered incorrect credentials - which was simply not true. Long story short: the AST2600 firmware has a bug. It allows you to set a password that exceeds the maximum supported length without showing any error. Internally, it appears to hash the overlong string as-is, so even trimming the password to the maximum length during login doesn't work. The result is a self-inflicted lockout.&lt;/p&gt;
    &lt;p&gt;I found it out by using ipmitool:&lt;/p&gt;
    &lt;code&gt;~ % ipmitool -I lanplus -H 192.168.1.152 -U admin -P $PASS
chassis status lanplus: password is longer than 20 bytes.&lt;/code&gt;
    &lt;p&gt;The only way to recover from this is to log into the host system and reset the BMC password using &lt;code&gt;ipmitool&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Now that the Phanteks Enthoo Pro II case had finally arrived, it was time to move the system into its new home.&lt;/p&gt;
    &lt;p&gt;During installation, I accidentally left a standoff in the wrong place. The motherboard's backplate drew first blood - and I almost cried.&lt;/p&gt;
    &lt;p&gt;The system was finally mounted in the chassis, and the Enthoo Pro II's brackets allowed me to mount the NIC fan quite elegantly, even if it couldn't be placed perfectly close.&lt;/p&gt;
    &lt;p&gt;But one last piece of the puzzle was still missing: air circulation. So I came up with a concept:&lt;/p&gt;
    &lt;p&gt;and a new shopping list:&lt;/p&gt;
    &lt;p&gt;I decided to go with the more expensive Noctua fans again to keep the noise profile as low as possible. The NF-A12x25 G2 PWM fans are optimized for static pressure, and one of them would be responsible for providing sufficient airflow through the NIC heatsinks.&lt;/p&gt;
    &lt;p&gt;A short while later, they arrived:&lt;/p&gt;
    &lt;p&gt;And I had lots of fun installing them. Just kidding - it was tedious work, and my back hurt so much afterward that not even a good hot bath helped.&lt;/p&gt;
    &lt;p&gt;Once again, I grossly underestimated the effort required for cable management, planning, and execution. I even had to reroute and rewire parts of the setup after realizing that one connector sat far too close to a very hot heatsink - clearly asking for trouble.&lt;/p&gt;
    &lt;p&gt;But in the end, I was really happy with how it turned out:&lt;/p&gt;
    &lt;p&gt;Cable management was decent. Temperatures were great and sustainable, and even at 30-50% fan speed the system was barely audible, even in complete silence.&lt;/p&gt;
    &lt;code&gt;~$ sensors
coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +23.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 0:        +21.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 1:        +22.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 2:        +20.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 3:        +22.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 4:        +23.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 5:        +22.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 6:        +21.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 7:        +22.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 8:        +20.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 9:        +20.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 10:       +21.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)
Core 11:       +21.0Â°C  (high = +92.0Â°C, crit = +100.0Â°C)

mlx5-pci-bc01
Adapter: PCI adapter
asic:         +62.0Â°C  (crit = +105.0Â°C, highest = +74.0Â°C)

mlx5-pci-8501
Adapter: PCI adapter
asic:         +57.0Â°C  (crit = +105.0Â°C, highest = +66.0Â°C)

mlx5-pci-4e01
Adapter: PCI adapter
asic:         +59.0Â°C  (crit = +105.0Â°C, highest = +70.0Â°C)

nvme-pci-0100
Adapter: PCI adapter
Composite:    +27.9Â°C  (low  = -273.1Â°C, high = +81.8Â°C)
                       (crit = +84.8Â°C)
Sensor 1:     +27.9Â°C  (low  = -273.1Â°C, high = +65261.8Â°C)
Sensor 2:     +31.9Â°C  (low  = -273.1Â°C, high = +65261.8Â°C)

mlx5-pci-bc00
Adapter: PCI adapter
asic:         +62.0Â°C  (crit = +105.0Â°C, highest = +74.0Â°C)

mlx5-pci-8500
Adapter: PCI adapter
asic:         +57.0Â°C  (crit = +105.0Â°C, highest = +66.0Â°C)

mlx5-pci-4e00
Adapter: PCI adapter
asic:         +59.0Â°C  (crit = +105.0Â°C, highest = +70.0Â°C)&lt;/code&gt;
    &lt;p&gt;One more thing left to do was to configure this workstation for upcoming work. I needed to figure out which NIC is which in Linux and interconnect them.&lt;/p&gt;
    &lt;p&gt;The easiest way to do so was to connect a card to itself (port 1 -&amp;gt; port 2), then check &lt;code&gt;sudo mst status&lt;/code&gt; and write down a map of &lt;code&gt;/dev/mst/mt*&lt;/code&gt;
to their respective &lt;code&gt;domain:bus:dev.fn&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Then check all mst devices &lt;code&gt;sudo mlxlink -d /dev/mst/mt41686_pciconf1 -p 1&lt;/code&gt; status.
The one where the "Troubleshooting Info - Recommendation" says "No issue was observed"
is the one currently wired one.&lt;/p&gt;
    &lt;p&gt;Under /etc/udev/rules.d/10-bf2-names.rules I placed a new config file mapping names to their respective MACs (MAC addresses are redacted here):&lt;/p&gt;
    &lt;code&gt;### TOP CARD â€” PCIEX16(G5)_3 â€” PCI domain:bus:dev.fn=0000:85:00.0
SUBSYSTEM=="net", ACTION=="add", ATTR{address}=="00:00:00:00:00:00", NAME="top_1"
SUBSYSTEM=="net", ACTION=="add", ATTR{address}=="00:00:00:00:00:00", NAME="top_2"

### CENTER CARD â€” PCIEX16(G5)_5 â€” PCI domain:bus:dev.fn=0000:4e:00.0
SUBSYSTEM=="net", ACTION=="add", ATTR{address}=="00:00:00:00:00:00", NAME="center_1"
SUBSYSTEM=="net", ACTION=="add", ATTR{address}=="00:00:00:00:00:00", NAME="center_2"

### BOTTOM CARD â€” PCIEX16(G5)_7 â€” PCI domain:bus:dev.fn=0000:bc:00.0
SUBSYSTEM=="net", ACTION=="add", ATTR{address}=="00:00:00:00:00:00", NAME="bottom_1"
SUBSYSTEM=="net", ACTION=="add", ATTR{address}=="00:00:00:00:00:00", NAME="bottom_2"&lt;/code&gt;
    &lt;p&gt;After reloading I could now easily identify the individual ports by their name.&lt;/p&gt;
    &lt;code&gt;sudo udevadm control --reload
sudo udevadm trigger&lt;/code&gt;
    &lt;code&gt;~$ ip -br link
lo               UNKNOWN        00:00:00:00:00:00 &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; 
eno3np0          UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
eno2np1          UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
enx96cd268bdb7b  UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
center_1         UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
center_2         UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
top_1            UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
top_2            UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
bottom_1         UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
bottom_2         UP             00:00:00:00:00:00 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; 
docker0          DOWN           00:00:00:00:00:00 &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; &lt;/code&gt;
    &lt;p&gt;The biggest challenge with our office is that it's located inside a banking infrastructure company's building. While they do provide a way for us to connect to the outside, we can't directly access anything on the inside. Unless, of course, we use Tailscale.&lt;/p&gt;
    &lt;p&gt;To access the BMC web interface from outside the office, I configured a small Linux machine in the office, connected to the same switch as the workstation, and tunneled the connection through Tailscale.&lt;/p&gt;
    &lt;p&gt;By the way, finding the BMC's LAN IP and MAC address turned into yet another multi-hour adventure, as they weren't written down anywhere - not in the documentation and not on the board itself. First, I tried to scan the local network, but that approach proved futile. Later, I connected my MacBook to the management port directly via ethernet cable and used &lt;code&gt;tcpdump&lt;/code&gt; to inspect the traffic and determine the MAC.
Unfortunately, it seems like the ASPEED AST2600 BMC doesn't speak proper
ARP.
There is however a short window of time right after a cold boot of the system
when it sends ICMPv6 packets, which reveal its MAC.&lt;/p&gt;
    &lt;p&gt;I started writing AF_XDP experiments, and you can find the code at: github.com/romshark/afxdp-bench-go&lt;/p&gt;
    &lt;p&gt;Eventually, I managed to send 1.5 TB of data in just over eight minutes from one card to another at 24.6 Gbit/s (practically line rate), using an MTU of 1500 bytes per packet:&lt;/p&gt;
    &lt;code&gt;FINAL REPORT
 Elapsed:           487.657 s
 TX:                1,000,000,000 packets
 RX:                1,000,000,000 packets
 TX Avg PPS:        2,050,621
 RX Avg PPS:        2,050,621
 TX Avg rate:       24,607.5 Mbps
 RX Avg rate:       24,607.5 Mbps
 Dropped:           0 (0.0000%)

real    8m10.217s
user    0m0.009s
sys     0m0.017s
Requested TX:       1000000000
Egress:
  tx_packets_phy delta: 1000000126
  tx_bytes_phy   delta: 1504000017713
Ingress:
  rx_packets_phy delta: 1000000126
  rx_bytes_phy   delta: 1504000017713&lt;/code&gt;
    &lt;p&gt;In the next post, I want to share implementation details and more benchmark results of the SCION OSS AF_XDP underlay, so stay tuned!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Quantity&lt;/cell&gt;
        &lt;cell role="head"&gt;Retailer&lt;/cell&gt;
        &lt;cell role="head"&gt;Price&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ASUS Pro WS W790E-SAGE SE (LGA4677, Intel W790, SSI EEB)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;digitec.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 962.90&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Intel Xeon W5-2455X (3.2 GHz, 12-core)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;galaxus.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 1106.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Corsair DDR5 RDIMM 64 GB (4Ã—16 GB, 5600 MT/s, ECC)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;galaxus.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 536.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Corsair RM850e (850 W)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;galaxus.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 113.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung 990 Pro w/ Heatsink (1 TB, M.2 2280)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;digitec.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 101.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Noctua NH-U14S DX-4677 CPU cooler&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;digitec.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 132.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Phanteks Enthoo Pro II Server Edition TG (SSI-EEB)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;galaxus.ch&lt;/cell&gt;
        &lt;cell&gt;~CHF 170.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Noctua NF-A14 PWM (140 mm)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;digitec.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 149.40&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Noctua NF-A12x25 G2 PWM Sx2-PP (120 mm, 2-pack)&lt;/cell&gt;
        &lt;cell&gt;1 (2 fans)&lt;/cell&gt;
        &lt;cell&gt;digitec.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 64.90&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Noctua NF-A12x25 G2 PWM (120 mm)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;digitec.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 34.90&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mellanox/NVIDIA BlueField-2 BF2H532C dual-port 25G (PCIe 4.0 x8)&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;piospartslap.de&lt;/cell&gt;
        &lt;cell&gt;289,92 â‚¬&lt;p&gt;+ 49,99 â‚¬ shipping&lt;/p&gt;&lt;p&gt;(CHF 318.09)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;25G SFP28 passive DAC cable 0.5 m&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;ricardo.ch&lt;/cell&gt;
        &lt;cell&gt;CHF 51.15&lt;p&gt;+ CHF 2.00 shipping&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Grand Total&lt;/cell&gt;
        &lt;cell&gt;CHF 3,741.34&lt;p&gt;(â‰ˆ$4,700 USD)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/scionassociation/blog-25gbit-workstation"/><published>2026-01-12T16:20:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46591100</id><title>Show HN: AI in SolidWorks</title><updated>2026-01-12T18:17:42.775378+00:00</updated><content>&lt;doc fingerprint="b3c6fa5900f7aa44"&gt;
  &lt;main&gt;
    &lt;p&gt;1.1.0&lt;/p&gt;
    &lt;p&gt;01-11-2026&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add planning mode&lt;/item&gt;
      &lt;item&gt;Add macro writing/running&lt;/item&gt;
      &lt;item&gt;Detect and report sketch issues&lt;/item&gt;
      &lt;item&gt;Improve caching efficiency&lt;/item&gt;
      &lt;item&gt;Various AI context improvements and bug fixes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;LAD (Language-Aided Designer)&lt;/p&gt;
    &lt;p&gt;A SolidWorks add-in to design with natural language using AI&lt;/p&gt;
    &lt;p&gt;Describe your design in plain language and LAD will translate it into SolidWorks operations, creating sketches, features, and assemblies all through natural conversation. LAD uses screenshots and the feature tree to understand your model's current state, verifying operations were completed correctly and correcting mistakes.&lt;/p&gt;
    &lt;p&gt;Design from Documentation and Images&lt;/p&gt;
    &lt;p&gt;Provide documentation files, images, or examples of previous parts and assemblies, and LAD will intelligently read and use them.&lt;/p&gt;
    &lt;p&gt;Write and Run Macros&lt;/p&gt;
    &lt;p&gt;LAD can write and run VBA macros for reproducibility and niche functionality not covered by standard LAD tools. When writing macros, LAD searches SolidWorks documentation and examples to better understand the API.&lt;/p&gt;
    &lt;p&gt;Permissioning and Versioning&lt;/p&gt;
    &lt;p&gt;LAD stores checkpoints so you can revert unwanted changes, lets you control which commands run automatically, and uses rules you provide to guide the AI as it works.&lt;/p&gt;
    &lt;p&gt;LAD (Language-Aided Designer) is a system that integrates directly into SolidWorks as an Add-in and can create sketches, features, macros, and other CAD objects.&lt;/p&gt;
    &lt;p&gt;1.1.0&lt;/p&gt;
    &lt;p&gt;01-11-2026&lt;/p&gt;
    &lt;p&gt;1.0.0&lt;/p&gt;
    &lt;p&gt;01-05-2026&lt;/p&gt;
    &lt;p&gt;Try LAD now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.trylad.com"/><published>2026-01-12T16:56:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46591170</id><title>Carma (YC W24 clients, A in 6mo) Eng hiring: Replace $500B human fleet ops with AI</title><updated>2026-01-12T18:17:42.655070+00:00</updated><content>&lt;doc fingerprint="b98cc6fbbd9605ac"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;TL;DR: Live product used daily by Fortune 500 clients; post-revenue, real operations, solving a real problem today. We are growing fast, hiring founding engineers, $200K+ cash base + 3.00%+ equity, build with us in-person in SF.&lt;/p&gt;
      &lt;p&gt;Weâ€™re building an AI platform that makes fleet operations â€” maintenance, repairs, approvals, pricing â€” fully autonomous. This massive $500B industry still runs on phone calls, PDFs, and spreadsheets.&lt;/p&gt;
      &lt;p&gt;Today: - Live product already used by multiple Fortune 500 clients nationwide&lt;/p&gt;
      &lt;p&gt;- Signing new enterprise clients every month&lt;/p&gt;
      &lt;p&gt;- $5.5M seed raised, Series A on track in mid-2026&lt;/p&gt;
      &lt;p&gt;- Your code runs in live fleet ops and gets same-day feedback&lt;/p&gt;
      &lt;p&gt;Stack: React, NestJS, PostgreSQL, AWS Location: On-site, San Francisco&lt;/p&gt;
      &lt;p&gt;Roles: Founding Head of Engineering Comp: $250K + 5% equity https://www.ycombinator.com/companies/carma/jobs/5Mwg4jp-fou...&lt;/p&gt;
      &lt;p&gt;Founding Full-Stack Engineer Comp: $200K + 3% equity https://www.ycombinator.com/companies/carma/jobs/YLaym2M-fou...&lt;/p&gt;
      &lt;p&gt;If you want: real % ownership, work w/ a top 0.1% business team (all STEM degrees) closing brand-name clients nonstop who use what you build every day, and experience real PMF â€” this is one of those rare early roles.&lt;/p&gt;
      &lt;p&gt;Interested? Let's chat ASAP. Apply above or email me directly: muhammad at joincarma.com&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46591170"/><published>2026-01-12T17:00:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46591402</id><title>Message Queues: A Simple Guide with Analogies</title><updated>2026-01-12T18:17:42.543333+00:00</updated><content>&lt;doc fingerprint="e71cd8526aa188cb"&gt;
  &lt;main&gt;
    &lt;p&gt;I find stories and analogies very fascinating and â€” to explain message queues in a super approachable way, we will use some analogies: databases, warehouses and post offices.&lt;/p&gt;
    &lt;p&gt;Stay with me â€¦&lt;/p&gt;
    &lt;p&gt;Databases are primarily used for data persistence â€” think Postgres or MongoDB. Like databases, message queues also perform some storage function. But why use message queues for data storage when there are databases? Think of databases and message queues in terms of warehouses and post offices.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Databases are like warehouses - they are designed to hold a lot of different things, most times, over a long period of time.&lt;/item&gt;
      &lt;item&gt;Message queues on the other hand are like post offices â€” Where letters and packages stop briefly on their way to being delivered. The packages don't stay there long; they're just sorted and sent off to where they need to go.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Essentially, databases are primarily designed for scenarios where you need to store and manage some state over a long period of time. In contrast, you would want to use a message queue for data that you do not want to keep around for very longâ€” A message queue holds information just long enough to send it to the next stop.&lt;/p&gt;
    &lt;p&gt;If you look at message queues from this post office perspective, then you will begin to appreciate the fact that a message queue is simply a medium through which data flows from a source system to a destination system.&lt;/p&gt;
    &lt;p&gt;Looking at message queues as medium of communication is just one perspective, but itâ€™s sufficient to help you get started with message queues â€” Letâ€™s double down on that perspective.&lt;/p&gt;
    &lt;p&gt;A message queue is a technology that simply receives data, formally called messages in the message queueing world from a source system(s) (producer), lines up these messages in the order they arrive, then sends each message to some final destination, usually another system called the consumer.&lt;/p&gt;
    &lt;p&gt;Note that both the producer and consumer could also just be modules in the same application.&lt;/p&gt;
    &lt;p&gt;Now that we understand the core essence of message queues, letâ€™s explore how they work.&lt;/p&gt;
    &lt;head rend="h2"&gt;How a Message Queue Works&lt;/head&gt;
    &lt;p&gt;Typically, producers and consumers would connect and communicate with a message queue via some protocol that the message queue supports.&lt;/p&gt;
    &lt;p&gt;In other words, a message queue would implement a protocol or some set of protocols. To communicate with a message queue, a producer or consumer would leverage some client library that also implements the protocol or one of the protocols supported by the broker.&lt;/p&gt;
    &lt;p&gt;Most message brokers commonly implement at least one of these protocols : AMQP, MQTT and STOMP. You can learn more about these protocols in our AMQP vs MQTT guide or the AMQP, MQTT and STOMP guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;When to Use a Message Queue&lt;/head&gt;
    &lt;p&gt;Weâ€™ve already seen how message queues allow messages to flow from a source system to a destination system. This inherent nature of message queues makes them perfect for communication between systems in a microservice architecture.&lt;/p&gt;
    &lt;p&gt;What is the microservice architecture? Again, letâ€™s start with something you are familiar with â€” Monoliths.&lt;/p&gt;
    &lt;p&gt;A monolith is characterized by the entire codebase being inside one application. This is a great approach for smaller projects, and many new applications start out as a monolith. This is because on a smaller monoliths are faster to develop, easier to test, and easier to deploy.&lt;/p&gt;
    &lt;p&gt;However when an application starts to grow, the more problems you will see with this architecture. Even with a structured approach, the code often starts to feel messy and the development experience becomes inconvenient. Changes become more difficult to implement, and the risk of introducing bugs is higher.&lt;/p&gt;
    &lt;p&gt;Many times the solution to these problems is to break up your monolith application into microservices. And microservices are smaller, more modular services that focus on a single area of responsibility.&lt;/p&gt;
    &lt;p&gt;The microservice approach has some benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With microservices, there is fault isolationâ€” if one service is buggy, that bug is isolated to just that service. This in turn makes your application more reliable compared to a monolith where a single component error could take down the entire application.&lt;/item&gt;
      &lt;item&gt;There is also the opportunity of being able to diversify the technology stack from service to service, which helps you optimize your services for its purpose. For example, a performance critical service has the chance to make certain performance trade-offs, without putting limits to the rest of the services.&lt;/item&gt;
      &lt;item&gt;Naturally, scaling becomes much easier because you can just scale one of your services instead of scaling the entire application and save a lot of resources.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that we understand what microservices are, letâ€™s cycle back to: Using message queues for communication between systems in a microservice architecture.&lt;/p&gt;
    &lt;p&gt;But before we get to that, note that message queueing isnâ€™t the only way to get services to communicate â€” There is one other common way:&lt;/p&gt;
    &lt;p&gt;Synchronous communication, where network requests are sent directly from one service to another via REST API calls, for example. Service A will initiate a request and then wait for Service B to finish handling the request and send a response back before it continues on with the activity it was doing.&lt;/p&gt;
    &lt;p&gt;With message queueing, the communication is asynchronous â€” In this case, Service A can send messages to a message broker and instead of waiting for Service B, it will receive a super quick acknowledgement back from the broker and then it can carry on doing what it was doing while Service B fetches the message from the queue and handles it.&lt;/p&gt;
    &lt;p&gt;This will save your service from overloading if there is a suddenly increased workload, instead the messages are buffered by the queue and your services can just handle them when they have the capacity.&lt;/p&gt;
    &lt;p&gt;There you have it, a very gentle introduction to message queues. Now, letâ€™s do a recap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In summary, message queues are like post offices for your data, moving messages from one place to another. They work by receiving messages from producers, lining them up in the order they arrive, and sending them to consumers. This makes them perfect for situations where systems need to communicate without waitingâ€” think microservice architectures.&lt;/p&gt;
    &lt;p&gt;Understanding how message queues work and when to use them can help you build more reliable and scalable applications.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cloudamqp.com/blog/message-queues-exaplined-with-analogies.html"/><published>2026-01-12T17:17:09+00:00</published></entry></feed>