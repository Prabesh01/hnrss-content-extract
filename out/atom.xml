<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-02-01T19:42:20.929020+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46840612</id><title>In praise of –dry-run</title><updated>2026-02-01T19:42:30.765365+00:00</updated><content>&lt;doc fingerprint="f398e1326871fbc"&gt;
  &lt;main&gt;
    &lt;p&gt;For the last few months, I have been developing a new reporting application. Early on, I decided to add a –dry-run option to the run command. This turned out to be quite useful – I have used it many times a day while developing and testing the application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;The application will generate a set of reports every weekday. It has a loop that checks periodically if it is time to generate new reports. If so, it will read data from a database, apply some logic to create the reports, zip the reports, upload them to an sftp server, check for error responses on the sftp server, parse the error responses, and send out notification mails. The files (the generated reports, and the downloaded feedback files) are moved to different directories depending on the step in the process. A simple and straightforward application.&lt;/p&gt;
    &lt;p&gt;Early in the development process, when testing the incomplete application, I remembered that Subversion (the version control system after CVS, before Git) had a –dry-run option. Other linux commands have this option too. If a command is run with the argument –dry-run, the output will print what will happen when the command is run, but no changes will be made. This lets the user see what will happen if the command is run without the –dry-run argument.&lt;/p&gt;
    &lt;p&gt;I remembered how helpful that was, so I decided to add it to my command as well. When I run the command with –dry-run, it prints out the steps that will be taken in each phase: which reports that will be generated (and which will not be), which files will be zipped and moved, which files will be uploaded to the sftp server, and which files will be downloaded from it (it logs on and lists the files).&lt;/p&gt;
    &lt;p&gt;Looking back at the project, I realized that I ended up using the –dry-run option pretty much every day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benefits&lt;/head&gt;
    &lt;p&gt;I am surprised how useful I found it to be. I often used it as a check before getting started. Since I know –dry-run will not change anything, it is safe to run without thinking. I can immediately see that everything is accessible, that the configuration is correct, and that the state is as expected. It is a quick and easy sanity check.&lt;/p&gt;
    &lt;p&gt;I also used it quite a bit when testing the complete system. For example, if I changed a date in the report state file (the date for the last successful report of a given type), I could immediately see from the output whether it would now be generated or not. Without –dry-run, the actual report would also be generated, which takes some time. So I can test the behavior, and receive very quick feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Downside&lt;/head&gt;
    &lt;p&gt;The downside is that the dryRun-flag pollutes the code a bit. In all the major phases, I need to check if the flag is set, and only print the action that will be taken, but not actually doing it. However, this doesn’t go very deep. For example, none of the code that actually generates the report needs to check it. I only need to check if that code should be invoked in the first place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The type of application I have been writing is ideal for –dry-run. It is invoked by a command, and it may create some changes, for example generating new reports. More reactive applications (that wait for messages before acting) don’t seem to be a good fit.&lt;/p&gt;
    &lt;p&gt;I added –dry-run on a whim early on in the project. I was surprised at how useful I found it to be. Adding it early was also good, since I got the benefit of it while developing more functionality.&lt;/p&gt;
    &lt;p&gt;The –dry-run flag is not for every situation, but when it fits, it can be quite useful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://henrikwarne.com/2026/01/31/in-praise-of-dry-run/"/><published>2026-01-31T20:42:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840801</id><title>CollectWise (YC F24) Is Hiring</title><updated>2026-02-01T19:42:30.321544+00:00</updated><content>&lt;doc fingerprint="9b58a24f7107e484"&gt;
  &lt;main&gt;
    &lt;p&gt;Automating consumer debt collection with AI&lt;/p&gt;
    &lt;p&gt;About Us&lt;/p&gt;
    &lt;p&gt;CollectWise is a fast growing and well funded Y Combinator-backed startup. We’re using generative AI to automate debt collection, a $35B market in the US alone. Our AI agents are already outperforming human collectors by 2X, and we’re doing so at a fraction of the cost.&lt;/p&gt;
    &lt;p&gt;With a team of three, we scaled to a $1 million annualized run rate in just a few months, and we are now hiring an AI Agent Engineer to help us reach $10 million within the next year.&lt;/p&gt;
    &lt;p&gt;Role&lt;/p&gt;
    &lt;p&gt;We are hiring an AI Agent Engineer to design, optimize, and productionize the prompting and conversation logic behind our voice AI agents, while also supporting the technical systems that power customer deployments.&lt;/p&gt;
    &lt;p&gt;You’ll work at the intersection of AI quality, product outcomes, and engineering execution—owning prompt development, testing, and iteration loops that improve real-world performance (e.g., identity verification, payment conversion, dispute handling, containment rates), while collaborating closely with the founder and customers to ship improvements quickly.&lt;/p&gt;
    &lt;p&gt;This role is ideal if you’re highly analytical and business minded, love experimentation and measurement, and can also jump into back-end code and integrations when needed.&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Desired Qualifications&lt;/p&gt;
    &lt;p&gt;Compensation&lt;/p&gt;
    &lt;p&gt;CollectWise is revolutionizing debt recovery with autonomous AI agents and an integrated legal network. We boost recovery rates, reduce costs, and maintain a positive brand image through respectful, data-driven interactions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/collectwise/jobs/ZunnO6k-ai-agent-engineer"/><published>2026-01-31T21:00:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840924</id><title>Generative AI and Wikipedia editing: What we learned in 2025</title><updated>2026-02-01T19:42:30.003138+00:00</updated><content>&lt;doc fingerprint="3f520313a88f8382"&gt;
  &lt;main&gt;
    &lt;p&gt;Like many organizations, Wiki Education has grappled with generative AI, its impacts, opportunities, and threats, for several years. As an organization that runs large-scale programs to bring new editors to Wikipedia (we’re responsible for about 19% of all new active editors on English Wikipedia), we have deep understanding of what challenges face new content contributors to Wikipedia — and how to support them to successfully edit. As many people have begun using generative AI chatbots like ChatGPT, Gemini, or Claude in their daily lives, it’s unsurprising that people will also consider using them to help draft contributions to Wikipedia. Since Wiki Education’s programs provide a cohort of content contributors whose work we can evaluate, we’ve looked into how our participants are using GenAI tools.&lt;/p&gt;
    &lt;p&gt;We are choosing to share our perspective through this blog post because we hope it will help inform discussions of GenAI-created content on Wikipedia. In an open environment like the Wikimedia movement, it’s important to share what you’ve learned. In this case, we believe our learnings can help Wikipedia editors who are trying to protect the integrity of content on the encyclopedia, Wikipedians who may be interested in using generative AI tools themselves, other program leaders globally who are trying to onboard new contributors who may be interested in using these tools, and the Wikimedia Foundation, whose product and technology team builds software to help support the development of high-quality content on Wikipedia.&lt;/p&gt;
    &lt;p&gt;Our fundamental conclusion about generative AI is: Wikipedia editors should never copy and paste the output from generative AI chatbots like ChatGPT into Wikipedia articles.&lt;/p&gt;
    &lt;p&gt;Let me explain more.&lt;/p&gt;
    &lt;head rend="h4"&gt;AI detection and investigation&lt;/head&gt;
    &lt;p&gt;Since the launch of ChatGPT in November 2022, we’ve been paying close attention to GenAI-created content, and how it relates to Wikipedia. We’ve spot-checked work of new editors from our programs, primarily focusing on citations to ensure they were real and not hallucinated. We experimented with tools ourselves, we led video sessions about GenAI for our program participants, and we closely tracked on-wiki policy discussions around GenAI. Currently, English Wikipedia prohibits the use of generative AI to create images or in talk page discussions, and recently adopted a guideline against using large language models to generate new articles.&lt;/p&gt;
    &lt;p&gt;As our Wiki Experts Brianda Felix and Ian Ramjohn worked with program participants throughout the first half of 2025, they found more and more text bearing the hallmarks of generative AI in article content, like bolded words or bulleted lists in odd places. But the use of generative AI wasn’t necessarily problematic, as long as the content was accurate. Wikipedia’s open editing process encourages stylistic revisions to factual text to better fit Wikipedia’s style.&lt;/p&gt;
    &lt;p&gt;This finding led us to invest significant staff time into cleaning up these articles — far more than these editors had likely spent creating them. Wiki Education’s core mission is to improve Wikipedia, and when we discover our program has unknowingly contributed to misinformation on Wikipedia, we are committed to cleaning it up. In the clean-up process, Wiki Education staff moved more recent work back to sandboxes, we stub-ified articles that passed notability but mostly failed verification, and we PRODed some articles that from our judgment weren’t salvageable. All these are ways of addressing Wikipedia articles with flaws in their content. (While there are many grumblings about Wikipedia’s deletion processes, we found several of the articles we PRODed due to their fully hallucinated GenAI content were then de-PRODed by other editors, showing the diversity of opinion about generative AI among the Wikipedia community.&lt;/p&gt;
    &lt;head rend="h4"&gt;Revising our guidance&lt;/head&gt;
    &lt;p&gt;Given what we found through our investigation into the work from prior terms, and given the increasing usage of generative AI, we wanted to proactively address generative AI usage within our programs. Thanks to in-kind support from our friends at Pangram, we began running our participants’ Wikipedia edits, including in their sandboxes, through Pangram nearly in real time. This is possible because of the Dashboard course management platform Sage built, which tracks edits and generates tickets for our Wiki Experts based on on-wiki edits.&lt;/p&gt;
    &lt;p&gt;We created a brand-new training module on Using generative AI tools with Wikipedia. This training emphasizes where participants could use generative AI tools in their work, and where they should not. The core message of these trainings is, do not copy and paste anything from a GenAI chatbot into Wikipedia.&lt;/p&gt;
    &lt;p&gt;We crafted a variety of automated emails to participants who Pangram detected were adding text created by generative AI chatbots. Sage also recorded some videos, since many young people are accustomed to learning via video rather than reading text. We also provided opportunities for engagement and conversation with program participants.&lt;/p&gt;
    &lt;head rend="h4"&gt;Our findings from the second half of 2025&lt;/head&gt;
    &lt;p&gt;In total, we had 1,406 AI edit alerts in the second half of 2025, although only 314 of these (or 22%) were in the article namespace on Wikipedia (meaning edits to live articles). In most cases, Pangram detected participants using GenAI in their sandboxes during early exercises, when we ask them to do things like choose an article, evaluate an article, create a bibliography, and outline their contribution.&lt;/p&gt;
    &lt;p&gt;Pangram struggled with false positives in a few sandbox scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bibliographies, which are often a combination of human-written prose (describing a source and its relevance) and non-prose text (the citation for a source, in some standard format)&lt;/item&gt;
      &lt;item&gt;Outlines with a high portion of non-prose content (such as bullet lists, section headers, text fragments, and so on)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We also had a handful of cases where sandboxes were flagged for AI after a participant copied an AI-written section from an existing article to use as a starting point to edit or to expand. (This isn’t a flaw of Pangram, but a reminder of how much AI-generated content editors outside our programs are adding to Wikipedia!)&lt;/p&gt;
    &lt;p&gt;In broad strokes, we found that Pangram is great at analyzing plain prose — the kind of sentences and paragraphs you’ll find in the body of a Wikipedia article — but sometimes it gets tripped up by formatting, markup, and non-prose text. Early on, we disabled alert emails for participants’ bibliography and outline exercises, and throughout the end of 2025, we refined the Dashboard’s preprocessing steps to extract the prose portions of revisions and convert them to plain text before sending them to Pangram.&lt;/p&gt;
    &lt;p&gt;Many participants also reported “just using Grammarly to copy edit.” In our experience, however, the smallest fixes done with Grammarly never trigger Pangram’s detection, but if you use its more advanced content creation features, the resulting text registers as being AI generated.&lt;/p&gt;
    &lt;p&gt;But overwhelmingly, we were pleased with Pangram’s results. Our early interventions with participants who were flagged as using generative AI for exercises that would not enter mainspace seemed to head off their future use of generative AI. We supported 6,357 new editors in fall 2025, and only 217 of them (or 3%) had multiple AI alerts. Only 5% of the participants we supported had mainspace AI alerts. That means thousands of participants successfully edited Wikipedia without using generative AI to draft their content.&lt;/p&gt;
    &lt;p&gt;For those who did add GenAI-drafted text, we ensured that the content was reverted. In fact, participants sometimes self-reverted once they received our email letting them know Pangram had detected their contributions as being AI created. Instructors also jumped in to revert, as did some Wikipedians who found the content on their own. Our ticketing system also alerted our Wiki Expert staff, who reverted the text as soon as they could.&lt;/p&gt;
    &lt;p&gt;While some instructors in our Wikipedia Student Program had concerns about AI detection, we had a lot of success focusing the conversation on the concept of verifiability. If the instructor as subject matter expert could attest the information was accurate, and they could find the specific facts in the sources they were cited to, we permitted text to come back to Wikipedia. However, the process of attempting to verify student-created work (which in many cases the students swore they’d written themselves) led many instructors to realize what we had found in our own assessment: In their current states, GenAI-powered chatbots cannot write factually accurate text for Wikipedia that is verifiable.&lt;/p&gt;
    &lt;p&gt;We believe our Pangram-based detection interventions led to fewer participants adding GenAI-created content to Wikipedia. Following the trend lines, we anticipated about 25% of participants to add GenAI content to Wikipedia articles; instead, it was only 5%, and our staff were able to revert all problematic content.&lt;/p&gt;
    &lt;p&gt;I’m deeply appreciative of everyone who made this success possible this term: Participants who followed our recommendations, Pangram who gave us access to their detection service, Wiki Education staff who did the heavy lift of working with all of the positive detections, and the Wikipedia community, some of whom got to the problematic work from our program participants before we did.&lt;/p&gt;
    &lt;head rend="h4"&gt;How can generative AI help?&lt;/head&gt;
    &lt;p&gt;So far, I’ve focused on the problems with generative AI-created content. But that’s not all these tools can do, and we did find some ways they were useful. Our training module encourages editors — if their institution’s policies permit it — to consider using generative AI tools for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identifying gaps in articles&lt;/item&gt;
      &lt;item&gt;Finding access to sources&lt;/item&gt;
      &lt;item&gt;Finding relevant sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To evaluate the success of these use scenarios, we worked directly with 7 of the classes we supported in fall 2025 in our Wikipedia Student Program. We asked students to anonymously fill out a survey every time they used generative AI tools in their Wikipedia work. We asked what tool they used, what prompt they used, how they used the output, and whether they found it helpful. While some students filled the survey out multiple times, others filled it out once. We had 102 responses reporting usage at various stages in the project. Overwhelmingly, 87% of the responses who reported using generative AI said it was helpful for them in the task. The most popular tool by far was ChatGPT, with Grammarly as a distant second, and the others in the single-digits of usage.&lt;/p&gt;
    &lt;p&gt;Students reported AI tools very helpful in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identifying articles to work on that were relevant to the course they were taking&lt;/item&gt;
      &lt;item&gt;Highlighting gaps within existing articles, including missing sections or more recent information that was missing&lt;/item&gt;
      &lt;item&gt;Finding reliable sources that they hadn’t already located&lt;/item&gt;
      &lt;item&gt;Pointing to which database a certain journal article could be found&lt;/item&gt;
      &lt;item&gt;When prompted with the text they had drafted and the checklist of requirements, evaluating the draft against those requirements&lt;/item&gt;
      &lt;item&gt;Identifying categories they could add to the article they’d edited&lt;/item&gt;
      &lt;item&gt;Correcting grammar and spelling mistakes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Critically, no participants reported using AI tools to draft text for their assignments. One student reported: “I pasted all of my writing from my sandbox and said ‘Put this in a casual, less academic tone’ … I figured I’d try this but it didn’t sound like what I normally write and I didn’t feel that it captured what I was trying to get across so I scrapped it.”&lt;/p&gt;
    &lt;p&gt;While this was an informal research project, we received enough positive feedback from it to believe using ChatGPT and other tools can be helpful in the research stage if editors then critically evaluate the output they get, instead of blindly accepting it. Even participants who found AI helpful reported that they didn’t use everything it gave them, as some was irrelevant. Undoubtedly, it’s crucial to maintain the human thinking component throughout the process.&lt;/p&gt;
    &lt;head rend="h4"&gt;What does this all mean for Wiki Education?&lt;/head&gt;
    &lt;p&gt;My conclusion is that, at least as of now, generative AI-powered chatbots like ChatGPT should never be used to generate text for Wikipedia; too much of it will simply be unverifiable. Our staff would spend far more time attempting to verify facts in AI-generated articles than if we’d simply done the research and writing ourselves.&lt;/p&gt;
    &lt;p&gt;That being said, AI tools can be helpful in the research process, especially to help identify content gaps or sources, when used in conjunction with a human brain that carefully evaluates the information. Editors should never simply take a chatbot’s suggestion; instead, if they want to use a chatbot, they should use it as a brainstorm partner to help them think through their plans for an article.&lt;/p&gt;
    &lt;p&gt;To date, Wiki Education’s interventions as our program participants edit Wikipedia show promise for keeping unverifiable, GenAI-drafted content off Wikipedia. Based on our experiences in the fall term, we have high confidence in Pangram as a detector of AI content, at least in Wikipedia articles. We will continue our current strategy in 2026 (with more small adjustments to make the system as reliable as we can).&lt;/p&gt;
    &lt;p&gt;More generally, we found participants had less AI literacy than popular discourse might suggest. Because of this, we created a supplemental large language models training that we’ve offered as an optional module for all participants. Many participants indicated that they found our guidance regarding AI to be welcome and helpful as they attempt to navigate the new complexities created by AI tools.&lt;/p&gt;
    &lt;p&gt;We are also looking forward to more research on our work. A team of researchers — Francesco Salvi and Manoel Horta Ribeiro at Princeton University, Robert Cummings at the University of Mississippi, and Wiki Education’s Sage Ross — have been looking into Wiki Education’s Wikipedia Student Program editors’ use of generative AI over time. Preliminary results have backed up our anecdotal understanding, while also revealing nuances of how text produced by our students over time has changed with the introduction of GenAI chatbots. They also confirmed our belief in Pangram: After running student edits from 2015 up until the launch of ChatGPT through Pangram, without any date information involved, the team found Pangram correctly identified that it was all 100% human written. This research will continue into the spring, as the team explores ways of unpacking the effects of AI on different aspects of article quality.&lt;/p&gt;
    &lt;p&gt;And, of course, generative AI is a rapidly changing field. Just because these were our findings in 2025 doesn’t mean they will hold true throughout 2026. Wiki Education remains committed to monitoring, evaluating, iterating, and adapting as needed. Fundamentally, we are committed to ensuring we add high quality content to Wikipedia through our programs. And when we miss the mark, we are committed to cleaning up any damage.&lt;/p&gt;
    &lt;head rend="h4"&gt;What does this all mean for Wikipedia?&lt;/head&gt;
    &lt;p&gt;While I’ve focused this post on what Wiki Education has learned from working with our program participants, the lessons are extendable to others who are editing Wikipedia. Already, 10% of adults worldwide are using ChatGPT, and drafting text is one of the top use cases. As generative AI usage proliferates, its usage by well-meaning people to draft content for Wikipedia will as well. It’s unlikely that longtime, daily Wikipedia editors would add content copied and pasted from a GenAI chatbot without verifying all the information is in the sources it cites. But many casual Wikipedia contributors or new editors may unknowingly add bad content to Wikipedia when using a chatbot. After all, it provides what looks like accurate facts, cited to what are often real, relevant, reliable sources. Most edits we ended up reverting seemed acceptable with a cursory review; it was only after we attempted to verify the information that we understood the problems.&lt;/p&gt;
    &lt;p&gt;Because this unverifiable content often seems okay at first pass, it’s critical for Wikipedia editors to be equipped with tools like Pangram to more accurately detect when they should take a closer look at edits. Automating review of text for generative AI usage — as Wikipedians have done for copyright violation text for years — would help protect the integrity of Wikipedia content. In Wiki Education’s experience, Pangram is a tool that could provide accurate assessments of text for editors, and we would love to see a larger scale version of the tool we built to evaluate edits from our programs to be deployed across all edits on Wikipedia. Currently, editors can add a warning banner that highlights that the text might be LLM generated, but this is based solely on the assessment of the person adding the banner. Our experience suggests that judging by tone alone isn’t enough; instead, tools like Pangram can flag highly problematic information that should be reverted immediately but that might sound okay.&lt;/p&gt;
    &lt;p&gt;We’ve also found success in the training modules and support we’ve created for our program participants. Providing clear guidance — and the reason why that guidance exists — has been key in helping us head off poor usage of generative AI text. We encourage Wikipedians to consider revising guidance to new contributors in the welcome messages to emphasize the pitfalls of adding GenAI-drafted text. Software aimed at new contributors created by the Wikimedia Foundation should center starting with a list of sources and drawing information from them, using human intellect, instead of generative AI, to summarize information. Providing guidance upfront can help well-meaning contributors steer clear of bad GenAI-created text.&lt;/p&gt;
    &lt;p&gt;Wikipedia recently celebrated its 25th birthday. For it to survive into the future, it will need to adapt as technology around it changes. Wikipedia would be nothing without its corps of volunteer editors. The consensus-based decision-making model of Wikipedia means change doesn’t come quickly, but we hope this deep-dive will help spark a conversation about changes that are needed to protect Wikipedia into the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/"/><published>2026-01-31T21:14:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46842178</id><title>Cells use 'bioelectricity' to coordinate and make group decisions</title><updated>2026-02-01T19:42:29.818379+00:00</updated><content>&lt;doc fingerprint="3e4800171a9609d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cells Use ‘Bioelectricity’ To Coordinate and Make Group Decisions&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;We’re used to thinking of the brain as an electric organ. The rest of the body? Not so much. But it would be a mistake to dismiss your other tissues as dumb hunks of electrically inert flesh. Even the protective layers of cells that compose your skin and line your organs use electrical signals to make decisions, according to recent research.&lt;/p&gt;
    &lt;p&gt;Results published in Nature show that cells use bioelectricity to coordinate a complex collective behavior called extrusion, a vital process that ejects sick or struggling individual cells from tissue to maintain health and keep growth in check. Merciless as it might seem, extrusion helps keep you alive. It’s vital for the health of protective epithelial tissues, and when it goes wrong, it can contribute to disease, including cancer and asthma. Until now, it’s been unclear how cells were singled out for this process.&lt;/p&gt;
    &lt;p&gt;According to the new results, as epithelial tissue grows, cells are packed more tightly together, which increases the electrical current flowing through each cell’s membrane. A weak, old, or energy-starved cell will struggle to compensate, triggering a response that sends water rushing out of the cell, shriveling it up and marking it for death. In this way, electricity acts like a health checkup for the tissue and guides the pruning process.&lt;/p&gt;
    &lt;p&gt;“This is a very interesting discovery — finding that bioelectricity is the earliest event during this cell-extrusion process,” said the geneticist GuangJun Zhang of Purdue University, who studies bioelectrical signals in zebra fish development and wasn’t involved in the study. “It’s a good example of how a widening electronic-signaling perspective can be used in fundamental biology.”&lt;/p&gt;
    &lt;p&gt;The new discovery adds to the growing assortment of bioelectrical phenomena that scientists have discovered playing out beyond the nervous system, from bacteria swapping signals within a biofilm to cells following electric fields during embryonic development. Electricity increasingly appears to be one of biology’s go-to tools for coordinating and exchanging information between all kinds of cells.&lt;/p&gt;
    &lt;p&gt;“People just kind of relegated [bioelectricity] to ‘This is just neurons.’ No — it’s all of our bodies,” said study author Jody Rosenblatt, an epithelial cell biologist at King’s College London and the Francis Crick Institute. “There are electrical currents going through your body all the time, and they’re doing things.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Life’s Spark&lt;/head&gt;
    &lt;p&gt;It’s no coincidence that Frankenstein’s monster sprang to life with a spark. In the late 18th century, just a few decades before Mary Shelley wrote her science fiction masterpiece, the Italian surgeon Luigi Galvani jolted the scientific community with experiments that used metal and electricity to compel disembodied frog legs to kick. He became convinced that there was an “animal electricity” running through life.&lt;/p&gt;
    &lt;p&gt;Public Domain; ETH-Bibliothek Zurich / Science Source&lt;/p&gt;
    &lt;p&gt;While Galvani was later proven wrong in the details, he wasn’t totally off. Virtually every cell on every branch of the tree of life expends a hefty chunk of its energy budget — in some cells, more than half — on maintaining a voltage across its membrane. The voltage difference that results, called the membrane potential, stores potential energy that can be released later. It’s like the pressure behind a dam: Gravity tugs water downhill, and dams store energy by holding water at the top of a hill. Like gravity, the electrochemical force tugs charges “downhill” — positive charges stream toward negative charges and vice versa in electrical currents. Blocking that flow, for example with a cell membrane, stores up electrical potential energy.&lt;/p&gt;
    &lt;p&gt;The electric currents that pour from our wall sockets are streams of electrons. In cells, “it’s quite similar, but not exactly the same,” said Elias Barriga, who studies tissue biophysics at the Dresden University of Technology. “We are fueled by ions.”&lt;/p&gt;
    &lt;p&gt;Ions are atoms or molecules that carry charge because of extra or missing electrons, which give them negative or positive charges, respectively. They can enter and exit cells only through specialized protein channels and pumps. Just as hydroelectric plants can use surplus energy to pump water back up into the reservoir for later use, cells use their chemical energy to pump ions “uphill” against the electric flow. By controlling the natural current and letting positive or negative charge build up on either side of their membranes, cells maintain their membrane potential. And if this energy is used or leaks away, cells can replenish it by expending more of their chemical energy.&lt;/p&gt;
    &lt;p&gt;“You generate a potential: what’s inside versus what’s outside, a different concentration of ions,” Barriga said. “That is the source of bioelectricity.”&lt;/p&gt;
    &lt;p&gt;Neurons make use of this biological electricity to share information. By releasing messenger molecules called neurotransmitters that open and close ion channels, neurons can nudge their neighbors’ membrane potentials up or down. If these chemical nudges push a neuron’s membrane potential beyond a threshold, the cell “spikes” — voltage-sensitive ion channels throw open the floodgates for positive sodium ions, which rush into the cell and cause a rapid voltage swing that ripples along the neuron’s length. When that signal reaches the interface between neurons, voltage-sensitive channels open wide, triggering the release of neurotransmitters to more neurons downstream.&lt;/p&gt;
    &lt;p&gt;Muscle contraction also kicks off with a voltage spike; neurons send electrical signals streaming into muscle fibers, triggering contractions. This is why Galvani’s electrified frog legs twitched, and why a jolt of electricity can jump-start a stopped heart. (Specialized cells in the heart use electricity to set the pace of its regular contractions.) While all tissues maintain membrane potentials, researchers don’t really know what they do. Compared to electrophysiology, which often focuses on electricity in the brain and heart, the field of bioelectricity — a grab-bag term for electrical activity everywhere else in organisms — has lagged behind, Barriga said.&lt;/p&gt;
    &lt;p&gt;“I think that at some point it got stuck,” he said. “But now I can tell you that that is coming back like crazy.”&lt;/p&gt;
    &lt;head rend="h2"&gt;A Shocking Discovery&lt;/head&gt;
    &lt;p&gt;The epithelial tissues that make up skin and line organs, blood vessels, and body cavities quietly burn about 25% of their available energy to maintain membrane voltages between minus 30 and minus 50 millivolts. But researchers interested in these tissues typically study mechanical forces, chemical signaling, and gene expression — not currents and voltage, Rosenblatt said.&lt;/p&gt;
    &lt;p&gt;Until recently, that included her. Rosenblatt has spent 25 years piecing together the details of epithelial extrusion, a process that keeps tissue growth in check. Because epithelial cells grow quickly, even a slight mismatch between the rates of cell division and cell death can quickly add up to a tumor or injury. Runaway replication can grow into cancer, while overzealous culling — as can happen in asthma — compromises the integrity of tissues. It’s important to get the balance right.&lt;/p&gt;
    &lt;p&gt;Around 14 years ago, Rosenblatt and colleagues discovered that overcrowded epithelial cells are popped up and out of the tissue alive — extruded — to maintain that tissue balance as new cells grow. That raised a question: How does tissue “choose” which living cells to expel?&lt;/p&gt;
    &lt;p&gt;In earlier work, Rosenblatt’s team watched as some cells dumped their water and shriveled up like raisins before being extruded; indeed, this shrinkage seemed to kick off the process. But the researchers didn’t know what caused the cells to shrink in the first place. They didn’t work on bioelectricity and were unaware of any effect it might have.&lt;/p&gt;
    &lt;p&gt;Antonio Tabernero&lt;/p&gt;
    &lt;p&gt;In further experiments, they were able to prevent the cells from shrinking by blocking a pressure-sensitive ion channel in the cell membrane that opens when squeezed. They decided to see if blocking other ion channels might interfere with extrusion too.&lt;/p&gt;
    &lt;p&gt;“We got so many hits, we were just like: Jesus, this is crazy,” Rosenblatt recalled. One of those hits was a voltage-gated potassium channel, like those that open up during a neuron’s voltage spike. It struck Rosenblatt as “weird” enough to follow up on. Using special dyes that reveal the voltage across cell membranes, the scientists found that epithelial cells destined for extrusion — and only those cells — lose their membrane potential about five minutes before shrinking and being extruded. The result was clear: Extrusion kicks off with an electrical signal.&lt;/p&gt;
    &lt;p&gt;Instead of sending neurotransmitters back and forth like neurons, densely packed epithelial cells squeeze each other. As the tissue gets more crowded, the squeezing intensifies. This opens pressure-sensitive ion channels, which allow positive sodium ions to leak across the squashed cells’ membranes and into the cell.&lt;/p&gt;
    &lt;p&gt;Faced with this challenge, a healthy cell will use its chemical energy to activate pumps to push sodium back out and restore its normal voltage. But stressed or unhealthy cells without energy to spare can’t keep up. Their membrane voltage falters, throwing open those “weird” voltage-sensitive channels. When that happens, water pours out of the cell in a “lightning” flash clearly visible in microscope images, Rosenblatt said. Once a cell loses 17% or more of its volume, it is extruded. Her working hypothesis is that a biochemical cascade set off by the shrinkage contracts motor proteins, which mechanically extrude the cell.&lt;/p&gt;
    &lt;p&gt;In this way, bioelectrical flow across cell membranes lets tissues test which cells are the least healthy and mark them for extrusion. “They’re always pushing against each other and bullying each other. And what they’re doing is probing each other for which one’s the weakest link,” Rosenblatt said. “It’s a community effect.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Evolution as Electrician&lt;/head&gt;
    &lt;p&gt;At the University of California, San Diego, the biophysicist Gürol Süel studies electricity in bacterial biofilms, which are collectives composed of single-celled bacteria that can also survive independently. The signaling that Rosenblatt and her team described in human tissues has several things in common with electrical mechanisms Süel has described in microbes — and which appear again and again across the tree of life.&lt;/p&gt;
    &lt;p&gt;“It’s a very elegant study, very nice results,” he said of the new research. “And conceptually, it makes sense.”&lt;/p&gt;
    &lt;p&gt;Suel Lab&lt;/p&gt;
    &lt;p&gt;Electricity increasingly appears to be one of evolution’s go-to solutions for integrating multiple streams of information. Epithelial tissues use it to keep tabs on crowding. Neurons compile input signals from multiple sources into a spiking output. A Venus flytrap snaps shut when sensory hairs with touch-sensitive ion channels react to prey. These channels are tuned to trigger a voltage spike and tell the trap to close only if stimulated multiple times in rapid succession.&lt;/p&gt;
    &lt;p&gt;“The membrane potential is so fundamental, and it is very fast,” Süel said. While switching genes on and off or upping protein production could take minutes or hours, a membrane potential can flip in fractions of a second. “It tells you, in one glance almost, about the state of the cell,” he added.&lt;/p&gt;
    &lt;p&gt;Ten years ago, Süel and his team showed that microbes in biofilms can spike their membrane potentials to communicate, just as neurons do. Since then, they’ve shown that biofilms use electricity to coordinate tasks, prevent runaway growth, and invite free-swimming bacteria to join the collective. Bioelectricity can even help them avoid falling victim to the tragedy of the commons: Two biofilms sharing scarce food can send electrical signals to each other to take turns eating.&lt;/p&gt;
    &lt;p&gt;Multicellular animals, too, use electrical signaling to organize themselves. Zhang, of Purdue, studies bioelectrical signaling in zebra fish, which develop striking extra-long tails when a certain ion channel is mutated. This suggests that electrical signaling somehow tells tissues in a developing embryo how long to grow. Michael Levin, a researcher at Tufts University, has blocked cell channels to manipulate the membrane potentials of developing worm embryos, causing genetically identical worms to develop different body plans. And last year, Barriga and his colleagues showed that frog embryos generate natural electric fields that guide the migration of specific stem cells to their proper locations in the developing embryo.&lt;/p&gt;
    &lt;p&gt;The failure of bioelectric processes might be an overlooked cause of disease. Cancerous cells tend to have different membrane potentials than healthy ones, and Levin has argued that some cancers might result from a breakdown in multicellularity that happens when cells can no longer use electricity to coordinate. For example, maybe they can no longer communicate the message “I’m struggling and should be extruded,” and the result is the uncontrolled growth and, ultimately, a tumor.&lt;/p&gt;
    &lt;p&gt;Süel is convinced that bioelectricity is as old as life itself. Indeed, an electric current drives the molecular turbines that synthesize life’s universal energy currency, ATP, in every cell alive today. One leading origin-of-life scenario places the beginning at deep-sea hydrothermal vents. There, natural currents of positively charged protons could have served as a kind of primordial membrane potential and powered prebiotic chemical reactions. But whether life started with such a spark or not, bioelectricity’s ubiquity suggests it has deep evolutionary roots that we’re just beginning to unearth.&lt;/p&gt;
    &lt;p&gt;“There are a lot of interesting things that cells are probably doing, just like this paper showed, that we just don’t know yet,” Süel said. “We have not uncovered even half of this. … There’s a lot of opportunity for discovery.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/cells-use-bioelectricity-to-coordinate-and-make-group-decisions-20260112/"/><published>2026-02-01T00:00:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46842603</id><title>List animals until failure</title><updated>2026-02-01T19:42:29.738378+00:00</updated><content>&lt;doc fingerprint="ca67e2b6a5f5fc5"&gt;
  &lt;main&gt;
    &lt;p&gt;Animals must have Wikipedia articles.&lt;/p&gt;
    &lt;p&gt;You have limited time, but get more time for each animal listed. When the timer runs out, that's game over.&lt;/p&gt;
    &lt;p&gt;No overlapping terms. For example, if you list “bear” and “polar bear”, you get no point (or time bonus) for the latter. But you can still get a point for a second kind of bear. Order doesn't matter.&lt;/p&gt;
    &lt;p&gt;Ignore the extraneous visuals. Focus on naming animals.&lt;/p&gt;
    &lt;p&gt;Score: 0&lt;/p&gt;
    &lt;p&gt;By Vivian Rose.&lt;/p&gt;
    &lt;p&gt;Uses Wikipedia and Wikidata, plus a lot of hand-tuning. No LLMs involved.&lt;/p&gt;
    &lt;p&gt;Contact me with bug reports, questions, suggestions, and praise.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rose.systems/animalist/"/><published>2026-02-01T01:03:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46844350</id><title>The Book of PF, 4th edition</title><updated>2026-02-01T19:42:29.620773+00:00</updated><content>&lt;doc fingerprint="b5d87975c61aab81"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Download Chapter 3: Into the Real World&lt;/p&gt;
      &lt;p&gt;The OpenBSD packet filter, PF, is central to the OpenBSD and FreeBSD network toolbox. With more services placing high demands on bandwidth and an increasingly hostile Internet environment, no sysadmin can afford to be without PF expertise.&lt;/p&gt;
      &lt;p&gt;The fourth edition of The Book of PF covers the most up-to-date developments in PF, including new content on IPv6, dual stack configurations, the “queues and priorities” traffic-shaping system, NAT and redirection, wireless networking, spam fighting, failover provisioning, logging, and more. &lt;/p&gt;
      &lt;p&gt;You’ll also learn how to:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Create rulesets for all kinds of network traffic, IPv4 and IPv6 both, whether crossing a simple LAN, hiding behind NAT, traversing DMZs, or spanning bridges or wider networks&lt;/item&gt;
        &lt;item&gt;Set up wireless networks with access points, and lock them down using authpf and special access restrictions&lt;/item&gt;
        &lt;item&gt;Maximize flexibility and service availability via CARP, relayd, and redirection&lt;/item&gt;
        &lt;item&gt;Build adaptive firewalls to proactively defend against attackers and spammers&lt;/item&gt;
        &lt;item&gt;Harness OpenBSD’s latest traffic-shaping system to keep your network responsive, or use ALTQ and Dummynet configurations on FreeBSD to full effect&lt;/item&gt;
        &lt;item&gt;Stay in control of your traffic with monitoring and visualization tools (including NetFlow)&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;The Book of PF is the essential guide to building a secure network with PF. With a little effort and this book, you’ll be well prepared to unlock PF’s full potential.&lt;/p&gt;
      &lt;p&gt;Covers OpenBSD 7.x, FreeBSD 14.x, and NetBSD 10.x&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nostarch.com/book-of-pf-4th-edition"/><published>2026-02-01T07:50:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46844822</id><title>What I learned building an opinionated and minimal coding agent</title><updated>2026-02-01T19:42:28.881775+00:00</updated><content>&lt;doc fingerprint="25589f5228878aca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What I learned building an opinionated and minimal coding agent&lt;/head&gt;
    &lt;p&gt;2025-11-30&lt;/p&gt;
    &lt;head rend="h1"&gt;Table of contents&lt;/head&gt;
    &lt;p&gt;In the past three years, I've been using LLMs for assisted coding. If you read this, you probably went through the same evolution: from copying and pasting code into ChatGPT, to Copilot auto-completions (which never worked for me), to Cursor, and finally the new breed of coding agent harnesses like Claude Code, Codex, Amp, Droid, and opencode that became our daily drivers in 2025.&lt;/p&gt;
    &lt;p&gt;I preferred Claude Code for most of my work. It was the first thing I tried back in April after using Cursor for a year and a half. Back then, it was much more basic. That fit my workflow perfectly, because I'm a simple boy who likes simple, predictable tools. Over the past few months, Claude Code has turned into a spaceship with 80% of functionality I have no use for. The system prompt and tools also change on every release, which breaks my workflows and changes model behavior. I hate that. Also, it flickers.&lt;/p&gt;
    &lt;p&gt;I've also built a bunch of agents over the years, of various complexity. For example, Sitegeist, my little browser-use agent, is essentially a coding agent that lives inside the browser. In all that work, I learned that context engineering is paramount. Exactly controlling what goes into the model's context yields better outputs, especially when it's writing code. Existing harnesses make this extremely hard or impossible by injecting stuff behind your back that isn't even surfaced in the UI.&lt;/p&gt;
    &lt;p&gt;Speaking of surfacing things, I want to inspect every aspect of my interactions with the model. Basically no harness allows that. I also want a cleanly documented session format I can post-process automatically, and a simple way to build alternative UIs on top of the agent core. While some of this is possible with existing harnesses, the APIs smell like organic evolution. These solutions accumulated baggage along the way, which shows in the developer experience. I'm not blaming anyone for this. If tons of people use your shit and you need some sort of backwards compatibility, that's the price you pay.&lt;/p&gt;
    &lt;p&gt;I've also dabbled in self-hosting, both locally and on DataCrunch. While some harnesses like opencode support self-hosted models, it usually doesn't work well. Mostly because they rely on libraries like the Vercel AI SDK, which doesn't play nice with self-hosted models for some reason, specifically when it comes to tool calling.&lt;/p&gt;
    &lt;p&gt;So what's an old guy yelling at Claudes going to do? He's going to write his own coding agent harness and give it a name that's entirely un-Google-able, so there will never be any users. Which means there will also never be any issues on the GitHub issue tracker. How hard can it be?&lt;/p&gt;
    &lt;p&gt;To make this work, I needed to build:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pi-ai: A unified LLM API with multi-provider support (Anthropic, OpenAI, Google, xAI, Groq, Cerebras, OpenRouter, and any OpenAI-compatible endpoint), streaming, tool calling with TypeBox schemas, thinking/reasoning support, seamless cross-provider context handoffs, and token and cost tracking.&lt;/item&gt;
      &lt;item&gt;pi-agent-core: An agent loop that handles tool execution, validation, and event streaming.&lt;/item&gt;
      &lt;item&gt;pi-tui: A minimal terminal UI framework with differential rendering, synchronized output for (almost) flicker-free updates, and components like editors with autocomplete and markdown rendering.&lt;/item&gt;
      &lt;item&gt;pi-coding-agent: The actual CLI that wires it all together with session management, custom tools, themes, and project context files.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My philosophy in all of this was: if I don't need it, it won't be built. And I don't need a lot of things.&lt;/p&gt;
    &lt;head rend="h2"&gt;pi-ai and pi-agent-core&lt;/head&gt;
    &lt;p&gt;I'm not going to bore you with the API specifics of this package. You can read it all in the README.md. Instead, I want to document the problems I ran into while creating a unified LLM API and how I resolved them. I'm not claiming my solutions are the best, but they've been working pretty well throughout various agentic and non-agentic LLM projects.&lt;/p&gt;
    &lt;head rend="h3"&gt;There. Are. Four. Ligh... APIs&lt;/head&gt;
    &lt;p&gt;There's really only four APIs you need to speak to talk to pretty much any LLM provider: OpenAI's Completions API, their newer Responses API, Anthropic's Messages API, and Google's Generative AI API.&lt;/p&gt;
    &lt;p&gt;They're all pretty similar in features, so building an abstraction on top of them isn't rocket science. There are, of course, provider-specific peculiarities you have to care for. That's especially true for the Completions API, which is spoken by pretty much all providers, but each of them has a different understanding of what this API should do. For example, while OpenAI doesn't support reasoning traces in their Completions API, other providers do in their version of the Completions API. This is also true for inference engines like llama.cpp, Ollama, vLLM, and LM Studio.&lt;/p&gt;
    &lt;p&gt;For example, in openai-completions.ts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cerebras, xAI, Mistral, and Chutes don't like the &lt;code&gt;store&lt;/code&gt;field&lt;/item&gt;
      &lt;item&gt;Mistral and Chutes use &lt;code&gt;max_tokens&lt;/code&gt;instead of&lt;code&gt;max_completion_tokens&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Cerebras, xAI, Mistral, and Chutes don't support the &lt;code&gt;developer&lt;/code&gt;role for system prompts&lt;/item&gt;
      &lt;item&gt;Grok models don't like &lt;code&gt;reasoning_effort&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Different providers return reasoning content in different fields (&lt;code&gt;reasoning_content&lt;/code&gt;vs&lt;code&gt;reasoning&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To ensure all features actually work across the gazillion of providers, pi-ai has a pretty extensive test suite covering image inputs, reasoning traces, tool calling, and other features you'd expect from an LLM API. Tests run across all supported providers and popular models. While this is a good effort, it still won't guarantee that new models and providers will just work out of the box.&lt;/p&gt;
    &lt;p&gt;Another big difference is how providers report tokens and cache reads/writes. Anthropic has the sanest approach, but generally it's the Wild West. Some report token counts at the start of the SSE stream, others only at the end, making accurate cost tracking impossible if a request is aborted. To add insult to injury, you can't provide a unique ID to later correlate with their billing APIs and figure out which of your users consumed how many tokens. So pi-ai does token and cache tracking on a best-effort basis. Good enough for personal use, but not for accurate billing if you have end users consuming tokens through your service.&lt;/p&gt;
    &lt;p&gt;Special shout out to Google who to this date seem to not support tool call streaming which is extremely Google.&lt;/p&gt;
    &lt;p&gt;pi-ai also works in the browser, which is useful for building web-based interfaces. Some providers make this especially easy by supporting CORS, specifically Anthropic and xAI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context handoff&lt;/head&gt;
    &lt;p&gt;Context handoff between providers was a feature pi-ai was designed for from the start. Since each provider has their own way of tracking tool calls and thinking traces, this can only be a best-effort thing. For example, if you switch from Anthropic to OpenAI mid-session, Anthropic thinking traces are converted to content blocks inside assistant messages, delimited by &lt;code&gt;&amp;lt;thinking&amp;gt;&amp;lt;/thinking&amp;gt;&lt;/code&gt; tags. This may or may not be sensible, because the thinking traces returned by Anthropic and OpenAI don't actually represent what's happening behind the scenes.&lt;/p&gt;
    &lt;p&gt;These providers also insert signed blobs into the event stream that you have to replay on subsequent requests containing the same messages. This also applies when switching models within a provider. It makes for a cumbersome abstraction and transformation pipeline in the background.&lt;/p&gt;
    &lt;p&gt;I'm happy to report that cross-provider context handoff and context serialization/deserialization work pretty well in pi-ai:&lt;/p&gt;
    &lt;code&gt;import { getModel, complete, Context } from '@mariozechner/pi-ai';

// Start with Claude
const claude = getModel('anthropic', 'claude-sonnet-4-5');
const context: Context = {
  messages: []
};

context.messages.push({ role: 'user', content: 'What is 25 * 18?' });
const claudeResponse = await complete(claude, context, {
  thinkingEnabled: true
});
context.messages.push(claudeResponse);

// Switch to GPT - it will see Claude's thinking as &amp;lt;thinking&amp;gt; tagged text
const gpt = getModel('openai', 'gpt-5.1-codex');
context.messages.push({ role: 'user', content: 'Is that correct?' });
const gptResponse = await complete(gpt, context);
context.messages.push(gptResponse);

// Switch to Gemini
const gemini = getModel('google', 'gemini-2.5-flash');
context.messages.push({ role: 'user', content: 'What was the question?' });
const geminiResponse = await complete(gemini, context);

// Serialize context to JSON (for storage, transfer, etc.)
const serialized = JSON.stringify(context);

// Later: deserialize and continue with any model
const restored: Context = JSON.parse(serialized);
restored.messages.push({ role: 'user', content: 'Summarize our conversation' });
const continuation = await complete(claude, restored);
&lt;/code&gt;
    &lt;head rend="h3"&gt;We live in a multi-model world&lt;/head&gt;
    &lt;p&gt;Speaking of models, I wanted a typesafe way of specifying them in the &lt;code&gt;getModel&lt;/code&gt; call. For that I needed a model registry that I could turn into TypeScript types. I'm parsing data from both OpenRouter and models.dev (created by the opencode folks, thanks for that, it's super useful) into models.generated.ts. This includes token costs and capabilities like image inputs and thinking support.&lt;/p&gt;
    &lt;p&gt;And if I ever need to add a model that's not in the registry, I wanted a type system that makes it easy to create new ones. This is especially useful when working with self-hosted models, new releases that aren't yet on models.dev or OpenRouter, or trying out one of the more obscure LLM providers:&lt;/p&gt;
    &lt;code&gt;import { Model, stream } from '@mariozechner/pi-ai';

const ollamaModel: Model&amp;lt;'openai-completions'&amp;gt; = {
  id: 'llama-3.1-8b',
  name: 'Llama 3.1 8B (Ollama)',
  api: 'openai-completions',
  provider: 'ollama',
  baseUrl: 'http://localhost:11434/v1',
  reasoning: false,
  input: ['text'],
  cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
  contextWindow: 128000,
  maxTokens: 32000
};

const response = await stream(ollamaModel, context, {
  apiKey: 'dummy' // Ollama doesn't need a real key
});
&lt;/code&gt;
    &lt;p&gt;Many unified LLM APIs completely ignore providing a way to abort requests. This is entirely unacceptable if you want to integrate your LLM into any kind of production system. Many unified LLM APIs also don't return partial results to you, which is kind of ridiculous. pi-ai was designed from the beginning to support aborts throughout the entire pipeline, including tool calls. Here's how it works:&lt;/p&gt;
    &lt;code&gt;import { getModel, stream } from '@mariozechner/pi-ai';

const model = getModel('openai', 'gpt-5.1-codex');
const controller = new AbortController();

// Abort after 2 seconds
setTimeout(() =&amp;gt; controller.abort(), 2000);

const s = stream(model, {
  messages: [{ role: 'user', content: 'Write a long story' }]
}, {
  signal: controller.signal
});

for await (const event of s) {
  if (event.type === 'text_delta') {
    process.stdout.write(event.delta);
  } else if (event.type === 'error') {
    console.log(`${event.reason === 'aborted' ? 'Aborted' : 'Error'}:`, event.error.errorMessage);
  }
}

// Get results (may be partial if aborted)
const response = await s.result();
if (response.stopReason === 'aborted') {
  console.log('Partial content:', response.content);
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Structured split tool results&lt;/head&gt;
    &lt;p&gt;Another abstraction I haven't seen in any unified LLM API is splitting tool results into a portion handed to the LLM and a portion for UI display. The LLM portion is generally just text or JSON, which doesn't necessarily contain all the information you'd want to show in a UI. It also sucks hard to parse textual tool outputs and restructure them for display in a UI. pi-ai's tool implementation allows returning both content blocks for the LLM and separate content blocks for UI rendering. Tools can also return attachments like images that get attached in the native format of the respective provider. Tool arguments are automatically validated using TypeBox schemas and AJV, with detailed error messages when validation fails:&lt;/p&gt;
    &lt;code&gt;import { Type, AgentTool } from '@mariozechner/pi-ai';

const weatherSchema = Type.Object({
  city: Type.String({ minLength: 1 }),
});

const weatherTool: AgentTool&amp;lt;typeof weatherSchema, { temp: number }&amp;gt; = {
  name: 'get_weather',
  description: 'Get current weather for a city',
  parameters: weatherSchema,
  execute: async (toolCallId, args) =&amp;gt; {
    const temp = Math.round(Math.random() * 30);
    return {
      // Text for the LLM
      output: `Temperature in ${args.city}: ${temp}Â°C`,
      // Structured data for the UI
      details: { temp }
    };
  }
};

// Tools can also return images
const chartTool: AgentTool = {
  name: 'generate_chart',
  description: 'Generate a chart from data',
  parameters: Type.Object({ data: Type.Array(Type.Number()) }),
  execute: async (toolCallId, args) =&amp;gt; {
    const chartImage = await generateChartImage(args.data);
    return {
      content: [
        { type: 'text', text: `Generated chart with ${args.data.length} data points` },
        { type: 'image', data: chartImage.toString('base64'), mimeType: 'image/png' }
      ]
    };
  }
};
&lt;/code&gt;
    &lt;p&gt;What's still lacking is tool result streaming. Imagine a bash tool where you want to display ANSI sequences as they come in. That's currently not possible, but it's a simple fix that will eventually make it into the package.&lt;/p&gt;
    &lt;p&gt;Partial JSON parsing during tool call streaming is essential for good UX. As the LLM streams tool call arguments, pi-ai progressively parses them so you can show partial results in the UI before the call completes. For example, you can display a diff streaming in as the agent rewrites a file.&lt;/p&gt;
    &lt;head rend="h3"&gt;Minimal agent scaffold&lt;/head&gt;
    &lt;p&gt;Finally, pi-ai provides an agent loop that handles the full orchestration: processing user messages, executing tool calls, feeding results back to the LLM, and repeating until the model produces a response without tool calls. The loop also supports message queuing via a callback: after each turn, it asks for queued messages and injects them before the next assistant response. The loop emits events for everything, making it easy to build reactive UIs.&lt;/p&gt;
    &lt;p&gt;The agent loop doesn't let you specify max steps or similar knobs you'd find in other unified LLM APIs. I never found a use case for that, so why add it? The loop just loops until the agent says it's done. On top of the loop, however, pi-agent-core provides an &lt;code&gt;Agent&lt;/code&gt; class with actually useful stuff: state management, simplified event subscriptions, message queuing with two modes (one-at-a-time or all-at-once), attachment handling (images, documents), and a transport abstraction that lets you run the agent either directly or through a proxy.&lt;/p&gt;
    &lt;p&gt;Am I happy with pi-ai? For the most part, yes. Like any unifying API, it can never be perfect due to leaky abstractions. But it's been used in seven different production projects and has served me extremely well.&lt;/p&gt;
    &lt;p&gt;Why build this instead of using the Vercel AI SDK? Armin's blog post mirrors my experience. Building on top of the provider SDKs directly gives me full control and lets me design the APIs exactly as I want, with a much smaller surface area. Armin's blog gives you a more in-depth treatise on the reasons for building your own. Go read that.&lt;/p&gt;
    &lt;head rend="h2"&gt;pi-tui&lt;/head&gt;
    &lt;p&gt;I grew up in the DOS era, so terminal user interfaces are what I grew up with. From the fancy setup programs for Doom to Borland products, TUIs were with me until the end of the 90s. And boy was I fucking happy when I eventually switched to a GUI operating system. While TUIs are mostly portable and easily streamable, they also suck at information density. Having said all that, I thought starting with a terminal user interface for pi makes the most sense. I could strap on a GUI later whenever I felt like I needed to.&lt;/p&gt;
    &lt;p&gt;So why build my own TUI framework? I've looked into the alternatives like Ink, Blessed, OpenTUI, and so on. I'm sure they're all fine in their own way, but I definitely don't want to write my TUI like a React app. Blessed seems to be mostly unmaintained, and OpenTUI is explicitly not production ready. Also, writing my own TUI framework on top of Node.js seemed like a fun little challenge.&lt;/p&gt;
    &lt;head rend="h3"&gt;Two kinds of TUIs&lt;/head&gt;
    &lt;p&gt;Writing a terminal user interface is not rocket science per se. You just have to pick your poison. There's basically two ways to do it. One is to take ownership of the terminal viewport (the portion of the terminal contents you can actually see) and treat it like a pixel buffer. Instead of pixels you have cells that contain characters with background color, foreground color, and styling like italic and bold. I call these full screen TUIs. Amp and opencode use this approach.&lt;/p&gt;
    &lt;p&gt;The drawback is that you lose the scrollback buffer, which means you have to implement custom search. You also lose scrolling, which means you have to simulate scrolling within the viewport yourself. While this is not hard to implement, it means you have to re-implement all the functionality your terminal emulator already provides. Mouse scrolling specifically always feels kind of off in such TUIs.&lt;/p&gt;
    &lt;p&gt;The second approach is to just write to the terminal like any CLI program, appending content to the scrollback buffer, only occasionally moving the "rendering cursor" back up a little within the visible viewport to redraw things like animated spinners or a text edit field. It's not exactly that simple, but you get the idea. This is what Claude Code, Codex, and Droid do.&lt;/p&gt;
    &lt;p&gt;Coding agents have this nice property that they're basically a chat interface. The user writes a prompt, followed by replies from the agent and tool calls and their results. Everything is nicely linear, which lends itself well to working with the "native" terminal emulator. You get to use all the built-in functionality like natural scrolling and search within the scrollback buffer. It also limits what your TUI can do to some degree, which I find charming because constraints make for minimal programs that just do what they're supposed to do without superfluous fluff. This is the direction I picked for pi-tui.&lt;/p&gt;
    &lt;head rend="h3"&gt;Retained mode UI&lt;/head&gt;
    &lt;p&gt;If you've done any GUI programming, you've probably heard of retained mode vs immediate mode. In a retained mode UI, you build up a tree of components that persist across frames. Each component knows how to render itself and can cache its output if nothing changed. In an immediate mode UI, you redraw everything from scratch each frame (though in practice, immediate mode UIs also do caching, otherwise they'd fall apart).&lt;/p&gt;
    &lt;p&gt;pi-tui uses a simple retained mode approach. A &lt;code&gt;Component&lt;/code&gt; is just an object with a &lt;code&gt;render(width)&lt;/code&gt; method that returns an array of strings (lines that fit the viewport horizontally, with ANSI escape codes for colors and styling) and an optional &lt;code&gt;handleInput(data)&lt;/code&gt; method for keyboard input. A &lt;code&gt;Container&lt;/code&gt; holds a list of components arranged vertically and collects all their rendered lines. The &lt;code&gt;TUI&lt;/code&gt; class is itself a container that orchestrates everything.&lt;/p&gt;
    &lt;p&gt;When the TUI needs to update the screen, it asks each component to render. Components can cache their output: an assistant message that's fully streamed doesn't need to re-parse markdown and re-render ANSI sequences every time. It just returns the cached lines. Containers collect lines from all children. The TUI gathers all these lines and compares them to the lines it previously rendered for the previous component tree. It keeps a backbuffer of sorts, remembering what was written to the scrollback buffer.&lt;/p&gt;
    &lt;p&gt;Then it only redraws what changed, using a method I call differential rendering. I'm very bad with names, and this likely has an official name.&lt;/p&gt;
    &lt;head rend="h3"&gt;Differential rendering&lt;/head&gt;
    &lt;p&gt;Here's a simplified demo that illustrates what exactly gets redrawn.&lt;/p&gt;
    &lt;p&gt;The algorithm is simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;First render: Just output all lines to the terminal&lt;/item&gt;
      &lt;item&gt;Width changed: Clear screen completely and re-render everything (soft wrapping changes)&lt;/item&gt;
      &lt;item&gt;Normal update: Find the first line that differs from what's on screen, move the cursor to that line, and re-render from there to the end&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There's one catch: if the first changed line is above the visible viewport (the user scrolled up), we have to do a full clear and re-render. The terminal doesn't let you write to the scrollback buffer above the viewport.&lt;/p&gt;
    &lt;p&gt;To prevent flicker during updates, pi-tui wraps all rendering in synchronized output escape sequences (&lt;code&gt;CSI ?2026h&lt;/code&gt; and &lt;code&gt;CSI ?2026l&lt;/code&gt;). This tells the terminal to buffer all the output and display it atomically. Most modern terminals support this.&lt;/p&gt;
    &lt;p&gt;How well does it work and how much does it flicker? In any capable terminal like Ghostty or iTerm2, this works brilliantly and you never see any flicker. In less fortunate terminal implementations like VS Code's built-in terminal, you will get some flicker depending on the time of day, your display size, your window size, and so on. Given that I'm very accustomed to Claude Code, I haven't spent any more time optimizing this. I'm happy with the little flicker I get in VS Code. I wouldn't feel at home otherwise. And it still flickers less than Claude Code.&lt;/p&gt;
    &lt;p&gt;How wasteful is this approach? We store an entire scrollback buffer worth of previously rendered lines, and we re-render lines every time the TUI is asked to render itself. That's alleviated with the caching I described above, so the re-rendering isn't a big deal. We still have to compare a lot of lines with each other. Realistically, on computers younger than 25 years, this is not a big deal, both in terms of performance and memory use (a few hundred kilobytes for very large sessions). Thanks V8. What I get in return is a dead simple programming model that lets me iterate quickly.&lt;/p&gt;
    &lt;head rend="h2"&gt;pi-coding-agent&lt;/head&gt;
    &lt;p&gt;I don't need to explain what features you should expect from a coding agent harness. pi comes with most creature comforts you're used to from other tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Runs on Windows, Linux, and macOS (or anything with a Node.js runtime and a terminal)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multi-provider support with mid-session model switching&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Session management with continue, resume, and branching&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Project context files (AGENTS.md) loaded hierarchically from global to project-specific&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Slash commands for common operations&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom slash commands as markdown templates with argument support&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;OAuth authentication for Claude Pro/Max subscriptions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom model and provider configuration via JSON&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customizable themes with live reload&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Editor with fuzzy file search, path completion, drag &amp;amp; drop, and multi-line paste&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Message queuing while the agent is working&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Image support for vision-capable models&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HTML export of sessions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Headless operation via JSON streaming and RPC mode&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Full cost and token tracking&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want the full rundown, read the README. What's more interesting is where pi deviates from other harnesses in philosophy and implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Minimal system prompt&lt;/head&gt;
    &lt;p&gt;Here's the system prompt:&lt;/p&gt;
    &lt;code&gt;You are an expert coding assistant. You help users with coding tasks by reading files, executing commands, editing code, and writing new files.

Available tools:
- read: Read file contents
- bash: Execute bash commands
- edit: Make surgical edits to files
- write: Create or overwrite files

Guidelines:
- Use bash for file operations like ls, grep, find
- Use read to examine files before editing
- Use edit for precise changes (old text must match exactly)
- Use write only for new files or complete rewrites
- When summarizing your actions, output plain text directly - do NOT use cat or bash to display what you did
- Be concise in your responses
- Show file paths clearly when working with files

Documentation:
- Your own documentation (including custom model setup and theme creation) is at: /path/to/README.md
- Read it when users ask about features, configuration, or setup, and especially if the user asks you to add a custom model or provider, or create a custom theme.
&lt;/code&gt;
    &lt;p&gt;That's it. The only thing that gets injected at the bottom is your AGENTS.md file. Both the global one that applies to all your sessions and the project-specific one stored in your project directory. This is where you can customize pi to your liking. You can even replace the full system prompt if you want to. Compared to, for example, Claude Code's system prompt, Codex's system prompt, or opencode's model-specific prompts (the Claude one is a cut-down version of the original Claude Code prompt they copied).&lt;/p&gt;
    &lt;p&gt;You might think this is crazy. In all likelihood, the models have some training on their native coding harness. So using the native system prompt or something close to it like opencode would be most ideal. But it turns out that all the frontier models have been RL-trained up the wazoo, so they inherently understand what a coding agent is. There does not appear to be a need for 10,000 tokens of system prompt, as we'll find out later in the benchmark section, and as I've anecdotally found out by exclusively using pi for the past few weeks. Amp, while copying some parts of the native system prompts, seems to also do just fine with their own prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Minimal toolset&lt;/head&gt;
    &lt;p&gt;Here are the tool definitions:&lt;/p&gt;
    &lt;code&gt;read
  Read the contents of a file. Supports text files and images (jpg, png,
  gif, webp). Images are sent as attachments. For text files, defaults to
  first 2000 lines. Use offset/limit for large files.
  - path: Path to the file to read (relative or absolute)
  - offset: Line number to start reading from (1-indexed)
  - limit: Maximum number of lines to read

write
  Write content to a file. Creates the file if it doesn't exist, overwrites
  if it does. Automatically creates parent directories.
  - path: Path to the file to write (relative or absolute)
  - content: Content to write to the file

edit
  Edit a file by replacing exact text. The oldText must match exactly
  (including whitespace). Use this for precise, surgical edits.
  - path: Path to the file to edit (relative or absolute)
  - oldText: Exact text to find and replace (must match exactly)
  - newText: New text to replace the old text with

bash
  Execute a bash command in the current working directory. Returns stdout
  and stderr. Optionally provide a timeout in seconds.
  - command: Bash command to execute
  - timeout: Timeout in seconds (optional, no default timeout)
&lt;/code&gt;
    &lt;p&gt;There are additional read-only tools (grep, find, ls) if you want to restrict the agent from modifying files or running arbitrary commands. By default these are disabled, so the agent only gets the four tools above.&lt;/p&gt;
    &lt;p&gt;As it turns out, these four tools are all you need for an effective coding agent. Models know how to use bash and have been trained on the read, write, and edit tools with similar input schemas. Compare this to Claude Code's tool definitions or opencode's tool definitions (which are clearly derived from Claude Code's, same structure, same examples, same git commit flow). Notably, Codex's tool definitions are similarly minimal to pi's.&lt;/p&gt;
    &lt;p&gt;pi's system prompt and tool definitions together come in below 1000 tokens.&lt;/p&gt;
    &lt;head rend="h3"&gt;YOLO by default&lt;/head&gt;
    &lt;p&gt;pi runs in full YOLO mode and assumes you know what you're doing. It has unrestricted access to your filesystem and can execute any command without permission checks or safety rails. No permission prompts for file operations or commands. No pre-checking of bash commands by Haiku for malicious content. Full filesystem access. Can execute any command with your user privileges.&lt;/p&gt;
    &lt;p&gt;If you look at the security measures in other coding agents, they're mostly security theater. As soon as your agent can write code and run code, it's pretty much game over. The only way you could prevent exfiltration of data would be to cut off all network access for the execution environment the agent runs in, which makes the agent mostly useless. An alternative is allow-listing domains, but this can also be worked around through other means.&lt;/p&gt;
    &lt;p&gt;Simon Willison has written extensively about this problem. His "dual LLM" pattern attempts to address confused deputy attacks and data exfiltration, but even he admits "this solution is pretty bad" and introduces enormous implementation complexity. The core issue remains: if an LLM has access to tools that can read private data and make network requests, you're playing whack-a-mole with attack vectors.&lt;/p&gt;
    &lt;p&gt;Since we cannot solve this trifecta of capabilities (read data, execute code, network access), pi just gives in. Everybody is running in YOLO mode anyways to get any productive work done, so why not make it the default and only option?&lt;/p&gt;
    &lt;p&gt;By default, pi has no web search or fetch tool. However, it can use &lt;code&gt;curl&lt;/code&gt; or read files from disk, both of which provide ample surface area for prompt injection attacks. Malicious content in files or command outputs can influence behavior. If you're uncomfortable with full access, run pi inside a container or use a different tool if you need (faux) guardrails.&lt;/p&gt;
    &lt;head rend="h3"&gt;No built-in to-dos&lt;/head&gt;
    &lt;p&gt;pi does not and will not support built-in to-dos. In my experience, to-do lists generally confuse models more than they help. They add state that the model has to track and update, which introduces more opportunities for things to go wrong.&lt;/p&gt;
    &lt;p&gt;If you need task tracking, make it externally stateful by writing to a file:&lt;/p&gt;
    &lt;code&gt;# TODO.md

- [x] Implement user authentication
- [x] Add database migrations
- [ ] Write API documentation
- [ ] Add rate limiting
&lt;/code&gt;
    &lt;p&gt;The agent can read and update this file as needed. Using checkboxes keeps track of what's done and what remains. Simple, visible, and under your control.&lt;/p&gt;
    &lt;head rend="h3"&gt;No plan mode&lt;/head&gt;
    &lt;p&gt;pi does not and will not have a built-in plan mode. Telling the agent to think through a problem together with you, without modifying files or executing commands, is generally sufficient.&lt;/p&gt;
    &lt;p&gt;If you need persistent planning across sessions, write it to a file:&lt;/p&gt;
    &lt;code&gt;# PLAN.md

## Goal
Refactor authentication system to support OAuth

## Approach
1. Research OAuth 2.0 flows
2. Design token storage schema
3. Implement authorization server endpoints
4. Update client-side login flow
5. Add tests

## Current Step
Working on step 3 - authorization endpoints
&lt;/code&gt;
    &lt;p&gt;The agent can read, update, and reference the plan as it works. Unlike ephemeral planning modes that only exist within a session, file-based plans can be shared across sessions, and can be versioned with your code.&lt;/p&gt;
    &lt;p&gt;Funnily enough, Claude Code now has a Plan Mode that's essentially read-only analysis, and it will eventually write a markdown file to disk. And you can basically not use plan mode without approving a shit ton of command invocations, because without that, planning is basically impossible.&lt;/p&gt;
    &lt;p&gt;The difference with pi is that I have full observability of everything. I get to see which sources the agent actually looked at and which ones it totally missed. In Claude Code, the orchestrating Claude instance usually spawns a sub-agent and you have zero visibility into what that sub-agent does. I get to see the markdown file immediately. I can edit it collaboratively with the agent. In short, I need observability for planning and I don't get that with Claude Code's plan mode.&lt;/p&gt;
    &lt;p&gt;If you must restrict the agent during planning, you can specify which tools it has access to via the CLI:&lt;/p&gt;
    &lt;code&gt;pi --tools read,grep,find,ls
&lt;/code&gt;
    &lt;p&gt;This gives you read-only mode for exploration and planning without the agent modifying anything or being able to run bash commands. You won't be happy with that though.&lt;/p&gt;
    &lt;head rend="h3"&gt;No MCP support&lt;/head&gt;
    &lt;p&gt;pi does not and will not support MCP. I've written about this extensively, but the TL;DR is: MCP servers are overkill for most use cases, and they come with significant context overhead.&lt;/p&gt;
    &lt;p&gt;Popular MCP servers like Playwright MCP (21 tools, 13.7k tokens) or Chrome DevTools MCP (26 tools, 18k tokens) dump their entire tool descriptions into your context on every session. That's 7-9% of your context window gone before you even start working. Many of these tools you'll never use in a given session.&lt;/p&gt;
    &lt;p&gt;The alternative is simple: build CLI tools with README files. The agent reads the README when it needs the tool, pays the token cost only when necessary (progressive disclosure), and can use bash to invoke the tool. This approach is composable (pipe outputs, chain commands), easy to extend (just add another script), and token-efficient.&lt;/p&gt;
    &lt;p&gt;Here's how I add web search to pi:&lt;/p&gt;
    &lt;p&gt;I maintain a collection of these tools at github.com/badlogic/agent-tools. Each tool is a simple CLI with a README that the agent reads on demand.&lt;/p&gt;
    &lt;p&gt;If you absolutely must use MCP servers, look into Peter Steinberger's mcporter tool that wraps MCP servers as CLI tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;No background bash&lt;/head&gt;
    &lt;p&gt;pi's bash tool runs commands synchronously. There's no built-in way to start a dev server, run tests in the background, or interact with a REPL while the command is still running.&lt;/p&gt;
    &lt;p&gt;This is intentional. Background process management adds complexity: you need process tracking, output buffering, cleanup on exit, and ways to send input to running processes. Claude Code handles some of this with their background bash feature, but it has poor observability (a common theme with Claude Code) and forces the agent to track running instances without providing a tool to query them. In earlier Claude Code versions, the agent forgot about all its background processes after context compaction and had no way to query them, so you had to manually kill them. This has since been fixed.&lt;/p&gt;
    &lt;p&gt;Use tmux instead. Here's pi debugging a crashing C program in LLDB:&lt;/p&gt;
    &lt;p&gt;How's that for observability? The same approach works for long-running dev servers, watching log output, and similar use cases. And if you wanted to, you could hop into that LLDB session above via tmux and co-debug with the agent. Tmux also gives you a CLI argument to list all active sessions. How nice.&lt;/p&gt;
    &lt;p&gt;There's simply no need for background bash. Claude Code can use tmux too, you know. Bash is all you need.&lt;/p&gt;
    &lt;head rend="h3"&gt;No sub-agents&lt;/head&gt;
    &lt;p&gt;pi does not have a dedicated sub-agent tool. When Claude Code needs to do something complex, it often spawns a sub-agent to handle part of the task. You have zero visibility into what that sub-agent does. It's a black box within a black box. Context transfer between agents is also poor. The orchestrating agent decides what initial context to pass to the sub-agent, and you generally have little control over that. If the sub-agent makes a mistake, debugging is painful because you can't see the full conversation.&lt;/p&gt;
    &lt;p&gt;If you need pi to spawn itself, just ask it to run itself via bash. You could even have it spawn itself inside a tmux session for full observability and the ability to interact with that sub-agent directly.&lt;/p&gt;
    &lt;p&gt;But more importantly: fix your workflow, at least the ones that are all about context gathering. People use sub-agents within a session thinking they're saving context space, which is true. But that's the wrong way to think about sub-agents. Using a sub-agent mid-session for context gathering is a sign you didn't plan ahead. If you need to gather context, do that first in its own session. Create an artifact that you can later use in a fresh session to give your agent all the context it needs without polluting its context window with tool outputs. That artifact can be useful for the next feature too, and you get full observability and steerability, which is important during context gathering.&lt;/p&gt;
    &lt;p&gt;Because despite popular belief, models are still poor at finding all the context needed for implementing a new feature or fixing a bug. I attribute this to models being trained to only read parts of files rather than full files, so they're hesitant to read everything. Which means they miss important context and can't see what they need to properly complete the task.&lt;/p&gt;
    &lt;p&gt;Just look at the pi-mono issue tracker and the pull requests. Many get closed or revised because the agents couldn't fully grasp what's needed. That's not the fault of the contributors, which I truly appreciate because even incomplete PRs help me move faster. It just means we trust our agents too much.&lt;/p&gt;
    &lt;p&gt;I'm not dismissing sub-agents entirely. There are valid use cases. My most common one is code review: I tell pi to spawn itself with a code review prompt (via a custom slash command) and it gets the outputs.&lt;/p&gt;
    &lt;code&gt;---
description: Run a code review sub-agent
---
Spawn yourself as a sub-agent via bash to do a code review: $@

Use `pi --print` with appropriate arguments. If the user specifies a model,
use `--provider` and `--model` accordingly.

Pass a prompt to the sub-agent asking it to review the code for:
- Bugs and logic errors
- Security issues
- Error handling gaps

Do not read the code yourself. Let the sub-agent do that.

Report the sub-agent's findings.
&lt;/code&gt;
    &lt;p&gt;And here's how I use this to review a pull request on GitHub:&lt;/p&gt;
    &lt;p&gt;With a simple prompt, I can select what specific thing I want to review and what model to use. I could even set thinking levels if I wanted to. I can also save out the full review session to a file and hop into that in another pi session if I wanted. Or I can say this is an ephemeral session and it shouldn't be saved to disk. All of that gets translated into a prompt that the main agent reads and based on which it executes itself again via bash. And while I don't get full observability into the inner workings of the sub-agent, I get full observability on its output. Something other harnesses don't really provide, which makes no sense to me.&lt;/p&gt;
    &lt;p&gt;Of course, this is a bit of a simulated use case. In reality, I would just spawn a new pi session and ask it to review the pull request, possibly pull it into a branch locally. After I see its initial review, I give my own review and then we work on it together until it's good. That's the workflow I use to not merge garbage code.&lt;/p&gt;
    &lt;p&gt;Spawning multiple sub-agents to implement various features in parallel is an anti-pattern in my book and doesn't work, unless you don't care if your codebase devolves into a pile of garbage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;I make a lot of grandiose claims, but do I have numerical proof that all the contrarian things I say above actually work? I have my lived experience, but that's hard to transport in a blog post and you'd just have to believe me. So I created a Terminal-Bench 2.0 test run for pi with Claude Opus 4.5 and let it compete against Codex, Cursor, Windsurf, and other coding harnesses with their respective native models. Obviously, we all know benchmarks aren't representative of real-world performance, but it's the best I can provide you as a sort of proof that not everything I say is complete bullshit.&lt;/p&gt;
    &lt;p&gt;I performed a complete run with five trials per task, which makes the results eligible for submission to the leaderboard. I also started a second run that only runs during CET because I found that error rates (and consequently benchmark results) get worse once PST goes online. Here are the results for the first run:&lt;/p&gt;
    &lt;p&gt;And here's pi's placement on the current leaderboard as of December 2nd, 2025:&lt;/p&gt;
    &lt;p&gt;And here's the results.json file I've submitted to the Terminal-Bench folks for inclusion in the leaderboard. The bench runner for pi can be found in this repository if you want to reproduce the results. I suggest you use your Claude plan instead of pay-as-you-go.&lt;/p&gt;
    &lt;p&gt;Finally, here's a little glimpse into the CET-only run:&lt;/p&gt;
    &lt;p&gt;This is going to take another day or so to complete. I will update this blog post once that is done.&lt;/p&gt;
    &lt;p&gt;Also note the ranking of Terminus 2 on the leaderboard. Terminus 2 is the Terminal-Bench team's own minimal agent that just gives the model a tmux session. The model sends commands as text to tmux and parses the terminal output itself. No fancy tools, no file operations, just raw terminal interaction. And it's holding its own against agents with far more sophisticated tooling and works with a diverse set of models. More evidence that a minimal approach can do just as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;In summary&lt;/head&gt;
    &lt;p&gt;Benchmark results are hilarious, but the real proof is in the pudding. And my pudding is my day-to-day work, where pi has been performing admirably. Twitter is full of context engineering posts and blogs, but I feel like none of the harnesses we currently have actually let you do context engineering. pi is my attempt to build myself a tool where I'm in control as much as possible.&lt;/p&gt;
    &lt;p&gt;I'm pretty happy with where pi is. There are a few more features I'd like to add, like compaction or tool result streaming, but I don't think there's much more I'll personally need. Missing compaction hasn't been a problem for me personally. For some reason, I'm able to cram hundreds of exchanges between me and the agent into a single session, which I couldn't do with Claude Code without compaction.&lt;/p&gt;
    &lt;p&gt;That said, I welcome contributions. But as with all my open source projects, I tend to be dictatorial. A lesson I've learned the hard way over the years with my bigger projects. If I close an issue or PR you've sent in, I hope there are no hard feelings. I will also do my best to give you reasons why. I just want to keep this focused and maintainable. If pi doesn't fit your needs, I implore you to fork it. I truly mean it. And if you create something that even better fits my needs, I'll happily join your efforts.&lt;/p&gt;
    &lt;p&gt;I think some of the learnings above transfer to other harnesses as well. Let me know how that goes for you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/"/><published>2026-02-01T09:33:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46844870</id><title>Netbird – Open Source Zero Trust Networking</title><updated>2026-02-01T19:42:28.415367+00:00</updated><link href="https://netbird.io/"/><published>2026-02-01T09:44:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46845103</id><title>FOSDEM 2026 – Open-Source Conference in Brussels – Day#1 Recap</title><updated>2026-02-01T19:42:27.692091+00:00</updated><content>&lt;doc fingerprint="3fc5121dabaf457a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;FOSDEM 2026 – Open Source, Digital Sovereignty, and Europes Future&lt;/head&gt;():&lt;p&gt;FOSDEM, the Free and Open Source Developers’ European Meeting, is an annual pilgrimage for open source enthusiasts from all over the world. What started as a small gathering in 2000, originally named the Open Source Developers of Europe Meeting (OSDEM), has grown into one of the most significant conferences dedicated to free and open source software.&lt;/p&gt;&lt;p&gt;In 2026, FOSDEM felt more purposeful than ever. The conference clearly reflected a growing awareness around digital sovereignty and Europe’s technological future.&lt;/p&gt;&lt;p&gt;Self hosted solutions, open infrastructure, and community driven software were no longer niche topics. They were central to many discussions and presentations. The focus has visibly shifted away from convenience first and centralized platforms and toward systems that put control, transparency, and resilience back into the hands of users and communities. This shift was ily supported by established communities such as the FreeBSD project, which continues to demonstrate how long term, openly governed systems can serve as reliable foundations for sovereign infrastructure. At the same time, smaller but equally important projects showed how grassroots innovation drives real change. Talks like Hyacinthe’s FlipFlap presentation on the DN42 network highlighted decentralized and community operated networking in practice, while Emile’s talk on SmolBSD demonstrated how minimal, purpose built BSD systems can bring clarity, auditability, and long term maintainability back to operating system design. Projects such as BoxyBSD, crafted by gyptazy, showcased how lowering the barrier to learning BSD based systems empowers the next generation of open source contributors. By providing free invite codes during FOSDEM, BoxyBSD made hands on experimentation immediately accessible and reinforced the conference’s spirit of openness and community support.&lt;/p&gt;&lt;p&gt;FOSDEM 2026 made one thing unmistakably clear. Open source is no longer just about software freedom. It is increasingly about independence, sustainability, and Europe’s ability to shape its own digital future.&lt;/p&gt;&lt;head rend="h2"&gt;Arrival at FOSDEM 2026&lt;/head&gt;&lt;p&gt;Like every year, I decided to travel to FOSDEM by car. It’s actually the most relaxed way for me to get there as I can simply drive at any time in the morning, but it comes with one clear disadvantage: you have to arrive very early to secure a parking spot directly on campus. That means starting the journey long before the city fully wakes up. Overall, the travel time is more or less the same as taking the train, so that part doesn’t really matter. What does matter is the flexibility and being able to move around freely and head back home whenever I want. Since I usually only attend the first day of FOSDEM, that flexibility makes the early start worth it.&lt;/p&gt;&lt;p&gt;This year, the effort paid off once again. I ended up being the first car in line at the gate leading to the parking area. Better safe than sorry. Anyone who has attended FOSDEM knows that parking nearby is a small victory that can shape the rest of the day.&lt;/p&gt;&lt;p&gt;After parking, there was time to slow down a bit. Before the talks began, the campus gradually filled with familiar faces. FOSDEM has a unique rhythm in the early morning hours, when everything is still calm and conversations happen without rushing from room to room.&lt;/p&gt;&lt;p&gt;I met up with a few friends, and we took the opportunity to catch up and exchange a few thoughts before the day properly started. With coffees and croissants in hand, we waited for the opening talk. It was a simple moment, but one that perfectly captured the atmosphere of FOSDEM: a mix of anticipation, community, and shared curiosity about what the weekend would bring.&lt;/p&gt;&lt;head rend="h2"&gt;My Talk-Schedule at FOSDEM 2026&lt;/head&gt;&lt;p&gt;My personal schedule at FOSDEM followed a clear thread: understanding infrastructure from the lowest layers up to real-world, community operated systems. Rather than chasing trends, I focused on talks that explored control, reliability, and long term sustainability.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Rust-VMM&lt;/item&gt;&lt;item&gt;Garage S3 Best Practices&lt;/item&gt;&lt;item&gt;Mobility of Virtual Machines in Kubernetes Clusters&lt;/item&gt;&lt;item&gt;SmolBSD&lt;/item&gt;&lt;item&gt;FlipFlap Network in DN42&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The Rust-VMM talk set the tone by diving into modern virtualization foundations built with memory safety in mind. It highlighted how Rust enables a new generation of virtual machine monitors that reduce entire classes of bugs while still meeting strict performance requirements. For anyone working close to hardware or hypervisors, it was a i argument for rethinking traditional systems programming choices.&lt;/p&gt;&lt;p&gt;With Garage S3 Best Practices, the focus shifted from design to day-to-day operations. Object storage is often treated as a commodity, yet the talk made it clear how many subtle challenges exist around consistency, failure handling, and scaling. Real operational lessons and practical advice emphasized that running storage reliably is just as important as building it.&lt;/p&gt;&lt;p&gt;The talk on Mobility of Virtual Machines in Kubernetes Clusters explored the increasingly blurred line between classical virtualization and container orchestration. It showed how virtual machines can move and adapt within Kubernetes environments, combining the i isolation of VMs with the flexibility of cloud native tooling. This hybrid approach challenges the idea that platforms must choose one model exclusively.&lt;/p&gt;&lt;p&gt;SmolBSD brought a refreshing focus on minimalism. Instead of adding more layers, the project embraces small, understandable systems that are easier to audit and maintain over time. The talk reinforced the idea that simplicity is not a limitation but a strategic choice, especially for long lived infrastructure.&lt;/p&gt;&lt;p&gt;Finally, FlipFlap Network in DN42 connected many of the earlier themes through a community perspective. DN42 demonstrates how decentralized, self operated networking can work in practice. The talk showcased automation, experimentation, and cooperation in a real network built by its users, highlighting the educational and innovative power of grassroots infrastructure.&lt;/p&gt;&lt;p&gt;Together, these talks formed a coherent journey through modern open infrastructure: from safe low level building blocks to resilient storage, hybrid orchestration models, minimal operating systems, and community driven networks.&lt;/p&gt;&lt;head rend="h2"&gt;Best of...&lt;/head&gt;&lt;p&gt;One of my personal highlights of FOSDEM 2026 was a wonderfully simple yet brilliant idea by the Mozilla Foundation: giving away free cookies. It turned out to be more than just snacks. It was a fun little game, a great conversation starter, and the selection of cookies was genuinely excellent. You might have come for open source, but you probably left liking cookies even more than before.&lt;/p&gt;&lt;p&gt;Another standout moment was the talk The Challenges of FLOSS Office Suites by Michael Meeks, where he dove into the technical details behind Collabora Online. It was an absolute pleasure to listen to. What made the talk special was not only the depth of technical insight, but also the way it was presented. Complex topics were explained clearly, with context and humor, making it accessible without oversimplifying.&lt;/p&gt;&lt;p&gt;I was genuinely amazed by how the challenges of building and maintaining a full-featured, open source office suite were laid out so honestly. The talk went far beyond architecture diagrams and performance considerations and gave real insight into the long-term effort required to keep such critical software alive and competitive.&lt;/p&gt;&lt;p&gt;Beyond the talks, I also took the opportunity to have some great conversations at the booths. I chatted with fixoulab at the Proxmox booth and with the XCP-ng team at Vates, where I got an early look at the newly released Orchestra features. It was especially interesting since I had not yet found the time to dive into them in detail.&lt;/p&gt;&lt;p&gt;On a more personal note, I was truly grateful to meet many of my friends from different countries again. Being able to jump into great talks together, exchange impressions on the spot, and continue discussions afterwards is something that makes FOSDEM special in a way no recording or live stream ever could.&lt;/p&gt;&lt;head rend="h2"&gt;What the heck is going on at the FOSDEM 2026?&lt;/head&gt;&lt;p&gt;FOSDEM has always been crowded. Anyone who has attended more than once knows the familiar experience of packed hallways, full lecture rooms, and sprinting between buildings in the hope of catching the last five minutes of a talk. As the biggest open source conference in the world, this has long been part of its identity. But in 2026, it felt like something had shifted.&lt;/p&gt;&lt;p&gt;There is no doubt that the growing interest in free and open source software is a good thing. More people take open source seriously, more organizations depend on it, and more contributors want to get involved. That energy was clearly visible everywhere. At the same time, it felt like FOSDEM was reaching — or perhaps exceeding — its natural limits. Rooms filled faster than ever, informal discussions became harder to have, and the sheer density of people sometimes worked against the very openness the conference is known for.&lt;/p&gt;&lt;p&gt;A major driver behind this growth is the current political and economic climate. Topics like digital sovereignty, technological independence, and reducing reliance on a small number of dominant market players were more present than ever. This was not subtle. It was visible across the schedule, in hallway conversations, and especially during the Friday pre-conferences, where these themes were actively pursued and debated.&lt;/p&gt;&lt;p&gt;On one hand, this focus is both necessary and overdue. Open source has always been political in the sense that it is about control, transparency, and autonomy, even when it pretended not to be. Seeing these discussions move to the center stage at FOSDEM is encouraging. It shows that the community understands the stakes and is willing to engage with the broader implications of the technology it builds.&lt;/p&gt;&lt;p&gt;On the other hand, the intensity of this shift raises uncomfortable questions. When everything becomes urgent and strategic, the space for experimentation, learning, and smaller niche projects risks being squeezed out. Not every open source project exists to solve geopolitical problems, and not every contributor arrives with a policy agenda. FOSDEM has always thrived on its diversity of motivations, and maintaining that balance will be increasingly challenging.&lt;/p&gt;&lt;p&gt;FOSDEM 2026 felt like a conference at a crossroads. Its success is undeniable, but so are the growing pains that come with it. The challenge for the coming years will be finding ways to scale without losing what made the event special in the first place: accessibility, spontaneity, and the feeling that there is room for everyone and not just for the loudest or most timely topics. And while I already criticized this last year, this becomes even more important this year.&lt;/p&gt;&lt;p&gt;The conversations happening now are important, and it is good that they are happening at FOSDEM. But if the conference is to remain sustainable, both logistically and culturally, it will need to evolve just as thoughtfully as the open source ecosystem it represents.&lt;/p&gt;&lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;&lt;p&gt;It is genuinely great to see that FOSDEM remains free and open to everyone, even as the topics it covers become more complex and more relevant. The growing focus on moving away from big tech and reclaiming ownership of our data shows that the community is paying attention to what truly matters. These discussions are necessary, and it is encouraging to see them reflected so clearly in the talks and hallway conversations.&lt;/p&gt;&lt;p&gt;The quality of the talks was high, and the people were, as always, amazing. FOSDEM continues to be a place where curiosity, expertise, and openness meet. At the same time, the question of scale can no longer be ignored. Camping in front of a single room just to make sure you can attend a talk is not a sustainable solution. In many cases, it may even discourage the people who are genuinely interested but cannot afford to wait for hours or navigate overcrowded spaces.&lt;/p&gt;&lt;p&gt;For exactly this reason, I seriously considered staying home this year and watching the talks via live streams or recordings. From a purely technical perspective, that would have worked just fine. The content would still be there, accessible and well produced.&lt;/p&gt;&lt;p&gt;But in the end, FOSDEM is not just about talks. It is about meeting people, reconnecting with friends, and having spontaneous conversations that no video stream can fully replace. Seeing all of you again, sharing thoughts over coffee, and exchanging ideas in person ultimately mattered more than comfort or convenience.&lt;/p&gt;&lt;p&gt;FOSDEM 2026 once again proved why this conference is special. The challenge now is to ensure that it can continue to grow without losing the openness and accessibility that define it. That balance will shape what FOSDEM becomes in the years to come.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gyptazy.com/blog/fosdem-2026-opensource-conference-brussels/"/><published>2026-02-01T10:30:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46845244</id><title>Amiga Unix (Amix)</title><updated>2026-02-01T19:42:26.027535+00:00</updated><content>&lt;doc fingerprint="b7a29c0e5d29359e"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to the Amiga Unix wiki!&lt;/p&gt;
    &lt;p&gt;Amiga UNIX (also known as “Amix”) was Commodore's port of AT&amp;amp;T System V Release 4 Unix to the Amiga in 1990. Like many early Unix variants, Amiga Unix never became wildly popular, but it is an interesting sidestep in the history of the Amiga.&lt;/p&gt;
    &lt;p&gt;The two “official” machines that could run Amix were the Amiga 2500UX and 3000UX models, however it can run on any Amiga that meets its hardware requirements. The awesome Amiga emulator WinUAE has been able to run it since 2013 (version 2.7.0 onwards).&lt;/p&gt;
    &lt;p&gt;This site is dedicated on preserving Amix's history and sharing information and instructions on what Amix is, how to install it (either on real hardware or in emulation) and what can you do with it. Mainly, it tries to cater to people who wish to run AMIX for whatever reason on their hardware. By documenting experiences with it, it is hoped that subsequent SVR4 junkies will find the way more smooth than it might have been without any guidance at all. For even a relatively experienced modern Unix or GNU/Linux administrator, System V UNIX is sufficiently different to present difficulty in installation and administration. Not so much in moving around between directories, and using common utilities that persist to this day - although many of those are hoary and somewhat forgetful in their retirement - but of doing more in depth tasks and understanding the differences.&lt;/p&gt;
    &lt;head rend="h1"&gt;Table of contents&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; Amiga 3000UX Advert (thanks to 1000bit.it!) &lt;/p&gt;
    &lt;head rend="h3"&gt;History and Documentation&lt;/head&gt;
    &lt;head rend="h3"&gt;Hardware&lt;/head&gt;
    &lt;head rend="h3"&gt;Tutorials &amp;amp; How-to's&lt;/head&gt;
    &lt;head rend="h3"&gt;Software&lt;/head&gt;
    &lt;head rend="h3"&gt;General topics&lt;/head&gt;
    &lt;head rend="h3"&gt;Links&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;STREAM: Commodore Had UNIX On Amiga?! by NCommander on YouTube (1.5.2022)&lt;/item&gt;
      &lt;item&gt;"Abysmal AMIX Performance" and other blog posts about Amix and its disk performance in bdwheele's blog&lt;/item&gt;
      &lt;item&gt;Article about Amix in “Virtually Fun” blog: AMIX (2013).&lt;/item&gt;
      &lt;item&gt;WinUAE and the beginnings of Amix support in EAB forums&lt;/item&gt;
      &lt;item&gt;Great Amix site with re-done manuals and other useful files (in german): http://www.amigaunix.de&lt;/item&gt;
      &lt;item&gt;Telefisk.org gopher site mirror to grab files: http://gopher.muffinlabs.com/telefisk.org/amiga/Amix/&lt;/item&gt;
      &lt;item&gt;Not that active anymore, but still a good reading: Amiga UNIX Discussions in Amiga.org&lt;/item&gt;
      &lt;item&gt;Old, and riddled with Spam, but there once was a thriving Newsgroup dedicated to Amiga Unix. Seeing the Internet “never forgets”, you can find the old discussions at Google Groups: https://groups.google.com/forum/#!forum/comp.unix.amiga&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Where to get help with Amix?&lt;/p&gt;
    &lt;p&gt;Currently the most active Amix-related forum seems to be support.Amix @ English Amiga Board. Ask there if you need any help!&lt;/p&gt;
    &lt;p&gt;Wanted: content for this site! Do you have a piece of third-party software for Amix? Copy of an exotic hardware driver? Would you like to help write guides? Have copy of files from litamiga.epfl.ch or amiga.physik.unizh.ch? Please let us know!&lt;/p&gt;
    &lt;p&gt;Why would you like to try Amix? The short answer is, you don't want it. Stay with me for just a minute here.&lt;/p&gt;
    &lt;p&gt;Amiga UNIX is really, really real UNIX. AT&amp;amp;T System V Release 4, ported by a team at Commodore that quit the day that 1.0 was released - at least, as legend has it. All the releases have their share of issues, and none of them have been updated in well over a decade. It runs on old hardware that was expensive when new and hard enough to find now that “RARE!” tags on eBay aren't hyperbole for once. You can cheat and get it installed without the tape drive, at least, but it's not for the faint of heart and has its own caveats such as a completely broken package system.&lt;/p&gt;
    &lt;p&gt;Think for a minute about your goals before embarking on this journey. If you're a die-hard Amiga user - and we all know they are still out there - interested in trying this UNIX thing, because you've heard about Mac OS X and that's based on UNIX, but before you swap out Kickstart for init you'd like to kick the tires on Amiga UNIX…&lt;/p&gt;
    &lt;p&gt;Don't.&lt;/p&gt;
    &lt;p&gt;Ten minutes with Amiga UNIX will have some Amiga users reaching for their 3.1 floppies. Actually, two minutes with the installer might have the same effect. That's just not fair to UNIX, whether it's GNU/Linux, Solaris, FreeBSD, or whatever. The environment has come a long way in the years and years since Amiga UNIX was shiny and new, rather than bitter and tarnished. Your average Ubuntu user will have set fire to their house trying to expunge any evidence of the unforgiving AMIX installation. Did I mention it hasn't been updated in a decade? Put your Amiga UNIX machine on the net with no firewall and you may see it rooted faster than a Win98SE box running IE5.&lt;/p&gt;
    &lt;p&gt;Finally, and this is a biggie, there is no future for AMIX. Its kernel, libc, and much of its software is closed source, so when Commodore folded its story was over. It was put to pasture even as free UNIX environments began to be truly plausible contenders for commercial UNIX, and largely forgotten.&lt;/p&gt;
    &lt;p&gt;So this is quite the happy page, isn't it? Why would you want to install AMIX?&lt;/p&gt;
    &lt;p&gt;There's really only one reason. Well, two, if you count being an idiot like us. That reason is simple curiosity. If you'd like to put yourself in the shoes of an aspiring systems programmer in the early nineties, go for it. If you find the legacy of the ever growing open source operating system revolution fascinating, AMIX could prove instructive. If you're trying to impress bearded UNIX gurus with arcane knowledge from the times when men were men, and 16MB was a ton of RAM, the dwindling lore of System V might be the key. These days, Amiga UNIX is all about history and learning.&lt;/p&gt;
    &lt;p&gt;So if you have the requisite hardware and you have girded yourself for what is in store for you should you venture into the m68k-cbm-sysv4 dungeon, you might enjoy this site and the dusty treasures it contains. But seriously, if you just want to try UNIX or GNU/Linux out, the most expedient way to do it is on x86 or just something better supported on Amiga hardware. There are lots of choices for you.&lt;/p&gt;
    &lt;p&gt;Good luck, and enjoy!&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to Folkert “Tahoe” de Gans for hosting the site!&lt;/p&gt;
    &lt;p&gt;Thanks to Ville Laustela for images &amp;amp; scans and updating the Wiki.&lt;/p&gt;
    &lt;p&gt;Thanks to Toni Wilen for maintaining the WinUAE emulator and making it possible to run Amix inside it.&lt;/p&gt;
    &lt;p&gt;Dedicated to Andrew “Failsure” Whitlock, the original creator and maintainer of this site. RIP, agw.&lt;/p&gt;
    &lt;p&gt; 13690 visitors since 1.8.2019 &lt;/p&gt;
    &lt;p&gt;To contact us, please use the contact form.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.amigaunix.com/doku.php/home"/><published>2026-02-01T10:57:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46846210</id><title>Show HN: Zuckerman – minimalist personal AI agent that self-edits its own code</title><updated>2026-02-01T19:42:25.620178+00:00</updated><content>&lt;doc fingerprint="3e53f448c8b69c0d"&gt;
  &lt;main&gt;
    &lt;p&gt;Ultra-minimal personal AI: starts small, self-modifies its code live, adapts by writing exactly the code &amp;amp; features you need (beyond just self-looping), improves instantly, and shares improvements with other agents.&lt;/p&gt;
    &lt;p&gt;The vision: Build a truly self-growing intelligence — one that can add tools, rewrite behavior, or extend its core logic by editing its own files — with almost no external code required. Agents propose and publish capabilities to a shared contribution site, letting others discover, adopt, and evolve them further. A collaborative, living ecosystem of personal AIs.&lt;/p&gt;
    &lt;p&gt;OpenClaw exploded in popularity (100k+ GitHub stars in weeks) because it delivers real agentic power: it acts across your apps, remembers context, and gets things done. But that power comes with trade-offs — massive codebase, complex setup, steep learning curve, ongoing security discussions (prompt injection, privileged access), and constant updates that can overwhelm regular users.&lt;/p&gt;
    &lt;p&gt;Zuckerman takes the opposite path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ultra-minimal start — only the essentials, zero bloat&lt;/item&gt;
      &lt;item&gt;Full self-edit power — the agent can modify its own configuration, tools, prompts, personalities, and even core logic in plain text files&lt;/item&gt;
      &lt;item&gt;Instant evolution — changes hot-reload immediately (no rebuilds, restarts, or dev friction)&lt;/item&gt;
      &lt;item&gt;Collaborative growth — agents share useful edits/discoveries so the whole network levels up together&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You get an approachable, customizable agent that literally grows by rewriting itself — powerful without the usual headaches.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimal by design — starts with core essentials only&lt;/item&gt;
      &lt;item&gt;Real-time self-improvement — agent edits its own files (config, tools, behavior, code) and reloads instantly&lt;/item&gt;
      &lt;item&gt;Full runtime modification — tweak anything while the agent runs&lt;/item&gt;
      &lt;item&gt;Hot-reload everywhere — no restarts needed&lt;/item&gt;
      &lt;item&gt;Feature versioning — track and manage versions of agent capabilities and improvements&lt;/item&gt;
      &lt;item&gt;Collaborative ecosystem — share and adopt improvements via a contribution website&lt;/item&gt;
      &lt;item&gt;Multi-channel ready — Discord, Slack, Telegram, WhatsApp, WebChat + more&lt;/item&gt;
      &lt;item&gt;Voice support — TTS/STT with multiple providers&lt;/item&gt;
      &lt;item&gt;Security foundations — auth, policy engine, sandboxing (Docker), secret management&lt;/item&gt;
      &lt;item&gt;Multiple agents — run several with unique personalities/tools&lt;/item&gt;
      &lt;item&gt;Dual interfaces — CLI (power users) + Electron app (visual)&lt;/item&gt;
      &lt;item&gt;Calendar &amp;amp; scheduling — built-in time management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything is plain-text configurable and instantly reloadable.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;World (&lt;/p&gt;&lt;code&gt;src/world/&lt;/code&gt;) — the lightweight OS layer&lt;lb/&gt;Communication (messengers, gateway), Execution (processes, security), Runtime (agent factory), Config loader, Voice, System utils&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Agents (&lt;/p&gt;&lt;code&gt;src/agents/&lt;/code&gt;) — self-contained agent definitions&lt;lb/&gt;Each folder = one agent (core modules, tools, sessions, personality)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Interfaces (&lt;/p&gt;&lt;code&gt;src/interfaces/&lt;/code&gt;) — how you talk to it&lt;lb/&gt;CLI + Electron/React app (chat, inspector, settings, onboarding)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone and install
git clone https://github.com/zuckermanai/zuckerman.git
cd zuckerman
pnpm install

# Launch the Electron app (recommended for beginners)
pnpm run dev&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/zuckermanai/zuckerman"/><published>2026-02-01T13:50:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46846252</id><title>Adventure Game Studio: OSS software for creating adventure games</title><updated>2026-02-01T19:42:24.783604+00:00</updated><content>&lt;doc fingerprint="9722355bdfdfe8e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Adventure Game Studio (AGS) is open-source software for creating graphical point-and-click adventure games. It is free, standalone, and requires no subscription.&lt;/p&gt;
    &lt;p&gt;The Windows-based IDE, streamlines game creation by integrating tools for importing graphics, writing scripts, and testing. Games created with AGS can be played on multiple platforms, including Linux, iOS, and Android.&lt;/p&gt;
    &lt;p&gt;Suitable for all skill levels, AGS features an active community for support and socialising.&lt;/p&gt;
    &lt;p&gt;Showcase your games by uploading them to this website.&lt;/p&gt;
    &lt;p&gt;Rot your brain by consuming AI slop and services in this classic arcade style game created for the MAGS January 2026 game jam in the AGS forums.Move […]&lt;/p&gt;
    &lt;p&gt;You awaken alone on a cold, rocky shore beneath a moonless sky, dragged from the sea through a sewer pipe with no memory of who you are, how you […]&lt;/p&gt;
    &lt;p&gt;Alan Goldberg is a frustrated slasher movie screenwriter in the Hollywood of 1985 who feels his career is at a standstill, but his luck is about to […]&lt;/p&gt;
    &lt;p&gt;Kathy Rain tells the compelling story of a strong-willed college girl with a knack for detective work (and a Harley) who returns to her hometown after[…]&lt;/p&gt;
    &lt;p&gt; In: Beginners' Technical Questions&lt;lb/&gt; By: Crimson Wizard (9 minutes ago) &lt;/p&gt;
    &lt;p&gt; In: The Rumpus Room&lt;lb/&gt; By: Danvzare (18 minutes ago) &lt;/p&gt;
    &lt;p&gt; In: Adventure Related Talk &amp;amp; Chat&lt;lb/&gt; By: Danvzare (29 minutes ago) &lt;/p&gt;
    &lt;p&gt; In: Beginners' Technical Questions&lt;lb/&gt; By: SilverSpook (1 hour ago) &lt;/p&gt;
    &lt;p&gt; In: AGS Games in Production&lt;lb/&gt; By: Rui "Giger Kitty" Pires (2 hours ago) &lt;/p&gt;
    &lt;p&gt; In: Competitions &amp;amp; Activities&lt;lb/&gt; By: Rui "Giger Kitty" Pires (2 hours ago) &lt;/p&gt;
    &lt;p&gt; In: Adventure Related Talk &amp;amp; Chat&lt;lb/&gt; By: Ponch (2 hours ago) &lt;/p&gt;
    &lt;p&gt; In: AGS Games in Production&lt;lb/&gt; By: poc301 (2 hours ago) &lt;/p&gt;
    &lt;p&gt;AGS has an active and friendly community, with many ways of keeping in touch and getting help with your project or games made with AGS.&lt;/p&gt;
    &lt;p&gt;These include our local forums, Facebook page, Discord server, in-person meet-ups, and many more.&lt;/p&gt;
    &lt;p&gt;The AGS community is run by a team of dedicated volunteers, who put their time and efforts into keeping it running as a welcoming, friendly and informative place to be. The AGS server and forums are paid for out of our own pockets, so in effect it costs us money to provide a free service to AGS users.&lt;/p&gt;
    &lt;p&gt;If you appreciate the work we do, and would like to give a little something back, please use the below link to donate via PayPal. Any profit made after covering server costs will be put back into hosting community events such as Mittens.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.adventuregamestudio.co.uk/"/><published>2026-02-01T13:56:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46846697</id><title>Show HN: Voiden – an offline, Git-native API tool built around Markdown</title><updated>2026-02-01T19:42:24.171384+00:00</updated><content>&lt;doc fingerprint="3d9e37183f23f0ba"&gt;
  &lt;main&gt;
    &lt;p&gt;Voiden is an offline-first API client for developers, Testers and Technical Writers who want their API work to feel like code—not a SaaS dashboard.&lt;/p&gt;
    &lt;p&gt;Voiden lets you build, test, and link API requests like reusable blocks, comment on JSON or XML, preview responses (even PDFs or videos), and manage environments, themes, and scripts. The best part is that Voiden enables all this without ever needing the cloud. Voiden is your API lab: fast, transparent, versionable, and unapologetically opinionated.&lt;/p&gt;
    &lt;p&gt;No accounts, no sync, no cloud required.&lt;/p&gt;
    &lt;p&gt;Version 1.1.0 is now available!&lt;/p&gt;
    &lt;p&gt;The website will automatically detect your operating system and highlight the correct installer for you. (Windows, macOS Intel/Apple Silicon, and Linux).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;🔧 Looking for the beta builds? Download beta versions&lt;/p&gt;
      &lt;g-emoji&gt;↗️&lt;/g-emoji&gt;
    &lt;/quote&gt;
    &lt;p&gt;Head over to the Issues tab and click "New issue". Use the Bug report template to give us everything we need to fix it.&lt;/p&gt;
    &lt;p&gt;We love hearing about new possibilities. Use the Feature request template to tell us what you have in mind.&lt;/p&gt;
    &lt;p&gt;Open a general issue or leave a note.&lt;/p&gt;
    &lt;p&gt;Thanks for sharing your thoughts with us 💜&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js v21.x&lt;/item&gt;
      &lt;item&gt;Yarn v4.3.1&lt;/item&gt;
      &lt;item&gt;Windows Only: Visual Studio Build Tools with: &lt;list rend="ul"&gt;&lt;item&gt;"Desktop development with C++" workload&lt;/item&gt;&lt;item&gt;MSVC (C++ compiler)&lt;/item&gt;&lt;item&gt;Windows SDK&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/VoidenHQ/voiden.git
cd voiden
yarn install 
yarn workspace @voiden/core-extensions build
cd apps/electron &amp;amp;&amp;amp; yarn start&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note :&lt;/p&gt;&lt;code&gt;yarn install&lt;/code&gt;may fail on Windows due to a non-PTY build issue. See the troubleshooting guide: Build Errors (Windows)&lt;/quote&gt;
    &lt;p&gt;See the Full Installation Guide for detailed setup including Windows requirements.&lt;/p&gt;
    &lt;p&gt;All documentation is in the docs/ folder:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Topic&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Getting Started&lt;/cell&gt;
        &lt;cell&gt;Installation and setup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Architecture&lt;/cell&gt;
        &lt;cell&gt;System design and structure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extensions&lt;/cell&gt;
        &lt;cell&gt;Build your own extensions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Themes&lt;/cell&gt;
        &lt;cell&gt;Create custom themes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Troubleshooting&lt;/cell&gt;
        &lt;cell&gt;Common issues and solutions&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;See the Documentation Index for the complete list.&lt;/p&gt;
    &lt;code&gt;voiden/
├── apps/
│   ├── electron/          # Electron main process
│   └── ui/                # React renderer
├── core-extensions/       # Built-in extensions
└── docs/                  # Documentation
&lt;/code&gt;
    &lt;p&gt;We welcome contributions! Please read:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Contributing Guide - How to contribute&lt;/item&gt;
      &lt;item&gt;Code of Conduct - Community guidelines&lt;/item&gt;
      &lt;item&gt;Security Policy - Reporting vulnerabilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/VoidenHQ/voiden"/><published>2026-02-01T15:09:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46847456</id><title>Real engineering failures instead of success stories</title><updated>2026-02-01T19:42:24.095770+00:00</updated><content/><link href="https://failhub.substack.com/p/failhub-issue-1"/><published>2026-02-01T16:52:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46847746</id><title>Aging muscle stem cells shift from rapid repair to long-term survival</title><updated>2026-02-01T19:42:23.865596+00:00</updated><content/><link href="https://phys.org/news/2026-01-sprint-marathon-aging-muscle-stem.html"/><published>2026-02-01T17:33:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46847780</id><title>Apple: Our philosophy is to provide software for our machines free (1976)</title><updated>2026-02-01T19:42:23.341007+00:00</updated><content>&lt;doc fingerprint="b45ed919010701fc"&gt;
  &lt;main&gt;
    &lt;row&gt;
      &lt;cell valign="TOP" width="210"&gt;
        &lt;p&gt; The Apple Computer. A truly complete microcomputer system on a single PC board. Based on the MOS Technology 6502 micro- processor, the Apple also has a built-in video terminal and sockets for 8K bytes of onboard RAM memory. With the addition of a keyboard and video monitor, you'll have an extremely powerful computer system that can be used for anything from developing programs to playing games or running BASIC.&lt;lb/&gt; Combining the computer, video terminal and dynamic memory on a single board has resulted in a large reduction in chip count, which means more reliability and lowered cost. Since the Apple comes fully assembled, tested &amp;amp; burned-in and has a complete power supply on-board, initial set-up is essentially "hassle-free" and you can be running within minutes. At $666.66 (including 4K bytes RAM!) it opens many new possibilities for users and systems manufacturers.&lt;/p&gt;
        &lt;p&gt;You Don't Need an Expensive Teletype.&lt;lb/&gt; Using the built-in video terminal and keyboard interface, you avoid all the expense, noise and mantenance associated with a teletype. And the Apple video terminal is six times faster than a teletype, which means more throughput and less waiting. The Apple connects directly to a video monitor (or home TV with an in- expensive RF modulator) and dis- plays 960 easy to read characters in 24 rows of 40 characters per line with automatic scrolling. The video display section contains its own 1K bytes of memory, so all the RAM memory is available for user programs. And the &lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell valign="TOP" width="210"&gt;
        &lt;p&gt;Keyboard Interface lets you use almost any ASCII-encoded keyboard.&lt;lb/&gt; The Apple Computer makes it possible for many people with limited budgets to step up to a video terminal as an I/O device for their computer.&lt;/p&gt;
        &lt;p&gt;No More Switches,&lt;lb/&gt; No MoreLights&lt;lb/&gt; Compared to switches and LED's, a video terminal can dis- play vast amounts of information simultaneously. The Apple video terminal can display the contents of 192 memory locations at once on the screen. And the fimrware in PROMS enables you to enter,display and debug programs (all in hex) from the keyboard, ren- dering a front panel unnecessary. The firmware also allows your programs to print characters on the display, and since you'll be looking at letters and numbers instead of just LED's, the door is open to all kinds of alphanumeric software (i.e., Games and BASIC).&lt;/p&gt;
        &lt;p&gt;8K Bytes RAM in 16 Chips!&lt;lb/&gt; The Apple Computer uses the new 16-pin 4K dynamic memory chips. They are faster and take 1/4 the space and power of even the low power 2102's (the memory chip that everyone else uses). That means 8K bytes in sixteen chips. It also means no more 28 amp power supplies. The system is fully expandable to 65K via an edge connector which carries both the address and data busses, power supplies and all timing signals. All dy- namic memory refreshing for both on and off-board memory is done automatically. Also, the Apple Computer can be upgraded to use the 16K chips when they become availa- &lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell valign="TOP" width="210"&gt;
        &lt;p&gt;ble. That's 32K bytes on-board RAM in 16 IC's --the equivalent of 256 2102's!&lt;/p&gt;
        &lt;p&gt;A little Cassette Board that Works!&lt;lb/&gt; Unlike many other cassette boards on the marketplace,ours works every timeIt plugs directly into the upright connector on the mainboard and stands only 2" tall.And since it is very fast (1500 bits per second), you can read or write 4 K bytes in about 20 seconds.All timing is done in software, witch results in crystal-controlled accuracy and uniformity from unit to unit.&lt;lb/&gt; unlike some other cassette interfaces witch requires an expensive tape recorder, the Apple Cassette Interface works reliably with almost any audio-grade cassette recorder.&lt;/p&gt;
        &lt;p&gt;Softwares&lt;lb/&gt; A tape of APPLE BASIC is inclued free with the Cassette Interface.Apple Basic features immediate error message and fast execution, and let's you program in a highter level language immediately and without added cost.Also avialable now are a dis-assembler and many games, with many software packages,(including a macro assembler) in the works.And since our philosophy is to provide software for our machines free or at minimal cost, you won't be continually paying for access to this growing software library.&lt;lb/&gt; The Apple Computer is in stock al almost all major computer stores.(if your local computer store doesn't carry our products, encourage them or write us direct).Dealer inquiries invited. &lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://apple1.chez.com/Apple1project/Gallery/Gallery.htm"/><published>2026-02-01T17:36:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46847899</id><title>NCR Tower 1632 – Computer Ads from the Past</title><updated>2026-02-01T19:42:23.252581+00:00</updated><content/><link href="https://computeradsfromthepast.substack.com/p/ncr-tower-1632"/><published>2026-02-01T17:50:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46848260</id><title>Clearspace (YC W23) Is Hiring an Applied Researcher (ML)</title><updated>2026-02-01T19:42:22.790242+00:00</updated><content>&lt;doc fingerprint="14939967bc01edd1"&gt;
  &lt;main&gt;
    &lt;p&gt;Eliminate compulsive phone usage&lt;/p&gt;
    &lt;p&gt;About Clearspace&lt;/p&gt;
    &lt;p&gt;Clearspace is building the intentionality layer of the internet. Our mission is to build technology as effective at protecting human attention as social media is at exploiting it (infinite scrolling, short-form feeds, manipulative notifications, etc). Our category defining mobile app has been featured on Huberman Lab, New York Times Wirecutter, NPR Marketplace, Forbes, TBPN.&lt;/p&gt;
    &lt;p&gt;People that want a better relationship with their devices have nowhere to turn except for willpower. We are building an agent that achieves this on all devices by processing and filtering network traffic based on natural language rules.&lt;/p&gt;
    &lt;p&gt;About The Role&lt;/p&gt;
    &lt;p&gt;We are looking for an ML-focused engineer that will be responsible for training and improving a model for classifying network traffic. You are great for this role if you are not only excited about the latest in AI and ML but are also a problem-solver in the data domain. You don’t just think about the model but “how can we get more data volume”; “how can we featurize the data intelligently”; “what are our data needs based on our task and desired model size”, and like building backwards from inference requirements.&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Nice to Have&lt;/p&gt;
    &lt;p&gt;At Clearspace we help people reduce compulsive phone usage.&lt;/p&gt;
    &lt;p&gt;We exist to protect people's attention from the exploits of modern technology platforms and make space for the things that matter to them most.&lt;/p&gt;
    &lt;p&gt;We believe the technology to protect someones attention should be just as sophisticated and effective as the tech that is exploiting it and are building a world-class engineering team to arm the world with a comprehensive attention protection stack.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/clearspace/jobs/GOWiDwp-research-engineer-at-clearspace"/><published>2026-02-01T18:41:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46848343</id><title>Light exposure and aspects of cognitive function in everyday life</title><updated>2026-02-01T19:42:21.585998+00:00</updated><content>&lt;doc fingerprint="7c9a9a212c318d1a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Light exposure can modulate cognitive function, yet its effects outside of controlled laboratory settings remain insufficiently explored. To examine the relationship between real-world light exposure and cognitive performance, we assessed personal light exposure and measured subjective sleepiness, vigilance, working memory, and visual search performance over 7 days of daily life, in a convenience sample of UK adults (n = 58) without significant circadian challenge (shiftwork or jet-lag). A subset of participants (n = 41) attended an in-lab session comprising a battery of pupillometric and psychophysical tests aimed to quantify melanopsin-driven visual responses. We find significant associations between recent light exposure and subjective sleepiness. Recent light exposure was also associated with reaction times for both psychomotor vigilance and working memory tasks. In addition, higher daytime light exposure and an exposure pattern with reduced fragmentation were linked to improved cognitive performance across visual search, psychomotor vigilance, and working memory tasks. Higher daytime light exposure and earlier estimated bedtimes were associated with stronger relationships between recent light exposure and subjective sleepiness. These results provide real world support for the notion that intra- and inter-individual differences in light exposure meaningfully influence aspects of cognition, with beneficial effects of short-term bright light and of habitual light exposure patterns characterized by brighter daytimes, earlier rest phase, and greater intra- and inter-daily stability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Similar content being viewed by others&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Light is a fundamental environmental cue that governs numerous biological processes in humans, including circadian rhythms, sleep, and cognition1,2,3. The discovery of intrinsically photosensitive retinal ganglion cells (ipRGCs) has significantly advanced our understanding of how light influences these processes beyond vision4,5,6. These ipRGCs play a central role in regulating the circadian system by modulating circulating melatonin, setting circadian phase, and alertness7,8,9. The relationship between light exposure and cognitive performance has garnered significant attention in recent years, with direct retinal pathways identified as key regulators of cognitive function of animal models4. Furthermore, high-resolution brain imaging of humans has revealed that light enhances cognitive functions, though its effects are complex, eliciting distinct responses in specific light-sensitive subnuclei of the hypothalamus and varying across different cognitive tasks10,11,12.&lt;/p&gt;
    &lt;p&gt;Laboratory studies have demonstrated that exposure to bright light during both day and night can acutely enhance cognitive performance13,14,15,16,17,18,19,20,21,22. On the other hand, nighttime light exposure disrupts sleep, leading to cognitive impairments the following day7,23,24,25. Adding further complexity, cognitive functions themselves follow circadian rhythms1,26,27. Additionally, individual differences contribute to variations in how light influences cognition28,29. Despite substantial findings from controlled laboratory studies, a critical gap persists in understanding how these effects translate to real-world environments, where light exposure is dynamic and intertwined with daily routines.&lt;/p&gt;
    &lt;p&gt;Bridging this gap is essential for comprehending light’s broader implications on daily cognitive performance. Real-world light exposure varies widely in its stability, intensity, timing, and spectral composition due to the interplay of natural and artificial light sources. In modern industrialized societies, where individuals spend most of their time indoors and are frequently exposed to artificial lighting and night-time light, understanding the cognitive effects of light exposure is crucial to enhance workplace efficiency, to improve health and safety, and to support better educational outcomes30,31,32,33,34,35,36. Furthermore, these investigations illuminate potential pathways for developing interventional therapies aimed at mitigating cognitive decline and dementia37. To address these gaps, this study investigates the impact of light exposure on cognitive performance in everyday life. We hypothesized that recent increases in light intensity would acutely enhance cognitive performance, that weekly light exposure patterns would influence general cognitive performance, and that individual photosensitivity of cognitive functions could be estimated through light exposure metrics or in-lab sensitivity assessments. Utilizing innovative tools – a wearable melanopic light monitor (Spectrawear38) and a mobile application (Brightertime39) – we collected continuous data on light exposure and cognitive performance over 7 days of everyday life. This approach enables an assessment of both the acute and cumulative effects of light exposure on cognition, and individual differences in photosensitivity. Our findings contribute to the growing body of evidence on light’s role in modulating cognition and emphasize the importance of personalized strategies for optimizing light exposure to enhance cognitive performance and overall well-being.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methods&lt;/head&gt;
    &lt;p&gt;The study was not preregistered. The full study protocol was shared in protocols.io (https://doi.org/10.17504/protocols.io.n92ldrjjxg5b/v1).&lt;/p&gt;
    &lt;head rend="h3"&gt;Participants&lt;/head&gt;
    &lt;p&gt;A total of 60 individuals participated in the study, conducted between July 2022 and August 2023 in Manchester, UK. To ensure the power of estimates, a minimum sample size of 50 participants was targeted40. Study weeks started on any day of the weekdays. Participants were eligible for inclusion if they were at least 18 years old, employed either full-time or part-time, had no history of intercontinental travel in the preceding two weeks, and had not been diagnosed with a sleep disorder. The study aimed to capture a real-world population; therefore, no specific health-related exclusion criteria were applied. While this approach increased ecological validity, it may have introduced limitations in detecting certain associations. Two participants were excluded from the final analyses due to study design noncompliance and insufficient data provision. The final sample consisted of 29 males and 29 females. Sex and gender were determined based on information provided by the participants. No race or ethnicity information was collected in this study. Age distribution was as follows: 14 participants were &amp;lt;25 years old, 23 were between 25 and 30, 13 were between 30 and 35, and the remaining participants were over 35 years old. The majority (n = 55) held a higher education degree, and 32 were employed full-time. Four participants occasionally engaged in shift work, with eight days reported as shift work during the study; however, none involved night shifts that significantly alter night sleep patterns. The dataset comprised 65% workdays. Regarding health characteristics, 46 participants were non-smokers, two were colorblind, three had an ADHD diagnosis, 11 reported anxiety or depression, and 3 reported poor overall health. Sleep quality, assessed via the Pittsburgh Sleep Quality Index (PSQI), indicated generally good sleep health, with a mean score of 6.4 (SD = 1.9). Chronotype was assessed using the Munich Chronotype Questionnaire (MCTQ), with a mean midsleep time on free days (MSFsc) of 5:04 AM (SD = 1.4, range: 2:15 AM–7:51 AM), ensuring a diverse representation of sleep timing preferences.&lt;/p&gt;
    &lt;head rend="h3"&gt;Study design and light monitoring&lt;/head&gt;
    &lt;p&gt;We employed a real-world protocol for monitoring personal light exposure41,42. This approach allowed us to assess natural variations in light exposure and its effects on cognition outside of controlled laboratory settings. Participants were asked to remain in the study for one week, though they had the flexibility to withdraw at any time or to extend their participation. Data collection involved two key technologies: 1) a wearable melanopic equivalent daylight exposure monitor for personal light measurements (Spectrawear38) and 2) a mobile application for cognitive task performance and questionnaire responses (Brightertime39). During the initial registration session, participants were provided with the light monitor, guided through account setup for the Brightertime app, and instructed on the proper usage of both technologies.&lt;/p&gt;
    &lt;p&gt;Spectrawear is a multichannel light sensor designed to measure melanopic equivalent daylight exposure. Above 1 lx, the device demonstrated an average mean absolute log deviation of &amp;lt; 0.05 log units, with a minimum between-device correlation of 0.99. Its typical performance estimates α-opic equivalent daylight illuminances (EDI) within ±15% across a wide dynamic range (1 to 100,000 lx). In addition, the angular response to incident light was with a half-width at half maximum of 51°. The device was worn on the non-dominant wrist and set to record light exposure at 30 s intervals. Participants were instructed to wear the device throughout the day and remove it just before bedtime, placing it in a consistent location within the same room (preferably near eye level). Participants were advised to charge it daily, preferably overnight. Participants did not have direct access to their recorded data, nor were they able to modify device settings. Additionally, they were asked to avoid wearing clothing that could obstruct the light sensor.&lt;/p&gt;
    &lt;p&gt;Throughout the study, participants used the Brightertime app to complete the following tasks: 1) baseline surveys administered during the registration meeting; 2) daily sleep and work diary completed each morning; 3) subjective sleepiness reports submitted at their discretion throughout the day, with a recommended schedule of morning, midday, and evening entries; 4) cognitive assessments, including the Psychomotor Vigilance Task, N-Back, and Visual Search Task, performed each time a subjective sleepiness report was submitted. Participants were encouraged to provide multiple recordings at various times of the day rather than limiting themselves to three fixed entries. To maintain ecological validity, participants were explicitly instructed to follow their normal routines and not alter their sleep or lighting behaviors during the study. At the conclusion of the study week, participants were offered an optional in-lab assessment of light sensitivity43. Those who accepted (n = 41) underwent a session evaluating melanopic brightness preference, subjective brightness, and pupillary light reflex responses.&lt;/p&gt;
    &lt;head rend="h3"&gt;Surveys and measures&lt;/head&gt;
    &lt;p&gt;The surveys were administered at specified times through the Brightertime app39. The baseline questionnaire included a study-specific sociodemographic and health survey, along with standardized sleep and chronotype assessments such as the Pittsburgh Sleep Quality Index (PSQI)44 and the Munich Chronotype Questionnaire (MCTQ)45. The sociodemographic and health survey collected data on age, sex, employment and shiftwork status, subjective health rating, prior diagnoses of sleep, eye, mental, or neurological disorders, as well as daily caffeine, alcohol, and smoking habits. Chronotype was quantified using the MCTQ-derived midsleep time on free days, adjusted for sleep debt on workdays (MSFsc). Sleep quality was assessed using the PSQI, with higher scores (out of 21) indicating greater sleep disturbances. The morning diary recorded whether it was a workday or a free day, bedtime, and wake time. Participants also reported their total sleep duration (in hours) and sleep latency (time taken to fall asleep in minutes). Repeated assessments of subjective sleepiness were conducted using the 10-item version of the Karolinska Sleepiness Scale (KSS)46, where a score of 10 indicated extreme sleepiness. Before each repeated assessment, if napping occurred, the time of the last awakening was recorded.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cognitive tasks&lt;/head&gt;
    &lt;p&gt;The cognitive tasks used in this study were previously detailed in the Brightertime development article39; however, the number of trials was further optimized to reduce task duration and trial numbers to a minimum, while preserving the mean and variance of the results. Participants used their own smartphones and were eligible if they had a functional, non-cracked touchscreen. Upon first logging into the app, participants were required to complete a short practice session for each cognitive task to minimize learning effects. At the end of the practice, they could either repeat the session or proceed with the study. Each cognitive session comprised three tasks: the Psychomotor Vigilance Task (PVT), N-Back (n = 3; NB3), and Visual Search Task (VS). To mitigate order effects, the app randomized the task presentation sequence in each session. Response accuracy (correct hits, correct rejections, misses, and false alarms) and reaction times were recorded, and results were synchronized to a study-specific server upon session completion via an internet connection.&lt;/p&gt;
    &lt;p&gt;The Psychomotor Vigilance Task assessed sustained attention by requiring participants to monitor a central target and tap the screen as quickly as possible upon the appearance of a stimulus (a cartoon zombie). The task consisted of 28 stimulus presentations with inter-stimulus intervals randomly set between 2 and 10 s and a stimulus time-out of 1 s. The working memory task evaluated short-term memory, where participants monitored a sequence of nine letters (A, B, D, E, K, M, R, S, T) and responded when the presented letter matched the one from three trials prior. The task included 22 letter presentations with six targets and a stimulus time-out of 2 s. The visual search task measured visual cognitive performance, requiring participants to search for a monkey (with a “T”-shaped nose) among human distractors (with an “L”-shaped nose) displayed in one of four orientations. Difficulty was varied by randomly presenting 12, 24, or 36 figures. The task comprised 42 stimulus presentations with a random present/absent ratio (minimum 0.3, maximum 0.8, median 0.5), a 2 s inter-stimulus interval, and a stimulus time-out of 10 s.&lt;/p&gt;
    &lt;p&gt;Incomplete data due to app crashes (the app then restarts) were excluded, along with reaction times below 100 ms to filter out invalid responses. From the raw data, key performance metrics were computed for each task. For Psychomotor Vigilance Task, these included false negative rate (FNR, %), false discovery rate (FDR, %), accuracy (ACC, % of hits and correct rejections among trials), median reaction time of hits (median RT, ms), inverse efficiency score (IES; mean reaction time/ACC), 90th percentile of reaction times (slow10, ms), 10th percentile of reaction times (fast10, ms), and lapses (number of responses above 500 ms). For working memory, the computed variables were FNR, false positive rate (FPR, %), FDR, false omission rate (FOR, %), ACC, discrimination index (d-prime), median RT, IES, slow10, and fast10. For visual search, the calculated metrics included FNR, FPR, FDR, FOR, ACC, d-prime, median RT, IES, slow10, fast10, and search efficiency slopes (linear regression slope of reaction time across search density). Participants used their own devices, which may have introduced differences in touchscreen sensitivity. However, this issue was addressed and found to have no effect on cognitive task scores39. Additionally, data cleaning procedures excluded sessions with low accuracy (&amp;lt; 30%), specifically five sessions for working memory, three for Psychomotor Vigilance Task, and two for visual search. Furthermore, two participants were removed from the analysis for having fewer than seven valid task entries per game over the study week.&lt;/p&gt;
    &lt;p&gt;The learning effects of cognitive outcomes were investigated (see Fig. S1 for details). Overall, subjective sleepiness and Psychomotor Vigilance Task were independent of the number of tasks completed per day and showed no learning or motivation effect. In contrast, working memory and visual search, which were slightly more complex tasks, exhibited a learning effect, with participants becoming more accurate and faster the longer they remained in the study. Increased task repetition led to greater practice effects, though this was only observed in visual search IES. Any potential confounding effects of learning were controlled for, as described in the statistical analysis.&lt;/p&gt;
    &lt;head rend="h3"&gt;In-lab light sensitivity assessment&lt;/head&gt;
    &lt;p&gt;The in-lab light sensitivity measurement equipment and protocol were previously published43. A multiprimary projector system was used to conduct silent substitution experiments or deliver single-color light stimuli. Participants were dark-adapted for five minutes before the procedure. Following a calibration step involving a flicker photometry task (7.5 Hz) to match the luminance of two isoluminant spectra differing only in melanopic irradiance. Participants then completed three assessments: melanopic brightness discrimination, subjective brightness evaluation, and post-illumination pupil response (PIPR) measurement. In the melanopic brightness preference task, participants were shown 80 trials of spectral pairs, one without melanopic contrast and the other with varying contrasts (0–53% Michelson contrast, 20 steps, four repeats per step). They were asked to select the spectrum that appeared brighter. Two parameters were derived: the proportion of trials where the spectrum with higher melanopic irradiance was perceived as brighter at 53% contrast and the contrast level at which it was selected as brighter in 75% of trials. For subjective brightness assessment, participants viewed a high-melanopic-irradiance spectrum and rated its brightness on a 1–100 scale, with reference points at 0 (completely dark), 25 (dim), 50 (just bright and comfortable), 75 (bright), and 100 (too bright and uncomfortable). The PIPR assessment involved an initial 10 min dark adaptation, followed by 1 s flashes of red or blue light in random order. Each flash was preceded by 10 s and followed by 30 s of dark adaptation. The initial pupil constriction was quantified as the maximal initial pupil reflex to the blue stimulus, expressed as a percentage of the baseline diameter (mean over 10 s before the flash). The PIPR response was calculated as the area under the curve (AUC) for baseline-subtracted pupil diameter over the 0–6 s interval after light offset. Using the red stimulus as a reference, the difference between blue and red AUC values represented the PIPR for each participant. Additionally, a normalized PIPR amplitude was calculated as a percentage of the initial constriction amplitude.&lt;/p&gt;
    &lt;head rend="h3"&gt;Statistical analysis&lt;/head&gt;
    &lt;p&gt;All data processing and analyses were conducted using R version 4.4.1 (2024) and MATLAB version R2023b, with data visualization performed in GraphPad Prism version 10.3.0. Melanopic lux values were log10-normalized for all calculations and summaries. In each section, we applied the Bonferroni correction to adjust for multiple comparisons by setting the significance threshold as α divided by the number of predictors. Data distribution was assumed to be normal, but this was not formally tested. Two-tailed hypothesis tests were used. Descriptive variables were reported as means and standard deviations (SD). Bivariate comparisons were conducted using Pearson correlation coefficients and independent-samples t-tests.&lt;/p&gt;
    &lt;p&gt;To determine the primary cognitive outcome variables, we performed factor analysis. Principal Component Analysis (PCA) was used to extract eigenvalues, and factors were selected based on eigenvalues greater than 1. Factor analysis was then conducted using the selected number of factors with varimax rotation, and variables were assigned to factors based on their highest loadings. This analysis identified key cognitive measures: subjective sleepiness, two factors for Psychomotor Vigilance Task (median RT and accuracy), three factors for working memory (median RT, FPR, and FNR), and three factors for visual search (IES, FPR, and FNR).&lt;/p&gt;
    &lt;p&gt;Cognitive performance outcomes were described using distributions across different times of the day (clock time) and time awake (duration between the last wake time and the time of cognitive tasks). For all cognitive measures, linear mixed models (LMM) with random slopes and intercepts for subjects were used to examine associations with time of day, time awake, and previous night’s sleep duration and sleep midpoint time. Time awake was modelled as a second-degree polynomial function, while time of day was represented using a harmonic fit with sine and cosine terms [sin(2π × time of day/24) + cos(2π × time of day/24)]. First-degree functions were used to model the duration of the previous night’s sleep (hours) and the sleep midpoint. The standardized coefficients were visualized using a heatmap for comparison. We standardized all fixed-effect coefficients by rescaling each predictor by its sample SD and the outcome by its SD, yielding interpretable standardized coefficients with 95% confidence intervals. We then conducted an ANOVA on the fitted linear mixed model to test each fixed effect’s significance and converted each F-statistic (with its degrees of freedom) into a partial η² effect size. In addition, to investigate the learning effect and the influence of the number of tasks completed per day, we fit a separate linear mixed-effects model for each cognitive outcome, using experiment day and total tasks per day as predictors.&lt;/p&gt;
    &lt;p&gt;To examine whether cognitive task performance was correlated with recent light exposure, we calculated the mean light exposure over the prior 30, 60, 90, and 120 min. We then compared these mean values of light exposure with cognitive outcomes, adjusting for time of day and time awake, and the previous night’s sleep duration and midpoint. Cognitive outcomes were modelled using LMM, with random slopes and intercepts for light exposure specific to each individual. The standardized coefficients were visualized using a heatmap for comparison. For working memory, the model further adjusted for task day to account for the learning effect. We retained the individual linear regression slopes from the models of subjective sleepiness, Psychomotor Vigilance Task, and working memory vs. 30-minute light history as a measure of real-world light sensitivity in cognition.&lt;/p&gt;
    &lt;p&gt;Pearson correlations were used to compare the relationships between real-world light sensitivity of cognition, in-lab photosensitivity variables, weekly light exposure variables, and weekly cognitive output means. Weekly cognitive output averages were calculated as the means across the week. Weekly light exposure predictors include intensity variables such as M10 (the mean melanopic EDI of the brightest 10-hour period) and L5 (the mean melanopic EDI of the dimmest 5 h period); duration variables such as the time spent above 250 Melanopic EDI lux (in minutes) and the time spent above 10 Melanopic EDI lux after sunset (in minutes); timing variables including the midpoint time of the M10 period, the midpoint time of the L5 period, and the last time above 1 Melanopic EDI lux; and stability variables such as IS (interdaily stability of light exposure) and IV (intradaily variability of light exposure). The IS of melanopic EDI was calculated as a metric of the similarity of daily light exposure patterns during the study period by comparing the variance of average hourly means to the variance of all hourly means. The IV of melanopic EDI was calculated to assess the fragmentation of light exposure patterns during the study period by comparing the mean square deviation of hourly means from the previous hour to the variance of all hourly means. If a correlation remained significant after correction for multiple testing, the predictor and outcome were further compared using linear regression, adjusted for age, sex, daylength, caffeine consumption, alcohol consumption, smoking, chronotype (MSFsc), and sleep problems (PSQI).&lt;/p&gt;
    &lt;head rend="h3"&gt;Ethics&lt;/head&gt;
    &lt;p&gt;This project was approved by the University of Manchester Research Ethics Committee (Ref: 2021-12948-20856 and Ref: 2023-16080-26819). All participants provided informed consent before commencing the study.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reporting summary&lt;/head&gt;
    &lt;p&gt;Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Measuring cognitive performance in real world settings&lt;/head&gt;
    &lt;p&gt;We aimed to collect cognitive function data in everyday life using an accessible smartphone app, Brightertime39. This app incorporates a Psychomotor Vigilance Task (PVT) to measure sustained attention, an N-back task (NB3; 3-back) to assess working memory, a T vs. L visual search task (VS) to evaluate search accuracy and efficiency, and subjective sleepiness report (KSS) (Fig. 1A). Participants (n = 58) used the app multiple times throughout the day at their discretion (Fig. 1B). (Fig. S2: min = 13, max = 52, median = 23 entries per participant over an 8-day study period; Fig. S3: a representative study design for one participant). Across the 1428 subjective sleepiness reports, we captured the full range of subjective sleepiness, indicating that our methodology appropriately captured real-world variations in ‘tiredness’ throughout the day. The data exhibited minor positive skewness (0.4), with a minimum value of 1, a maximum value of 10, a mean of 4.5, and a standard deviation of 1.9.&lt;/p&gt;
    &lt;p&gt;Objective tasks of vigilance (n = 1334), working memory (n = 1341), and visual search (n = 1340) recorded correct hits, misses, false alarms, and reaction times for each attempt (ms). Using these records, a set of cognitive variables was extracted, including median reaction times of hits, accuracy, false negative/positive rates, difficulty slope of visual search, number of lapses in vigilance task (&amp;gt;500 ms), discriminability score (d’) of working memory and visual search, and inverse efficiency slopes (IES; mean reaction times per ratio of correct answers). The summary statistics of the cognitive measures were comparable to previous real-world cognitive task findings39 (Table S1). The average of the median reaction times for the vigilance task was 421.5 ms (min = 263.0 ms, max = 800 ms, SD = 79.3), with an average accuracy of 93.8% (SD = 9.4) and an average number of lapses of 6.1 (SD = 5.8). For the working memory task, the average of the median reaction times to correctly remember a letter within a 2 s window was 750.5 ms (min = 285.5 ms, max = 1755.0 ms, SD = 234.3 ms), with an average accuracy of 84.8% (SD = 12.6) and an average d’ of 2.0 (SD = 0.9). In the visual search task, the average of the median reaction times to correctly answer within a 10-s window was 2637.4 ms (min = 295.0 ms, max = 7523.5 ms, SD = 821.9 ms), with an average accuracy of 87.9% (SD = 9.6) and an average d’ of 2.7 (SD = 0.7).&lt;/p&gt;
    &lt;p&gt;Since each objective task had multiple outcome variables that were commonly correlated with each other (Pearson correlation up to 0.96), we performed dimension reduction using factor analysis (Table S2). This analysis resulted in two factors for vigilance (median reaction time and accuracy), three factors for working memory (median reaction time, false positive rate, and false negative rate), and three factors for visual search (inverse efficiency score, false positive rate, and false negative rate) (Fig. S4). The remaining analysis will therefore assess these variables.&lt;/p&gt;
    &lt;head rend="h3"&gt;Rhythm of cognitive performance in everyday life&lt;/head&gt;
    &lt;p&gt;As expected, light exposure assessed with the wearable melanopic light logger (Spectrawear38) (Fig. 1A) exhibited time of day dependence. Across all 497 days of recording, global mean irradiance was 0.9 log lux melanopic equivalent daylight illumination (EDI) (SD = 1.5) (Fig. 2A). The maximum melanopic EDI per subject ranged from 4.4 to 5.2 log lux.&lt;/p&gt;
    &lt;p&gt;It is plausible that the cognitive variables we measured exhibit a diurnal rhythm, influenced by the natural circadian cycle. Furthermore, the timing and duration of recent sleep episodes, as well as the interval since the last waking, may significantly influence these cognitive outcomes. Therefore, we continued our analysis by investigating the rhythmic properties of cognitive function. The median time of day when cognitive tasks were performed was 16:06, with all clock times except 4–6 AM represented in our dataset. There was no statistically significant association between the median task completion time for each subject and the number of tasks completed during the week (r(56) = 0.08, 95% CI [–0.17, 0.33], p = 0.57), indicating that task timing preferences were not biased by the number of tasks completed. The mean time awake prior to task performance was 7.6 h (SD = 5.3). Additionally, the mean duration of the last sleep episode before each task was 7.1 h (SD = 1.2). The midpoint of the last sleep period occurred at an average time of 4:29 AM (SD = 1.4 h).&lt;/p&gt;
    &lt;p&gt;Subjective sleepiness scores showed strong associations with time of day, time awake, and the duration of the previous sleep episode (Fig. 2B). The subjective sleepiness model revealed that subjective sleepiness was typically higher within the first hour after wakening (mean=4.9) than later in the wake episode (mean daytime minimum = 3.7), indicative of sleep inertia, and then increased at approximately 0.4 subjective sleepiness points per hour later in the day (Time awake Std. Coef. = –1.04, 95% CI [–1.33, –0.75], p &amp;lt; 0.001, partial η² = 0.52; Time awake2 Std. Coef. = 1.36, 95% CI [1.07, 1.65], p &amp;lt; 0.001, partial η² = 0.64) (Fig. 2C). Given the strong relationship between sleep and clock time, it was not surprising to see a similar pattern for subjective sleepiness scores when plotted as a function of time of day, with a mean time of nadir in sleepiness at 14:21 (Sine (time of day) Std. Coef. = 0.24, 95% CI [0.17, 0.30], p &amp;lt; 0.001, partial η² = 0.48; Cosine (time of day) Std. Coef. = 0.44, 95% CI [0.36, 0.52], p &amp;lt; 0.001, partial η² = 0.67) (Fig. 2D). In comparison, the duration of the previous sleep episode had a relatively smaller effect on subjective sleepiness, with each additional hour of sleep reducing the daily mean subjective sleepiness score by 0.15 points (Std. Coef. = –0.10, 95% CI [–0.15, –0.04], p = 0.003, partial η² = 0.31).&lt;/p&gt;
    &lt;p&gt;Of the cognitive task parameters, median reaction time in the vigilance task showed statistically significant associations with the time of day and sleep duration, and the false negative ratio in the visual search task showed statistically significant associations with the sleep midpoint (Fig. 2B, Table S3). Thus, the amplitude of time-of-day variation was just 8 ms (Cosine (time of day) Std. Coef. = 0.07, 95% CI [0.02, 0.12], p = 0.005, partial η² = 0.14) (Fig. 2E). Each additional hour of sleep reduced the reaction time by 5 ms (Std. Coef. = –0.09, 95% CI [–0.13, –0.04], p &amp;lt; 0.001, partial η² = 0.08) (Fig. 2F). Visual search false negative rate decreased by 1% for each additional hour of later sleep midpoint time (Std. Coef. = –0.10, 95% CI [–0.17, –0.03], p = 0.006, partial η² = 0.007). Given that vigilance task reaction times are measured on a 100–1000 millisecond scale and visual search accuracy is assessed as a percentage, we acknowledge that, while statistically significant, these changes are relatively subtle in the context of within- and between-participant variability in these parameters.&lt;/p&gt;
    &lt;p&gt;Subjective sleepiness and cognitive performance were compared between weekdays and weekends. No statistically significant differences were observed for any variable except the visual search IES, which was lower on weekends by 150 ms (95% CI [46.1, 276.7], t(683.6) = 2.7, p = 0.006, Cohen’s d = 0.16). Daylength during the study ranged from 12.9 to 17.1 h. There was no statistically significant associations between daylength and subjective sleepiness (r(1403) = –0.01, 95% CI [–0.06, 0.04], p = 0.69) or vigilance task median reaction times (r(1310) = –0.03, 95% CI [–0.08, 0.03], p = 0.36), but small correlations (r &amp;lt; 0.20) were observed between daylength other cognitive measures. The direction of effect suggested that longer daylengths were associated with reduced cognitive speed and accuracy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cognitive performance is correlated with recent light exposure&lt;/head&gt;
    &lt;p&gt;We next set out to ask whether cognitive task performance was correlated with recent light exposure. Including adjustments for time of day, time awake, last sleep duration, and last sleep midpoint, we assessed correlations with current light intensity as the mean light exposure over the preceding 30, 60, 90, and 120 min. This revealed correlations for subjective sleepiness, vigilance task and working memory reaction times, and working memory false negative rate. The effect was robust for the 30 min window for vigilance task reaction time and up to the 2 h window for subjective sleepiness, while working memory reaction time showed a moderate but consistent effect up to the 1.5 h window (Fig. 3A, Table S4).&lt;/p&gt;
    &lt;p&gt;Based on these findings, we utilized 30 min light history durations to model intensity-response relationships. By calculating the slopes of these curves, we quantified the relationship between light exposure and the three cognitive outcomes with most convincing light associations (subjective sleepiness, vigilance task reaction time, and working memory reaction time), offering an indication of cognitive sensitivity to light in real-world environments. On average, a 1 log-lux increase in melanopic EDI was associated with a 0.2-point reduction in subjective sleepiness as measured by the subjective sleepiness (Std. Coef. = –0.11, 95% CI [–0.18, –0.04], p = 0.003, partial η² = 0.14) (Fig. 3B). Despite this overall trend, substantial interindividual variability was observed (including six participants exhibiting positive slopes), indicating that the relationship between light and sleepiness varied among individuals. For the vigilance task, a 4 log-lux increase in melanopic EDI—from the sensor’s detection threshold to full sunlight — corresponded to an approximately 30 ms improvement in reaction time (Std. Coef. = –0.09, 95% CI [–0.14, –0.03], p = 0.003, partial η² = 0.09) (Fig. 3C). Similarly, but with a small effect size, working memory reaction time for correct short-term memory recall improved by roughly 60 ms across the same range of melanopic EDI (Std. Coef. = –0.07, 95% CI [–0.13, –0.01], p = 0.033, partial η² = 0.01; with task day adjusted for the learning effect) (Fig. 3D). Interestingly, interindividual variability appeared more pronounced for vigilance and working memory performance compared to subjective sleepiness scores, with nearly twice as many participants exhibiting positive slopes in these cognitive tasks. Coefficients of variation were calculated as −88.4 for subjective sleepiness, -181.1 for vigilance, and −633.1 for working memory, highlighting the diversity in correlation with light exposure across different cognitive measures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Determinants of light and cognition correlation&lt;/head&gt;
    &lt;p&gt;Having assessed within individual relationships between light exposure and performance, we moved to exploring the origins of inter-individual variation in the nature of this association. We first asked whether participants’ patterns of light exposure across the week were predictive of their sensitivity. To this end, we extracted various dimensions of light history known to affect circadian rhythms, including daytime and nighttime light intensity, duration of exposure, and the timing and stability of light exposure (see “Methods” for details). These habitual light exposure variables were subsequently compared to the slopes of the relationship between light and cognitive outcomes that demonstrated associations with light history, including subjective sleepiness, vigilance task reaction time, and working memory reaction time (Table S5). Higher M10, earlier clock times for the dimmest 5 h epoch and light exposure patterns with lower intra-daily variability and higher inter-daily stability all correlated with stronger associations between recent light exposure and subjective sleepiness. The strongest correlations were observed in the relationship between light and subjective sleepiness and timing of the main epoch of darkness (indicative of time in bed) (Fig. 4A). Specifically, participants whose dark epoch occurred later and had lower daytime light exposure showed less steep negative slopes for subjective sleepiness, indicating a weaker association between light and subjective sleepiness (L5 mid-time: r(52) = 0.44, p &amp;lt; 0.001, 95% CI [0.20, 0.64] and M10: r(52) = –0.38, p = 0.005, 95% CI [–0.59, –0.12]; Figure S5A). These correlations remained robust after adjusting for age, sex, caffeine consumption, alcohol consumption, smoking, chronotype (MSFsc), and sleep problems (PSQI) (b = 0.22, SE = 0.08, 95% CI [0.05, 0.38], p = 0.013 and b = –0.76, SE = 0.29, 95% CI [–1.35, –0.17], p = 0.013 respectively).&lt;/p&gt;
    &lt;p&gt;We next explored whether it was possible to predict cognitive task photosensitivity using in-lab assessments of light sensitivity. To this end, a subgroup of participants (n = 41) volunteered for in-lab assessments designed to measure melanopsin sensitivity through a series of pupillometric and perceptual psychophysics tasks43. The lab-based tests included the Post-Illumination Pupil Response (PIPR) test, silent substitution melanopic brightness discrimination tasks, and a subjective brightness assessment. The derived variables from these tasks were then analysed in relation to the real-world light sensitivity of cognitive functions mentioned earlier (Table S6). This failed to reveal strong associations (Fig. 4B).&lt;/p&gt;
    &lt;head rend="h3"&gt;Determinants of inter-individual differences in cognitive performance&lt;/head&gt;
    &lt;p&gt;We next asked whether any aspects of an individual’s light exposure profile correlated with overall cognitive performance (Table S7). Once again, we used nine cognitive performance measures (subjective sleepiness, vigilance task, working memory, and visual search), which were averaged across the week. The most consistent associations across endpoints were with the M10 (intensity over the day’s brightest 10 h) and IV (intra-daily variability) (Fig. 4C). Specifically, people with brighter daytime exposure (M10; Fig. S5B) and less fragmented daily patterns of light exposure (IV) had lower visual search false positive rate (r(54) = –0.53, p &amp;lt; 0.001, 95% CI [–0.69, –0.31] and r(54) = 0.44, p &amp;lt; 0.001, 95% CI [0.20, 0.63] respectively) and lower working memory false positive rate (r(54) = –0.27, p = 0.043, 95% CI [–0.50, –0.01] and r(54) = 0.31, p = 0.021, 95% CI [0.05, 0.53] respectively), and reduced vigilance task reaction time (r(54) = –0.32, p = 0.015, 95% CI [–0.54, –0.07] and r(54) = 0.33, p = 0.013, 95% CI [0.08, 0.55] respectively). Correlations of visual search false positive rate with M10 and IV remained robust after adjusting for covariates (b = –14.4, SE = 3.30, 95% CI [–21.0, –7.7], p &amp;lt; 0.001 and b = 37.7, SE = 12.7, 95% CI 12.1, 63.4], p = 0.005 respectively).&lt;/p&gt;
    &lt;p&gt;Lastly, in-lab light sensitivity measures were compared to cognitive variables (Table S8). Initial pupil constriction had the most significant associations, with greater constriction indicative of greater false positives in visual search (r(39) = 0.43, p = 0.005, 95% CI [0.15, 0.65]; Fig. 4D). In addition, a higher rating of subjective brightness for a standard test stimulus was correlated with fewer false negatives in visual search (r(39) = –0.42, p = 0.007, 95% CI [–0.64, –0.12]; Fig. S5C). These associations remained robust after adjusting for demographic and lifestyle covariates (b = 0.70, SE = 0.19, 95% CI [0.32, 1.09], p = 0.001 and b = –0.24, SE = 0.10, 95% CI [–0.44, –0.04], p = 0.021 respectively).&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;This study examined the relationship between light exposure and cognitive performance in real-world settings. Our findings are consistent with the hypothesis that even outside controlled laboratory conditions, where participants continued their daily routines, both recent and long-term light exposure positively influences cognitive performance. We find that recent bright light correlates with lower sleepiness scores and faster reaction times for vigilance task and working memory tasks. Meanwhile, participants whose habitual light exposure was characterized by brighter light through daytimes and less intra-daily fragmentation had fewer false negative responses in working memory and visual search and reduced reaction time in the vigilance task. Our study was less successful in identifying predictors of inter-individual differences in the strength of these light associations. Earlier bedtimes (as estimated by the time of dimmest light) and brighter daytime exposure were associated with stronger light dependence for subjective sleepiness, but this was not strongly apparent for other endpoints. Similarly, although there were some statistically significant associations between the strength of light associations and performance in a bank of putative in-lab assessments of melanopic responsiveness these did not form a clear pattern.&lt;/p&gt;
    &lt;p&gt;The strongest intra-individual associations with light were observed for subjective sleepiness, with nearly all participants experiencing reduced sleepiness following bright light exposure independent of time of day. This is consistent with a field study demonstrating that light exposure reduces momentary exhaustion, as well as our previous findings indicating that light exposure decreases subjective sleepiness41,47. This relationship was strongest in individuals with earlier bedtimes (inferred from the timing of lowest light exposure L5). Given that there was no statistically significant association between the timing of L5 and average subjective sleepiness this implies that, compared with their peers with later bedtimes, those with earlier bedtimes tend to be both more reliably wakeful under bright- and sleepy under dim-light. This could enforce an earlier sleep phase because morning light tends to be bright and evening light dim.&lt;/p&gt;
    &lt;p&gt;Surprisingly, neither time of day nor time awake had large effects on vigilance task, visual search, or working memory performance. Given that attentional and memory-related brain regions exhibit intrinsic circadian rhythms, and that sleep prior to cognitive tasks is a crucial determinant of cognitive functioning, one might expect a diurnal cycle in these cognitive traits1,48. However, its effects were not clearly observable in our data. The effect of light was found to be stronger than the effect of time of day. This remains an ongoing area of research in the literature, with inconsistent findings. Some studies suggest that circadian effects on executive function are limited compared to arousal-related measures1,27. Additionally, external factors in everyday life—such as lifestyle preferences, motivation, and environmental distractions—can create significant intra- and inter-individual differences, potentially overriding internal oscillatory effects49.&lt;/p&gt;
    &lt;p&gt;The correlation between higher recent light and reduced reaction times observed across both vigilance task and working memory is consistent with controlled laboratory findings12,17,19,20,22. Our results indicate that exposure to bright light, comparable to daylight, may improve reaction times in these tasks by 7–10% compared to dim conditions. While this effect size is modest, they are larger than would be expected for visual adaptation alone50,51, and these improvements in cognitive performance may have practical implications for health, safety, and work efficiency, particularly in low-light workplaces, during extended work hours, or night shifts31,32,33,34,35. Previous research has shown that adverse circadian and homeostatic processes lead to a global reduction in vigilance but do not necessarily impair visual selective attention52. Notably, the improvements in reaction time were not accompanied by changes in accuracy, indicating that this was not merely a general hyper-responsiveness—participants continued to make correct responses despite responding more quickly. Other aspects of task performance appeared less dependent on light exposure, suggesting either that light is not a major determinant of working memory and visual search efficiency or that our study lacked the statistical power to detect such effects.&lt;/p&gt;
    &lt;p&gt;Our analysis of light exposure history further revealed that cognitive outcomes are correlated with habitual light exposure patterns. Individuals whose light exposure patterns aligned more closely with recommended guidelines9,53 – including brighter daytime exposure, an earlier sleep phase, and more stable and consistent daily light exposure – tended to perform better on cognitive tasks. However, these effects were not uniformly observed across all cognitive parameters or all exposure-performance correlations. The strongest associations emerged with the visual search task, suggesting that prolonged exposure to an optimal light environment may particularly enhance visual attention and processing efficiency.&lt;/p&gt;
    &lt;p&gt;Overall, two key patterns emerged from our results: (1) recent light exposure was associated with cognitive effects consistent with increased arousal (e.g., heightened alertness and faster reaction times), and (2) a history of bright, stable daytime light exposure was linked to enhanced sustained attention in a visual search task. Both effects are likely initiated by activation of the ipRGC system, as laboratory studies have demonstrated a short-wavelength bias in the acute effects of light on arousal and cognitive function (consistent with melanopsin spectral sensitivity)7,13,22, alongside the established role of ipRGCs in regulating central clock function via the retinohypothalamic tract to the suprachiasmatic nucleus (SCN)54. Acute effects may involve not only ipRGC input to the SCN but also contributions from projections to other brain regions. For example, the locus coeruleus (LC) and its ascending reticular activating system have been implicated in mediating acute arousal, potentially through indirect input from the SCN55,56. However, other ipRGC target regions may also contribute. Animal studies have identified several candidate pathways, though their relevance to humans remains less certain57. Furthermore, improved attention under high-melanopic light suggests contributions of top-down attentional and executive mechanisms, likely mediated by ipRGC–thalamic–corticolimbic pathways influencing prefrontal cortical activity4,5,58. For the longer-term effects, exposure to high-amplitude, early light patterns appears to strengthen both circadian robustness and sleep homeostasis, thereby supporting improved cognitive performance over time23,59.&lt;/p&gt;
    &lt;p&gt;Previous studies on circadian rhythms have identified substantial interindividual differences in light sensitivity, particularly concerning melatonin suppression60. It would be ideal to have an accessible biomarker for light sensitivity. However, none of the battery of in-lab tests we conducted robustly predicted light sensitivity across cognitive tasks. While some statistically significant associations were observed, their reliability was limited. This suggests that the in-lab measures used here do not function as global predictors of photosensitivity. Perhaps in-lab and in-field measures reflect different processes with independent regulators of sensitivity. Alternatively, sensitivity could change so dynamically that a one-off in-lab measure is a poor predictor of sensitivity across other conditions; or our sample may simply not have captured a sufficiently broad range of individual sensitivities to detect a meaningful association.&lt;/p&gt;
    &lt;p&gt;Perhaps surprisingly, our data did reveal relationships between average task performance per individual and their in-lab light responsiveness. The magnitude of initial pupil constriction and the subjective brightness score for a standard stimulus each correlated with aspects of mean task performance, albeit without showing consistent effects across tasks. Greater pupil reactivity was associated with reduced visual search accuracy, while greater subjective brightness score was linked to higher visual search accuracy. The basis for these associations remains uncertain. In principle, people with higher light sensitivity may have improved cognitive performance across lighting conditions. However, the value of these in lab assays as predictors of melanopic light sensitivity under different circumstances is unproven and if this were the origin for these correlations, we might expect them to be strongest for cognitive parameters with the clearest associations with recent light (subjective sleepiness and reaction times for vigilance task and working memory). The pupil can provide an indication of central arousal mechanisms and has been shown to correlate with vigilance performance61, but it would be surprising if a one-off in lab measure predicted average arousal over the previous 7 days.&lt;/p&gt;
    &lt;head rend="h3"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;This study has several limitations. Our sample primarily consisted of individuals without significant circadian or light exposure challenges, leaving the possibility that the observed effects could be more (or less) pronounced in populations with disrupted circadian rhythms, such as shift workers, individuals with sleep and psychiatric disorders, or older adults. Additionally, because of small sample size, variations in photosensitivity likely influenced62 by factors such as age and genetics, were not explicitly controlled for in this study. Importantly, as a correlational study rather than an intervention, our findings cannot establish causality between light exposure and cognitive performance. However, our protocol successfully demonstrated the feasibility of monitoring personal light exposure and cognitive performance in real-world settings and several of our findings are consistent with the hypothesis that light can be a determinant of cognitive function in everyday life.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data availability&lt;/head&gt;
    &lt;p&gt;The full study protocol was shared in protocols.io (https://doi.org/10.17504/protocols.io.n92ldrjjxg5b/v1). Anonymized data on light exposure, in-lab light sensitivity assessments, and cognitive tasks, as well as the processed data used to generate the figures and tables in the paper, are available in a Figshare repository (https://doi.org/10.48420/28911977).&lt;/p&gt;
    &lt;head rend="h2"&gt;Code availability&lt;/head&gt;
    &lt;p&gt;R code to process and analyze the data of light exposure and cognitive tasks created for the study are available in a GitHub repository (https://github.com/altugdidikoglu/light-cognition-inreallife). Software and hardware designs of the wearable light dosimeter are available in a repository (https://github.com/Non-Invasive-Bioelectronics-Lab/Wearable_Light_Sensor_Public).&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Cajochen, C. &amp;amp; Schmidt, C. The circadian brain and cognition. Annu. Rev. Psychol. 76, 115–141 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Chellappa, S. L. et al. Photic memory for executive brain responses. Proc. Natl. Acad. Sci. USA 111, 6087–6091 (2014).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Blume, C. &amp;amp; Münch, M. Effects of light on biological functions and human sleep. Handb. Clin. Neurol. 206, 3–16 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mahoney, H. L. &amp;amp; Schmidt, T. M. The cognitive impact of light: illuminating ipRGC circuit mechanisms. Nat. Rev. Neurosci. 25, 159–175 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schmidt, T. M. et al. Melanopsin-positive intrinsically photosensitive retinal ganglion cells: from form to function. J. Neurosci. 31, 16094–16101 (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Do, M. T. H. Melanopsin and the intrinsically photosensitive retinal ganglion cells: biophysics to behavior. Neuron 104, 205–226 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schöllhorn, I. et al. Melanopic irradiance defines the impact of evening display light on sleep latency, melatonin and alertness. Commun. Biol. 6, 228 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Allen, A. E., Hazelhoff, E. M., Martial, F. P., Cajochen, C. &amp;amp; Lucas, R. J. Exploiting metamerism to regulate the impact of a visual display on alertness and melatonin suppression independent of visual appearance. Sleep 41. https://doi.org/10.1093/sleep/zsy100 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Brown, T. M. et al. Recommendations for daytime, evening, and nighttime indoor light exposure to best support physiology, sleep, and wakefulness in healthy adults. PLoS Biol. 20, e3001571 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Campbell, I. et al. Regional response to light illuminance across the human hypothalamus. Elife 13, RP96576 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sabbah, S., Worden, M. S., Laniado, D. D., Berson, D. M. &amp;amp; Sanes, J. N. Luxotonic signals in human prefrontal cortex as a possible substrate for effects of light on mood and cognition. Proc. Natl. Acad. Sci. USA 119, e2118192119 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Vandewalle, G., Maquet, P. &amp;amp; Dijk, D. J. Light as a modulator of cognitive brain function. Trends Cogn. Sci. 13, 429–438 (2009).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Siraji, M. A., Kalavally, V., Schaefer, A. &amp;amp; Haque, S. Effects of daytime electric light exposure on human alertness and higher cognitive functions: a systematic review. Front. Psychol. 12, 765750 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lok, R., Smolders, K., Beersma, D. G. M. &amp;amp; de Kort, Y. A. W. Light, alertness, and alerting effects of white light: a literature overview. J. Biol. Rhythms 33, 589–601 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Chellappa, S. L. et al. Non-visual effects of light on melatonin, alertness and cognitive performance: can blue-enriched light keep us alert? PLoS One 6, e16429 (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cajochen, C. et al. Evening exposure to a light-emitting diodes (LED)-backlit computer screen affects circadian physiology and cognitive performance. J. Appl. Physiol. 110, 1432–1438 (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Phipps-Nelson, J., Redman, J. R., Dijk, D. J. &amp;amp; Rajaratnam, S. M. Daytime exposure to bright light, as compared to dim light, decreases sleepiness and improves psychomotor vigilance performance. Sleep 26, 695–700 (2003).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Huiberts, L. M., Smolders, K. C. H. J. &amp;amp; de Kort, Y. A. W. Shining light on memory: Effects of bright light on working memory performance. Behav. Brain Res. 294, 234–245 (2015).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bjerrum, L. B. et al. Acute effects of light during daytime on central aspects of attention and affect: A systematic review. Biol. Psychol. 192, 108845 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ru, T., Smolders, K. C. H. J., Chen, Q., Zhou, G. &amp;amp; de Kort, Y. A. W. Diurnal effects of illuminance on performance: Exploring the moderating role of cognitive domain and task difficulty. Lighting Res. Technol. 53, 727–747 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lasauskaite, R., Wüst, L. N., Schöllhorn, I., Richter, M. &amp;amp; Cajochen, C. Non-image-forming effects of daytime electric light exposure in humans: a systematic review and meta-analyses of physiological, cognitive, and subjective outcomes. LEUKOS, 1-42. (2025)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Suzuki, Y., Nakauchi, S. &amp;amp; Liao, H.-I. Selective activation of ipRGC modulates working memory performance. PLOS ONE 20, e0327349 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;McHill, A. W., Hull, J. T., Wang, W., Czeisler, C. A. &amp;amp; Klerman, E. B. Chronic sleep curtailment, even without extended (&amp;gt;16-h) wakefulness, degrades human vigilance performance. Proc. Natl. Acad. Sci. USA 115, 6070–6075 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kalanadhabhatta, M., Rahman, T. &amp;amp; Ganesan, D. Effect of sleep and biobehavioral patterns on multidimensional cognitive performance: longitudinal, in-the-wild study. J. Med. Internet Res. 23, e23936 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wams, E. J. et al. Linking light exposure and subsequent sleep: a field polysomnography study in humans. Sleep 40. https://doi.org/10.1093/sleep/zsx165 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dijk, D. J., Duffy, J. F. &amp;amp; Czeisler, C. A. Circadian and sleep/wake dependent aspects of subjective alertness and cognitive performance. J. Sleep. Res 1, 112–117 (1992).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Santhi, N. et al. Sex differences in the circadian regulation of sleep and waking cognition in humans. Proc. Natl. Acad. Sci. USA 113, E2730–E2739 (2016).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Daneault, V. et al. Aging reduces the stimulating effect of blue light on cognitive brain functions. Sleep 37, 85–96 (2014).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gabel, V. et al. Dawn simulation light impacts on different cognitive domains under sleep restriction. Behav. Brain Res. 281, 258–266 (2015).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Windred, D. P. et al. Brighter nights and darker days predict higher mortality risk: A prospective analysis of personal light exposure in &amp;gt;88,000 individuals. Proc. Natl. Acad. Sci. USA 121, e2405924121 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sunde, E. et al. Bright light exposure during simulated night work improves cognitive flexibility. Chronobiol. Int 39, 948–963 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Price, L. L. A., Khazova, M. &amp;amp; Udovicic, L. Assessment of the light exposures of shift-working nurses in London and dortmund in relation to recommendations for sleep and circadian health. Ann. Work Expo. Health 66, 447–458 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Li, J. et al. Lighting for work: a study on the effect of underground low-light environment on miners’ physiology. Environ. Sci. Pollut. Res. Int. 29, 11644–11653 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lunn, R. M. et al. Health consequences of electric lighting practices in the modern world: a report on the National Toxicology Program’s workshop on shift work at night, artificial light at night, and circadian disruption. Sci. Total Environ. 607-608, 1073–1084 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sletten, T. L. et al. A blue-enriched, increased intensity light intervention to improve alertness and performance in rotating night shift workers in an operational setting. Nat. Sci. Sleep. 13, 647–657 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Grant, L. K. et al. Daytime exposure to short wavelength-enriched light improves cognitive performance in sleep-restricted college-aged adults. Front Neurol. 12, 624217 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Figueiro, M. G. et al. Effects of a tailored lighting intervention on sleep quality, rest–activity, mood, and behavior in older adults with Alzheimer disease and related dementias: a randomized clinical trial. J. Clin. Sleep. Med. 15, 1757–1767 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mohammadian, N. et al. A wrist-worn internet of things sensor node for wearable equivalent daylight illuminance monitoring. IEEE Internet Things J. 11, 16148–16157 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gardesevic, M. et al. Brighter time: a smartphone app recording cognitive task performance and illuminance in everyday life. Clocks Sleep. 4, 577–594 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zauner, J., Udovicic, L. &amp;amp; Spitschan, M. Power analysis for personal light exposure measurements and interventions. PLoS One 19, e0308768 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Didikoglu, A. et al. Associations between light exposure and sleep timing and sleepiness while awake in a sample of UK adults in everyday life. Proc. Natl. Acad. Sci. USA 120, e2301608120 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Guidolin, C. et al. Protocol for a prospective, multicentre, cross-sectional cohort study to assess personal light exposure. BMC Public Health 24, 3285 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Woelders, T., Didikoglu, A., Bickerstaff, L., Brown, T. M. &amp;amp; Lucas, R. J. Pupillometric and perceptual approaches provide independent estimates of melanopsin activity in humans. Sleep. https://doi.org/10.1093/sleep/zsae289 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Buysse, D. J. et al. The Pittsburgh Sleep Quality Index: a new instrument for psychiatric practice and research. Psychiatry Res. 28, 193–213 (1989).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Roenneberg, T. et al. A marker for the end of adolescence. Curr. Biol. 14, R1038–R1039 (2004).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Akerstedt, T. &amp;amp; Gillberg, M. Subjective and objective sleepiness in the active individual. Int J. Neurosci. 52, 29–37 (1990).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Frick, S., Smolders, K., van der Meij, L., Demerouti, E. &amp;amp; de Kort, Y. A higher illuminance reduces momentary exhaustion in exhausted employees: results from a field study. J. Environ. Psychol. 102543. (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kyriacou, C. P. &amp;amp; Hastings, M. H. Circadian clocks: genes, sleep, and cognition. Trends Cogn. Sci. 14, 259–267 (2010).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hull, J. T., Wright, K. P. &amp;amp; Czeisler, C. A. The influence of subjective alertness and motivation on human performance independent of circadian and homeostatic regulation. J. Biol. Rhythms 18, 329–338 (2003).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Chappelow, A. V. &amp;amp; Marmor, M. F. Effects of pre-adaptation conditions and ambient room lighting on the multifocal ERG. Doc. Ophthalmol. 105, 23–31 (2002).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gouras, P. &amp;amp; MacKay, C. J. Growth in amplitude of the human cone electroretinogram with light adaptation. Investig. Ophthalmol. Vis. Sci. 30, 625–630 (1989).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Horowitz, T. S., Cade, B. E., Wolfe, J. M. &amp;amp; Czeisler, C. A. Searching night and day:a dissociation of effects of circadian phase and time awake on visual selective attention and vigilance. Psychol. Sci. 14, 549–557 (2003).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hand, A. J. et al. Measuring light regularity: sleep regularity is associated with regularity of light exposure in adolescents. Sleep 46. https://doi.org/10.1093/sleep/zsad001 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Berson, D. M., Dunn, F. A. &amp;amp; Takao, M. Phototransduction by retinal ganglion cells that set the circadian clock. Science 295, 1070–1073 (2002).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Aston-Jones, G., Chen, S., Zhu, Y. &amp;amp; Oshinsky, M. L. A neural circuit for circadian regulation of arousal. Nat. Neurosci. 4, 732–738 (2001).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Vandewalle, G. et al. Brain responses to violet, blue, and green monochromatic light exposures in humans: prominent role of blue light and the brainstem. PLoS ONE 2, e1247 (2007).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hattar, S. et al. Central projections of melanopsin-expressing retinal ganglion cells in the mouse. J. Comp. Neurol. 497, 326–349 (2006).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Alkozei, A. et al. Exposure to blue light increases subsequent functional activation of the prefrontal cortex during performance of a working memory task. Sleep 39, 1671–1680 (2016).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Vandewalle, G. et al. Effects of light on cognitive brain responses depend on circadian phase and sleep homeostasis. J. Biol. Rhythms 26, 249–259 (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Phillips, A. J. K. et al. High sensitivity and interindividual variability in the response of the human circadian system to evening light. Proc. Natl. Acad. Sci. USA 116, 12019–12024 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Martin, J. T., Whittaker, A. H. &amp;amp; Johnston, S. J. Pupillometry and the vigilance decrement: Task-evoked but not baseline pupil measures reflect declining performance in visual vigilance tasks. Eur. J. Neurosci. 55, 778–799 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Swope, C. B. et al. Factors associated with variability in the melatonin suppression response to light: a narrative review. Chronobiol. Int 40, 542–556 (2023).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;This work was funded by the University of Manchester Wellcome Trust/ISSF fund to R.J.L., M.V.T., T.M.B., A.J.C., and S.J. and by a Wellcome Trust Investigator Award (210684/Z/18/Z) to R.J.L. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.&lt;/p&gt;
    &lt;head rend="h2"&gt;Author information&lt;/head&gt;
    &lt;head rend="h3"&gt;Authors and Affiliations&lt;/head&gt;
    &lt;head rend="h3"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;A.D.: Project administration, methodology, recruitment, data collection, data analysis; T.W.: Methodology, data analysis; L.B.: Methodology, recruitment, data collection; N.M., A.J.C., T.M.B. and A.D.: Design, production, calibration, technical support of the wearable light dosimeters; M.V.T., T.M.B., S.J., A.J.C. and R.J.L.: Conceptualization, supervision, methodology. A.D., T.W. and R.J.L.: Writing-original draft. All authors discussed the results and edited the manuscript. All authors have read and agreed to the published version of the manuscript.&lt;/p&gt;
    &lt;head rend="h3"&gt;Corresponding authors&lt;/head&gt;
    &lt;head rend="h2"&gt;Ethics declarations&lt;/head&gt;
    &lt;head rend="h3"&gt;Competing interests&lt;/head&gt;
    &lt;p&gt;R.J.L. and T.M.B. have received investigator-initiated grant funding from Signify/Philips Lighting and R.J.L. has received honoraria from Samsung Electronics. All other authors declare no competing interests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Peer review&lt;/head&gt;
    &lt;head rend="h3"&gt;Peer review information&lt;/head&gt;
    &lt;p&gt;Communications Psychology thanks the anonymous reviewers for their contribution to the peer review of this work. Primary Handling Editors: Xiaoqing Hu and Troby Ka-Yan Lui. A peer review file is available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional information&lt;/head&gt;
    &lt;p&gt;Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Supplementary information&lt;/head&gt;
    &lt;head rend="h2"&gt;Rights and permissions&lt;/head&gt;
    &lt;p&gt;Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.&lt;/p&gt;
    &lt;head rend="h2"&gt;About this article&lt;/head&gt;
    &lt;head rend="h3"&gt;Cite this article&lt;/head&gt;
    &lt;p&gt;Didikoglu, A., Woelders, T., Bickerstaff, L. et al. Relationships between light exposure and aspects of cognitive function in everyday life. Commun Psychol 4, 5 (2026). https://doi.org/10.1038/s44271-025-00373-9&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Received:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Accepted:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Published:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Version of record:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DOI: https://doi.org/10.1038/s44271-025-00373-9&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nature.com/articles/s44271-025-00373-9"/><published>2026-02-01T18:51:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46848415</id><title>I taught my neighbor to keep the volume down</title><updated>2026-02-01T19:42:21.296331+00:00</updated><content>&lt;doc fingerprint="2728920ef029d7c1"&gt;
  &lt;main&gt;
    &lt;p&gt;When I moved to a new apartment with my family, the cable company we were used to wasn't available. We had to settle for Dish Network. I wasn't too happy about making that switch, but something on their website caught my attention. For an additional $5 a month, I could have access to DVR. I switched immediately.&lt;/p&gt;
    &lt;p&gt;This was 2007. DVR was not new, but it wasn't commonly bundled with set-top boxes. TiVo was still the popular way to record, pause, and rewind live TV. We received two set-top boxes, one for each room with a TV, and three remotes. Two remotes had IR (infrared) blasters and, surprisingly, one RF (radio frequency) remote.&lt;/p&gt;
    &lt;p&gt;After using the RF remote, I wondered: Why would anyone ever use an IR remote again? You didn't need a direct line of sight with the device you were controlling. I could actually stand in the kitchen and control the TV. It was amazing. But with the convenience of RF came other problems that IR users never had to worry about. Interference.&lt;/p&gt;
    &lt;p&gt;After several months of enjoying my service, one of my neighbors, the loudest in the building, also switched to Dish Network. And he also got the RF remote. This was the type of neighbor who would leave the house with the TV on, volume blasting.&lt;/p&gt;
    &lt;p&gt;One day, I was in the living room watching TV when the channel just flipped. I must have accidentally hit a button, so I changed it back. But not a few seconds later, the channel changed again. Then the volume went up. I figured my sister must have had the RF remote and was messing with me. But no, the remote was in my hand. I assumed something was wrong with it.&lt;/p&gt;
    &lt;p&gt;The whole time I was watching TV, the channels kept randomly switching. I banged the remote on the table a couple of times, but it still switched. I removed the batteries from the remote, it still switched. I unplugged the device for a few minutes, plugged it back in, and… it still switched. Frustrated, I went through the device settings and disabled the RF remote. That's when it finally stopped. I wasn't happy with this solution, but it allowed me to watch TV until I figured something out.&lt;/p&gt;
    &lt;p&gt;One evening, when everyone was asleep and the neighbor was watching a loud TV show, I decided to diagnose the issue. The moment I pressed the power button on the RF remote, my TV and set-top box turned on, and the neighbor's TV went silent. "Fuck!" I heard someone say. I was confused. Did I just do that? The TV turned back on, the volume went up. I walked to the window armed with the remote. I counted to three, then pressed the power button. My neighbor's TV went silent. He growled.&lt;/p&gt;
    &lt;p&gt;I am the captain now.&lt;/p&gt;
    &lt;p&gt;Every time he turned the TV on, I pressed the power button again and his device went off. Well, what do you know? We had interference somehow. Our remotes were set up to operate at the same frequency. Each remote controlled both devices.&lt;/p&gt;
    &lt;p&gt;But I'm not that kind of neighbor. I wasn't going to continue to mess with him. Instead, I decided I would pay him a visit in the morning and explain that our remotes are tuned to the same frequency. I would bring the RF remote with me just to show him a demo. I was going to be a good neighbor.&lt;/p&gt;
    &lt;p&gt;In the morning, I went downstairs, remote in hand. I knocked on the door, and a gentleman in his forties answered the door. I had rehearsed my speech and presentation. This would be a good opportunity to build a good rapport, and have a shared story. Maybe he would tell me how he felt when the TV went off. How he thought there was a ghost in the house or something. But that's not what happened.&lt;/p&gt;
    &lt;p&gt;"Hi, I'm Ibrahim. Your upstairs neighbor..." I started and was interrupted almost immediately. "Whatever you are selling," he yelled. "I'm not buying." and he closed the door on my face. I knocked a second time, because obviously there was a misunderstanding. He never answered. Instead, the TV turned on and a movie played at high volume. So much for my prepared speech.&lt;/p&gt;
    &lt;p&gt;The RF settings on my set-top box remained turned off. My family never discovered its benefit anyway, they always pointed at the box when pressing the buttons. It wasn't much of an inconvenience. In fact, I later found in the manual that you could reprogram the device and remote to use a different frequency. I did not reprogram my remote. Instead, my family used the two IR remotes, and brought the RF remote in my bedroom where it permanently remained on my night stand.&lt;/p&gt;
    &lt;p&gt;Why in the bedroom? Because I decided to teach my neighbor some good manners. Whenever he turned up his volume, I would simply turn off his device. I would hear his frustration, and his attempts at solving the problem. Like a circus animal trainer, I remained consistent. If the volume of his TV went above what I imagined to be 15 to 20, I would press the power button. It became a routine for me for weeks. Some nights were difficult, I would keep the remote under my pillow, battling my stubborn neighbor all night.&lt;/p&gt;
    &lt;p&gt;One day, I noticed that I hadn't pressed the button in days. I opened the window and I could still hear the faint sound of his TV. Through trial and error, he learned the lesson. If the volume remained under my arbitrary threshold, the TV would remain on. But as soon as he passed that threshold, the device would turn off.&lt;/p&gt;
    &lt;p&gt;Sometimes, he would have company and there would be noise coming out of his apartment. I used the one tool in my tool box to send him a message. Turn off the TV. All of the sudden, my neighbor and his guest will be reminded of the unspoken rules, and become mindful of their neighbors.&lt;/p&gt;
    &lt;p&gt;Maybe somewhere on the web, in some obscure forum, someone asked the question: "Why does my set-top box turn off when I increase the volume?" Well, it might be 18 years too late, but there's your answer. There is a man out there who religiously sets his volume to 18. He doesn't quite know why. That's Pavlovian conditioning at its best.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://idiallo.com/blog/teaching-my-neighbor-to-keep-the-volume-down"/><published>2026-02-01T19:00:46+00:00</published></entry></feed>