<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-25T00:45:38.886936+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45359604</id><title>How to Lead in a Room Full of Experts</title><updated>2025-09-25T00:45:46.452203+00:00</updated><content>&lt;doc fingerprint="f3751d93156404b7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Here is a realization I made recently. I'm sitting in a room full of smart people. On one side are developers who understand the ins and outs of our microservice architecture. On the other are the front-end developers who can debug React in their sleep. In front of me is the product team that has memorized every possible user path that exists on our website. And then, there is me. The lead developer. I don't have the deepest expertise on any single technology.&lt;/p&gt;
      &lt;p&gt;So what exactly is my role when I'm surrounded by experts? Well, that's easy. I have all the answers.&lt;/p&gt;
      &lt;head rend="h2"&gt;Technical Leadership&lt;/head&gt;
      &lt;p&gt;OK. Technically, I don't have all the answers. But I know exactly where to find them and connect the pieces together. &lt;/p&gt;
      &lt;p&gt;When the backend team explains why a new authentication service would take three weeks to build, I'm not thinking about the OAuth flows or JWT token validation. Instead, I think about how I can communicate it to the product team who expects it done "sometime this week." When the product team requests a "simple" feature, I'm thinking about the 3 teams that need to be involved to update the necessary microservices.&lt;/p&gt;
      &lt;p&gt;Leadership in technical environments isn't about being the smartest person in the room. It's about being the most effective translator.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is a Social Skill&lt;/head&gt;
      &lt;p&gt;I often get "eye rolls" when I say this to developers: You are not going to convince anyone with facts. In a room full of experts, your technical credibility gets you a seat at the table, but your social skills determine whether anything productive happens once you're there.&lt;/p&gt;
      &lt;p&gt;Where ideally you will provide documentation that everyone can read and understand, in reality, you need to talk to get people to understand. People can get animated when it comes to the tools they use. When the database team and the API team are talking past each other about response times, your role isn't to lay down the facts. Instead it's to read the room and find a way to address technical constraints and unclear requirements. It means knowing when to let a heated technical debate continue because it's productive, and when to intervene because it's become personal.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is Remembering the Goal&lt;/head&gt;
      &lt;p&gt;When you are an expert in your field, you love to dive deep. It's what makes you experts. But someone needs to keep one eye on the forest while everyone else is examining the trees.&lt;/p&gt;
      &lt;p&gt;I've sat through countless meetings where engineers debated the merits of different caching strategies while the real issue was that we hadn't clearly defined what "fast enough" meant for the user experience. The technical discussion was fascinating, but it wasn't moving us toward shipping.&lt;/p&gt;
      &lt;p&gt;As a leader, your job isn't to have sophisticated technical opinions. It's to ask how this "discussion" can move us closer to solving our actual problem.&lt;/p&gt;
      &lt;p&gt;When you understand a problem, and you have a room full of experts, the solution often emerges from the discussion. But someone needs to clearly articulate what problem we're actually trying to solve.&lt;/p&gt;
      &lt;p&gt;When a product team says customers are reporting the app is too slow, that's not a clear problem. It's a symptom. It might be that users are not noticing when the shopping cart is loaded, or that maybe we have an event that is not being triggered at the right time. Or maybe the app feels sluggish during peak hours. Each of those problems has different solutions, different priorities, and different trade-offs. Each expert might be looking at the problem with their own lense, and may miss the real underlying problem.&lt;/p&gt;
      &lt;p&gt;Your role as a leader is to make sure the problem is translated in a way the team can clearly understand the problem.&lt;/p&gt;
      &lt;head rend="h3"&gt;Leading is Saying "I Don't Know"&lt;/head&gt;
      &lt;p&gt;By definition, leading is knowing the way forward. But in reality, in a room full of experts, pretending to know everything makes you look like an idiot.&lt;/p&gt;
      &lt;p&gt;Instead, "I don't know, but let's figure it out" becomes a superpower. It gives your experts permission to share uncertainty. It models intellectual humility. And it keeps the focus on moving forward rather than defending ego. It's also an opportunity to let your experts shine.&lt;/p&gt;
      &lt;p&gt;Nothing is more annoying than a lead who needs to be the smartest person in every conversation. Your database expert spent years learning how to optimize queries - let them be the hero when performance issues arise. Your security specialist knows threat models better than you, give them the floor when discussing architecture decisions.&lt;/p&gt;
      &lt;p&gt;Make room for some productive discussion. When two experts disagree about implementation approaches, your job isn't to pick the "right" answer. It's to help frame the decision in terms of trade-offs, timeline, and user impact.&lt;/p&gt;
      &lt;p&gt;Your value isn't in having all the expertise. It's in recognizing which expertise is needed when, and creating space for the right people to contribute their best work. &lt;/p&gt;
      &lt;head rend="h3"&gt;The Translation Challenge&lt;/head&gt;
      &lt;p&gt;There was this fun blog post I read recently about how non-developers read tutorials written by developers. What sounds natural to you, can be complete gibberish to someone else. As a lead, you constantly need to think about your audience. You need to learn multiple languages to communicate the same thing:&lt;/p&gt;
      &lt;p&gt;Developer language: "The authentication service has a dependency on the user service, and if we don't implement proper circuit breakers, we'll have cascading failures during high load."&lt;/p&gt;
      &lt;p&gt;Product language: "If our login system goes down, it could take the entire app with it. We need to build in some safeguards, which will add about a week to the timeline but prevent potential outages."&lt;/p&gt;
      &lt;p&gt;Executive language: "We're prioritizing system reliability over feature velocity for this sprint. This reduces risk of user-facing downtime that could impact revenue."&lt;/p&gt;
      &lt;p&gt;All three statements describe the same technical decision, but each is crafted for its audience. Your experts shouldn't have to learn product speak, and your product team shouldn't need to understand circuit breaker patterns. But someone needs to bridge that gap.&lt;/p&gt;
      &lt;head rend="h2"&gt;Beyond "Because, that's why!"&lt;/head&gt;
      &lt;p&gt;"I'm the lead, and we are going to do it this way." That's probably the worst way to make a decision. That might work in the short term, but it erodes trust and kills the collaborative culture that makes expert teams thrive.&lt;/p&gt;
      &lt;p&gt;Instead, treat your teams like adults and communicate the reason behind your decision:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;"We're choosing the more conservative approach because the cost of being wrong is high, and we can iterate later."&lt;/item&gt;
        &lt;item&gt;"I know this feels like extra work, but it aligns with our architectural goals and will save us time on the next three features."&lt;/item&gt;
        &lt;item&gt;"This isn't the most elegant solution, but it's the one we can ship confidently within our timeline."&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;The more comfortable you become with not being the expert, the more effective you become as a leader.&lt;/p&gt;
      &lt;p&gt;When you stop trying to out-expert the experts, you can focus on what expert teams actually need:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Clear problem definitions&lt;/item&gt;
        &lt;item&gt;Context for decision-making&lt;/item&gt;
        &lt;item&gt;Translation between different perspectives&lt;/item&gt;
        &lt;item&gt;Protection from unnecessary complexity&lt;/item&gt;
        &lt;item&gt;Space to do their best work&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Your role isn't to have all the answers. It's to make sure the right questions get asked, the right people get heard, and the right decisions get made for the right reasons.&lt;/p&gt;
      &lt;p&gt;Technical leadership in expert environments is less about command and control, and more about connection and context. You're not the conductor trying to play every instrument. You're the one helping the orchestra understand what song they're playing together.&lt;/p&gt;
      &lt;p&gt;That's a much more interesting challenge than trying to be the smartest person in the room.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://idiallo.com/blog/how-to-lead-in-a-room-full-of-experts"/><published>2025-09-24T12:52:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45360475</id><title>Just let me select text</title><updated>2025-09-25T00:45:46.103824+00:00</updated><content>&lt;doc fingerprint="db02884631c8e70e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Just Let Me Select Text&lt;/head&gt;By Artyom Bologov&lt;head rend="h2"&gt;Untranslatable Bios #&lt;/head&gt;&lt;p&gt;I‚Äôm lonely. Like everyone-ish else. Naturally, I‚Äôm on Bumble. (Because Tinder is a rape-friendly lure trap.) When work calls get boring I inevitably start swiping (mostly left üò¢)&lt;/p&gt;&lt;p&gt;There are lots of tourists in Armenia in the summer. From all over the world really. Speaking a stupefying range of languages. With bios and prompt answers in these numerous languages. Not necessarily discernible to me due to my language learning stagnation.&lt;/p&gt;&lt;p&gt;So there‚Äôs this profile of a pretty German girl. With bio and prompts in (an undeniably beautiful) German. Speaking English, she made the decision to use her mother tongue for the bio. A totally valid choice.&lt;/p&gt;&lt;p&gt;So I want to know the story she tells with her profile:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Select her bio,&lt;/item&gt;&lt;item&gt;copy it,&lt;/item&gt;&lt;item&gt;paste into a translator,&lt;/item&gt;&lt;item&gt;look up the exact meaning of some mistranslated German word,&lt;/item&gt;&lt;item&gt;and realize the unexpected poetic meaning she put into these 300 chars.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Except‚Ä¶ I can‚Äôt do that. The text is not selectable/copyable in Bumble app. I have to do a bunch of relatively unsurmountable steps to do what should‚Äôve taken half a minute. Like screenshot the profile and scrape the text with iOS Photos text recognition. Or use some OCR (web)app elsewhere. It‚Äôs‚Ä¶ discouraging. Thus I give up and swipe left. A shame‚Äîshe was beautiful at the very least!&lt;/p&gt;&lt;head rend="h2"&gt;Media #&lt;/head&gt;&lt;p&gt;By making the text in your UI non-selectable, you turn it into‚Ä¶ an image essentially? Images, audio, video, and interactive JS-heavy pages are multidimentional media. Not really manipulable and referenceable in any reasonable way. (Not even with Media Fragments‚Äîthey were turned down by everyone.) You lose a whole dimension (ü•Å) of functionality and benefit by going with such media or their semblance text.&lt;/p&gt;&lt;p&gt; Podcasts are not easy to roll back to useful part. Video transcripts don‚Äôt make sense without the visuals. Web graphics are opaque &lt;code&gt;&amp;lt;canvas&amp;gt;&lt;/code&gt;-es you can‚Äôt gut.

&lt;/p&gt;&lt;p&gt;Text is copyable. Text is translatable. Text is accessible (as in a11y.) Text is lightweight. Text is fundamental to how we people process information.&lt;/p&gt;&lt;p&gt;That‚Äôs why we still use text in our UIs. We want to convey the meaning. We strive to provide unambiguous instructions. We need to be understood. So why make the text harder to process and understand?&lt;/p&gt;&lt;head rend="h2"&gt;Stop It #&lt;/head&gt;&lt;p&gt;Whenever you disable text selection/copying on your UI, you commit a crime against the user. Crime against comprehension. Crime against accessibility. Crime against the meaning. Stop incapacitating your users, allow them to finally use the text.&lt;/p&gt;&lt;head&gt;But why might one even do that?&lt;/head&gt;&lt;p&gt;Some reasons and their rebuttals:&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Copyright‚Äîpreventing content scraping&lt;/item&gt;&lt;item rend="dd-1"&gt;Most of the scraping is done by bots, not humans copy-pasting text. Disabling selection does nothing to these. It only breaks users‚Äô workflows and makes your site less friendly.&lt;/item&gt;&lt;item rend="dt-2"&gt;UI clickability/draggability&lt;/item&gt;&lt;item rend="dd-2"&gt;Some UIs are really interactive and need a click/drag to be unambiguous on some elements. The downside is that bug reports from users can‚Äôt reference the exact UI element that‚Äôs broken. Wording is important.&lt;/item&gt;&lt;item rend="dt-3"&gt;Mimicking for native/professional apps&lt;/item&gt;&lt;item rend="dd-3"&gt;So petty, going against the user in pursuit of some ephemeral image. Focus. On. The. User.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aartaka.me/select-text.html"/><published>2025-09-24T13:56:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361578</id><title>Engineering a fixed-width bit-packed integer vector in Rust</title><updated>2025-09-25T00:45:45.756146+00:00</updated><content>&lt;doc fingerprint="7ce9541545936309"&gt;
  &lt;main&gt;
    &lt;p&gt;If you‚Äôve ever worked with massive datasets, you know that memory usage can quickly become a bottleneck. While developing succinct data structures, I found myself needing to store large arrays of integers‚Äîvalues with no monotonicity or other exploitable patterns, that I knew came from a universe much smaller than their type‚Äôs theoretical capacity.&lt;/p&gt;
    &lt;p&gt;In this post, we will explore the engineering challenges involved in implementing an efficient vector-like data structure in Rust that stores integers in a compressed, bit-packed format. We will focus on achieving O(1) random access performance while minimizing memory usage. We will try to mimic the ergonomics of Rust‚Äôs standard &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; as closely as possible, including support for mutable access and zero-copy slicing.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All the code can be found on github: compressed-intvec&lt;/item&gt;
      &lt;item&gt;This is also published as a crate on crates.io: compressed-intvec&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Memory Waste in Standard Vectors&lt;/head&gt;
    &lt;p&gt;In Rust, the contract of a &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; (where &lt;code&gt;T&lt;/code&gt; is a primitive integer type like &lt;code&gt;u64&lt;/code&gt; or &lt;code&gt;i32&lt;/code&gt;) is simple: O(1) random access in exchange for a memory layout that is tied to the static size of &lt;code&gt;T&lt;/code&gt;. This is a good trade-off, until it isn‚Äôt. When the dynamic range of the stored values is significantly smaller than the type‚Äôs capacity, this memory layout leads to substantial waste.&lt;/p&gt;
    &lt;p&gt;Consider storing the value &lt;code&gt;5&lt;/code&gt; within a &lt;code&gt;Vec&amp;lt;u64&amp;gt;&lt;/code&gt;. Its 8-byte in-memory representation is:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000101&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Only 3 bits are necessary to represent the value, leaving 61 bits as zero-padding. The same principle applies, albeit less dramatically, when storing &lt;code&gt;5&lt;/code&gt; in a &lt;code&gt;u32&lt;/code&gt; or &lt;code&gt;u16&lt;/code&gt;. At scale, this overhead becomes prohibitive. A vector of one billion &lt;code&gt;u64&lt;/code&gt; elements consumes &lt;code&gt;10^9 * std::mem::size_of::&amp;lt;u64&amp;gt;()&lt;/code&gt;, or approximately 8 GB of memory, even if every element could fit within a single byte.&lt;/p&gt;
    &lt;p&gt;The canonical solution is bit packing, which aligns data end-to-end in a contiguous bitvector. However, this optimization has historically come at the cost of random access performance. The O(1) access guarantee of &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; is predicated on simple pointer arithmetic: &lt;code&gt;address = base_address + index * std::mem::size_of::&amp;lt;T&amp;gt;()&lt;/code&gt;. Tightly packing the bits invalidates this direct address calculation, seemingly forcing a trade-off between memory footprint and access latency.&lt;/p&gt;
    &lt;p&gt;This raises the central question that with this post we aim to answer: is it possible to design a data structure that decouples its memory layout from the static size of &lt;code&gt;T&lt;/code&gt;, adapting instead to the data‚Äôs true dynamic range, without sacrificing the O(1) random access that makes &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; so effective?&lt;/p&gt;
    &lt;head rend="h1"&gt;Packing and Accessing Bits&lt;/head&gt;
    &lt;p&gt;If the dynamic range of our data is known, we can define a fixed &lt;code&gt;bit_width&lt;/code&gt; for every integer. For instance, if the maximum value in a dataset is &lt;code&gt;1000&lt;/code&gt;, we know every number can be represented in 10 bits, since &lt;code&gt;2^10 = 1024&lt;/code&gt;. Instead of allocating 64 bits per element, we can store them back-to-back in a contiguous bitvector, forming the core of what from now on we‚Äôll refer as &lt;code&gt;FixedVec&lt;/code&gt;. We can immagine such data strucutre as a logical array of &lt;code&gt;N&lt;/code&gt; integers, each &lt;code&gt;bit_width&lt;/code&gt; bits wide, stored in a backing buffer of &lt;code&gt;u64&lt;/code&gt; words.&lt;/p&gt;
    &lt;code&gt;struct FixedVec {
    limbs: Vec&amp;lt;u64&amp;gt;, // Backing storage
    bit_width: usize, // Number of bits per element
    len: usize, // Number of elements
    mask: u64, // Precomputed mask for extraction
}&lt;/code&gt;
    &lt;p&gt;Where the role of the &lt;code&gt;mask&lt;/code&gt; field is to isolate the relevant bits during extraction. For a &lt;code&gt;bit_width&lt;/code&gt; of 10, the mask would be &lt;code&gt;0b1111111111&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This immediately solves the space problem, but how can we find the &lt;code&gt;i&lt;/code&gt;-th element in O(1) time if it doesn‚Äôt align to a byte boundary? The answer lies in simple arithmetic. The starting bit position of any element is a direct function of its index. Given a backing store of &lt;code&gt;u64&lt;/code&gt; words, we can locate any value by calculating its absolute bit position and then mapping that to a specific word and an offset within that word.&lt;/p&gt;
    &lt;code&gt;let bit_pos = index * bit_width; // Absolute bit position of the value
let word_index = bit_pos / 64; // Which u64 word to read
let bit_offset = bit_pos % 64; // Where the value starts in that word&lt;/code&gt;
    &lt;p&gt;With these values, the implementation of &lt;code&gt;get_unchecked&lt;/code&gt; becomes straightforward. It‚Äôs a two-step process: fetch the correct word from our backing &lt;code&gt;Vec&amp;lt;u64&amp;gt;&lt;/code&gt;, then use bitwise operations to isolate the specific bits we need.&lt;/p&gt;
    &lt;code&gt;// A simplified look at the core get_unchecked logic
unsafe fn get_unchecked(&amp;amp;self, index: usize) -&amp;gt; u64 {
    let bit_width = self.bit_width();
    let bit_pos = index * bit_width;
    let word_index = bit_pos / 64;
    let bit_offset = bit_pos % 64;

    // 1. Fetch the word from the backing store
    let word = *self.limbs.get_unchecked(word_index);

    // 2. Shift and mask to extract the value
    (word &amp;gt;&amp;gt; bit_offset) &amp;amp; self.mask
}&lt;/code&gt;
    &lt;p&gt;Let‚Äôs trace an access with &lt;code&gt;bit_width = 10&lt;/code&gt; for the element at &lt;code&gt;index = 7&lt;/code&gt;. The starting bit position is &lt;code&gt;7 * 10 = 70&lt;/code&gt;. This maps to &lt;code&gt;word_index = 1&lt;/code&gt; and &lt;code&gt;bit_offset = 6&lt;/code&gt;. Our 10-bit integer begins at the 6th bit of the second &lt;code&gt;u64&lt;/code&gt; word in our storage.&lt;/p&gt;
    &lt;p&gt;The right-shift &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; operation moves the bits of the entire &lt;code&gt;u64&lt;/code&gt; word to the right by &lt;code&gt;bit_offset&lt;/code&gt; positions. This aligns the start of our desired 10-bit value with the least significant bit (LSB) of the word. The final step is to isolate our value. A pre-calculated &lt;code&gt;mask&lt;/code&gt; (e.g., &lt;code&gt;0b1111111111&lt;/code&gt; for 10 bits) is applied with a bitwise AND &lt;code&gt;&amp;amp;&lt;/code&gt;. This zeroes out any high-order bits from the word, leaving just our target integer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Crossing Word Boundaries&lt;/head&gt;
    &lt;p&gt;The single-word access logic is fast, but it only works as long as &lt;code&gt;bit_offset + bit_width &amp;lt;= 64&lt;/code&gt;. This assumption breaks down as soon as an integer‚Äôs bit representation needs to cross the boundary from one &lt;code&gt;u64&lt;/code&gt; word into the next. This is guaranteed to happen for any &lt;code&gt;bit_width&lt;/code&gt; that is not a power of two. For example, with a 10-bit width, the element at &lt;code&gt;index = 6&lt;/code&gt; starts at bit position 60. Its 10 bits will occupy bits 60-63 of the first word and bits 0-5 of the second. The simple right-shift-and-mask trick fails here.&lt;/p&gt;
    &lt;p&gt;To correctly decode the value, we must read two consecutive &lt;code&gt;u64&lt;/code&gt; words and combine their bits. This splits our &lt;code&gt;get_unchecked&lt;/code&gt; implementation into two paths. The first is the fast path we‚Äôve already seen. The second is a new path for spanning values.&lt;/p&gt;
    &lt;p&gt;To get the lower bits of the value, we read the first word and shift right, just as before. This leaves the upper bits of the word as garbage.&lt;/p&gt;
    &lt;code&gt;let low_part = *limbs.get_unchecked(word_index) &amp;gt;&amp;gt; bit_offset;&lt;/code&gt;
    &lt;p&gt;To get the upper bits of the value, we read the next word. The bits we need are at the beginning of this word, so we shift them left to align them correctly.&lt;/p&gt;
    &lt;code&gt;let high_part = *limbs.get_unchecked(word_index + 1) &amp;lt;&amp;lt; (64 - bit_offset);&lt;/code&gt;
    &lt;p&gt;Finally, we combine the two parts with a bitwise OR and apply the mask to discard any remaining high-order bits from the &lt;code&gt;high_part&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;(low_part | high_part) &amp;amp; self.mask&lt;/code&gt;
    &lt;p&gt;The line &lt;code&gt;limbs.get_unchecked(word_index + 1)&lt;/code&gt; introduces a safety concern: if we are reading the last element of the vector, &lt;code&gt;word_index + 1&lt;/code&gt; could point past the end of our buffer, leading to undefined behavior. To prevent this, our builder must always allocate one extra padding word at the end of the storage.&lt;/p&gt;
    &lt;p&gt;Integrating these two paths gives us our final &lt;code&gt;get_unchecked&lt;/code&gt; implementation:&lt;/p&gt;
    &lt;code&gt;pub unsafe fn get_unchecked(&amp;amp;self, index: usize) -&amp;gt; u64 {
    let bit_width = self.bit_width;
    let bit_pos = index * bit_width;
    let word_index = bit_pos / 64;
    let bit_offset = bit_pos % 64;

    let limbs = self.bits.as_ref();

    if bit_offset + bit_width &amp;lt;= 64 {
        // Fast path: value is fully within one word
        (*limbs.get_unchecked(word_index) &amp;gt;&amp;gt; bit_offset) &amp;amp; self.mask
    } else {
        // Slow path: value spans two words.
        let low_part = *limbs.get_unchecked(word_index) &amp;gt;&amp;gt; bit_offset;
        let high_part = *limbs.get_unchecked(word_index + 1) &amp;lt;&amp;lt; (64 - bit_offset);
        (low_part | high_part) &amp;amp; self.mask
    }
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Faster Reads: Unaligned Access&lt;/head&gt;
    &lt;p&gt;Our &lt;code&gt;get_unchecked&lt;/code&gt; implementation is correct, but the slow path for spanning values requires two separate, aligned memory reads. The instruction sequence for this method involves at least two load instructions, multiple shifts, and a bitwise OR. These instructions have data dependencies: the shifts cannot execute until the loads complete, and the OR cannot execute until both shifts are done. This dependency chain can limit the CPU‚Äôs instruction-level parallelism and create pipeline stalls if the memory accesses miss the L1 cache.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs have a look at the machine code generated by this method. We can create a minimal binary with an &lt;code&gt;#[inline(never)]&lt;/code&gt; function that calls &lt;code&gt;get_unchecked&lt;/code&gt; on a known spanning index, then use &lt;code&gt;cargo asm&lt;/code&gt; to inspect the disassembly.&lt;/p&gt;
    &lt;code&gt;; asm_test::aligned::get_aligned (spanning path)
.LBB14_2:
        ; let low = *limbs.get_unchecked(word_index) &amp;gt;&amp;gt; bit_offset;
        mov rsi, qword ptr [r8 + 8*rdx]      ; &amp;lt;&amp;lt;&amp;lt; LOAD #1 (low_part)

        ; let high = *limbs.get_unchecked(word_index + 1) &amp;lt;&amp;lt; (64 - bit_offset);
        mov rdx, qword ptr [r8 + 8*rdx + 8]  ; &amp;lt;&amp;lt;&amp;lt; LOAD #2 (high_part)

        shr rsi, cl                          ; shift right of low_part
        shl rdx, cl                          ; shift left of high_part
        or rdx, rsi                          ; combine results&lt;/code&gt;
    &lt;p&gt;The instruction &lt;code&gt;mov rsi, qword ptr [r8 + 8*rdx]&lt;/code&gt; is our first memory access. It loads a 64-bit value (&lt;code&gt;qword&lt;/code&gt;) into the &lt;code&gt;rsi&lt;/code&gt; register. The address is calculated using base-plus-index addressing: &lt;code&gt;r8&lt;/code&gt; holds the base address of our &lt;code&gt;limbs&lt;/code&gt; buffer, and &lt;code&gt;rdx&lt;/code&gt; holds the &lt;code&gt;word_index&lt;/code&gt;. This corresponds directly to &lt;code&gt;limbs[word_index]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Immediately following is &lt;code&gt;mov rdx, qword ptr [r8 + 8*rdx + 8]&lt;/code&gt;. This is our second, distinct memory access. It loads the next 64-bit word from memory by adding an 8-byte offset to the previous address. This corresponds to &lt;code&gt;limbs[word_index + 1]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Only after both of these &lt;code&gt;mov&lt;/code&gt; instructions complete can the CPU proceed. The &lt;code&gt;shr rsi, cl&lt;/code&gt; instruction (shift right &lt;code&gt;rsi&lt;/code&gt; by the count in &lt;code&gt;cl&lt;/code&gt;) cannot execute until the first &lt;code&gt;mov&lt;/code&gt; has placed a value in &lt;code&gt;rsi&lt;/code&gt;. Similarly, &lt;code&gt;shl rdx, cl&lt;/code&gt; depends on the second &lt;code&gt;mov&lt;/code&gt;. Finally, &lt;code&gt;or rdx, rsi&lt;/code&gt; depends on both shifts.&lt;/p&gt;
    &lt;p&gt;This sequence of operations‚Äîloading two adjacent 64-bit words, shifting each, and combining them is a software implementation of what is, conceptually, a 128-bit barrel shifter. We are selecting a 64-bit window from a virtual 128-bit integer formed by concatenating the two words from memory.&lt;/p&gt;
    &lt;p&gt;Can we do better then this? Potentially, yes. We can replace this multi-instruction sequence with something more direct by delegating the complexity to the hardware and performing a single unaligned memory read. Modern x86-64 CPUs handle this directly: when an unaligned load instruction is issued, the CPU‚Äôs memory controller fetches the necessary cache lines and the load/store unit reassembles the bytes into the target register. This entire process is a single, optimized micro-operation.&lt;/p&gt;
    &lt;p&gt;We can try to implement a more aggressive access method. The strategy is to calculate the exact byte address where our data begins and perform a single, unaligned read of a full &lt;code&gt;W&lt;/code&gt; word from that position.&lt;/p&gt;
    &lt;p&gt;The implementation first translates the absolute bit position into a byte address and a residual bit offset within that byte.&lt;/p&gt;
    &lt;code&gt;let bit_pos = index * self.bit_width;
let byte_pos = bit_pos / 8;
let bit_rem = bit_pos % 8;&lt;/code&gt;
    &lt;p&gt;With the byte-level address, we get a raw &lt;code&gt;*const u8&lt;/code&gt; pointer to our storage and perform the unaligned read. The read_unaligned intrinsic in Rust compiles down to a single machine instruction that the hardware can execute efficiently.&lt;/p&gt;
    &lt;code&gt;let limbs_ptr = self.as_limbs().as_ptr() as *const u8;
// This read may cross hardware word boundaries, but the CPU handles it.
let word: W = (limbs_ptr.add(byte_pos) as *const W).read_unaligned();&lt;/code&gt;
    &lt;p&gt;On a Little-Endian system, the loaded &lt;code&gt;word&lt;/code&gt; now contains our target integer, but it‚Äôs shifted by &lt;code&gt;bit_rem&lt;/code&gt; positions. A simple right shift aligns our value to the LSB, and applying the mask isolates it.&lt;/p&gt;
    &lt;code&gt;let extracted_word = word &amp;gt;&amp;gt; bit_rem;
let final_value = extracted_word &amp;amp; self.mask;&lt;/code&gt;
    &lt;p&gt;This operation is safe only because, as said before, we are supposing that our builder guarantees a padding word at the end of the storage buffer. Combining all these steps, we get our final implementation:&lt;/p&gt;
    &lt;code&gt;pub unsafe fn get_unaligned_unchecked(&amp;amp;self, index: usize) -&amp;gt; u64 {
    let bit_pos = index * self.bit_width;
    let byte_pos = bit_pos / 8;
    let bit_rem = bit_pos % 8;

    let limbs_ptr = self.as_limbs().as_ptr() as *const u8;
    // SAFETY: The builder guarantees an extra padding word at the end.
    let word = (limbs_ptr.add(byte_pos) as *const W).read_unaligned();
    let extracted_word = word &amp;gt;&amp;gt; bit_rem;
    extracted_word &amp;amp; self.mask
}&lt;/code&gt;
    &lt;p&gt;Let‚Äôs have a look at the generated machine code for this new method when accessing an index that spans words:&lt;/p&gt;
    &lt;code&gt;; asm_test::unaligned::get_unaligned
.LBB15_1:
        ; let bit_pos = index * self.bit_width;
        imul rcx, rax
        ; let byte_pos = bit_pos / 8;
        mov rax, rcx
        shr rax, 3
        ; self.bits.as_ref()
        mov rdx, qword ptr [rdi + 8]
        ; unsafe { crate::intrinsics::copy_nonoverlapping(src, dst, count) }
        mov rax, qword ptr [rdx + rax]       ; &amp;lt;&amp;lt;&amp;lt; SINGLE UNALIGNED LOAD
        ; self &amp;gt;&amp;gt; other
        and cl, 7
        shr rax, cl
        ; fn bitand(self, rhs: $t) -&amp;gt; $t { self &amp;amp; rhs }
        and rax, qword ptr [rdi + 32]&lt;/code&gt;
    &lt;p&gt;The initial &lt;code&gt;imul&lt;/code&gt; and &lt;code&gt;shr rax, 3&lt;/code&gt; (a fast division by 8) correspond to the calculation of &lt;code&gt;byte_pos&lt;/code&gt;. The instruction &lt;code&gt;mov rdx, qword ptr [rdi + 8]&lt;/code&gt; loads the base address of our &lt;code&gt;limbs&lt;/code&gt; buffer into the &lt;code&gt;rdx&lt;/code&gt; register.&lt;/p&gt;
    &lt;p&gt;The instruction &lt;code&gt;mov rax, qword ptr [rdx + rax]&lt;/code&gt; is our single unaligned load. The address &lt;code&gt;[rdx + rax]&lt;/code&gt; is the sum of the buffer‚Äôs base address and our calculated &lt;code&gt;byte_pos&lt;/code&gt;. This &lt;code&gt;mov&lt;/code&gt; instruction reads 8 bytes (a &lt;code&gt;qword&lt;/code&gt;) directly from this potentially unaligned memory location into the &lt;code&gt;rax&lt;/code&gt; register. We can see that as we hoped, the read_unaligned intrinsic has been compiled down to a single hardware instruction.&lt;/p&gt;
    &lt;p&gt;The next instructions handle the extraction. The &lt;code&gt;and cl, 7&lt;/code&gt; and &lt;code&gt;shr rax, cl&lt;/code&gt; sequence corresponds to our &lt;code&gt;&amp;gt;&amp;gt; bit_rem&lt;/code&gt;. &lt;code&gt;cl&lt;/code&gt; holds the lower bits of the original &lt;code&gt;bit_pos&lt;/code&gt; (our &lt;code&gt;bit_rem&lt;/code&gt;), and the shift aligns our desired value to the LSB of the &lt;code&gt;rax&lt;/code&gt; register. Finally, &lt;code&gt;and rax, qword ptr [rdi + 32]&lt;/code&gt; applies the pre-calculated mask, which is stored at an offset from the &lt;code&gt;self&lt;/code&gt; pointer in &lt;code&gt;rdi&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Random Access Performance&lt;/head&gt;
    &lt;p&gt;We can now benchmark the latency of 1 million random access operations on a vector containing 10 million elements. For each &lt;code&gt;bit_width&lt;/code&gt;, we generate data with a uniform random distribution in the range &lt;code&gt;[0, 2^bit_width)&lt;/code&gt;. The code for the benchmark is available here: &lt;code&gt;bench-intvec&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Our baseline is the smallest standard &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; capable of holding the data (&lt;code&gt;Vec&amp;lt;u8&amp;gt;&lt;/code&gt; for &lt;code&gt;bit_width &amp;lt;= 8&lt;/code&gt;, etc.). We also include results from &lt;code&gt;sux&lt;/code&gt;, &lt;code&gt;succinct&lt;/code&gt;, and &lt;code&gt;simple-sds-sbwt&lt;/code&gt; for context. I am not aware of any other Rust crates that implement fixed-width bit-packed integer vectors, so if you know of any, please let me know!&lt;/p&gt;
    &lt;p&gt;We can see that for &lt;code&gt;bit_width&lt;/code&gt; values below 32, the &lt;code&gt;get_unaligned_unchecked&lt;/code&gt; of our &lt;code&gt;FixedVec&lt;/code&gt; is almost always faster than the corresponding &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; baseline. This is a result of improved cache locality. A 64-byte L1 cache line can hold 64 elements from a &lt;code&gt;Vec&amp;lt;u8&amp;gt;&lt;/code&gt;. With a &lt;code&gt;bit_width&lt;/code&gt; of 4, the same cache line holds &lt;code&gt;(64 * 8) / 4 = 128&lt;/code&gt; elements from our &lt;code&gt;FixedVec&lt;/code&gt;. This increased density improves the cache hit rate for random access patterns, and the latency reduction from avoiding DRAM access outweighs the instruction cost of the bitwise extraction. For values of &lt;code&gt;bit_width&lt;/code&gt; above 32, the performance of &lt;code&gt;FixedVec&lt;/code&gt; are very slightly worse than the &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; baseline, as the cache locality advantage diminishes. However, the memory savings remain.&lt;/p&gt;
    &lt;p&gt;The performance delta between &lt;code&gt;get_unaligned_unchecked&lt;/code&gt; and &lt;code&gt;get_unchecked&lt;/code&gt; confirms the unaligned access strategy discussed before: a single &lt;code&gt;read_unaligned&lt;/code&gt; instruction is more efficient than the two dependent aligned reads required by the logic for spanning words.&lt;/p&gt;
    &lt;p&gt;We can see that the implementation of &lt;code&gt;sux&lt;/code&gt; is almost on par with ours. The other two crates, &lt;code&gt;succinct&lt;/code&gt; and &lt;code&gt;simple-sds-sbwt&lt;/code&gt;, are significantly slower (note that the Y-axis is logarithmic). Tho, it‚Äôs worth noting that neither of these last two crates provides unchecked or unaligned access methods, so their implementations are inherently more conservative.&lt;/p&gt;
    &lt;head rend="h2"&gt;Iterating Over Values&lt;/head&gt;
    &lt;p&gt;The most common operation on any &lt;code&gt;Vec&lt;/code&gt;-like structure is, after all, a simple &lt;code&gt;for&lt;/code&gt; loop. The simplest way to implement &lt;code&gt;iter()&lt;/code&gt; would be to just wrap &lt;code&gt;get()&lt;/code&gt; in a loop:&lt;/p&gt;
    &lt;code&gt;// A naive, inefficient iterator
for i in 0..vec.len() {
    let value = vec.get(i);
    // ... do something with value
}&lt;/code&gt;
    &lt;p&gt;This works, but it‚Äôs terribly inefficient. Every single call to &lt;code&gt;get(i)&lt;/code&gt; independently recalculates the &lt;code&gt;word_index&lt;/code&gt; and &lt;code&gt;bit_offset&lt;/code&gt; from scratch. We‚Äôre throwing away valuable state, our current position in the bitstream, on every iteration, forcing the CPU to perform redundant multiplications and divisions.&lt;/p&gt;
    &lt;p&gt;We can think then about a stateful iterator. It should operate directly on the bitvector, maintaining its own position. Instead of thinking in terms of logical indices, it should think in terms of a ‚Äúbit window‚Äù, a local &lt;code&gt;u64&lt;/code&gt; register that holds the current chunk of bits being processed.&lt;/p&gt;
    &lt;p&gt;The idea is simple: the iterator loads one &lt;code&gt;u64&lt;/code&gt; word from the backing store into its window. It then satisfies &lt;code&gt;next()&lt;/code&gt; calls by decoding values directly from this in-register window. Only when the window runs out of bits does it need to go back to memory for the next &lt;code&gt;u64&lt;/code&gt; word. This amortizes the cost of memory access over many &lt;code&gt;next()&lt;/code&gt; calls.&lt;/p&gt;
    &lt;p&gt;For forward iteration, the state is minimal:&lt;/p&gt;
    &lt;code&gt;struct FixedVecIter&amp;lt;'a, ...&amp;gt; {
    // ...
    front_window: u64,
    front_bits_in_window: usize,
    front_word_index: usize,
    // ...
}&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;next()&lt;/code&gt; method first checks if the current &lt;code&gt;front_window&lt;/code&gt; has enough bits to satisfy the request. If &lt;code&gt;self.front_bits_in_window &amp;gt;= bit_width&lt;/code&gt;, it‚Äôs the fast path: a simple shift and mask on a register, which is incredibly fast.&lt;/p&gt;
    &lt;code&gt;// Inside next(), fast path:
if self.front_bits_in_window &amp;gt;= bit_width {
    let value = self.front_window &amp;amp; self.mask;
    self.front_window &amp;gt;&amp;gt;= bit_width;
    self.front_bits_in_window -= bit_width;
    return Some(value);
}&lt;/code&gt;
    &lt;p&gt;If the window is running low on bits, we hit the slower path. The next value spans the boundary between our current window and the next &lt;code&gt;u64&lt;/code&gt; word in memory. We must read the next word, combine its bits with the remaining bits in our current window, and then extract the value. This is the same logic as the spanning-word &lt;code&gt;get()&lt;/code&gt;, but it‚Äôs performed incrementally.&lt;/p&gt;
    &lt;head rend="h3"&gt;Double-Ended Iteration&lt;/head&gt;
    &lt;p&gt;But I want my iterator to be bidirectional! Well, then we need to ensure it implements &lt;code&gt;DoubleEndedIterator&lt;/code&gt; and supports &lt;code&gt;next_back()&lt;/code&gt;. This throws a wrench in our simple stateful model. A single window and cursor can only move in one direction.&lt;/p&gt;
    &lt;p&gt;The solution is to maintain two independent sets of state: one for the front and one for the back. The &lt;code&gt;FixedVecIter&lt;/code&gt; needs to track two windows, two bit counters, and two word indices.&lt;/p&gt;
    &lt;code&gt;struct FixedVecIter&amp;lt;'a, ...&amp;gt; {
    // ...
    front_index: usize,
    back_index: usize,

    // State for forward iteration
    front_window: u64,
    front_bits_in_window: usize,
    front_word_index: usize,

    // State for backward iteration
    back_window: u64,
    back_bits_in_window: usize,
    back_word_index: usize,
    // ...
}&lt;/code&gt;
    &lt;p&gt;Initializing the front is easy: we load &lt;code&gt;limbs[0]&lt;/code&gt; into &lt;code&gt;front_window&lt;/code&gt;. The back is more complex. We must calculate the exact word index and the number of valid bits in the last word that contains data. This requires a bit of arithmetic to handle cases where the data doesn‚Äôt perfectly fill the final word.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;next()&lt;/code&gt; method consumes from the &lt;code&gt;front_window&lt;/code&gt;, advancing the front state. The &lt;code&gt;next_back()&lt;/code&gt; method consumes from the &lt;code&gt;back_window&lt;/code&gt;, advancing the back state. The iterator is exhausted when &lt;code&gt;front_index&lt;/code&gt; meets &lt;code&gt;back_index&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The full implementation can be found in the iter module of the library&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Writing bits&lt;/head&gt;
    &lt;p&gt;We have solved the read problem, but we may also want to modify values in place. A method like &lt;code&gt;set(index, value)&lt;/code&gt; seems simple, but it opens up the same can of worms as &lt;code&gt;get&lt;/code&gt;, just in reverse. We can‚Äôt just write the new value; we have to do so without clobbering the adjacent, unrelated data packed into the same &lt;code&gt;u64&lt;/code&gt; word.&lt;/p&gt;
    &lt;p&gt;Just like with reading, the logic splits into two paths. The ‚Äúfast path‚Äù handles values that are fully contained within a single &lt;code&gt;u64&lt;/code&gt;. Here, we can‚Äôt just overwrite the word. We first need to clear out the bits for the element we‚Äôre replacing and then merge in the new value.&lt;/p&gt;
    &lt;head rend="h2"&gt;In-Word Write&lt;/head&gt;
    &lt;p&gt;Our goal here is to update a &lt;code&gt;bit_width&lt;/code&gt;-sized slice of a &lt;code&gt;u64&lt;/code&gt; word while leaving the other bits untouched. This operation must be a read-modify-write sequence to avoid corrupting adjacent elements. The most efficient way to implement this is to load the entire word into a register, perform all bitwise modifications locally, and then write the final result back to memory in a single store operation.&lt;/p&gt;
    &lt;p&gt;First, we load the word from our backing &lt;code&gt;limbs&lt;/code&gt; slice.&lt;/p&gt;
    &lt;code&gt;let mut word = *limbs.get_unchecked(word_index);&lt;/code&gt;
    &lt;p&gt;Next, we need to create a ‚Äúhole‚Äù in our local copy where the new value will go. We do this by creating a mask that has ones only in the bit positions we want to modify, and then inverting it to create a clearing mask.&lt;/p&gt;
    &lt;code&gt;// For a value at bit_offset, the mask must also be shifted.
let clear_mask = !(self.mask &amp;lt;&amp;lt; bit_offset);
// Applying this mask zeroes out the target bits in our register copy.
word &amp;amp;= clear_mask;&lt;/code&gt;
    &lt;p&gt;With the target bits zeroed, we can merge our new value. The value is first shifted left by &lt;code&gt;bit_offset&lt;/code&gt; to align it correctly within the 64-bit word. Then, a bitwise OR merges it into the ‚Äúhole‚Äù we just created.&lt;/p&gt;
    &lt;code&gt;// Shift the new value into position and merge it.
word |= value_w &amp;lt;&amp;lt; bit_offset;&lt;/code&gt;
    &lt;p&gt;Finally, with the modifications complete, we write the updated word from the register back to memory in a single operation.&lt;/p&gt;
    &lt;code&gt;*limbs.get_unchecked_mut(word_index) = word;&lt;/code&gt;
    &lt;p&gt;This entire sequence of one read, two bitwise operations in-register, one write is the canonical and most efficient way to perform a sub-word update.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spanning Write&lt;/head&gt;
    &lt;p&gt;Now for the hard part: writing a value that crosses a word boundary. This operation must modify two separate &lt;code&gt;u64&lt;/code&gt; words in our backing store. It‚Äôs the inverse of the spanning read. We need to split our &lt;code&gt;value_w&lt;/code&gt; into a low part and a high part and write each to the correct word, minimizing memory accesses.&lt;/p&gt;
    &lt;p&gt;To operate on two distinct memory locations, &lt;code&gt;limbs[word_index]&lt;/code&gt; and &lt;code&gt;limbs[word_index + 1]&lt;/code&gt;, we first need mutable access to both. In a safe, hot path like this, we can use &lt;code&gt;split_at_mut_unchecked&lt;/code&gt; to bypass Rust‚Äôs borrow checker bounds checks, as we have already guaranteed through our logic that both indices are valid.&lt;/p&gt;
    &lt;code&gt;// SAFETY: We know word_index and word_index + 1 are valid.
let (left, right) = limbs.split_at_mut_unchecked(word_index + 1);&lt;/code&gt;
    &lt;p&gt;Our strategy is to read both words into registers, perform all bitwise logic locally, and then write both modified words back to memory. This minimizes the time we hold mutable references and can improve performance.&lt;/p&gt;
    &lt;p&gt;First, we handle the &lt;code&gt;low_word&lt;/code&gt;. We need to replace its high bits (from &lt;code&gt;bit_offset&lt;/code&gt; onwards) with the low bits of our new value. The most direct way is to create a mask for the bits we want to keep. The expression &lt;code&gt;(1 &amp;lt;&amp;lt; bit_offset) - 1&lt;/code&gt; is a bit-twiddling trick to generate a mask with &lt;code&gt;bit_offset&lt;/code&gt; ones at the least significant end.&lt;/p&gt;
    &lt;code&gt;let mut low_word_val = *left.get_unchecked(word_index);

// Create a mask to preserve the low `bit_offset` bits of the word.
let low_mask = (1u64 &amp;lt;&amp;lt; bit_offset).wrapping_sub(1);
low_word_val &amp;amp;= low_mask;&lt;/code&gt;
    &lt;p&gt;With the target bits zeroed out, we merge in the low part of our new value. A left shift aligns it correctly, and the high bits of &lt;code&gt;value_w&lt;/code&gt; are naturally shifted out of the register.&lt;/p&gt;
    &lt;code&gt;// Merge in the low part of our new value.
low_word_val |= value_w &amp;lt;&amp;lt; bit_offset;
*left.get_unchecked_mut(word_index) = low_word_val;&lt;/code&gt;
    &lt;p&gt;Next, we handle the &lt;code&gt;high_word&lt;/code&gt; in a symmetrical fashion. We need to write the remaining high bits of &lt;code&gt;value_w&lt;/code&gt; into the low-order bits of this second word. First, we calculate how many bits of our value actually belong in the first word:&lt;/p&gt;
    &lt;code&gt;let remaining_bits_in_first_word = 64 - bit_offset;&lt;/code&gt;
    &lt;p&gt;Now, we read the second word and create a mask to clear the low-order bits where our data will be written. With the operation &lt;code&gt;self.mask &amp;gt;&amp;gt; remaining_bits_in_first_word&lt;/code&gt; we can determine how many bits of our value spill into the second word, creating a mask for them. Inverting this gives us a mask to preserve the existing high-order bits of the &lt;code&gt;high_word&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;let mut high_word_val = *right.get_unchecked(0);

// Clear the low bits of the second word that will be overwritten.
high_word_val &amp;amp;= !(self.mask &amp;gt;&amp;gt; remaining_bits_in_first_word);&lt;/code&gt;
    &lt;p&gt;Finally, we isolate the high part of &lt;code&gt;value_w&lt;/code&gt; by right-shifting it by the number of bits we already wrote, and merge it into the cleared space.&lt;/p&gt;
    &lt;code&gt;// Merge in the high part of our new value.
high_word_val |= value_w &amp;gt;&amp;gt; remaining_bits_in_first_word;
*right.get_unchecked_mut(0) = high_word_val;&lt;/code&gt;
    &lt;head rend="h2"&gt;Random Write Performance&lt;/head&gt;
    &lt;p&gt;As with reads, we can benchmark the latency of 1 million random write operations on a vector containing 10 million elements. The code for the benchmark is available here: [&lt;code&gt;bench-intvec-writes&lt;/code&gt;].&lt;/p&gt;
    &lt;p&gt;Here, the &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; baseline is the clear winner across almost all bit-widths. This isn‚Äôt surprising. A &lt;code&gt;set&lt;/code&gt; operation in a &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; compiles down to a single &lt;code&gt;MOV&lt;/code&gt; instruction with a simple addressing mode (&lt;code&gt;[base + index * element_size]&lt;/code&gt;). It‚Äôs about as fast as the hardware allows.&lt;/p&gt;
    &lt;p&gt;As for the reads, the performance of our &lt;code&gt;FixedVec&lt;/code&gt; is almost identical to that of &lt;code&gt;sux&lt;/code&gt;. The other two crates, &lt;code&gt;succinct&lt;/code&gt; and &lt;code&gt;simple-sds-sbwt&lt;/code&gt;, are again slower. It‚Äôs worth noting that also for the writes, neither of these last two crates provides unchecked methods.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;For the 64-bit width case, I honestly have no idea what is going on with&lt;/p&gt;&lt;code&gt;sux&lt;/code&gt;being so much faster than everything else, even then&lt;code&gt;Vec&amp;lt;u64&amp;gt;&lt;/code&gt;! Mybe some weird compiler optimization? If you have any insight, please let me know.&lt;/quote&gt;
    &lt;head rend="h1"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;With the access patterns defined, we need to think about the overall architecture of this data structure. A solution hardcoded to &lt;code&gt;u64&lt;/code&gt; would lack the flexibility to adapt to different use cases. We need a structure that is generic over the its principal components: the logical type, the physical storage type, the bit-level ordering, and ownership. We can define a struct that is generic over these four parameters:&lt;/p&gt;
    &lt;code&gt;pub struct FixedVec&amp;lt;T: Storable&amp;lt;W&amp;gt;, W: Word, E: Endianness, B: AsRef&amp;lt;[W]&amp;gt; = Vec&amp;lt;W&amp;gt;&amp;gt; {
    bits: B,
    bit_width: usize,
    mask: W,
    len: usize,
}&lt;/code&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;T&lt;/code&gt; is the logical element type, the type as seen by the user of the API (e.g., &lt;code&gt;i16&lt;/code&gt;, &lt;code&gt;u32&lt;/code&gt;). By abstracting &lt;code&gt;T&lt;/code&gt;, we divide the user-facing type from the internal storage representation.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;W&lt;/code&gt; is the physical storage word, which must implement our &lt;code&gt;Word&lt;/code&gt; trait. It defines the primitive unsigned integer (&lt;code&gt;u32&lt;/code&gt;, &lt;code&gt;u64&lt;/code&gt;, &lt;code&gt;usize&lt;/code&gt;) of the backing buffer and sets the granularity for all bitwise operations.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;E&lt;/code&gt; requires the &lt;code&gt;dsi-bitstream::Endianness&lt;/code&gt; trait, allowing us to specify either Little-Endian (&lt;code&gt;LE&lt;/code&gt;) or Big-Endian (&lt;code&gt;BE&lt;/code&gt;) byte order.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;B&lt;/code&gt;, which must implement &lt;code&gt;AsRef&amp;lt;[W]&amp;gt;&lt;/code&gt;, represents the backing storage. This abstraction over ownership allows &lt;code&gt;FixedVec&lt;/code&gt; to be either an owned container where &lt;code&gt;B = Vec&amp;lt;W&amp;gt;&lt;/code&gt;, or a zero-copy, borrowed view where &lt;code&gt;B = &amp;amp;[W]&lt;/code&gt;. This makes it possible to, for example, construct a &lt;code&gt;FixedVec&lt;/code&gt; directly over a memory-mapped slice without any heap allocation.&lt;/p&gt;
    &lt;p&gt;In this way, the compiler monomorphizes the struct and its methods for each concrete instantiation (e.g., &lt;code&gt;FixedVec&amp;lt;i16, u64, LE, Vec&amp;lt;u64&amp;gt;&amp;gt;&lt;/code&gt;), resulting in specialized code with no runtime overhead from the generic abstractions.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;Word&lt;/code&gt; Trait: The Physical Storage Layer&lt;/head&gt;
    &lt;p&gt;The first step is to abstract the physical storage layer. The &lt;code&gt;W&lt;/code&gt; parameter must be a primitive unsigned integer type that supports bitwise operations. We can define a &lt;code&gt;Word&lt;/code&gt; trait that captures these requirements:&lt;/p&gt;
    &lt;code&gt;pub trait Word:
    UnsignedInt + Bounded + ToPrimitive + dsi_bitstream::traits::Word
    + NumCast + Copy + Send + Sync + Debug + IntoAtomic + 'static
{
    const BITS: usize = std::mem::size_of::&amp;lt;Self&amp;gt;() * 8;
}&lt;/code&gt;
    &lt;p&gt;The numeric traits (&lt;code&gt;UnsignedInt&lt;/code&gt;, &lt;code&gt;Bounded&lt;/code&gt;, &lt;code&gt;NumCast&lt;/code&gt;, &lt;code&gt;ToPrimitive&lt;/code&gt;) are necessary for the arithmetic of offset and mask calculations. The &lt;code&gt;dsi_bitstream::traits::Word&lt;/code&gt; bound allows us to integrate with its &lt;code&gt;BitReader&lt;/code&gt; and &lt;code&gt;BitWriter&lt;/code&gt; implementations, offloading the bitstream logic. &lt;code&gt;Send&lt;/code&gt; and &lt;code&gt;Sync&lt;/code&gt; are non-negotiable requirements for any data structure that might be used in a concurrent context. The &lt;code&gt;IntoAtomic&lt;/code&gt; bound is particularly forward-looking: it establishes a compile-time link between a storage word like &lt;code&gt;u64&lt;/code&gt; and its atomic counterpart, &lt;code&gt;AtomicU64&lt;/code&gt;. We will use it later to build a thread safe, atomic version of &lt;code&gt;FixedVec&lt;/code&gt;. Finally, the &lt;code&gt;const BITS&lt;/code&gt; associated constant lets us write architecture-agnostic code that correctly adapts to &lt;code&gt;u32&lt;/code&gt;, &lt;code&gt;u64&lt;/code&gt;, or &lt;code&gt;usize&lt;/code&gt; words without &lt;code&gt;cfg&lt;/code&gt; flags.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;Storable&lt;/code&gt; Trait: The Logical Type Layer&lt;/head&gt;
    &lt;p&gt;With the physical storage layer defined, we need a formal contract to connect it to the user‚Äôs logical type &lt;code&gt;T&lt;/code&gt;. We can do this by creating the &lt;code&gt;Storable&lt;/code&gt; trait, which defines a bidirectional, lossless conversion.&lt;/p&gt;
    &lt;code&gt;pub trait Storable&amp;lt;W: Word&amp;gt;: Sized + Copy {
    fn into_word(self) -&amp;gt; W;
    fn from_word(word: W) -&amp;gt; Self;
}&lt;/code&gt;
    &lt;p&gt;For unsigned types, the implementation is a direct cast. For signed types, however, we must map the &lt;code&gt;iN&lt;/code&gt; domain to the &lt;code&gt;uN&lt;/code&gt; domain required for bit-packing. A simple two‚Äôs complement bitcast is unsuitable, as &lt;code&gt;i64(-1)&lt;/code&gt; would become &lt;code&gt;u64::MAX&lt;/code&gt;, a value requiring the maximum number of bits. We need a better mapping that preserves small absolute values.&lt;/p&gt;
    &lt;p&gt;We can use ZigZag encoding, which maps integers with small absolute values to small unsigned integers. This is implemented via the &lt;code&gt;ToNat&lt;/code&gt; and &lt;code&gt;ToInt&lt;/code&gt; traits from &lt;code&gt;dsi-bitstream&lt;/code&gt;. The core encoding logic in &lt;code&gt;to_nat&lt;/code&gt; is:&lt;/p&gt;
    &lt;code&gt;// From dsi_bitstream::traits::ToNat
fn to_nat(self) -&amp;gt; Self::UnsignedInt {
    (self &amp;lt;&amp;lt; 1).to_unsigned() ^ (self &amp;gt;&amp;gt; (Self::BITS - 1)).to_unsigned()
}&lt;/code&gt;
    &lt;p&gt;This operation works as follows: &lt;code&gt;(self &amp;lt;&amp;lt; 1)&lt;/code&gt; creates a space at the LSB. The term &lt;code&gt;(self &amp;gt;&amp;gt; (Self::BITS - 1))&lt;/code&gt; is an arithmetic right shift, which generates a sign mask‚Äîall zeros for non-negative numbers, all ones for negative numbers. The final XOR uses this mask to interleave positive and negative integers: 0 becomes 0, -1 becomes 1, 1 becomes 2, -2 becomes 3, and so on.&lt;/p&gt;
    &lt;p&gt;The decoding reverses this transformation:&lt;/p&gt;
    &lt;code&gt;// From dsi_bitstream::traits::ToInt
fn to_int(self) -&amp;gt; Self::SignedInt {
    (self &amp;gt;&amp;gt; 1).to_signed() ^ (-(self &amp;amp; 1).to_signed())
}&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;(self &amp;gt;&amp;gt; 1)&lt;/code&gt; shifts the value back. The term &lt;code&gt;-(self &amp;amp; 1)&lt;/code&gt; creates a mask from the LSB (the original sign bit). In two‚Äôs complement, this becomes &lt;code&gt;0&lt;/code&gt; for even numbers (originally positive) and &lt;code&gt;-1&lt;/code&gt; (all ones) for odd numbers (originally negative). The final XOR with this mask correctly restores the original two‚Äôs complement representation.&lt;/p&gt;
    &lt;p&gt;We might ask ourselves: why not just a direct bitcast for signed types, perhaps via a &lt;code&gt;uN -&amp;gt; iN&lt;/code&gt; chain. Well, while a direct transmute between same-sized integer types is a no-op, the approach fails when the logical &lt;code&gt;bit_width&lt;/code&gt; is smaller than the physical type size. &lt;code&gt;FixedVec&lt;/code&gt;‚Äôs core logic extracts a &lt;code&gt;bit_width&lt;/code&gt;-sized unsigned integer from its storage. For example, when reading a 4-bit representation of &lt;code&gt;-1&lt;/code&gt; (binary &lt;code&gt;1111&lt;/code&gt;), the &lt;code&gt;from_word&lt;/code&gt; function receives the value &lt;code&gt;15u64&lt;/code&gt;. At this point, the context that those four bits represented a negative number is lost. A cast chain like &lt;code&gt;15u64 as u8 as i8&lt;/code&gt; would simply yield &lt;code&gt;15i8&lt;/code&gt;, not &lt;code&gt;-1i8&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To correctly reconstruct the signed value, one would need to manually perform sign extension based on the known &lt;code&gt;bit_width&lt;/code&gt;. This involves checking the most significant bit of the extracted value and, if set, filling the higher-order bits of the word with ones. This logic requires a conditional branch, which can introduce pipeline stalls and degrade performance in tight loops. ZigZag decoding, by contrast, is a purely arithmetic, branch-free transformation. Its reconstruction logic is a simple sequence of bitwise shifts and XORs, making it a faster and more consistent choice for the hot path of data access.&lt;/p&gt;
    &lt;p&gt;With this logic within the trait system, the main &lt;code&gt;FixedVec&lt;/code&gt; implementation remains clean and agnostic to the signedness of the data it stores.&lt;/p&gt;
    &lt;head rend="h2"&gt;Builder Pattern&lt;/head&gt;
    &lt;p&gt;Once the structure logic is in place, we have to design an ergonomic way to construct it. A simple &lt;code&gt;new()&lt;/code&gt; function isn‚Äôt sufficient because the vector‚Äôs memory layout depends on parameters that must be determined before allocation, most critically the &lt;code&gt;bit_width&lt;/code&gt;. This is a classic scenario for a builder pattern.&lt;/p&gt;
    &lt;p&gt;The central problem is that the optimal &lt;code&gt;bit_width&lt;/code&gt; often depends on the data itself. We need a mechanism to specify the strategy for determining this width. We can create the &lt;code&gt;BitWidth&lt;/code&gt; enum:&lt;/p&gt;
    &lt;code&gt;pub enum BitWidth {
    Minimal,
    PowerOfTwo,
    Explicit(usize),
}&lt;/code&gt;
    &lt;p&gt;With this enum, the user can choose between three strategies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Minimal&lt;/code&gt;: The builder scans the input data to find the maximum value, then calculates the minimum number of bits required to represent it. This is the most space-efficient option but requires a full pass over the data.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PowerOfTwo&lt;/code&gt;: Similar to&lt;code&gt;Minimal&lt;/code&gt;, but rounds the bit width up to the next power of two. This can simplify certain bitwise operations and align better with hardware word sizes, at the cost of some additional space.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Explicit(n)&lt;/code&gt;: The user provides a fixed bit width. This avoids the data scan but requires the user to ensure that all values fit within the specified width.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Note: Yes, I could have also made three different build functions:&lt;/p&gt;&lt;code&gt;new_with_minimal_bit_width()&lt;/code&gt;,&lt;code&gt;new_with_power_of_two_bit_width()&lt;/code&gt;, and&lt;code&gt;new_with_explicit_bit_width(n)&lt;/code&gt;. However, this would lead to a combinatorial explosion if we later wanted to add more configuration options. The builder pattern scales better.&lt;/quote&gt;
    &lt;p&gt;With this, the &lt;code&gt;FixedVecBuilder&lt;/code&gt; could be designed as a state machine. It holds the chosen &lt;code&gt;BitWidth&lt;/code&gt; strategy. The final &lt;code&gt;build()&lt;/code&gt; method takes the input slice and executes the appropriate logic.&lt;/p&gt;
    &lt;code&gt;// A look at the builder's logic flow
pub fn build(self, input: &amp;amp;[T]) -&amp;gt; Result&amp;lt;FixedVec&amp;lt;...&amp;gt;, Error&amp;gt; {

    let final_bit_width = match self.bit_width_strategy {
        BitWidth::Explicit(n) =&amp;gt; n,
        _ =&amp;gt; {
            // For Minimal or PowerOfTwo, we first find the max value.
            let max_val = input.iter().map(|v| v.into_word()).max().unwrap_or(0);
            let min_bits = (64 - max_val.leading_zeros()).max(1) as usize;

            match self.bit_width_strategy {
                BitWidth::Minimal =&amp;gt; min_bits,
                BitWidth::PowerOfTwo =&amp;gt; min_bits.next_power_of_two(),
                _ =&amp;gt; unreachable!(),
            }
        }
    };

    // ... (rest of the logic: allocate buffer, write data) ...
}&lt;/code&gt;
    &lt;p&gt;This design cleanly separates the configuration phase from the execution phase. The user can declaratively state their requirements, and the builder handles the implementation details, whether that involves a full data scan or a direct construction. For example:&lt;/p&gt;
    &lt;code&gt;use compressed_intvec::prelude::*;

let data: &amp;amp;[u32] = &amp;amp;[100, 200, 500]; // Max value 500 requires 9 bits

// The builder will scan the data, find max=500, calculate min_bits=9,
// and then round up to the next power of two.
let vec_pow2: UFixedVec&amp;lt;u32&amp;gt; = FixedVec::builder()
    .bit_width(BitWidth::PowerOfTwo)
    .build(data)
    .unwrap();

assert_eq!(vec_pow2.bit_width(), 16);&lt;/code&gt;
    &lt;quote&gt;&lt;code&gt;UFixedVec&amp;lt;T&amp;gt;&lt;/code&gt;is a type alias for&lt;code&gt;FixedVec&amp;lt;T, u64, LE, Vec&amp;lt;u64&amp;gt;&amp;gt;&lt;/code&gt;, the most common instantiation.&lt;/quote&gt;
    &lt;head rend="h1"&gt;Doing more than just reading&lt;/head&gt;
    &lt;p&gt;The design of &lt;code&gt;FixedVec&lt;/code&gt; allows for more than just efficient reads. We can extend it to support mutation and even thread-safe (almost atomic) concurrent access.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mutability: Proxy Objects&lt;/head&gt;
    &lt;p&gt;A core feature of &lt;code&gt;std::vec::Vec&lt;/code&gt; is mutable, index-based access via &lt;code&gt;&amp;amp;mut T&lt;/code&gt;. This is fundamentally impossible for &lt;code&gt;FixedVec&lt;/code&gt;. An element, such as a 10-bit integer, is not a discrete, byte-aligned entity in memory. It is a virtual value extracted from a bitstream, potentially spanning the boundary of two different &lt;code&gt;u64&lt;/code&gt; words. It has no stable memory address, so a direct mutable reference cannot be formed.&lt;/p&gt;
    &lt;p&gt;To provide an ergonomic mutable API, we must emulate the behavior of a mutable reference. We achieve this through a proxy object pattern, implemented in a struct named &lt;code&gt;MutProxy&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;pub struct MutProxy&amp;lt;'a, T, W, E, B&amp;gt;
where
    T: Storable&amp;lt;W&amp;gt;,
    W: Word,
    E: Endianness,
    B: AsRef&amp;lt;[W]&amp;gt; + AsMut&amp;lt;[W]&amp;gt;,
{
    vec: &amp;amp;'a mut FixedVec&amp;lt;T, W, E, B&amp;gt;,
    index: usize,
    value: T, // A temporary, decoded copy of the element's value.
}&lt;/code&gt;
    &lt;p&gt;When we call a method like &lt;code&gt;at_mut(index)&lt;/code&gt;, it does not return a reference. Instead, it constructs and returns a &lt;code&gt;MutProxy&lt;/code&gt; instance. The proxy‚Äôs lifecycle manages the entire modification process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Construction: The proxy is created. Its first action is to call the parent &lt;code&gt;FixedVec&lt;/code&gt;‚Äôs internal&lt;code&gt;get&lt;/code&gt;logic to read and decode the value at the specified&lt;code&gt;index&lt;/code&gt;. This decoded value is stored as a temporary copy inside the proxy object itself.&lt;/item&gt;
      &lt;item&gt;Modification: The &lt;code&gt;MutProxy&lt;/code&gt;implements&lt;code&gt;Deref&lt;/code&gt;and&lt;code&gt;DerefMut&lt;/code&gt;, allowing the user to interact with the temporary copy as if it were the real value. Any modifications (&lt;code&gt;*proxy = new_value&lt;/code&gt;,&lt;code&gt;*proxy += 1&lt;/code&gt;) are applied to this local copy, not to the underlying bitstream.&lt;/item&gt;
      &lt;item&gt;Destruction: When the &lt;code&gt;MutProxy&lt;/code&gt;goes out of scope, its&lt;code&gt;Drop&lt;/code&gt;implementation is executed. This is the critical step where the potentially modified value from the temporary copy is taken, re-encoded, and written back into the correct bit position in the parent&lt;code&gt;FixedVec&lt;/code&gt;‚Äôs storage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a classic copy-on-read, write-on-drop mechanism. It provides a safe and ergonomic abstraction for mutating non-addressable data, preserving the feel of direct manipulation while correctly handling the bit-level operations under the hood. The overhead is a single read at the start of the proxy‚Äôs life and a single write at the end.&lt;/p&gt;
    &lt;code&gt;use compressed_intvec::fixed::{FixedVec, UFixedVec, BitWidth};

let data: &amp;amp;[u32] = &amp;amp;[10, 20, 30];
let mut vec: UFixedVec&amp;lt;u32&amp;gt; = FixedVec::builder()
    .bit_width(BitWidth::Explicit(7))
    .build(data)
    .unwrap();

// vec.at_mut(1) returns an Option&amp;lt;MutProxy&amp;lt;...&amp;gt;&amp;gt;
if let Some(mut proxy) = vec.at_mut(1) {
    // The DerefMut trait allows us to modify the proxy's internal copy.
    *proxy = 99;
} // The proxy is dropped here. Its Drop impl writes 99 back to the vec.

assert_eq!(vec.get(1), Some(99));&lt;/code&gt;
    &lt;p&gt;The overhead of this approach is a single read on the proxy‚Äôs construction and a single write on its destruction, which is an acceptable trade-off for an ergonomic and safe mutable API.&lt;/p&gt;
    &lt;head rend="h3"&gt;Zero-Copy Views&lt;/head&gt;
    &lt;p&gt;A &lt;code&gt;Vec&lt;/code&gt;-like API needs to support slicing. Creating a &lt;code&gt;FixedVec&lt;/code&gt; that borrows its data (&lt;code&gt;B = &amp;amp;[W]&lt;/code&gt;) is the first step for this, but we also need a dedicated slice type to represent a sub-region of another &lt;code&gt;FixedVec&lt;/code&gt; without copying data. For this we can create &lt;code&gt;FixedVecSlice&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We can implement this as a classic ‚Äúfat pointer‚Äù struct. It holds a reference to the parent &lt;code&gt;FixedVec&lt;/code&gt; and a &lt;code&gt;Range&amp;lt;usize&amp;gt;&lt;/code&gt; that defines the logical boundaries of the view.&lt;/p&gt;
    &lt;code&gt;// A zero-copy view into a contiguous portion of a FixedVec.
#[derive(Debug)]
pub struct FixedVecSlice&amp;lt;V&amp;gt; {
    pub(super) parent: V,
    pub(super) range: Range&amp;lt;usize&amp;gt;,
}&lt;/code&gt;
    &lt;p&gt;The generic parameter &lt;code&gt;V&lt;/code&gt; is a reference to the parent vector. This allows the same &lt;code&gt;FixedVecSlice&lt;/code&gt; struct to represent both immutable (&lt;code&gt;V = &amp;amp;FixedVec&amp;lt;...&amp;gt;&lt;/code&gt;) and mutable (&lt;code&gt;V = &amp;amp;mut FixedVec&amp;lt;...&amp;gt;&lt;/code&gt;) views.&lt;/p&gt;
    &lt;p&gt;We can implement all the operations on the slice by translating the slice-relative index into an absolute index in the parent vector. For example, we can easily implement &lt;code&gt;get_unchecked&lt;/code&gt; with this delegation:&lt;/p&gt;
    &lt;code&gt;// Index translation within the slice's get_unchecked
pub unsafe fn get_unchecked(&amp;amp;self, index: usize) -&amp;gt; T {
    debug_assert!(index &amp;lt; self.len());
    // The index is relative to the slice, so we add the slice's start
    // offset to get the correct index in the parent vector.
    self.parent.get_unchecked(self.range.start + index)
}&lt;/code&gt;
    &lt;p&gt;In this way there is no code duplication; the slice re-uses the access logic of the parent &lt;code&gt;FixedVec&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mutable Slices&lt;/head&gt;
    &lt;p&gt;For mutable slices (&lt;code&gt;V = &amp;amp;mut FixedVec&amp;lt;...&amp;gt;&lt;/code&gt; ), we can provide mutable access to the slice‚Äôs elements. We can easily implement the &lt;code&gt;at_mut&lt;/code&gt; method on &lt;code&gt;FixedVecSlice&lt;/code&gt; using the same principle of index translation:&lt;/p&gt;
    &lt;code&gt;pub fn at_mut(&amp;amp;mut self, index: usize) -&amp;gt; Option&amp;lt;MutProxy&amp;lt;'_, T, W, E, B&amp;gt;&amp;gt; {
    if index &amp;gt;= self.len() {
        return None;
    }
    // The index is translated to the parent vector's coordinate space.
    Some(MutProxy::new(&amp;amp;mut self.parent, self.range.start + index))
}&lt;/code&gt;
    &lt;p&gt;A mutable slice borrows the parent &lt;code&gt;FixedVec&lt;/code&gt; mutably. This means that while the slice exists, the parent vector cannot be accessed directly due to Rust‚Äôs borrowing rules. Let‚Äôs consider the following situation: we may need to split a mutable slice into two non-overlapping mutable slices. This is common for example in algorithms that operate on sub-regions of an array. However, implementing such a method requires to use some unsafe code. The method, let‚Äôs say &lt;code&gt;split_at_mut&lt;/code&gt;, must produce two &lt;code&gt;&amp;amp;mut&lt;/code&gt; references from a single one. In order to be safe, we must prove to the compiler that the logical ranges they represent (&lt;code&gt;0..mid&lt;/code&gt; and &lt;code&gt;mid..len&lt;/code&gt;) are disjoint.&lt;/p&gt;
    &lt;code&gt;pub fn split_at_mut(&amp;amp;mut self, mid: usize) -&amp;gt; (FixedVecSlice&amp;lt;&amp;amp;mut Self&amp;gt;, FixedVecSlice&amp;lt;&amp;amp;mut Self&amp;gt;) {
    assert!(mid &amp;lt;= self.len, "mid &amp;gt; len in split_at_mut");
    // SAFETY: The two slices are guaranteed not to overlap.
    unsafe {
        let ptr = self as *mut Self;
        let left = FixedVecSlice::new(&amp;amp;mut *ptr, 0..mid);
        let right = FixedVecSlice::new(&amp;amp;mut *ptr, mid..self.len());
        (left, right)
    }
}&lt;/code&gt;
    &lt;p&gt;This combination of a generic slice struct and careful pointer manipulation allows us to build a rich, safe, and zero-copy API for both immutable and mutable views, mirroring Rust‚Äôs native slice&lt;/p&gt;
    &lt;head rend="h1"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;Our &lt;code&gt;FixedVec&lt;/code&gt; works pretty well at the current state, but its strengths are tied to two key assumptions: a single thread of execution and uniformly bounded data. Real-world systems often challenge both. This opens up two distinct paths for us to extend our design: one to handle concurrency, and another to adapt to more complex data distributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Concurrent Access&lt;/head&gt;
    &lt;p&gt;The design of &lt;code&gt;FixedVec&lt;/code&gt; is fundamentally single-threaded. Its mutable access relies on direct writes to the underlying bit buffer, a model that offers no guarantees in a concurrent setting. The core conflict lies between our data layout and the hardware‚Äôs atomic primitives. CPU instructions like compare-and-swap operate on aligned machine words, typically &lt;code&gt;u64&lt;/code&gt;. Our elements, however, are packed at arbitrary bit offsets and can span the boundary between two words.&lt;/p&gt;
    &lt;p&gt;This means a single logical update‚Äîmodifying one element‚Äîmight require writing to two separate &lt;code&gt;u64&lt;/code&gt; words. Performing this as two distinct atomic writes would create a race condition, leaving the data in a corrupt state if another thread reads between them. A single atomic write to one of the underlying &lt;code&gt;u64&lt;/code&gt; words would be equally disastrous, as it could simultaneously alter parts of two different elements. The problem then is how to build atomic semantics on top of a non-atomic memory layout. In the next post, we will construct a solution that provides thread-safe, atomic operations for our bit-packed vector.&lt;/p&gt;
    &lt;head rend="h2"&gt;Variable Length Encoding&lt;/head&gt;
    &lt;p&gt;Let‚Äôs consider the following scenario: we have a large collection of integers, all of which are small, say in the range &lt;code&gt;[0, 255]&lt;/code&gt;, but with a few outliers that are much larger, perhaps up to &lt;code&gt;u64::MAX&lt;/code&gt;. If we were to use our &lt;code&gt;FixedVec&lt;/code&gt; with a &lt;code&gt;bit_width&lt;/code&gt; of 64 to accommodate the outliers, we would waste a significant amount of memory on the small integers. Conversely, if we chose a smaller &lt;code&gt;bit_width&lt;/code&gt;, we would be unable to represent the outliers at all.&lt;/p&gt;
    &lt;p&gt;The fixed-width model rests on that critical assumption of uniformly bounded data. Its performance comes from this predictability, but this rigid structure is also its main weakness.&lt;/p&gt;
    &lt;p&gt;For skewed data distributions, we need a different model. Instead of a fixed number of bits per element, we can use variable-length instantaneous codes, where smaller numbers are represented by shorter bit sequences. This gives us excellent compression, but it breaks our O(1) random access guarantee. We can no longer compute the location of the i-th element with a simple multiplication. The solution is to trade a small amount of space for a speedup in access time. We can build a secondary index that stores the bit-offset of every k-th element. This sampling allows us to seek to a nearby checkpoint and decode sequentially from there, restoring amortized O(1) access. In a future article, we‚Äôll explore this second vector type, its own set of performance trade-offs, and how we can choose the best encoding for our data.&lt;/p&gt;
    &lt;p&gt;We will explore this two paths in future articles. In the meantime, if you want to try them out, they are both already implemented in the library. You can find the atomic version in the atomic module and the variable-length version in the variable module.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lukefleed.xyz/posts/compressed-fixedvec/"/><published>2025-09-24T15:17:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362023</id><title>Python on the Edge: Fast, sandboxed, and powered by WebAssembly</title><updated>2025-09-25T00:45:45.213626+00:00</updated><content>&lt;doc fingerprint="87ab2dfdccf438d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Python on the Edge: Fast, sandboxed, and powered by WebAssembly&lt;/head&gt;
    &lt;p&gt;We are excited to announce full Python support in Wasmer Edge (Beta)&lt;/p&gt;
    &lt;p&gt;Founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;With AI workloads on the rise, the demand for Python support on WebAssembly on the Edge has grown rapidly.&lt;/p&gt;
    &lt;p&gt;However, bringing Python to WebAssembly isn't trivial as it means supporting native modules like &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, and &lt;code&gt;pydantic&lt;/code&gt;.&amp;#13;
While projects like &lt;code&gt;pyodide&lt;/code&gt; made strides in running Python in the browser via WebAssembly, their trade-offs don't fully fit server-side needs.&lt;/p&gt;
    &lt;p&gt;After months of hard work, today we're thrilled to announce full Python support in Wasmer Edge (Beta) powered by WebAssembly and WASIX.&lt;/p&gt;
    &lt;p&gt;Now you can run FastAPI, Streamlit, Django, LangChain, and more directly on Wasmer and Wasmer Edge! To accomplish it we had to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add support for dynamic linking (&lt;code&gt;dlopen&lt;/code&gt;/&lt;code&gt;dlsym&lt;/code&gt;) into WASIX&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;libffi&lt;/code&gt;support (so Python libraries using&lt;code&gt;ctypes&lt;/code&gt;could be supported)&lt;/item&gt;
      &lt;item&gt;Polish Sockets and threading support in WASIX&lt;/item&gt;
      &lt;item&gt;Release our own Python Package Index with many of the most popular Python Native libraries compiled to WASIX&lt;/item&gt;
      &lt;item&gt;Create our own alternative to Heroku Buildpacks / Nixpacks / Railpack / Devbox to automatically detect a project type from its source code and deploy it (including running with Wasmer or deploying to Wasmer Edge!). Updates will be shared soon!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How fast is it?&lt;/head&gt;
    &lt;p&gt;This Python release is much faster than any of the other Python releases we did in the past.&lt;/p&gt;
    &lt;p&gt;It is fast. Insa‚Ä¶natively fast (it's even faster than our py2wasm project!)&lt;/p&gt;
    &lt;code&gt;$ wasmer run python/python@=0.2.0 --dir=. -- pystone.py&amp;#13;
Pystone(1.1) time for 50000 passes = 0.562538&amp;#13;
This machine benchmarks at 88882.9 pystones/second&amp;#13;
$ wasmer run python/python --dir=. -- pystone.py # Note: first run may take time&amp;#13;
Pystone(1.1) time for 50000 passes = 0.093556&amp;#13;
This machine benchmarks at 534439 pystones/second&amp;#13;
$ python3 pystone.py&amp;#13;
Pystone(1.1) time for 50000 passes = 0.0827736&amp;#13;
This machine benchmarks at 604057 pystones/second
&lt;/code&gt;
    &lt;p&gt;That's 6x faster, and nearly indistinguishable from native Python performance‚Ä¶ quite good, considering that your Python apps can now run fully sandboxed anywhere!&lt;/p&gt;
    &lt;p&gt;Note: the first time you run Python, it will take a few minutes to compile. We are working to improve this so no time will be spent on compilation locally.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;üöÄ Even faster performance coming soon: we are trialing an optimization technique that will boost Python performance in Wasm to 95% of native Python speed. This is already powering our PHP server in production. Result: Near-native Python performance, fully sandboxed. Stay tuned!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;What it can run&lt;/head&gt;
    &lt;p&gt;Now, you can run any kind of Python API server, powered by &lt;code&gt;fastapi&lt;/code&gt;, &lt;code&gt;django&lt;/code&gt;, &lt;code&gt;flask&lt;/code&gt;, or &lt;code&gt;starlette&lt;/code&gt;, connected to a MySQL database automatically when needed (FastAPI template, Django template).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;fastapi&lt;/code&gt; with websockets (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;mcp&lt;/code&gt; servers (deploy using our MCP template, demo).&lt;/p&gt;
    &lt;p&gt;You can run image processors like &lt;code&gt;pillow&lt;/code&gt;  (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;ffmpeg&lt;/code&gt; inside Python (example repo, demo).&lt;/p&gt;
    &lt;p&gt;You can run &lt;code&gt;streamlit&lt;/code&gt; and &lt;code&gt;langchain&lt;/code&gt; (deploy using our LangChain template, demo).&lt;/p&gt;
    &lt;p&gt;You can even run &lt;code&gt;pypandoc&lt;/code&gt;!  (example repo, demo).&lt;/p&gt;
    &lt;p&gt;Soon, we'll have full support for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;curl_cffi&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;polars&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gevent&lt;/code&gt;/&lt;code&gt;greenlet&lt;/code&gt;(more on this soon!)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Pytorch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Wasmer VS alternatives&lt;/head&gt;
    &lt;p&gt;Python on Wasmer Edge is just launching, but it's already worth asking: how does it stack up existing solutions?&lt;/p&gt;
    &lt;head rend="h3"&gt;Quick Comparison&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Feature / Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Wasmer Edge&lt;/cell&gt;
        &lt;cell role="head"&gt;Cloudflare&lt;/cell&gt;
        &lt;cell role="head"&gt;AWS Lambda&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Native modules (&lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, etc.)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported*&lt;/cell&gt;
        &lt;cell&gt;‚ùå Limited (no &lt;code&gt;libcurl&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multithreading &amp;amp; multiprocessing (&lt;code&gt;ffmpeg&lt;/code&gt;, &lt;code&gt;pandoc&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ASGI / WSGI frameworks (&lt;code&gt;uvicorn&lt;/code&gt;, &lt;code&gt;daphne&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
        &lt;cell&gt;‚ùå Patched / limited&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Needs wrappers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;WebSockets (&lt;code&gt;streamlit&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Raw sockets (&lt;code&gt;libcurl&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
        &lt;cell&gt;‚ùå JS &lt;code&gt;fetch&lt;/code&gt; only&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multiple Python versions&lt;/cell&gt;
        &lt;cell&gt;‚úÖ In Roadmap (3.12, 3.14‚Ä¶)&lt;/cell&gt;
        &lt;cell&gt;‚ùå Tied to bundled runtime&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cold starts&lt;/cell&gt;
        &lt;cell&gt;‚ö° Extremely fast&lt;/cell&gt;
        &lt;cell&gt;‚è≥ Medium (V8 isolates)&lt;/cell&gt;
        &lt;cell&gt;‚è≥ Slow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Code changes required&lt;/cell&gt;
        &lt;cell&gt;‚úÖ None&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Some&lt;/cell&gt;
        &lt;cell&gt;‚ö†Ô∏è Wrappers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pricing&lt;/cell&gt;
        &lt;cell&gt;üí∞ Affordable&lt;/cell&gt;
        &lt;cell&gt;üí∞ Higher&lt;/cell&gt;
        &lt;cell&gt;üí∞ Higher&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare Workers (Python) / Pyodide&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;‚ÑπÔ∏è Most of the demos that we showcased on this article, are not runnable inside of Cloudflare:&lt;/p&gt;&lt;code&gt;ffmpeg&lt;/code&gt;,&lt;code&gt;streamlit&lt;/code&gt;,&lt;code&gt;pypandoc&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Cloudflare launched Python support ~18 months ago, by using Pyodide inside workerd, their JavaScript-based Workers runtime.&lt;/p&gt;
    &lt;p&gt;While great for browser-like environments, Pyodide has trade-offs that make it less suitable server-side. Here are the limitations when running Python in Cloudflare:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå No support for &lt;code&gt;uvloop&lt;/code&gt;,&lt;code&gt;uvicorn&lt;/code&gt;, or similar event-native frameworks (JS event loop patches break compatibility with native).&lt;/item&gt;
      &lt;item&gt;‚ùå No pthreads or multiprocessing support, you can't call subprocesses like &lt;code&gt;ffmpeg&lt;/code&gt;or&lt;code&gt;pypandoc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;‚ùå No raw HTTP client sockets (HTTP clients are patched to use JS &lt;code&gt;fetch&lt;/code&gt;, no&lt;code&gt;libcurl&lt;/code&gt;available).&lt;/item&gt;
      &lt;item&gt;‚ùå Limited to a bundled Python version and package set.&lt;/item&gt;
      &lt;item&gt;‚è≥ Cold starts slower due to V8 isolate warmup.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why the limitations? Cloudflare relies on Pyodide: great in-browser execution, but server-side it implies no sockets, threads, or multiprocessing. The result: convenient for lightweight browser use, but might not be the best fit for real Python workloads on the server.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In contrast, Wasmer Edge runs real Python on WASIX unmodified, so everything "just works", with near-native speed and fast cold starts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Amazon Lambda&lt;/head&gt;
    &lt;p&gt;AWS Lambda doesn't natively run unmodified Python apps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå You need adapters (such as https://github.com/slank/awsgi or https://github.com/Kludex/mangum) for running your WSGI sites.&lt;/item&gt;
      &lt;item&gt;‚ùå WebSockets are unsupported.&lt;/item&gt;
      &lt;item&gt;‚ö†Ô∏è Setup is complex, adapters are often unmaintained.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why the limits? AWS Lambda requires you to use their HTTP lambda handler, which can cause incompatibility into your own HTTP servers. Also, because their lambda handlers are HTTP-based, there's no easy support for WebSockets.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In contrast, Wasmer Edge supports any Python HTTP servers without requiring any code adaptation from your side.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Why Wasmer Edge Stands Out&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Closer to native Python than Pyodide (no JS involvement at all).&lt;/item&gt;
      &lt;item&gt;Faster cold starts and more compatibility than Cloudflare's Workers.&lt;/item&gt;
      &lt;item&gt;More compatible than AWS Lambda (no wrappers/adapters).&lt;/item&gt;
      &lt;item&gt;More affordable across the board.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;üêç It's Showtime!&lt;/head&gt;
    &lt;p&gt;Python support in Wasmer and Wasmer Edge is already available and ready to use. We have set up many Python templates to help you get started in no time.&lt;/p&gt;
    &lt;p&gt;https://wasmer.io/templates?language=python&lt;/p&gt;
    &lt;p&gt;To make things even better, we are working on a MCP server for Wasmer, so you will be able to plug Wasmer into ChatGPT or Anthropic and have your websites deploying from your vibe-coded projects. Stay tuned!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ö†Ô∏è Python in Wasmer Edge is still in Beta, so expect some rough edges if your project doesn't work out of the box‚Ä¶ if you encounter any issues, please report them so we can work on enabling your workloads on Wasmer Edge.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Create your first MCP Server in Wasmer&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to https://wasmer.io/templates/mcp-chatgpt-starter?intent=at_vRxJIdtPCbKe&lt;/item&gt;
      &lt;item&gt;Connect your Github account&lt;/item&gt;
      &lt;item&gt;Create a git repo from the template&lt;/item&gt;
      &lt;item&gt;Deploy and enjoy!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: source code available here: https://github.com/wasmer-examples/python-mcp-chatgpt-starter&lt;/p&gt;
    &lt;head rend="h2"&gt;Create your first Django app&lt;/head&gt;
    &lt;p&gt;We have set up a template for using Django + Uvicorn in Wasmer Edge.&lt;/p&gt;
    &lt;p&gt;You can start using it very easily, just click Deploy: https://wasmer.io/templates/django-starter?intent=at_WK0DIkt3CeKX&lt;/p&gt;
    &lt;p&gt;Deploying a Django app will create a MySQL DB for you in Wasmer Edge (Postgres support is coming soon), run migrations and prepare everything to run your website seamlessly.&lt;/p&gt;
    &lt;p&gt;Note: source code available here: https://github.com/wasmer-examples/django-wasmer-starter&lt;/p&gt;
    &lt;p&gt;Ready to deploy your first Python app on Wasmer Edge?&lt;/p&gt;
    &lt;p&gt;Here are the best places to begin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üöÄ Starter Templates ‚Üí Browse Python templates&lt;/item&gt;
      &lt;item&gt;üìñ Docs &amp;amp; Examples ‚Üí Wasmer GitHub&lt;/item&gt;
      &lt;item&gt;üí¨ Community Support ‚Üí Join our Discord&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üëâ Deploy your first Python app now&lt;/p&gt;
    &lt;p&gt;With WebAssembly and Wasmer, Python is now portable, sandboxed, and running at near-native speeds. Ready for AI workloads, APIs, and anything you can imagine at the edge.&lt;lb/&gt; The sky is the limit ‚ù§Ô∏è.&lt;/p&gt;
    &lt;head rend="h5"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;Syrus Akbary is an enterpreneur and programmer. Specifically known for his contributions to the field of WebAssembly. He is the Founder and CEO of Wasmer, an innovative company that focuses on creating developer tools and infrastructure for running Wasm&lt;/p&gt;
    &lt;p&gt;Founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;How fast is it?&lt;/p&gt;
    &lt;p&gt;What it can run&lt;/p&gt;
    &lt;p&gt;Wasmer VS alternatives&lt;/p&gt;
    &lt;p&gt;Quick Comparison&lt;/p&gt;
    &lt;p&gt;Cloudflare Workers (Python) / Pyodide&lt;/p&gt;
    &lt;p&gt;Amazon Lambda&lt;/p&gt;
    &lt;p&gt;Why Wasmer Edge Stands Out&lt;/p&gt;
    &lt;p&gt;üêç It's Showtime!&lt;/p&gt;
    &lt;p&gt;Create your first MCP Server in Wasmer&lt;/p&gt;
    &lt;p&gt;Create your first Django app&lt;/p&gt;
    &lt;p&gt;Deploy your first Python site in seconds with our managed cloud solution.&lt;/p&gt;
    &lt;head rend="h5"&gt;Read more&lt;/head&gt;
    &lt;p&gt;wasmerwasmer edgerustprojectsedgeweb scraper&lt;/p&gt;
    &lt;head rend="h6"&gt;Build a Web Scraper in Rust and Deploy to Wasmer Edge&lt;/head&gt;
    &lt;p&gt;RudraAugust 14, 2023&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wasmer.io/posts/python-on-the-edge-powered-by-webassembly"/><published>2025-09-24T15:48:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362206</id><title>SedonaDB: A new geospatial DataFrame library written in Rust</title><updated>2025-09-25T00:45:45.052797+00:00</updated><content>&lt;doc fingerprint="3bdb989c4036eb8a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing SedonaDB: A single-node analytical database engine with geospatial as a first-class citizen&lt;/head&gt;
    &lt;p&gt;The Apache Sedona community is excited to announce the initial release of SedonaDB! √∞&lt;/p&gt;
    &lt;p&gt;SedonaDB is the first open-source, single-node analytical database engine that treats spatial data as a first-class citizen. It is developed as a subproject of Apache Sedona.&lt;/p&gt;
    &lt;p&gt;Apache Sedona powers large-scale geospatial processing on distributed engines like Spark (SedonaSpark), Flink (SedonaFlink), and Snowflake (SedonaSnow). SedonaDB extends the Sedona ecosystem with a single-node engine optimized for small-to-medium data analytics, delivering the simplicity and speed that distributed systems often cannot.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ What is SedonaDB¬∂&lt;/head&gt;
    &lt;p&gt;Written in Rust, SedonaDB is lightweight, blazing fast, and spatial-native. Out of the box, it provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;√∞¬∫√Ø¬∏ Full support for spatial types, joins, CRS (coordinate reference systems), and functions on top of industry-standard query operations.&lt;/item&gt;
      &lt;item&gt;√¢¬° Query optimizations, indexing, and data pruning features under the hood that make spatial operations just work with high performance.&lt;/item&gt;
      &lt;item&gt;√∞ Pythonic and SQL interfaces familiar to developers, plus APIs for R and Rust.&lt;/item&gt;
      &lt;item&gt;√¢√Ø¬∏ Flexibility to run in single-machine environments on local files or data lakes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SedonaDB utilizes Apache Arrow and Apache DataFusion, providing everything you need from a modern, vectorized query engine. What sets it apart is the ability to process spatial workloads natively, without extensions or plugins. Installation is straightforward, and SedonaDB integrates easily into both local development and cloud pipelines, offering a consistent experience across environments.&lt;/p&gt;
    &lt;p&gt;The initial release of SedonaDB provides a comprehensive suite of geometric vector operations and seamlessly integrates with GeoArrow, GeoParquet, and GeoPandas. Future versions will support all popular spatial functions, including functions for raster data.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ SedonaDB quickstart example¬∂&lt;/head&gt;
    &lt;p&gt;Start by installing SedonaDB:&lt;/p&gt;
    &lt;code&gt;pip install "apache-sedona[db]"
&lt;/code&gt;
    &lt;p&gt;Now instantiate the connection:&lt;/p&gt;
    &lt;code&gt;import sedona.db

sd = sedona.db.connect()
&lt;/code&gt;
    &lt;p&gt;Let's perform a spatial join using SedonaDB.&lt;/p&gt;
    &lt;p&gt;Suppose you have a &lt;code&gt;cities&lt;/code&gt; table with latitude and longitude points representing the center of each city, and a &lt;code&gt;countries&lt;/code&gt; table with a column containing a polygon of the country's geographic boundaries.&lt;/p&gt;
    &lt;p&gt;Here are a few rows from the &lt;code&gt;cities&lt;/code&gt; table:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢     name     √¢            geometry           √¢
√¢   utf8view   √¢      geometry &amp;lt;epsg:4326&amp;gt;     √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Vatican City √¢ POINT(12.4533865 41.9032822)  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ San Marino   √¢ POINT(12.4417702 43.9360958)  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Vaduz        √¢ POINT(9.5166695 47.1337238)   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
&lt;/code&gt;
    &lt;p&gt;And here are a few rows from the countries table:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢             name            √¢   continent   √¢                      geometry                      √¢
√¢           utf8view          √¢    utf8view   √¢                geometry &amp;lt;epsg:4326&amp;gt;                √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Fiji                        √¢ Oceania       √¢ MULTIPOLYGON(((180 -16.067132663642447,180 -16.55√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ United Republic of Tanzania √¢ Africa        √¢ POLYGON((33.90371119710453 -0.9500000000000001,34√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Western Sahara              √¢ Africa        √¢ POLYGON((-8.665589565454809 27.656425889592356,-8√¢¬¶ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
&lt;/code&gt;
    &lt;p&gt;Here√¢s how to perform a spatial join to compute the country of each city:&lt;/p&gt;
    &lt;code&gt;sd.sql(
    """
select
    cities.name as city_name,
    countries.name as country_name,
    continent
from cities
join countries
where ST_Intersects(cities.geometry, countries.geometry)
"""
).show(3)
&lt;/code&gt;
    &lt;p&gt;The code utilizes &lt;code&gt;ST_Intersects&lt;/code&gt; to determine if a city is contained within a given country.&lt;/p&gt;
    &lt;p&gt;Here's the result of the query:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢   city_name   √¢         country_name        √¢ continent √¢
√¢    utf8view   √¢           utf8view          √¢  utf8view √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢ Suva          √¢ Fiji                        √¢ Oceania   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Dodoma        √¢ United Republic of Tanzania √¢ Africa    √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢ Dar es Salaam √¢ United Republic of Tanzania √¢ Africa    √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;The example above performs a point-in-polygon join, mapping city locations (points) to the countries they fall within (polygons). SedonaDB executes these joins efficiently by leveraging spatial indices where beneficial and dynamically adapting join strategies at runtime using input data samples. While many general-purpose engines struggle with the performance of such operations, SedonaDB is purpose-built for spatial workloads and delivers consistently fast results.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ Apache Sedona SpatialBench¬∂&lt;/head&gt;
    &lt;p&gt;To test our work on SedonaDB, we also needed to develop a mechanism to evaluate its performance and speed. This led us to develop Apache Sedona SpatialBench, a benchmark for assessing geospatial SQL analytics query performance across database systems.&lt;/p&gt;
    &lt;p&gt;Let's compare the performance of SedonaDB vs. GeoPandas and DuckDB Spatial for some representative spatial queries as defined in SpatialBench.&lt;/p&gt;
    &lt;p&gt;Here are the results from SpatialBench v0.1 for Queries 1√¢12 at scale factor 1 (SF1) and scale factor 10 (SF10).&lt;/p&gt;
    &lt;p&gt;SedonaDB demonstrates balanced performance across all query types and scales effectively to SF 10. DuckDB excels at spatial filters and some geometric operations but faces challenges with complex joins and KNN queries. GeoPandas, while popular in the Python ecosystem, requires manual optimization and parallelization to handle larger datasets effectively. An in-depth performance analysis can be found in the SpatialBench website.&lt;/p&gt;
    &lt;p&gt;Here√¢s an example of the SpatialBench Query #8 that works for SedonaDB and DuckDB:&lt;/p&gt;
    &lt;code&gt;SELECT b.b_buildingkey, b.b_name, COUNT(*) AS nearby_pickup_count
FROM trip t JOIN building b ON ST_DWithin(ST_GeomFromWKB(t.t_pickuploc), ST_GeomFromWKB(b.b_boundary), 0.0045) -- ~500m
GROUP BY b.b_buildingkey, b.b_name
ORDER BY nearby_pickup_count DESC
&lt;/code&gt;
    &lt;p&gt;This query intentionally performs a distance-based spatial join between points and polygons, followed by an aggregation of the results.&lt;/p&gt;
    &lt;p&gt;Here's what the query returns:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ b_buildingkey √¢  b_name  √¢ nearby_pickup_count √¢
√¢     int64     √¢ utf8view √¢        int64        √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢          3779 √¢ linen    √¢                  42 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢         19135 √¢ misty    √¢                  36 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢          4416 √¢ sienna   √¢                  26 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;Here√¢s the equivalent GeoPandas code:&lt;/p&gt;
    &lt;code&gt;trips_df = pd.read_parquet(data_paths["trip"])
trips_df["pickup_geom"] = gpd.GeoSeries.from_wkb(
    trips_df["t_pickuploc"], crs="EPSG:4326"
)
pickups_gdf = gpd.GeoDataFrame(trips_df, geometry="pickup_geom", crs="EPSG:4326")

buildings_df = pd.read_parquet(data_paths["building"])
buildings_df["boundary_geom"] = gpd.GeoSeries.from_wkb(
    buildings_df["b_boundary"], crs="EPSG:4326"
)
buildings_gdf = gpd.GeoDataFrame(
    buildings_df, geometry="boundary_geom", crs="EPSG:4326"
)

threshold = 0.0045  # degrees (~500m)
result = (
    buildings_gdf.sjoin(pickups_gdf, predicate="dwithin", distance=threshold)
    .groupby(["b_buildingkey", "b_name"], as_index=False)
    .size()
    .rename(columns={"size": "nearby_pickup_count"})
    .sort_values(["nearby_pickup_count", "b_buildingkey"], ascending=[False, True])
    .reset_index(drop=True)
)
&lt;/code&gt;
    &lt;head rend="h2"&gt;√∞¬∫√Ø¬∏ SedonaDB CRS management¬∂&lt;/head&gt;
    &lt;p&gt;SedonaDB manages the CRS when reading/writing files, as well as in DataFrames, making your pipelines safer and saving you from manual work.&lt;/p&gt;
    &lt;p&gt;Let's compute the number of buildings in the state of Vermont to highlight the CRS management features embedded in SedonaDB.&lt;/p&gt;
    &lt;p&gt;Start by reading in a FlatGeobuf file that uses the EPSG 32618 CRS with GeoPandas and then convert it to a SedonaDB DataFrame:&lt;/p&gt;
    &lt;code&gt;import geopandas as gpd

path = "https://raw.githubusercontent.com/geoarrow/geoarrow-data/v0.2.0/example-crs/files/example-crs_vermont-utm.fgb"
gdf = gpd.read_file(path)
vermont = sd.create_data_frame(gdf)
&lt;/code&gt;
    &lt;p&gt;Let√¢s check the schema of the &lt;code&gt;vermont&lt;/code&gt; DataFrame:&lt;/p&gt;
    &lt;code&gt;vermont.schema

SedonaSchema with 1 field:
  geometry: wkb &amp;lt;epsg:32618&amp;gt;
&lt;/code&gt;
    &lt;p&gt;We can see that the &lt;code&gt;vermont&lt;/code&gt; DataFrame maintains the CRS that√¢s specified in the FlatGeobuf file.  SedonaDB doesn√¢t have a native FlatGeobuf reader yet, but it√¢s easy to use the GeoPandas FlatGeobuf reader and then convert it to a SedonaDB DataFrame with a single line of code.&lt;/p&gt;
    &lt;p&gt;Now read a GeoParquet file into a SedonaDB DataFrame.&lt;/p&gt;
    &lt;code&gt;buildings = sd.read_parquet(
    "https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/microsoft-buildings_point_geo.parquet"
)
&lt;/code&gt;
    &lt;p&gt;Check the schema of the DataFrame:&lt;/p&gt;
    &lt;code&gt;buildings.schema

SedonaSchema with 1 field:
  geometry: geometry &amp;lt;ogc:crs84&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Let√¢s expose these two tables as views and run a spatial join to see how many buildings are in Vermont:&lt;/p&gt;
    &lt;code&gt;buildings.to_view("buildings", overwrite=True)
vermont.to_view("vermont", overwrite=True)

sd.sql(
    """
select count(*) from buildings
join vermont
where ST_Intersects(buildings.geometry, vermont.geometry)
"""
).show()
&lt;/code&gt;
    &lt;p&gt;This command correctly errors out because the tables have different CRSs. For safety, SedonaDB errors out rather than give you the wrong answer! Here's the error message that's easy to debug:&lt;/p&gt;
    &lt;code&gt;SedonaError: type_coercion
caused by
Error during planning: Mismatched CRS arguments: ogc:crs84 vs epsg:32618
Use ST_Transform() or ST_SetSRID() to ensure arguments are compatible.
&lt;/code&gt;
    &lt;p&gt;Let√¢s rewrite the spatial join to convert the &lt;code&gt;vermont&lt;/code&gt; CRS to EPSG:4326, so it√¢s compatible with the &lt;code&gt;buildings&lt;/code&gt; CRS.&lt;/p&gt;
    &lt;code&gt;sd.sql(
    """
select count(*) from buildings
join vermont
where ST_Intersects(buildings.geometry, ST_Transform(vermont.geometry, 'EPSG:4326'))
"""
).show()
&lt;/code&gt;
    &lt;p&gt;We now get the correct result!&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ count(*) √¢
√¢   int64  √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢   361856 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;SedonaDB tracks the CRS when reading/writing files, converting to/from GeoPandas DataFrames, or when performing DataFrame operations, so your spatial computations run safely and correctly!&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬Ø Realistic example with SedonaDB¬∂&lt;/head&gt;
    &lt;p&gt;Let's now turn our attention to a KNN join, which is a more complex spatial operation.&lt;/p&gt;
    &lt;p&gt;Suppose you're analyzing ride-sharing data and want to identify which buildings are most commonly near pickup points, helping understand the relationship between trip origins and nearby landmarks, businesses, or residential structures that might influence ride demand patterns.&lt;/p&gt;
    &lt;p&gt;This query finds the five closest buildings to each trip pickup location using spatial nearest neighbor analysis. For every trip, it identifies the five buildings that are geographically closest to where the passenger was picked up and calculates the exact distance to each of those buildings.&lt;/p&gt;
    &lt;p&gt;Here√¢s the query:&lt;/p&gt;
    &lt;code&gt;WITH trip_with_geom AS (
    SELECT t_tripkey, t_pickuploc, ST_GeomFromWKB(t_pickuploc) as pickup_geom
    FROM trip
),
building_with_geom AS (
    SELECT b_buildingkey, b_name, b_boundary, ST_GeomFromWKB(b_boundary) as boundary_geom
    FROM building
)
SELECT
    t.t_tripkey,
    t.t_pickuploc,
    b.b_buildingkey,
    b.b_name AS building_name,
    ST_Distance(t.pickup_geom, b.boundary_geom) AS distance_to_building
FROM trip_with_geom t JOIN building_with_geom b
ON ST_KNN(t.pickup_geom, b.boundary_geom, 5, FALSE)
ORDER BY distance_to_building ASC, b.b_buildingkey ASC
&lt;/code&gt;
    &lt;p&gt;Here are the results of the query:&lt;/p&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢ t_tripkey √¢          t_pickuploc          √¢ b_buildingkey √¢ building_name √¢ distance_to_building √¢
√¢   int64   √¢             binary            √¢     int64     √¢      utf8     √¢        float64       √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬™√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬°
√¢   5854027 √¢ 01010000001afa27b85825504001√¢¬¶ √¢            79 √¢ gainsboro     √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢   3326828 √¢ 01010000001bfcc5b8b7a95d4083√¢¬¶ √¢           466 √¢ deep          √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬º√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬§
√¢   1239844 √¢ 0101000000ce471770d6ce2a40f9√¢¬¶ √¢           618 √¢ ivory         √¢                  0.0 √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¥√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;This is one of the queries from SpatialBench.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬¶ Why SedonaDB was built in Rust¬∂&lt;/head&gt;
    &lt;p&gt;SedonaDB is built in Rust, a high-performance, memory-safe language that offers fine-grained memory management and a mature ecosystem of data libraries. It takes full advantage of this ecosystem by integrating with projects such as Apache DataFusion, GeoArrow, and georust/geo.&lt;/p&gt;
    &lt;p&gt;While Spark provides extension points that let SedonaSpark optimize spatial queries in distributed settings, DataFusion offers stable APIs for pruning, spatial operators, and optimizer rules on a single node. This enabled us to embed deep spatial awareness into the engine while preserving full non-spatial functionality. Thanks to the DataFusion project and community, the experience was both possible and enjoyable.&lt;/p&gt;
    &lt;head rend="h2"&gt;√¢√Ø¬∏ Why SedonaDB and SedonaSpark are Both Needed¬∂&lt;/head&gt;
    &lt;p&gt;SedonaSpark is well-suited for large-scale geospatial workloads or environments where Spark is already part of your production stack. For instance, joining a 100 GB vector dataset with a large raster dataset. For smaller datasets, however, Spark's distributed architecture can introduce unnecessary overhead, making it slower to run locally, harder to install, and more difficult to tune.&lt;/p&gt;
    &lt;p&gt;SedonaDB is better for smaller datasets and when running computations locally. The SedonaDB spatial functions are compatible with the SedonaSpark functions, so SQL chunks that work for one engine will usually work for the other. Over time, we will ensure that both project APIs are fully interoperable. Here's an example of a chunk to analyze the Overture buildings table that works for both engines.&lt;/p&gt;
    &lt;code&gt;nyc_bbox_wkt = (
    "POLYGON((-74.2591 40.4774, -74.2591 40.9176, -73.7004 40.9176, -73.7004 40.4774, -74.2591 40.4774))"
)

sd.sql(f"""
SELECT
    id,
    height,
    num_floors,
    roof_shape,
    ST_Centroid(geometry) as centroid
FROM
    buildings
WHERE
    is_underground = FALSE
    AND height IS NOT NULL
    AND height &amp;gt; 20
    AND ST_Intersects(geometry, ST_SetSRID(ST_GeomFromText('{nyc_bbox_wkt}'), 4326))
LIMIT 5;
&lt;/code&gt;
    &lt;head rend="h2"&gt;√∞ Next steps¬∂&lt;/head&gt;
    &lt;p&gt;While SedonaDB is well-tested and provides a core set of features that can perform numerous spatial analyses, it remains an early-stage project with multiple opportunities for new features.&lt;/p&gt;
    &lt;p&gt;Many more ST functions are required. Some are relatively straightforward, but others are complex.&lt;/p&gt;
    &lt;p&gt;The community will add built-in support for other spatial file formats, such as GeoPackage and GeoJSON, to SedonaDB. You can read data in these formats into GeoPandas DataFrames and convert them to SedonaDB DataFrames in the meantime.&lt;/p&gt;
    &lt;p&gt;Raster support is also on the roadmap, which is a complex undertaking, so it's an excellent opportunity to contribute if you're interested in solving challenging problems with Rust.&lt;/p&gt;
    &lt;p&gt;Refer to the SedonaDB v0.2 milestone for more details on the specific tasks outlined for the next release. Additionally, feel free to create issues, comment on the Discord, or start GitHub discussions to brainstorm new features.&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞¬§ Join the community¬∂&lt;/head&gt;
    &lt;p&gt;The Apache Sedona community has an active Discord community, monthly user meetings, and regular contributor meetings.&lt;/p&gt;
    &lt;p&gt;SedonaDB welcomes contributions from the community. Feel free to request to take ownership of an issue, and we will be happy to assign it to you. You're also welcome to join the contributor meetings, and the other active contributors will be glad to help you get your pull request over the finish line!&lt;/p&gt;
    &lt;p&gt;Info&lt;/p&gt;
    &lt;p&gt;We√¢re celebrating the launch of SedonaDB &amp;amp; SpatialBench with a special Apache Sedona Community Office Hour!&lt;/p&gt;
    &lt;p&gt;√∞ October 7, 2025&lt;/p&gt;
    &lt;p&gt;√¢¬∞ 8√¢9 AM Pacific Time&lt;/p&gt;
    &lt;p&gt;√∞ Online&lt;/p&gt;
    &lt;p&gt;√∞ Sign up here&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sedona.apache.org/latest/blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/"/><published>2025-09-24T16:00:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362254</id><title>New bacteria, and two potential antibiotics, discovered in soil</title><updated>2025-09-25T00:45:44.836586+00:00</updated><content>&lt;doc fingerprint="7c5460762d250ab9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Hundreds of new bacteria, and two potential antibiotics, found in soil&lt;/head&gt;
    &lt;p&gt;Most bacteria cannot be cultured in the lab‚Äîand that‚Äôs been bad news for medicine. Many of our frontline antibiotics originated from microbes, yet as antibiotic resistance spreads and drug pipelines run dry, the soil beneath our feet has a vast hidden reservoir of untapped lifesaving compounds.&lt;/p&gt;
    &lt;p&gt;Now, researchers have developed a way to access this microbial goldmine. Their approach, published in Nature Biotechnology, circumvents the need to grow bacteria in the lab by extracting very large DNA fragments directly from soil to piece together the genomes of previously hidden microbes, and then mines resulting genomes for bioactive molecules.&lt;/p&gt;
    &lt;p&gt;From a single forest sample, the team generated hundreds of complete bacterial genomes never seen before, as well as two new antibiotic leads. The findings offer a scalable way to scour unculturable bacteria for new drug leads‚Äîand expose the vast, uncharted microbial frontier that shapes our environment.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe finally have the technology to see the microbial world that have been previously inaccessible to humans,‚Äù says Sean F. Brady, head of the Laboratory of Genetically Encoded Small Molecules at Rockefeller. ‚ÄúAnd we‚Äôre not just seeing this information; we‚Äôre already turning it into potentially useful antibiotics. This is just the tip of the spear.‚Äù&lt;/p&gt;
    &lt;p&gt;Microbial dark matter&lt;/p&gt;
    &lt;p&gt;When hunting for bacteria, soil is an obvious choice. It‚Äôs the largest, most biodiverse reservoir of bacteria on the planet‚Äîa single teaspoon of it may contain thousands of different species. Many important therapeutics, including most of our antibiotic arsenal, were discovered in the tiny fraction of soil bacteria that can be grown in the laboratory. And soil is dirt cheap.&lt;/p&gt;
    &lt;p&gt;Yet we know very little about the millions of microbes packed into the earth. Scientists suspect that these hidden bacteria hold not only an untapped reservoir of new therapeutics, but clues as to how microbes shape climate, agriculture, and the larger environment that we live in. ‚ÄúAll over the world there‚Äôs this hidden ecosystem of microbes that could have dramatic effects on our lives,‚Äù Brady adds. ‚ÄúWe wanted to finally see them.‚Äù&lt;/p&gt;
    &lt;p&gt;Getting that glimpse involved weaving together several approaches. First, the team optimized a method for isolating large, high-quality DNA fragments directly from soil. Pairing this advance with emerging long-read nanopore sequencing allowed Jan Burian, a postdoctoral associate in the Brady lab, to produce continuous stretches of DNA that were tens of thousands of base pairs long‚Äî200 times longer than any previously existing technology could manage. Soil DNA contains a huge number of different bacteria; without such large DNA sequences to work with, resolving that complex genetic puzzle into complete and contiguous genomes for disparate bacteria proved exceedingly difficult.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs easier to assemble a whole genome out of bigger pieces of DNA, rather than the millions of tiny snippets that were available before,‚Äù Brady says. ‚ÄúAnd that makes a dramatic difference in your confidence in your results.‚Äù&lt;/p&gt;
    &lt;p&gt;Unique small molecules, like antibiotics, that bacteria produce are called ‚Äúnatural products‚Äù. To convert the newly uncovered sequences into bioactive molecules, the team applied a synthetic bioinformatic natural products (synBNP) approach. They bioinformatically predicted the chemical structures of natural products directly from the genome data and then chemically synthesized them in the lab. With the synBNP approach, Brady and colleagues managed to turn the genetic blueprints from uncultured bacteria into actual molecules‚Äîincluding two potent antibiotics.&lt;/p&gt;
    &lt;p&gt;Brady describes the method, which is scalable and can be adapted to virtually any metagenomic space beyond soil, as a three-step strategy that could kick off a new era of microbiology: ‚ÄúIsolate big DNA, sequence it, and computationally convert it into something useful.‚Äù&lt;/p&gt;
    &lt;p&gt;Two new drug candidates, and counting&lt;/p&gt;
    &lt;p&gt;Applied to their single forest soil sample, the team‚Äôs approach produced 2.5 terabase-pairs of sequence data‚Äîthe deepest long-read exploration of a single soil sample to date. Their analysis uncovered hundreds of complete contiguous bacterial genomes, more than 99 percent of which were entirely new to science and identified members from 16 major branches of the bacterial family tree.&lt;/p&gt;
    &lt;p&gt;The two lead compounds discovered could translate into potent antibiotics. One, called erutacidin, disrupts bacterial membranes through an uncommon interaction with the lipid cardiolipin and is effective against even the most challenging drug-resistant bacteria. The other, trigintamicin, acts on a protein-unfolding motor known as ClpX, a rare antibacterial target.&lt;/p&gt;
    &lt;p&gt;Brady emphasizes that these discoveries are only the beginning. The study demonstrates that previously inaccessible microbial genomes can now be decoded and mined for bioactive molecules at scale without culturing the organisms. Unlocking the genetic potential of microbial dark matter may also provide new insights into the hidden microbial networks that sustain ecosystems.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre mainly interested in small molecules as therapeutics, but there are applications beyond medicine,‚Äù Burian says. ‚ÄúStudying culturable bacteria led to advances that helped shape the modern world and finally seeing and accessing the uncultured majority will drive a new generation of discovery.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.rockefeller.edu/news/38239-hundreds-of-new-bacteria-and-two-potential-antibiotics-found-in-soil/"/><published>2025-09-24T16:03:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362486</id><title>Better Curl Saul: a lightweight API testing CLI focused on UX and simplicity</title><updated>2025-09-25T00:45:44.394050+00:00</updated><content>&lt;doc fingerprint="2d3c9d587d87d1a0"&gt;
  &lt;main&gt;
    &lt;code&gt;curl -X POST https://api.github.com/repos/owner/repo/issues \
  -H "Authorization: Bearer ghp_token123" \
  -H "Content-Type: application/json" \
  -H "Accept: application/vnd.github.v3+json" \
  -d '{
    "title": "Bug Report",
    "body": "Something is broken",
    "labels": ["bug", "priority-high"],
    "assignees": ["developer1", "developer2"]
  }'&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workspace-based - Each API gets its own organized folder&lt;/item&gt;
      &lt;item&gt;Smart variables - &lt;code&gt;{@token}&lt;/code&gt;persists,&lt;code&gt;{?name}&lt;/code&gt;prompts every time&lt;/item&gt;
      &lt;item&gt;Response filtering - Show only the fields you care about&lt;/item&gt;
      &lt;item&gt;Git-friendly - TOML files version control beautifully&lt;/item&gt;
      &lt;item&gt;Unix composable - Script it, pipe it, shell it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Supports: Linux, macOS, Windows (I hope)&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/DeprecatedLuar/better-curl-saul/main/install.sh | bash&lt;/code&gt;
    &lt;head&gt;Other Install Methods&lt;/head&gt;
    &lt;p&gt;Manual Install (boring)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download binary for your OS from releases&lt;/item&gt;
      &lt;item&gt;Make executable: &lt;code&gt;chmod +x saul-*&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Move to PATH: &lt;code&gt;sudo mv saul-* /usr/local/bin/saul&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From Source (for try-harders)&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/DeprecatedLuar/better-curl-saul.git
cd better-curl-saul
./other/install-local.sh  # Local development build&lt;/code&gt;
    &lt;p&gt;In case you already have Saul (hardcore)&lt;/p&gt;
    &lt;code&gt;saul set url https://raw.githubusercontent.com/DeprecatedLuar/better-curl-saul/main/install.sh &amp;amp;&amp;amp; saul call --raw | bash #(maybe works, who knows)&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;[!NOTE] Quick install auto-detects your system and downloads binaries or builds from source as fallback. Windows users: I don't know powershell I expect, just have bash üëç&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head&gt;Quick Start&lt;/head&gt;
    &lt;code&gt;# Create a test workspace
saul demo set url https://jsonplaceholder.typicode.com/posts/1
saul demo set method GET
saul demo call

# Try with variables
saul api set url https://httpbin.org/post
saul api set method POST
saul api set body name={?your_name} message="Hello from Saul"
saul api call

# Oh... yeah, for nesting just use dot notation like obj.field=idk&lt;/code&gt;
    &lt;head&gt;Core Commands&lt;/head&gt;
    &lt;p&gt;Alright so you can:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;set&lt;/code&gt;, &lt;code&gt;get&lt;/code&gt;, &lt;code&gt;edit&lt;/code&gt;, &lt;code&gt;rm&lt;/code&gt;&lt;lb/&gt;your&lt;code&gt;body&lt;/code&gt;, &lt;code&gt;header&lt;/code&gt;, &lt;code&gt;query&lt;/code&gt;, &lt;code&gt;request&lt;/code&gt;, &lt;code&gt;history&lt;/code&gt; or maybe even
&lt;code&gt;response&lt;/code&gt;
&lt;lb/&gt;also&lt;code&gt;url&lt;/code&gt;, &lt;code&gt;method&lt;/code&gt;, &lt;code&gt;timeout&lt;/code&gt;, &lt;code&gt;history&lt;/code&gt;&lt;/p&gt;
    &lt;head&gt;Example&lt;/head&gt;
    &lt;code&gt;# Configure your API workspace (or preset, same thing)
saul [workspace] set url https://api.example.com
saul set method POST
saul set header Authorization="Bearer {@token}"
saul set body user.name={?username} user.email=john@test.com

# Execute the request
saul call

# Check your configuration, note that preset/workspace name keeps
# stored in memory after first mention on syntax

saul [anoter_workspace] check url
saul check body

# View response history
saul check history&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;There are 2 variable types&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;soft variables {?} prompt you at EVERY call&lt;/item&gt;
      &lt;item&gt;hard variables {@} require manual update by running the flag -v or running &lt;code&gt;saul set variable varname value&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start watchin Better Call Saul&lt;/item&gt;
      &lt;item&gt;Think of a bad joke&lt;/item&gt;
      &lt;item&gt;Workspace-based configuration&lt;/item&gt;
      &lt;item&gt; Smart variable system (&lt;code&gt;{@}&lt;/code&gt;/&lt;code&gt;{?}&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;In line terminal field editing&lt;/item&gt;
      &lt;item&gt;Response filtering&lt;/item&gt;
      &lt;item&gt;Response history&lt;/item&gt;
      &lt;item&gt;Terminal session memory&lt;/item&gt;
      &lt;item&gt;Bulk operations&lt;/item&gt;
      &lt;item&gt;Fix history response parsing and filtering&lt;/item&gt;
      &lt;item&gt;GET specific response stuff from history (aka Headers/Body...)&lt;/item&gt;
      &lt;item&gt;Flags, we've got none basically&lt;/item&gt;
      &lt;item&gt;Stateless command support&lt;/item&gt;
      &lt;item&gt;Support pasting raw JSON template&lt;/item&gt;
      &lt;item&gt;User config system using the super cool github.com/DeprecatedLuar/toml-vars-letsgooo library&lt;/item&gt;
      &lt;item&gt;Add the eastereggs&lt;/item&gt;
      &lt;item&gt;Polish code&lt;/item&gt;
      &lt;item&gt;Actual Documentation&lt;/item&gt;
      &lt;item&gt;Touch Grass (not a priority)&lt;/item&gt;
      &lt;item&gt;Think of more features&lt;/item&gt;
      &lt;item&gt;Think of even more features&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Beta software - Core features work, documentation in progress.&lt;/p&gt;
    &lt;p&gt;Bug or feedback? I will be very, very, very happy if you let me know your thoughts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DeprecatedLuar/better-curl-saul"/><published>2025-09-24T16:17:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362697</id><title>Terence Tao: The role of small organizations in society has shrunk significantly</title><updated>2025-09-25T00:45:44.025290+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mathstodon.xyz/@tao/115259943398316677"/><published>2025-09-24T16:32:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362813</id><title>Show HN: Vibe Linking</title><updated>2025-09-25T00:45:43.903449+00:00</updated><content/><link href="https://vb.lk/"/><published>2025-09-24T16:40:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45362914</id><title>Launch HN: Flywheel (YC S25) ‚Äì Waymo for Excavators</title><updated>2025-09-25T00:45:43.487196+00:00</updated><content>&lt;doc fingerprint="2c7449d6851a3652"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, We're Jash and Mahimana, cofounders of Flywheel AI (&lt;/p&gt;https://useflywheel.ai&lt;p&gt;). We‚Äôre building a remote teleop and autonomous stack for excavators.&lt;/p&gt;&lt;p&gt;Here's a video: https://www.youtube.com/watch?v=zCNmNm3lQGk.&lt;/p&gt;&lt;p&gt;Interfacing with existing excavators for enabling remote teleop (or autonomy) is hard. Unlike cars which use drive-by-wire technology, most of the millions of excavators are fully hydraulic machines. The joysticks are connected to a pilot hydraulic circuit, which proportionally moves the cylinders in the main hydraulic circuit which ultimately moves the excavator joints. This means excavators mostly do not have an electronic component to control the joints. We solve this by mechanically actuating the joysticks and pedals inside the excavators.&lt;/p&gt;&lt;p&gt;We do this with retrofits which work on any excavator model/make, enabling us to augment existing machines. By enabling remote teleoperation, we are able to increase site safety, productivity and also cost efficiency.&lt;/p&gt;&lt;p&gt;Teleoperation by the operators enables us to prepare training data for autonomy. In robotics, training data comprises observation and action. While images and videos are abundant on the internet, egocentric (PoV) observation and action data is extremely scarce, and it is this scarcity that is holding back scaling robot learning policies.&lt;/p&gt;&lt;p&gt;Flywheel solves this by preparing the training data coming from our remote teleop-enabled excavators which we have already deployed. And we do this with very minimal hardware setup and resources.&lt;/p&gt;&lt;p&gt;During our time in YC, we did 25-30 iterations of sensor stack and placement permutations/combinations, and model hyperparams variations. We called this ‚Äúevolution of the physical form of our retrofit‚Äù. Eventually, we landed on our current evolution and have successfully been able to train some levels of autonomy with only a few hours of training data.&lt;/p&gt;&lt;p&gt;The big takeaway was how much more important data is than optimizing hyperparams of the model. So today, we‚Äôre open sourcing 100hrs of excavator dataset that we collected using Flywheel systems on real construction sites. This is in partnership with Frodobots.ai.&lt;/p&gt;&lt;p&gt;Dataset: https://huggingface.co/datasets/FlywheelAI/excavator-dataset&lt;/p&gt;&lt;p&gt;Machine/retrofit details:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;  Volvo EC380 (38 ton excavator)
  4xcamera (25fps)
  25 hz expert operator‚Äôs action data
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; The dataset contains observation data from 4 cameras and operator's expert action data which can be used to train imitation learning models to run an excavator autonomously for the workflows in those demonstrations, like digging and dumping. We were able to train a small autonomy model for bucket pick and place on Kubota U17 from just 6-7 hours of data collected during YC.&lt;/p&gt;&lt;p&gt;We‚Äôre just getting started. We have good amounts of variations in daylight, weather, tasks, and would be adding more hours of data and also converting to lerobot format soon. We‚Äôre doing this so people like you and me can try out training models on real world data which is very, very hard to get.&lt;/p&gt;&lt;p&gt;So please checkout the dataset here and feel free to download and use however you like. We would love for people to do things with it! I‚Äôll be around in the thread and look forward to comments and feedback from the community!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45362914"/><published>2025-09-24T16:48:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45364450</id><title>How fast is Go? simulating particles on a smart TV</title><updated>2025-09-25T00:45:43.169661+00:00</updated><content>&lt;doc fingerprint="66bcdd582b200d99"&gt;
  &lt;main&gt;
    &lt;p&gt;The challenge, simulate millions of particles in golang, multi-player enabled, cpu only, smart tv compatible.&lt;/p&gt;
    &lt;p&gt;Let's go, pun very much intended.&lt;/p&gt;
    &lt;p&gt;So, during my day job I had to write a ws server which merged multiple upstream ws servers into a single ws server. (Don't ask) I was lost and not even the power of claude, gemini, and cursor could save me. The vibes were simply not enough to get the project done. I had to learn the real real stuff.&lt;/p&gt;
    &lt;p&gt;To learn the real stuff I decided to write a particle simulation and see how many particles I could get golang to push. I had already done this in the lands of javascript, rust, and swift. But given Go doesn't support simd, I knew it wouldn't be a fair fight against the other languages and more importantly, it would be boring let alone helping my day job.&lt;/p&gt;
    &lt;p&gt;I figured to give go the best shake I needed to up the difficulty by adding multi-player to the simulation. After all, go is best known as a snappy productive backend language. Is it snappy and productive enough for simulating a million particles and syncing it across hundreds of clients?&lt;/p&gt;
    &lt;p&gt;Only one way to find out.&lt;/p&gt;
    &lt;p&gt;There is a rule and an important one. No client simulation allowed only server. The client should be a simple web page. It should work anywhere browser runs.&lt;/p&gt;
    &lt;p&gt;Determinism. This is a key word in computer science and it is here too. If you start with the same initial state and apply the same input, you will always get the same result. Predictable and reproducible.&lt;/p&gt;
    &lt;p&gt;Many multi-player games use determinism to effectively decouple the relationship of bigger game states from the data the server must sync across clients.&lt;/p&gt;
    &lt;p&gt;Real-time strategy games are a poster child for this. Rather than send the positions of thousands of units, projectiles, unit health, etc to clients, you would send only player input data which the client can use to derive the game state at a given point in time. Add a little bit of client side prediction, rollback, and the like and you get a smooth game experience, usually.&lt;/p&gt;
    &lt;p&gt;But this requires the client to be able to simulate the game and not all clients are fast. How can this run everywhere a browser does if I want to simulate millions of particles?&lt;/p&gt;
    &lt;p&gt;I know what the ol'gafferongames would say right now. Even if I didn't use determinism I'd still need to simulate SOMETHING on the client. If I want to send millions of particle positions to the client, surely I'd need to send at least the positions right? I could derive the velocities and quantize the crap out of everything too. I am sure there are other tricks I don't remember from his excellent blog series on game state networking code.&lt;/p&gt;
    &lt;p&gt;I still call that cheating too as the client has to simulate something and it may not scale to millions of particles.&lt;/p&gt;
    &lt;p&gt;I have another idea. I can decouple the simulation size much like determinism does by taking a hint from graphics programming.&lt;/p&gt;
    &lt;p&gt;Way back when in the days of Doom 3 when Mr. Carmack was in his prime, games would calculate lighting based on building shadow geometry from polygons. This was done in Doom 3 and looked amazing. However, as you increased polygon count, the cost of shadows would go up.&lt;/p&gt;
    &lt;p&gt;The industry then figured out how to decouple polygon count from lighting by using a graphics buffer and deferred shading. The details are heavy but the important part is that the cost of lighting is no longer proportional to number of polygons in the scene. Instead, it is based on a ‚Äúfixed‚Äù g-buffer size. The buffer is proportional to the render resolution.&lt;/p&gt;
    &lt;p&gt;This is why 4k is so expensive to render and the games industry has stoked on AI upscaling and frame interpolation. Fewer pixels means faster rendering. Geometry is making a come back with slick virtualization too but I digress. The important part is that deferred shading decouples polygons count from lighting.&lt;/p&gt;
    &lt;p&gt;Well, what if, I went full SSR and did all the rendering on the server sending the simulation frames to the clients? All a client needs to do is just, play a video onto an html canvas element. I would still need to send the input to the server but the size of the simulation wouldn't matter. The cost would be fixed to the resolution of the client.&lt;/p&gt;
    &lt;p&gt;I could have a billion particles and the data the client needs would remain the same. Now, if there were only a few thousand particles it wouldn't be worth the trade off.&lt;/p&gt;
    &lt;p&gt;How do I know it is worth it though?&lt;/p&gt;
    &lt;p&gt;I want to simulate 1 million particles at HD resolution so 1920x1080 pixels. There are 4 bytes per pixel rgba but I only need rgb. Actually, I only need one byte per pixel since that is how the previous particle simulations in the other languages worked. That means I am sending 2,073,600 bytes per frame or a little over 2mb. Oof. That is 120mb/s at 60 fps.&lt;/p&gt;
    &lt;p&gt;If I were to compress the frames they could drop down to a fraction of that. H264 or H265 (if I pay) could cut that down to 260kb per frame. I could also send only 24 frames/s making it pretty close to streaming regular video content.&lt;/p&gt;
    &lt;p&gt;Note were are talking full bytes here not bits.&lt;/p&gt;
    &lt;p&gt;If I were to send the particle data instead, each particle is made up of an x, y, dx, and dy. They are floating point numbers at 4 bytes each so a particle takes up 16 bytes. I could, derive the dx and dy so I will say 8 bytes per particle. That makes it 8,000,000 bytes or 8mb. 4x as big as the raw frame buffer. I am sure with a bit of quantization that could be brought down more and I wouldn't need to send the data 60/s but what if I want 10 million particles? The frame buffer starts to look more appealing.&lt;/p&gt;
    &lt;p&gt;The other thing that is nice here is that by sending the frames, the complexity drops. I don't need to worry about prediction or interpolation on the client. This wouldn't be great for a twitch shooter but they also don't sync millions of data points either.&lt;/p&gt;
    &lt;p&gt;No lossy compression. A particle simulation like this gets destroyed by lossy compression due to how dense the information is. There is little repitition in the data so HEVC or other lossy codecs will wreck it. Lossy compression is out of the question. It HAS to be lossless.&lt;/p&gt;
    &lt;p&gt;For example, look at this image, see all that noise? That doesn't compress well especially in motion.&lt;/p&gt;
    &lt;p&gt;Additionally, compression isn't free. If I want to scale to hundreds of clients, I cannot spend all my time compressing data. This is an important consideration.&lt;/p&gt;
    &lt;p&gt;I am going to use TCP via websockets. For low latency realtime apps UDP would be better as it allows unordered messages preventing head of line blocking. However, it is significantly more complicated and not well supported on the web. TCP's guaranteed ordering of messages will cause some lag spikes but alas it is the best the web has to offer. QUIC is a thing however it only helps the blocking when there are multiple connections which this won't have.&lt;/p&gt;
    &lt;p&gt;TCP websockets it is. Onwards!&lt;/p&gt;
    &lt;p&gt;My end game is to have a configurable play area where each client has its own view into the world. The best starting point would be to have a single view that is sent to all the clients.&lt;/p&gt;
    &lt;p&gt;The simulation itself is pretty simple. A particle struct distributing updates to many go routines. I knew that I couldn't write to a buffer while sending it to a client so I setup double buffering of the frames. A &lt;code&gt;ticker&lt;/code&gt; is the way to get a game loop going though I am not a fan of it.&lt;/p&gt;
    &lt;p&gt;Here is the gist.&lt;/p&gt;
    &lt;code&gt;type Particle struct {
  x  float32
  y  float32
  dx float32
  dy float32
}

type Input struct {
  X           float32
  Y           float32
  IsTouchDown bool
}

type SimState struct {
  dt     float32
  width  uint32
  height uint32
}

// setup variables

func startSim() {
  // setup code

  for range ticker.C {
    // metadata code

    wg.Add(numThreads)
    // no locking, that is fine.
    numClients := len(clients)
    const friction = 0.99

    for i := 0; i &amp;lt; numThreads; i++ {
      go func(threadID int) {
        defer wg.Done()

        startIndex := threadID * particlesPerThread
        endIndex := startIndex + particlesPerThread

        if threadID == numThreads-1 {
          endIndex = particleCount
        }

        for p := startIndex; p &amp;lt; endIndex; p++ {
          for i := 0; i &amp;lt; numClients; i++ {
            input := inputs[i]
            if input.IsTouchDown {
              // apply gravity
            }
          }

          particles[p].x += particles[p].dx
          particles[p].y += particles[p].dy
          particles[p].dx *= friction
          particles[p].dy *= friction

          // bounce if outside bounds
        }
      }(i)
    }

    // wait for them to complete
    wg.Wait()

    framebuffer := getWriteBuffer()
    copy(framebuffer, bytes.Repeat([]byte{0}, len(framebuffer)))
    for _, p := range particles {
      // build frame buffer
    }
    swapBuffers()

    go func(data []byte) {
      // Non-blocking send: if the channel is full, the oldest frame is dropped
      select {
      case fameChannel &amp;lt;- framebuffer:
      default:
        // Channel is full, drop the frame
      }
    }(framebuffer)
  }
}
&lt;/code&gt;
    &lt;p&gt;A few notes. The simulation is basic. Particles can be pulled around by players slowing down by some friction value.&lt;/p&gt;
    &lt;p&gt;I want to avoid locking as much as possible. This means I am going to try and create a fixed amount of memory based the max number of clients I can handle then use the clients index as a fast way to access their respective data, in this case input data.&lt;/p&gt;
    &lt;p&gt;I also want to avoid writing a frame buffer per client as I know that writing to the frame buffer is not cache friendly and should only be done once. As a matter of fact, building the frame buffer is the most expensive part. I learned this from previous languages. Cache locality and all that.&lt;/p&gt;
    &lt;p&gt;One thing I learned about go is that it can be verbose. Not java levels but it is pretty lengthy.&lt;/p&gt;
    &lt;p&gt;I will skip the boiler plate code.&lt;/p&gt;
    &lt;code&gt;func main() {
  go startSim()

  http.HandleFunc("/ws", wsHandler)
  // for webpage
  http.Handle("/", http.FileServer(http.Dir("./public")))

  log.Println("Server started on :8080")
  if err := http.ListenAndServe(":8080", nil); err != nil {
    log.Fatal("ListenAndServe:", err)
  }
}
&lt;/code&gt;
    &lt;p&gt;Initially, each websocket spins up their own ticker writing the latest frame buffer to the client. This is ugly and has sync issues but as a first version it is fine.&lt;/p&gt;
    &lt;code&gt;func wsHandler(w http.ResponseWriter, r *http.Request) {
  // boiler plate

  clientsMu.Lock()
  // safely add client
  clientsMu.Unlock()

  defer func() {
    // safely clean up
  }()

  go func() {
    // wait for messages
    var input Input
    err := binary.Read(bytes.NewReader(message), binary.LittleEndian, &amp;amp;input)

    // maybe later we lock or figure out way to not need a lock in a hot path
    // this is fine as only chance of bad data is if someone connect or drops whilst updating input
    idx := findClientIndex(conn)
    if idx != -1 &amp;amp;&amp;amp; idx &amp;lt; maxClients {
      inputs[idx] = input
    }
  }()

  ticker := time.NewTicker(time.Second / 30)
  for range ticker.C {
    data := getReadBuffer()
    if err := conn.WriteMessage(websocket.BinaryMessage, data); err != nil {
      log.Println("Write failed:", err)
      break
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The client index is pretty silly but I really don't want to lock the hot path here.&lt;/p&gt;
    &lt;p&gt;I'll skip over the client javascript/html. It isn't that interesting, setup a web socket, write data to a canvas element using &lt;code&gt;setPixels&lt;/code&gt; the usually.&lt;/p&gt;
    &lt;p&gt;So, how well does it work? Pretty well. But it is not fast, and is writing a shit load of data. It also sometimes flickers when the tickers touch the frame buffer at the same time the simulation thread does.&lt;/p&gt;
    &lt;p&gt;Still, a good starting point.&lt;/p&gt;
    &lt;p&gt;I used &lt;code&gt;pprof&lt;/code&gt; to run some samples to see where things are slow. I noticed that time was spent creating go routines. While light they are not free. The idea is to have go routines listening in on channels. I refactored the simulation to wait for &lt;code&gt;SimJob&lt;/code&gt; requests to come in.&lt;/p&gt;
    &lt;code&gt;type SimJob struct {
  startIndex int
  endIndex   int
  simState   SimState
  inputs     [maxClients]Input
  numClients int
}

// other code

jobs := make(chan SimJob, numThreads)
var wg sync.WaitGroup
for i := 0; i &amp;lt; numThreads; i++ {
  go worker(jobs, &amp;amp;wg)
}

// in sim loop
for i := 0; i &amp;lt; numThreads; i++ {
  startIndex := i * particlesPerThread
  endIndex := startIndex + particlesPerThread
  if i == numThreads-1 {
    endIndex = particleCount
  }
  jobs &amp;lt;- SimJob{startIndex, endIndex, simState, &amp;amp;inputs, numClients, &amp;amp;framebuffer}
}
&lt;/code&gt;
    &lt;p&gt;I also reused frame buffers with a &lt;code&gt;pool&lt;/code&gt; and then fixed the syncing by pushing to a channel which another thread would listen on. This thread would then write the new frame out to all the clients before adding it back to the pool. At first I used a simple for loop but that made it write as fast as the slowest client so instead I had it prep the frame and then push it to ANOTHER set of channels per client. Threads on each of those channels would then actually write data to the client.&lt;/p&gt;
    &lt;code&gt;func broadcastFrames(ch &amp;lt;-chan *Frame, pool *sync.Pool) {
  for {
    frame := &amp;lt;-ch
    // setup data
    clientsMu.Lock()
    for i, conn := range clients {
      // send to other channel
      select {
      case clientSendChannelMap[conn] &amp;lt;- dataToSend:
      default:
        log.Printf("Client %d's channel is full, dropping frame. Requesting full frame.", i)
      }
    }
    clientsMu.Unlock()
    pool.Put(frame)
  }
}

// other code
func writePump(conn *websocket.Conn) {
  var channel = clientSendChannelMap[conn]
  for {
    message, ok := &amp;lt;-channel
    if !ok {
      return
    }

    if err := conn.WriteMessage(websocket.BinaryMessage, message); err != nil {
      log.Printf("Write to client failed: %v", err)
      return
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;I pump when a client connects and stop when they disconnect. This works pretty well and fixes the sync issue.&lt;/p&gt;
    &lt;p&gt;I experimented with compression a bit using the standard &lt;code&gt;flate&lt;/code&gt; lib. It worked but was pretty slow. I then experimented with every kind of encoding I could think of to reduce the data transferred.&lt;/p&gt;
    &lt;p&gt;I tried a bit mask where I would sent 0/1 flags per pixel followed by the in order pixel data that changed. This was fast per client but ended up being larger than the raw frame data when particles started moving around.&lt;/p&gt;
    &lt;p&gt;I tried run length encoding, variable length encoding, and delta encoding all combined in different ways.&lt;/p&gt;
    &lt;p&gt;There are some gotchas I ran into For RLE to work you need a sentinel byte to know if there is a run of data. This drops the available space per pixel. It could be possible to encode multiple pixels at a time but that didn't work that well. VLE is just run length using varints to drop the size down for small numbers this worked and did help reduce size by almost 8% on average.&lt;/p&gt;
    &lt;p&gt;Delta encoding was the most fun. The way I tried it was to delta encode between frames. That way passing the delta into RLE would massively compress the data. The issue is that particle counts can go up or down per pixel. This required &lt;code&gt;zigzag&lt;/code&gt; encoding but there was an issue.&lt;/p&gt;
    &lt;p&gt;If each pixel is one byte, and the byte can go from 0 to 255 in a single frame, then the size of the deltas would need to go from -255 to +255 which doesn't fit in a byte. The &lt;code&gt;zigzag&lt;/code&gt; encoding worked and I figured I could just drop down to 7-bits per pixel.&lt;/p&gt;
    &lt;p&gt;I read about zigzag encoding from a microsoft paper on optimized compression for depth data from multiple connect sensors. I don't remember the name. To save you some time, just ask AI about it.&lt;/p&gt;
    &lt;code&gt;func CreateDeltaBuffer(oldBuffer, newBuffer []byte) []byte {
  if len(oldBuffer) != len(newBuffer) {
    return newBuffer
  }
  deltaBuffer := make([]byte, len(newBuffer))

  for i := 0; i &amp;lt; len(newBuffer); i++ {
    diff := int16(newBuffer[i]) - int16(oldBuffer[i])
    deltaBuffer[i] = zigZagEncode(int8(diff))
  }

  return deltaBuffer
}
&lt;/code&gt;
    &lt;p&gt;And it works. Unless the whole frame was moving around, the size was ~30% smaller than the raw frame data. If nothing moved, it would send only a few bytes! But again, there were problems.&lt;/p&gt;
    &lt;p&gt;The code is complicated. I had to track if I was sending delta frames or full frames, and if a client dropped a frame, I'd need to send a full frame again. This would cause a brief flash on the screen. I could add frame counting logic so the client could drop out of sync deltas but the code was already a bit much.&lt;/p&gt;
    &lt;code&gt;func broadcastFrames(ch &amp;lt;-chan *Frame, pool *sync.Pool) {
  for {
    frame := &amp;lt;-ch
    fullFrameBuffer.Reset()
    fullFrameBuffer.WriteByte(OpCodeFullFrame)
    fullFrameBuffer.Write(frame.FullBuffer)
    fullFrameBytes := fullFrameBuffer.Bytes()

    deltaFrameBuffer.Reset()
    deltaFrameBuffer.WriteByte(OpCodeDeltaFrame)
    deltaFrameBuffer.Write(frame.Delta)
    deltaFrameBytes := deltaFrameBuffer.Bytes()

    clientsMu.Lock()
    for i, conn := range clients {

      var dataToSend []byte
      if clientFullFrameRequestFlags[i] {
        dataToSend = fullFrameBytes
        clientFullFrameRequestFlags[i] = false
      } else {
        dataToSend = deltaFrameBytes
      }

      select {
      case clientSendChannelMap[conn] &amp;lt;- dataToSend:
      default:
        log.Printf("Client %d's channel is full, dropping frame. Requesting full frame.", i)
        clientFullFrameRequestFlags[i] = true
      }
    }
    clientsMu.Unlock()
    pool.Put(frame)
  }
}
&lt;/code&gt;
    &lt;p&gt;It does work though.&lt;/p&gt;
    &lt;p&gt;However, I want each client to have their own ‚Äúview‚Äù into a bigger frame where the can pan around the world fighting for particles. For this to work, with the current setup, I'd need to track delta frames per client and figure out how to handle dropped delta frames better.&lt;/p&gt;
    &lt;p&gt;Back to the drawing board.&lt;/p&gt;
    &lt;p&gt;Stupid simple, is simple stupid.&lt;/p&gt;
    &lt;p&gt;I am going to have 1 bit per pixel. That means only a single ‚Äúluminance‚Äù value. In the previous video I was only effectively using a few bits on the client having only about 8 ‚Äúluminance‚Äù values. Bet you thought it was more right?&lt;/p&gt;
    &lt;p&gt;This will simplify everything. The only hard part is packing and unpacking the data. I also figured I could implement client camera state too. I waffled a bit around how to store this state and update it.&lt;/p&gt;
    &lt;p&gt;Does the client sent their camera position directly? What if the window resizes? What if the client sends a negative camera size? What if they send an infinite one? How do I prevent blocking?&lt;/p&gt;
    &lt;p&gt;I ended up copying what I did for the input data.&lt;/p&gt;
    &lt;code&gt;type ClientCam struct {
  X      float32
  Y      float32
  Width  int32
  Height int32
}
var (
  inputs     [maxClients]Input
  cameras    [maxClients]ClientCam
)
&lt;/code&gt;
    &lt;p&gt;I would have the client send the camera data with the existing input message.&lt;/p&gt;
    &lt;code&gt;  for {
    mt, message, err := conn.ReadMessage()
    if err != nil {
      log.Println("Read failed:", err)
      return
    }
    if mt == websocket.BinaryMessage {
      reader := bytes.NewReader(message)
      var touchInput Input
      // interpret x and y as offsets.
      var camInput ClientCam
      // read input
      errInput := binary.Read(reader, binary.LittleEndian, &amp;amp;touchInput)
      errCam := binary.Read(reader, binary.LittleEndian, &amp;amp;camInput)
      // set client input/camera state
      // handle errors in the input, bounds, negatives etc.
    }
  }
&lt;/code&gt;
    &lt;p&gt;For sending frame data, at first, I naively packed the buffer and then unpacked it again based on the region a client was rendering.&lt;/p&gt;
    &lt;code&gt;// in simulation thread
for _, p := range particles {
  x := int(p.x)
  y := int(p.y)
  if x &amp;gt;= 0 &amp;amp;&amp;amp; x &amp;lt; int(simState.width) &amp;amp;&amp;amp; y &amp;gt;= 0 &amp;amp;&amp;amp; y &amp;lt; int(simState.height) {
    idx := (y*int(simState.width) + x)
    if idx &amp;lt; int(simState.width*simState.height) {
      byteIndex := idx / 8
      bitOffset := idx % 8
      if byteIndex &amp;lt; len(framebuffer) {
        framebuffer[byteIndex] |= (1 &amp;lt;&amp;lt; bitOffset)
      }
    }
  }
}

// in frame send channel before sending to client 
// get client camera info
for row := int32(0); row &amp;lt; height; row++ {
  for col := int32(0); col &amp;lt; width; col++ {
    mainFrameIndex := ((y+row)*int32(simState.width) + (x + col))
    dataToSendIndex := (row*width + col)

    if mainFrameIndex/8 &amp;lt; int32(len(frameBuffer)) &amp;amp;&amp;amp; dataToSendIndex/8 &amp;lt; int32(len(dataToSend)) {
      isSet := (frameBuffer[mainFrameIndex/8] &amp;gt;&amp;gt; (mainFrameIndex % 8)) &amp;amp; 1
      if isSet == 1 {
        dataToSend[dataToSendIndex/8] |= (1 &amp;lt;&amp;lt; (dataToSendIndex % 8))
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;This makes HD only a few hundred kb/frame.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pprof&lt;/code&gt; has a niffy way of downloading a simple of the running up you can then load and look at with their cli tool.&lt;/p&gt;
    &lt;p&gt;I checked a sample using &lt;code&gt;pprof&lt;/code&gt; and noticed the sim was slow when it was building the frame buffer so I moved the frame building to the simulation workers. This is possible now because each thread will only mark a bit as on if there is a particle there. Which means I don't need to lock anything and can let the last writer win.&lt;/p&gt;
    &lt;p&gt;Before&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 35
      flat  flat%   sum%        cum   cum%
    16.75s 52.54% 52.54%     17.05s 53.48%  main.worker
     6.42s 20.14% 72.68%      6.49s 20.36%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;And after&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 36
      flat  flat%   sum%        cum   cum%
    13.01s 46.46% 46.46%     13.28s 47.43%  main.worker
     6.98s 24.93% 71.39%      7.08s 25.29%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;The simulation is a bit faster but notice that the broadcast frames is slow. You know that packing I am doing? Ya, that ended up being very very slow. 25% of the time is spent broadcasting frames to a few clients due to all the bit opps packing and unpacking.&lt;/p&gt;
    &lt;p&gt;A simple fix is to use full bytes even if the count is only ever 0-1 and then using a lookup map to skip the bit opps like so.&lt;/p&gt;
    &lt;code&gt;var uint64ToByteLUT = make(map[uint64]byte)

func init() {
  var byteSlice = make([]byte, 8)
  for i := 0; i &amp;lt; 256; i++ {
    for bit := 0; bit &amp;lt; 8; bit++ {
      if (i&amp;gt;&amp;gt;bit)&amp;amp;1 == 1 {
        byteSlice[bit] = 1
      } else {
        byteSlice[bit] = 0
      }
    }
    // trust me bro
    uint64ToByteLUT[BytesToUint64Unsafe(byteSlice)] = byte(i)
  }
}

for row := int32(0); row &amp;lt; height; row++ {
  yOffset := (y + row) * int32(simState.width)
  for col := int32(0); col &amp;lt; width; col += 8 {
    fullBufferIndex := yOffset + (x + col)
    chunk := frameBuffer[fullBufferIndex : fullBufferIndex+8]
    key := BytesToUint64Unsafe(chunk)

    packedByte, _ := uint64ToByteLUT[key]

    if outputByteIndex &amp;lt; int32(len(dataToSend)) {
      dataToSend[outputByteIndex] = packedByte
      outputByteIndex++
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Now slicing the frame region for a client is almost free. The idea is pretty simple, interpret 8 bytes at a time as a uint64 which maps to the corresponding single byte with the right bits set. I did something similar on the client too to make unpacking faster. The extra memory here is pretty small and worth it to me.&lt;/p&gt;
    &lt;p&gt;Not bad. I am sure more can be done but the main sim is taking up the majority of the time.&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 49
      flat  flat%   sum%        cum   cum%
    15.62s 58.85% 58.85%     15.83s 59.65%  main.worker
     4.66s 17.56% 76.41%      4.66s 17.56%  runtime.pthread_cond_wait
     2.96s 11.15% 87.57%      2.96s 11.15%  runtime.pthread_cond_signal
     0.90s  3.39% 90.96%      0.90s  3.39%  runtime.kevent
     0.47s  1.77% 92.73%      0.80s  3.01%  runtime.mapaccess2_fast64
     0.36s  1.36% 94.08%      1.17s  4.41%  main.broadcastFrames
     0.20s  0.75% 94.84%      0.20s  0.75%  runtime.asyncPreempt
     0.17s  0.64% 95.48%      0.17s  0.64%  runtime.madvise
     0.17s  0.64% 96.12%      0.17s  0.64%  runtime.pthread_kill
     0.17s  0.64% 96.76%      0.17s  0.64%  runtime.usleep
&lt;/code&gt;
    &lt;p&gt;On the topic of memory, with millions of particles the server barely breaks over 100mb. Not rust good but node would be well over a few gigs and I cannot imagine python would be much lighter.&lt;/p&gt;
    &lt;p&gt;I waffled about again for well over a day trying to get some go assembly to work for those sweet simd gains but it wasn't to be. There are some libraries I tried too but none of them did better than what I already had. There are some interesting things with go's compiler around how to optimize out dereferences and bounds checking which did give me a 30% boost.&lt;/p&gt;
    &lt;p&gt;For example, before I was updating the particles like so&lt;/p&gt;
    &lt;code&gt;for i := job.startIndex; i &amp;lt; job.endIndex; i++ {
  particles[i].x += particles[i].dx
  // etc
&lt;/code&gt;
    &lt;p&gt;But by doing this&lt;/p&gt;
    &lt;code&gt;for i := job.startIndex; i &amp;lt; job.endIndex; i++ {
  p := &amp;amp;particles[i]
  p.x += p.dx
&lt;/code&gt;
    &lt;p&gt;Big boost. It makes sense. Bounds checking was happening way too often. I did a few more micro optimizations which helped but nothing significant. I did try SoA too and that actually did worse than AoS. Go figure.&lt;/p&gt;
    &lt;p&gt;I tried to use a fast inverse square root function but for some reason the go version of it didn't work at all which is weird because js lands had no issue. Oh well.&lt;/p&gt;
    &lt;p&gt;The previous profile samples were taken simulating a dozen or so clients with a few million particles. If I bump that up to 20m particles, I get this.&lt;/p&gt;
    &lt;code&gt;Showing top 10 nodes out of 21
      flat  flat%   sum%        cum   cum%
   113.16s 93.06% 93.06%    115.22s 94.75%  main.worker
     2.27s  1.87% 94.93%      2.27s  1.87%  runtime.pthread_cond_wait
     2.04s  1.68% 96.60%      2.04s  1.68%  runtime.asyncPreempt
     1.23s  1.01% 97.62%      1.23s  1.01%  runtime.pthread_cond_signal
     0.63s  0.52% 98.13%      1.56s  1.28%  runtime.mapaccess2_fast64
     0.30s  0.25% 98.38%      1.92s  1.58%  main.broadcastFrames
&lt;/code&gt;
    &lt;p&gt;The simulation is the clear bottle neck and without simd, I am not sure how much better I can get it.&lt;/p&gt;
    &lt;p&gt;It works though. Time to deploy it to the cloud.&lt;/p&gt;
    &lt;p&gt;I was going to use a digital ocean docklet but it would cost more than an Equinox gym membership for anything faster than a toaster. I ended up spending ~$8/month at netcup for a 16 gig 10 core arm vm Manassas, Virginia with a ping of over 300ms. Fantastic. Not sure why the DNS on their panel says germany though.&lt;/p&gt;
    &lt;p&gt;I opened a few ports under root like the psychopath I and gave it a spin.&lt;/p&gt;
    &lt;p&gt;Pretty slick. More clients barely impacts cpu usage. But will it scale to 1k clients? I am not sure.&lt;/p&gt;
    &lt;p&gt;The netcup vps has a 2.5 Gbit pipe. At a 1920x1080 resolution with 1 bit per pixel at a rate of 30 frames/s down the pipe, that should theoretically support well over 300 clients. Now, if these were mobile devices with a fraction of that resolution say 390x844 or so, (we are not talking native), it may actually scale.&lt;/p&gt;
    &lt;p&gt;It is worth noting it does suffer from head of line blocking at times especially as the resolution gets cranked up. Clients with a resolution larger than the max particle grid are ignored so your 5k display won't work.&lt;/p&gt;
    &lt;p&gt;I did a bit of polishing on the client, edge scrolling, panning, mobile support, etc but this post isn't about javascript it is about golang.&lt;/p&gt;
    &lt;p&gt;Go is able to simulate 2.5 million particles at 60fps while sending data at 30 fps to in theory over 300 clients maybe even a thousand. And because this happens on the server it even runs on a smart tv. Don't believe me? Just visit, howfastisgo.dev and see for yourself or watch the video below.&lt;/p&gt;
    &lt;p&gt;While go certainly isn't a compute workhorse, that is pretty impressive. Anywhere a browser runs this works! Go is just THAT fast.&lt;/p&gt;
    &lt;p&gt;This is just so cursed. I love it.&lt;/p&gt;
    &lt;p&gt;Until next time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dgerrells.com/blog/how-fast-is-go-simulating-millions-of-particles-on-a-smart-tv"/><published>2025-09-24T18:48:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45365878</id><title>SonyShell ‚Äì An effort to ‚ÄúSSH into my Sony DSLR‚Äù</title><updated>2025-09-25T00:45:41.334580+00:00</updated><content>&lt;doc fingerprint="34960c72d72e66ed"&gt;
  &lt;main&gt;
    &lt;p&gt;A Linux-only helper built on Sony‚Äôs official Camera Remote SDK. It connects to a Sony A6700 camera over Wi-Fi/Ethernet, listens for new photos, downloads them automatically, and can optionally run a script on each downloaded file.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Auto-connect via enumeration or direct IP/MAC.&lt;/item&gt;
      &lt;item&gt;Watches for new capture events and fetches the newest files.&lt;/item&gt;
      &lt;item&gt;Saves into a chosen directory with unique filenames.&lt;/item&gt;
      &lt;item&gt;Post-download hook: run any executable/script with the saved file path as argument.&lt;/item&gt;
      &lt;item&gt;Keepalive mode: auto-retry on startup failure or after disconnects.&lt;/item&gt;
      &lt;item&gt;Cleaned, Linux-only code (no Windows ifdefs, simpler logging).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./sony-remote --dir /photos [options]&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--dir &amp;lt;path&amp;gt;&lt;/code&gt;: Directory to save files (required in most real setups).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--ip &amp;lt;addr&amp;gt;&lt;/code&gt;: Connect directly by IPv4 (e.g.&lt;code&gt;192.168.10.184&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--mac &amp;lt;hex:mac&amp;gt;&lt;/code&gt;: Optional MAC (e.g.&lt;code&gt;10:32:2c:2a:1a:6d&lt;/code&gt;) for direct IP.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--cmd &amp;lt;path&amp;gt;&lt;/code&gt;: Executable/script to run after each download, invoked as&lt;code&gt;cmd /photos/DSC01234.JPG&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--keepalive &amp;lt;ms&amp;gt;&lt;/code&gt;: Retry interval when offline or after disconnect.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-v&lt;/code&gt;,&lt;code&gt;--verbose&lt;/code&gt;: Verbose property-change logging.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Enumerate + keep retrying every 2s, run a hook after each file:&lt;/p&gt;
    &lt;code&gt;./sony-remote --dir /photos --keepalive 2000 --cmd /usr/local/bin/ingest-photo&lt;/code&gt;
    &lt;p&gt;Direct IP connect, verbose logs, retry every 3s:&lt;/p&gt;
    &lt;code&gt;./sony-remote --ip 192.168.10.184 --mac 10:32:2c:2a:1a:6d --dir /photos -v --keepalive 3000&lt;/code&gt;
    &lt;p&gt;Requires Linux, g++, and the Sony Camera Remote SDK.&lt;/p&gt;
    &lt;p&gt;See INSTALL.md&lt;/p&gt;
    &lt;p&gt;or (untested)&lt;/p&gt;
    &lt;code&gt;g++ -std=c++17 sony-a6700-remote-cleaned.cpp \
    -I/path/to/CrSDK/include \
    -L/path/to/CrSDK/lib -lCameraRemoteSDK \
    -lpthread -o sony-remote&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Connect to the camera (via IP or enumeration). Stores/reuses SDK fingerprint under &lt;code&gt;~/.cache/sonshell/&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Wait for notifications: when the camera signals new contents, spawn a download thread.&lt;/item&gt;
      &lt;item&gt;Download newest files to &lt;code&gt;--dir&lt;/code&gt;. Safe naming ensures no overwrite (&lt;code&gt;file_1.jpg&lt;/code&gt;, etc.).&lt;/item&gt;
      &lt;item&gt;Hook: if &lt;code&gt;--cmd&lt;/code&gt;is set, fork/exec the script with the saved path.&lt;/item&gt;
      &lt;item&gt;Reconnect on errors/disconnects if &lt;code&gt;--keepalive&lt;/code&gt;is set.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built on/for Ubuntu 24.04&lt;/item&gt;
      &lt;item&gt;It uses Sony's official Camera Remote SDK (not included here).&lt;/item&gt;
      &lt;item&gt;See DOCS.md for a deep dive into the internals.&lt;/item&gt;
      &lt;item&gt;I leaned heavily on ChatGPT while creating this, so please don't mind the mess! ;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sony Camera Remote SDK: https://support.d-imaging.sony.co.jp/app/sdk/en/index.html&lt;/item&gt;
      &lt;item&gt;See LICENSE for licensing details.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/goudvuur/sonyshell"/><published>2025-09-24T21:00:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45366474</id><title>Snapdragon X2 Elite ARM Laptop CPU</title><updated>2025-09-25T00:45:41.120317+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qualcomm.com/products/mobile/snapdragon/laptops-and-tablets/snapdragon-x2-elite"/><published>2025-09-24T22:01:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45366566</id><title>Everything that's wrong with Google Search in one image</title><updated>2025-09-25T00:45:41.052271+00:00</updated><content/><link href="https://bitbytebit.substack.com/p/everything-thats-wrong-with-google"/><published>2025-09-24T22:11:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45366867</id><title>Helium Browser</title><updated>2025-09-25T00:45:40.422517+00:00</updated><content>&lt;doc fingerprint="170e82509195b1e4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Internet without interruptions&lt;/head&gt;
    &lt;p&gt;Best privacy and unbiased ad-blocking by default. Handy features like native !bangs and split view. No adware, no bloat, no noise. People-first and fully open source.&lt;/p&gt;
    &lt;head rend="h1"&gt;Best privacy by default, not as a hidden option&lt;/head&gt;
    &lt;p&gt;Helium blocks ads, trackers, fingerprinting, third-party cookies, cryptominers, and phishing websites by default thanks to preinstalled uBlock Origin. No extra steps are needed, and there are no biased exceptions √¢ unlike other browsers. &lt;lb/&gt; The browser itself doesn't have any ads, trackers, or analytics. Helium also doesn't make any web requests without your explicit consent, it makes zero web requests on first launch. &lt;lb/&gt; Not enough? Increase privacy even further with ungoogled-chromium flags or uBlock Origin filters. You're finally at the steering wheel of your privacy on the Internet √¢ not in a toy car, but in a real race car. &lt;lb/&gt; We will always stand by our promise of the best privacy and will never prioritize profit over people, unlike big corporations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Respectful by design&lt;/head&gt;
    &lt;p&gt;Helium doesn't annoy you with anything and never will. It doesn't do anything without your consent: no unprovoked tabs about updates or sponsors, no persistent popups telling you about features you don't care about, no weird restarts. &lt;lb/&gt; Nothing interrupts you, jumps in your face, or breaks your flow. Everything just makes sense. You're in full control.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast, efficient, and light&lt;/head&gt;
    &lt;p&gt;Helium is based on Chromium, the fastest and most optimized browser yet. Helium builds on this base to improve performance and save even more energy. You will notice a difference after using Helium for a day. It doesn't slow down over time. &lt;lb/&gt; All bloat is removed: Helium is one of the lightest modern browsers available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Powerful when you need it&lt;/head&gt;
    &lt;p&gt;Open pages side-by-side with split view to get even more things done at once. Quickly copy page links with √¢+Shift+C and share your discoveries with ease. Install any web apps and use them as standalone desktop apps without duplicating Chromium.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed to get out of your way&lt;/head&gt;
    &lt;p&gt;Helium's interface is compact and minimalistic, but it doesn't compromise on beauty or functionality. More web content fits on the screen at once, and the browser interface doesn't get in your way. You can hide everything extra from the toolbar if it annoys you. &lt;lb/&gt; Helium is built with attention to detail. Nothing jiggles or flickers abnormally. Your actions aren't throttled or stopped by lag. Everything's fast, smooth, and simple. Comfort and simplicity are among our top priorities.&lt;/p&gt;
    &lt;head rend="h2"&gt;Works with all Chromium extensions, privately&lt;/head&gt;
    &lt;p&gt;All Chromium extensions are supported and work right away, by default, including all MV2 extensions. We'll keep support for MV2 extensions for as long as possible. &lt;lb/&gt; Helium anonymizes all internal requests to the Chrome Web Store via Helium services. Thanks to this, Google can't track your extension downloads or target ads using this data. No other browser does this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Free and fully open-source&lt;/head&gt;
    &lt;p&gt;All parts of the Helium browser are open source, including online services. You can self-host Helium services and use your own instance in your browser. &lt;lb/&gt; Everything is available on GitHub. No exceptions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Always safe and sound&lt;/head&gt;
    &lt;p&gt;We release new Chromium updates (such as security patches) as soon as possible. Your browser will always be safe and up to date. &lt;lb/&gt; Helium updates itself automatically on macOS, with auto-updating options available on Linux and Windows. &lt;lb/&gt; All builds are available on GitHub, and you can even make one yourself. The choice is yours!&lt;/p&gt;
    &lt;head rend="h2"&gt;Best security practices for everyone, by default&lt;/head&gt;
    &lt;p&gt;Helium enforces HTTPS on all websites and warns you when a website doesn't support it. Passkeys just work. &lt;lb/&gt; There's no built-in password manager. Passwords should be separate from a web browser to be truly secure and immutable. &lt;lb/&gt; There's also no cloud-based history/data sync. You should be the only one with access to your browsing data, not some conglomerate.&lt;/p&gt;
    &lt;head rend="h1"&gt;Browse the Internet faster with !bangs&lt;/head&gt;
    &lt;p&gt;Skip the search engine and go directly to the website you want. Choose from over 13,000 bangs that make the Internet a breeze to browse, such as !w for Wikipedia, !gh for GitHub, and !wa for Wolfram Alpha. &lt;lb/&gt; Want to chat with AI? Just add !chatgpt or any other AI provider name at the start of your query. Helium will start a new chat for you without sending your prompt anywhere else. &lt;lb/&gt; Helium bangs are the fastest and most private implementation of bangs yet. They work offline, directly in your browser. &lt;lb/&gt; Not sure which bang to use? Check out the full list of bangs!&lt;/p&gt;
    &lt;head rend="h1"&gt;The web browser made for people, with love&lt;/head&gt;
    &lt;p&gt;We're making a web browser that we enjoy using ourselves. Helium's main goal is to provide an honest, comfortable, privacy-respecting, and non-invasive browsing experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perfect for developers&lt;/head&gt;
    &lt;p&gt;Helium is based on Chromium and doesn't break any web APIs or standards, despite the focus on privacy. DevTools have been cleaned up and no longer nag you with anything. There's nothing that gets in your way of creating the Internet of the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perfect for everyone on the go&lt;/head&gt;
    &lt;p&gt;Helium's efficiency makes it handy for everyone with their laptop on the go. Split view and quick link copying make it easier than ever to get things done faster. Helium loads pages faster and saves data by blocking ads and other crap.&lt;/p&gt;
    &lt;head rend="h1"&gt;Ready to try Helium?&lt;/head&gt;
    &lt;p&gt;It's never too late to get your internet life back on the right track. Helium can transfer your most important stuff from other browsers in one click. We hope you'll love it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://helium.computer/"/><published>2025-09-24T22:51:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45367044</id><title>I built a CLI to test and eval MCP servers</title><updated>2025-09-25T00:45:40.242032+00:00</updated><content>&lt;doc fingerprint="df9fcfadfd9028c0"&gt;
  &lt;main&gt;
    &lt;p&gt;We built a CLI that performs MCP evals and End to End (E2E) testing. The CLI creates a simulated end user√¢s environment and tests popular user flows.&lt;/p&gt;
    &lt;p&gt;Evals helps you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discover workflows that are breaking your server and get actionable ways on resolving them.&lt;/item&gt;
      &lt;item&gt;Benchmark your server√¢s performance and catch regressions in future changes.&lt;/item&gt;
      &lt;item&gt;Programatically test queries on a MCP server with a command. No more doing QA one by one.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm install -g @mcpjam/cli&lt;/code&gt;
    &lt;p&gt;To set up, create a new folder directory for your test. In that directory, create three files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;environment.json&lt;/code&gt;to set up your MCP server connections&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tests.json&lt;/code&gt;to configure your tests&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;llms.json&lt;/code&gt;to store your LLM API keys&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This file is configured very similar to a &lt;code&gt;mcp.json&lt;/code&gt; file. For servers with OAuth, you must provide your own &lt;code&gt;Bearer&lt;/code&gt; API tokens. MCPJam CLI does not handle OAuth flows / DCR. For bearer tokens, make sure to wrap your header with &lt;code&gt;requestInit&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;{
  "servers": {
    "asana": {
      "url": "https://mcp.asana.com/sse",
      "requestInit": {
        "headers": {
          "Authorization": "Bearer &amp;lt;ASANA_API_KEY&amp;gt;"
        }
      }
    },
    "sequential-thinking": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"],
      "env": {
        "ENV_1": "&amp;lt;ENV_1&amp;gt;"
      }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;The test file is an array of tests.&lt;/p&gt;
    &lt;code&gt;[
  {
    "title": "Workspace test",
    "query": "What is my asana workspace?",
    "runs": 1, // Number of times to run this test
    "model": "anthropic/claude-3.7-sonnet",
    "provider": "openrouter", // Provider name: "anthropic" | "openai" | "openrouter"
    "expectedToolCalls": ["asana_list_workspaces"]
  },
  {
    "title": "Workspace users test",
    "query": "Can you figure out who is in the workspace?",
    "runs": 1,
    "model": "anthropic/claude-3.7-sonnet",
    "provider": "openrouter",
    "expectedToolCalls": ["asana_list_workspaces", "asana_get_workspace_users"]
  }
]&lt;/code&gt;
    &lt;code&gt;{
  "anthropic": "&amp;lt;ANTHROPIC_API_KEY&amp;gt;",
  "openai": "&amp;lt;OPENAI_API_KEY&amp;gt;",
  "openrouter": "&amp;lt;OPENROUTER_API_KEY&amp;gt;"
}&lt;/code&gt;
    &lt;code&gt;mcpjam evals run --tests tests.json --environment environment.json --llms llms.json&lt;/code&gt;
    &lt;code&gt;mcpjam evals run -t tests.json -e environment.json -l llms.json&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--tests, -t &amp;lt;file&amp;gt;&lt;/code&gt;: Path to the tests configuration file (required)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--environment, -e &amp;lt;file&amp;gt;&lt;/code&gt;: Path to the environment configuration file (required)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--llms, -l &amp;lt;file&amp;gt;&lt;/code&gt;: Path to the LLM API key configuration file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--help, -h&lt;/code&gt;: Show help information&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--version, -V&lt;/code&gt;: Display version number&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.npmjs.com/package/@mcpjam/cli"/><published>2025-09-24T23:12:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45367046</id><title>Do YC after you graduate: Early decision for students</title><updated>2025-09-25T00:45:39.933568+00:00</updated><content>&lt;doc fingerprint="b61d01d108cf1634"&gt;
  &lt;main&gt;
    &lt;p&gt;Apply now, do YC after you graduate. For students who want to finish school before doing YC. Get funded the moment you're accepted.&lt;/p&gt;
    &lt;p&gt;Sneha and Anushka, founders of Spur (S24), applied in Fall 2023 for the S24 batch using Early Decision. This allowed them to graduate in May 2024 and then do YC. They've since raised $4.5M from top investors for their AI-powered QA testing tools.&lt;/p&gt;
    &lt;p&gt;Early Decision lets you apply to YC while you're still in school and reserve your spot in a future batch. For example, you apply in Fall of this year, for a spot in the summer batch of the following year. You submit the same YC application as if you were applying for the upcoming batch. If you're accepted, we'll fund you immediately and hold your place for after you graduate.&lt;/p&gt;
    &lt;p&gt;This program is designed for students who want to finish their degree before starting a company. If you're considering working on your own startup after graduation, Early Decision makes it easy to lock in your spot.&lt;/p&gt;
    &lt;p&gt;Even if you're not completely sure yet if you want to do a startup, you should still apply. There is no downside.&lt;/p&gt;
    &lt;p&gt;Also, if you're not in your final year, you can still apply for Early Decision. You'll be able to finish the school year you're currently in, and then either join a later batch or decide to drop out and start sooner.&lt;/p&gt;
    &lt;p&gt;The most common path is students applying in the fall of their final year and joining the summer batch after graduating in Spring. But you can apply for any batch in the future within reason. The application and interview process is the same as if you were applying for the upcoming batch. Once you're accepted, YC funds you right away and confirms your future batch.&lt;/p&gt;
    &lt;p&gt;When you fill out your YC application, you'll see a question asking which batch you want to apply for. Simply select "A batch after Winter 2026" to indicate you're applying for Early Decision, and tell us which batch you'd like to be considered for.&lt;/p&gt;
    &lt;p&gt;The batch preference question in the YC application&lt;/p&gt;
    &lt;p&gt;Many students want to finish their degree or complete more of their education before starting a company. Also we know that many students spend a lot of time in Fall or during their final year applying for jobs or internships. Early Decision gives students another option: apply to YC and bet on yourself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/early-decision"/><published>2025-09-24T23:12:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45367085</id><title>Docker Hub Is Down</title><updated>2025-09-25T00:45:39.700656+00:00</updated><content>&lt;doc fingerprint="3db760639d6d31d9"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;Docker Hub Registry, Docker Authentication, Docker Hub Web Services, Docker Scout, Docker Build Cloud, Docker Cloud, Docker Hardened Images&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;September 24, 2025 16:19 PDT&lt;lb/&gt;September 24, 2025 23:19 UTC&lt;/p&gt;
        &lt;p&gt;[Identified] We have identified the cause and are working on implementing a fix. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;September 24, 2025 16:09 PDT&lt;lb/&gt;September 24, 2025 23:09 UTC&lt;/p&gt;
        &lt;p&gt;[Investigating] We are investigating elevated error rates for authentication requests on hub.docker.com and the Docker Hub Registry APIs.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dockerstatus.com/pages/incident/533c6539221ae15e3f000031/68d47a2f93c09e05486d93a9"/><published>2025-09-24T23:15:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45367086</id><title>How did sports betting become legal in the US?</title><updated>2025-09-25T00:45:39.626005+00:00</updated><content/><link href="https://shreyashariharan.substack.com/p/how-did-sports-betting-become-legal"/><published>2025-09-24T23:15:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45367364</id><title>Harvard's Public Health Dean Was Paid $150k to Testify Tylenol Causes Autism</title><updated>2025-09-25T00:45:39.434276+00:00</updated><content>&lt;doc fingerprint="2754c8c5b0b30b80"&gt;
  &lt;main&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;p&gt;Harvard Is No Longer Last in FIRE‚Äôs Free Speech Rankings. What‚Äôs Behind the Numbers?&lt;/p&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;p&gt;What Happened to Harvard Business School‚Äôs $25 Million Racial Equity Plan? The School Won‚Äôt Say.&lt;/p&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;p&gt;Harvard Economists Warn of Risks from Trump‚Äôs Economic Policies&lt;/p&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;p&gt;Trump Uses Harvard Public Health School Dean‚Äôs Research to Link Tylenol to Autism&lt;/p&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;p&gt;LGBTQ Student Groups Host Funeral To Mourn QuOffice Closure&lt;/p&gt;
    &lt;p&gt;Updated September 24, 2025, at 11:44 a.m.&lt;/p&gt;
    &lt;p&gt;Harvard School of Public Health Dean Andrea A. Baccarelli received at least $150,000 to testify against Tylenol‚Äôs manufacturer in 2023 ‚Äî two years before he published research used by the Trump administration to link the drug to autism, a connection experts say is tenuous at best.&lt;/p&gt;
    &lt;p&gt;Baccarelli served as an expert witness on behalf of parents and guardians of children suing Johnson &amp;amp; Johnson, the manufacturer of Tylenol at the time. U.S. District Court Judge Denise L. Cote dismissed the case last year due to a lack of scientific evidence, throwing out Baccarelli‚Äôs testimony in the process.&lt;/p&gt;
    &lt;p&gt;‚ÄúHe cherry-picked and misrepresented study results and refused to acknowledge the role of genetics in the etiology‚Äù of autism spectrum disorder or ADHD, Cote wrote in her decision, which the plaintiffs have since appealed.&lt;/p&gt;
    &lt;p&gt;Baccarelli, who was a professor at Columbia University‚Äôs public health school at the time, declined to comment on his involvement in the case.&lt;/p&gt;
    &lt;p&gt;The plaintiffs paid Baccareli $700 an hour for his expert testimony, according to a 2023 deposition.&lt;/p&gt;
    &lt;p&gt;‚ÄúI work for more than 200 hours, so it‚Äôs about $150,000,‚Äù Baccarelli said in the deposition.&lt;/p&gt;
    &lt;p&gt;But Catherine E. Lord ‚Äî a professor of Psychiatry and Education at the University of California, Los Angeles ‚Äî said it is not uncommon for medical professionals to be paid for expert testimony.&lt;/p&gt;
    &lt;p&gt;‚ÄúPeople are routinely paid, and they‚Äôre paid generally quite a lot,‚Äù Lord said. ‚ÄúThere are people who, for as a job, testify as experts.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúI think what it does suggest, given that he would testify about this, that he is invested in finding something more than most of us would be,‚Äù Lord said.&lt;/p&gt;
    &lt;p&gt;In a statement to the New York Times, HSPH spokesperson Stephanie Simon wrote that Baccarelli ‚Äúconfirmed that his testimony in the deposition was accurate and that his work on the case culminated in the deposition; he worked just a handful of additional hours following the deposition.‚Äù&lt;/p&gt;
    &lt;p&gt;Trump administration officials have trumpeted Baccarelli‚Äôs work as evidence that acetaminophen ‚Äî the active ingredient in Tylenol ‚Äî causes autism.&lt;/p&gt;
    &lt;p&gt;‚ÄúTo quote the dean of the Harvard School of Public Health,‚Äù Food and Drug Commissioner Marty A. Makary said in a press conference on Monday, ‚Äúthere is a causal relationship between prenatal acetaminophen use and neurodevelopmental disorders of ADHD and autism spectrum disorder.‚Äù&lt;/p&gt;
    &lt;p&gt;Though the study the Trump administration latched onto is a more recent paper, published in August, and Baccarelli has studied the subject for years, scientists say his work has demonstrated only a correlation between the drug and autism ‚Äî and not a causal link, as the Trump administration asserted in the press conference.&lt;/p&gt;
    &lt;p&gt;Baccarelli‚Äôs published statement to the White House only referred to the ‚Äúpossibility of a causal relationship‚Äù between acetaminophen and autism. In both his statement and the August paper, Baccarelli called for further study.&lt;/p&gt;
    &lt;p&gt;His work rocketed into public view after the press conference on Monday, which took place weeks after he met with Health and Human Services Secretary Robert F. Kennedy Jr. ‚Äô76 and Jay Bhattacharya, the director of the National Institutes of Health, on phone calls to discuss his recent research.&lt;/p&gt;
    &lt;p&gt;Baccarelli‚Äôs recent survey of 46 human studies, published online in August alongside three coauthors, concluded that prenatal acetaminophen usage was associated with increased incidence of neurodevelopmental disorders ‚Äî including autism and attention deficit hyperactivity disorder. Experts believe that so far, the evidence only shows a correlation.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe idea that acetaminophen causes autism is, at best, a massive overstatement and might be completely untrue,‚Äù Samuel S. Wang, a professor of neuroscience at Princeton University, said.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe best study to date that I‚Äôve been able to see in the literature shows no additional risk,‚Äù he added.&lt;/p&gt;
    &lt;p&gt;A 2024 study in the Journal of the American Medical Association involved 2.5 million children born in Sweden, and controlled for compounds across these millions of samples. The research determined that there was no causal relationship between acetaminophen consumption and autism.&lt;/p&gt;
    &lt;p&gt;‚ÄúThere needs to be much more work done and additional studies to be able to identify causal mechanisms,‚Äù Dennis P. Wall, professor of Pediatrics and Biomedical Data Science at Stanford University, said. ‚ÄúThat simply hasn‚Äôt been done.‚Äù&lt;/p&gt;
    &lt;p&gt;Lord said that she would not fault Baccarelli or the other authors for the study.&lt;/p&gt;
    &lt;p&gt;‚ÄúI just think that to take it the next step and say this is causal, is really irresponsible,‚Äù she added.&lt;/p&gt;
    &lt;p&gt;Wang said that there is a ‚Äúconsensus view of what causes autism‚Äù within the scientific community. Genetic inheritance, combinations of genes, environmental causes, stresses in mid to late pregnancy, and other biological factors all contribute to autism. If those variables are associated with acetaminophen use, they could have created the appearance of a relationship that may not exist in studies that failed to properly control for them.&lt;/p&gt;
    &lt;p&gt;‚ÄúIn the case of acetaminophen, pregnant women take acetaminophen for a reason. They take it because they have a fever, or they have an infection or they are in pain, and these themselves are risk factors ‚Äî potential risk factors ‚Äî for autism,‚Äù Wang said. ‚ÄúThey may be correlated with other causes that we can‚Äôt see.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄîStaff writer Abigail S. Gerstein can be reached at abigail.gerstein@thecrimson.com. Follow her on X @abbysgerstein.&lt;/p&gt;
    &lt;p&gt;‚ÄîStaff writer Ella F. Niederhelman can be reached at ella.niederhelman@thecrimson.com. Follow her on X @eniederhelman.&lt;/p&gt;
    &lt;p&gt;Want to keep up with breaking news? Subscribe to our email newsletter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.thecrimson.com/article/2025/9/24/autism-dean-public-health/"/><published>2025-09-24T23:46:08+00:00</published></entry></feed>