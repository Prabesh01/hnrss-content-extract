<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-06T03:05:46.207923+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45136103</id><title>I bought the cheapest EV, a used Nissan Leaf</title><updated>2025-09-06T03:05:59.029584+00:00</updated><content>&lt;doc fingerprint="6260145ed9271db7"&gt;
  &lt;main&gt;
    &lt;p&gt;I bought a used 2023 Nissan Leaf in 2025, my first 'new' car in 15 years. The above photo was taken by the dealership; apparently their social media team likes to post photos of all purchasers.&lt;/p&gt;
    &lt;p&gt;I test drove a Tesla in 2012, and quickly realized my mistake. No gasoline-powered car (outside of supercars, maybe? Never drove one of those) could match the feel of pressing the throttle on an electric.&lt;/p&gt;
    &lt;p&gt;I started out with a used minivan, which I drove into the ground. Then I bought a used Olds that I drove into the ground. Then I bought a used Camry that I bought before we had kids, when I had a 16 mile commute.&lt;/p&gt;
    &lt;p&gt;Fast forward about 15 years, and I found myself with a very short commute, only driving a few miles a day, and a family minivan we use for nearly all the 'driving around the kids' stuff.&lt;/p&gt;
    &lt;p&gt;So I wanted a smaller car (get back a foot or so of garage space...) that was also more efficient.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video and GitHub EV Project&lt;/head&gt;
    &lt;p&gt;If you don't like reading blog posts (why are you here?), I also posted a video going over most of this, with a little more color, on my YouTube channel:&lt;/p&gt;
    &lt;p&gt;Also, this blog post is also the centerpiece of my new GitHub project geerlingguy/electric-car, where I detail all the steps on my nascent EV journey.&lt;/p&gt;
    &lt;head rend="h2"&gt;Equipment and Add-ons&lt;/head&gt;
    &lt;p&gt;Before I go further, I thought I'd mention some of the things I've added to my Leaf to make the EV experience a little nicer (some links are Amazon affiliate links. I earn for qualifying referrals):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grizzl-E Level 2 Charger for the garage (see Issue #5)&lt;/item&gt;
      &lt;item&gt;Lectron L1 J1772 EV charger for a more portable charger, when I just need to top off the car for a few hours&lt;/item&gt;
      &lt;item&gt;J1772 Wall mount for cable and plug - I was going to 3D print one, but figured the metal product would hold up better in a garage in the midwest&lt;/item&gt;
      &lt;item&gt;NACS to J1772 AC L1/L2 charging adapter&lt;/item&gt;
      &lt;item&gt;CCS1 to CHAdeMO L3 DC Fast charge adapter (see Issue #9)&lt;/item&gt;
      &lt;item&gt;CarlinKit 5.0 Wireless CarPlay/Android Auto adapter because the Leaf only supports wired CarPlay by default&lt;/item&gt;
      &lt;item&gt;VIOFO A119 Mini Dashcam with a Dongar wiring harness adapter (see Issue #3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Monitoring the Battery&lt;/head&gt;
    &lt;p&gt;If you're considering a used Leaf, or if you have a Leaf already, it's a good idea to keep tabs on the battery health, especially since the meter on your dash is painfully basic in how much data it provides.&lt;/p&gt;
    &lt;p&gt;Individual cell charge, 'State of Health' of the overall battery, and much more are available through the car's OBD-II port.&lt;/p&gt;
    &lt;p&gt;Soon after I bought my Leaf, I ordered a LeLink 2 ($35) and bought the LeafSpy Pro App for my iPhone ($20).&lt;/p&gt;
    &lt;p&gt;I plugged the LeLink 2 into the OBD-II diagnostics port under the steering column, and fired up LeafSpy Pro. It gives me some helpful metrics like:&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;SOH: State of Health&lt;/item&gt;
      &lt;item&gt;Hx: Conductance&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;See Issue #8: Document battery health for all my notes monitoring my own Leaf's battery. But bottom line, my battery showed a 93.16% 'SoH' (State of Health), meaning it still has most of its capacity.&lt;/p&gt;
    &lt;p&gt;I've been reading up on various forums about managing the Leaf's battery, and am trying to do some things to extend the battery's life as long as possible:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limiting the number of QCs (Quick Charges / DC Fast Charge), as this heats up the uncooled Leaf battery, degrading it slightly each time, especially on hotter days&lt;/item&gt;
      &lt;item&gt;Keeping the charge between 50-80% when manageable&lt;/item&gt;
      &lt;item&gt;Charging up to 100% at least once a month, and letting it 'top off' to rebalance the pack for at least a few hours afterwards&lt;/item&gt;
      &lt;item&gt;Not driving like a maniac, despite having more torque in this car than I've ever had in any of my previous cars&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why buy electric?&lt;/head&gt;
    &lt;p&gt;I overanalyze most things, so had been researching this purchase for about a decade now.&lt;/p&gt;
    &lt;p&gt;With EVs there are tradeoffs. Even in my situation, only driving a car a few miles a day, I do take my car on one or two regional road trips every year.&lt;/p&gt;
    &lt;p&gt;Having the ability to hop in at 6 am and be in Chicago or KC by late morning is nice. Having to plan a long break somewhere halfway to charge is not.&lt;/p&gt;
    &lt;p&gt;But if I only take that trip once a year, I can either (a) rent a gas car that gets me there a little more quickly, and ensures I don't have to find a spot in the destination city to do a full charge before the return trip. Or (b) plan for an extra X hours total during the trip to ensure I have padding for charging.&lt;/p&gt;
    &lt;p&gt;Charging infrastructure's improving in the US (and in many parts of the world), but it's nowhere near as ubiquitous as gas stations.&lt;/p&gt;
    &lt;p&gt;Hopefully this improves over time, but for now, I plan on using the electric car for local travel, likely only going more than 100 miles or so in a day once or twice a year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why buy Leaf?&lt;/head&gt;
    &lt;p&gt;Price.&lt;/p&gt;
    &lt;p&gt;That's mostly it. And I drove a Nissan Sentra rental on a recent trip, and realized Nissan isn't half bad. They seem to not require an Internet connection for their cars, they offer basic lane following and adaptive cruise control, they have CarPlay/Android Auto...&lt;/p&gt;
    &lt;p&gt;The Leaf ticks all the little 'convenience' checkboxes, but is also not 'extravagant'.&lt;/p&gt;
    &lt;p&gt;And the later model years also aren't "look at me I drive an EV" ugly (though they're not amazing-looking, either).&lt;/p&gt;
    &lt;p&gt;But I drove a minivan, an olds, and a Camry, so obviously I'm function &amp;gt; form when it comes to my car!&lt;/p&gt;
    &lt;p&gt;Because of the smaller battery (and up until 2026, a battery with no active cooling), combined with the use of a DC fast charging connector (CHAdeMO) that's going out of style in the US, used Nissan Leafs are priced considerably lower than competitors.&lt;/p&gt;
    &lt;p&gt;Well, all except maybe Teslas around a year or two older right now. But Teslas don't have native CarPlay. And I'm not a fan of how Tesla is trying to turn the car into some kind of appliance, RoboTaxi, self-driving thing, versus it being a transportation vehicle that I can do what I want with.&lt;/p&gt;
    &lt;p&gt;No judgement on Tesla owners, the used Tesla market was enticing at the time I bought the Leaf.&lt;/p&gt;
    &lt;p&gt;I also looked a lot at the Hyundai Ioniq and Kona; both were just a little bit too large for my liking, but they could've worked. The problem was used models in good condition were a lot more expensive than I was willing to pay.&lt;/p&gt;
    &lt;p&gt;So back to the Leaf: Nissan's probably not the best right now when it comes to EVs and features, but they're certainly the cheapest. And 'good enough' is fine by me.&lt;/p&gt;
    &lt;p&gt;She's got it where it counts, kid.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gripes about my Leaf&lt;/head&gt;
    &lt;p&gt;There are a few things that baffle me about the Leaf, some that have been frustrating from the first test drive; others that are more subtle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There is no 'play/pause' button. Anywhere. At least not on the steering wheel or the display area. You have to go into the music section on the entertainment display, then press the software play/pause button. That's dumb. I've resorted to just turning Audio on/off using the volume knob, which accomplishes the same goal but is not always ideal.&lt;/item&gt;
      &lt;item&gt;Going into 'Neutral' is an exercise in frustration. I thought you just put your foot on the brake and move the shifter knob to the left. But you have to do it with the right timing, I think.&lt;/item&gt;
      &lt;item&gt;There's no way to open the tailgate short of pressing the release button. At least as far as I'm aware. There's no button in the cabin or key fob to unlatch it. The manual says the other way to open it is with a screwdriver, from inside the car, pushing on the latch (lol). I'm not alone here. At least there's a button on the remote to open the charge port.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The joy of electric&lt;/head&gt;
    &lt;p&gt;I don't care about engine noise. I appreciate it, though. My brother had a 1992 Forumula Firebird. And I nearly owned it after he moved away, instead of my Olds! (But I'm a boring-car person, so I think I was happier with the Olds).&lt;/p&gt;
    &lt;p&gt;The nice things about electric vehicles that swayed me in their favor, in descending order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One pedal driving Seriously, why doesn't every EV have this mode? It makes driving one feel SO much better than any gas car, in terms of connection between driver and car movement.&lt;/item&gt;
      &lt;item&gt;Sprightly torque: Outside of exotic tiny gas cars, you're not going to get the same zip even a cheap EV like a Leaf gives you—smash the accelerator in non-Eco mode and any passenger will giggle, every time.&lt;/item&gt;
      &lt;item&gt;Blissful quiet: Though some cars have annoying noises (Nissan calls this VSP, or "Vehicle Sound for Pedestirians") they play at low speeds.&lt;/item&gt;
      &lt;item&gt;Lower maintenance requirements: I hate every time I have to jack up my car and change the brakes, or take it in for oil/fluid changes. EVs (usually) require less maintenance, besides maybe tires.&lt;/item&gt;
      &lt;item&gt;Conveniences: Like running climate control to cool down/heat up the car prior to hopping in, even while it's in the garage! Or plugging it in to charge at home, and not having to stop by a gas station.&lt;/item&gt;
      &lt;item&gt;Long-term economics: in general, charging with electricity, at least here in St. Louis, is cheaper than filling up with gas, on a dollar-per-mile basis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The pain of electric&lt;/head&gt;
    &lt;p&gt;All that said, I knew going into this there would be some pain. Maybe in 10 or 20 years these things will get solved, but off the top of my head:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Price: The Leaf (especially used, right now) is the cheapest, but it is by no means cheap. It takes a few years to break even with a similarly-specced gas car. But buying a gas car, you have a lot more options on the low-low end.&lt;/item&gt;
      &lt;item&gt;Range Anxiety: Yes, it's overblown, but no, it's not non-existent. The day I bought my used EV, the dealership (which doesn't sell many EVs, even new) didn't have a 'Level 3' DC fast charger—and they had only charged it to about 16%. Letting it top off at L2 while I was dealing with finance, we got to 23%. I wasn't quite sure I'd make it home off the lot! Luckily I did, with 12 miles of range remaining. Road tripping or day trips require more planning when driving an EV.&lt;/item&gt;
      &lt;item&gt;Lack of standards: For 'L3' DC Fast Charging, the Leaf has a CHAdeMO port. Teslas and many newer EVs have NACS. Then there's CCS1 and CCS2. And charging stations are run by multiple vendors with multiple apps and payment methods. It's not like gas stations, like with Shell, BP, Buckee's, etc. where you just drive up, stick the gas nozzle in your tank, and squeeze. Even adapters can be complicated and annoying, and many EV charging stations only support one or two standards—and some may only have one CHAdeMO plug, and that plug may have been ripped off the unit to be scrapped by a copper thief!&lt;/item&gt;
      &lt;item&gt;Lack of standards, part 2: For L1/L2 charging, some cars use J1772, some use NACS... and then wall charging units are all over the board with supporting 6, 12, or 16 Amps for L1 (they shouldn't do 16 on a 15A circuit but it seems like some do!), or various different amperages for L2. Some of these units require apps to configure them, others have dip switches, and yet others are not configurable, and don't list their exact specs in an easy-to-find location. Usually forum posts from users who buy the chargers offer more information than product manufacturers' own websites!&lt;/item&gt;
      &lt;item&gt;Being an EV: For some reason, most EVs look like... EVs. I honestly was holding out hope Tesla would just make a Corolla, but an EV version. All the cheap EVs like the Bolt, i3, Leaf, etc. just look... sorta ugly. Subjective, sure, but at least my Olds looked kinda sleek. Even if it was an Olds. EVs stand out, and that I don't enjoy. I want an EV that looks like a Camry. Just blend in and don't stand out.&lt;/item&gt;
      &lt;item&gt;Cables and chargers: The Leaf has slightly less trunk space than my slightly-larger Camry. I didn't realize how big L1/L2 charge cables are. Even L1-only cables (which charge at a very anemic pace, like 10 miles / hour of charge) are fairly thick, bulky affairs. About 1/10 of my trunk is devoted to my charging cable. And on a road trip, I will likely carry my NACS to J1772 and CCS1 to CHAdeMO adapters. And the latter adapter includes its own battery (that has to be charged) and firmware (that might need to be updated)!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Further Reading&lt;/head&gt;
    &lt;p&gt;Be sure to check the Issues in my GitHub project for more of my EV adventures.&lt;/p&gt;
    &lt;p&gt;I don't plan on becoming an EV advocate by any means.&lt;/p&gt;
    &lt;p&gt;The Leaf is the perfect option for me, but I wouldn't recommend an EV for most car owners yet, especially considering the price disparity and infrastructure requirements that exclude large swaths of the population!&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;I have a much older Leaf (2015) with much less range (allegedly 80, possibly more like 50). The worst moment of range anxiety is when the car jumps from claiming you have 12 miles left, to admitting it has no idea and you'd better do something about it. Happened to me only once on the way back from the airport, but it was awful. I tell everyone to get an L2 charger before they get an EV.&lt;/p&gt;
    &lt;p&gt;But I love my Leaf. It's fun to drive, and the price still makes me happy.&lt;/p&gt;
    &lt;p&gt;Congrats on the Leaf! I've had two (a first gen, and a second gen) and now have a Mach-E.&lt;/p&gt;
    &lt;p&gt;I loved those Leafs, but I hated Nissan. My second Leaf (purchased new) had a bad battery pack. It took me a long time to realize it but while the pack behaved normally from ~100% to ~60%, it would drop from ~50% to ~15% over the course of a few miles. I didn't realize because most of my use was very local, so I only discovered this on my first 100-mile road trip. It very nearly stranded me in the mountains outside Portland.&lt;/p&gt;
    &lt;p&gt;It took me ~9 months, numerous trips to multiple dealers, and eventually threatening them with a lemon law lawsuit to get them to fix the issue. I presented them with videos ( https://youtu.be/b3z2BWc63LI ), a multi-page PDF showing the voltage levels of all the cells, and identified the single cell that had a bad voltage dip. They refused to do anything until I hired a lawyer.&lt;/p&gt;
    &lt;p&gt;The eventual fix took them about 15 minutes and resolved the problem completely. Getting to that point wasted many, many, many hours. So frustrating.&lt;/p&gt;
    &lt;p&gt;Most of your listed pain points are related to your specific choice in EV. The Leaf has less range, a more temperamental battery, uses a long-deprecated charging standard, and has fewer quality of life improvements than modern EVs. They are cheap for a reason...&lt;/p&gt;
    &lt;p&gt;I agree on the charging infrastructure and cost of entry. Those are absolutely barriers that most people won't be able to overcome. Juggling multiple apps, dealing with multiple charging standards and speeds, and just having to plan your route around charging stops is likely a bridge too far for most people. Europe had the right idea by just choosing a national standard that everyone had to follow. The last remaining caveat is towing, where a hybrid makes significantly more sense until energy density increases substantially.&lt;/p&gt;
    &lt;p&gt;I have a '22 Volvo XC40 Recharge that I use to make an annual trip from Kansas City to the Wisconsin Northwoods. Many vehicles on the market (some of them cheaper) would be more ideal for this trip. The vehicle has a midrange charging speed and is on the lower end of range at only 200 miles or so. For this trip, we typically end up stopping about 6 times for a duration of 20-45 minutes each. For a top-performing EV, you'd probably do the same trip with 4 stops for 15-30 minutes each. Just enough time to get out, stretch your legs, use the bathroom, and hit the road again. The car even plans the route and charging stops for us with the built-in Google Maps. The experience is still not quite ready for "main-stream" use, but it's pretty close.&lt;/p&gt;
    &lt;p&gt;This is very much from the perspective of a used car buyer and thats cool. I look around and see people buying/leasing $30k+ new gas cars and wonder what compels them to buy gas propulsion. If you are spending 30k+ there are a lot of great options in the new ev market. Granted new EV prices are a big unknown for next month, but right now you can pull off the lot in a 2025 Chevy Equinox EV for about $30k OTD. The used market is going to be very interesting in two to three years. Will my Blazer EV be worthless because prices came down so much and battery tech went so far or will it hold its value well given that it seems like most new cars are 30k+. Doesn't matter either way to me because I love it and hope to drive it until there is nothing left of it. If you can charge at home or at work, I cant imagine wanting to stop at a gas station. Enjoy those Es.&lt;/p&gt;
    &lt;p&gt;The new car market is just out of my price range. I'm weird and old school I guess, but I don't want to lease a car, I'd much prefer to purchase it outright, but if you do that, car dealership finance departments get all snippy with you, so I do the loan at whatever the going rate is (I think this one was 6%? my last one was 3%) and pay it off over a few months instead of the 36 month term.&lt;/p&gt;
    &lt;p&gt;I hate that so many markets are pushing towards people never owning the thing they're 'buying' (and understand that leasing + purchasing can still make more financial sense, I'm just allergic to that model of ownership!).&lt;/p&gt;
    &lt;p&gt;Snippy finance departments. Love it.&lt;/p&gt;
    &lt;p&gt;Bought our last 2 cars with cash. Drove both dealers crazy. I still don't know why they required me to unlock my credit history so they could do a credit bureau check when I was giving them a personal check for the whole $ 40k or so.&lt;/p&gt;
    &lt;p&gt;Car buying is just a brutal experience.&lt;/p&gt;
    &lt;p&gt;Probably to check your check won't bounce.&lt;/p&gt;
    &lt;p&gt;Meanwhile here in Europe, you can do a SEPA Instant Transfer and 10 seconds later the whole price shows up on the dealership's online banking.&lt;/p&gt;
    &lt;p&gt;I just upgraded my EV recently. I purchased a Tesla Model 3 Performance in 2019, and drove it for nearly 6 years on trips long and short. Range anxiety was my own personal problem for about 6 months until I got used to the slightly different behavior patterns. Electricity in my area was exceptionally cheap and my L1 charger on a 15A circuit (12A limit) was more than enough to handle daily work driving. Took quite a few interstate trips and never had a problem charging anywhere. Car was a fairly expensive, but a great car overall, until Elon made it difficult to justify, so we traded it in for a Hyundai Ioniq 5 (800V bat) with NACS. Between the Tesla trade in value and a few thousand in cash, the new car loan was $20K, which was actually pretty damnn good. For the L1 chargers with a 16A setting, at least the Tesla mobile charger has plug adapter options and can use 20A 120V circuits, of which my new house actually has several. I actually had a Tesla L2 wall charger that I couldn't install in the apartment we previously lived in, but now that I have the Ioniq 5 I ended up installing it and it works fantastically. It's a Gen2 without wifi or any of the extra crap, and limiting it to 32A@240v gives me enough power to charge fully overnight while not requiring a huge load on my panel. Car came with 2 plug adapters (no batteries) and we've used them at a couple of different hotels with L2 guest chargers, but we use Superchargers on longer trips if needed. Honestly, the wife and I never see going back to gas. There's just no reason. We'll be putting in solar and home battery storage eventually, so we'll pay a bit more for a bit more energy independence. We did like the Volkswagen ID Buzz van in concept, but they wanted an extremely high price for them and the range sucks. They were actually pretty huge in person, which was kind of nice and had more space than my old Caravan had.&lt;/p&gt;
    &lt;p&gt;I bought a 2022 Polestar 2 at the end of July, replacing a car I'd purchased new in 2002. The Polestar was half the price it'd been when it was new and only had 16,000 miles on it. I'm absolutely loving the electric car experience here in Seattle, and I took a road trip of 500 miles each way to my sister's home in Idaho as a test last week. The Volvo / Polestar NACS power adapter cost close to $300 after taxes but was absolutely worth it to open up more fast charging opportunities during a road trip.&lt;/p&gt;
    &lt;p&gt;The built in Google maps experience will suggest charging locations if you set a destination that is farther away than your current charge will take you. When I stopped at her home with 20% charge left it asked if I wanted it to find a nearby charging location. (I didn't because she had a power receptacle I could fully charge overnight.)&lt;/p&gt;
    &lt;p&gt;RE "...Not driving like a maniac, despite having more torque in this car ...."&lt;/p&gt;
    &lt;p&gt;I've always thought , the acceleration of electric cars should be limited. to conserve the battery and conserve battery charge.&lt;/p&gt;
    &lt;p&gt;Most of them do have some sort of drive mode where you can set it to an "eco" state. This slows the pedal response and makes it slightly more difficult to get the fast acceleration. Jamming on the accelerator makes it go fast if you need it to get out of the way or whatever, so that's nice.&lt;/p&gt;
    &lt;p&gt;When war* broke out in Ukraine's, quite a few bought an old leaf and swapped the battery with a newer versions battey pack. They bought them because diesel and gas deliveries were severely disrupted and expensive. Eventually this stopped, because of the electrid grid destruction and c.e. fuel was ok again.&lt;/p&gt;
    &lt;p&gt;We bought a used Hyundai Ioniq Electric 38kWh. I had to replace the small 12V battery (~150 dollars) as soon we got the car. A few months later, a light came up and it was to do with the cooling of the main battery: apparently the original fluid goes bad and clogs the cooling circuit (didn't cost anything, still under warranty and I believe Hyundai is fixing this for free anyway).&lt;/p&gt;
    &lt;p&gt;Other than this, we've only had to change tires, which seem to last more or less than before. Driving in eco mode removes some of that initial power and probably helps. The breaks will probably be changed in the next check next year.&lt;/p&gt;
    &lt;p&gt;I've managed to do a 170 miles trip in summer with a full battery and some AC. In winter, it would probably be ~140 miles or so with heating. I do 80-100 miles a day. Charge it over night on a slow 5kW charger, it's done when I'm ready to leave in the morning.&lt;/p&gt;
    &lt;p&gt;We looked at the Leaf, but on top of the issue with the port (CCS2 is the standard here), the lack of battery cooling worried me as I have no idea how the previous owner was using the car. The 2021 Ioniq Electric doesn't have these issues and so the choice then was between the 28 or 38 kWh battery. The smaller one charges much faster, which can be useful in longer trips, but I wanted to charge less and in 99% of times, the extra range is useful as I don't always start with 100%.&lt;/p&gt;
    &lt;p&gt;Everything got easier with time. I already have an idea of what 10% of battery can do (the car prediction is often wrong). Apps like ABetterRoutePlanner can help you plan long trips, charging stops, etc, if needed and I have 2 apps to check chargers around me if needed for long trips.&lt;/p&gt;
    &lt;p&gt;Coming from a diesel car, there was an adaptation period. For example, waiting for 100% during a trip makes no sense. Use 80% as your "full" and the last 20% as a extra can of fuel, which should only be filled if you have the time to do so (eg: charging while you're eating, over night, etc) or absolutely need the range. The panic when I had 20% and the car would tell me to charge it... lol. I also started taking advantage of EV chargers, especially slow ones, to park for free (as I was charging) while going to do something. Last month we went on a one day road trip, arrived there with 40%, parked somewhere where it slowly charged to 100% while we explored the place. I lost no time charging the car.&lt;/p&gt;
    &lt;p&gt;I'm not going back to ICE cars, unless absolutely needed.&lt;/p&gt;
    &lt;p&gt;To get into Neutral you just hold it to the left without going up or down (while pressing the brake). It takes a second or two of holding so that it doesn't accidentally put it into neutral while you're shifting to reverse or drive, but that's all you need to do.&lt;/p&gt;
    &lt;p&gt;Great article!&lt;lb/&gt; (on the 2022 Leaf at least) take the plate off the left side in the trunk, and you can fit the whole Nissan L1/L2 charger up in there. The Lectron brick looks to be about the same size...&lt;/p&gt;
    &lt;p&gt;Yes; I have the flat tire repair kit, and an AC NACS to J1772 adapter in there right now. Trying to figure out a way to securely store a couple other things in that bit, in a way they don't rattle around.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/i-bought-cheapest-ev-used-nissan-leaf"/></entry><entry><id>https://news.ycombinator.com/item?id=45137245</id><title>Interview with Japanese Demoscener 0b5vr</title><updated>2025-09-06T03:05:57.226818+00:00</updated><content>&lt;doc fingerprint="27325121b8a72b9e"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to “Interviews with Demosceners”! This time, we welcome Japanese demoscener 0b5vr, who mainly creates 64K and 4K intros.&lt;/p&gt;
    &lt;p&gt;For many, 0b5vr is best remembered for his 64K demo “0b5vr GLSL Techno Live Set”, released at Revision 2023. In this interview, he talks about how this piece was created, as well as his recent live music performance.&lt;/p&gt;
    &lt;p&gt;He also talks about trends around the Japanese demoscene, like music production with GLSL, machine live, and generative VJ. I also took the chance to ask how he feels about sceners like me—that is, people who know nothing about programming or technology! Happy reading!&lt;/p&gt;
    &lt;p&gt;Note: If you don’t know what demoscene is, you may want to start from here!&lt;/p&gt;
    &lt;p&gt;First of all, could you introduce yourself?&lt;/p&gt;
    &lt;p&gt;I’m 0b5vr, and I don’t belong to any particular group. I mainly work on 64k intros and 4k intros/exegfx using WebGL. I also compete in Shader Jam and perform live coding and VJ sets at club events and similar venues.&lt;/p&gt;
    &lt;p&gt;Your demo “0b5vr GLSL Techno Live Set” had a strong impact on me. I was curious about this. It says “Live Set,” but was released in the 64K category. What is this exactly? Is this live coding?&lt;/p&gt;
    &lt;p&gt;0b5vr GLSL Techno Live Set (“0mix”) is indeed a 64K intro demo. Just like any other 64K intro, this audiovisual piece is generated from a 64KB file―an HTML file, in this case.&lt;/p&gt;
    &lt;p&gt;That said, as described in the title, its format is “Live Set.” It can be somewhat tricky, because it looks like a recorded video of a live performance at an event, but it’s actually a 64K intro.&lt;/p&gt;
    &lt;p&gt;Hmm… I’m still not sure if I understood correctly. Could you elaborate a bit more?&lt;/p&gt;
    &lt;p&gt;0mix was inspired by three different scenes: techno demos, live coding, and 64K intros.&lt;/p&gt;
    &lt;p&gt;Let me start with techno demos. There are many techno-themed demos in the history of the demoscene. If you look at the demos such as “Medium” by Einklang.net, “X-MIX 2004: Ion Traxx” by Kewlers &amp;amp; mfx, and “Emix” by Epoch, they use multiple tracks mixed together like a DJ set, rather than a single techno soundtrack. They also use VJ-style visuals to create an atmosphere similar to a club event. Emix has black-and-white visuals with unique textures that fit perfectly with cold, mechanical techno, and it’s one of my favorites.&lt;/p&gt;
    &lt;p&gt;Next is live coding. Live coding is a live performance where visuals and music are generated with programming in real time. On the screen, you’ll see the visuals and sound waveforms being generated alongside the code you’re writing. This highlights that the artwork is generated by code. In the demoscene, live coding sessions focus mostly on visuals in GLSL (eg, Shader Showdown, Shader Jam). But in live coding events like Algorave and Eulerroom, music live coding is as popular as, or even more popular than, visual coding. From what I see, Tidal Cycles and Sonic Pi are the most commonly used tools in those environments. (Reference video)&lt;/p&gt;
    &lt;p&gt;Finally, there’s the 64K intro. It’s a category where you create visuals and audio with an executable file of just 64KB. This is the most challenging category since every element has to be procedurally generated within the intro. Most 64K creators build their own engines and tools from scratch. This category requires a broad range of knowledge and skills including modeling, animation, rendering, post-processing, music, and compression.&lt;/p&gt;
    &lt;p&gt;If I managed to merge all three inspirations and create a 64K techno demo with music generated by live coding, I knew I could present it to demosceners and other creators around the scene with confidence. I came up with the idea about a year before Revision 2023. Over the course of that year, I refined a demo engine, built a live coding environment, composed music, and created visual assets almost entirely on my own.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Here’s the working environment for 0mix. The top screen shows the preview, timeline, etc., while the bottom screen is the code editor. Basically, I spend most of the time in the code editor.&lt;/p&gt;
    &lt;p&gt;So, you climbed the highest mountain by yourself. What was the process like?&lt;/p&gt;
    &lt;p&gt;It was extremely tough and painful to spend a year working on a challenging 64K project by myself. My advice is to collaborate with others. At the very least, you should find someone you can discuss the progress with. It was indeed fun to surprise many friends at demoparties, but at the end of the day, completing the project is more important.&lt;/p&gt;
    &lt;p&gt;You entered 64K compo, but it ended up being released in the PC Demo compo. Did that bother you?&lt;/p&gt;
    &lt;p&gt;It’s true that 0mix was released in the PC Demo Compo at Revision 2023. That was because it was the only entry in the PC 64K intro, which wasn’t enough to hold a separate compo. So the two compos were merged. The same thing happened at Revision 2022. PC 64K intro compo was incorporated into the 8K intro compo because there were only two entries. Nevertheless, I’ve always pursued uncompromising quality, so I was down with it. Along with the works of other demo groups (such as Fairlight, mfx, and Still), I think I could contribute to making that compo interesting.&lt;/p&gt;
    &lt;p&gt;Ah, you’re right. That felt like a never-ending compo!&lt;/p&gt;
    &lt;p&gt;There were so many entries for Revision 2023, and from the chat I got the impression that many participants and viewers were exhausted after the compo. Still, it was a great compo. All of the top works featured demoscene-style visuals built with their own engines, and their narratives were also impressive. So I’m happy with my result. When there’s a big entry in the compo I’m in, I feel more accomplished because it means I helped make that compo exciting together with those great pieces.&lt;/p&gt;
    &lt;p&gt;That’s right, I remember some big names rushing in at the end. Nevertheless, this demo stood out for its originality.&lt;/p&gt;
    &lt;p&gt;Thank you. Revision has an award called “Crowd Favorite” where viewers can vote for their favorite demo in any category, and 0mix received first prize. 0mix is a piece that reflects what I love, so I felt happy that everyone else enjoyed it, too.&lt;/p&gt;
    &lt;p&gt;Congratulations! It was indeed a cool demo.&lt;/p&gt;
    &lt;p&gt;Oh, I have a question for you. How do you feel about the code constantly shown in 0mix? What kind of impression does it give you? &lt;lb/&gt; (Interviewer’s note: I’m not from the programming field. I’m the type of person who chooses a laptop by its color.)&lt;/p&gt;
    &lt;p&gt;Maybe it’s more like a design or typography? It says “live coding,” so I figured this code is for its visuals, but I have absolutely no idea if the code itself is cool or not. If I didn’t know what live coding is, I’d probably just look at it as part of the design, just like seeing the typography in a language I don’t understand.&lt;/p&gt;
    &lt;p&gt;Ah, that’s interesting! Actually, the code displayed on the screen is not for visuals but for music. I use a programming language called GLSL, which is normally used to generate visuals. But 0mix is a live performance-themed demo where I use GLSL for music, and that’s why it’s called “GLSL Techno Live.” If you look at the code closely, you’ll see the parts for instruments, like “KICK,” “HIHAT,” and “BASS.” And by adding and subtracting these elements, I shaped the flow of music.&lt;/p&gt;
    &lt;p&gt;Ohh, so that was code for music! But even after knowing this fact, my impression of this piece hasn’t really changed. I guess that shows I interpreted the code as part of the design. Is it okay if a viewer like me sees it that way? (laughs)&lt;/p&gt;
    &lt;p&gt;In my post about this production on Scrapbox, I wrote, “for viewers without coding knowledge, it feels like music-making magic. And for viewers who know programming languages and environments, it’s a hint to guess the next move.” So I expected that some people would see it as part of the design.&lt;/p&gt;
    &lt;p&gt;To reveal a bit more about my understanding, now I do understand that “demo is generated from an executable file” and that “a 64K piece has a 64KB file.” But I still don’t see things like “this is real-time rendering, so it’s more impressive than live-action” or “it’s great quality considering this is 64KB.” Basically, I watch demos like I watch music videos, and the only thing that matters to me is whether I find it cool or not.&lt;/p&gt;
    &lt;p&gt;Ryoji Ikeda has a work that presents data including planets and genes using 5×5 pixel fonts. Of course, only experts can truly understand such data, so most of us simply enjoy the visual design that comes out of it. Even if we try to find deeper meaning in it, we probably just end up saying something like, “Wow, the world is huge.” I’ve read that Ikeda actually intended for viewers to see it that way.&lt;/p&gt;
    &lt;p&gt;Oh, then I’m actually one of his intended viewers. When I first saw his installation video, I knew him as a musician, so I thought, “Wow, that’s his MV? Cool! Very futuristic!” I later realized that it wasn’t just design. It’s nice to know that creators and demosceners expected viewers like me, and personally, I feel relieved. I’d always thought they might be annoyed to hear a clueless person like me commenting on their piece. (laughs)&lt;/p&gt;
    &lt;p&gt;To me, how others first got interested in a piece or in the culture is as fascinating as the motives behind its creation. So I do appreciate sceners who are not from the tech side!&lt;/p&gt;
    &lt;p&gt;Thank you! That’s really nice and reassuring to hear!&lt;lb/&gt; OK, let’s go back to that music code. You wrote in your post on Scrapbox that you put a lot of time and effort into the music.&lt;/p&gt;
    &lt;p&gt;Actually, I had never really made this type of techno music before, so I watched a lot of live performances of this style and tutorials on YouTube. I also bought and tried hardware for “machine live” performances, like the Elektron Syntakt and Dirtywave M8, for research.&lt;/p&gt;
    &lt;p&gt;What is “machine live”?&lt;/p&gt;
    &lt;p&gt;“Machine live” is a type of music performance similar to live coding. Performers use music equipment like grooveboxes and modular synths in real-time to control the sound during the performance. What you can do depends on the features of the equipment, so performers always have to be aware of limitations—something somewhat similar to the demoscene. It’s a fascinating culture. There’s even a “DAWless Live” category where you perform without using a DAW, the standard PC-based music production system. For 0mix, I drew a lot of inspiration from the philosophy and methods of machine live and applied them to GLSL live coding. (Reference video)&lt;/p&gt;
    &lt;p&gt;I just watched the reference video you sent me. Does everyone in this scene really use that much gear?&lt;/p&gt;
    &lt;p&gt;Of course not. Not everyone uses this much equipment, or equipment of this size, for live performance. Lately, it seems like the palm-sized Dirtywave M8 is trending for live sets. The Dirtywave M8 uses a tracker-style UI, and it’s fun to compose with. Plus, it fits well with the demoscene aesthetic.&lt;/p&gt;
    &lt;p&gt;I did a lot of research on machine live and live coding performances, and this gave me ideas about how to create sound and how to evolve live performance. But that only covered the technical side. When it comes to making techno, especially abstract sounds, I had to learn through trial and error and trust my feelings. Even after I learned how to make sounds on standard hardware or software, GLSL follows a completely different set of rules, and I had to be really fired up to tackle it.&lt;/p&gt;
    &lt;p&gt;I heard that you did a live performance recently. What kind of event was it?&lt;/p&gt;
    &lt;p&gt;I performed a live coding set at “draw(tokyo); #2” in March 2025. “draw(); ” is a club event focused on audiovisuals, especially live coding and generative VJ (the so-called “gene-kei” performances). It takes place from time to time in VRChat and at physical venues.&lt;/p&gt;
    &lt;p&gt;At draw(tokyo); #2, I performed using Wavenerd, my custom GLSL live coding environment. For my 40-minute live set, I mainly used techno patterns created for 0mix. It was a really memorable experience, since it was my first time doing a live music performance with Wavenerd. I’d love to do more live performances in the future.&lt;/p&gt;
    &lt;p&gt;The “Wavenerd” system I used for my live coding performance at draw(tokyo); #2. Since we were chroma keying with VJ visuals, the background is blue. The performers are always lit up in blue.&lt;/p&gt;
    &lt;p&gt;When a coder does a live music performance, aren’t you too busy typing code in front of the PC to even look at the audience’s reaction?&lt;/p&gt;
    &lt;p&gt;During the performances, I rewrite parts of prewritten code, so I don’t need to constantly keep typing. But I’m busy adding and removing parts, changing parameters, and doing some DJ-style mixing, so basically I completely zone in on the screen. That said, I can still see the audience’s reactions to some extent, and I felt really happy when they reacted at the moments I expected.&lt;/p&gt;
    &lt;p&gt;Do you know who the primary audience is? I guess this kind of live performance requires some knowledge to really enjoy it.&lt;/p&gt;
    &lt;p&gt;I still don’t know what kind of audience it attracts. From what I saw, I got the impression that many of them are interested in musical experiences and visual production at least. But I’m not sure how many are interested in coding, or actually create things with code. How technical it should get, how strictly you stick to the technical restrictions, and how much you make the audience dance—I think performers are expected to balance these elements well. Probably, this is something gene-kei performers constantly have to tackle. In fact, quite a few performers change their set depending on the tone of the event.&lt;/p&gt;
    &lt;p&gt;Did you have VJs for your live performance?&lt;/p&gt;
    &lt;p&gt;Yes, I asked fellow demosceners, ukonpower and Renard, and they generated visuals that matched the techno. I just told them, “I’m going to do 0mix,” and they both knew what it meant, so everything went very smoothly. (laughs) They created visuals in my style, but their own personalities also shone through. It was really cool.&lt;/p&gt;
    &lt;p&gt;Oh, that’s really cool! &lt;lb/&gt; According to your discography, you also have 4K as well as 64K works. Is there a reason for that?&lt;/p&gt;
    &lt;p&gt;For the 4K intros I’ve released lately, I can usually create them in one or two weeks. But 64K is my soul, so I want to keep making 64K intros. The thing is, 64K requires hundreds of times more work than 4K. So, when I don’t have the time or motivation but still want to contribute to a demoparty, I just make a 4K intro.&lt;/p&gt;
    &lt;p&gt;I must say that the production environment for 4K intros is well-supported in the current demoscene. Recently I’ve been using 0x4015’s minimalGL. With this demotool, I can easily create 4K intros just by writing GLSL. That being said, I wouldn’t recommend it to everyone, because you also have to write the music in GLSL.&lt;/p&gt;
    &lt;p&gt;In 2023, I released a 4K intro called “Architectural Shapeshifter” with Renard. For this piece, Renard was in charge of the concept and visuals, while I was in charge of the music and direction. We used minimalGL for this piece as well. It was the first time for Renard to create a 4K intro, but he was able to create it easily. We collaborated by tweaking the source code on GitHub and communicating via Discord. We exchanged ideas and suggestions on each other’s code, and it turned out to be a very efficient workflow.&lt;/p&gt;
    &lt;p&gt;There are many coders who can write GLSL in Japan, but not many of them take on 4K. So I’d love to collaborate more using minimalGL.&lt;/p&gt;
    &lt;p&gt;What’s hot in the Japanese demoscene these days? What category is popular? I noticed there was a demoparty called SESSIONS in Japan last year.&lt;/p&gt;
    &lt;p&gt;It seems like a lot of people are coming into the demoscene from shader culture centered around VRChat. The people I got to know at demoparties like SESSIONS were mostly active in VRChat. In particular, the event draw(); seems to have a strong influence, and many of the people who got interested in live coding or generative VJ through draw();’s audiovisual experience also developed an interest in the demoscene.&lt;/p&gt;
    &lt;p&gt;Live coding and generative VJ becoming a gateway into the demoscene sounds like a new path to me.&lt;/p&gt;
    &lt;p&gt;Yes, indeed. draw();’s main crew, Saina-san, purposefully aims for a crossover with demoscene culture, like SESSIONS, and this accelerates the influx. We’re really grateful for that.&lt;/p&gt;
    &lt;p&gt;I’m sure a person like that is supporting the demoscene in Japan and around the world. &lt;lb/&gt; OK, let’s go back to the production. Is there anything you do in everyday life to get inspired for your creations?&lt;/p&gt;
    &lt;p&gt;I check Pouet and Demozoo as much as possible to stay in the know about recent demoscene productions. If I ever stopped checking Pouet and Demozoo, I think that would be the end of me as a demoscener.&lt;/p&gt;
    &lt;p&gt;I also try to take in other cultures as well. Recently, I’ve been fascinated by the flashy audiovisual productions in pachinko and pachislot machines. They use dazzling visuals and music to stir up the spirit of gambling. These productions thoroughly pursue how to exploit the human reward system, all within machines that operate under very strict legal restrictions. In a way, I think this represents the highest peak of visual entertainment.&lt;/p&gt;
    &lt;p&gt;I also go for walks frequently. Especially walking around Tokyo late at night gives me a strong sense of urban life and social activity, and it inspires me a lot. “Domain“, a 64K intro I released at Tokyo Demo Fest 2021, was heavily inspired by night Tokyo. I find the concept of the night city very interesting, and I’d like to explore it further.&lt;/p&gt;
    &lt;p&gt;Which areas do you usually walk around?&lt;/p&gt;
    &lt;p&gt;I mainly walk around downtown. I can feel the rhythm of social activity through people’s movements, clothing, and buildings. It’s also very fun to walk around residential areas. When I imagine that this is someone’s everyday life, I can sense their presence through the scenery.&lt;/p&gt;
    &lt;p&gt;Do you have anything you always keep in mind when you create, like a routine or your own personal rule?&lt;/p&gt;
    &lt;p&gt;For my demo source code, I use Git for version control and share as much of the code as possible on GitHub. Basically, I publish my source code under the Creative Commons BY-NC 4.0 license, and users can adapt and use it freely for non-profit purposes. By publishing my source code, I allow other people to refer to my production methods. In fact, I’ve often heard that people have made demos based on my code. Getting more chances to discover other demosceners’ great works is valuable for me too, so I’ll continue to publish my source code.&lt;/p&gt;
    &lt;p&gt;Also, when I do version control on Git, I try to write commit logs—comments you can add to each version—as detailed as possible. Commit logs explain which part of the code I changed, and they also serve as a kind of production journal. In addition to information like what type of change I made and for what purpose, they help me recall my state of mind or what I was thinking during the creative process.&lt;/p&gt;
    &lt;p&gt;For programmers, is it a hassle to write detailed commit logs?&lt;/p&gt;
    &lt;p&gt;Commit logs aren’t considered a direct contribution to a program, just like READMEs or documentation. So, engineers who want to focus on coding and dislike communicating often don’t write them at all. Usually, detailed commit logs are recommended when you work with other people on business projects. However, even for a one-off piece of code written by a single person, I think we should consider how detailed we make the commit logs, because someone else—or even yourself—may end up reading them like archaeology.&lt;/p&gt;
    &lt;p&gt;Archaeology… that’s interesting. &lt;lb/&gt; Okay, let me go to the classic question: your favorite demo, a memorable demo, or a demo that changed your life… anything. Tell us about a demo, or demos that are special to you.&lt;/p&gt;
    &lt;p&gt;As I mentioned, “Emix” by Epoch is the demo I like the most. From the theme of each effect to the color grading, glitch effects, music, and direction, this piece defined what a demo should have, for me. Other pieces that helped define my standards include “cdak” by Quite &amp;amp; Orange, “Transformer 3” by Limp Ninja, and “Clean Slate” by Conspiracy. I put them together in my Pouet playlist “0b5vr’s bible”, if you’re interested.&lt;/p&gt;
    &lt;p&gt;Among many other forms of self-expression, why did you choose the demoscene? Or are you trapped by this culture? Tell me what’s so attractive about it.&lt;/p&gt;
    &lt;p&gt;The demoscene is a creative activity free from art as a capital asset or from commercial value. We mostly create and present pieces in a format that has little value in today’s society, and we purely inspire one another’s technical curiosity and the craving for expression. Also, the demoscene ecosystem is cooperative. Anyone can access demotools, ask questions to veterans, and start creating a piece. I respect the works, workflows, and ideas of active demosceners in the community, and that’s what motivates me to create something that earns their recognition.&lt;/p&gt;
    &lt;p&gt;On the other hand, due to the methods used in the demoscene, a lot of pieces look similar, and that’s clearly a weak point of the scene. If I only keep exploring the demoscene, I can’t expand my range of expression. As a creator, I think it’s important to look at various cultures and absorb many different methods of expression. The easy exchange of fresh inspiration is one of the features of the demoscene, so I’d like to take in many forms of expression both inside and outside the scene, and keep inspiring each other.&lt;/p&gt;
    &lt;p&gt;Is there anything you want to do in the future?&lt;/p&gt;
    &lt;p&gt;What I want to do most is live music performance using GLSL, as I mentioned. Seemingly, this format of live music with GLSL is currently performed only by me and “Rakuto-ice” san. So I want to perform more to develop my style further, and I hope more people will enjoy it.&lt;/p&gt;
    &lt;p&gt;And of course, I want to create demos like 64K, but right now I don’t have enough motivation or ideas. To find more motivation and inspiration, I think it’s about time I formed a demogroup.&lt;/p&gt;
    &lt;p&gt;Sounds like there’s much to look forward to! &lt;lb/&gt; Finally, your message for demosceners and demo fans out there, please.&lt;/p&gt;
    &lt;p&gt;For those of you who are not yet demosceners: &lt;lb/&gt; I’ve seen many people who have an interest in the demoscene but also fears about the culture itself. And it’s not just Japanese people, people in other countries have reacted that way too. Please don’t be afraid of us. If you are interested in creating something with a computer and having fun at a demoparty, then you are a demoscener. Whether you already have a medium of expression or not, if you join the party, you may naturally feel inspired to think, “I want to express myself too.” Demoparties like Tokyo Demo Fest, SESSIONS, and Revision have various compos, including simple programs, illustration, photography, music, along with the demo compo. Of course, if you want to create a demo, fellow creators will help you. We demosceners hope you will have fun in this scene.&lt;/p&gt;
    &lt;p&gt;For those who are already demosceners (including me): &lt;lb/&gt; Make 64K!&lt;/p&gt;
    &lt;p&gt;Thank you very much for answering my question, 0b5vr!&lt;/p&gt;
    &lt;p&gt;0b5vr’s works can be found on Pouet and Demozoo. Also, be sure to check his essay on the production of 0mix on Scrapbox, where he goes deeper into his thoughts on the demoscene and the creative process.&lt;/p&gt;
    &lt;p&gt;Thank you very much for reading this to the end!&lt;/p&gt;
    &lt;p&gt;—————-&lt;/p&gt;
    &lt;p&gt;In case you’re wondering what “demo” or “demoscene” is, better check out the well-made documentary called Moleman2. (and the director, M. Szilárd Matusik’s interview can be read in here.)&lt;/p&gt;
    &lt;p&gt;#1: q from nonoil/gorakubu is here. &lt;lb/&gt; #2: Gargaj from Conspiracy, Ümlaüt Design is here. &lt;lb/&gt; #3: Preacher from Brainstorm, Traction is here. &lt;lb/&gt; #4: Zavie from Ctrl-Alt-Test is here. &lt;lb/&gt; #5: Smash from Fairlight is here. &lt;lb/&gt; #6: Gloom from Excess, Dead Roman is here. &lt;lb/&gt; #7: kioku from System K is here. &lt;lb/&gt; #8: kb from Farbrausch is here. &lt;lb/&gt; #9: iq from RGBA is here.&lt;lb/&gt; #10: Navis from Andromeda Software Development is here.&lt;lb/&gt; #11: Pixtur from Still, LKCC is here.&lt;lb/&gt; #12: Cryptic from Approximate is here.&lt;lb/&gt; #13: 0x4015 aka Yosshin is here.&lt;lb/&gt; #14: Flopine from Cookie Collective is here. &lt;lb/&gt; #15: noby from Epoch, Prismbeings is here.&lt;/p&gt;
    &lt;p&gt;Why I’m interested in demoscene is explained in this article.&lt;lb/&gt; And for some of my other posts related to “demo and “demoscene” culture is here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://6octaves.com/2025/09/interview-with-demoscener-0b5vr.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45137373</id><title>ML needs a new programming language – Interview with Chris Lattner</title><updated>2025-09-06T03:05:56.723023+00:00</updated><content>&lt;doc fingerprint="272a001512c6a995"&gt;
  &lt;main&gt;
    &lt;p&gt;Listen in on Jane Street’s Ron Minsky as he has conversations with engineers working on everything from clock synchronization to reliable multicast, build systems to reconfigurable hardware. Get a peek at how Jane Street approaches problems, and how those ideas relate to tech more broadly.&lt;/p&gt;
    &lt;p&gt;Chris Lattner is the creator of LLVM and led the development of the Swift language at Apple. With Mojo, he’s taking another big swing: How do you make the process of getting the full power out of modern GPUs productive and fun? In this episode, Ron and Chris discuss how to design a language that’s easy to use while still providing the level of control required to write state of the art kernels. A key idea is to ask programmers to fully reckon with the details of the hardware, but making that work manageable and shareable via a form of type-safe metaprogramming. The aim is to support both specialization to the computation in question as well as to the hardware platform. “Somebody has to do this work,” Chris says, “if we ever want to get to an ecosystem where one vendor doesn’t control everything.”&lt;/p&gt;
    &lt;p&gt;Chris Lattner is the creator of LLVM and led the development of the Swift language at Apple. With Mojo, he’s taking another big swing: How do you make the process of getting the full power out of modern GPUs productive and fun? In this episode, Ron and Chris discuss how to design a language that’s easy to use while still providing the level of control required to write state of the art kernels. A key idea is to ask programmers to fully reckon with the details of the hardware, but making that work manageable and shareable via a form of type-safe metaprogramming. The aim is to support both specialization to the computation in question as well as to the hardware platform. “Somebody has to do this work,” Chris says, “if we ever want to get to an ecosystem where one vendor doesn’t control everything.”&lt;/p&gt;
    &lt;p&gt;Some links to topics that came up in the discussion:&lt;/p&gt;
    &lt;p&gt;Welcome to Signals and Threads, in-depth conversations about every layer of the tech stack, from Jane Street. I’m Ron Minsky. It is my great pleasure to have Chris Lattner on the show. Typically on Signals and Threads, we end up talking to engineers who work here at Jane Street, but sometimes we like to grab outside folk, and Chris is an amazing figure to bring on because he’s been so involved in a bunch of really foundational pieces of computing that we all use—LLVM, and Clang, and MLIR, and OpenCL, and Swift, and now Mojo. And this has happened at a bunch of different storied institutions—Apple, and Tesla, and Google, and SiFive, and now Modular. So anyway, it’s a pleasure to have you joining us, Chris.&lt;/p&gt;
    &lt;p&gt;Thank you, Ron. I’m so happy to be here.&lt;/p&gt;
    &lt;p&gt;I guess I want to start by just hearing a little bit more about your origin story. How did you get into computing and how did you get into this world of both compiler engineering and programming language design?&lt;/p&gt;
    &lt;p&gt;So I grew up in the ’80s and back before computers were really a thing. We had PCs, but they weren’t considered cool. And so I fell in love with understanding how the computer worked. And back then, things were way simpler. I started with a BASIC interpreter, for example, and you’d get a book from the store. Remember when we had books? [laughs] And you’d learn things from books?&lt;/p&gt;
    &lt;p&gt;Did you do the thing where you’d get the hobbyist magazine and copy out the listing of the program?&lt;/p&gt;
    &lt;p&gt;That’s exactly right. And so we didn’t have vibe coding, but we did have books. And so just by typing things in, you could understand how things work, and then when you broke it—because inevitably you’re typing something in and you don’t really know what you’re doing—you have to figure out what went wrong and so it encouraged a certain amount of debugging. I really love computer games. Again, back then, things were a little bit simpler. Computer games drove graphics and performance and things like this. And so I spent some time on these things called bulletin board systems and the early internet reading about how game programmers are trying to push the limits of the hardware. And so that’s where I got interested in performance and computers and systems. I went on to college and had an amazing professor at my school, shout out to University of Portland in Portland, Oregon, and he was a compiler nerd.&lt;/p&gt;
    &lt;p&gt;And so, I think that his love for compilers was infectious. His name was Steven Vegdahl, and that caused me to go on to pursue compilers at University of Illinois. And there again, continue to fall down this rabbit hole of compilers and systems, and build LLVM. And ever since I got into the compiler world, I loved it. I love compilers because they’re large-scale systems, there’s multiple different components that all work together. And in the university setting, it was really cool in the compiler class, because unlike most of the assignments where you do an assignment, turn it in, forget about it—in compilers, you would do an assignment, turn it in, get graded, and then build on it. And it felt much more realistic like software engineering, rather than just doing a project to get graded.&lt;/p&gt;
    &lt;p&gt;Yeah, I think for a lot of people, the OS class is their first real experience of doing a thing where you really are building layer on top of layer. I think it’s an incredibly important experience for people as they start engineering.&lt;/p&gt;
    &lt;p&gt;It’s also one where you get to use some of those data structures. I took this, almost academic, here’s what a binary tree is, and here’s what a graph is. And particularly when I went through it, it was taught from a very math-forward perspective, but it really made it useful. And so that was actually really cool. I’m like, ‘Oh, this is why I learned this stuff.’&lt;/p&gt;
    &lt;p&gt;So one thing that strikes me about your career is that you’ve ended up going back and forth between compiler engineering and language design space, whereas I feel like a lot of people are on one side or the other—they’re mostly compilers people and they don’t care that much about the language, and just, how do we make this thing go fast? And there are some people who are really focusing on language design and the work on the compiler is a secondary thing towards that design. And you’ve both popped back and forth. And then also a lot of your compiler engineering work, really starting with LLVM, in some sense is itself, very language-forward. With LLVM, there’s a language in there that’s this intermediate language that you’re surfacing as a tool for people to use. So I’m just curious to hear more about how you think about the back and forth between compiler engineering and language design.&lt;/p&gt;
    &lt;p&gt;The reason I do this is that effectively, my career is following my own interests. And so my interests are not static. I want to work on different kinds of problems and solve useful problems and build into things. And so the more technology and capability you have, the higher you can reach. And so with LLVM, for example, built and learned a whole bunch of cool stuff about deep code generation for an X86 chip and that category of technology with register allocation, stuff like this. But then it made it possible to go, say, let’s go tackle C++ and let’s go use this to build the world’s best implementation of something that lots more people use and understand than deep backend code generation technology. And then with Swift, it was, build even higher and say, ‘Okay, well C++, maybe some people like it, but I think we can do better and let’s reach higher.’ I’ve also been involved in AI systems, been involved in building an iPad app to help teach kids how to code. And so, lots of different things over time. And so for me, the place I think I’m most useful and where a lot of my experience is valuable ends up being at this hardware-software boundary.&lt;/p&gt;
    &lt;p&gt;I’m curious how you ended up making the leap to working on Swift. From my perspective, Swift looks from the outside, like one of these points of arrival in mainstream programming contexts of a bunch of ideas that I have long thought are really great ideas in other programming languages. And I’m curious, in some ways a step away from like, oh, I’m going to work on really low-level stuff and compiler optimization, and then we will go much higher level and do a C++ implementation, which is still a pretty low level. How did the whole Swift thing happen?&lt;/p&gt;
    &lt;p&gt;Great question. I mean, the timeframe for people that aren’t familiar is that LLVM started in 2000. So by 2005, I had exited university and I joined Apple. And so LLVM was an advanced research project at that point. By the 2010 timeframe, LLVM was much more mature and we had just shipped C++ support in Clang, and so it could bootstrap itself, which means the compiler could compile itself. It’s all written in C++, it could build advanced libraries like the Boost template library, which is super crazy advanced template stuff. And so the C++ implementation that I and the team had built was real. Now, C++ in my opinion, is not a beautiful programming language. And so implementing it is a very interesting technical challenge. For me, a lot of problem-solving ends up being, how do you factor the system the right way?&lt;/p&gt;
    &lt;p&gt;And so Clang has some really cool stuff that allowed it to scale and things like that, but I was also burned out. We had just shipped it. It was amazing. I’m like, there has to be something better. And so, Swift really came starting in 2010. It was a nights and weekends project. It wasn’t like top-down management said, ‘Let’s go build a new programming language.’ It was ‘Chris being burned out’—I was running a 20 to 40 person team at the time, being an engineer during the day, and being a technical leader, but then needing an escape hatch. And so I said, ‘Okay, well, I think we can have something better. I have a lot of good ideas. Turns out, programming languages are a mature space. It’s not like you need to invent pattern matching at this point. It’s embarrassing that C++ doesn’t have good pattern matching.&lt;/p&gt;
    &lt;p&gt;We should just pause for a second, because I think this is like a small but really essential thing. I think the single best feature coming out of language like ML in the mid-seventies is, first of all, this notion of an algebraic data type, meaning every programming language on earth has a way of saying this and that and the other, a record, or a class, or a tuple.&lt;/p&gt;
    &lt;p&gt;A weird programming language, I think it was Barbara Liskov?&lt;/p&gt;
    &lt;p&gt;Yeah. And she did a lot of the early theorizing about, ‘What are abstract data types?’ But the ability to do this or that or the other, to have data types that are a union of different possible shapes of the data—and then having this pattern matching facility that lets you basically in a reliable way do the case analysis so you can break down what the possibilities are—is just incredibly useful. And very few mainstream languages have picked it up. I mean Swift again is an example, but languages like ML, SML, and Haskell, and OCaml—&lt;/p&gt;
    &lt;p&gt;Standard!&lt;/p&gt;
    &lt;p&gt;That’s right. SML. Standard ML. It’s been there for a long time.&lt;/p&gt;
    &lt;p&gt;I mean pattern matching, it is not an exotic feature. Here we’re talking about 2010. C# didn’t have it. C++ didn’t have it. Obviously Java didn’t have it. I don’t think JavaScript had it. None of these mainstream languages had it, but it’s obvious. And so part of my opinion about that—and so by the way, I represent as engineer, I’m not actually a mathematician, and so type theory goes way over my head. I don’t really understand this. The thing that gets me frustrated about the academic approach to programming languages is that people approach it by saying there’s sum types, and there’s intersection types, and there’s these types, and they don’t start from utility forward. And so pattern matching, when I learned OCaml, it’s so beautiful. It makes it so easy and expressive to build very simple things. And so to me, I always identify to the utility and then yes, there’s amazing formal type theory behind it, and that’s great and that’s why it actually works and composes. But bringing that stuff forward and focusing on utility and the problems it solves, and how it makes people happy, ends up being the thing that I think moves the needle in terms of adoption, at least in mainstream.&lt;/p&gt;
    &lt;p&gt;Yeah, I mean I think that’s right. My approach also, and my interest in language is also very much not from the mathematical perspective, although my undergraduate degree is in math. I like math a lot, but I mostly approach these things as a practitioner. But the thing I’ve been struck by over the years is the value of having these features have a really strong mathematical foundation is they generalize, and as you were saying, compose much better. If they are in the end mathematically simple, you’re way more likely to have a feature that actually pans out as it gets used way beyond your initial view as to what the thing was for.&lt;/p&gt;
    &lt;p&gt;That’s right. This is actually a personal defect because I don’t understand the math in the way that maybe theoretically would be ideal. I end up having to rediscover certain truths that are obvious. The cliche, ‘If the Russian mathematician invented it 50 years ago…’ And so a lot of what I find is that I can find truth and beauty when things compose and things fit together, and often I’ll find out it’s already been discovered because everything in programming language has been done. There’s almost nothing novel, but still that design process of saying, let’s pull things together, let’s reason about why it doesn’t quite fit together. Let’s go figure out how to better factor this. Let’s figure out how to make it simpler these days. That process to me, I think is kind of like people working on physics, [from what] I hear. The simpler the outcome becomes, the more close to truth it feels like it is. And so I share that—and maybe it’s more design gene or engineer-design combination, but it’s probably what you mathematicians actually know inherently, and I just haven’t figured it out yet.&lt;/p&gt;
    &lt;p&gt;Do you find yourself doing things after you come to it from an engineering perspective, trying to figure out whether there are useful mathematical insights? Do you go back and read the papers? Do you have other PL people who are more mathematically oriented who you talk to? How do you extend your thinking to cover some of that other stuff?&lt;/p&gt;
    &lt;p&gt;See, the problem is math is scary to me. So I see Greek letters and I run away. I do follow arXiv and things like this, and there’s a programming language section on that. And so I get into some of it, but what I get attracted to in that is the examples and the results section and the future-looking parts of it. And so it’s not necessarily the ‘how,’ it’s the ‘what it means.’ And so I think a lot of that really speaks to me. The other thing that really speaks to me when you talk about language design and things like this is blog posts from some obscure academic programming language that I’ve never heard of. You just have somebody talking about algebraic effect systems for this and that and the other thing, or something really fancy, but they figure out how to explain it in a way that’s useful. And so when it’s not just, ‘Let me explain to you the type system,’ but it’s, ‘Let me explain this problem this fancy feature enables,’ that’s where I get excited. That’s where it speaks to me because, again, I’m problem-oriented, and having a beautiful way to express and solve problems, I appreciate.&lt;/p&gt;
    &lt;p&gt;I think there’s a lot of value in the work that’s done in papers of really working out in detail the theory and the math and how it all fits together. [And] I think the fact that the world has been filled with a lot of interesting blog posts from the same people has been great because I think it’s another modality where it often encourages you to pull out the simpler and easier-to-consume versions of those ideas. And I think that is just a different kind of insight and it’s valuable to surface that too.&lt;/p&gt;
    &lt;p&gt;And also when I look at those blog posts, sometimes they design smell. Particularly the C++ community, there’s a lot of really good work to fix C++. They’re adding a lot of stuff to it, and C++ will never get simpler—you can’t really remove things, right? And so a lot of the challenge there is, it’s constrained problem-solving. And so when I look at that, often what I’ll see when I’m reading one of those posts, and again, these are brilliant people and they’re doing God’s work trying to solve problems with C++, best of luck with that. But you look at that and you realize there’s a grain of sand in the system that didn’t need to be there. And so to me, it’s like if you remove that grain of sand, then the entire system gets relaxed and suddenly all these constraints fall away and you can get to something much simpler. Swift, for example, it’s a wonderful language and it’s grown really well and the community is amazing, but it has a few grains of sand in it that cause it to be a lot more complicated. And so this is where I’m not just happy with things that got built. LLVM is amazing, it’s very practical, but it has lots of problems. That’s why when I get a chance to build a next generation system, I want to learn from that and actually try to solve these problems.&lt;/p&gt;
    &lt;p&gt;So this is the great privilege of getting to work on a new language, which is a thing you’re doing now. There’s this new language called Mojo, and it’s being done by this company that you co-founded called Modular. Maybe just so we understand the context a little bit, can you tell me a little bit about, what is Modular? What’s the basic offering? What’s the business model?&lt;/p&gt;
    &lt;p&gt;Before I even get there, I’ll share more of how I got here. If you oversimplify my background, I did this LLVM thing and its foundational compiler technology for CPUs. It helped unite a lot of CPU-era infrastructure and it provided a platform for languages like Swift, but also Rust, and Julia, and many different systems that all got built on top of, and I think it really catalyzed and enabled a lot of really cool applications of accelerated compiler technology. People use LLVM in databases and for query engine optimization, lots of cool stuff. Maybe you use it for trading or something. I mean, there can be tons of different applications for this kind of technology—and then [I] did programming language stuff with Swift. But in the meantime, AI happened. And so with AI brought this entirely new generation of compute: GPUs, tensor processing units, large-scale AI training systems, FPGAs, and ASICs and all this complexity for compute, and LLVM never really worked in that system.&lt;/p&gt;
    &lt;p&gt;And so one of the things that I built when I was at Google was a bunch of foundational compiler technology for that category of systems. And there’s this compiler technology called MLIR. MLIR is basically LLVM 2.0. And so take everything you learn from building LLVM and helping solve this, but then bring it forward into this next generation of compiler technology so that you can go hopefully unify the world’s compute for this GPU and AI and ASIC kind of world. MLIR has been amazingly successful, and I think it’s used in roughly every one of these AI systems and GPUs. It’s used by Nvidia, it’s used by Google, it’s used by roughly everybody in this space. But one of the challenges is that there hasn’t been unification. And so you have these very large-scale AI software platforms. You have CUDA from Nvidia, you have XLA from Google, you have ROCm from AMD.&lt;/p&gt;
    &lt;p&gt;It’s countless. Every company has their own software stack. And one of the things that I discovered and encountered, and I think the entire world sees, is that there’s this incredible fragmentation driven by the fact that each of these software stacks built by a hardware maker are just all completely different. And some of them work better than others, but regardless, it’s a gigantic mess. And there’s these really cool high-level technologies like PyTorch that we all love and we want to use. But if PyTorch is built on completely different stacks and schooling together these megalithic worlds from different vendors, it’s very difficult to get something that works.&lt;/p&gt;
    &lt;p&gt;Right. They’re both complicated trade-offs around the performance that you get out of different tools and then also a different set of complicated trade-offs around how hard they are to use, how complicated it is to write something in them, and then what hardware you can target from each individual one. And each of these ecosystems is churning just incredibly fast. There’s always new hardware coming out and new vendors in new places, and there’s also new little languages popping up into existence, and it makes the whole thing pretty hard to wrangle.&lt;/p&gt;
    &lt;p&gt;Exactly. And AI is moving so fast. There’s a new model every week. It’s crazy. And new applications, new research, the amount of money being dumped into this by everybody is just incredible. And so how does anybody keep up? It’s a structural problem in the industry. And so the structural problem is that the people doing this kind of work, the people doing code generation for advanced GPUs and things like this, they’re all at hardware companies. And the hardware companies, every single one of them is building their own stack because they have to. There is nothing to plug into. There’s nothing like ‘LLVM but for AI,’ that doesn’t exist. And so as they go and build their own vertical software stack, of course they’re focused on their hardware, they got advanced roadmaps, they have a new chip coming out next year, they’re plowing their energy and time into solving for their hardware. But we, out in the industry, we actually want something else. We want to be able to have software that runs across multiple pieces of hardware. And so, if everybody doing the work is at a hardware company, it’s very natural that you get this fragmentation across vendors because nobody’s incentivized to go work together. And even if they’re incentivized, they don’t have time to go work on somebody else’s chip. AMD is not going to pay to work on Nvidia GPUs or something like this.&lt;/p&gt;
    &lt;p&gt;That’s true when you think about this, kind of, a split between low-level and high-level languages. So Nvidia has CUDA and AMD has ROCm, which is mostly a clone of CUDA, and then the XLA tools from Google work incredibly well on TPUs, and so on and so forth. Different vendors have different things. Then there’s the high-level tools, PyTorch, and JAX, and Triton, and various things like that. And those are typically actually not made by the hardware vendors. Those are made by different kinds of users—I guess Google is responsible for some of these and they’re also sometimes a hardware vendor—but a lot of the time it’s more stepped back. Although even there, the cross-platform support is complicated and messy and incomplete.&lt;/p&gt;
    &lt;p&gt;Because they’re built on top of fundamentally incompatible things. And so that’s the fundamental nature. And so again, you go back to Chris’s dysfunction and my weird career choices, I always end up back at the hardware-software boundary, and there’s a lot of other folks that are really good at adding very high-level abstractions. If you go back a few years ago, MLOps was the cool thing, and it was, ‘Let’s build a layer of Python on top of TensorFlow and PyTorch and build a unified AI platform.’ But the problem with that, is that building abstractions on top of two things that don’t work very well, can’t solve performance, or liability, or management, or these other problems. You can only add a layer of duct tape, but as soon as something goes wrong, you end up having to debug this entire crazy stack of stuff that you really didn’t want to have to know about.&lt;/p&gt;
    &lt;p&gt;And so it’s a leaky abstraction. And so the genesis of Modular (bringing it back to this) was realizing there are structural problems in the industry. There is nobody that’s incentivized to go build a unifying software platform and do that work at the bottom level. And so what we set off to do is we said, ‘Okay, let’s go build…’—and there’s different ways of explaining this. You could say ‘a replacement for CUDA,’ that’s like a flamboyant way to say this, but ‘let’s go build a successor to all of this technology that is better than what the hardware makers are building, and is portable.’ And so what this takes, is doing the work that these hardware companies are doing, and I set the goal for the team of saying, let’s do it better than, for example, Nvidia is doing it for their own hardware.&lt;/p&gt;
    &lt;p&gt;Which is no easy feat, right? They’ve got a lot of very strong engineers and they understand their hardware better than anyone does. Beating them on their own hardware is tough.&lt;/p&gt;
    &lt;p&gt;That is really hard. And they’ve got a 20-year head start, because CUDA is about 20 years old. They’ve got all the momentum. They’re a pretty big company. As you say, lots of smart people. And so that was a ridiculous goal. Why did I do that? Well, I mean a certain amount of confidence in understanding how the technology worked, having a bet on what I thought we could build and the approach, and some insight and intuition, but also realizing that it’s actually destiny. Somebody has to do this work. If we ever want to get to an ecosystem where one vendor doesn’t control everything, if we want to get the best out of the hardware, if we want to get new programming language technologies, if we want pattern matching on a GPU—I mean, come on, this isn’t rocket science—then we need at some point to do this. And if nobody else is going to do it, I’ll step up and do that. That’s where Modular came from—saying, ‘Let’s go crack this thing open. I don’t know how long it will take, but sometimes it’s worthwhile doing really hard things if they’re valuable to the world.’ And the belief was it could be profoundly impactful and hopefully get more people into even just being able to use this new form of compute with GPUs and accelerators and all this stuff, and just really redemocratize AI compute.&lt;/p&gt;
    &lt;p&gt;So you pointed out that there’s a real structural problem here, and I’m actually wondering how, at a business model level, do you want to solve the structural problem? Which is, the history of computing is these days littered with the bodies of companies that try to sell a programming language. It’s a really hard business. How is Modular set up so that it’s incented to build this platform in a way that can be a shared platform that isn’t subject to just one other vendor’s lock-in?&lt;/p&gt;
    &lt;p&gt;First answer is, don’t sell a programming language. As you say, that’s very difficult. So we’re not doing that. Go take Mojo, go use it for free. We’re not selling a programming language. What we’re doing is we’re investing in this foundational technology to unify hardware. Our view is, as we’ve seen in many other domains, once you fix the foundation, now you can build high-value services for enterprises. And so our enterprise layer, often what we talk to, you end up with these groups where you have hundreds or thousands of GPUs. Often it’s rented from a cloud on a three-year commit. You have a platform team that’s carrying pagers and they need to keep all this stuff running and all the production workloads running. And then you have these product teams that are inventing new stuff all the time, and there’s new research, there’s a new model that comes out and they want to get it on the production infrastructure, but none of this stuff actually works.&lt;/p&gt;
    &lt;p&gt;And so the software ecosystem we have with all these brilliant but crazy open source tools that are thrashing around, all these different versions of CUDA and libraries, all this different hardware happening, is just a gigantic mess. And so, helping solve this for the platform engineering team that actually needs to have stuff work, and want to be able to reason about it, and want good observability and manageability and scalability and things like this is actually, we think, very interesting. We’ve gotten a lot of good responses from people on that. The cost of doing this is we want to actually make it work, that’s where we do fundamental language compiler underlying systems technology and help bring together these accelerators so that we can get, for example, the best performance on an AMD GPU and get it so that the software comes out in the same release train as support for an Nvidia GPU. And being able to pull that together, again, it just multiplicatively reduces complexity, which then leads to a product that actually works, which is really cool and very novel in AI.&lt;/p&gt;
    &lt;p&gt;So the way that Mojo plays in here, is it basically lets you provide the best possible performance and I guess the best possible performance across multiple different hardware platforms. Are you primarily thinking about this as an inference platform, or, how does the training world fit in?&lt;/p&gt;
    &lt;p&gt;So let me zoom in and I’ll explain our technology components. I have a blog post series I encourage you and any viewers or listeners to check out, called, ‘Democratizing AI Compute.’ It goes through the history of all the systems and the problems and challenges that they’ve run into, and it gets to, ‘What is Modular doing about it?’ So Part 11 talks about our architecture and the inside is Mojo, which is a programming language. I’ll explain Mojo in a second. Next level out is called MAX. And so you can think of MAX as being a PyTorch replacement or a vLLM replacement, something that you can run on a single node and then get high performance LLM surveying, that kind of use case. And then the next level out is called Mammoth, and this is the cluster management Kubernetes layer. And so if you zoom in all the way back to Mojo, you say—your experience, you know what programming languages are, they’re incredibly difficult and expensive to build.&lt;/p&gt;
    &lt;p&gt;Why would you do that in the first place? And the answer is, we had to. In fact, when we started Modular, I was like, ‘I’m not going to invent a programming language.’ I know that’s a bad idea, it takes too long, it’s too much work. You can’t convince people to adopt a new language. I know all the reasons why creating language is actually a really bad idea. But it turns out, we were forced to do this because there is no good way to solve the problem. And the problem is, how do you write code that is portable across accelerators? So, that problem, I want portability across—for example, make it simple AMD and Nvidia GPUs, but then you layer on the fact that you’re using a GPU because you want performance. And so I don’t want a simplified, watered down—I want Java that runs on a GPU.&lt;/p&gt;
    &lt;p&gt;I want the full power of the GPU. I want to be able to deliver performance that meets and beats Nvidia on their own hardware. I want to have portability and unify this crazy compute where you have these really fancy heterogeneous systems and you have tensor cores and you have this explosion of complexity and innovation happening in this hardware platform layer. Most programming languages don’t even know that there’s an 8-bit floating point that exists. And so we looked around and I really did not want to have to do this, but it turns out that there really is no good answer. And again, we decided that, hey, the stakes are high, we want to do something impactful. We’re willing to invest. I know what it takes to build a programming language. It’s not rocket science, it’s just a lot of really hard work and you need to set the team up to be incentivized the right way. But we decided that, yeah, let’s do that.&lt;/p&gt;
    &lt;p&gt;So I want to talk more about Mojo and its design, but before we do, maybe let’s talk a little bit more about the pre-existing environment. I did actually read that blog post series. I recommended it to everyone. I think it’s really great, and I want to talk a little bit about what the existing ecosystem of languages looks like, but even before then, can we talk more about the hardware? What does the space of hardware look like that people want to run these ML models on?&lt;/p&gt;
    &lt;p&gt;Yeah, so the one that most people zero in on is the GPU. And so GPUs are, I think, getting better understood now. And so if you go back before that though, you have CPUs. So, modern CPUs in a data center, often you’ll have—I mean today you guys are probably riding quite big iron, but you got 100 cores in a CPU and you got a server with two-to-four CPUs on a motherboard, and then you go and you scale that. And so, you’ve got traditional threaded workloads that have to run on CPUs, and we know how to scale that for internet servers and things like this. If you get to a GPU, the architecture shifts. And so they have basically these things called SMs. And now the programming model is that you have effectively much more medium-sized compute that’s now put together on much higher performance memory fabrics and the programming model shifts. And one of the things that really broke CUDA, for example, was when GPUs got this thing called a tensor core—and the way to think about a tensor core is it’s a dedicated piece of hardware for matrix multiplication. And so, why’d we get that? Well, a lot of AI is matrix multiplication. And so, if you design the hardware to be good at a specific workload, you can have dedicated silicon for that and you can make things go really fast.&lt;/p&gt;
    &lt;p&gt;There are really these two quite different models sitting inside of the GPU space. Of course, the name itself is weird. GPU is ‘graphics processing unit,’ which is what they were originally for. And then this SM model is really interesting. They have this notion of a warp. A warp is a collection of typically 32 threads that are operating together in lockstep, always doing the same thing—a slight variation on what’s called the SIMD model, same instruction, multiple data. It’s a little more general than that, but more or less, you can think of it as the same thing. And you just have to run a lot of them. And then there’s a ton of hardware inside of these systems basically to make switching between threads incredibly cheap. So you pay a lot of silicon to add extra registers. So the context switch is super cheap, so you can do a ton of stuff in parallel.&lt;/p&gt;
    &lt;p&gt;Each thing you’re doing is itself 32-wise parallel. And then because you can do all this very fast context switching, you can hide a lot of latency. And that worked for a while. And then we’re like, actually, we need way more of this matrix multiplication stuff. And you can sort of do reasonably efficient matrix multiplication through this warp model, but not really that good. And then there’s a bunch of quite idiosyncratic hardware, which changes its performing characteristics from generation to generation, just for doing these matrix multiplications. So that’s the Nvidia GPU story, and Volta is like V100 and A100 and H100. They just keep on going and changing, pretty materially from generation to generation in terms of the performance characteristics, and then also the memory model, which keeps on changing.&lt;/p&gt;
    &lt;p&gt;You go back to intuition, CUDA was never designed for this world. CUDA was not designed for modern GPUs. It was designed for a much simpler world. And CUDA being 20 years old, it hasn’t really caught up. And it’s very difficult because, as you say, the hardware keeps changing. And so CUDA was designed from a world where—almost like C is designed for a very simple programming model that it expected to scale, but then as the hardware changed, it couldn’t adapt. Now, if you get beyond GPUs, you get to Google TPU and many other dedicated AI systems. They blow this way out and they say, ‘Okay, well, let’s get rid of the threads that you have on a GPU and let’s just have matrix multiplication units and have really big matrix multiplication units and build the entire chip around that. And you get much more specialization, but you get a much higher throughput for those AI workloads.&lt;/p&gt;
    &lt;p&gt;Going back to, ‘Why Mojo?’ Well, Mojo was designed from first principles to support this kind of system. Each of these chips, as you’re saying, even within Nvidia’s family, from Volta, to Ampere, to Hopper, to Blackwell, these things are not compatible with each other. Actually, Blackwell just broke compatibility with Hopper, so it can’t run Hopper kernels always on Blackwell. Oops, well, why are they doing that? Well, AI software is moving so fast. They decided that was the right trade-off to make. And meanwhile, we all software people need the ability to target this. When you look at other existing systems, with Triton for example, their goal was, ‘Let’s make it easier to program a GPU,’ which I love, that’s awesome. But then they said, ‘We’ll just give up 20% of the performance of the silicon to do it.’ Wait a second. I want all the performance. And so if I’m using a GPU—GPUs are quite expensive by the way—&lt;/p&gt;
    &lt;p&gt;I want all the performance. And if it’s not going to be able to deliver the same quality of results you get by writing CUDA, well then, you’re always going to run to this head room, where you get going quickly, but then you run into a ceiling and then have to switch to a different system to get full performance. And so this is where Mojo is really trying to solve this problem where we can get more usability, more portability, and full performance of the silicon because it’s designed for these wacky architectures like tensor cores.&lt;/p&gt;
    &lt;p&gt;And if we look at the other languages that are out there, there’s languages like CUDA, and OpenCL, which are low level, typically look like variations on C++, in that tradition are unsafe languages, which means that there’s a lot of rules you have to follow. And if you don’t exactly follow the rules, you’re in undefined behavior land, it’s very hard to reason about your program.&lt;/p&gt;
    &lt;p&gt;And just let me make fun of my C++ heritage because I’ve spent so many years, like, you just have a variable that you forget to initialize, it just shoots your foot off. [laughs] Like, it’s just unnecessary violence to programmers.&lt;/p&gt;
    &lt;p&gt;Right. And it’s done in the interest of making performance better because the idea is C++ and its related languages don’t really give you enough information to know when you’re making a mistake, and they want to have as much space as they can to optimize the programs they get. So the stance is just, if you do anything that’s not allowed, we have no obligation to maintain any kind of reasonable semantics or debug ability around that behavior. And we’re just going to try really, really hard to optimize correct programs, which is a super weird stance to take, because nobody’s programs are correct. There are bugs and undefined behavior in almost any C++ program of any size. And so, you’re in a very strange position in terms of the guarantees that you get from the compiler system you’re using.&lt;/p&gt;
    &lt;p&gt;Well, so I mean, I can be dissatisfied. I can also be sympathetic with people that work on C++. So again, I’ve spent decades in this language and around this ecosystem, and building compilers for it. I know quite a lot about it. The challenge is that C++ is established, and so there’s tons of code out there. By far, the code that’s already written is the code that’s the most valuable. And so if you’re building a compiler, or you have a new chip, or you have an optimizer, your goal is to get value out of the existing software. And so you can’t invent a new programming paradigm that’s a better way of doing things and defines away the problem. Instead, you have to work with what you’ve got. You have a SPEC benchmark you’re trying to make go fast, and so you invent some crazy heroic hack that makes some important benchmark work because you can’t go change the code.&lt;/p&gt;
    &lt;p&gt;In my experience, particularly for AI, but also I’m sure within Jane Street, if something’s going slow, go change the code. You have control over the architecture of the system. And so, what I think the world really benefits from, unlike benchmark hacking, is languages that give control and power and expressivity to the programmer. And this is something where I think that, again, you take a step back and you realize history is the way it is for lots of structural and very valid reasons, but the reasons don’t apply to this new age of compute. Nobody has a workload that they can pull forward to next year’s GPU—doesn’t exist. Nobody solved this problem. I don’t know the timeframe, but once we solve that problem, once we solve portability, you can start this new era of software that can actually go forward. And so now, to me, the burden is—make sure it’s actually good. And so, to your point about memory safety, don’t make it so that forgetting to initialize a variable is just going to shoot your foot off. [Instead] produce a good compiler error saying, ‘Hey, you forgot to initialize a variable,’ right? These basic things are actually really profound and important, and the tooling and all this usability and this DNA, these feelings and thoughts, are what flow into Mojo.&lt;/p&gt;
    &lt;p&gt;And GPU programming is just a very different world from traditional CPU programming just in terms of the basic economics and how humans are involved. You end up dealing with much smaller programs. You have these very small but very high-value programs whose performance is super critical, and in the end, a relatively small coterie of experts who end up programming in it. And so it pushes you ever in the direction, you’re saying, of performance engineering, right? You want to give people the control they need to make the thing behave as it should, and you want to do it in a way that allows people to be highly productive. And the idea that you have an enormous amount of legacy code that you need to bring over, it’s like, actually you kind of don’t. The entire universe of software is actually shockingly small, and it’s really about how to write these small programs as well as possible.&lt;/p&gt;
    &lt;p&gt;And also there’s another huge change. And so this is something that I don’t think that the programming language community has recognized yet, but AI coding has massively changed the game because now you can take a CUDA kernel and say, ‘Hey, Claude, go make that into Mojo.’&lt;/p&gt;
    &lt;p&gt;And actually, how good have you guys found the experience of that? Of doing translation?&lt;/p&gt;
    &lt;p&gt;Well, we do hackathons and people do amazing things, having never touched Mojo, having never done GPU programming, and within a day they can make things happen that are just shocking. Now, AI coding tools are not magic. You cannot just vibe code DeepSeek-R1 or something, right? But it’s amazing what that can do in terms of learning new languages, learning new tools, and getting into and catalyzing ecosystems. And so this is one of the things where, again, you go back five or 10 years—everybody knows nobody can learn a new language, and nobody’s willing to adopt new things. But the entire system has changed.&lt;/p&gt;
    &lt;p&gt;So let’s talk a little bit more in detail about the architecture of Mojo. What kind of language is Mojo, and what are the design elements that you chose in order to make it be able to address this set of problems?&lt;/p&gt;
    &lt;p&gt;Yeah, again, just to relate how different the situation is—back when I was working on Swift, one of the major problems to solve was, objective C was very difficult for people to use, and you had pointers, and you had square brackets, and it was very weird. And so the goal in the game of the day was, invent new syntax and bring together modern programming language features to build a new language. Fast forward to today, actually, some of that is true. AI people don’t like C++. C++ has pointers, and it’s ugly, and it’s a 40-year-old-plus language, and has actually the same problem that Swift had to solve back in the day. But today there’s something different, which is that AI people do actually love a thing. It’s called Python. And so, one of the really important things about Mojo is, it’s a member of the Python family. And so, this is polarizing to some, because yes—I get it that some people love curly braces, but it’s hugely powerful because so much of the AI community is Pythonic already.&lt;/p&gt;
    &lt;p&gt;And so we started out by saying, let’s keep the syntax like Python and only diverge from that if there’s a really good reason. But then what are the good reasons? Well, the good reasons are, we want—as we were talking about—performance, power, full control over the system. And for GPUs, there’s these very important things you want to do that require metaprogramming. And so Mojo has a very fancy metaprogramming system, kind of inspired by this language called Zig, that brings runtime and compile time together to enable really powerful library designs. And the way you crack open this problem with tensor cores and things like this, is you enable really powerful libraries to be built in the language as libraries, instead of hard coding into the compiler.&lt;/p&gt;
    &lt;p&gt;Let’s take it a little bit to the metaprogramming idea. What is metaprogramming and why does it matter for performance in particular?&lt;/p&gt;
    &lt;p&gt;Yeah, it’s a great question, and I think you know the answer to this too, and I know you, but—&lt;/p&gt;
    &lt;p&gt;[Laughs] We are also working on metaprogramming features in our own world.&lt;/p&gt;
    &lt;p&gt;Exactly. And so the observation here is, when you’re writing a for loop in a programming language, for example, typically that for loop executes at runtime, so you’re writing code that when you execute the program, it’s the instructions that the computer will follow to execute the algorithm within your code. But when you get into designing higher level type systems, suddenly you want to be able to run code at compile time as well. And so there’s many languages out there. Some of them have macro systems, C++ has templates. What you end up getting is, you end up getting, in many languages, this duality between what happens at runtime, and then a different language almost that happens at compile time. And C++ is the most egregious, because templates that you have a for loop in runtime, but then you have unrolled recursive templates, or something like that at compile time.&lt;/p&gt;
    &lt;p&gt;Well, so the insight is, hey, these two problems are actually the same. They just run at different times. And so what Mojo does is says, let’s allow the use of effectively any code that you would use at runtime to also work at compile time. And so you can have a list, or a string, or whatever you want in the algorithms—go do memory allocation, deallocation—and you can run those at compile time, enabling you to build really powerful high-level abstractions and put them into libraries. So why is this cool? Well, the reason it’s cool is that on a GPU, for example, you’ll have a tensor core. Tensor cores are weird. We probably don’t need to deep dive into all the reasons why, but the indexing and the layout that tensor cores use is very specific and very vendor different. And so the tensor core you have on AMD, or the tensor cores you have on different versions of Nvidia GPUs are all very different.&lt;/p&gt;
    &lt;p&gt;And so what you want, is you want to build as a GP programmer a set of abstractions so you can reason about all of these things in one common ecosystem and have the layouts much higher level. And so what this enables, it enables very powerful libraries—and very powerful libraries where a lot of the logic is actually done at compile time, but you can debug it because it’s the same language that you use at runtime. And it makes the language much more simpler, much more powerful, and just be able to scale into these complexities in a way that’s possible with C++. But in C++, you get some crazy template stack trace that is maddening and impossible to understand. In Mojo, you can get a very simple error message. You can actually debug your code, and debugger things like this.&lt;/p&gt;
    &lt;p&gt;So maybe an important point here is that metaprogramming is really an old solution to this performance problem. Maybe a good way of thinking about this is, imagine you have some piece of data that you have that represents a little embedded domain-specific language that you’ve written, that you want to execute via a program that you wrote. You can, in a nice high-level way, write a little interpreter for that language that just—you know, I have maybe a Boolean expression language or who knows what else. Maybe it’s a language for computing on tensors in a GPU. And you could write a program that just executes that mini domain-specific language and does the thing that you want and you can do it, but it’s really slow. Writing an interpreter is just inherently slow because of all this interpretation overhead where you are dynamically making decisions about what the behavior of the program is. And sometimes what you want, is, you just want to actually emit exactly the code that you want and boil away the control structure and just get the direct lines of machine code that you want to do the thing that’s necessary.&lt;/p&gt;
    &lt;p&gt;And various forms of code generation let you get past in a simpler way, lets you get past all of this control structure that you have to execute at runtime and instead be able to execute it at compile time and get this minified program that just does exactly the thing that you want. So that’s a really old idea. It goes back to all sorts of programming languages. There’s a lot of Lisps that did a lot of this metaprogramming stuff, but then the problem is this stuff is super hard to think about and reason about and debug. And that’s certainly true if you think about in C, all this macro language, if you use the various C preprocessors to do this kind of stuff in C, it’s pretty painful to reason about. And then C++ made it richer and more expressive, but still really hard to reason about. And you write a C++ template and you don’t really know what it’s going to do or if it’s going to compile until you give it all the inputs and let it go and it—&lt;/p&gt;
    &lt;p&gt;Feels good in the simple case. But then when you get to more advanced cases, suddenly the complexity compounds and it gets out of hand.&lt;/p&gt;
    &lt;p&gt;And it sounds like the thing that you’re going for in Mojo is it feels like one language. It has one type system that covers both the stuff you’re generating statically and the stuff that you’re doing at runtime. It sounds like debugging works in the same way across both of these layers, but you still get the actual runtime behavior you want from a language that you could more explicitly just be like, here’s exactly the code that I want to generate.&lt;/p&gt;
    &lt;p&gt;[…] metaprogramming is one of the fancy features. One of the cool features is it feels and looks like Python, but with actual types.&lt;/p&gt;
    &lt;p&gt;Right.&lt;/p&gt;
    &lt;p&gt;And let’s not forget the basics. Having something that looks and feels like Python but it’s a thousand times faster or something is actually pretty cool. For example, if you’re on a CPU, you have access to SIMD, the SIMD registers that allow you to do multiple operations at a time and [to] be able to get the full power of your hardware even without using the fancy features is also really cool. And so the challenge with any of these systems is, how do you make something that’s powerful, but it’s also easy to use? I think your team’s been playing with Mojo and doing some cool stuff. I mean, what have you seen and what’s your experience been?&lt;/p&gt;
    &lt;p&gt;We’re all still pretty new to it, but I think it’s got a lot of exciting things going for it. I mean, the first thing is, yeah, it gives you the kind of programming model you want to get the performance that you need. And actually, in many ways the same kind of programming model that you get out of something like CUTLASS or CuTe DSL, which are these Nvidia-specific, some at the C++ level, some at the Python DSL level—and by the way, every tool you can imagine nowadays is done once in C++ and once in Python. We don’t need to implement programming languages in any other way anymore. They’re all either skins on C++ or skins on Python. But depending on which path you go down, whether you go the C++ path or the Python path, you get all sorts of complicated trade-offs.&lt;/p&gt;
    &lt;p&gt;Like in the C++ path in particular, you get very painful compilation times. The thing you said about template metaprogramming is absolutely true. The error messages are super bad. If you look at these more Python-embedded DSLs, the compile times tend to be better. It still can be hard to reason about though. One nice thing about Mojo is the overall discipline seems very explicit when you want to understand: Is this a value that’s happening at execution time at the end, or is it a value that is going to be dealt with at compile time? It’s just very explicit in the syntax, you can look and understand. Whereas in some of these DSLs, you have to actively go and poke the value and ask it what kind of value it is. And I think that kind of explicitness is actually really important for performance engineering, making it easy to understand just what precisely you’re doing.&lt;/p&gt;
    &lt;p&gt;You actually see this a ton, not even with these very low-level things, but if you look at PyTorch, which is a much higher level tool, PyTorch does this thing where you get to write a thing that looks like an ordinary Python program, but really it’s got a much trickier execution model. Python’s an amazing and terrible ecosystem in which to do this kind of stuff, because what guarantees do you have when you’re using Python? None. What can you do? Anything. You have an enormous amount of freedom. The PyTorch people in particular have leveraged this freedom in a bunch of very clever ways, where you can write a Python program that looks like it’s doing something very simple and straightforward that would be really slow, but no—it’s very carefully delaying and making some operations lazy so it can overlap compute on the GPU and CPU and make stuff go really fast. And that’s really nice, except sometimes it just doesn’t work.&lt;/p&gt;
    &lt;p&gt;This is the trap again, this is my decades of battle scars now. So as a compiler guy, I can make fun of other compiler people. There’s this trap and it’s an attractive trap, which is called the ‘sufficiently smart compiler.’ And so what you can do is you can take something and you can make it look good on a demo and you can say, ‘Look! I make it super easy and I’m going to make my compiler super smart, and it’s going to take care of all this and make it easy through magic.’ But magic doesn’t exist. And so anytime you have one of those ‘sufficiently smart compilers,’ if you go back in the days, it was like auto-parallelization, just write C code is sequential logic, and then we’re going to automatically map it into running on 100 cores on a supercomputer or something like that.&lt;/p&gt;
    &lt;p&gt;They often actually do work, they work in very simple cases and they work in the demos. But the problem is that you go and you’re using them and then you change one thing and suddenly everything breaks. Maybe the compiler crashes, it just doesn’t work. Or you go and fix a bug and now instead of 100-times speedup, you get 100-times slowdown because it foiled the compiler. A lot of AI tools, a lot of these systems, particularly these DSLs, have this design point of, let me pretend like it’s easy and then I will take care of it behind the scenes. But then when something breaks, you have to end up looking at compiler dumps, right? And this is because magic doesn’t exist. And so this is where predictability and control is really, I think, the name of the game, particularly if you want to get the most out of a piece of hardware, which is how we ended up here.&lt;/p&gt;
    &lt;p&gt;It’s funny, the same issue of, “How clever is the underlying system you’re using?” comes up when you look at the difference between CPUs and GPUs. CPUs themselves are trying to do a weird thing where a chip is a fundamentally parallel substrate. It’s got all of these circuits that in principle could be running in parallel and then it is yoked to running this extremely sequential programming language, which is just trying to do one thing after another. And then how does that actually work with any reasonable efficiency? Well, there’s all sorts of clever dirty tricks happening under the covers where it’s trying to predict what you’re going to do, this speculation that allows it to dispatch multiple instructions in a row by guessing what you’re going to do in the future. There’s things like memory prefetching where it has heuristics to estimate what memory you’re going to ask in the future so it can dispatch multiple memory requests at the same time.&lt;/p&gt;
    &lt;p&gt;And then if you look at things like GPUs, and I think even more, TPUs, and then also totally other things like FPGAs, the field-programmable gate arrays where you put basically a circuit design on it. It’s a very different kind of software system. But all of them are in some sense simpler and more deterministic and more explicitly parallel. Like when you write down your program, you have to write an explicitly parallel program—that’s actually harder to write. I don’t want to complain too much about CPUs. The great thing about CPUs is they’re extremely flexible and incredibly easy to use and all of that dark magic actually works a pretty large fraction of the time.&lt;/p&gt;
    &lt;p&gt;Yeah, remarkably well. But your point here, I think it’s really great, and what you’re saying is, you’re saying CPUs are the magic box that makes sequential code go in parallel pretty fast. And then we have new, more explicit machines, somewhat harder to program because they’re not a magic box, but you get something from it. You get performance and power because that magic box doesn’t come without a cost. It comes with a very significant cost, often the amount of power that your machine dissipates. And so it’s not efficient. And so a lot of the reasons we’re getting these new accelerators is because people really do care about it being a hundred times faster, or using way less power, or things like this. And I’d never thought about it, but your analogy of Triton to Mojo kind of follows a similar pattern, right? Triton is trying to be the magic box, and it doesn’t give you the full performance, and it burns more power, and all that kind of stuff. And so Mojo is saying, look, let’s go back to being simple. Let’s give the programmer more control. And that more explicit approach, I think, is a good fit for people that are building crazy advanced hardware like you’re talking about—but also people that want to get the best performance out of the existing hardware we have.&lt;/p&gt;
    &lt;p&gt;So we talked about how metaprogramming lets you write faster programs by boiling away this control structure that you don’t really need. So that part’s good. How does it give you portable performance? How does it help you on the portability front?&lt;/p&gt;
    &lt;p&gt;Yeah, so this is another great question. So in this category of ‘sufficiently smart compilers,’ and particularly for AI compilers, there’s been years of work and MLIR has catalyzed a lot of this work building these magic AI compilers that take TensorFlow or even the new PyTorch stuff and trying to generate optimal code for some chip. So take some PyTorch model and put it through a compiler, and magically get out high performance. And so there’s tons of these things, and there’s a lot of great work done here, and a lot of people have shown that you can take kernels and accelerate them with compilers. The challenge with this is that people don’t ever measure—what is the full performance of the chip? And so people always measure from a somewhat unfortunate baseline and then try to climb higher instead of saying—what is the speed of light? And so if you measure from speed of light, suddenly you say, okay, how do I achieve several different things?&lt;/p&gt;
    &lt;p&gt;Even if you zero into one piece of silicon, how do I achieve the best performance for one use case? And then how do I make it so the software I write can generalize even within the domain? And so for example, take a matrix multiplication, well, you want to work on maybe float32, but then you want to generalize it to float16. Okay, well, templates and things like this are easy ways to do this. Then programming allows you to say, okay, I will tackle that. And then the next thing that happens is, because you went from float32 to float16, your effective cache size has doubled, because twice as many elements fit into cache if there’s 16 bits than if there are 32 bits. Well, if that’s the case, now suddenly the access pattern needs to change. And so you get a whole bunch of this conditional logic that now changes in a very parametric way as a result of one simple change that happened with float32 to float16.&lt;/p&gt;
    &lt;p&gt;Now you play that forward and you say, okay, well actually matrix multiplication is a recursive hierarchical problem. There’s specializations for tall and skinny matrices, and a dimension is one or something. There’s all these special cases. Just one algorithm for one chip becomes this very complicated subsystem that you end up wanting to do a lot of transformations to so you can go specialize it for different use cases. And so Mojo with the metaprogramming allows you to tackle that. Now you bring in other hardware, and so think of matrix multiplication these days as being almost an operating system, and there’s so many different subsystems, and special cases, and different D types, and crazy float4 and six and other stuff going on.&lt;/p&gt;
    &lt;p&gt;At some point they’re going to come out with a floating point number so small that it will be a joke. But every time I think that they’re just kidding, it turns out it’s real.&lt;/p&gt;
    &lt;p&gt;Seriously, I heard somebody talking about 1.2-bit floating point, right? It’s exactly like you’re saying, is that a joke? You can’t be serious. And so now when you bring in other hardware, other hardware brings in more complexity because suddenly the tensor core has a different layout in AMD than it does on Nvidia. Or maybe to your point about warps, you have 64 threads in a warp on one and 32 threads in a warp on the other. But what you realize is, wait a second—this really has nothing to do with hardware vendors. This is actually true even within, for example, the Nvidia line, because across these different data types, the tensor cores are changing. The way the tensor core works for float32 is different from the way it works for float4 or something. And so you already—within one vendor—have to have this very powerful metaprogramming to be able to handle the complexity and do so in the scaffolding of a single algorithm like matrix multiplication.&lt;/p&gt;
    &lt;p&gt;And so now as you bring in other vendors, well it turns out hey, they all have things that look roughly like tensor cores. And so we’re coming at this with a software engineering perspective, and so we’re forced to build abstractions. We have this powerful metaprogramming system so we can actually achieve this. And so even for one vendor, we get this thing called LayoutTensor. LayoutTensor is saying, okay, well I have the ability to reason about not just an array of numbers or a multidimensional array of numbers, but also how it’s laid out in memory and how it gets accessed. And so now we can declaratively map these things onto the hardware that you have and these abstractions stack. And so it’s this really amazing triumvirate between having a type system that works well and this very important basis. I know you’re a fan of type systems also.&lt;/p&gt;
    &lt;p&gt;You then bring in metaprogramming, and so you can build powerful abstractions and run a compile time so you get no runtime overhead. And then you bring in the most important part of this entire equation, which is programmers who understand the domain. I am not going to write a fast matrix multiplication. I’m sorry, that’s not my experience. But there are people in that space that are just fricking brilliant. They understand exactly how the hardware works, they understand the use cases and the latest research and the new crazy quantized format of the day, but they’re not compiler people. And so the magic of Mojo is it says, ‘Hey, you have a type system, you have metaprogramming, you have effectively the full power of a compiler that you have when you’re building libraries.’ And so now these people that are brilliant at unlocking the power of the hardware can actually do this. And now they can write software that scales both across the complexity of the domain but also across hardware. And to me, that’s what I find so exciting and so powerful about this. It’s unlocking the power of the Mojo programmer instead of trying to put it into the compiler, which is what a lot of earlier systems have tried to do.&lt;/p&gt;
    &lt;p&gt;So maybe the key point here is that you get to build these abstractions that allow you to represent different kinds of hardware, and then you can conditionally have your code execute based on the kind of hardware that it’s on. It’s not like an #ifdef where you’re picking between different hardware platforms. There are complicated data structures like these layout values that tell you how you traverse data.&lt;/p&gt;
    &lt;p&gt;Which is kind of a tree. This isn’t just a simple int that you’re passing around. This is like a recursive hierarchical tree that you need at compile time.&lt;/p&gt;
    &lt;p&gt;The critical thing is you get to write a thing that feels like one synthetic program with one understandable behavior, but then parts of it are actually going to execute at compile time, so that the thing that you generate is in fact specialized for the particular platform that you’re going to run it on. So one concern I have over this is it sounds like the configuration space of your programs is going to be massive, and I feel like there are two directions where this seems potentially hard to do from an engineering perspective. One is, can you really create abstractions that within the context of the program hide the relevant complexity? So it’s possible for people to think in a modular way about the program they’re building, so their brains don’t explode with the 70 different kinds of hardware that they might be running it on. And then the other question is, how do you think about testing? Because there’s just so many configurations. How do you know whether it’s working in all the places? Because it sounds like it has an enormous amount of freedom to do different things, including wrong things in some cases. How do you deal with those two problems, both controlling the complexity of the abstractions and then having a testing story that works out?&lt;/p&gt;
    &lt;p&gt;Okay, Ron, I’m going to blow your mind. I know you’re going to be resistant to this, but let me convince you that types are cool.&lt;/p&gt;
    &lt;p&gt;Okay!&lt;/p&gt;
    &lt;p&gt;I know you’re going to fight me on this. Well, so this is again, you go back to the challenges and opportunities of working with either Python or C++. Python doesn’t have types really. I mean it has some stuff, but it doesn’t really have a type system. C++ has a type system, but it’s just incredibly painful to work with. And so what Mojo does is it says, again, it’s not rocket science. We see it all around us. Let’s bring in traits. Let’s bring in a reasonable way to write code so that we can build abstractions that are domain-specific and they can be checked modularly. And so one of the big problems with C++ is that you get error messages when you instantiate layers and layers and layers and layers of templates. And so if you get some magic number wrong, it explodes spectacularly in a way that you can’t reason about. And so what Mojo does, it says, cool, let’s bring in traits that feel very much like protocols in Swift, or traits in Rust, or type classes in Haskell. Like, this isn’t novel.&lt;/p&gt;
    &lt;p&gt;This is like a mechanism for what’s called ad hoc polymorphism, meaning I want to have some operation or function that has some meaning, but actually it’s going to get implemented in different ways for different types. And these are basically all mechanisms of a way of, given the thing that you’re doing and the types involved, looking up the right implementation that’s going to do the thing that you want.&lt;/p&gt;
    &lt;p&gt;Yeah, I mean a very simple case is an iterator. So Mojo has an iterator trait and you can say, ‘Hey, what is an iterator over a collection?’ Well, you can either check, see if there’s an element, or you can get the value at the current element. And then as you keep pulling things out of an iterator, it will eventually decide to stop. And so this concept can be applied to things like a linked list, or an array, or a dictionary, or an unbounded sequence of packets coming off a network. And so you can write code that’s generic across these different—call them “backends” or “models”—that implement this trait. And what the compiler will do for you is it will check to make sure when you’re writing that generic code, you’re not using something that won’t work. And so what that does, is it means that you can check the generic code without having to instantiate it, which is good for compile time. It’s good for user experience, because if you get something wrong as a programmer, that’s important. It’s good for reasoning about the modularity of these different subsystems, because now you have an interface that connects the two components.&lt;/p&gt;
    &lt;p&gt;I think it’s an underappreciated problem with the C++ templates approach to the world, where C++ templates seem like a deep language feature, but really they’re just a code generation feature.&lt;/p&gt;
    &lt;p&gt;They’re like C macros.&lt;/p&gt;
    &lt;p&gt;That’s right. It both means they’re hard to think about and reason about because it sort of seems at first glance not to be so bad—this property that you don’t really know when your template expands, if it’s actually going to compile. But as you start composing things more deeply, it gets worse and worse because something somewhere is going to fail, and it’s just going to be hard to reason about and understand. Whereas when you have type-level notions of genericity that are guaranteed to compose correctly and won’t just blow up, you just drive that error right down. So that’s one thing that’s nice about getting past templates as a language feature. And then the other thing is it’s just crushingly slow. You’re generating the code, almost exactly the same code, over and over and over again. And so that just means you can’t save any of the compilation work. You just have to redo the whole thing from scratch.&lt;/p&gt;
    &lt;p&gt;That’s exactly right. And so this is where again, we were talking about the sand in the system—these little things that if you get wrong, they play forward and they cause huge problems. The metaprogramming approach in Mojo is cool, both for usability and compile time and correctness. Coming back to your point about portability, it’s also valuable for portability because what it means is that the compiler parses your code, and it parses it generically and has no idea what the target is. And so when Mojo generates the first level of intermediate representation, the compiler representation for the code, it’s not hard coding and the pointers are 32 bit or 64 bit, or that you’re on a x86 or whatever. And what this means is that you can take generic code in Mojo and you can put it on a CPU and you can put it on a GPU. Same code, same function. And again, these crazy compilery things that Chris gets obsessed about, it means that you can slice out the chunk of code that you want to put onto your GPU in a way that it looks like a distributed system, but it’s a distributed system where the GPU is actually a crazy embedded device that wants this tiny snippet of code and it wants it fully self-contained. These worlds of things that normal programming languages haven’t even thought about.&lt;/p&gt;
    &lt;p&gt;So does that mean when I compile a Mojo program, I get a shippable executable that contains within it another little compiler that can take the Mojo code and specialize it to get the actual machine code for the final destination that you need? Do I bundle together all the compilers for all the possible platforms in every Mojo executable?&lt;/p&gt;
    &lt;p&gt;The answer is no. The world’s not ready for that. And there are use cases for JIT compilers and things like this, and that’s cool, but the default way of building, if you just run mojo build, then it will give you just an a.out executable, a normal thing. But if you build a Mojo package, the Mojo package retains portability. This is a big difference. This is what Java does. If you think about Java in a completely different way and for different reasons in a different ecosystem universe, it parses all your source code without knowing what the target is, and it generates Java bytecode. And so it’s not 1995 anymore. The way we do this is completely different. And we’re not Java obviously, and we have a type system that’s very different. But this concept is something that’s been well known, and is something that at least the world of compiled languages like Swift, and C++, and Rust have kind of forgotten.&lt;/p&gt;
    &lt;p&gt;So the Mojo package is kind of shipped with the compiler technology required to specialize to the different domains.&lt;/p&gt;
    &lt;p&gt;Yes. And so again, by default, if you’re a user, you’re sitting on your laptop and you say, ‘Compile a Mojo program,’ you just want an executable. But the compiler technology has all of these powerful features and they can be used in different ways. This is similar to LLVM, where LLVM had a just-in-time compiler, and that’s really important if you’re Sony Pictures and you’re rendering shaders for some fancy movie, but that’s not what you’d want to use if you’re just running a C++ code that needs to be ahead-of-time compiled.&lt;/p&gt;
    &lt;p&gt;I mean, there’s some echoes here also of the PTX story with Nvidia. Nvidia has this thing that they sort of hide that it’s an intermediate representation, but this thing called PTX, which is a portable bytecode essentially. And they for many years maintained compatibility across many, many different generations of GPUs. They have a thing called the assembler that’s part of the driver thing for loading on, and it’s really not an assembler. It’s like a real compiler that takes the PTX and compiles it down to SASS, the accelerator-specific machine code, which they very carefully do not fully document because they don’t want to give away all of their secrets. And so there’s a built-in portability story there where it’s meant to actually be portable in the future across new generations. Although as you were pointing out before, it in fact doesn’t always succeed. And there are now some programs that will not actually make the transition to Blackwell.&lt;/p&gt;
    &lt;p&gt;So that’s in the category that I’d consider to be like a virtual machine, a very low-level virtual machine by the way. And so when you’re looking at these systems, the thing I’d ask is, what is the type system? And so if you look at PTX, because as you’re saying, you’re totally right, it’s an abstraction between a whole bunch of source code on the top end and then that specific SASS hardware thing on the backend, but the type system isn’t very interesting. It’s pointers and registers and memory. And so Java, what is the type system? Well, Java achieves portability by making the type system in its bytecode expose objects. And so it’s a much higher level abstraction, dynamic virtual dispatch, that’s all part of the Java ecosystem. It’s not a bytecode, but the representation that’s portable maintains the full generic system. And so this is what makes it possible to say, ‘Okay, well I’m going to take this code, compile it once to a package, and now go specialize and instantiate this for a device.’ So the way that works is a little bit different, but it enables, coming back to your original question of safety and correctness, it enables all the checking to happen the right way.&lt;/p&gt;
    &lt;p&gt;Right, there’s also a huge shift in control. With PTX, the machine-specific details of how it’s compiled are totally out of the programmer’s control. You can generate the best PTX you can, and then it’s going to get compiled. How? Somehow, don’t ask too many questions, it’s going to do what it’s going to do. Whereas here, you’re preserving in the portable object, the programmer-driven instructions about how the specialization is going to work. You’ve just partially executed your compilation, you’ve got partway down, and then there’s some more that’s going to be done at the end when you pick actually where you’re going to run it.&lt;/p&gt;
    &lt;p&gt;Exactly. And so these are all very nerdy pieces that go into the stack, but the thing that I like is if you bubble out of that, it’s easy to use. It works. It gives good error messages, right? I don’t understand the Greek letters, but I do understand a lot of the engineering that goes into this. The way this technology stack builds up, the whole purpose is to unlock compute, and we want new programmers to be able to get into the system. And if they know Python, if they understand some of the basics of the hardware, they can be effective and then they don’t get limited to 80% of the performance. They can keep driving and keep growing in sophistication, and maybe not everybody wants to do that. They can stop at 80%, but if you do want to go all the way, then you can get there.&lt;/p&gt;
    &lt;p&gt;One thing I’m curious about is, how do you actually manage to keep it simple? You said that Mojo is meant to be Pythonic and you talked a bunch about the syntax, but actually one of the nice things about Python is it’s simple in some ways in a deeper sense. The fact that there isn’t by default a complicated type system with complicated type errors to think about—there’s a lot of problems with that, but it’s also a real source of simplicity for users who are trying to learn the system. Dynamic errors at runtime are in some ways easier to understand. ‘I wrote a program and it tried to do a thing and it tripped over this particular thing and you can see it tripping over,’ and in some ways that’s easier to understand when you’re going to a language which, for both safety and performance reasons, needs much more precise type level control. How do you do that in a way that still feels Pythonic in terms of the base simplicity that you’re exposing to users?&lt;/p&gt;
    &lt;p&gt;I can’t give you the perfect answer, but I can tell you my current thoughts. So again, learn from history. Swift had a lot of really cool features, but it spiraled and got a lot of complexity that got layered in over time. And also one of the challenges with Swift is it had a team that was paid to add features to swift.&lt;/p&gt;
    &lt;p&gt;It’s never a good thing.&lt;/p&gt;
    &lt;p&gt;Well, you have a C++ committee, what is the C++ committee going to do? They’re going to keep adding features to C++. Don’t expect C++ to get smaller. It’s common sense. And so with Mojo, there’s a couple of different things. So one of which is, start from Python. So Python being the surface-level syntax enables me as management to be able to push back and say, ‘Look, let’s make sure we’re implementing the full power of the Python ecosystem. Let’s have lists, and for-comprehensions, and all this stuff before just inventing random stuff because it might be useful.’ But there’s also, for me personally, a significant back pressure on complexity. How can we factor these things? How can we get, for example, the metaprogramming system to subsume a lot of complexity that would otherwise exist? And there are fundamental things that I want us to add.&lt;/p&gt;
    &lt;p&gt;For example, checked generics, things like this because they have a better UX, they’re part of the metaprogramming system, they’re part of the core addition that we’re adding, but I don’t want Mojo to turn into a ‘add every language feature’ that every other language has just because it’s useful to somebody. I was actually inspired by and learned a lot from Go, and it’s a language that people are probably surprised to hear me talk about. Go, I think, did a really good job of intentionally constraining the language with Go 1. And they took a lot of heat for that. They didn’t add a generic system, and everybody, myself included, were like, ‘Ha ha ha, why doesn’t this language even have a generic system? You’re not even a modern language.’ But they held the line, they understood how far people could get, and then they did a really good job of adding generics to Go 2, and I thought they did a great job.&lt;/p&gt;
    &lt;p&gt;There was a recent blog post I was reading, talking about Go, and apparently they have an 80-20 rule, and they say they want to have 80% of the features with 20% of the complexity, something like that. And the observation is that that’s a point in the space that annoys everybody, because everybody wants 81% of the features, but 81% of the features maybe gives you 35% of the complexity. And so, figuring out where to draw that line and figuring out where to say no—for example, we have people in the community that are asking for very reasonable things that exist in Rust. And Rust is a wonderful language. I love it. There’s a lot of great ideas and we shamelessly pull good ideas from everywhere. But I don’t want the complexity.&lt;/p&gt;
    &lt;p&gt;I often like to say that one of the most critical things about a language design is maintaining the power-to-weight ratio.&lt;/p&gt;
    &lt;p&gt;You want to get an enormous amount of good functionality, and power, and good user experience while minimizing that complexity. I think it is a very challenging thing to manage, and it’s actually a thing that we are seeing a lot as well. We are also doing a lot to extend OCaml in all sorts of ways, pulling from all sorts of languages, including Rust, and again, doing it in a way where the language maintains its basic character and maintains its simplicity is a real challenge. And it’s kind of hard to know if you’re hitting the actual right point on that. And it’s easier to do in a world where you can take things back, try things out and decide that maybe they don’t work, and then adjust your behavior. And we’re trying to iterate a lot in that mode, which is a thing you can do under certain circumstances. It gets harder as you have a big open-source language that lots of people are using.&lt;/p&gt;
    &lt;p&gt;That’s a really great point. And so one of the other lessons I’ve learned with Swift, is that with Swift, I pushed very early to have an open design process where anybody could come in, write a proposal, and then it would be evaluated by the language committee, and then if it was good, it would be implemented and put into Swift. Again, be careful what you wish for. That enabled a lot of people with really good ideas to add a bunch of features to Swift. And so with Mojo as a counterbalance, I really want the core team to be small. I want the core team not just to be able to add a whole bunch of stuff because it might be useful someday, but to be really deliberate about how we add things, how we evolve things.&lt;/p&gt;
    &lt;p&gt;How are you thinking about maintaining backwards compatibility guarantees as you evolve it forward?&lt;/p&gt;
    &lt;p&gt;We’re actively debating and discussing what Mojo 1.0 looks like. And so I’m not going to give you a timeframe, but it will hopefully not be very far away. And what I am fond of is this notion of semantic versioning, and saying we’re going to have a 1.0, and then we’re going to have a 2.0, and we’re going to have a 3.0, and we’re going to have a 4.0, et cetera. And each of these will be able to be incompatible, but they can link together. And so one of the big challenges and a lot of the damage in the Python ecosystem was from the Python two-to-three conversion. It took 15 years and it was a heroic mess for many different reasons. The reason it took so long is because you have to convert the entire package ecosystem before you can be 3.0. And so if you contrast that to something like C++, let me say good things about C++, they got the ABI right.&lt;/p&gt;
    &lt;p&gt;And so once the ABI was set, then you could have one package built in C++ 98, and one package built in C++ 23, and these things would interoperate and be compatible even if you took new keywords or other things in the future language version. And so what I see for Mojo is much more similar to the—maybe the C++ ecosystem or something like this, but that allows us to be a little bit more aggressive in terms of migrating code, in terms of fixing bugs, and in moving language forward. But I want to make sure that Mojo 2.0 and Mojo 1.0 packages work together and that there’s good tooling, probably AI-driven, but good tooling to move from 1.0 to 2.0 and be able to manage the ecosystem that way.&lt;/p&gt;
    &lt;p&gt;I think the type system also helps an enormous amount. I think one of the reasons the Python migration was so hard is that you couldn’t be like, ‘And then let me try and build this with Python 3 and see what’s broken.’ You could only see what’s broken by actually walking all of the execution paths of your program. And if you didn’t have enough testing, that would be very hard. And even if you did, it wasn’t that easy. Whereas with a strong type system, you can get an enormous amount of very precise guidance. And actually the combination of a strong type system and an agentic coding system is awesome. We actually have a bunch of experience of just trying these things out now, where you make some small change to the type of something and then you’re like, ‘Hey, AI system, please run down all the type errors, fix them all.’ And it does surprisingly well.&lt;/p&gt;
    &lt;p&gt;I absolutely agree. There’s other components to it. So Rust has done a very good job with the stabilization approach with crates and APIs. And I think that’s a really good thing. And so I think we’ll take good ideas from many of these different ecosystems and hopefully do something that works well, and works well for the ecosystem, and allows us to scale without being completely constrained by never being able to fix something once you ship a 1.0.&lt;/p&gt;
    &lt;p&gt;I’m actually curious, just to go to the agentic programming thing for a second, which is having AI agents that write good kernels is actually pretty hard. And I’m curious what your experience is of how things work with Mojo. Mojo is obviously not a language deeply embedded in the training set that these models were built on, but on the other hand, you have this very strong type structure that can guide the process of the AI agent trying to write and modify code. I’m curious how that pans out in practice as you try and use these tools.&lt;/p&gt;
    &lt;p&gt;So this is why Mojo being open source, and—so we have hundreds of thousands of lines of Mojo code that are public with all these GPU kernels, and like, all this other cool stuff. And we have a community of people writing more code. Having hundreds of thousand lines of Mojo code is fantastic. You can point your coding tool cursor, or whatever it is, at that repo and say, ‘Go learn about this repo and index it.’ So it’s not that you have to train the model to know the language, just having access to it—that enables it to do good work. And these tools are phenomenal. And so that’s been very, very, very important. And so we have instructions on our webpage for how to set up these tools, and there’s a huge difference if you set it up right, so that it can index that, or if you don’t, and make sure to follow that markdown file that explains how to set up the tool.&lt;/p&gt;
    &lt;p&gt;So, I want to talk a little bit about the future of Mojo. I think that the current way that Modular and you have been talking about Mojo, these days at least—it’s a replacement for CUDA, an alternate full top-to-bottom stack for building GPU kernels, for writing programs that execute on GPUs. But that’s not the only way you’ve ever talked about Mojo. You’ve also, especially earlier on I think, there was more discussion of Mojo as an extension, and maybe evolution of, and maybe eventually replacement of Python. And I’m curious, how do you think about that now? To what degree do you think of Mojo as its own new language that takes inspiration and syntax from Python, and to what degree do you want something that’s more deeply integrated over time?&lt;/p&gt;
    &lt;p&gt;So today, to pull it back to, ‘What is Mojo useful for today, and how do we explain it?’ Mojo is useful if you want code to go fast. If you have code on a CPU or a GPU and you want it to go fast, Mojo is a great thing. One of the really cool things that is available now—but it’s in preview and it’ll solidify in the next month or something—is it’s also the best way to extend Python. And so if you have a large-scale Python code base, again, tell me if this sounds familiar, you are coding away and you’re doing cool stuff in Python and then it starts to get slow. Typically what people do is, they have to either go rewrite the whole thing in Rust or C++, or they carve out some chunk of it and move some chunk of that package to C++ or Rust. This is what NumPy, or PyTorch, or all modern large-scale Python code bases end up doing.&lt;/p&gt;
    &lt;p&gt;If you look up on the mirrors and look at the percentage of programs that have C extensions in them, it’s shockingly high. A really large fraction of Python stuff is actually part Python and part some other language, almost always C and C++, a little bit of Rust.&lt;/p&gt;
    &lt;p&gt;That’s right. And so today—this isn’t distant future—today, you can take your Python package and you can create a Mojo file and you can say, ‘Okay, well these for loops are slow, move it over to Mojo.’ And we have people, for example, doing bioinformatics and other crazy stuff I know nothing about, saying, ‘Okay, well I’m just taking my Python code, I move it over to Mojo. Wow, now I get types, I get these benefits, but there’s no bindings. The pip experience is beautiful. It’s super simple.’ You don’t have to have FFI’s and nanobind and all this complexity to be able to do this. You also are not moving from Python with its syntax to curly braces and borrow checkers and other craziness. You now get a very simple and seamless way to extend your Python package. And we have people that say, okay, well I did that and I got it first 10x, and 100x, and 1000x faster on CPU.&lt;/p&gt;
    &lt;p&gt;But then because it was easy, I just put it on a GPU. And so to me, this is amazing because these are people that didn’t even think and would never have gotten it on a GPU if they switched to Rust or something like that. Again, the way I explain it is, Mojo is good for performance. It’s good if you want to go fast on a GPU, on a CPU, if you want to make Python go fast, or if you want to—I mean, some people are crazy enough to go whole hog and just write entirely from scratch Mojo programs, and that’s super cool. If you fast forward six, nine months, something, I think that Mojo will be a very credible top-to-bottom replacement for Rust.&lt;/p&gt;
    &lt;p&gt;And so we need a few more extensions to the generic system. And there’s a few things I want to bake out a little bit. Some of the dynamic features that Rust has for the existentials, the ability to make a runtime trait is missing in Mojo. And so we’ll add a few of those kinds of features. And as we do that, I think that’ll be really interesting as an applications-level programming language for people who care about this kind of stuff. You fast forward, I might even project a timeframe, maybe a year, 18 months from now, it depends on how we prioritize things, and we’ll add classes. And so as we add classes, suddenly it will look and feel to a Python programmer much more familiar. The classes in Mojo will be intentionally designed to be very similar to Python, and at that point we’ll have something that looks and feels kind of like a Python 4.&lt;/p&gt;
    &lt;p&gt;It’s very much cut from the same mold as Python. It integrates really well from Python. It’s really easy to extend Python, and so it’s very much a member of the Python family, but it’s not compatible with Python. And so what we’ll do over the course of N years, and I can’t predict exactly how long that is, is continue to run down the line of, okay, well how much compatibility do we want to add to this thing? And then I think that at some point people will consider it to be a Python superset, and effectively it will feel just like the best way to do Python in general. And I think that that will come in time. But to bring it all the way back, I want us to be very focused on, ‘What is Mojo useful for today?’ Great claims require great proof.&lt;/p&gt;
    &lt;p&gt;We have no proof that we can do this. I have a vision and a future in my brain, and I’ve built a few languages and some scale things before, and so I have quite high confidence that we can do this. But I want people to zero back into, okay, if you’re writing performance code, if you’re writing GPU kernels or AI, if you have Python code, you don’t want it to go slow, a few of us have that problem, then Mojo can be very useful. And hopefully it’ll be even more useful to more people in the future.&lt;/p&gt;
    &lt;p&gt;And I think that already, the practical short-term thing is already plenty ambitious and exciting on its own. Seems like a great thing to focus on.&lt;/p&gt;
    &lt;p&gt;Yeah, let’s solve heterogeneous compute and AI. That’s actually a pretty useful thing, right?&lt;/p&gt;
    &lt;p&gt;Alright, that seems like a great place to stop. Thank you so much for joining me.&lt;/p&gt;
    &lt;p&gt;Yeah, well thank you for having me. I love nerding out with you and I hope it’s useful and interesting to other people too. But even if not, I had a lot of fun with you.&lt;/p&gt;
    &lt;p&gt;You’ll find a complete transcript of the episode along with show notes and links at signalsandthreads.com. Thanks for joining us. See you next time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://signalsandthreads.com/why-ml-needs-a-new-programming-language/"/></entry><entry><id>https://news.ycombinator.com/item?id=45137525</id><title>I ditched Docker for Podman</title><updated>2025-09-06T03:05:56.502958+00:00</updated><content/><link href="https://codesmash.dev/why-i-ditched-docker-for-podman-and-you-should-too"/></entry><entry><id>https://news.ycombinator.com/item?id=45139088</id><title>Purposeful animations</title><updated>2025-09-06T03:05:55.957983+00:00</updated><content>&lt;doc fingerprint="f7eb51a5ad29df05"&gt;
  &lt;main&gt;&lt;p&gt;When done right, animations make an interface feel predictable, faster, and more enjoyable to use. They help you and your product stand out.&lt;/p&gt;&lt;p&gt;But they can also do the opposite. They can make an interface feel unpredictable, slow, and annoying. They can even make your users lose trust in your product.&lt;/p&gt;&lt;p&gt;So how do you know when and how to animate to improve the experience?&lt;/p&gt;&lt;p&gt;Step one is making sure your animations have a purpose.&lt;/p&gt;&lt;head rend="h2"&gt;Purposeful animations&lt;/head&gt;&lt;p&gt;Before you start animating, ask yourself: what’s the purpose of this animation? &lt;lb/&gt;As an example, what’s the purpose of this marketing animation we built at Linear?&lt;/p&gt;&lt;p&gt;You can view the full animation on linear.app/ai.&lt;/p&gt;&lt;p&gt;This animation explains how Product Intelligence (Linear’s feature) works. We could have used a static asset, but the animated version helps the user understand what this feature does, straight in the initial viewport of the page.&lt;/p&gt;&lt;p&gt;Another purposeful animation is this subtle scale down effect when pressing a button. It’s a small thing, but it helps the interface feel more alive and responsive.&lt;/p&gt;&lt;p&gt;Sonner’s enter animation, on the other hand, has two purposes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;- Having a toast suddenly appear would feel off, so we animate it in.&lt;/item&gt;&lt;item&gt;- Because it comes from and leaves in the same direction, it creates spatial consistency, making the swipe-down-to-dismiss gesture feel more intuitive.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;But sometimes the purpose of an animation might just be to bring delight.&lt;/p&gt;&lt;p&gt;Morphing of the feedback component below helps make the experience more unique and memorable. This works as long as the user will rarely interact with it. It’ll then become a pleasant surprise, rather than a daily annoyance.&lt;/p&gt;&lt;p&gt;Press on the button to see it morph.&lt;/p&gt;&lt;p&gt;Used multiple times a day, this component would quickly become irritating. The initial delight would fade and the animation would slow users down.&lt;/p&gt;&lt;p&gt;How often users will see an animation is a key factor in deciding whether to animate or not. Let’s dive deeper into it next.&lt;/p&gt;&lt;head rend="h2"&gt;Frequency of use&lt;/head&gt;&lt;p&gt;I use Raycast hundreds of times a day. If it animated every time I opened it, it would be very annoying. But there’s no animation at all. That’s the optimal experience.&lt;/p&gt;&lt;p&gt;To see it for yourself, try to toggle the open state of the menu below by using the buttons belowpressing &lt;code&gt;J&lt;/code&gt; and then &lt;code&gt;K&lt;/code&gt;. Which one feels better if used hundreds of times a day?&lt;/p&gt;&lt;p&gt;When I open Raycast, I have a clear goal in mind. I don’t expect to be “delighted”, I don’t need to be. I just want to do my work with no unnecessary friction.&lt;/p&gt;&lt;p&gt;Think about what the user wants to achieve and how often they will see an animation. A hover effect is nice, but if used multiple times a day, it would likely benefit the most from having no animation at all.&lt;/p&gt;&lt;p&gt;Imagine you interact with this list often during the day.&lt;/p&gt;&lt;p&gt;Imagine you interact with this list often during the day.&lt;/p&gt;&lt;p&gt;The same goes for keyboard-initiated actions. These actions may be repeated hundreds of times a day, an animation would make them feel slow, delayed, and disconnected from the user’s actions. You should never animate them.&lt;/p&gt;&lt;p&gt;Since we can’t really use a keyboard on touch devices, you can press the buttons below to see how it feels with and without animation.&lt;/p&gt;&lt;p&gt;To see it for yourself, focus on the input below and use arrow keys to navigate through the list. Notice how the highlight feels delayed compared to the keys you press. Now press (shift) and see how this interaction feels without animation.&lt;/p&gt;&lt;p&gt;But even if your animation won’t be used too often and it fulfills a clear purpose, you still have to think about its speed…&lt;/p&gt;&lt;head rend="h2"&gt;Perception of speed&lt;/head&gt;&lt;p&gt;Unless you are working on marketing sites, your animations have to be fast. They improve the perceived performance of your app, stay connected to user’s actions, and make the interface feel as if it’s truly listening to the user.&lt;/p&gt;&lt;p&gt;To give you an example, a faster-spinning spinner makes the app seem to load faster, even though the load time is the same. This improves perceived performance.&lt;/p&gt;&lt;p&gt;Which one works harder to load the data?&lt;/p&gt;&lt;p&gt;A &lt;code&gt;180ms&lt;/code&gt; dropdown animation feels more responsive than a &lt;code&gt;400ms&lt;/code&gt; one:&lt;/p&gt;&lt;p&gt;Click on the buttons to compare the speed.&lt;/p&gt;&lt;p&gt;As a rule of thumb, UI animations should generally stay under &lt;code&gt;300ms&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Another example of the importance of speed: tooltips should have a slight delay before appearing to prevent accidental activation. Once a tooltip is open however, hovering over other tooltips should open them with no delay and no animation.&lt;/p&gt;&lt;p&gt;This feels faster without defeating the purpose of the initial delay.&lt;/p&gt;&lt;p&gt;Radix UI and Base UI skip the delay once a tooltip is shown.&lt;/p&gt;&lt;p&gt;Radix UI and Base UI skip the delay once a tooltip is shown.&lt;/p&gt;&lt;head rend="h2"&gt;Building great interfaces&lt;/head&gt;&lt;p&gt;The goal is not to animate for animation’s sake, it’s to build great user interfaces. The ones that users will happily use, even on a daily basis. Sometimes this requires animations, but sometimes the best animation is no animation.&lt;/p&gt;&lt;p&gt;Knowing when to animate is just one of many things you need to know in order to craft great animations. If you’d like to dive deeper into the theory and practice of it, I’ve created a course that covers everything you need to know:&lt;/p&gt;Check out "Animations on the Web"&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://emilkowal.ski/ui/you-dont-need-animations"/></entry><entry><id>https://news.ycombinator.com/item?id=45140381</id><title>MentraOS – open-source Smart glasses OS</title><updated>2025-09-06T03:05:55.252891+00:00</updated><content>&lt;doc fingerprint="d2f052307bbfb732"&gt;
  &lt;main&gt;
    &lt;p&gt;Works with Even Realities G1, Mentra Mach 1, Mentra Live. See smart glasses compatibility list here.&lt;/p&gt;
    &lt;p&gt;The Mentra Store already has a ton of useful apps that real users are running everyday. Here are some apps already published by developers on the Mentra Store:&lt;/p&gt;
    &lt;p&gt;MentraOS is how developers build smart glasses apps. We handle the pairing, connection, data streaming, and cross-compatibility, so you can focus on creating amazing apps. Every component is 100% open source (MIT license).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cross Compatibility: Your app runs on any pair of smart glasses&lt;/item&gt;
      &lt;item&gt;Speed: TypeScript SDK means you're making apps in minutes, not months&lt;/item&gt;
      &lt;item&gt;Control: Access smart glasses I/O - displays, microphones, cameras, speakers&lt;/item&gt;
      &lt;item&gt;Distribution: Get your app in front of everyone using smart glasses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MentraOS Community is a group of developers, companies, and users dedicated to ensuring the next personal computer is open, cross-compatible, and user-controlled. That's why we're building MentraOS.&lt;/p&gt;
    &lt;p&gt;To get involved, join the MentraOS Community Discord server.&lt;/p&gt;
    &lt;p&gt;Have questions or ideas? We'd love to hear from you!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Email: team@mentra.glass&lt;/item&gt;
      &lt;item&gt;Discord: Join our community&lt;/item&gt;
      &lt;item&gt;Twitter: Follow @mentralabs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MentraOS is made by a community and we welcome PRs. Here's the Contributors Guide: docs.mentra.glass/contributing&lt;/p&gt;
    &lt;p&gt;MIT License Copyright 2025 MentraOS Community&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Mentra-Community/MentraOS"/></entry><entry><id>https://news.ycombinator.com/item?id=45140730</id><title>European Commission fines Google €2.95B over abusive ad tech practices</title><updated>2025-09-06T03:05:54.698388+00:00</updated><link href="https://ec.europa.eu/commission/presscorner/detail/en/ip_25_1992"/></entry><entry><id>https://news.ycombinator.com/item?id=45140786</id><title>Freeway guardrails are now a favorite target of thieves</title><updated>2025-09-06T03:05:54.524262+00:00</updated><content>&lt;doc fingerprint="e7584515bd97c7f9"&gt;
  &lt;main&gt;
    &lt;p&gt;Congress has cut federal funding for public media — a $3.4 million loss for LAist. We count on readers like you to protect our nonprofit newsroom. Become a monthly member and sustain local journalism.&lt;/p&gt;
    &lt;head rend="h1"&gt;Freeway guardrails are there to protect us. They're now a favorite target of thieves&lt;/head&gt;
    &lt;p&gt;On a recent Thursday evening, traffic was slow on the 10 Freeway as cars crawled into the downtown Los Angeles area. It was an average commute back to Pasadena for Bryan Gonzalez until he saw a man on the other side of the freeway cutting through a guardrail with a reciprocating saw.&lt;/p&gt;
    &lt;p&gt;“OK, there's something weird going on,” Gonzalez said about his observations from Aug. 21, which he captured on video and reported to Caltrans. “I was debating whether or not saying anything, but I said, ‘Hey, I'm in slow traffic. He has a saw. No, thank you.’”&lt;/p&gt;
    &lt;p&gt;What Gonzalez observed was not a fluke. Guardrail theft is a problem that has been on the rise for the last eight years, according to the local Caltrans office covering L.A. and Ventura counties. Over the last two years, the state transportation agency has spent more than $62,000 on repairs related to guardrail theft in the region.&lt;/p&gt;
    &lt;p&gt;The man who appeared to be trying to steal the guardrail apparently didn’t succeed. A photo Gonzalez shared with LAist the following week shows an intact guardrail with a mark where the saw was used.&lt;/p&gt;
    &lt;p&gt;California Highway Patrol, whose jurisdiction includes state-owned highways, said it’s investigating the incident. A spokesperson for the agency said it hasn’t found the man in the video.&lt;/p&gt;
    &lt;p&gt;Aluminum, which Caltrans’ guardrails are made from, is just one metal used in public infrastructure that is increasingly stolen and sold to metal scrapyards and recyclers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Missing guardrails&lt;/head&gt;
    &lt;p&gt;Over the last several months since he’s been commuting to work in Culver City, Gonzalez said he’s noticed guardrailing disappearing on the 10 Freeway near the 110 Freeway interchange.&lt;/p&gt;
    &lt;p&gt;“I was just equating it to normal wear and tear of accidents and the guardrailing doing what it's supposed to do,” Gonzalez said.&lt;/p&gt;
    &lt;p&gt;When he saw the man with the saw, Gonzalez said the dots began to connect.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;LAist learned about guardrail thefts because a listener and reader sent us a video. If there’s a transportation-related issue you feel hasn’t gotten the attention it deserves, let us know. I can be reached at kharjai@laist.com. Or if you prefer something more secure, you can reach me on Signal under the username kharjai.61 or follow this link.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A statement from Caltrans calls guardrail theft an “ongoing” problem in downtown L.A. that causes safety issues for drivers. More specifically, the state transportation agency said there has been “a marked increase in rail theft” along a stretch of the 10 Freeway between Santa Fe Avenue and the 110 interchange.&lt;/p&gt;
    &lt;p&gt;Nearly 470 sections of guardrailing were stolen in the last fiscal year alone in L.A. and Ventura counties, costing the state transportation agency $17,000 to replace, the agency said.&lt;/p&gt;
    &lt;p&gt;Guardrail theft spiked in 2023 after an arsonist set fire to a storage yard beneath the 10 Freeway. Caltrans said it spent $45,000 on repairs in the 2023-24 fiscal year.&lt;/p&gt;
    &lt;p&gt;Google Map Street View images from May 2024 near the San Pedro Street and Central Avenue ramps leading to the 10 Freeway in downtown show additional sections of missing guardrails.&lt;/p&gt;
    &lt;p&gt;To prevent thefts, Caltrans said it tried welding bolts into the guardrails, but thieves were able to breach that deterrent.&lt;/p&gt;
    &lt;p&gt;The next step the agency is considering is using fiberglass composite instead of aluminum to construct guard rails “to remove the value to the thieves.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The value of aluminum&lt;/head&gt;
    &lt;p&gt;Aluminum is critical to transportation infrastructure, said Lance Hastings, the president and CEO of the California Manufacturers and Technology Association.&lt;/p&gt;
    &lt;p&gt;“Like most base metals, the price of aluminum has increased in recent years,” Hastings said. “Tariffs further disrupt the global supply chain, driving up costs and creating market instability.”&lt;/p&gt;
    &lt;p&gt;President Donald Trump announced 50% tariffs on imported steel and aluminum earlier this summer. In August, the Department of Commerce announced that hundreds of other “derivative” steel and aluminum products would also be subject to tariffs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Metal theft&lt;/head&gt;
    &lt;p&gt;Metal theft in L.A. is a persistent problem that damages telecom, lighting, train and other critical infrastructure. It also hamstrings government agencies, which, as a result of increasing thefts, say they're unable to keep up with repairs.&lt;/p&gt;
    &lt;p&gt;The head of the L.A.’s street lighting department told LAist earlier this year that copper wire theft causes 40% of all repairs, up from a quarter just two years ago.&lt;/p&gt;
    &lt;p&gt;Earlier in August, 60 feet of copper cable was stolen from a portion of the track that services L.A. Metro’s A-line train, resulting in reduced service for about 16 hours, according to a statement from the countywide transportation agency.&lt;/p&gt;
    &lt;p&gt;Ally Happel, an executive at security company ECAM, said thieves will target any materials that are “accessible and easy to sell at a high price.”&lt;/p&gt;
    &lt;p&gt;ECAM uses AI surveillance systems to “detect and predict suspicious activity.” It has partnered with Foothill Transit, as well as the L.A. Police Department and port police.&lt;/p&gt;
    &lt;p&gt;Bronze, brass, iron, lead and steel are also “vulnerable to theft,” Happel said.&lt;/p&gt;
    &lt;p&gt;It’s not just an L.A. problem. Nearly a third of all copper theft and telecom infrastructure vandalism nationwide happened in California, according to state Attorney General Rob Bonta.&lt;/p&gt;
    &lt;p&gt;It’s unclear what the LAPD is doing to address the problem for metal stolen from city infrastructure. The department disbanded its metal theft detail six years ago, according to a report it delivered to City Council last year.&lt;/p&gt;
    &lt;p&gt;There were at least two LAPD metal theft task forces funded by council offices that were active last year. It’s unclear if they still exist.&lt;/p&gt;
    &lt;p&gt;LAPD has not responded to LAist’s repeated questions about the continued existence of those task forces and whether there has been more recent metal theft enforcement actions.&lt;/p&gt;
    &lt;p&gt;As Editor-in-Chief of our newsroom, I’m extremely proud of the work our top-notch journalists are doing here at LAist. We’re doing more hard-hitting watchdog journalism than ever before — powerful reporting on the economy, elections, climate and the homelessness crisis that is making a difference in your lives. At the same time, it’s never been more difficult to maintain a paywall-free, independent news source that informs, inspires, and engages everyone.&lt;/p&gt;
    &lt;p&gt;Simply put, we cannot do this essential work without your help. Federal funding for public media has been clawed back by Congress and that means LAist has lost $3.4 million in federal funding over the next two years. So we’re asking for your help. LAist has been there for you and we’re asking you to be here for us.&lt;/p&gt;
    &lt;p&gt;We rely on donations from readers like you to stay independent, which keeps our nonprofit newsroom strong and accountable to you.&lt;/p&gt;
    &lt;p&gt;No matter where you stand on the political spectrum, press freedom is at the core of keeping our nation free and fair. And as the landscape of free press changes, LAist will remain a voice you know and trust, but the amount of reader support we receive will help determine how strong of a newsroom we are going forward to cover the important news from our community.&lt;/p&gt;
    &lt;p&gt;Please take action today to support your trusted source for local news with a donation that makes sense for your budget.&lt;/p&gt;
    &lt;p&gt;Thank you for your generous support and believing in independent news.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; With less to prove than LA, the city is becoming a center of impressive culinary creativity.&lt;/item&gt;
      &lt;item&gt; Monarch butterflies are on a path to extinction, but there is a way to support them — and maybe see them in your own yard — by planting milkweed.&lt;/item&gt;
      &lt;item&gt; With California voters facing a decision on redistricting this November, Surf City is poised to join the brewing battle over Congressional voting districts.&lt;/item&gt;
      &lt;item&gt; The drug dealer, the last of five defendants to plead guilty to federal charges linked to the 'Friends' actor’s death, will face a maximum sentence of 65 years in prison.&lt;/item&gt;
      &lt;item&gt; The weather’s been a little different lately, with humidity, isolated rain and wind gusts throughout much of Southern California. What’s causing the late-summer bout of gray?&lt;/item&gt;
      &lt;item&gt; Hexavalent chromium is the same carcinogen Erin Brockovich warned about in the 1990s, but researchers say more study is needed on the potential health effects of nanoparticles detected earlier this year. Experts will answer questions at a webinar this evening.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://laist.com/news/transportation/guardrails-aluminum-theft"/></entry><entry><id>https://news.ycombinator.com/item?id=45140921</id><title>Show HN: Open-sourcing our text-to-CAD app</title><updated>2025-09-06T03:05:53.794311+00:00</updated><content>&lt;doc fingerprint="1c069d7ee8df9fd2"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🤖 AI-Powered Generation - Transform natural language and images into 3D models&lt;/item&gt;
      &lt;item&gt;🎛️ Parametric Controls - Interactive sliders for instant dimension adjustments&lt;/item&gt;
      &lt;item&gt;📦 Multiple Export Formats - Export as .STL or .SCAD files&lt;/item&gt;
      &lt;item&gt;🌐 Browser-Based - Runs entirely in your browser using WebAssembly&lt;/item&gt;
      &lt;item&gt;📚 Library Support - Includes BOSL, BOSL2, and MCAD libraries&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Natural Language Input&lt;/cell&gt;
        &lt;cell&gt;Describe your 3D model in plain English&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Image References&lt;/cell&gt;
        &lt;cell&gt;Upload images to guide model generation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Real-time Preview&lt;/cell&gt;
        &lt;cell&gt;See your model update instantly with Three.js&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Parameter Extraction&lt;/cell&gt;
        &lt;cell&gt;Automatically identifies adjustable dimensions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Smart Updates&lt;/cell&gt;
        &lt;cell&gt;Efficient parameter changes without AI re-generation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Custom Fonts&lt;/cell&gt;
        &lt;cell&gt;Built-in Geist font support for text in models&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;🎬 Try it live: https://adam.new/cadam&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/Adam-CAD/CADAM.git
cd CADAM

# Install dependencies
npm install

# Start Supabase
npx supabase start
npx supabase functions serve --no-verify-jwt

# Start the development server
npm run dev&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js and npm&lt;/item&gt;
      &lt;item&gt;Supabase CLI&lt;/item&gt;
      &lt;item&gt;ngrok (for local webhook development)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Copy &lt;code&gt;.env.local.template&lt;/code&gt;to&lt;code&gt;.env.local&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Update all required keys in &lt;code&gt;.env.local&lt;/code&gt;:&lt;code&gt;VITE_SUPABASE_ANON_KEY="&amp;lt;Test Anon Key&amp;gt;" VITE_SUPABASE_URL='http://127.0.0.1:54321'&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Copy &lt;code&gt;supabase/functions/.env.template&lt;/code&gt;to&lt;code&gt;supabase/functions/.env&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Update all required keys in &lt;code&gt;supabase/functions/.env&lt;/code&gt;, including:&lt;code&gt;ANTHROPIC_API_KEY="&amp;lt;Test Anthropic API Key&amp;gt;" ENVIRONMENT="local" NGROK_URL="&amp;lt;NGROK URL&amp;gt;" # Your ngrok tunnel URL, e.g., https://xxxx-xx-xx-xxx-xx.ngrok.io&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CADAM uses ngrok to send image URLs to Anthropic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install ngrok if you haven't already:&lt;/p&gt;
        &lt;code&gt;npm install -g ngrok # or brew install ngrok&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Start an ngrok tunnel pointing to your Supabase instance:&lt;/p&gt;
        &lt;quote&gt;ngrok http 54321&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Copy the generated ngrok URL (e.g., https://xxxx-xx-xx-xxx-xx.ngrok.io) and add it to your&lt;/p&gt;&lt;code&gt;supabase/functions/.env&lt;/code&gt;file:&lt;code&gt;NGROK_URL="https://xxxx-xx-xx-xxx-xx.ngrok.io"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ensure&lt;/p&gt;&lt;code&gt;ENVIRONMENT="local"&lt;/code&gt;is set in the same file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm i&lt;/code&gt;
    &lt;code&gt;npx supabase start
npx supabase functions serve --no-verify-jwt&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React 18 + TypeScript + Vite&lt;/item&gt;
      &lt;item&gt;3D Rendering: Three.js + React Three Fiber&lt;/item&gt;
      &lt;item&gt;CAD Engine: OpenSCAD WebAssembly&lt;/item&gt;
      &lt;item&gt;Backend: Supabase (PostgreSQL + Edge Functions)&lt;/item&gt;
      &lt;item&gt;AI: Anthropic Claude API&lt;/item&gt;
      &lt;item&gt;Styling: Tailwind CSS + shadcn/ui&lt;/item&gt;
      &lt;item&gt;Libraries: BOSL, BOSL2, MCAD&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the Project&lt;/item&gt;
      &lt;item&gt;Create your Feature Branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Commit your Changes (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Push to the Branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This app wouldn't be possible without the work of:&lt;/p&gt;
    &lt;p&gt;This distribution is licensed under the GNU General Public License v3.0 (GPLv3). See &lt;code&gt;LICENSE&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Components and attributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Portions of this project are derived from &lt;code&gt;openscad-web-gui&lt;/code&gt;(GPLv3).&lt;/item&gt;
      &lt;item&gt;This distribution includes unmodified binaries from OpenSCAD WASM under GPL v2 or later; distributed here under GPLv3 as part of the combined work. See &lt;code&gt;src/vendor/openscad-wasm/SOURCE-OFFER.txt&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Adam-CAD/CADAM"/></entry><entry><id>https://news.ycombinator.com/item?id=45141636</id><title>Making a font of my handwriting</title><updated>2025-09-06T03:05:52.847484+00:00</updated><content>&lt;doc fingerprint="3714a019faa705f8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Making a font of my handwriting&lt;/head&gt;
    &lt;p&gt;Published on&lt;/p&gt;
    &lt;p&gt;Recently Iâve been on a small campaign to try to make my personal website moreâ¦ personal. Little ways to make it obvious itâs mine and personal, not just another piece of the boring corporate dystopia that is most of the web these days. I donât quite want to fully regress to the Geocities era and fill the screen with animated under construction GIFs, but I do want to capture some of that vibe.&lt;/p&gt;
    &lt;p&gt;Iâd added some bits and pieces along those lines: floating images in articles now look like theyâre stuck to the page with sellotape, related post links have a wavy border that animates when you hover over them, and so on. Next, I wanted to change the heading fonts from a monospace font to something cursive, to resemble handwriting. Less terminal output, more handwritten letter. I couldnât find one I liked, though. So why not make my own? It canât be that hard, right?&lt;/p&gt;
    &lt;head rend="h3"&gt;Failing to do it myself&lt;/head&gt;
    &lt;p&gt;I set out to try to make the font myself using open source tools. After doing a bit of research, it seemed like the general approach was to create vectors of each character and then import them into a font editor. That seems to mean either Adobe Illustrator and FontLab (if you have too much money) or Inkscape and FontForge (if you like open source). I fall firmly into the latter category, so I grabbed my graphics tablet and opened Inkscape.&lt;/p&gt;
    &lt;p&gt;I wrote out my first three letters: capital A, B and C. Saved them in Inkscape, and attempted to import them into FontForge. Then I remembered one crucial thing that had slipped my mind: I absolutely loathe using FontForge. Itâs a bit like when you open an old version of GIMP and get a bunch of weird looking windows floating all over the place; it feels like youâre fighting against the tool to do even the most basic operations. The difference is I have cause to edit images a lot more than I edit fonts, and GIMP has actually significantly improved their UI over the years.&lt;/p&gt;
    &lt;p&gt;Here are the rough steps I went through with FontForge:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Launch Font Forge. It shows a weird bit of art in one window, and an open file dialog in another.&lt;/item&gt;
      &lt;item&gt;I donât want to open a file, so I close that dialog. The program exits.&lt;/item&gt;
      &lt;item&gt;Relaunch Font Forge, and realise that within the âOpen Fontâ dialog is a âNewâ button. Click it.&lt;/item&gt;
      &lt;item&gt;Get to the standard font-editing UI. Right-click on the âAâ looking for a way to import an SVG. Donât see one.&lt;/item&gt;
      &lt;item&gt;Click around a bit, exploring the menus. Everything feels a bit off. You canât open one menu then hover over the next to see its content, like basically every UI toolkit in existence. I think FontForge has eschewed QT and GTK in favour of doing things itself.&lt;/item&gt;
      &lt;item&gt;Find the âImportâ option in the File menu. Hope itâs for a single glyph not the whole font.&lt;/item&gt;
      &lt;item&gt;A file picker opens. Again itâs all a bit off from normal desktop conventions. Try to resize it, and just get blank gray space at the bottom.&lt;/item&gt;
      &lt;item&gt;Type the absolute path I want to go to in the text field.&lt;/item&gt;
      &lt;item&gt;Get a dialog saying âNot a bdf file /home/chris/etcâ. Press OK.&lt;/item&gt;
      &lt;item&gt;Get a dialog saying âCould not find a bitmap font inâ. Press OK.&lt;/item&gt;
      &lt;item&gt;Press Ctrl+L to see if that lets me enter a path. Click everything in the dialog to try to find a way to enter a path. Get annoyed. Give up. Click through folder-by-folder to get to where I want to be.&lt;/item&gt;
      &lt;item&gt;Get to the folder and donât see any files. Change the format to âSVGâ. Double-click the newly-visible SVG file.&lt;/item&gt;
      &lt;item&gt;Get a dialog saying âYou must select a glyph before you can import an image into itâ. Press OK.&lt;/item&gt;
      &lt;item&gt;The import dialog goes away, having not imported.&lt;/item&gt;
      &lt;item&gt;Select the glyph in the main tool area, then repeat the FileâImport dance.&lt;/item&gt;
      &lt;item&gt;Itâs actually there now! Open the glyph in the editor and see itâs a complete mess of BÃ©zier curves. I canât click what I want without accidentally moving a handle for an adjacent curve.&lt;/item&gt;
      &lt;item&gt;Rage-quit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Iâm sure FontForge is less anger inducing once youâre used to it. And you definitely could use it to build a font like this if you had much more patience than me. Iâd had enough of death-by-a-thousand-paper-cuts though.&lt;/p&gt;
    &lt;p&gt;I briefly tried Inkscapeâs built-in support for making an SVG font. It annoyed me a lot less, but itâs fiddly: it seemed like each font had to be a single path, so you had to convert the glyphs to paths, then merge them correctly. If you merge them incorrectly then the wrong bits of your letters end up filled (like the inside of the âBâ). Path manipulation is getting towards the limit of my knowledge of vector editing, and it took a bit of trial and error for each letter that had more than a single stroke. I didnât fancy doing that for every letter.&lt;/p&gt;
    &lt;p&gt;Iâm usually a big advocate of open source, but this was one of those painful times where it feels like it just falls short. Clunky, painful UI and processes where commercial tools just let you get on with your work.&lt;/p&gt;
    &lt;head rend="h3"&gt;You can exchange money for goods and services&lt;/head&gt;
    &lt;p&gt;When Iâd been looking for open source tutorials, I found many mentions of a closed source, hosted tool: Calligraphr. It has a free version with limitations (no ligatures, no variations, 75 glyphs per font), and a pro version for Â£8/month. Iâd normally balk at the idea of a subscription for this, but they have the perfect answer: you can make a one-time payment, and your account automatically downgrades back to free after a month. Itâs not a hidden option, either, itâs the most prominent button on the upgrade page. That made me happy to give them Â£8 to play around with the service for a month.&lt;/p&gt;
    &lt;p&gt;Calligraphr works by having you print templates, write out the letters, then scan them in. It does some magical processing to extract the glyphs, provides tools to tidy them up, align them, etc, and then produces a TTF file for you. You can see some of my completed templates here:&lt;/p&gt;
    &lt;p&gt;Calligraphr has a nice UI to generate the templates, allowing you to select which glyphs to include. I added the âminimal Englishâ, âbasic punctuationâ and âLigaturesâ sets. That gave me four pages to fill out, and I did them all twice. That let me filter out versions that didnât work well, and have variants for some letters so the font wasnât too repetitive. Later on, I went back and added some custom ligatures based on blog post titles that didnât look quite right: âReâ, âToâ, âersâ, âeyâ, âhyâ, âraâ, âreâ and âtyâ. Ligatures like this help it look more natural: when we write we donât just stamp out identical letters regardless of their surroundings, instead they will connect to their neighbours, or overlap slightly, or even share a stroke.&lt;/p&gt;
    &lt;p&gt;I filled these templates in with a Sharpie, as I wanted a fairly informal, scrap-booky look, and it would also give good solid shapes that should be easy to pick out of the template. I scanned them with the âScan Documentâ function on my iPhone, and uploaded the PDFs to Calligraphr.&lt;/p&gt;
    &lt;head rend="h3"&gt;Iterating and tweaking&lt;/head&gt;
    &lt;p&gt;The Calligraphr UI allows you to preview the font, but I found it a lot more useful to just download a copy and use it on a local copy of my website. That let me test it with real text, and see how itâd look at the different font sizes I use on the site.&lt;/p&gt;
    &lt;p&gt;The first version was not great. Despite the guidelines on the template, I apparently wasnât good at sticking to them. Some letters were floating way off the baseline, and some were sunken below. When those opposites met it looked terrible. Fortunately Calligraphr has a pretty easy tool to slide each letter up and down, and scale it up or down if needed, and you can see it next to other letters as you do it. It took a little bit of time to go through all the variants of all the letters, but the next version looked a lot better.&lt;/p&gt;
    &lt;p&gt;Another tweak I ended up doing was reducing the spacing between letters. The defaults Calligraphr uses are probably good for a blocky font, but I wanted to put the letters close together to give it more of a joined-up look. Again, this is an easy tool to use, you just drag the sides in or out as desired. While these tweaking steps were probably as fiddly as some of the Inkscape steps I refused to do earlier, theyâre a lot more rewarding as you see things improving with each one. Itâs a lot easier for me to commit time and effort to improving something thatâs already working reasonably, than put that time and energy into an unknown.&lt;/p&gt;
    &lt;p&gt;Later, I noticed that occasionally there would be a huge gap in a title. Not âthe kerning is slightly offâ but âthereâs enough room to park a busâ. It took me a while to figure out what was happening: a couple of glyphs hadnât been isolated perfectly and had picked up a few pixels from the template lines at the edge of their boxes. That meant the glyph had a width that covered the actual written glyph, a big gap, and then the rogue marks. At first, I fixed this by just adjusting the width, but that left the little pixels floating awkwardly down-sentence. The proper fix was to use the editing tool and simply delete them, and then Calligraphr snapped the width back to what it should be.&lt;/p&gt;
    &lt;p&gt;These iterations took a while to do, but I just dipped in and out occasionally over the course of a week, so it didnât actually feel like too much work. I quite enjoy the process of refining things, too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Result and a surprise&lt;/head&gt;
    &lt;p&gt;If youâre viewing this post on my website[1], you can see the font in the headers, captions, and a few other places. Hereâs how it compares to my actual handwriting:&lt;/p&gt;
    &lt;p&gt;Itâs not close enough to forge documents, but I think it definitely gets across my style, and thatâs exactly what I wanted. Itâs surprisingly legible even at smaller font sizes â I think the weight of the Sharpie helps here â and at Â£8 and a bit of manual work was a lot more economical than spending days wresting with open source tools.&lt;/p&gt;
    &lt;p&gt;A few weeks after I put the finishing touches on the font, I got an e-mail from Calligraphr. As my account had lapsed back to the free version, I was no longer eligible for the âserver-side backupâ feature. So what did they do? They e-mailed me an exported copy! Itâs a JSON file with the properties of each glyph and a base64 encoded image. Not only can I re-upload this to Calligraphr if I resubscribe, I can probably hook something up to edit it should I ever need to. Iâm blown away by how pro-user Calligraphrâs business practices are. Theyâre up-front about pricing, donât try and get you stuck on an auto-renewing subscription, and automatically export your data. Itâs like a breath of fresh air compared to the barrage of dark patterns that other websites foist on us. If you want to make this kind of font, Iâd definitely recommend them just because of how nice they are.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;And I havenât changed everything since writing this postâ¦ â©ï¸&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Thanks for reading!&lt;/head&gt;
    &lt;head rend="h3"&gt;Related posts&lt;/head&gt;
    &lt;head rend="h3"&gt;Escaping Spotify the hard way&lt;/head&gt;
    &lt;p&gt;For the longest time I used Spotify for all my music needs. And I listen to a lot of music: sometimes actively, but mostly passively as background noise. I cancelled my premium subscription last December, and stopped using the service entirely. Why? Thereâs a bunch of reasons.&lt;/p&gt;
    &lt;head rend="h3"&gt;How I use Tailscale&lt;/head&gt;
    &lt;p&gt;Iâve been using Tailscale for around four years to connect my disparate devices, servers and apps together. I wanted to talk a bit about how I use it, some cool features you might not know about, and some stumbling blocks I encountered.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Ethics of LLMs&lt;/head&gt;
    &lt;p&gt;Iâve written about LLMs a few times recently, carefully dodging the issue of ethics each time. I didnât want to bog down the other posts with it, and I wanted some time to think over the issues. Now Iâve had time to think, itâs time to remove my head from the sand. There are a lot of different angles to consider, and a lot of it is more nuanced than is often presented. Itâs not all doom and gloom, and itâs also not the most amazing thing since sliced bread. Who would have thought?&lt;/p&gt;
    &lt;head rend="h3"&gt;If all you have is a hammerâ¦&lt;/head&gt;
    &lt;p&gt;I presume everyone is familiar with the idiom âif all you have is a hammer, everything looks like a nailâ. If not, well, there it is. Itâs generally used pejoratively about being single-minded, but I think it also gives a glimpse into something more interesting: mental and perceptual sets.&lt;/p&gt;
    &lt;p&gt;Recently Iâve been on a small campaign to try to make my personal website moreâ¦ personal. Little ways to make it obvious itâs mine and personal, not just another piece of the boring corporate dystopia that is most of the web these days. I donât quite want to fully regress to the Geocities era and fill the screen with animated under construction GIFs, but I do want to capture some of that vibe.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chameth.com/making-a-font-of-my-handwriting/"/></entry><entry><id>https://news.ycombinator.com/item?id=45142397</id><title>My Own DNS Server at Home – Part 1: IPv4</title><updated>2025-09-06T03:05:51.661685+00:00</updated><content>&lt;doc fingerprint="67234351e5b0b027"&gt;
  &lt;main&gt;
    &lt;p&gt;Posted:&lt;/p&gt;
    &lt;head rend="h1"&gt;My Own DNS Server At Home - Part 1: IPv4&lt;/head&gt;
    &lt;p&gt;âItâs always DNSâ is a famous meme among network people. Name resolution is technically quite simple. Itâs âjustâ translating a hostname like &lt;code&gt;jan.wildeboer.net&lt;/code&gt; to an IP address. What could possibly go wrong? I am a radical optimist and detail-obsessed knowledge collector, so I decided to find out. As part of my goal to make my home network a little island of Digital Sovereignty, meaning that everything at home should JustWorkâ¢, even with no working internet connection, a DNS server is needed.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on and extended from my gist Bind on Fedora 42 as DNS server.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I admit, I have a lot of experience with DNS and BIND. But I still consider myself to be merely on the GoodEnoughâ¢ side of things. I know how to get DNS configured for my domains. And I want you to feel fearless too. The best place to fail with DNS is the network at home. It limits the impact :)&lt;/p&gt;
    &lt;p&gt;So read this blog post either as report or as a HOWTO. Both ways can be fun!&lt;/p&gt;
    &lt;p&gt;In my homelab I have a Raspberry Pi 4 that runs infrastructure services. DNS is one of them, my private CA (Certificate Authority) another. The CA runs as a container on Podman. For DNS I use Bind. It thus has to serve 3 networks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;192.168.1.0/24&lt;/code&gt;My home IPv4 network&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;172.16.0.0/16&lt;/code&gt;IPv4 Network on the second ethernet ports of my homelab servers&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;10.88.0.0/16&lt;/code&gt;The (virtual) podman network&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It uses my Fritz box (7490) as forwarder, so I can resolve all hosts, including the DHCP entries that the Fritz Box hands out under its default local domain name &lt;code&gt;fritz.box&lt;/code&gt;. For my homelab however, I use the &lt;code&gt;homelab.jhw&lt;/code&gt; domain name. Thatâs what the Bind DNS server has to take care of.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;WARNING&lt;/p&gt;&lt;lb/&gt;I really should use the official&lt;code&gt;.internal&lt;/code&gt;TLD (Top Level Domain) for my homelab network, but I decided against it. This introduces the risk of name resolution problems, should someone offer a public&lt;code&gt;.jhw&lt;/code&gt;TLD in future. Itâs a risk I am willing to accept in exchange for using a 3 letter TLD at home. Donât be like me! Use&lt;code&gt;.internal&lt;/code&gt;instead. With that out of the way, letâs continue.&lt;/quote&gt;
    &lt;head rend="h2"&gt;What we (well, I) have&lt;/head&gt;
    &lt;p&gt;Letâs gather what I have in my home network.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;inf01.homelab.jhw&lt;/code&gt;at&lt;code&gt;192.168.1.10&lt;/code&gt;: A Raspberry Pi 4 4GB, running Fedora 42 and podman with my Certificate Authority as a container that should be reachable as&lt;code&gt;ca.homelab.jhw&lt;/code&gt;. See Be the LetsEncrypt in your homelab with step-ca for more details.&lt;/item&gt;
      &lt;item&gt;3 ThinkCentre Tiny PCs in the &lt;code&gt;homelab.jhw&lt;/code&gt;zone, called hl01 (&lt;code&gt;192.168.1.11&lt;/code&gt;), hl02 (&lt;code&gt;192.168.1.12&lt;/code&gt;) and hl03 (&lt;code&gt;192.168.1.13&lt;/code&gt;), running RHEL10 (Red Hat Enterprise Linux)&lt;/item&gt;
      &lt;item&gt;A Fritz Box 7490 at &lt;code&gt;192.168.1.254&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Letâs install BIND on inf01&lt;/head&gt;
    &lt;p&gt;We need to do two things. Install BIND and some utilities on &lt;code&gt;inf01&lt;/code&gt; and open the firewall for DNS traffic.&lt;/p&gt;
    &lt;code&gt;dnf install bind bind-utils
firewall-cmd --add-service=dns --permanent
&lt;/code&gt;
    &lt;p&gt;That was easy enough :)&lt;/p&gt;
    &lt;head rend="h3"&gt;Configure BIND&lt;/head&gt;
    &lt;p&gt;To run BIND in the correct way, we need to work on 3 configuration files.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/etc/named.conf&lt;/code&gt;The main configuration file where we tell BIND on which networks it should listen and what zones it will serve.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/var/named/forward.homelab.jhw&lt;/code&gt;The forward zone file that maps hostnames in the&lt;code&gt;homelab.jhw&lt;/code&gt;domain to IP addresses on my home network&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/var/named/reverse.homelab.jhw&lt;/code&gt;The reverse zone for the&lt;code&gt;192.168.1.0/24&lt;/code&gt;network range, that looks a bit confusing, that does the opposite. It maps IP addresses to hostnames.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/var/named/reverse2.homelab.jhw&lt;/code&gt;The second reverse zone for the&lt;code&gt;172.16.0.0/16&lt;/code&gt;network range.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Letâs start with &lt;code&gt;/etc/named.conf&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;//
// named.conf
//

options {
  listen-on port 53 { 127.0.0.1; 192.168.1.10; 172.16.1.10; 10.88.0.1; };
  listen-on-v6 port 53 { ::1; fdda:a4da:69a5:0:2783:8c26:b2f1:a6f7; };
  allow-query     { localhost; 192.168.1.0/24; 172.16.0.0/16; 10.88.0.0/16; };

  directory       "/var/named";

  dump-file       "/var/named/data/cache_dump.db";
  statistics-file "/var/named/data/named_stats.txt";
  memstatistics-file "/var/named/data/named_mem_stats.txt";
  secroots-file   "/var/named/data/named.secroots";
  recursing-file  "/var/named/data/named.recursing";

  forwarders { 192.168.1.254; };
  recursion yes;

  dnssec-validation no;

  managed-keys-directory "/var/named/dynamic";
  geoip-directory "/usr/share/GeoIP";

  pid-file "/run/named/named.pid";
  session-keyfile "/run/named/session.key";

  /* https://fedoraproject.org/wiki/Changes/CryptoPolicy */
  include "/etc/crypto-policies/back-ends/bind.config";
};

logging {
        channel default_debug {
                file "data/named.run";
                severity dynamic;
        };
};

zone "." IN {
	type hint;
	file "named.ca";
};

zone "homelab.jhw" IN {
	type master;
	file "forward.homelab.jhw";
	allow-update { none; };
	allow-query { any; };
};

zone "1.168.192.in-addr.arpa" IN {
	type master;
	file "reverse.homelab.jhw";
	allow-update { none; };
	allow-query { any; };
};

zone "16.172.in-addr.arpa" IN {
        type master;
        file "reverse2.homelab.jhw";
        allow-update { none; };
        allow-query { any; };
};

include "/etc/named.rfc1912.zones";
include "/etc/named.root.key";
&lt;/code&gt;
    &lt;p&gt;The first block declare the general options. Yes, it looks complicated and it is, but letâs walk you through every relevant line (the lines not mentioned are default entries that donât need to be changed).&lt;/p&gt;
    &lt;code&gt;listen-on port 53 { 127.0.0.1; 192.168.1.10; 172.16.1.10; 10.88.0.1; };
listen-on-v6 port 53 { ::1; fdda:a4da:69a5:0:2783:8c26:b2f1:a6f7; };
allow-query     { localhost; 192.168.1.0/24; 172.16.0.0/16; 10.88.0.0/16; };
&lt;/code&gt;
    &lt;p&gt;Here we tell BIND that it should listen for queries on port 53 on &lt;code&gt;localhost&lt;/code&gt;, &lt;code&gt;192.168.1.10&lt;/code&gt;, the IPv4 address in my hoem network, &lt;code&gt;172.16.1.10&lt;/code&gt;, the second IPv4 address configured and &lt;code&gt;10.88.0.1&lt;/code&gt;, the virtual IPv4 address the Raspberry uses to bridge to the local podman containers.&lt;/p&gt;
    &lt;p&gt;The second line does the same for IPv6, but that is something we will discuss in Part 2.&lt;/p&gt;
    &lt;p&gt;The third line tells BIND from whom to accept queries. Essentially from everyone on the three IPv4 networks we are listening to.&lt;/p&gt;
    &lt;code&gt;directory       "/var/named";
&lt;/code&gt;
    &lt;p&gt;This is the directory where BIND will look for its zone files, that we will define later.&lt;/p&gt;
    &lt;code&gt;forwarders { 192.168.1.254; };
recursion yes;
&lt;/code&gt;
    &lt;p&gt;Now what if someone asks for a hostname that is outside of &lt;code&gt;homelab.jhw&lt;/code&gt;? In that case we tell BIND to forward that question to &lt;code&gt;192.168.1.254&lt;/code&gt;, our Fritz Box. We will allow recursion and cache results we get from our Fritz box to avoid unneeded traffic.&lt;/p&gt;
    &lt;code&gt;dnssec-validation no;
&lt;/code&gt;
    &lt;p&gt;Our simple setup will not bother with DNSSEC at the moment. Maybe we will have a Part 3 for that.&lt;/p&gt;
    &lt;p&gt;OK. That was the options part. We will ignore the &lt;code&gt;logging&lt;/code&gt; part and the &lt;code&gt;zone "." IN&lt;/code&gt; block.&lt;/p&gt;
    &lt;p&gt;Next (and finally) we define three zone entries (and zone files). A forward zone called &lt;code&gt;homelab.jhw&lt;/code&gt; for our domain and two reverse zones for the IP addresses in the &lt;code&gt;192.168.1.0/24&lt;/code&gt; range called &lt;code&gt;1.168.192.in-addr.arpa&lt;/code&gt;. Yep. Thatâs 192.168.1 reversed. 1.168.192. Thatâs why itâs called the reverse zone ;) We also have &lt;code&gt;16.172.in-addr.arpa&lt;/code&gt; for the &lt;code&gt;172.16.0.0/16&lt;/code&gt; range. Letâs look at them.&lt;/p&gt;
    &lt;code&gt;zone "homelab.jhw" IN {
	type master;
	file "forward.homelab.jhw";
	allow-update { none; };
	allow-query { any; };
};
&lt;/code&gt;
    &lt;p&gt;Itâs a zone, all right. Itâs the &lt;code&gt;master&lt;/code&gt; for this zone, meaning that this DNS server will be the Source of Truth to  answer all queries for the &lt;code&gt;homelab.jhw&lt;/code&gt; hostnames.&lt;/p&gt;
    &lt;p&gt;The exact mapping of all hostnames to IP addresses is in a file called &lt;code&gt;forward.homelab.jhw&lt;/code&gt; in the directory &lt;code&gt;/var/named&lt;/code&gt;. Remember how we defined that path at the beginning in the &lt;code&gt;options&lt;/code&gt; part? Great! We also tell BIND that we do not allow dynamic updates for this zone, meaning that whatâs in the file is all we will look at. Finally we tell BIND that any machine in the network is allowed to ask for a reply.&lt;/p&gt;
    &lt;code&gt;zone "1.168.192.in-addr.arpa" IN {
	type master;
	file "reverse.homelab.jhw";
	allow-update { none; };
	allow-query { any; };
};

zone "16.172.in-addr.arpa" IN {
        type master;
        file "reverse2.homelab.jhw";
        allow-update { none; };
        allow-query { any; };
};
&lt;/code&gt;
    &lt;p&gt;The reverse zones with the weird looking zone names are almost the same, except that we define these in two files called &lt;code&gt;reverse.homelab.jhw&lt;/code&gt; for the reverse lookup of the &lt;code&gt;192.168.1.0/24&lt;/code&gt; range and &lt;code&gt;reverse2.homelab.jhw&lt;/code&gt; for the &lt;code&gt;172.16.0.0/16&lt;/code&gt; range. Why these zones have weird names will be explained later.&lt;/p&gt;
    &lt;p&gt;So now we go to the zone files!&lt;/p&gt;
    &lt;head rend="h3"&gt;Forward zone for homelab.jhw&lt;/head&gt;
    &lt;p&gt;The forward zone resolves names to IP addresses using A records (and other types like TXT, CAA and many more exist, but we wonât cover that in this post). It also contains CNAME entries, if you have services on one machine that should be reachable via more than one hostnames. In my homelab the CA (Certificate Authority) server is a container that runs on &lt;code&gt;inf01.homelab.jhw&lt;/code&gt;, but should be reachable as &lt;code&gt;ca.homelab.jhw&lt;/code&gt; in the home network. The CNAME entry does exactly that. It tells clients that when they want to talk to &lt;code&gt;ca.homelab.jhw&lt;/code&gt; they can. By actually talking to &lt;code&gt;inf01.homelab.jhw&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Now here is the big, important lessen for zone files. They have a serial number. Which MUST be incremented with every change. If you donât, weird things WILL happen. So:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NEVER FORGET TO INCREASE THE SERIAL WITH EVERY CHANGE TO A ZONE FILE. OR RISK DNS HELL.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;
      &lt;code&gt;/var/named/forward.homelab.jhw&lt;/code&gt;
    &lt;/p&gt;
    &lt;code&gt;$TTL 3600
@   IN  SOA     inf01.homelab.jhw. root.homelab.jhw. (
        2025082706  ;Serial
        3600        ;Refresh
        1800        ;Retry
        604800      ;Expire
        86400       ;Minimum TTL
)
@       IN  NS          inf01.homelab.jhw.
@       IN  A           192.168.1.10

inf01           IN  A     192.168.1.10
hl01            IN  A     192.168.1.11
hl02            IN  A     192.168.1.12
hl03            IN  A     192.168.1.13

ca              IN  CNAME inf01.homelab.jhw.

inf01-m         IN  A     172.16.1.10
hl01-m          IN  A     172.16.1.11
hl02-m          IN  A     172.16.1.12
hl03-m          IN  A     172.16.1.13
&lt;/code&gt;
    &lt;p&gt;Again, letâs go through this.&lt;/p&gt;
    &lt;code&gt;$TTL 3600
&lt;/code&gt;
    &lt;p&gt;The default Time To Live (TTL) for DNS entries is set at 3600 seconds. Thatâs 1 hour. This means that when a machine in the network gets a DNS reply, it will not ask again for the same thing until the TTL has passed.&lt;/p&gt;
    &lt;code&gt;@   IN  SOA     inf01.homelab.jhw. root.homelab.jhw. (
        2025082706  ;Serial
        3600        ;Refresh
        1800        ;Retry
        604800      ;Expire
        86400       ;Minimum TTL
)
&lt;/code&gt;
    &lt;p&gt;The Start Of Authority (SOA) block. Here we say which DNS server is the owner of this domain. Itâs &lt;code&gt;inf01.homelab.jhw.&lt;/code&gt; (yes, that dot at the end is REALLY important). The &lt;code&gt;root.homelab.jhw&lt;/code&gt; actually means &lt;code&gt;root@homelab.jhw&lt;/code&gt; and is the email address responsible for this domain. Donât think to much about why and what :)&lt;/p&gt;
    &lt;code&gt;@       IN  NS          inf01.homelab.jhw.
@       IN  A           192.168.1.10
&lt;/code&gt;
    &lt;p&gt;The first ârealâ DNS entries! They are special, as the &lt;code&gt;@&lt;/code&gt; indicates, which means they represent the domain itself. We first define the nameserver (again? yes, don*ât ask) as NS record. And right after that we define the &lt;code&gt;A&lt;/code&gt; record as the IP address &lt;code&gt;192.168.1.10&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Did you notice that &lt;code&gt;.&lt;/code&gt; at the end of &lt;code&gt;inf01.homelab.jhw.&lt;/code&gt;? Thatâs another VERY important thing. The TL;DR is that this final &lt;code&gt;.&lt;/code&gt; tells DNS to stop doing fancy recursion and lookups. Just look for the hostname `inf01.homelab.jhw. Period. (pun intended). Donât care too much about this. Just remember:&lt;/p&gt;
    &lt;p&gt;EVERY HOSTNAME RECORD ENDS WITH A &lt;code&gt;.&lt;/code&gt; YOU WILL FORGET THIS. YOU WILL FIX THIS.&lt;/p&gt;
    &lt;code&gt;inf01           IN  A     192.168.1.10
hl01            IN  A     192.168.1.11
hl02            IN  A     192.168.1.12
hl03            IN  A     192.168.1.13
&lt;/code&gt;
    &lt;p&gt;Here come the &lt;code&gt;A&lt;/code&gt; records for &lt;code&gt;192.168.1.0/24&lt;/code&gt;! We finally get to map hostnames to IP addresses. For real! It now is quite self-explanatory, isnât it? The hostname gets an A record that is the IP address in my local network. And as these are IP addresses, no &lt;code&gt;.&lt;/code&gt; is needed at the end.&lt;/p&gt;
    &lt;code&gt;ca              IN  CNAME inf01.homelab.jhw.
&lt;/code&gt;
    &lt;p&gt;And here is the CNAME record. Which maps the hostname &lt;code&gt;ca.homelab.jhw&lt;/code&gt; to the Canonical NAME (CNAME) &lt;code&gt;inf01.homelab.jhw.&lt;/code&gt;. This is a hostname at the end! So it needs the &lt;code&gt;.&lt;/code&gt; Period :)&lt;/p&gt;
    &lt;code&gt;inf01-m         IN  A     172.16.1.10
hl01-m          IN  A     172.16.1.11
hl02-m          IN  A     172.16.1.12
hl03-m          IN  A     172.16.1.13
&lt;/code&gt;
    &lt;p&gt;And here we create another set of &lt;code&gt;A&lt;/code&gt; records for the same machines, but this time in the &lt;code&gt;172.16.0.0/16&lt;/code&gt; range. This range is used for management stuff, hence the &lt;code&gt;-m&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And thatâs the gist of it. If you add a new machine to your network, configure it with an IP address (statically or with DHCP) and add it as an A record to the forward zone. Increment the serial and tell DNS to read the updated zone with &lt;code&gt;systemctl reload named&lt;/code&gt;. Done.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reverse zones for 192.168.1.0/24 and 172.16.0.0/16&lt;/head&gt;
    &lt;p&gt;The reverse zone maps IP addresses to hostnames. Often called the PTR or pointer record. You have to make sure that the entries here are synced to the forward zone.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NEVER FORGET TO INCREASE THE SERIAL WITH EVERY CHANGE TO A ZONE FILE. Or risk DNS hell.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here is the reverse zone for the &lt;code&gt;192.168.1.0/24&lt;/code&gt; range.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;/var/named/reverse.homelab.jhw&lt;/code&gt;
    &lt;/p&gt;
    &lt;code&gt;$TTL 3600
@   IN  SOA     inf01.homelab.jhw. root.homelab.jhw. (
        2025082601  ;Serial
        3600        ;Refresh
        1800        ;Retry
        604800      ;Expire
        86400       ;Minimum TTL
)
@       IN  NS          inf01.homelab.jhw.
@       IN  PTR         homelab.jhw.
10      IN  PTR         inf01.homelab.jhw.
11      IN  PTR         hl01.homelab.jhw.
12      IN  PTR         hl02.homelab.jhw.
13      IN  PTR         hl03.homelab.jhw.
&lt;/code&gt;
    &lt;p&gt;As this is more or less the same but the other way round, I will not go through everything but instead explain the differences. Itâs the reverse zone, so now we have &lt;code&gt;PTR&lt;/code&gt; (pointer) entries that map an IPv4 address in the &lt;code&gt;192.168.1.0/24&lt;/code&gt; range to hostnames. WITH A DOT AT THE END. DO NOT FORGET THE DOT!&lt;/p&gt;
    &lt;p&gt;As this is a /24 block, we only need to set the last digit of the IPv4 address.&lt;/p&gt;
    &lt;p&gt;You might wonder, where is &lt;code&gt;ca&lt;/code&gt; here? Well, itâs CNAME is &lt;code&gt;info1.homelab.jhw&lt;/code&gt; and that already is in this reverse zone. That is good enough. No separate entry needed.&lt;/p&gt;
    &lt;p&gt;We also need the reverse zone for the &lt;code&gt;172.16.0.0/16&lt;/code&gt; range:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;/var/named/reverse2.homelab.jhw&lt;/code&gt;
    &lt;/p&gt;
    &lt;code&gt;$TTL 3600
@   IN  SOA     inf01.homelab.jhw. root.homelab.jhw. (
        2025082901  ;Serial
        3600        ;Refresh
        1800        ;Retry
        604800      ;Expire
        86400       ;Minimum TTL
)
@       IN  NS          inf01.homelab.jhw.
@       IN  PTR         homelab.jhw.
10.1      IN  PTR         inf01-m.homelab.jhw.
11.1      IN  PTR         hl01-m.homelab.jhw.
12.1      IN  PTR         hl02-m.homelab.jhw.
13.1      IN  PTR         hl03-m.homelab.jhw.
&lt;/code&gt;
    &lt;p&gt;Looks deceivingly similar. But there is a big difference. This is a /16 network, so we have to define the last two parts of the IPv4 address. And as it is a reverse zone file, yep, we have to reverse it. So now we need &lt;code&gt;10.1&lt;/code&gt; to define the entry for &lt;code&gt;172.16.1.10&lt;/code&gt;, which is the hostname &lt;code&gt;inf01-m.homelab.jhw&lt;/code&gt;. WITH THE DOT AT THE END. AND DID YOU UPDATE THE SERIAL? :)&lt;/p&gt;
    &lt;p&gt;Phew. Thatâs the config done!&lt;/p&gt;
    &lt;p&gt;A final check with the &lt;code&gt;named-checkconf&lt;/code&gt; command, which should say nothing when all files are OK. If not, it will tell you what is wrong so you get the chance to fix stuff. You did add all the &lt;code&gt;.&lt;/code&gt; at the end of hostnames and you did update the serial of that zone file after you made changes, yes?&lt;/p&gt;
    &lt;head rend="h2"&gt;Start Bind&lt;/head&gt;
    &lt;p&gt;The only thing remaining is to start BIND. And persist it as a service, so it starts after every boot. Itâs DNS. It must always be available.&lt;/p&gt;
    &lt;code&gt;systemctl enable named
systemctl start named
&lt;/code&gt;
    &lt;p&gt;You most likely will make typos in your config. So do check with &lt;code&gt;named-checkconf &lt;/code&gt; and &lt;code&gt;systemctl status named&lt;/code&gt; and &lt;code&gt;journalctl -u named&lt;/code&gt;. If something breaks, read this whole entry again. Find that missing &lt;code&gt;.&lt;/code&gt; in a zone file. Increment the &lt;code&gt;serial&lt;/code&gt; that you forgot to do. You will get there. Donât give up!&lt;/p&gt;
    &lt;head rend="h2"&gt;Result&lt;/head&gt;
    &lt;p&gt;Machines, containers etc can now be resolved in my home network. All with mow own DNS! Yay!&lt;/p&gt;
    &lt;code&gt;% nslookup jhwfritz.fritz.box
Server:		192.168.1.10
Address:	192.168.1.10#53

Non-authoritative answer:
Name:	jhwfritz.fritz.box
Address: 192.168.1.254

% nslookup ca.homelab.jhw    
Server:		192.168.1.10
Address:	192.168.1.10#53

ca.homelab.jhw	canonical name = inf01.homelab.jhw.
Name:	inf01.homelab.jhw
Address: 192.168.1.10
&lt;/code&gt;
    &lt;p&gt;And now you should be able to &lt;code&gt;ping&lt;/code&gt; the machines with their hostname. ssh into them. Get certificates with the CA that runs in the podman container. Life is good!&lt;/p&gt;
    &lt;p&gt;I hope you enjoyed this post and could learn something new! Feel free to comment or send corrections vie the Toot linked below that collects the comments!&lt;/p&gt;
    &lt;head rend="h4"&gt;COMMENTS&lt;/head&gt;
    &lt;p&gt;You can use your Mastodon or other ActivityPub account to comment on this article by replying to the associated post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jan.wildeboer.net/2025/08/My-DNS-Part-1/"/></entry><entry><id>https://news.ycombinator.com/item?id=45142885</id><title>Anthropic agrees to pay $1.5B to settle lawsuit with book authors</title><updated>2025-09-06T03:05:51.565173+00:00</updated><content/><link href="https://www.nytimes.com/2025/09/05/technology/anthropic-settlement-copyright-ai.html?unlocked_article_code=1.jk8.bTTt.Zir9wmtPaTp2&amp;smid=url-share"/></entry><entry><id>https://news.ycombinator.com/item?id=45143548</id><title>Gym Class VR (YC W22) Is Hiring – UX Design Engineer</title><updated>2025-09-06T03:05:50.915313+00:00</updated><content>&lt;doc fingerprint="29966ae1084f87a4"&gt;
  &lt;main&gt;
    &lt;p&gt;Play sports in virtual reality&lt;/p&gt;
    &lt;p&gt;Gym Class is the top rated social sports game on Meta Quest - millions of downloads, 79,000+ reviews, and a 4.9-star rating. We’re hiring our founding UX Design Engineer to drive the development of our upcoming mobile web app (embedded in native), and web surfaces inside our flagship, social VR experience.&lt;/p&gt;
    &lt;p&gt;You’ll own key UX surfaces end-to-end - crafting in Figma, then building responsive, production-grade UI with React/Node/CSS - and you’ll set a clear quality bar for speed, polish, and accessibility. If you love living at the intersection of design taste and front-end engineering, owning a high-compact roadmap for a startup, and shipping to a highly engaged social audience - this role is for you.&lt;/p&gt;
    &lt;p&gt;WHAT YOU'LL DO&lt;/p&gt;
    &lt;p&gt;QUALIFICATIONS - you are…&lt;/p&gt;
    &lt;p&gt;Salary ranges may be inclusive of several career levels and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Additional benefits for this role include: equity; and medical, dental, and vision benefits; and 401k retirement plan with matching.&lt;/p&gt;
    &lt;p&gt;Gym Class is a top-rated social VR game on Meta Quest with over +79,000 reviews and a 4.9-star rating. Millions of players join to connect with friends, explore social worlds, and engage in competitive games. The community launched around basketball, and is now expanding rapidly into more categories.&lt;/p&gt;
    &lt;p&gt;The company's mission is to connect the world by simulating it. To achieve this, Gym Class recently raised over $8M from the world's top technology investors, including Andreessen Horowitz (a16z), Y Combinator, the National Basketball Association, the Golden State Warriors, top NBA players like Kevin Durant, Lonzo Ball, Andre Iguodala, Soma Capital, Founders Inc., Danny Green, Zaza Pachulia, Todd and Rahul's Angel Fund, Balaji Srinivasan, and more. In 2023, the company announced a licensing relationship with the NBA, and in 2025, its licensing relationship with the MLB.&lt;/p&gt;
    &lt;p&gt;Learn more at gymclass.com or follow @gymclassvr.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/gym-class-by-irl-studios/jobs/ywXHGBv-ux-design-engineer-senior-staff-principal"/></entry><entry><id>https://news.ycombinator.com/item?id=45143879</id><title>Nest 1st gen and 2nd gen thermostats no longer supported from 10/25/2025</title><updated>2025-09-06T03:05:50.106495+00:00</updated><content>&lt;doc fingerprint="740907299b43180"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;simon5
1&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I just received an email from Google stating that they are no longer it's going to support the Nest 1st gen and 2nd gen thermostats. While they will continue to operate locally, it appears that they will no longer work with the Nest app or Home app controls. I currently use the Nest app to control the thermostats, but does this mean that we won't even be able to control the thermostats via a Hubitat integration?&lt;/p&gt;
        &lt;p&gt;Cheers,&lt;lb/&gt; Simon&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;Gergor
2&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I got email too. WHat a bummer. It says the API will be removed, so my guess is unless someone comes up with a heck, there's no way to interact with the thermostat other than thru the physical dial.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;simon5
4&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Thanks. That's so annoying. I have 8 Nest thermostats in my house and will have to replace them all.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;aaiyar
5&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;If you have relatively simple HVAC systems (eg. 1H/1C, or single-stage HP with 1 AUX stage), consider getting locally controlled zigbee/z-wave thermostats.&lt;/p&gt;
        &lt;p&gt;You could also get ecobee thermostats that can be controlled locally, if you are also using Home Assistant.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;GuyMan
6&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The latest Nests (from 2020 on - G3 or greater?) now support local Matter - So that's a long term option that's not cloud dependant. That all said, I certainly understand if you don't want to go back to the well with Google/Nest&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;G4 or the 'E'. G3 is not matter compatible currently, and i dare say Google will probably never do it. Sadly, the G4 was only released in the US - those of us in Europe are a bit stuck!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;nclark
8&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Planned obsolescence is exactly this, that is the first reason why you never buy anything that needs the internet to function the way you want it to function. If you do buy it, be sure this will happen and more than less in the coming years, it's either this sh!t or the now ever so popular pay a monthly fee. YOU OWN NOTHING AND WE OWN YOU&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;I have 5 2nd gen Thermostats. Not a fan of shelling out $1k to get them all replaced.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;simon5
10&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;These replies are all really helpful, thank you! I just checked and I have 9 Gen 1 and 2 Nest thermostats, all of which I will need to replace if I want to control them remotely (which I do). Google have offered the Gen 4 thermostats for $150 each, but I don't want to be in the same position as I am now if Google stop supporting them in the future. Is this a concern, or are we saying that these Gen 4 thermostats will ALWAYS be able to be controlled via the Hubitat hub? If not, what other thermostats do you all recommend that are more futureproof?&lt;/p&gt;
        &lt;p&gt;Cheers&lt;lb/&gt; Simon&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;GuyMan
11&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Yes, theoritically this - Assuming your using Matter as the interface protocol. That said, I think the support for matter T-Stats is fairly limited, setpoints, on/off, mode, and current temps, that's about it.&lt;/p&gt;
        &lt;p&gt;But you should definitely read the following threads - There is a HE built in Googe Nest Thermostat driver available - But some questions to be considered are:&lt;/p&gt;
        &lt;p&gt;Hence my comment above about "the technically" are supported, but @bcopeland would have to weigh in on what works, and what's actually exposed on the matter side. - But yes, after provisioning (setup), Matter is all local (so no Google cloud dependancy). The second thread mentioned above, has a good screenshot at the top that shows what the Matter driver exposes.&lt;/p&gt;
        &lt;p&gt;Regardless, I would just buy 1 and test (with a site with a good return policy), before going "all in", but obviously, YMMV&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;If always having local control over your thermostat while maintaining full functionality while being able to control remotely I can't recommend enough getting the Ecobee thermostats and setting up an instance of Home Assistant on a RPi or a cheap second hand computer.&lt;/p&gt;
        &lt;p&gt;Then through the Homekit Device integration in HA you connect the Ecobee via the Homekit code. (Just connect the Ecobee to your wifi first, not the Homekit app).&lt;/p&gt;
        &lt;p&gt;Then using the Home Assistant Device Bridge you can bring in any amount of entities you want of your thermostats into Hubitat and have full use of them. You can write all your rules/automations in Hubitat.&lt;/p&gt;
        &lt;p&gt;Then if wifi support is dropped by Ecobee you will always have local control. The benefit of this above matter is you will have every function you would have through the Ecobee app. Unless you didn't bring them all over and that choice is yours.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;simon5
13&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Thank you both! It looks as though the Nest integration is a bit messy/painful and could have limited functionality. I have Ecobee thermostats in another house and I haven't had any trouble with them, so while IMO they don't look as cool or feel as good as the Nest thermostats, it sounds as though they are currently more reliable.&lt;/p&gt;
        &lt;p&gt;@aaiyar I have a Hubitat C8-Pro, but are you saying that I can't just pair the Ecobees with the Hubitat and I need to run a RaspberryPi to somehow connect the Ecobees to the Hubitat? I'm afraid I don't have the time right now to play around with it and just need thermostats that can connect to and be controlled by the Hubitat.&lt;/p&gt;
        &lt;p&gt;EDIT: this article appears to say I can just connect the Ecobees directly to the Hubitat: Ecobee Integration | Hubitat Documentation&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;aaiyar
14&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;You can. Using the cloud integration you linked to.&lt;/p&gt;
        &lt;p&gt;If you want a local (non-cloud) integration, then you have to include Home Assistant in the mix.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;simon5
15&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Ah thanks. So I could run the ecobee thermostats with the cloud integration for now and then in a few years time if ecobee stop supporting the cloud integration then I could run a local instance of HA and still be able to control my thermostats via habita?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;aaiyar
16&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Might be easier in a few years when ecobee adds Matter compatibility to their products.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Having been burned by Google on their Nest Secure and Nest Protect offerings, I fear it is only a matter of time before they end of life their Nest 3rd generation thermostat. So I have already been researching it's replacement. Take a look at Go Control z-wave thermostat. Looks promising and it is natively supported in Hubitat. Plus we won't be bugged by Google into ceeding our autonomy to the utility company over the environmental settings. I have not which it why they bug me about it. Do not believe Google about being always being able to override their remote adjustments. A lesson I learned by reading complaints from users in California. Seems the state declared a grid emergency so commanded participant devices to set to much higher cooling temperatures. Ok fine but also locked out any ability to change it back. A "feature" Google never mentions and a permanent deal breaker for me.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Hopefully Ecobee comes out with a matter solution before October.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;of you truly want to avoid manufacturers dropping support, learn your lesson now.&lt;/p&gt;
        &lt;p&gt;get a completely offline solution like zwave/zigbee thermostat. then the rug can never be pulled from under you. so&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;aaiyar
20&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;FYI, ecobee thermostats can be controlled fully locally by Hubitat, using HomeKit. These thermostats do not have to connect to ecobee’s cloud for anything, including their initial onboarding.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://community.hubitat.com/t/nest-1st-gen-and-2nd-gen-thermostats-no-longer-supported-by-google-from-10-25-2025/152952"/></entry><entry><id>https://news.ycombinator.com/item?id=45143945</id><title>Should we revisit Extreme Programming in the age of AI?</title><updated>2025-09-06T03:05:49.950204+00:00</updated><content>&lt;doc fingerprint="8d4b1a3baaecedc9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Should we revisit Extreme Programming in the age of AI?&lt;/head&gt;
    &lt;p&gt;The pace of software output has never been faster. AI tooling and decades of platform innovation have dramatically lowered the barrier to code creation. With just a few prompts or API calls, it is now possible to generate entire products, features, infrastructure, and functionality in hours rather than weeks.&lt;/p&gt;
    &lt;p&gt;And yet, despite all this acceleration, delivery outcomes remain stubbornly poor. Too many initiatives underdeliver, budgets continue to overrun, and users are left underserved. If cheaper and faster code has not solved delivery, then the bottleneck must lie elsewhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;Output is not the problem&lt;/head&gt;
    &lt;p&gt;Typing has never been the bottleneck. We have seen successive waves of acceleration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The rise of high-level languages&lt;/item&gt;
      &lt;item&gt;Widespread adoption of frameworks and package managers&lt;/item&gt;
      &lt;item&gt;The move to DevOps and serverless computing&lt;/item&gt;
      &lt;item&gt;Developer platforms that abstract away infrastructure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And now, AI-enabled code generation.&lt;/p&gt;
    &lt;p&gt;Despite this acceleration, outcomes remain stubbornly inconsistent. The long-running Standish Chaos study still finds that most IT projects miss expectations, while McKinsey reports that 70% of digital transformations fail. More output has not meant better software.&lt;/p&gt;
    &lt;p&gt;If faster code generation alone doesn’t deliver value, the answer is not more acceleration, but smarter constraints. For me, that’s why XP resonates now more than ever: it teaches us to slow down just enough to learn, align, and build with intent.&lt;/p&gt;
    &lt;head rend="h3"&gt;XP as a counterweight&lt;/head&gt;
    &lt;p&gt;Unbounded acceleration leaves no time to steer. Without slowing down to notice mistakes, learn, and correct course, teams risk shipping software no one asked for.&lt;/p&gt;
    &lt;p&gt;Extreme Programming (XP), developed in the late 1990s, was never intended to maximise throughput. Quite the opposite: it introduced deliberate friction and constraints that enabled teams to learn, raising the probability they were moving in the right direction. One of its most radical principles, pair programming everything, halves raw output by design.&lt;/p&gt;
    &lt;p&gt;The principle is simple: go slower in the small so you can go faster in the large.&lt;/p&gt;
    &lt;p&gt;Take pair programming. On paper, you cut output in half. In practice, you double shared understanding. You surface assumptions early. You build trust. You improve quality. You raise the baseline of capability across the team.&lt;/p&gt;
    &lt;p&gt;This is the sociotechnical nature of XP practices. They shape collaboration as much as they shape code. They are an investment in learning, not just shipping. And in doing so, they provide direction, not just speed.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI magnifies the problem XP was built to solve&lt;/head&gt;
    &lt;p&gt;As code generation becomes effortless, a new risk emerges: producing software faster than we can validate it.&lt;/p&gt;
    &lt;p&gt;This is especially pronounced in agentic AI systems, where multiple autonomous agents generate, refine, and ship code. Without constraints, these systems can rapidly layer unvalidated logic on top of itself, entrenching assumptions and amplifying architectural complexity.&lt;/p&gt;
    &lt;p&gt;Recent research reinforces the risk: LLM accuracy declines the longer the context window. In practice, this means the more you “vibe code,” the more brittle the output becomes over time, because large language models perform best at the start and end of the context window, while the middle is poorly generalised and prone to error.&lt;/p&gt;
    &lt;p&gt;The result is brittle, tangled code that becomes increasingly expensive to change. Extreme Programming was designed to prevent exactly this kind of runaway entropy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Software is still human&lt;/head&gt;
    &lt;p&gt;Despite AI’s rise, software remains a profoundly human discipline. Code is written by people, for people, in organisations shaped by culture, incentives, and communication.&lt;/p&gt;
    &lt;p&gt;While tools evolve, the persistent barriers to delivery remain: alignment, shared context, clarity of outcome, and user validation.&lt;/p&gt;
    &lt;p&gt;XP’s values are still powerful today:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simplicity helps reduce system complexity&lt;/item&gt;
      &lt;item&gt;Communication keeps teams cohesive&lt;/item&gt;
      &lt;item&gt;Feedback drives learning and adaptation&lt;/item&gt;
      &lt;item&gt;Respect builds safety and trust&lt;/item&gt;
      &lt;item&gt;Courage empowers transparency and change&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;From feature factories to value delivery&lt;/head&gt;
    &lt;p&gt;The most successful teams do not chase speed at all costs. They prioritise flow over velocity and feedback over features.&lt;/p&gt;
    &lt;p&gt;XP’s emphasis on small batches, continuous integration, automated testing, and shared ownership helps teams remain adaptable, resilient, and user-focused.&lt;/p&gt;
    &lt;p&gt;As AI accelerates output, these practices will be critical to managing quality, risk, and intent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lessons from the past&lt;/head&gt;
    &lt;p&gt;The CHAOS report data is telling:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1994: only 16% of software projects delivered on time and on budget&lt;/item&gt;
      &lt;item&gt;2012: improved to 37%&lt;/item&gt;
      &lt;item&gt;2020: regressed to 31%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In over two decades of innovation: agile, DevOps, cloud-native platforms, and now AI, the net difference in reliable software delivery is just +14 percentage points.&lt;/p&gt;
    &lt;p&gt;Toolchains alone have not solved delivery. Methodology still matters.&lt;/p&gt;
    &lt;head rend="h3"&gt;What needs to change?&lt;/head&gt;
    &lt;p&gt;As we enter the next phase of software acceleration, three things become clear:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Output creation is no longer the constraint. We can produce code faster than we can validate or align it with real-world needs.&lt;/item&gt;
      &lt;item&gt;We must invest in outcome-generating capabilities. Stronger feedback loops, clearer product direction, tighter team collaboration, and greater design discipline.&lt;/item&gt;
      &lt;item&gt;The process needs to become more human, not less. Even as AI capabilities expand, sustainable delivery will always depend on human collaboration.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our view of the Product Operating Model is built on this principle: technology only delivers when teams are optimised for collaboration, clarity, and flow. By aligning product strategy, operating rhythms, and engineering practices around people, not just platforms, you create the conditions for sustainable delivery in the AI era.&lt;/p&gt;
    &lt;head rend="h2"&gt;Should we revisit XP in the age of AI?&lt;/head&gt;
    &lt;p&gt;Yes.&lt;/p&gt;
    &lt;p&gt;We need frameworks that anchor us to human-centred practices, even as tools become more powerful. XP provides both discipline and empathy. It centres the team. It elevates shared understanding above raw speed. It asks the most important question: are we building the right thing?&lt;/p&gt;
    &lt;p&gt;In an era defined by faster code and fewer constraints, XP is a rare methodology that reminds us: software is about people, not just code.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.hyperact.co.uk/blog/should-we-revisit-xp-in-the-age-of-ai"/></entry><entry><id>https://news.ycombinator.com/item?id=45144234</id><title>Quantum Mechanics, Concise Book</title><updated>2025-09-06T03:05:49.291457+00:00</updated><content>&lt;doc fingerprint="31d9813041c7d10a"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A concise text on quantum mechanics, intended for a general audience including CS, engineering, math, and physics undergrads, as well as anyone interested in a concise intro/overview of QM. Prerequisites: linear algebra, calculus, high school physics&lt;/p&gt;
    &lt;head rend="h1"&gt;basketballguy999/Quantum-Mechanics-Concise-Book&lt;/head&gt;
    &lt;head rend="h2"&gt;About&lt;/head&gt;
    &lt;p&gt;A concise text on quantum mechanics, intended for a general audience including CS, engineering, math, and physics undergrads, as well as anyone interested in a concise intro/overview of QM. Prerequisites: linear algebra, calculus, high school physics&lt;/p&gt;
    &lt;head rend="h3"&gt;Stars&lt;/head&gt;
    &lt;head rend="h3"&gt;Watchers&lt;/head&gt;
    &lt;head rend="h3"&gt;Forks&lt;/head&gt;
    &lt;head rend="h2"&gt;Releases&lt;/head&gt;
    &lt;p&gt;No releases published&lt;/p&gt;
    &lt;head rend="h2"&gt;Packages 0&lt;/head&gt;
    &lt;p&gt; No packages published &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/basketballguy999/Quantum-Mechanics-Concise-Book"/></entry><entry><id>https://news.ycombinator.com/item?id=45144337</id><title>The Universe Within 12.5 Light Years</title><updated>2025-09-06T03:05:48.741188+00:00</updated><content>&lt;doc fingerprint="b21697bf7e9b7625"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Universe within 12.5 Light Years&lt;lb/&gt;The Nearest Stars&lt;/head&gt;&lt;div&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt; Number of stars within 12.5 light years = 33&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;head rend="h3"&gt;About the Map&lt;/head&gt; This map shows all the star systems that lie within 12.5 light years of our Sun. Most of the stars are red dwarfs - stars with a tenth of the Sun's mass and less than one hundredth the luminosity. Roughly eighty percent of all the stars in the universe are red dwarfs, and the nearest star - Proxima - is a typical example. &lt;div&gt;&lt;table&gt;&lt;row&gt;&lt;cell colspan="2" role="head"&gt;Additional Maps&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt; This is a diagram that zooms out from the Earth's orbit to the nearest star system. It tries to show just how large the distance to the nearest star really is.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt; Here is a map of all the known stars that lie within 20 light years plotted using the data provided below.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell colspan="2" role="head"&gt;Data and Catalogs&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt; There are over 100 stars within 20 light years. This is a list of the known stars that lie within this distance.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt; This is a page showing some simple animations of double, triple and quadruple star systems, to demonstrate how stars orbit each other.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Information on the Nearest Stars&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Sun - Type=G2, Magnitude=-26.8, Distance=0.0000158 ly&lt;/item&gt;&lt;item rend="dd-1"&gt;A typical yellow dwarf star. It has eight planets orbiting it.&lt;/item&gt;&lt;item rend="dt-2"&gt;Proxima Centauri - Type=M5, Magnitude=11.0, Distance=4.22 ly&lt;/item&gt;&lt;item rend="dd-2"&gt;This dim red dwarf is the nearest star to the Sun, and it is a member of the Alpha Centauri system despite lying 0.24 light years from the main pair of stars, requiring over one million years to orbit them. Proxima was discovered in 1915 by Robert Innes and was at that time the least luminous star known. It is also a flare star - capable of brightening a magnitude or more in minutes.&lt;/item&gt;&lt;item rend="dt-3"&gt;Alpha Centauri A,B - Type=G2+K0, Magnitudes=0.0+1.4, Distance=4.39 ly&lt;/item&gt;&lt;item rend="dd-3"&gt;Just slightly further from us than Proxima, lie the orange and yellow dwarf stars that make up Alpha Centauri. Orbiting each other in an 80 year period, together they make up one of the brightest objects in southern hemisphere skies. Seen from Alpha Centauri, the third member of the system, Proxima, is a dim (magnitude 4.8) star.&lt;/item&gt;&lt;item rend="dt-4"&gt;Barnard's Star - Type=M5, Magnitude=9.6, Distance=5.94 ly&lt;/item&gt;&lt;item rend="dd-4"&gt;Famous for having the largest proper motion of any star, this dim red dwarf travels 0.29 degrees against the background sky in a century. Discovered by E Barnard in 1916, it was thought in the 1960's to have a couple of unseen planets orbiting it, but later observations disproved this. In another 8000 years Barnard's Star will become the closest star to us.&lt;/item&gt;&lt;item rend="dt-5"&gt;Wolf 359 - Type=M6, Magnitude=13.5, Distance=7.80 ly&lt;/item&gt;&lt;item rend="dd-5"&gt;An excessively dim red dwarf discovered by Max Wolf in 1918. For 25 years it was the least luminous star known.&lt;/item&gt;&lt;item rend="dt-6"&gt;Lalande 21185 - Type=M2, Magnitude=7.5, Distance=8.31 ly&lt;/item&gt;&lt;item rend="dd-6"&gt;Recorded in JJ Lalande's star catalogue compiled in the 1790's, this is one of the brightest red dwarfs in the sky, but it still needs binoculars to see it. G Gatewood reported in 1996 the possible indications of a couple of Jupiter sized planets orbiting it but this remains unconfirmed.&lt;/item&gt;&lt;item rend="dt-7"&gt;Sirius A,B - Type=A1+DA, Magnitudes=-1.4+8.4, Distance=8.60 ly&lt;/item&gt;&lt;item rend="dd-7"&gt;This brilliant white star is the brightest star in the night sky and the most luminous star within 25 light years. Its white dwarf companion was first seen in 1852, the first white dwarf ever seen. The orbital period is 50 years.&lt;/item&gt;&lt;item rend="dt-8"&gt;Luyten 726-8 A,B - Type=M5+M5, Magnitudes=12.4+13.3, Distance=8.73 ly&lt;/item&gt;&lt;item rend="dd-8"&gt;This is a dim binary system consisting of two red dwarfs. The system is perhaps more famously known as UV Ceti, the variable-star name of the second star in the system. It is a famous flare star and can visibly brighten by several magnitudes as it ejects flares from its surface similar to the ones seen on the surface of the Sun, but far more energetic. Both stars require about 200 years to orbit each other.&lt;/item&gt;&lt;item rend="dt-9"&gt;Ross 154 - Type=M4, Magnitude=10.4, Distance=9.69 ly&lt;/item&gt;&lt;item rend="dd-9"&gt;A dim red dwarf. It is one of a number of nearby stars catalogued by Frank Ross in the 1930's. It is also a known flare star.&lt;/item&gt;&lt;item rend="dt-10"&gt;Ross 248 - Type=M6, Magnitude=12.3, Distance=10.33 ly&lt;/item&gt;&lt;item rend="dd-10"&gt;Another dim red dwarf.&lt;/item&gt;&lt;item rend="dt-11"&gt;Epsilon Eridani - Type=K2, Magnitude=3.7, Distance=10.50 ly&lt;/item&gt;&lt;item rend="dd-11"&gt;An orange dwarf star. This star was searched for signs of intelligent life with the Green Bank radio telescope in 1960. The results, predictably, were negative. The IRAS satellite detected a lot of dust orbiting this star indicating a possible forming solar system, and even more recently, (Aug 2000), a Jupiter sized planet has been detected orbiting this star at a distance of 3.2 AU (480 million km).&lt;/item&gt;&lt;item rend="dt-12"&gt;Lacaille 9352 - Type=M2, Magnitude=7.4, Distance=10.73 ly&lt;/item&gt;&lt;item rend="dd-12"&gt;A fairly bright red dwarf which can easily be seen with binoculars, it was first recorded in Nicolas de Lacaille's catalogue of southern hemisphere stars compiled around 1752.&lt;/item&gt;&lt;item rend="dt-13"&gt;Ross 128 - Type=M4, Magnitude=11.1, Distance=10.89 ly&lt;/item&gt;&lt;item rend="dd-13"&gt;A dim red dwarf, also known as FI Vir - its variable star designation.&lt;/item&gt;&lt;item rend="dt-14"&gt;Luyten 789-6 A,B,C - Type=M5+M5+M7, Magnitudes=13.3+13.3+14.0, Distance=11.1 ly&lt;/item&gt;&lt;item rend="dd-14"&gt;There seems to be three red dwarfs in this system. The main pair orbiting each other in a 2 year period, and a dim third star orbiting the first at a very close range.&lt;/item&gt;&lt;item rend="dt-15"&gt;Procyon A,B - Type=F5+DA, Magnitudes=0.4+10.7, Distance=11.41 ly&lt;/item&gt;&lt;item rend="dd-15"&gt;A brilliant yellow-white star, and the eighth brightest star in the sky. With twice the diameter of the Sun, Procyon is also the largest star within 25 light years. Procyon is orbited by a white dwarf companion first seen optically in 1896. The orbital period is 41 years.&lt;/item&gt;&lt;item rend="dt-16"&gt;61 Cygni A,B - Type=K5+K7, Magnitudes=5.2+6.1, Distance=11.41 ly&lt;/item&gt;&lt;item rend="dd-16"&gt;This binary system of two orange dwarf stars is famous for being the first star ever to have its distance measured by F Bessel in 1838. Both stars are very similar but are widely separated (86 AU) requiring about 700 years to orbit each other.&lt;/item&gt;&lt;item rend="dt-17"&gt;Struve 2398 A,B - Type=M4+M5, Magnitudes=8.9+9.7, Distance=11.6 ly&lt;/item&gt;&lt;item rend="dd-17"&gt;A binary system of two red dwarfs named Struve 2398 from a catalogue of double stars published in 1827. This system is also known by the rather more boring name of BD+59°1915. The two stars are quite widely separated (50 AU) and orbit each other in a 450 year period.&lt;/item&gt;&lt;item rend="dt-18"&gt;Groombridge 34 A,B - Type=M2+M6, Magnitudes=8.1+11.1, Distance=11.64 ly&lt;/item&gt;&lt;item rend="dd-18"&gt;Another pair of red dwarfs, this system is usually called Groombridge 34 from an 1838 catalogue of northern stars or sometimes BD+43°44. Both stars are variable in brightness and have the variable star names of GX And and GQ And. Both stars lie far apart from each other (150 AU) and orbit each other in a 2500 year period.&lt;/item&gt;&lt;item rend="dt-19"&gt;Giclas 51-15 - Type=M6, Magnitude=14.8, Distance=11.8 ly&lt;/item&gt;&lt;item rend="dd-19"&gt;This excessively dim red dwarf is the least luminous star within 14 light years. It shines with just 0.01% of the Sun's luminosity.&lt;/item&gt;&lt;item rend="dt-20"&gt;Epsilon Indi A,B,C - Type=K5+T1+T6, Magnitude=4.7, Distance=11.83 ly&lt;/item&gt;&lt;item rend="dd-20"&gt;An orange dwarf. It is a similar star to Epsilon Eridani, although a little bit smaller and dimmer. Epsilon Indi is orbited by a pair of brown dwarfs - failed stars that are too small to burn. They were discovered in 2003 and they orbit each other in a 16 year period, and they are 1500 AU (220 billion km) from the main star and they require about 70 000 years to orbit it.&lt;/item&gt;&lt;item rend="dt-21"&gt;Tau Ceti - Type=G8, Magnitude=3.5, Distance=11.90 ly&lt;/item&gt;&lt;item rend="dd-21"&gt;The nearest, single, sun-like star. It was searched (unsuccessfully) for any signs of intelligent life in 1960, along with Epsilon Eridani.&lt;/item&gt;&lt;item rend="dt-22"&gt;Luyten 372-58 - Type=M5, Magnitude=13.0, Distance=12.1 ly&lt;/item&gt;&lt;item rend="dd-22"&gt;A very dim red dwarf. Although this star was catalogued decades ago, it has only recently had its distance determined with any accuracy.&lt;/item&gt;&lt;item rend="dt-23"&gt;Luyten 725-32 - Type=M5, Magnitude=12.1, Distance=12.1 ly&lt;/item&gt;&lt;item rend="dd-23"&gt;Another dim red dwarf.&lt;/item&gt;&lt;item rend="dt-24"&gt;Luyten's Star - Type=M3, Magnitude=9.8, Distance=12.39 ly&lt;/item&gt;&lt;item rend="dd-24"&gt;A red dwarf. It is named after Willem Luyten who realised it was a nearby star in 1935. The star lies just 1.2 light years away from Procyon, but it is not associated with it.&lt;/item&gt;&lt;/list&gt;&lt;p&gt; Epsilon Eridani is orbited by a large planet which might look like this. &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://www.atlasoftheuniverse.com/12lys.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45145457</id><title>GLM 4.5 with Claude Code</title><updated>2025-09-06T03:05:48.343517+00:00</updated><content>&lt;doc fingerprint="ccd2e51a790b4291"&gt;
  &lt;main&gt;&lt;code&gt;thinking.type&lt;/code&gt;parameter (with &lt;code&gt;enabled&lt;/code&gt; and &lt;code&gt;disabled&lt;/code&gt; settings), and dynamic thinking is enabled by default.
&lt;p&gt;Our most powerful reasoning model, with 355 billion parameters&lt;/p&gt;&lt;p&gt;Cost-Effective Lightweight Strong Performance&lt;/p&gt;&lt;p&gt;High Performance Strong Reasoning Ultra-Fast Response&lt;/p&gt;&lt;p&gt;Lightweight Strong Performance Ultra-Fast Response&lt;/p&gt;&lt;p&gt;Free Strong Performance Excellent for Reasoning Coding &amp;amp; Agents&lt;/p&gt;&lt;code&gt;thinking.type&lt;/code&gt; parameter. This parameter supports two values: &lt;code&gt;enabled&lt;/code&gt; (enabled) and &lt;code&gt;disabled&lt;/code&gt; (disabled). By default, dynamic thinking is enabled.
&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.z.ai/guides/llm/glm-4.5"/></entry><entry><id>https://news.ycombinator.com/item?id=45145624</id><title>The sunscreen scandal shocking Australia - the world's skin cancer capital</title><updated>2025-09-06T03:05:47.966140+00:00</updated><content>&lt;doc fingerprint="a560015c3eb28c5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The sunscreen scandal shocking Australia - the world's skin cancer capital&lt;/head&gt;
    &lt;p&gt;Like many Australians, Rach grew up "terrified of the sun" in a country that has the unenviable title of skin cancer capital of the world.&lt;/p&gt;
    &lt;p&gt;Her childhood was characterised by the infamous "no hat, no play" rule that is commonplace in Australian schools, 90s advertisements that warned the sun would give you cancer, and sunscreen tubes that stood guard at every door in her home.&lt;/p&gt;
    &lt;p&gt;It made the now 34-year-old the kind of person who religiously applies sunscreen multiple times a day and rarely leaves the house without a hat.&lt;/p&gt;
    &lt;p&gt;So she was shocked when doctors found a skin cancer on her nose during a check last November, something they said was abnormal given her age and ray-dodging regime.&lt;/p&gt;
    &lt;p&gt;Though technically classified as a "low grade" skin cancer – a basel cell carcinoma – it had to be surgically removed, leaving the Newcastle mum with a scar just below her eye.&lt;/p&gt;
    &lt;p&gt;"I was just confused, and I was a little bit angry because I was like, 'Are you kidding me?'" Rach – who asked that her surname not be used – told the BBC. "I thought I'd done all the right stuff and it still happened to me."&lt;/p&gt;
    &lt;p&gt;That rage grew when she learned the sunscreen she had been using for years was unreliable and, according to some tests, offered next to no sun protection at all.&lt;/p&gt;
    &lt;p&gt;Independent analysis by a trusted consumer advocacy group has found that several of Australia's most popular, and expensive, sunscreens are not providing the protection they claim to, kicking off a national scandal.&lt;/p&gt;
    &lt;p&gt;There has been a massive backlash from customers, a probe launched by the country's medical watchdog, multiple products pulled from shelves, and questions raised about the regulation of sunscreen around the globe.&lt;/p&gt;
    &lt;p&gt;"It's definitely not an issue isolated to Australia," cosmetic chemist Michelle Wong told the BBC.&lt;/p&gt;
    &lt;head rend="h2"&gt;The reckoning&lt;/head&gt;
    &lt;p&gt;Australians have a complicated relationship with the sun: they love it, but they also fear it.&lt;/p&gt;
    &lt;p&gt;Effective public health messaging – which has drilled "Slip, Slop, Slap" into their heads – competes with a beauty culture which often idolises bronzed skin.&lt;/p&gt;
    &lt;p&gt;The country has the highest incidence of skin cancers in the world and it is estimated that two out of three Australians will have at least one cut out in their lifetime.&lt;/p&gt;
    &lt;p&gt;So when Choice Australia released its damning report in June, it immediately made waves. The group had tested 20 sunscreens in an independent accredited Australian lab, finding 16 did not meet the SPF, or skin protection factor, rating listed on the packet.&lt;/p&gt;
    &lt;p&gt;Ultra Violette's Lean Screen SPF 50+ Mattifying Zinc Skinscreen, a facial product that Rach says she used exclusively, was the "most significant failure" identified. It returned a result of SPF 4, something that shocked Choice so much it commissioned a second test that produced a similar reading.&lt;/p&gt;
    &lt;p&gt;Other products that did not meet their SPF claims included those from Neutrogena, Banana Boat, Bondi Sands and the Cancer Council - but they all rejected Choice's findings and said their own independent testing showed their sunscreens worked as advertised.&lt;/p&gt;
    &lt;p&gt;The uproar was immediate for the brands named in the report, and also prompted a swift response from the Therapeutic Goods Association (TGA), which said it would investigate the findings and "take regulatory action as required".&lt;/p&gt;
    &lt;p&gt;Ultra Violette bit back, saying they were "confident that Lean Screen is safe and effective" and detailing extensive testing of the product – which has been sold in almost 30 countries, including the UK, and retails for upwards of A$50 (£24, $33).&lt;/p&gt;
    &lt;p&gt;But less than two months later, it announced that Lean Screen would be recalled after it returned inconsistent results across eight different sets of lab testing.&lt;/p&gt;
    &lt;p&gt;"We are deeply sorry that one of our products has fallen short of the standards we pride ourselves on and that you have come to expect of us," read a statement published to the brand's Instagram account.&lt;/p&gt;
    &lt;p&gt;It added that it has "since ended the relationship with the initial testing lab".&lt;/p&gt;
    &lt;p&gt;In the past fortnight, other brands have "paused" the sale of at least four more products, none of which were included in the Choice report.&lt;/p&gt;
    &lt;p&gt;Rach knows there is no way to prove that there is a link between her diagnosis and the brand of sunscreen she relied on. She says she is not alleging there is such a connection.&lt;/p&gt;
    &lt;p&gt;But she said Ultra Violette's response to the scandal was like "a kick in the guts".&lt;/p&gt;
    &lt;p&gt;She felt that they took no real accountability for the pitfalls of their product, and was let down by their decision to continue selling it for two months despite doubts over its efficacy.&lt;/p&gt;
    &lt;p&gt;"I just had like the five stages of grief, you know?" she said. "I was angry, I was upset, I was almost in denial."&lt;/p&gt;
    &lt;p&gt;Like Rach, a horde of annoyed customers say the saga has shaken their faith in the industry.&lt;/p&gt;
    &lt;p&gt;"A refund isn't really going to reverse years of sun damage, is it?" one wrote in response to Ultra Violette's recall statement.&lt;/p&gt;
    &lt;p&gt;Choice has urged the TGA to conduct further investigations into the sunscreen market, and also urged any brands who had reason to question the SPF protection listed on their products to remove them from sale immediately.&lt;/p&gt;
    &lt;p&gt;"It is clear there is a serious issue in the Australian sunscreen industry that urgently needs to be addressed," said Rosie Thomas, the director of campaigns, in a statement to the BBC.&lt;/p&gt;
    &lt;head rend="h2"&gt;How did this happen?&lt;/head&gt;
    &lt;p&gt;While in Europe sunscreen is classed as a cosmetic, Australia regulates it as a therapeutic good – essentially a medicine – which means it is subject to some of the most robust sunscreen regulations in the world.&lt;/p&gt;
    &lt;p&gt;And that's something many of the brands caught up in this saga trade on. So, how did this happen?&lt;/p&gt;
    &lt;p&gt;An investigation by the Australian Broadcasting Corporation found that a single US-based laboratory had certified at least half of the products that had failed Choice's testing, and that this facility routinely recorded high test results.&lt;/p&gt;
    &lt;p&gt;It also found that several of the sunscreens pulled from shelves shared a similar base formula and linked them to a manufacturer in Western Australia.&lt;/p&gt;
    &lt;p&gt;The TGA says it does not usually speak about ongoing investigations because it does not want to compromise them, but that it is also looking into "reviewing existing SPF testing requirements" which can be "highly subjective".&lt;/p&gt;
    &lt;p&gt;"The TGA is also aware that it is common practice for different sunscreen products to share the same or similar base formulations," a spokesperson said in a statement to the BBC.&lt;/p&gt;
    &lt;p&gt;"Ultimately it is the sponsor's [seller's] responsibility to ensure that their medicine remains compliant with all applicable legislative requirements."&lt;/p&gt;
    &lt;p&gt;Consistent and comfortable sunscreens which offer high protection are very technical and difficult to make, says Dr Wong, founder of Lab Muffin Beauty Science.&lt;/p&gt;
    &lt;p&gt;Everyone's skin responds differently to the product, he adds, and it's one that is almost always being stress-tested – by sweat, water, or makeup.&lt;/p&gt;
    &lt;p&gt;It is very difficult to rate effectively for the same reasons. Historically, it has been done by spreading the sunscreen on 10 people at the same thickness, then timing how long it takes for their skin to start burning both with and without the product applied.&lt;/p&gt;
    &lt;p&gt;While there are clear guidelines as to what you are looking for, Dr Wong says there is still a lot of variability. That is down to skin texture or tone, or even the colour of the walls, and "different labs get different results".&lt;/p&gt;
    &lt;p&gt;But she says results are also quite easy to fake, pointing to a 2019 probe by US authorities into a sunscreen testing laboratory which resulted in the owner being jailed for fraud.&lt;/p&gt;
    &lt;p&gt;Many sunscreen brands from all over the world use the same manufacturers and testing labs - and so this issue is unlikely to be isolated to Australia, she adds.&lt;/p&gt;
    &lt;p&gt;"Until someone goes out and tests a whole bunch of sunscreens in other countries, we just don't know the extent of it."&lt;/p&gt;
    &lt;p&gt;She says the scandal is a reminder that regulations are only as good as they are enforced.&lt;/p&gt;
    &lt;p&gt;But while it has touched a nerve for many people who are at high risk for skin cancer simply by virtue of being Australian, Dr Wong said she felt the panic triggered by the investigation was blown out of proportion.&lt;/p&gt;
    &lt;p&gt;She points to the world's largest clinical trial of sunscreen, done in the 90s, which found that the daily use of an SPF 16 sunscreen dramatically dropped skin cancer rates.&lt;/p&gt;
    &lt;p&gt;"95% of the sunscreens tested [by Choice] have high enough SPF to more than halve the incidence of skin cancer," Dr Wong said.&lt;/p&gt;
    &lt;p&gt;"Some of the SPF testing, I feel, has become a bit more of a marketing exercise than a real reflection of efficacy."&lt;/p&gt;
    &lt;p&gt;The most important thing you can do when choosing a sunscreen, she says, is actually wear enough of it – a full teaspoon at least for each part of your body, face included.&lt;/p&gt;
    &lt;p&gt;And ideally you should apply it about every two hours, especially if you have been sweating a lot or swimming.&lt;/p&gt;
    &lt;p&gt;Experts also advise that you combine the sunscreen with other safety methods, such as wearing protective clothing and seeking out shade.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c4gzl41rpdqo"/></entry><entry><id>https://news.ycombinator.com/item?id=45145794</id><title>Developing a Space Flight Simulator in Clojure</title><updated>2025-09-06T03:05:46.833377+00:00</updated><content>&lt;doc fingerprint="4c23b196c6d1b91"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Developing a Space Flight Simulator in Clojure&lt;/head&gt;05 Sep 2025&lt;p&gt;In 2017 I discovered the free of charge Orbiter 2016 space flight simulator which was proprietary at the time and it inspired me to develop a space flight simulator myself. I prototyped some rigid body physics in C and later in GNU Guile and also prototyped loading and rendering of Wavefront OBJ files. I used GNU Guile (a Scheme implementation) because it has a good native interface and of course it has hygienic macros. Eventually I got interested in Clojure because unlike GNU Guile it has multi-methods as well as fast hash maps and vectors. I finally decided to develop the game for real in Clojure. I have been developing a space flight simulator in Clojure for almost 5 years now. While using Clojure I have come to appreciate the immutable values and safe parallelism using atoms, agents, and refs.&lt;/p&gt;&lt;p&gt;In the beginning I decided to work on the hard parts first, which for me were 3D rendering of a planet, an atmosphere, shadows, and volumetric clouds. I read the OpenGL Superbible to get an understanding on what functionality OpenGL provides. When Orbiter was eventually open sourced and released unter MIT license here, I inspected the source code and discovered that about 90% of the code is graphics-related. So starting with the graphics problems was not a bad decision.&lt;/p&gt;&lt;head rend="h2"&gt;Software dependencies&lt;/head&gt;&lt;p&gt;The following software is used for development. The software libraries run on both GNU/Linux and Microsoft Windows.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Clojure the programming language&lt;/item&gt;&lt;item&gt;LWJGL provides Java wrappers for various libraries &lt;list rend="ul"&gt;&lt;item&gt;lwjgl-opengl for 3D graphics&lt;/item&gt;&lt;item&gt;lwjgl-glfw for windowing and input devices&lt;/item&gt;&lt;item&gt;lwjgl-nuklear for graphical user interfaces&lt;/item&gt;&lt;item&gt;lwjgl-stb for image I/O and using truetype fonts&lt;/item&gt;&lt;item&gt;lwjgl-assimp to load glTF 3D models with animation data&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Jolt Physics to simulate wheeled vehicles and collisions with meshes&lt;/item&gt;&lt;item&gt;Fastmath for fast matrix and vector math as well as spline interpolation&lt;/item&gt;&lt;item&gt;Comb for templating shader code&lt;/item&gt;&lt;item&gt;Instaparse to parse NASA Planetary Constant Kernel (PCK) files&lt;/item&gt;&lt;item&gt;Gloss to parse NASA Double Precision Array Files (DAF)&lt;/item&gt;&lt;item&gt;Coffi as a foreign function interface&lt;/item&gt;&lt;item&gt;core.memoize for least recently used caching of function results&lt;/item&gt;&lt;item&gt;Apache Commons Compress to read map tiles from tar files&lt;/item&gt;&lt;item&gt;Malli to add schemas to functions&lt;/item&gt;&lt;item&gt;Immuconf to load the configuration file&lt;/item&gt;&lt;item&gt;Progrock a progress bar for long running builds&lt;/item&gt;&lt;item&gt;Claypoole to implement parallel for loops&lt;/item&gt;&lt;item&gt;tools.build to build the project&lt;/item&gt;&lt;item&gt;clj-async-profiler Clojure profiler creating flame graphs&lt;/item&gt;&lt;item&gt;slf4j-timbre Java logging implementation for Clojure&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The deps.edn file contains operating system dependent LWJGL bindings. For example on GNU/Linux the deps.edn file contains the following:&lt;/p&gt;&lt;p&gt;In order to manage the different dependencies for Microsoft Windows, a separate Git branch is maintained.&lt;/p&gt;&lt;head rend="h2"&gt;Atmosphere rendering&lt;/head&gt;&lt;p&gt;For the atmosphere, Brunetonâs precomputed atmospheric scattering was used. The implementation uses a 2D transmittance table, a 2D surface scattering table, a 4D Rayleigh scattering, and a 4D Mie scattering table. The tables are computed using several iterations of numerical integration. Higher order functions for integration over a sphere and over a line segment were implemented in Clojure. Integration over a ray in 3D space (using fastmath vectors) was implemented as follows for example:&lt;/p&gt;&lt;p&gt;Precomputing the atmospheric tables takes several hours even though pmap was used. When sampling the multi-dimensional functions, pmap was used as a top-level loop and map was used for interior loops. Using java.nio.ByteBuffer the floating point values were converted to a byte array and then written to disk using a clojure.java.io/output-stream:&lt;/p&gt;&lt;p&gt;When launching the game, the lookup tables get loaded and copied into OpenGL textures. Shader functions are used to lookup and interpolate values from the tables. When rendering the planet surface or the space craft, the atmosphere essentially gets superimposed using ray tracing. After rendering the planet, a background quad is rendered to display the remaining part of the atmosphere above the horizon.&lt;/p&gt;&lt;head rend="h2"&gt;Templating OpenGL shaders&lt;/head&gt;&lt;p&gt;It is possible to make programming with OpenGL shaders more flexible by using a templating library such as Comb. The following shader defines multiple octaves of noise on a base noise function:&lt;/p&gt;&lt;p&gt;One can then for example define the function fbm_noise using octaves of the base function noise as follows:&lt;/p&gt;&lt;head rend="h2"&gt;Planet rendering&lt;/head&gt;&lt;p&gt;To render the planet, NASA Bluemarble data, NASA Blackmarble data, and NASA Elevation data was used. The images were converted to a multi resolution pyramid of map tiles. The following functions were implemented for color map tiles and for elevation tiles:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;a function to load and cache map tiles of given 2D tile index and level of detail&lt;/item&gt;&lt;item&gt;a function to extract a pixel from a map tile&lt;/item&gt;&lt;item&gt;a function to extract the pixel for a specific longitude and latitude&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The functions for extracting a pixel for given longitude and latitude then were used to generate a cube map with a quad tree of tiles for each face. For each tile, the following files were generated:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A daytime texture&lt;/item&gt;&lt;item&gt;A night time texture&lt;/item&gt;&lt;item&gt;An image of 3D vectors defining a surface mesh&lt;/item&gt;&lt;item&gt;A water mask&lt;/item&gt;&lt;item&gt;A normal map&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Altogether 655350 files were generated. Because the Steam ContentBuilder does not support a large number of files, each row of tile data was aggregated into a tar file. The Apache Commons Compress library allows you to open a tar file to get a list of entries and then perform random access on the contents of the tar file. A Clojure LRU cache was used to maintain a cache of open tar files for improved performance.&lt;/p&gt;&lt;p&gt;At run time, a future is created, which returns an updated tile tree, a list of tiles to drop, and a path list of the tiles to load into OpenGL. When the future is realized, the main thread deletes the OpenGL textures from the drop list, and then uses the path list to get the new loaded images from the tile tree, load them into OpenGL textures, and create an updated tile tree with the new OpenGL textures added. The following functions to manipulate quad trees were implemented to realize this:&lt;/p&gt;&lt;head rend="h2"&gt;Other topics&lt;/head&gt;&lt;head rend="h3"&gt;Solar system&lt;/head&gt;&lt;p&gt;The astronomy code for getting the position and orientation of planets was implemented according to the Skyfield Python library. The Python library in turn is based on the SPICE toolkit of the NASA JPL. The JPL basically provides sequences of Chebyshev polynomials to interpolate positions of Moon and planets as well as the orientation of the Moon as binary files. Reference coordinate systems and orientations of other bodies are provided in text files which consist of human and machine readable sections. The binary files were parsed using Gloss (see Wiki for some examples) and the text files using Instaparse.&lt;/p&gt;&lt;head rend="h3"&gt;Jolt bindings&lt;/head&gt;&lt;p&gt;The required Jolt functions for wheeled vehicle dynamics and collisions with meshes were wrapped in C functions and compiled into a shared library. The Coffi Clojure library (which is a wrapper for Javaâs new Foreign Function &amp;amp; Memory API) was used to make the C functions and data types usable in Clojure.&lt;/p&gt;&lt;p&gt;For example the following code implements a call to the C function add_force:&lt;/p&gt;&lt;p&gt;Here ::vec3 refers to a custom composite type defined using basic types. The memory layout, serialisation, and deserialisation for ::vec3 are defined as follows:&lt;/p&gt;&lt;head rend="h3"&gt;Performance&lt;/head&gt;&lt;p&gt;The clj-async-profiler was used to create flame graphs visualising the performance of the game. In order to get reflection warnings for Java calls without sufficient type declarations, *unchecked-math* was set to :warn-on-boxed.&lt;/p&gt;&lt;p&gt;Furthermore to discover missing declarations of numerical types, *warn-on-reflection* was set to true.&lt;/p&gt;&lt;p&gt;To reduce garbage collector pauses, the ZGC low-latency garbage collector for the JVM was used. The following section in deps.edn ensures that the ZGC garbage collector is used when running the project with clj -M:run:&lt;/p&gt;&lt;p&gt;The option to use ZGC is also specified in the Packr JSON file used to deploy the application.&lt;/p&gt;&lt;head rend="h3"&gt;Building the project&lt;/head&gt;&lt;p&gt;In order to build the map tiles, atmospheric lookup tables, and other data files using tools.build, the project source code was made available in the build.clj file using a :local/root dependency:&lt;/p&gt;&lt;p&gt;Various targets were defined to build the different components of the project. For example the atmospheric lookup tables can be build by specifying clj -T:build atmosphere-lut on the command line.&lt;/p&gt;&lt;p&gt;The following section in the build.clj file was added to allow creating an âUberjarâ JAR file with all dependencies by specifying clj -T:build uber on the command-line.&lt;/p&gt;&lt;p&gt;To create a Linux executable with Packr, one can then run java -jar packr-all-4.0.0.jar scripts/packr-config-linux.json where the JSON file has the following content:&lt;/p&gt;&lt;p&gt;In order to distribute the game on Steam, three depots were created:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;a data depot with the operating system independent data files&lt;/item&gt;&lt;item&gt;a Linux depot with the Linux executable and Uberjar including LWJGLâs Linux native bindings&lt;/item&gt;&lt;item&gt;and a Windows depot with the Windows executable and an Uberjar including LWJGLâs Windows native bindings&lt;/item&gt;&lt;/list&gt;&lt;p&gt;When updating a depot, the Steam ContentBuilder command line tool creates and uploads a patch in order to preserve storage space and bandwidth.&lt;/p&gt;&lt;head rend="h2"&gt;Future work&lt;/head&gt;&lt;p&gt;Although the hard parts are mostly done, there are still several things to do:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;control surfaces and thruster graphics&lt;/item&gt;&lt;item&gt;launchpad and runway graphics&lt;/item&gt;&lt;item&gt;sound effects&lt;/item&gt;&lt;item&gt;a 3D cockpit&lt;/item&gt;&lt;item&gt;the Moon&lt;/item&gt;&lt;item&gt;a space station&lt;/item&gt;&lt;/list&gt;&lt;p&gt;It would also be interesting to make the game modable in a safe way (maybe evaluating Clojure files in a sandboxed environment?).&lt;/p&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;You can find the source code on Github. Currently there is only a playtest build, but if you want to get notified, when the game gets released, you can wishlist it here.&lt;/p&gt;&lt;p&gt;Anyway, let me know any comments and suggestions.&lt;/p&gt;&lt;p&gt;Enjoy!&lt;/p&gt;&lt;head rend="h2"&gt;Related blog posts&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Flight dynamics model for simulating Venturestar style spacecraft&lt;/item&gt;&lt;item&gt;Test Driven Development with OpenGL&lt;/item&gt;&lt;item&gt;Implementing GUIs using Clojure and LWJGL Nuklear bindings&lt;/item&gt;&lt;item&gt;Procedural Volumetric Clouds&lt;/item&gt;&lt;item&gt;Procedural generation of global cloud cover&lt;/item&gt;&lt;item&gt;Reversed-Z Rendering in OpenGL&lt;/item&gt;&lt;item&gt;Specifying Clojure function schemas with Malli&lt;/item&gt;&lt;item&gt;Implement an Interpreter using Clojure Instaparse&lt;/item&gt;&lt;item&gt;Orbits with Jolt Physics&lt;/item&gt;&lt;item&gt;Getting started with the Jolt Physics Engine&lt;/item&gt;&lt;item&gt;Create Blender bones and animate and import with Assimp&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wedesoft.de/software/2025/09/05/clojure-game/"/></entry></feed>