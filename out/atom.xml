<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-08-30T07:31:59.002769+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45038074</id><title>Delete tests</title><updated>2025-08-30T07:32:06.094178+00:00</updated><content>&lt;doc fingerprint="ca910c8667853ef7"&gt;
  &lt;main&gt;
    &lt;p&gt;30 Jun 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;You should delete tests&lt;/head&gt;
    &lt;p&gt;We’ve had decades of thought leadership around testing, especially coming from holistic development philosophies like Agile, TDD, and BDD. After all that time and several supposedly superseding movements, the developers I talk to seem to have developed a folk wisdom around tests.&lt;/p&gt;
    &lt;p&gt;That consensus seems to boil down to simple but mostly helpful axioms, like “include tests for your changes” and “write a new test when you fix a bug to prevent regressions”. Unfortunately, one of those consensus beliefs seems to be “it is blasphemy to delete a test”, and that belief is not just wrong but actively harmful.&lt;/p&gt;
    &lt;p&gt;Let’s talk about why you should delete tests.&lt;/p&gt;
    &lt;p&gt;To know why we should delete tests, let’s start with why we write tests in the first place. Why do we write tests? At the surface level, it’s to see if our program works the way we expect. But that doesn’t explain why we would write automated tests rather than simply run our program and observe if it works.&lt;/p&gt;
    &lt;p&gt;If you’ve ever tried to work on a project with no tests, I’m sure you’ve experienced the sinking sensation of backing yourself into a corner over time. The longer the project runs, the worse it gets, and eventually every possible change includes stressfully wondering if you broke something, wondering what you missed, and frantically deploying fix after revert after fix after revert as fast as possible because each frantic fix broke something else.&lt;/p&gt;
    &lt;p&gt;That nightmare is why automated tests exist: we need to be able to write software with confidence that we aren’t accidentally breaking everything we made before. The bigger a project gets, the more possible failures there are, and the more tests you need to have confidence it all works at the same time.&lt;/p&gt;
    &lt;p&gt;Confidence is the point of writing tests.&lt;/p&gt;
    &lt;p&gt;You run the tests so you can be confident when you open a pull request. CI runs the tests so you can be confident when you merge. CD runs the tests so you can be confident when you deploy. Tests exist to increase human confidence that a change will succeed.&lt;/p&gt;
    &lt;p&gt;Now that we know why tests exist, we can extrapolate when tests need to stop existing: any time they decrease confidence in a change.&lt;/p&gt;
    &lt;p&gt;How can a test possibly decrease confidence, you ask?&lt;/p&gt;
    &lt;p&gt;The biggest way tests decrease confidence is by failing at random. Flaky tests seem to fall into a cognitive hole, spreading the insidious costs across every engineer for months, and continuing to flake even after hours or days of attempts to fix them. “Oh, it’s failing because of that flaky test, it’s actually fine” is something I have heard more times than I can count, even when the code is broken and the tests are really failing. If your test is creating confidence in broken code with failing tests, it would be better for it to not exist. Delete the tests.&lt;/p&gt;
    &lt;p&gt;Another argument I’ve heard several times in favor of keeping flaky tests is “what if one day this flaky tests stops something from breaking”. It’s an understandable fear, maybe this test could catch a real problem in the future! But the costs of a flaky test can quickly be measured in days or even weeks of derailed work, and a future bug isn’t likely to take days or weeks to fix. Allow the possibility of a bug in the future to save everyone’s sanity and productivity now. If the future bug occurs, fix it and write a new test that doesn’t flake. Today, delete the tests.&lt;/p&gt;
    &lt;p&gt;Intermittent failures aren’t the only reason to delete tests, though. What if your tests are written so that a one line code change means updating 150 tests? Does 150 checks on your change really make you more confident than two, or maybe three checks? Delete the tests.&lt;/p&gt;
    &lt;p&gt;What if your tests take so long to run that you can’t run them all between merges, and you start skipping some? Ideally, you want tiers of confidence, with quick tests for commits, longer tests for merges, and (maybe) a giant suite so long that it can only fully run against production. The most common quick fix that I’ve seen, though, is the opposite: just turn off a chunk of tests because they always pass and the suite takes too long. When a test doesn’t get run, but people act like it’s green, that’s another kind of flaky test. A false pass harms your confidence just as much as a false failure. You will get bitten by the mismatch, if you have any tests that don’t get run. Stop fooling yourself, and delete the tests.&lt;/p&gt;
    &lt;p&gt;Even worse, what if your business requirements have changed, and now you have thousands of lines of tests failing because they test the wrong thing? Updating irrelevant tests to pass isn’t going to increase your confidence in the new behavior. You don’t need to update the old tests, you need to test the new behavior directly. Delete the tests.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andre.arko.net/2025/06/30/you-should-delete-tests/"/></entry><entry><id>https://news.ycombinator.com/item?id=45041744</id><title>I'm working on implementing a programming language all my own</title><updated>2025-08-30T07:32:05.520783+00:00</updated><content>&lt;doc fingerprint="74fe8813038c3b81"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;To the surprise of literally no one, I'm working on implementing a programming language all my own&lt;/head&gt;
    &lt;p&gt;Inspired by conversation at a recent Future of Coding event, I decided I’d write up a little something about the programming language I’ve been working on (for what feels like forever) before I’ve gotten it to a totally shareable state. I have a working interpreter that I’m pretty pleased with, but I don’t yet have an interactive environment for creating, exploring, debugging, and running code — I have this idea for a Smalltalk-flavored infinite canvas dev experience that’ll work in the browser. Hoping that’ll be ready soon(ish)!&lt;/p&gt;
    &lt;p&gt;Author’s note: Cutting in real fast from the future with an after-the-fact update! I pulled together a relatively simple, standalone playground for folks to explore Baba Yaga a bit more!&lt;/p&gt;
    &lt;head rend="h2"&gt;Meet Baba Yaga!&lt;/head&gt;
    &lt;p&gt;Baba Yaga started as a purely aesthetic endeavor. Like a beaver drawn to slowing the flow of a river, I had this idea that was haunting me about what I wanted a language to look like on screen and kinda worked backwards from there. To start, I wrote a few fantasy programs, and then started to think through how to get that code to run.&lt;/p&gt;
    &lt;p&gt;I have no intention whatsoever of Baba Yaga becoming anything but a curiosity and an exploratory project for me. Because of this, it is really, really biased towards the things I adore in a programming experience, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;immutability&lt;/item&gt;
      &lt;item&gt;functional-first everything everywhere&lt;/item&gt;
      &lt;item&gt;minimal syntax to learn&lt;/item&gt;
      &lt;item&gt;some batteries included&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is no real groundbreaking single feature that I think is especially notable about Baba Yaga. Instead, it mixes a bunch of familiar functional programming concepts into a hopefully cohesive and expressive combination that is orders of magnitude simpler than other such functional languages. Baba Yaga is kinda like Toki Pona but for Haskell.&lt;/p&gt;
    &lt;head rend="h3"&gt;Basic Syntax and Data Types&lt;/head&gt;
    &lt;p&gt;The syntax aims for visual clarity with minimal punctuation. Over the last few years, I’ve really fallen for typed languages, but I also don’t have a big capital T type theory brain. So, Baba Yaga only has some core types that fell out of its approach to control flow with pattern matching.&lt;/p&gt;
    &lt;head rend="h4"&gt;Declarations&lt;/head&gt;
    &lt;p&gt;Variables and functions use the same declaration pattern. Function calls don’t require parentheses, though you can use them for grouping and disambiguation.&lt;/p&gt;
    &lt;code&gt;// Variables
transport : "Chicken House";
number : 42;

// Functions
add : x y -&amp;gt; x + y;
square : x -&amp;gt; x * x;

// Function calls
result : add 5 3;
nested : add (square 4) 2;&lt;/code&gt;
    &lt;p&gt;Functions can also be curried without additional ceremony:&lt;/p&gt;
    &lt;code&gt;// Curried function
addCurried : x -&amp;gt; y -&amp;gt; x + y;
add5 : addCurried 5;
result : add5 3;  // 8&lt;/code&gt;
    &lt;head rend="h4"&gt;Data Types&lt;/head&gt;
    &lt;p&gt;The language includes a few essential, immutable data types.&lt;/p&gt;
    &lt;code&gt;// Numbers (Int and Float are distinct)
age : 36;
temperature : 32.6;
accountBalance: -12000;

// Strings with the '..' concatenation operator
fullName : "Lucy" .. " " .. "Snowe";

// Booleans
isActive : true;

// Lists (immutable)
numbers : [1, 2, 3, 4, 5];
firstItem : numbers.0;  // zero-indexed access

// Tables (immutable key-value structures)
person : {name: "Lucy Snowe", age: 23, city: "Villette"};
personName : person.name;&lt;/code&gt;
    &lt;head rend="h4"&gt;Types and Validation&lt;/head&gt;
    &lt;p&gt;Types are optional but can be explicitly declared. Without declarations, values carry runtime type tags (&lt;code&gt;Int&lt;/code&gt;, &lt;code&gt;Float&lt;/code&gt;, &lt;code&gt;String&lt;/code&gt;, &lt;code&gt;Bool&lt;/code&gt;, &lt;code&gt;List&lt;/code&gt;, &lt;code&gt;Table&lt;/code&gt;). Parameter and return type validation happens at call time only for that stuff which includes annotations. Anything without type annotations is left to do what it likes. Slip slip slip-and-slide.&lt;/p&gt;
    &lt;code&gt;// Type declaration (name Type)
myValue Int;
myOtherValue Int;

// Assignment must match declared type
myValue : 10;        // OK
myOtherValue : "x";  // Error: expected Int&lt;/code&gt;
    &lt;head rend="h3"&gt;Control Flow with Pattern Matching&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;when&lt;/code&gt; expression is the language’s only control flow construct. No &lt;code&gt;if&lt;/code&gt;/&lt;code&gt;else&lt;/code&gt; or &lt;code&gt;switch&lt;/code&gt; statements exist. I did this mostly to see if I could, but also to help encourage thinking about the shape of data rather than imperative conditions. Philosophically speaking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;if/else asks “is this condition true?”&lt;/item&gt;
      &lt;item&gt;switch asks “is this value equal to one of these constants?”&lt;/item&gt;
      &lt;item&gt;pattern matching asks “does this data look like this? If so, give me the pieces.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can match on simple values, types, and use guards for more complex conditions.&lt;/p&gt;
    &lt;code&gt;// Match on simple values
describe : x -&amp;gt;
  when x is
    0 then "Zero"
    1 then "One"
    _ then "Something else"; // _ is the wildcard

// Match on types
processValue : value -&amp;gt;
  when value is
    Int    then "Got an integer"
    String then "Got text"
    _      then "Got something else";

// Use pattern guards for complex conditions
categorize : n -&amp;gt;
  when n is
    x if (x &amp;gt; 0 and x &amp;lt; 10) then "small positive"
    x if (x &amp;gt;= 10)          then "large positive"
    x if (x &amp;lt; 0)            then "negative"
    0                       then "zero";&lt;/code&gt;
    &lt;p&gt;Baba Yaga also supports matching against multiple discriminants and destructuring data structures like tables.&lt;/p&gt;
    &lt;code&gt;// Multiple discriminants
checkPosition : x y -&amp;gt;
  when x y is
    0 0 then "Origin"
    1 1 then "Diagonal"
    _ _ then "Somewhere else";

// Destructuring and nested matching
processUser : user -&amp;gt;
  when user is
    {name: n, age: a, active: true} then
      when a is
        age if (age &amp;lt; 18) then "Minor: " .. n
        age if (age &amp;gt;= 18 and age &amp;lt; 65) then "Adult: " .. n
        _ then "Senior: " .. n
    {name: n, active: false} then "Inactive: " .. n
    _ then "Invalid user";&lt;/code&gt;
    &lt;p&gt;Here’s a chunk of Conway’s Game of Life that I used to test if the language could handle non-trivial algorithms. Like the famous implementation in APL, each generation is a pure transformation of the previous state.&lt;/p&gt;
    &lt;code&gt;// Apply Game of Life rules
nextCell : grid row col -&amp;gt;
  with (
    current : grid.(row).(col);
    neighbors : countNeighbors grid row col;
  ) -&amp;gt;
    when current is
      1 then when (neighbors = 2 or neighbors = 3) is true then 1 _ then 0
      _ then when (neighbors = 3) is true then 1 _ then 0;&lt;/code&gt;
    &lt;head rend="h3"&gt;Working with Collections&lt;/head&gt;
    &lt;p&gt;All list operations are immutable and return new lists. This includes standard functional staples as well as utilities inspired by other array programming languages.&lt;/p&gt;
    &lt;head rend="h4"&gt;Functional Staples&lt;/head&gt;
    &lt;p&gt;The higher-order functions that are the bread and butter of most functional programming work as expected.&lt;/p&gt;
    &lt;code&gt;// Basic list operations
original : [1, 2, 3];
withFour : append original 4;      // [1, 2, 3, 4]
withZero : prepend 0 original;     // [0, 1, 2, 3]
combined : concat [1, 2] [3, 4];   // [1, 2, 3, 4]

// Higher-order functions
numbers : [1, 2, 3, 4, 5, 6];
doubled : map (x -&amp;gt; x * 2) numbers;           // [2, 4, 6, 8, 10, 12]
evens : filter (x -&amp;gt; x % 2 = 0) numbers;      // [2, 4, 6]
sum : reduce (acc x -&amp;gt; acc + x) 0 numbers;    // 21&lt;/code&gt;
    &lt;head rend="h4"&gt;Array Programming Utilities&lt;/head&gt;
    &lt;p&gt;The array programming features follow the same pattern as the other higher-order functions.&lt;/p&gt;
    &lt;code&gt;data : [10, 21, 30, 43, 50];

// Find indices where a predicate is true
evenIndices : where (x -&amp;gt; x % 2 = 0) data;        // [0, 2, 4]

// Select elements at specific indices
selected : at [0, 2, 4] data;                     // [10, 30, 50]

// Cumulative operations (like APL's scan)
cumulative : cumsum [1, 2, 3, 4, 5];              // [0, 1, 3, 6, 10, 15]

// Broadcasting (apply a function with a scalar to each element)
addTen : broadcast (x y -&amp;gt; x + y) 10 [1, 2, 3];   // [11, 12, 13]

// Reshape a flat array into a matrix
matrix : reshape [2, 3] [1, 2, 3, 4, 5, 6]; // [[1, 2, 3], [4, 5, 6]]&lt;/code&gt;
    &lt;p&gt;These operations compose pretty well with the functional style. You can chain &lt;code&gt;where&lt;/code&gt; to find indices, &lt;code&gt;at&lt;/code&gt; to select values, and then process those with &lt;code&gt;map&lt;/code&gt; without experiencing any wild dissonance, I don’t think.&lt;/p&gt;
    &lt;head rend="h3"&gt;Error Handling&lt;/head&gt;
    &lt;p&gt;Instead of exceptions, Baba Yaga uses a &lt;code&gt;Result&lt;/code&gt; type (&lt;code&gt;Ok value&lt;/code&gt; or &lt;code&gt;Err message&lt;/code&gt;) for operations that might fail. This introduces explicit error representation and handling.&lt;/p&gt;
    &lt;code&gt;divide : x y -&amp;gt;
  when y is
    0 then Err "Cannot divide by zero"
    _ then Ok (x / y);

handleDivision : x y -&amp;gt;
  when (divide x y) is
    Ok result   then result
    Err message then 0; // Provide a default value on failure&lt;/code&gt;
    &lt;head rend="h3"&gt;Local Bindings and Utilities&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;with&lt;/code&gt; keyword creates immutable local bindings within an expression. For mutually recursive local functions, use &lt;code&gt;with rec&lt;/code&gt;. Mutually recursive functions work without any special syntax at the global scope.&lt;/p&gt;
    &lt;code&gt;// 'with' for local, immutable bindings
processOrder : order -&amp;gt;
  with (
    taxRate : 0.08;
    subtotal : order.items * order.price;
    total : subtotal + (subtotal * taxRate);
  ) -&amp;gt;
    {subtotal: subtotal, total: total};

// 'with rec' for mutual recursion
evenOdd : n -&amp;gt;
  with rec (
    isEven : x -&amp;gt; when x is 0 then true _ then isOdd (x - 1);
    isOdd : x -&amp;gt; when x is 0 then false _ then isEven (x - 1);
  ) -&amp;gt;
    {even: isEven n, odd: isOdd n};&lt;/code&gt;
    &lt;p&gt;The language also comes with a handful of utilities organized in namespaces like &lt;code&gt;str&lt;/code&gt;, &lt;code&gt;math&lt;/code&gt;, and &lt;code&gt;validate&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;words : str.split "hello,world,banana" ",";    // ["hello", "world", "banana"]
absolute : math.abs -5;                        // 5
valid : validate.range 1 10 5;                 // true
debug.print "Processing" someValue;&lt;/code&gt;
    &lt;head rend="h3"&gt;JavaScript Interoperability&lt;/head&gt;
    &lt;p&gt;Since Baba Yaga runs on JavaScript, it needs a way to call JS functions…or, like, I wanted to have a way to call JS from Baba Yaga as a sort of escape hatch so that I didn’t have to remake the whole entire world. But JS doesn’t have the same kinda functional guarantees as Baba Yaga is trying to ensure, so all JS calls return &lt;code&gt;Result&lt;/code&gt; types. This helps to maintain the functional programming model even when crossing into imperative territory. This means you can’t accidentally throw exceptions from JS that crash your functional program.&lt;/p&gt;
    &lt;code&gt;// Call JavaScript functions (returns a Result type)
jsonResult : io.callJS "JSON.parse" ["{\"x\": 10}"];

// Convert between JS and Baba Yaga data
jsArray : io.listToJSArray [1, 2, 3];
babaTable : when (io.callJS "JSON.parse" ["{}"]) is
  Ok obj then io.objectToTable obj
  Err msg then Err msg;&lt;/code&gt;
    &lt;p&gt;This is likely the roughest edge of the entire language and where I need to spend the most time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don’t ya know / They’re talking about a implementation?&lt;/head&gt;
    &lt;p&gt;In the past I’ve implemented a lot of toy languages. Working on Baba Yaga has been a great learning experience on how to implement a quote real language unquote. I’ve tried not to be slap-dash about my approach, and, while I don’t want Baba Yaga to take the world by storm, nor do I really expect many/any folks to really ever use it, I wanted it to be performant enough to use for games and sketches and maybe some live code stuff.&lt;/p&gt;
    &lt;p&gt;To help meet those goals, some things that I considered during th implementation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two engines: The project has a stable “legacy” engine and an optimized one. The optimized one doesn’t work super duper well, though, so, it is turned off by default, but I should revisit that.&lt;/item&gt;
      &lt;item&gt;Enormous test suite: Working on rawk I realized the only way to test a language is to use that language a lot, so having a test suite that covers all of the language features is really really useful…but also even a tiny language ends up having a whole freaking lot of surface area to test when it has non-trivial syntax rules more complicated than you get in a forth or a scheme.&lt;/item&gt;
      &lt;item&gt;Developer-focused, Elm-inspired error handling: The error system provides detailed messages with source location, visual pointers, and suggestions…because Elm poisoned me, and because I think error handling shouldn’t be an implementation after thought but like the whole-freaking-point of the exercise.&lt;/item&gt;
      &lt;item&gt;Security-focused interop: The JavaScript integration includes function allowlisting, execution timeouts, and memory limits to maintain functional guarantees and also to prevent ya from just writing JS all the time (looking at myself).&lt;/item&gt;
      &lt;item&gt;Performance-aware design: This is a space I had literally no understanding of before starting this. I read a bunch of blog posts and combed through a number of implementations of other languages. The implementation uses stuff like object pooling for AST nodes and includes benchmarks to try and measure the impact of optimizations, like using arrays instead of hash maps for variable lookups…but I’ll be honest, this is the bit of the project where I likely have the most to learn, still.&lt;/item&gt;
      &lt;item&gt;Runtime configuration: Because Baba Yaga targets a handful of JavaScript runtimes, the engine supports type-safe presets for the different environments (development, production, sandbox), allowing for different recursion depths and memory limits. I haven’t actually ever needed to tune these away from the default, but the option exists!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Full Language Reference Card&lt;/head&gt;
    &lt;code&gt;BABA YAGA LANGUAGE REFERENCE
============================

SYNTAX
------
var : value;                    // assignment
var Type; var : value;          // typed assignment  
f : x -&amp;gt; body;                  // function
f : x y -&amp;gt; body;                // multi-param function
f : (x: Type) -&amp;gt; Type -&amp;gt; body;  // typed function
f : x -&amp;gt; y -&amp;gt; body;             // curried function
f : x -&amp;gt; with (locals) -&amp;gt; body; // with locals
f : x -&amp;gt; with rec (fns) -&amp;gt; body;// with mutual recursion

LITERALS
--------
42                              // Int
3.14                            // Float  
"text"                          // String
true false                      // Bool
[1,2,3]                         // List
{a:1, b:2}                      // Table
PI INFINITY                     // constants

OPERATORS (precedence high→low)
-------------------------------
f x, obj.prop                   // call, access
- !                             // unary minus, not
* / %                           // multiply, divide, modulo
+ -                             // add, subtract  
= != &amp;lt; &amp;lt;= &amp;gt; &amp;gt;=                  // comparison
and or                          // logical
..                              // string concat

CONTROL FLOW
------------
when x is                       // pattern match
  0 then "zero"
  Int then "number" 
  _ then "other";

when x y is                     // multi-discriminant
  0 0 then "origin"
  _ _ then "other";

x if (condition) then result    // pattern guard
_ then "fallback";

TYPES
-----
Int Float Number String Bool List Table Result Function

Int ⊂ Float ⊂ Number            // type hierarchy
Ok value | Err message          // Result variants

ARRAY OPERATIONS
----------------
// Core HOFs
map f xs                        // [f x | x &amp;lt;- xs]
filter p xs                     // [x | x &amp;lt;- xs, p x]
reduce f z xs                   // f(...f(f(z,x1),x2)...,xn)

// Array programming  
scan f z xs                     // cumulative reduce
cumsum xs                       // cumulative sum
cumprod xs                      // cumulative product
at indices xs                   // xs[indices]
where p xs                      // indices where p x is true
take n xs                       // first n elements
drop n xs                       // drop first n elements
broadcast f scalar xs           // f scalar to each x
zipWith f xs ys                 // [f x y | (x,y) &amp;lt;- zip xs ys]
reshape dims xs                 // reshape flat array to matrix
flatMap f xs                    // concat (map f xs)

// List manipulation
append xs x                     // xs ++ [x]
prepend x xs                    // [x] ++ xs  
concat xs ys                    // xs ++ ys
update xs i x                   // xs with xs[i] = x
removeAt xs i                   // xs without xs[i]
slice xs start end              // xs[start:end]
length xs                       // |xs|

// Utilities
chunk xs n                      // split xs into chunks of size n
range start end                 // [start..end]
repeat n x                      // [x,x,...] (n times)
sort.by xs f                    // sort xs by key function f
group.by xs f                   // group xs by key function f

TABLE OPERATIONS
----------------
set tbl k v                     // tbl with tbl[k] = v
remove tbl k                    // tbl without tbl[k]
merge tbl1 tbl2                 // tbl1 ∪ tbl2
keys tbl                        // [k | k &amp;lt;- tbl]
values tbl                      // [tbl[k] | k &amp;lt;- tbl]
shape x                         // metadata: kind, rank, shape, size

STRING OPERATIONS
-----------------
str.concat s1 s2 ...            // s1 + s2 + ...
str.split s delim               // split s by delim
str.join xs delim               // join xs with delim
str.length s                    // |s|
str.substring s start end       // s[start:end]
str.replace s old new           // replace old with new in s
str.trim s                      // strip whitespace
str.upper s                     // uppercase
str.lower s                     // lowercase
text.lines s                    // split by newlines
text.words s                    // split by whitespace

MATH OPERATIONS
---------------
// Arithmetic
math.abs x                      // |x|
math.sign x                     // -1, 0, or 1
math.min x y, math.max x y      // min/max
math.clamp x lo hi              // clamp x to [lo,hi]

// Rounding  
math.floor x, math.ceil x       // ⌊x⌋, ⌈x⌉
math.round x, math.trunc x      // round, truncate

// Powers &amp;amp; logs
math.pow x y                    // x^y
math.sqrt x                     // √x
math.exp x, math.log x          // e^x, ln(x)

// Trigonometry
math.sin x, math.cos x, math.tan x
math.asin x, math.acos x, math.atan x, math.atan2 y x
math.deg x, math.rad x          // degrees ↔ radians

// Random
math.random                     // [0,1)
math.randomInt lo hi            // [lo,hi]

FUNCTION COMBINATORS
--------------------
flip f                          // λx y. f y x
apply f x                       // f x  
pipe x f                        // f x (reverse apply)
compose f g                     // λx. f (g x) (binary compose)

VALIDATION &amp;amp; DEBUG
------------------
// Validation
validate.notEmpty x             // x is not empty
validate.range lo hi x          // lo ≤ x ≤ hi  
validate.type "Type" x          // x has type Type
validate.email x                // x is valid email

// Debugging
debug.print [name] value        // print with optional name
debug.inspect x                 // detailed inspection
assert condition message        // throw if condition false

I/O
---
io.out value                    // print value
io.in                           // read stdin

JAVASCRIPT INTEROP
------------------
io.callJS fnName args           // call JS function synchronously
io.callJSAsync fnName args      // call JS function asynchronously
io.getProperty obj propName     // get JS object property
io.setProperty obj propName val // set JS object property
io.hasProperty obj propName     // check if JS property exists
io.jsArrayToList jsArray        // convert JS array to Baba Yaga list
io.listToJSArray list           // convert Baba Yaga list to JS array
io.objectToTable jsObj          // convert JS object to Baba Yaga table
io.tableToObject table          // convert Baba Yaga table to JS object
io.getLastJSError               // get last JS error (if available)
io.clearJSError                 // clear last JS error (if available)

EXAMPLES
--------
// Fibonacci
fib : n -&amp;gt; when n is 0 then 0 1 then 1 _ then (fib (n-1)) + (fib (n-2));

// Array processing pipeline
process : xs -&amp;gt;
  with (
    filtered : filter (x -&amp;gt; (x % 2) = 0) xs;
    doubled : map (x -&amp;gt; x * 2) filtered;
    summed : reduce (acc x -&amp;gt; acc + x) 0 doubled;
  ) -&amp;gt; summed;

// Result handling
safeDivide : x y -&amp;gt; when y is 0 then Err "div by zero" _ then Ok (x / y);
use : r -&amp;gt; when r is Ok v then v Err _ then 0;

// Pattern matching with guards
classify : x -&amp;gt; when x is
  n if ((n &amp;gt; 0) and (n &amp;lt; 10)) then "small positive"
  n if (n &amp;gt;= 10) then "large positive"  
  n if (n &amp;lt; 0) then "negative"
  _ then "zero";

// Mutual recursion
evenOdd : n -&amp;gt; with rec (
  even : x -&amp;gt; when x is 0 then true _ then odd (x - 1);
  odd : x -&amp;gt; when x is 0 then false _ then even (x - 1);
) -&amp;gt; {even: even n, odd: odd n};

// Array programming
matrix : reshape [2,3] [1,2,3,4,5,6];   // [[1,2,3],[4,5,6]]
indices : where (x -&amp;gt; x &amp;gt; 3) [1,2,3,4,5]; // [3,4] 
selected : at indices [10,20,30,40,50];   // [40,50]&lt;/code&gt;
    &lt;p&gt;Published&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eli.li/to-the-surprise-of-literally-no-one-im-working-on-implementing-a-programming-language-all-my-own"/></entry><entry><id>https://news.ycombinator.com/item?id=45063559</id><title>Grok Code Fast 1</title><updated>2025-08-30T07:32:05.265915+00:00</updated><content/><link href="https://x.ai/news/grok-code-fast-1"/></entry><entry><id>https://news.ycombinator.com/item?id=45064329</id><title>Deploying DeepSeek on 96 H100 GPUs</title><updated>2025-08-30T07:32:04.954960+00:00</updated><content>&lt;doc fingerprint="ec5d845b04b8c994"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deploying DeepSeek with PD Disaggregation and Large-Scale Expert Parallelism on 96 H100 GPUs&lt;/head&gt;
    &lt;p&gt;by: The SGLang Team, May 05, 2025&lt;/p&gt;
    &lt;p&gt;DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which uses Multi-head Latent Attention (MLA) and Mixture of Experts (MoE), require an advanced system for efficient serving at scale. In this blog, we explain how we match DeepSeek's inference system performance with SGLang.&lt;/p&gt;
    &lt;p&gt;Our implementation, shown in the figure above, runs on 12 nodes in the Atlas Cloud, each equipped with 8 H100 GPUs. It uses prefill-decode disaggregation and large-scale expert parallelism (EP), achieving a speed of 52.3k input tokens per second and 22.3k output tokens per second per node for 2000-token input sequences. To the best of our knowledge, this represents the first open-source implementation to nearly match the throughput reported in the official DeepSeek blog at large scale. By deploying this implementation locally, it translates to a cost of $0.20/1M output tokens, which is about one-fifth the cost of the official DeepSeek Chat API. Compared to vanilla tensor parallelism using the same resources, this optimized strategy improves the output throuhgput by up to 5x. This blog dives into our parallelism design, optimization methods, and results. All components of our work are fully open-source, allowing others to explore and build on our efforts. The instructions for reproducing our experiments are fully available here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlight&lt;/head&gt;
    &lt;p&gt;✅ SGLang now supports prefill-decode (PD) disaggregation and large-scale EP, including the full functionality of DeepEP, DeepGEMM, and EPLB.&lt;/p&gt;
    &lt;p&gt;✅ Leveraging these new features, our team successfully replicated DeepSeek's inference system using 12 nodes, each with 8 H100 GPUs. In total, SGLang achieves a throughput of 52.3k input tokens per second and 22.3k output tokens per second per node for input sequences of 2000 tokens.&lt;/p&gt;
    &lt;p&gt;✅ This blog explains technical details of our approach, focusing on optimizations for efficiency, peak memory usage reduction, and workload balancing. The profile results show that our implementation achieves nearly on-par performance with the official DeepSeek’s report.&lt;/p&gt;
    &lt;p&gt;✅ All experiments and code are fully open-sourced for community access and further development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parallelism Design&lt;/item&gt;
      &lt;item&gt;Prefill and Decode Disaggregation&lt;/item&gt;
      &lt;item&gt;Large-scale Expert Parallelism&lt;/item&gt;
      &lt;item&gt;Evaluation&lt;/item&gt;
      &lt;item&gt;Toolkits&lt;/item&gt;
      &lt;item&gt;Limitations and Future Work&lt;/item&gt;
      &lt;item&gt;Conclusion&lt;/item&gt;
      &lt;item&gt;Acknowledgment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Parallelism Design&lt;/head&gt;
    &lt;p&gt;Efficient parallelism is essential to manage the computational complexity and memory demands of DeepSeek's architecture. This section outlines our approach to optimizing key components: attention layers, dense feed-forward networks (FFNs), sparse FFNs, and the language model (LM) head. Each component leverages tailored parallelism strategies to enhance scalability, memory efficiency, and performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Attention Layers&lt;/head&gt;
    &lt;p&gt;DeepSeek employs Multi-head Latent Attention (MLA) to effectively model complex dependencies within input sequences. To optimize this mechanism, we implement DP Attention, a data parallelism strategy that eliminates KV cache duplication across devices, significantly reducing memory overhead. Introduced in SGLang v0.4, this approach has been extended to support hybrid data and tensor parallelism, offering flexibility for processing small batch sizes efficiently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dense FFNs&lt;/head&gt;
    &lt;p&gt;Despite using only three dense FFN layers, DeepSeek-V3's computation can significantly increase peak memory usage, potentially leading to system crashes if not carefully managed. To address this, we adopt Data Parallelism (DP) over tensor parallelism (TP), leveraging the following advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enhanced Scalability: With an intermediate dimension of 18,432, high TP degrees (e.g., TP32) result in inefficient fragmentation into small-unit segments (e.g., 576 units), which are not divisible by 128—a common alignment boundary for modern GPUs such as H100. This misalignment hampers computational efficiency and memory utilization. DP provides a more scalable solution by avoiding fragmentation, ensuring balanced workload distribution across devices.&lt;/item&gt;
      &lt;item&gt;Optimized Memory Efficiency: Traditionally, TP reduces memory usage as worker size increases, but this advantage diminishes under DP attention. In a pure TP setup, memory demand for a single-layer Transformer model scales with DP size as: $$\text{Memory}=\frac{N_{\text{param}}}{\text{TP}}+(1+k)N_{\text{hidden_state}}\cdot \text{DP}\notag$$ Here, $N_{\text{hidden_state}}=n_\text{token}\times n_\text{hidden_size}$ is the size of the hidden state on each device (DP rank), $N_{\text{param}}=n_\text{intermediate_size}\times n_\text{hidden_size}$ is the number of model parameters, and $k$ is a coefficient representing extra memory overhead from CUDA Graph duplication. By assuming $\text{DP}=\text{TP}$, this memory usage function is minimized when $\text{TP}=\sqrt{\frac{N_{\text{param}}}{(1+k)N_{\text{hidden_state}}}}$. DeepSeek-V3 uses an intermediate size of 18,432. During the prefill phase, CUDA Graph is typically disabled, so $k = 0$. However, the token size per device can easily exceed 2,048, resulting in an optimal TP size of 3 or less. In the decode phase, a practical configuration might use 128 tokens per device and set $k = 3$. In this case, the memory-optimal TP size is 6. In both phases, a lower TP degree minimizes memory usage per device. As a result, DP may offer a more memory-efficient approach for scaling compared to relying solely on TP.&lt;/item&gt;
      &lt;item&gt;Minimized Communication Overhead: In pure TP, each FFN necessitates two all-reduce operations, resulting in substantial communication overhead. By leveraging DP, we optimize this process to a single reduce-scatter following the prior attention layer and an all-gather before the next, reducing communication costs by 50%. Furthermore, when attention is also computed under pure DP, inter-device communication is entirely eliminated, significantly enhancing overall efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The integration of DP dense FFN with DP attention is illustrated in the left figure below. Users can enable this feature by setting &lt;code&gt;--moe-dense-tp-size=1&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sparse FFNs&lt;/head&gt;
    &lt;p&gt;In DeepSeek-V3's Mixture of Experts (MoE) architecture, sparse FFNs require substantial expert weights, creating a significant memory bottleneck. To address this, we implement Expert Parallelism (EP), which distributes expert weights across multiple devices. This approach effectively scales memory capacity while maintaining high performance, though it does introduce challenges like irregular all-to-all communication and workload imbalance.&lt;/p&gt;
    &lt;p&gt;The figure in the right figure above illustrates our EP implementation using the DeepEP framework, with further details on our EP design and optimizations provided in the following sections.&lt;/p&gt;
    &lt;head rend="h3"&gt;LM Head&lt;/head&gt;
    &lt;p&gt;The LM head computes output probabilities over a large vocabulary, a resource-intensive operation traditionally handled with vocabulary parallelism to aggregate token logits from TP groups. To enhance scalability and efficiency, we adopt Data Parallelism (DP), mirroring our dense FFN strategy. This reduces memory overhead and simplifies communication across devices, delivering a more streamlined solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prefill and Decode Disaggregation&lt;/head&gt;
    &lt;p&gt;LLM inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce Prefill and Decode (PD) Disaggregation in SGLang.&lt;/p&gt;
    &lt;head rend="h3"&gt;Issues with Unified Scheduling&lt;/head&gt;
    &lt;p&gt;The conventional unified engine, which processes prefill and decode batches together, results in three significant problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prefill Interruption: Incoming prefill batches frequently interrupt ongoing decode batches, causing substantial delays in token generation.&lt;/item&gt;
      &lt;item&gt;DP Attention Imbalance: In DP attention, one DP worker may process a prefill batch while another handles a decode batch simultaneously, leading to increased decode latency.&lt;/item&gt;
      &lt;item&gt;Incompatible with DeepEP: As we will discuss in a later section, DeepEP executes different dispatch modes for prefill and decode, making unified scheduling imcompatible with DeepEP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PD Disaggregation resolves these by separating the two stages, enabling tailored optimizations for each.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation Details&lt;/head&gt;
    &lt;p&gt;The PD Disaggregation design in SGLang, depicted in the diagram below, interleaves execution between a Prefill Server and a Decode Server:&lt;/p&gt;
    &lt;p&gt;Upon receiving an input request, the workflow proceeds as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A Prefill Server and a Decode Server pair via a handshake, establishing a local sender and receiver, respectively.&lt;/item&gt;
      &lt;item&gt;The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin the model forward pass and compute the KV caches.&lt;/item&gt;
      &lt;item&gt;Once computed, the data transfers to the Decode Server, which handles iterative token generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This separation ensures each phase operates under optimal conditions, maximizing GPU resource utilization. To further enhance performance, our implementation incorporates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-blocking Transfer: Data send and receive operations run in a background thread, keeping the scheduler’s event loop uninterrupted.&lt;/item&gt;
      &lt;item&gt;RDMA-Based Transfer: Remote Direct Memory Access (RDMA) leverages queue pairs for connections and scatter-gather elements (SGE) for efficient transfer of non-contiguous memory chunks.&lt;/item&gt;
      &lt;item&gt;Flexible API Integration: SGLang offers adaptable APIs that integrate high-performance RDMA libraries like Mooncake and NIXL, streamlining data transfers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details can be found in our design document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Large-scale Expert Parallelism&lt;/head&gt;
    &lt;head rend="h3"&gt;Expert Parallelism with DeepEP&lt;/head&gt;
    &lt;p&gt;DeepEP, implemented by the DeepSeek team, is a communication library designed to streamline EP in MoE models. It tackles the challenge of efficiently routing tokens to specific experts across multiple GPUs. By providing optimized communication kernels, DeepEP reduces latency and boosts throughput, making it ideal for large-scale inference tasks.&lt;/p&gt;
    &lt;p&gt;DeepEP provides two specialized dispatch modes to address varying workload demands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Normal Dispatch: Optimized for handling long input sequences, such as during the prefill phase, this mode prioritizes maximum computational throughput. However, it generates symbolic shapes that are incompatible with CUDA Graph, rendering it less effective for the decode phase, where kernel launch overhead becomes a significant bottleneck.&lt;/item&gt;
      &lt;item&gt;Low-Latency Dispatch: Tailored for generating output tokens during the decode phase, this mode prioritizes minimal delay to ensure real-time performance. It supports CUDA Graph but requires preallocating a fixed memory size. If the memory demand exceeds this preallocation, a runtime error occurs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In SGLang, the integration of DeepEP provides auto mode that dynamically selects between these two dispatch modes based on the workload. However, without PD disaggregation, the auto mode faces a limitation: it cannot simultaneously support both normal dispatch (for prefill) and low-latency dispatch (for decode) within the same communication group. This restriction hinders its compatibility with DP attention, which is crucial for memory-efficient inference. The compatibility of each mode is outlined in the table below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Long Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Long Output&lt;/cell&gt;
        &lt;cell role="head"&gt;DP Attention&lt;/cell&gt;
        &lt;cell role="head"&gt;CUDA Graph&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Normal&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Low-Latency&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Auto&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;PD disaggregation addresses this by separating prefill and decode phases, allowing normal dispatch for the prefill phase and low-latency dispatch for the decode phase, both under DP attention. This integration optimizes resource utilization and enhances overall performance by aligning the dispatch mode with the specific needs of each phase.&lt;/p&gt;
    &lt;head rend="h3"&gt;DeepGEMM Integration&lt;/head&gt;
    &lt;p&gt;DeepGEMM is another high-efficient library developed by the DeepSeek team, specifically designed to optimize computations in MoE models. It provides two specialized functions for handling MoE-related matrix multiplications (Grouped GEMMs), each tailored to different phases of the inference process.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grouped GEMMs (contiguous layout): This kernel is designed for dynamic input shapes, making it ideal for the prefill phase of MoE inference. It processes inputs where the data for different experts is concatenated contiguously, allowing for flexible handling of varying input sizes.&lt;/item&gt;
      &lt;item&gt;Grouped GEMMs (masked layout): This kernel assumes a fixed input shape and uses a mask tensor to compute only the valid portions of the input. It is compatible with CUDA Graph, which optimizes kernel launches, making it well-suited for the decode phase where reducing overhead is critical.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DeepGEMM integrates smoothly with the dispatch modes of DeepEP:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For the contiguous layout kernel, which is used with normal dispatch in the prefill phase, an additional step is required. Since normal dispatch outputs a symbolic shape, a permutation is needed to transform the output into the contiguous format expected by the kernel. We referred to the LightLLM project and implemented a custom Triton kernel for efficient permutation. This kernel ensures that the output from normal dispatch is correctly rearranged, enabling smooth integration with the contiguous GEMM kernel.&lt;/item&gt;
      &lt;item&gt;The masked layout kernel pairs seamlessly with DeepEP’s low-latency dispatch, as both are optimized for the decode phase and support CUDA Graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SGLang also integrates DeepGEMM for MoE computation under tensor parallelism. Additionally, DeepGEMM provides a highly efficient general GeMM kernel, which can be activated in SGLang by setting the environment variable &lt;code&gt;SGL_ENABLE_JIT_DEEPGEMM&lt;/code&gt; to 1, offering even greater computational efficiency for non-MoE operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Two-batch Overlap&lt;/head&gt;
    &lt;p&gt;In multi-node environments, limited communication bandwidth can significantly increase overall latency. To tackle this challenge, we implemented Two-batch Overlap (TBO) following DeepSeek's system design. TBO splits a single batch into two micro-batches, allowing computation and communication to overlap, which also lowers peak memory usage by halving the effective batch size. However, putting TBO into practice introduces specific implementation difficulties.&lt;/p&gt;
    &lt;head rend="h5"&gt;Implementation Challenges&lt;/head&gt;
    &lt;p&gt;Although DeepSeek released the design framework of TBO, there are two slight implementation challenges.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code Complexity: Directly coding TBO can lead to duplicated logic for managing multiple micro-batches. This increases the complexity of the codebase, making it harder to maintain and prone to errors, especially as the number of micro-batches or overlapping scenarios grows.&lt;/item&gt;
      &lt;item&gt;Synchronization Issues in the Prefill Phase: Achieving effective overlap between computation and communication needs consideration when the normal dispatch in DeepEP block the CPU. This blocking behavior can stall the pipeline, leaving the GPU idle and undermining the performance benefits of TBO.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Abstraction for Clean Implementation&lt;/head&gt;
    &lt;p&gt;To create a more maintainable and reusable codebase, we use an abstraction layer consisting of operations and yield points. This method simplifies development by allowing us to write code as if handling a single micro-batch, while strategically pausing execution by inserting yield points to let other micro-batches proceed. It eliminates code duplication, reduces the potential need for variable postfixes, and efficiently manages cases where some executions complete at a layer's end while others have not. Additionally, it supports easy adaptation to different overlapping region choices or future enhancements, like a three-batch overlap, with minimal code changes. Below is a concise demonstration of this approach:&lt;/p&gt;
    &lt;code&gt;operations = [
    self._forward_attn,
    YieldOperation(),  # Pause execution for other micro-batches
    self._forward_dispatch,
    self._forward_mlp,
    YieldOperation(),  # Another pause point
    self._forward_combine,
]

# Process a single micro-batch without duplicating code
def _forward_attn(self, state):
    state.hidden_states = self.self_attn(state.hidden_states, ...)
&lt;/code&gt;
    &lt;head rend="h5"&gt;Prefill Overlapping Implementation&lt;/head&gt;
    &lt;p&gt;We refine the launch order during the prefill phase to avoid CPU-blocking via the dispatch operation in DeepEP, even though we are using its asynchronous mode. Specifically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The dispatch operation blocks the CPU until the GPU receives metadata from other ranks to allocate correctly sized tensors.&lt;/item&gt;
      &lt;item&gt;An improper implementation would leave the computation stream idle during this period, as no computation tasks are submitted to the GPU.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To optimize, we prioritize submitting computation tasks to the GPU before launching CPU-blocking communication. This ensures the GPU remains active during communication. As illustrated in the figure below, TBO with a proper launch order, indicated by bolded borders, avoids bubble caused by a CPU-blocking operation (i.e., normal dispatch).&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert Parallelism Load Balancer&lt;/head&gt;
    &lt;p&gt;In MoE models, EP often leads to uneven workload distribution across GPUs. This imbalance forces the system to wait for the slowest GPU computation or communication, wasting compute cycles and increasing memory usage due to expert activations. As the number of GPUs (EP size) increases, the imbalance issue gets more severe.&lt;/p&gt;
    &lt;p&gt;To address this, DeepSeek developed the Expert Parallelism Load Balancer (EPLB). EPLB takes expert distribution statistics as input and computes an optimal arrangement of experts to minimize imbalance. Users can allocate redundant experts (e.g., 32 additional experts), which, when combined with the original 256, create a pool of 288 experts. This pool allows EPLB to strategically place or replicate experts—for instance, duplicating the most frequently used expert multiple times or grouping a moderately used expert with rarely used ones on a single GPU.&lt;/p&gt;
    &lt;p&gt;Beyond balancing workloads, EPLB offers greater flexibility in parallelism design. With the original 256 experts, parallelism sizes are restricted to powers of two. EPLB’s use of 288 experts enables more diverse configurations, such as parallelism sizes of 12 or 72.&lt;/p&gt;
    &lt;p&gt;In the figure below, we demonstrate the effects of scale and EPLB algorithm to the imbalance issue via simulation. We compute GPU balancedness as the ratio between mean computation time and maximum computation time for a MoE layer among GPUs, and we use the number of tokens for a GPU to estimate the computation time for it. As can be seen, utilization rate decreases when the system scales with the number of nodes, and enabling EPLB significantly improves the utilization.&lt;/p&gt;
    &lt;head rend="h5"&gt;EPLB for Real-World Serving&lt;/head&gt;
    &lt;p&gt;For EPLB to be effective, the input distribution must closely match the actual serving workload. Two strategies enhance this alignment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Increasing Batch Size: Larger batches reduce random fluctuations in expert usage, which improves balance, which can be achieved by scaling the cluster or using techniques like Multi-Token Prediction (MTP).&lt;/item&gt;
      &lt;item&gt;Periodic Rebalancing: Regularly updating the expert arrangement leverages temporal locality but requires efficient reloading of experts. This necessitates minimizing the cost of expert reloading operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with EPLB, some imbalance is inevitable, making further optimization a valuable future direction.&lt;/p&gt;
    &lt;head rend="h5"&gt;Implementation of Rebalancing&lt;/head&gt;
    &lt;p&gt;SGLang implements expert rebalancing in three stages to ensure efficiency and minimal disruption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;System Loading Stage: Weights are optionally preloaded from disk to main memory for faster rebalancing or kept on disk with memory mapping (mmap) for reduced memory usage.&lt;/item&gt;
      &lt;item&gt;Rebalance Preparation Stage: Required weights are asynchronously transferred to device memory in the background, utilizing free DMA hardware engines without interrupting ongoing GPU operations.&lt;/item&gt;
      &lt;item&gt;Rebalance Execution Stage: A device-to-device copy updates the weights. This step can be further optimized through physical memory rebinding techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This staged approach ensures that rebalancing is both efficient and non-disruptive, maintaining system performance during updates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluation&lt;/head&gt;
    &lt;head rend="h3"&gt;End-to-end Performance&lt;/head&gt;
    &lt;head rend="h5"&gt;Experimental Setup&lt;/head&gt;
    &lt;p&gt;We evaluated the end-to-end performance of different configurations of SGLang using DeepSeek-V3 on a cluster of 12 nodes, connected via InfiniBand and each equipped with 8 H100 GPUs. This evaluation highlights the throughput improvements enabled by our advanced optimization techniques. We compared the following four settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SGLang with TP16 x 6: Every two nodes are paired with an independent group, running DeepSeek-V3 inference with a TP size of 16 and DP attention.&lt;/item&gt;
      &lt;item&gt;SGLang with PD Disaggregation: This version incorporates PD disaggregation and full EP optimization. For the EPLB, we adopt a distribution matching the input/output data, as real-time serving statistics are unavailable.&lt;/item&gt;
      &lt;item&gt;SGLang with PD Disaggregation and simulated MTP: To simulate MTP’s effects, we firstly double the batch size and halve the Key-Value KV cache length to maintain the same workload for GroupedGeMM computation and memory access. Moreover, we insert dummy kernels after the real attention computation to ensure the attention phase takes the same time as in DeepSeek’s profile, accurately reflecting the slowdown caused by MTP’s attention mechanism. We conservatively assume a 70% acceptance rate under MTP.&lt;/item&gt;
      &lt;item&gt;DeepSeek Profile Results: Throughput estimates are derived from DeepSeek’s official profiling data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Performance Analysis of Prefill and Decode Phases&lt;/head&gt;
    &lt;p&gt;To accommodate varying workload demands, we independently evaluated the prefill (P) and decode (D) phases, assuming unlimited resources for the non-tested phase to isolate and maximize the load on the tested nodes—mirroring the setup used by DeepSeek. The results are summarized below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prefill Phase: On 4 nodes (4×8×H100, EP32), the system achieved per-node throughputs of 57,674, 54,543, and 50,302 tokens per second for prompt lengths of 1K, 2K, and 4K, respectively. As shown in the bar chart below, this represents up to a 3.3× improvement over the TP16 baseline, largely attributable to the optimized GroupedGeMM kernel (DeepGEMM) and two-batch overlap. Assuming a perfectly balanced workload, our system’s throughput is within 5.6% of DeepSeek's official profile.&lt;/item&gt;
      &lt;item&gt;Decode Phase: Evaluated on 9 nodes (9×8×H100, EP72; half the scale of DeepSeek), the system achieved 22,282 tokens/sec per node for 2K inputs—representing a 5.2× speedup over the TP16 baseline. Under simulated MTP conditions—with attention kernels intentionally slowed to reflect real-world latency—the system sustained a high throughput of 17,373 tokens/sec per node for 4K inputs, just 6.6% below DeepSeek’s official profile. As shown in the figure on the right, these performance gains are largely attributed to 4× larger batch sizes enabled by EP, which enhances scalability by significantly reducing per-GPU memory consumption of model weights.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Profile Results&lt;/head&gt;
    &lt;p&gt;This section compares SGLang’s performance with DeepSeek’s inference system, aligning our experimental setup as closely as possible to DeepSeek’s production environment. We analyze overall throughput and detailed kernel breakdowns, benchmarking against DeepSeek’s blog and public profile data.&lt;/p&gt;
    &lt;head rend="h5"&gt;Overall Throughput&lt;/head&gt;
    &lt;p&gt;For prefill, we tested a scenario with 16,384 tokens per device and an input length of 4,096. Due to uncertainty in DeepSeek’s expert distribution, we evaluated two cases: one with default expert distribution and another with simulated perfect EPLB (random expert selection following group-limited routing semantics) as a performance upper bound.&lt;/p&gt;
    &lt;p&gt;The results are presented below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;DeepSeek Blog (excl. cache hit)&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek Profile&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang (Default)&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang + Simulated Perfect EPLB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
        &lt;cell&gt;16,384&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Input Length&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput (per node)&lt;/cell&gt;
        &lt;cell&gt;32,206&lt;/cell&gt;
        &lt;cell&gt;62,713&lt;/cell&gt;
        &lt;cell&gt;50,302&lt;/cell&gt;
        &lt;cell&gt;59,337&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;DeepSeek’s profile reports a throughput roughly twice that of its production environment. SGLang with default expert imbalance is 20% slower than DeepSeek’s profile, while the simulated perfect EPLB case narrows the gap to 6%.&lt;/p&gt;
    &lt;p&gt;For decode, the results are shown below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;DeepSeek Blog&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek Profile&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang (Default)&lt;/cell&gt;
        &lt;cell role="head"&gt;SGLang + Simulated MTP (Slow Attention)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
        &lt;cell&gt;256&lt;/cell&gt;
        &lt;cell&gt;128&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;KV Cache Length&lt;/cell&gt;
        &lt;cell&gt;4,989&lt;/cell&gt;
        &lt;cell&gt;4,096&lt;/cell&gt;
        &lt;cell&gt;2,000&lt;/cell&gt;
        &lt;cell&gt;4,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Number of Nodes&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput (per node)&lt;/cell&gt;
        &lt;cell&gt;14,800&lt;/cell&gt;
        &lt;cell&gt;18,598&lt;/cell&gt;
        &lt;cell&gt;22,282&lt;/cell&gt;
        &lt;cell&gt;17,373&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Using half the nodes of DeepSeek, SGLang with simulated MTP is only slightly slower than DeepSeek’s profile. In a higher batch size setting (256 sequences, 2,000 input length), SGLang achieves 22,282 tokens per second per node, demonstrating strong scalability.&lt;/p&gt;
    &lt;head rend="h5"&gt;Detail Breakdown&lt;/head&gt;
    &lt;p&gt;The figure below breaks down kernel execution times for prefill, including unit test results as a theoretical upper bound:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default EPLB: Communication kernels exhibit longer execution times and higher variance compared to DeepSeek’s profile, likely due to greater expert imbalance. This leads to extended computation stream bubbles, slowing down overall performance.&lt;/item&gt;
      &lt;item&gt;Simulated Perfect EPLB: This setup aligns more closely with DeepSeek’s profile, though discrepancies remain, indicating potential areas for optimization.&lt;/item&gt;
      &lt;item&gt;Comparison with Unit Tests: Both DeepSeek and SGLang have a communication time slower than unit test results, while the latter is achievable when disabling TBO, revealing a potential optimization direction if communication is the bottleneck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SGLang’s decode kernel breakdown aligns closely with DeepSeek’s, as shown below:&lt;/p&gt;
    &lt;p&gt;Key observations include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Combine Time Discrepancy: SGLang’s combine operation appears 2x slower than DeepSeek’s due to shorter attention computation, causing communication kernels to busy-wait. In the simulated slow attention experiment, combine time matches DeepSeek’s, confirming this hypothesis.&lt;/item&gt;
      &lt;item&gt;MoE Performance: SGLang’s MoE kernels are 25% slower, possibly because DeepSeek’s 18 nodes (versus our 9) distribute experts more efficiently, reducing memory access overhead for GEMM operations.&lt;/item&gt;
      &lt;item&gt;Dispatch Optimization Potential: Both DeepSeek and SGLang show dispatch times of ~0.17ms per layer, but unit tests with DeepEP reveal a potential of 0.06ms occupying SMs. Currently, dispatch spends significant time busy-waiting for data. Inserting slow dummy kernels between send/receive operations reduces dispatch time to 0.09ms, and in-flight duration analysis using unit test data suggests further improvements are possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While minor enhancements remain—primarily in kernel fusion under "Other Kernels"—SGLang’s decode performance is largely aligned with DeepSeek’s, with prefill optimization as the next focus.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ablation Study: Two-batch Overlap&lt;/head&gt;
    &lt;head rend="h5"&gt;Impact of Batch Size and Attention Time&lt;/head&gt;
    &lt;p&gt;This section investigates TBO performance across varying batch sizes and simulated MTP scenarios.&lt;/p&gt;
    &lt;p&gt;TBO delivers two significant benefits in the prefill phase, as evidenced by throughput comparisons and memory usage optimizations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for Larger Batch Sizes: In the vanilla configuration, each device processes up to 8,192 tokens before encountering out-of-memory (OOM) errors at 16,384 tokens. TBO mitigates this by optimizing memory usage for input tokens, enabling inference with batches as large as 16,384 tokens per device. This further boosts performance to 40.5% increase when comparing the TBO flag with all other configurations made optimal.&lt;/item&gt;
      &lt;item&gt;Enhanced Throughput: By overlapping computation (e.g., attention and MLP phases) with communication (e.g., DeepEP Combine and Dispatch), TBO achieves a 27% to 35% throughput increase compared to the vanilla setup, even when processing the same token count per device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TBO’s impact in the decode phase varies by scenario, with performance tied to batch size and attention processing time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real Test Cases: Speedup in practical scenarios is contingent on batch size exceeding a threshold between 64 and 128 tokens. Below this, TBO yields minimal or negative gains (e.g., -27% at 32 tokens/device), as small decode batch sizes hinder kernel efficiency. The speedup reaches 25.5% at 256 tokens with a performance of 22,310 tokens per second.&lt;/item&gt;
      &lt;item&gt;Simulated MTP Scenario: TBO provides the most substantial speedup in simulated MTP cases when processing 128 requests to generate 256 tokens per decode step. This is due to prolonged attention processing time, which aligns computation (e.g., DP Attention layers) with DeepEP communication overhead (e.g., combine and dispatch steps). The evaluation shows a 35% speedup at 128 sequences/device, with throughput 17,552 tokens per second compared to 12,929 without TBO.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Detail Breakdown&lt;/head&gt;
    &lt;p&gt;We evaluated three prefill scenarios: TBO with 16k tokens per batch, TBO with 8k tokens, and no-TBO with 8k tokens. The figure below reveals key insights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TBO Efficiency: Comparing the 8k cases, TBO improves overall efficiency by overlapping computation and communication, as expected.&lt;/item&gt;
      &lt;item&gt;Batch Size Impact: Reducing the batch size from 16k to 8k with TBO results in a slight slowdown, reflecting diminished kernel efficiency with smaller batches.&lt;/item&gt;
      &lt;item&gt;Kernel Performance: Interestingly, the no-TBO 8k case outperforms the TBO 16k case in per-kernel speed, despite both having an effective batch size of 8k for kernels. This may stem from reduced streaming multiprocessors (SMs) with TBO, potential noisy neighbor effects during overlap, or kernel incompatibility between computation and communication. These findings suggest future optimization directions for SGLang.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the decode phase, we analyzed three configurations: TBO with a batch size of 256, no-TBO with 256, and no-TBO with 128. The time breakdown is shown below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TBO vs. No-TBO (Batch Size 256): Without TBO, communication time increases significantly due to the lack of overlap. However, computation kernels, particularly GEMM, benefit from a larger effective batch size, resulting in faster execution.&lt;/item&gt;
      &lt;item&gt;TBO (256) vs. No-TBO (128): Comparing cases with the same kernel batch size, only non-overlapped communication slows down in the no-TBO setup, while computation remains consistent. Unlike prefill, decode communication kernels either fully utilize SMs (during send/receive) or none (during inflight waiting), avoiding resource contention with computation kernels.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ablation Study: EPLB&lt;/head&gt;
    &lt;p&gt;This section evaluates the impact of the EPLB on system performance through overall throughput analysis and detailed case studies. Given EPLB's sensitivity to workload distribution and distribution shifts in production environments, we focus on qualitative and generalizable insights rather than real-world performance, which requires production data.&lt;/p&gt;
    &lt;head rend="h5"&gt;Overall Results&lt;/head&gt;
    &lt;p&gt;The figure below illustrates EPLB's effect on throughput in large-scale settings. EPLB delivers a significant speedup of 1.49x (prefill) and 2.54x (decode), as expected, due to its ability to mitigate workload imbalances across GPUs. As the number of ranks scales, imbalances grow, and EPLB effectively addresses this in our large-scale experiments, leading to notable throughput improvements.&lt;/p&gt;
    &lt;head rend="h5"&gt;Case Study: Workload Imbalance Versus Overall Throughput&lt;/head&gt;
    &lt;p&gt;To explore the relationship between workload imbalance and throughput, we conducted a case study using a decode experiment with 1800 input tokens, 100 output tokens, and a batch size of 256. Throughput and balancedness (average token count divided by maximum token count across experts) were plotted against decoding steps:&lt;/p&gt;
    &lt;p&gt;The results reveal a strong correlation between balancedness and throughput, emphasizing the importance of maintaining high balancedness for optimal performance.&lt;/p&gt;
    &lt;head rend="h5"&gt;Case Study: Expert Distribution Statistics&lt;/head&gt;
    &lt;p&gt;The following figure presents expert distribution statistics for prefill and decode sample data:&lt;/p&gt;
    &lt;p&gt;Key observations include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Imbalance in Expert Usage: Most experts are infrequently used, while a small subset is heavily utilized, underscoring the inherent imbalance in MoE models.&lt;/item&gt;
      &lt;item&gt;Prefill vs. Decode Differences: Although prefill and decode distributions share similarities, notable differences exist. This supports the use of PD disaggregation, which enables distinct expert placements for each phase, optimizing performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These findings highlight EPLB's role in addressing workload imbalances and the value of tailoring expert placement to phase-specific demands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Toolkits&lt;/head&gt;
    &lt;head rend="h3"&gt;Disposable Tensor&lt;/head&gt;
    &lt;p&gt;Memory management in PyTorch can be challenging due to persistent object references, especially in GPU-intensive workflows where CUDA memory is a scarce resource. Consider the following example:&lt;/p&gt;
    &lt;code&gt;def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    del hidden_state  # Attempt to free memory, but no effect due to external reference
    return linear2(nn.ReLU(intermediate_state))

hidden_state = ffn(hidden_state, linear1, linear2)
&lt;/code&gt;
    &lt;p&gt;In this code, &lt;code&gt;del hidden_state&lt;/code&gt; is intended to release the memory occupied by &lt;code&gt;hidden_state&lt;/code&gt; after &lt;code&gt;intermediate_state&lt;/code&gt; is computed. However, as &lt;code&gt;hidden_state&lt;/code&gt; is still referenced outside the function, the &lt;code&gt;del&lt;/code&gt; operation has no effect. This increases peak memory usage, risking performance slowdowns or out-of-memory errors.&lt;/p&gt;
    &lt;p&gt;SGLang addresses this with the DisposableTensor class, a subclass of &lt;code&gt;torch.Tensor&lt;/code&gt; which introduces a dispose() method to explicitly and immediately release a tensor’s memory, circumventing Python’s reference counting limitations. Here’s how it works:&lt;/p&gt;
    &lt;code&gt;def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    hidden_state.dispose()  # Immediately releases CUDA memory
    return linear2(nn.ReLU(intermediate_state))

# Wrap the tensor in DisposableTensor
hidden_state = DisposableTensor(hidden_state)
hidden_state = ffn(hidden_state, linear1, linear2)
&lt;/code&gt;
    &lt;p&gt;By wrapping &lt;code&gt;hidden_state&lt;/code&gt; in a &lt;code&gt;DisposableTensor&lt;/code&gt; and calling &lt;code&gt;dispose()&lt;/code&gt; when it’s no longer needed, the CUDA memory is freed right away. This ensures that memory is released as soon as the tensor’s role in the computation is complete, reducing peak memory usage and improving overall efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert Workload Extraction and Simulation&lt;/head&gt;
    &lt;p&gt;SGLang also includes a toolset for analyzing and simulating expert workload distribution in MoE models. This feature enables users to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dump Expert Workload Statistics: Extract either accumulated statistics or per-batch workload data. Accumulated stats support the EPLB manager for real-time optimization, while per-batch data provides granular insights for analysis and simulation.&lt;/item&gt;
      &lt;item&gt;Simulate Expert Utilization: Model expert balance across various configurations without requiring costly hardware or repeated trials. For instance, users can gather workload data from a modest setup (e.g., 2x8xH100 or 8xH200) and simulate the performance for a large-scale 22-node deployment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This simulation capability allows users to evaluate how factors like rebalancing frequency, node count, or batch size impact system performance. It’s a cost-effective way to fine-tune configurations before scaling up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations and Future Work&lt;/head&gt;
    &lt;p&gt;While our implementation of SGLang for DeepSeek-V3 inference demonstrates significant throughput improvements, several limitations and areas for future enhancement remain:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Latency Optimization: The current focus on throughput leaves Time to First Token (TTFT) at 2–5 seconds and Inter-Token Latency (ITL) at approximately 100ms, requiring further optimizations for real-time use cases.&lt;/item&gt;
      &lt;item&gt;Sequence Length Constraints: Limited to shorter sequences due to the use of 96 GPUs. Expanding GPU resources would support longer sequences, essential for specific applications.&lt;/item&gt;
      &lt;item&gt;Multi-Token Prediction (MTP) Integration: SGLang supports MTP but lacks full integration with DP attention, reducing efficiency in mixed parallelism configurations.&lt;/item&gt;
      &lt;item&gt;EPLB Distribution: The experiments in this blog utilizes in-distribution data for Expert Parallelism Load Balancer (EPLB), which may not reflect real-world variability. Future work should experiment performances when having distribution shifts.&lt;/item&gt;
      &lt;item&gt;Flexible Tensor Parallelism (TP) Sizes: For DeepSeek-V3, memory-optimal TP sizes for dense FFNs are small but larger than 1. Currently, SGLang only supports pure TP or DP, leading to suboptimal memory use. Flexible TP options are needed.&lt;/item&gt;
      &lt;item&gt;Blackwell Support: Currently, our implementation supports only the NVIDIA Hopper architecture. We are actively working to extend compatibility to the next-generation Blackwell architecture. If you are interested in supporting or sponsoring this development, welcome to contact lmsys.org@gmail.com.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;By leveraging PD disaggregation, EP, and a carefully crafted parallelism design, we’ve reproduced DeepSeek’s inference framework in SGLang with exceptional performance. Our open-source efforts—achieving 52.3k input tokens per second and 22.3k output tokens per second—demonstrate SGLang’s power for large-scale LLM inference. We invite the community to explore, replicate, and extend this work to push the boundaries of efficient AI deployment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgment&lt;/head&gt;
    &lt;p&gt;We would like to express our heartfelt gratitude to the following teams and collaborators:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SGLang Core Team and Community Contributors — Jingyi Chen, Cheng Wan, Liangsheng Yin, Baizhou Zhang, Ke Bao, Jiexin Liang, Xiaoyu Zhang, Yanbo Yang, Fan Yin, Chao Wang, Laixin Xie, Runkai Tao, Yuhong Guo, Kaihong Zhang, Lei Yu, Yu-Hsuan Tseng, Qilin Tian, Peng Zhang, Yi Zhang, Yineng Zhang, Byron Hsu, and many others.&lt;/item&gt;
      &lt;item&gt;Atlas Cloud Team — Jerry Tang, Wei Xu, Simon Xue, Harry He, Eva Ma, and colleagues — for providing a 96-device NVIDIA H100 cluster and offering responsive engineering support.&lt;/item&gt;
      &lt;item&gt;NVIDIA Solution Architect Team — Xuting Zhou, Jinyan Chen, and colleagues — for their work on the seamless integration of expert parallelism.&lt;/item&gt;
      &lt;item&gt;NVIDIA Enterprise Product Team — Trevor Morris, Elfie Guo, Kaixi Hou, Kushan Ahmadian, and colleagues — for optimizing the DeepSeek R1 kernels.&lt;/item&gt;
      &lt;item&gt;LinkedIn Team — Biao He, Qingquan Song, Chunan Zeng, Yun Dai, Yubo Wang, and colleagues — for optimizing the Flash-Attention 3 backend.&lt;/item&gt;
      &lt;item&gt;Mooncake Team — Shangming Cai, Teng Ma, Mingxing Zhang, and colleagues — for their collaboration on PD disaggregation in SGLang.&lt;/item&gt;
      &lt;item&gt;FlashInfer Team — Zihao Ye, Yong Wu, Yaxing Cai — for additional DeepSeek R1 kernel optimizations.&lt;/item&gt;
      &lt;item&gt;Dynamo Team - Kyle Kranen, Vikram Sharma Mailthody, and colleagues - for extra support on PD disaggregation in SGLang.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you all for your invaluable support and collaboration.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;Related PRs: #1970 #2925 #4068 #4165 #4232 #4390 #4435 #4521 #4654 #4767 #4770 #4836 #4880 #4957 #5068 #5085 #5295 #5415 #5432 #5435 #5530 #5558 #5561 #5626 #5657 #5805 #5819 #5890 DeepEP#142&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lmsys.org/blog/2025-05-05-large-scale-ep/"/></entry><entry><id>https://news.ycombinator.com/item?id=45065705</id><title>Essential Coding Theory [pdf]</title><updated>2025-08-30T07:32:04.534092+00:00</updated><content/><link href="https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf"/></entry><entry><id>https://news.ycombinator.com/item?id=45066060</id><title>Wikipedia as a Graph</title><updated>2025-08-30T07:32:04.403450+00:00</updated><content/><link href="https://wikigrapher.com/paths"/></entry><entry><id>https://news.ycombinator.com/item?id=45066258</id><title>The web does not need gatekeepers: Cloudflare’s new “signed agents” pitch</title><updated>2025-08-30T07:32:04.298645+00:00</updated><content/><link href="https://positiveblue.substack.com/p/the-web-does-not-need-gatekeepers"/></entry><entry><id>https://news.ycombinator.com/item?id=45066395</id><title>John Carmack's arguments against building a custom XR OS at Meta</title><updated>2025-08-30T07:32:03.949977+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/ID_AA_Carmack/status/1961172409920491849"/></entry><entry><id>https://news.ycombinator.com/item?id=45066999</id><title>SQLite's documentation about its durability properties is unclear</title><updated>2025-08-30T07:32:03.854118+00:00</updated><content>&lt;doc fingerprint="b0513adf2aa79b48"&gt;
  &lt;main&gt;
    &lt;p&gt;August 29, 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;SQLite's Durability Settings are a Mess&lt;/head&gt;
    &lt;p&gt;One of the most important properties of a database is durability. Durability means that after a transaction commits, you can be confident that, absent catastrophic hardware failure, the changes made by the commit won't be lost. This should remain true even if the operating system crashes or the system loses power soon after the commit. On Linux, and most other Unix operating systems, durability is ensured by calling the fsync system call at the right time.&lt;/p&gt;
    &lt;p&gt;Durability comes at a performance cost, and sometimes applications don't need durability. Some applications can tolerate losing the last several seconds of commits in the event of a power failure, as long as the database doesn't end up corrupted. Thus, databases typically provide knobs to configure if and when they call fsync. This is fine, but it's essential that the database clearly documents what its default durability properties are, and what each configuration setting guarantees.&lt;/p&gt;
    &lt;p&gt;Unfortunately, SQLite's documentation about its durability properties is far from clear. I cannot tell whether SQLite is durable by default, and if not, what are the minimal settings you need to use to ensure durability.&lt;/p&gt;
    &lt;p&gt;The two relevant configuration options are &lt;code&gt;journal_mode&lt;/code&gt; and &lt;code&gt;synchronous&lt;/code&gt;.  &lt;code&gt;journal_mode&lt;/code&gt; has several possible values, but most people use either DELETE or WAL.  &lt;code&gt;synchronous&lt;/code&gt; has four possible values: EXTRA, FULL, NORMAL, and OFF.
&lt;/p&gt;
    &lt;p&gt;This is how I interpret SQLite's documentation after a careful reading:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The default value of&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;is DELETE:&lt;quote&gt;The DELETE journaling mode is the normal behavior (source; archived)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The default value of&lt;/p&gt;&lt;code&gt;synchronous&lt;/code&gt;is FULL:&lt;quote&gt;If not overridden at compile-time, the default setting is 2 (FULL) (source; archived)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The default value of&lt;/p&gt;&lt;code&gt;synchronous&lt;/code&gt;is FULL even in WAL mode:&lt;quote&gt;If not overridden at compile-time, this value is the same as SQLITE_DEFAULT_SYNCHRONOUS. (source; archived)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;When&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;is DELETE, you need to set&lt;code&gt;synchronous&lt;/code&gt;to EXTRA to get durability:&lt;quote&gt;EXTRA synchronous is like FULL with the addition that the directory containing a rollback journal is synced after that journal is unlinked to commit a transaction in DELETE mode. EXTRA provides additional durability if the commit is followed closely by a power loss. (source; archived)&lt;/quote&gt;&lt;p&gt;Edited to add: I confirmed this to be true through testing - see my Hacker News comment for the methodology.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;When&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;is WAL, FULL is sufficient for durability:&lt;quote&gt;With synchronous=FULL in WAL mode, an additional sync operation of the WAL file happens after each transaction commit. The extra WAL sync following each transaction helps ensure that transactions are durable across a power loss (source; archived)&lt;/quote&gt;&lt;p&gt;Note that this is not mentioned under the definition of FULL, but rather further down in the documentation for&lt;/p&gt;&lt;code&gt;synchronous&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on the above, I conclude that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;By default, SQLite is not durable, because the default value of&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;is DELETE, and the default value of&lt;code&gt;synchronous&lt;/code&gt;is FULL, which doesn't provide durability in DELETE mode.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you change&lt;/p&gt;&lt;code&gt;journal_mode&lt;/code&gt;to WAL, then SQLite is durable, because&lt;code&gt;synchronous=FULL&lt;/code&gt;provides durability in WAL mode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, a recent Hacker News comment by a user who credibly claims to be Richard Hipp, the creator of SQLite, says:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;"In its default configuration, SQLite is durable."&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"If you switch to WAL mode, the default behavior is that transactions ... are not necessarily durable across OS crashes or power failures"&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's literally the opposite of what the documentation seems to say!&lt;/p&gt;
    &lt;p&gt;A Hacker News commenter who agrees with my reading of the documentation asked Hipp how his comment is consistent with the documentation, but received no reply.&lt;/p&gt;
    &lt;p&gt;Hipp also says that WAL mode used to be durable by default, but it was changed after people complained about poor performance. This surprised me, since I had the impression that SQLite cared deeply about backwards compatibility, and weakening the default durability setting is a nasty breaking change for any application which needs durability.&lt;/p&gt;
    &lt;p&gt;There are a couple other pitfalls around SQLite durability that you should be aware of, though I don't necessarily blame the SQLite project for these:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Libraries that wrap SQLite can override the default value of&lt;/p&gt;&lt;code&gt;synchronous&lt;/code&gt;. For example, the most popular Go driver for SQLite sets it to NORMAL when in WAL mode, which does not provide durability.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On macOS, fsync is nerfed to make macOS appear faster. If you want a real fsync, you have to make a different, macOS-specific system call. SQLite can do this, but it's off by default.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; My takeaway is that if you need durability, you'd better set the &lt;code&gt;synchronous&lt;/code&gt; option explicitly because who knows what the default is, or what it will be in the future.  With WAL mode, FULL seems to suffice.  As for DELETE mode, who knows if FULL is enough, so you'd better go with EXTRA to be safe. And if your application might be used on macOS, enable &lt;code&gt;fullfsync&lt;/code&gt;.
&lt;/p&gt;
    &lt;p&gt; The SQLite project ought to clarify their documentation. Since the meaning of &lt;code&gt;synchronous&lt;/code&gt; depends on the value of &lt;code&gt;journal_mode&lt;/code&gt;, I think it would be quite helpful to document the values of &lt;code&gt;synchronous&lt;/code&gt; separately for each possible &lt;code&gt;journal_mode&lt;/code&gt;, rather than mixing it all together.  A table with &lt;code&gt;synchronous&lt;/code&gt; values on one axis and &lt;code&gt;journal_mode&lt;/code&gt; on the other which tells you if the combination provides durability would do wonders.
&lt;/p&gt;
    &lt;p&gt;By the way, there are definitely many applications for which losing a few seconds of data in exchange for better performance is a great tradeoff, which is why SQLite and macOS have made the choices they have made. But programmers need to know what guarantees their tools provide, which is why unclear documentation and breaking previously-held assumptions is not cool.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments&lt;/head&gt;
    &lt;p&gt;No comments yet.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post a Comment&lt;/head&gt;
    &lt;p&gt;Your comment will be public. To contact me privately, email me. Please keep your comment polite, on-topic, and comprehensible. Your comment may be held for moderation before being published.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.agwa.name/blog/post/sqlite_durability"/></entry><entry><id>https://news.ycombinator.com/item?id=45067423</id><title>Income Equality in Nordic Countries: Myths, Facts, and Lessons</title><updated>2025-08-30T07:32:02.887818+00:00</updated><content>&lt;doc fingerprint="6f19898e131b06ca"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Journal of Economic Literature&lt;/head&gt;&lt;p&gt;ISSN 0022-0515 (Print) | ISSN 2328-8175 (Online)&lt;/p&gt;&lt;head rend="h1"&gt;Income Equality in the Nordic Countries: Myths, Facts, and Lessons&lt;/head&gt;&lt;p&gt;Journal of Economic Literature &lt;/p&gt;&lt;p&gt;(pp. 791–839)&lt;/p&gt;&lt;head rend="h4"&gt;Abstract&lt;/head&gt;Policymakers, public commentators, and researchers often cite the Nordic countries as examples of a socioeconomic model that combines low income inequality with prosperity and growth. This article critically assesses that claim by integrating theoretical perspectives and empirical evidence to explain how the Nordic model functions and why these countries experience low inequality. Our analysis suggests that income equality in the Nordics is largely driven by a significant compression of hourly wages, reducing returns to labor market skills and education. This appears to result from a wage bargaining system characterized by strong coordination within and across industries. This finding challenges other commonly cited explanations for Nordic income equality, such as redistribution through the tax transfer system, public spending on goods that complement employment, and public policies promoting equal skills and human capital. We consider broader lessons for economies aiming to reduce inequality and conclude by highlighting several under-explored or unresolved questions.&lt;head rend="h4"&gt;Citation&lt;/head&gt;Mogstad, Magne, Kjell G. Salvanes, and Gaute Torsvik. 2025. "Income Equality in the Nordic Countries: Myths, Facts, and Lessons." Journal of Economic Literature 63 (3): 791–839. DOI: 10.1257/jel.20251636&lt;head rend="h4"&gt;Additional Materials&lt;/head&gt;&lt;head rend="h4"&gt;JEL Classification&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;D31 Personal Income, Wealth, and Their Distributions&lt;/item&gt;&lt;item&gt;E23 Macroeconomics: Production&lt;/item&gt;&lt;item&gt;H23 Taxation and Subsidies: Externalities; Redistributive Effects; Environmental Taxes and Subsidies&lt;/item&gt;&lt;item&gt;J24 Human Capital; Skills; Occupational Choice; Labor Productivity&lt;/item&gt;&lt;item&gt;J31 Wage Level and Structure; Wage Differentials&lt;/item&gt;&lt;item&gt;J52 Dispute Resolution: Strikes, Arbitration, and Mediation; Collective Bargaining&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.aeaweb.org/articles?id=10.1257/jel.20251636"/></entry><entry><id>https://news.ycombinator.com/item?id=45068091</id><title>Do the simplest thing that could possibly work</title><updated>2025-08-30T07:32:02.690899+00:00</updated><content>&lt;doc fingerprint="7d78985f2d453935"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Do the simplest thing that could possibly work&lt;/head&gt;
    &lt;p&gt;When designing software systems, do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;p&gt;It’s surprising how far you can take this piece of advice. I genuinely think you can do this all the time. You can follow this approach for fixing bugs, for maintaining existing systems, and for architecting new ones.&lt;/p&gt;
    &lt;p&gt;A lot of engineers design by trying to think of the “ideal” system: something well-factored, near-infinitely scalable, elegantly distributed, and so on. I think this is entirely the wrong way to go about software design. Instead, spend that time understanding the current system deeply, then do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple can be underwhelming&lt;/head&gt;
    &lt;p&gt;System design requires competence with a lot of different tools: app servers, proxies, databases, caches, queues, and so on. As they gain familiarity with these tools, junior engineers naturally want to use them. It’s fun to construct systems out of many different components! And it feels very satisfying to draw boxes and arrows on a whiteboard - like you’re doing real engineering.&lt;/p&gt;
    &lt;p&gt;However, as with many skills, real mastery often involves learning when to do less, not more. The fight between an ambitious novice and an old master is a well-worn cliche in martial arts movies: the novice is a blur of motion, flipping and spinning. The master is mostly still. But somehow the novice’s attacks never seem to quite connect, and the master’s eventual attack is decisive.&lt;/p&gt;
    &lt;p&gt;In software, this means that great software design looks underwhelming. It doesn’t look like anything much is happening at all. You can tell you’re in the presence of great software design because you start having thoughts like “oh, I didn’t realise the problem was that easy” or “oh nice, you don’t actually have to do anything difficult”.&lt;/p&gt;
    &lt;p&gt;Unicorn is great software design, because it delivers all the most important guarantees in a web server (request isolation, horizontal scaling, crash recovery) by leaning on Unix primitives1. The industry-standard Rails REST API is great software design, because it gives you exactly what you need for a CRUD app in the most boring way possible. I don’t think any of these are impressive software. But they’re impressive feats of design, because they do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;p&gt;You should do that too! Suppose you’ve got a Golang application that you want to add some kind of rate limiting to. What’s the simplest thing that could possibly work? Your first idea might be to add some kind of persistent storage (say, Redis) to track per-user request counts with a leaky-bucket algorithm. That would work! But do you need a whole new piece of infrastructure? What if instead you kept those per-user request counts in-memory? Sure, you’d lose some rate limiting data when the application is restarted, but does that matter? Actually, are you sure your edge proxy2 doesn’t support rate limiting already? Could you just write a couple of lines in a config file instead of implementing the feature at all?&lt;/p&gt;
    &lt;p&gt;Maybe your edge proxy doesn’t support rate limiting. Maybe you can’t track it in-memory because you have too many server instances running in parallel, so the tightest rate limit you could enforce that way is too wide. Maybe it’s a dealbreaker if you ever lose rate limiting data, because people are hammering your service that hard. In that case, the simplest thing that could possibly work is adding persistent storage, so you should go and do that. But if you could do one of the easier approaches, wouldn’t you want to?&lt;/p&gt;
    &lt;p&gt;You really can build a whole application from scratch this way: start with the absolute simplest thing, and then only extend it when you have new requirements that force you to. It sounds silly, but it works. Think of it as taking YAGNI as the ultimate design principle: above single-responsibility, above choosing the best tool for the job, and above “good design”.&lt;/p&gt;
    &lt;head rend="h3"&gt;What’s wrong with doing the simplest thing?&lt;/head&gt;
    &lt;p&gt;Of course, there are three big problems with always doing the simplest thing that could possibly work. The first is that, by not anticipating future requirements, you end up with an inflexible system or a big ball of mud. The second is that it’s not clear what “simplest” means, so at worst I’m saying “to design well, always do good design”. The third is that you ought to be building systems that can scale, not systems that just work right now. Let’s take those objections in order.&lt;/p&gt;
    &lt;head rend="h4"&gt;Big balls of mud&lt;/head&gt;
    &lt;p&gt;To some engineers, “do the simplest thing that could possibly work” sounds like I’m telling them to stop doing engineering. If the simplest thing is usually a quick kludge, does that mean this advice will inevitably lead to a complete mess? We’ve all seen codebases with hacks stacked on top of hacks, and they definitely don’t look like good design.&lt;/p&gt;
    &lt;p&gt;But are hacks simple? I actually don’t think so. The problem with a hack or a kludge is precisely that it isn’t simple: that it adds complexity to the codebase by introducing another thing you have to always remember. Hacks are just easier to think of. Figuring out the proper fix is hard because it requires having to understand the entire codebase (or large sections of it). In fact, the proper fix is almost always much simpler than the hack.&lt;/p&gt;
    &lt;p&gt;It is not easy to do the simplest thing that could possibly work. When you’re looking at a problem, the first few solutions that come to mind are unlikely to be the simplest ones. Figuring out the simplest solution requires considering many different approaches. In other words, it requires doing engineering.&lt;/p&gt;
    &lt;head rend="h4"&gt;What is simplicity?&lt;/head&gt;
    &lt;p&gt;Engineers disagree a lot about what constitutes simple code. If “simplest” already means “with good design”, is it just a tautology to say “you should do the simplest thing that could possibly work?” In other words, is Unicorn really simpler than Puma3? Is adding in-memory rate limiting really simpler than using Redis? Here’s a rough, intuitive definition of simplicity4:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple systems have fewer “moving pieces”: fewer things you have to think about when you’re working with them&lt;/item&gt;
      &lt;item&gt;Simple systems are less internally-connected. They are composed from components with clear, straightforward interfaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unix processes are simpler than threads (and thus Unicorn is simpler than Puma) because processes are less connected: they do not share memory. This makes a lot of sense to me! But I don’t think it gives you the tools to figure out what’s simpler in every case.&lt;/p&gt;
    &lt;p&gt;What about in-memory rate limiting vs Redis? On the one hand, in-memory is simpler because you don’t have to think about all the things involved in standing up a separate service with persistent memory. On the other hand, Redis is simpler because the rate limiting guarantees it offers are more straightforward - you don’t have to worry about the case where one server instance thinks a user is rate limited and another one doesn’t.&lt;/p&gt;
    &lt;p&gt;When I’m not sure what “seems” simpler to me, I like to use this tiebreaker: simple systems are stable. If you’re comparing two states of a software system, and one will require more ongoing work if no requirements change, the other one is simpler. Redis must be deployed and maintained, it can have its own incidents, it requires its own monitoring, it requires a separate deployment in any new environments the service finds itself in, and so on. Thus in-memory rate limiting is simpler than Redis5.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why wouldn’t you want to be scalable?&lt;/head&gt;
    &lt;p&gt;A certain type of engineer is now screaming to themselves “but in-memory rate limiting won’t scale!” Doing the simplest thing that could possibly work will emphatically not deliver the most web-scale system. It will deliver a system that works well at the current scale. Is this irresponsible engineering?&lt;/p&gt;
    &lt;p&gt;No. In my view, the cardinal sin of big tech SaaS engineering is an obsession with scale. I’ve seen so much unavoidable pain caused by over-engineering systems to prepare for several orders of magnitude more than the current scale.&lt;/p&gt;
    &lt;p&gt;The main reason to not try this is that it doesn’t work. In my experience, for any non-trivial codebase, you can’t anticipate how it will behave at several orders of magnitude more traffic, because you don’t know ahead of time where all the bottlenecks are going to be. At most you can try to make sure you’re ready for 2x or 5x the current traffic, and then stand by to deal with problems as they come in.&lt;/p&gt;
    &lt;p&gt;The other reason not to try this is that it makes your codebase inflexible. It’s fun to decouple your service into two pieces so they can be scaled independently (I have seen this happen maybe ten times, and I have seen them actually be usefully scaled independently maybe once). But that makes certain features very hard to implement, because they now require coordination over the wire. In the worst case, they require transactions over the wire, which is a genuinely hard engineering problem. Most of the time you just don’t have to do any of this!&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;The longer I spend working in tech, the less optimistic I become about our collective ability to predict where a system is going. It’s hard enough to get your head around where a system currently is. And in fact, that’s the main practical difficulty in doing good design: getting an accurate big-picture understanding of the system. Most design is done without that understanding, and most design is thus pretty bad.&lt;/p&gt;
    &lt;p&gt;There are, broadly speaking, two ways to develop software. The first is to predict what your requirements might look like six months or a year from now, and then design the best system for that purpose. The second is to design the best system for what your requirements actually look like right now: in other words, to do the simplest thing that could possibly work.&lt;/p&gt;
    &lt;p&gt;edit: this article has gotten some comments on Hacker News.&lt;/p&gt;
    &lt;p&gt;One interesting comment thread says that simplicity of architecture doesn’t matter at scale, because the complexity of “state space exploration in implementation” (I think that means something like what I wrote about here) dominates any other complexity. I disagree - the more complex your feature interactions become, the more important a simple architecture becomes, because your “complexity budget” is almost exhausted.&lt;/p&gt;
    &lt;p&gt;I also want to credit Ward Cunningham and Kent Beck for inventing the expression - I genuinely thought I’d just come up with the wording myself, but I almost certainly just remembered it. Oops! Thanks to the HN user ternaryoperator for pointing this out.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;It’s just Unix sockets and forked processes! I love Unicorn.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Every tech company has some kind of edge proxy.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I do like Puma and think it’s a good web server. There are definitely use cases where you’d pick it over Unicorn (though in those cases I would personally think hard about using a different language than Ruby).&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I’m influenced here by Rich Hickey’s great talk Simple Made Easy. I don’t agree with all of it (I think familiarity does in fact contribute to simplicity in practice) but it’s definitely worth watching.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Of course, if the system has to scale horizontally more than a little bit, in-memory rate limiting won’t work and must be replaced with something like Redis. But in my experience a Golang service can scale a lot without having to scale horizontally to more than a handful of replicas.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;August 28, 2025 │ Tags: software design, shipping&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/the-simplest-thing-that-could-possibly-work/"/></entry><entry><id>https://news.ycombinator.com/item?id=45068986</id><title>The Theoretical Limitations of Embedding-Based Retrieval</title><updated>2025-08-30T07:32:02.441215+00:00</updated><content>&lt;doc fingerprint="7d9ca80b5cf390ce"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Information Retrieval&lt;/head&gt;&lt;p&gt; [Submitted on 28 Aug 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:On the Theoretical Limitations of Embedding-Based Retrieval&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.IR&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2508.21038"/></entry><entry><id>https://news.ycombinator.com/item?id=45070602</id><title>Nginx-CGI brings support for CGI to Nginx and angie</title><updated>2025-08-30T07:32:01.895021+00:00</updated><content>&lt;doc fingerprint="3c0a88587425f3c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Brings CGI support to Nginx and Angie webserver.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;OS&lt;/cell&gt;
        &lt;cell role="head"&gt;Tested with&lt;/cell&gt;
        &lt;cell role="head"&gt;Nginx&lt;/cell&gt;
        &lt;cell role="head"&gt;Angie&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Linux&lt;/cell&gt;
        &lt;cell&gt;AlmaLinux 9, Debian 12 and Ubuntu 24.04/20.04&lt;/cell&gt;
        &lt;cell&gt;okay&lt;/cell&gt;
        &lt;cell&gt;okay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Darwin&lt;/cell&gt;
        &lt;cell&gt;MacOS 15.1&lt;/cell&gt;
        &lt;cell&gt;okay&lt;/cell&gt;
        &lt;cell&gt;okay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BSD&lt;/cell&gt;
        &lt;cell&gt;FreeBSD 14.2 and OpenBSD 7.6&lt;/cell&gt;
        &lt;cell&gt;okay&lt;/cell&gt;
        &lt;cell&gt;okay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Solaris&lt;/cell&gt;
        &lt;cell&gt;OmniOS r1510521&lt;/cell&gt;
        &lt;cell&gt;okay&lt;/cell&gt;
        &lt;cell&gt;okay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows&lt;/cell&gt;
        &lt;cell&gt;No plan, nginx barely supports Windows&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;CGI is neither a demon nor an angel. It is simply a tool. Just like a chef's knife in the hands of a cook or a sword in the hands of a warrior, you won't use a sword for cooking, nor you take a chef's knife to the battlefield. The same goes for CGI, it has its appropriate scenarios, and it should not be misused or demonized.&lt;/p&gt;
    &lt;p&gt;CGI is good for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Low frequency applications, such as system management&lt;/item&gt;
      &lt;item&gt;Resource limited systems, such as embeding system&lt;/item&gt;
      &lt;item&gt;Low budget projects, such as personal websites&lt;/item&gt;
      &lt;item&gt;Prototyping, for fast iterate&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CGI is bad for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High QPS&lt;/item&gt;
      &lt;item&gt;High traffic&lt;/item&gt;
      &lt;item&gt;High concurrency&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I created a discord channel. If:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You are also a fun of CGI&lt;/item&gt;
      &lt;item&gt;If you have any problem with nginx-cgi&lt;/item&gt;
      &lt;item&gt;If you want to get update of nginx-cgi&lt;/item&gt;
      &lt;item&gt;If you want to know more friends&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please join us: https://discord.gg/EJSfqHHmaR.&lt;/p&gt;
    &lt;p&gt;Build and install:&lt;/p&gt;
    &lt;code&gt;# checkout source code
git clone https://github.com/pjincz/nginx-cgi
cd nginx-cgi

# build deb package
./build-deb-package.sh

# install built package
dpkg -i ../libnginx-mod-http-cgi_*_amd64.deb &lt;/code&gt;
    &lt;p&gt;Then enable cgi in nginx. If you have a newly installed nginx, you can find a default site at &lt;code&gt;/etc/nginx/sites-enabled/default&lt;/code&gt;. The default one looks like
this:&lt;/p&gt;
    &lt;code&gt;server {
    listen 80 default_server;
    listen [::]:80 default_server;

    root /var/www/html;

    index index.html index.htm index.nginx-debian.html;

    server_name _;

    location / {
        try_files $uri $uri/ =404;
    }
}
&lt;/code&gt;
    &lt;p&gt;The default &lt;code&gt;root&lt;/code&gt; points to &lt;code&gt;/var/www/html&lt;/code&gt;, keep it as it as, and add
following section after &lt;code&gt;location /&lt;/code&gt; section.&lt;/p&gt;
    &lt;code&gt;    location /cgi-bin {
        cgi on;
    }
&lt;/code&gt;
    &lt;p&gt;The newly added section means, for all request under &lt;code&gt;/cgi-bin&lt;/code&gt;, turns on cgi
support. Now restart nginx:&lt;/p&gt;
    &lt;code&gt;systemctl restart nginx&lt;/code&gt;
    &lt;p&gt;Save following content to /var/www/html/cgi-bin/hello.sh&lt;/p&gt;
    &lt;code&gt;#!/bin/bash

echo "Content-Type: text/plain"
echo

echo Hello CGI&lt;/code&gt;
    &lt;p&gt;Add x perm to cgi script:&lt;/p&gt;
    &lt;code&gt;chmod +x /var/www/html/cgi-bin/hello.sh&lt;/code&gt;
    &lt;p&gt;Now, try it:&lt;/p&gt;
    &lt;code&gt;curl http://127.0.0.1/cgi-bin/hello.sh&lt;/code&gt;
    &lt;p&gt;If you nothing wrong, you will get an output of &lt;code&gt;Hello CGI&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you are using latest deb based system, such as Debian and Ubuntu, and not willing to debug the plugin, you can just following the &lt;code&gt;Quick start&lt;/code&gt; to get a
usable deb package.&lt;/p&gt;
    &lt;p&gt;If you are using Angie, the cgi plugin has already in Angie's official repo. Please have a look here: https://en.angie.software/angie/docs/installation/oss_packages/#install-thirdpartymodules-oss&lt;/p&gt;
    &lt;p&gt;Manual build guide:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Checkout nginx and this plugin&lt;/p&gt;
        &lt;quote&gt;cd &amp;lt;some-where-you-like&amp;gt; git clone https://github.com/nginx/nginx git clone https://github.com/pjincz/nginx-cgi&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Generate Makefile in nginx dir&lt;/p&gt;&lt;quote&gt;cd nginx ./auto/configure --add-dynamic-module=$PWD/../nginx-cgi [...other option...]&lt;/quote&gt;&lt;p&gt;If you want to debug the plugin, you may also want&lt;/p&gt;&lt;code&gt;--with-debug&lt;/code&gt;.&lt;p&gt;If you want to build a module compatible with system's nginx, you need run&lt;/p&gt;&lt;code&gt;nginx -V&lt;/code&gt;to checkout system nginx's build options first.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Make the binary&lt;/p&gt;
        &lt;quote&gt;make&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If everything is good, then you will find &lt;code&gt;ngx_http_cgi_module.so&lt;/code&gt; under &lt;code&gt;objs&lt;/code&gt;
directory.&lt;/p&gt;
    &lt;p&gt;If this plugin is installed to nginx's default module path (such as &lt;code&gt;/usr/lib/nginx/modules&lt;/code&gt;), the plugin will be loaded automatically.
Otherwise, you need to manually load the plugin by &lt;code&gt;load_module&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Add following statement to nginx's top level context to load the plugin:&lt;/p&gt;
    &lt;code&gt;load_module &amp;lt;dir-of-plugin&amp;gt;/ngx_http_cgi_module.so;
&lt;/code&gt;
    &lt;p&gt;After loading the plugin, you can add &lt;code&gt;cgi on&lt;/code&gt; to location contexts to enable
cgi. Example:&lt;/p&gt;
    &lt;code&gt;location /cgi-bin {
    cgi on;
}
&lt;/code&gt;
    &lt;p&gt;Once cgi turned on on a location, all nested locations will also have cgi turned on. If you want to disable cgi for a child location, just use &lt;code&gt;cgi off&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When the location is accessed, nginx-cgi will find the script under the document root (it's specified by &lt;code&gt;root&lt;/code&gt; statement). For example, if you have specify the
document root as &lt;code&gt;/var/www/html&lt;/code&gt;, then when you access &lt;code&gt;/cgi-bin/hello.sh&lt;/code&gt;,
&lt;code&gt;/var/www/html/cgi-bin/hello.sh&lt;/code&gt; will be executed.&lt;/p&gt;
    &lt;p&gt;Nginx-cgi also support &lt;code&gt;alias&lt;/code&gt;, it like &lt;code&gt;root&lt;/code&gt; statement in nginx, the only
difference is the location prefix will be removed from uri. For example, if you
want &lt;code&gt;/cgi/hello.sh&lt;/code&gt; also reference to the same script, you can do this:&lt;/p&gt;
    &lt;code&gt;location /cgi {
    alias /var/www/html/cgi-bin;
    cgi on;
}
&lt;/code&gt;
    &lt;p&gt;A cgi script can be wrotten by any language. Here's an exmaple with shell. You can save it to &lt;code&gt;/var/www/html/cgi-bin/hello.sh&lt;/code&gt; for testing (if you didn't
change the default document root):&lt;/p&gt;
    &lt;code&gt;#!/bin/sh

echo "Status: 200 OK"
echo "Content-Type: text/plain"
echo

echo "Hello world"&lt;/code&gt;
    &lt;p&gt;The first line of the script is a shebang. If you clearly set &lt;code&gt;cgi_interpreter&lt;/code&gt;,
it's okay to remove this line, otherwise missing of shebang will causes a 500
error. Some shell allows script be executable even without shebang, but it's not
allowed here. If a script runable by shell, but return 500 error, check the
shebang.&lt;/p&gt;
    &lt;p&gt;The output of cgi script contains 2 sections: the header section and body section. The first 2 &lt;code&gt;echo&lt;/code&gt; statements output the header section, and the last
&lt;code&gt;echo&lt;/code&gt; statement outputs the body section. The &lt;code&gt;echo&lt;/code&gt; statement in middle
outputs the separator. Both header section and body section can be empty, but
the separator is mandatory. Missing of separator will causes an 500 error.&lt;/p&gt;
    &lt;p&gt;All lines in header section will be parsed as normal http response header line. And then passed to the client side. There's one special header &lt;code&gt;Status&lt;/code&gt;, it will
be passed in response status line. If &lt;code&gt;cgi_strict&lt;/code&gt; is on, nginx-cgi will check
all cgi output headers, and 500 error will be responsed if invalid header found.
Otherwise, invalid headers will be forwarded to client side too. It's fully
recommanded to keep &lt;code&gt;cgi_strict&lt;/code&gt; on.&lt;/p&gt;
    &lt;p&gt;After separator, all output will be sent to client as body as it is.&lt;/p&gt;
    &lt;p&gt;After all, you need to add the x permission to the file:&lt;/p&gt;
    &lt;code&gt;chmod +x /var/www/html/cgi-bin/hello.sh&lt;/code&gt;
    &lt;p&gt;Normally, you need x-permission to make script runable. Missing of x-permission can cause 403 error. If can't do this for any reason, &lt;code&gt;cgi_interpreter&lt;/code&gt; can
help.&lt;/p&gt;
    &lt;p&gt;Request headers will be parsed and then translated to environment variables and then passed to cgi script.&lt;/p&gt;
    &lt;p&gt;For example, you can find the query string in &lt;code&gt;QUERY_STRING&lt;/code&gt; environment var.
And access &lt;code&gt;Http-Accept&lt;/code&gt; by &lt;code&gt;HTTP_ACCPET&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's an example:&lt;/p&gt;
    &lt;code&gt;#!/bin/sh
echo ""

echo "query string: $QUERY_STRING"
echo "http accept: $HTTP_ACCEPT"&lt;/code&gt;
    &lt;p&gt;For full list of environment variables, see environment section.&lt;/p&gt;
    &lt;p&gt;The request body will be passed via stdin. Here's an example to read all request body and echo it:&lt;/p&gt;
    &lt;code&gt;#!/bin/sh
echo ""

body=$(cat)

echo "request body: $body"&lt;/code&gt;
    &lt;p&gt;Nginx-cgi has streaming support for both request and response body. For example, we can implement a simplest online caculator by &lt;code&gt;bc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;#!/bin/sh
echo ""

bc 2&amp;gt;&amp;amp;1&lt;/code&gt;
    &lt;p&gt;Then we can test our caculator by &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;curl 127.0.0.1/cgi-bin/bc.sh --no-progress-meter -T .&lt;/code&gt;
    &lt;p&gt;The nginx-cgi plugin is smart enough to choose the correct way to return the request body. If it got all output soon enough, it will output the body in once. If the output is delayed, it will output the body chunkly(HTTP 1.1) or streamingly (HTTP 1.0).&lt;/p&gt;
    &lt;p&gt;Hop-by-hop http headers are not allowed in cgi script output. If it appears in response here, a 500 error will response to the client.&lt;/p&gt;
    &lt;p&gt;For more information: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers#hop-by-hop_headers&lt;/p&gt;
    &lt;p&gt;Put following script to your cgi directory, and curl it form your terminal:&lt;/p&gt;
    &lt;code&gt;#!/bin/sh

echo 'Content-Type: text/plain'
echo

printenv&lt;/code&gt;
    &lt;p&gt;Put a sudo file to &lt;code&gt;/etc/sudoers.d&lt;/code&gt; and run &lt;code&gt;sudo&lt;/code&gt; in your script or set
&lt;code&gt;cgi_interpreter&lt;/code&gt; as &lt;code&gt;/usr/bin/sudo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's an example of sudo config file:&lt;/p&gt;
    &lt;code&gt;# allow wwww-data run /var/www/bin/my-danger-script with root account
www-data ALL=(root) NOPASSWD: /var/www/bin/my-danger-script

# allow all CGI script be launched with sudo by nginx-cgi directly
www-data ALL=(root) NOPASSWD: SETENV: /var/www/html/cgi-bin/*
&lt;/code&gt;
    &lt;p&gt;It's highly not recommanded to run CGI script with chroot. Because chroot is not designed for security purpose. It still shared a lot of kernel spaces with host system. For example, run &lt;code&gt;ps -ef&lt;/code&gt; in chrooted process, all processes in host
system will return. That sould not too aweful? No, that's really terrible,
because you can also do &lt;code&gt;kill&lt;/code&gt; in chrooted script for the same reason. And
people normally run programs with root permission in chrooted environment.
That's terribly bad. It causes system on high risk than just run script with
&lt;code&gt;www-data&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you want a sandbox environment, &lt;code&gt;lxc&lt;/code&gt;, &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;jails&lt;/code&gt; are much better
for this purpose.&lt;/p&gt;
    &lt;p&gt;If you still want &lt;code&gt;chroot&lt;/code&gt;, okay let me show you how to do it.&lt;/p&gt;
    &lt;p&gt;In this example, I assume you're using &lt;code&gt;/var/www/html&lt;/code&gt; as the document root.&lt;/p&gt;
    &lt;p&gt;Prepare a CGI script first:&lt;/p&gt;
    &lt;code&gt;mkdir -p /var/www/html/cgi-bin
cat &amp;gt; /var/www/html/cgi-bin/ls.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
echo "Status: 200"
echo "Content-Type: text-plain"
echo
echo "files under /:"
ls /
EOF
chmod +x /var/www/html/cgi-bin/ls.sh

# try it
/var/www/html/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;Step 1: prepare a chroot directory.&lt;/p&gt;
    &lt;p&gt;That're a lot of ways to do this step. &lt;code&gt;debootstrap&lt;/code&gt; is a popular way on debian
based system. &lt;code&gt;busybox&lt;/code&gt; is the most light way. &lt;code&gt;docker&lt;/code&gt; is a modern way.&lt;/p&gt;
    &lt;p&gt;Let's make a lightest directory with &lt;code&gt;busybox&lt;/code&gt; here:&lt;/p&gt;
    &lt;code&gt;# In this example, I put everything to /var/www/chroot
# Be careful, I download x86_64 busybox version here, you may need to change it
# You need root permission to run all following commands, I'm too lazy to
# prepend sudo to every commands here.

root_dir=/var/www/chroot

mkdir -p "$root_dir/bin" &amp;amp;&amp;amp; cd "$root_dir/bin"
wget https://www.busybox.net/downloads/binaries/1.35.0-x86_64-linux-musl/busybox
chmod +x busybox

cd "$root_dir"
mkdir -p $(dirname $(./bin/busybox --list-full) | sort -u)
./bin/busybox --list-full | while read line; do ln -sf /bin/busybox $line; done

# try it
chroot "$root_dir" ls&lt;/code&gt;
    &lt;p&gt;Step 2: mount document root into chroot dir&lt;/p&gt;
    &lt;code&gt;mkdir -p /var/www/chroot/var/www/html
mount --bind /var/www/html /var/www/chroot/var/www/html

# try it
ls /var/www/chroot/var/www/html&lt;/code&gt;
    &lt;p&gt;Notice:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I use a trick here, after chroot, the document root is still the same. By this we can same some time to do path mapping.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The mounting will not persist after a reboot. You may need to add an entry to /etc/fstab. Or move /var/www/html into chroot, and make a symbolic link outside.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Step 3: allow &lt;code&gt;www-data&lt;/code&gt; to run &lt;code&gt;chroot&lt;/code&gt; with root permission.&lt;/p&gt;
    &lt;code&gt;cat &amp;gt;/etc/sudoers.d/www-run-with-chroot &amp;lt;&amp;lt;EOF
# allow and only allow www-data run chroot with /var/www/chroot
www-data ALL=(root) NOPASSWD: /usr/sbin/chroot /var/www/chroot *
EOF&lt;/code&gt;
    &lt;p&gt;Now everything is ready, add following section to your nginx/angie:&lt;/p&gt;
    &lt;code&gt;location /cgi-bin {
    cgi on;
    cgi_interpreter /usr/bin/sudo /usr/sbin/chroot /var/www/chroot;
}
&lt;/code&gt;
    &lt;p&gt;try it:&lt;/p&gt;
    &lt;code&gt;curl 127.0.0.1/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;In this example, I assume you're using &lt;code&gt;/var/www/html&lt;/code&gt; as the document root.&lt;/p&gt;
    &lt;p&gt;Prepare a CGI script first:&lt;/p&gt;
    &lt;code&gt;mkdir -p /var/www/html/cgi-bin
cat &amp;gt; /var/www/html/cgi-bin/ls.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
echo "Status: 200"
echo "Content-Type: text-plain"
echo
echo "files under /:"
ls /
EOF
chmod +x /var/www/html/cgi-bin/ls.sh

# try it
/var/www/html/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;Create a container and keep running in the background:&lt;/p&gt;
    &lt;code&gt;# Change -v if necessary
# -d: runs background
# -i -t: keep a terminal
# --restart always: keep container alive
docker run -dit --restart always --name my_cgi_docker -v /var/www:/var/www busybox sh

# try it
docker exec my_cgi_docker /var/www/html/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;Allow &lt;code&gt;www-data&lt;/code&gt; to run &lt;code&gt;docker&lt;/code&gt; commands:&lt;/p&gt;
    &lt;code&gt;sudo usermod -aG docker www-data

# try it
sudo -u www-data docker exec my_cgi_docker /var/www/html/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;Now everything is ready, add following section to your nginx/angie:&lt;/p&gt;
    &lt;code&gt;location /cgi-bin {
    cgi on;
    cgi_interpreter /usr/bin/docker exec my_cgi_docker;
}
&lt;/code&gt;
    &lt;p&gt;Okay, you're a fan of FreeBSD? Me too.&lt;/p&gt;
    &lt;p&gt;It's really similar to running scripts with &lt;code&gt;chroot&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here I assume you're using &lt;code&gt;/var/www/html&lt;/code&gt; as the document root too.&lt;/p&gt;
    &lt;p&gt;Prepare a CGI script first:&lt;/p&gt;
    &lt;code&gt;mkdir -p /var/www/html/cgi-bin
cat &amp;gt; /var/www/html/cgi-bin/ls.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
echo "Status: 200"
echo "Content-Type: text-plain"
echo
echo "files under /:"
ls /
EOF
chmod +x /var/www/html/cgi-bin/ls.sh

# try it
/var/www/html/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;Step 1: create a jail&lt;/p&gt;
    &lt;p&gt;Let's put the jail to &lt;code&gt;/var/www/jail&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;mkdir -p /var/www/jail &amp;amp;&amp;amp; cd /var/www/jail
fetch https://download.freebsd.org/ftp/releases/$(uname -m)/$(uname -m)/$(uname -r)/base.txz
tar -xvf base.txz -C .

# create mount points
mkdir -p /var/www/jail/var/www/html
touch /var/www/jail/etc/resolv.conf&lt;/code&gt;
    &lt;p&gt;Put following config to &lt;code&gt;/etc/jail.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;www-jail {
    path = "/var/www/jail";
    host.hostname = "www-jail.local";

    exec.clean;
    exec.start = "/bin/sh /etc/rc";
    exec.stop = "/bin/sh /etc/rc.shutdown";

    # mount /var/www/html =&amp;gt; /var/www/jail/var/www/html
    exec.prestart += "mount_nullfs /var/www/html /var/www/jail/var/www/html || true";
    mount.devfs;

    # uncomment following lines, if you want to allow network access in jail
    # ip4 = inherit;
    # ip6 = inherit;
    # exec.prestart += "mount_nullfs /etc/resolv.conf /var/www/jail/etc/resolv.conf || true";

    # uncomment fowlling lines, if you also want `ping` available in jail
    # allow.raw_sockets = 1;

    persist; # keep jail if no process runs
}
&lt;/code&gt;
    &lt;p&gt;And ensure that following line appears in &lt;code&gt;/etc/rc.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;jail_enable="YES"
&lt;/code&gt;
    &lt;p&gt;And start the jail:&lt;/p&gt;
    &lt;code&gt;service jail start www-jail

# try it
jexec www-jail ls /
jexec www-jail /var/www/html/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;Step 2: allow &lt;code&gt;www&lt;/code&gt; to run &lt;code&gt;jexec&lt;/code&gt; with root permission.&lt;/p&gt;
    &lt;p&gt;I uses &lt;code&gt;sudo&lt;/code&gt; here. I'm not familiar with &lt;code&gt;doas&lt;/code&gt;, if you prefer &lt;code&gt;doas&lt;/code&gt; you can
try it yourself. Anyhow, neither &lt;code&gt;sudo&lt;/code&gt; nor &lt;code&gt;doas&lt;/code&gt; preloaded with FreeBSD. You
need to manually install one of them.&lt;/p&gt;
    &lt;code&gt;cat &amp;gt;/usr/local/etc/sudoers.d/www-jexec &amp;lt;&amp;lt;EOF
# allow and only allow `www` run `jexec` with `www-jail`
www ALL=(root) NOPASSWD: /usr/sbin/jexec www-jail *
EOF

# try it
sudo -u www sudo jexec www-jail /var/www/html/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;Now everything is ready, add following section to your nginx/angie:&lt;/p&gt;
    &lt;code&gt;location /cgi-bin {
    cgi on;
    cgi_interpreter /usr/local/bin/sudo /usr/sbin/jexec www-jail;
}
&lt;/code&gt;
    &lt;p&gt;try it:&lt;/p&gt;
    &lt;code&gt;curl 127.0.0.1/cgi-bin/ls.sh&lt;/code&gt;
    &lt;p&gt;Just make sure not to inherit &lt;code&gt;stdout&lt;/code&gt; when creating the process (ideally, avoid
inheriting &lt;code&gt;stdin&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt; as well). Here's an example write in shell.&lt;/p&gt;
    &lt;code&gt;taskid=1234
logfile="/var/lib/my-project/$taskid"
./long-run-task.sh "$taskid" &amp;lt;/dev/null &amp;gt;"$logfile" 2&amp;gt;&amp;amp;1 &amp;amp;&lt;/code&gt;
    &lt;p&gt;Or if you are familiar with pipe operation, just close &lt;code&gt;stdout&lt;/code&gt; (also, it's
better to close &lt;code&gt;stdin&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt; as well), http request will finished
immediently. And you can use the process as background process.&lt;/p&gt;
    &lt;code&gt;exec &amp;lt;/dev/null &amp;gt;somewhere 2&amp;gt;&amp;amp;1

# now http response is done, do what every you like
sleep 9999&lt;/code&gt;
    &lt;p&gt;As you see abvoing. In CGI world, http request's lifecycle depends on pipe's (stdout's) lifecycle.&lt;/p&gt;
    &lt;p&gt;Each child process might inherit the CGI process's pipe. If any process that inherited stdout remains alive, the HTTP request will never finish.&lt;/p&gt;
    &lt;p&gt;This may causes confiusing, when you want a long run background or killing CGI process.&lt;/p&gt;
    &lt;p&gt;For creating long-run process, see aboving topic.&lt;/p&gt;
    &lt;p&gt;For killing CGI process, kill the whole process group rather than CGI process itself.&lt;/p&gt;
    &lt;code&gt;cgi_pid=...

# don't do this
# kill "$cgi_pid"

# do this
kill -- "-$cgi_pid"&lt;/code&gt;
    &lt;p&gt;See aboving topic.&lt;/p&gt;
    &lt;p&gt;Traditionally, people use rewriting to archive this. But it's much easier here. You can do it with &lt;code&gt;cgi pass&lt;/code&gt;. Here's an example to render markdone dynamically:&lt;/p&gt;
    &lt;code&gt;{
    location ~ ^.*\.md$ {
        cgi_pass /var/www/bin/cgi/render-markdown.sh;
    }
}
&lt;/code&gt;
    &lt;code&gt;#!/bin/sh

set -e

if [ ! -f "${DOCUMENT_ROOT}${PATH_INFO}" ]; then
    echo "Status: 404"
    echo
    exit
fi

echo "Status: 200"
echo "Content-Type: text/html"
echo

echo "&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;"
markdown "${DOCUMENT_ROOT}${PATH_INFO}"
echo "&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;"&lt;/code&gt;
    &lt;p&gt;Way 1: Removing CGI script's suffix&lt;/p&gt;
    &lt;p&gt;Way 2: do rewriting&lt;/p&gt;
    &lt;p&gt;Way 3: &lt;code&gt;cgi pass&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;#!/bin/sh

echo "Status: 404"
echo "Content-Type: text/plain"
echo

echo "Welcome to the void"&lt;/code&gt;
    &lt;code&gt;#!/bin/sh

echo "Status: 302"
echo "Location: https://theuselessweb.com"
echo&lt;/code&gt;
    &lt;p&gt;You can read the request body from &lt;code&gt;stdin&lt;/code&gt;. If you're using shell, &lt;code&gt;cat&lt;/code&gt; can
quickly save request body to a file.&lt;/p&gt;
    &lt;p&gt;For small files, you can write file to &lt;code&gt;stdout&lt;/code&gt; directly.&lt;/p&gt;
    &lt;p&gt;For large files, it's much better to send a 302 response. Because CGI response is streaming, protocol cannot easily handle caching, chunked downloads, or resume support.&lt;/p&gt;
    &lt;p&gt;Go for it. Nginx-cgi don't care what language you use. Just grabs information from environment var, and read request body from &lt;code&gt;stdin&lt;/code&gt;, and write output to
&lt;code&gt;stdout&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Enable or disable cgi module on giving location block.&lt;/p&gt;
    &lt;p&gt;If you specify &lt;code&gt;on&lt;/code&gt; here, the plugin will work in traditional mode. It parses
the request uri first, and then locate the script under document root directory
with request uri. After all it splits request uri to &lt;code&gt;SCRIPT_NAME&lt;/code&gt; and
&lt;code&gt;PATH_INFO&lt;/code&gt;. This is good if you have an old CGI project or you want to strictly
follow rfc3875.&lt;/p&gt;
    &lt;p&gt;I also provided a nginx style syntax here. If you specify &lt;code&gt;cgi pass&lt;/code&gt; here, the
plugin will skip the step to locate the CGI script. It uses the the value you
provided directly. You can references nginx variables in the second argument,
eg: &lt;code&gt;cgi pass $document_root$uri&lt;/code&gt;. The aboving example do something similar to
rfc3875, but not equal. In this form, request uri will be assigned to
&lt;code&gt;PATH_INFO&lt;/code&gt; directly. And &lt;code&gt;SCRIPT_NAME&lt;/code&gt; will be empty. This form is really good
for dynamic content generating. It gets around the complex and unnecessary uri
re-writing.&lt;/p&gt;
    &lt;p&gt;Additionally, the second form also provides you the ability to pass additional args to script, eg: &lt;code&gt;cgi pass my_script.sh $uri&lt;/code&gt;. With this, you can totally
avoid confusing rfc3875 environment variables.&lt;/p&gt;
    &lt;p&gt;If you specify &lt;code&gt;off&lt;/code&gt; here, the plugin will be disabled.&lt;/p&gt;
    &lt;p&gt;Default: off&lt;/p&gt;
    &lt;p&gt;Alias of &lt;code&gt;cgi pass &amp;lt;script_path&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Set interpreter and interpreter args for cgi script.&lt;/p&gt;
    &lt;p&gt;When this option is not empty, cgi script will be run with giving interpreter. Otherwise, script will be executed directly.&lt;/p&gt;
    &lt;p&gt;This option can contains nginx variables, see https://nginx.org/en/docs/varindex.html for more details.&lt;/p&gt;
    &lt;p&gt;This option is extremely useful in a lot of senarios, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;run CGI scripts missing x-perm&lt;/item&gt;
      &lt;item&gt;do sudo before executing CGI script&lt;/item&gt;
      &lt;item&gt;wrap general binary as CGI script&lt;/item&gt;
      &lt;item&gt;filter CGI script output&lt;/item&gt;
      &lt;item&gt;...&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Default: empty&lt;/p&gt;
    &lt;p&gt;Set the working directory of CGI script.&lt;/p&gt;
    &lt;p&gt;If this value is set to empty, CGI scripts will inherit nginx' working directory.&lt;/p&gt;
    &lt;p&gt;If this value is set to an non-empty string, the CGI script will be launched with giving working directory.&lt;/p&gt;
    &lt;p&gt;The action of changing working directory may failed. For example, giving directory doesn't exist, no perm or name too long. In this case, script will failed to execute.&lt;/p&gt;
    &lt;p&gt;This option doesn't change the way to find interpreter or script (if they are specified with related path, they are always related to nginx' working directory).&lt;/p&gt;
    &lt;p&gt;This option can contain nginx variable. Althrough I don't know what use this is. Maybe you can setup different working dir for different server_name by this.&lt;/p&gt;
    &lt;p&gt;Default: empty&lt;/p&gt;
    &lt;p&gt;A standard CGI script should output two parts: header and body. And an empty line to split those two parts.&lt;/p&gt;
    &lt;p&gt;If you want to simply run a normal program as CGI program. You can turn this on.&lt;/p&gt;
    &lt;p&gt;Once this option is enabled, all outout will be treated as response body, and be sent to the client.&lt;/p&gt;
    &lt;p&gt;Default: off&lt;/p&gt;
    &lt;p&gt;Change cgi script PATH environment variable.&lt;/p&gt;
    &lt;p&gt;Default: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&lt;/p&gt;
    &lt;p&gt;Enable or disable strict mode.&lt;/p&gt;
    &lt;p&gt;When strict mode turns on, bad cgi header will cause 500 error. When strict mode turns off, bad cgi header be forward as it is.&lt;/p&gt;
    &lt;p&gt;Default: on&lt;/p&gt;
    &lt;p&gt;Add and pass extra environment variables to CGI script. The first argument of this command is the name of environment variable. It should contains only alphabets, numbers and underscore, and doesn't start with number. The second argument of this command is the value express of the var. It can contains nginx variables, see https://nginx.org/en/docs/varindex.html for more details.&lt;/p&gt;
    &lt;p&gt;This option can appears more than 1 time to set multiple variables. If more than one option set the same var, then the last one works. These directives are inherited from the previous configuration level if and only if there's no cgi_set_var directives defined on the current level.&lt;/p&gt;
    &lt;p&gt;This option can also be used to override standard CGI vars. This may be useful in some case, for example hacking old CGI script or simulate standard vars that are not supported by this plugin now (Such as &lt;code&gt;PATH_TRANSLATED&lt;/code&gt;,
&lt;code&gt;REMOTE_IDENT&lt;/code&gt;). But it's not recommanded, it may introduce confusing issues to
your system.&lt;/p&gt;
    &lt;p&gt;Redirect cgi stderr to giving file.&lt;/p&gt;
    &lt;p&gt;By default, nginx-cgi grab cgi script's stderr output and dump it to nginx log. But this action is somewhat expensive, because it need to create an extra connection to listen stderr output. If you want to avoid this, you can use this option to redirect cgi script's stderr output to a file. Or you can even discard all stderr output by redirect to &lt;code&gt;/dev/null&lt;/code&gt;. Also you can use this to redirect
all stderr output to nginx's stderr by set it as &lt;code&gt;/dev/stderr&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Enable or disable reverse dns.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;off&lt;/code&gt;: disable rdns feature.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;on&lt;/code&gt;: Do reverse dns before launching cgi script, and pass rdns result to cgi
script via &lt;code&gt;REMOTE_HOST&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;double&lt;/code&gt;: After reverse dns, do a forward dns again to check the rdns result. if
result matches, pass result as &lt;code&gt;REMOTE_HOST&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;required&lt;/code&gt;: If rdns failed, 403, 503 or 500 returns to the client. Depends on
the failure reason of rdns.&lt;/p&gt;
    &lt;p&gt;If you turns this option on, you need to setup a &lt;code&gt;resolver&lt;/code&gt; in nginx too.
Otherwise you will get an error of &lt;code&gt;no resolver defined to resolve&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;author notes: do not enable this option, it will makes every request slower. this feature can be easily implemented by &lt;code&gt;dig -x&lt;/code&gt; or &lt;code&gt;nslookup&lt;/code&gt; in script. the
only reason I implement this is just to make the module fully compliant with the
rfc3875 standard.&lt;/p&gt;
    &lt;p&gt;Send &lt;code&gt;TERM&lt;/code&gt;/&lt;code&gt;KILL&lt;/code&gt; signals to the CGI process if it runs too long.&lt;/p&gt;
    &lt;p&gt;If both &lt;code&gt;t1&lt;/code&gt; and &lt;code&gt;t2&lt;/code&gt; equal to &lt;code&gt;0&lt;/code&gt;. Timeout feature is disabled.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;t1&lt;/code&gt; or &lt;code&gt;t2&lt;/code&gt; doesn't equal to &lt;code&gt;0&lt;/code&gt;. A &lt;code&gt;TERM&lt;/code&gt; or &lt;code&gt;KILL&lt;/code&gt; signal will be sent to
the process after timeout.&lt;/p&gt;
    &lt;p&gt;If both &lt;code&gt;t1&lt;/code&gt; and &lt;code&gt;t2&lt;/code&gt; not zero. Send &lt;code&gt;TERM&lt;/code&gt; at &lt;code&gt;t1&lt;/code&gt; timestamp first. And send
&lt;code&gt;KILL&lt;/code&gt; again at &lt;code&gt;t1+t2&lt;/code&gt; timestamp (if process still alive at that timestamp).&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;t2&lt;/code&gt; doesn't present, it treated as &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Default: 0 0&lt;/p&gt;
    &lt;p&gt;Nginx-cgi implemented almost all rfc3875 standard variables. If they cannot cover all of your usage, you can add your own variable by &lt;code&gt;cgi_set_var&lt;/code&gt;. Also
those variables can be overrided by &lt;code&gt;cgi_set_var&lt;/code&gt; if you really want to.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;AUTH_TYPE&lt;/code&gt;,&lt;code&gt;REMOTE_USER&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If cgi script is behind an authorization module (such as &lt;code&gt;ngx_http_auth_basic_module&lt;/code&gt;), and the authorization is succeed, the value is
set to auth type (such as &lt;code&gt;Basic&lt;/code&gt;) and authorized user.&lt;/p&gt;
    &lt;p&gt;If no authorization module enabled, no matter client passes autoriazation header or not. Those 2 fields are not present.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Authorization&lt;/code&gt; header is not visible in cgi script for security reason. If you
really want to do authorization in CGI script, try &lt;code&gt;cgi_set_var&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CONTENT_LENGTH&lt;/code&gt;,&lt;code&gt;CONTENT_TYPE&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Same to request header's &lt;code&gt;Content-Length&lt;/code&gt; and &lt;code&gt;Content-Type&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GATEWAY_INTERFACE&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Always be &lt;code&gt;CGI/1.1&lt;/code&gt; in this plugin.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;PATH_INFO&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's say if you have a script under &lt;code&gt;/cgi-bin/hello.sh&lt;/code&gt;, and you access
&lt;code&gt;http://127.0.0.1/cgi-bin/hello.sh/somewhat&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Then &lt;code&gt;PATH_INFO&lt;/code&gt; contains the string &lt;code&gt;/somewhat&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Combination with url &lt;code&gt;rewrite&lt;/code&gt; or &lt;code&gt;cgi pass&lt;/code&gt;, this variable can be used for
dynamic content generating.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;PATH_TRANSLATED&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: this option is not implemented strictly compliant with rfc3875. Please avoid this, if you are writing new CGI script.&lt;/p&gt;
    &lt;p&gt;This is related to &lt;code&gt;PATH_INFO&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let's say if you have a script under &lt;code&gt;/cgi-bin/hello.sh&lt;/code&gt;, and you access
&lt;code&gt;http://127.0.0.1/cgi-bin/hello.sh/somewhat&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The standard says, the server should try again with &lt;code&gt;http://127.0.0.1/somewhat&lt;/code&gt;,
and found out where the uri should mapped to.&lt;/p&gt;
    &lt;p&gt;For technical reason, I just construct this variable by document root and &lt;code&gt;PATH_INFO&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The behaviour may be changed in future version.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;QUERY_STRING&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contains the query string of the request. For example, if you are accessing &lt;code&gt;http://127.0.0.1/cgi-bin/hello.sh?a=1&amp;amp;b=2&lt;/code&gt;, &lt;code&gt;QUERY_STRING&lt;/code&gt; will contains
&lt;code&gt;a=1&amp;amp;b=2&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;REMOTE_ADDR&lt;/code&gt;, (rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Client ip address.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;REMOTE_HOST&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Client host name. Only available if &lt;code&gt;cgi_rdns&lt;/code&gt; is turns on.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;cgi_rdns&lt;/code&gt; is on, nginx-cgi will do a reverse DNS, and find a domain matches
&lt;code&gt;REMOTE_ADDR&lt;/code&gt;. If any found, it will be set to &lt;code&gt;REMOTE_HOST&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;cgi_rdns&lt;/code&gt; is double, after the RDNS, nginx-cgi will do a forward DNS again.
&lt;code&gt;REMOTE_HOST&lt;/code&gt; will only be set if the forward DNS result matches the original
address.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;cgi_rdns&lt;/code&gt; for more information.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;REMOTE_IDENT&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nginx-cgi plugin doesn't support this for security reason.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;REQUEST_METHOD&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Request method of the request, for example: &lt;code&gt;GET&lt;/code&gt;, &lt;code&gt;POST&lt;/code&gt;...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SCRIPT_NAME&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Path to current script. Normally, you don't need this. It doesn't contains the full path. See &lt;code&gt;SCRIPT_FILENAME&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The only reason to use this is construct the URI after rewriting. You can use &lt;code&gt;SCRIPT_NAME&lt;/code&gt; + &lt;code&gt;PATH_INFO&lt;/code&gt; to get the URI after rewriting.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SERVER_NAME&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Server name, normally it equals to &lt;code&gt;Host&lt;/code&gt; header without port part. If &lt;code&gt;Host&lt;/code&gt;
header doesn't appear in the request (HTTP/1.0) or contains invalid value, then
this value is set to the reflect server ip address. If the ip address is an ipv6
address, it will be quoted with bracket like &lt;code&gt;[::1]&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SERVER_PORT&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Server listening port, such as &lt;code&gt;80&lt;/code&gt;, &lt;code&gt;443&lt;/code&gt;...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SERVER_PROTOCOL&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The protocol used between client and server. Such as &lt;code&gt;HTTP/1.0&lt;/code&gt;, &lt;code&gt;HTTP/1.1&lt;/code&gt;...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SERVER_SOFTWARE&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contains a string of nginx and version, such as &lt;code&gt;nginx/1.27.4&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;X_&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All &lt;code&gt;X-&lt;/code&gt; prefixed http request header will be convert to &lt;code&gt;X_&lt;/code&gt; variables. For
example:&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;X-a: 123&lt;/code&gt; appears in header, &lt;code&gt;X_A&lt;/code&gt; will be set to &lt;code&gt;123&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HTTP_&lt;/code&gt;(rfc3875 standard)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All other http request header will be convert to &lt;code&gt;HTTP_&lt;/code&gt; variables, for example:&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;Accept: */*&lt;/code&gt; appears in header, &lt;code&gt;HTTP_ACCEPT&lt;/code&gt; will be set to &lt;code&gt;*/*&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;DOCUMENT_ROOT&lt;/code&gt;(non-standard, impled by apache2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Document root of current location block, see &lt;code&gt;root&lt;/code&gt; stmt in nginx.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;REMOTE_PORT&lt;/code&gt;(non-standard, impled by apache2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Client port number.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;REQUEST_SCHEME&lt;/code&gt;(non-standard, impled by apache2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;http&lt;/code&gt; or &lt;code&gt;https&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;REQUEST_URI&lt;/code&gt;(non-standard, impled by apache2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The raw uri before rewriting. If you want the URL after rewriting, try &lt;code&gt;SCRIPT_NAME&lt;/code&gt; + &lt;code&gt;PATH_INFO&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note: this variable doesn't same to nginx varible &lt;code&gt;$request_uri&lt;/code&gt;. You can find
the document at https://httpd.apache.org/docs/2.4/mod/mod_rewrite.html.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SCRIPT_FILENAME&lt;/code&gt;(non-standard, impled by apache2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The full path to the CGI script.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SERVER_ADDR&lt;/code&gt;(non-standard, impled by apache2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Server ip address. If the server has multiple ip addresses. The value of this variable can be different if requests came from different interfaces.&lt;/p&gt;
    &lt;p&gt;By rfc3875, &lt;code&gt;PATH_TRANSLATED&lt;/code&gt; should point to the file that as if &lt;code&gt;$PATH_INFO&lt;/code&gt;
accessed as &lt;code&gt;uri&lt;/code&gt;. But that's really hard to impl on nginx, it need re-trigger
nginx's location process. And those functions are private, cannot access by
plugin directly. The another way to impl it is starting a sub-request, but it's
too expensive, and this var is really rearly used. It's really not worth to do
it. So I simply construct this var by document root and &lt;code&gt;path_info&lt;/code&gt; vars.&lt;/p&gt;
    &lt;p&gt;Nginx's resolver impl doesn't access /etc/hosts. I don't want to impl an extra resolver in plugin. So I just ignore this problem.&lt;/p&gt;
    &lt;p&gt;https://datatracker.ietf.org/doc/html/rfc3875&lt;/p&gt;
    &lt;p&gt;https://nginx.org/en/docs/dev/development_guide.html https://hg.nginx.org/nginx-tests&lt;/p&gt;
    &lt;p&gt;https://datatracker.ietf.org/doc/html/rfc2616#section-13.5.1&lt;/p&gt;
    &lt;p&gt;https://datatracker.ietf.org/doc/html/rfc3875#section-4.1&lt;/p&gt;
    &lt;p&gt;https://httpd.apache.org/docs/2.4/howto/cgi.html&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/pjincz/nginx-cgi"/></entry><entry><id>https://news.ycombinator.com/item?id=45070793</id><title>Why Romania excels in international Olympiads</title><updated>2025-08-30T07:32:01.582858+00:00</updated><content>&lt;doc fingerprint="b679f819390b9407"&gt;
  &lt;main&gt;
    &lt;p&gt;Olympiads are international student intellectual competitions in which students from across the world go toe-to-toe answering questions in mathematics, physics, informatics, chemistry, and more. The best performers tend to be from countries like China, the United States, India, and Japan. But, somehow, the southeastern European country of Romania also frequently tops the list.&lt;/p&gt;
    &lt;p&gt;Since 2020, Romania’s performance in the International Mathematical Olympiad (IMO) has been nothing short of amazing. In 2022, Romania came in fifth overall, fourth in 2023, and twelfth in 2024. In 2023, Romania placed fourth globally and first in Europe at the International Physics Olympiad, seventeenth globally and third in Europe at the International Olympiad in Informatics, sixth globally and second in Europe in the European Girls’ Mathematical Olympiad, first in the Balkan Mathematical Olympiad—which also included France, Italy, and the United Kingdom—and first in the Central European Olympiad in Informatics. Romania also performed well in the International Chemistry Olympiad and many others.&lt;/p&gt;
    &lt;p&gt;It’s an understatement to call Romania’s skill in Olympiads merely “overperformance”. Romania’s lackluster performance in international assessments and its relatively small population size of just over 19 million people makes the things they do in Olympiads downright miraculous.&lt;/p&gt;
    &lt;p&gt;Average Romanian educational performance is unimpressive. Romanian youth routinely perform below the average of OECD countries and near the bottom of the pack of European nations. Romania has a poor-to-mediocre showing whether you include or exclude migrants from the calculations, and its scores on assessments like the PISA aren’t low due to being tainted by bias in the examinations. Romania genuinely underperforms. But underperformance is not the impression you would get if you only knew of Romanian education from Olympiads.&lt;/p&gt;
    &lt;p&gt;One possibility is that Romanian students have more variable performance on international assessments than students in other countries. No dice: they aren’t much more variable than the student populations in other countries, and a handful of comparably-sized nations with worse Olympiad performance are more variable. Another possibility is that, for some reason, there’s a fat right tail in Romanian educational performance. If this is true, it just doesn’t show up in any existing data. Given the fact that international assessments indicate Romania’s sampling tends to be population-representative, we should have a strong prior against this possibility. Romanian test scores tend to be distributed along a symmetrical bell curve.&lt;/p&gt;
    &lt;p&gt;Yet another possibility is that Romania has an undersampled ethnic group that overperforms, but whose schools aren’t tested very well. The only group this might be is Romanian Jews and using them as an explanation is problematic for two reasons. The first is that there are too few to realistically explain Romanian Olympiad performance. The second is that we know the identities of Olympiad participants from Romania, and they don’t seem to be Jewish.&lt;/p&gt;
    &lt;p&gt;Something else, something more mysterious, explains why Romania is such an outlier in international intellectual competitions. That thing is, in fact, the unique design of the Romanian educational system.&lt;/p&gt;
    &lt;p&gt;In the late 19th century, Romanian prince regnant Alexandru Ioan Cuza attempted to raise the status of the nation by instituting a mass literacy campaign centered around building free schools that children were compelled to attend. This effort was largely a failure, with literacy failing to break 50% by the 1930s. But World War II precipitated change. In 1948, Romania’s new governing communist party began to bring about serious educational reform at a breakneck pace.The Education Law of 1948 was passed to provoke a military-grade offensive against illiteracy, involving the mass participation of the literate from all walks of life in uplifting the poor, the abandoned, and those who simply shunned education. By the end of the 1950s, illiteracy was practically eradicated among Romania’s youth.&lt;/p&gt;
    &lt;p&gt;The education system that existed in Romania’s communist period was modeled on the system in place in the Soviet Union, and it included a fair helping of political propaganda in addition to physical labor. The system also overproduced schools, resulting in shoddy but widely available facilities dotting the country. Like the Soviet school system, Romania’s was marked by increasing lengths of compulsory education, poor availability of qualified teachers and educational supplies, high budgetary costs, and an extreme level of credential inflation.&lt;/p&gt;
    &lt;p&gt;After the fall of communism, the new democratic government went on to shutter many of these schools and to immediately lower compulsory schooling requirements to put an end to the bureaucratic nightmare that Soviet influence had saddled the country with. In the following years, how Romania wished to ration scarce governmental resources for education was a matter of intense debate, and out of that debate came a strong sentiment that, whatever the system, Romanian education would be structured competitively.&lt;/p&gt;
    &lt;p&gt;Nowadays, the most prestigious Romanian high schools are the National Colleges, or Colegiu Național. These schools are often international and frequently uphold old educational traditions sometimes dating back more than a century. Below these schools are the Liceu Teoretic, which are the norm, offering standard educations. Romania also has three military colleges—Colegiu Militar—managed directly by the Ministry of National Defense. There are also schools focused on service, technical schools, vocational schools, and apprenticeship programs. The brightest students get their pick among these schools after they take the national placement test, the Evaluarea Națională, when they are graduating the 8th grade around ages fourteen to fifteen.&lt;/p&gt;
    &lt;p&gt;The high school placement test is a standardized test covering Romanian language and literature as well as mathematics. Performance on the examination is reported publicly when students are issued a score on a one-to-ten scale with precision to two decimal places. A student who receives a high grade—say 9.65—would have their pick from most any school, whereas a student scoring 5.00 or below would usually be constrained to a less academically-focused form of education like a vocational program. Most students elect to go to the best school they are able to test into, and so the degree of sorting across schools is very high. To make this setup even more extreme, there is also often—but not universally—sorting within schools, as students select into educational tracks. This is done directly when applying to schools.&lt;/p&gt;
    &lt;p&gt;At the end of the Romanian high school experience, there is a graduation test, the Bacalaureat, or bac. This test is marked like the entrance examination and, to pass, students must obtain a score of at least five in the subjects they have elected to take. This testing includes written and oral examinations, assessments of foreign language and computer skills, and, for ethnic minorities, assessment of their skill with their maternal language other than Romanian. The need for a given score on this examination can range from requiring just passing to requiring a high score, depending on the university one intends to attend, if that is their goal.&lt;/p&gt;
    &lt;p&gt;The design of Romania’s educational system makes it perhaps the most stratified educational system in the world. The fact that they have a centralized repository containing all student and teacher educational data makes their system perfect for a high-powered evaluation of exactly what happens when a country opts to hyper-stratify education.&lt;/p&gt;
    &lt;p&gt;One of the cruel parts of the Romanian system is that, though sorting is nationally available, students do not have equal opportunities to sort. Students located in smaller towns have fewer high school options to select from unless they’re among the few who opt into a military academy, which means joining the military. The extent of sorting is far more intense in areas with larger numbers of schools. In a recent paper, the Romanian economist Andrei Munteanu provided an illustration of how this works: essentially, the fewer schools in a locale, the more each individual school contains students with a wider range of ability and, the more schools in a locale, the more each individual school will be stratified into low, middle, or high ability.&lt;/p&gt;
    &lt;p&gt;This combined sorting between schools and tracks means that low-ability students get stuck with other low-ability students, and high-ability students are surrounded by other high-ability students. In effect, peer groups throughout high school are extremely homogeneous. This matters because then low-performing students drag down low-performing students, and high performers cause each other to rise. Romania’s educational system has causal peer impacts on student performance on the graduation test that are very large in both directions, but primarily where there are opportunities for sorting to take place.&lt;/p&gt;
    &lt;p&gt;But peer effects are not everything to Romania’s exceptional Olympiad performance; they are just the fertile ground in which exceptional performance is fostered. The next part has to do with teachers. Like students, Romania’s teachers must take tests to be able to do what they want to do. Teachers naturally prefer to lecture smarter students, and the smartest teachers have their pick of the schools, and even of the tracks. In a paper with extremely robust results, researchers from the last decade described this as such:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[Teachers] with higher certification standards are more likely to work at better-ranked schools. This sorting persists even within schools as one moves from a weaker to a stronger track, and even within tracks as one moves from a weaker to a stronger class.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The best teachers also opt into towns with more schools. It’s apparent, then, that teachers prefer teaching in the highest-achieving places they can be, both within and between towns. The effect of teacher-student ability pairing is accentuated even more by incentives to compete. The government of Romania is not unique in providing monetary rewards for those who win Olympiads, those who teach winners of Olympiads, or those schools Olympiad winners attend, but they are unique in having all the previously-mentioned institutional characteristics on top of providing comprehensive monetary incentives for Olympiad achievement.&lt;/p&gt;
    &lt;p&gt;Romania’s immense success in Olympiads and the widely recognized importance of Olympiad wins for signaling student human capital has also spawned a small number of private schools that advertise their prominence and tutoring capabilities. Many teachers also recommend to parents that they obtain additional tutoring for their brighter pupils, and tutoring services are commonplace. The commonality of tutoring for Olympiad winners is a global constant, whereas the things distinguishing Romania are not.&lt;/p&gt;
    &lt;p&gt;Two notable factors do not increase performance in the same direction. These are very slight decrements in funding allocated to the highest-ability schools, and when parents reduce the time they spend helping their students with homework, conditional on their kids matching into better schools. Another potential factor that militates against the synchrony of resource allocation in Romania is that children in more selective schools report feeling marginalized because they realize that they’re not as strong of students as they believed. The decrements in funding are likely to be unproblematic, because higher-scoring schools tend to be larger and more urban, lending them economies of scale. Due to this, they may have effectively more funding.&lt;/p&gt;
    &lt;p&gt;With all the pieces on the board, the key to Romania’s Olympiad success is three-fold: put the best students in the same classrooms, put the best teachers with the best students, and then incentivize schools, teachers, and students each to win Olympiads.&lt;/p&gt;
    &lt;p&gt;This system has proved amazingly fruitful. Given its underlying human capital, the poverty from its communist legacy, and its modest population size, Romania should not perform the way it does in academic Olympiads. And yet it does. The trade-off for Romania, however, is palpable.&lt;/p&gt;
    &lt;p&gt;Large portions of Romania’s Olympiad winners leave the country. Because Romania is a member state of the European Union, the people the country has put great effort into training and credentialing are easily able to leave the country and acquire jobs elsewhere.&lt;/p&gt;
    &lt;p&gt;Losing the right tail to brain drain is damaging for many countries, but it’s arguably worse for Romania because its educational system is so zero-sum: the top performers do better, while the low-performers do worse. This sorting does not “lift all boats,” as it were. In Romania, the system makes for an incredibly well-trained right tail and a neglected left tail, and that left tail might hurt more than the right tail is helped, if effects on test scores are any indication. On its own, Romania’s system might be a stellar boon to the country. But with free movement of talent between countries, Romania ends up subsidizing talent discovery for other countries with less apt educational systems.&lt;/p&gt;
    &lt;p&gt;Most of the growth we see around us is due to the innovations of the right tail, and if they do better, we all do better. Though I doubt Romania’s schooling raises the intelligence of the right tail, even raising aptitude is worth something, because we must get capable people to the frontiers of their respective fields in order to innovate, and Romania has fostered a system that seems to do just that. Moreover, even if Olympiad training does not make those on the right tail more capable but instead simply prepares them better, then it can still have large, socially beneficial effects simply through providing Romania’s highly capable people with a means of having their talents recognized internationally.&lt;/p&gt;
    &lt;p&gt;But these benefits are returned only very indirectly to Romania, if at all on net. Rather than changing Romania’s educational system or closing the borders, the right solution is for more nations to choose to be like Romania, getting a lot more juice out of their smart kids by designing a system just for them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.palladiummag.com/2025/08/29/why-romania-excels-in-international-olympiads/"/></entry><entry><id>https://news.ycombinator.com/item?id=45071677</id><title>SynthID – A tool to watermark and identify content generated through AI</title><updated>2025-08-30T07:32:01.232450+00:00</updated><content>&lt;doc fingerprint="b7b099db758f4fa7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SynthID&lt;/head&gt;
    &lt;p&gt;A tool to watermark and identify content generated through AI&lt;/p&gt;
    &lt;head rend="h2"&gt;What is SynthID?&lt;/head&gt;
    &lt;p&gt;Generative AI can help us all to be more creative, productive, and innovative. But it can be hard to tell the difference between content that’s been AI-generated, and content created without AI.&lt;/p&gt;
    &lt;p&gt;SynthID is our new watermarking tool, designed specifically for AI-generated content. It empowers users to identify AI-generated (or altered) content, helping to foster transparency and trust in generative AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;How SynthID works&lt;/head&gt;
    &lt;p&gt;SynthID embeds digital watermarks directly into AI-generated images, audio, text or video. The watermarks are embedded across Google’s generative AI consumer products, and are imperceptible to humans – but can be detected by SynthID's technology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://deepmind.google/science/synthid/"/></entry><entry><id>https://news.ycombinator.com/item?id=45071703</id><title>Accelerating life sciences research</title><updated>2025-08-30T07:32:00.989889+00:00</updated><content>&lt;doc fingerprint="e65d40dfb2358f88"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Accelerating life sciences research&lt;/head&gt;
    &lt;p&gt;OpenAI and Retro Biosciences achieve 50x increase in expressing stem cell reprogramming markers.&lt;/p&gt;
    &lt;p&gt;At OpenAI, we believe that AI can meaningfully accelerate life science innovation. To test this belief, we collaborated with the Applied AI team at Retro Bio(opens in a new window), a longevity biotech startup, to create and research the impact of GPT‑4b micro, a miniature version of GPT‑4o specialized for protein engineering.&lt;/p&gt;
    &lt;p&gt;We are excited to share that we’ve successfully leveraged GPT‑4b micro to design novel and significantly enhanced variants of the Yamanaka factors, a set of proteins which led to a Nobel Prize for their role in generating induced pluripotent stem cells (iPSCs) and rejuvenating cells. They have also been used to develop therapeutics to combat blindness(opens in a new window), reverse diabetes(opens in a new window), treat infertility(opens in a new window), and address organ shortages(opens in a new window).&lt;/p&gt;
    &lt;p&gt;In vitro, these redesigned proteins achieved greater than a 50-fold higher expression of stem cell reprogramming markers than wild-type controls. They also demonstrated enhanced DNA damage repair capabilities, indicating higher rejuvenation potential compared to baseline. This finding, made in early 2025, has now been validated by replication across multiple donors, cell types, and delivery methods, with confirmation of full pluripotency and genomic stability in derived iPSC lines. To ensure the findings are discoverable and replicable to benefit the life sciences industry, we are now sharing insights into the research and development of GPT‑4b micro.&lt;/p&gt;
    &lt;p&gt;To test our belief that AI can be used to accelerate life sciences research, we designed and trained a custom model—GPT‑4b micro—to possess a broad base of knowledge and skills across biology, with a particular focus on steerability and flexibility to enable advanced use cases such as protein engineering. We initialized it from a scaled-down version of GPT‑4o to take advantage of GPT models’ existing knowledge, then further trained it on a dataset composed mostly of protein sequences, along with biological text and tokenized 3D structure data, elements most protein language models omit.&lt;/p&gt;
    &lt;p&gt;A large portion of the data was enriched to contain additional contextual information about the proteins in the form of textual descriptions, co-evolutionary homologous sequences, and groups of proteins that are known to interact. This context allows GPT‑4b micro to be prompted to generate sequences with specific desired properties and, since most of the data is structure-free, the model handles proteins with intrinsically disordered regions just as well as structured proteins. This is particularly useful for targets like the Yamanaka factors, whose activity depends on forming numerous transient interactions with a diverse array of binding partners, rather than adopting a single stable structure (Figure 2).&lt;/p&gt;
    &lt;p&gt;By training on proteins enriched with additional evolutionary and functional context, we substantially increased the effective context length of our training examples beyond that of standalone sequences. Consequently, we found that we could run prompts as large as 64,000 tokens during inference and continue to observe gains in controllability and output quality. While common in text LLMs, this context size is unprecedented in protein sequence models.&lt;/p&gt;
    &lt;p&gt;During development we observed the emergence of scaling laws similar to those seen in language models—larger models trained on larger datasets yielded predictable gains in perplexity and downstream protein benchmarks, allowing us to iterate at small scale before training the final GPT‑4b micro model. However, in silico evals for protein AI models are often of limited value, as it is unclear if such improvements translate to increased utility in the real world. To demonstrate that the model is capable of accelerating therapeutic development, we worked with Retro’s scientists, who used the model to re-engineer proteins relevant to their cell-reprogramming research program.&lt;/p&gt;
    &lt;p&gt;The Yamanaka factors—OCT4, SOX2, KLF4, and MYC (OSKM)—are some of the most important proteins in regenerative biology today, and are named after Shinya Yamanaka, who discovered their ability to reprogram adult cells into pluripotent stem cells, a breakthrough that earned him the Nobel Prize in Physiology or Medicine in 2012. Unfortunately, they suffer from poor efficiency: typically less than 0.1% of cells convert during treatment, and the process can take three weeks or more. Efficiency drops further in cells from aged or diseased donors(opens in a new window), so finding more efficient variants remains an active and important research focus.&lt;/p&gt;
    &lt;p&gt;Directly optimizing the protein sequences is hard. SOX2 contains 317 amino acids and KLF4 513; the number of possible variants is on the order of 10^1000, so traditional “directed-evolution” screens that mutate a handful of residues at a time are able to explore only a miniscule fraction of the design space. A leading academic effort(opens in a new window) tested a few thousand SOX2 mutants and found a handful of triple-mutants with a modest gain, while 15 years of work on chimeric SOX(opens in a new window) proteins has yielded variants that differ from natural SOX constituents by only five residues.&lt;/p&gt;
    &lt;p&gt;The team at Retro built a wet lab screening platform using human fibroblast (skin and connective tissue) cells, initially validating it with baseline OSKM and SOX2 variants manually designed by Retro’s scientists as part of their pilot screen (Figure 3). Then, they asked GPT‑4b micro to propose a diverse set of “RetroSOX” sequences. Over 30% of the model’s suggestions in the screen outperformed wild‑type SOX2 at expressing key pluripotency markers, even though they differed by more than 100 amino acids on average. For comparison, in traditional screens(opens in a new window), hit rates below 10% are typical.&lt;/p&gt;
    &lt;p&gt;The team next tackled reengineering KLF4, the largest of the Yamanaka factors. KLF4 is known to be replaceable with other KLF-family factors(opens in a new window) but without an increase in reprogramming efficacy. Prior attempts to improve KLF4 by expert-guided single amino acid substitutions(opens in a new window) produced a single hit out of 19. As with RetroSOX, we prompted the model to generate a set of enhanced RetroKLF variants. Overall, 14 model-generated variants were superior to the best cocktails from the RetroSOX screen—a hit rate of nearly 50% (Figure 4).&lt;/p&gt;
    &lt;p&gt;Combining the top RetroSOX and RetroKLF variants produced the largest gains. Across three independent experiments, fibroblasts showed a dramatic rise in both early (SSEA-4) and late (TRA-1-60, NANOG) markers, with late markers appearing several days sooner than under the wild-type OSKM cocktail (Figure 5).&lt;/p&gt;
    &lt;p&gt;In addition, the RetroSOX and RetroKLF variants were validated by alkaline phosphatase (AP) staining at day 10, confirming that the colonies not only express late-stage pluripotency markers but also exhibit robust AP activity indicative of pluripotency (Figure 6).&lt;/p&gt;
    &lt;p&gt;To further confirm the improved reprogramming efficiency and explore clinical potential, we tested a different delivery method (mRNA instead of viral vectors) and another cell type—mesenchymal stromal cells (MSCs)—derived from three middle-aged human donors (over 50 years old). Within just 7 days, more than 30% of the cells began expressing key pluripotency markers (SSEA4 and TRA-1-60), and by day 12, numerous colonies appeared with morphology similar to typical iPSCs (Figure 7, left and center). Over 85% of these cells activated endogenous expression of critical stem cell markers, including OCT4, NANOG, SOX2, and TRA-1-60.&lt;/p&gt;
    &lt;p&gt;We then verified that these RetroFactor-derived iPSCs could successfully differentiate into all three primary germ layers (endoderm, ectoderm, and mesoderm). Additionally, we expanded multiple monoclonal iPSC lines over several passages, confirming healthy karyotypes (Figure 7, right) and genomic stability suitable for cell therapies. These results consistently surpassed benchmarks obtained from conventional iPSC lines generated by contract research organizations using standard factors, further supporting the robustness of our engineered variants. Moreover, they provide evidence of enhanced iPSC generation across different delivery modalities and cell types.&lt;/p&gt;
    &lt;p&gt;Taken together, the high hit rates, deep sequence edits, accelerated marker onset, and AP+ colony formation provide early evidence that AI-guided protein design can substantially accelerate progress in stem cell reprogramming research.&lt;/p&gt;
    &lt;p&gt;Motivated by these results, we next investigated the rejuvenation potential of our re-engineered variants, specifically examining their ability to restore youthful characteristics to aged cells. We focus on DNA damage, which causes impaired cellular function and is a canonical hallmark of aging(opens in a new window). Earlier work(opens in a new window) has demonstrated that Yamanaka factors can erase DNA damage-related aging markers in cells derived from mice without fully reverting cell identity. We sought to find out whether our variants showed enhanced rejuvenation capabilities relative to baseline OSKM.&lt;/p&gt;
    &lt;p&gt;In our DNA‑damage assay, cells treated with the RetroSOX/KLF cocktail showed visibly less γ‑H2AX intensity—a marker of double‑strand breaks—than cells reprogrammed with standard OSKM or a fluorescent control (Figure 8).&lt;/p&gt;
    &lt;p&gt;These results suggest that the RetroSOX/KLF cocktail reduces DNA damage more effectively than the original Yamanaka factors. By ameliorating one of the core hallmarks of cellular aging, the engineered variants offer a potential path toward improved cell rejuvenation and use in future therapies.&lt;/p&gt;
    &lt;p&gt;To OpenAI, this work is an illustration of how quickly a domain-specific model can deliver breakthrough results on a focused scientific problem. “When researchers bring deep domain insight to our language-model tooling, problems that once took years can shift in days,” says Boris Power, who leads research partnerships at OpenAI. “We look forward to seeing what other advances emerge as more teams pair their expertise with the models we’re building.”&lt;/p&gt;
    &lt;p&gt;OpenAI and Retro Contributors&lt;/p&gt;
    &lt;p&gt;Model development leads: John Hallman, Aaron Jaech, Rico Meinl&lt;/p&gt;
    &lt;p&gt;Science, dry lab: Madison Ueland (PhD student), Andrei Tarkhov (lead)&lt;/p&gt;
    &lt;p&gt;Science, wet lab: Kevin Joseph (PhD student), Jacqueline Larouche (lead)&lt;/p&gt;
    &lt;p&gt;Core contributors: Swathi Karthikeyan, Darren Yu, Sonja Mihailovic, Jonathan Hales, Tomas Matteson&lt;/p&gt;
    &lt;p&gt;Leadership: Boris Power, Joe Betts-LaCroix&lt;/p&gt;
    &lt;p&gt;Special thanks to: Chris Koch, Morgan Griffiths, Raul Arora, Tobias Fehlmann, Jared Tumiel, Bry Nguyen, Shelly Huynh&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;Notes: GPT-4b micro was developed for research purposes and is not broadly available. Sam Altman is an investor in Retro Biosciences. OpenAI’s research team formed the collaboration.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/accelerating-life-sciences-research-with-retro-biosciences/"/></entry><entry><id>https://news.ycombinator.com/item?id=45071722</id><title>Show HN: Hacker News em dash user leaderboard pre-ChatGPT</title><updated>2025-08-30T07:32:00.707910+00:00</updated><content>&lt;doc fingerprint="7599563f9395cd2e"&gt;
  &lt;main&gt;
    &lt;p&gt;TEXT 123 += !? @#&amp;amp; GAME LIMITS Endless Timer Timer length 15s 30s 1 min 2 min 5 min Words Word limit 5 10 15 20 25 30 50 100 200 ⏱️ Off 15s 30s 1 min 2 min 5 min #️⃣ Off 5 10 15 20 25 30 50 100 200 THEME Capy Paradise Cyberpunk Christian Roman 1 Roman 2 Murica Vaporwave Brazil HD Capy Boba Capy Creepy Forest Coral Reef Mountains Italy Coast Wet Capy Japan Anime Russian Capy Toast Capy Creepy Capy Wood Middle East Lost Castle City Night Earth from Space Cafe Capy Cottage Dark Academia Space Coffee City Brutalist Minimalist Cream Neutral SOUND Music Off Holiday Forest Sounds Eastern African Middle Eastern Classical Ocean Waves Piano Lofi Guitar Synthwave Spooky Jazz Zen Fantasy Ukelele Volume Typing Off Pentatonic Blip Click Volume NOOB You typed: x PRO Hide Off Current Current &amp;amp; Next Highlight Off Next 2nd Ghost Ghost WPM End if WPM &amp;lt; End at # Errors errors 🙈 Off Current Current &amp;amp; Next 🔦 Off Next Word 2nd Word SHIFT TAB for new text You typed: WPM: 0 Accuracy: 100% ⌥ ENTER / ALT ENTER for new game mode TAB to retry SHIFT TAB for new game Final WPM: 0 Accuracy: 100% Start New Session ⌥ ENTER / ALT ENTER for new game mode TAB to retry SHIFT TAB for new game&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gally.net/miscellaneous/hn-em-dash-user-leaderboard.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45071838</id><title>Fun and Immersive Typing Game</title><updated>2025-08-30T07:32:00.127251+00:00</updated><content>&lt;doc fingerprint="7599563f9395cd2e"&gt;
  &lt;main&gt;
    &lt;p&gt;TEXT 123 += !? @#&amp;amp; GAME LIMITS Endless Timer Timer length 15s 30s 1 min 2 min 5 min Words Word limit 5 10 15 20 25 30 50 100 200 ⏱️ Off 15s 30s 1 min 2 min 5 min #️⃣ Off 5 10 15 20 25 30 50 100 200 THEME Capy Paradise Cyberpunk Christian Roman 1 Roman 2 Murica Vaporwave Brazil HD Capy Boba Capy Creepy Forest Coral Reef Mountains Italy Coast Wet Capy Japan Anime Russian Capy Toast Capy Creepy Capy Wood Middle East Lost Castle City Night Earth from Space Cafe Capy Cottage Dark Academia Space Coffee City Brutalist Minimalist Cream Neutral SOUND Music Off Holiday Forest Sounds Eastern African Middle Eastern Classical Ocean Waves Piano Lofi Guitar Synthwave Spooky Jazz Zen Fantasy Ukelele Volume Typing Off Pentatonic Blip Click Volume NOOB You typed: x PRO Hide Off Current Current &amp;amp; Next Highlight Off Next 2nd Ghost Ghost WPM End if WPM &amp;lt; End at # Errors errors 🙈 Off Current Current &amp;amp; Next 🔦 Off Next Word 2nd Word SHIFT TAB for new text You typed: WPM: 0 Accuracy: 100% ⌥ ENTER / ALT ENTER for new game mode TAB to retry SHIFT TAB for new game Final WPM: 0 Accuracy: 100% Start New Session ⌥ ENTER / ALT ENTER for new game mode TAB to retry SHIFT TAB for new game&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://keybara.io"/></entry><entry><id>https://news.ycombinator.com/item?id=45072160</id><title>From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms</title><updated>2025-08-30T07:31:59.982324+00:00</updated><content>&lt;doc fingerprint="3518d6c6043b3b90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms&lt;/head&gt;
    &lt;head rend="h2"&gt;What is attention?&lt;/head&gt;
    &lt;p&gt;In any autoregressive model, the prediction of the future tokens is based on some preceding context. However, not all the tokens within this context equally contribute to the prediction, because some tokens might be more relevant than others. The attention mechanism addresses this by allowing the model to concentrate on the important context words selectively, while generating each output word or token. Consider the popular example that explains the attention mechanism.&lt;/p&gt;
    &lt;p&gt;“The animal didn’t cross the street because it was too tired”.&lt;/p&gt;
    &lt;p&gt;In this sentence, the pronoun “it” could refer to either “animal” or “street”. Attention helps the model to associate “it” with “animal” rather than “street” by weighing the relative importance of each word. This helps the model to understand the relationships between words and capture the contextual meaning in various NLP tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;How is attention calculated?&lt;/head&gt;
    &lt;p&gt;There are various types of attention mechanisms today, beginning with the Multi-Head Attention (MHA), which introduced the attention concept in the seminal paper. More recently, advanced variants like Multi-Latent Head Attention (MHLA) have been employed in popular models like Deepseek. This blog aims to cover the fundamentals of each attention mechanism, including the core ideas, advantages, limitations, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Concepts in Attention Mechanisms&lt;/head&gt;
    &lt;p&gt;Before diving into specific types of attention, we need to understand some fundamental concepts that underpin all the various attention mechanisms.&lt;/p&gt;
    &lt;p&gt;The main idea behind the attention mechanism is to dynamically weigh, and focus on relevant parts of inputs. Attention is required in both the encoding and decoding stages. But in this blog, we will be discussing this from a decoder's point of view.&lt;/p&gt;
    &lt;p&gt;During each generation step, we need to understand the attention weights, which help us to get a better contextual representation for the next word prediction. At its core, attention operates through three fundamental components — queries, keys, and values — that work together with attention scores to create a flexible, context-aware vector representation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Query (Q): The query is a vector that represents the current token for which the model wants to compute attention.&lt;/item&gt;
      &lt;item&gt;Key (K): Keys are vectors that represent the elements in the context against which the query is compared, to determine the relevance.&lt;/item&gt;
      &lt;item&gt;Attention Scores: These are computed using Query and Key vectors to determine the amount of attention to be paid to each context token.&lt;/item&gt;
      &lt;item&gt;Value (V): Values are the vectors that represent the actual contextual information. After calculating the attention scores using Query and Key vectors, these scores are applied against Value vectors to get the final context vector&lt;/item&gt;
      &lt;item&gt;KV Caching: Since the key and value vectors are for previous tokens, we can skip this computation for those tokens that are already calculated. KV caching stores the precomputed keys and values from the previous computations, which helps in faster decoding in autoregressive models by reusing the cached vectors. However, the Query vectors cannot be cached, since they are calculated for the current token.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To understand how each of these vectors are scores are calculated you can refer to this blog.&lt;/p&gt;
    &lt;p&gt;The high-level concepts remain consistent across all types of attention mechanisms. However, the key difference lies in how efficiently each of them executes the attention process without compromising on performance. Innovations focus on computational speed, reducing memory usage, improving scalability across longer sequences, etc.&lt;/p&gt;
    &lt;p&gt;Now, let's dive into each of these techniques&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Head Attention (MHA)&lt;/head&gt;
    &lt;p&gt;In multi-head attention, for computing the attention weights for the ith token, first, a query vector is calculated for that token. To calculate the attention weights for the token, this query vector is compared with all the preceding tokens. For that, key vectors are calculated for all the preceding tokens. These comparisons will generate an attention score, which is then used to produce a weighted score for each token using the corresponding value vectors.&lt;/p&gt;
    &lt;p&gt;In multi-head attention, this process is repeated in parallel across multiple attention “heads”. Each head has its own query, value, and key vectors, using which it calculates the relationship between the words. The final output context vector will be the concatenated output from all the attention heads.&lt;/p&gt;
    &lt;p&gt;Now, this seems straightforward. However, as the context grows, the number of Key and Value vectors will increase dramatically, because these vectors need to be calculated and stored for all the context tokens. For a sequence length of n, each query vector must be compared against all n key vectors and then perform the weighted combination using n value vectors. This results in a quadratic complexity in both computation and memory.&lt;/p&gt;
    &lt;p&gt;KV cache can help in reducing the computation and memory overhead during inference. But as the context grows, the size of the cache grows linearly with sequence length to store all the keys and values for all the preceding tokens. KV cache reduces the redundant computations, but will not reduce the fundamental cost of attending to all the previous tokens.&lt;/p&gt;
    &lt;p&gt;Models using MHA – Bert, RoBerta, T5, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Query Attention (MQA)&lt;/head&gt;
    &lt;p&gt;A significant challenge with MHA was the high computational and memory overhead associated with storing and processing separate Key and Value vectors for each attention head.&lt;/p&gt;
    &lt;p&gt;MQA addresses this problem by using multiple query heads but sharing a common set of Key and Value vectors across all the heads. In other words, there are still “h” distinct Query projections using which the model attends the current token from multiple perspectives. But the same Key and Value vectors are used for every head.&lt;/p&gt;
    &lt;p&gt;This approach will greatly reduce the memory bandwidth requirements without significantly sacrificing the model performance. By sharing the Key and Value vectors, MQA enables an efficient inference, especially for Large language models with long context lengths.&lt;/p&gt;
    &lt;p&gt;Here, the Key and Value vectors need to be calculated only once for a token instead of “h” times, which reduces the computation cost of Key/Value projection. But note that for calculating the attention score, each query head is still multiplied by the Key vectors and then weighed using the Value vectors. So this remains the same.&lt;/p&gt;
    &lt;p&gt;Also, with MQA only one set of Key-Value pairs needs to be cached, regardless of the number of Query heads. This lets the KV cache size grow gradually as the sequence length grows, leading to much lower memory requirements when compared to MHA&lt;/p&gt;
    &lt;p&gt;Models using MQA – PaLM, Falcon&lt;/p&gt;
    &lt;head rend="h2"&gt;Grouped Query Attention (GQA)&lt;/head&gt;
    &lt;p&gt;Grouped Query attention offers a balance between the MHA and MQA. As we saw earlier, traditional MHA requires significant memory and computation overhead due to separate Key-Value vectors for each Query head, and the computation overhead even increases as the number of heads increases. MQA addresses this by having a shared Key-Value, which reduces the computation cost and memory, but it may impact the model performance.&lt;/p&gt;
    &lt;p&gt;GQA offers a compromise between these two extremes. Instead of having a common Key-Value for all the heads, GQA divides the Query heads into “g” groups and lets each group share a common Key and Value head. We can say, MHA and MQA come as two extreme cases of GQA, with g=1 leading to MQA and g=h leading to MHA. This approach reduces the memory and computational requirements compared to MHA while retaining a better performance than MQA.&lt;/p&gt;
    &lt;p&gt;Models using GQA – Llama2, Llama3, Mistral&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Head Latent Attention (MHLA)&lt;/head&gt;
    &lt;p&gt;While GQA performs better than MQA, but still may not match MHA’s performance in some complex tasks.&lt;/p&gt;
    &lt;p&gt;MHLA is a recent innovation in transformer architecture introduced in models like DeepSeek. Its main goal is to dramatically reduce memory usage and accelerate inference, especially for large language models (LLMs), without loss in model performance.&lt;/p&gt;
    &lt;p&gt;The idea is to attain a performance near MHA. So we need to consider separate Key value heads for each attention head, like in MHA, but also improve the inference speed by reducing the memory overhead for storing the large amounts of Key value vectors.&lt;/p&gt;
    &lt;p&gt;MHLA addresses the challenge of high memory usage and slow inference by compressing the Key and Value representations into a much smaller latent space using low-rank projections. Specifically, instead of storing the full Key and Value vectors for every token and head, MHLA applies a linear transformation that projects these vectors into a lower-dimensional space.&lt;/p&gt;
    &lt;p&gt;So during the inference:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A down-projection weight matrix W(DKV) is introduced and is multiplied with the input sequence to obtain a compressed latent vector C(KV) for keys and Values. This latent vector is stored in cache, which is significantly smaller in size when compared to the full key and Value vectors&lt;/item&gt;
      &lt;item&gt;This is then multiplied by an up-projection matrix W(UK) and W(UV) to get the Key and Value vectors&lt;/item&gt;
      &lt;item&gt;Additionally, the matrix W(KR) is used to produce a decoupled Key that carries the Rotary Positional embedding&lt;/item&gt;
      &lt;item&gt;Additionally, the same process is done for attention Queries as well, which will reduce the activation memory during training&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MHLA supports switching between two computation paradigms for different stages. During the training stage, which is computationally intensive, it operates similarly to MHA, where the computational overhead is slightly lower than conventional MHA. During inference, it can seamlessly switch to a paradigm similar to MQA. Here, the cached KV head interacts with all query heads to produce the final output.&lt;/p&gt;
    &lt;p&gt;Models using MHLA– Deepseek- V2, Deep seek V2&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In addition to the topics discussed, there are various innovative methods that are designed to optimise the challenges of the traditional attention technique. Some of these include sparse attention, efficient attention, memory augmented attention, etc. These approaches reflect the focus on ongoing research for making the attention more scalable, faster, and adaptable across various tasks and requirements.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this post! Let me know if you liked it, have questions, or spotted an error. Please feel free to contact or follow me through LinkedIn, Twitter, or Medium.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24"/></entry><entry><id>https://news.ycombinator.com/item?id=45072599</id><title>Milan's expat 'explosion' brings new buzz to Italy's financial centre</title><updated>2025-08-30T07:31:59.378564+00:00</updated><content>&lt;doc fingerprint="8c7b5d10db8a93e9"&gt;
  &lt;main&gt;
    &lt;quote&gt;Milan’s expat ‘explosion’ brings new buzz to Italy’s financial centre&lt;/quote&gt;
    &lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel anytime during your trial.&lt;/p&gt;
    &lt;p&gt;Essential digital access to quality FT journalism on any device. Pay a year upfront and save 20%.&lt;/p&gt;
    &lt;p&gt;Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.&lt;/p&gt;
    &lt;p&gt;Complete digital access to quality analysis and expert insights, complemented with our award-winning Weekend Print edition.&lt;/p&gt;
    &lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;
    &lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;
    &lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;
    &lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;
    &lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ft.com/content/f33a01dc-f873-4c62-886f-f69562fb2e46"/></entry></feed>