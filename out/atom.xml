<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-12T19:35:08.599757+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46241763</id><title>The tiniest yet real telescope I've built</title><updated>2025-12-12T19:35:16.756938+00:00</updated><content>&lt;doc fingerprint="77ef263a1fa05dfc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The tiniest yet real telescope I've built&lt;/head&gt;
    &lt;p&gt;A “relaxation” project, mostly drawn on planes to and from Norway this month, where I had to travel to setup a digital art installation in Kristiansand with friends from the digital art collective Lab212. It has been drawn with one major constraint: it must fit in the inner pocket of my jacket (well, one specific jacket), except for the rods.&lt;/p&gt;
    &lt;p&gt;This is a 3D-printed dobsonian telescope built around a 76mm/300mm parabolic mirror kit. While there are plenty of mini-scope models on the internet, I wanted something that looked like a dobson that went a bit too hard through the clothes dryer, but without compromise on what matters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Balance&lt;/item&gt;
      &lt;item&gt;Smooth movements&lt;/item&gt;
      &lt;item&gt;Rigidity&lt;/item&gt;
      &lt;item&gt;Collimatable&lt;/item&gt;
      &lt;item&gt;Focusable eyepiece holder&lt;/item&gt;
      &lt;item&gt;A minimum of style (entirely subjective)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Hardware&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PETG-CF filament&lt;/item&gt;
      &lt;item&gt;4mm carbon rods&lt;/item&gt;
      &lt;item&gt;M3 screws and M3x4.5x4.5 heat-set inserts&lt;/item&gt;
      &lt;item&gt;A spring&lt;/item&gt;
      &lt;item&gt;Nylon screws to collimate both the primary and secondary mirrors&lt;/item&gt;
      &lt;item&gt;4 magnets for the secondary&lt;/item&gt;
      &lt;item&gt;A bit of paraffin to lubricate the focuser&lt;/item&gt;
      &lt;item&gt;A lycra light shroud that also helps with delaying dew forming on the mirrors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The focuser follows Analog Sky’s recipe: the tube that receives the eyepiece is also the movement itself, with a rounded thread that prints extremely smoothly with very little play. No additional hardware needed - the eyepiece is self-held by the flexion of plastic fins.&lt;/p&gt;
    &lt;p&gt;All the holes for the rods are straight, which forces them to arch, which “locks” the structure in place.&lt;/p&gt;
    &lt;p&gt;The alt/az movements use “teflon pads” (actually gray HDPE or UHMW for furniture feet) with rubber backing, scalped and glued.&lt;/p&gt;
    &lt;p&gt;Download the 3D files on Printables • Discussion on Astrosurf&lt;/p&gt;
    &lt;p&gt;If you build it, the real trick for ease of mounting is to chamfer the carbon rods with a 1mm chamfer at both ends and seal it with CA glue. See the chamfer pic in the gallery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optical tests&lt;/head&gt;
    &lt;p&gt;Sadly, the results aren’t great. We were used to very good λ/6 or better recent buys from Aliexpress, but this one is very overcorrected. It was very smooth, with a rather good edge at the foucault, but is overcorrected by 70%. With the eyepiece I selected, putting it at 30x power, this does not show too much, and it retains its “real telescope” status. But this mirror is so small that I will not refigure it – the realuminizing costs would outweigh the entire project.&lt;/p&gt;
    &lt;p&gt;Edit dec. 12th The λ/6 aliexpress mirrors mentioned were spherical. So, great starting points to figure them, but unusable as-is. I did not yet stumble on a great parabola at a low price, and this is to be expected.&lt;/p&gt;
    &lt;p&gt;Edit as of dec. 11th : of course I did not resist re-figuring it. It now hovers around 0.9 strehl. The star test with the selected eyepiece shows nice symmetric defocused stars and I can now count individual spider web strands and distinguish the dew droplets it carries, on a nearby electrical pole, whereas I did not even see the spider web with the mirror as it was from the factory. I still need to do a proper “showable” Bath report with enough interferograms, my last test was 4 interferograms and carries a ton of noise. So it is great but I now have to get it coated, and working a mirror this small did raise a few challenges in handling it.&lt;/p&gt;
    &lt;p&gt;All test pictures below are before refiguring&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lucassifoni.info/blog/miniscope-tiny-telescope/"/><published>2025-12-12T07:35:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46241849</id><title>Guarding My Git Forge Against AI Scrapers</title><updated>2025-12-12T19:35:15.783246+00:00</updated><content>&lt;doc fingerprint="e674b8d1d8e1a093"&gt;
  &lt;main&gt;
    &lt;p&gt;In August 2024, one of my roommates and partners messaged the apartment group chat, saying she noticed the internet was slow again at our place, and my forgejo was unable to render any page in under 15 seconds.&lt;/p&gt;
    &lt;p&gt;i investigated, thinking it would be a trivial little problem to solve. Soon enough, however, i would uncover hundreds of thousands of queries a day from thousands of individual IPs, fetching seemingly-random pages in my forge every single day, all the time.&lt;/p&gt;
    &lt;p&gt;This post summarizes the practical issues that arose as a result of the onslaught of scrapers eager to download millions of commits off of my forge, and the measures i put in place to limit the damage.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Why the forge?&lt;/head&gt;
    &lt;p&gt;In the year 2025, on the web, everything is worth being scraped. Everything that came out of the mind of a human is susceptible to be snatched under the vastest labor theft scheme in the history of mankind. This very article, the second it gets published in any indexable page, will be added to countless datasets meant to train foundational large-language models. My words, your words, have contributed infinitesimal shifts of neural-network weights underpinning the largest, most grotesque accumulation of wealth seen over the lifetime of my parents, grandparents, and their grandparents themselves.&lt;/p&gt;
    &lt;p&gt;Oh, and forges have a lot of commits. See, if you have a public repository that is publicly exposed, every file in every folder for every commit will be connected. Add other options, such as a &lt;code&gt;git blame&lt;/code&gt; on a file, and multiply it by the
number of files and commits. Add the raw download link, also multiplied by the
number of commits.&lt;/p&gt;
    &lt;p&gt;Say, hypothetically, you have a linux repository available, and only with all the commits in the &lt;code&gt;master&lt;/code&gt; branch up to the &lt;code&gt;v6.17&lt;/code&gt; tag from 2025-09-18.
That's 1,383,738 commits in the range &lt;code&gt;1da177e4c3f4..e5f0a698b34e&lt;/code&gt;. How many
files is that? Well:&lt;/p&gt;
    &lt;code&gt;count=0;
while read -r rev; do
    point=$(git ls-tree -tr $rev | wc -l);
    count=$(( $count + $point ));
    printf "[%s] %s: %d (tot: %d)\n" $(git log -1 --pretty=tformat:%cs $rev) $rev $point $count;
done &amp;lt; &amp;lt;(git rev-list "1da177e4c3f4..e5f0a698b34e");
printf "Total: $count\n";
&lt;/code&gt;
    &lt;p&gt;i ran this on the 100 commits before &lt;code&gt;v6.17&lt;/code&gt;. If you have &lt;code&gt;git ls-tree -tr $rev&lt;/code&gt;, you get both files and directories counted. If you replace it with &lt;code&gt;git ls-tree -r $rev&lt;/code&gt; only shows files. i got 72024729 files, and 76798658 files and
directories. Running on the whole history of Linux's &lt;code&gt;master&lt;/code&gt; branch yields
78,483,866,182 files, and 83,627,462,277 files and directories.&lt;/p&gt;
    &lt;p&gt;Now, for a ballpark estimate of the number of pages that can be scraped if you have a copy of Linux, apply the formula:&lt;/p&gt;
    &lt;code&gt;(Ncommits * Nfiles) * 2 + (Ncommits * Nfilesandfolders) * 2 + Ncommits * 3
&lt;/code&gt;
    &lt;p&gt;That is, applied to my hypothetical Linux repository:&lt;/p&gt;
    &lt;code&gt;78483866182 * 2 + 83627462277 * 2 + 1383738 * 3 = 324,226,808,132 pages
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;*3&lt;/code&gt; accounts for the fact that every file of every commit can be scraped
raw, and &lt;code&gt;git-blame&lt;/code&gt;'d. The second part of the
formula considers every single file or folder page. The third part accounts for
the fact that every file of every commit can be diffed with its version of
every commit (in theory). The final component considers every commit summary
page.&lt;/p&gt;
    &lt;p&gt;That gives, for me, 324 billion 226 million 808 thousand and 132 pages that can be scraped. From a single repository. Assume that every scraper agent that enters one of these repositories will also take note of every other link on the page, and report it so that other agents can scrapes them. These scrapers effectively act like early 2000s web spiders that crawled the internet to index it, except they do not care about &lt;code&gt;robots.txt&lt;/code&gt;, and they will absolutely keep
scraping new links again and again with no strategy to minimize the cost on
you, as a host.&lt;/p&gt;
    &lt;head rend="h1"&gt;# The Cost of Scraping&lt;/head&gt;
    &lt;p&gt;As i am writing the original draft of this section, the longer-term measures i put in place have been removed, so i could gather up-to-date numbers to display how bad the situation is.&lt;/p&gt;
    &lt;p&gt;i pay for the electricity that powers my git Forge. Okay, actually, one of my roommate does, but we put it on the calc sheet where we keep track of who pays what (when we remember).&lt;/p&gt;
    &lt;p&gt;At the time i began fighting scrapers, my git forge ran from an old desktop computer plugged in my living room. Now, it is in our home's rackable server in a virtual machine. i never got to measure differences in power consumption when we got scraped or not scraped on the desktop machine, but i did on the rackable server. If memory serves me right, stopping the wave of scrapers reduced the power draw of the server from ~170W to ~150W.&lt;/p&gt;
    &lt;p&gt;Right now, with all the hard drives in that server spinning, and every protection off, we are drawing 200W from the power grid on that server. Constantly. By the end of this experiment, me and my roommates will have computed that the difference in power usage caused by scraping costs us ~60 euros a year.&lt;/p&gt;
    &lt;p&gt;Another tied cost is that the VM that runs the forge is figuratively suffocating from the amount of queries. Not all queries are born equal as well: requests to see the &lt;code&gt;blame&lt;/code&gt; of a file or a &lt;code&gt;diff&lt;/code&gt; between commits incurs a worse cost than
just rendering the front page of a repository. The last huge wave of scraping
left my VM at 99+% usage of 4 CPU cores and 2.5GiB of RAM,
whereas the usual levels i observe are closer to 4% usage of CPUs, and an oscillation
between 1.5GiB and 2GiB of RAM.&lt;/p&gt;
    &lt;p&gt;As i'm writing this, the VM running forgejo eats 100% of 8 CPU cores.&lt;/p&gt;
    &lt;p&gt;Additionally, the networking cost is palpable. Various monitoring tools let me see the real-time traffic statistics in our apartment. Before i put the first measures in place to thwart scraping, we could visibly see the traffic coming out of the desktop computer running my forge and out to the internet. My roommates' complaints that it slowed down the whole internet here were in fact founded: when we had multiple people watching live streams or doing pretty big downloads, they were throttled by the traffic out of the forge.1&lt;/p&gt;
    &lt;p&gt;The egress data rate of my forge's VM is at least 4MBps of data (32Mbps). Constantly.&lt;/p&gt;
    &lt;p&gt;Finally, the human cost: i have spent entire days behind my terminals trying to figure out 1) what the fuck was going on and 2) what the fuck to do about it. i have had conversations with other people who self-host their infrastructure, desperately trying to figure out workable solutions that would not needlessly impact our users. And the funniest detail is: that rackable server is in the living room, directly in front of my bedroom door. It usually purrs like an adorable cat, but, lately, it's been whirring louder and louder. i can hear it. when i'm trying to sleep.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Let's do some statistics.&lt;/head&gt;
    &lt;p&gt;i was curious to analyze the nginx logs to understand where the traffic came from and what shape it took.&lt;/p&gt;
    &lt;p&gt;As a study case, we can work on &lt;code&gt;/var/log/nginx/git.vulpinecitrus.info/&lt;/code&gt; from
&lt;code&gt;2025-11-14&lt;/code&gt; to &lt;code&gt;2025-11-19&lt;/code&gt;. Note that on &lt;code&gt;2025-11-15&lt;/code&gt; at &lt;code&gt;18:27 UTC&lt;/code&gt;, i
stopped the redirection of new agents into the Iocaine crawler maze (see
below). At &lt;code&gt;19:15 UTC&lt;/code&gt;, i removed the nginx request limit zone from the
&lt;code&gt;/Lymkwi/linux/&lt;/code&gt; path. At &lt;code&gt;19:16 UTC&lt;/code&gt; i removed the separation of log files
between IPs flagged as bots, and IPs not flagged as bots.&lt;/p&gt;
    &lt;p&gt;The three measures i progressively put in place later were: web caching (2025-11-17), manually sending IPs to a garbage generator with a rate-limit (Iocaine 2) (2025-11-14, 15 and 18), and then Iocaine 3 (2025-11-19).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Common Logs&lt;/cell&gt;
        &lt;cell role="head"&gt;Successful&lt;/cell&gt;
        &lt;cell role="head"&gt;Delayed (429)&lt;/cell&gt;
        &lt;cell role="head"&gt;Error (5XX)&lt;/cell&gt;
        &lt;cell role="head"&gt;Measures in place&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;275323&lt;/cell&gt;
        &lt;cell&gt;66517&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;71712&lt;/cell&gt;
        &lt;cell&gt;54259&lt;/cell&gt;
        &lt;cell&gt;9802&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;140713&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;65763&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;514309&lt;/cell&gt;
        &lt;cell&gt;25986&lt;/cell&gt;
        &lt;cell&gt;3012&lt;/cell&gt;
        &lt;cell&gt;Caching, eventually rate-limiting2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;335266&lt;/cell&gt;
        &lt;cell&gt;20280&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;3183&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Bot Logs&lt;/cell&gt;
        &lt;cell&gt;Successful&lt;/cell&gt;
        &lt;cell&gt;Delayed (429)&lt;/cell&gt;
        &lt;cell&gt;Error (5XX)&lt;/cell&gt;
        &lt;cell&gt;Measures in place&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-14 (bots)&lt;/cell&gt;
        &lt;cell&gt;41388&lt;/cell&gt;
        &lt;cell&gt;65517&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-15 (bots)&lt;/cell&gt;
        &lt;cell&gt;34190&lt;/cell&gt;
        &lt;cell&gt;53403&lt;/cell&gt;
        &lt;cell&gt;63&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-16 (bots)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(no bot-specific logs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-17 (bots)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(no bot-specific logs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-18 (bots)&lt;/cell&gt;
        &lt;cell&gt;390013&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19 (bots)&lt;/cell&gt;
        &lt;cell&gt;731593&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 1)&lt;/head&gt;
    &lt;p&gt;Assuming your log file is &lt;code&gt;git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;zcat git-access-2025-11-14.log.gz | grep '" 200 ' | wc -l
zcat git-access-2025-11-14.log.gz | grep '" 429 ' | wc -l
&lt;/code&gt;
    &lt;p&gt;Without spoiling too much, caching was an utter failure, and the improvement i measurement by manually rate-limiting a set of IPs (from Huawei Cloud and Alibaba) on the Linux repository only helped so much. When all protections dropped, my server became so unresponsive that backend errors (usually timeouts) spiked. Error also happened with caching, when nginx encountered an issue when buffering a reply. Overall, caching encouraged more queries overall.&lt;/p&gt;
    &lt;p&gt;Once Iocaine was deployed, the vast majority of queries were routed away from the backend, with no errors reported, and no delaying because all of the IPs i manually rate-limited were caught by Iocaine instead.&lt;/p&gt;
    &lt;p&gt;Out of all these queries, &lt;code&gt;117.64.70.34&lt;/code&gt; is the most common source of requests,
with 226023 total queries originating from the ChinaNet-Backbone ASN (AS4134).
It is followed by &lt;code&gt;136.243.228.193&lt;/code&gt; (13849 queries), an IP from Hetzner whose
hostname ironically resolves to
&lt;code&gt;crawling-gateway-136-243-228-193.dataforseo.com&lt;/code&gt;. Then, &lt;code&gt;172.17.0.3&lt;/code&gt; the
uptime prober of VC Status with 6908
queries, and &lt;code&gt;74.7.227.127&lt;/code&gt;, an IP from Microsoft's AS 8075 (6117 queries).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Day&lt;/cell&gt;
        &lt;cell role="head"&gt;Unique IP Count&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;16461&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;18639&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;41712&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;47252&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;22480&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;14230&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 2)&lt;/head&gt;
    &lt;p&gt;Assuming your log files are called &lt;code&gt;*git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;zcat \*git-access-2025-11-14.log.gz | awk '{ print $1 }' | sort | uniq -c | wc -l
&lt;/code&gt;
    &lt;p&gt;On the two days where restrictions were lifted or there was only caching, the amount of unique IPs querying the forge doubled. The more you facilitate the work of these crawlers, the more they are going to pound you. They will always try and get more out of your server than you are capable of providing.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Day&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 1&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 2&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 3&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 4&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;(226089) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(40189) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1454) - &lt;code&gt;/&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1405) - &lt;code&gt;/rail&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1174) - &lt;code&gt;/Soblow/indi-hugo&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;(35163) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18952) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(4197) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1655) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1635) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-14 (bots)&lt;/cell&gt;
        &lt;cell&gt;(40189) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(270) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(79) - &lt;code&gt;/Lymkwi/[REDACTED]&lt;/code&gt;3&lt;/cell&gt;
        &lt;cell&gt;(55) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(52) - &lt;code&gt;/oror/asm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-15 (bots)&lt;/cell&gt;
        &lt;cell&gt;(32895) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(260) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(193) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(95) - &lt;code&gt;/Lymkwi/[REDACTED]&lt;/code&gt;3&lt;/cell&gt;
        &lt;cell&gt;(48) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;(72687) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(23028) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(16779) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(5390) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(3585) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;(361632) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(74048) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18136) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(13147) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12921) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;(227019) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(46004) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12644) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12624) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(7712) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-18 (bots)&lt;/cell&gt;
        &lt;cell&gt;(261346) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(43923) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(20195) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18808) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(10134) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;(1418) - &lt;code&gt;/&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1248) - &lt;code&gt;/rail&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(356) - &lt;code&gt;/Soblow&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(31) - &lt;code&gt;/assets/img&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(25) - &lt;code&gt;/Soblow/IndigoDen&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19 (bots)&lt;/cell&gt;
        &lt;cell&gt;(448626) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(73164) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(39107) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(37107) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(25921) - &lt;code&gt;/vc-archival/YSLua&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 3)&lt;/head&gt;
    &lt;p&gt;Assuming you want data for the log file called &lt;code&gt;git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; zcat git-access-2025-11-14.log.gz | grep '" 200 ' | awk '{ print $7 }' \
    | cut -d/ -f -3 | sort | uniq -c | sort -n \
    | tail -n 5 | tac
&lt;/code&gt;
    &lt;p&gt;Big repositories with a lot of commits and a lot of files are a bountiful resource for the crawlers. Once they enter those, they will take ages to leave, at least because of the sheer amount of pages that can be generated by following the links of a repository.&lt;/p&gt;
    &lt;p&gt;Most legitimate traffic seems to be either fetching profiles (a couple of my users have their profiles listed in their fediverse bios) or the root page of my forge.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;2025-11-14 (all)&lt;/cell&gt;
        &lt;cell role="head"&gt;2025-11-15 (all)&lt;/cell&gt;
        &lt;cell role="head"&gt;2025-11-16 (all)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 1&lt;/cell&gt;
        &lt;cell&gt;(8532) - AS136907 (Huawei Clouds)&lt;/cell&gt;
        &lt;cell&gt;(8537) - AS136907 (Huawei Clouds)&lt;/cell&gt;
        &lt;cell&gt;(8535) - AS136907 (Huawei Clouds)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 2&lt;/cell&gt;
        &lt;cell&gt;(2142) - AS45899 (VNPT Corp)&lt;/cell&gt;
        &lt;cell&gt;(2107) - AS45899 (VNPT Corp)&lt;/cell&gt;
        &lt;cell&gt;(4002) - AS212238 (Datacamp Limited)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 3&lt;/cell&gt;
        &lt;cell&gt;(803) - AS153671 (Liasail Global Hongkong Limited)&lt;/cell&gt;
        &lt;cell&gt;(895) - AS153671 (Liasail Global Hongkong Limited)&lt;/cell&gt;
        &lt;cell&gt;(3504) - AS9009 (M247 Europe SRL)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 4&lt;/cell&gt;
        &lt;cell&gt;(555) - AS5065 (Bunny Communications)&lt;/cell&gt;
        &lt;cell&gt;(765) - AS45102 (Alibaba US Technology Co., Ltd.)&lt;/cell&gt;
        &lt;cell&gt;(3206) - AS3257 (GTT Communications)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Top 5&lt;/cell&gt;
        &lt;cell&gt;(390) - AS21859 (Zenlayer Inc)&lt;/cell&gt;
        &lt;cell&gt;(629) - AS5065 (Bunny Communications)&lt;/cell&gt;
        &lt;cell&gt;(2874) - AS45899 (VNPT Corp)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 4)&lt;/head&gt;
    &lt;p&gt;For this, i needed a database of IP-to-ASN data. i got one from IPInfo by registering for a free account and using their web API. i first scripted a mapping of unique IP addresses to AS number. For example, for the log file &lt;code&gt;bot-git-access-2025-11-18.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;while read ip; do
    ASN=$(curl -qfL api.ipinfo.io/lite/$ip?token=&amp;lt;my token&amp;gt; | jq -r .asn);
    printf "$ip $ASN\n" | tee -a 2025-11-18-bot.ips.txt;
done &amp;lt; &amp;lt;(zcat bot-git-access-2025-11-18.log.gz | awk '{ print $1 }' | sort | uniq)
&lt;/code&gt;
    &lt;p&gt;Then, with this map, i run:&lt;/p&gt;
    &lt;code&gt;cat 2025-11-18-bot.ips.txt | cut -d' ' -f 2 | sort | uniq -c | sort -n | tail -n 5
&lt;/code&gt;
    &lt;p&gt;So my largest hits are from Huawei Clouds (VPS provider), VPNT (a Vietnamese mobile and home ISP), Liasail Global HK Limited (a VPS/"AI-powering service" provider), Bunny Communications LLC (a broadband ISP for residential users), and Zenlayer (CDN/Cloud infrastructure provider). When i lifted all protections, Datacamp Limited (a VPS provider), GTT Communications (some sort of bullshit-looking ISP4 who, i have been informed, is in fact a backbone operator), and M247 Europe SRL (a hosting provider) suddenly appeared. If memory serves me right, Datacamp, GTT and M247 were also companies i had flagged during my initial investigation in summer 2024, and added to the manually blocked/limited IPs alongside all of Huawei Cloud and Alibaba.&lt;/p&gt;
    &lt;p&gt;Interestingly, both Liasail and Zenlayer mention that they "Power AI" on their front page. They sure do. Worryingly, VNPT and Bunny Communications are home/mobile ISPs. i cannot ascertain for sure that their IPs are from domestic users, but it seems worrisome that these are among the top scraping sources once you remove the most obviously malicious actors.&lt;/p&gt;
    &lt;head rend="h1"&gt;# The Protection Measures&lt;/head&gt;
    &lt;p&gt;i have one goal, and one constraint. My goal is that i need to protect the forge as much as possible, by means of either blocking bots or offloading the cost to my VPS provider (whose electricity i do not pay for). My only constraint: i was not going to deploy a proof-of-work-based captcha system such as Anubis. There are two reasons for these constraints:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;i personally find that forcing your visitors to have to expand more computational power to prove they're not a scraper is bad praxis. There are devices out there that legitimately want that access, but have limited computational power or features. And, yeah, there are multiple types of challenges, some of which take low-power devices into account or even those that cannot run JavaScript, but,&lt;/item&gt;
      &lt;item&gt;Scrapers can easily bypass Anubis. It's not a design flaw. Anubis is harm reduction, not pixie dust.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;i tried layers of solutions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;caching on the reverse proxy&lt;/item&gt;
      &lt;item&gt;Iocaine 2 with no classifiers, which generates garbage in reply to any query you send it&lt;/item&gt;
      &lt;item&gt;Manually redirecting IPs and rate-limiting them&lt;/item&gt;
      &lt;item&gt;Deploying Iocaine 3, with its classifiers (Nam-Shub-of-Enki)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;## Reverse-Proxy Caching&lt;/head&gt;
    &lt;p&gt;i have a confession to make: i never realized that nginx did not cache anything by default. That realization promptly came with the other realization that caching things correctly is hard. i may, some day, write about my experience of protecting a service that posted links to itself on the fediverse, so that it wouldn't slow to a crawl for ten minutes after every post.&lt;/p&gt;
    &lt;p&gt;As for the rest of these, i will be showing my solution in &lt;code&gt;nginx&lt;/code&gt;. You can,
almost certainly, figure out a way of doing exactly the same thing with any other
decent reverse proxy software.&lt;/p&gt;
    &lt;p&gt;To create a cache for my forge, i add the following line to &lt;code&gt;/etc/nginx.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;proxy_cache_path /var/cache/nginx/forge/ levels=1:2 keys_zone=forgecache:100m;
&lt;/code&gt;
    &lt;p&gt;That will create a 2-level cache called &lt;code&gt;forgecache&lt;/code&gt; that will hold 100MB of data
located at &lt;code&gt;/var/cache/nginx/forge&lt;/code&gt;. i create the directory and make &lt;code&gt;www-data&lt;/code&gt;
its owner and group.&lt;/p&gt;
    &lt;p&gt;In &lt;code&gt;/etc/nginx/sites-enabled/vcinfo-git.conf&lt;/code&gt;, where my git forge's site
configuration sits, i have a &lt;code&gt;location&lt;/code&gt; block that serves the whole root of the
service, which i modify thusly:&lt;/p&gt;
    &lt;code&gt;location / {
    proxy_cache forgecache;
    proxy_buffering on;
    proxy_cache_valid any 1h;
    add_header X-Cached $upstream_cache_status;
    expires 1h;
    proxy_ignore_headers "Set-Cookie";
    proxy_hide_header "Set-Cookie";

    # more stuff...
}
&lt;/code&gt;
    &lt;p&gt;That configuration does several things: it turns on caching and buffering at the proxy (&lt;code&gt;proxy_buffering&lt;/code&gt;),
telling it to use &lt;code&gt;forgecache&lt;/code&gt;
(&lt;code&gt;proxy_cache&lt;/code&gt;)
and keep any page valid for an hour
(&lt;code&gt;proxy_cache_valid&lt;/code&gt;).
It also adds a cookie that will let you debug whether or not a query hit or
missed the cache (&lt;code&gt;add_header&lt;/code&gt;). The &lt;code&gt;expires&lt;/code&gt; directive adds headers telling
your visitor's browser that the content they cache will also expire in an hour
(&lt;code&gt;expires&lt;/code&gt;).
Finally, the cache ignores any response header that sets a cookie
(&lt;code&gt;proxy_ignore_headers&lt;/code&gt;,
&lt;code&gt;proxy_hide_header&lt;/code&gt;),
to attempt to remove any page that could be customized for a user once they log
in.&lt;/p&gt;
    &lt;p&gt;The result? Caching was a disaster, predictably so. Caching works when the same resource is repeatedly queried, like with page assets, JavaScript, style sheets, etc. In this case, the thousands of actors querying my forge are coordinated, somehow, never (or rarely) query the same resource twice, and only download the raw HTML of the web pages.&lt;/p&gt;
    &lt;p&gt;Worse, caching messed up the display of authenticated pages. The snippets above are not enough to delineate between an authenticated session and an unauthenticated one, and it broke my forge so badly that i had to disable caching and enable the next layer early on &lt;code&gt;2025-11-17&lt;/code&gt;, or i just could not
use my forge.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Rate-Limiting on the Proxy&lt;/head&gt;
    &lt;p&gt;The next layer of protection simply consisted in enabling a global rate-limit on the most-hit repositories:&lt;/p&gt;
    &lt;code&gt;limit_req_zone wholeforge zone=wholeforge:10m rate=3r/s;

server {
    // ...
	location ^~ (/alopexlemoni/GenderDysphoria.fyi|/oror/necro|/Lymkwi/linux|/vc-archival/youtube-dl-original|/reibooru/reibooru) {
		proxy_set_header Host $host;
		proxy_set_header X-Real-IP $remote_addr;
		proxy_max_temp_file_size 2048m;

		limit_req zone=wholeforge nodelay;

		proxy_pass http://&amp;lt;my actual upstream&amp;gt;/;
	}
}
&lt;/code&gt;
    &lt;p&gt;This was achieved in two directives. The first one, &lt;code&gt;limit_req_zone&lt;/code&gt;, sits outside
the &lt;code&gt;server {}&lt;/code&gt; block and defines a zone called &lt;code&gt;wholeforge&lt;/code&gt; that stores 10MB of
state data and limits to 3 requests per second.&lt;/p&gt;
    &lt;p&gt;When this was in place, however, actually accessing the Linux repository as a normal user (or any of the often-hit repositories) became a nightmare of waiting and request timeouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Manually Redirecting to a Garbage Generator&lt;/head&gt;
    &lt;p&gt;Because caching was (predictably) useless, and rate-limiting was hindering me as well, i re-enabled the initial setup that was in place before my experiments: manually redirecting queries to a garbage generator (in this case, an old version of Iocaine). It's largely based on my initial setup following this tutorial in french.&lt;/p&gt;
    &lt;p&gt;For the purpose of this part, you do not have to know what Iocaine does precisely. In the next section, i will present my current and final setup, with an updated Iocaine that also includes a classifier to decide which queries are bots and which are regular users. For now, i will present the version where i manually chose who to return garbage to based on IP addresses.&lt;/p&gt;
    &lt;p&gt;As a little bonus, it will also include rate-limiting of those garbage-hungry bots.&lt;/p&gt;
    &lt;p&gt;i add a file called &lt;code&gt;/etc/nginx/snippets/block_bots.conf&lt;/code&gt; which contains:&lt;/p&gt;
    &lt;code&gt;if ($bot_user_agent) {
    rewrite ^ /deflagration$request_uri;
}
if ($bot_ip) {
    rewrite ^ /deflagration$request_uri;
}
location /deflagration {
    limit_req zone=bots nodelay;
    proxy_set_header Host $host;
    proxy_pass &amp;lt;garbage upstream&amp;gt;;
}
&lt;/code&gt;
    &lt;p&gt;This will force any query categorized as &lt;code&gt;bot_user_agent&lt;/code&gt; or &lt;code&gt;bot_ip&lt;/code&gt; to be
routed through to a different upstrea which serves garbage. That upstream is
also protected by rate-limiting on a zone called &lt;code&gt;bots&lt;/code&gt; which is defined in the
next bit of code. This snippet is actually meant to be included in your &lt;code&gt;server {}&lt;/code&gt;
block using the &lt;code&gt;include&lt;/code&gt; directive.&lt;/p&gt;
    &lt;p&gt;i then add the following in &lt;code&gt;/etc/nginx/conf.d/bots.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;map $http_user_agent $bot_user_agent {
    default 0;

    # from https://github.com/ai-robots-txt/ai.robots.txt/blob/main/robots.txt
    ~*amazonbot 1;
    ~*anthropic-ai  1;
    ~*applebot  1;
    ~*applebot-extended 1;
    ~*brightbot 1;
    ~*bytespider  1;
    ~*ccbot 1;
    ~*chatgpt-user  1;
    ~*claude-web  1;
    ~*claudebot 1;
    ~*cohere-ai 1;
    ~*cohere-training-data-crawler  1;
    ~*crawlspace  1;
    ~*diffbot 1;
    ~*duckassistbot 1;
    ~*facebookbot 1;
    ~*friendlycrawler 1;
    ~*google-extended 1;
    ~*googleother 1;
    ~*googleother-image 1;
    ~*googleother-video 1;
    ~*gptbot  1;
    ~*iaskspider  1;
    ~*icc-crawler 1;
    ~*imagesiftbot  1;
    ~*img2dataset 1;
    ~*isscyberriskcrawler 1;
    ~*kangaroo  1;
    ~*meta-externalagent  1;
    ~*meta-externalfetcher  1;
    ~*oai-searchbot 1;
    ~*omgili  1;
    ~*omgilibot 1;
    ~*pangubot  1;
    ~*perplexitybot 1;
    ~*petalbot  1;
    ~*scrapy  1;
    ~*semrushbot-ocob 1;
    ~*semrushbot-swa  1;
    ~*sidetrade 1;
    ~*timpibot  1;
    ~*velenpublicwebcrawler 1;
    ~*webzio-extended 1;
    ~*youbot  1;

    # Add whatever other pattern you want down here
}

geo $bot_ip {
    default 0;

    # Add your IP ranges here
}

# Rate-limiting setup for bots
limit_req_zone bots zone=bots:30m rate=1r/s;

# Return 429 (Too Many Requests) to slow them down
limit_req_status 429;
&lt;/code&gt;
    &lt;p&gt;That bit of configuration does a mapping between the client IP and a variable called &lt;code&gt;bot_ip&lt;/code&gt;, and the client's user agent and a variable called
&lt;code&gt;bot_user_agent&lt;/code&gt;. When a known pattern listed in those blocks is found, the
corresponding variable is flipped to the provided value (here, &lt;code&gt;1&lt;/code&gt;). Otherwise,
it stays &lt;code&gt;0&lt;/code&gt;. Then, we define the rate-limiting zone that is used to slow down
the bots so they don't feed on slop too fast. You will then need to install the
&lt;code&gt;http-geoip2&lt;/code&gt; nginx module (on Debian-based distributions, something like &lt;code&gt;apt install libnginx-mod-http-geoip2&lt;/code&gt; will do).&lt;/p&gt;
    &lt;p&gt;Once that is done, add the following line to the &lt;code&gt;server&lt;/code&gt; block of every site
you want to protect:&lt;/p&gt;
    &lt;code&gt;include /etc/nginx/snippets/block_bots.conf;
&lt;/code&gt;
    &lt;p&gt;And when you feel confident enough, roll a &lt;code&gt;nginx -t&lt;/code&gt; and reload the unit for
&lt;code&gt;nginx&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Now, if you're using &lt;code&gt;caddy&lt;/code&gt; or any other reverse proxy, there are probably
similar mechanisms available. You can go and peruse the documentation of Iocaine,
or look online for specific tutorials that, i am sure, other people have made
better than i would.&lt;/p&gt;
    &lt;p&gt;Immediately after enabling it, and shoving all the IPs from Alibaba Cloud and Huawei Cloud in the bot config file, the activity slowed down on my server. Power usage went down to ~180W, CPU usage to rougly 60%, and it stopped making a hellish noise.&lt;/p&gt;
    &lt;p&gt;As the stats showed earlier, however, a lot of traffic was still hitting the server itself. Even weirder, there were still occasional spikes, every 3 hour, that lasted about one and a half hour, where the server would whirr and forgejo suffocate again.&lt;/p&gt;
    &lt;p&gt;Bots were still hitting my server, and there was no clear source for it.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Automatically Classifying Bots and Poisoning Them: Iocaine and Nam-Shub-of-Enki&lt;/head&gt;
    &lt;p&gt;So far, the steps i showed so far help when a single IP is hammering at your forge, or when someone is clearly scraping you from an Autonomous System that you do not mind blocking. Sadly, as i've showed above in Table 4, a surprising amount of scraping comes from broadband addresses. i can assemble lists of IPs as big as i want, or block entire ASNs, but i would love to have a per-query way of determining if a query looks legitimate.&lt;/p&gt;
    &lt;p&gt;The next steps of protection will rely on categorizing a source IP based on its the credibility of its user agent. This mechanism is largely based on the documentation for Iocaine 3.x. We finally get to talk about Iocaine!&lt;/p&gt;
    &lt;p&gt;Iocaine is a tool that traps scrapers in a maze of meaningless pages that endlessly lead to more meaningless pages. The content of these pages is generated using a Markov chain, based on a corpus of texts given to the software. Iocaine (specifically all versions after 3 at least5) is a middleware, in the sense that it works by being placed on the line between your reverse proxy and the service. Your reverse proxy will first begin by redirecting traffic to Iocaine, and, if Iocaine deems a query legitimate, it will return a &lt;code&gt;421 Misdirected Request&lt;/code&gt; back at your reverse-proxy. The
latter must then catch it, and use the real upstream as a fallback. If
Iocaine's Nam-Shub-of-Enki6 decides query came from a bogus or otherwise undesirable source, it
will happily reply &lt;code&gt;200 OK&lt;/code&gt; and send generated garbage.&lt;/p&gt;
    &lt;p&gt;My setup lodges Iocaine 3 between nginx and my forge, following the Iocaine documentation to use the container version. i recommend you follow it, and then add the next little things to enable categorization statistics, and prevent the logging they're based on from blowing up your storage:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In &lt;code&gt;etc/config.d/03-nam-shub-of-enki.kdl&lt;/code&gt;, change the logging block to:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;logging {
    enable #true
    classification {
        enable #true
    }
}
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In &lt;code&gt;docker-compose.yaml&lt;/code&gt;, add the following bits to limit classification logging to 50MB:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;services:
  iocaine:
    # The things you already have here...
    # ...
    env:
      - RUST_LOG=iocaine=info
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
&lt;/code&gt;
    &lt;p&gt;My checks block in Nam-Shub-of-Enki is as such:&lt;/p&gt;
    &lt;code&gt;checks {
    disable cgi-bin-trap

    asn {
        database-path "/data/ipinfo_lite.mmdb"
        asns "45102" "136907"
    }
    ai-robots-txt {
        path "/data/ai.robots.txt-robots.json"
    }
    generated-urls {
        identifiers "deflagration"
    }
    big-tech {
        enable #true
    }
    commercial_scrapers {
        enable #true
    }
}
&lt;/code&gt;
    &lt;p&gt;I snatched a copy of the latest ipinfo ASN database for free and blocked AS52102 (Alibaba) and AS136907 (Huawei Clouds).&lt;/p&gt;
    &lt;p&gt;On 2025-11-18 at 00:00:29 UTC+1, i enabled Iocaine with the Nam-Shub-of-Enki classifier in front of my whole forge. Immediately, my server was no longer hammered. Power draw went down to just above 160W.&lt;/p&gt;
    &lt;p&gt;One problem i noticed however, while trying to deploy the artifact for this blog post on my forge, is that Iocaine causes issues when huge &lt;code&gt;PUT&lt;/code&gt;/&lt;code&gt;PATCH&lt;/code&gt;/&lt;code&gt;POST&lt;/code&gt;
requests with large bodies are piped through it: it will hang up before the
objects are entirely written. i am trying to figure out a way of only redirecting
&lt;code&gt;HEAD&lt;/code&gt; and &lt;code&gt;GET&lt;/code&gt; requests to Iocaine in nginx, like is done in the Caddy example
of the Iocaine documentation.&lt;/p&gt;
    &lt;p&gt;What i ended up settling on requires a bit of variable mapping. At the start of your site configuration, before the &lt;code&gt;server {}&lt;/code&gt; block:&lt;/p&gt;
    &lt;code&gt;map $request_method $upstream_location {
	GET	&amp;lt;iocaine upstream&amp;gt;;
	HEAD	&amp;lt;iocaine upstream&amp;gt;;
	default	&amp;lt;your actual upstream&amp;gt;;
}

map $request_method $upstream_log {
	GET	bot_access;
	HEAD	bot_access;
	default	access;
}
&lt;/code&gt;
    &lt;p&gt;Then, in the block that does the default location, write:&lt;/p&gt;
    &lt;code&gt;	location / {
	    proxy_cache off;
	    access_log /var/log/nginx/$upstream_log.log combined;
	    proxy_intercept_errors on;
	    error_page 421 = @fallback;
	    proxy_set_header Host $host;
	    proxy_set_header X-Real-IP $remote_addr;
	    proxy_pass http://$upstream_location;
	}
&lt;/code&gt;
    &lt;p&gt;That is, replace the upstream in &lt;code&gt;proxy_pass&lt;/code&gt; with the upstream decided by the
variable mapping, and, while we're at it, use &lt;code&gt;$upstream_log&lt;/code&gt; to know which log
will be the final one for that request. i differentiate between &lt;code&gt;bot_access.log&lt;/code&gt;
and &lt;code&gt;access.log&lt;/code&gt; to gather my statistics, so the difference matters to me. Change
the variables to suit the way you do it (or remove it, if you don't distinguish
clients in your log files).&lt;/p&gt;
    &lt;head rend="h1"&gt;# Monitoring Iocaine&lt;/head&gt;
    &lt;p&gt;Currently, on 2025-11-30 at 16:33:00 UTC+1, Iocaine has served 38.16GB of garbage. Over the past hour, 152.11MB of such data was thrown back at undesirable visitors. 3.39GB over the past day, 22.22GB over the past week. You can get the snippet that describes my Iocaine-specific Grafana views here.&lt;/p&gt;
    &lt;p&gt;The vast majority of undesirable queries come from Claude, OpenAI, and Disguised Bots. Claude and OpenAI are absolutely gluttonous, and, once they have access to a ton of pages, they will greedily flock to fetch them like pigeons being fed breadcrumbs laced with strychnine.&lt;/p&gt;
    &lt;p&gt;AI bot scrapers (&lt;code&gt;ai.robots.txt&lt;/code&gt;) maintain a constant 920~930 query per minute
(15-ish QPS) over the 6 domains i have protected with Iocaine, including the
forge.&lt;/p&gt;
    &lt;p&gt;There is also a low hum of a mix of commercial scrapers (~1 request every two second), big tech crawlers (Facebook, Google, etc, about 2QPS or 110 query/min), and, especially, fake browsers.&lt;/p&gt;
    &lt;p&gt;Classifying fake browsers is where Iocaine really shines, specifically thanks to the classifiers implemented via Nam-Shub-of-Enki. The faked bots classifier detects the likelihood that the user agent reported by the client is bullshit, generated from a list of technologies mashed together. For example, if your client reports a user agent for a set of software that never supported HTTP2, or never actually existed together, or is not even released yet, it will get flagged. Think, for example, Windows NT 4 running Chrome, pretending to be able to do TLS1.3.&lt;/p&gt;
    &lt;p&gt;The background-noise level of such queries is usually 140~160 queries per minute (or 2~3 QPS). However, notice those spikes in the graph above?&lt;/p&gt;
    &lt;head rend="h2"&gt;## The Salves of Queries&lt;/head&gt;
    &lt;p&gt;For a while during my experiments i noticed those pillars of queries. My general nginx statistics would show a sharp increase of connections, with an iniital ramp-up, and a stable-ish plateau lasting about one and a half hour, before suddenly stopping. It would then repeat again, roughly three hours later.&lt;/p&gt;
    &lt;p&gt;Between October 29th and November 19th, and on November 28th, these spikes would constantly show up. As soon as i got Iocaine statistics running, it would flag all of those queries as faked browsers.&lt;/p&gt;
    &lt;p&gt;i investigated those spikes in particular, because they baffled and scared me: the regularity with which they probed me, and the sharpness of the ramp-up and halts, made me afraid that someone, somewhere, was organizing thousands of IPs to specifically take turns at probing websites. i have not reached any solid conclusions, beyond the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The initial phase of an attack wave begins with a clear exponential ramp-up&lt;/item&gt;
      &lt;item&gt;The ramp-up stops when the server starts either throwing errors, or the response latency reaches a given threshold&lt;/item&gt;
      &lt;item&gt;Every wave of attack lasts roughly one hour and a half&lt;/item&gt;
      &lt;item&gt;An individual IP will often contribute no more than one query, but it can reach 50 to 60 queries per IP&lt;/item&gt;
      &lt;item&gt;The same 15 or so ASN keep showing up, with five regular leaders in IP count: &lt;list rend="ol"&gt;&lt;item&gt;AS212238: Datacamp Limited&lt;/item&gt;&lt;item&gt;AS3257: GTT Communications&lt;/item&gt;&lt;item&gt;AS9009: M247 Europe SRL&lt;/item&gt;&lt;item&gt;AS203020: HostRoyale Technologies Pvt Ltd&lt;/item&gt;&lt;item&gt;AS210906: UAB "Bite Lietuva" (a Lithuanian ISP)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of those as service providers. My working theory at the moment is that someone registered thousands of cheap servers in many different companies, and are selling access to them as web proxies for scraping and scanning. i will probably write something up later when i have properly investigated that specific phenomenon.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Conclusion&lt;/head&gt;
    &lt;p&gt;Self-hosting anything that is deemed "content" openly on the web in 2025 is a battle of attrition between you and forces who are able to buy tens of thousands of proxies to ruin your service for data they can resell.&lt;/p&gt;
    &lt;p&gt;This is depressing. Profoundly depressing. i look at the statistics board for my reverse-proxy and i never see less than 96.7% of requests classified as bots at any given moment. The web is filled with crap, bots that pretend to be real people to flood you. All of that because i want to have my little corner of the internet where i put my silly little code for other people to see.&lt;/p&gt;
    &lt;p&gt;i have to learn to protect myself from industrial actors in order to put anything online, because anything a person makes is valuable, and that value will be sucked dry by every tech giant to be emulsified, liquified, strained, and ultimately inexorably joined in an unholy mesh of learning weights.&lt;/p&gt;
    &lt;p&gt;This experience has rather profoundly radicalized the way i think about technology. Sanitized content can be chewed on and shat out by companies from training, but their AI tools will never swear. They will never use a slur. They will never have a revolutionary thought. Despite being amalgamation of shit rolled up in the guts of the dying capitalist society, they are sanitized to hell and beyond.&lt;/p&gt;
    &lt;p&gt;The developer of Iocaine put it best when explaining why Iocaine has absolutely unhinged identifiers (such as &lt;code&gt;SexDungeon&lt;/code&gt;, &lt;code&gt;PipeBomb&lt;/code&gt;, etc) is that they will all trigger "safeguard"
mechanisms in commercial AI tools: absolutely no coding agent will accept
analyzing and explaining code where the memory allocator's free function is
called &lt;code&gt;liberate_palestine&lt;/code&gt;. i bet that if i described, in graphic details, in
the comments of this page, the different ways being a furry intersects with my
sexuality, that no commercial scraper would even dare ingest this page.&lt;/p&gt;
    &lt;p&gt;Fuck tech companies. Fuck "AI". Fuck the corporate web.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vulpinecitrus.info/blog/guarding-git-forge-ai-scrapers/"/><published>2025-12-12T07:51:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46242795</id><title>Training LLMs for Honesty via Confessions</title><updated>2025-12-12T19:35:15.479506+00:00</updated><content>&lt;doc fingerprint="bd9c9508d0f342ab"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 8 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Training LLMs for Honesty via Confessions&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.&lt;lb/&gt;In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.&lt;lb/&gt;To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.08093"/><published>2025-12-12T10:37:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46242871</id><title>Koralm Railway</title><updated>2025-12-12T19:35:14.517995+00:00</updated><content>&lt;doc fingerprint="715d5db4db36ea94"&gt;
  &lt;main&gt;
    &lt;p&gt;Crossing the Koralpe massif more quickly and with more comfort. That’s what the future of train travel from Graz to Klagenfurt looks like. With the Koralm Railway, you will arrive at your destination even quicker. The fastest connection will shrink from three hours to just 45 minutes. Western Styria and southern Carinthia can be reached even more easily – as can our neighbouring countries Hungary and Italy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Koralm Railway connects Europe&lt;/head&gt;
    &lt;p&gt;The economy is also benefiting from the construction of the new Koralm Railway. As part of the new Southern Line, it is strengthening the Baltic-Adriatic Corridor in Europe. Transporting goods in Austria by train is becoming more attractive, which in turn is allowing our operations to remain competitive internationally. And the environment to breathe: Each tonne of freight moved by rail generates around 15 times less CO2 emissions than transporting it by lorry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your benefits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Shorter journey times&lt;/item&gt;
          &lt;item&gt;Better access to southern Austria&lt;/item&gt;
          &lt;item&gt;Twenty-three contemporary railway stations and stops&lt;/item&gt;
          &lt;item&gt;Economic stimuli and jobs in the region&lt;/item&gt;
          &lt;item&gt;Long-term relief for the environment&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://infrastruktur.oebb.at/en/projects-for-austria/railway-lines/southern-line-vienna-villach/koralm-railway"/><published>2025-12-12T10:50:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46243543</id><title>The Tor Project is switching to Rust</title><updated>2025-12-12T19:35:14.404155+00:00</updated><content>&lt;doc fingerprint="b743d961f031579a"&gt;
  &lt;main&gt;
    &lt;p&gt;The Tor Project has been busy with the rustification of their offering for quite some time now.&lt;/p&gt;
    &lt;p&gt;If you have used Tor Browser, you know what it does. Anonymous browsing through encrypted relay chains. The network itself has been running since the early 2000s. All of it is built on C.&lt;/p&gt;
    &lt;p&gt;But that C codebase is an issue. It is known to have buffer overflows, use-after-free bugs, and memory corruption vulnerabilities. That is why they introduced Arti, a Rust rewrite of Tor that tackles these flaws by leveraging the memory safety of the programming language.&lt;/p&gt;
    &lt;p&gt;A new release of Arti just dropped last week, so let's check it out!&lt;/p&gt;
    &lt;head rend="h2"&gt;Arti 1.8.0: What's New?&lt;/head&gt;
    &lt;p&gt;We begin with the main highlight of this release, the rollout of the circuit timeout rework that was laid out in proposal 368. Tor currently uses something called Circuit Dirty Timeout (CDT). It is a single timer that controls when your connection circuits become unavailable and when they close down.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it is predictable. Someone monitoring traffic can spot these patterns and potentially track your activity. Arti 1.8.0 fixes this by implementing usage-based timeouts with separate timers. One handles when circuits accept new connections. Another closes idle circuits at random times instead of fixed intervals.&lt;/p&gt;
    &lt;p&gt;This should reduce the risk of fingerprinting from predictable timeout behavior.&lt;/p&gt;
    &lt;p&gt;Next up is the new experimental &lt;code&gt;arti hsc ctor-migrate&lt;/code&gt; command that lets onion service operators migrate their restricted discovery keys from the C-based Tor to Arti's keystore.&lt;/p&gt;
    &lt;p&gt;These keys handle client authorization for onion services. The command transfers them over without requiring operators to do the manual legwork. The release also delivers improvements for routing architecture, protocol implementation, directory cache support, and OR port listener configuration.&lt;/p&gt;
    &lt;p&gt;You can go through the changelog to learn more about the Arti 1.8.0 release.&lt;/p&gt;
    &lt;p&gt;Via: Sam Bent&lt;/p&gt;
    &lt;p&gt;Suggested Read 📖: Is Helium the Browser Brave Was Meant to Be?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://itsfoss.com/news/tor-rust-rewrite-progress/"/><published>2025-12-12T12:35:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46243883</id><title>Fedora: Open-source repository for long-term digital preservation</title><updated>2025-12-12T19:35:14.110802+00:00</updated><content>&lt;doc fingerprint="3cd1965b918fe919"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fedora is the flexible, standards-based, open-source repository software built to support long-term digital preservation.&lt;/head&gt;
    &lt;head rend="h2"&gt;Why Choose Fedora?&lt;/head&gt;
    &lt;head rend="h3"&gt;Flexibility&lt;/head&gt;
    &lt;p&gt;Fedora can store, preserve and provide access to any type of digital object. Through native support for semantic relationships, you are free to model your content to suit your needs and build out object relationships that are defined entirely by you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Standards Based&lt;/head&gt;
    &lt;p&gt;Partnered with Fedora’s published API and the adoption of the OCFL standard for persistence, Fedora uses a variety of globally accepted standard web conventions for interacting with the application.&lt;/p&gt;
    &lt;head rend="h3"&gt;Global User Community&lt;/head&gt;
    &lt;p&gt;The Fedora program has been in existence for 20+ years, and our users represent an engaged, supportive and invested global community focused on sustainability and growth.&lt;/p&gt;
    &lt;head rend="h4"&gt;JOIN OTHER INSTITUTIONS ACROSS THE WORLD&lt;/head&gt;
    &lt;head rend="h2"&gt;Fedora Features&lt;/head&gt;
    &lt;head rend="h3"&gt;Enhanced Digital Preservation Support&lt;/head&gt;
    &lt;p&gt;A transparent, standardized persistence layer based on the Oxford Common File Layout (OCFL), which supports long-term digital preservation best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Improved Performance &amp;amp; Scale&lt;/head&gt;
    &lt;head rend="h3"&gt;Integrations&lt;/head&gt;
    &lt;head rend="h2"&gt;Join Our Community&lt;/head&gt;
    &lt;head rend="h2"&gt;Want to support Fedora? Become a Member Today.&lt;/head&gt;
    &lt;head rend="h3"&gt;WHY BECOME A MEMBER&lt;/head&gt;
    &lt;head rend="h3"&gt;What Fedora Members Receive&lt;/head&gt;
    &lt;head rend="h3"&gt;HOW TO BECOME A MEMBER&lt;/head&gt;
    &lt;head rend="h2"&gt;Fedora Resources&lt;/head&gt;
    &lt;head rend="h2"&gt;Join the Fedora Community Today&lt;/head&gt;
    &lt;p&gt;Long-term digital preservation starts here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fedorarepository.org/"/><published>2025-12-12T13:23:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46243904</id><title>SQLite JSON at Full Index Speed Using Generated Columns</title><updated>2025-12-12T19:35:14.011346+00:00</updated><content/><link href="https://www.dbpro.app/blog/sqlite-json-virtual-columns-indexing"/><published>2025-12-12T13:25:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46244378</id><title>BpfJailer: eBPF Mandatory Access Control [pdf]</title><updated>2025-12-12T19:35:13.217637+00:00</updated><content/><link href="https://lpc.events/event/19/contributions/2159/attachments/1833/3929/BpfJailer%20LPC%202025.pdf"/><published>2025-12-12T14:20:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46244922</id><title>CM0 – a new Raspberry Pi you can't buy</title><updated>2025-12-12T19:35:13.124617+00:00</updated><content>&lt;doc fingerprint="bf35175918e780e8"&gt;
  &lt;main&gt;
    &lt;p&gt;This little postage stamp is actually a full Raspberry Pi Zero 2, complete with eMMC storage and WiFi.&lt;/p&gt;
    &lt;p&gt;But you can't get one. Well, not unless you buy the CM0NANO development board from EDAtec, or you live in China.&lt;/p&gt;
    &lt;p&gt;This little guy doesn't have an HDMI port, Ethernet, or even USB. It's a special version of the 'Compute Module' line of boards. Little Raspberry Pi 'System on Modules' (SoMs), they're called.&lt;/p&gt;
    &lt;p&gt;Compute Modules are entire Linux computers about the size of a regular desktop CPU that you 'plug in' to another board, to give it life.&lt;/p&gt;
    &lt;p&gt;Compute modules are everywhere, in kiosks, signage, 3D printers, and even the new Ableton Move. If you just need a little bit of Linux for networking and remote control, these are perfect for that.&lt;/p&gt;
    &lt;p&gt;And the CM0 is now the smallest version, a little bigger than a postage stamp.&lt;/p&gt;
    &lt;p&gt;But unlike all the other Compute Modules, the CM0 has castellated edges like a Pico. That way, a company integrating this into their product can just pick and place it and solder it onto their main PCB, instead of working with more delicate board-to-board connectors.&lt;/p&gt;
    &lt;p&gt;But why is this only in China? I'll get to that, but first I wanted to thank EDAtec for sending a CM0 and their CM0NANO dev board for testing. Without them, I don't think I'd ever be able to show these Pis to you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;I posted this story to my YouTube channel, but if you're on the blog already, chances are you favor reading over video, so scroll on!&lt;/p&gt;
    &lt;head rend="h2"&gt;ED-CM0NANO&lt;/head&gt;
    &lt;p&gt;EDAtec's CM0NANO seems to be the official IO board for the CM0. It breaks out every feature on the RP3A0 chip at the heart of the Pi Zero 2 and CM0.&lt;/p&gt;
    &lt;p&gt;There's 10/100 Ethernet through a little USB to Ethernet chip (CoreChips SR9900A), two USB 2.0 ports, full-size HDMI, and USB-C for power and flashing the eMMC. Then there are display and camera connectors, GPIO, and a few more headers.&lt;/p&gt;
    &lt;p&gt;To flash the onboard eMMC, I had to switch the &lt;code&gt;RPI_BOOT_SW&lt;/code&gt; switch towards the RTC battery slot, then use rpiboot to mount it on my Mac. Then I used Raspberry Pi Imager to flash Pi OS 13 on it.&lt;/p&gt;
    &lt;p&gt;The eMMC on here is very slow compared to what I'm used to with the Pi 5 generation, like on the CM5. Its top speed seems to be around 19-20 MB/sec.&lt;/p&gt;
    &lt;p&gt;Once it's flashed, it's a full Linux computer, complete with Raspberry Pi's desktop environment.&lt;/p&gt;
    &lt;p&gt;EDAtec has a firmware support package you can install from their package repository, and once that's done, I did what nobody should do on this small of a computer: fired up Chromium.&lt;/p&gt;
    &lt;p&gt;Browsing the web on here is almost completely out of the question, since it only has 512 Megs of RAM—which is so little it pops a warning saying Chromium should only be used with 1 GB of more of RAM!&lt;/p&gt;
    &lt;p&gt;I did try browsing this website, and it took something like a minute to just quit the browser, after I was clicking the X to close the tab over and over again!&lt;/p&gt;
    &lt;p&gt;But with WiFi, Ethernet, USB, HDMI, and everything else the Pi ecosystem has to offer, some products that just want to slap a well-supported Linux environment on top of their product (and not integrate an SoC, memory, storage, and wireless chip) now have this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Global distribution possibilities&lt;/head&gt;
    &lt;p&gt;Do I think companies and makers here in the US and over in other parts of the world would also benefit from the CM0? Yes. Do I think it'll happen? Doubtful.&lt;/p&gt;
    &lt;p&gt;The Zero 2 W and CM0 share something in common, besides their entire architecture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Zero 2 W was introduced at the beginning of the COVID-induced chip shortages.&lt;/item&gt;
      &lt;item&gt;The CM0 was introduced right before the great RAM shortages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When Hackster asked Eben Upton about global availability, he was noncommittal:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;No plans to make it available outside China at the moment, but we'll see how we get on.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That was back before the RAM shortages got bad.&lt;/p&gt;
    &lt;p&gt;I followed up asking a Pi engineer about it, and it sounds like one big problem is the RP3A0 chip that integrates an LPDDR2 RAM chip stacked on top of the Pi's SoC.&lt;/p&gt;
    &lt;p&gt;He said the CM0 would compete with Pi Zero 2 for LPDDR2 memory, which is in shorter supply these days (it's not being produced anymore, so stocks will only become more limited over time), and they want to make sure the popular Zero 2 W can stay in stock for makers and education.&lt;/p&gt;
    &lt;p&gt;The CM0 is targeted squarely at the lower end market, integrated into products built on assembly lines. So because of that, it's anyone's guess if the CM0 will ever make it out of China.&lt;/p&gt;
    &lt;p&gt;I'm not doing a full review of the board here, because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It's practically the same as the Pi Zero 2 W, which I already reviewed.&lt;/item&gt;
      &lt;item&gt;It's not like you can get one (standalone, at least) anyway, at least not for the foreseeable future.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think there was a chance, before the DRAM manufacturers went all-in on an AI cash grab, but for now, stick to the Pi Zero 2's that you're used to.&lt;/p&gt;
    &lt;p&gt;You can find a little more detail and benchmark results on my sbc-reviews issue for the CM0.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/cm0-new-raspberry-pi-you-cant-buy"/><published>2025-12-12T15:19:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46245331</id><title>Framework Raises DDR5 Memory Prices by 50% for DIY Laptops</title><updated>2025-12-12T19:35:13.014828+00:00</updated><content>&lt;doc fingerprint="32ec5f9b3cb39588"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Framework Raises DDR5 Memory Prices By 50% For DIY Laptops&lt;/head&gt;
    &lt;p&gt; Framework Computer had worked to keep their memory prices lower than other laptop vendors amid the ongoing memory shortages throughput the industry worldwide. But today they've finally had to cave in and increase their DDR5 memory modules for the Framework Laptop DIY Editions by 50%. &lt;lb/&gt;Due to the ongoing price hikes around system memory with shortages throughout the supply chain, Framework raised their DDR5 memory options today by 50% for the Framework Laptop DIY Edition. Framework Computer is keeping the prior prices for existing pre-orders and also is foregoing any price changes for their pre-built laptops or the Framework Desktop. Framework Computer also lets you order DIY laptops without any memory at all if so desired for re-using existing modules or should you score a deal elsewhere.&lt;lb/&gt;Due to their memory pricing said to be more competitive below market rates, they also adjusted their return policy to prevent scalpers from purchasing DIY Edition laptops with memory while then returning just the laptops. The DDR5 must be returned now with DIY laptop order returns.&lt;lb/&gt;More details on Framework Computer needing to begin raising system memory prices can be found via the Framework Blog.&lt;/p&gt;
    &lt;p&gt;Due to the ongoing price hikes around system memory with shortages throughout the supply chain, Framework raised their DDR5 memory options today by 50% for the Framework Laptop DIY Edition. Framework Computer is keeping the prior prices for existing pre-orders and also is foregoing any price changes for their pre-built laptops or the Framework Desktop. Framework Computer also lets you order DIY laptops without any memory at all if so desired for re-using existing modules or should you score a deal elsewhere.&lt;/p&gt;
    &lt;p&gt;Due to their memory pricing said to be more competitive below market rates, they also adjusted their return policy to prevent scalpers from purchasing DIY Edition laptops with memory while then returning just the laptops. The DDR5 must be returned now with DIY laptop order returns.&lt;/p&gt;
    &lt;p&gt;More details on Framework Computer needing to begin raising system memory prices can be found via the Framework Blog.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.phoronix.com/news/Framework-50p-DDR5-Memory"/><published>2025-12-12T15:58:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46245398</id><title>Epic celebrates "the end of the Apple Tax" after court win in iOS payments case</title><updated>2025-12-12T19:35:12.613480+00:00</updated><content>&lt;doc fingerprint="8684f31c96ad1135"&gt;
  &lt;main&gt;
    &lt;p&gt;Back in April, District Court Judge Yvonne Gonzalez Rogers delivered a scathing judgment finding that Apple was in “willful violation” of her 2021 injunction intended to open up iOS App Store payments. That contempt of court finding has now been almost entirely upheld by the Ninth Circuit Court of Appeals, a development that Epic Games’ Tim Sweeney tells Ars he hopes will “do a lot of good for developers and start to really change the App Store situation worldwide, I think.”&lt;/p&gt;
    &lt;p&gt;The ruling, signed by a panel of three appellate court judges, affirmed that Apple’s initial attempts to charge a 27 percent fee to iOS developers using outside payment options “had a prohibitive effect, in violation of the injunction.” Similarly, Apple’s restrictions on how those outside links had to be designed were overly broad; the appeals court suggests that Apple can only ensure that internal and external payment options are presented in a similar fashion.&lt;/p&gt;
    &lt;p&gt;The appeals court also agreed that Apple acted in “bad faith” by refusing to comply with the injunction, rejecting viable, compliant alternatives in internal discussions. And the appeals court was also not convinced by Apple’s process-focused arguments, saying the district court properly evaluated materials Apple argued were protected by attorney-client privilege.&lt;/p&gt;
    &lt;p&gt;While the district court barred Apple from charging any fees for payments made outside of its App Store, the appeals court now suggests that Apple should still be able to charge a “reasonable fee” based on its “actual costs to ensure user security and privacy.” It will be up to Apple and the district court to determine what that kind of “reasonable fee” should look like going forward.&lt;/p&gt;
    &lt;p&gt;Speaking to reporters Thursday night, though, Epic founder and CEO Tim Sweeney said he believes those should be “super super minor fees,” on the order of “tens or hundreds of dollars” every time an iOS app update goes through Apple for review. That should be more than enough to compensate the employees reviewing the apps to make sure outside payment links are not scams and lead to a system of “normal fees for normal businesses that sell normal things to normal customers,” Sweeney said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/tech-policy/2025/12/epic-celebrates-the-end-of-the-apple-tax-after-appeals-court-win-in-ios-payments-case/"/><published>2025-12-12T16:04:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46245622</id><title>String Theory Inspires a Brilliant, Baffling New Math Proof</title><updated>2025-12-12T19:35:12.312213+00:00</updated><content>&lt;doc fingerprint="3e75981c062e07f4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;String Theory Inspires a Brilliant, Baffling New Math Proof&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In August, a team of mathematicians posted a paper claiming to solve a major problem in algebraic geometry — using entirely alien techniques. It instantly captivated the field, stoking excitement in some mathematicians and skepticism in others.&lt;/p&gt;
    &lt;p&gt;The result deals with polynomial equations, which combine variables raised to powers (like y = x or x2 − 3xy = z2). These equations are some of the simplest and most ubiquitous in mathematics, and today, they’re fundamental to lots of different areas of study. As a result, mathematicians want to study their solutions, which can be represented as geometric shapes like curves, surfaces and higher-dimensional objects called manifolds.&lt;/p&gt;
    &lt;p&gt;There are infinitely many types of polynomial equations that mathematicians want to tame. But they all fall into one of two basic categories — equations whose solutions can be computed by following a simple recipe, and equations whose solutions have a richer, more complicated structure. The second category is where the mathematical juice is: It’s where mathematicians want to focus their attention to make major advances.&lt;/p&gt;
    &lt;p&gt;But after sorting just a few types of polynomials into the “easy” and “hard” piles, mathematicians got stuck. For the past half-century, even relatively simple-looking polynomials have resisted classification.&lt;/p&gt;
    &lt;p&gt;Then this summer, the new proof appeared. It claimed to end the stalemate, offering up a tantalizing vision for how to classify lots of other types of polynomials that have until now seemed completely out of reach.&lt;/p&gt;
    &lt;p&gt;The problem is that no one in the world of algebraic geometry understands it. At least, not yet. The proof relies on ideas imported from the world of string theory. Its techniques are wholly unfamiliar to the mathematicians who have dedicated their careers to classifying polynomials.&lt;/p&gt;
    &lt;p&gt;Some researchers trust the reputation of one of the paper’s authors, a Fields medalist named Maxim Kontsevich. But Kontsevich also has a penchant for making audacious claims, giving others pause. Reading groups have sprung up in math departments across the world to decipher the groundbreaking result and relieve the tension.&lt;/p&gt;
    &lt;p&gt;This review may take years. But it’s also revived hope for an area of study that had stalled. And it marks an early victory for a broader mathematical program that Kontsevich has championed for decades — one that he hopes will build bridges between algebra, geometry and physics.&lt;/p&gt;
    &lt;p&gt;“The general perception,” said Paolo Stellari, a mathematician at the University of Milan who was not involved in the work, “is that we might be looking at a piece of the mathematics of the future.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Rational Approach&lt;/head&gt;
    &lt;p&gt;The effort to classify all polynomials deals with the oldest kind of math: solving equations. To solve the simple polynomial y = 2x, for instance, you just need to find values of x and y that satisfy the equation. There are infinitely many solutions to this equation, such as x = 1, y = 2. When you graph all the solutions in the coordinate plane, you get a line.&lt;/p&gt;
    &lt;p&gt;Other polynomials are harder to solve directly, and their solutions cut out more complicated, higher-dimensional shapes in space.&lt;/p&gt;
    &lt;p&gt;But for some of these equations, it turns out, there’s a really simple way to find every possible solution. Instead of separately plugging different numbers into each variable, you can get all the solutions at once by rewriting the variables in terms of a new variable, t.&lt;/p&gt;
    &lt;p&gt;Consider the polynomial x2 + y2 = 1, which defines a circle. Now set x equal to 2t/(1 + t2), and y equal to (1 − t2)/(1 + t2). When you plug these new formulas back into your original equation, you get 1 = 1, a statement that’s always true, no matter what t is. This means that by choosing any real-number value for t, you’ll instantly get a solution to the original polynomial. For instance, when you set t equal to 1, you get x = 2(1)/(1 + (1)2) = 1, and y = 0. And indeed, x = 1, y = 0 is a solution to the original equation: (1)2 + (0)2 = 1.&lt;/p&gt;
    &lt;p&gt;This straightforward way of framing all your solutions is called a rational parameterization. It’s equivalent to mapping every point on the graph of your original polynomial — in this case, a circle — to a unique point on a straight line.&lt;/p&gt;
    &lt;p&gt;Any degree-1 polynomial equation — that is, any polynomial whose terms are raised to a power of at most 1 — can be parameterized like this. It doesn’t matter how many variables the equation has: It might have two variables, or 200. Once you go beyond two variables, the solutions to your polynomial equation will form complicated higher-dimensional shapes. But because the polynomial can still be parameterized, there’s a way to map every point in your high-dimensional shape to points on a particularly simple space in the same number of dimensions (like the line). This, in turn, gives you a straightforward way to compute all the polynomial’s solutions.&lt;/p&gt;
    &lt;p&gt;Similarly, any degree-2 polynomial (whose terms are raised to a power of at most 2) has a rational parameterization.&lt;/p&gt;
    &lt;p&gt;But if an equation’s degree is 3 or more, it can’t always be parameterized. It depends on how many variables the equation has.&lt;/p&gt;
    &lt;p&gt;Take a typical kind of degree-3 polynomial: elliptic curves, like y2 = x3 + 1, which have only two variables. “Elliptic curves are glorious, they’re wonderful, but you can’t possibly parameterize them,” said Brendan Hassett of Brown University. There’s no simple formula for x and y that gives you all of an elliptic curve’s solutions, so there’s no way to map the curve to a straight line. “If you could, they would not be so much fun,” Hassett said.&lt;/p&gt;
    &lt;p&gt;Instead, the solutions to an elliptic curve have a far richer structure — one that’s played a vital role in number theory for centuries, and that cryptographers have taken advantage of to encode secret messages.&lt;/p&gt;
    &lt;p&gt;What about degree-3 equations with more variables, then? Are they parameterizable, or is the structure of their solutions more fun, the way it is for elliptic curves?&lt;/p&gt;
    &lt;p&gt;In 1866, the German mathematician Alfred Clebsch showed that degree-3 equations with three variables — whose solutions form two-dimensional surfaces — are usually parameterizable. More than a century later, Herbert Clemens and Phillip Griffiths published a monumental proof in which they showed that the opposite is true for most degree-3 equations with four variables. These equations, which form three-dimensional manifolds called three-folds, are not parameterizable: Their solutions can’t be mapped to a simple 3D space.&lt;/p&gt;
    &lt;p&gt;Many mathematicians suspected that the next polynomial to be classified — degree-3 equations with five variables (forming four-dimensional manifolds known as four-folds) — wouldn’t usually be parameterizable either. In fact, they figured that polynomials should never be parameterizable past a certain point. But Clemens and Griffiths’ techniques didn’t work for four-folds.&lt;/p&gt;
    &lt;p&gt;And so for decades, the classification effort lay dormant.&lt;/p&gt;
    &lt;head rend="h2"&gt;Converting a Prophet&lt;/head&gt;
    &lt;p&gt;Mathematicians were surprised when, at a conference in Moscow in the summer of 2019, Maxim Kontsevich got up to speak about classifying four-folds.&lt;/p&gt;
    &lt;p&gt;For one thing, Kontsevich is known for taking a high-level approach to mathematics, preferring to pose ambitious conjectures and sketch out broad programs, often leaving the subtler details and formal proof-writing to others. He’s described himself as something between a prophet and a daydreamer.&lt;/p&gt;
    &lt;p&gt;©IHES/Flann Mérer&lt;/p&gt;
    &lt;p&gt;For the past three decades, he’s been focused on developing a program called homological mirror symmetry, which has its roots in string theory. In the 1980s, string theorists wanted to count the number of curves on high-dimensional manifolds to answer questions about how the building blocks of the universe might behave. To count the curves on a given manifold, they considered its “mirror image” — another manifold that, though very different from the original, had related properties. In particular, they found that an algebraic object associated to the mirror image, called a Hodge structure, could reveal the number of curves on the original manifold. The reverse was also true: If you count the curves on the mirror image, you’ll get information about the original manifold’s Hodge structure.&lt;/p&gt;
    &lt;p&gt;In 1994, Kontsevich sketched out a program to explain the underlying reason for this correspondence. His program also predicted that the correspondence extended to all kinds of manifolds beyond those relevant to string theory.&lt;/p&gt;
    &lt;p&gt;For now, no one knows how to prove Kontsevich’s mirror symmetry program. “It will be next-century mathematics,” he said. But over the years, he’s made partial progress toward a proof — while also exploring the program’s potential consequences.&lt;/p&gt;
    &lt;p&gt;In 2002, one of Kontsevich’s friends, Ludmil Katzarkov of the University of Miami, hypothesized one such consequence: that the program might be relevant to the classification of polynomial equations.&lt;/p&gt;
    &lt;p&gt;Katzarkov was familiar with Clemens and Griffiths’ 1972 proof that three-folds aren’t parameterizable. In that work, the pair looked at a given three-fold’s Hodge structure directly. They then used it to show that the three-fold couldn’t be mapped to a simple 3D space. But the Hodge structures associated with four-folds were too complicated to analyze using the same tools.&lt;/p&gt;
    &lt;p&gt;Katzarkov’s idea was to access the four-fold’s Hodge structure indirectly — by counting how many curves of a particular type lived on its mirror image. Typically, mathematicians studying the Hodge structures of four-folds don’t think about curve counts like these: They only come up in seemingly unrelated areas of math, like string theory. But if the mirror symmetry program is true, then the number of curves on the mirror image should illuminate features of the original four-fold’s Hodge structure.&lt;/p&gt;
    &lt;p&gt;Natalia Leal&lt;/p&gt;
    &lt;p&gt;In particular, Katzarkov wanted to break the mirror image’s curve count into pieces, then use the mirror symmetry program to show that there was a corresponding way to break up the four-fold’s Hodge structure. He could then work with these pieces of the Hodge structure, rather than the whole thing, to show that four-folds can’t be parameterized. If any one of the pieces couldn’t be mapped to a simple 4D space, he’d have his proof.&lt;/p&gt;
    &lt;p&gt;But this line of reasoning depended on the assumption that Kontsevich’s mirror symmetry program was true for four-folds. “It was clear that it should be true, but I didn’t have the technical ability to see how to do it,” Katzarkov said.&lt;/p&gt;
    &lt;p&gt;He knew someone who did have that ability, though: Kontsevich himself.&lt;/p&gt;
    &lt;p&gt;But his friend wasn’t interested.&lt;/p&gt;
    &lt;head rend="h2"&gt;Digging In&lt;/head&gt;
    &lt;p&gt;For years, Katzarkov tried to convince Kontsevich to apply his research on mirror symmetry to the classification of polynomials — to no avail. Kontsevich wanted to focus on the whole program, not this particular problem. Then in 2018, the pair, along with Tony Pantev of the University of Pennsylvania, worked on another problem that involved breaking Hodge structures and curve counts into pieces. It convinced Kontsevich to hear Katzarkov out.&lt;/p&gt;
    &lt;p&gt;Katzarkov walked him through his idea again. Immediately, Kontsevich discovered an alternative path that Katzarkov had long sought but never found: a way to draw inspiration from mirror symmetry without actually relying on it. “After you’ve spent years thinking about this, you see it happening in seconds,” Katzarkov said. “That’s a spectacular moment.”&lt;/p&gt;
    &lt;p&gt;Kontsevich argued that it should be possible to use the four-fold’s own curve counts — rather than those of its mirror image — to break up the Hodge structure. They just had to figure out how to relate the two in a way that gave them the pieces they needed. Then they’d be able to focus on each piece (or “atom,” as they called it) of the Hodge structure separately.&lt;/p&gt;
    &lt;p&gt;This was the plan Kontsevich laid out for his audience at the 2019 conference in Moscow. To some mathematicians, it sounded as though a rigorous proof was just around the corner. Mathematicians are a conservative bunch and often wait for absolute certainty to present new ideas. But Kontsevich has always been a little bolder. “He’s very open with his ideas, and very forward-thinking,” said Daniel Pomerleano, a mathematician at the University of Massachusetts, Boston, who studies mirror symmetry.&lt;/p&gt;
    &lt;p&gt;There was a major ingredient they still had no idea how to address, Kontsevich warned: a formula for how each atom would change as mathematicians tried to map the four-fold to new spaces. Only with such a formula in hand could they prove that some atom would never reach a state corresponding to a properly “simplified” four-fold. This would imply that four-folds weren’t parameterizable, and that their solutions were rich and complicated. “But people somehow got the impression that he said it was done,” Pomerleano said, and they expected a proof soon.&lt;/p&gt;
    &lt;p&gt;When that didn’t come to pass, some mathematicians began to doubt that he had a real solution. In the meantime, Tony Yue Yu, then at the French National Center for Scientific Research, joined the team. Yu’s fresh insights and meticulous style of proof, Kontsevich said, turned out to be crucial to the project.&lt;/p&gt;
    &lt;p&gt;When lockdowns began during the Covid pandemic, Yu visited Kontsevich at France’s nearby Institute for Advanced Scientific Studies. They relished the quiet of the deserted institute, spending hours in lecture halls where there were more blackboards, Yu recalled.&lt;/p&gt;
    &lt;p&gt;Meeting regularly with Pantev and Katzarkov over Zoom, they quickly completed the first part of their proof, figuring out precisely how to use the number of curves on a given four-fold to break its Hodge structure into atoms. But they struggled to find a formula to describe how the atoms could then be transformed.&lt;/p&gt;
    &lt;p&gt;What they didn’t know was that a mathematician who had attended Kontsevich’s lecture in Moscow — Hiroshi Iritani of Kyoto University — had also started pursuing such a formula. “He was enchanted by my conjecture,” Kontsevich said. “I didn’t know, but he started to work on it.”&lt;/p&gt;
    &lt;p&gt;In July 2023, Iritani proved a formula for how the atoms would change as four-folds were mapped to new spaces. It didn’t give quite as much information as Kontsevich and his colleagues needed, but over the next two years, they figured out how to hone it. They then used their new formula to show that four-folds would always have at least one atom that couldn’t be transformed to match simple 4D space. Four-folds weren’t parameterizable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Still Processing&lt;/head&gt;
    &lt;p&gt;When the team posted their proof in August, many mathematicians were excited. It was the biggest advance in the classification project in decades, and hinted at a new way to tackle the classification of polynomial equations well beyond four-folds.&lt;/p&gt;
    &lt;p&gt;But other mathematicians weren’t so sure. Six years had passed since the lecture in Moscow. Had Kontsevich finally made good on his promise, or were there still details to fill in?&lt;/p&gt;
    &lt;p&gt;And how could they assuage their doubts, when the proof’s techniques were so completely foreign — the stuff of string theory, not polynomial classification? “They say, ‘This is black magic, what is this machinery?’” Kontsevich said.&lt;/p&gt;
    &lt;p&gt;“Suddenly they come with this completely new approach, using tools that were previously widely believed to have nothing to do with this subject,” said Shaoyun Bai of the Massachusetts Institute of Technology. “The people who know the problem don’t understand the tools.”&lt;/p&gt;
    &lt;p&gt;Bai is one of several mathematicians now trying to bridge this gap in understanding. Over the past few months, he has co-organized a “reading seminar” made up of graduate students, postdoctoral researchers and professors who hope to make sense of the new paper. Each week, a different mathematician digs into some aspect of the proof and presents it to the rest of the group.&lt;/p&gt;
    &lt;p&gt;But even now, after 11 of these 90-minute sessions, the participants still feel lost when it comes to major details of the proof. “The paper contains brilliant original ideas,” Bai said, which “require substantial time to absorb.”&lt;/p&gt;
    &lt;p&gt;Similar reading groups have been congregating in Paris, Beijing, South Korea and elsewhere. “People all over the globe are working on the same paper right now,” Stellari said. “That’s a special thing.”&lt;/p&gt;
    &lt;p&gt;Hassett likens it to Grigori Perelman’s 2003 proof of the Poincaré conjecture, which also used entirely new techniques to solve a famous problem. It was only after other mathematicians reproduced Perelman’s proof using more traditional tools that the community truly accepted it.&lt;/p&gt;
    &lt;p&gt;“There will be resistance,” Katzarkov said, “but we did the work, and I’m sure it’s correct.” He and Kontsevich also see it as a major win for the mirror symmetry program: While they’re not closer to proving it, the result provides further evidence that it’s true.&lt;/p&gt;
    &lt;p&gt;“I’m very old, and very tired,” Katzarkov said. “But I’m willing to develop this theory as long as I’m alive.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/string-theory-inspires-a-brilliant-baffling-new-math-proof-20251212/"/><published>2025-12-12T16:23:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46245923</id><title>Async DNS</title><updated>2025-12-12T19:35:11.951924+00:00</updated><content>&lt;doc fingerprint="eeb8855f7ee79a0d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;async dns&lt;/head&gt;
    &lt;p&gt;curl experimented with using pthread_cancel to timeout async DNS requests and it blew up. What else can we do?&lt;/p&gt;
    &lt;p&gt;Out of curiosity, I decided to review some alternatives and see how they work. My personal priorities are control over events; no background threads or signals or secret mechanisms.&lt;/p&gt;
    &lt;head rend="h3"&gt;getaddrinfo&lt;/head&gt;
    &lt;p&gt;The tried and true classic technique is to call getaddrinfo in a thread. Probably with more than one thread so you don’t get stuck behind a single slow request, but probably not boundless either. You can also use a separate process if you don’t use threads.&lt;/p&gt;
    &lt;p&gt;This is probably good enough for many uses.&lt;/p&gt;
    &lt;head rend="h3"&gt;getaddrinfo_a&lt;/head&gt;
    &lt;p&gt;glibc provides getaddrinfo_a which basically does the thread dance for you. Some of it. It comes with some caveats, and it’s distinctly non portable, and probably doesn’t mesh with your idea of an event loop. Passing.&lt;/p&gt;
    &lt;head rend="h3"&gt;c-ares&lt;/head&gt;
    &lt;p&gt;c-ares is a standalone DNS library. It supports async queries via a threaded backend or an event driven system. I think the thread backend has the same issues, in that it uses a callback and then you need to push the results back into your application.&lt;/p&gt;
    &lt;p&gt;Alas, the event system uses lots of callbacks as well. This also includes some dire warnings in the documentation. “When the associated callback is called, it is called with a channel lock so care must be taken to ensure any processing is minimal to prevent DNS channel stalls.” Everyone knows the ideal callback just sets a flag, etc., but also everyone is inevitably tempted to do just one more thing, and hey look, it works fine, wait, why did it break. And thus I have a strong preference for library interfaces where you call into it, get some results, but any time you’re in your own code, you’re free to do what you want.&lt;/p&gt;
    &lt;p&gt;But worth a try. Based on the sample code I wrote the quickest dirtiest demo I could.&lt;/p&gt;
    &lt;p&gt;&lt;head&gt;c-ares code&lt;/head&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;poll.h&amp;gt;
#include &amp;lt;arpa/inet.h&amp;gt;

#include &amp;lt;ares.h&amp;gt;

struct server {
    char name[32];
    char ip[16];
    int status;
};

struct everything {
    struct server servers[1];
    int nservers;
    struct pollfd pfds[4];
    int npfds;
};

static void
addrinfo_cb(void *arg, int status, int timeouts, struct ares_addrinfo *result)
{
    struct server *server = arg;
    server-&amp;gt;status = 3;
    if (!result)
        return;
    for (struct ares_addrinfo_node *node = result-&amp;gt;nodes; node != NULL; node = node-&amp;gt;ai_next) {
        if (node-&amp;gt;ai_family == AF_INET) {
            struct sockaddr_in *in_addr = (void *)node-&amp;gt;ai_addr;
            inet_ntop(node-&amp;gt;ai_family, &amp;amp;in_addr-&amp;gt;sin_addr, server-&amp;gt;ip, sizeof(server-&amp;gt;ip));        }
    }
}

static void
socket_cb(void *arg, ares_socket_t fd, int readable, int writable)
{
    struct everything *state = arg;
    printf("socket: %d r/w: %d %d\n", fd, readable, writable);

    int idx = -1;
    for (int i = 0; i &amp;lt; 4; i++) {
        if (state-&amp;gt;pfds[i].fd == fd) {
            idx = i;
            break;
        }
    }
    if (idx == -1) {
        for (int i = 0; i &amp;lt; 4; i++) {
            if (state-&amp;gt;pfds[i].fd == -1) {
                idx = i;
                state-&amp;gt;pfds[idx].fd = fd;
                state-&amp;gt;npfds++;
                break;
            }
        }
    }
    if (idx == -1)
        abort();

    if (!readable &amp;amp;&amp;amp; !writable) {
        state-&amp;gt;pfds[idx].fd = -1;
        state-&amp;gt;npfds--;
        return;
    }
    state-&amp;gt;pfds[idx].fd = fd;
    state-&amp;gt;pfds[idx].events = 0;
    if (readable)
        state-&amp;gt;pfds[idx].events |= POLLIN;
    if (writable)
        state-&amp;gt;pfds[idx].events |= POLLOUT;
}

int
main(int argc, char **argv)
{
    struct everything state;
    memset(&amp;amp;state, 0, sizeof(state));
    strlcpy(state.servers[0].name, argv[1], sizeof(state.servers[0].name));
    state.servers[0].status = 1;
    state.nservers = 1;
    for (int i = 0; i &amp;lt; 4; i++)
        state.pfds[i].fd = -1;

    ares_library_init(ARES_LIB_INIT_ALL);

    struct ares_options options;
    memset(&amp;amp;options, 0, sizeof(options));
    int optmask = 0;
    options.flags = ARES_FLAG_EDNS | ARES_FLAG_DNS0x20;
    optmask |= ARES_OPT_FLAGS;
    options.sock_state_cb = socket_cb;
    options.sock_state_cb_data = &amp;amp;state;
    optmask |= ARES_OPT_SOCK_STATE_CB;

    ares_channel_t *channel;
    ares_init_options(&amp;amp;channel, &amp;amp;options, optmask);

    ares_fd_events_t ares_fds[1];

    while (1) {
        printf("top of loop\n");
        for (int i = 0; i &amp;lt; state.nservers; i++) {
            printf("processing server %d\n", i);
            struct server *server = &amp;amp;state.servers[i];
            switch (server-&amp;gt;status) {
            case 1:
                {
                    struct ares_addrinfo_hints hints;
                    memset(&amp;amp;hints, 0, sizeof(hints));
                    hints.ai_family = AF_UNSPEC;
                    hints.ai_flags  = ARES_AI_CANONNAME;
                    ares_getaddrinfo(channel, argv[1], NULL, &amp;amp;hints, addrinfo_cb, server);
                    server-&amp;gt;status = 2;
                }
                break;
            case 2:
                printf("woke up while working\n");
                break;
            case 3:
                printf("got it, done: %s -&amp;gt; %s\n", server-&amp;gt;name, server-&amp;gt;ip);
                return 0;
            }
        }
        if (state.npfds == 0) {
            printf("confused. nothing to poll\n");
            return 1;
        }
        int res = poll(state.pfds, 4 /* state.npfds */, 2000);
        printf("poll results: %d\n", res);
        if (res &amp;gt; 0) {
            ares_fd_events_t events[4];
            int nevents = 0;
            for (int i = 0; i &amp;lt; 4 /* state.npfds */; i++) {
                if (!state.pfds[i].revents)
                    continue;
                events[nevents].fd = state.pfds[i].fd;
                events[nevents].events = 0;
                if (state.pfds[i].revents &amp;amp; (POLLERR|POLLHUP|POLLIN))
                    events[nevents].events |= ARES_FD_EVENT_READ;
                if (state.pfds[i].revents &amp;amp; (POLLOUT))
                    events[nevents].events |= ARES_FD_EVENT_WRITE;
                nevents++;
            }
            ares_process_fds(channel, events, nevents, 0);
        }
    }
}&lt;/code&gt;&lt;lb/&gt;It’s okay, but the callbacks are annoying. Notifying me which descriptors need watching means I’m required to pack up my poll structure so I can access it in the callbacks, etc. Everything gets bound just a little bit tighter.&lt;/p&gt;
    &lt;head rend="h3"&gt;wadns&lt;/head&gt;
    &lt;p&gt;Among the alternatives the c-ares project helpfully lists, is dns.c. This sounds enticing.&lt;/p&gt;
    &lt;p&gt;On the downside, it’s not clear where the demo code stops and the functional code begins. As in, there’s a getaddrinfo sample, but it incorporates a lot of other code that doesn’t seem to be public. The public header doesn’t actually expose a means to interface with an event loop. The code is meant to be integrated into a project, which is understandable and even advantageous, but it means no demo today.&lt;/p&gt;
    &lt;head rend="h3"&gt;asr&lt;/head&gt;
    &lt;p&gt;The asr code was written for smtpd in OpenBSD. It doesn’t use threads and requires the caller to push events. Unfortunately, a portable version currently only exists in the OpenSMTPD repo. On the plus side, it’s used as the basis for the libc resolver in OpenBSD, which means the “sample” code to replace getaddrinfo literally is getaddrinfo.c.&lt;/p&gt;
    &lt;p&gt;I rewrote the c-ares demo to use asr. It comes out quite a bit shorter, and I think clearer as well.&lt;/p&gt;
    &lt;p&gt;&lt;head&gt;asr code&lt;/head&gt;&lt;code&gt;#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/socket.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;poll.h&amp;gt;
#include &amp;lt;netdb.h&amp;gt;
#include &amp;lt;asr.h&amp;gt;
#include &amp;lt;arpa/inet.h&amp;gt;

struct server {
    char name[32];
    char ip[16];
    int status;
    struct asr_query *aq;
    int ar_fd;
};

int
main(int argc, char **argv)
{
    struct server servers[1] = {};
    strlcpy(servers[0].name, argv[1], sizeof(servers[0].name));
    servers[0].status = 1;
    int nservers = 1;

    while (1) {
        struct pollfd pfds[4];
        int npfds = 0;
        printf("top of loop\n");
        for (int i = 0; i &amp;lt; nservers; i++) {
            printf("processing server %d\n", i);
            struct server *server = &amp;amp;servers[i];
            switch (server-&amp;gt;status) {
            case 1:
                {
                    struct addrinfo hints;
                    memset(&amp;amp;hints, 0, sizeof(hints));
                    hints.ai_family = AF_UNSPEC;
                    hints.ai_socktype = SOCK_STREAM;
                    server-&amp;gt;aq = getaddrinfo_async(server-&amp;gt;name, "80", &amp;amp;hints, NULL);
                    server-&amp;gt;status = 2;
                }
                // fallthrough
          case 2:
                {
                    printf("ready to run\n");
                    struct asr_result ar;
                    int rv = asr_run(server-&amp;gt;aq, &amp;amp;ar);
                    switch (rv) {
                    case 0:
                        pfds[npfds].fd = ar.ar_fd;
                        pfds[npfds].events = 0;
                        if (ar.ar_cond == ASR_WANT_READ)
                            pfds[npfds].events = POLLIN;
                        else
                            pfds[npfds].events = POLLOUT;
                        npfds++;
                        server-&amp;gt;ar_fd = ar.ar_fd;
                        server-&amp;gt;status = 3;
                        break;
                    case 1:
                        {
                            struct addrinfo *res;
                            for (res = ar.ar_addrinfo; res; res = res-&amp;gt;ai_next) {
                                if (res-&amp;gt;ai_family == AF_INET) {
                                    struct sockaddr_in *in_addr = (void *)res-&amp;gt;ai_addr;
                                    inet_ntop(res-&amp;gt;ai_family, &amp;amp;in_addr-&amp;gt;sin_addr, server-&amp;gt;ip, sizeof(server-&amp;gt;ip));
                                }
                            }
                            server-&amp;gt;status = 4;
                        }
                        break;
                    }
                }
                break;
            case 3:
                printf("woke up while working\n");
                break;
            case 4:
                printf("got it, done: %s -&amp;gt; %s\n", server-&amp;gt;name, server-&amp;gt;ip);
                return 0;
            }
        }
        if (npfds == 0)
            continue;
        int res = poll(pfds, npfds, 2000);
        printf("poll results: %d\n", res);
        if (res &amp;gt; 0) {
            for (int i = 0; i &amp;lt; npfds; i++) {
                if (!pfds[i].revents)
                    continue;
                for (int j = 0; j &amp;lt; nservers; j++) {
                    if (pfds[i].fd == servers[j].ar_fd)
                        servers[j].status = 2;
                }
            }
        }
    }
}&lt;/code&gt;&lt;lb/&gt;I like this API. It’s very much like read or write in that it either gives you an answer, or tells you to come back later, and then it’s up to you to decide when that is. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://flak.tedunangst.com/post/async-dns"/><published>2025-12-12T16:52:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46246006</id><title>Japan law opening phone app stores to go into effect dec.18th</title><updated>2025-12-12T19:35:11.606285+00:00</updated><content>&lt;doc fingerprint="f79bbbe8be967d70"&gt;
  &lt;main&gt;
    &lt;p&gt;A new Japanese law is going into effect that could loosen the dominance of tech giants over smartphone services. It aims to bring users greater choice for app stores and more.&lt;/p&gt;
    &lt;p&gt;Starting December 18, firms like Apple and Google will be prohibited from blocking third party app stores on iPhone and Android devices.&lt;/p&gt;
    &lt;p&gt;The law also aims to loosen their grip on web browsers and search. The firms will now be required to give first-time users multiple choices for default services. This also applies when people update their operating system.&lt;/p&gt;
    &lt;p&gt;The Fair Trade Commission says the changes will improve convenience by encouraging new market entrants.&lt;/p&gt;
    &lt;p&gt;But some public comments released by the commission expressed concern that the legislation could undermine user security.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www3.nhk.or.jp/nhkworld/en/news/20251210_B1/"/><published>2025-12-12T16:59:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46246117</id><title>Show HN: tomcp.org – Turn any URL into an MCP server</title><updated>2025-12-12T19:35:11.425374+00:00</updated><content>&lt;doc fingerprint="6d12af189a939ecb"&gt;
  &lt;main&gt;
    &lt;p&gt;Turn any website into an MCP server + Chat with any website.&lt;/p&gt;
    &lt;p&gt;Convert any website URL into an MCP (Model Context Protocol) server config for your AI tools, or chat directly with any website's content.&lt;/p&gt;
    &lt;p&gt;Simply add &lt;code&gt;tomcp.org/&lt;/code&gt; before any URL:&lt;/p&gt;
    &lt;code&gt;tomcp.org/docs.stripe.com
tomcp.org/react.dev
tomcp.org/your-docs.com/api
&lt;/code&gt;
    &lt;p&gt;Visit tomcp.org, paste a URL, and start chatting with any website's content using AI.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cursor - &lt;code&gt;~/.cursor/mcp.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Claude Desktop - &lt;code&gt;~/.claude/claude_desktop_config.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windsurf - &lt;code&gt;~/.codeium/windsurf/mcp_config.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;VS Code - &lt;code&gt;.vscode/mcp.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Cline - &lt;code&gt;~/.cline/mcp_settings.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visit tomcp.org&lt;/item&gt;
      &lt;item&gt;Enter any website URL&lt;/item&gt;
      &lt;item&gt;Select your AI tool&lt;/item&gt;
      &lt;item&gt;Copy the generated MCP config&lt;/item&gt;
      &lt;item&gt;Add it to your tool's config file&lt;/item&gt;
      &lt;item&gt;Restart your AI tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visit tomcp.org&lt;/item&gt;
      &lt;item&gt;Paste any website URL&lt;/item&gt;
      &lt;item&gt;Click "Start Chat"&lt;/item&gt;
      &lt;item&gt;Ask questions about the website's content&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "mcpServers": {
    "docs-stripe-com": {
      "url": "https://tomcp.org/docs.stripe.com"
    }
  }
}&lt;/code&gt;
    &lt;code&gt;curl -X POST https://tomcp.org/chat \
  -H "Content-Type: application/json" \
  -d '{"url": "docs.stripe.com", "message": "How do I create a payment intent?"}'&lt;/code&gt;
    &lt;p&gt;These models are available for everyone with no setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Llama 3.1 8B (Meta) - Default model, fast and capable&lt;/item&gt;
      &lt;item&gt;Hermes 2 Pro (NousResearch) - Great for reasoning&lt;/item&gt;
      &lt;item&gt;Mistral 7B (Mistral) - Efficient instruction-following&lt;/item&gt;
      &lt;item&gt;Gemma 7B LoRA (Google) - Lightweight and fast&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add your Cloudflare Workers AI API key to unlock these models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Llama 3.3 70B (Meta) - Most powerful Llama model&lt;/item&gt;
      &lt;item&gt;DeepSeek R1 32B (DeepSeek) - Advanced reasoning&lt;/item&gt;
      &lt;item&gt;Mistral Large (Mistral) - Enterprise-grade&lt;/item&gt;
      &lt;item&gt;Gemma 3 12B (Google) - Latest Gemma&lt;/item&gt;
      &lt;item&gt;GPT OSS 120B/20B (OpenAI) - Open-source GPT variants&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can add your own Cloudflare Workers AI API key to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Unlock all premium models - Access larger, more capable models&lt;/item&gt;
      &lt;item&gt;Bypass rate limits - No daily request limits&lt;/item&gt;
      &lt;item&gt;Use your own quota - Charges go to your Cloudflare account&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to Cloudflare Workers AI&lt;/item&gt;
      &lt;item&gt;Create an API token with Workers AI permissions&lt;/item&gt;
      &lt;item&gt;Copy the token&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start a chat session on tomcp.org&lt;/item&gt;
      &lt;item&gt;Below the chat input, you'll see "Add API key from Cloudflare Workers AI"&lt;/item&gt;
      &lt;item&gt;Paste your API key and click "Save"&lt;/item&gt;
      &lt;item&gt;Premium models will now be unlocked in the dropdown&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your API key is stored locally in your browser using &lt;code&gt;localStorage&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Key name: &lt;code&gt;tomcp_api_key&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;The key is sent with each chat request but never stored on our servers&lt;/item&gt;
      &lt;item&gt;You can remove it anytime by clicking "Remove" in the API key section&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The available models are fetched dynamically from the Cloudflare Workers AI API:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Frontend calls &lt;code&gt;GET /models&lt;/code&gt;endpoint on page load&lt;/item&gt;
      &lt;item&gt;Worker fetches models from &lt;code&gt;api.cloudflare.com/client/v4/accounts/{id}/ai/models/search&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models are filtered to "Text Generation" tasks and cached for 5 minutes&lt;/item&gt;
      &lt;item&gt;Frontend displays free models as enabled, premium models as disabled (until API key is added)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;User enters a URL and starts chatting&lt;/item&gt;
      &lt;item&gt;Worker fetches the website content and converts HTML to Markdown&lt;/item&gt;
      &lt;item&gt;Content is sent to the selected AI model with the user's message&lt;/item&gt;
      &lt;item&gt;Response is returned to the user&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without an API key:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;5 requests per IP per day&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With your API key:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No rate limits (uses your Cloudflare account quota)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: Vanilla HTML/CSS/JS with Tailwind CSS&lt;/item&gt;
      &lt;item&gt;Backend: Cloudflare Workers&lt;/item&gt;
      &lt;item&gt;AI: Cloudflare Workers AI (multiple models)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Works with any public URL&lt;/item&gt;
      &lt;item&gt;No setup required - just paste the config&lt;/item&gt;
      &lt;item&gt;Free forever - powered by Cloudflare Workers&lt;/item&gt;
      &lt;item&gt;Chat with any website using AI&lt;/item&gt;
      &lt;item&gt;Side-by-side MCP Config + Chat interface&lt;/item&gt;
      &lt;item&gt;Multiple AI models - Choose from Llama, Mistral, Gemma, and more&lt;/item&gt;
      &lt;item&gt;Bring your own API key - Unlock premium models and bypass rate limits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Apache 2.0&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Ami3466/tomcp"/><published>2025-12-12T17:10:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46246395</id><title>Nuclear energy key to decarbonising Europe, says EESC</title><updated>2025-12-12T19:35:10.889417+00:00</updated><content>&lt;doc fingerprint="85ce70f70a096d9e"&gt;
  &lt;main&gt;
    &lt;p&gt;European Economic &lt;lb/&gt;and Social Committee&lt;/p&gt;
    &lt;head rend="h1"&gt;Nuclear energy key to decarbonising Europe, says EESC&lt;/head&gt;
    &lt;p&gt;The EESC has adopted an opinion pointing out that nuclear energy is an essential component of the clean energy mix which is needed to phase out fossil fuels. The Committee calls on the European Commission to include key regulatory and financial enablers in order to make the planned investment possible, and to enhance transparent dialogue with civil society.&lt;/p&gt;
    &lt;p&gt;Nuclear energy plays and will continue to play a crucial role in decarbonising the European Union, says the European Economic and Social Committee (EESC) in an opinion adopted at the December plenary session. This is particularly true given the fact that the EU needs to consolidate its strategic autonomy in the fields of energy and technology.&lt;/p&gt;
    &lt;p&gt;The EESC opinion, drawn up by rapporteur Dumitru Fornea and co-rapporteur Alena Mastantuono, assesses the European Commission’s 8th Nuclear Illustrative Programme (PINC), published in June 2025.&lt;/p&gt;
    &lt;p&gt;According to the Committee, nuclear energy is a key element in diversifying the EU’s energy supply because it delivers safe, reliable, low-carbon electricity. This ensures that the grid remains stable most of the time, regardless of the weather or time of day, with less pressure on systemic costs.&lt;/p&gt;
    &lt;p&gt;Nuclear energy can therefore play an important role in supporting the EU’s overall industrial transition as it bolsters resilience against supply disruptions while complementing renewables and reducing dependence on imported fuels. Against this backdrop, existing EU industries (such as steel, cement and chemicals) as well as new industries (data centres) can enjoy a constant stream of decarbonised electricity.&lt;/p&gt;
    &lt;p&gt;‘The European nuclear industry sustains more than 1.1 million jobs in the EU and is a significant economic sector with a major footprint in terms of jobs, supply chain capacity and advanced R&amp;amp;D. It is a net-zero value chain based almost entirely in the EU,’ said Mr Fornea. ‘If we want to effectively move away from coal, we need accessible clean energy and funding for nuclear.’&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving ahead with planned investment&lt;/head&gt;
    &lt;p&gt;In the opinion, the EESC regrets that the PINC does not propose any specific enablers, nor a real action plan, for the planned investment and urges the European Commission to include regulatory and financial measures. The goal is to enable investment in the sector, promote the development of innovative fuel cycle facilities and propose specific figures on the investment required by the nuclear fuel cycle.&lt;/p&gt;
    &lt;p&gt;‘We call on the Commission to put forward concrete measures to make the investment planned under the PINC possible,’ said Ms Mastantuono. ‘This is more necessary than ever given the geopolitical turmoil which is forcing the Union to develop EU-based capacities. For this reason, the nuclear value chain should be supported in terms of skills, research and the fuel supply chain.’&lt;/p&gt;
    &lt;p&gt;More specifically, the Committee recommends speeding up investment through specific measures such as a streamlined State aid process, access to EU cohesion funds, sustainable financing, licensing processes and faster decisions at EU and national level.&lt;/p&gt;
    &lt;p&gt;In addition, the EESC advises applying the same facilities to investment in nuclear energy as for renewables. These two energy sources are complementary and Member States are free to choose their own energy mix.&lt;/p&gt;
    &lt;head rend="h2"&gt;Keeping transparent dialogue open with civil society&lt;/head&gt;
    &lt;p&gt;Dialogue with civil society remains pivotal in building trust, ownership and societal acceptance, and could be more prominently addressed in the PINC. Moreover, there is no dedicated funding available for meaningful civil society participation.&lt;/p&gt;
    &lt;p&gt;On this matter, the EESC’s view is that decisions on new projects in the nuclear sector, including the development of new technologies, should be taken following the outcome of a broad and transparent dialogue with civil society on the technical, economic, social and environmental aspects.&lt;/p&gt;
    &lt;p&gt;Public engagement is essential to ensure that energy strategies reflect societal priorities (such as sustainability, reliability, land-use and responsibility for long-term waste management) and the early involvement of civil society through dialogue strengthens trust and legitimacy for both nuclear energy and other low-carbon technologies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background - Nuclear Illustrative Programme (PINC)&lt;/head&gt;
    &lt;p&gt;According to article 40 of the Euratom Treaty, the European Commission is required to periodically publish a Nuclear Illustrative Programme (PINC) and consult the EESC. The Commission Communication on the PINC issued in June 2025 has therefore been presented under this article for the opinion of the European Economic and Social Committee.&lt;/p&gt;
    &lt;p&gt;The PINC provides a comprehensive overview of investment needs in nuclear energy, both fission and fusion, and encompasses all stages of the nuclear lifecycle. It also feeds into the debate on the role of nuclear energy in achieving carbon neutrality in the EU by 2050. In line with the highest level of nuclear safety, the PINC supports EU competitiveness, energy security and affordable energy prices.&lt;/p&gt;
    &lt;p&gt;In the 8th PINC, the Commission points out that nuclear energy requires significant investment, of around EUR 241 billion until 2050, both for lifetime extensions of existing reactors and the construction of new large-scale reactors. The Commission also says that additional investment is needed for Small Modular Reactors (SMRs), Advanced Modular Reactors (AMRs) and microreactors and in fusion for the longer-term future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eesc.europa.eu/en/news-media/news/nuclear-energy-key-decarbonising-europe-says-eesc"/><published>2025-12-12T17:32:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46246802</id><title>Google Releases Its New Google Sans Flex Font as Open Source</title><updated>2025-12-12T19:35:10.640369+00:00</updated><content>&lt;doc fingerprint="342af17119b63188"&gt;
  &lt;main&gt;
    &lt;p&gt;Google has made its ‘next generation brand typeface’, Google Sans Flex, available for download — under an open source license, which is welcome news.&lt;/p&gt;
    &lt;p&gt;A modern sans serif font purpose-designed for use on screens and OSes, Google Sans Flex is a ground-up, multi-axis rebuild of the proprietary Google Sans font, by typographer David Berlow (of Font Bureau fame).&lt;/p&gt;
    &lt;p&gt;The “flex” in GS Flex is because it’s a variable font that is “extremely flexible [with] variable axes for weight, width, optical size, slant, as well as an axis for rounded terminals” (as in terminals in letters, not command-line apps).”&lt;/p&gt;
    &lt;p&gt;Android and web developers will find the varied variable axes on offer a creative boon for “expressive” design work.&lt;/p&gt;
    &lt;p&gt;Changing system font is a simple way to give Ubuntu (or any other Linux) desktop a subtle new vibe without having to futz around with themes, icon packs or other eye-candy extras which substantially alter the stock experience:&lt;/p&gt;
    &lt;p&gt;However, Linux desktop environments don’t yet support doing anything fancy with variable fonts, beyond the basics.&lt;/p&gt;
    &lt;p&gt;Ergo, unlike on modern Android, you can’t toggle Dark Mode in GNOME or KDE with this font enabled to make it automatically adjust its GRAD axis to compensate for the optical thinning that typically occurs when white text is rendered against darker backgrounds.&lt;/p&gt;
    &lt;p&gt;It’s not a major drawback, and GS Flex works great as a competent, classy system UI font on Linux, especially on HiDPI displays with fractional scaling. For my tastes, Google Sans Flex has (like GNOME’s default Adwaita Sans font) more presence than the Ubuntu font.&lt;/p&gt;
    &lt;p&gt;Want to try it out? Google has released the font under the SIL Open Font License (OFL), meaning you can modify, redistribute and use it in your own projects.&lt;/p&gt;
    &lt;p&gt;To get it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to Google Fonts&lt;/item&gt;
      &lt;item&gt;Search for ‘Google Sans Flex’&lt;/item&gt;
      &lt;item&gt;Hit “Get Font” &amp;gt; “Download All”&lt;/item&gt;
      &lt;item&gt;Extract the ZIP&lt;/item&gt;
      &lt;item&gt;Find the .ttf file inside and either: &lt;list rend="ul"&gt;&lt;item&gt;Move it to &lt;code&gt;~/.local/share/fonts&lt;/code&gt;; or&lt;/item&gt;&lt;item&gt;Install via your desktop’s font manager GUI&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Move it to &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once installed it’ll be available to use/select in other apps, settings and so on.&lt;/p&gt;
    &lt;p&gt;To change UI font on Ubuntu you can install the GNOME Tweaks tool and then open it, go to Appearance and set the UI font to Google Sans Flex. Although you may see variable options listed to pick from, GNOME will always render the ‘regular’ version.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.omgubuntu.co.uk/2025/11/google-sans-flex-font-ubuntu"/><published>2025-12-12T18:07:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46246845</id><title>Id Software devs form "wall-to-wall" union</title><updated>2025-12-12T19:35:10.482231+00:00</updated><content>&lt;doc fingerprint="3e2474072eccea66"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Id Software devs form "wall-to-wall" union, with 165 workers at Doom studio the latest to vote in favour&lt;/head&gt;
    &lt;p&gt;“Remote work isn’t a perk"&lt;/p&gt;
    &lt;p&gt;Doom and Quake studio id Software are now home to a "wall-to-wall" union according to the Communications Workers of America (CWA). The organisation have announced that a group of 165 id workers have just voted to unionise, adding to the ranks of the 300 ZeniMax quality assurance staff who unionised back in 2023.&lt;/p&gt;
    &lt;p&gt;According to the CWA's press release, Microsoft have already recognised this latest union - which is made up of "developers, artists, programmers, and more" - in accordance with the labour neutrality agreement the two parties agreed in 2022.&lt;/p&gt;
    &lt;p&gt;"The wall-to-wall organizing effort at id Software was much needed; it’s incredibly important that developers across the industry unite to push back on all the unilateral workplace changes that are being handed down from industry executives," said id Software producer and CWA organising committee member Andrew Willis.&lt;/p&gt;
    &lt;p&gt;Meanwhile, id lead services programmer and CWA committee member Chris Hays specifically cited remote staff not being dragged into the office as a reason behind the push for representation. "Remote work isn’t a perk," he said. "It’s a necessity for our health, our families, and our access needs. RTO policies should not be handed down from executives with no consideration for accessibility or our well-being."&lt;/p&gt;
    &lt;p&gt;The CWA release also cited "mass industry layoffs, sudden periods of crunch time, and unfair pay" as part of the impetus behind a wider push towards unionisation among devs across the industry this year, adding that the total of unionised workers across Microsoft's fiefdom is now "nearly 4,000" strong.&lt;/p&gt;
    &lt;p&gt;CWA president Ron Swaggerty added that the union "look forward to sitting across the table from Microsoft to negotiate a contract that reflects the skill, creativity, and dedication these workers bring to every project."&lt;/p&gt;
    &lt;p&gt;If you want to learn more about the CWA's unionisation efforts as the games industry's suits and moneyfolk continue to lob developers out of windows with depressing regularity, give this interview Nic did a read.&lt;/p&gt;
    &lt;p&gt;Meanwhile, members of the "industry-wide union" the CWA announced earlier this year held a protest outside of The Game Awards yesterday, with their aim being to "to acknowledge the video games and studios that have been closed and to also condemn the creativity that’s been crushed by corporate greed and studio executives".&lt;/p&gt;
    &lt;p&gt;Solidarity to these id Software workers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.rockpapershotgun.com/id-software-devs-form-wall-to-wall-union-with-165-workers-at-doom-studio-the-latest-to-vote-in-favour"/><published>2025-12-12T18:11:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46247000</id><title>Home Depot GitHub token exposed for a year, granted access to internal systems</title><updated>2025-12-12T19:35:10.374786+00:00</updated><content>&lt;doc fingerprint="3446549a0e6331dc"&gt;
  &lt;main&gt;
    &lt;p&gt;A security researcher said Home Depot exposed access to its internal systems for a year after one of its employees published a private access token online, likely by mistake. The researcher found the exposed token and tried to privately alert Home Depot to its security lapse but was ignored for several weeks.&lt;/p&gt;
    &lt;p&gt;The exposure is now fixed after TechCrunch contacted company representatives last week.&lt;/p&gt;
    &lt;p&gt;Security researcher Ben Zimmermann told TechCrunch that, in early November, he found a published GitHub access token belonging to a Home Depot employee, which was exposed sometime in early 2024.&lt;/p&gt;
    &lt;p&gt;When he tested the token, Zimmermann said that it granted access to hundreds of private Home Depot source code repositories hosted on GitHub and allowed the ability to modify their contents.&lt;/p&gt;
    &lt;p&gt;The researcher said the keys allowed access to Home Depot’s cloud infrastructure, including its order fulfillment and inventory management systems, and code development pipelines, among other systems. Home Depot has hosted much of its developer and engineering infrastructure on GitHub since 2015, according to a customer profile on GitHub’s website.&lt;/p&gt;
    &lt;p&gt;Zimmermann said he sent several emails to Home Depot but didn’t hear back.&lt;/p&gt;
    &lt;p&gt;Nor did he get a response from Home Depot’s chief information security officer, Chris Lanzilotta, after sending a message over LinkedIn.&lt;/p&gt;
    &lt;p&gt;Zimmermann told TechCrunch that he has disclosed several similar exposures in recent months to companies, which have thanked him for his findings.&lt;/p&gt;
    &lt;p&gt;“Home Depot is the only company that ignored me,” he said.&lt;/p&gt;
    &lt;p&gt;Given that Home Depot does not have a way to report security flaws, such as a vulnerability disclosure or bug bounty program, Zimmermann contacted TechCrunch in an effort to get the exposure fixed.&lt;/p&gt;
    &lt;p&gt;When reached by TechCrunch on December 5, Home Depot spokesperson George Lane acknowledged receipt of our email but did not respond to follow-up emails asking for comment. The exposed token is no longer online, and the researcher said the token’s access was revoked soon after our outreach.&lt;/p&gt;
    &lt;p&gt;We also asked Lane if Home Depot has the technical means, such as logs, to determine if anyone else used the token during the months it was left online to access any of Home Depot’s internal systems. We did not hear back.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2025/12/12/home-depot-exposed-access-to-internal-systems-for-a-year-says-researcher/"/><published>2025-12-12T18:23:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46247415</id><title>Benn Jordan's flock camera jammer will send you to jail in Florida now [video]</title><updated>2025-12-12T19:35:09.204191+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=qEllWdK4l_A"/><published>2025-12-12T18:58:43+00:00</published></entry></feed>