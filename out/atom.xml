<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-05T16:38:34.148213+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45475808</id><title>Blog Feeds</title><updated>2025-10-05T16:40:57.334777+00:00</updated><content>&lt;doc fingerprint="779f1fb99e6586b5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tired of social media?&lt;/p&gt;
    &lt;p&gt;Keep doom scrolling through addicting feeds?&lt;/p&gt;
    &lt;p&gt;Miss the days when the web was just about connecting with people and their thoughts or ideas?&lt;/p&gt;
    &lt;p&gt;We believe there's an answer to that problem, and it's called&lt;/p&gt;
    &lt;p&gt;Starting a blog is actually a lot simpler than what you're probably thinking. This doesn't have to be some well polished highly viewed monetization machine, or even something professional or formal. It's just a simple website where you can casually talk about whatever you want to talk about! It can be long, short, a list of small things, or just a quote. It should be how you talk with other people in your own life, or how you communicate with the outside world. It should be you on a page. Here's a few places you can make a blog that are RSS enabled:&lt;/p&gt;
    &lt;p&gt;RSS is actually already familiar to you if you have ever subscribed to a newsletter. You put your email into someoneâs website, and when they have updates, they send you emails to your inbox so you can stay in the loop. In the case of RSS, you have a dedicated app, called an RSS reader usually, and you can put in someoneâs website into the app to subscribe. When they make a new post, just open your news reader app, and their posts will be retrieved and ready to read. Some reader apps even let you make folders and tags to organize blogs you are subscribed to, similar to how an email app lets you make folders to sort mail. Would highly recommend trying a few of the apps or services and seeing which works best!&lt;/p&gt;
    &lt;p&gt;This takes us to our final point: Feeds. You can probably get away with just the first two items and then sharing it with people you already know, but what about meeting or talking to people you don't know? That's where Feeds come in. The idea is to create another page on your blog that has all the RSS feeds you're subscribed to. By keeping this public and always up to date, someone can visit your page, find someone new and follow them. Perhaps that person also has a feeds page, and the cycle continues until there is a natural and organic network of people all sharing with each other. So if you have a blog, consider making a feeds page and sharing it! If your RSS reader supports OPML file exports and imports, perhaps you can share that file as well to make it easier to share your feeds.&lt;/p&gt;
    &lt;p&gt;Here's an example Feeds Page which should help get the idea across!&lt;/p&gt;
    &lt;p&gt;The best part about blog feeds? It's just an idea. There's no central authority. There's no platform. No massive tech giant trying to take your data. It's just you, basic web standards, and the people you care about.&lt;/p&gt;
    &lt;p&gt;Made by Steve&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blogfeeds.net"/><published>2025-10-04T19:08:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45477206</id><title>NSA and IETF: Can an attacker purchase standardization of weakened cryptography?</title><updated>2025-10-05T16:40:57.166606+00:00</updated><content>&lt;doc fingerprint="3e9999123e0289dd"&gt;
  &lt;main&gt;
    &lt;p&gt;It's normal for post-quantum cryptography to be rolled out as an extra layer of security on top of traditional pre-quantum cryptography, rather than as a replacement.&lt;/p&gt;
    &lt;p&gt;For example, Google's CECPQ1 experiment was double encryption with traditional pre-quantum ECC (specifically X25519) and post-quantum NewHope1024. CECPQ2, a joint experiment between Google and Cloudflare, was ECC+NTRUHRSS701. CECPQ2b was ECC+SIKEp434. Ten SSH implementations support ECC+sntrup761. Today's usage of post-quantum cryptography by browsers is approaching half of the connections seen by Cloudflare, where 95% of that is ECC+MLKEM768 and 5% is ECC+Kyber768.&lt;/p&gt;
    &lt;p&gt;If post-quantum cryptography is designed to be super-strong, so strong that it even survives future quantum computers, then why are we keeping the ECC layer? Same reason that you wear your seatbelt: in the real world, cars sometimes crash, and seatbelts reduce the damage.&lt;/p&gt;
    &lt;p&gt;Google already explained this back in 2016: "The post-quantum algorithm might turn out to be breakable even with today's computers, in which case the elliptic-curve algorithm will still provide the best security that today's technology can offer." We've seen many breaks of post-quantum proposals since then, including the sudden public collapse of SIKE three years after CECPQ2b applied SIKE to tens of millions of user connections. The only reason that this user data wasn't immediately exposed to attackers is that CECPQ2b encrypted data with SIKE and with ECC, rather than switching from ECC to just SIKE. As another example, the reference Kyber/ML-KEM software went through two rounds of security patches for KyberSlash at the end of 2023, and then had another security patch in mid-2024.&lt;/p&gt;
    &lt;p&gt;Deploying ECC+PQ rather than just PQ is an easy common-sense win. ECC software is practically everywhere anyway, and nobody has identified an application that can afford PQ without being able to afford ECC+PQ.&lt;/p&gt;
    &lt;p&gt;Typically people talk about deploying ECC+PQ as deploying "hybrids" rather than "non-hybrids", although you have to be careful with this terminology since the word "hybrid" also has other meanings in cryptography. It's more descriptive to talk about "double encryption" and "double signatures" rather than "single encryption" and "single signatures".&lt;/p&gt;
    &lt;p&gt;The problem in a nutshell. Surveillance agency NSA and its partner GCHQ are trying to have standards-development organizations endorse weakening ECC+PQ down to just PQ.&lt;/p&gt;
    &lt;p&gt;Part of this is that NSA and GCHQ have been endlessly repeating arguments that this weakening is a good thing (in much the same way that NSA advertised Dual EC as providing "increased assurance"). I have a previous blog post showing that those arguments collapse upon examination. But that's not today's topic. In today's blog post I'm instead looking at how easy it is for NSA to simply spend money to corrupt the standardization process.&lt;/p&gt;
    &lt;p&gt;Two TLS encryption drafts. For concreteness, I'll focus on what's happening in a particular standards-development organization called the Internet Engineering Task Force (IETF). Within that, I'll focus on current proposals within an IETF "working group" (WG) that sets standards for Transport Layer Security (TLS), the security layer inside HTTPS and inside various other protocols. I'll look specifically at how the TLS WG handled two drafts specifying post-quantum encryption mechanisms for TLS:&lt;/p&gt;
    &lt;p&gt;Hybrid (double encryption): "Post-quantum hybrid ECDHE-MLKEM Key Agreement for TLSv1.3". This draft specifies ECC+PQ in TLS, specifically usage in TLS of "X25519MLKEM768, SecP256r1MLKEM768, and SecP384r1MLKEM1024". The first of those is also what I mentioned above as 95% of current post-quantum connections to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Non-hybrid (single encryption): "ML-KEM Post-Quantum Key Agreement for TLS 1.3". This draft specifies usage in TLS of "ML-KEM-512, ML-KEM-768, and ML-KEM-1024" without seatbelts.&lt;/p&gt;
    &lt;p&gt;The non-hybrid draft was first posted in March 2024. Of course someone asked "what the motivation is for being 'fully post-quantum' rather than hybrid". The draft author responded: "FIPS / CNSA 2.0 compliance guidelines ... currently are a big 'maybe' at best for 'hybrid solutions', and the timetables for compliant browsers, servers, and services are to exclusively use FIPS 203 at level V (ML-KEM-1024) by 2033. I figure there will be demand for pure ML-KEM key agreement, not hybrid (with no questions that come along with it of whether it's actually allowed or not)."&lt;/p&gt;
    &lt;p&gt;As context, the massive U.S. military budget now publicly requires cryptographic "components" to have NSA approval. "CNSA 2.0" refers to a public NSA document "Commercial National Security Algorithm Suite 2.0". The document says up front that its requirements apply to "all NSS use of public cryptographic algorithms (as opposed to algorithms NSA developed), including those on all unclassified and classified NSS". The legal definition of "national security system" (NSS) covers practically all military information systems, except for "routine administrative and business applications" such as "payroll".&lt;/p&gt;
    &lt;p&gt;In June 2024, NSA's William Layton wrote that "we do not anticipate supporting hybrid in NSS".&lt;/p&gt;
    &lt;p&gt;In December 2024, a Cisco employee wrote the following: "There are people whose cryptographic expertise I cannot doubt who say that pure ML-KEM is the right trade-off for them, and more importantly for my employer, that's what they're willing to buy. Hence, Cisco will implement it; I am essentially just asking for code points." Certainly "willing to buy" is a statement about funding, evidently from a source large enough to dictate Cisco actions, evidently from a source asking for non-hybrids, evidently from "people whose cryptographic expertise I cannot doubt"; if that source isn't NSA, who is it?&lt;/p&gt;
    &lt;p&gt;(Side note: If you think the word "pure" in "pure ML-KEM" sounds good, remember that replacing CECPQ2's ECC+SIKE with "pure SIKE" would have been a disaster.)&lt;/p&gt;
    &lt;p&gt;In June 2025, NSA's Mike Jenkins posted the following: "As the CNSA 2.0 profiles should make clear, we are looking for products that support /standalone/ ML-DSA-87 and /standalone/ ML-KEM-1024. If there is one vendor that produces one product that complies, then that is the product that goes on the compliance list and is approved for use. Our interactions with vendors suggests that this won't be a problem in most cases." Evidently there are many companies happy to jump when NSA says jump.&lt;/p&gt;
    &lt;p&gt;Pretending to eat your own dog food. For software engineers, "dogfooding" (a term perhaps coined by Paul Maritz in the 1980s) refers to making regular use of the software that you're writing. This builds your confidence that the software works, and helps iron out problems.&lt;/p&gt;
    &lt;p&gt;But there's also a marketing version of the same concept, where you publicly say that you're using your own software as a way to build other people's confidence in the software. As in other types of marketing, what you're saying doesn't have to be true.&lt;/p&gt;
    &lt;p&gt;Once upon a time, NSA weakened the Data Encryption Standard to just 56 bits. In public, NSA claimed that it hadn't tampered with the standard, and that the "implausibility of public allegations is further demonstrated by the fact that NSA has endorsed the use of DES for the encryption of national security-related information, including selected classified information".&lt;/p&gt;
    &lt;p&gt;This is powerful marketing. Many people hearing this last quote will think "Oh, okay, NSA is using DES, so DES is strong". Koblitz and Menezes claimed that it's "far-fetched" that NSA would have intentionally selected something weak "for U.S. government usage (for both unclassified and classified communications [41])". Many people today will think "Oh, okay, NSA is buying single encryption, so double encryption is unimportant".&lt;/p&gt;
    &lt;p&gt;But DES wasn't strong. NSA had engineered DES to be "weak enough" for NSA to break. NSA wanted DES to "drive out competitors", to "reduce the field that NSA had to be concerned about".&lt;/p&gt;
    &lt;p&gt;It's perfectly plausible that NSA was using DES, but surely NSA was then using DES multiple times (Triple-DES or beyond), which makes it much harder to break (as long as you switch keys frequently). Obviously NSA wouldn't have said "use multiple layers" publicly: NSA wanted to fool people into thinking that DES was secure.&lt;/p&gt;
    &lt;p&gt;Today we have better ciphers than DES. However, for data that it cares about, NSA still uses two independent encryption layers "to mitigate the ability of an adversary to exploit a single cryptographic implementation". Gee, maybe multiple encryption is important after all!&lt;/p&gt;
    &lt;p&gt;Try to put yourself in the mindset of NSA as an attacker. You have a massive budget to "covertly influence and/or overtly leverage" systems to "make the systems in question exploitable"; "to the consumer and other adversaries, however, the systems' security remains intact". One of your action items is to "influence policies, standards and specification for commercial public key technologies". Another is to "shape the worldwide commercial cryptography marketplace to make it more tractable to advanced cryptanalytic capabilities being developed by NSA/CSS".&lt;/p&gt;
    &lt;p&gt;You spend this money pursuing many different attack paths, taking whatever surveillance wins you can get. It's not that everybody was using Dual EC, for example, but you managed to manipulate some people into using it, and for you that's a win.&lt;/p&gt;
    &lt;p&gt;Weakening ECC+PQ to just PQ, normalizing the practice of driving without seatbelts, is another win for you as the attacker. It's adding further vulnerabilities to the cryptographic ecosystem. The point is that, beyond SIKE and many other publicly broken cryptosystems, there will be some further cases where your "advanced cryptanalytic capabilities" break the PQ part while the "consumer and other adversaries" think the PQ part is secure.&lt;/p&gt;
    &lt;p&gt;What do you do with your control over the U.S. military budget? That's another opportunity to "shape the worldwide commercial cryptography marketplace". You can tell people that you won't authorize purchasing double encryption. You can even follow through on having the military publicly purchase single encryption. Meanwhile you quietly spend a negligible amount of money on an independent encryption layer to protect the data that you care about, so you're actually using double encryption.&lt;/p&gt;
    &lt;p&gt;Adoption of double encryption in TLS. "Adoption" in IETF is a preliminary step before standardization: when a WG is "ready to develop a particular document, the most common mechanism is for it to 'adopt' an existing document as a starting point".&lt;/p&gt;
    &lt;p&gt;In March 2025, after the close of a two-week "WG adoption call", the TLS WG chairs declared "consensus to adopt" the "Post-quantum hybrid ECDHE-MLKEM Key Agreement for TLSv1.3" draft.&lt;/p&gt;
    &lt;p&gt;There were no objections to the declaration of consensus on adopting this draft. I had pointed out that the patents on Kyber/ML-KEM create two issues related to IETF's patent policy, but I said that the first issue can be fixed after adoption (before standardization), and I now think that this is also true for the second issue. The risks from patents are orthogonal to the risks from non-hybrids, and I won't say more about patents in this blog post.&lt;/p&gt;
    &lt;p&gt;Why worry about a weaker standard if there's a stronger standard? At this point you might be wondering: if people are driving with seatbelts and this is on its way to being standardized, what's the problem with also having a driving-without-seatbelts standard for reckless fools who want to use that?&lt;/p&gt;
    &lt;p&gt;Think about Dual EC. Dual EC wasn't the only randomness-generation standard. But companies paid for FIPS certification of at least 62 different implementations of Dual EC. NSA bribed the RSA company to change its popular cryptographic library to use Dual EC by default.&lt;/p&gt;
    &lt;p&gt;These companies saw that Dual EC was a standard from a reputable standards organization (in fact, from three such organizations, namely ANSI, ISO, and NIST). Even for companies realizing that Dual EC was a controversial standard pushed by NSA, how many companies would risk losing money by refusing to implement Dual EC? It's easy for purchasing managers to use standards to set purchasing requirements.&lt;/p&gt;
    &lt;p&gt;What's particularly pernicious about a driving-without-seatbelts standard is that a purchasing manager who looks at it has an incentive to pick it instead of the driving-with-seatbelts standard. Wow, I can save $50 for every seatbelt that I skip! Wow, I can save 50 picodollars for every ECC operation that I skip! The purchasing manager doesn't care whether this cost reduction matters in context: every penny saved sounds good, right? The purchasing manager also doesn't realize the standard is dangerous: on the contrary, why would it be a standard if it were unsafe?&lt;/p&gt;
    &lt;p&gt;Soon we're faced with widespread non-usage of seatbelts. And then, years too late, we realize that, oops, something people used and thought was secure actually wasn't, just as in the case of SIKE.&lt;/p&gt;
    &lt;p&gt;Adoption of single encryption in TLS. On 1 April 2025âunfortunately not as a jokeâthe TLS WG chairs issued a two-week "WG adoption call for the ML-KEM Post-Quantum Key Agreement for TLS 1.3 I-D", the non-hybrid draft mentioned above.&lt;/p&gt;
    &lt;p&gt;Here are some quotes (some from me, some from other people) illustrating objections raised on the TLS mailing list during the call period:&lt;/p&gt;
    &lt;p&gt;The draft creates security risks. Sample quote: "SIKE was applied to large volumes of user data as part of the CECPQ2 experiment in 2019. SIKE was publicly broken in 2022. [paragraph break] The only reason that this didn't immediately give away the user data to attackers is that CECPQ2 was ECC+SIKE, rather than just SIKE. [paragraph break] Should we keep rolling out post-quantum cryptosystems to try to stop future quantum attacks? Yes, of course. But, just in case this goes horribly wrong again, let's make sure to keep ECC in place. Any draft violating this should be rejected as a security risk not just by WGs but also by the ISE."&lt;/p&gt;
    &lt;p&gt;The draft violates BCP 188. Sample quote: "To the extent that this is an allusion to NSA purchasing, it violates BCP 188 ('IETF Will Work to Mitigate Pervasive Monitoring')."&lt;/p&gt;
    &lt;p&gt;The draft violates the WG charter. Sample quote: "the draft's regression from ECC+PQ to just PQ is certainly a technology issue; and this is fatal, as a contravention of the 'improve security' goal in the WG charter".&lt;/p&gt;
    &lt;p&gt;There are no principles supporting the adoption decision. Sample quote: "I don't see what criteria we might use in adopting this that wouldn't leave the WG open to accusations of favouritism if we don't adopt other pure PQ national standards that will certainly arise".&lt;/p&gt;
    &lt;p&gt;The draft's motivation section is circular. Sample quote: there is "a preliminary step that has been skipped here, namely identifying why the proposal is claimed to be adding something important. The draft's motivation sentence consists of rearranging buzzwords without answering the question: 'Having a fully post-quantum (not hybrid) key agreement option for TLS 1.3 is necessary for migrating beyond hybrids and for users that need to be fully post-quantum.' "&lt;/p&gt;
    &lt;p&gt;The draft increases software complexity. Sample quote: "The main stated benefit of using a standalone ML-KEM is complexity reduction, but with the current progress in the deployment of the ML-KEM + ECC hybrid method, a standalone ML-KEM method actually increases overall complexity in software stacks." (This was responding to a claim during the adoption-call period that the draft provided a "compute / dependency base that is minimalist".)&lt;/p&gt;
    &lt;p&gt;This is just a high-level survey of the objections. These quotes aren't intended to convey the full text of objections. They also aren't intended to convey the number of people objecting; I'll get back to that below.&lt;/p&gt;
    &lt;p&gt;Standardization procedures. How does a standards-development organization handle objections? The law on this topic in the United States has been settled for decades.&lt;/p&gt;
    &lt;p&gt;The starting point is that agreements in restraint of interstate commerce are illegal. Courts interpret this to cover various types of agreements that are illegal per se, such as price fixing and group boycotts, along with further agreements that unreasonably interfere with competition.&lt;/p&gt;
    &lt;p&gt;Here's an example from the 1980s. Agents of a company that was leading its market, McDonnell and Miller, took control of a subcommittee of the American Society of Mechanical Engineers, a standards-development organization. They generated a letter, under ASME's apparent authority, declaring that a new competitor's product wasn't compliant. They distributed that letter to buyers, of course damaging the new competitor's business.&lt;/p&gt;
    &lt;p&gt;The competitor, HydroLevel, sued the conspiratorsâincluding ASME, which didn't even know the abuse was happening. HydroLevel won. ASME was ultimately forced to pay millions of dollars. The Supreme Court didn't mince words in describing the anti-competitive power of standards-development organizations:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ASME wields great power in the Nation's economy. Its codes and standards influence the policies of numerous States and cities, and, as has been said about "so-called voluntary standards" generally, its interpretations of its guidelines "may result in economic prosperity or economic failure, for a number of businesses of all sizes throughout the country," as well as entire segments of an industry. ... ASME can be said to be "in reality, an extragovernmental agency which prescribes rules for the regulation and restraint of interstate commerce." ... When it cloaks its subcommittee officials with the authority of its reputation, ASME permits those agents to affect the destinies of businesses, and thus gives them the power to frustrate competition in the marketplace.&lt;/p&gt;
      &lt;p&gt;... Many of ASME's officials are associated with members of the industries regulated by ASME's codes. Although undoubtedly most serve ASME without concern for the interests of their corporate employers, some may well view their positions with ASME, at least in part, as an opportunity to benefit their employers. When the great influence of ASME's reputation is placed at their disposal, the less altruistic of ASME's agents have an opportunity to harm their employers' competitors through manipulation of ASME's codes.&lt;/p&gt;
      &lt;p&gt;... Only ASME can take systematic steps to make improper conduct on the part of all its agents unlikely, and the possibility of civil liability will inevitably be a powerful incentive for ASME to take those steps. Thus, a rule that imposes liability on the standard-setting organization -- which is best situated to prevent antitrust violations through the abuse of its reputation -- is most faithful to the congressional intent that the private right of action deter antitrust violations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Another Supreme Court case rejected an argument of antitrust immunity for another standards-development organization. The organization made various decisions by majority vote, and had allowed steel manufacturers to pack a standards-development group, filling the group with pro-steel agents to take over a vote. The Supreme Court again recognized the importance of procedural safeguards preventing abuse:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The antitrust validity of these efforts is not established, without more, by petitioner's literal compliance with the rules of the Association, for the hope of procompetitive benefits depends upon the existence of safeguards sufficient to prevent the standard-setting process from being biased by members with economic interests in restraining competition. An association cannot validate the anticompetitive activities of its members simply by adopting rules that fail to provide such safeguards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Supreme Court declined at that point to draw a dividing line saying which safeguards were required. In 2004, Congress passed a law pinning this down: the new law said that "standards development activity" by a "standards development organization" isn't illegal per se, and gave definitions of the quoted phrases.&lt;/p&gt;
    &lt;p&gt;In particular, a "standards development organization" is required by law to "incorporate the attributes of openness, balance of interests, due process, an appeals process, and consensus in a manner consistent with the Office of Management and Budget Circular Number A-119, as revised February 10, 1998".&lt;/p&gt;
    &lt;p&gt;That OMB rule, in turn, defines "consensus" as follows: "general agreement, but not necessarily unanimity, and includes a process for attempting to resolve objections by interested parties, as long as all comments have been fairly considered, each objector is advised of the disposition of his or her objection(s) and the reasons why, and the consensus body members are given an opportunity to change their votes after reviewing the comments".&lt;/p&gt;
    &lt;p&gt;The Antitrust Division of the Department of Justice inserted itself into a private court case in 2019 to say that "the United States has a significant interest in the correct interpretation of the exemption from per se treatment for standards development organizations engaging in standard setting activities".&lt;/p&gt;
    &lt;p&gt;Deputy Assistant Attorney General Alexander Okuliar in the same division presented a longer statement to ANSI in 2020 regarding antitrust and standards. The statement mentioned ANSI's compliance with the same requirements and said "From an antitrust perspective, these requirements are central".&lt;/p&gt;
    &lt;p&gt;Here's a random example of what an objection-response document looks like in ISO, IEC, etc. Not the best user interface, but it gets the job done.&lt;/p&gt;
    &lt;p&gt;There was not general agreement to adopt the non-hybrid draft. Now that we have the concept of consensus in mind, let's go back to what happened in the IETF TLS WG regarding the non-hybrid draft.&lt;/p&gt;
    &lt;p&gt;During the adoption-call period, there were statements from 20 people unequivocally supporting adoption: David Adrian from Google, Joseph Birr-Pixton, Uri Blumenthal from Department of Defense subsidiary Lincoln Labs, "Flo D" from GCHQ, Quynh Dang from NIST, Viktor Dukhovni, Scott Fluhrer from Cisco, Rebecca Guthrie from NSA, Russ Housley, Alicja Kario from IBM subsidiary Red Hat, Kris Kwiatkowski, Andrei Popov from Microsoft, Tirumal Reddy from Cisco, Yaroslav Rosomakho, Jan Schaumann, Sophie Schmieg from Google, Martin Thomson from Mozilla, Filippo Valsorda formerly from Google, Loganaden Velvindron, and Thom Wiggers.&lt;/p&gt;
    &lt;p&gt;There were also statements from 2 people conditionally supporting adoption: John Mattsson from Ericsson ("I support adoption as long as reuse of ephemeral keys is normatively forbidden, i.e. MUST NOT reuse") and Yaakov Stein ("I support adoption of pure PQC KEMs drafts with Intended status: Informational (meaning that the IETF is not recommending using)").&lt;/p&gt;
    &lt;p&gt;However, there were statements from 7 people unequivocally opposing adoption: Thomas Bellebaum ("I agree with Stephen on this one and would not support adoption of non-hybrids"), Andrey Jivsov ("I am opposed to the adoption of ML-KEM at this time"), Stephen Farrell ("I'm opposed to adoption, at this time"), Rich Salz ("I was all set to say that I am in favor of adoption, but Stephen's post changed my mind. [paragraph break] The conservative and safe thing is to stick to hybrids and that is what the IETF should do for now"), Rob Sayre ("I oppose adoption"), Sun Shuzhou ("I'm opposed to adoption"), and me.&lt;/p&gt;
    &lt;p&gt;Even assuming that the 2 statements of conditional support are treated as positive votes, the overall situation here, 22 positive votes and 7 negative votes, does not qualify as general agreement. "General" means "shared by or affecting most people, or most of the people in a group"; "most" means "nearly all of the people or things in a group, or nearly all of something"; the phrase "general agreement" means that nearly everyone agrees. Merely having three quarters agree is not good enough.&lt;/p&gt;
    &lt;p&gt;What happens if a standards-development organization issues a rule declaring that "general agreement" exists even when a quarter of the votes are in opposition? I haven't found any court cases on point, but I would expect courts to reject this as being inconsistent with the plain meaning of "general agreement".&lt;/p&gt;
    &lt;p&gt;Anyway, IETF hasn't attempted to issue such a rule. On the contrary, IETF claims that WG decisions are not taken by voting: "Decisions within WGs, as with the broader IETF, are taken by 'rough consensus' and not by voting." This begs the question of what IETF thinks "rough consensus" means. Letting chairs make arbitrary decisions is a violation of due process.&lt;/p&gt;
    &lt;p&gt;More to the point, IETF can't override the definition of "consensus" in the law. That definition requires general agreement. Adoption of this draft was controversial, and didn't reach general agreement.&lt;/p&gt;
    &lt;p&gt;Objections were not handled properly. Within the statements in favor of adoption, most of the statements were very short: e.g., just the words "I support adoption" with no further comments.&lt;/p&gt;
    &lt;p&gt;Some statements in favor of adoption did say more, such as stating circular arguments for the draft (e.g.: "as time progresses, non-hybrid key exchanges will become more and more commonplace, so why not have it already defined?"), or expressing concerns about key reuse (e.g.: "I also share John's concerns about key reuse, but would prefer to litigate that in the working group, rather than during adoption"), without responding to the content of the objections.&lt;/p&gt;
    &lt;p&gt;There was a response to one word in the lack-of-principles objection. (The response was as follows: "The NIST competition was international, and Kyber was developed by an international team. I struggle to understand how adopting this document would somehow be 'favoritism'.") A brief note by one supporter tangentially related to one objection falls far short of fair consideration of each objection by the group as a whole.&lt;/p&gt;
    &lt;p&gt;I tried to engage that supporter in discussion. I started by quoting the following earlier statement in the commentator's message: "I find it to be cognitive dissonance to simultaneously argue that the quantum threat requires immediate work, and yet we are also somehow uncertain of if the algorithms are totally broken. Both cannot be true at the same time." I responded as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Rolling out PQ is trying to reduce the damage from an attacker having a quantum computer within the security lifetime of the user data. Doing that as ECC+PQ instead of just PQ is trying to reduce the damage in case the PQ part is broken. These actions are compatible, so how exactly do you believe they're contradictory?&lt;/p&gt;
      &lt;p&gt;Here's an analogous example of basic risk mitigation: there's endless work that goes into having planes not crash, not hit turbulence, etc., but we still ask airplane passengers to keep their seatbelts on whenever they're in their seats.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There was still no reply to this by the time the adoption call closed two weeks later.&lt;/p&gt;
    &lt;p&gt;The broader pattern was that objectors were engaging in discussion while supporters were not. The majority process wasn't "attempting to resolve each objection"; it was simply collecting positive votes, trying to override objections from the minority without even answering those objections, let alone trying to resolve them.&lt;/p&gt;
    &lt;p&gt;That's in an organization saying that decisions aren't taken by voting. The same organization also says, as part of explaining why it's supposedly complying with antitrust law: "IETF activities are conducted with extreme transparency, in public forums. Decision-making requires achieving broad consensus via these public processes."&lt;/p&gt;
    &lt;p&gt;When there's an objection, the legal concept of consensus requires not just fairly considering the objection, and not just attempting to resolve the objection, butâif resolution failsâhaving the group agree on the contents of a response to the objection. That's an official statement of why the objection was overridden. It's something that can be appealed if it's wrong. Consider, for example, ISO's simple rule saying "Committees are required to respond to all comments received". In IETF, there weren't even informal responses to the objections listed above, let alone official responses.&lt;/p&gt;
    &lt;p&gt;The chairs declared consensus anyway. Shortly before the end of the specified adoption-call period, the chairs declared "consensus to adopt this draft as a working group item". There were some notes on followup procedures, but there was no explanation of the rationale for this claim of consensus.&lt;/p&gt;
    &lt;p&gt;I challenged the claim of consensus:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Um, what? There were several people (including me) raising objections on list to basic flaws in this draft, such as (1) the failure to provide an ECC backup to limit the damage from further security problems in the PQ layer, (2) the failure to provide an engineering justification for this option, and (3) the lack of any principles that would justify saying no to options selected by other governments if this option is allowed.&lt;/p&gt;
      &lt;p&gt;Your message doesn't explain how you came to the conclusion that there's consensus. Surely you aren't relying on some tally of positive votes to ram this document through while ignoring objections; voting isn't how IETF is supposed to work. So how did you come to this conclusion?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A few days later, the chairs responded that they had declared consensus "because there is clearly sufficient interest to work on this draft".&lt;/p&gt;
    &lt;p&gt;I said that this was ambiguous (sufficient for what?); said that in any case this criterion was improper since it "would allow a draft to be adopted over amply justified objections of almost all WG participants, simply because the chairs and a few participants say they have enough interest in working on the draft"; and asked for an explicit statement of whether this was the complete explanation of why the chairs had declared consensus.&lt;/p&gt;
    &lt;p&gt;The chairs responded that "sufficient" means "that there were enough people willing to review the draft". They added that "WGs groups have adopted drafts with much less support than this one received." Gee, that's confidence-inspiring.&lt;/p&gt;
    &lt;p&gt;Meanwhile an IETF "security area director" had jumped into the discussion, in particular writing "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions".&lt;/p&gt;
    &lt;p&gt;Remember that the actual tallies were 20 supporters, 2 conditional supporters, and 7 opponents, even if some people (for example, me) had sent multiple messages. Nobody had posted the actual tallies at this point: there was just this "security area director" claiming that the "vast majority" of the "67 responses" were in favor while there were only "a few dissenting opinions". Also remember that this is an organization that claims that it doesn't make decisions by voting.&lt;/p&gt;
    &lt;p&gt;The "security area director" continued that "you cherry-picking when to call consensus evaluation 'voting' depending on whether misnaming this is in your advantage ... is dishonestly manipulative"; that I was violating the "code of conduct"; and that if I did not "voluntarily stop this kind of behaviour" there would be "measures under the terms of RFC3934 which is part of BCP25".&lt;/p&gt;
    &lt;p&gt;In a followup message, the "security area director" wrote "you calling into question this consensus call of the WG chair is abusive and follows a repetitive pattern. Nevertheless, for now this is your right ... you are attempting to bait the chairs to say they took inventory of the public emails ... there comes a point where you will be prevented from further playing these games".&lt;/p&gt;
    &lt;p&gt;Wait a minute: "for now this is your right" (emphasis added) and "you will be prevented from further playing these games"? Sounds ominous. What did the "security area director" mean by this? No more objections in IETF? No more appeals? NSA's minions can just ram their non-consensual drafts through IETF without opponents even being allowed to speak up?&lt;/p&gt;
    &lt;p&gt;Actually, yes, there's a stealth activity going on right now that will have this effect unless enough people take action by Tuesday the 7th. I hope to have another blog post up in a day or two saying what's going on here.&lt;/p&gt;
    &lt;p&gt;Anyway, I've filed a formal complaint regarding the claim of consensus to adopt. So far the complaint hasn't been handled properly, but hope springs eternal. I don't have an answer yet to the subtitle question of this blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cr.yp.to/20251004-weakened.html"/><published>2025-10-04T22:16:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45478033</id><title>Parrot – type-safe SQL in Gleam, supports SQlite, PostgreSQL and MySQL</title><updated>2025-10-05T16:40:56.664730+00:00</updated><content>&lt;doc fingerprint="e20ba422ef3c70fe"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;🚨 Exciting News&lt;/p&gt;&lt;lb/&gt;Parrot got listed a community project on the sqlc website! 🦜🎉&lt;lb/&gt;Check it out here: https://docs.sqlc.dev/en/latest/reference/language-support.html&lt;/quote&gt;
    &lt;p&gt;Table of contents generated with markdown-toc&lt;/p&gt;
    &lt;p&gt;Most of the heavy lifting features are provided by / built into sqlc, I do not aim to take credit for them.&lt;/p&gt;
    &lt;p&gt;☑️ Supports SQlite, PostgreSQL and MySQL.&lt;lb/&gt; ☑️ Multiple queries per file.&lt;lb/&gt; ☑️ Database client agnostic.&lt;lb/&gt; ☑️ Utility wrappers for popular gleam database libraries (lpil/sqlight, lpil/pog).&lt;lb/&gt; ☑️ Automatically pulls the schema of your database.&lt;lb/&gt; ☑️ Automatically downloads sqlc binary.&lt;lb/&gt; ☑️ Named parameters.*1 &lt;/p&gt;
    &lt;p&gt;*1: Meaning that it infers the names of the parameters from your sql queries in the gleam function you call. for example for a query called &lt;code&gt;FindUser&lt;/code&gt;, defined as &lt;code&gt;SELECT * FROM user WHERE username = $1&lt;/code&gt;, parrot will produce a function where the arguments match those column names: &lt;code&gt;pub fn find_user(username: String) { ... }&lt;/code&gt;. If you have multiple parameters of the same data types this can avoid confusion and bugs.&lt;/p&gt;
    &lt;code&gt;$ gleam add parrot&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parrot will look for all *.sql files in any sql directory under your project's src directory.&lt;/item&gt;
      &lt;item&gt;Each *.sql file can contain as many SQL queries as you want.&lt;/item&gt;
      &lt;item&gt;All of the queries will compile into a single &lt;code&gt;src/[project name]/sql.gleam&lt;/code&gt;module.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some links to help you start out, if you are unfamiliar with the sqlc annotation syntax:&lt;/p&gt;
    &lt;p&gt;Here is an example of the file structure:&lt;/p&gt;
    &lt;code&gt;├── gleam.toml
├── README.md
├── src
│   ├── app.gleam
│   └── sql
│       ├── auth.sql
│       └── posts.sql
└── test
   └── app_test.gleam&lt;/code&gt;
    &lt;code&gt;# automatically detects database &amp;amp; engine from env (DATABASE_URL by default)
$ gleam run -m parrot

# provide connection string from different environment variable
$ gleam run -m parrot -- -e PG_DATABASE_URL

# specify sqlite file
$ gleam run -m parrot -- --sqlite &amp;lt;file_path&amp;gt;

# see all options
$ gleam run -m parrot help&lt;/code&gt;
    &lt;p&gt;If you use SQLite, you also need to have installed sqlite3.&lt;/p&gt;
    &lt;p&gt;If you use MySQL, you also need to have installed mysqldump (comes by default if you have a mysql client installed).&lt;/p&gt;
    &lt;p&gt;If you use PostgreSQL, you also need to have installed pg_dump (comes by default if you have a postgresql client installed).&lt;/p&gt;
    &lt;p&gt;You now have type safe access to your sql queries.&lt;/p&gt;
    &lt;p&gt;You might want to write wrapper functions for the database client library of your choice. If you are using lpil/pog or lpil/sqlight, you are in luck! You can find functions to copy &amp;amp; paste into your codebase here: wrappers&lt;/p&gt;
    &lt;p&gt;An example with lpil/sqlight:&lt;/p&gt;
    &lt;code&gt;import app/sql
import parrot/dev

fn parrot_to_sqlight(param: dev.Param) -&amp;gt; sqlight.Value {
  // ...
}

pub fn main() {
  // ...

  let #(sql, with, expecting) = sql.get_user_by_username("alice")
  let with = parrot_to_sqlight(with)
  let row = sqlight.query(sql, on:, with:, expecting:)

  // ...
}&lt;/code&gt;
    &lt;p&gt;If you want to see how this library works in action, take a look at the integration tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL: integration/psql&lt;/item&gt;
      &lt;item&gt;MySQL: integration/mysql&lt;/item&gt;
      &lt;item&gt;SQlite: integration/sqlite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;just is used to run project commands.&lt;/p&gt;
    &lt;p&gt;There are scripts to spawn a MySQL or PostgreSQL docker container:&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;$ ./bin/mysql.sh
# or
$ ./bin/psql.sh&lt;/code&gt;
    &lt;code&gt;$ just test-sqlite
$ just test-mysql
$ just test-psql&lt;/code&gt;
    &lt;p&gt;As with everything in software, there are some quirks with this library, due to the nature of your database of choice and sqlc.&lt;/p&gt;
    &lt;p&gt;If you have an &lt;code&gt;INTEGER[][]&lt;/code&gt; column in Postgres, &lt;code&gt;pg_dump&lt;/code&gt; does not correctly identify
the column as a two-dimensional array and therefore only gives you a &lt;code&gt;List(Int)&lt;/code&gt; instead
of a &lt;code&gt;List(List(Int))&lt;/code&gt;. If this is a problem for you, you can raise an issue and
we might come up with a solution or workaround.&lt;/p&gt;
    &lt;p&gt;There are a couple of complex data types that are explictly made &lt;code&gt;dynamic&lt;/code&gt;
since they are too complex to handle with the current implementation.
There is a plan for a better and more flexible implementation. Until then,
it will be wrapped in a dynamic type.&lt;/p&gt;
    &lt;p&gt;So here is the catch: you can only execute parrot in an erlang gleam application. However the generated code will also run in a javascript environment. So if you need parrot for a javascript project, you can create a separate package and copy over the generated module and that will work.&lt;/p&gt;
    &lt;p&gt;This library supports everything that sqlc supports. As the time of this writing that would be MySQL, PostgreSQL and SQlite.&lt;/p&gt;
    &lt;p&gt;You can read more on language &amp;amp; SQL support here: https://docs.sqlc.dev/en/stable/reference/language-support.html&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;embeddeding structs (https://docs.sqlc.dev/en/stable/howto/embedding.html)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Certain query annotations are not supported and will panic the process:&lt;/p&gt;&lt;code&gt;:execrows&lt;/code&gt;,&lt;code&gt;:execlastid&lt;/code&gt;,&lt;code&gt;:batchexec&lt;/code&gt;,&lt;code&gt;:batchone&lt;/code&gt;,&lt;code&gt;:batchmany&lt;/code&gt;,&lt;code&gt;:copyfrom&lt;/code&gt;. You can read more about it here: https://docs.sqlc.dev/en/stable/reference/query-annotations.html&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ideas and actionable tasks are collected and organised here: https://github.com/daniellionel01/parrot/issues&lt;/p&gt;
    &lt;p&gt;Contributions are welcomed!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This project was heavily inspired by &lt;code&gt;squirrel&lt;/code&gt;(Hex, GitHub). Thank you @giacomocavalieri!&lt;/item&gt;
      &lt;item&gt;Thank you to &lt;code&gt;sqlc&lt;/code&gt;(GitHub, Website)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/daniellionel01/parrot"/><published>2025-10-05T00:51:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45478749</id><title>Americans increasingly see legal sports betting as a bad thing for society</title><updated>2025-10-05T16:40:56.377987+00:00</updated><content>&lt;doc fingerprint="59d2d4ad970264c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Public awareness of legal sports betting has grown in recent years – and so has the perception that it is a bad thing for society and sports, according to a new Pew Research Center survey.&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;For society (July 2022)&lt;/cell&gt;
        &lt;cell role="head"&gt;For society (July 2025)&lt;/cell&gt;
        &lt;cell role="head"&gt;For sports (July 2022)&lt;/cell&gt;
        &lt;cell role="head"&gt;For sports (July 2025)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;A bad thing&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
        &lt;cell&gt;33%&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;A good thing&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
        &lt;cell&gt;7%&lt;/cell&gt;
        &lt;cell&gt;16%&lt;/cell&gt;
        &lt;cell&gt;17%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Neither good nor bad&lt;/cell&gt;
        &lt;cell&gt;57%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;49%&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;Today, 43% of U.S. adults say the fact that sports betting is now legal in much of the country is a bad thing for society. That’s up from 34% in 2022. And 40% of adults now say it’s a bad thing for sports, up from 33%.&lt;/p&gt;
    &lt;p&gt;Despite these increasingly critical views of legal sports betting, many Americans continue to say it has neither a bad nor good impact on society and on sports. Fewer than one-in-five see positive impacts.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the share of Americans who have bet money on sports in the past year has not changed much since 2022.&lt;/p&gt;
    &lt;p&gt;Today, 22% of adults say they’ve personally bet money on sports in the past year. That’s a slight uptick from 19% three years ago. This figure includes betting in any of three ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With friends or family, such as in a private betting pool, fantasy league or casual bet&lt;/item&gt;
      &lt;item&gt;Online with a betting app, sportsbook or casino&lt;/item&gt;
      &lt;item&gt;In person at a casino, racetrack or betting kiosk&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;July 2022&lt;/cell&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;With friends and family, such as in a private betting pool, fantasy league or casual bet&lt;/cell&gt;
        &lt;cell&gt;15%&lt;/cell&gt;
        &lt;cell&gt;15%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Online with a betting app, sportsbook or casino&lt;/cell&gt;
        &lt;cell&gt;6%&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;In person at a casino, racetrack or betting kiosk&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ANY of the above ways&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All of this increase has come through online sports betting: 10% of adults now say they’ve placed a bet this way in the past year, up from 6% in 2022. There has been no change in the shares of adults who have bet on sports with family or friends or in person at a casino, racetrack or betting kiosk.&lt;/p&gt;
    &lt;p&gt;Commercial sports betting has spread rapidly across the United States since a Supreme Court ruling in 2018 gave states the green light to legalize it. At least 38 states, the District of Columbia and Puerto Rico now allow commercial sports betting in some form, according to the National Conference of State Legislatures.&lt;/p&gt;
    &lt;p&gt;In our new survey, 63% of adults say they’ve heard or read a lot or a little about the fact that sports betting is now legal in much of the U.S. That’s up from 56% in 2022. The increase in public awareness comes as betting-related advertisements have become common during sports broadcasts.&lt;/p&gt;
    &lt;p&gt;The rest of this analysis takes a closer look at Americans’ views of and experiences with sports betting. It’s based on the survey of 9,916 U.S. adults, conducted in July and August.&lt;/p&gt;
    &lt;head rend="h4"&gt;Many demographic groups increasingly view legal sports betting as a bad thing&lt;/head&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;July 2022&lt;/cell&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;U.S. adults&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Men&lt;/cell&gt;
        &lt;cell&gt;35%&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Women&lt;/cell&gt;
        &lt;cell&gt;33%&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ages 18-29&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
        &lt;cell&gt;41%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;30-49&lt;/cell&gt;
        &lt;cell&gt;29%&lt;/cell&gt;
        &lt;cell&gt;39%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;50-64&lt;/cell&gt;
        &lt;cell&gt;37%&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;65+&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
        &lt;cell&gt;49%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;White&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
        &lt;cell&gt;46%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Black&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hispanic&lt;/cell&gt;
        &lt;cell&gt;29%&lt;/cell&gt;
        &lt;cell&gt;37%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Asian*&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
        &lt;cell&gt;48%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;College grad&lt;/cell&gt;
        &lt;cell&gt;39%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Non-college grad&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
        &lt;cell&gt;38%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Upper income&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Middle income&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;44%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Lower income&lt;/cell&gt;
        &lt;cell&gt;28%&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Rep/Lean Rep&lt;/cell&gt;
        &lt;cell&gt;38%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dem/Lean Dem&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sports bettor&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Non-sports bettor&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;Since 2022, Americans in many demographic groups have become more likely to view the widespread legalization of sports betting as a bad thing for society, as well as for sports.&lt;/p&gt;
    &lt;p&gt;This is true for men and women; college graduates and non-college graduates; and upper-, middle- and lower-income Americans alike. It is also the case among Democrats and Republicans, as well as among those who have personally placed a sports bet in the past year and those who have not.&lt;/p&gt;
    &lt;p&gt;Some of the biggest shifts in attitudes about sports betting’s societal impact have come among young Americans – especially young men. Today, 47% of men under 30 say legal sports betting is a bad thing for society, up from 22% who said this in 2022. Women under 30 have also become more likely to express this view: 35% see legal sports betting as bad for society, up from 25% three years ago.&lt;/p&gt;
    &lt;p&gt;The legalization of sports betting has generated revenue for state governments and gambling operators, but it has also raised concerns about gambling addiction and other societal harms. Critics have also cautioned that it may compromise the integrity of sports. In recent years, several professional and college athletes and team personnel have been punished for violating betting rules.&lt;/p&gt;
    &lt;head rend="h4"&gt;Who has bet money on sports in the past year?&lt;/head&gt;
    &lt;p&gt;Note: White, Black and Asian adults include those who report being only one race and not Hispanic. Hispanic adults are of any race. Family income tiers are based on adjusted 2024 earnings. The full question wording was “With friends or family (such as a private betting pool, fantasy league, or casual bet).”&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;U.S. adults&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Men&lt;/cell&gt;
        &lt;cell&gt;25%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Women&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ages 18-29&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;30-49&lt;/cell&gt;
        &lt;cell&gt;26%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50-64&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;65+&lt;/cell&gt;
        &lt;cell&gt;12%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;White&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Black&lt;/cell&gt;
        &lt;cell&gt;30%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Hispanic&lt;/cell&gt;
        &lt;cell&gt;27%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Asian*&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;College grad&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Non-college grad&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Upper income&lt;/cell&gt;
        &lt;cell&gt;26%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Middle income&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Lower income&lt;/cell&gt;
        &lt;cell&gt;21%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Rep/Lean Rep&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Dem/Lean Dem&lt;/cell&gt;
        &lt;cell&gt;24%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: White, Black and Asian adults include those who report being only one race and not Hispanic. Hispanic adults are of any race. Family income tiers are based on adjusted 2024 earnings. The full question wording was “With friends or family (such as a private betting pool, fantasy league, or casual bet).”&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;As was the case in 2022, some groups of Americans are more likely than others to say they’ve personally bet money on sports in the past year in any of the ways we asked about.&lt;/p&gt;
    &lt;p&gt;Young adults are more likely than older Americans to say they’ve placed a sports bet in the past year. Some 31% of adults under 30 say this, including 36% of men and 29% of women in this age group. Sports betting is less common in all older age groups.&lt;/p&gt;
    &lt;p&gt;Black and Hispanic adults are also especially likely to have bet money on sports in the past year: 30% and 27%, respectively, say they have done so. Roughly two-in-ten Asian (22%) and White (19%) adults say the same.&lt;/p&gt;
    &lt;p&gt;There are no differences between college graduates and non-college graduates on this question. In each group, 22% say they have bet on sports in the past year. Nor are there major partisan differences: 24% of Democrats and Democratic-leaning independents say they have done so, as have 22% of Republicans and Republican leaners.&lt;/p&gt;
    &lt;head rend="h4"&gt;Who has placed an online sports bet in the past year?&lt;/head&gt;
    &lt;p&gt;When it comes to online sports betting, young adults and Black Americans again stand out.&lt;/p&gt;
    &lt;p&gt;Overall, 17% of adults under 30 – including 21% of men and 16% of women in this age group – say they’ve placed an online sports wager in the past year. Three years ago, 7% of those under 30 had done so – including 9% of men and 6% of women.&lt;/p&gt;
    &lt;p&gt;Among Black adults, 19% say they’ve placed an online sports bet in the past year, up from 10% in 2022. Smaller shares of Hispanic (12%), Asian (11%) and White (8%) adults say they’ve done this in the past year.&lt;/p&gt;
    &lt;p&gt;Note: Senior Writer Drew DeSilver and Research Analyst Ted Van Green contributed to this analysis. Here are the questions used, the topline and the survey methodology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pewresearch.org/short-reads/2025/10/02/americans-increasingly-see-legal-sports-betting-as-a-bad-thing-for-society-and-sports/"/><published>2025-10-05T04:01:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45478780</id><title>Ambigr.am</title><updated>2025-10-05T16:40:55.791105+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ambigr.am/hall-of-fame"/><published>2025-10-05T04:11:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45479006</id><title>Managing context on the Claude Developer Platform</title><updated>2025-10-05T16:40:55.634232+00:00</updated><content>&lt;doc fingerprint="bf43ca3f38eb8046"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Managing context on the Claude Developer Platform&lt;/head&gt;
    &lt;p&gt;Today, we’re introducing new capabilities for managing your agents’ context on the Claude Developer Platform: context editing and the memory tool.&lt;/p&gt;
    &lt;p&gt;With our latest model, Claude Sonnet 4.5, these capabilities enable developers to build AI agents capable of handling long-running tasks at higher performance and without hitting context limits or losing critical information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context windows have limits, but real work doesn’t&lt;/head&gt;
    &lt;p&gt;As production agents handle more complex tasks and generate more tool results, they often exhaust their effective context windows—leaving developers stuck choosing between cutting agent transcripts or degrading performance. Context management solves this in two ways, helping developers ensure only relevant data stays in context and valuable insights get preserved across sessions.&lt;/p&gt;
    &lt;p&gt;Context editing automatically clears stale tool calls and results from within the context window when approaching token limits. As your agent executes tasks and accumulates tool results, context editing removes stale content while preserving the conversation flow, effectively extending how long agents can run without manual intervention. This also increases the effective model performance as Claude focuses only on relevant context.&lt;/p&gt;
    &lt;p&gt;The memory tool enables Claude to store and consult information outside the context window through a file-based system. Claude can create, read, update, and delete files in a dedicated memory directory stored in your infrastructure that persists across conversations. This allows agents to build up knowledge bases over time, maintain project state across sessions, and reference previous learnings without having to keep everything in context.&lt;/p&gt;
    &lt;p&gt;The memory tool operates entirely client-side through tool calls. Developers manage the storage backend, giving them complete control over where the data is stored and how it’s persisted.&lt;/p&gt;
    &lt;p&gt;Claude Sonnet 4.5 enhances both capabilities with built-in context awareness—tracking available tokens throughout conversations to manage context more effectively.&lt;/p&gt;
    &lt;p&gt;Together, these updates create a system that improves agent performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable longer conversations by automatically removing stale tool results from context&lt;/item&gt;
      &lt;item&gt;Boost accuracy by saving critical information to memory—and bring that learning across successive agentic sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Building long-running agents&lt;/head&gt;
    &lt;p&gt;Claude Sonnet 4.5 is the best model in the world for building agents. These features unlock new possibilities for long-running agents—processing entire codebases, analyzing hundreds of documents, or maintaining extensive tool interaction histories. Context management builds on this foundation, ensuring agents can leverage this expanded capacity efficiently while still handling workflows that extend beyond any fixed limit. Use cases include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coding: Context editing clears old file reads and test results while memory preserves debugging insights and architectural decisions, enabling agents to work on large codebases without losing progress.&lt;/item&gt;
      &lt;item&gt;Research: Memory stores key findings while context editing removes old search results, building knowledge bases that improve performance over time.&lt;/item&gt;
      &lt;item&gt;Data processing: Agents store intermediate results in memory while context editing clears raw data, handling workflows that would otherwise exceed token limits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance improvements with context management&lt;/head&gt;
    &lt;p&gt;On an internal evaluation set for agentic search, we tested how context management improves agent performance on complex, multi-step tasks. The results demonstrate significant gains: combining the memory tool with context editing improved performance by 39% over baseline. Context editing alone delivered a 29% improvement.&lt;/p&gt;
    &lt;p&gt;In a 100-turn web search evaluation, context editing enabled agents to complete workflows that would otherwise fail due to context exhaustion—while reducing token consumption by 84%.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;These capabilities are available today in public beta on the Claude Developer Platform, natively and in Amazon Bedrock and Google Cloud’s Vertex AI. Explore the documentation for context editing and the memory tool, or visit our cookbook to learn more.&lt;/p&gt;
    &lt;p&gt;Anthropic is not affiliated with, endorsed by, or sponsored by CATAN GmbH or CATAN Studio. The CATAN trademark and game are the property of CATAN GmbH.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/context-management"/><published>2025-10-05T05:20:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45479165</id><title>Social Cooling (2017)</title><updated>2025-10-05T16:38:39.281829+00:00</updated><content>&lt;doc fingerprint="6ed2ac1a2f24f8dc"&gt;
  &lt;main&gt;&lt;p&gt;My colleagues and friends know that I prefer to communicate with them via email rather than chat messaging. There are many benefits in such a choice. You may want to consider them and adopt the same stance.&lt;/p&gt;&lt;p&gt;My messages arrive in a single program, where I can process and tag them. With messaging programs Iâd have to iterate through Teams, Signal, WhatsApp, Slack, Viber, FaceTime, LinkedIn, Messenger, Google Meet, Discord, Mattermost, Instagram, WebEx, and possibly others, to collect and process the messages sent on each platform.&lt;/p&gt;&lt;p&gt;Similarly, if I want to find a past message I have exactly one place to search: my email archive.&lt;/p&gt;&lt;p&gt;Companies get out of business or become acquired and services can easily be discontinued; for a reminder have a look at the 64 services Google has discontinued. If you ever exchanged messages on ICQ, AIM, MSN Messenger, Skype, Yahoo! Messenger, Google Hangouts, GChat, BlackBerry Messenger, or Campfire your messages are now gone. With email and local message storage you control the lifetime of your messages (provided you perform regular backups). My email archive contains the messages I have sent and received from 1986 onward.&lt;/p&gt;&lt;p&gt;Email clients offer rich functionality. In the Thunderbird email client, I use the following features:&lt;/p&gt;&lt;p&gt;Some messaging systems offer some of these features, but all features are certainly not universally available.&lt;/p&gt;&lt;p&gt;Having a single messaging interface allows me to invest in becoming maximally productive in the email client application Iâm using. I can learn its features in-depth, I can tailor it with plug-ins, and I can extend it to fit my needs. When using it (many hours a day) my mind and muscles memorize how to perform common actions. With messaging platforms Iâd only be able to dabble in each.&lt;/p&gt;&lt;p&gt;Rather than having flow and concentration interrupted by incoming message notifications, with email I can easily decide when to fetch and process messages.&lt;/p&gt;&lt;p&gt;Some âfreeâ messaging services serve together with the messages ads or addictive content, such as short-form videos. Email clients will only display email messages.&lt;/p&gt;&lt;p&gt;Depending on the email provider I choose, I can obtain strong guarantees on who reads my email messages. Some, like Proton Mail are explicitly targeting people who want to protect their privacy. In contrast, many messaging platform will scan my messages to send me targeted ads or train their AI systems on them.&lt;/p&gt;&lt;p&gt;Email is transported with open protocols (SMTP, IMAP), which means I can use any email client and operating system I want and obtain any functionality I need, without depending on the business model or whims of the company controlling a proprietary messaging platform. I can even develop my own clients, something I have often done to automate the sending of multi-part email messages to students or conference committee members.&lt;/p&gt;&lt;p&gt;My messages are stored as plain text files in the super-simple Mbox file format, which means I can easily process them with other tools, reliably create backup copies, and move them from one email client to another.&lt;/p&gt;&lt;p&gt;For example, I have a small script that removes all attachments from old email messages, allowing me to keep my email archive in a manageable size. In other cases Iâve run on my message files scripts to analyze the messages I send and receive, and Iâve opened them in my editor to fix hardware-induced corruption.&lt;/p&gt;&lt;p&gt;In short, email can be an amazingly open and reliable environment that fosters exceptional productivity. We shouldnât settle for anything less.&lt;/p&gt;Comments Post Toot! Tweet&lt;p&gt;Last modified: Saturday, September 27, 2025 11:07 pm&lt;/p&gt;&lt;p&gt;Unless otherwise expressly stated, all original material on this page created by Diomidis Spinellis is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.socialcooling.com/"/><published>2025-10-05T06:01:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45479820</id><title>Benefits of choosing email over messaging</title><updated>2025-10-05T16:38:38.353639+00:00</updated><content>&lt;doc fingerprint="6ed2ac1a2f24f8dc"&gt;
  &lt;main&gt;&lt;p&gt;My colleagues and friends know that I prefer to communicate with them via email rather than chat messaging. There are many benefits in such a choice. You may want to consider them and adopt the same stance.&lt;/p&gt;&lt;p&gt;My messages arrive in a single program, where I can process and tag them. With messaging programs Iâd have to iterate through Teams, Signal, WhatsApp, Slack, Viber, FaceTime, LinkedIn, Messenger, Google Meet, Discord, Mattermost, Instagram, WebEx, and possibly others, to collect and process the messages sent on each platform.&lt;/p&gt;&lt;p&gt;Similarly, if I want to find a past message I have exactly one place to search: my email archive.&lt;/p&gt;&lt;p&gt;Companies get out of business or become acquired and services can easily be discontinued; for a reminder have a look at the 64 services Google has discontinued. If you ever exchanged messages on ICQ, AIM, MSN Messenger, Skype, Yahoo! Messenger, Google Hangouts, GChat, BlackBerry Messenger, or Campfire your messages are now gone. With email and local message storage you control the lifetime of your messages (provided you perform regular backups). My email archive contains the messages I have sent and received from 1986 onward.&lt;/p&gt;&lt;p&gt;Email clients offer rich functionality. In the Thunderbird email client, I use the following features:&lt;/p&gt;&lt;p&gt;Some messaging systems offer some of these features, but all features are certainly not universally available.&lt;/p&gt;&lt;p&gt;Having a single messaging interface allows me to invest in becoming maximally productive in the email client application Iâm using. I can learn its features in-depth, I can tailor it with plug-ins, and I can extend it to fit my needs. When using it (many hours a day) my mind and muscles memorize how to perform common actions. With messaging platforms Iâd only be able to dabble in each.&lt;/p&gt;&lt;p&gt;Rather than having flow and concentration interrupted by incoming message notifications, with email I can easily decide when to fetch and process messages.&lt;/p&gt;&lt;p&gt;Some âfreeâ messaging services serve together with the messages ads or addictive content, such as short-form videos. Email clients will only display email messages.&lt;/p&gt;&lt;p&gt;Depending on the email provider I choose, I can obtain strong guarantees on who reads my email messages. Some, like Proton Mail are explicitly targeting people who want to protect their privacy. In contrast, many messaging platform will scan my messages to send me targeted ads or train their AI systems on them.&lt;/p&gt;&lt;p&gt;Email is transported with open protocols (SMTP, IMAP), which means I can use any email client and operating system I want and obtain any functionality I need, without depending on the business model or whims of the company controlling a proprietary messaging platform. I can even develop my own clients, something I have often done to automate the sending of multi-part email messages to students or conference committee members.&lt;/p&gt;&lt;p&gt;My messages are stored as plain text files in the super-simple Mbox file format, which means I can easily process them with other tools, reliably create backup copies, and move them from one email client to another.&lt;/p&gt;&lt;p&gt;For example, I have a small script that removes all attachments from old email messages, allowing me to keep my email archive in a manageable size. In other cases Iâve run on my message files scripts to analyze the messages I send and receive, and Iâve opened them in my editor to fix hardware-induced corruption.&lt;/p&gt;&lt;p&gt;In short, email can be an amazingly open and reliable environment that fosters exceptional productivity. We shouldnât settle for anything less.&lt;/p&gt;Comments Post Toot! Tweet&lt;p&gt;Last modified: Saturday, September 27, 2025 11:07 pm&lt;/p&gt;&lt;p&gt;Unless otherwise expressly stated, all original material on this page created by Diomidis Spinellis is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.spinellis.gr/blog/20250926/?li"/><published>2025-10-05T08:12:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480106</id><title>Personal data storage is an idea whose time has come</title><updated>2025-10-05T16:38:37.754089+00:00</updated><content>&lt;doc fingerprint="f5359ed38cb08d8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Personal data storage is an idea whose time has come&lt;/head&gt;
    &lt;p&gt;Back in 2009 Tim Berners-Lee drafted a web-specification for "Socially Aware Cloud Storage":&lt;/p&gt;
    &lt;quote&gt;There is an architecture in which a few existing or Web protocols are gathered together with some glue to make a world wide system in which applications (desktop or web application) can work on top of a layer of commodity read-write storage.&lt;lb/&gt;Crucial design issues are that principals (users) and groups are identifies by URIs, and so are global in scope, and that elements of storage are access controlled using those global identifiers. The result is that storage becomes a commodity, independent of the application running on it.&lt;/quote&gt;
    &lt;p&gt;Several of these ideas were going around in the late 2000s, shortly after the explosive growth of "web2" monoliths like Facebook.&lt;/p&gt;
    &lt;p&gt;Another spiritually similar idea being championed at the time came from the Opera browser folks who wanted to put "a web server in your browser".&lt;/p&gt;
    &lt;p&gt;While 'Opera Unite' never fully materialized, Tim's spec got significant traction some years down the road as one privacy crisis after another made the case for stronger web agency self-evident.&lt;/p&gt;
    &lt;p&gt;In 2015 Tim &amp;amp; co. secured some funding for the Solid Protocol.&lt;/p&gt;
    &lt;quote&gt;Right now we have the worst of both worlds, in which people not only cannot control their data, but also can’t really use it, due to it being spread across a number of different silo-ed websites. Our goal is to develop a web architecture that gives users ownership over their data, including the freedom to switch to new applications in search of better features, pricing, and policies.”&lt;/quote&gt;
    &lt;quote&gt;On the better web Berners-Lee envisions, users control where their data is stored and how it's accessed. For example, social networks would still run in the cloud. But you could store your data locally. Alternately, you could choose a different cloud server run by a company or community you trust.&lt;lb/&gt;You might have different servers for different types of information—for health and fitness data, say—that is completely separate from the one you use for financial records.&lt;/quote&gt;
    &lt;p&gt;To this day, Tim continues to eloquently champion the virtues of the Solid vision.&lt;/p&gt;
    &lt;quote&gt;We have the technical capability to give that power back to the individual. Solid is an open-source interoperable standard that I and my team developed at MIT more than a decade ago. Apps running on Solid don’t implicitly own your data – they have to request it from you and you choose whether to agree, or not. Rather than being in countless separate places on the internet in the hands of whomever it had been resold to, your data is in one place, controlled by you.&lt;lb/&gt;Sharing your information in a smart way can also liberate it. Why is your smartwatch writing your biological data to one silo in one format? Why is your credit card writing your financial data to a second silo in a different format? Why are your YouTube comments, Reddit posts, Facebook updates and tweets all stored in different places? Why is the default expectation that you aren’t supposed to be able to look at any of this stuff? You generate all this data – your actions, your choices, your body, your preferences, your decisions. You should own it. You should be empowered by it.&lt;/quote&gt;
    &lt;p&gt;The Solid Protocol remains an excellent idea and has even culminated in an official web specification, but Solid has not yet amounted to any mainstream adoption on the web. Its primary financial sponsor Inrupt (of which Tim is co-founder &amp;amp; CTO) has focused on the enterprise market as a path to sustainability; it remains to be seen what resources will be directed towards web-scale adoption of Solid.&lt;/p&gt;
    &lt;p&gt;Thankfully those of us who want data ownership and agency in our web applications now don't have to wait. AT Protocol was ushered in by the folks at Bluesky, now with a network of over 30M people strong and increasingly spread across multiple federated platforms/communities like Blacksky or Tangled.&lt;/p&gt;
    &lt;p&gt;While the respective architectures of the Solid and AT protocols are quite different, they're pointing to the same Open Social Web, re-built on the principles of user-sovereign data storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Personal Data Storage&lt;/head&gt;
    &lt;p&gt;What web-user sovereignty looks like in practice, from the vantage point of atproto, has been expertly illustrated by danabra.mov&lt;/p&gt;
    &lt;quote&gt;Notice that Alice’s handle is now&lt;code&gt;@alice.com&lt;/code&gt;. It is not allocated by a social media company [like facebook.com/alice]. Rather, her handle is the universal “internet handle”, i.e. a domain. Alice owns the&lt;code&gt;alice.com&lt;/code&gt;domain, so she can use it as a handle on any open social app. (On most open social apps, she goes by&lt;code&gt;@alice.com&lt;/code&gt;, but for others she wants a distinct disconnected identity, so she owns another handle she’d rather not share.)&lt;lb/&gt;Bob owns a domain too, even though he isn’t technical. He might not even know what a “domain” is. Bob just thinks of&lt;code&gt;@bob.com&lt;/code&gt;as his “internet handle”. Some open social apps will offer you a free subdomain on registration, just like Gmail gives you a free Gmail address, or may offer an extra flow for buying a domain. You’re not locked into your first choice, and can swap to a different domain later.&lt;lb/&gt;(...) With open social, Alice’s data—her posts, likes, follows, etc—is hosted on the web itself. Alongside her personal site, Alice now has a personal repository of her data.&lt;/quote&gt;
    &lt;p&gt;This new paradigm is made technically possible by what the AT protocol refers to as a Personal Data Server or PDS for short (what Solid calls a Pod).&lt;/p&gt;
    &lt;p&gt;The notion of a 'PDS' quickly comes off as something very technical and nerdy which is why it's not mentioned once in Dan's explainer, even though it's still targeted at an audience of web nerds. But really the only obscure word here is the Server, which in this context is interchangeable with Storage, as in Personal Data Storage.&lt;/p&gt;
    &lt;p&gt;Even regular internet users have some mental model of what personalized data storage entails, especially with the complementary framing of collectively owned and operated data storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data-banking Coops&lt;/head&gt;
    &lt;p&gt;If you're a regular internet user the PDS paradigm won't move your data from the cloud to your personal computer. Most people will still rely on an institutional cloud service, but instead of data-banking with a shareholder-controlled corporation most people’s data can be entrusted to the equivalent of member-owned credit unions for data storage.&lt;/p&gt;
    &lt;p&gt;One in every three US adults banks with a Credit Union. Achieving similar or better numbers for data storage is far from inconceivable considering how much our collective experience with Big Banking mirrors that of Big Tech/Social.&lt;/p&gt;
    &lt;p&gt;The concept of data cooperatives has already gained a lot of traction in the fediverse with several providers like social.coop, data.coop and cosocial.ca being operational for many years and still going strong. Soon the AT network will have a similarly co-owned institution in Northsky.&lt;/p&gt;
    &lt;p&gt;Whether these providers are strictly cooperatives in the formal sense isn't what's most important here though; any suffuciently transparent, democratic and community-oriented data bank (like the aforementioned Blacksky, or the forthcoming Eurosky) is a valid steward and co-creator of an Open Social.&lt;/p&gt;
    &lt;p&gt;Data Ownership as a conversation changes when data resides primarily with people-governed institutions rather than corporations. Rather than arguing for what kinds of data we ought to be able to download from the corporate silos, the platforms should be asking us what kinds of data they may copy from our servers, and only with strictly temporary allowances.&lt;/p&gt;
    &lt;p&gt;And while the separation of user data and social platform is most fully realized today in the AT network, there are exciting signs of cross-pollination happening in the ongoing development of atproto’s predecessor ActivityPub. I hope to see similar openness towards technological convergence in Solid for a more pluralistic social web.&lt;/p&gt;
    &lt;p&gt;Personal Data Storage has long since escaped containment as a concept pertaining to any specific protocol. Some implementations of it will be more mainstream than others, but pragmatic data coops can be protocol-agnostic and storage formats are transmutable.&lt;/p&gt;
    &lt;p&gt;As long as we have sufficient control of our own data there will always be a way to restart our social graph and digital presence elsewhere in the event of platform collapse. Let’s make the web personal again.&lt;/p&gt;
    &lt;p&gt;See also:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.muni.town/personal-data-storage-idea/"/><published>2025-10-05T09:07:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480317</id><title>Self hosting 10TB in S3 on a framework laptop and disks</title><updated>2025-10-05T16:38:37.328609+00:00</updated><content>&lt;doc fingerprint="108fdf7d4527a9d6"&gt;
  &lt;main&gt;
    &lt;p&gt;About 5 months ago I made the decision to start self hosting my own S3. I was working on AppGoblin’s SDK tracking of the top 100k Android and iOS apps so was wanting a lot of space, but for cheap.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;I got really lucky with getting a second hand Framework laptop. The laptop was missing it’s screen, and was one of the older ones, so it was perfect for a home server. In addition I bought a “just a bunch of disks” JBOD. The framework laptop is running ZFS + garage S3. &lt;/p&gt;
    &lt;head rend="h2"&gt;I’m happy to report I haven’t thought about this laptop for months&lt;/head&gt;
    &lt;p&gt;I’ve been away, I’ve been working, I’ve been busy, and I’ve definitely been using my S3. But I hadn’t thought about the laptop in 4 months. When I finally logged in, I saw I’ve used 10TB of space and it was patiently waiting for a restart for some upgrades. I nervously restarted, and was so relieved to see everything come right back up.&lt;/p&gt;
    &lt;head rend="h2"&gt;I updated garage s3 with no issues as well&lt;/head&gt;
    &lt;p&gt;I also saw a pending upgrade for garage v1 to v2. This went along without a hitch too. Feels like it’s been a good weekend.&lt;/p&gt;
    &lt;head rend="h2"&gt;I’ve been warned…&lt;/head&gt;
    &lt;p&gt;Just so you know, I understand my use case for ZFS is possibly a bit non standard as I’m using a USB to connect the laptop and JBOD. This initially caused me issues with ZFS when garage was heavily reading and writing (the initial setup had the SQLite metadata also stored on the JBOD/ZFS).&lt;/p&gt;
    &lt;p&gt;I moved my metadata to the laptop, which has so far resolved any ZFS issues again.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jamesoclaire.com/2025/10/05/self-hosting-10tb-in-s3-on-a-framework-laptop-disks/"/><published>2025-10-05T09:51:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480414</id><title>Hobby Hilbert Simplex</title><updated>2025-10-05T16:38:37.094501+00:00</updated><content>&lt;doc fingerprint="26baa0840e3d0b99"&gt;
  &lt;main&gt;
    &lt;p&gt;An exploration and explanation of how to generate interesting swoopy art.&lt;/p&gt;
    &lt;p&gt;I saw a generative art piece I liked and wanted to learn how it was made. Starting with the artist’s Kotlin code, I dug into three new algorithms, hacked together some Python code, experimented with alternatives, and learned a lot. Now I can explain it to you.&lt;/p&gt;
    &lt;p&gt;It all started with this post by aBe on Mastodon:&lt;/p&gt;
    &lt;code&gt;
      &lt;p&gt;I love how these lines separate and reunite. And the fact that I can express this idea in 3 or 4 lines of code.&lt;/p&gt;
      &lt;p&gt;For me they’re lives represented by closed paths that end where they started, spending part of the journey together, separating while we go in different directions and maybe reconnecting again in the future.&lt;/p&gt;
      &lt;p&gt;#CreativeCoding #algorithmicart #proceduralArt #OPENRNDR #Kotlin&lt;/p&gt;
    &lt;/code&gt;
    &lt;p&gt;The drawing is made by choosing 10 random points, drawing a curve through those points, then slightly scooching the points and drawing another curve. There are 40 curves, each slightly different than the last. Occasionally the next curve makes a jump, which is why they separate and reunite.&lt;/p&gt;
    &lt;p&gt;Eventually I made something similar:&lt;/p&gt;
    &lt;p&gt;Along the way I had to learn about three techniques I got from the Kotlin code: Hobby curves, Hilbert sorting, and simplex noise.&lt;/p&gt;
    &lt;p&gt;Each of these algorithms tries to do something “natural” automatically, so that we can generate art that looks nice without any manual steps.&lt;/p&gt;
    &lt;head rend="h1"&gt;Hobby curves&lt;/head&gt;
    &lt;p&gt;To draw swoopy curves through our random points, we use an algorithm developed by John Hobby as part of Donald Knuth’s Metafont type design system. Jake Low has a great interactive page for playing with Hobby curves, you should try it.&lt;/p&gt;
    &lt;p&gt;Here are three examples of Hobby curves through ten random points:&lt;/p&gt;
    &lt;p&gt;The curves are nice, but kind of a scribble, because we’re joining points together in the order we generated them (shown by the green lines). If you asked a person to connect random points, they wouldn’t jump back and forth across the canvas like this. They would find a nearby point to use next, producing a more natural tour of the set.&lt;/p&gt;
    &lt;p&gt;We’re generating everything automatically, so we can’t manually intervene to choose a natural order for the points. Instead we use Hilbert sorting.&lt;/p&gt;
    &lt;head rend="h1"&gt;Hilbert sorting&lt;/head&gt;
    &lt;p&gt;The Hilbert space-filling fractal visits every square in a 2D grid. Hilbert sorting uses a Hilbert fractal traversing the canvas, and sorts the points by when their square is visited by the fractal. This gives a tour of the points that corresponds more closely to what people expect. Points that are close together in space are likely (but not guaranteed) to be close in the ordering.&lt;/p&gt;
    &lt;p&gt;If we sort the points using Hilbert sorting, we get much nicer curves. Here are the same points as last time:&lt;/p&gt;
    &lt;p&gt;Here are pairs of the same points, unsorted and sorted side-by-side:&lt;/p&gt;
    &lt;p&gt;If you compare closely, the points in each pair are the same, but the sorted points are connected in a better order, producing nicer curves.&lt;/p&gt;
    &lt;head rend="h1"&gt;Simplex noise&lt;/head&gt;
    &lt;p&gt;Choosing random points would be easy to do with a random number generator, but we want the points to move in interesting graceful ways. To do that, we use simplex noise. This is a 2D function (let’s call the inputs u and v) that produces a value from -1 to 1. The important thing is the function is continuous: if you sample it at two (u,v) coordinates that are close together, the results will be close together. But it’s also random: the continuous curves you get are wavy in unpredictable ways. Think of the simplex noise function as a smooth hilly landscape.&lt;/p&gt;
    &lt;p&gt;To get an (x,y) point for our drawing, we choose a (u,v) coordinate to produce an x value and a completely different (u,v) coordinate for the y. To get the next (x,y) point, we keep the u values the same and change the v values by just a tiny bit. That makes the (x,y) points move smoothly but interestingly.&lt;/p&gt;
    &lt;p&gt;Here are the trails of four points taking 50 steps using this scheme:&lt;/p&gt;
    &lt;p&gt;If we use seven points taking five steps, and draw curves through the seven points at each step, we get examples like this:&lt;/p&gt;
    &lt;p&gt;I’ve left the points visible, and given them large steps so the lines are very widely spaced to show the motion. Taking out the points and drawing more lines with smaller steps gives us this:&lt;/p&gt;
    &lt;p&gt;With 40 lines drawn wider with some transparency, we start to see the smoky fluidity:&lt;/p&gt;
    &lt;head rend="h1"&gt;Jumps&lt;/head&gt;
    &lt;p&gt;In his Mastodon post, aBe commented on the separating of the lines as one of the things he liked about this. But why do they do that? If we are moving the points in small increments, why do the curves sometimes make large jumps?&lt;/p&gt;
    &lt;p&gt;The first reason is because of Hobby curves. They do a great job drawing a curve through a set of points as a person might. But a downside of the algorithm is sometimes changing a point a small amount makes the entire curve take a different route. If you play around with the interactive examples on Jake Low’s page you will see the curve can unexpectedly take a different shape.&lt;/p&gt;
    &lt;p&gt;As we inch our points along, sometimes the Hobby curve jumps.&lt;/p&gt;
    &lt;p&gt;The second reason is due to Hilbert sorting. Each of our lines is sorted independently of how the previous line was sorted. If a point’s small motion moves it into a different grid square, it can change the sorting order, which changes the Hobby curve even more.&lt;/p&gt;
    &lt;p&gt;If we sort the first line, and then keep that order of points for all the lines, the result has fewer jumps, but the Hobby curves still act unpredictably:&lt;/p&gt;
    &lt;head rend="h1"&gt;Colophon&lt;/head&gt;
    &lt;p&gt;This was all done with Python, using other people’s implementations of the hard parts: hobby.py, hilbertcurve, and super-simplex. My code is on GitHub (nedbat/fluidity), but it’s a mess. Think of it as a woodworking studio with half-finished pieces and wood chips strewn everywhere.&lt;/p&gt;
    &lt;p&gt;A lot of the learning and experimentation was in my Jupyter notebook. Part of the process for work like this is playing around with different values of tweakable parameters and seeds for the random numbers to get the effect you want, either artistic or pedagogical. The notebook shows some of the thumbnail galleries I used to pick the examples to show.&lt;/p&gt;
    &lt;p&gt;I went on to play with animations, which led to other learnings, but those will have to wait for another blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nedbatchelder.com/blog/202509/hobby_hilbert_simplex.html"/><published>2025-10-05T10:14:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480506</id><title>Beginner Guide to VPS Hetzner and Coolify</title><updated>2025-10-05T16:38:36.827902+00:00</updated><content>&lt;doc fingerprint="fe08e21335a407ea"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VPS Setup and Security Checklist: Complete Self-Hosting Guide for 2025&lt;/head&gt;
    &lt;p&gt;I set up my own VPS, documented every step, and ended up with a repeatable deployment pipeline. This is both a checklist for my future self and a guide for anyone curious about self-hosting. Along the way I'll explain why I picked Hetzner and Coolify, and how they compare with other options like DigitalOcean, AWS, Render, or Fly.io.&lt;/p&gt;
    &lt;p&gt;This comprehensive checklist covers every essential step for setting up a secure, production-ready VPS. Each section includes commands, verification steps, and troubleshooting tips based on real-world experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pre-Setup Checklist&lt;/head&gt;
    &lt;p&gt;Before You Begin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Choose your VPS provider (Hetzner recommended for price/performance)&lt;/item&gt;
      &lt;item&gt;Select server specifications (minimum 1GB RAM, 20GB storage)&lt;/item&gt;
      &lt;item&gt;Note down server IP address and root credentials&lt;/item&gt;
      &lt;item&gt;Prepare your local machine with SSH client&lt;/item&gt;
      &lt;item&gt;Have a strong password generator ready&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Picking the VPS provider&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chose Hetzner Cloud (cheap, fast, reliable in Europe)&lt;/item&gt;
      &lt;item&gt;Alternatives I considered: &lt;list rend="ul"&gt;&lt;item&gt;DigitalOcean → smoother onboarding, great docs, slightly more expensive&lt;/item&gt;&lt;item&gt;AWS Lightsail → decent for small apps, but tied to AWS ecosystem (complex for beginners)&lt;/item&gt;&lt;item&gt;Linode → reliable, but Hetzner wins on price/performance&lt;/item&gt;&lt;item&gt;Render/Fly.io → easier PaaS, but more opinionated and costly at scale&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why Hetzner?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2–3x cheaper for the same specs compared to DO/AWS&lt;/item&gt;
      &lt;item&gt;Strong European datacenter presence (latency advantage for my use case)&lt;/item&gt;
      &lt;item&gt;Transparent pricing and no surprise bills&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Initial Server Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;First Login and System Updates&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Initial login as root&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh root@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Update package lists and upgrade system&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;apt update &amp;amp;&amp;amp; apt upgrade -y&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify system information&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;uname -a cat /etc/os-release&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Root Account Security&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change root password&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;passwd&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Use strong password with mixed case, numbers, symbols
- Store securely in password manager
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create secondary user account&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;adduser your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Choose descriptive username (not 'admin' or 'user')
- Set strong password
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add user to sudo group&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;usermod -aG sudo your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify user groups&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;groups your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show: `your-username : your-username sudo`
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test sudo access&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;su - your-username sudo whoami&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should return: `root`
&lt;/code&gt;
    &lt;head rend="h4"&gt;SSH Key Authentication Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generate SSH keys on LOCAL machine (not server)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#### Ed25519 (recommended) ssh-keygen -t ed25519 -C "your-email@example.com" ##### Or RSA if Ed25519 not supported ssh-keygen -t rsa -b 4096 -C "your-email@example.com"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Display public key on local machine&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;cat ~/.ssh/id_ed25519.pub #### or cat ~/.ssh/id_rsa.pub&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Copy public key to clipboard&lt;/item&gt;
      &lt;item&gt;Create .ssh directory on server (as your user, not root)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;mkdir -p ~/.ssh chmod 700 ~/.ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create authorized_keys file&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;nano ~/.ssh/authorized_keys&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Paste your public key
- Save and exit
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set correct permissions&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;chmod 600 ~/.ssh/authorized_keys&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH key login (from local machine)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh your-username@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should login without password prompt
&lt;/code&gt;
    &lt;head rend="h4"&gt;Disable Password Authentication&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edit SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modify these settings:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;PasswordAuthentication no PubkeyAuthentication yes&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check cloud-init config if exists&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config.d/50-cloud-init.conf&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Set `PasswordAuthentication no` here too if file exists
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo sshd -t&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show no errors
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart SSH service&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart ssh #### or sudo service ssh restart&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify service status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show active (running) with green dot
&lt;/code&gt;
    &lt;head rend="h4"&gt;Disable Root Login&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edit SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change root login setting&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;PermitRootLogin no&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart SSH service&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test root login is blocked (from another terminal)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh root@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should get "Permission denied"
&lt;/code&gt;
    &lt;head rend="h2"&gt;Firewall Configuration Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;UFW (Uncomplicated Firewall) Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check UFW status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set default policies&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw default deny incoming sudo ufw default allow outgoing&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow SSH before enabling firewall&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow ssh #### or if you changed SSH port: sudo ufw allow 2022/tcp&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow HTTP and HTTPS for web apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow 80/tcp sudo ufw allow 443/tcp&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable firewall&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw enable&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Type 'y' when prompted
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify firewall rules&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status verbose&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Advanced Firewall Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restrict SSH to your IP (optional but recommended)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow from YOUR_IP_ADDRESS to any port 22 sudo ufw delete allow ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change default SSH port (optional security through obscurity)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Change `Port 22` to `Port 2022` (or your chosen port)
- Update firewall: `sudo ufw allow 2022/tcp`
- Remove old rule: `sudo ufw delete allow 22/tcp`
- Restart SSH: `sudo systemctl restart ssh`
&lt;/code&gt;
    &lt;head rend="h2"&gt;Automatic Updates Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Unattended Upgrades Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install unattended-upgrades&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install unattended-upgrades apt-listchanges&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable automatic updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo dpkg-reconfigure unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Select "Yes" in the dialog
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure update settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/apt/apt.conf.d/50unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uncomment security updates line&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;"${distro_id}:${distro_codename}-security";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure email notifications (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;Unattended-Upgrade::Mail "your-email@example.com";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable automatic reboots if needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;Unattended-Upgrade::Automatic-Reboot "true"; Unattended-Upgrade::Automatic-Reboot-Time "02:00";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo unattended-upgrades --dry-run&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check service status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Production Application Deployment Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Node.js Production Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Node.js LTS&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash - sudo apt-get install -y nodejs&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify installation&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;node --version npm --version&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install PM2 globally&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo npm install -g pm2&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Upload your application files&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;scp -r ./your-app your-username@your-server-ip:~/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;cd ~/your-app npm install --production&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create production build&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;npm run build&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Process Manager Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start application with PM2&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;NODE_ENV=production pm2 start app.js --name "your-app"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure PM2 for clustering (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 start app.js -i max --name "your-app-cluster"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Save PM2 configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 save&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable PM2 startup&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 startup #### Run the command it outputs&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test application restart&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 restart all pm2 status&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Reverse Proxy Setup (Nginx)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Nginx&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create site configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/nginx/sites-available/your-app&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Basic Nginx configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;server { listen 80; server_name your-domain.com; location / { proxy_pass http://localhost:3000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_cache_bypass $http_upgrade; } }&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable site&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ln -s /etc/nginx/sites-available/your-app /etc/nginx/sites-enabled/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test Nginx configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nginx -t&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart Nginx&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;SSL Certificate Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Let's Encrypt with Certbot&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Certbot&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install certbot python3-certbot-nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Obtain SSL certificate&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo certbot --nginx -d your-domain.com&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test automatic renewal&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo certbot renew --dry-run&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify SSL grade&lt;/item&gt;
      &lt;item&gt;Visit: https://www.ssllabs.com/ssltest/&lt;/item&gt;
      &lt;item&gt;Should get A or A+ rating&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Monitoring and Maintenance Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Basic Monitoring Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install monitoring tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install htop iotop netstat-nat&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check system resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop df -h free -h&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo tail -f /var/log/syslog sudo tail -f /var/log/auth.log&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up log rotation&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/logrotate.d/your-app&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Backup Strategy&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create backup script&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;nano ~/backup.sh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sample backup script&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#!/bin/bash DATE=$(date +%Y%m%d_%H%M%S) tar -czf ~/backups/app_backup_$DATE.tar.gz ~/your-app #### Add database backup commands if needed&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make script executable&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;chmod +x ~/backup.sh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up automated backups&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;crontab -e&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Add: `0 2 * * * /home/username/backup.sh`
&lt;/code&gt;
    &lt;head rend="h2"&gt;Troubleshooting Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Common Issues and Solutions&lt;/head&gt;
    &lt;p&gt;SSH Connection Problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check firewall rules: &lt;code&gt;sudo ufw status&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Verify SSH service: &lt;code&gt;sudo systemctl status ssh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check SSH logs: &lt;code&gt;sudo tail -f /var/log/auth.log&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Test from different network&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Permission Denied Errors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check file permissions: &lt;code&gt;ls -la&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Verify user groups: &lt;code&gt;groups username&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check sudo configuration: &lt;code&gt;sudo -l&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Service Not Starting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check service status: &lt;code&gt;sudo systemctl status service-name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;View service logs: &lt;code&gt;sudo journalctl -u service-name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check configuration files syntax&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High Resource Usage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identify processes: &lt;code&gt;htop&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check disk usage: &lt;code&gt;df -h&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monitor network: &lt;code&gt;netstat -tulpn&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Review application logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Final Verification Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Security Verification&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH key authentication works&lt;/item&gt;
      &lt;item&gt;Verify password authentication is disabled&lt;/item&gt;
      &lt;item&gt;Confirm root login is blocked&lt;/item&gt;
      &lt;item&gt;Check firewall is active and configured&lt;/item&gt;
      &lt;item&gt;Verify automatic updates are working&lt;/item&gt;
      &lt;item&gt;Test application runs in production mode&lt;/item&gt;
      &lt;item&gt;Confirm SSL certificate is valid&lt;/item&gt;
      &lt;item&gt;Verify backups are being created&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Performance Testing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run basic load test&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#### Install Apache Bench sudo apt install apache2-utils #### Test with 100 requests, 10 concurrent ab -n 100 -c 10 http://your-domain.com/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor resource usage during load&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check application logs for errors&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 logs&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Quick Reference Commands&lt;/head&gt;
    &lt;p&gt;System Information:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop # System monitor df -h # Disk usage free -h # Memory usage uname -a # System info&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Process Management:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 status # PM2 process status pm2 restart all # Restart all processes pm2 logs # View logs pm2 monit # Real-time monitoring&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Security:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status # Firewall status sudo fail2ban-client status # Fail2ban status sudo lynis audit system # Security audit&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Services:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status nginx # Service status sudo systemctl restart nginx # Restart service sudo journalctl -u nginx # Service logs&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;This checklist provides a complete approach to VPS setup and management. This isn’t just about saving money. It’s about control and understanding. By self-hosting with Hetzner + Coolify, I built muscle memory for devops that paid off in confidence and freedom.&lt;/p&gt;
    &lt;p&gt;If you’ve been meaning to try VPS hosting, consider this a nudge.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bhargav.dev/blog/VPS_Setup_and_Security_Checklist_A_Complete_Self_Hosting_Guide"/><published>2025-10-05T10:39:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45480622</id><title>The deadline isn't when AI outsmarts us – it's when we stop using our own minds</title><updated>2025-10-05T16:38:36.604206+00:00</updated><content>&lt;doc fingerprint="3f37101d29eb8c85"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;“You have 18 months”&lt;/head&gt;
    &lt;head rend="h3"&gt;The real deadline isn’t when AI outsmarts us — it’s when we stop using our own minds.&lt;/head&gt;
    &lt;p&gt;In fitness, there is a concept called “time under tension.” Take a simple squat, where you hold a weight and lower your hips from a standing position. With the same weight, a person can do a squat in two seconds or 10 seconds. The latter is harder, but it also builds more muscle. More time is more tension; more pain is more gain.&lt;/p&gt;
    &lt;p&gt;Thinking benefits from a similar principle of “time under tension.” It is the ability to sit patiently with a group of barely connected or disconnected ideas that allows a thinker to braid them together into something that is combinatorially new. It’s very difficult to defend this idea by describing other people’s thought processes, so I’ll describe my own.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, The Argument Editor-in-Chief Jerusalem Demsas asked me to write an essay about the claim that AI systems would take all of our jobs within 18 months. My initial reaction was … no?&lt;/p&gt;
    &lt;p&gt;The prediction is so stupendously aggressive and almost certainly wrong, so my instinct was there was really nothing more to say on the subject. Certainly not 1,799 words more. But as I sat with the prompt, several pieces of a puzzle began to slide together: a Financial Times essay I’d read, an Atlantic article I liked, a National Assessment of Educational Progress study I’d saved in a tab, an interview with Cal Newport I’d recorded, a Walter Ong book I was encouraged to read, a stray thought I’d had in the gym recently while trying out eccentric pullups for the first time about how time multiplies both pain and gain in fitness settings. The contours of a framework came into view.&lt;/p&gt;
    &lt;p&gt;The problem of the next 18 months isn’t AI disemploying all workers, or students losing competition after competition to nonhuman agents. The problem is whether we will degrade our own capabilities in the presence of new machines. We are so fixated on how technology will outskill us that we miss the many ways that we can deskill ourselves.&lt;/p&gt;
    &lt;p&gt;You have 18 months.&lt;/p&gt;
    &lt;p&gt;That’s the message from several leading AI executives and thinkers about how long people will retain their advantage over artificial intelligence in the workforce. By the summer of 2027, the story goes, AI’s explosion in capabilities will leave carbon-based life forms in the dust. Up to “half of all entry-level white-collar jobs” will be wiped out, and even Nobel Prize-worthy minds will cower in fear that AI’s architects will have built a “country of geniuses in a datacenter.”&lt;/p&gt;
    &lt;p&gt;This doomsday clock seems true enough to many people, because the question I’ve fielded more than any other from parents in the last few months is some version of: “If AI is about to be better than us at everything, what should my children do?” If generative AI is better at coding, diagnosing, and problem-solving than any software programmer, radiologist, or mathematician, then even the traditionally “safe” majors like computer science, medicine, and math could be anything but safe.&lt;/p&gt;
    &lt;p&gt;I understand the anxiety behind the question, but rather than try to forecast the future as it might turn out, I’d prefer to describe reality as it already exists. While we have no idea how AI might make working people obsolete at some imaginary date, we can already see how technology is affecting our capacity to think deeply right now. And I am much more concerned about the decline of thinking people than I am about the rise of thinking machines.&lt;/p&gt;
    &lt;head rend="h3"&gt;The end of writing, the end of reading&lt;/head&gt;
    &lt;p&gt;In March, New York Magazine published the sort of cover story that goes instantly viral, not because of its shock value, but, quite the opposite, because it loudly proclaimed what most people were already thinking: Everybody is using AI to cheat in school.&lt;/p&gt;
    &lt;p&gt;By allowing high-school and college students to summon into existence any essay on any topic, large language models have created an existential crisis for teachers trying to evaluate their students’ ability to actually write. “College is just how well I can use ChatGPT at this point,” one student told New York Magazine. “Massive numbers of students are going to emerge from university with degrees, and into the workforce, who are essentially illiterate,” a professor echoed.&lt;/p&gt;
    &lt;p&gt;The demise of writing matters because writing is not a second thing that happens after thinking. The act of writing is an act of thinking. This is as true for professionals as it is for students. In “Writing is thinking,” an editorial in Nature, the authors argued that “outsourcing the entire writing process to LLMs” deprives scientists of the important work of understanding what they’ve discovered and why it matters.&lt;/p&gt;
    &lt;p&gt;Students, scientists, and anyone else who lets AI do the writing for them will find their screens full of words and their minds emptied of thought.&lt;/p&gt;
    &lt;p&gt;As writing skills have declined, reading has declined even more. “Most of our students are functionally illiterate,” a pseudonymous college professor using the name Hilarius Bookbinder wrote in a March Substack essay on the state of college campuses. “This is not a joke.” Nor is it hyperbole.&lt;/p&gt;
    &lt;p&gt;Achievement scores in literacy and numeracy are declining across the West for the first time in decades, leading the Financial Times reporter John Burn-Murdoch to wonder if humans have “passed peak brain power” at the very moment that we are building machines to think for us. In the U.S., the so-called Nation’s Report Card, published by the NAEP, recently found that average reading scores hit a 32-year low in 2024 — which is troubling, since the data series only goes back 32 years.&lt;/p&gt;
    &lt;p&gt;Of course, Americans are reading words all the time: email, texts, social media newsfeeds, subtitles on Netflix shows. But these words live in writing fragments that hardly require any kind of sustained focus necessary to make sense of a larger text. Indeed, Americans in the digital age don’t seem interested in or capable of sitting with anything longer than a tweet. The share of Americans overall who say they read books for leisure has declined by nearly 40% since the 2000s.&lt;/p&gt;
    &lt;p&gt;Even America’s highest-performing students have essentially stopped reading anything longer than a paragraph. Last year, The Atlantic’s Rose Horowitch reported that students are matriculating into America’s most-elite colleges without having ever read a full book for school. “Daniel Shore, the chair of Georgetown’s English department, told me that his students have trouble staying focused on even a sonnet,” Horowitch wrote.&lt;/p&gt;
    &lt;p&gt;Nat Malkus, an education researcher at the American Enterprise Institute, suggested to me that high schools have chunkified books to prepare students for the reading-comprehension sections of standardized exams. By optimizing the assessment of reading skills, the U.S. education system appears to have accidentally killed book reading.&lt;/p&gt;
    &lt;p&gt;The decline of writing and reading matters because writing and reading are the twin pillars of deep thinking, according to Cal Newport, a computer science professor and the author of several bestselling books, including Deep Work. The modern economy prizes the sort of symbolic logic and systems thinking for which deep reading and writing are the best practice.&lt;/p&gt;
    &lt;p&gt;AI is “the latest in multiple heavyweight entrances into the prize fight against our ability to actually think,” Newport said. The rise of TV corresponded with the decline in per capita newspaper subscriptions and a slow demise of reading for pleasure. Then along came the internet, followed by social media, the smartphone, and streaming TV.&lt;/p&gt;
    &lt;p&gt;“The one-two punch of reading and writing is like the serum we have to take in a superhero comic book to gain the superpower of deep symbolic thinking,” Newport said. “And so I have been ringing this alarm bell that we have to keep taking the serum.”&lt;/p&gt;
    &lt;p&gt;Newport’s warning echoes an observation made by the scholar Walter Ong in his book “Orality and Literacy.” According to Ong, literacy is no passing skill. It was a means of restructuring human thought and knowledge to create space for complex ideas.&lt;/p&gt;
    &lt;p&gt;Stories can be memorized by people who cannot read or write. But nothing as advanced as, say, Newton’s “Principia” could be passed down from generation to generation without the ability to write down calculus formulas. Oral dialects commonly have only a few thousand words, while “the grapholect known as standard English has … at least a million and a half words,” Ong wrote. If reading and writing “rewired” the logic engine of the human brain, the decline of reading and writing are unwiring our cognitive superpower at the very moment that a greater machine appears to be on the horizon.&lt;/p&gt;
    &lt;p&gt;So what should our children study in an age of thinking machines? While I don’t know what field any particular student should major in, I do feel strongly about what skill they should value: It’s the very same skill that I see in decline. It’s the patience to read long and complex texts; to hold conflicting ideas in our heads and enjoy their dissonance; to engage in hand-to-hand combat at the sentence level within a piece of writing — and to value these things at a time when valuing them is a choice, because video entertainment is replacing reading and ChatGPT essays are replacing writing. As AI becomes abundant, there is a clear and present threat that deep human thinking will become scarce.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theargumentmag.com/p/you-have-18-months"/><published>2025-10-05T11:08:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45481008</id><title>86 GB/s bitpacking with ARM SIMD (single thread)</title><updated>2025-10-05T16:38:36.165064+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ashtonsix/perf-portfolio/tree/main/bytepack"/><published>2025-10-05T12:27:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45481298</id><title>Show HN: Pyscn – Python code quality analyzer for vibe coders</title><updated>2025-10-05T16:38:35.689255+00:00</updated><content>&lt;doc fingerprint="9e863d005f1db92f"&gt;
  &lt;main&gt;
    &lt;p&gt;Building with Cursor, Claude, or ChatGPT? pyscn performs structural analysis to keep your codebase maintainable.&lt;/p&gt;
    &lt;code&gt;# Run analysis without installation
uvx pyscn analyze .
# or
pipx run pyscn analyze .&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;pyscn_20251005.mov&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔍 CFG-based dead code detection – Find unreachable code after exhaustive if-elif-else chains&lt;/item&gt;
      &lt;item&gt;📋 Clone detection with APTED + LSH – Identify refactoring opportunities with tree edit distance&lt;/item&gt;
      &lt;item&gt;🔗 Coupling metrics (CBO) – Track architecture quality and module dependencies&lt;/item&gt;
      &lt;item&gt;📊 Cyclomatic complexity analysis – Spot functions that need breaking down&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;100,000+ lines/sec • Built with Go + tree-sitter&lt;/p&gt;
    &lt;p&gt;Run comprehensive analysis with HTML report&lt;/p&gt;
    &lt;code&gt;pyscn analyze .                              # All analyses with HTML report
pyscn analyze --json .                       # Generate JSON report
pyscn analyze --select complexity .          # Only complexity analysis
pyscn analyze --select deps .                # Only dependency analysis
pyscn analyze --select complexity,deps,deadcode . # Multiple analyses&lt;/code&gt;
    &lt;p&gt;Fast CI-friendly quality gate&lt;/p&gt;
    &lt;code&gt;pyscn check .                      # Quick pass/fail check
pyscn check --max-complexity 15 .  # Custom thresholds&lt;/code&gt;
    &lt;p&gt;Create configuration file&lt;/p&gt;
    &lt;code&gt;pyscn init                         # Generate .pyscn.toml&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;💡 Run&lt;/p&gt;&lt;code&gt;pyscn --help&lt;/code&gt;or&lt;code&gt;pyscn &amp;lt;command&amp;gt; --help&lt;/code&gt;for complete options&lt;/quote&gt;
    &lt;p&gt;Create a &lt;code&gt;.pyscn.toml&lt;/code&gt; file or add &lt;code&gt;[tool.pyscn]&lt;/code&gt; to your &lt;code&gt;pyproject.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# .pyscn.toml
[complexity]
max_complexity = 15

[dead_code]
min_severity = "warning"

[output]
directory = "reports"&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;⚙️ Run&lt;/p&gt;&lt;code&gt;pyscn init&lt;/code&gt;to generate a full configuration file with all available options&lt;/quote&gt;
    &lt;code&gt;# Install with pipx (recommended)
pipx install pyscn

# Or run directly with uvx
uvx pyscn&lt;/code&gt;
    &lt;head&gt;Alternative installation methods&lt;/head&gt;
    &lt;code&gt;git clone https://github.com/ludo-technologies/pyscn.git
cd pyscn
make build&lt;/code&gt;
    &lt;code&gt;go install github.com/ludo-technologies/pyscn/cmd/pyscn@latest&lt;/code&gt;
    &lt;code&gt;# .github/workflows/code-quality.yml
name: Code Quality
on: [push, pull_request]

jobs:
  quality-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: pip install pyscn
      - name: Quick quality check
        run: pyscn check .
      - name: Generate detailed report
        run: pyscn analyze --json --select complexity,deadcode,deps src/
      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-report
          path: .pyscn/reports/&lt;/code&gt;
    &lt;p&gt;📚 Development Guide • Architecture • Testing&lt;/p&gt;
    &lt;p&gt;MIT License — see LICENSE&lt;/p&gt;
    &lt;p&gt;Built with ❤️ using Go and tree-sitter&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ludo-technologies/pyscn"/><published>2025-10-05T13:22:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45481609</id><title>Retiring Test-Ipv6.com</title><updated>2025-10-05T16:38:35.346831+00:00</updated><content>&lt;doc fingerprint="3e399bddbaeb15e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Retiring test-ipv6.com&lt;/head&gt;
    &lt;p&gt;TL;DR: I will retire test-ipv6.com in December 2025.&lt;/p&gt;
    &lt;p&gt;I have provided test-ipv6.com to the public since 2010. I've sunk significant resources - engineering, support, equipment, and hosting fees - into what is a revenue-free product.&lt;/p&gt;
    &lt;p&gt;Without going into details: I feel now is the time for me to refocus my resources within the family.&lt;lb/&gt; I hope people will understand, and respect this decision.&lt;/p&gt;
    &lt;p&gt;I am shutting the site down, with a target of "during winter break" (December) 2025.&lt;/p&gt;
    &lt;p&gt;Mirror operators: Should you wish to keep your mirrors up, they will stop getting updates in December.&lt;/p&gt;
    &lt;p&gt;Service providers: If you have runbooks for your support team based on this site, or based on RIPE-631, you'll need to update those.&lt;/p&gt;
    &lt;p&gt;FAQ:&lt;/p&gt;
    &lt;p&gt;Q: Will I (jfesler) transfer the source?&lt;/p&gt;
    &lt;p&gt;A: These portions are already public.&lt;/p&gt;
    &lt;p&gt;These are already public.&lt;lb/&gt; http://github.com/falling-sky/source&lt;lb/&gt; https://github.com/falling-sky/fsbuilder - used to build what's in source&lt;lb/&gt; https://github.com/falling-sky/mod_ip - the /ip/ handler for Apache&lt;lb/&gt; https://github.com/falling-sky/mtu1280d - the synthetic MTU180 netfilter daemon.&lt;/p&gt;
    &lt;p&gt;The remaining parts, such as geolocation and service provider lookups, I am contractually unable to release. Please do not ask.&lt;/p&gt;
    &lt;p&gt;Q: Will I (jfesler) transfer the domain?&lt;/p&gt;
    &lt;p&gt;A: I’d consider a reputable RIR or NIC organization serving the public interest taking things over.&lt;/p&gt;
    &lt;p&gt;Q: Should mirrors be retired?&lt;/p&gt;
    &lt;p&gt;A: I would suggest it. Once the primary site is retired, I will stop monitoring the functionality of your mirror, and stop providing geolocation and service provider lookups.&lt;/p&gt;
    &lt;p&gt;Q: I have more questions or comments!&lt;/p&gt;
    &lt;p&gt;A: If we ever meet for coffee or beer, ask me then.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://retire.test-ipv6.com/"/><published>2025-10-05T14:11:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482309</id><title>Language Agnostic Programming: Why you may still need code</title><updated>2025-10-05T16:38:35.173996+00:00</updated><content/><link href="https://joaquimrocha.com/2025/08/31/language-agnostic-programming-why-you-may-still-need-code/"/><published>2025-10-05T15:34:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482394</id><title>Westjet is going to make you pay to recline your seat</title><updated>2025-10-05T16:38:35.084179+00:00</updated><content/><link href="https://www.thestreet.com/travel/a-major-airline-is-going-to-make-you-pay-to-recline-your-seat"/><published>2025-10-05T15:41:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482484</id><title>If the University of Chicago Won't Defend the Humanities, Who Will?</title><updated>2025-10-05T16:38:34.842201+00:00</updated><content>&lt;doc fingerprint="76417213ac8b1428"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If the University of Chicago Won’t Defend the Humanities, Who Will?&lt;/head&gt;
    &lt;p&gt;Why it matters that the University of Chicago is pausing admissions to doctoral programs in literature, the arts, and languages&lt;/p&gt;
    &lt;p&gt;Listen to more stories on the Noa app.&lt;/p&gt;
    &lt;p&gt;Updated at 1:15 p.m. ET on September 12, 2025&lt;/p&gt;
    &lt;p&gt;The Rockefeller Center Christmas Tree was lit, COVID-19 was still a mysterious respiratory illness in Wuhan, and I was a Ph.D. candidate in a dying field: comparative literature. I was getting ready to Zoom interview for a tenure-track job near Boston that I almost certainly wouldn’t get (and didn’t). Sardined with me in a Greenwich Village coffee shop in December 2019, one of my faculty mentors talked me through, for the thousandth time, the questions I should expect the hiring committee to ask me and dispensed advice about how I should answer them. Then we walked back to his office, lined in handsome foreign-language editions of various novels and works of philosophy, where I would sit for the interview. There, he offered a final piece of wisdom: “Don’t be nervous. It’s just Harvard,” he said, grinning. “It’s not like it’s Chicago.”&lt;/p&gt;
    &lt;p&gt;A joke, but not entirely. For as long as I can remember, and certainly much longer than that, the University of Chicago has been widely viewed as the destination for humanities students and scholars. Some other elite schools might have the coveted Ivy League branding, or a few more famous faculty members, or a couple more dollars to tack onto the salaries of its professors and graduate students. But perhaps nowhere is the study of literature, philosophy, the arts, and languages more valued, their spirit more authentically preserved, their frontiers more doggedly pursued, than at Chicago. The university has had several household names on its humanities faculty, including the firebrand critic Allan Bloom, the novelist Saul Bellow, and the ethicist Martha Nussbaum, as well as scholars who may be less well known to the general public but whose work has been deeply influential in their fields, including the brilliant literary critic Sianne Ngai and Fred Donner, the pathbreaking and Guggenheim-winning historian of early Islam. In short, Chicago is a place for scholars’ scholars. At least, that’s the reputation. And Chicago’s reputation is no doubt why, when the university announced recently that it was reducing Ph.D. admissions for seven departments—among them art history and English language and literature—and outright freezing admissions to others, including classics, the decision was met, in some quarters, with fury and disbelief. “Chicago!” as one stunned academic friend put it in a text to me.&lt;/p&gt;
    &lt;p&gt;In an August 12 email to faculty, Deborah Nelson, Chicago’s arts and humanities dean, said that the changes were necessitated by “this moment of uncertainty” and “evolving fiscal realities.” These bits of bureaucratese appear to be allusions to both the Trump administration’s war on higher education and Chicago’s homegrown financial troubles, which include an eye-popping $6.3 billion in debt. “To be anything but cautious at this moment,” the dean’s email continued, “would be irresponsible.”&lt;/p&gt;
    &lt;p&gt;Chicago’s social-sciences division has also announced doctoral-admissions pauses, primarily in humanistic-leaning programs such as anthropology and social thought, where towering figures including the philosopher Hannah Arendt once taught. What’s happening at Chicago is a particular gut-punch to the humanities, not just at the university itself, but nationally and even globally. The school is, as the classics professor Catherine Kearns put it in a message to me, “a singular center for the pursuit of humanistic knowledge and intellectual growth.” Of the nearly 30 Chicago humanities professors I spoke with for this article, many emphasized that the stakes are much higher than the fate of prospective graduate students or the professors who might teach them. Chicago has long helped to keep alive tiny fields and esoteric areas of humanistic study, particularly in the languages. Without the university’s support, and the continued training of graduate students who can keep these bodies of knowledge going, entire spheres of human learning might eventually blink out.&lt;/p&gt;
    &lt;p&gt;Of course, some might view these comments as self-serving complaints. But the primary fears of the people I spoke with were not about their own careers or futures, but instead about their fields—about knowledge that, once lost, cannot be easily regained. “If you allow a field to die, there’s a loss to something like humanity,” Clifford Ando, a Chicago classicist who has been outspoken about the administration’s maneuvers, told me. “There’s also a real practical risk that a field simply cannot be re-created just because you have books.” I heard this sentiment echoed over and over. “If we stop producing people who are trained or educated to help undergraduates understand the most important things thought or written or painted in human history,” the renowned philosopher Robert Pippin said, “we might not be able to recover that.” Elaine Hadley, an emerita professor of English, told me, “Part of what we do is we’re conservators, keeping a body of knowledge going. We want to innovate and we want to think new things about it, and, you know, we want to make it relevant to the present day, but we’re also trying to keep this knowledge alive.”&lt;/p&gt;
    &lt;p&gt;These responses emphasize the cultural costs of shrinking the number of people trained in humanities fields, rather than focusing on the question of whether universities should be calibrating the production of Ph.D.s to the academic job market. No one I spoke to was insensitive to the pressures their grad students face when confronting the vanishing opportunities for tenure-track employment. But the professors also seemed reluctant to define the success of a program by how many professors it creates—after all, most humanities PhD students at Chicago do not pay tuition and receive stipends to cover their living costs, and getting paid to learn and read is not the worst fate.&lt;/p&gt;
    &lt;p&gt;These faculty perspectives also stood in stark contrast with the reigning image of elite higher educators in right-wing media outlets: that humanities professors are “woke” activists whose primary concern is the political indoctrination of “the youth.” Most of the Chicago faculty I spoke with saw—and defended—their disciplines in terms that were, if anything, conservative. Implicit in their impassioned defenses was the belief that the role of a humanist is to preserve knowledge, safeguard learning from the market and the tides of popular interest, and ward off coarse appeals to economic utility.&lt;/p&gt;
    &lt;p&gt;Depending on whom I asked, the move to scale back humanities doctoral programs is either a prudent acknowledgment of the cratered job market for tenure-track professorships and a wise attempt to protect the university’s humanities division from looming financial and political risks, or it is a cynical effort, under cover of the Trump administration’s assaults, to transfer resources away from “impractical,” unprofitable, and largely jobless fields (such as, say, comparative literature) and toward areas that the university’s senior leadership seems to care about (such as, say, STEM and “innovation”). One faculty member I spoke with mentioned a consulting firm that was brought on to help Chicago as it considers changes to its humanities division, including possibly consolidating the departments from 15 down to eight. Many professors worried that the move to impose uneven changes—reducing admissions in some while halting them in others—may be an attempt to create circumstances that will ultimately make it easier to dissolve the paused programs. “Let no good crisis go unleveraged,” Holly Shissler, an associate professor in the Middle Eastern Studies department, said with a dark laugh. “You engineer a situation in which there are no students, and then you turn around and say, ‘Why are we supporting all these departments and faculty when they have no students?’”&lt;/p&gt;
    &lt;p&gt;When I emailed Nelson and asked whether the changes were part of a plan to kill off the paused departments, she said, “A one-year pause is exactly that—a discrete decision that applies merely to a single admissions cycle.” She seemed to acknowledge, however, that a divisional reorganization could happen. “My goal is to sustain the full scope of our faculty’s research and teaching,” she said. “To do so, we must be open to new ideas and structures.” She added, “There’s no magic number of departments in the arts and humanities.” In the meantime, Chicago’s humanities professors appear largely determined to resist being evaluated in terms of expediency. In a meeting with Nelson a few days after the announcement, 14 out of 15 chairs in the humanities division told the dean that she should pause enrollment in all of their departments or none of them. Targeting some and not others was unacceptable, they argued, because it sent the message that some fields matter and others do not.&lt;/p&gt;
    &lt;p&gt;The department chairs’ wager seems to be that acting as a unified bloc will make reorganizing the division and cutting programs more difficult, even if the division-wide pause causes short-term pain for the next academic year. As anyone who has served on a faculty anywhere can tell you, this degree of cross-department solidarity and willingness to sacrifice for less-favored colleagues is remarkable, and even moving. Last Wednesday afternoon, the dean announced that the chairs had gotten their wish: With the exception of philosophy and music composition (owing to previous pauses in those programs), doctoral admissions will be frozen across the humanities for the 2026–27 academic year.&lt;/p&gt;
    &lt;p&gt;It’s a bittersweet victory, of course, one that will result in fewer doctoral students in the short term and is not guaranteed to strengthen the division in the long term. And it does not settle the most pressing question raised by all this turmoil. If even Chicago is not willing to support and protect American arts and letters, who will? One Chicago administrator, in an attempt to defend the university’s admissions pauses, pointed out that other prestigious peer institutions were expected to make similar announcements about their Ph.D. admissions in the coming weeks, and noted that Harvard is cutting nearly $2 million from its own humanities division. I would like to think that my (and others’) alarm about the future of the humanities is overblown. But the evidence doesn’t give me much hope.&lt;/p&gt;
    &lt;p&gt;The subheading of this article originally incorrectly stated that philosophy was one of the University of Chicago doctoral programs whose graduate admissions were paused.&lt;/p&gt;
    &lt;p&gt;This article originally stated that the University of Chicago’s investments in cryptocurrency are part of its financial troubles. The university maintains that it has not lost money on its cryptocurrency investments.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theatlantic.com/culture/archive/2025/08/university-chicago-humanities-doctorate/684004/"/><published>2025-10-05T15:51:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45482719</id><title>Lina Khan I told you so: The Activision-Blizzard buyout harms gamers&amp;developers</title><updated>2025-10-05T16:38:34.690903+00:00</updated><content>&lt;doc fingerprint="227429327ee674b3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;As Microsoft lays off thousands and jacks up Game Pass prices, former FTC chair says I told you so: The Activision-Blizzard buyout is 'harming both gamers and developers'&lt;/head&gt;
    &lt;p&gt;When you're right, you're right.&lt;/p&gt;
    &lt;p&gt;As Microsoft slashes jobs and raises prices, former US Federal Trade Commission chair Lina Khan has taken to X to say that the company's actions since completing its acquisition of Activision Blizzard in 2023 is pretty much what the FTC warned would happen when it opposed the deal.&lt;/p&gt;
    &lt;p&gt;Khan, you may recall, was head of the FTC when it challenged Microsoft's proposed acquisition of Activision Blizzard, a convoluted process that didn't formally end until May of 2025—almost two years after the deal closed.&lt;/p&gt;
    &lt;p&gt;"Microsoft’s acquisition of Activision has been followed by significant price hikes and layoffs, harming both gamers and developers," Khan wrote on X. "As we’ve seen across sectors, increasing market consolidation and increasing prices often go hand-in-hand.&lt;/p&gt;
    &lt;p&gt;"As dominant firms become too-big-to-care, they can make things worse for their customers without having to worry about the consequences."&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Microsoft’s acquisition of Activision has been followed by significant price hikes and layoffs, harming both gamers and developers. As we’ve seen across sectors, increasing market consolidation and increasing prices often go hand-in-hand. As dominant firms become… https://t.co/FoI50tlEsLOctober 3, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Well, when you're right, you're right, and it's hard to argue that Khan wasn't right on this one. The FTC filed a lawsuit to block the deal in 2022 over concerns that the impact of the proposed acquisition was "reasonably likely to substantially lessen competition and/or tend to create a monopoly in both well-developed and new, burgeoning markets" if it was allowed to go through.&lt;/p&gt;
    &lt;p&gt;Microsoft and Activision, of course, insisted otherwise: Bobby Kotick, then the CEO of Activision Blizzard, said in a July 2023 statement that the merger "will benefit consumers and workers," and also "enable competition rather than allow entrenched market leaders to continue to dominate our rapidly growing industry."&lt;/p&gt;
    &lt;p&gt;The deal was closed in October 2023, even though the FTC's legal action against it was still pending, and it's been one shitty thing after another since then. Just a few months after the deal was sealed, Microsoft laid off 1,900 workers at Activision Blizzard and Xbox, and cancelled the studio's long-awaited survival game; then in September 2024, another 650 people were shown the door. That was followed by the layoff of 9,000 more employees across Microsoft in July 2025, a spot of unpleasantness that also saw multiple game cancellations, the closure of The Initiative, and knock-on impacts on other studios, even as Xbox boss Phil Spencer said the company's gaming business "never looked stronger."&lt;/p&gt;
    &lt;p&gt;Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.&lt;/p&gt;
    &lt;p&gt;Meanwhile, in case you hadn't heard, the cost of Game Pass Ultimate and PC Game Pass also jumped significantly this week. Which is actually the second price hike for Game Pass since the Activision Blizzard deal was concluded: The FTC had some harsh words for the previous (and, ironically, much smaller) price increase in July 2024.&lt;/p&gt;
    &lt;p&gt;Khan was replaced as chair of the FTC in January 2025 by incoming president Donald Trump, so her comments on X don't carry any regulatory weight. But even if this is a hollow I-told-you-so, I'd say it's a well-earned one.&lt;/p&gt;
    &lt;p&gt;2025 games: This year's upcoming releases&lt;lb/&gt;Best PC games: Our all-time favorites&lt;lb/&gt;Free PC games: Freebie fest&lt;lb/&gt;Best FPS games: Finest gunplay&lt;lb/&gt;Best RPGs: Grand adventures&lt;lb/&gt;Best co-op games: Better together&lt;/p&gt;
    &lt;p&gt;Andy has been gaming on PCs from the very beginning, starting as a youngster with text adventures and primitive action games on a cassette-based TRS80. From there he graduated to the glory days of Sierra Online adventures and Microprose sims, ran a local BBS, learned how to build PCs, and developed a longstanding love of RPGs, immersive sims, and shooters. He began writing videogame news in 2007 for The Escapist and somehow managed to avoid getting fired until 2014, when he joined the storied ranks of PC Gamer. He covers all aspects of the industry, from new game announcements and patch notes to legal disputes, Twitch beefs, esports, and Henry Cavill. Lots of Henry Cavill.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pcgamer.com/gaming-industry/as-microsoft-lays-off-thousands-and-jacks-up-game-pass-prices-former-ftc-chair-says-i-told-you-so-the-activision-blizzard-buyout-is-harming-both-gamers-and-developers/"/><published>2025-10-05T16:13:58+00:00</published></entry></feed>