<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-05T13:47:25.565135+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46146451</id><title>I ignore the spotlight as a staff engineer</title><updated>2025-12-05T13:47:31.712018+00:00</updated><content>&lt;doc fingerprint="f7d094139d346c2f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Ignore The Spotlight as a Staff Engineer&lt;/head&gt;
    &lt;p&gt;Lately I’ve been reading Sean Goedecke’s essays on being a Staff+ engineer. His work (particularly Software engineering under the spotlight and It’s Not Your Codebase) is razor-sharp and feels painfully familiar to anyone in Big Tech.&lt;/p&gt;
    &lt;p&gt;On paper, I fit the mold he describes: I’m a Senior Staff engineer at Google. Yet, reading his work left me with a lingering sense of unease. At first, I dismissed this as cynicism. After reflecting, however, I realized the problem wasn’t Sean’s writing but my reading.&lt;/p&gt;
    &lt;p&gt;Sean isn’t being bleak; he is accurately describing how to deal with a world where engineers are fungible assets and priorities shift quarterly. But my job looks nothing like that and I know deep down that if I tried to operate in that environment or in the way he described I’d burn out within months.&lt;/p&gt;
    &lt;p&gt;Instead I’ve followed an alternate path, one that optimizes for systems over spotlights, and stewardship over fungibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;We Live in Different Worlds&lt;/head&gt;
    &lt;p&gt;The foundational reason for our diverging paths is that Sean and I operate in entirely different worlds with different laws governing them.&lt;/p&gt;
    &lt;p&gt;From Sean’s resume, my understanding is that he has primarily worked in product teams 1 building for external customers. Business goals pivot quarterly, and success is measured by revenue or MAU. Optimizing for the “Spotlight” makes complete sense in this environment. Product development at big tech scale is a crowded room: VPs, PMs and UX designers all have strong opinions. To succeed, you have to be agile and ensure you are working specifically on what executives are currently looking at.&lt;/p&gt;
    &lt;p&gt;On the other hand, I’ve spent my entire career much more behind the scenes: in developer tools and infra teams.&lt;/p&gt;
    &lt;p&gt;My team’s customers are thousands of engineers in Android, Chrome, and throughout Google 2. End users of Google products don’t even know we exist; our focus is on making sure developers have the tools to collect product and performance metrics and debug issues using detailed traces.&lt;/p&gt;
    &lt;p&gt;In this environment, our relationship with leadership is very different. We’re never the “hot project everyone wants,” so execs are not fighting to work with us. In fact, my team has historically struggled to hire PMs. The PM career ladder at Google incentivizes splashy external launches so we cannot provide good “promotion material” for them. Also, our feedback comes directly from engineers. Adding a PM in the middle causes a loss in translation, slowing down a tight, high-bandwidth feedback loop.&lt;/p&gt;
    &lt;p&gt;All of this together means our team operates “bottom-up”: instead of execs telling us “you should do X”, we figure out what we think will have the most impact to our customers and work on building those features and tools. Execs ensure that we’re actually solving these problems by considering our impact on more product facing teams.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compounding Returns of Stewardship&lt;/head&gt;
    &lt;p&gt;In the product environments Sean describes, where goals pivot quarterly and features are often experimental, speed is the ultimate currency. You need to ship, iterate, and often move on before the market shifts. But in Infrastructure and Developer Experience, context is the currency.&lt;/p&gt;
    &lt;p&gt;Treating engineers as fungible assets destroys context. You might gain fresh eyes, but you lose the implicit knowledge of how systems actually break. Stewardship, staying with a system long-term, unlocks compounding returns that are impossible to achieve on a short rotation.&lt;/p&gt;
    &lt;p&gt;The first is efficiency via pattern matching. When you stay in one domain for years, new requests are rarely truly “new.” I am not just debugging code; I am debugging the intersection of my tools and hundreds of diverse engineering teams. When a new team comes to me with a “unique” problem, I can often reach back in time: “We tried this approach in 2021 with the Camera team; here is exactly why it failed, and here is the architecture that actually works”.&lt;/p&gt;
    &lt;p&gt;But the more powerful return is systemic innovation. If you rotate teams every year, you are limited to solving acute bugs that are visible right now. Some problems, however, only reveal their shape over long horizons.&lt;/p&gt;
    &lt;p&gt;Take Bigtrace, a project I recently led; it was a solution that emerged solely because I stuck around long enough to see the shape of the problem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Start of 2023 (Observation): I began noticing a pattern. Teams across Google were collecting terabytes or even petabytes of performance traces, but they were struggling to process them. Engineers were writing brittle, custom pipelines to parse data, often complaining about how slow and painful it was to iterate on their analysis.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most of 2023 (Research): I didn’t jump to build a production system. Instead, I spent the best part of a year prototyping quietly in the background while working on other projects. I gathered feedback from these same engineers who had complained and because I had established long-term relationships, they gave me honest and introspective feedback. I learned what sort of UX, latency and throughput requirements they had and figured out how I could meet them.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;End of 2023 to Start of 2024 (Execution): We built and launched Bigtrace, a distributed big data query engine for traces. Today, it processes over 2 billion traces a month and is a critical part of the daily workflow for 100+ engineers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If I had followed the advice to “optimize for fungibility” (i.e. if I had switched teams in 2023 to chase a new project) Bigtrace would not exist.&lt;/p&gt;
    &lt;p&gt;Instead, I would have left during the research phase and my successor would have seen the same “noise” of engineers complaining. But without the historical context to recognize a missing puzzle piece, I think they would have struggled to build something like Bigtrace.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Power of “No”&lt;/head&gt;
    &lt;p&gt;One of the most seductive arguments for chasing the “Spotlight” is that it guarantees resources and executive attention. But that attention is a double-edged sword.&lt;/p&gt;
    &lt;p&gt;High-visibility projects are often volatile. They come with shifting executive whims, political maneuvering, and often end up in situations where long-term quality is sacrificed for short-term survival. For some engineers, navigating this chaos is a thrill. For those of us who care about system stability, it feels like a trap.&lt;/p&gt;
    &lt;p&gt;The advantage of stewardship is that it generates a different kind of capital: trust. When you have spent years delivering reliable tools, you earn the political capital to say “No” to the spotlight when it threatens the product.&lt;/p&gt;
    &lt;p&gt;Recently, the spotlight has been on AI. Every team is under pressure to incorporate it. We have been asked repeatedly: “Why don’t you integrate LLMs into Perfetto?” If I were optimizing for visibility, the answer would be obvious: build an LLM wrapper, demo it to leadership, and claim we are “AI-first.” It would be an easy win for my career.&lt;/p&gt;
    &lt;p&gt;But as a steward of the system, I know that one of Perfetto’s core values is precision. When a kernel developer is debugging a race condition, they need exact timestamps, not a hallucination. Users trust that when we tell them “X is the problem” that it actually is the problem and they’re not going to go chasing their tail for the next week, debugging an issue which doesn’t exist.&lt;/p&gt;
    &lt;p&gt;But it’s important not to take this too far: skepticism shouldn’t become obstructionism. With AI, it’s not “no forever” but “not until it can be done right” 3.&lt;/p&gt;
    &lt;p&gt;A spotlight-seeking engineer might view this approach as a missed opportunity; I view it as protecting what makes our product great: user trust.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Alternate Currency of Impact&lt;/head&gt;
    &lt;p&gt;The most common fear engineers have about leaving the “Spotlight” is career stagnation. The logic goes: If I’m not launching flashy features at Google I/O, and my work isn’t on my VP’s top 5 list, how will I ever get promoted to Staff+?&lt;/p&gt;
    &lt;p&gt;It is true that you lose the currency of “Executive Visibility.” But in infrastructure, you gain two alternate currencies that are just as valuable, and potentially more stable.&lt;/p&gt;
    &lt;p&gt;Shadow Hierarchy&lt;/p&gt;
    &lt;p&gt;In a product organization, you often need to impress your manager’s manager. In an infrastructure organization, you need to impress your customers’ managers.&lt;/p&gt;
    &lt;p&gt;I call this the Shadow Hierarchy. You don’t need your VP to understand the intricacies of your code. You need the Staff+ Engineers in other critical organizations to need your tools.&lt;/p&gt;
    &lt;p&gt;When a Senior Staff Engineer in Pixel tells their VP, “We literally cannot debug the next Pixel phone without Perfetto”, that statement carries immense weight. It travels up their reporting chain, crosses over at the Director/VP level, and comes back down to your manager.&lt;/p&gt;
    &lt;p&gt;This kind of advocacy is powerful because it is technical, not political. It is hard to fake. When you are a steward of a critical system, your promotion packet is filled with testimonials from the most respected engineers in the company saying, “This person’s work enabled our success”.&lt;/p&gt;
    &lt;p&gt;Utility Ledger&lt;/p&gt;
    &lt;p&gt;While product teams might be poring over daily active users or revenue, we rely on metrics tracking engineering health:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Utility: Every bug fixed using our tools is an engineer finding us useful. It is the purest measure of utility.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Criticality: If the Pixel team uses Perfetto to debug a launch-blocking stutter, or Chrome uses it to fix a memory leak, our impact is implicitly tied to their success.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ubiquity: Capturing a significant percentage of the engineering population proves you’ve created a technical “lingua franca”. This becomes especially obvious when you see disconnected parts of the company collaborating with each other, using shared Perfetto traces as a “reference everyone understands”.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scale: Ingesting petabytes of data or processing billions of traces proves architectural resilience better than any design doc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you combine Criticality (VIP teams need this) with Utility (bugs are being fixed), you create a promotion case that is immune to executive reorganizations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Archetypes and Agency&lt;/head&gt;
    &lt;p&gt;Staff Archetypes&lt;/p&gt;
    &lt;p&gt;I am far from the first to notice the idea of “there are multiple ways to be a staff software engineer”. In his book Staff Engineer, Will Larson categorizes Staff-plus engineers into four distinct archetypes.&lt;/p&gt;
    &lt;p&gt;Sean describes the Solver or the Right Hand: engineers who act as agents of executive will, dropping into fires and moving on once the problem is stabilized. I am describing the Architect or the Tech Lead: roles defined by long-term ownership of a specific domain and deep technical context.&lt;/p&gt;
    &lt;p&gt;The “Luck” Rebuttal&lt;/p&gt;
    &lt;p&gt;I can hear the criticism already: “You just got lucky finding your team. Most of us don’t have that luxury.”&lt;/p&gt;
    &lt;p&gt;There are two caveats to all my advice in this post. First, the strategy I have employed so far requires a company profitable enough to sustain long-term infrastructure. This path generally does not exist in startups or early growth companies; it is optimized for Big Tech.&lt;/p&gt;
    &lt;p&gt;Second, luck does play a role in landing on a good team. It is very hard to accurately evaluate team and company culture from the outside. But while finding the team might have involved luck, staying there for almost a decade was a choice.&lt;/p&gt;
    &lt;p&gt;And, at least in my experience, my team is not particularly special: I can name five other teams in Android alone 4. Sure, they might have a director change here or a VP change there, but the core mission and the engineering team remained stable.&lt;/p&gt;
    &lt;p&gt;The reason these teams seem rare is not that they don’t exist, but that they are often ignored. Because they don’t offer the rapid, visible “wins” of a product launch nor are they working on the “shiny cool features”, they attract less competition. If you are motivated by “shipping to billions of users” or seeing your friends and family use something you built, you won’t find that satisfaction here. That is the price of admission.&lt;/p&gt;
    &lt;p&gt;But if you want to build long-term systems and are willing to trade external validation for deep technical ownership, you just need to look behind the curtain.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The tech industry loves to tell you to move fast. But there is another path. It is a path where leverage comes from depth, patience, and the quiet satisfaction of building the foundation that others stand on.&lt;/p&gt;
    &lt;p&gt;You don’t have to chase the spotlight to have a meaningful, high-impact career at a big company. Sometimes, the most ambitious thing you can do is stay put, dig in, and build something that lasts. To sit with a problem space for years until you understand it well enough to build a Bigtrace.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;By product team I don’t mean “frontend team”: even as a backend engineer, you are still working on some part of what is being served directly to end users. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is not exhaustive, Perfetto is open source and we do also care about external developers but that’s not why we get paid. From the company perspective, time we spent on open source bugs is “wasted” time but we do it because we believe in the mission of open source. I talked about this more in a recent post, On Perfetto, Open Source, and Company Priorities. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For what it’s worth, LLMs might not even be the best solution to “let’s put AI into Perfetto”: in my opinion there is lots of value with “old school” machine learning techniques like neural networks. A lot of trace analysis is just pattern matching. This is something I’m hoping to explore more in the coming year! ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Android Kernel, Android System Health, Android Runtime, Android Camera HAL, Android Bionic ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lalitm.com/software-engineering-outside-the-spotlight/"/><published>2025-12-04T11:36:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46147493</id><title>Fighting the age-gated internet</title><updated>2025-12-05T13:47:31.445050+00:00</updated><content>&lt;doc fingerprint="3b8f985538b1b5e0"&gt;
  &lt;main&gt;
    &lt;p&gt;Members of Congress considered 19 online safety bills Tuesday that may soon have a major impact on the future of the internet as age-verification laws have spread to half of the US and around the world.&lt;/p&gt;
    &lt;p&gt;In response, digital and human rights organization Fight for the Future is hosting a week of events—across Reddit, LinkedIn, and various livestreams—to raise awareness of how it believes these bills are setting a dangerous precedent by making the internet more exploitative rather than safer. Many of the proposed bills include a clause for ID or age verification, which forces people to upload an ID, allow a face scan, or otherwise authenticate that they are not a minor before viewing adult content. Fight for the Future says the policies will lead to increased censorship and surveillance.&lt;/p&gt;
    &lt;p&gt;Among the 19 bills considered at the hearing conducted by the House Energy and Commerce Committee was the Kids Online Safety Act (KOSA), which passed with sweeping bipartisan approval in the Senate last year, and the Reducing Exploitative Social Media Exposure for Teens Act, which would ban tech companies from allowing minors under the age of 16 on their platforms. In addition to age verification, the bills raised concerns over issues of parental controls, consumer research of minors, AI, and data privacy.&lt;/p&gt;
    &lt;p&gt;“We’re seeing this huge wave toward ID checks being the norm in tech policy, and it felt like we needed to capture the already activated communities who are not feeling heard in Congress,” says Sarah Philips, a campaigner with Fight for the Future. “If you look on YouTube, if you see people making content about KOSA or responding to a lot of this legislation, it’s very unpopular with people. But it’s viewed on the Hill as very common-sense.”&lt;/p&gt;
    &lt;p&gt;Missouri’s age-gate law took effect earlier this week, meaning 25 US states have passed a form of age verification. The process usually involves third-party services, which can be especially prone to data breaches. This year, the UK also passed a mandate for age verification—the Online Safety Act—and Australia’s teen social media ban, which requires social media companies to deactivate the accounts of users under the age of 16, goes into effect on December 10. Instagram, YouTube, Snap, and TikTok are complying with the historic ban.&lt;/p&gt;
    &lt;p&gt;Philips believes the laws are a direct threat to democratic freedom. “These are censorship laws,” she says. “In the South, where I live, these same proposals mimic a lot of the arguments that you see behind book bans and behind laws that criminalize gender-affirming health care or abortion information.”&lt;/p&gt;
    &lt;p&gt;In March, over 90 human rights advocacy groups signed a coalition letter opposing online ID-check mandates. “The internet is not improved by treating its users like criminal suspects and our lives as opportunities for corporate profit,” David Swanson, campaign coordinator at RootsAction.org, wrote in the letter. “Legislators defunding education to invest in wars, police, prisons, borders, and constant surveillance should think hard before claiming to be acting on behalf of children.”&lt;/p&gt;
    &lt;p&gt;Though Tuesday’s hearing did not advance any legislation, it included testimonies from Joel Thayer, president of the Digital Progress Institute, and Kate Ruane, director of the Free Expression Project at the Center for Democracy and Technology. “The government and social media platforms should not be—indeed, with respect to the government, cannot be—the sole arbiters of the content children can see and services that they can access online,” Ruane said during her testimony.&lt;/p&gt;
    &lt;p&gt;The package of bills is indicative of how Congress has failed to deliver real solutions, Philips says. “We have repeatedly asked them to focus on comprehensive privacy legislation, on antitrust issues, and on things that actually protect us from the surveillance capitalist business model of big tech companies. Congress says they’re holding big tech accountable, but most of the options on the table just mandate verification.” According to The Verge, a revamped version of KOSA removes tech companies’ liability in mitigating potential harms caused by their platforms.&lt;/p&gt;
    &lt;p&gt;In an op-ed for Teen Vogue published in October, Fight for the Future director Evan Greer and campaigner Janus Rose criticized Democratic lawmakers who support KOSA, including the bill’s cowriter, Senator Richard Blumenthal of Connecticut. “KOSA takes the same logic of the bans on drag shows and LGBTQ+ books and applies it to the internet, allowing censorship of a broad range of information in the name of protecting kids from real online harm,” Greer noted.&lt;/p&gt;
    &lt;p&gt;But since KOSA and the Children and Teens’ Online Privacy Protection Act failed to gain approval last year, “it’ll be interesting to see what actually floats to the top right now,” Philips says, concerned that some of the bills could be attached to the National Defense Authorization Act or have the Trump administration’s 10-year moratorium on state AI regulations attached to them, “which is a disaster tornado of tech policies.”&lt;/p&gt;
    &lt;p&gt;Philips tells me she isn’t disheartened by the work, because she wants people to understand what’s really at stake in the fight ahead.&lt;/p&gt;
    &lt;p&gt;“The thing that people misunderstand most about age verification is that it actually applies to all of us,” she says. “A lot of the people pushing for age verification solely focus on kids, because that’s the discussion happening in Congress or on the Hill. But in actuality, if we age-gate the internet and implement mandates, that means that you have to prove that you’re not a child—whether you’re 18 or 50. Everyone will have to interact with this.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wired.com/story/age-verification-is-sweeping-the-us-activists-are-fighting-back/"/><published>2025-12-04T13:34:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46147540</id><title>Transparent leadership beats servant leadership</title><updated>2025-12-05T13:47:31.039186+00:00</updated><content>&lt;doc fingerprint="bce3d0111682d491"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Transparent Leadership Beats Servant Leadership&lt;/head&gt;
    &lt;p&gt;tl:dr: Parenting and leadership is similar. Teach a man to fish, etc.&lt;/p&gt;
    &lt;p&gt;I spent a couple of years managing a team, and I entered that role – like many – without knowing anything about how to do it. I tried to figure out how to be a good manager, and doing so I ended up reading a lot about servant leadership. It never quite sat right with me, though. Servant leadership seems to me a lot like curling parenting: the leader/parent anticipate problems and sweep the way for their direct reports/children.&lt;/p&gt;
    &lt;p&gt;To be clear, this probably feels very good (initially, anyway) for the direct reports/children. But the servant leader/curling parent quickly becomes an overworked single point of failure, and once they leave there is nobody else who knows how to handle the obstacles the leader moved out of the way for everyone. In the worst cases, they leave behind a group of people who have been completely isolated from the rest of the organisation, and has no idea what their purpose is and how to fit in with the rest of the world.&lt;/p&gt;
    &lt;p&gt;I would like to invent my own buzzword: transparent leadership. In my book, a good leader&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;coaches people,&lt;/item&gt;
      &lt;item&gt;connects people,&lt;/item&gt;
      &lt;item&gt;teaches people methodical problem solving,&lt;/item&gt;
      &lt;item&gt;explains values and principles embraced by the organisation to aid them in making aligned decisions on their own,&lt;/item&gt;
      &lt;item&gt;creates direct links between supply and demand (instead of deliberately making themselves a middle man),&lt;/item&gt;
      &lt;item&gt;allows their direct reports career growth by gradually taking over leadership responsibilities,&lt;/item&gt;
      &lt;item&gt;continuously trains their replacement, and&lt;/item&gt;
      &lt;item&gt;generally makes themselves redundant.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The middle manager that doesn’t perform any useful work is a fun stereotype, but I also think it’s a good target to aim for. The difference lies in what to do once one has rendered oneself redundant. A common response is to invent new work, ask for status reports, and add bureaucracy.&lt;/p&gt;
    &lt;p&gt;A better response is to go back to working on technical problems. This keeps the manager’s skills fresh and gets them more respect from their reports. The manager should turn into a high-powered spare worker, rather than a paper-shuffler.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/transparent-leadership-beats-servant-leadership"/><published>2025-12-04T13:40:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149813</id><title>Multivox: Volumetric Display</title><updated>2025-12-05T13:47:30.640394+00:00</updated><content>&lt;doc fingerprint="87cd881f7921b9c9"&gt;
  &lt;main&gt;
    &lt;p&gt;This is the code I currently use to drive my volumetric displays.&lt;/p&gt;
    &lt;p&gt;It supports two closely related devices which are configured in the &lt;code&gt;src/driver/gadgets&lt;/code&gt; directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotovox is a 400mm Orb featuring two 128x64 panels arranged vertically side by side.&lt;/item&gt;
      &lt;item&gt;Vortex is a 300mm Orb featuring two 128x64 panels arranged horizontally, back to back.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rotovox has a higher vertical resolution and better horizontal density; Vortex is brighter and has a higher refresh rate.&lt;/p&gt;
    &lt;p&gt;The 3D printable parts for Vortex are available here.&lt;/p&gt;
    &lt;p&gt;This code was originally written for a single display, and the device specific code was later somewhat abstracted out to support a second similar gadget. There are assumptions about the hardware that are pretty well baked in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It consists of two HUB75 LED panels spinning around a vertical axis.&lt;/item&gt;
      &lt;item&gt;The panels use either ABCDE addressing or ABC shift register addressing.&lt;/item&gt;
      &lt;item&gt;It uses a single GPIO (a photodiode or similar) to sync to rotation - high for 180°, low for 180°.&lt;/item&gt;
      &lt;item&gt;It's running on a Raspberry Pi 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The GPIO mappings and panel layout are defined in &lt;code&gt;src/driver/gadgets/gadget_&amp;lt;name&amp;gt;.h&lt;/code&gt;. GPIO is via memory mapped
access - if you're using a different model of Pi you'll need to change &lt;code&gt;BCM_BASE&lt;/code&gt; in the GPIO code. I haven't tested
this, and you should probably assume it doesn't work.&lt;/p&gt;
    &lt;p&gt;Input is via a bluetooth gamepad - I've been using an Xbox controller, and the input system is based on the default mapping for that.&lt;/p&gt;
    &lt;p&gt;Audio out is also via bluetooth. I haven't had success with the higher quality codecs, but the headset protocol works.&lt;/p&gt;
    &lt;p&gt;There are two parts to this code - the driver, which creates a voxel buffer in shared memory and scans its contents out in sync with rotation, and the client code which generates content and writes it into the voxel buffer. Both driver and client code are designed to run on the same device, a Raspberry Pi embedded in the hardware and spinning at several hundred RPM. There is a demo included in the Python directory which streams point clouds from a PC over wifi to the device, but fundamentally it's designed as a self contained gadget, like an alternate timeline Vectrex. A bluetooth gamepad is used to control the demos.&lt;/p&gt;
    &lt;code&gt;├── src
│   ├── driver
│   │   ├── gadgets         -- the different volumetric display configurations
│   │   │   └──             
│   │   └── vortex.c        -- driver code - creates a voxel buffer in shared memory,
│   │                          and handles scanning it out to the led panels in sync with
│   │                          the rotation
│   ├── simulator
│   │   └── virtex.c        -- software simulator - presents the same voxel buffer as
│   │                          the driver would, but renders the contents into an X11 window
│   │
│   ├── multivox            -- front end / launcher for the various volumetric toys
│   │   └──
│   ├── platform            -- common client code
│   │   └──
│   └── toys                -- a collection of volumetric demos using the shared voxel buffer
│       ├── eighty          -- multiplayer light cycles
│       ├── fireworks.c     -- cheesy first demo
│       ├── flight.c        -- some kind of 70s scifi thing
│       ├── tesseract.c     -- a 4D cubube
│       ├── viewer.c        -- viewer for .obj and .png files
│       └── zander          -- lander/zarch/virus-esque
├── python  
│   ├── calibration.py      -
│   ├── grid.py             -- some pattern generators, useful when calibrating the device
│   ├── colourwheel.py      -
│   ├── obj2c.py            -- tool for embedding .obj models in a header file
│   ├── pointvision.py      -- receive point clouds streamed from vortexstream.py
│   └── vortexstream.py     -- stream point clouds to pointvision.py
└── README.md               -- you are here
&lt;/code&gt;
    &lt;p&gt;On the Raspberry Pi, clone the repository:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/AncientJames/multivox.git
&lt;/code&gt;
    &lt;p&gt;Configure the project for your hardware:&lt;/p&gt;
    &lt;code&gt;cd multivox
mkdir build
cd build
cmake -DMULTIVOX_GADGET=vortex ..
cmake --build .
&lt;/code&gt;
    &lt;p&gt;First, the driver has to be running:&lt;/p&gt;
    &lt;code&gt;sudo ./vortex
&lt;/code&gt;
    &lt;p&gt;When invoked from the command line it periodically outputs profiling information (frame rate, rotation rate), and accepts keyboard input for various diagnostics:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;esc&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;b&lt;/cell&gt;
        &lt;cell&gt;Bit depth - cycles through 1, 2 or 3 bits per channel. Higher bit depths result in lower refresh rates&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;u&lt;/cell&gt;
        &lt;cell&gt;Uniformity - cycles through different strategies for trading off brightness against uniformity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;Trails - adjusts how far back to accumulate skipped voxels when the rotation rate is too high for the refresh rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;l&lt;/cell&gt;
        &lt;cell&gt;Lock - whether to adjust the rotation sync to keep it facing one way&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;d D&lt;/cell&gt;
        &lt;cell&gt;Drift - rotisserie mode. Introduces some explicit drift to the rotation sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;Panel - selectively disable the panels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xyz&lt;/cell&gt;
        &lt;cell&gt;Axis - When the display isn't spinning, it shows an othographic view. This lets you choose the axis&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;While that's running, try one of the toys:&lt;/p&gt;
    &lt;code&gt;./tesseract
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;viewer&lt;/code&gt; takes a list of .obj and .png files as arguments. You can scale, rotate and so on using the gamepad, and it
also accepts keyboard input when run remotely from the command line.&lt;/p&gt;
    &lt;code&gt;./viewer ~/Multivox/models/*.obj
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;esc&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LB/RB&lt;/cell&gt;
        &lt;cell&gt;[ / ]&lt;/cell&gt;
        &lt;cell&gt;Cycle through models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Walkthrough / Orbit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;X&lt;/cell&gt;
        &lt;cell&gt;Zoom to fit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;Toggle wireframe&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you don't have a physical volumetric display, there's a simulator, &lt;code&gt;virtex&lt;/code&gt;, which you can run in place of &lt;code&gt;vortex&lt;/code&gt;. It exposes the same voxel buffer in shared memory, but renders the contents using OpenGL in an X11 window.&lt;/p&gt;
    &lt;p&gt;Run without command line arguments it creates a display compatible with the currently configured gadget, but there are some options to let you experiment with different geometries:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-s X&lt;/cell&gt;
        &lt;cell&gt;slice count - the number of vertical slices per revolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-o X X&lt;/cell&gt;
        &lt;cell&gt;offsets - distance the front and back screens are offset from the axis, as a fraction of screen radius&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-b X&lt;/cell&gt;
        &lt;cell&gt;bits per channel (1 - 3)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-w X Y&lt;/cell&gt;
        &lt;cell&gt;panel resolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-g X&lt;/cell&gt;
        &lt;cell&gt;scan geometry - radial or linear. Linear looks better, but it's a lot harder to build.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;An idealised device with linear scanning and 3 bits per channel can be invoked like this:&lt;/p&gt;
    &lt;code&gt;./virtex -g l -s 128 -w 1280 1280 -b 3
&lt;/code&gt;
    &lt;p&gt;The simulator is fill rate intensive; if you're running it on a Raspberry Pi you'll probably want to reduce the slice count.&lt;/p&gt;
    &lt;p&gt;If you want it to start up automatically on boot, you can install &lt;code&gt;vortex&lt;/code&gt; as a service, and set &lt;code&gt;multivox&lt;/code&gt; to run on startup.&lt;/p&gt;
    &lt;p&gt;First install everything to its default location &lt;code&gt;~/Multivox&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;make install&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This will build the executable files and copy them into the destination directory, as well as creating &lt;code&gt;.mct&lt;/code&gt; files in &lt;code&gt;~/Multivox/carts&lt;/code&gt; for the built in toys.&lt;/p&gt;
    &lt;p&gt;Create the driver service:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/lib/systemd/system/vortex.service
&lt;/code&gt;
    &lt;p&gt;and fill in the following information:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=Vortex Display Driver
After=multi-user.target

[Service]
ExecStart=/home/pi/Multivox/bin/vortex

[Install]
WantedBy=multi-user.target
&lt;/code&gt;
    &lt;p&gt;Then start it up:&lt;/p&gt;
    &lt;code&gt;sudo systemctl daemon-reload
sudo systemctl enable vortex.service
&lt;/code&gt;
    &lt;p&gt;The driver assigns itself to core 3 - you can add &lt;code&gt;isolcpus=3&lt;/code&gt; to the end of &lt;code&gt;/boot/cmdline.txt&lt;/code&gt; to ensure it's the only thing running on that core.&lt;/p&gt;
    &lt;p&gt;You'll also want the launcher to start up on boot:&lt;/p&gt;
    &lt;code&gt;crontab -e
&lt;/code&gt;
    &lt;p&gt;And add the line:&lt;/p&gt;
    &lt;code&gt;@reboot /home/pi/Multivox/bin/multivox
&lt;/code&gt;
    &lt;p&gt;If everything goes smoothly, when you turn on the device it will boot up into &lt;code&gt;Multivox&lt;/code&gt;. This is a fantasy console which
acts as a launcher for all the games and demos you run on the hardware. The bundled toys are automatically installed in
the &lt;code&gt;~/Multivox/carts/&lt;/code&gt; directory as &lt;code&gt;.mct&lt;/code&gt; files, and external apps can be launched by adding a &lt;code&gt;.mct&lt;/code&gt; file containing
its command, path and arguments.&lt;/p&gt;
    &lt;p&gt;Each &lt;code&gt;.mct&lt;/code&gt; file appears as a cartridge in the Multivox front end. They should each have a label on the side; at the moment
all you can do to distinguish between them is change their colour in the &lt;code&gt;.mct&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When you exit an app back to the launcher, it saves a snapshot of the voxel volume, and this gives a preview of what you'll see when you launch a cart. This means there are two competing representations of the same information, and any future work on the front end will probably start with overhauling the entire approach.&lt;/p&gt;
    &lt;p&gt;Some basic UI for controls such as changing bit depth, rebooting and so on would also be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LB/RB&lt;/cell&gt;
        &lt;cell&gt;Cycle through carts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Launch cart&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;⧉&lt;/cell&gt;
        &lt;cell&gt;Exit / resume running cart&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;△ ▽&lt;/cell&gt;
        &lt;cell&gt;Change bit depth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;☰ x5&lt;/cell&gt;
        &lt;cell&gt;Power off&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/AncientJames/multivox"/><published>2025-12-04T16:58:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46150715</id><title>Why are 38 percent of Stanford students saying they're disabled?</title><updated>2025-12-05T13:47:30.222408+00:00</updated><content>&lt;doc fingerprint="1e71405b38cb1c86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Are 38 Percent of Stanford Students Saying They're Disabled?&lt;/head&gt;
    &lt;head rend="h2"&gt;If you get into an elite college, you probably don't have a learning disability.&lt;/head&gt;
    &lt;p&gt;The students at America's elite universities are supposed to be the smartest, most promising young people in the country. And yet, shocking percentages of them are claiming academic accommodations designed for students with learning disabilities.&lt;/p&gt;
    &lt;p&gt;In an article published this week in The Atlantic, education reporter Rose Horowitch lays out some shocking numbers. At Brown and Harvard, 20 percent of undergraduate students are disabled. At Amherst College, that's 34 percent. At Stanford University, it's a galling 38 percent. Most of these students are claiming mental health conditions and learning disabilities, like anxiety, depression, and ADHD.&lt;/p&gt;
    &lt;p&gt;Obviously, something is off here. The idea that some of the most elite, selective universities in America—schools that require 99th percentile SATs and sterling essays—would be educating large numbers of genuinely learning disabled students is clearly bogus. A student with real cognitive struggles is much more likely to end up in community college, or not in higher education at all, right?&lt;/p&gt;
    &lt;p&gt;The professors Horowitz interviewed largely back up this theory. "You hear 'students with disabilities' and it's not kids in wheelchairs," one professor told Horowitch. "It's just not. It's rich kids getting extra time on tests." Talented students get to college, start struggling, and run for a diagnosis to avoid bad grades. Ironically, the very schools that cognitively challenged students are most likely to attend—community colleges—have far lower rates of disabled students, with only three to four percent of such students getting accommodations.&lt;/p&gt;
    &lt;p&gt;To be fair, some of the students receiving these accommodations do need them. But the current language of the Americans with Disabilities Act (ADA) allows students to get expansive accommodations with little more than a doctor's note.&lt;/p&gt;
    &lt;p&gt;While some students are no doubt seeking these accommodations as semi-conscious cheaters, I think most genuinely identify with the mental health condition they're using to get extra time on tests. Over the past few years, there's been a rising push to see mental health and neurodevelopmental conditions as not just a medical fact, but an identity marker. Will Lindstrom, the director of the Regents' Center for Learning Disorders at the University of Georgia, told Horowitch that he sees a growing number of students with this perspective. "It's almost like it's part of their identity," Lindstrom told her. "By the time we see them, they're convinced they have a neurodevelopmental disorder."&lt;/p&gt;
    &lt;p&gt;What's driving this trend? Well, the way conditions like ADHD, autism, and anxiety get talked about online—the place where most young people first learn about these conditions—is probably a contributing factor. Online creators tend to paint a very broad picture of the conditions they describe. A quick scroll of TikTok reveals creators labeling everything from always wearing headphones, to being bad at managing your time, to doodling in class as a sign that someone may have a diagnosable condition. According to these videos, who isn't disabled?&lt;/p&gt;
    &lt;p&gt;The result is a deeply distorted view of "normal." If ever struggling to focus or experiencing boredom is a sign you have ADHD, the implication is that a "normal," nondisabled person has essentially no problems. A "neurotypical" person, the thinking goes, can churn out a 15-page paper with no hint of procrastination, maintain perfect focus during a boring lecture, and never experience social anxiety or awkwardness. This view is buffeted by the current way many of these conditions are diagnosed. As Horowitch points out, when the latest issue of the DSM, the manual psychiatrists use to diagnose patients, was released in 2013, it significantly lowered the bar for an ADHD diagnosis. When the definition of these conditions is set so liberally, it's easy to imagine a highly intelligent Stanford student becoming convinced that any sign of academic struggle proves they're learning disabled, and any problems making friends are a sign they have autism.&lt;/p&gt;
    &lt;p&gt;Risk-aversion, too, seems like a compelling factor driving bright students to claim learning disabilities. Our nation's most promising students are also its least assured. So afraid of failure—of bad grades, of a poorly-received essay—they take any sign of struggle as a diagnosable condition. A few decades ago, a student who entered college and found the material harder to master and their time less easily managed than in high school would have been seen as relatively normal. Now, every time she picks up her phone, a barrage of influencers is clamoring to tell her this is a sign she has ADHD. Discomfort and difficulty are no longer perceived as typical parts of growing up.&lt;/p&gt;
    &lt;p&gt;In this context, it's easy to read the rise of academic accommodations among the nation's most intelligent students as yet another manifestation of the risk-aversion endemic in the striving children of the upper middle class. For most of the elite-college students who receive them, academic accommodations are a protection against failure and self-doubt. Unnecessary accommodations are a two-front form of cheating—they give you an unjust leg-up on your fellow students, but they also allow you to cheat yourself out of genuine intellectual growth. If you mask learning deficiencies with extra time on texts, soothe social anxiety by forgoing presentations, and neglect time management skills with deadline extensions, you might forge a path to better grades. But you'll also find yourself less capable of tackling the challenges of adult life.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://reason.com/2025/12/04/why-are-38-percent-of-stanford-students-saying-theyre-disabled/"/><published>2025-12-04T18:04:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46151299</id><title>I have been writing a niche history blog for 15 years</title><updated>2025-12-05T13:47:30.127200+00:00</updated><content/><link href="https://resobscura.substack.com/p/why-i-have-been-writing-a-niche-history"/><published>2025-12-04T18:49:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46153058</id><title>CUDA-l2: Surpassing cuBLAS performance for matrix multiplication through RL</title><updated>2025-12-05T13:47:29.707227+00:00</updated><content>&lt;doc fingerprint="df7c7dfdcca34d95"&gt;
  &lt;main&gt;
    &lt;p&gt;CUDA-L2 is a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art NVIDIA closed-source libraries (cuBLAS, cuBLASLt-heuristic, cuBLASLt-AutoTuning). Paper&lt;/p&gt;
    &lt;p&gt;Speedup of CUDA-L2 over torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning across 1000 (M,N,K) configurations on A100.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[Dec 2, 2025] Released A100 optimized HGEMM kernels across 1,000 configurations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Release HGEMM with 32-bit accumulator (SM80_16x8x16_F16F16F16F32 and F32F16F16F32 officially) for A100. Current version only support 16-bit accumulator (SM80_16x8x16_F16F16F16F16).&lt;/item&gt;
      &lt;item&gt;Support denser matrix configurations (more configurations).&lt;/item&gt;
      &lt;item&gt;Extend to more GPUs (Ada Lovelace, Hopper, Blackwell).&lt;/item&gt;
      &lt;item&gt;Easy deployment for open-source LLMs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Do A100 kernels apply to other machines like RTX 3090 or H100?&lt;/p&gt;
    &lt;p&gt;A: Ideally, kernels trained on A100 should only be used on A100 if you are targeting speedup. They might have speedup on other machines, but it's not guaranteed. We will progressively release kernels trained on different machines.&lt;/p&gt;
    &lt;p&gt;Q: What if I need matrix dimensions (M, N, K) not found in your configurations?&lt;/p&gt;
    &lt;p&gt;A: 1. You can find the nearest neighbor configuration (larger than yours) and pad with zeros. 2. Feel free to post your dimensions on GitHub issues. We are happy to release kernels for your configuration.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python: Ensure you have a working Python environment.&lt;/item&gt;
      &lt;item&gt;PyTorch: This project requires PyTorch version 2.6.0 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project depends on NVIDIA CUTLASS. You must clone specific tag &lt;code&gt;v4.2.1&lt;/code&gt; into a directory named &lt;code&gt;cutlass&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;git clone -b v4.2.1 https://github.com/NVIDIA/cutlass.git cutlass&lt;/code&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Warning: Please ensure you download the correct CUTLASS version (&lt;code&gt;v4.2.1&lt;/code&gt;) and set the&lt;code&gt;CUTLASS_DIR&lt;/code&gt;environment variable correctly. Incorrect CUTLASS setup may cause the project to fail silently or produce no results.&lt;/quote&gt;
    &lt;p&gt;Before building or running the project, you must configure the following environment variables:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CUTLASS_DIR&lt;/code&gt;: Points to the directory where you cloned CUTLASS.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TORCH_CUDA_ARCH_LIST&lt;/code&gt;: Specifies the target GPU architecture (e.g., "8.0" for NVIDIA Ampere / A100 / RTX 30 series).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run the following commands:&lt;/p&gt;
    &lt;code&gt;export CUTLASS_DIR=/path/to/your/cutlass
export TORCH_CUDA_ARCH_LIST="8.0"&lt;/code&gt;
    &lt;p&gt;To run the evaluation, use the &lt;code&gt;eval_one_file.sh&lt;/code&gt; script. Below is an example command for offline mode:&lt;/p&gt;
    &lt;code&gt;./eval_one_file.sh --mnk 64_4096_64 --warmup_seconds 5 --benchmark_seconds 10 --base_dir ./results --gpu_device_id 7 --mode offline&lt;/code&gt;
    &lt;p&gt;For server mode, you need to specify &lt;code&gt;--target_qps&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;./eval_one_file.sh --mnk 64_4096_64 --warmup_seconds 5 --benchmark_seconds 10 --base_dir ./results --gpu_device_id 7 --mode server --target_qps 100&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Argument&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mnk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Specifies the problem size (e.g., &lt;code&gt;64_4096_64&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--warmup_seconds&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Duration of warmup in seconds before timing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--benchmark_seconds&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Duration of benchmarking in seconds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--base_dir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Directory to save the compile and output results.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--gpu_device_id&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The ID of the GPU to use (e.g., &lt;code&gt;7&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mode&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execution mode. Options are:&lt;p&gt;•&lt;/p&gt;&lt;code&gt;offline&lt;/code&gt;: Runs the evaluation in offline/batch processing mode.&lt;p&gt;•&lt;/p&gt;&lt;code&gt;server&lt;/code&gt;: Runs the evaluation in server mode (simulating request-based scenarios).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--target_qps&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Target Queries Per Second (QPS) for server mode. Required if mode is &lt;code&gt;server&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you have any questions, please open a GitHub issue or reach out to us at jiwei_li@deep-reinforce.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/deepreinforce-ai/CUDA-L2"/><published>2025-12-04T21:04:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154344</id><title>StardustOS: Library operating system for building light-weight Unikernels</title><updated>2025-12-05T13:47:29.269900+00:00</updated><content>&lt;doc fingerprint="d09a987b17a69106"&gt;
  &lt;main&gt;
    &lt;p&gt;Stardust is a unikernel operating system designed to run Cloud applications in a protected, single-address space environment. It delegates the management of physical resources to an underlying hypervisor which is treated as a trusted platform. Stardust has a small code base that can be maintained easily, and relies on static linking to combine a minimal kernel with a single application, along with the libraries and associated programming language run-time required for the execution of the application. Due to static linking, an executable binary of Stardust is packaged within an immutable single-purpose virtual machine image. Stardust supports multiple cores, preemptive threads, and basic block and networking drivers, and provides a collection of standard POSIX-compatible libraries.&lt;/p&gt;
    &lt;p&gt;Stardust is being used in supporting the teaching and research activities at the University of St Andrews.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stardust provides the unikernel implementation in C.&lt;/item&gt;
      &lt;item&gt;Stardust-oxide is a re-implementation of the unikernel in Rust.&lt;/item&gt;
      &lt;item&gt;Duster provides a small debugger for para-virtualised Unikernels written in C that run on the Xen hypervisor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jaradat, W., Dearle A. and Lewis, J. Unikernel Support for Lambda Functions. In the Fifth Annual UK System Research Challenges Workshop, United Kingdom, 2020. Accepted Talk&lt;/item&gt;
      &lt;item&gt;Ahmad, K., Dearle A., Lewis, J. and Jaradat, W. Debugging Unikernel Operating Systems (Slides). In the Fifth Annual UK System Research Challenges Workshop, United Kingdom, 2020. Accepted Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W. On Engineering Unikernels, Systems Seminars Series, University of St Andrews, United Kingdom, 2018. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. Unikernel support for the deployment of light-weight, self-contained, and latency avoiding services. In the Third Annual UK System Research Challenges Workshop, United Kingdom, 2018. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W. Towards Unikernel Support for Distributed Microservices. Adobe Tech Summit, San Francisco, United States of America, 2019. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. The Case for Unikernels. In the Fourth Annual UK System Research Challenges Workshop, United Kingdom, 2019. Lightning Talk&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. Unikernel support for the deployment of light-weight, self-contained, and latency avoiding services. In the Third Annual UK System Research Challenges Workshop, United Kingdom, 2018.&lt;/item&gt;
      &lt;item&gt;McKeogh, F., Stardust Oxide, Dissertation, University of St Andrews, United Kingdom.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/StardustOS"/><published>2025-12-04T22:56:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154491</id><title>We gave 5 LLMs $100K to trade stocks for 8 months</title><updated>2025-12-05T13:47:28.923247+00:00</updated><link href="https://www.aitradearena.com/research/we-ran-llms-for-8-months"/><published>2025-12-04T23:08:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155085</id><title>Trick users and bypass warnings – Modern SVG Clickjacking attacks</title><updated>2025-12-05T13:47:28.114729+00:00</updated><content>&lt;doc fingerprint="120802a0bcc92fb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;SVG Filters - Clickjacking 2.0&lt;/head&gt;&lt;p&gt;Clickjacking is a classic attack that consists of covering up an iframe of some other website in an attempt to trick the user into unintentionally interacting with it. It works great if you need to trick someone into pressing a button or two, but for anything more complicated it’s kind of unrealistic.&lt;/p&gt;&lt;p&gt;I’ve discovered a new technique that turns classic clickjacking on its head and enables the creation of complex interactive clickjacking attacks, as well as multiple forms of data exfiltration.&lt;/p&gt;&lt;p&gt;I call this technique “SVG clickjacking”.&lt;/p&gt;&lt;head rend="h2"&gt;Liquid SVGs&lt;/head&gt;&lt;p&gt;The day Apple announced its new Liquid Glass redesign was pretty chaotic. You couldn’t go on social media without every other post being about the new design, whether it was critique over how inaccessible it seemed, or awe at how realistic the refraction effects were.&lt;/p&gt;&lt;p&gt;Drowning in the flurry of posts, a thought came to mind - how hard would it be to re-create this effect? Could I do this, on the web, without resorting to canvas and shaders? I got to work, and about an hour later I had a pretty accurate CSS/SVG recreation of the effect1.&lt;/p&gt;&lt;p&gt;EMERGENCY!&lt;/p&gt;&lt;p&gt;Girls Rituals&lt;/p&gt;&lt;p&gt;This Won't Be The Last Time&lt;/p&gt;&lt;p&gt;acloudyskye&lt;/p&gt;&lt;p&gt;SOUND BANDIT FUCKING LIVES&lt;/p&gt;&lt;p&gt;Sound Bandit&lt;/p&gt;&lt;p&gt;Love &amp;amp; Ponystep&lt;/p&gt;&lt;p&gt;Vylet Pony&lt;/p&gt;&lt;p&gt;I Love My Computer&lt;/p&gt;&lt;p&gt;Ninajirachi&lt;/p&gt;&lt;p&gt;You can drag around the effect with the bottom-right circle control thing in the demo above (chrome/firefox desktop, chrome mobile).&lt;/p&gt;&lt;p&gt;My little tech demo made quite a splash online, and even resulted in a news article with what is probably the wildest quote about me to date: “Samsung and others have nothing on her”.&lt;/p&gt;&lt;p&gt;A few days passed, and another thought came to mind - would this SVG effect work on top of an iframe?&lt;/p&gt;&lt;p&gt;Like, surely not? The way the effect “refracts light”2 is way too complex to work on a cross-origin document.&lt;/p&gt;&lt;p&gt;But, to my surprise, it did.&lt;/p&gt;&lt;p&gt;The reason this was so interesting to me is that my liquid glass effect uses the &lt;code&gt;feColorMatrix&lt;/code&gt; and &lt;code&gt;feDisplacementMap&lt;/code&gt; SVG filters - changing the colors of pixels, and moving them, respectively. And I could do that on a cross-origin document?&lt;/p&gt;&lt;p&gt;This got me wondering - do any of the other filters work on iframes, and could we turn that into an attack somehow? It turns out that it’s all of them, and yes!&lt;/p&gt;&lt;head rend="h2"&gt;Building blocks&lt;/head&gt;&lt;p&gt;I got to work, going through every &amp;lt;fe*&amp;gt; SVG element and figuring out which ones can be combined to build our own attack primitives.&lt;/p&gt;&lt;p&gt;These filter elements take in one or more input images, apply operations to them, and output a new image. You can chain a bunch of them together within a single SVG filter, and refer to the output of any of the previous filter elements in the chain.&lt;/p&gt;&lt;p&gt;Let’s take a look at some of the more useful base elements we can play with:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&amp;lt;feImage&amp;gt; - load an image file;&lt;/item&gt;&lt;item&gt;&amp;lt;feFlood&amp;gt; - draw a rectangle;&lt;/item&gt;&lt;item&gt;&amp;lt;feOffset&amp;gt; - move stuff around;&lt;/item&gt;&lt;item&gt;&amp;lt;feDisplacementMap&amp;gt; - move pixels according to a map;&lt;/item&gt;&lt;item&gt;&amp;lt;feGaussianBlur&amp;gt; - blur stuff;&lt;/item&gt;&lt;item&gt;&amp;lt;feTile&amp;gt; - tiling and cropping utility;&lt;/item&gt;&lt;item&gt;&amp;lt;feMorphology&amp;gt; - expand/grow light or dark areas;&lt;/item&gt;&lt;item&gt;&amp;lt;feBlend&amp;gt; - blend two inputs according to the mode;&lt;/item&gt;&lt;item&gt;&amp;lt;feComposite&amp;gt; - compositing utilities, can be used to apply an alpha matte, or do various arithmetics on one or two inputs;&lt;/item&gt;&lt;item&gt;&amp;lt;feColorMatrix&amp;gt; - apply a color matrix, this allows moving colors between channels and converting between alpha and luma mattes;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That’s quite a selection of utilities!&lt;/p&gt;&lt;p&gt;If you’re a demoscener3 you’re probably feeling right at home. These are the fundamental building blocks for many kinds of computer graphics, and they can be combined into many useful primitives of our own. So let’s see some examples.&lt;/p&gt;&lt;head rend="h3"&gt;Fake captcha&lt;/head&gt;&lt;p&gt;I’ll start off with an example of basic data exfiltration. Suppose you’re targeting an iframe that contains some sort of sensitive code. You could ask the user to retype it by itself, but that’d probably seem suspicious.&lt;/p&gt;&lt;p&gt;What we can do instead is make use of &lt;code&gt;feDisplacementMap&lt;/code&gt; to make the text seem like a captcha! This way, the user is far more likely to retype the code.&lt;/p&gt;&lt;p&gt;Here is your secret code:&lt;/p&gt;&lt;p&gt;6c79 7261 706f 6e79&lt;/p&gt;&lt;p&gt;Don't share it with anyone!&lt;/p&gt;&lt;p&gt;Here is your secret code:&lt;/p&gt;&lt;p&gt;6c79 7261 706f 6e79&lt;/p&gt;&lt;p&gt;Don't share it with anyone!&lt;/p&gt;&lt;p&gt;(tapclick to edit if you're not a girl)&lt;/p&gt;&lt;p&gt;Note: Only the part inside the &lt;code&gt;&amp;lt;filter&amp;gt;&lt;/code&gt; block is relevant, the rest is just an example of using filters.&lt;/p&gt;&lt;p&gt;Add to this some color effects and random lines, and you’ve got a pretty convincing cap-tcha!&lt;/p&gt;&lt;p&gt;Out of all the attack primitives I’ll be sharing, this one is probably the least useful as sites rarely allow you to frame pages giving out magic secret codes. I wanted to show it though, as it’s a pretty simple introduction to the attack technique.&lt;/p&gt;&lt;p&gt;Still, it could come in handy because often times you’re allowed to frame read-only API endpoints, so maybe there’s an attack there to discover.&lt;/p&gt;&lt;head rend="h3"&gt;Grey text hiding&lt;/head&gt;&lt;p&gt;The next example is for situations where you want to trick someone into, for example, interacting with a text input. Oftentimes the inputs have stuff like grey placeholder text in them, so showing the input box by itself won’t cut it.&lt;/p&gt;&lt;p&gt;Let’s take a look at our example target (try typing in the box).&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;In this example we want to trick the user into setting an attacker-known password, so we want them to be able to see the text they’re entering, but not the grey placeholder text, nor the red “too short” text.&lt;/p&gt;&lt;p&gt;Let’s start off by using &lt;code&gt;feComposite&lt;/code&gt; with arithmetics to make the grey text disappear. The &lt;code&gt;arithmetic&lt;/code&gt; operation takes in two images, &lt;code&gt;i1&lt;/code&gt; (&lt;code&gt;in=...&lt;/code&gt;) and &lt;code&gt;i2&lt;/code&gt; (&lt;code&gt;in2=...&lt;/code&gt;), and lets us do per-pixel maths with &lt;code&gt;k1&lt;/code&gt;, &lt;code&gt;k2&lt;/code&gt;, &lt;code&gt;k3&lt;/code&gt;, &lt;code&gt;k4&lt;/code&gt; as the arguments according to this formula: 4.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Tip! You can leave out the in/in2 parameters if you just want it to be the previous output.&lt;/p&gt;&lt;p&gt;It’s getting there - by multiplying the brightness of the input we’ve made the grey text disappear, but now the black text looks a little suspicious and hard to read, especially on 1x scaling displays.&lt;/p&gt;&lt;p&gt;We could play around with the arguments to find the perfect balance between hiding the grey text and showing the black one, but ideally we’d still have the black text look the way usually does, just without any grey text. Is that possible?&lt;/p&gt;&lt;p&gt;So here’s where a really cool technique comes into play - masking. We’re going to create a matte to “cut out” the black text and cover up everything else. It’s going to take us quite a few steps to get to the desired result, so lets go through it bit-by-bit.&lt;/p&gt;&lt;p&gt;We start off by cropping the result of our black text filter with &lt;code&gt;feTile&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Note: Safari seems to be having some trouble with &lt;code&gt;feTile&lt;/code&gt;, so if the examples flicker or look blank, read this post in a browser such as Firefox or Chrome. If you're writing an attack for Safari, you can also achieve cropping by making a luma matte with &lt;code&gt;feFlood&lt;/code&gt; and then applying it.&lt;/p&gt;&lt;p&gt;Then we use &lt;code&gt;feMorphology&lt;/code&gt; to increase the thickness of the text.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Now we have to increase the contrast of the mask. I’m going to do it by first using &lt;code&gt;feFlood&lt;/code&gt; to create a solid white image, which we can then &lt;code&gt;feBlend&lt;/code&gt; with &lt;code&gt;difference&lt;/code&gt; to invert our mask. And then we can use &lt;code&gt;feComposite&lt;/code&gt; to multiply5 the mask for better contrast.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;We have a luma matte now! All that’s left is to convert it into an alpha matte with &lt;code&gt;feColorMatrix&lt;/code&gt;, apply it to the source image with &lt;code&gt;feComposite&lt;/code&gt;, and make the background white with &lt;code&gt;feBlend&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Looks pretty good, doesn’t it! If you empty out the box (try it!) you might notice some artifacts that give away what we’ve done, but apart from that it’s a pretty good way to sort of sculpt and form various inputs around a bit for an attack.&lt;/p&gt;&lt;p&gt;There are all sorts of other effects you can add to make the input seem just right. Let’s combine everything together into a complete example of an attack.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;You can see how the textbox is entirely recontextualized now to fit a different design while still being fully functional.&lt;/p&gt;&lt;head rend="h3"&gt;Pixel reading&lt;/head&gt;&lt;p&gt;And now we come to what is most likely the most useful attack primitive - pixel reading. That’s right, you can use SVG filters to read color data off of images and perform all sorts of logic on them to create really advanced and convincing attacks.&lt;/p&gt;&lt;p&gt;The catch is of course, that you’ll have to do everything within SVG filters - there is no way to get the data out6. Despite that, it is very powerful if you get creative with it.&lt;/p&gt;&lt;p&gt;On a higher level, what this lets us do is make everything in a clickjacking attack responsive - fake buttons can have hover effects, pressing them can show fake dropdowns and dialogs, and we can even have fake form validation.&lt;/p&gt;&lt;p&gt;Let’s start off with a simple example - detecting if a pixel is pure black, and using it to turn another filter on or off.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;For this target, we want to detect when the user clicks on the box to change its color, and use that to toggle a blur effect.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;Let’s start off by using two copies of the &lt;code&gt;feTile&lt;/code&gt; filter to first crop out the few pixels we’re interested in and then tile those pixels across the entire image.&lt;/p&gt;&lt;p&gt;The result is that we now have the entire screen filled with the color of the area we are interested in.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;We can turn this result into a binary on/off value by using &lt;code&gt;feComposite&lt;/code&gt;’s arithmetic the same way as in the last section, but with a way larger &lt;code&gt;k2&lt;/code&gt; value. This makes it so that the output image is either completely black or completely white.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;And just as before, this can be used as a mask. We once again convert it into an alpha matte, but this time apply it to the blur filter.&lt;/p&gt;&lt;p&gt;So that’s how you can find out whether a pixel is black and use that to toggle a filter!&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;Uh oh! It seems that somebody has changed the target to have a pride-themed button instead!&lt;/p&gt;&lt;p&gt;How can we adapt this technique to work with arbitrary colors and textures?&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;The solution is pretty simple - we can simply use &lt;code&gt;feBlend&lt;/code&gt;’s difference combined with a &lt;code&gt;feColorMatrix&lt;/code&gt; to join the color channels to turn the image into a similar black/white matte as before. For textures we can use &lt;code&gt;feImage&lt;/code&gt;, and for non-exact colors we can use a bit of &lt;code&gt;feComposite&lt;/code&gt;’s arithmetic to make the matching threshold more lenient.&lt;/p&gt;&lt;p&gt;And that’s it, a simple example of how we can read a pixel value and use it to toggle a filter.&lt;/p&gt;&lt;head rend="h3"&gt;Logic gates&lt;/head&gt;&lt;p&gt;But here’s the part where it gets fun! We can repeat the pixel-reading process to read out multiple pixels, and then run logic on them to program an attack.&lt;/p&gt;&lt;p&gt;By using &lt;code&gt;feBlend&lt;/code&gt; and &lt;code&gt;feComposite&lt;/code&gt;, we can recreate all logic gates and make SVG filters functionally complete. This means that we can program anything we want, as long as it is not timing-based7 and doesn’t take up too many resources8.&lt;/p&gt;&lt;p&gt;Input: &lt;/p&gt;&lt;p&gt; NOT: &lt;code&gt;&amp;lt;feBlend mode=difference in2=white /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; AND: &lt;code&gt;&amp;lt;feComposite operator=arithmetic k1=1 /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; OR: &lt;code&gt;&amp;lt;feComposite operator=arithmetic k2=1 k3=1 /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; XOR: &lt;code&gt;&amp;lt;feBlend mode=difference in=a in2=b /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; NAND: &lt;code&gt;(AND + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt; NOR: &lt;code&gt;(OR + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt; XNOR: &lt;code&gt;(XOR + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt;These logic gates are what modern computers are made of. You could build a computer within an SVG filter if you wanted to. In fact, here’s a basic calculator I made:&lt;/p&gt;&lt;p&gt;SVG Adder&lt;/p&gt;&lt;p&gt;This is a full adder circuit. This filter implements the logic gates for the output and for the carry bit using the logic gates described above. There are more efficient ways to implement an adder in SVG filters, but this is meant to serve as proof of the ability to implement arbitrary logic circuits.&lt;/p&gt;&lt;p&gt;Anyways, for an attacker, what all of this means is that you can make a multi-step clickjacking attack with lots of conditions and interactivity. And you can run logic on data from cross-origin frames.&lt;/p&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;This is an example target where we want to trick the user into marking themselves as hacked, which requires a few steps:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Clicking a button to open a dialog&lt;/item&gt;&lt;item&gt;Waiting for the dialog to load&lt;/item&gt;&lt;item&gt;Clicking a checkbox within the dialog&lt;/item&gt;&lt;item&gt;Clicking another button in the dialog&lt;/item&gt;&lt;item&gt;Checking for the red text that appeared&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;Win free iPod by following the steps below.&lt;/p&gt;&lt;p&gt;A traditional clickjacking attack against this target would be difficult to pull off. You’d need to have the user click on multiple buttons in a row with no feedback in the UI.&lt;/p&gt;&lt;p&gt;There are some tricks you could do to make a traditional attack more convincing than what you see above, but it’s still gonna look sketch af. And the moment you throw something like a text input into the mix, it’s just not gonna work.&lt;/p&gt;&lt;p&gt;Anyways, let’s build out a logic tree for a filter-based attack:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Is the dialog open? &lt;list rend="ul"&gt;&lt;item&gt;(No) Is the red text present? &lt;list rend="ul"&gt;&lt;item&gt;(No) Make the user press the button&lt;/item&gt;&lt;item&gt;(Yes) Show the end screen&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;(Yes) Is the dialog loaded? &lt;list rend="ul"&gt;&lt;item&gt;(No) Show loading screen&lt;/item&gt;&lt;item&gt;(Yes) Is the checkbox checked? &lt;list rend="ul"&gt;&lt;item&gt;(No) Make the user check the checkbox&lt;/item&gt;&lt;item&gt;(Yes) Make the user click the button&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;(No) Is the red text present? &lt;/item&gt;&lt;/list&gt;&lt;p&gt;Which can be expressed in logic gates9 as:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Inputs &lt;list rend="ul"&gt;&lt;item&gt;D (dialog visible) = check for background dim&lt;/item&gt;&lt;item&gt;L (dialog loaded) = check for the button in dialog&lt;/item&gt;&lt;item&gt;C (checkbox checked) = check whether the button is blue or grey&lt;/item&gt;&lt;item&gt;R (red text visible) = &lt;code&gt;feMorphology&lt;/code&gt;and check for red pixels&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Outputs &lt;list rend="ul"&gt;&lt;item&gt;(¬D) ∧ (¬R) =&amp;gt; button1.png&lt;/item&gt;&lt;item&gt;D ∧ (¬L) =&amp;gt; loading.png&lt;/item&gt;&lt;item&gt;D ∧ L ∧ (¬C) =&amp;gt; checkbox.png&lt;/item&gt;&lt;item&gt;D ∧ L ∧ C =&amp;gt; button2.png&lt;/item&gt;&lt;item&gt;(¬D) ∧ R =&amp;gt; end.png&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;And this is how we would implement it in SVG:&lt;/p&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;Play around with this and see just how much more convincing it is as an attack. And we could easily make it better by, for example, adding some extra logic to also add hover visuals to the buttons. The demo has debug visuals for the four inputs (D, L, C, R) in the bottom left as squares to make it easier to understand what’s going on.&lt;/p&gt;&lt;p&gt;But yeah, that’s how you can make complex and long clickjacking attacks that have not been realistic with the traditional clickjacking methods.&lt;/p&gt;&lt;p&gt;I kept this example here pretty short and simple, but real-world attacks can be a lot more involved and polished.&lt;/p&gt;&lt;p&gt;In fact…&lt;/p&gt;&lt;head rend="h2"&gt;The Docs bug&lt;/head&gt;&lt;p&gt;I’ve actually managed to pull off this attack against Google Docs!&lt;/p&gt;&lt;p&gt;Take a look at the demo videos here (alt links: bsky, twitter).&lt;/p&gt;&lt;p&gt;What this attack does is:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Makes the user click on the “Generate Document” button&lt;/item&gt;&lt;item&gt;Once pressed, detects the popup and shows a textbox for the user to type a “captcha” into &lt;list rend="ul"&gt;&lt;item&gt;The textbox starts off with a gradient animation, which must be handled&lt;/item&gt;&lt;item&gt;The textbox has focus states, which must also be present in the attack visuals, so they must be detected by the background color of the textbox&lt;/item&gt;&lt;item&gt;The textbox has grey text for both a placeholder AND suggestions, which must be hidden with the technique discussed earlier&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Once the captcha is typed, makes the user seemingly click on a button (or press enter), which causes a suggested Docs item to be added into the textbox &lt;list rend="ul"&gt;&lt;item&gt;This item must be detected by looking for its background color in the textbox&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Once the item is detected, the textbox must be hidden and another button must be shown instead &lt;list rend="ul"&gt;&lt;item&gt;Once that button is clicked, a loading screen appears, which must be detected&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;If the loading screen is present, or the dialog is not visible and the “Generate Document” button is not present, the attack is over and the final screen must be shown&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In the past, individual parts of such an attack could’ve been pulled off through traditional clickjacking and some basic CSS, but the entire attack would’ve been way too long and complex to be realistic. With this new technique of running logic inside SVG filters, such attacks become realistic.&lt;/p&gt;&lt;p&gt;Google VRP awarded me $3133.70 for the find. That was, of course, right before they introduced a novelty bonus for new vulnerability classes. Hmph!10&lt;/p&gt;&lt;head rend="h2"&gt;The QR attack&lt;/head&gt;&lt;p&gt;Something I see in online discussions often is the insistence on QR codes being dangerous. It kind of rubs me the wrong way because QR codes are not any more dangerous than links.&lt;/p&gt;&lt;p&gt;I don’t usually comment on this too much because it’s best to avoid suspicious links, and the same goes for QR codes, but it does nag me to see people make QR codes out to be this evil thing that can somehow immediately hack you.&lt;/p&gt;&lt;p&gt;I turns out though, that my SVG filters attack technique can be applied to QR codes as well!&lt;/p&gt;&lt;p&gt;The example from earlier in the blog with retyping a code becomes impractical once the user realizes they’re typing something they shouldn’t. We can’t stuff the data we exfiltrate into a link either, because an SVG filter cannot create a link.&lt;/p&gt;&lt;p&gt;But since an SVG filter can run logic and provide visual output, perhaps we could generate a QR code with a link instead?&lt;/p&gt;&lt;head rend="h3"&gt;Creating the QR&lt;/head&gt;&lt;p&gt;Creating a QR code within an SVG filter is easier said than done however. We can shape binary data into the shape of a QR code by using &lt;code&gt;feDisplacementMap&lt;/code&gt;, but for a QR code to be scannable it also needs error correction data.&lt;/p&gt;&lt;p&gt;QR codes use Reed-Solomon error correction, which is some fun math stuff that’s a bit more advanced than a simple checksum. It does math with polynomials and stuff and that is a bit annoying to reimplement in an SVG.&lt;/p&gt;&lt;p&gt;Luckily for us, I’ve faced the same problem before! Back in 2021 I was the first person11 to make a QR code generator in Minecraft, so I’ve already figured out the things necessary.&lt;/p&gt;&lt;p&gt;In my build I pre-calculated some lookup tables for the error correction, and used those instead to make the build simpler - and we can do the same with the SVG filter.&lt;/p&gt;&lt;p&gt;This post is already getting pretty long, so I’ll leave figuring out how this filter works as an exercise to the reader ;).&lt;/p&gt;&lt;p&gt;This is a demo that displays a QR code telling you how many seconds you’ve been on this page for. It’s a bit fiddly, so if it doesn’t work make sure that you aren’t using any &lt;/p&gt;&lt;p&gt;This demo &lt;/p&gt;&lt;p&gt;Similarly, in a real attack, the scaling and color profile issues could be worked around using some JavaScript tricks or simply by implementing the filter a bit differently - this here is just a proof of concept that’s a bit rough around the edges.&lt;/p&gt;&lt;p&gt;But yeah, that’s a QR code generator built inside an SVG filter!&lt;/p&gt;&lt;p&gt;Took me a while to make, but I didn’t want to write about it just being “theoretically possible”.&lt;/p&gt;&lt;head rend="h3"&gt;Attack scenario&lt;/head&gt;&lt;p&gt;So the attack scenario with the QR code is that you’d read pixels from a frame, process them to extract the data you want, encode them into a URL that looks something like https://lyra.horse/?ref=c3VwZXIgc2VjcmV0IGluZm8 and render it as a QR code.&lt;/p&gt;&lt;p&gt;Then, you prompt the user to scan the QR code for whatever reason (eg anti-bot check). To them, the URL will seem like just a normal URL with a tracking ID or something in it.&lt;/p&gt;&lt;p&gt;Once the user opens the URL, your server gets the request and receives the data from the URL.&lt;/p&gt;&lt;head rend="h2"&gt;And so on..&lt;/head&gt;&lt;p&gt;There are so many ways to make use of this technique I won’t have time to go over them all in this post. Some examples would be reading text by using the difference blend mode, or exfiltrating data by making the user click on certain parts of the screen.&lt;/p&gt;&lt;p&gt;You could even insert data from the outside to have a fake mouse cursor inside the SVG that shows the pointer cursor and reacts to fake buttons inside your SVG to make the exfiltration more realistic.&lt;/p&gt;&lt;p&gt;Or you could code up attacks with CSS and SVG where CSP doesn’t allow for any JS.&lt;/p&gt;&lt;p&gt;Anyways, this post is long as is, so I’ll leave figuring out these techniques as homework.&lt;/p&gt;&lt;head rend="h2"&gt;Novel technique&lt;/head&gt;&lt;p&gt;This is the first time in my security research I’ve found a completely new technique!&lt;/p&gt;&lt;p&gt;I introduced it briefly at my BSides talk in September, and this post here is a more in-depth overview of the technique and how it can be used.&lt;/p&gt;&lt;p&gt;Of course, you can never know 100% for sure that a specific type of attack has never been found by anyone else, but my extensive search of existing security research has come up with nothing, so I suppose I can crown myself as the researcher who discovered it?&lt;/p&gt;&lt;p&gt;Here’s some previous research I’ve found:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You click, I steal: analyzing and detecting click hijacking attacks in web pages,&lt;lb/&gt;On the fragility and limitations of current Browser-provided Clickjacking protection schemes&lt;list rend="ul"&gt;&lt;item&gt;The papers mention SVG filters in clickjacking attacks, but only in the context of obscuring the underlying elements, not running logic.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Pixel Perfect Timing - Attacks with HTML5,&lt;lb/&gt;Security: SVG Filter Timing Attack&lt;list rend="ul"&gt;&lt;item&gt;Research on reading pixels through SVG filter timing attacks, which is a technique that is mitigated in modern browsers.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The Human Side Channel &lt;list rend="ul"&gt;&lt;item&gt;Some pretty cool clickjacking techniques, though no multi-step attacks or SVG logic.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;SVG is turing-complete-ish &lt;list rend="ul"&gt;&lt;item&gt;Another example of logic gates in SVG I found after writing my blog. It’s fun because it comes with reddit and hn threads - I particularly like the comment asking about whether this turing completeness is useful or just a fun fact, which got a reply confirming the latter. I like turning fun facts into vulnerabilities ^^.&lt;/item&gt;&lt;item&gt;Note that whether SVG filters are actually turing complete is questionable because filters are implemented in constant-time and can’t run in a loop. This doesn’t mean they can’t be turing complete, but it also doesn’t prove that they are.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I don’t think me discovering this technique was just luck though. I have a history of seeing things such as CSS as programming languages to exploit and be creative with. It wasn’t a stretch for me to see SVG filters as a programming language either.&lt;/p&gt;&lt;p&gt;That, and my overlap between security research and creative projects - I often blur the lines between the two, which is what Antonymph was born out of.&lt;/p&gt;&lt;p&gt;In any case, &lt;/p&gt;&lt;head rend="h2"&gt;afterword&lt;/head&gt;&lt;p&gt;whoa this post took such a long time for me to get done!&lt;/p&gt;&lt;p&gt;i started work on it in july, and was expecting to release it alongside my CSS talk in september, but it has taken me so much longer than expected to actually finish this thing. i wanted to make sure it was a good in-depth post, rather than something i just get out as soon as possible.&lt;/p&gt;&lt;p&gt;unlike my previous posts, i did unfortunately have to break my trend of using no images, since i needed a few data URIs within the SVG filters for demos. still, no images anywhere else in the post, no javascript, and just 42kB (gzip) of handcrafted html/css/svg.&lt;/p&gt;&lt;p&gt;also, i usually hide a bunch of easter eggs in my post that link to stuff i’ve enjoyed recently, but i have a couple links i didn’t want to include without content warnings. finding responsibility is a pretty dark talk about the ethics of making sure your work won’t end up killing people, and youre the one ive always wanted is slightly nsfw doggyhell vent art.&lt;/p&gt;&lt;p&gt;btw i’ll soon be giving talks at 39c3 and disobey 2026! the 39c3 one is titled “css clicker training” and will be about css crimes and making games in css. and the disobey one is the same talk as the bsides one about using css to hack stuff and get bug bounties, but i’ll make sure to throw some extra content in there to keep it fun.&lt;/p&gt;&lt;p&gt;see y’all around!!&lt;/p&gt;&lt;p&gt;&amp;lt;3&lt;/p&gt;&lt;p&gt;Note: I you’re making content (articles, videos etc) based on this post, feel free to reach out to me to ask for questions or feedback.&lt;/p&gt;&lt;p&gt;Discuss this post on: twitter, mastodon, lobsters&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;What I actually had after an hour was this, the Codepen link is an updated version that I added controls to later on. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This is a fancy way of saying it does a basic displacement of pixels. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;…or After Effects/Blender/Fusion etc user. Or anything else computer graphics. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;result = k1*i1*i2 + k2*i1 + k3*i2 + k4 in programmer language (I just couldn’t resist trying out the &amp;lt;math&amp;gt; tag for fun). ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The multiplication in this case is kind of the opposite of what you’d expect from the “multiply” blend mode - things will get lighter, not darker. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;It’s not possible to get the pixel data out of a SVG filter as they’re implemented in constant-time. If you can find a way to retrieve the data then it’s a browser bug and you can most likely get bounty for it. Happy to collaborate if you’d like to turn such a finding into a working proof of concept for a report :). ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;We can actually pass the current time into an SVG filter, but we can’t do attacks such as “if a pixel changes, wait 1 second and then show a dialog” unless we can piggyback off an animation in the source frame. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Since SVG filters are implemented in constant-time, they become pretty resource-intensive for complex filters on high-resolution targets. One optimization would be to have a full-resolution filter just for picking out the pixels, then a tiny-resolution backdrop-filter to run all the logic, and then another full-resolution filter to display the attack. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;¬ - NOT, ∧ - AND, ∨ - OR, ⊕ - XOR etc, see List of logic symbols. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This is kind of similar to how I reported the Docs/YouTube/Slides chain right before they 5x’d the VRP rewards. I seem to have the worst luck with timing my reports… ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I released my QR code generator in 2021, making it the earliest publicly released Minecraft QR code generator. I know, however, that DavidJR was independently working on a QR code generator at the same time as I was, eventually releasing it in 2023. Then there’s one from Sep 2024 by 37meliodas, and lastly there’s the probably most well-known one by mattbatwings from Dec 2024. The latter has an awesome video explaining everything in-depth, so I definitely recommend checking it out if you’re interested in Minecraft redstone. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lyra.horse/blog/2025/12/svg-clickjacking/"/><published>2025-12-05T00:03:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155135</id><title>Netflix’s AV1 Journey: From Android to TVs and Beyond</title><updated>2025-12-05T13:47:27.927356+00:00</updated><content/><link href="https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80"/><published>2025-12-05T00:09:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155619</id><title>BMW PHEV: Safety fuse replacement is extremely expensive</title><updated>2025-12-05T13:47:27.582131+00:00</updated><content/><link href="https://evclinic.eu/2025/12/04/2021-phev-bmw-ibmucp-21f37e-post-crash-recovery-when-eu-engineering-becomes-a-synonym-for-unrepairable-generating-waste/"/><published>2025-12-05T01:05:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155701</id><title>NeurIPS 2025 Best Paper Awards</title><updated>2025-12-05T13:47:27.459398+00:00</updated><content/><link href="https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/"/><published>2025-12-05T01:15:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46157594</id><title>UniFi 5G</title><updated>2025-12-05T13:47:27.262328+00:00</updated><content>&lt;doc fingerprint="3e818a12e14516a1"&gt;
  &lt;main&gt;
    &lt;p&gt;The UniFi 5G Max lineup was created with a clear goal in mind: deliver a sleek, versatile, and exceptionally powerful 5G internet experience that works effortlessly in any environment.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G Max lineup was created with a clear goal in mind: deliver a sleek, versatile, and exceptionally powerful 5G internet experience that works effortlessly in any environment.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G Max makes deployment easy, whether installed locally or at a remote site. Plug it into any PoE port and it instantly appears as a ready to use WAN interface, no matter whether plugged directly into your UniFi gateway or into your office switch. No new cable runs needed! It sits neatly on a desk, but you can reposition it for the best possible signal using the included wall or window mount.&lt;/p&gt;
    &lt;p&gt;The 5G Max delivers downlink speeds up to 2 Gbps with ultra low latency that makes it reliable as a primary connection and seamless as a backup WAN. UniFi routing policies and SLAs let you choose exactly how and when 5G is used, and for which clients and VLANs. Easily set per-SIM usage limits to avoid overage costs with just a few clicks.&lt;/p&gt;
    &lt;p&gt;For tougher environments or deployments with poor indoor cellular coverage, the outdoor model maintains the same high performance cellular connectivity with improved antenna performance in a durable IP67 rated enclosure. It is built for rooftop installs, off site locations, and mobile deployments where reliability is critical. Just like its indoor counterpart, you can also connect it via any PoE port, anywhere on your network, greatly simplifying cabling requirements.&lt;/p&gt;
    &lt;p&gt;If you want everything UniFi in one device, the DreamRouter 5G Max combines 5G connectivity with WiFi 7, local storage, and full UniFi OS application support. Deploy it anywhere 5G is available and run an entire high-performance and scalable network stack instantly.&lt;/p&gt;
    &lt;p&gt;Every device in the UniFi 5G lineup supports both physical SIMs and eSIM, giving you the freedom to choose your carrier and switch whenever needed with zero friction. All are equipped with dual SIM slots, with one SIM replaceable by eSIM, and are fully unlocked: any major carrier, any type of deployment, with one piece of hardware.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G lineup brings sleek design, powerful performance, easy installation, and genuine WAN flexibility to every deployment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.ui.com/article/introducing-unifi-5g"/><published>2025-12-05T07:06:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46158578</id><title>Kenyan court declares law banning seed sharing unconstitutional</title><updated>2025-12-05T13:47:26.934588+00:00</updated><content>&lt;doc fingerprint="d8e624ea1bbbd655"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Kenyan court declares law banning seed sharing unconstitutional&lt;/head&gt;
    &lt;p&gt;KISUMU, Kenya (AP) — A high court in Kenya on Thursday declared unconstitutional sections of a seed law that prevented farmers from sharing and selling indigenous seeds in what food campaigners have called a landmark win for food security.&lt;/p&gt;
    &lt;p&gt;Farmers in Kenya could face up to two years’ imprisonment and a fine of 1 million Kenya shillings ($7,700) for sharing seeds through their community seed banks, according to a seed law signed in 2012.&lt;/p&gt;
    &lt;p&gt;Justice Rhoda Rutto on Thursday said sections of the seed law that gave government officials powers to raid seed banks and seize seeds were also unconstitutional.&lt;/p&gt;
    &lt;p&gt;The law was introduced as a measure to curb growing sale of counterfeit seeds that were causing loses in the agricultural sector and gave sole seed trading rights to licensed companies.&lt;/p&gt;
    &lt;p&gt;The case had been filed by 15 smallholder farmers, who are members of community seed banks that have been in operation for years, preserving and sharing seeds among colleagues.&lt;/p&gt;
    &lt;p&gt;A farmer, Samuel Wathome, who was among the 15, said the old farming practices had been vindicated.&lt;/p&gt;
    &lt;p&gt;“My grandmother saved seeds, and today the court has said I can do the same for my grandchildren without fear of the police or of prison,” he said.&lt;/p&gt;
    &lt;p&gt;Elizabeth Atieno, a food campaigner at Greenpeace Africa, called the win a “victory for our culture, our resilience, and our future.”&lt;/p&gt;
    &lt;p&gt;“By validating indigenous seeds, the court has struck a blow against the corporate capture of our food system. We can finally say that in Kenya, feeding your community with climate-resilient, locally adapted seeds is no longer a crime,” she said.&lt;/p&gt;
    &lt;p&gt;Food campaigners have in the past encouraged governments to work with farmers to preserve indigenous seeds as a way of ensuring food security by offering farmers more plant varieties.&lt;/p&gt;
    &lt;p&gt;Indigenous seeds are believed to be drought resistant and adaptable to the climate conditions of their native areas, and hence often outperform hybrid seeds.&lt;/p&gt;
    &lt;p&gt;Kenya has a national seed bank based near the capital Nairobi where indigenous seeds are stored in cold rooms, but farmers say community seed banks are equally important for variety and proximity to the farmer.&lt;/p&gt;
    &lt;p&gt;The country has faced challenges in the seed sector where counterfeit seeds were sold to farmers, leading to losses amounting to millions of shillings in a country that relies on rain-fed agriculture.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apnews.com/article/kenya-seed-sharing-law-ruling-ad4df5a364299b3a9f8515c0f52d5f80"/><published>2025-12-05T09:09:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46159193</id><title>The US polluters that are rewriting the EU's human rights and climate law</title><updated>2025-12-05T13:47:26.843182+00:00</updated><content/><link href="https://www.somo.nl/the-secretive-cabal-of-us-polluters-that-is-rewriting-the-eus-human-rights-and-climate-law/"/><published>2025-12-05T09:58:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46160239</id><title>Sugars, Gum, Stardust Found in NASA's Asteroid Bennu Samples</title><updated>2025-12-05T13:47:26.617439+00:00</updated><content>&lt;doc fingerprint="a654d1fe9bb70191"&gt;
  &lt;main&gt;
    &lt;p&gt;The asteroid Bennu continues to provide new clues to scientists’ biggest questions about the formation of the early solar system and the origins of life. As part of the ongoing study of pristine samples delivered to Earth by NASA’s OSIRIS-REx (Origins, Spectral Interpretation, Resource Identification, and Security-Regolith Explorer) spacecraft, three new papers published Tuesday by the journals Nature Geosciences and Nature Astronomy present remarkable discoveries: sugars essential for biology, a gum-like substance not seen before in astromaterials, and an unexpectedly high abundance of dust produced by supernova explosions.&lt;/p&gt;
    &lt;p&gt;Sugars essential to life&lt;/p&gt;
    &lt;p&gt;Scientists led by Yoshihiro Furukawa of Tohoku University in Japan found sugars essential for biology on Earth in the Bennu samples, detailing their findings in the journal Nature Geoscience. The five-carbon sugar ribose and, for the first time in an extraterrestrial sample, six-carbon glucose were found. Although these sugars are not evidence of life, their detection, along with previous detections of amino acids, nucleobases, and carboxylic acids in Bennu samples, show building blocks of biological molecules were widespread throughout the solar system.&lt;/p&gt;
    &lt;p&gt;For life on Earth, the sugars deoxyribose and ribose are key building blocks of DNA and RNA, respectively. DNA is the primary carrier of genetic information in cells. RNA performs numerous functions, and life as we know it could not exist without it. Ribose in RNA is used in the molecule’s sugar-phosphate “backbone” that connects a string of information-carrying nucleobases.&lt;/p&gt;
    &lt;p&gt;“All five nucleobases used to construct both DNA and RNA, along with phosphates, have already been found in the Bennu samples brought to Earth by OSIRIS-REx,” said Furukawa. “The new discovery of ribose means that all of the components to form the molecule RNA are present in Bennu.”&lt;/p&gt;
    &lt;p&gt;The discovery of ribose in asteroid samples is not a complete surprise. Ribose has previously been found in two meteorites recovered on Earth. What is important about the Bennu samples is that researchers did not find deoxyribose. If Bennu is any indication, this means ribose may have been more common than deoxyribose in environments of the early solar system.&lt;/p&gt;
    &lt;p&gt;Researchers think the presence of ribose and lack of deoxyribose supports the “RNA world” hypothesis, where the first forms of life relied on RNA as the primary molecule to store information and to drive chemical reactions necessary for survival.&lt;/p&gt;
    &lt;p&gt;“Present day life is based on a complex system organized primarily by three types of functional biopolymers: DNA, RNA, and proteins,” explains Furukawa. “However, early life may have been simpler. RNA is the leading candidate for the first functional biopolymer because it can store genetic information and catalyze many biological reactions.”&lt;/p&gt;
    &lt;p&gt;The Bennu samples also contained one of the most common forms of “food” (or energy) used by life on Earth, the sugar glucose, which is the first evidence that an important energy source for life as we know it was also present in the early solar system.&lt;/p&gt;
    &lt;p&gt;Mysterious, ancient ‘gum’&lt;/p&gt;
    &lt;p&gt;A second paper, in the journal Nature Astronomy led by Scott Sandford at NASA’s Ames Research Center in California’s Silicon Valley and Zack Gainsforth of the University of California, Berkeley, reveals a gum-like material in the Bennu samples never seen before in space rocks – something that could have helped set the stage on Earth for the ingredients of life to emerge. The surprising substance was likely formed in the early days of the solar system, as Bennu’s young parent asteroid warmed.&lt;/p&gt;
    &lt;p&gt;Once soft and flexible, but since hardened, this ancient “space gum” consists of polymer-like materials extremely rich in nitrogen and oxygen. Such complex molecules could have provided some of the chemical precursors that helped trigger life on Earth, and finding them in the pristine samples from Bennu is important for scientists studying how life began and whether it exists beyond our planet.&lt;/p&gt;
    &lt;head rend="h2"&gt;On this primitive asteroid that formed in the early days of the solar system, we’re looking at events near the beginning of the beginning.&lt;/head&gt;
    &lt;p&gt;Scott SandFord&lt;/p&gt;
    &lt;p&gt;Astrophysicist, NASA's Ames Research Center&lt;/p&gt;
    &lt;p&gt;Bennu’s ancestral asteroid formed from materials in the solar nebula – the rotating cloud of gas and dust that gave rise to the solar system – and contained a variety of minerals and ices. As the asteroid began to warm, due to natural radiation, a compound called carbamate formed through a process involving ammonia and carbon dioxide. Carbamate is water soluble, but it survived long enough to polymerize, reacting with itself and other molecules to form larger and more complex chains impervious to water. This suggests that it formed before the parent body warmed enough to become a watery environment.&lt;/p&gt;
    &lt;p&gt;“With this strange substance, we’re looking at, quite possibly, one of the earliest alterations of materials that occurred in this rock,” said Sandford. “On this primitive asteroid that formed in the early days of the solar system, we’re looking at events near the beginning of the beginning.”&lt;/p&gt;
    &lt;p&gt;Using an infrared microscope, Sandford’s team selected unusual, carbon-rich grains containing abundant nitrogen and oxygen. They then began what Sandford calls “blacksmithing at the molecular level,” using the Molecular Foundry at Lawrence Berkeley National Laboratory (Berkeley Lab) in Berkeley, California. Applying ultra-thin layers of platinum, they reinforced a particle, welded on a tungsten needle to lift the tiny grain, and shaved the fragment down using a focused beam of charged particles.&lt;/p&gt;
    &lt;p&gt;When the particle was a thousand times thinner than a human hair, they analyzed its composition via electron microscopy at the Molecular Foundry and X-ray spectroscopy at Berkeley Lab’s Advanced Light Source. The ALS’s high spatial resolution and sensitive X-ray beams enabled unprecedented chemical analysis.&lt;/p&gt;
    &lt;p&gt;“We knew we had something remarkable the instant the images started to appear on the monitor,” said Gainsforth. “It was like nothing we had ever seen, and for months we were consumed by data and theories as we attempted to understand just what it was and how it could have come into existence.”&lt;/p&gt;
    &lt;p&gt;The team conducted a slew of experiments to examine the material’s characteristics. As the details emerged, the evidence suggested the strange substance had been deposited in layers on grains of ice and minerals present in the asteroid.&lt;/p&gt;
    &lt;p&gt;It was also flexible – a pliable material, similar to used gum or even a soft plastic. Indeed, during their work with the samples, researchers noticed the strange material was bendy and dimpled when pressure was applied. The stuff was translucent, and exposure to radiation made it brittle, like a lawn chair left too many seasons in the sun.&lt;/p&gt;
    &lt;p&gt;“Looking at its chemical makeup, we see the same kinds of chemical groups that occur in polyurethane on Earth,” said Sandford, “making this material from Bennu something akin to a ‘space plastic.’”&lt;/p&gt;
    &lt;p&gt;The ancient asteroid stuff isn’t simply polyurethane, though, which is an orderly polymer. This one has more “random, hodgepodge connections and a composition of elements that differs from particle to particle,” said Sandford. But the comparison underscores the surprising nature of the organic material discovered in NASA’s asteroid samples, and the research team aims to study more of it.&lt;/p&gt;
    &lt;p&gt;By pursuing clues about what went on long ago, deep inside an asteroid, scientists can better understand the young solar system – revealing the precursors to and ingredients of life it already contained, and how far those raw materials may have been scattered, thanks to asteroids much like Bennu.&lt;/p&gt;
    &lt;p&gt;Abundant supernova dust&lt;/p&gt;
    &lt;p&gt;Another paper in the journal Nature Astronomy, led by Ann Nguyen of NASA’s Johnson Space Center in Houston, analyzed presolar grains – dust from stars predating our solar system – found in two different rock types in the Bennu samples to learn more about where its parent body formed and how it was altered by geologic processes. It is believed that presolar dust was generally well-mixed as our solar system formed. The samples had six-times the amount of supernova dust than any other studied astromaterial, suggesting the asteroid’s parent body formed in a region of the protoplanetary disk enriched in the dust of dying stars.&lt;/p&gt;
    &lt;p&gt;The study also reveals that, while Bennu’s parent asteroid experienced extensive alteration by fluids, there are still pockets of less-altered materials within the samples that offer insights into its origin.&lt;/p&gt;
    &lt;p&gt;“These fragments retain a higher abundance of organic matter and presolar silicate grains, which are known to be easily destroyed by aqueous alteration in asteroids,” said Nguyen. “Their preservation in the Bennu samples was a surprise and illustrates that some material escaped alteration in the parent body. Our study reveals the diversity of presolar materials that the parent accreted as it was forming.”&lt;/p&gt;
    &lt;p&gt;NASA’s Goddard Space Flight Center provided overall mission management, systems engineering, and the safety and mission assurance for OSIRIS-REx. Dante Lauretta of the University of Arizona, Tucson, is the principal investigator. The university leads the science team and the mission’s science observation planning and data processing. Lockheed Martin Space in Littleton, Colorado, built the spacecraft and provided flight operations. Goddard and KinetX Aerospace were responsible for navigating the OSIRIS-REx spacecraft. Curation for OSIRIS-REx takes place at NASA’s Johnson Space Center in Houston. International partnerships on this mission include the OSIRIS-REx Laser Altimeter instrument from CSA (Canadian Space Agency) and asteroid sample science collaboration with JAXA’s (Japan Aerospace Exploration Agency’s) Hayabusa2 mission. OSIRIS-REx is the third mission in NASA’s New Frontiers Program, managed by NASA’s Marshall Space Flight Center in Huntsville, Alabama, for the agency’s Science Mission Directorate in Washington.&lt;/p&gt;
    &lt;p&gt;For more information on the OSIRIS-REx mission, visit:&lt;/p&gt;
    &lt;p&gt;https://www.nasa.gov/osiris-rex&lt;/p&gt;
    &lt;p&gt;Karen Fox / Molly Wasser&lt;lb/&gt;Headquarters, Washington&lt;lb/&gt;202-285-5155 / 240-419-1732&lt;lb/&gt;karen.c.fox@nasa.gov / molly.l.wasser@nasa.gov&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nasa.gov/missions/osiris-rex/sugars-gum-stardust-found-in-nasas-asteroid-bennu-samples/"/><published>2025-12-05T12:12:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46160315</id><title>Netflix to Acquire Warner Bros</title><updated>2025-12-05T13:47:26.539797+00:00</updated><content/><link href="https://about.netflix.com/en/news/netflix-to-acquire-warner-bros"/><published>2025-12-05T12:21:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46160698</id><title>Making RSS More Fun</title><updated>2025-12-05T13:47:26.309721+00:00</updated><content>&lt;doc fingerprint="2b4893113586319f"&gt;
  &lt;main&gt;
    &lt;p&gt;I don't like RSS readers. I know, this is blasphemous especially on a website where I'm actively encouraging you to subscribe through RSS. As someone writing stuff, RSS is great for me. I don't have to think about it, the requests are pretty light weight, I don't need to think about your personal data or what client you are using. So as a protocol RSS is great, no notes.&lt;/p&gt;
    &lt;p&gt;However as something I'm going to consume, it's frankly a giant chore. I feel pressured by RSS readers, where there is this endlessly growing backlog of things I haven't read. I rarely want to read all of a websites content from beginning to end, instead I like to jump between them. I also don't really care if the content is chronological, like an old post about something interesting isn't less compelling to me than a newer post.&lt;/p&gt;
    &lt;p&gt;What I want, as a user experience, is something akin to TikTok. The whole appeal of TikTok, for those who haven't wasted hours of their lives on it, is that I get served content based on an algorithm that determines what I might think is useful or fun. However what I would like is to go through content from random small websites. I want to sit somewhere and passively consume random small creators content, then upvote some of that content and the service should show that more often to other users. That's it. No advertising, no collecting tons of user data about me, just a very simple "I have 15 minutes to kill before the next meeting, show me some random stuff."&lt;/p&gt;
    &lt;p&gt;In this case the "algorithm" is pretty simple: if more people like a thing, more people see it. But with Google on its way to replacing search results with LLM generated content, I just wanted to have something that let me play around with the small web the way that I used to.&lt;/p&gt;
    &lt;p&gt;There actually used to be a service like this called StumbleUpon which was more focused on pushing users towards popular sites. It has been taken down, presumably because there was no money in a browser plugin that sent users to other websites whose advertising you didn't control.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;You can go download the Firefox extension now and try this out and skip the rest of this if you want. https://timewasterpro.xyz/ If you hate it or find problems, let me know on Mastodon. https://c.im/@matdevdug&lt;/p&gt;
    &lt;head rend="h3"&gt;Functionality&lt;/head&gt;
    &lt;p&gt;So I wanted to do something pretty basic. You hit a button, get served a new website. If you like the website, upvote it, otherwise downvote it. If you think it has objectionable content then hit report. You have to make an account (because I couldn't think of another way to do it) and then if you submit links and other people like it, you climb a Leaderboard.&lt;/p&gt;
    &lt;p&gt;On the backend I want to (very slowly so I don't cost anyone a bunch of money) crawl a bunch of RSS feeds, stick the pages in a database and then serve them up to users. Then I want to track what sites get upvotes and return those more often to other users so that "high quality" content shows up more often. "High quality" would be defined by the community or just me if I'm the only user.&lt;/p&gt;
    &lt;p&gt;It's pretty basic stuff, most of it copied from tutorials scattered around the Internet. However I really want to drive home to users that this is not a Serious Thing. I'm not a company, this isn't a new social media network, there are no plans to "grow" this concept beyond the original idea unless people smarter than me ping with me ideas. So I found this amazing CSS library: https://sakofchit.github.io/system.css/&lt;/p&gt;
    &lt;p&gt;The Apple's System OS design from the late-80s to the early 90s was one of my personal favorites and I think would send a strong signal to a user that this is not a professional, modern service.&lt;/p&gt;
    &lt;p&gt;Great, the basic layout works. Let's move on!&lt;/p&gt;
    &lt;head rend="h3"&gt;Backend&lt;/head&gt;
    &lt;p&gt;So I ended up doing FastAPI because it's very easy to write. I didn't want to spend a ton of time writing the API because I doubt I nailed the API design on the first round. I use sqlalchemy for the database. The basic API layout is as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;admin - mostly just generating read-only reports of like "how many websites are there"&lt;/item&gt;
      &lt;item&gt;leaderboard - So this is my first attempt at trying to get users involved. Submit a website that other people like? Get points, climb leaderboard.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The source for the RSS feeds came from the (very cool) Kagi small web Github. https://github.com/kagisearch/smallweb. Basically I assume that websites that have submitted their RSS feeds here are cool with me (very rarely) checking for new posts and adding them to my database. If you want the same thing as this does, but as an iFrame, that's the Kagi small web service.&lt;/p&gt;
    &lt;p&gt;The scraping work is straightforward. We make a background worker, they grab 5 feeds every 600 seconds, they check for new content on each feed and then wait until the 600 seconds has elapsed to grab 5 more from the smallweb list of RSS feeds. Since we have a lot of feeds, this ends up look like we're checking for new content less than once a day which is the interval that I want.&lt;/p&gt;
    &lt;p&gt;Then we write it out to a sqlite database and basically track "has this URL been reported", if so, put it into a review queue and then how many times this URL has been liked or disliked. I considered a "real" database but honestly sqlite is getting more and more scalable every day and its impossible to beat the immediate start up and functionality. Plus very easy to back up to encrypted object storage which is super nice for a hobby project where you might wipe the prod database at any moment.&lt;/p&gt;
    &lt;p&gt;In terms of user onboarding I ended up doing the "make an account with an email, I send a link to verify the email". I actually hate this flow and I don't really want to know a users email. I never need to contact you and there's not a lot associated with your account, which makes this especially silly. I have a ton of email addresses and no real "purpose" in having them. I'd switch to Login with Apple, which is great from a security perspective but not everybody has an Apple ID.&lt;/p&gt;
    &lt;p&gt;I also did a passkey version, which worked fine but the OSS passkey handling was pretty rough still and most people seem to be using a commercial service that handled the "do you have the passkey? Great, if not, fall back to email" flow. I don't really want to do a big commercial login service for a hobby application.&lt;/p&gt;
    &lt;p&gt;Auth is a JWT, which actually was a pain and I regret doing it. I don't know why I keep reaching for JWTs, they're a bad user experience and I should stop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I just have the source code?&lt;/head&gt;
    &lt;p&gt;I'm more than happy to release the source code once I feel like the product is in a somewhat stable shape. I'm still ripping down and rewriting relatively large chunks of it as I find weird behavior I don't like or just decide to do things a different way.&lt;/p&gt;
    &lt;p&gt;In the end it does seem to do whats on the label. We have over 600,000 individual pages indexed.&lt;/p&gt;
    &lt;head rend="h3"&gt;So how is it to use?&lt;/head&gt;
    &lt;p&gt;Honestly I've been pretty pleased. But there are some problems.&lt;/p&gt;
    &lt;p&gt;First I couldn't find a reliable way of switching the keyboard shortcuts to be Mac/Windows specific. I found some options for querying platform but they didn't seem to work, so I ended up just hardcoding them as Alt which is not great.&lt;/p&gt;
    &lt;p&gt;The other issue is that when you are making an extension, you spend a long time working with these manifests.json. The specific part I really wasn't sure about was:&lt;/p&gt;
    &lt;code&gt;"browser_specific_settings": {
    "gecko": {
      "id": "[email protected]",
      "strict_min_version": "80.0",
      "data_collection_permissions": {
        "required": ["authenticationInfo"]
      }
    }
  }&lt;/code&gt;
    &lt;p&gt;I'm not entirely sure if that's all I'm doing? I think so from reading the docs.&lt;/p&gt;
    &lt;p&gt;Anyway I built this mostly for me. I have no idea if anybody else will enjoy it. But if you are bored I encourage you to give it a try. It should be pretty light weight and straight-forward if you crack open the extension and look at it. I'm not loading any analytics into the extension so basically until people complain about it, I don't really know if its going well or not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Future stuff&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I need to sort stuff into categories so that you get more stuff in genres you like. I don't 100% know how to do that, maybe there is a way to scan a website to determine the "types" of content that is on there with machine learning? I'm still looking into it.&lt;/item&gt;
      &lt;item&gt;There's a lot of junk in there. I think if we reach a certain number of downvotes I might put it into a special "queue".&lt;/item&gt;
      &lt;item&gt;I want to ensure new users see the "best stuff" early on but there isn't enough data to determine "best vs worst".&lt;/item&gt;
      &lt;item&gt;I wish there were more independent photography and science websites. Also more crafts. That's not really a "future thing", just me putting a hope out into the universe. Non-technical beta testers get overwhelmed by technical content.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://matduggan.com/making-rss-more-fun/"/><published>2025-12-05T13:00:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46160773</id><title>Most Technical Problems Are People Problems</title><updated>2025-12-05T13:47:26.088480+00:00</updated><content>&lt;doc fingerprint="2ed1d053672c65da"&gt;
  &lt;main&gt;
    &lt;p&gt;I once worked at a company which had an enormous amount of technical debt - millions of lines of code, no unit tests, based on frameworks that were well over a decade out of date. On one specific project, we had a market need to get some Windows-only modules running on Linux, and rather than cross-compiling, another team had simply copied &amp;amp; pasted a few hundred thousand lines of code, swapping Windows-specific components for Linux-specific.&lt;/p&gt;
    &lt;p&gt;For the non-technical reader, this is an enormous problem because now two versions of the code exist. So, all features &amp;amp; bug fixes must be solved in two separate codebases that will grow apart over time. When I heard about this, a young &amp;amp; naive version of me set out to fix the situation....&lt;/p&gt;
    &lt;head rend="h2"&gt;People Problems&lt;/head&gt;
    &lt;p&gt;Tech debt projects are always a hard sell to management, because even if everything goes flawlessly, the code just does roughly what it did before. This project was no exception, and the optics weren't great. I did as many engineers do and "ignored the politics", put my head down, and got it done. But, the project went long, and I lost a lot of clout in the process.&lt;/p&gt;
    &lt;p&gt;I realized I was essentially trying to solve a people problem with a technical solution. Most of the developers at this company were happy doing the same thing today that they did yesterday...and five years ago. As Andrew Harmel-Law points out, code tends to follow the personalities of the people that wrote it. The code was calcified because the developers were also. Personality types who dislike change tend not to design their code with future change in mind.&lt;/p&gt;
    &lt;p&gt;Most technical problems are really people problems. Think about it. Why does technical debt exist? Because requirements weren't properly clarified before work began. Because a salesperson promised an unrealistic deadline to a customer. Because a developer chose an outdated technology because it was comfortable. Because management was too reactive and cancelled a project mid-flight. Because someone's ego wouldn't let them see a better way of doing things.&lt;/p&gt;
    &lt;p&gt;The core issue with the project was that admitting the need for refactoring was also to admit that the way the company was building software was broken and that individual skillsets were sorely out of date. My small team was trying to fix one module of many, while other developers were writing code as they had been for decades. I had one developer openly tell me, "I don't want to learn anything new." I realized that you'll never clean up tech debt faster than others create it. It is like triage in an emergency room, you must stop the bleeding first, then you can fix whatever is broken.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Ideal World&lt;/head&gt;
    &lt;p&gt;The project also disabused me of the engineer's ideal of a world in which engineering problems can be solved in a vacuum - staying out of "politics" and letting the work speak for itself - a world where deadlines don't exist...and let's be honest, neither do customers. This ideal world rarely exists. The vast majority of projects have non-technical stakeholders, and telling them "just trust me; we're working on it" doesn't cut it. I realized that the perception that your team is getting a lot done is just as important as getting a lot done.&lt;/p&gt;
    &lt;p&gt;Non-technical people do not intuitively understand the level of effort required or the need for tech debt cleanup; it must be communicated effectively by engineering - in both initial estimates &amp;amp; project updates. Unless leadership has an engineering background, the value of the technical debt work likely needs to be quantified and shown as business value.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heads Up&lt;/head&gt;
    &lt;p&gt;Perhaps these are the lessons that prep one for more senior positions. In my opinion, anyone above senior engineer level needs to know how to collaborate cross-functionally, regardless of whether they choose a technical or management track. Schools teach Computer Science, not navigating personalities, egos, and personal blindspots.&lt;/p&gt;
    &lt;p&gt;I have worked with some incredible engineers, better than myself - the type that have deep technical knowledge on just about any technology you bring up. When I was younger, I wanted to be that engineer - the "engineer's engineer". But I realize now, that is not my personality. I'm too ADD for that. :)&lt;/p&gt;
    &lt;p&gt;For all of their (considerable) strengths, more often than not, those engineers shy away from the interpersonal. The tragedy is that they are incredibly productive ICs, but may fail with bigger initiatives because they are only one person - a single processor core can only go so fast. Perhaps equally valuable is the "heads up coder" - the person who is deeply technical, but also able to pick their head up &amp;amp; see project risks coming (technical &amp;amp; otherwise) and steer the team around them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.joeschrag.com/2023/11/most-technical-problems-are-really.html"/><published>2025-12-05T13:07:59+00:00</published></entry></feed>