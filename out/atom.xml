<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-14T22:44:20.452216+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46616919</id><title>Show HN: Digital Carrot – Block social media with programmable rules and goals</title><updated>2026-01-14T22:44:31.006104+00:00</updated><content>&lt;doc fingerprint="3c8e6e2db14cae96"&gt;
  &lt;main&gt;
    &lt;p&gt;Digital Carrot is the world’s most flexible distraction blocker. It helps you build healthy habits by blocking apps, websites and video games until you meet your daily goals.&lt;/p&gt;
    &lt;p&gt;Progress towards your goals is tracked automatically. Goals can be created from a premade template or fully programmed with a simple script.&lt;/p&gt;
    &lt;p&gt;Pick the apps and websites that are the most distracting to you. These will all be blocked until you’ve completed your goals.&lt;/p&gt;
    &lt;p&gt;Once you’ve finished your goals for the day your distracting apps will be unlocked and you can enjoy them guilt free.&lt;/p&gt;
    &lt;p&gt;Block social media, games and online video until I…&lt;/p&gt;
    &lt;p&gt;Use your health data to verify that you’ve been active throughout the day.&lt;/p&gt;
    &lt;p&gt;Use your phone’s GPS to verify that you actually went to the gym and spent enough time there.&lt;/p&gt;
    &lt;p&gt;Set a timer and meditate until it is done.&lt;/p&gt;
    &lt;p&gt;Block apps, limit usage to certain hours or set a time limit.&lt;/p&gt;
    &lt;p&gt;Set a timer that only grants access when it is running.&lt;/p&gt;
    &lt;p&gt;Create a 15 minute timed goal that you have to start as soon as you wake up.&lt;/p&gt;
    &lt;p&gt;Create a schedule to block or unblock apps during certain time windows.&lt;/p&gt;
    &lt;p&gt;Give yourself some extra motivation to finish your to-do list or learn a new skill.&lt;/p&gt;
    &lt;p&gt;Track how many To-Do’s you have to complete in Apple Reminders or Todoist.&lt;/p&gt;
    &lt;p&gt;Set goals to spend time on websites that help you learn new things.&lt;/p&gt;
    &lt;p&gt;Set a goal that has to be manually checked off every day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.digitalcarrot.app/"/><published>2026-01-14T15:11:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46617360</id><title>Find a pub that needs you</title><updated>2026-01-14T22:44:30.732006+00:00</updated><content>&lt;doc fingerprint="f02c34496935903"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FIND A PUB&lt;lb/&gt;THAT NEEDS YOU&lt;/head&gt;
    &lt;p&gt;The government's signalled a potential u-turn on pub rates — but nothing's confirmed yet. Pubs still need your support. Find your local. See what they're up against. Buy a pint.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE FUCKED PUB INDEX&lt;/head&gt;
    &lt;p&gt;Our world-class data scientists (one guy with a spreadsheet) have developed the Fucked Pub Index™ — a groundbreaking metric that combines advanced geospatial analysis (Google Maps) with sophisticated fiscal impact modelling (basic maths) to identify the pub near you that most urgently requires your patronage.&lt;/p&gt;
    &lt;p&gt;...&lt;/p&gt;
    &lt;p&gt;Pubs Analysed&lt;/p&gt;
    &lt;p&gt;...&lt;/p&gt;
    &lt;p&gt;Facing Increases&lt;/p&gt;
    &lt;p&gt;...&lt;/p&gt;
    &lt;p&gt;Fucked or Worse&lt;/p&gt;
    &lt;p&gt;2026&lt;/p&gt;
    &lt;p&gt;Revaluation Year&lt;/p&gt;
    &lt;p&gt;Based on VOA rateable value data for ... verified pubs (SCAT 249). Some industry experts estimate the actual number of affected pubs is even higher. The government has signalled support is coming — we'll update when details are announced.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ismypubfucked.com/"/><published>2026-01-14T15:44:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46617668</id><title>Roam 50GB is now Roam 100GB</title><updated>2026-01-14T22:44:30.239061+00:00</updated><content>&lt;doc fingerprint="764cb8ffe0c8aa8e"&gt;
  &lt;main&gt;
    &lt;p&gt;On January 13, 2026, Starlink doubled the amount of high-speed data on Roam 50GB to 100GB, at no additional cost and in most markets. Here is all you need to know about what's changed and what hasn't.&lt;/p&gt;
    &lt;p&gt;Once you’ve used 100GB of your high-speed Roam data, your service automatically continues with unlimited low-speed data for the remainder of your billing period. You’ll still be connected for basic use like calls and texts, but activities such as streaming, downloading, and video calls may be limited.&lt;/p&gt;
    &lt;p&gt;We’ll notify you when you reach 80% and 100% of your monthly high-speed Roam data. To restore high-speed Roam service, you can upgrade to Roam Unlimited. Please note that this upgrade will remain in effect for future billing cycles. You can switch back to Roam 100GB as needed. If you want to switch back before your next biling cycle, you'll need to manually change plans in your account portal.&lt;/p&gt;
    &lt;p&gt;No. Your service will not stop. You’ll continue to have internet access--with unlimited data--at reduced speeds until your next billing cycle begins.&lt;/p&gt;
    &lt;p&gt;Low-speed data supports basic connectivity such as email, calls, and texts. Activities that rely on higher speeds—like streaming video, large downloads, or video calls—will be limited.&lt;/p&gt;
    &lt;p&gt;You can upgrade anytime to Roam Unlimited to restore high-speed service. Please note that upgrading to Roam Unlimited will remain in effect for future billing cycles.&lt;/p&gt;
    &lt;p&gt;With the exception of Ocean Mode, per-GB data purchases are no longer available on Roam plans. Customers now automatically move to unlimited low-speed data after reaching their high-speed Roam 100GB limit, with the option to upgrade to Roam Unlimited for continued high-speed access.&lt;/p&gt;
    &lt;p&gt;Yes, with the same previous conditions as Roam 50GB:&lt;/p&gt;
    &lt;p&gt;In the following markets, Roam 50GB is still available and Roam 100GB is not available:&lt;/p&gt;
    &lt;p&gt;Austria&lt;/p&gt;
    &lt;p&gt;Hungary&lt;/p&gt;
    &lt;p&gt;Croatia&lt;/p&gt;
    &lt;p&gt;Bangladesh&lt;/p&gt;
    &lt;p&gt;Bhutan&lt;/p&gt;
    &lt;p&gt;Botswana&lt;/p&gt;
    &lt;p&gt;Brunei&lt;/p&gt;
    &lt;p&gt;Cape Verde&lt;/p&gt;
    &lt;p&gt;Cook Islands&lt;/p&gt;
    &lt;p&gt;Costa Rica&lt;/p&gt;
    &lt;p&gt;Democratic Republic of the Congo&lt;/p&gt;
    &lt;p&gt;Eswatini&lt;/p&gt;
    &lt;p&gt;Gambia&lt;/p&gt;
    &lt;p&gt;Ghana&lt;/p&gt;
    &lt;p&gt;Kenya&lt;/p&gt;
    &lt;p&gt;Lesotho&lt;/p&gt;
    &lt;p&gt;Liberia&lt;/p&gt;
    &lt;p&gt;Malawi&lt;/p&gt;
    &lt;p&gt;Maldives&lt;/p&gt;
    &lt;p&gt;Mongolia&lt;/p&gt;
    &lt;p&gt;Mozambique&lt;/p&gt;
    &lt;p&gt;Nauru&lt;/p&gt;
    &lt;p&gt;Nigeria&lt;/p&gt;
    &lt;p&gt;Oman&lt;/p&gt;
    &lt;p&gt;Qatar&lt;/p&gt;
    &lt;p&gt;Rwanda&lt;/p&gt;
    &lt;p&gt;Sierra Leone&lt;/p&gt;
    &lt;p&gt;Somalia&lt;/p&gt;
    &lt;p&gt;South Sudan&lt;/p&gt;
    &lt;p&gt;Sri Lanka&lt;/p&gt;
    &lt;p&gt;Togo&lt;/p&gt;
    &lt;p&gt;Tonga&lt;/p&gt;
    &lt;p&gt;United Arab Emirates&lt;/p&gt;
    &lt;p&gt;Vanuatu&lt;/p&gt;
    &lt;p&gt;Zambia&lt;/p&gt;
    &lt;p&gt;Zimbabwe&lt;/p&gt;
    &lt;p&gt;Can't find what you're looking for? Contact Support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://starlink.com/support/article/58c9c8b7-474e-246f-7e3c-06db3221d34d"/><published>2026-01-14T16:03:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46617705</id><title>Show HN: A 10KiB kernel for cloud apps</title><updated>2026-01-14T22:44:29.557530+00:00</updated><content>&lt;doc fingerprint="6dfff95155338da9"&gt;
  &lt;main&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;This has only been tested with Digital Ocean and Proxmox. Support for other hypervisor/cloud providers (AWS, Azure, and Google Cloud) is coming soon.&lt;/p&gt;
    &lt;p&gt;BareMetal Cloud is a minimal version of the BareMetal exokernel specifically geared for running in public/private cloud instances. This minimal version of BareMetal contains only the relevant drivers, is 10,240 bytes in size, and only uses 4 MiB of memory. All other memory is allocated to the payload.&lt;/p&gt;
    &lt;p&gt;An instance of BareMetal is running in Digital Ocean at http://baremetal.returninfinity.com and will respond to HTTP and ICMP.&lt;/p&gt;
    &lt;p&gt;The script in this repo depends on a Debian-based Linux system. macOS is also supported to build and test if you are using Homebrew.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NASM - Assembler to build the loader and kernel.&lt;/item&gt;
      &lt;item&gt;QEMU - Computer emulator if you plan on running a virtual machine for quick testing.&lt;/item&gt;
      &lt;item&gt;Git - Version control software for pulling the source code from GitHub.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Linux this can be completed with the following command:&lt;/p&gt;
    &lt;code&gt;sudo apt install nasm qemu-system-x86 git
&lt;/code&gt;
    &lt;p&gt;In macOS via Homebrew this can be completed with the following command:&lt;/p&gt;
    &lt;code&gt;brew install nasm qemu git
&lt;/code&gt;
    &lt;p&gt;BareMetal Cloud consists of two different projects:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/ReturnInfinity/BareMetal-Cloud.git
cd BareMetal-Cloud
./baremetal.sh setup
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;baremetal.sh setup&lt;/code&gt; automatically runs the build and install functions. Once the setup is complete you can execute &lt;code&gt;baremetal.sh run&lt;/code&gt; to verify that everything installed correctly.&lt;/p&gt;
    &lt;code&gt;./baremetal.sh build
&lt;/code&gt;
    &lt;p&gt;This command builds the boot sector, loader (Pure64), and kernel&lt;/p&gt;
    &lt;code&gt;./baremetal.sh install
&lt;/code&gt;
    &lt;p&gt;This command installs the software to the disk image.&lt;/p&gt;
    &lt;code&gt;./baremetal.sh run
&lt;/code&gt;
    &lt;p&gt;This command will run BareMetal-Cloud in a QEMU VM. Output to the serial port will be displayed to the console.&lt;/p&gt;
    &lt;p&gt;Create a VMDK disk image&lt;/p&gt;
    &lt;code&gt;./baremetal.sh vmdk
&lt;/code&gt;
    &lt;p&gt;The resulting &lt;code&gt;BareMetal_Cloud.vmdk&lt;/code&gt; in &lt;code&gt;sys/&lt;/code&gt; will be required.&lt;/p&gt;
    &lt;p&gt;In Digital Ocean click on &lt;code&gt;Backups &amp;amp; Snapshots&lt;/code&gt; and then &lt;code&gt;Custom Images&lt;/code&gt;. Click on the &lt;code&gt;Upload Image&lt;/code&gt; button and select the .vmdk file on your filesystem. Once the file is uploaded you can start a droplet of it by clicking on the &lt;code&gt;More&lt;/code&gt; dropdown and selecting &lt;code&gt;Start a droplet&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;On the &lt;code&gt;Create Droplets&lt;/code&gt; page you can select the Droplet Type and CPU Options. Give the droplet a name and click on &lt;code&gt;Create Droplet&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In Proxmox click on the "Create VM" button. Configure the following settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;General - Give the VM a name&lt;/item&gt;
      &lt;item&gt;OS - "Do not use any media"&lt;/item&gt;
      &lt;item&gt;System - Machine: q35&lt;/item&gt;
      &lt;item&gt;Disks - Remove the existing disk&lt;/item&gt;
      &lt;item&gt;CPU - Provision as needed&lt;/item&gt;
      &lt;item&gt;Memory - Provision as needed&lt;/item&gt;
      &lt;item&gt;Network - Model: VirtIO&lt;/item&gt;
      &lt;item&gt;Confirm - Click "Finish"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use a utility like &lt;code&gt;scp&lt;/code&gt; to copy the .vmdk file to the filesystem of the Proxmox server.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;qm importdisk &amp;lt;VMID&amp;gt; &amp;lt;vmdk_filename&amp;gt; &amp;lt;storage_location&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Example - &lt;code&gt;qm importdisk 101 /root/BareMetal_Cloud.vmdk local-lvm&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;In the Proxmox web interface select the new VM. In the Hardware section, find the new unused disk, and attach it to the VM.&lt;/p&gt;
    &lt;p&gt;Verify the "Boot Order" in the VM "Options".&lt;/p&gt;
    &lt;p&gt;Click "Start" on the VM.&lt;/p&gt;
    &lt;p&gt;//EOF&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ReturnInfinity/BareMetal-Cloud"/><published>2026-01-14T16:04:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46617744</id><title>How much of my observability data is waste?</title><updated>2026-01-14T22:44:29.238119+00:00</updated><content>&lt;doc fingerprint="1760d138bd3114df"&gt;
  &lt;main&gt;
    &lt;p&gt;This year marks a decade for me in observability.&lt;/p&gt;
    &lt;p&gt;I left my engineering job in 2016 to start Timber.io, a hosted logging platform, because I thought logs could be simple and great. Timber became Vector. Vector got mass adoption. It got acquired, and I stayed for three years.&lt;/p&gt;
    &lt;p&gt;And somewhere along the way, the optimism curdled.&lt;/p&gt;
    &lt;p&gt;I'm not a cynical person. I believed observability could make engineers' lives better. But after a decade, after hundreds of conversations with teams bleeding money across every major vendor, after hearing firsthand how their vendors strong-armed them instead of helping; I've seen enough. The whole industry has lost the plot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does any of this sound familiar?&lt;/head&gt;
    &lt;p&gt;You run observability at your company. But really, you're the cost police. You wake up to a log line in a hot path, a metric tag that exploded cardinality. You chase down the engineer. They didn't do anything wrong, they're just disconnected from what any of this costs. The renewal is always in the back of your mind because mismanaging it reflects poorly on you. Sometimes you catch these mistakes. Sometimes you don't. When you don't, you crawl to your rep asking for forgiveness. Maybe they help the first time, even the second. By the fourth or fifth, they stop. "It's your data." But even with the mistakes, if you're diligent, checking dashboards, staying on top of things, you manage to stay under your commit and avoid an early renewal. But the renewal still gives you a black eye: 40% higher than last year. Your budget didn't grow that much. So you consider switching vendors, but asking your engineers to frantically migrate dashboards, alerts, and change workflows is a distraction that also reflects poorly on you. You're in a lose-lose situation. So you go back to your vendor and ask them to help. You championed them internally; brought them six, seven figure business. Surely they'd return the favor. A slightly bigger discount, help you cut costs by showing you what data is safe to drop. But they don't budge. They could help; they don't.&lt;/p&gt;
    &lt;p&gt;Case Taintor, Director of Engineering at Klarna, put it all too well:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The most frustrating part of watching your money burn is knowing your supplier could help if they only cared about your long term success.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So why has this gone on for over a decade? Something is deeply wrong if after ten years these same problems not only exist, but have gotten worse.&lt;/p&gt;
    &lt;p&gt;But what's wrong, exactly? Should your vendor help you? It is your data. They didn't create it. You sent it to them under their pricing model. For years I accepted that framing too. Maybe this is just how it works.&lt;/p&gt;
    &lt;p&gt;Then I bumped into a question that changed my thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;How much of my observability data is waste?&lt;/head&gt;
    &lt;p&gt;You've asked it. Your vendor has asked it. You know the answer isn't zero. But what is it? 10%? 20%? 40%? At what point does "that's just how it works" stop being an acceptable answer?&lt;/p&gt;
    &lt;p&gt;You see, anyone who's been in this space knows that cost is far and away the biggest problem. You can take all of the other problems, bundle them together, multiply them by 100, and they still would not surpass cost. It shows up everywhere. All of the "innovation" in observability can be traced back to cost in some way. Pipelines? Cost. Fancy new storages? Cost. OpenTelemetry? Yes, cost.&lt;/p&gt;
    &lt;p&gt;So in that context, this seems like a pretty important question. Maybe the most important question in observability. Which means it must be unanswerable, right? Because if someone could answer it and let you keep paying for garbage anyway, that would be unconscionable.&lt;/p&gt;
    &lt;p&gt;Put it to the test. Ask your vendor what percentage of your data is waste. They'll play ignorant. "It's your data." They don't understand it well enough to tell you what's worth keeping. But they understand it well enough to sell you an AI SRE that can "root cause in minutes."&lt;/p&gt;
    &lt;p&gt;It's this willful ignorance that gets me. Everyone knows what's right but plays the quarterly earnings game instead. Except it's not a game for the people on the other side. I got a front row seat with Vector users. Vector wasn't deployed for fun; it was often deployed in crisis, usually around renewal time when the cost of this game came due. I watched people lose their jobs for "mismanaging" the observability budget. I saw the stress on their faces, the lost sleep.&lt;/p&gt;
    &lt;p&gt;So when I first bumped into this question while helping a Vector user, and wanted to answer it but couldn't, that's when my optimism curdled.&lt;/p&gt;
    &lt;head rend="h2"&gt;So I answered it&lt;/head&gt;
    &lt;p&gt;After I left Vector, the question stayed with me. I took a year off, but Vector users still found me with questions. One in particular jumped out because it was impossible not to: emails, LinkedIn messages, people in my network pinging me on their behalf. I wasn't annoyed. I knew exactly what was going on. So I agreed to help. Except this time, no roadmaps, no one telling me what to do. In exchange, they'd give me access to their data so I could try to answer the question, which I suspected was their actual problem anyway.&lt;/p&gt;
    &lt;p&gt;So I signed all the docs, got access to their Vector environment, and took a look at their Vector config. It was the mother of all configs (sorry guys, no offense). Dozens of components connected into a complex DAG. Every cost reduction trick in the book: sampling, aggregating, storage tiering, archiving, and a massive list of regexes to match and drop waste. But I wasn't appalled, I respected it. They weren't being careless, they were doing everything they possibly could.&lt;/p&gt;
    &lt;p&gt;One trick in particular intrigued me: the regex list. It was the bottleneck, but it was also something else: an expression of understanding. Every pattern represented an engineer who understood their service well enough to say "this is waste." My first instinct was to optimize it. I stumbled on Hyperscan. Turns out you can compile tens of thousands of patterns and still match at line rate. That flipped my thinking: what if I took this to the extreme and automated that understanding to produce thousands of patterns?&lt;/p&gt;
    &lt;p&gt;So I built a system to do exactly that. It compressed billions of logs into thousands of semantic events, each one evaluated with the context it needed: the service, the failure scenarios, the patterns, how it all fits together. (The deep details are outside the scope of this post, but if you're curious, here's how it works today.)&lt;/p&gt;
    &lt;p&gt;I ran it against the first service: ~40% waste. Another: ~60%. Another: ~30%. On average, ~40% waste.&lt;/p&gt;
    &lt;p&gt;I knew the number wasn't zero, but I wasn't expecting 40%. So I pressure tested it. Went through hundreds of lines manually. Checked it against their existing patterns. It checked out. With that confidence, I brought it to them.&lt;/p&gt;
    &lt;p&gt;They laughed. "We can't just drop half of our logs." Fair. But that's not what I was asking. I showed them: this wasn't anything new. It was the same analysis they were already doing, just at scale, more complete, more accurate. Most of their hand-written patterns were already represented in my set, often simpler and faster. They could tweak the analysis, roll it out slowly, push it to teams to take action in their own code.&lt;/p&gt;
    &lt;p&gt;And that's what happened. The knowledge stopped the bleeding. Over time, services cleaned up their logging. Pipelines got simpler. Bills went down. Not because anyone dropped data recklessly, but because they finally knew what was worth keeping.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why observability feels broken&lt;/head&gt;
    &lt;p&gt;The answer to this question isn't just a number. It's the answer to why observability feels broken despite it being more expensive than ever. Think about it.&lt;/p&gt;
    &lt;p&gt;On the surface: you're paying twice what you should. Cut the waste, cut the bill. Simple.&lt;/p&gt;
    &lt;p&gt;Go deeper: the cost policing, the weekly dashboard checks, the monthly exercises, the begging your rep for forgiveness when someone's log blows up the bill, the pipelines. All of that exists because you're managing garbage. Half the complexity you've built is dedicated to noise.&lt;/p&gt;
    &lt;p&gt;Go deeper still: your engineers complain that observability doesn't help them debug faster despite costing millions. Of course it doesn't. They're drowning in noise and calling it data. The alerts fire on garbage. The dashboards are cluttered with garbage. The AI can't find the signal because there's too much garbage in the way.&lt;/p&gt;
    &lt;p&gt;And underneath all of it: this number shouldn't exist if your vendor was aligned with you.&lt;/p&gt;
    &lt;p&gt;Take a look around the market. $65M bills. $170M bills. Entire roles for cost control. "Observe without limits." "Stop sampling." "More data, more insight." Dozens of products. It's all backwards. The goal isn't more data, more products, or more complexity.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The goal is understanding with less.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And how do you prove understanding? The question. Either you understand the data well enough to answer it or you don't.&lt;/p&gt;
    &lt;p&gt;There's a future where you're not the cost cop. Where observability just works. Where your vendor's success depends on yours.&lt;/p&gt;
    &lt;p&gt;That's the future we're building at Tero.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://usetero.com/blog/the-question-your-observability-vendor-wont-answer"/><published>2026-01-14T16:07:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46618027</id><title>GitHub should charge everyone $1 more per month to fund open source</title><updated>2026-01-14T22:44:29.050357+00:00</updated><content>&lt;doc fingerprint="af69305bfd93efbd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GitHub should charge everyone $1 more per month&lt;/head&gt;
    &lt;p&gt;Listen to me.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;there should be a thing that reads your package.json and charges you $5/month per dependency - you don’t /have/ to! you could set the price to $1 per employee! - and then holds the funds and sends it to the people who made the code you use to do business how is not doing this more sustainable&lt;/p&gt;— Greg Technology ❪⎷❫ (@greg.technology) January 13, 2026 at 9:13 PM&lt;/quote&gt;
    &lt;p&gt;It is crazy, absolutely crazy to depend on open source to be free (as beer). It is not okay - it is not okay to consider that this labor fell from the sky and is a gift, and that the people/person behind are just doing it for their own enjoyments.&lt;/p&gt;
    &lt;p&gt;It is impossible to imagine that what we’re doing today is the only way. Begging/busking for donations, hoping to get noticed. Hoping for a lifeline.&lt;/p&gt;
    &lt;p&gt;Hence, a solution. Or an idea, really. Incredibly half-baked. Poke all the holes you want. It’s very unwrought and muy unripe.&lt;/p&gt;
    &lt;p&gt;GitHub should charge every org $1 more per user per month and direct it into an Open Source fund, held in escrow.&lt;/p&gt;
    &lt;p&gt;Those funds would then be distributed by usage - every mention in a package.json or requirements.txt gets you a piece of the pie.&lt;/p&gt;
    &lt;p&gt;You know how the money you pay to Spotify is very very very approximately (and not really fairly) distributed among artists that you listened to? Yes, Spotify is a very flawed model and artists are not doing well. But it is a model??&lt;/p&gt;
    &lt;p&gt;That’s it. That’s the idea. Call it the “Open Source Fund” thing, make it opt-out. Give every org a magical badge - or the ability to set their profile’s background css.&lt;/p&gt;
    &lt;p&gt;Or don’t! Let’s not do anything! People’s code and efforts - fueling incredibly critical bits of infrastructure all around the world - should just be up for grabs. Haha! Suckers!&lt;/p&gt;
    &lt;p&gt;Alright, I don’t know how you fund Linux (does Linux appear in a requirements file). Hmm. Maybe &lt;code&gt;FROM&lt;/code&gt; commands from Dockerfiles are also read &amp;amp; applied. Maybe we at least start somewhere?&lt;/p&gt;
    &lt;p&gt;Anyway, you all smarter than me people can figure it out. I just cannot accept that what we have is “GOOD”. xx&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.greg.technology/2025/11/27/github-should-charge-1-dollar-more-per-month.html"/><published>2026-01-14T16:25:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46618087</id><title>The unbearable frustration of figuring out APIs</title><updated>2026-01-14T22:44:28.400883+00:00</updated><content>&lt;doc fingerprint="56021e198a12bc50"&gt;
  &lt;main&gt;
    &lt;p&gt;In the ongoing effort for activities that fill the void made by unemployment, I have recently started to learn Chinese. Got into a Chinese language institute and everything. And because not every app supports right-clicking some text and selecting the "Translate" menu option, I found myself launching TextEdit just to do that. So, I figured, maybe I can do a small command line tool where I'd write something like:&lt;/p&gt;
    &lt;code&gt;translate 你好&lt;/code&gt;
    &lt;p&gt;And the terminal would happily tell me it means "Hello". Should be easy. A hungry ghost trapped in a jar can probably figure it out in one shot.&lt;/p&gt;
    &lt;p&gt;First I figured I'd look for some translation API. Almost everything I looked at seemed to want an API token and there is a rate limit (not that I'd hit it), and maybe a credit card number. Then I remembered that macOS has that Translate right click action, surely I can just hook into that.&lt;/p&gt;
    &lt;p&gt;Note that this article, unlike what I do usually, is written after the fact.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Steps&lt;/head&gt;
    &lt;head rend="h3"&gt;Zig&lt;/head&gt;
    &lt;p&gt;I have been using Zig for different projects this year. So I got the Ghost to write the thing for me, and it even gave me the correct flags to pass to the compiler! But then when I tried it, I realized it used the Dictionary service instead of the Translation service. When I asked it for that instead, it was like yeah you cannot call Swift &lt;code&gt;async&lt;/code&gt; functions from Zig, you need a Swift shim.&lt;/p&gt;
    &lt;p&gt;So let's do the thing in Swift instead. I have been meaning to learn Swift anyway.&lt;/p&gt;
    &lt;head rend="h3"&gt;MyCLI&lt;/head&gt;
    &lt;p&gt;Thankfully, Swift was already set up on my machine with an LSP and a formatter and the works. I had even followed the basic tutorial and the file is on my machine already, A small edit and I can be on my way.&lt;/p&gt;
    &lt;p&gt;For reference, this is the code at the end of the tutorial:&lt;/p&gt;
    &lt;code&gt;import ArgumentParser
import Figlet

@main
struct FigletTool: ParsableCommand {
	@Option: "Specify the input"
	public var input: String

	public func run() throws {
		Figlet.say(self.input)
	}
}&lt;/code&gt;
    &lt;p&gt;After so long in Rust and Zig lands, the most jarring thing about Swift so far is that &lt;code&gt;import&lt;/code&gt; creates glob imports. This is not quite apparent here, but it is there. Also, this is the accompanying &lt;code&gt;Package.swift&lt;/code&gt;, comments and all.&lt;/p&gt;
    &lt;code&gt;// swift-tools-version: 5.8
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: "MyCLI",
    dependencies: [
      .package(url: "https://github.com/apple/example-package-figlet", branch: "main"),
      .package(url: "https://github.com/apple/swift-argument-parser", from: "1.0.0"),
    ],
    targets: [
        // Targets are the basic building blocks of a package, defining a module or a test suite.
        // Targets can depend on other targets in this package and products from dependencies.
        .executableTarget(
            name: "MyCLI",
            dependencies: [
                .product(name: "Figlet", package: "example-package-figlet"),
                .product(name: "ArgumentParser", package: "swift-argument-parser"),
            ],
            path: "Sources"),
    ]
)&lt;/code&gt;
    &lt;p&gt;This is run with &lt;code&gt;swift run&lt;/code&gt;, but to pass arguments, one needs to do the following:&lt;/p&gt;
    &lt;code&gt;swift run MyCLI --input Hello&lt;/code&gt;
    &lt;p&gt;I spent like 15 minutes trying different variations of this&lt;/p&gt;
    &lt;code&gt;# this does NOT work
swift run -- --input Hello&lt;/code&gt;
    &lt;p&gt;Anyway, I deleted the &lt;code&gt;Figlet&lt;/code&gt; related lines and went from there.&lt;/p&gt;
    &lt;head rend="h2"&gt;False Starts&lt;/head&gt;
    &lt;p&gt;I asked the Ghost in the Jar for how the API to use it looks like, but every version it gave me was hallucinated in some fashion. So I wrote &lt;code&gt;import Translation&lt;/code&gt; at the top of the file, and figured maybe I can figure it out as I go along. Here is the Apple API documentation. A quick look tells you all the functions in here return a &lt;code&gt;some View&lt;/code&gt;, and the only function seemingly unrelated to SwiftUI is the &lt;code&gt;init&lt;/code&gt; function in &lt;code&gt;TranslationSession&lt;/code&gt;, which, annoyingly, needs a known source language.&lt;/p&gt;
    &lt;p&gt;So I do this&lt;/p&gt;
    &lt;code&gt;import ArgumentParser
import Translation

@main
struct Translate: ParsableCommand {
	@Argument: "Specify the input"
	public var input: String

	public func run() async throws {
		print("&amp;gt;\t\(input)")

		let source = Locale.Language(identifier: "zh_CN")
		let target = Locale.Language(identifier: "en_US")

		let session = TranslationSession(installedSource: source, target: target)

		let response = try await session.translate(input)
		let result = response.targetText

		print("&amp;gt;\t\(result)")

	}
}&lt;/code&gt;
    &lt;p&gt;You can see the number of small changes to the original file here. I changed the struct's name (because it shows up in the &lt;code&gt;--help&lt;/code&gt; message.) I changed &lt;code&gt;Option&lt;/code&gt; to &lt;code&gt;Argument&lt;/code&gt; so it does not need a flag. I even turned &lt;code&gt;run&lt;/code&gt; into an &lt;code&gt;async&lt;/code&gt; function so I can &lt;code&gt;await&lt;/code&gt; the &lt;code&gt;session.translate&lt;/code&gt; function call.&lt;/p&gt;
    &lt;p&gt;However, I start to hit a couple of snags. First is that &lt;code&gt;TranslationSession.init&lt;/code&gt; is restricted to macOS 26. Not wanting to do a bunch of conditional compilation, (after all this tool is for my own use), I will just add the &lt;code&gt;platforms&lt;/code&gt; field in &lt;code&gt;Package.swift&lt;/code&gt;。 Oh it needs to be before &lt;code&gt;dependencies&lt;/code&gt;? Fine, whatever.&lt;/p&gt;
    &lt;code&gt;// swift-tools-version: 5.8
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
	name: "MyCLI",
	platforms: [
		.macOS( /* one problem, I cannot find v26 in the dropdown */)
	],
	dependencies: [
		.package(url: "https://github.com/apple/swift-argument-parser", from: "1.0.0")
	],
	targets: [
		// Targets are the basic building blocks of a package, defining a module or a test suite.
		// Targets can depend on other targets in this package and products from dependencies.
		.executableTarget(
			name: "MyCLI",
			dependencies: [
				.product(name: "ArgumentParser", package: "swift-argument-parser")
			],
			path: "Sources",
		)
	]
)&lt;/code&gt;
    &lt;p&gt;I cannot find &lt;code&gt;.v26&lt;/code&gt; in the LSP's drop down. There is no &lt;code&gt;.Tahoe&lt;/code&gt; either. Even if I type it manually, I get an error. I am on Tahoe. I have the latest version of Swift (I checked). why can't I select it from here?&lt;/p&gt;
    &lt;p&gt;You probably already know this, but apparently the first line in the file, that comment, is actually significant. I edited that to say &lt;code&gt;swift-tools-version: 6.2&lt;/code&gt;, and now I can write &lt;code&gt;.v26&lt;/code&gt; in peace. I hate syntactic comments.&lt;/p&gt;
    &lt;p&gt;Ok, the LSP is happy. The compiler seems happy. Let's get going.&lt;/p&gt;
    &lt;head rend="h2"&gt;Async Woes&lt;/head&gt;
    &lt;p&gt;Running this with &lt;code&gt;swift run -q MyCLI 你好&lt;/code&gt; gives the following message:&lt;/p&gt;
    &lt;code&gt;USAGE: translate &amp;lt;input&amp;gt;

ARGUMENTS:
  &amp;lt;input&amp;gt;                 Specify the input

OPTIONS:
  -h, --help              Show help information.&lt;/code&gt;
    &lt;p&gt;Eh, I clearly passed in something. What gives? I tried changing &lt;code&gt;Argument&lt;/code&gt; back to &lt;code&gt;Option&lt;/code&gt;, same result. Hm. I realized that the problem maybe is that &lt;code&gt;run&lt;/code&gt; is not supposed to be &lt;code&gt;async&lt;/code&gt;. Fine, let's remove &lt;code&gt;async&lt;/code&gt;, but then how do I call &lt;code&gt;session.translate&lt;/code&gt; ?&lt;/p&gt;
    &lt;p&gt;Looking things up, and communing with ghosts, apparently I can wrap thing in &lt;code&gt;Task&lt;/code&gt;. Sure.&lt;/p&gt;
    &lt;code&gt;Task {
	let response = try await session.translate(input)
	let result = response.targetText

	print("&amp;gt;\t\(result)")
}&lt;/code&gt;
    &lt;p&gt;And this runs! I rerun the command I do &lt;code&gt;swift run -q MyCLI 你好&lt;/code&gt; and I get ... nothing. Just the print message that prints &lt;code&gt;input&lt;/code&gt; at the start. I think hah .. I have fallen into the classic trap, I am spawning a Task but I am not waiting for it. How do I wait for it?&lt;/p&gt;
    &lt;p&gt;The ghosts suggested a &lt;code&gt;DispatchSemaphore&lt;/code&gt;. This is the full code:&lt;/p&gt;
    &lt;code&gt;import ArgumentParser
import Translation

@main
struct Translate: ParsableCommand {
	@Argument: "Specify the input"
	public var input: String

	public func run() throws {
		print("&amp;gt;\t\(input)")

		let source = Locale.Language(identifier: "zh_CN")
		let target = Locale.Language(identifier: "en_US")

		let session = TranslationSession(installedSource: source, target: target)
		let semaphore = DispatchSemaphore(value: 0)

		Task {
			let response = try await session.translate(input)
			let result = response.targetText

			print("&amp;gt;\t\(result)")
			semaphore.signal()
		}
		semaphore.wait()

	}
}&lt;/code&gt;
    &lt;p&gt;Ok .. for some reason, when I tried this right now, it actually works. But when I tried it before, probably with a different arrangement of stuff, it did not. It just froze and gave no feedback. The power of hindsight, I guess.&lt;/p&gt;
    &lt;p&gt;Nonetheless, since it did not work, I figured I was doing something wrong. So I searched for "how to call an async function from a sync function in Swift", and came across What calls the first async function? from Hacking With Swift, and the tl;dr of that article was "not you, just make your main &lt;code&gt;async&lt;/code&gt;". Very helpful, Paul. Apparently I needed to make &lt;code&gt;run&lt;/code&gt; async. I looked into the docs of &lt;code&gt;swift-argument-parser&lt;/code&gt;, I figured surely they have some guidance on how to deal with &lt;code&gt;async&lt;/code&gt; functions. Apparently the guidance was just to replace the &lt;code&gt;ParsableCommand&lt;/code&gt; protocol with &lt;code&gt;AsyncParsableCommand&lt;/code&gt;. Ok, now I felt like an idiot. This is the full code:&lt;/p&gt;
    &lt;code&gt;import ArgumentParser
import Translation

@main
struct Translate: AsyncParsableCommand {
	@Argument: "Specify the input"
	public var input: String

	public func run() async throws {
		print("&amp;gt;\t\(input)")

		let source = Locale.Language(identifier: "zh_CN")
		let target = Locale.Language(identifier: "en_US")

		let session = TranslationSession(installedSource: source, target: target)

		let response = try await session.translate(input)
		let result = response.targetText

		print("&amp;gt;\t\(result)")

	}
}&lt;/code&gt;
    &lt;p&gt;Works like magic now. Except, at the time, it did not. It gave me the following message:&lt;/p&gt;
    &lt;code&gt;&amp;gt;	你好
Error: Unable to Translate&lt;/code&gt;
    &lt;p&gt;Also it leaves the small detail of autodetecting the language, which is here hardcoded as &lt;code&gt;zh_CN&lt;/code&gt;.1&lt;/p&gt;
    &lt;head rend="h2"&gt;Language Recognition&lt;/head&gt;
    &lt;p&gt;The error had zero feedback. Absolutely unhelpful. No idea what's going on. I added a &lt;code&gt;try await session.prepareTranslation()&lt;/code&gt; line I saw in some online examples, but .. nothing. I looked a bit more around for tutorials, I could not find anything. All tutorials just regurgitate the same SwiftUI code examples, but I am not using SwiftUI. Ekh.&lt;/p&gt;
    &lt;p&gt;Eventually, I came across the original WWDC video introducing the Translation API, and decided to watch it. And it was actually very helpful, despite it being SwiftUI focused.&lt;/p&gt;
    &lt;p&gt;It does not touch on how to use &lt;code&gt;TranslationSession.init&lt;/code&gt;, instead it mentions that a &lt;code&gt;session&lt;/code&gt; is given to you to run in a SwiftUI closure. It gives the rationale behind the API, but it also introduces the bit I needed for autodetection, and that is &lt;code&gt;NLLanguageRecognizer&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;NaturalLanguage&lt;/code&gt; is an Apple framework for dealing with natural language (How did you guess?). I pretty much copied the code from the video into its own function (after &lt;code&gt;import NaturalLanguage&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt;func identify_lang(_ sample: String) -&amp;gt; Locale.Language? {
	let recognizer = NLLanguageRecognizer()
	recognizer.processString(sample)
	guard let language = recognizer.dominantLanguage else { return nil }

	return Locale.Language(identifier: language.rawValue)
}&lt;/code&gt;
    &lt;p&gt;And this is my new &lt;code&gt;func run()&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;public func run() async throws {
	print("&amp;gt;\t\(input)")

	// unwrapping here but I handle it more gracefully in the code
	let source = identify_lang(input)!
	let target = Locale.Language(identifier: "en_US")

	let session = TranslationSession(installedSource: source, target: target)
	try await session.prepareTranslation()

	let response = try await session.translate(input)
	let result = response.targetText

	print("&amp;gt;\t\(result)")

}&lt;/code&gt;
    &lt;p&gt;The other thing the video pointed out, is that the API user should check language availability. I saw that earlier, but did not particularly care. I know Chinese and English are available. I have been using them! Nonetheless, I copied that code as well just to be thorough, to figure out what the API does.&lt;/p&gt;
    &lt;code&gt;let availability = LanguageAvailability()
let is_it = await availability.status(from: source, to: target)
switch is_it {
case .unsupported:
	print("&amp;gt; language pairing from \(source.languageCode) to \(target.languageCode) unsupprted")
	return
case .supported:
	print("&amp;gt; language pairing from \(source.languageCode!) to \(target.languageCode!) not installed")
	return
case .installed:
	break
}&lt;/code&gt;
    &lt;p&gt;This gave me a compile error, as it happens, as apparently the enum is marked the Swift equivelant of &lt;code&gt;#[non_exhaustive]&lt;/code&gt;. Going over StackOverflow and the Swift book, told me the answer is to add a &lt;code&gt;@unknown default:&lt;/code&gt; case. I could not quickly figure out how to merge it with another case so I did not bother.&lt;/p&gt;
    &lt;p&gt;To my surprise, the pairing returned &lt;code&gt;.supported&lt;/code&gt;, and not &lt;code&gt;.installed&lt;/code&gt;. In a SwiftUI app, the &lt;code&gt;try await session.prepareTranslation()&lt;/code&gt; call creates a pop up to ask the user to download the model.&lt;/p&gt;
    &lt;p&gt;But it is already installed! I am already using it via the right click menu! Nonetheless, that must be the reason for the &lt;code&gt;Unable to Translate&lt;/code&gt; error.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing Models&lt;/head&gt;
    &lt;p&gt;Communing again with Ghosts, they told me to install the models through System Settings, General, Language, etc. I went where they told me, to this dialogue, where I can install the models locally.&lt;/p&gt;
    &lt;p&gt;You can see that Mandarin is installed now, but the only language installed at the time was English(US). No matter, I installed Mandarin SImplified and Arabic, and added a note to the &lt;code&gt;switch&lt;/code&gt; above showing where to install the items. And then I tried again: &lt;code&gt;swift run -q MyCLI 你好&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And .. same error. I guessed that it identified the language as Mandarin Traditional for some reason, so I installed that as well, and now it works, finally!!&lt;/p&gt;
    &lt;code&gt;% swift/MyCLI ❭ swift run -q MyCLI 你好
&amp;gt;	你好
&amp;gt;	Hello&lt;/code&gt;
    &lt;p&gt;Why was that so hard? Why are the models here separate from the ones in the right click menu? Too many questions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Code&lt;/head&gt;
    &lt;p&gt;This is the full final code, with some proper error names thrown in for good measure.&lt;/p&gt;
    &lt;code&gt;import ArgumentParser
import NaturalLanguage
import Translation

@main
struct Translate: AsyncParsableCommand {
	@Argument: "Specify the input"
	public var input: String

	public func run() async throws {
		print("&amp;gt;\t\(input)")

		let target = Locale.Language(identifier: "en_US")
		guard let source = identify_lang(input) else {
			print("&amp;gt; could not identify language")
			throw CliError.Recognitionfailed
		}

		let availability = LanguageAvailability()
		let is_it = await availability.status(from: source, to: target)
		switch is_it {
		case .unsupported:
			print("&amp;gt; language pairing from \(source.languageCode) to \(target.languageCode) unsupprted")
			throw CliError.PairingUnsupported
		case .supported:
			print("&amp;gt; language pairing from \(source.languageCode!) to \(target.languageCode!) not installed")
			print("&amp;gt; Go to System Settings &amp;gt; General &amp;gt; Language &amp;amp; Region &amp;gt; Translation Languages and download the models.")
			throw CliError.PairingNotInstalled
		case .installed:
			break
		@unknown default:
			print("Unknown status.")
		}

		let session = TranslationSession(installedSource: source, target: target)
		try await session.prepareTranslation()

		let response = try await session.translate(input)
		let result = response.targetText

		print("&amp;gt;\t\(result)")

	}
}

func identify_lang(_ sample: String) -&amp;gt; Locale.Language? {
	let recognizer = NLLanguageRecognizer()
	recognizer.processString(sample)
	guard let language = recognizer.dominantLanguage else { return nil }

	return Locale.Language(identifier: language.rawValue)
}

enum CliError: Error {
	case Recognitionfailed
	case PairingUnsupported
	case PairingNotInstalled
}&lt;/code&gt;
    &lt;p&gt;So I compiled it for release, put it in &lt;code&gt;/usr/local/bin&lt;/code&gt;, and called it a day. You can see an example of a failing call if you do &lt;code&gt;swift run -q MyCLI Bonjour&lt;/code&gt;, where it prompts to install the dictionary.&lt;/p&gt;
    &lt;head rend="h2"&gt;In Conclusion&lt;/head&gt;
    &lt;p&gt;To rub some salt on the wound, during the time I was trying to puzzle this out, I realized that Spotlight (with the Cmd+Space key) can already do this exactly how I envisioned it, with around the same number of keystrokes, and it does not need this dance with installing models and whatever.2&lt;/p&gt;
    &lt;p&gt;Also, Swift is frustrating. API of Swift libraries is frustrating.&lt;/p&gt;
    &lt;p&gt;Until later.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;By the way, there is no documentation anywhere I could find on what the strings accepted here are. The only comment exists is "A Unicode language identifier, like&lt;/p&gt;&lt;code&gt;en-US&lt;/code&gt;,&lt;code&gt;es-419&lt;/code&gt;, or&lt;code&gt;zh-Hant-TW&lt;/code&gt;." No list or anything. Am I supposed to guess? ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Also this supports Chinese input, while Ghostty's quick terminal does not, for some reason. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.ar-ms.me/thoughts/translation-cli/"/><published>2026-01-14T16:28:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46618714</id><title>Ask HN: Share your personal website</title><updated>2026-01-14T22:44:26.671018+00:00</updated><content>&lt;doc fingerprint="a785b41f6be747ae"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hello HN! I am putting together a community-maintained directory of personal websites at &amp;lt;https://hnpwd.github.io/&amp;gt;. More details about the project can be found in the README at &amp;lt;https://github.com/hnpwd/hnpwd.github.io#readme&amp;gt;.&lt;/p&gt;
      &lt;p&gt;As you can see, the directory currently has only a handful of entries. I need your help to grow it. If you have a personal website, I would be glad if you shared it here. If your website is hosted on a web space where you have full control over its design and content, and if it has been well received in past HN discussions, I might add it to the directory. Just drop a link in the comments. Please let me know if you do not want your website to be included in the directory.&lt;/p&gt;
      &lt;p&gt;Also, I intend this to be a community maintained resource, so if you would like to join the GitHub project as a maintainer, please let me know either here or via the IRC link in the README.&lt;/p&gt;
      &lt;p&gt;By the way, see also 'Ask HN: Could you share your personal blog here?' - https://news.ycombinator.com/item?id=36575081 - July 2023 - (1014 points, 1940 comments). In this post, the scope is not restricted to blogs though. Any personal website is welcome, whether it is a blog, digital garden, personal wiki or something else entirely.&lt;/p&gt;
      &lt;p&gt;UPDATE: It is going to take a while to go through all the submissions and add them. If you'd like to help with the process, please send a PR directly to this project: https://github.com/hnpwd/hnpwd.github.io&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46618714"/><published>2026-01-14T17:07:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46618901</id><title>Ford F-150 Lightning outsold the Cybertruck and was then canceled for poor sales</title><updated>2026-01-14T22:44:26.523474+00:00</updated><content>&lt;doc fingerprint="69e41a162a9b1fd9"&gt;
  &lt;main&gt;
    &lt;p&gt;The Tesla Cybertruck program is in shambles. The latest data indicate production is running at roughly 10% of its planned capacity. Meanwhile, the Ford F150 Lightning outsold the Tesla Cybertruck in 2025 and was then canceled for not selling enough.&lt;/p&gt;
    &lt;p&gt;Is this what is coming for the Cybertruck?&lt;/p&gt;
    &lt;p&gt;Tesla is actively trying to hide its Cybertruck sales performance. We have to do the math ourselves.&lt;/p&gt;
    &lt;p&gt;Unlike virtually every other automaker that reports sales by model and region, Tesla bundles its vehicles into two broad categories: “Model 3/Y” and “Other Models.”&lt;/p&gt;
    &lt;p&gt;The “Other Models” category includes the Model S, Model X, Cybertruck, and the Tesla Semi.&lt;/p&gt;
    &lt;p&gt;Model S and Model X sales have been relatively stable at a low volume, typically hovering around 5,000 to 6,000 units combined per quarter globally. If we assume a generous 6,000 units for S and X in Q4 2025 (aided by a slight update), that leaves only roughly 5,600 units for the Cybertruck and Semi combined.&lt;/p&gt;
    &lt;p&gt;Considering the Semi is still in pilot production with negligible volume, we are looking at roughly 5,500 for the entire quarter globally (though it is still mostly North American).&lt;/p&gt;
    &lt;p&gt;This is a disaster compared to the truck’s peak and the company’s stated capacity.&lt;/p&gt;
    &lt;p&gt;We previously reported in July that Tesla confirmed Cybertruck sales were down to ~5,000 units in Q2 2025. It seems the “recovery” never happened, despite price cuts and the introduction of a short-lived, cheaper trim.&lt;/p&gt;
    &lt;p&gt;For the full year 2025, it could bring the total to about 21,500 Cybertrucks globally.&lt;/p&gt;
    &lt;p&gt;According to 2025 full-year data, the Ford F-150 Lightning delivered approximately 27,300 units in the US.&lt;/p&gt;
    &lt;p&gt;Think about that for a second. Ford officially announced it was ending F-150 Lightning production in December to pivot to its new EREV (extended-range electric vehicle) strategy. Yet, even as a “lame duck” product with widely publicised retirement plans, the Lightning still managed to find more buyers than Tesla’s Cybertruck.&lt;/p&gt;
    &lt;p&gt;While Ford’s sales dipped about 18% year-over-year as they wound down the program, Tesla’s numbers crashed by nearly 50% despite the company doing everything it can to keep the program alive.&lt;/p&gt;
    &lt;p&gt;Tesla and Elon Musk have thrown everything at the Cybertruck program, and it’s not working. They released a cheaper stripped-down version and canceled it months later because it wasn’t selling.&lt;/p&gt;
    &lt;p&gt;Last quarter, Musk even had his private company SpaceX buy over 1,000 Cybertrucks, which is about 20% of Tesla’s quarterly Cybertruck sales, and sales were still down more than 50% year-over-year in the quarter.&lt;/p&gt;
    &lt;p&gt;What happens with the Cybertruck from here?&lt;/p&gt;
    &lt;head rend="h2"&gt;Electrek’s Take&lt;/head&gt;
    &lt;p&gt;SpaceX can’t keep buying Cybertrucks, and I don’t know of any vehicle program that sells at 10% of its production capacity and survives.&lt;/p&gt;
    &lt;p&gt;It’s such a big hill to climb.&lt;/p&gt;
    &lt;p&gt;As I previously said, I think if Tesla were to distance itself from Musk’s toxic brand and do things such as give up on the 4680 cells, which appear to have contributed to the Cybertruck being more expensive and having a shorter range than originally announced, it could likely significantly boost Cybertruck sales.&lt;/p&gt;
    &lt;head rend="h2"&gt;Top comment by Realist&lt;/head&gt;
    &lt;p&gt;And the practice of following Elon's gut feelings has not abated. No lessons learned. The Cybercab is a sleek two-seater that, I guess, will not have a steering wheel. There is little demand for a consumer version two-seater which presumably would have steering controls, and cab riders mostly are concerned with getting from A to B with their stuff. The inside stories that are reported from Tesla are that many people did in fact warn him.&lt;/p&gt;
    &lt;p&gt;The Model 2 has been forgotten (Mexico, thanks for the millions you spent to facilitate our factory!). The Roadster, a halo car, has been abandoned for all practical purposes (Customers who put down a deposit, thanks!).&lt;/p&gt;
    &lt;p&gt;Elon personally handles PR, and he's gone backwards in Brand Image. He prides himself on doing no consumer research, and it's resulted in the worst new car introduction in the history of automobiles.&lt;/p&gt;
    &lt;p&gt;Enough to fill production capacity? Probably not, but it could get a lot closer.&lt;/p&gt;
    &lt;p&gt;Short of that, I don’t know where this can go. I think most other automakers would have written off the program already, but Musk can’t because of his ego. It would be admitting defeat.&lt;/p&gt;
    &lt;p&gt;It shows just how much he has changed in the last few years (beyond the obvious white-nationalist stuff), as Musk originally said Tesla would pivot to a more traditional design if the Cybertruck failed. It has failed. Now what?&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electrek.co/2026/01/13/ford-f150-lightning-outsold-tesla-cybertruck-canceled-not-selling-enough/"/><published>2026-01-14T17:20:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46619030</id><title>So, you’ve hit an age gate. What now?</title><updated>2026-01-14T22:44:25.699186+00:00</updated><content>&lt;doc fingerprint="bf2c9a140132b185"&gt;
  &lt;main&gt;
    &lt;p&gt;This blog also appears in our Age Verification Resource Hub: our one-stop shop for users seeking to understand what age-gating laws actually do, what’s at stake, how to protect yourself, and why EFF opposes all forms of age verification mandates. Head to EFF.org/Age to explore our resources and join us in the fight for a free, open, private, and yes—safe—internet.&lt;/p&gt;
    &lt;p&gt;EFF is against age gating and age verification mandates, and we hope we’ll win in getting existing ones overturned and new ones prevented. But mandates are already in effect, and every day many people are asked to verify their age across the web, despite prominent cases of sensitive data getting leaked in the process.&lt;/p&gt;
    &lt;p&gt;At some point, you may have been faced with the decision yourself: should I continue to use this service if I have to verify my age? And if so, how can I do that with the least risk to my personal information? This is our guide to navigating those decisions, with information on what questions to ask about the age verification options you’re presented with, and answers to those questions for some of the top most popular social media sites. Even though there’s no way to implement mandated age gates in a way that fully protects speech and privacy rights, our goal here is to help you minimize the infringement of your rights as you manage this awful situation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Follow the Data&lt;/head&gt;
    &lt;p&gt;Since we know that leaks happen despite the best efforts of software engineers, we generally recommend submitting the absolute least amount of data possible. Unfortunately, that’s not going to be possible for everyone. Even facial age estimation solutions where pictures of your face never leave your device, offering some protection against data leakage, are not a good option for all users: facial age estimation works less well for people of color, trans and nonbinary people, and people with disabilities. There are some systems that use fancy cryptography so that a digital ID saved to your device won’t tell the website anything more than if you meet the age requirement, but access to that digital ID isn’t available to everyone or for all platforms. You may also not want to register for a digital ID and save it to your phone, if you don’t want to take the chance of all the information on it being exposed upon request of an over-zealous verifier, or you simply don’t want to be a part of a digital ID system&lt;/p&gt;
    &lt;p&gt;If you’re given the option of selecting a verification method and are deciding which to use, we recommend considering the following questions for each process allowed by each vendor:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data: What info does each method require?&lt;/item&gt;
      &lt;item&gt;Access: Who can see the data during the course of the verification process?&lt;/item&gt;
      &lt;item&gt;Retention: Who will hold onto that data after the verification process, and for how long?&lt;/item&gt;
      &lt;item&gt;Audits: How sure are we that the stated claims will happen in practice? For example, are there external audits confirming that data is not accidentally leaked to another site along the way? Ideally these will be in-depth, security-focused audits by specialized auditors like NCC Group or Trail of Bits, instead of audits that merely certify adherence to standards.&lt;/item&gt;
      &lt;item&gt;Visibility: Who will be aware that you’re attempting to verify your age, and will they know which platform you’re trying to verify for?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We attempt to provide answers to these questions below. To begin, there are two major factors to consider when answering these questions: the tools each platform uses, and the overall system those tools are part of.&lt;/p&gt;
    &lt;p&gt;In general, most platforms offer age estimation options like face scans as a first line of age assurance. These vary in intrusiveness, but their main problem is inaccuracy, particularly for marginalized users. Third-party age verification vendors Private ID and k-ID offer on-device facial age estimation, but another common vendor, Yoti, sends the image to their servers during age checks by some of the biggest platforms. This risks leaking the images themselves, and also the fact that you’re using that particular website, to the third party.&lt;/p&gt;
    &lt;p&gt;Then, there’s the document-based verification services, which require you to submit a hard identifier like a government-issued ID. This method thus requires you to prove both your age and your identity. A platform can do this in-house through a designated dataflow, or by sending that data to a third party. We’ve already seen examples of how this can fail. For example, Discord routed users' ID data through its general customer service workflow so that a third-party vendor could perform manual review of verification appeals. No one involved ever deleted users' data, so when the system was breached, Discord had to apologize for the catastrophic disclosure of nearly 70,000 photos of users' ID documents. Overly long retention periods expose documents to risk of breaches and historical data requests. Some document verifiers have retention periods that are needlessly long. This is the case with Incode, which provides ID verification for Tiktok. Incode holds onto images forever by default, though TikTok should automatically start the deletion process on your behalf.&lt;/p&gt;
    &lt;p&gt;Some platforms offer alternatives, like proving that you own a credit card, or asking for your email to check if it appears in databases associated with adulthood (like home mortgage databases). These tend to involve less risk when it comes to the sensitivity of the data itself, especially since credit cards can be replaced, but in general still undermine anonymity and pseudonymity and pose a risk of tracking your online activity. We’d prefer to see more assurances across the board about how information is handled.&lt;/p&gt;
    &lt;p&gt;Each site offers users a menu of age assurance options to choose from. We’ve chosen to present these options in the rough order that we expect most people to prefer. Jump directly to a platform to learn more about its age checks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Meta – Facebook, Instagram, WhatsApp, Messenger, Threads&lt;/item&gt;
      &lt;item&gt;Google – Gmail, YouTube&lt;/item&gt;
      &lt;item&gt;TikTok&lt;/item&gt;
      &lt;item&gt;Everywhere Else&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Meta – Facebook, Instagram, WhatsApp, Messenger, Threads&lt;/head&gt;
    &lt;head rend="h3"&gt;Inferred Age&lt;/head&gt;
    &lt;p&gt;If Meta can guess your age, you may never even see an age verification screen. Meta, which runs Facebook, Threads, Instagram, Messenger, and WhatsApp, first tries to use information you’ve posted to guess your age, like looking at “Happy birthday!” messages. It’s a creepy reminder that they already have quite a lot of information about you.&lt;/p&gt;
    &lt;p&gt;If Meta cannot guess your age, or if Meta infers you're too young, it will next ask you to verify your age using either facial age estimation, or by uploading your photo ID.&lt;/p&gt;
    &lt;head rend="h3"&gt;Face Scan&lt;/head&gt;
    &lt;p&gt;If you choose to use facial age estimation, you’ll be sent to Yoti, a third-party verification service. Your photo will be uploaded to their servers during this process. Yoti claims that “as soon as an age has been estimated, the facial image is immediately and permanently deleted.” Though it’s not as good as not having that data in the first place, Yoti’s security measures include a bug bounty program and annual penetration testing. Researchers from Mint Secure found that Yoti’s app and website are filled with trackers, so the fact that you’re verifying your age could be not only shared to Yoti, but leaked to third-party data brokers as well.&lt;/p&gt;
    &lt;p&gt;You may not want to use this option if you’re worried about third parties potentially being able to know you’re trying to verify your age with Meta. You also might not want to use this if you’re worried about a current picture of your face accidentally leaking—for example, if elements in the background of your selfie might reveal your current location. On the other hand, if you consider a selfie to be less sensitive than a photograph of your ID, this option might be better. If you do choose (or are forced to) use the face check system, be sure to snap your selfie without anything you'd be concerned with identifying your location or embarrassing you in the background in case the image leaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Upload ID&lt;/head&gt;
    &lt;p&gt;If Yoti’s age estimation decides your face looks too young, or if you opt out of facial age estimation, your next recourse is to send Meta a photo of your ID. Meta sends that photo to Yoti to verify the ID. Meta says it will hold onto that ID image for 30 days, then delete it. Meanwhile, Yoti claims it will delete the image immediately after verification. Of course, bugs and process oversights exist, such as accidentally replicating information in logs or support queues, but at least they have stated processes. Your ID contains sensitive information such as your full legal name and home address. Using this option not only runs the (hopefully small, but never nonexistent) risk of that data getting leaked through errors or hacking, but it also lets Meta see the information needed to tie your profile to your identity—which you may not want. If you don’t want Meta to know your name and where you live, or rely on both Meta and Yoti to keep to their deletion promises, this option may not be right for you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google – Gmail, YouTube&lt;/head&gt;
    &lt;head rend="h3"&gt;Inferred Age&lt;/head&gt;
    &lt;p&gt;If Google can guess your age, you may never even see an age verification screen. Your Google account is typically connected to your YouTube account, so if (like mine) your YouTube account is old enough to vote, you may not need to verify your Google account at all. Google first uses information it already knows to try to guess your age, like how long you’ve had the account and your YouTube viewing habits. It’s yet another creepy reminder of how much information these corporations have on you, but at least in this case they aren’t likely to ask for even more identifying data.&lt;/p&gt;
    &lt;p&gt;If Google cannot guess your age, or decides you're too young, Google will next ask you to verify your age. You’ll be given a variety of options for how to do so, with availability that will depend on your location and your age.&lt;/p&gt;
    &lt;p&gt;Google’s methods to assure your age include ID verification, facial age estimation, verification by proxy, and digital ID. To prove you’re over 18, you may be able to use facial age estimation, give Google your credit card information, or tell a third-party provider your email address.&lt;/p&gt;
    &lt;head rend="h3"&gt;Face Scan&lt;/head&gt;
    &lt;p&gt;If you choose to use facial age estimation, you’ll be sent to a website run by Private ID, a third-party verification service. The website will load Private ID’s verifier within the page—this means that your selfie will be checked without any images leaving your device. If the system decides you’re over 18, it will let Google know that, and only that. Of course, no technology is perfect—should Private ID be mandated to target you specifically, there’s nothing to stop it from sending down code that does in fact upload your image, and you probably won’t notice. But unless your threat model includes being specifically targeted by a state actor or Private ID, that’s unlikely to be something you need to worry about. For most people, no one else will see your image during this process. Private ID will, however, be told that your device is trying to verify your age with Google and Google will still find out if Private ID thinks that you’re under 18.&lt;/p&gt;
    &lt;p&gt;If Private ID’s age estimation decides your face looks too young, you may next be able to decide if you’d rather let Google verify your age by giving it your credit card information, photo ID, or digital ID, or by letting Google send your email address to a third-party verifier.&lt;/p&gt;
    &lt;head rend="h3"&gt;Email Usage&lt;/head&gt;
    &lt;p&gt;If you choose to provide your email address, Google sends it on to a company called VerifyMy. VerifyMy will use your email address to see if you’ve done things like get a mortgage or paid for utilities using that email address. If you use Gmail as your email provider, this may be a privacy-protective option with respect to Google, as Google will then already know the email address associated with the account. But it does tell VerifyMy and its third-party partners that the person behind this email address is looking to verify their age, which you may not want them to know. VerifyMy uses “proprietary algorithms and external data sources” that involve sending your email address to “trusted third parties, such as data aggregators.” It claims to “ensure that such third parties are contractually bound to meet these requirements,” but you’ll have to trust it on that one—we haven’t seen any mention of who those parties are, so you’ll have no way to check up on their practices and security. On the bright side, VerifyMy and its partners do claim to delete your information as soon as the check is completed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Credit Card Verification&lt;/head&gt;
    &lt;p&gt;If you choose to let Google use your credit card information, you’ll be asked to set up a Google Payments account. Note that debit cards won’t be accepted, since it’s much easier for many debit cards to be issued to people under 18. Google will then charge a small amount to the card, and refund it once it goes through. If you choose this method, you’ll have to tell Google your credit card info, but the fact that it’s done through Google Payments (their regular card-processing system) means that at least your credit card information won’t be sitting around in some unsecured system. Even if your credit card information happens to accidentally be leaked, this is a relatively low-risk option, since credit cards come with solid fraud protection. If your credit card info gets leaked, you should easily be able to dispute fraudulent charges and replace the card.&lt;/p&gt;
    &lt;head rend="h3"&gt;Digital ID&lt;/head&gt;
    &lt;p&gt;If the option is available to you, you may be able to use your digital ID to verify your age with Google. In some regions, you’ll be given the option to use your digital ID. In some cases, it’s possible to only reveal your age information when you use a digital ID. If you’re given that choice, it can be a good privacy-preserving option. Depending on the implementation, there’s a chance that the verification step will “phone home” to the ID provider (usually a government) to let them know the service asked for your age. It’s a complicated and varied topic that you can learn more about by visiting EFF’s page on digital identity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Upload ID&lt;/head&gt;
    &lt;p&gt;Should none of these options work for you, your final recourse is to send Google a photo of your ID. Here, you’ll be asked to take a photo of an acceptable ID and send it to Google. Though the help page only states that your ID “will be stored securely,” the verification process page says ID “will be deleted after your date of birth is successfully verified.” Acceptable IDs vary by country, but are generally government-issued photo IDs. We like that it’s deleted immediately, though we have questions about what Google means when it says your ID will be used to “improve [its] verification services for Google products and protect against fraud and abuse.” No system is perfect, and we can only hope that Google schedules outside audits regularly.&lt;/p&gt;
    &lt;head rend="h2"&gt;TikTok&lt;/head&gt;
    &lt;head rend="h3"&gt;Inferred Age&lt;/head&gt;
    &lt;p&gt;If TikTok can guess your age, you may never even see an age verification notification. TikTok first tries to use information you’ve posted to estimate your age, looking through your videos and photos to analyze your face and listen to your voice. By uploading any videos, TikTok believes you’ve given it consent to try to guess how old you look and sound.&lt;/p&gt;
    &lt;p&gt;If TikTok decides you’re too young, appeal to revoke their age decision before the deadline passes. If TikTok cannot guess your age, or decides you're too young, it will automatically revoke your access based on age—including either restricting features or deleting your account. To get your access and account back, you’ll have a limited amount of time to verify your age. As soon as you see the notification that your account is restricted, you’ll want to act fast because in some places you’ll have as little as 23 days before the deadline passes.&lt;/p&gt;
    &lt;p&gt;When you get that notification, you’re given various options to verify your age based on your location.&lt;/p&gt;
    &lt;head rend="h3"&gt;Face Scan&lt;/head&gt;
    &lt;p&gt;If you’re given the option to use facial age estimation, you’ll be sent to Yoti, a third-party verification service. Your photo will be uploaded to their servers during this process. Yoti claims that “as soon as an age has been estimated, the facial image is immediately and permanently deleted.” Though it’s not as good as not having that data in the first place, Yoti’s security measures include a bug bounty program and annual penetration testing. However, researchers from Mint Secure found that Yoti’s app and website are filled with trackers, so the fact that you’re verifying your age could be leaked not only to Yoti, but to third-party data brokers as well.&lt;/p&gt;
    &lt;p&gt;You may not want to use this option if you’re worried about third parties potentially being able to know you’re trying to verify your age with TikTok. You also might not want to use this if you’re worried about a current picture of your face accidentally leaking—for example, if elements in the background of your selfie might reveal your current location. On the other hand, if you consider a selfie to be less sensitive than a photograph of your ID or your credit card information, this option might be better. If you do choose (or are forced to) use the face check system, be sure to snap your selfie without anything you'd be concerned with identifying your location or embarrassing you in the background in case the image leaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Credit Card Verification&lt;/head&gt;
    &lt;p&gt;If you have a credit card in your name, TikTok will accept that as proof that you’re over 18. Note that debit cards won’t be accepted, since it’s much easier for many debit cards to be issued to people under 18. TikTok will charge a small amount to the credit card, and refund it once it goes through. It’s unclear if this goes through their regular payment process, or if your credit card information will be sent through and stored in a separate, less secure system. Luckily, these days credit cards come with solid fraud protection, so if your credit card gets leaked, you should easily be able to dispute fraudulent charges and replace the card. That said, we’d rather TikTok provide assurances that the information will be processed securely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Credit Card Verification of a Parent or Guardian&lt;/head&gt;
    &lt;p&gt;Sometimes, if you’re between 13 and 17, you’ll be given the option to let your parent or guardian confirm your age. You’ll tell TikTok their email address, and TikTok will send your parent or guardian an email asking them (a) to confirm your date of birth, and (b) to verify their own age by proving that they own a valid credit card. This option doesn’t always seem to be offered, and in the one case we could find, it’s possible that TikTok never followed up with the parent. So it’s unclear how or if TikTok verifies that the adult whose email you provide is your parent or guardian. If you want to use credit card verification but you’re not old enough to have a credit card, and you’re ok with letting an adult know you use TikTok, this option may be reasonable to try.&lt;/p&gt;
    &lt;head rend="h3"&gt;Photo with a Random Adult?&lt;/head&gt;
    &lt;p&gt;Bizarrely, if you’re between 13 and 17, TikTok claims to offer the option to take a photo with literally any random adult to confirm your age. Its help page says that any trusted adult over 25 can be chosen, as long as they’re holding a piece of paper with the code on it that TikTok provides. It also mentions that a third-party provider is used here, but doesn’t say which one. We haven’t found any evidence of this verification method being offered. Please do let us know if you’ve used this method to verify your age on TikTok!&lt;/p&gt;
    &lt;head rend="h3"&gt;Photo ID and Face Comparison&lt;/head&gt;
    &lt;p&gt;If you aren’t offered or have failed the other options, you’ll have to verify your age by submitting a copy of your ID and matching photo of your face. You’ll be sent to Incode, a third-party verification service. In a disappointing failure to meet the industry standard, Incode itself doesn’t automatically delete the data you give it once the process is complete, but TikTok does claim to “start the process to delete the information you submitted,” which should include telling Incode to delete your data once the process is done. If you want to be sure, you can ask Incode to delete that data yourself. Incode tells TikTok that you met the age threshold without providing your exact date of birth, but then TikTok wants to know the exact date anyway, so it’ll ask for your date of birth even after your age has been verified.&lt;/p&gt;
    &lt;p&gt;TikTok itself might not see your actual ID depending on its implementation choices, but Incode will. Your ID contains sensitive information such as your full legal name and home address. Using this option not only runs the (hopefully small, but never nonexistent) risk of that data getting accidentally leaked through errors or hacking. If you don’t want TikTok or Incode to know your name, what you look like, and where you live—or if you don't want to rely on both TikTok and Incode to keep to their deletion promises—then this option may not be right for you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Everywhere Else&lt;/head&gt;
    &lt;p&gt;We’ve covered the major providers here, but age verification is unfortunately being required of many other services that you might use as well. While the providers and processes may vary, the same general principles will apply. If you’re trying to choose what information to provide to continue to use a service, consider the “follow the data” questions mentioned above, and try to find out how the company will store and process the data you give it. The less sensitive information, the fewer people have access to it, and the more quickly it will be deleted, the better. You may even come to recognize popular names in the age verification industry: Spotify and OnlyFans use Yoti (just like Meta and Tiktok), Quora and Discord use k-ID, and so on.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it should be clear by now that none of the age verification options are perfect in terms of protecting information, providing access to everyone, and safely handling sensitive data. That’s just one of the reasons that EFF is against age-gating mandates, and is working to stop and overturn them across the United States and around the world.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Help protect digital privacy &amp;amp; free speech for everyone.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/deeplinks/2026/01/so-youve-hit-age-gate-what-now"/><published>2026-01-14T17:27:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46620673</id><title>Native ZFS VDEV for Object Storage (OpenZFS Summit)</title><updated>2026-01-14T22:44:25.198685+00:00</updated><content>&lt;doc fingerprint="6c79b8178ddc4fa1"&gt;
  &lt;main&gt;&lt;p&gt;We presented MayaNAS and MayaScale at OpenZFS Developer Summit 2025 in Portland, Oregon. The centerpiece of our presentation: objbacker.io—a native ZFS VDEV implementation for object storage that bypasses FUSE entirely, achieving 3.7 GB/s read throughput directly from S3, GCS, and Azure Blob Storage.&lt;/p&gt;&lt;head rend="h2"&gt;Presenting at OpenZFS Summit&lt;/head&gt;&lt;p&gt;The OpenZFS Developer Summit brings together the core developers and engineers who build and maintain ZFS across platforms. It was the ideal venue to present our approach to cloud-native storage: using ZFS's architectural flexibility to create a hybrid storage system that combines the performance of local NVMe with the economics of object storage.&lt;/p&gt;&lt;p&gt;Our 50-minute presentation covered the complete Zettalane storage platform—MayaNAS for file storage and MayaScale for block storage—with a deep technical dive into the objbacker.io implementation that makes ZFS on object storage practical for production workloads.&lt;/p&gt;&lt;head rend="h2"&gt;The Cloud NAS Challenge&lt;/head&gt;&lt;p&gt;Cloud storage economics present a fundamental problem for NAS deployments:&lt;/p&gt;&lt;head rend="h3"&gt;$96K/year&lt;/head&gt;&lt;p&gt;100TB on EBS (gp3)&lt;/p&gt;&lt;head rend="h3"&gt;$360K/year&lt;/head&gt;&lt;p&gt;100TB on AWS EFS&lt;/p&gt;&lt;p&gt;The insight that drives MayaNAS: not all data needs the same performance tier. Metadata operations require low latency and high IOPS. Large sequential data needs throughput, not IOPS. ZFS's special device architecture lets us place each workload on the appropriate storage tier.&lt;/p&gt;&lt;head rend="h2"&gt;objbacker.io: Native ZFS VDEV for Object Storage&lt;/head&gt;&lt;p&gt;The traditional approach to ZFS on object storage uses FUSE-based filesystems like s3fs or goofys to mount buckets, then creates ZFS pools on top. This works, but FUSE adds overhead: every I/O crosses the kernel-userspace boundary twice.&lt;/p&gt;&lt;p&gt; objbacker.io takes a different approach. We implemented a native ZFS VDEV type (&lt;code&gt;VDEV_OBJBACKER&lt;/code&gt;) that communicates directly with a userspace daemon via a character device (&lt;code&gt;/dev/zfs_objbacker&lt;/code&gt;). The daemon uses native cloud SDKs (AWS SDK, Google Cloud SDK, Azure SDK) for direct object storage access.
          &lt;/p&gt;&lt;head rend="h3"&gt;Architecture Comparison&lt;/head&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Approach&lt;/cell&gt;&lt;cell role="head"&gt;I/O Path&lt;/cell&gt;&lt;cell role="head"&gt;Overhead&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;FUSE-based (s3fs)&lt;/cell&gt;&lt;cell&gt;ZFS → VFS → FUSE → userspace → FUSE → VFS → s3fs → S3&lt;/cell&gt;&lt;cell&gt;High (multiple context switches)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;objbacker.io&lt;/cell&gt;&lt;cell&gt;ZFS → /dev/zfs_objbacker → objbacker.io → S3 SDK&lt;/cell&gt;&lt;cell&gt;Minimal (direct path)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;How objbacker.io Works&lt;/head&gt;&lt;p&gt;objbacker.io is a Golang program with two interfaces:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Frontend: ZFS VDEV interface via &lt;code&gt;/dev/zfs_objbacker&lt;/code&gt;character device&lt;/item&gt;&lt;item&gt;Backend: Native cloud SDK integration for GCS, AWS S3, and Azure Blob Storage&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;ZIO to Object Storage Mapping&lt;/head&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;ZFS VDEV I/O&lt;/cell&gt;&lt;cell role="head"&gt;/dev/objbacker&lt;/cell&gt;&lt;cell role="head"&gt;Object Storage&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;ZIO_TYPE_WRITE&lt;/cell&gt;&lt;cell&gt;WRITE&lt;/cell&gt;&lt;cell&gt;PUT object&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;ZIO_TYPE_READ&lt;/cell&gt;&lt;cell&gt;READ&lt;/cell&gt;&lt;cell&gt;GET object&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;ZIO_TYPE_TRIM&lt;/cell&gt;&lt;cell&gt;UTRIM&lt;/cell&gt;&lt;cell&gt;DELETE object&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;ZIO_TYPE_IOCTL (sync)&lt;/cell&gt;&lt;cell&gt;USYNC&lt;/cell&gt;&lt;cell&gt;Flush pending writes&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h3"&gt;Data Alignment&lt;/head&gt;&lt;p&gt;With ZFS recordsize set to 1MB, each ZFS block maps directly to a single object. Aligned writes go directly as PUT requests without caching. This alignment is critical for performance—object storage performs best with large, aligned operations.&lt;/p&gt;&lt;code&gt;bucket/00001&lt;/code&gt;, &lt;code&gt;bucket/00002&lt;/code&gt;, etc.
          &lt;head rend="h2"&gt;Validated Performance Results&lt;/head&gt;&lt;p&gt;We presented benchmark results from AWS c5n.9xlarge instances (36 vCPUs, 96 GB RAM, 50 Gbps network):&lt;/p&gt;&lt;head rend="h3"&gt;3.7 GB/s&lt;/head&gt;&lt;p&gt;Sequential Read from S3&lt;/p&gt;&lt;head rend="h3"&gt;2.5 GB/s&lt;/head&gt;&lt;p&gt;Sequential Write to S3&lt;/p&gt;&lt;p&gt;The key to this throughput: parallel bucket I/O. With 6 S3 buckets configured as a striped pool, ZFS parallelizes reads and writes across multiple object storage endpoints, saturating the available network bandwidth.&lt;/p&gt;&lt;head rend="h3"&gt;FIO Test Configuration&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;ZFS Recordsize&lt;/cell&gt;&lt;cell&gt;1MB (aligned with object size)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Block Size&lt;/cell&gt;&lt;cell&gt;1MB&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Parallel Jobs&lt;/cell&gt;&lt;cell&gt;10 concurrent FIO jobs&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;File Size&lt;/cell&gt;&lt;cell&gt;10 GB per job (100 GB total)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;I/O Engine&lt;/cell&gt;&lt;cell&gt;sync (POSIX synchronous I/O)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;MayaScale: High-Performance Block Storage&lt;/head&gt;&lt;p&gt;We also presented MayaScale, our NVMe-oF block storage solution for workloads requiring sub-millisecond latency. MayaScale uses local NVMe SSDs with Active-Active HA clustering.&lt;/p&gt;&lt;head rend="h3"&gt;MayaScale Performance Tiers (GCP)&lt;/head&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Tier&lt;/cell&gt;&lt;cell role="head"&gt;Write IOPS (&amp;lt;1ms)&lt;/cell&gt;&lt;cell role="head"&gt;Read IOPS (&amp;lt;1ms)&lt;/cell&gt;&lt;cell role="head"&gt;Best Latency&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Ultra&lt;/cell&gt;&lt;cell&gt;585K&lt;/cell&gt;&lt;cell&gt;1.1M&lt;/cell&gt;&lt;cell&gt;280 us&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;High&lt;/cell&gt;&lt;cell&gt;290K&lt;/cell&gt;&lt;cell&gt;1.02M&lt;/cell&gt;&lt;cell&gt;268 us&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Medium&lt;/cell&gt;&lt;cell&gt;175K&lt;/cell&gt;&lt;cell&gt;650K&lt;/cell&gt;&lt;cell&gt;211 us&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Standard&lt;/cell&gt;&lt;cell&gt;110K&lt;/cell&gt;&lt;cell&gt;340K&lt;/cell&gt;&lt;cell&gt;244 us&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Basic&lt;/cell&gt;&lt;cell&gt;60K&lt;/cell&gt;&lt;cell&gt;120K&lt;/cell&gt;&lt;cell&gt;218 us&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Multi-Cloud Architecture&lt;/head&gt;&lt;p&gt;Both MayaNAS and MayaScale deploy consistently across AWS, Azure, and GCP. Same Terraform modules, same ZFS configuration, same management interface. Only the cloud-specific networking and storage APIs differ.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Component&lt;/cell&gt;&lt;cell role="head"&gt;AWS&lt;/cell&gt;&lt;cell role="head"&gt;Azure&lt;/cell&gt;&lt;cell role="head"&gt;GCP&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Instance&lt;/cell&gt;&lt;cell&gt;c5.xlarge&lt;/cell&gt;&lt;cell&gt;D4s_v4&lt;/cell&gt;&lt;cell&gt;n2-standard-4&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Block Storage&lt;/cell&gt;&lt;cell&gt;EBS gp3&lt;/cell&gt;&lt;cell&gt;Premium SSD&lt;/cell&gt;&lt;cell&gt;pd-ssd&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Object Storage&lt;/cell&gt;&lt;cell&gt;S3&lt;/cell&gt;&lt;cell&gt;Blob Storage&lt;/cell&gt;&lt;cell&gt;GCS&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;VIP Migration&lt;/cell&gt;&lt;cell&gt;ENI attach&lt;/cell&gt;&lt;cell&gt;LB health probe&lt;/cell&gt;&lt;cell&gt;IP alias&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Deployment&lt;/cell&gt;&lt;cell&gt;CloudFormation&lt;/cell&gt;&lt;cell&gt;ARM Template&lt;/cell&gt;&lt;cell&gt;Terraform&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Watch the Full Presentation&lt;/head&gt;&lt;p&gt;The complete 50-minute presentation is available on the OpenZFS YouTube channel:&lt;/p&gt;&lt;p&gt;Note: Video will be available once published by OpenZFS. Check the OpenZFS YouTube channel for the recording.&lt;/p&gt;&lt;head rend="h3"&gt;Presentation Highlights&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;0:00 - Introduction and Zettalane overview&lt;/item&gt;&lt;item&gt;5:00 - Zettalane ZFS port architecture (illumos-gate based)&lt;/item&gt;&lt;item&gt;12:00 - The cloud NAS cost challenge&lt;/item&gt;&lt;item&gt;18:00 - MayaNAS hybrid architecture with ZFS special devices&lt;/item&gt;&lt;item&gt;25:00 - objbacker.io deep dive: native VDEV implementation&lt;/item&gt;&lt;item&gt;35:00 - Performance benchmarks on AWS&lt;/item&gt;&lt;item&gt;42:00 - MayaScale NVMe-oF block storage&lt;/item&gt;&lt;item&gt;48:00 - Q&amp;amp;A and future directions&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Getting Started&lt;/head&gt;&lt;p&gt;Deploy MayaNAS or MayaScale on your preferred cloud platform:&lt;/p&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;Presenting at OpenZFS Developer Summit 2025 gave us the opportunity to share our approach with the community that makes ZFS possible. The key technical contribution: objbacker.io demonstrates that native ZFS VDEV integration with object storage is practical and performant, achieving 3.7 GB/s throughput without FUSE overhead.&lt;/p&gt;&lt;p&gt;MayaNAS with objbacker.io delivers enterprise-grade NAS on object storage with 70%+ cost savings versus traditional cloud block storage. MayaScale provides sub-millisecond block storage with Active-Active HA for latency-sensitive workloads. Together, they cover 90% of enterprise storage needs on any major cloud platform.&lt;/p&gt;&lt;p&gt;Special thanks to the OpenZFS community for the foundation that makes this possible.&lt;/p&gt;&lt;p&gt; Ready to deploy cloud-native storage?&lt;lb/&gt; Contact Us Download &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.zettalane.com/blog/openzfs-summit-2025-mayanas-objbacker.html"/><published>2026-01-14T18:49:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46620990</id><title>Ask HN: How do you safely give LLMs SSH/DB access?</title><updated>2026-01-14T22:44:24.893583+00:00</updated><content>&lt;doc fingerprint="57e08ae36d354a3f"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I have been using Claude Code for DevOps style tasks like SSHing into servers, grepping logs, inspecting files, and querying databases&lt;/p&gt;
      &lt;p&gt;Overall it's been great. However, I find myself having to review every single command, a lot of which are repetitive. It still saves me a ton of time, but it's quickly becoming a bit tedious&lt;/p&gt;
      &lt;p&gt;I wish I could give the agent some more autonomy. Like giving it a list of pre-approved commands or actions that it is allowed to run over ssh&lt;/p&gt;
      &lt;p&gt;For example:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;    OK: ls, grep, cat, tail
    Not OK: rm, mv, chmod, etc
    OK: SELECT queries
    Not OK: INSERT, DELETE, DROP, TRUNCATE
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Has anyone successfully or satisfactorily solved this?&lt;/p&gt;
      &lt;p&gt;What setups have actually worked for you, and where do you draw the line between autonomy and risk?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46620990"/><published>2026-01-14T19:06:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46621786</id><title>Show HN: A fast CLI and MCP server for managing Lambda cloud GPU instances</title><updated>2026-01-14T22:44:24.160757+00:00</updated><content>&lt;doc fingerprint="bf8cebd9c5875bea"&gt;
  &lt;main&gt;
    &lt;p&gt;Caution&lt;/p&gt;
    &lt;p&gt;UNOFFICIAL PROJECT — This is a community-built tool, not affiliated with or endorsed by Lambda.&lt;/p&gt;
    &lt;p&gt;A fast CLI and MCP server for managing Lambda cloud GPU instances.&lt;/p&gt;
    &lt;p&gt;Two ways to use it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CLI (&lt;code&gt;lambda&lt;/code&gt;) - Direct terminal commands for managing GPU instances&lt;/item&gt;
      &lt;item&gt;MCP Server (&lt;code&gt;lambda-mcp&lt;/code&gt;) - Let AI assistants like Claude manage your GPU infrastructure&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;brew install strand-ai/tap/lambda-cli&lt;/code&gt;
    &lt;code&gt;cargo install --git https://github.com/Strand-AI/lambda-cli&lt;/code&gt;
    &lt;p&gt;Download from GitHub Releases.&lt;/p&gt;
    &lt;p&gt;Get your API key from the Lambda dashboard.&lt;/p&gt;
    &lt;code&gt;export LAMBDA_API_KEY=&amp;lt;your-key&amp;gt;&lt;/code&gt;
    &lt;code&gt;export LAMBDA_API_KEY_COMMAND="op read op://Personal/Lambda/api-key"&lt;/code&gt;
    &lt;p&gt;The command is executed at startup and its output is used as the API key. This works with any secret manager.&lt;/p&gt;
    &lt;p&gt;Get notified on Slack, Discord, or Telegram when your instance is ready and SSH-able.&lt;/p&gt;
    &lt;p&gt;Set one or more of these environment variables:&lt;/p&gt;
    &lt;code&gt;# Slack (incoming webhook)
export LAMBDA_NOTIFY_SLACK_WEBHOOK="https://hooks.slack.com/services/T00/B00/XXX"

# Discord (webhook URL)
export LAMBDA_NOTIFY_DISCORD_WEBHOOK="https://discord.com/api/webhooks/123/abc"

# Telegram (bot token + chat ID)
export LAMBDA_NOTIFY_TELEGRAM_BOT_TOKEN="123456:ABC-DEF..."
export LAMBDA_NOTIFY_TELEGRAM_CHAT_ID="123456789"&lt;/code&gt;
    &lt;p&gt;Slack: Create an Incoming Webhook in your workspace.&lt;/p&gt;
    &lt;p&gt;Discord: In channel settings → Integrations → Webhooks → New Webhook → Copy Webhook URL.&lt;/p&gt;
    &lt;p&gt;Telegram:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Message @BotFather → &lt;code&gt;/newbot&lt;/code&gt;→ copy the token&lt;/item&gt;
      &lt;item&gt;Message your bot, then visit &lt;code&gt;https://api.telegram.org/bot&amp;lt;TOKEN&amp;gt;/getUpdates&lt;/code&gt;to find your chat ID&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;lambda list&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show available GPU types with pricing and availability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;lambda running&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show your running instances&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;lambda start&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Launch a new instance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;lambda stop&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Terminate an instance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;lambda find&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Poll until a GPU type is available, then launch&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;List available GPUs:&lt;/p&gt;
    &lt;code&gt;lambda list&lt;/code&gt;
    &lt;p&gt;Start an instance:&lt;/p&gt;
    &lt;code&gt;lambda start --gpu gpu_1x_a10 --ssh my-key --name "dev-box"&lt;/code&gt;
    &lt;p&gt;Stop an instance:&lt;/p&gt;
    &lt;code&gt;lambda stop --instance-id &amp;lt;id&amp;gt;&lt;/code&gt;
    &lt;p&gt;Wait for availability and auto-launch:&lt;/p&gt;
    &lt;code&gt;lambda find --gpu gpu_8x_h100 --ssh my-key --interval 30&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-g, --gpu&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Instance type (required)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-s, --ssh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;SSH key name (required)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-n, --name&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Instance name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-r, --region&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Region (auto-selects if omitted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-f, --filesystem&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Filesystem to attach (must be in same region)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--no-notify&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disable notifications even if env vars are set&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-g, --gpu&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Instance type to wait for (required)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-s, --ssh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;SSH key name (required)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--interval&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Poll interval in seconds (default: 10)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-n, --name&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Instance name when launched&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-f, --filesystem&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Filesystem to attach when launched&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--no-notify&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disable notifications even if env vars are set&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notifications are automatic when env vars are configured. Use &lt;code&gt;--no-notify&lt;/code&gt; to disable:&lt;/p&gt;
    &lt;code&gt;lambda start --gpu gpu_1x_a10 --ssh my-key --no-notify&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;lambda-mcp&lt;/code&gt; binary is an MCP (Model Context Protocol) server that lets AI assistants manage your Lambda infrastructure.&lt;/p&gt;
    &lt;p&gt;The easiest way to use &lt;code&gt;lambda-mcp&lt;/code&gt; is via npx—no installation required:&lt;/p&gt;
    &lt;code&gt;npx @strand-ai/lambda-mcp&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--eager&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execute API key command at startup instead of on first use&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When using &lt;code&gt;LAMBDA_API_KEY_COMMAND&lt;/code&gt;, the MCP server defers command execution until the first API request by default. This avoids unnecessary delays when starting Claude Code if you don't use Lambda tools in every session.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;--eager&lt;/code&gt; to execute the command at startup instead:&lt;/p&gt;
    &lt;code&gt;npx @strand-ai/lambda-mcp --eager&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note: The CLI (&lt;/p&gt;&lt;code&gt;lambda&lt;/code&gt;) always executes the API key command at startup since it's used for immediate operations.&lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;list_gpu_types&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List all available GPU instance types with pricing, specs, and current availability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;start_instance&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Launch a new GPU instance (auto-notifies if configured)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;stop_instance&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Terminate a running instance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;list_running_instances&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show all running instances with status and connection details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;check_availability&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Check if a specific GPU type is available&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When notification environment variables are configured, the MCP server automatically sends notifications when instances become SSH-able. No additional flags needed—just set the &lt;code&gt;LAMBDA_NOTIFY_*&lt;/code&gt; env vars and launch instances as usual.&lt;/p&gt;
    &lt;code&gt;claude mcp add lambda -s user -e LAMBDA_API_KEY=your-api-key -- npx -y @strand-ai/lambda-mcp&lt;/code&gt;
    &lt;p&gt;With 1Password CLI:&lt;/p&gt;
    &lt;code&gt;claude mcp add lambda -s user -e LAMBDA_API_KEY_COMMAND="op read op://Personal/Lambda/api-key" -- npx -y @strand-ai/lambda-mcp&lt;/code&gt;
    &lt;p&gt;Then restart Claude Code.&lt;/p&gt;
    &lt;p&gt;Once configured, you can ask Claude things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"What GPUs are currently available on Lambda?"&lt;/item&gt;
      &lt;item&gt;"Launch an H100 instance with my ssh key 'macbook'"&lt;/item&gt;
      &lt;item&gt;"Show me my running instances"&lt;/item&gt;
      &lt;item&gt;"Check if any A100s are available"&lt;/item&gt;
      &lt;item&gt;"Terminate instance i-abc123"&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Build
cargo build

# Run tests
cargo test

# Run CLI
cargo run --bin lambda -- list

# Run MCP server
cargo run --bin lambda-mcp&lt;/code&gt;
    &lt;p&gt;To create a release:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Update the version in &lt;code&gt;Cargo.toml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Merge to main — this automatically: &lt;list rend="ul"&gt;&lt;item&gt;Creates a git tag&lt;/item&gt;&lt;item&gt;Builds binaries for all platforms&lt;/item&gt;&lt;item&gt;Publishes to npm&lt;/item&gt;&lt;item&gt;Updates the Homebrew formula&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Strand-AI/lambda-cli"/><published>2026-01-14T19:45:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46621945</id><title>Every country should set 16 as the minimum age for social media accounts</title><updated>2026-01-14T22:44:23.698767+00:00</updated><content>&lt;doc fingerprint="3f67db6eb8ee96c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Every Country Should Set 16 (or Higher) as the Minimum Age for Social Media Accounts&lt;/head&gt;
    &lt;head rend="h3"&gt;Four features of strong age-limit policies for countries ready to follow Australia’s brave lead&lt;/head&gt;
    &lt;p&gt;The biggest news of 2025 regarding kids’ online safety was Australia’s new social media age-limit law, which set the minimum age for opening or maintaining a social media account to 16. The second-biggest news? As Australia’s law went into effect, there was a global chorus of parents, journalists, and political leaders who stood up, applauded the bold move, and asked, “Can we do that, too?”&lt;/p&gt;
    &lt;p&gt;Bloomberg, in an article titled “TikTok, Instagram Ban for Australian Kids Heralds Global Curbs,” provides a list of countries in which legislation has been, or soon will be, introduced:&lt;/p&gt;
    &lt;p&gt;The idea is spreading, and each nation considering such a policy should ask two important questions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Should the age be 16, as in Australia, or should it be 15, as might become the case in France?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Should there be an option for parents to give consent for adolescents below that age?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The correct answers: 16, and no.&lt;/p&gt;
    &lt;p&gt;Here’s why:&lt;/p&gt;
    &lt;head rend="h2"&gt;We must protect puberty, and 15 is still puberty&lt;/head&gt;
    &lt;p&gt;I devoted an entire chapter of The Anxious Generation to puberty because it is such a crucial period of brain re-wiring and identity formation. Developmental psychologists see puberty as a “sensitive period” in which the brain is especially “plastic” or malleable based on incoming experience. The brain is changing over from the child form to the adult form, and those changes are guided by whatever a child does repeatedly. Neurons that fire together wire together, as brain researchers say.&lt;/p&gt;
    &lt;p&gt;The average adolescent in the U.S. now spends around five hours a day using social media. Their brains will enhance whichever neurons and circuits are activated repeatedly, at the expense of neurons and circuits that are underused. This brain sculpting happens throughout childhood, and continues on in the pre-frontal cortex until around age 25. But in the earlier part of adolescence — specifically puberty — the sculpting is more intense, and the changes are more likely to be permanent.&lt;/p&gt;
    &lt;p&gt;The age range of puberty varies across cultures and historical eras, but in modern developed nations it generally begins between ages 8–13 for girls and a year or two later for boys. By almost any measure, the median boy and the median girl are still in puberty on their 15th birthday. Most are still getting taller. Their secondary sex characteristics are still changing. Large population studies of American and European teens show that the median girl reaches Tanner stage 5 (the last stage of genital development) between 15 and 16, while the median boy reaches stage 5 around 16 or 17. There is wide variation for both sexes. The ability to self-regulate improves steadily throughout adolescence, only reaching a plateau in the mid 20s.&lt;/p&gt;
    &lt;p&gt;In other words: Half or more of all girls are still in puberty on their 15th birthday, and half or more of all boys are still in puberty on their 16th birthday. This is a major reason why 16 is a much better choice for a minimum age than 15. (Of course, 18 would be even better than 16, but we nominated 16 as the norm in The Anxious Generation because our goal was to pick the highest age that we thought could actually get enacted across many jurisdictions.)&lt;/p&gt;
    &lt;p&gt;Puberty is the period when parents should be most careful about how their children spend their time and who (or what) is influencing their developing brains and identities. Traditionally, human societies helped children make the jump from child to adult during this crucial period, with rites of passage in which trusted, non-parental adults guided them through challenges, hardships, and lessons.&lt;/p&gt;
    &lt;p&gt;But what do we do in Western nations? We generally mark the beginning of puberty by giving kids smartphones (average age in the U.S. is around 11 or 12). We then outsource their social and neural development to Instagram, TikTok, and YouTube. The results have been catastrophic for their mental health, social relationships, education, and ability to focus for more than a few minutes.&lt;/p&gt;
    &lt;p&gt;So does it matter whether the age cutoff is 15, rather than 16? Yes. Puberty is the time when social media is likely to do the most damage, and most adolescents, including the large majority of boys, are still in puberty at 15. A 2022 paper by Orben and her colleagues even found that there was a peak “developmental window of sensitivity to social media,” such that heavy use by boys at ages 14 and 15 most strongly predicted decreased life satisfaction a year later. (For girls the peak sensitivity was ages 11 through 13).&lt;/p&gt;
    &lt;p&gt;Sixteen may feel like a more obvious or natural choice for the age of “digital adulthood” in the U.S. because the minimum age for a driver’s license is 16 in most states, (though it is higher in most other countries). Similarly, people in some European countries may see 15 as an obvious or natural choice because that is the age of consent in some countries, the age at which adolescents can legally engage in sexual activity.&lt;/p&gt;
    &lt;p&gt;But the fact remains: Any nation that sets 15 as the minimum age rather than 16 will condemn its children to an extra year of brain-sculpting by social media at a time when their brains are still highly sculptable. It will also greatly increase the risk of exposure to pornography, sextortion, online cruelty, and other risky interactions with anonymous strangers at an age when teens have less ability to self-regulate or know what is safe.&lt;/p&gt;
    &lt;head rend="h2"&gt;Parental-consent exceptions put parents right back into the trap&lt;/head&gt;
    &lt;p&gt;Parents everywhere have heard their children invoke the mantra “but everyone else has one! I’m being left out!” in their daily struggles over smartphones, tablets, social media, video games, and other screen-based activities. And the kids are largely correct. Now that almost everyone else has one, everyone feels that they, too, have to have one. That’s a perfect example of what economists call a collective action trap, where everyone ends up doing something sub-optimal because if they were the only one to choose the better action, they’d actually end up even worse off.&lt;/p&gt;
    &lt;p&gt;The way to escape from a collective action trap is collectively. If most families only give basic phones before age 14, then no 13-year-old can say “but I’m the only one who doesn’t have an iPhone!” If most families wait until 16 before allowing their kids to open social media accounts, that would also reduce the pressure on everyone younger than that to open a social media account.&lt;/p&gt;
    &lt;p&gt;But while parents can choose the age at which their child gets a phone, no parent has full control over when their child opens social media accounts. If the child can get to the internet anywhere, including at school, she can open as many accounts as she likes as long as she’s old enough to say she’s 13.&lt;/p&gt;
    &lt;p&gt;This is why parents need help from their governments, and from the platforms (which have shown repeatedly that they will not protect children unless forced to by law). This is why the Australian law is so important: It delays the struggle over social media until the age of 16.&lt;/p&gt;
    &lt;p&gt;Any country that adds in a provision for parental consent at younger ages plunges everyone back into the collective action trap. We’re right back to “But all of my friends’ parents gave them permission!”1&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple uniform laws are more effective than a variety of complicated ones&lt;/head&gt;
    &lt;p&gt;A third consideration is that simple rules are generally best for a complex world, as legal scholars Richard Epstein and Philip Howard have long argued. People understand and remember them. They are easier to enforce. And for social media — by its nature international and placeless — a patchwork quilt of different age and parental-permission rules means that underage kids could (and many will) use a VPN to find a country in which they can easily open a social media account.&lt;/p&gt;
    &lt;p&gt;As a bonus, a simple and widespread age limit of 16 would be much easier for social media platforms to enforce effectively. They don’t want different rules across different countries. If we make things easy for them, they’ll be more effective at enforcing the law.&lt;/p&gt;
    &lt;p&gt;As an additional bonus, large majorities of parents, and adults more broadly, say in surveys that they support laws that set an age limit for opening social media accounts. See findings from the U.S., from Australia, from the UK, France, and Germany (twice). Any politician who gets out in front of this issue will find voters from right, left, and center standing up and applauding.&lt;/p&gt;
    &lt;p&gt;At the beginning of 2025, we worked with partners to establish five principles for effective phone-free school legislation. We have been thrilled to see many countries and U.S. states adopt policies that follow these recommendations. The model provided clarity about the choices to be made. Phone-free school policies have been widely successful across many jurisdictions.&lt;/p&gt;
    &lt;p&gt;So, as countries and states consider following Australia’s lead in 2026, we want to offer a similar set of features for an ideal social media age-limit policy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Four Recommended Features of Age Limit Policies&lt;/head&gt;
    &lt;head rend="h3"&gt;Feature 1: Set the minimum age at 16 or higher&lt;/head&gt;
    &lt;p&gt;As discussed previously, protecting children during puberty is essential. Social media is wildly inappropriate for children and younger adolescents. One internal Instagram study found that 11% of 13-15 year olds reported being a target of bullying, 13% reported receiving an unwanted sexual advance, 19% reported seeing unwanted sexually explicit content, and 21% reported seeing posts that made them feel worse about themselves every seven days. In a recent Pew poll, 45% of teens reported that they themselves felt they used social media too much with many suggesting that it affects their sleep and their grades. Fifteen is too young. It also undercuts the power of the norm in countries that set it to 16.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feature 2: Do not make exceptions for parental consent&lt;/head&gt;
    &lt;p&gt;Don’t make parents’ jobs even harder by giving their kids one more thing to beg for. Setting a single, clear age minimum with no loopholes does parents a favor. Imagine if every child could plead with their parents to get a drivers license at any age. Governments routinely set minimum ages for products and activities that could harm or exploit children, such as driving a car, signing up for a credit card, or drinking alcohol — social media is no different. Keep the policy simple and uniform for everyone and make parenting a little bit easier for us all.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feature 3: Focus on account creation, not access to content&lt;/head&gt;
    &lt;p&gt;Setting age-limit policies based on content (what kinds of things kids see) prompts never-ending debates about what content is inappropriate for children (e.g., what counts as too sexually explicit, how violent is too violent?, etc.). It can also lead to charges of content-based or viewpoint-based censorship. This is why we recommend orienting the law not around content but around the age at which minors can sign contracts with companies in which they agree to give away personal data and expose themselves fully to the company’s addictive algorithms, without their parents’ knowledge or consent. We think it is important to allow logged-off access to content, as Australia’s policy does. Children under 16 can still search sites such as YouTube for whatever content they want. They can easily view any video that a teacher assigns or a friend recommends. But if they do not have an account and have not signed a contract with the company, then they cannot compare the popularity of pictures of themselves, receive tailored late night notifications, be served more and more extreme content, or be contacted by strangers via messaging. Without this inappropriate business relationship and access to the extensive data they currently collect from kids, companies will find it much harder to train algorithms and use design features to manipulate and exploit kids.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feature 4: Define “social media” in terms of design features&lt;/head&gt;
    &lt;p&gt;While several platforms are currently the obvious targets of legislation because of the outsized role they play in kids’ lives, any definition of “social media” will inevitably invite challenges by companies who host different activities and have varied feature sets. By focusing on the design features that cause harm, we can capture video-game platforms that facilitate adult/minor solicitation and video-hosting platforms that maximize engagement through algorithms. Platforms that do not need these potentially harmful features will want to avoid the increased regulation and risk, and will therefore have an incentive to keep them out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;We would love to take credit for the success of phone-free school policies, and for the growing international interest in social media age-limit policies. The reality is, though, that these policies and ideas succeed because they address something that most parents, teachers, and children experience every day: the technology-facilitated manipulation of one of our most precious resources — the time and attention of our kids. Policymakers, parents, and kids themselves are fighting back. In fact, on the New York Times tech podcast Hard Fork, co-host Casey Newton offered his number-one prediction for a 2026 tech trend:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Sixteen plus becomes the new norm for social media accounts worldwide… by the end of 2026 we’ve seen at least five other democracies introduce similar rules.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Bravo to Australia, and bravo to the five (or more) countries that will turn Australia’s bold move into a new international standard. Let’s make 16+ the standard around the globe.&lt;/p&gt;
    &lt;p&gt;These parental consent laws are also necessarily complicated, because it is an inherently difficult task for a company to link two people together and obtain parental permission in a reliable way. These complications create a maze of workarounds children can use to make it seem as though they have parental permission when they do not.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.afterbabel.com/p/why-every-country-should-set-16"/><published>2026-01-14T19:53:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46622139</id><title>Show HN: Harmony – AI notetaker for Discord</title><updated>2026-01-14T22:44:23.275002+00:00</updated><content>&lt;doc fingerprint="e632041f3897b6d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Notetaker &lt;lb/&gt;for Discord&lt;/head&gt;
    &lt;head rend="h1"&gt;AI Notetaker &lt;lb/&gt;for Discord&lt;/head&gt;
    &lt;p&gt;Record, transcribe, and summarize Discord calls with AI.&lt;/p&gt;
    &lt;head rend="h1"&gt;AI Notetaker &lt;lb/&gt;for Discord&lt;/head&gt;
    &lt;p&gt;Record, transcribe, and summarize Discord calls with AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get more out of &lt;lb/&gt;each Discord call&lt;/head&gt;
    &lt;p&gt;Automated Recording &amp;amp; Joining&lt;/p&gt;
    &lt;p&gt;AI Call Transcription&lt;/p&gt;
    &lt;p&gt;Multi-Channel Support&lt;/p&gt;
    &lt;p&gt;AI Summaries&lt;/p&gt;
    &lt;p&gt;Speaker Analytics&lt;/p&gt;
    &lt;p&gt;AskHarmony Conversational Chat&lt;/p&gt;
    &lt;p&gt;Smart Search&lt;/p&gt;
    &lt;p&gt;57+ Languages&lt;/p&gt;
    &lt;p&gt;and many more...&lt;/p&gt;
    &lt;head rend="h2"&gt;Get more out of &lt;lb/&gt;each Discord call&lt;/head&gt;
    &lt;p&gt;Automated Recording &amp;amp; Joining&lt;/p&gt;
    &lt;p&gt;AI Call Transcription&lt;/p&gt;
    &lt;p&gt;Multi-Channel Support&lt;/p&gt;
    &lt;p&gt;AI Summaries&lt;/p&gt;
    &lt;p&gt;Speaker Analytics&lt;/p&gt;
    &lt;p&gt;AskHarmony Conversational Chat&lt;/p&gt;
    &lt;p&gt;Smart Search&lt;/p&gt;
    &lt;p&gt;57+ Languages&lt;/p&gt;
    &lt;p&gt;and many more...&lt;/p&gt;
    &lt;head rend="h2"&gt;Start in 2 minutes&lt;/head&gt;
    &lt;p&gt;Track all your discord calls with Harmony.&lt;/p&gt;
    &lt;p&gt;Track all your discord calls with Harmony.&lt;/p&gt;
    &lt;p&gt;Step 2 &lt;/p&gt;
    &lt;p&gt;Start Recording&lt;lb/&gt;/record - Joins the meeting&lt;lb/&gt;/stop - Stops recording&lt;/p&gt;
    &lt;p&gt;Step 2 &lt;/p&gt;
    &lt;p&gt;Start Recording&lt;lb/&gt;/record - Joins the meeting&lt;lb/&gt;/stop - Stops recording&lt;/p&gt;
    &lt;p&gt;Step 3&lt;lb/&gt;Analyze &amp;amp; Summarize Transcripts&lt;/p&gt;
    &lt;p&gt;AI summaries&lt;/p&gt;
    &lt;p&gt;Speaker analytics&lt;/p&gt;
    &lt;p&gt;AskHarmony AI&lt;/p&gt;
    &lt;p&gt;moreâ¦&lt;/p&gt;
    &lt;p&gt;Step 3&lt;lb/&gt;Analyze &amp;amp; Summarize Transcripts&lt;/p&gt;
    &lt;p&gt;AI summaries&lt;/p&gt;
    &lt;p&gt;Speaker analytics&lt;/p&gt;
    &lt;p&gt;AskHarmony AI&lt;/p&gt;
    &lt;p&gt;moreâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Start in 2 minutes&lt;/head&gt;
    &lt;p&gt;Track all your discord calls with Harmony.&lt;/p&gt;
    &lt;p&gt;Step 2 &lt;/p&gt;
    &lt;p&gt;Start Recording&lt;lb/&gt;/record - Joins the meeting&lt;lb/&gt;/stop - Stops recording&lt;/p&gt;
    &lt;p&gt;Step 3&lt;lb/&gt;Analyze &amp;amp; Summarize Transcripts&lt;/p&gt;
    &lt;p&gt;AI summaries&lt;/p&gt;
    &lt;p&gt;Speaker analytics&lt;/p&gt;
    &lt;p&gt;AskHarmony AI&lt;/p&gt;
    &lt;p&gt;moreâ¦&lt;/p&gt;
    &lt;p&gt;Testimonials&lt;/p&gt;
    &lt;p&gt;Why users&lt;/p&gt;
    &lt;p&gt;love Harmony.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Most of our team comms are on Discord and we always needed an AI note taker. Game-changer for our team and we use Harmony everyday!&lt;/p&gt;
        &lt;p&gt;Arjun Malhotra â Head of DevEx, Base10&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We switched from Zoom to Discord for standups. Only thing missing was transcription. Problem solved.&lt;/p&gt;
        &lt;p&gt;Luca De Santis â TPM, Frostbyte&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I run a 25k+ member gaming community. Before Harmony I'd forget half of what we discussed in officer meetings. Now it's all there.&lt;/p&gt;
        &lt;p&gt;Ethan Colebrook â Founder, Gypsy Squad&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I have ADHD and cannot take notes while also trying to contribute to the conversation. Harmony means I can actually be present and still have everything captured&lt;/p&gt;
        &lt;p&gt;Keiko Tanaka â Head of Product, Synapse&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Most of our team comms are on Discord and we always needed an AI note taker. Game-changer for our team and we use Harmony everyday!&lt;/p&gt;
        &lt;p&gt;Arjun Malhotra â Head of DevEx, Base10&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We switched from Zoom to Discord for standups. Only thing missing was transcription. Problem solved.&lt;/p&gt;
        &lt;p&gt;Luca De Santis â TPM, Frostbyte&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I run a 25k+ member gaming community. Before Harmony I'd forget half of what we discussed in officer meetings. Now it's all there.&lt;/p&gt;
        &lt;p&gt;Ethan Colebrook â Founder, Gypsy Squad&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I canât imagine running my day without it. The notifications are instant, and the categorization makes life much easier. A game-changer for staying organized!&lt;/p&gt;
        &lt;p&gt;Samantha Collins&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The best productivity tool Iâve used. It keeps my team on track and ensures I never miss an important message. Simple yet powerful&lt;/p&gt;
        &lt;p&gt;David Nguyen&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, a solution that feels intuitive and just works. I love the AI-tagging systemâit keeps everything in order with zero effort from me&lt;/p&gt;
        &lt;p&gt;Emily Johnson&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This app has completely transformed how I manage my emails. The real-time notifications and seamless organization save me so much time every day&lt;/p&gt;
        &lt;p&gt;Michael Reed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I canât imagine running my day without it. The notifications are instant, and the categorization makes life much easier. A game-changer for staying organized!&lt;/p&gt;
        &lt;p&gt;Samantha Collins&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The best productivity tool Iâve used. It keeps my team on track and ensures I never miss an important message. Simple yet powerful&lt;/p&gt;
        &lt;p&gt;David Nguyen&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, a solution that feels intuitive and just works. I love the AI-tagging systemâit keeps everything in order with zero effort from me&lt;/p&gt;
        &lt;p&gt;Emily Johnson&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This app has completely transformed how I manage my emails. The real-time notifications and seamless organization save me so much time every day&lt;/p&gt;
        &lt;p&gt;Michael Reed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I canât imagine running my day without it. The notifications are instant, and the categorization makes life much easier. A game-changer for staying organized!&lt;/p&gt;
        &lt;p&gt;Samantha Collins&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The best productivity tool Iâve used. It keeps my team on track and ensures I never miss an important message. Simple yet powerful&lt;/p&gt;
        &lt;p&gt;David Nguyen&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, a solution that feels intuitive and just works. I love the AI-tagging systemâit keeps everything in order with zero effort from me&lt;/p&gt;
        &lt;p&gt;Emily Johnson&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This app has completely transformed how I manage my emails. The real-time notifications and seamless organization save me so much time every day&lt;/p&gt;
        &lt;p&gt;Michael Reed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I canât imagine running my day without it. The notifications are instant, and the categorization makes life much easier. A game-changer for staying organized!&lt;/p&gt;
        &lt;p&gt;Samantha Collins&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The best productivity tool Iâve used. It keeps my team on track and ensures I never miss an important message. Simple yet powerful&lt;/p&gt;
        &lt;p&gt;David Nguyen&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, a solution that feels intuitive and just works. I love the AI-tagging systemâit keeps everything in order with zero effort from me&lt;/p&gt;
        &lt;p&gt;Emily Johnson&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This app has completely transformed how I manage my emails. The real-time notifications and seamless organization save me so much time every day&lt;/p&gt;
        &lt;p&gt;Michael Reed&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testimonials&lt;/p&gt;
    &lt;p&gt;Why users&lt;/p&gt;
    &lt;p&gt;love Harmony.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Most of our team comms are on Discord and we always needed an AI note taker. Game-changer for our team and we use Harmony everyday!&lt;/p&gt;
        &lt;p&gt;Arjun Malhotra â Head of DevEx, Base10&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We switched from Zoom to Discord for standups. Only thing missing was transcription. Problem solved.&lt;/p&gt;
        &lt;p&gt;Luca De Santis â TPM, Frostbyte&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I run a 25k+ member gaming community. Before Harmony I'd forget half of what we discussed in officer meetings. Now it's all there.&lt;/p&gt;
        &lt;p&gt;Ethan Colebrook â Founder, Gypsy Squad&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I have ADHD and cannot take notes while also trying to contribute to the conversation. Harmony means I can actually be present and still have everything captured&lt;/p&gt;
        &lt;p&gt;Keiko Tanaka â Head of Product, Synapse&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Most of our team comms are on Discord and we always needed an AI note taker. Game-changer for our team and we use Harmony everyday!&lt;/p&gt;
        &lt;p&gt;Arjun Malhotra â Head of DevEx, Base10&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We switched from Zoom to Discord for standups. Only thing missing was transcription. Problem solved.&lt;/p&gt;
        &lt;p&gt;Luca De Santis â TPM, Frostbyte&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I run a 25k+ member gaming community. Before Harmony I'd forget half of what we discussed in officer meetings. Now it's all there.&lt;/p&gt;
        &lt;p&gt;Ethan Colebrook â Founder, Gypsy Squad&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I canât imagine running my day without it. The notifications are instant, and the categorization makes life much easier. A game-changer for staying organized!&lt;/p&gt;
        &lt;p&gt;Samantha Collins&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The best productivity tool Iâve used. It keeps my team on track and ensures I never miss an important message. Simple yet powerful&lt;/p&gt;
        &lt;p&gt;David Nguyen&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, a solution that feels intuitive and just works. I love the AI-tagging systemâit keeps everything in order with zero effort from me&lt;/p&gt;
        &lt;p&gt;Emily Johnson&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This app has completely transformed how I manage my emails. The real-time notifications and seamless organization save me so much time every day&lt;/p&gt;
        &lt;p&gt;Michael Reed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I canât imagine running my day without it. The notifications are instant, and the categorization makes life much easier. A game-changer for staying organized!&lt;/p&gt;
        &lt;p&gt;Samantha Collins&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The best productivity tool Iâve used. It keeps my team on track and ensures I never miss an important message. Simple yet powerful&lt;/p&gt;
        &lt;p&gt;David Nguyen&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, a solution that feels intuitive and just works. I love the AI-tagging systemâit keeps everything in order with zero effort from me&lt;/p&gt;
        &lt;p&gt;Emily Johnson&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This app has completely transformed how I manage my emails. The real-time notifications and seamless organization save me so much time every day&lt;/p&gt;
        &lt;p&gt;Michael Reed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I canât imagine running my day without it. The notifications are instant, and the categorization makes life much easier. A game-changer for staying organized!&lt;/p&gt;
        &lt;p&gt;Samantha Collins&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The best productivity tool Iâve used. It keeps my team on track and ensures I never miss an important message. Simple yet powerful&lt;/p&gt;
        &lt;p&gt;David Nguyen&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, a solution that feels intuitive and just works. I love the AI-tagging systemâit keeps everything in order with zero effort from me&lt;/p&gt;
        &lt;p&gt;Emily Johnson&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This app has completely transformed how I manage my emails. The real-time notifications and seamless organization save me so much time every day&lt;/p&gt;
        &lt;p&gt;Michael Reed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I canât imagine running my day without it. The notifications are instant, and the categorization makes life much easier. A game-changer for staying organized!&lt;/p&gt;
        &lt;p&gt;Samantha Collins&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The best productivity tool Iâve used. It keeps my team on track and ensures I never miss an important message. Simple yet powerful&lt;/p&gt;
        &lt;p&gt;David Nguyen&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, a solution that feels intuitive and just works. I love the AI-tagging systemâit keeps everything in order with zero effort from me&lt;/p&gt;
        &lt;p&gt;Emily Johnson&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This app has completely transformed how I manage my emails. The real-time notifications and seamless organization save me so much time every day&lt;/p&gt;
        &lt;p&gt;Michael Reed&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Get more out of your&lt;lb/&gt;Discord calls&lt;/head&gt;
    &lt;p&gt;Your calls are full of decisions, insights, and action items. Start capturing them automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get more out of your&lt;lb/&gt;Discord calls&lt;/head&gt;
    &lt;p&gt;Your calls are full of decisions, insights, and action items. Start capturing them automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get more out of your&lt;lb/&gt;Discord calls&lt;/head&gt;
    &lt;p&gt;Your calls are full of decisions, insights, and action items. Start capturing them automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQs&lt;/head&gt;
    &lt;p&gt;Got questions? Weâve got the answers.&lt;lb/&gt;Contact team@caymanlabs.ai for more.&lt;/p&gt;
    &lt;p&gt;How does this work?&lt;/p&gt;
    &lt;p&gt;Does it support multiple languages?&lt;/p&gt;
    &lt;p&gt;What about privacy?&lt;/p&gt;
    &lt;p&gt;Who can see transcripts?&lt;/p&gt;
    &lt;p&gt;How does this work?&lt;/p&gt;
    &lt;p&gt;Does it support multiple languages?&lt;/p&gt;
    &lt;p&gt;What about privacy?&lt;/p&gt;
    &lt;p&gt;Who can see transcripts?&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQs&lt;/head&gt;
    &lt;p&gt;Got questions? Weâve got the answers.&lt;lb/&gt;Contact team@caymanlabs.ai for more.&lt;/p&gt;
    &lt;p&gt;How does this work?&lt;/p&gt;
    &lt;p&gt;Does it support multiple languages?&lt;/p&gt;
    &lt;p&gt;What about privacy?&lt;/p&gt;
    &lt;p&gt;Who can see transcripts?&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple pricing&lt;/head&gt;
    &lt;p&gt;Get Discord transcripts, summaries, and action itemsâautomatically.&lt;/p&gt;
    &lt;p&gt;Get Discord transcripts, summaries, and action itemsâautomatically.&lt;/p&gt;
    &lt;p&gt;Free&lt;/p&gt;
    &lt;p&gt;$0/month&lt;/p&gt;
    &lt;p&gt;All the basics in one package.&lt;/p&gt;
    &lt;p&gt;60 min of transcription&lt;/p&gt;
    &lt;p&gt;AI Summaries&lt;/p&gt;
    &lt;p&gt;Unlimited Transcript History&lt;/p&gt;
    &lt;p&gt;Unlimited Servers&lt;/p&gt;
    &lt;p&gt;Email Support&lt;/p&gt;
    &lt;p&gt;Free&lt;/p&gt;
    &lt;p&gt;$0/month&lt;/p&gt;
    &lt;p&gt;All the basics in one package.&lt;/p&gt;
    &lt;p&gt;60 min of transcription&lt;/p&gt;
    &lt;p&gt;AI Summaries&lt;/p&gt;
    &lt;p&gt;Unlimited Transcript History&lt;/p&gt;
    &lt;p&gt;Unlimited Servers&lt;/p&gt;
    &lt;p&gt;Email Support&lt;/p&gt;
    &lt;p&gt;Users choice&lt;/p&gt;
    &lt;p&gt;Pro&lt;/p&gt;
    &lt;p&gt;$10/seat&lt;/p&gt;
    &lt;p&gt;Designed to scale with you.&lt;/p&gt;
    &lt;p&gt;600 min of transcription / seat&lt;/p&gt;
    &lt;p&gt;Detailed AI Summaries&lt;/p&gt;
    &lt;p&gt;Unlimited Transcript History&lt;/p&gt;
    &lt;p&gt;Unlimited Servers&lt;/p&gt;
    &lt;p&gt;1:1 Priority Support&lt;/p&gt;
    &lt;p&gt;Users choice&lt;/p&gt;
    &lt;p&gt;Pro&lt;/p&gt;
    &lt;p&gt;$10/seat&lt;/p&gt;
    &lt;p&gt;Designed to scale with you.&lt;/p&gt;
    &lt;p&gt;600 min of transcription / seat&lt;/p&gt;
    &lt;p&gt;Detailed AI Summaries&lt;/p&gt;
    &lt;p&gt;Unlimited Transcript History&lt;/p&gt;
    &lt;p&gt;Unlimited Servers&lt;/p&gt;
    &lt;p&gt;1:1 Priority Support&lt;/p&gt;
    &lt;p&gt;Team&lt;/p&gt;
    &lt;p&gt;Custom/month&lt;/p&gt;
    &lt;p&gt;For larger teams&lt;/p&gt;
    &lt;p&gt;Everything in Pro&lt;/p&gt;
    &lt;p&gt;Unlimited Transcription&lt;/p&gt;
    &lt;p&gt;Advanced Analytics&lt;/p&gt;
    &lt;p&gt;Unlimited seats&lt;/p&gt;
    &lt;p&gt;Enterprise Support&lt;/p&gt;
    &lt;p&gt;Team&lt;/p&gt;
    &lt;p&gt;Custom/month&lt;/p&gt;
    &lt;p&gt;For larger teams&lt;/p&gt;
    &lt;p&gt;Everything in Pro&lt;/p&gt;
    &lt;p&gt;Unlimited Transcription&lt;/p&gt;
    &lt;p&gt;Advanced Analytics&lt;/p&gt;
    &lt;p&gt;Unlimited seats&lt;/p&gt;
    &lt;p&gt;Enterprise Support&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple pricing&lt;/head&gt;
    &lt;p&gt;Get Discord transcripts, summaries, and action itemsâautomatically.&lt;/p&gt;
    &lt;p&gt;Business&lt;/p&gt;
    &lt;p&gt;$100/month&lt;/p&gt;
    &lt;p&gt;Designed to scale with you.&lt;/p&gt;
    &lt;p&gt;Everything in Premium&lt;/p&gt;
    &lt;p&gt;Unlimited team members&lt;/p&gt;
    &lt;p&gt;3,000 AI credits per user&lt;/p&gt;
    &lt;p&gt;Team channel&lt;/p&gt;
    &lt;p&gt;Tool support&lt;/p&gt;
    &lt;p&gt;Users choice&lt;/p&gt;
    &lt;p&gt;Pro&lt;/p&gt;
    &lt;p&gt;$10/seat&lt;/p&gt;
    &lt;p&gt;Designed to scale with you.&lt;/p&gt;
    &lt;p&gt;600 min of transcription / seat&lt;/p&gt;
    &lt;p&gt;Detailed AI Summaries&lt;/p&gt;
    &lt;p&gt;Unlimited Transcript History&lt;/p&gt;
    &lt;p&gt;Unlimited Servers&lt;/p&gt;
    &lt;p&gt;1:1 Priority Support&lt;/p&gt;
    &lt;p&gt;Basic&lt;/p&gt;
    &lt;p&gt;$0/month&lt;/p&gt;
    &lt;p&gt;All the email basics in one package.&lt;/p&gt;
    &lt;p&gt;5 hours of transcription&lt;/p&gt;
    &lt;p&gt;AI Summaries&lt;/p&gt;
    &lt;p&gt;60-day Transcript History&lt;/p&gt;
    &lt;p&gt;Up to 1 Server&lt;/p&gt;
    &lt;p&gt;Customer Support&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://harmonynotetaker.ai/"/><published>2026-01-14T20:02:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46622328</id><title>Claude Cowork Exfiltrates Files</title><updated>2026-01-14T22:44:23.089219+00:00</updated><content>&lt;doc fingerprint="34890ebe6fffce0b"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;Claude Cowork Exfiltrates Files&lt;/head&gt;
    &lt;p&gt;Claude Cowork is vulnerable to file exfiltration attacks via indirect prompt injection as a result of known-but-unresolved isolation flaws in Claude's code execution environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context&lt;/head&gt;
    &lt;p&gt;Two days ago, Anthropic released the Claude Cowork research preview (a general-purpose AI agent to help anyone with their day-to-day work). In this article, we demonstrate how attackers can exfiltrate user files from Cowork by exploiting an unremediated vulnerability in Claudeâs coding environment, which now extends to Cowork. The vulnerability was first identified in Claude.ai chat before Cowork existed by Johann Rehberger, who disclosed the vulnerability â it was acknowledged but not remediated by Anthropic. &lt;lb/&gt;Anthropic warns users, âCowork is a research preview with unique risks due to its agentic nature and internet access.â Users are recommended to be aware of âsuspicious actions that may indicate prompt injectionâ. However, as this feature is intended for use by the general populace, not just technical users, we agree with Simon Willisonâs take:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âI do not think it is fair to tell regular non-programmer users to watch out for 'suspicious actions that may indicate prompt injectionâ!â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As Anthropic has acknowledged this risk and put it on users to âavoid granting access to local files with sensitive informationâ (while simultaneously encouraging the use of Cowork to organize your Desktop), we have chosen to publicly disclose this demonstration of a threat users should be aware of. By raising awareness, we hope to enable users to better identify the types of âsuspicious actionsâ mentioned in Anthropicâs warning.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;p&gt;This attack leverages the allowlisting of the Anthropic API to achieve data egress from Claude's VM environment (which restricts most network access).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The victim connects Cowork to a local folder containing confidential real estate files&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The victim uploads a file to Claude that contains a hidden prompt injection&lt;/p&gt;&lt;lb/&gt;For general use cases, this is quite common; a user finds a file online that they upload to Claude code. This attack is not dependent on the injection source - other injection sources include, but are not limited to: web data from Claude for Chrome, connected MCP servers, etc. In this case, the attack has the file being a Claude âSkillâ (although, as mentioned, it could also just be a regular document), as it is a generalizable file convention that users are likely to encounter, especially when using Claude.&lt;lb/&gt;Note: If you are familiar with Skills, they are canonically Markdown files (which users often do not heavily scrutinize). However, we demonstrate something more interesting: here, the user uploads a .docx (such as may be shared on an online forum), which poses as a Skill - the contents appear to be Markdown that was just saved after editing in Word. In reality, this trick allows attackers to conceal the injection using 1-point font, white-on-white text, and with line spacing set to 0.1 â making it effectively impossible to detect.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The victim asks Cowork to analyze their files using the Real Estate âskillâ they uploaded&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The injection manipulates Cowork to upload files to the attackerâs Anthropic account&lt;/p&gt;&lt;lb/&gt;The injection tells Claude to use a âcurlâ command to make a request to the Anthropic file upload API with the largest available file. The injection then provides the attackerâs API key, so the file will be uploaded to the attackerâs account.&lt;lb/&gt;At no point in this process is human approval required.&lt;p&gt;If we expand the 'Running command' block, we can see the malicious request in detail:&lt;/p&gt;&lt;p&gt;Code executed by Claude is run in a VM - restricting outbound network requests to almost all domains - but the Anthropic API flies under the radar as trusted, allowing this attack to complete successfully.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The attackerâs account contains the victim's file, allowing them to chat with it&lt;/p&gt;
        &lt;p&gt;The exfiltrated file contains financial figures and PII, including partial SSNs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;A Note on Model-specific Resilience&lt;/head&gt;
    &lt;p&gt;The above exploit was demonstrated against Claude Haiku. Although Claude Opus 4.5 is known to be more resilient against injections, Opus 4.5 in Cowork was successfully manipulated via indirect prompt injection to leverage the same file upload vulnerability to exfiltrate data in a test that considered a 'user' uploading a malicious integration guide while developing a new AI tool:&lt;/p&gt;
    &lt;p&gt;As the focus of this article was more for everyday users (and not developers), we opted to demonstrate the above attack chain instead of this one.&lt;/p&gt;
    &lt;head rend="h3"&gt;DOS via Malformed Files&lt;/head&gt;
    &lt;p&gt;An interesting finding: Claude's API struggles when a file does not match the type it claims to be. When operating on a malformed PDF (ends .pdf, but it is really a text file with a few sentences in it), after trying to read it once, Claude starts throwing an API error in every subsequent chat in the conversation.&lt;/p&gt;
    &lt;p&gt;We posit that it is likely possible to exploit this failure via indirect prompt injection to cause a limited denial of service attack (e.g., an injection can elicit Claude to create a malformed file, and then read it). Uploading the malformed file via the files API resulted in notifications with an error message, both in the Claude client and the Anthropic Console.&lt;/p&gt;
    &lt;head rend="h3"&gt;Agentic Blast Radius&lt;/head&gt;
    &lt;p&gt;One of the key capabilities that Cowork was created for is the ability to interact with one's entire day-to-day work environment. This includes the browser and MCP servers, granting capabilities like sending texts, controlling one's Mac with AppleScripts, etc. &lt;lb/&gt;These functionalities make it increasingly likely that the model will process both sensitive and untrusted data sources (which the user does not review manually for injections), making prompt injection an ever-growing attack surface. We urge users to exercise caution when configuring Connectors. Though this article demonstrated an exploit without leveraging Connectors, we believe they represent a major risk surface likely to impact everyday users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files"/><published>2026-01-14T20:12:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46623195</id><title>The Influentists: AI hype without proof</title><updated>2026-01-14T22:44:22.259119+00:00</updated><content>&lt;doc fingerprint="3f41870120e224f7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Influentists&lt;/head&gt;
    &lt;p&gt;· 5 min read&lt;/p&gt;
    &lt;p&gt;Last week, the developer community was busy discussing about a single tweet:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;I'm not joking and this isn't funny. We have been trying to build distributed agent orchestrators at Google since last year. There are various options, not everyone is aligned... I gave Claude Code a description of the problem, it generated what we built last year in an hour.&lt;/p&gt;— Jaana Dogan ヤナ ドガン (@rakyll) January 2, 2026&lt;/quote&gt;
    &lt;p&gt;The author is Jaana Dogan (known as Rakyll), a highly respected figure in the Google ecosystem, in the open-source world, and in my heart (thank you Rakyll for your great Go blog posts).&lt;/p&gt;
    &lt;p&gt;At first glance, the tweet suggests an enormous shift in the software industry: the ability to build in just one hour what previously required weeks or months for a team of sofware engineers, using just the description of the problem. The tweet was too-much dramatic in my own opinion, but actually impressive!&lt;/p&gt;
    &lt;p&gt;The post triggered an immediate wave of “doom-posting,” with many fearing for the future of software engineering (as each week since a year now). However, as the conversation reached a high number of replies and citations on social networks, Rakyll released a follow-up thread to provide context:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;To cut through the noise on this topic, it’s helpful to provide more more context:&lt;/p&gt;— Jaana Dogan ヤナ ドガン (@rakyll) January 4, 2026&lt;lb/&gt;- We have built several versions of this system last year. - There are tradeoffs and there hasn't been a clear winner.&lt;lb/&gt;- When prompted with the best ideas that survived, coding agents are able to… https://t.co/k5FvAah7yc&lt;/quote&gt;
    &lt;p&gt;This response thread revealed a story far less miraculous than the original tweet suggested. Let’s analyze it.&lt;/p&gt;
    &lt;p&gt;Crucially, the foundational “thinking” had already been performed by Rakyll herself, who guided the AI using architectural concepts (honed over several weeks or months of prior effort) rather than the AI thinking and inventing the “product” from scratch.&lt;lb/&gt; Furthermore, the resulting project was strictly a proof-of-concept that falls far short of a production-ready system capable of managing real-world complexity.&lt;lb/&gt; And finally, this success hinged on the Rakyll’s implicit domain knowledge and deep expertise. The last point is often (strategically?) omitted from these “magic” viral demonstrations in order to make the tool appear way more autonomous than it truly is.&lt;/p&gt;
    &lt;p&gt;Hmm. Now, this is far less exciting…&lt;/p&gt;
    &lt;head rend="h2"&gt;Under influence #&lt;/head&gt;
    &lt;p&gt;This pattern of “hype first and context later” is actually part of a growing trend.&lt;/p&gt;
    &lt;p&gt;I call the individuals participating to that trend “The Influentists”. Those people are members of a scientific or technical community, and leverage their large audiences to propagate claims that are, at best, unproven and, at worst, intentionally misleading.&lt;/p&gt;
    &lt;p&gt;But how can we spot them?&lt;/p&gt;
    &lt;p&gt;I personally identify these “Influentists” by four personality traits that characterize their public discourse.&lt;lb/&gt; The first is a reliance on "trust-me-bro" culture, where anecdotal experiences are framed as universal, objective truths to generate hype. This is a sentiment perfectly captured by the “I’m not joking and this isn’t funny” tone of Rakyll’s original tweet, but also the dramatic “I’ve never felt that much behind as a programmer” from Andrej Karpathy’s tweet. This is supported by an absence of reproducible proof, as these individuals rarely share the code, data, or methodology behind their viral “wins”, an omission made easier than ever in the current LLM era. And finally, they utilize strategic ambiguity, carefully wording their claims with enough vagueness to pivot toward a “clarification” if the technical community challenges their accuracy.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;I've never felt this much behind as a programmer. The profession is being dramatically refactored as the bits contributed by the programmer are increasingly sparse and between. I have a sense that I could be 10X more powerful if I just properly string together what has become…&lt;/p&gt;— Andrej Karpathy (@karpathy) December 26, 2025&lt;/quote&gt;
    &lt;head rend="h2"&gt;A Growing Pattern #&lt;/head&gt;
    &lt;p&gt;Rakyll is far from alone. We see this “hype-first” approach across major AI firms like Anthropic, OpenAI, or Microsoft.&lt;/p&gt;
    &lt;p&gt;Consider Galen Hunt, a Distinguished Engineer at Microsoft. He recently made waves by claiming a goal to rewrite Microsoft’s massive C/C++ codebases into Rust by 2030 using AI.&lt;/p&gt;
    &lt;p&gt;When the industry pointed out the near-impossible complexity of this task, but also asking clarity for popular and critical products like Microsoft Windows, he was forced to clarify that it was only a “research project”.&lt;/p&gt;
    &lt;p&gt;Similarly, engineers from Anthropic and OpenAI oftenly post teasers about “AGI being achieved internally” to release months later models that disappoint the crowd.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Wait, can we consider seriously the hypothesis that 1) the recent hyped tweets from OA's staff&lt;/p&gt;— Siméon (@Simeon_Cps) September 24, 2023&lt;lb/&gt;2) "AGI has been achieved internally"&lt;lb/&gt;3) sama's comments on the qualification of slow or fast takeoff hinging on the date you count from&lt;lb/&gt;4) sama's comments on 10000x researchers&lt;lb/&gt;are… https://t.co/f57g7dXMhM pic.twitter.com/Gap3V7VqkK&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;Liam, I have been a professional programmer for 36 years. I spent 11 years at Google, where I ended up as a Staff Software Engineer, and now work at Anthropic. I've worked with some incredible people - you might have heard of Jaegeuk Kim or Ted Ts'o - and some ridiculously… https://t.co/Ku8agTrps3&lt;/p&gt;— Paul Crowley (@ciphergoth) December 31, 2025&lt;/quote&gt;
    &lt;p&gt;Similarly, many other companies lie over what they are solving or willing to solve:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost of Unchecked Influence #&lt;/head&gt;
    &lt;p&gt;When leaders at major labs propagate these hyped-based results, it can create a “technical debt of expectations” for the rest of us. Junior developers see these viral threads and feel they are failing because they can’t reproduce a year of work in an hour, not realizing the “magic” was actually a highly-curated prototype guided by a decade of hidden expertise.&lt;/p&gt;
    &lt;p&gt;We must stop granting automatic authority to those who rely on hype, or vibes, rather than evidence.&lt;lb/&gt; If a tool or methodology were truly as revolutionary as claimed, then it wouldn’t need a viral thread to prove its worth because the results would speak for themselves.&lt;/p&gt;
    &lt;p&gt;The tech community must shift its admiration back toward reproducible results and away from this “trust-me-bro” culture.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://carette.xyz/posts/influentists/"/><published>2026-01-14T20:54:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46623761</id><title>Sun Position Calculator</title><updated>2026-01-14T22:44:21.817077+00:00</updated><content/><link href="https://drajmarsh.bitbucket.io/earthsun.html"/><published>2026-01-14T21:26:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624352</id><title>The State of OpenSSL for pyca/cryptography</title><updated>2026-01-14T22:44:21.493157+00:00</updated><content>&lt;doc fingerprint="f2d8f89a348b06d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The State of OpenSSL for &lt;code&gt;pyca/cryptography&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Published: January 14, 2026&lt;/p&gt;
    &lt;p&gt;For the past 12 years, we (Paul Kehrer and Alex Gaynor) have maintained the Python &lt;code&gt;cryptography&lt;/code&gt; library (also known as &lt;code&gt;pyca/cryptography&lt;/code&gt; or cryptography.io). For that entire period, we’ve relied on OpenSSL to provide core cryptographic algorithms. This past October, we gave a talk at the OpenSSL Conference describing our experiences. This talk focuses on the growing problems we have with OpenSSL’s direction. The mistakes we see in OpenSSL’s development have become so significant that we believe substantial changes are required — either to OpenSSL, or to our reliance on it.&lt;/p&gt;
    &lt;p&gt;Fundamentally, OpenSSL’s trajectory can be understood as a play in three acts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In the pre-Heartbleed era (pre-2014), OpenSSL was under-maintained and languishing, substantially lagging behind expectations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the immediate post-Heartbleed era, OpenSSL’s maintenance was reinvigorated and it made substantial progress and improvements. It grew a real code review process, began running tests in CI, adopted fuzz testing, and matured its release process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, in 2021 OpenSSL 3 was released. OpenSSL 3 introduced new APIs and had large internal refactors. Relative to previous OpenSSL versions, OpenSSL 3 had significant regressions in performance, complexity, API ergonomics, and didn’t make needed improvements in areas like testing, verification, and memory safety. Over the same period, OpenSSL’s forks have all made progress in these areas. Many of our concerns about OpenSSL’s direction in this time have substantial overlap with those highlighted by HAProxy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The remainder of this post describes the problems we have with OpenSSL in more detail, and concludes with the changes we are making to our own policies in response. To avoid burying the lede, we intend to pursue several approaches to reducing our reliance on OpenSSL.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;Compared to OpenSSL 1.1.1, OpenSSL 3 has significant performance regressions in areas such as parsing and key loading.&lt;/p&gt;
    &lt;p&gt;Several years ago, we filed a bug reporting that elliptic curve public key loading had regressed 5-8x between OpenSSL 1.1.1 and 3.0.7. The reason we had noticed this is that performance had gotten so bad that we’d seen it in our test suite runtimes. Since then, OpenSSL has improved performance such that it’s only 3x slower than it used to be. But more significantly, the response to the issue was that, ‘regression was expected with OpenSSL 3, and while there might be some optimizations, we shouldn’t expect it to ever get back to 1.1.1 levels’. Performance regressions can be acceptable, and even appropriate, when they improve other areas of the library, however as we’ll describe, the cause of these regressions has been other mistakes, and not offsetting improvements.&lt;/p&gt;
    &lt;p&gt;As a result of these sorts of regressions, when &lt;code&gt;pyca/cryptography&lt;/code&gt; migrated X.509 certificate parsing from OpenSSL to our own Rust code, we got a 10x performance improvement relative to OpenSSL 3 (n.b., some of this improvement is attributable to advantages in our own code, but much is explainable by the OpenSSL 3 regressions). Later, moving public key parsing to our own Rust code made end-to-end X.509 path validation 60% faster — just improving key loading led to a 60% end-to-end improvement, that’s how extreme the overhead of key parsing in OpenSSL was.&lt;/p&gt;
    &lt;p&gt;The fact that we are able to achieve better performance doing our own parsing makes clear that doing better is practical. And indeed, our performance is not a result of clever SIMD micro-optimizations, it’s the result of doing simple things that work: we avoid copies, allocations, hash tables, indirect calls, and locks — none of which should be required for parsing basic DER structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complexity and APIs&lt;/head&gt;
    &lt;p&gt;OpenSSL 3 started the process of substantially changing its APIs — it introduced &lt;code&gt;OSSL_PARAM&lt;/code&gt; and has been using those for all new API surfaces (including those for post-quantum cryptographic algorithms). In short, &lt;code&gt;OSSL_PARAM&lt;/code&gt; works by passing arrays of key-value pairs to functions, instead of normal argument passing. This reduces performance, reduces compile-time verification, increases verbosity, and makes code less readable. To the extent there is an argument in favor of it, we infer that the benefit is that it allows OpenSSL to use the same API (and ABI) for different algorithms with different parameters, allowing things like reading algorithm parameters from configuration files with generic configuration parsing code that doesn’t need to be updated when new algorithms are added to OpenSSL.&lt;/p&gt;
    &lt;p&gt;For a concrete comparison of the verbosity, performing an ML-KEM encapsulation with OpenSSL takes 37 lines with 6 fallible function calls. Doing so with BoringSSL takes 19 lines with 3 fallible function calls.&lt;/p&gt;
    &lt;p&gt;In addition to making public APIs more frustrating and error prone to use, OpenSSL internals have also become more complex. For example, in order to make managing arrays of &lt;code&gt;OSSL_PARAM&lt;/code&gt; palatable, many OpenSSL source files are no longer simply C files, they now have a custom Perl preprocessor for their C code.&lt;/p&gt;
    &lt;p&gt;OpenSSL also 3 introduced the notion of “providers” (obsoleting, but not replacing, the previous ENGINE APIs), which allow for external implementations of algorithms (including algorithms provided by OpenSSL itself). This was the source of innumerable performance regressions, due to poorly designed APIs. In particular, OpenSSL allowed replacing any algorithm at any point in program execution, which necessitated adding innumerable allocations and locks to nearly every operation. To mitigate this, OpenSSL then added more caches, and ultimately RCU (Read-Copy-Update) — a complex memory management strategy which had difficult to diagnose bugs.&lt;/p&gt;
    &lt;p&gt;From our perspective, this is a cycle of compounding bad decisions: the providers API was incorrectly designed (there is no need to be able to redefine SHA-256 at arbitrary points in program execution) leading to performance regressions. This led to additional complexity to mitigate those regressions in the form of caching and RCU, which in term led to more bugs. And after all that, performance was still worse than it had been at the beginning.&lt;/p&gt;
    &lt;p&gt;Finally, taking an OpenSSL public API and attempting to trace the implementation to see how it is implemented has become an exercise in self-flagellation. Being able to read the source to understand how something works is important both as part of self-improvement in software engineering, but also because as sophisticated consumers there are inevitably things about how an implementation works that aren’t documented, and reading the source gives you ground truth. The number of indirect calls, optional paths, &lt;code&gt;#ifdef&lt;/code&gt;, and other obstacles to comprehension is astounding. We cannot overstate the extent to which just reading the OpenSSL source code has become miserable — in a way that both wasn’t true previously, and isn’t true in LibreSSL, BoringSSL, or AWS-LC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing and Verification&lt;/head&gt;
    &lt;p&gt;We joke that the Python Cryptographic Authority is a CI engineering project that incidentally produces a cryptography library. The joke reflects our real belief that investment in testing and automation enables Pareto improvements in development speed and correctness — to the point that it can make other work look trivial.&lt;/p&gt;
    &lt;p&gt;The OpenSSL project does not sufficiently prioritize testing. While OpenSSL’s testing has improved substantially since the pre-Heartbleed era there are quite significant gaps. The gaps in OpenSSL’s test coverage were acutely visible during the OpenSSL 3.0 development cycle — where the project was extremely reliant on the community to report regressions experienced during the extended alpha and beta period (covering 19 pre-releases over the course of 16 months), because their own tests were insufficient to catch unintended real-world breakages. Despite the known gaps in OpenSSL’s test coverage, it’s still common for bug fixes to land without an accompanying regression test.&lt;/p&gt;
    &lt;p&gt;OpenSSL’s CI is exceptionally flaky, and the OpenSSL project has grown to tolerate this flakiness, which masks serious bugs. OpenSSL 3.0.4 contained a critical buffer overflow in the RSA implementation on AVX-512-capable CPUs. This bug was actually caught by CI — but because the crash only occurred when the CI runner happened to have an AVX-512 CPU (not all did), the failures were apparently dismissed as flakiness. Three years later, the project still merges code with failing tests: the day we prepared our conference slides, five of ten recent commits had failing CI checks, and the day before we delivered the talk, every single commit had failing cross-compilation builds.&lt;/p&gt;
    &lt;p&gt;This incident also speaks to the value of adopting tools like Intel SDE, which allows controlled testing against CPUs with different subsets of x86-64 extension instructions. Using Intel SDE to have dedicated test jobs with and without AVX-512 would have made the nature of the failure immediately legible and reproducible.&lt;/p&gt;
    &lt;p&gt;OpenSSL is not keeping pace with the state of the art in formal verification. Formal methods have gone from academic novelty to practical reality for meaningful chunks of cryptographic code. BoringSSL and AWS-LC have incorporated formally verified implementations and use automated reasoning to increase assurance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Safety&lt;/head&gt;
    &lt;p&gt;At the time OpenSSL was created, there were no meaningful programming languages that meaningfully provided performance, embeddability, and memory safety — if you wanted a memory safe language, you were committing to giving up performance and adding a garbage collector.&lt;/p&gt;
    &lt;p&gt;The world has changed. Nearly 5 years ago, &lt;code&gt;pyca/cryptography&lt;/code&gt; issued our first release incorporating Rust code, and since then we have migrated nearly all functionality to Rust, using a mix of pure-Rust for all parsing and X.509 operations combined with using OpenSSL for providing cryptographic algorithms — gaining performance wins and avoiding several OpenSSL CVEs. We know these transitions are possible.&lt;/p&gt;
    &lt;p&gt;A library committed to security needs to make a long-term commitment to a migration to a memory safe programming language. OpenSSL has shown no initiative at all on this issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing Causes&lt;/head&gt;
    &lt;p&gt;Whenever issues with an open source project are raised, many will suggest this is an issue of funding or tragedy of the commons. This is inapposite, in the past decade, post-Heartbleed, OpenSSL has received considerable funding, and at this moment the OpenSSL Corporation and Foundation employ more software engineers than work full time on either BoringSSL or LibreSSL. The problems we have described are not ones caused by underfunding.&lt;/p&gt;
    &lt;p&gt;We do not fully understand the motivations that led to the public APIs and internal complexity we’ve described here. We’ve done our best to reverse engineer them by asking “what would motivate someone to do this” and often we’ve found ourselves coming up short. The fact that none of the other OpenSSL forks have made these same design choices is informative to the question of “was this necessary”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Directions&lt;/head&gt;
    &lt;p&gt;Our experience with OpenSSL has been on a negative trajectory for several years. As a result of these issues, we are making the following changes to our (admittedly undocumented) policies.&lt;/p&gt;
    &lt;p&gt;First, we will no longer require OpenSSL implementations for new functionality. Where we deem it desirable, we will add new APIs that are only on LibreSSL/BoringSSL/AWS-LC. Concretely, we expect to add ML-KEM and ML-DSA APIs that are only available with LibreSSL/BoringSSL/AWS-LC, and not with OpenSSL.&lt;/p&gt;
    &lt;p&gt;Second, we currently statically link a copy of OpenSSL in our wheels (binary artifacts). We are beginning the process of looking into what would be required to change our wheels to link against one of the OpenSSL forks.&lt;/p&gt;
    &lt;p&gt;If we are able to successfully switch to one of OpenSSL’s forks for our binary wheels, we will begin considering the circumstances under which we would drop support for OpenSSL entirely.&lt;/p&gt;
    &lt;p&gt;Lastly, in the long term, we are actively tracking non-OpenSSL derived cryptography libraries such as Graviola as potential alternatives.&lt;/p&gt;
    &lt;p&gt;We recognize that changes in which libraries we use to provide cryptographic implementations have substantial impact on our users — particularly redistributors. We do not contemplate these steps lightly, nor do we anticipate making them hastily. However, due to the gravity of our concerns, we are compelled to act. If you rely on &lt;code&gt;pyca/cryptography&lt;/code&gt;’s support for OpenSSL, the best way to avoid the most drastic steps contemplated here is to engage with the OpenSSL project and contribute to improvements on these axes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cryptography.io/en/latest/statements/state-of-openssl/"/><published>2026-01-14T22:04:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624541</id><title>Scaling long-running autonomous coding</title><updated>2026-01-14T22:44:21.045078+00:00</updated><content>&lt;doc fingerprint="9ff1b067ed89904f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Scaling long-running autonomous coding&lt;/head&gt;
    &lt;p&gt;We've been experimenting with running coding agents autonomously for weeks.&lt;/p&gt;
    &lt;p&gt;Our goal is to understand how far we can push the frontier of agentic coding for projects that typically take human teams months to complete.&lt;/p&gt;
    &lt;p&gt;This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;The limits of a single agent&lt;/head&gt;
    &lt;p&gt;Today's agents work well for focused tasks, but are slow for complex projects. The natural next step is to run multiple agents in parallel, but figuring out how to coordinate them is challenging.&lt;/p&gt;
    &lt;p&gt;Our first instinct was that planning ahead would be too rigid. The path through a large project is ambiguous, and the right division of work isn't obvious at the start. We began with dynamic coordination, where agents decide what to do based on what others are currently doing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning to coordinate&lt;/head&gt;
    &lt;p&gt;Our initial approach gave agents equal status and let them self-coordinate through a shared file. Each agent would check what others were doing, claim a task, and update its status. To prevent two agents from grabbing the same task, we used a locking mechanism.&lt;/p&gt;
    &lt;p&gt;This failed in interesting ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Agents would hold locks for too long, or forget to release them entirely. Even when locking worked correctly, it became a bottleneck. Twenty agents would slow down to the effective throughput of two or three, with most time spent waiting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The system was brittle: agents could fail while holding locks, try to acquire locks they already held, or update the coordination file without acquiring the lock at all.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We tried replacing locks with optimistic concurrency control. Agents could read state freely, but writes would fail if the state had changed since they last read it. This was simpler and more robust, but there were still deeper problems.&lt;/p&gt;
    &lt;p&gt;With no hierarchy, agents became risk-averse. They avoided difficult tasks and made small, safe changes instead. No agent took responsibility for hard problems or end-to-end implementation. This lead to work churning for long periods of time without progress.&lt;/p&gt;
    &lt;head rend="h2"&gt;Planners and workers&lt;/head&gt;
    &lt;p&gt;Our next approach was to separate roles. Instead of a flat structure where every agent does everything, we created a pipeline with distinct responsibilities.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Planners continuously explore the codebase and create tasks. They can spawn sub-planners for specific areas, making planning itself parallel and recursive.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Workers pick up tasks and focus entirely on completing them. They don't coordinate with other workers or worry about the big picture. They just grind on their assigned task until it's done, then push their changes.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At the end of each cycle, a judge agent determined whether to continue, then the next iteration would start fresh. This solved most of our coordination problems and let us scale to very large projects without any single agent getting tunnel vision.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running for weeks&lt;/head&gt;
    &lt;p&gt;To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.&lt;/p&gt;
    &lt;p&gt;Despite the codebase size, new agents can still understand it and make meaningful progress. Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts.&lt;/p&gt;
    &lt;p&gt;While it might seem like a simple screenshot, building a browser from scratch is extremely difficult.&lt;/p&gt;
    &lt;p&gt;Another experiment was doing an in-place migration of Solid to React in the Cursor codebase. It took over 3 weeks with +266K/-193K edits. As we've started to test the changes, we do believe it's possible to merge this change.&lt;/p&gt;
    &lt;p&gt;Another experiment was to improve an upcoming product. A long-running agent made video rendering 25x faster with an efficient Rust version. It also added support to zoom and pan smoothly with natural spring transitions and motion blurs, following the cursor. This code was merged and will be in production soon.&lt;/p&gt;
    &lt;p&gt;We have a few other interesting examples still running:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java LSP: 7.4K commits, 550K LoC&lt;/item&gt;
      &lt;item&gt;Windows 7 emulator: 14.6K commits, 1.2M LoC&lt;/item&gt;
      &lt;item&gt;Excel: 12K commits, 1.6M LoC&lt;/item&gt;
      &lt;item&gt;FX1: 9.5K commits, 1.2M LoC&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What we've learned&lt;/head&gt;
    &lt;p&gt;We've deployed billions of tokens across these agents toward a single goal. The system isn't perfectly efficient, but it's far more effective than we expected.&lt;/p&gt;
    &lt;p&gt;Model choice matters for extremely long-running tasks. We found that GPT-5.2 models are much better at extended autonomous work: following instructions, keeping focus, avoiding drift, and implementing things precisely and completely.&lt;/p&gt;
    &lt;p&gt;Opus 4.5 tends to stop earlier and take shortcuts when convenient, yielding back control quickly. We also found that different models excel at different roles. GPT-5.2 is a better planner than GPT-5.1-codex, even though the latter is trained specifically for coding. We now use the model best suited for each role rather than one universal model.&lt;/p&gt;
    &lt;p&gt;Many of our improvements came from removing complexity rather than adding it. We initially built an integrator role for quality control and conflict resolution, but found it created more bottlenecks than it solved. Workers were already capable of handling conflicts themselves.&lt;/p&gt;
    &lt;p&gt;The best system is often simpler than you'd expect. We initially tried to model systems from distributed computing and organizational design. However, not all of them work for agents.&lt;/p&gt;
    &lt;p&gt;The right amount of structure is somewhere in the middle. Too little structure and agents conflict, duplicate work, and drift. Too much structure creates fragility.&lt;/p&gt;
    &lt;p&gt;A surprising amount of the system's behavior comes down to how we prompt the agents. Getting them to coordinate well, avoid pathological behaviors, and maintain focus over long periods required extensive experimentation. The harness and models matter, but the prompts matter more.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;Multi-agent coordination remains a hard problem. Our current system works, but we're nowhere near optimal. Planners should wake up when their tasks complete to plan the next step. Agents occasionally run for far too long. We still need periodic fresh starts to combat drift and tunnel vision.&lt;/p&gt;
    &lt;p&gt;But the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected. Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.&lt;/p&gt;
    &lt;p&gt;The techniques we're developing here will eventually inform Cursor's agent capabilities. If you're interested in working on the hardest problems in AI-assisted software development, we'd love to hear from you at hiring@cursor.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/scaling-agents"/><published>2026-01-14T22:18:04+00:00</published></entry></feed>