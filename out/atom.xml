<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-07T16:11:02.620602+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46173825</id><title>HTML as an Accessible Format for Papers (2023)</title><updated>2025-12-07T16:11:14.536734+00:00</updated><content>&lt;doc fingerprint="e3d1ea5238d7dc4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;HTML as an accessible format for papers&lt;/head&gt;
    &lt;p&gt;Accessibility barriers in research are not new, but they are urgent. The message we have heard from our community is that arXiv can have the most impact in the shortest time by offering HTML papers alongside the existing PDF.&lt;/p&gt;
    &lt;p&gt;arXiv has successfully launched papers in HTML format. We are gradually backfilling HTML for arXiv's corpus of over 2 million papers over time. Not every paper can be successfully converted, so a small percentage of papers will not have an HTML version. We will work to improve conversion over time.&lt;/p&gt;
    &lt;p&gt;The link to the HTML format will appear on abstract pages below the existing PDF download link. Authors will have the opportunity to preview their paperâs HTML as a part of the submission process.&lt;/p&gt;
    &lt;p&gt;The beta rollout is just the beginning. We have a long way to go to improve HTML papers and will continue to solicit feedback from authors, readers, and the entire arXiv community to improve conversions from LaTeX.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why "experimental" HTML?&lt;/head&gt;
    &lt;p&gt;Did you know that 90% of submissions to arXiv are in TeX format, mostly LaTeX? That poses a unique accessibility challenge: to accurately convert from TeXâa very extensible language used in myriad unique ways by authorsâto HTML, a language that is much more accessible to screen readers and text-to-speech software, screen magnifiers, and mobile devices. In addition to the technical challenges, the conversion must be both rapid and automated in order to maintain arXivâs core service of free and fast dissemination.&lt;/p&gt;
    &lt;p&gt;Because of these challenges we know there will be some conversion and rendering issues. We have decided to launch in beta with âexperimentalâ HTML because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accessible papers are needed now. We have talked to the arXiv community, especially researchers with accessibility needs, and they overwhelmingly asked us not to wait.&lt;/item&gt;
      &lt;item&gt;We need your help. The obvious work is done. Reports from the community will help us identify issues we can track back to specific LaTeX packages that are not converting correctly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Error messages you may see in HTML papers&lt;/head&gt;
    &lt;p&gt;HTML papers on arXiv.org are a work in progress and will sometimes display errors. As we work to improve accessibility we share with you the causes of these errors and what authors can do to help minimize them. Learn more about error messages you may see in HTML papers&lt;/p&gt;
    &lt;head rend="h2"&gt;Ways to help&lt;/head&gt;
    &lt;head rend="h3"&gt;1) Read HTML papers and report issues&lt;/head&gt;
    &lt;p&gt;We encourage the community to try out HTML papers in your field:&lt;/p&gt;
    &lt;head rend="h4"&gt;Report an issue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the abstract page for a paper you are interested in reading.&lt;/item&gt;
      &lt;item&gt;Look in the section where you find the link to the PDF download, and click the new link for HTML.&lt;/item&gt;
      &lt;item&gt;Report issues by either a) clicking on the Open Issue button b) selecting text and clicking on the Open Issue for Selection button or c) use &lt;code&gt;Ctrl+?&lt;/code&gt;on your keyboard. If you are using a screen reader, use&lt;code&gt;Alt+y&lt;/code&gt;to toggle accessible reporting buttons per paragraph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please do not create reports that the HTML paper doesn't look exactly like the PDF paper&lt;/p&gt;
    &lt;p&gt;Our primary goal for this project is to make papers more accessible, so the focus during the beta phase will value function over form. HTML layouts that are incorrect or are illegible are important to report. But we do expect the HTML papers to present differently than the same paper rendered in PDF. Line breaks will occur in different places and there is likely to be more white space. In general, the HTML paper won't present as compactly. Intricate typographic layouts will not be rendered so intricately. This is by design.&lt;/p&gt;
    &lt;p&gt;HTML is a different medium and brings its own advantages versus PDF. In addition to being much more compatible with assistive technologies, HTML does a far better job adapting to the characteristics of the device you are reading on, including mobile devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;2) Help improve the conversion from LaTeX&lt;/head&gt;
    &lt;p&gt;If you are an author you can help us improve conversions to HTML by following our guide to LaTeX Markup Best Practices for Successful HTML Papers.&lt;/p&gt;
    &lt;p&gt;If you are a developer and have free development cycles, help us improve conversions! Our collaborators at LaTeXML maintain a list of issues and welcome feedback and developer contributions.&lt;/p&gt;
    &lt;p&gt;If you are a publisher, member of a society, or conference organizer you can help us improve conversions to HTML by reviewing the .cls files your organization recommends to authors for unsupported packages. Providing .cls files that use supported packages is an easy way to support and sow accessibility in the scientific community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you to our collaborators&lt;/head&gt;
    &lt;p&gt;First, we want to share a special thank you to all the scientists with disabilities who have generously shared their insights, expertise, and guidance throughout this project.&lt;/p&gt;
    &lt;p&gt;We want to thank two organizations without which HTML papers on arXiv would not be possible: The LaTeX Project, and the LaTeXML team from NIST. We deeply thank each member of these teams for their knowledge, incredible work, and commitment to accessibility.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://info.arxiv.org/about/accessible_HTML.html"/><published>2025-12-06T14:59:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46175112</id><title>Perl's decline was cultural</title><updated>2025-12-07T16:11:13.705234+00:00</updated><content>&lt;doc fingerprint="4418d15d9ae08ffd"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt; Perl's decline was cultural 2025-11-20 &lt;/head&gt;
    &lt;head rend="h3"&gt;According to the Discourse, somebody killed perl&lt;/head&gt;
    &lt;p&gt;There's been a flurry of discussion on Hacker News and other tech forums about what killed Perl. I wrote a lot of Perl in the mid 90s and subsequently worked on some of the most trafficked sites on the web in mod_perl in the early 2000s, so I have some thoughts. My take: it was mostly baked into the culture. Perl grew amongst a reactionary community with conservative values, which prevented it from evolving into a mature general purpose language ecosystem. Everything else filled the gap.&lt;/p&gt;
    &lt;head rend="h3"&gt;I remember Perl&lt;/head&gt;
    &lt;p&gt;Something to keep in mind, is that although this is my personal take, and therefore entirely an opinion piece, I was there at the time. I stopped doing Perl properly when I left Amazon, I think this would have been around 2005. It's based on the first hand impressions of somebody who was very deeply involved in Perl in its heyday, and moved on. I have a lot of experience, from both inside and outside the tent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Perl's roots are sysadmin&lt;/head&gt;
    &lt;p&gt;What culture? Perl always had a significant amount of what you might call "BOFH" culture, which came from its old UNIX sysadmin roots. All of those passive aggressive idioms and in jokes like "RTFM", "lusers", "wizards", "asking for help the wrong way" etc. None of this is literally serious, but it does encode and inform social norms that are essentially tribal and introverted. There implicitly is a privileged population, with a cost of entry to join. Dues must be paid. Cultural conservatism as a first principle.&lt;/p&gt;
    &lt;p&gt;This stems from the old locked-down data centre command culture. When computer resource was expensive, centralised, fragile, and manually operated, it was rigidly maintained by gatekeepers, defending against inappropriate use. I started my career as an apprentice programmer at the very end of this era, (late 80s) pre-web, and before microcomputers had made much inroads, and this really was the prevailing view from inside the fort. (This is a drawback about fort-building. Once you live in a fort, it's slightly too easy to develop a siege mentality). Computers are special, users are inconvenient, disruption is the main enemy.&lt;/p&gt;
    &lt;p&gt;An unfortunate feedback loop in this kind of "perilous" environment is that it easily turns prideful. It's difficult to thrive here, if you survive and do well you are skilled; you've performed feats; you should mark your rites of passage. This can become a dangerous culture trap. If you're not careful about it, you may start to think of the hazards and difficulties, the "foot guns", as necessary features - they teach you those essential survival skills that mark you out. More unkindly, they keep the stupid folk out, and help preserve the high status of those who survived long enough to be assimilated. Uh-oh, now you've invented class politics.&lt;/p&gt;
    &lt;p&gt;The problem with this thinking is that it's self-reinforcing. Working hard to master system complexities was genuinely rewarding - you really were doing difficult things and doing them well. This is actually the same mechanism behind what eventually became known as 'meritocracy'1, but the core point is simpler - if difficulty itself becomes a badge of honour, you've created a trap: anything that makes the system more approachable starts to feel like it's cheapening what you achieved. You become invested in preserving the barriers you overcame.&lt;/p&gt;
    &lt;p&gt;(This is the same mentality that built leetcode interview pipelines BTW, but let's leave that sidebar alone for now)&lt;/p&gt;
    &lt;p&gt;So the UNIX operator culture tended to operate as a tribal meritocracy (as opposed to the UNIX implementer culture, which fell out of a different set of cultural norms, quite an interesting side bar itself2), a cultural priesthood, somewhat self-regarding, rewarding of cleverness and knowledge hoarding, prone to feats of bravado, full of lore, with a defensive mentality of keeping the flame aloft, keeping the plebs happy and fed, and warding off the barbarians. As we entered the 90s it was already gently in decline, because centralised computing was giving way to the rise of the microcomputer, but the sudden explosive growth of the WWW pulled internet / Unix culture suddenly back into the mainstream with an enormous and public opportunity vacuum. Everyone suddenly has an urgent need to write programs that push text off UNIX file-systems (and databases) and into web pages, and Perl is uniquely positioned to have a strong first-mover advantage in this suddenly vital, novel ecosystem. But it's culture and values are very much pulled across from this previous era.&lt;/p&gt;
    &lt;p&gt;(Springing out of this, Perl had an, at best grudging, tolerance for 'difficult genius' types, alongside this baseline culture. Unfortunately, this kind of toxic personality tends to thrive in the type of culture I've described, and they do set to help the tone. I'm not here to call out people specifically, because I'm trying to make a point rather than feed a culture war, or dig up gossip, but there were several significant examples, you can probably find lore if you like. I think the kindest way I can describe the compounding effect of this is that there was a strong cultural norm along the lines of "It's OK to be rude, as long as it's for a good cause".)&lt;/p&gt;
    &lt;head rend="h3"&gt;A fort within a fort&lt;/head&gt;
    &lt;p&gt;I remember this tension as always being tangibly there. Perl IRC and mailing lists were quite cliquey and full of venerated experts and in-jokes, rough on naivety, keen on robust, verbose debate, and a little suspicious of newcomers. And very cult-like. The "TIMTOWTDI" rule, although ostensibly liberal, literally means 'there is more than one way to do it in Perl' - and you can perhaps infer from that that there's little to no reason to do it using anything else. Elevating extreme flexibility like this is paradoxically also an engine of conservatism. If Perl can already do anything, flexibly, in multiple ways, then the language itself doesn't need to change - 'we already have one of those here, we don't need new things'. This attitude determined how Perl intended to handle evolution: the core language would remain stable (a fort inside a fort, only accessible to high level wizards), while innovation was pushed outward to CPAN. You could add features outside of core by writing and consuming third party libraries, you could bend language behaviour with pragmas without modifying Perl itself. The very best CPAN modules could theoretically be promoted into core, allowing the language to evolve conservatively from proven, widely-used features.&lt;/p&gt;
    &lt;p&gt;On paper, this sounds reasonable. In practice, I think it encoded a fundamental conflict of interest into the community early on, and set the stage for many of the later growth problems. I'm not going to pretend that Perl invented dependency hell, but I think it turned out to be another one of those profound misfeatures that their cultural philosophy lead them to mistake for virtue, and embrace.&lt;/p&gt;
    &lt;p&gt;An interesting thing I think has been missed discussing the context of the original blog piece, about whether Perl 6 significantly impacted Perl growth, is the fact that Perl 6 itself manifested out of ongoing arguments. Perl 6 is a schism. Here's a oft-cited note from Larry Wall himself about the incident that sparked Perl 6, at &lt;del&gt; YAPC&lt;/del&gt; OSCON 2000 &lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We spent the first hour gabbing about all sorts of political and organizational issues of a fairly boring and mundane nature. Partway through, Jon Orwant comes in, and stands there for a few minutes listening, and then he very calmly walks over to the coffee service table in the corner, and there were about 20 of us in the room, and he picks up a coffee mug and throws it against the other wall and he keeps throwing coffee mugs against the other wall, and he says "we are f-ed unless we can come up with something that will excite the community, because everyone's getting bored and going off and doing other things".&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;(Pause a second and ask yourself about the sort of social culture that both allows this kind of behaviour at public events, and then chooses to embrace it as a key piece of cultural lore)&lt;/p&gt;
    &lt;head rend="h3"&gt;The impact of Perl 6&lt;/head&gt;
    &lt;p&gt;Perl 6 was really a schism. Perl was already under a great amount of strain trying to accommodate the modernising influx of post dot-com mainstream web application building, alongside the entrenched conservatism of the core maintainers, and the maintenance burden of a few years exponential growth of third-party libraries, starting to build a fractal mess of slightly differentiating, incompatible approaches of those multiple ways to do things that were effectively now table-stakes language features, as the deployment landscape started to tiptoe towards a more modern, ubiquitous WWW3.&lt;/p&gt;
    &lt;p&gt;So, while I agree that it's wrong to generalise that 'Perl 6 killed Perl', I would say that Perl 6 was a symptom of the irreconcilable internal forces that killed Perl. Although, I also intend to go on to point out that Perl isn't dead, nothing has actually killed Perl. Killed Perl is a very stupid way to frame the discussion, but here we are.&lt;/p&gt;
    &lt;p&gt;So... Perl 6 is created as a valve to offset that pressure, and it kind of works. Up to a point. Unfortunately I think the side effect really is that the two branches of the culture, in the process of forking, double down on their encoded norms. Perl 5.x beds down as the practical, already solved way to do all the same things, with little need to change. Any requirements for more modern application patterns that are emerging in the broader web development environment, like idk, Unicode, REST clients, strict data structures, asynchronous I/O, whatever? That can either wait for Perl6 or you can pull things together using the CPAN if you want to move right now. Perl 6 leans the other way - they don't need to ship immediately, we have Perl 5 already here for doing things, Perl 6 is going to innovate on everything, and spend it's time getting there, designing up-front.4 They spend at least two years writing high level requirement specs. They even spin out a side-project trying to build a universal virtual machine to run all dynamic programming languages that never delivers5&lt;/p&gt;
    &lt;p&gt;This is the landscape where Perl's central dominance of 'back end' web programming continues to slip. Unfortunately, alongside the now principled bias toward cultural conservatism, Perl 5 has an explicit excuse for it. The future is over there, and exciting, and meanwhile we're working usefully, and getting paid, and getting stuff done. Kind of OK from inside the fort. Some day we'll move to the newer fort, but right now this is fine. Not very attractive to newcomers though, really. And this is also sort of OK, because Perl doesn't really want those sort of newcomers, does it? The kind that turns up on IRC or forums and asks basic questions about Perl 6 and sadly often gets treated with open contempt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Meanwhile, over there&lt;/head&gt;
    &lt;p&gt;Ruby has sprouted "Ruby on Rails", and it's taken the dynamic web building world by storm. Rails is a second generation web framework, that's proudly an 'opinionated web framework'. Given that the web application architecture is starting to stabilise into a kind of three-tier system , with a client as a web browser, a middle tier as a monolithic application server, and a persistence layer as a relational database , and a split server architecture serving static and dynamic content from different routes, here is just one way to do that, with hugely developer friendly tooling turning this into a cookie-cutter solution for the 80% core, and a plugin and client-side decoration approach that allows for the necessary per-site customisation.&lt;/p&gt;
    &lt;p&gt;Ruby is interesting as well. Ruby is kind of a Perl6 really. More accurately it's a parallel universe Perl5 Ruby comes from Japan, and has developed as an attempt to build something similar to Perl, but it's developed much later, by programming language enthusiasts, and for the first ten years or so, it's mostly only used in Japan. To my line of thinking this is probably important. Ruby does not spring from decades of sysadmin or sysop culture. Ruby is a language for programmers, and is at this point an sensible candidate for building something like Rails with - a relatively blank canvas for dynamic programming, with many of the same qualities as Perl, with less legacy cruft, and more modern niceties, like an integrated object system, exceptions, straightforward data structures. Ruby also has adopted 'friendliness' as a core value, and the culture over there adopts a principled approach to aggressively welcoming newcomers, promoting easiness, and programmer happiness and convenience as strong first class principles.&lt;/p&gt;
    &lt;p&gt;Rails is a huge hit. At this point, which is around about the time I stopped significantly using Perl (2004-2005) (because I quit my job, not out of any core animosity toward it, in fact, in my day, I was really quite a Perl fan), Rails is the most appealing place to start as a new web programmer. Adoption rate is high, community is great, velocity of development is well-paced, and there's a lovely , well-lit, onboarding pipeline for how to start. You don't even really need to know ruby. It has a one-shot install tool, and generates working websites from templates, almost out of the box. It's an obvious starting point.&lt;/p&gt;
    &lt;p&gt;Perl being Perl, develops several analogue frameworks to Rails, all of them interdependently compatible and incompatible with each other and each other's dependencies, all of them designed to be as customisable and as user configurable as they possibly can be6&lt;/p&gt;
    &lt;head rend="h3"&gt;PHP&lt;/head&gt;
    &lt;p&gt;There are also the other obvious contenders. PHP has been there all along, and it's almost coming up from entirely the opposite cultural background of Perl. PHP is a users language. It's built to be deployed by copying script files to your home directory, with minimal server side impact or privileges. It's barely designed at all, but it encounters explosive growth all the way through the first (and through into the second) web era, almost entirely because it makes the barrier to onboarding so low as to be non-existent. PHP gets a couple of extra free shots in the arm&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Because it's architecture is so amenable to shared-server hosting, it is adopted as the primary implementation language of the blogging boom. An entire generation of web developers is born of installing and customising WordPress and text-pattern et. al by installing it directly into your home directory on a rented CPanel host account. It's the go-to answer for 'I'm not a programmer really but how do I get a personal web site'7 This zero gate-keeping approach keeps the PHP stack firmly on the table of 'basic' web programmers all through the history of the web up to the current day.&lt;/item&gt;
      &lt;item&gt;Because of these initially lightweight deployment targets, PHP scales like little else, mostly because it's execution model leans strongly towards idempotent execution, with each web request tearing up and tearing down the whole environment. In a sense, this is slower than keeping hot state around, but it does lend itself extremely well to shared-nothing horizontal scaling, which as the web user base increases gigantically throughout the 2000s era, is the simplest route to scaling out. Facebook famously, is built in PHP at this point in time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Python&lt;/head&gt;
    &lt;p&gt;There is of course one other big horse in the race in this era, and it's a particularly interesting one in many ways, certainly when contrasted with Perl. This is of course, Python. Python is a close contemporary of Perl's but once again, it's roots are somewhere very different. Python doesn't come from UNIX culture either. Python comes from academia, and programming language culture. It's kind of a forgotten footnote, but Python was originally built for the Amoeba operating system, and it's intention was to be a straightforward programming language for scripting this8. The idea was to build a language that could be the 'second programming language' for programmers. Given that this is the 1980s, early 1990s, the programmers would be expected to be mostly using C / C++ ,perhaps Pascal. Python was intended to allow faster development for lighter weight programs or scripting tasks. I suppose the idea was to take something that you might want to build in a shell script, but provide enough high level structured support that you could cleanly build the kind of things that quickly become a problem in shell scripts. So, it emphasises data structures, and scoped variables, and modules, and prioritises making it possible to extend the language with modules. Typical things that experienced programmers would want to use. The language was also designed to be portable between the different platforms programmers would use, running on the desktops of the day, but also on the server. As a consequence, it had a broad standard library of common portable abstractions around standard system features - file-systems, concurrency, time, FFI. For quite a long time, one of python's standard mottoes was 'batteries included'.&lt;/p&gt;
    &lt;p&gt;Python never set the world on fire at any particular moment, but it remained committed to a clear evolutionary incremental development, and clean engineering principles. Again, I think the key element here is cultural tone. Python is kind of boring, not trying to be anyone's best language, or even a universal language. Python was always a little fussy, maybe snobby, slightly abstracted away from the real world. It's almost as old as Perl and it just kept incrementally evolving, picking up users, picking up features, slowly broadening the standard library. The first time I saw Python pick up an undeniable mainstream advantage would also have been around the early 2000s, when Google publicly adopted it as one of their house standard languages. Never radical, just calmly evolving in it's environs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nature abhors a vacuum&lt;/head&gt;
    &lt;p&gt;When I sketch out this landscape, I remain firmly convinced that most of Perl's impedance to continued growth were cultural. Perl's huge moment of relevance in the 90s was because it cross-pollinated two diverging user cultures. Traditional UNIX / database / data-centre maintenance and admin users, and enthusiastic early web builders and scalers. It had a cultural shock phase from extremely rapid growth, the centre couldn't hold, and things slowly fell apart.&lt;/p&gt;
    &lt;p&gt;Circling back though, it's time to address the real elephant in the room. Perl manifestly did not die. It's here right now. It's installed I think by default, on almost every single computer I own and operate, without me doing a single thing to make that happen. It's still used every day by millions of people on millions of systems (even if that isn't deliberate). It's still used by many people entirely deliberately for building software, whether that's because they know it and like it and it works, or because they're interfacing with or working on legacy Perl systems (of which there are still many), or maybe they're using it still in it's original intentional role - A capable POSIX-native scripting language, with much better performance and a broader feature-set than any shell or awk. I still occasionally break it out myself, for small scripts I would like to use more than once, or as parts of CLI pipelines.&lt;/p&gt;
    &lt;p&gt;What I don't do any more is reach for Perl first to make anything new. In my case, it's just because I typically am spoilt for options that are a better fit for most tasks, depending on whatever it is I'm trying to achieve. By the time I came to Perl, (1998-ish), I was already on my third career phase, I had a strong UNIX background, and had already built real things in lisp, java, pascal, visual basic and C++. My attitude to languages was already informed by picking a tool to fit the task at hand. Boy did I love Perl for a few years. The product/market-fit for those early web days was just beautiful. The culture did have too much of the negative tropes I've been pointing at, but that wasn't really a problem personally for me, I'd grown up amongst the BOFHs inside the data centres already, it wasn't too hard for me to assimilate, nor pick up the core principles. I did occasionally bounce off a couple of abrasive characters in the community, but mostly this just kept me loosely coupled, I enjoyed how the language solved the problems I needed solving quickly, I enjoyed the flexibility, and I also enjoyed the way that it made me feel smart, and en-route to my wizard's robes and hat, when i used it to solve harder problems in creative ways, or designed ways around bugs and gremlins. For a good 3-4 years I would have immediately picked it as my favourite language.&lt;/p&gt;
    &lt;p&gt;So as I say, I didn't fall out of it with any sense of pique, I just naturally moved to different domains, and picked up tools that best fit. After Amazon, I spent t a lot of time concentrating on OS X and audio programming, and that involved a lot of objective C, C++. The scripting tools in that domain were often in ruby, sometimes python. For personal hacking, I picked up lisp again9 (which I'd always enjoyed in school). I dipped in and out of Perl here and there for occasional contract work, but I tended to gravitate more towards larger database stuff, where I typically found C, java and python. The next time I was building web things, it was all Rails and ruby, and then moving towards the web services / REST / cloud era, the natural fits were go, and of course node and JavaScript or Typescript. I've always been a polyglot, and I've always been pretty comfortable moving between programming languages. The truth of the matter is, that the majority of programming work is broadly similar, and the specific implementation details of the language you use don't matter all that much, if it's a good fit for the circumstances.&lt;/p&gt;
    &lt;p&gt;I can't imagine Perl disappearing entirely in my lifetime. I can remember entire programming environments and languages that are much, much deader than I can ever see Perl becoming.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pascal used to be huge for teaching and also for desktop development in the 8/16 bit era&lt;/item&gt;
      &lt;item&gt;Objective C - only really useful inside the Apple ecosystem, and they're hell bent on phasing it out.&lt;/item&gt;
      &lt;item&gt;Before I got into the Internet, I used to build application software for 16 bit Windows (3.11) which was a vast market, in a mixture of database 4GLs (like PowerBuilder, Gupta/Centura SQLWindows) and Win16 C APIs. This entire universe basically no longer exists, and is fully obsolete. There must be many similar cases.&lt;/item&gt;
      &lt;item&gt;I mean who the hell realistically uses common lisp any more outside of legacy or enthusiast markets? Less people than Perl I'm sure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Perl also got to be if not first, then certainly early to dominate a new market paradigm. Plenty of things never manage that. It's hard to see Perl as anything other than an enormous success on these terms. Perl innovated and influenced languages that came after in some truly significant ways.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tightly embedding regular expressions and extending regular expressions (the most commonly used regular expression dialect in other tools is Perl)&lt;/item&gt;
      &lt;item&gt;CPAN, for package/library distribution via the internet, with dependency resolution - and including important concepts like supply chain verification with strong package signatures&lt;/item&gt;
      &lt;item&gt;A huge emphasis on testing, automated test harnesses, and CI. Perl test format (TAP) is also widely found in other CI/harness systems&lt;/item&gt;
      &lt;item&gt;Blending the gap between shell / scripting / and system programming in a single tool. I suppose this is debatable, but the way Perl basically integrated all the fundamental POSIX/libc as native built-ins with broadly the same semantics, but with managed memory and shell conventions was really revolutionary. Before this, most languages I had ever seen broadly tended to sit in one box, afterwards, most languages tended to span across several.&lt;/item&gt;
      &lt;item&gt;Amazing integrated documentation, online, in-tool and also man pages. POD is maybe the most successful ever implementation of literate programming ideas (although most of the real docs don't intertwingle the documentation very much iirc)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just these points, and I'm sure there are many others that could be made, are enough of a legacy to be proud of.&lt;/p&gt;
    &lt;p&gt;Counterfactuals are stupid (but also fun). If I squint, I can imagine that a Perl with a less reactionary culture, and a healthier acceptance of other ideas and environmental change might have been able to evolve alongside the other tools in the web paradigm shift, and still occupy a more central position in today's development landscape. That's not the Perl we have though, and that didn't happen. And I'm very confident that without the Perl we did have, the whole of modern software practice would be differently shaped. I do think Perl now lives in a legacy role, with a declining influence, but that's really nothing to feel shame or regret for. Nobody is going to forcibly take Perl away as long as POSIX exists, and so far as I can see, that means forever. In 2025 too, I can see the invisible hand creeping up on some of these other systems I've mentioned. Rust is slowly absorbing C and C++. Ruby (and of course Rails) is clearly in decline, in a way that probably consigns it to become a similar legacy state. From a certain angle, it looks a lot like Typescript is slowly supplanting Python. I won't be entirely surprised if that happens, although at my age I kind of doubt I'll live to see the day.&lt;/p&gt;
    &lt;head rend="h3"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;1 : Meritocracy is a fun word. It was originally coined as a pejorative term to describe a dystopian mechanism by which modern i.e. Western / British society entrenches and justifies an unfair and unequal distribution of privilege&lt;/p&gt;
    &lt;p&gt;2 : The UNIX implementer culture, is scientific/academic and fell out of Bell Labs. I guess you could extend this school of thought as a cultural sweep towards building abstracted cloud operations, toward plan 9/ Inferno / go&lt;/p&gt;
    &lt;p&gt;3 : Web 2.0 was first defined in 1999 by Darcy DiNucci in a print article , the term didn't become mainstream until it was picked up and promoted by Tim O'Reilly (then owner/operator of perl.com, trivia fans), an astute inside observer of the forces driving web development&lt;/p&gt;
    &lt;p&gt;4: Another unfortunate bit of luck here. Right at the point of time that 'agile' starts getting some traction as a more natural way to embrace software development - i.e. iterating in small increments against a changing environment and requirements, Perl 6 decides to do perhaps the most waterfall open source development process ever attempted. . It is fifteen years before Perl 6 ships something resembling a usable programming language.&lt;/p&gt;
    &lt;p&gt;5 : The Parrot VM, a lovely quixotic idea, which sadly fizzled out, after even Perl 6 stopped trying to target it. Interestingly enough, both python and ruby both made relatively high profile ports to the JVM that were useful enough to be used for production deploys in certain niches.&lt;/p&gt;
    &lt;p&gt;6 : A side effect of this degree of abstraction, is that as well as being very hard to get started, it's easy to fall foul of performance overhead.&lt;/p&gt;
    &lt;p&gt;7 : This ubituitious ecosystem of small footprint wordpress custom installs gives birth to the web agency model of commercial website building / small ecommerce sites, which thrives and is suprisingly healthy today. Recent, and slighly optimistic surveys have pitched WordPress as powering over 40% of all websites today. Now this is certainly inflated, but even if the realistic number is half of that, that's still pretty damn healthy.&lt;/p&gt;
    &lt;p&gt;8 : It's often repeated that Python was designed as a teaching language, but as far as I know, that's not actually the case. The designer of Python, Guido Van Rossum was previously working on a project that was a intended as training language, called ABC, and many of ABC's syntax and structural features influenced or made their way into Python.&lt;/p&gt;
    &lt;p&gt;9 : Common lisp is a better answer to an infinitely flexible 'everything' chainsaw language than perl, IMHO&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical"/><published>2025-12-06T17:42:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46176289</id><title>Zebra-Llama – Towards efficient hybrid models</title><updated>2025-12-07T16:11:13.397087+00:00</updated><content>&lt;doc fingerprint="c1d340b445bd08b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 22 May 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Zebra-Llama: Towards Extremely Efficient Hybrid Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and &amp;gt;97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Mehdi Rezagholizadeh [view email]&lt;p&gt;[v1] Thu, 22 May 2025 20:39:57 UTC (12,646 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2505.17272"/><published>2025-12-06T20:15:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46176893</id><title>The past was not that cute</title><updated>2025-12-07T16:11:13.231804+00:00</updated><content>&lt;doc fingerprint="26e613518ba77ebb"&gt;
  &lt;main&gt;
    &lt;p&gt;I was excited when cottagecore became a thing. Maybe my interest in retro clothes and handicrafts would be less embarrassing now!&lt;/p&gt;
    &lt;p&gt;I still enjoy it. But in spaces focused on old-fashioned vibes, you encounter a lot of people who believe that the past was actually this charming.&lt;/p&gt;
    &lt;p&gt;Laura Ingalls Wilder‘s Little House on the Prairie books are problematic, and also I will always love them. She wrote about the beauty of family and hard work, but she wrote them because she spent her whole life supporting disabled family members. She and her daughter beautified her “pioneer girl” history to make good books. Her daughter describes the reality: “It took seven successive years of complete crop failure, with work, weather and sickness that wrecked [my father’s] health permanently, and interest rates of 36 percent on money borrowed to buy food, to dislodge us from that land.”&lt;/p&gt;
    &lt;p&gt;My own version of this mistake was thinking that people’s personalities were different in the past. I grew up listening to folk music and imagining a past where nice boys would admire a nice quiet girl like me, and I wouldn’t have to figure out dating because everything would just unfold, probably on a May morning. My mother pointed out that a lot of the songs along the lines of “my own true love proved false to me” were about unplanned pregnancies.&lt;/p&gt;
    &lt;p&gt;I also assumed the bonny lasses in these songs would be wholesome and nice. But were popular girls of the past nicer people than they are now?&lt;/p&gt;
    &lt;p&gt;Some of my picture came from growing up in the Anglo-American folk dance and music community: it had a lot of aging hippies with graduate degrees. So I came away imagining a past with a lot of the kind of people who become engineers and English teachers. A more accurate picture would have been “Imagine a small town where the same 19 kids form your entire group of peers and potential partners.”&lt;/p&gt;
    &lt;p&gt;Bookish girls like Belle didn’t really go to live in enchanted castles with huge libraries. They stayed in villages where everyone thought they were weird and their best option was Gaston.&lt;/p&gt;
    &lt;p&gt;Maybe my favorite podcast episode ever is Rachel Laudan on food history: “I did have the extraordinary good fortune to grow up eating what I think the romantic movement dreams of. We had milk fresh from the cow; I never had pasteurized milk until I went to school. We had fish from the river, pheasant from the farm. The food was extremely good. . . . everything was fresh from the garden. So, I do romanticize—some of that because the taste was often extraordinary. And then I tweak myself and I say, ‘Look, Rachel, your mother spent all day, every day gardening or cooking.’ Essentially. As well as doing other chores. And she said to you, ‘Rachel, it’s servitude. I want you to have a life I didn’t have.’ “&lt;/p&gt;
    &lt;p&gt;I love living in a time and place where we get to choose aesthetics. I have bread rising in my kitchen right now, and I’m looking forward to baking it in an electric oven that doesn’t require me stacking wood or putting smoke into my house.&lt;/p&gt;
    &lt;p&gt;So I’ll continue to enjoy retro vibes, and draw on the past for lessons on how to be a human. (For example, making music together is one of life’s great experiences, and it’s a mistake to entirely substitute recorded music for that.) But I’ll enjoy doing so with indoor plumbing, dental care, and a desk job.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://juliawise.net/the-past-was-not-that-cute/"/><published>2025-12-06T21:53:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46176905</id><title>Screenshots from developers: 2002 vs. 2015 (2015)</title><updated>2025-12-07T16:11:11.437675+00:00</updated><content>&lt;doc fingerprint="5e3b5b5dcbbf3eec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Screenshots from developers: 2002 vs. 2015&lt;/head&gt;
    &lt;p&gt;In 2002 I asked a number of developers/Unix people for screenshots of their desktops. I recently republished them, and, seeing the interest this generated, I thought it’d be fun to ask the same people* again 13 years later. To my delight I managed to reach many of them.&lt;/p&gt;
    &lt;p&gt;* Sans Dennis Ritchie and itojun, who are no longer with us.&lt;/p&gt;
    &lt;p&gt;So, without further ado:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;my desktop is pretty boring, since it consists of xterm windows to whatever unix system i am using at the moment. the machine itself is likely to be running some x-window server like exceed on some flavor of windows, though for many years i just used an x terminal.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;If you thought it was boring last time, check this out!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2002:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I don’t know how to make a screenshot, because I normally use my computer in text-mode. I have X and GNOME installed, but I use them only occasionally.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2015:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Under X, I use the standard environment of Trisquel, but mostly I type at Emacs in a console.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Well, my desktop is quite boring. I mostly work with four xterms and a few Netscape windows. The KDE bar hides automatically, you can only see a thin grey line at the bottom.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Here is the new one. You'll see that, like before, I have lots of xterms where I work on Vim, Zimbu and email. Now using the Chrome browser, showing off the Zimbu homepage. But clearly everything has become bigger!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Linux (2.4.20-pre5), Gnome2, vim, Pine.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Not that much has changed in 13 years. Still using Linux. Still just a browser window and a ton of terminals hiding behind them. The main change is that switched from Pine to Thunderbird for email at some point. The OS on my laptop here is Ubuntu with Unity although there are a lot of Debian packages installed so it is a bit of a hybrid at this point. Oh, and yes, my son Carl is a lot older now.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Ah, my desktop is pretty boring, I used fvwm 1.24 as my window manager and I try to have no more than 1 or 2 windows open per virtual desktop. I use FreeBSD 4-STABLE as my operating system. I first came across Unix when I got an account on a Pyramid 90x running OSx. This had a dual-universe setup: both AT&amp;amp;T and BSD-style environments, chosen by an environment variable. Initially I was given the AT&amp;amp;T environment, but my friends convinced me to ``come over” to BSD. Since then I’ve been a BSD afficionado.&lt;/p&gt;
      &lt;p&gt;After OSx, SunOS 3.5 and later SunOS releases, until 386BSD 0.1 came out and I started to run BSD at home. Then when 386BSD transmogrified to FreeBSD, I went with FreeBSD.&lt;/p&gt;
      &lt;p&gt;In terms of desktop, I’m a command-line guy, always will be. My favourite editor is vi, my favourite shell is tcsh (but kudos to rc for elegance). So I don’t really feel the need for GUI things like Gnome or KDE :-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.&lt;/p&gt;
      &lt;p&gt;There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.&lt;/p&gt;
      &lt;p&gt;My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.&lt;/p&gt;
      &lt;p&gt;The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.&lt;/p&gt;
      &lt;p&gt;I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;You’ll probably be sad (or perhaps not) to hear that my desktop hasn’t really changed much at all - still OS X, though because OS X has virtual desktops now I have multiple “desktops” (6 of them) where Mail.app runs on one, Safari on another, Calendar, Slack, etc - all on separate desktops. This makes it a bit boring, but here’s the one I probably spend the most time in - the terminal window desktop. :)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;There we go. Actually, that’s a condensate in one workspace cause I usually use about 4. Some of my favourite apps:&lt;/p&gt;
      &lt;item&gt;http://anjuta.sf.net/ (IDE)&lt;/item&gt;
      &lt;item&gt;http://quirc.org/ (IRC)&lt;/item&gt;
      &lt;item&gt;http://gaim.sf.net/ (IM)&lt;/item&gt;
      &lt;item&gt;http://multignometerm.sf.net/ (Term)&lt;/item&gt;
      &lt;p&gt;not on the shot, but worth noted&lt;/p&gt;
      &lt;item&gt;http://sylpheed.good-day.net/ (Email Client)&lt;/item&gt;
      &lt;p&gt;and of course a shot of RTCW&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;'screenshot as code', I maintain my desktop configuration through saltstack: https://github.com/TTimo/linux-salted/commits/master&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Discussion: Hacker News; reddit: /r/programming, /r/linux&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/"/><published>2025-12-06T21:55:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46177645</id><title>Kilauea erupts, destroying webcam [video]</title><updated>2025-12-07T16:11:10.286003+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=TK2N99BDw7A"/><published>2025-12-06T23:39:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46178347</id><title>Using LLMs at Oxide</title><updated>2025-12-07T16:11:09.606891+00:00</updated><content>&lt;doc fingerprint="ab11131f2d2c5aa1"&gt;
  &lt;main&gt;
    &lt;p&gt;Large language models (LLMs) are an indisputable breakthrough of the last five years, potentially profoundly changing the way that we work. As with any extraordinarily powerful tool, LLM use has both promise and peril — and that they are so general-purpose leaves real questions about how and when they should be used. The landscape is shifting so rapidly that static prescription is unlikely — but that LLMs are evolving so quickly also gives urgency to the question: how should LLMs be used at Oxide?&lt;/p&gt;
    &lt;head rend="h2"&gt;Values in LLM usage&lt;/head&gt;
    &lt;p&gt;As is our wont, it’s helpful to look at LLM use through the lens of our values, several of which come to mind, listed here in priority order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Responsibility: In terms of LLM use at Oxide, our lodestar is our sense of responsibility. However powerful they may be, LLMs are but a tool, ultimately acting at the behest of a human. Oxide employees bear responsibility for the artifacts we create, whatever automation we might employ to create them. That is, human judgement remains firmly in the loop: even if or as an LLM is generating an artifact that we will use (writing, test cases, documentation, code, etc.), their output is the responsibility of the human using them.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rigor: LLMs are double-edged with respect to rigor. On the one hand, wielded carefully, they can help us sharpen our own thinking by pointing out holes in our own reasoning or otherwise providing thought-provoking suggestions. On the other, if used recklessly or thoughtlessly, they can have the opposite effect, replacing crisp thinking with generated flotsam. LLMs are useful in as much as they promote and reinforce our rigor.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Empathy: Be we readers or writers, there are humans on the other end of our language use. As we use LLMs, we must keep in mind our empathy for that human, be they the one who is consuming our writing, or the one who has written what we are reading.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Teamwork: We are working together on a shared endeavor, and we must be sure that our LLM use does not undermine our sense of teamwork. Specifically, we must be careful to not use LLMs in such a way as to undermine the trust that we have in one another. This isn’t as simple as disclosure of usage: and in fact, volunteering that an LLM has been used to generate work product is to implicitly distance oneself from the responsibility for the content — and serves as to erode the trust that is essential for teamwork.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Urgency: Urgency seems natural with a tool that can seemingly do so much knowledge work so quickly, but with respect to LLM use, too many organizations have seemingly enshrined urgency over all else. These organizations treat LLMs as an opportunity to increase pace over all else, seemingly without regard for setting direction. Urgency is certainly important, and LLMs absolutely afford an opportunity to do work more quickly — but that pace must not come at the expense of our responsibility, rigor, empathy and teamwork.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Uses of LLMs&lt;/head&gt;
    &lt;p&gt;LLM use varies widely, and the ramifications of those uses vary accordingly; it’s worth taking apart several of the (many) uses for LLMs.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as readers&lt;/head&gt;
    &lt;p&gt;LLMs are superlative at reading comprehension, able to process and meaningfully comprehend documents effectively instantly. This can be extraordinarily powerful for summarizing documents — or of answering more specific questions of a large document like a datasheet or specification. (Ironically, LLMs are especially good at evaluating documents to assess the degree that an LLM assisted their creation!)&lt;/p&gt;
    &lt;p&gt;While use of LLMs to assist comprehension has little downside, it does come with an important caveat: when uploading a document to a hosted LLM (ChatGPT, Claude, Gemini, etc.), there must be assurance of data privacy — and specifically, assurance that the model will not use the document to train future iterations of itself. Note that this may be opt-out (that is, by default, a model may reserve the right to train on uploaded documents), but can generally be controlled via preferences — albeit occasionally via euphemism. (OpenAI shamelessly calls this checked-by-default setting "Improve the model for everyone", making anyone who doesn’t wish the model to train on their data feel as if they suffer from a kind of reactionary avarice.)&lt;/p&gt;
    &lt;p&gt;A final cautionary note: using LLMs to assist comprehension should not substitute for actually reading a document where such reading is socially expected. More concretely: while LLMs can be a useful tool to assist in the evaluating of candidate materials per [rfd3], their use should be restricted to be as a tool, not as a substitute for human eyes (and brain!).&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as editors&lt;/head&gt;
    &lt;p&gt;LLMs can be excellent editors. Engaging an LLM late in the creative process (that is, with a document already written and broadly polished), allows for LLMs to provide helpful feedback on structure, phrasing, etc. — all without danger of losing one’s own voice. A cautionary note here: LLMs are infamous pleasers — and you may find that the breathless praise from an LLM is in fact more sycophancy than analysis. This becomes more perilous the earlier one uses an LLM in the writing process: the less polish a document already has, the more likely it is that an LLM will steer to something wholly different — at once praising your groundbreaking genius while offering to rewrite it for you.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as writers&lt;/head&gt;
    &lt;p&gt;While LLMs are adept at reading and can be terrific at editing, their writing is much more mixed. At best, writing from LLMs is hackneyed and clichÃ©-ridden; at worst, it brims with tells that reveal that the prose is in fact automatically generated.&lt;/p&gt;
    &lt;p&gt;What’s so bad about this? First, to those who can recognize an LLM’s reveals (an expanding demographic!), it’s just embarrassing — it’s as if the writer is walking around with their intellectual fly open. But there are deeper problems: LLM-generated writing undermines the authenticity of not just one’s writing but of the thinking behind it as well. If the prose is automatically generated, might the ideas be too? The reader can’t be sure — and increasingly, the hallmarks of LLM generation cause readers to turn off (or worse).&lt;/p&gt;
    &lt;p&gt;Finally, LLM-generated prose undermines a social contract of sorts: absent LLMs, it is presumed that of the reader and the writer, it is the writer that has undertaken the greater intellectual exertion. (That is, it is more work to write than to read!) For the reader, this is important: should they struggle with an idea, they can reasonably assume that the writer themselves understands it — and it is the least a reader can do to labor to make sense of it.&lt;/p&gt;
    &lt;p&gt;If, however, prose is LLM-generated, this social contract becomes ripped up: a reader cannot assume that the writer understands their ideas because they might not so much have read the product of the LLM that they tasked to write it. If one is lucky, these are LLM hallucinations: obviously wrong and quickly discarded. If one is unlucky, however, it will be a kind of LLM-induced cognitive dissonance: a puzzle in which pieces don’t fit because there is in fact no puzzle at all. This can leave a reader frustrated: why should they spend more time reading prose than the writer spent writing it?&lt;/p&gt;
    &lt;p&gt;This can be navigated, of course, but it is truly perilous: our writing is an important vessel for building trust — and that trust can be quickly eroded if we are not speaking with our own voice. For us at Oxide, there is a more mechanical reason to be jaundiced about using LLMs to write: because our hiring process very much selects for writers, we know that everyone at Oxide can write — and we have the luxury of demanding of ourselves the kind of writing that we know that we are all capable of.&lt;/p&gt;
    &lt;p&gt;So our guideline is to generally not use LLMs to write, but this shouldn’t be thought of as an absolute — and it doesn’t mean that an LLM can’t be used as part of the writing process. Just please: consider your responsibility to yourself, to your own ideas — and to the reader.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as code reviewers&lt;/head&gt;
    &lt;p&gt;As with reading comprehension and editing, LLMs can make for good code reviewers. But they can also make nonsense suggestions or otherwise miss larger issues. LLMs should be used for review (and can be very helpful when targeted to look for a particular kind of issue), but that review should not be accepted as a human substitute.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as debuggers&lt;/head&gt;
    &lt;p&gt;LLMs can be surprisingly helpful debugging problems, but perhaps only because our expectations for them would be so low. While LLMs shouldn’t be relied upon (clearly?) to debug a problem, they can serve as a kind of animatronic rubber duck, helping to inspire the next questions to ask. (And they can be surprising: LLMs have been known to debug I2C issues from the screenshot of a scope capture!) When debugging a vexing problem one has little to lose by using an LLM — but perhaps also little to gain.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as programmers&lt;/head&gt;
    &lt;p&gt;LLMs are amazingly good at writing code — so much so that there is borderline mass hysteria about LLMs entirely eliminating software engineering as a craft. As with using an LLM to write prose, there is obvious peril here! Unlike prose, however (which really should be handed in a polished form to an LLM to maximize the LLM’s efficacy), LLMs can be quite effective writing code de novo. This is especially valuable for code that is experimental or auxiliary or otherwise throwaway. The closer code is to the system that we ship, the greater care needs to be shown when using LLMs. Even with something that seems natural for LLM contribution (e.g., writing tests), one should still be careful: it’s easy for LLMs to spiral into nonsense on even simple tasks. Still, they can be extraordinarily useful — and can help to provide an entire spectrum of utility in writing software; they shouldn’t be dismissed out of hand.&lt;/p&gt;
    &lt;p&gt;Wherever LLM-generated code is used, it becomes the responsibility of the engineer. As part of this process of taking responsibility, self-review becomes essential: LLM-generated code should not be reviewed by others if the responsible engineer has not themselves reviewed it. Moreover, once in the loop of peer review, generation should more or less be removed: if code review comments are addressed by wholesale re-generation, iterative review becomes impossible.&lt;/p&gt;
    &lt;p&gt;In short, where LLMs are used to generate code, responsibility, rigor, empathy and teamwork must remain top of mind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mechanics&lt;/head&gt;
    &lt;p&gt;Mechanical details of using LLMs — along with many specific tips and links — can be found in the (internal) LLMs at Oxide document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Determinations&lt;/head&gt;
    &lt;p&gt;Broadly speaking, LLM use is encouraged at Oxide, but that use must always be consistent with our deeply-held sense of responsibility: our responsibility to our product, our responsibility to our customers — and our responsibility to one another.&lt;/p&gt;
    &lt;head rend="h2"&gt;External References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;[rfd3] Oxide Computer Company. RFD 3 Oxide Hiring Process. https://rfd.shared.oxide.computer/rfd/0003.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rfd.shared.oxide.computer/rfd/0576"/><published>2025-12-07T01:17:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46178442</id><title>Eurydice: a Rust to C compiler (yes)</title><updated>2025-12-07T16:11:08.672527+00:00</updated><content>&lt;doc fingerprint="7e2eb37eada7ba38"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Eurydice: a Rust to C compiler (yes)&lt;/head&gt;
    &lt;p&gt;Perhaps the greatest surprise of the last two years was, for me, the realization that people not only care about compiling C to Rust (for obvious reasons, such as, ahem, memory safety) â they also care about compiling Rust to C! Wait, what?&lt;/p&gt;
    &lt;p&gt;I wrote about this briefly a couple years ago, but the level of interest for the project, I must say, took me somewhat by surprise. So letâs talk about compiling Rust to C a little more today.&lt;/p&gt;
    &lt;head rend="h1"&gt;Barriers to Rust adoption&lt;/head&gt;
    &lt;p&gt;Rust is making big progress in terms of adoption, and represents a great value proposition, especially for new code. Both my former employer and my new employer, like pretty much everyone else these days, have big projects that are written in pure Rust or can have Rust components. Even Windows kernel drivers can be written in Rust now. Amazing stuff.&lt;/p&gt;
    &lt;p&gt;However, if your project is, say, an open-source library that gets compiled on a wonderfully diverse set of target architectures, OSes, distributions and toolchains, well, chances areâ¦ one of these is not going to support Rust. Think of a crypto library: there will be people out there with an obscure compiler for a weird embedded target, and they really want to compile your library, because theyâve been told not to roll out their own crypto. Or perhaps you have a format library ridden with memory errors and you want to port it to Rust. Or maybe your company has an in-house analysis that only runs on C code. Regardless of the scenario, there will always be that one legacy use-case that prevents you from switching to Rust until itâs 2035, all those LTS versions (looking at you RHEL) are finally retired, and you yourself are too close to retirement to even care anymore.&lt;/p&gt;
    &lt;p&gt;That is, unless youâre willing to use a Rust to C compiler.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Having a backwards-compat scenario where Rust can be compiled to C serves several purposes.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It allows for a gradual transition. The codebase can be ported to Rust, and refactored / cleaned up / rewritten to use all the nice Rust things (data types, pattern-matching, polymorphism, memory safety), thus making you and your developers much, much happier. Meanwhile, the C version co-exists so that you donât alienate your userbase.&lt;/item&gt;
      &lt;item&gt;It only requires maintaining a single version. The Rust code is authoritative; the C code is derived from it automatically, either on CI, or at least with a CI job that checks that the two are in sync.&lt;/item&gt;
      &lt;item&gt;It allows for a census of problematic scenarios. By making the Rust version the default (and putting the fallback C behind a &lt;code&gt;--write-us-an-email&lt;/code&gt;flag), there is finally a way to enumerate those mythical users who cannot switch to Rust just yet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If that sounds appealing, meet Eurydice.&lt;/p&gt;
    &lt;head rend="h1"&gt;Eurydice&lt;/head&gt;
    &lt;p&gt;Eurydice is a compiler from Rust to C that aims to produce readable C code. Of course, readability is subjective; also, seeing that Rust relies on whole-program monomorphization, the C code is bound to be more verbose than the Rust code. But you can judge for yourself: hereâs the result of compiling libcrux to C.&lt;/p&gt;
    &lt;p&gt;The output of the test suite is under version control, and there are a lot more tests to peruse. See for instance this bit, compared to the Rust original.&lt;/p&gt;
    &lt;head rend="h1"&gt;The design of Eurydice&lt;/head&gt;
    &lt;p&gt;Eurydice plugs in directly at the MIR level, using Charon to avoid reimplementing the wheel and paying the price of interacting with the guts of &lt;code&gt;rustc&lt;/code&gt;. Our
paper on Charon says more about its
architecture.&lt;/p&gt;
    &lt;p&gt;The advantage of plugging in at the MIR level is that i) we do not have to interpret syntactic sugar, which means our translation is more faithful to the Rust semantics, and ii) we have way fewer constructs that need compiling to C. Even then, itâs no easy feat to translate Rust to C.&lt;/p&gt;
    &lt;p&gt;There is naturally, the need to perform whole-program monomorphization, over types and const-generic arguments; the compilation of pattern matches into tagged unions; recognizing instances of iterators that can be compiled to native C &lt;code&gt;for&lt;/code&gt;-loops. Then, there are more subtle things, such as compiling array
repeat expressions sensibly â zero-initializers when possible, initializer
lists otherwise, unless it generates too much code, in which case &lt;code&gt;for&lt;/code&gt;-loops are
preferable. And finally, there are all the rules about visibility, &lt;code&gt;static&lt;/code&gt;,
&lt;code&gt;inline&lt;/code&gt;, etc. that are very C-specific and depend on how you want to lay out
your C files.&lt;/p&gt;
    &lt;p&gt;The translation is complicated by the constraint that the generated code ought to be readable: for instance, we compile Rust structs to C structs, including DSTs, by relying on flexible array members. We also work hard to avoid using the fully-generic tagged union pattern when possible, instead eliminating the tag when e.g. the Rust enum only has a single case. Additionally, we rely on Charon to reconstruct control-flow, rather than compile the MIR CFG to C code ridden with &lt;code&gt;goto&lt;/code&gt;s; again, this is for code quality.&lt;/p&gt;
    &lt;p&gt;At a low-level, there were many interesting tidbits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Because arrays in Rust are values, we wrap them within C structs to give them value semantics in C, too; concretely, &lt;code&gt;[u32; 8]&lt;/code&gt;becomes&lt;code&gt;struct { uint32_t data[8]; }&lt;/code&gt;. (A previous version of Eurydice would emit&lt;code&gt;uint32_t *&lt;/code&gt;, and rely on various&lt;code&gt;memcpy&lt;/code&gt;s to implement value semantics, but this produced a translation that was not type-generic, and there were plenty of finicky corner cases. We revamped the compilation scheme recently.)&lt;/item&gt;
      &lt;item&gt;The notion of &lt;code&gt;lvalue&lt;/code&gt;in C means we need to insert more variable declarations than in Rust â for instance, you canât trivially compile&lt;code&gt;&amp;amp;[0u32; 1]&lt;/code&gt;without naming the array.&lt;/item&gt;
      &lt;item&gt;The fact that the evaluation order is so loosely defined in C means that intermediary computations need to be stored in intermediary variables to enforce the evaluation order.&lt;/item&gt;
      &lt;item&gt;Rust relies on whole-program monomorphization; this means that the C code is inevitably going to contains multiple copies of the same types and functions, but for different choices of type and const generic argumnets. This is currently done with a builtin phase in Eurydice (for historical reasons), but in the long run, we want to rely on Charonâs support for monomorphization.&lt;/item&gt;
      &lt;item&gt;There are plenty of peephole optimizations that are required for good code quality, such as recognizing &lt;code&gt;array::from_fn&lt;/code&gt;and generating sensible code that initializes the array in-place (instead of relying on the fully-general compilation scheme for closures), or recognizing instances of the&lt;code&gt;Eq&lt;/code&gt;trait that deserve dedicated treatment (such as using&lt;code&gt;memcmp&lt;/code&gt;for arrays and slices of flat data).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A final design choice is that for now, Eurydice may define more behaviors than Rust â for instance, Rust panics on integer overflow, but Eurydice-compiled code does not. This is because we assume the input code is verified, and therefore has been shown to be free of panics. This design choice can be easily changed, though.&lt;/p&gt;
    &lt;p&gt;In practice, as soon as you use traits, the C code becomes more voluminous than the Rust code. We rely on a configuration file mechanism to control the placement of monomorphized instances of a given function, rather than put everything in one big C file. This currently requires a lot of manual intervention to give good results on large projects.&lt;/p&gt;
    &lt;head rend="h1"&gt;Implementing of Eurydice&lt;/head&gt;
    &lt;p&gt;Eurydice starts by compiling the MIR AST obtained out of Charon into KaRaMeLâs internal AST. This is ~3000 lines of OCaml code, so thatâs already pretty involved. A lot of the work revolves around trait methods and their monomorphization, given Rustâs expressive trait system.&lt;/p&gt;
    &lt;p&gt;Then, about 30 nanopasses simplify the KaRaMeL AST until it becomes eligible for compilation to C. Of those, a handful were originally written for KaRaMeL and were somewhat reusable; this includes compilation of data types, as well as monomorphization. The rest was written from scratch for Eurydice, and totals about ~5000 lines of OCaml code.&lt;/p&gt;
    &lt;p&gt;A particularly gnarly phase was eliminating MIRâs variable assignments as much as possible: in MIR, every variable starts out uninitialized at the beginning of the function; then, in lieu of the variable declaration, we have an assignment with the initial value. Naturally, having a variable declaration in the right spot is better for code quality, so an initial phase tries to reconstruct these assignments. Thatâs a drawback of using MIR, but we still firmly believe that sticking to something that has clear semantics is ultimately better.&lt;/p&gt;
    &lt;p&gt;Fun fact: because there are so many peephole optimizations, I got tired of maintaining enormous pattern-matches that would try to catch every flavor of Rust iterator that can be compiled to a C for-loop. Instead, a custom OCaml syntax extension allows writing concrete syntax for the internal KaRaMeL language in OCaml patterns. Those magic patterns then get compiled at compile-time to OCaml AST nodes for an actual OCaml pattern that matches the (deeply-embedded) syntax of KaRaMeLâs AST. This relies on a &lt;code&gt;ppx&lt;/code&gt;
that lexes, parses and compiles the concrete syntax.&lt;/p&gt;
    &lt;head rend="h1"&gt;Deploying Eurydice-generated code&lt;/head&gt;
    &lt;p&gt;Eurydice-generated code expects some hand-written glue that contains macros and &lt;code&gt;static inline&lt;/code&gt; functions; sometimes, itâs simply more convenient to write a
single macro that uses a type, rather than have Eurydice generate N copies of a
polymorphic function that gets specialized each time. A typical example is
compiling the Eq trait for arrays: itâs nicer to emit &lt;code&gt;Eurydice_array_eq(a1, a2,
len, t)&lt;/code&gt;, which macro-expands to &lt;code&gt;!(memcmp(a1, a2, len*sizeof(t)))&lt;/code&gt;, rather than
have N such functions, each containing a for-loop specialized for different
values of &lt;code&gt;t&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Eurydice generates code that is either (C11 and C++20-compatible) or (C++-17 compatible, but not C-compatible). The reason for this is that Rust allows enum values (e.g. &lt;code&gt;Foo { bar: baz }&lt;/code&gt;) in any expression position. For simplicity,
Eurydice emits a compound initializer &lt;code&gt;(Foo) { .tag = bar, .value = { .case_Foo
= { .bar = baz }}}&lt;/code&gt;, or a C++20 aggregate that uses designated initializers,
relying on a macro (not shown here) to hide the syntax differences between the
two. But C++17 does not have designated initializers, so there is an option for
Eurydice to emit different code that relies on member pointers to achieve
sensibly the same effect.&lt;/p&gt;
    &lt;head rend="h1"&gt;Limitations of Eurydice&lt;/head&gt;
    &lt;p&gt;Naturally, there are many limitations to this approach. Here are the main ones that come to mind:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;we cannot guarantee that the layout of objects will be the same in C as in Rust; conceivably, one could parse the layout information from MIR, then emit compiler-specific alignment directives to keep the two identical, but this is not done currently;&lt;/item&gt;
      &lt;item&gt;the generated code violates strict aliasing, because creating a user-defined DST involves casting one pointer type (a struct containing an array) to another (a struct with a flexible array member instead); Iâm not sure what the best fix is, so for now, please compile your code with &lt;code&gt;-fno-strict-aliasing&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;the code that Eurydice sees is MIR after applying &lt;code&gt;cfg&lt;/code&gt;tweaks; this means that for code that is intended to be multi-platform, some tricks need to be applied, otherwise, Eurydice will only âseeâ one version of the code (AVX2, or ARM64, or something else)&lt;/item&gt;
      &lt;item&gt;because monorphization is so pervasive, the configuration language needs to express things such as âtypes that reference &lt;code&gt;__m256i&lt;/code&gt;, an AVX2-only type, need to go into a separate file to be compiled with&lt;code&gt;-mavx2&lt;/code&gt;â; this can get tedious real fast but Iâm not sure I know how to do better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Whatâs next?&lt;/head&gt;
    &lt;p&gt;There is ongoing work to integrate Eurydice-generated code for both Microsoft and Googleâs respective crypto libraries.&lt;/p&gt;
    &lt;p&gt;The community grew recently, with wonderful contributions by GitHub users @ssyram and @lin23299. There are more in the pipeline, and I look forward to seeing the supported subset of Rust grow even more. Next on the horizon is support for &lt;code&gt;dyn&lt;/code&gt; traits via vtables, and relying on Charonâs monomorphization
to get MIR exactly as the Rust compiler would monomorphize it, intead of relying
on a custom procedure in Eurydice.&lt;/p&gt;
    &lt;p&gt;An ambitious goal is for the whole standard library of Rust to be extractable via Eurydice in 2026. This is non-trivial, but I believe this achievement is within reach. Stay tuned.&lt;/p&gt;
    &lt;head rend="h1"&gt;PS: Why the name?&lt;/head&gt;
    &lt;p&gt;People keep asking about the name; because the project shares a large amount of infrastructure with Aeneas and Charon, I had to follow the Greek mythology theme. Specifically, the myth of Eurydice resonated with me: I thought I was saved from the hell of generating C code, and was going to go back to the world of the living, but alas, no.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jonathan.protzenko.fr/2025/10/28/eurydice.html"/><published>2025-12-07T01:41:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46178789</id><title>Z2 – Lithographically fabricated IC in a garage fab</title><updated>2025-12-07T16:11:08.382676+00:00</updated><content>&lt;doc fingerprint="6f28c43798a75591"&gt;
  &lt;main&gt;
    &lt;p&gt;Homemade 1000+ transistor array chip&lt;/p&gt;
    &lt;p&gt;In 2018 I made the first lithographically fabricated integrated circuits in my garage fab. I was a senior in high school when I made the Z1 amplifier, and now I’m a senior in college so there are some long overdue improvements to the amateur silicon process.&lt;lb/&gt; The Z1 had 6 transistors and was a great test chip to develop all the processes and equipment. The Z2 has 100 transistors on a 10µm polysilicon gate process – same technology as Intel’s first processor. My chip is a simple 10×10 array of transistors to test, characterize, and tweak the process but this is a huge step closer to more advanced DIY computer chips. The Intel 4004 has 2,200 transistors and I’ve now made 1,200 on the same piece of silicon.&lt;/p&gt;
    &lt;p&gt;Previously, I made chips with a metal gate process. The aluminum gate has a large work function difference with the silicon channel beneath it which results in a high threshold voltage (&amp;gt;10V). I used these metal gate transistors in a few fun projects like a guitar distortion pedal and a ring oscillator LED blinker but both of these required one or two 9V batteries to run the circuit due to high Vth. By switching to a polysilicon gate process, I get a ton of performance benefits (self aligned gate means lower overlap capacitances) including a much lower Vth which makes these chips compatible with 2.5V and 3.3V logic levels. The new FETs have excellent characteristics:&lt;/p&gt;
    &lt;quote&gt;NMOS Electrical Properties: Vth = 1.1 V Vgs MAX = 8 V Cgs = &amp;lt;0.9 pF Rise/fall time = &amp;lt;10 ns On/off ratio = 4.3e6 Leakage current = 932 pA (Vds=2.5V)&lt;/quote&gt;
    &lt;p&gt;I was particularly surprised by the super low leakage current. This value goes up about 100x in ambient room lighting.&lt;/p&gt;
    &lt;p&gt;Now we know that it’s possible to make really good transistors with impure chemicals, no cleanroom, and homemade equipment. Of course, yield and process repeatability are diminished. I’ll do more testing to collect data on the statistics and variability of FET properties but it’s looking good!&lt;/p&gt;
    &lt;p&gt;The chip is small, about one quarter the die area of my previous ICs (2.4mm^2) which makes it hard to probe. There’s a simple 10×10 array of N-channel FETs on each chip which will give me a lot of characterization data. Since it’s such a simple design, I was able to lay it out using Photoshop. Columns of 10 transistors share a common gate connection and each row is strung together in series with adjacent transistors sharing a source/drain terminal. It’s similar to NAND flash but I only did this to keep the metal pads large enough so I can reasonably probe them, if every FET had 3 pads for itself they would be too small.&lt;/p&gt;
    &lt;p&gt;It’s hard to convey the excitement of seeing a good FET curve displayed on the curve tracer after dipping a shard of rock into chemicals all day.&lt;/p&gt;
    &lt;p&gt;A single 10µm NMOS transistor can be see below, with slight misalignment in the metal layer (part of the left contact is uncovered). Red outline is polycrystalline silicon, blue is the source/drain.&lt;/p&gt;
    &lt;p&gt;So far I’ve made an opamp (Z1) and a memory-like array (Z2). More interesting circuits are definitely possible even with this low transistor density. The process needs some tweaking but now that I’m able to consistently make good quality transistors I should be able to design more complex digital and analog circuits. Testing each chip is very tedious so I am trying to automate the process and I’ll post more data then. I’ve made 15 chips (1,500 transistors) and know there’s at least one completely functional chip and at least two “mostly functional”, meaning ~80% of the transistors work instead of 100%. No proper yield data yet. The most common defect is a drain or source shorted to the bulk silicon channel, not a leaky or shorted gate like on my Z1 process.&lt;/p&gt;
    &lt;p&gt;I said before that the gate used to be made out of aluminum and now it’s silicon which makes the chips work a lot better. Silicon comes in three varieties that we care about: amorphous, polycrystalline, and monocrystalline. From left to right, these become more electrically conductive but also much harder to deposit. In fact, monocrystalline Si can’t be deposited, you can only grow it in contact with another mono-Si layer as a seed (epitaxy). Since the gate must be deposited on top of an insulating dielectric, poly is the best we can do. We can heavily dope the polysilicon anyway to make it more conductive.&lt;/p&gt;
    &lt;p&gt;A typical self-aligned polysilicon gate process requires silane, a toxic and explosive gas, to deposit polycrystalline silicon layers. It may also be possible by sputtering or evaporating amorphous silicon and annealing with a laser. A major theme of this DIY silicon process is to circumvent expensive, difficult, or dangerous steps. So, I came up with a modified process flow. It’s a variation on the standard self-aligned methods to allow doping via high temperature diffusion rather than ion implantation. The effect is that I’m able to buy a silicon wafer with the polysilicon already deposited on it from the factory and pattern it to make transistors instead of putting my own polysilicon down halfway through the process. This is a nice short term workaround but it would be best to design a polysilicon deposition process using the laser anneal method mentioned above.&lt;/p&gt;
    &lt;p&gt;Wafers are available with all kinds of materials deposited on them already, so I just had to find one with a thin layer of SiO2 (gate oxide, ~10nm) followed by a thicker polysilicon (300nm). I found a lot of 25 200mm (EPI, prime, [1-0-0], p-type) wafers on eBay for $45 which is essentially a lifetime supply, so email me if you want one. The gate oxide is the most fragile layer and requires the most care during fabrication. Since I bought the wafer with a nice high quality oxide on it already that was capped off and kept clean by the thick polysilicon layer, I was able to eliminate all the aggressive cleaning chemicals (sulfuric acid, etc) from the process and still make great transistors. Minimal process chemicals and tools are listed below.&lt;/p&gt;
    &lt;quote&gt;Chemicals used in home poly-gate process: -Water -Alcohol -Acetone -Phosphoric acid -Photoresist -Developer (2% KOH) -N type dopant (filmtronics P509) -HF (1%) or CF4/CHF3 RIE -HNO3 for poly etch or SF6 RIE&lt;/quote&gt;
    &lt;quote&gt;Equipment used in home poly-gate process: -Hotplate -Tube furnace -Lithography apparatus -Microscope -Vacuum chamber to deposit metal&lt;/quote&gt;
    &lt;p&gt;Z2 “gate first” process (similar to standard self-aligned process but without a field oxide):&lt;/p&gt;
    &lt;p&gt;I snapped one of the test chips in half (functional Z2 but with bad layer alignment and thin metal, about 300nm) and put it in my SEM for a cross section:&lt;/p&gt;
    &lt;p&gt;Find the dust particle in the red circle below, use that to get oriented in the coming cross section views.&lt;/p&gt;
    &lt;p&gt;Because I bought the wafer already with gate oxide and polysilicon on it, I can’t grow a field oxide. These thick oxide layers are typically used to mask dopants and require a long high temperature step which would oxidize all of my poly and there would be none remaining. So, my modified process uses an additional masking step (the “gate” mask is typically not found in a self-aligned process) that allows me to use the polysilicon itself as a dopant mask and hard-baked photoresist as the field dielectric. This alternative processing results in the stepped structure you can see in the orange region on the NMOS cross section above. This process subtlety is mentioned here, read this twitter thread.&lt;/p&gt;
    &lt;p&gt;This process isn’t ideal and I want to make some changes so it’s CMOS compatible but it simplifies fabrication and makes it possible with a minimal set of tools. The 1µm dielectric layer (orange) would ideally be CVD SiO2 (it’s possible to build a TEOS oxide reactor at home) but I used a photoresist instead. Most photoresists can be baked around 250°C to form a hard permanent dielectric layer that is an easy alternative to CVD or PECVD oxide. A spin-on-glass/sol-gel could also be used here. SiO2 etching is done with a buffered HF solution made from rust stain remover or RIE.&lt;/p&gt;
    &lt;p&gt;Huge composite stitched die image:&lt;/p&gt;
    &lt;p&gt;Thanks for following my work and feel free to contact me with your thoughts!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sam.zeloof.xyz/second-ic/"/><published>2025-12-07T03:03:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46178892</id><title>Discovering the indieweb with calm tech</title><updated>2025-12-07T16:11:08.265916+00:00</updated><content>&lt;doc fingerprint="7fa459cf38cdcf2d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Discovering the indieweb with calm tech&lt;/head&gt;
    &lt;p&gt;When social media first entered my life, it came with a promise of connection. Facebook connected college-aged adults in a way that was previously impossible, helping to shape our digital generation. Social media was our super-power and we wielded it to great effect.&lt;/p&gt;
    &lt;p&gt;Yet social media today is a noisy, needy, mental health hazard. They push distracting notifications, constantly begging us to “like and subscribe”, and trying to trap us in endless scrolling. They have become sirens that lure us into their ad-infested shores with their saccharine promise of dopamine.&lt;/p&gt;
    &lt;p&gt;How can we defeat these monsters that have invaded deep into our world, while still staying connected?&lt;/p&gt;
    &lt;head rend="h2"&gt;StreetPass for Mastodon&lt;/head&gt;
    &lt;p&gt;A couple weeks ago I stumbled into a great browser extension, StreetPass for Mastodon. The creator, tvler, built it to help people find each other on Mastodon. StreetPass autodiscovers Mastodon verification links as you browse the web, building a collection of Mastodon accounts from the blogs and personal websites you’ve encountered.&lt;/p&gt;
    &lt;p&gt;StreetPass is a beautiful example of calm technology . When StreetPass finds Mastodon profiles it doesn’t draw your attention with a notification, it quietly adds the profile to a list, knowing you’ll check in when you’re ready.&lt;/p&gt;
    &lt;p&gt;StreetPass recognizes that there’s no need for an immediate call to action. Instead it allows the user to focus on their browsing, enriching their experience in the background. The user engages with StreetPass when they are ready, and on their own terms.&lt;/p&gt;
    &lt;p&gt;StreetPass is open source and available for Firefox, Chrome, and Safari.&lt;/p&gt;
    &lt;p&gt;Inspired by StreetPass, I applied this technique to RSS feed discovery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Blog Quest&lt;/head&gt;
    &lt;p&gt;Blog Quest is a web browser extension that helps you discover and subscribe to blogs. Blog Quest checks each page for auto-discoverable RSS and Atom feeds (using &lt;code&gt;rel="alternate"&lt;/code&gt; links) and quietly collects them in the background.
When you’re ready to explore the collected feeds, open the extension’s drop-down window.&lt;/p&gt;
    &lt;p&gt;The extension integrates with several feed readers, making subscription management nearly effortless.&lt;/p&gt;
    &lt;p&gt;Blog Quest is available for both Firefox and Chrome. The project is open source and I encourage you to build your own variants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ubiquitous yet hidden&lt;/head&gt;
    &lt;p&gt;I reject the dead Internet theory: I see a vibrant Internet full of humans sharing their experiences and seeking connection. Degradation of the engagement-driven web is well underway, accelerated by AI slop. But the independent web works on a different incentive structure and is resistant to this effect. Humans inherently create, connect, and share: we always have and we always will. If you choose software that works in your interest you’ll find that it’s possible to make meaningful online connections without mental hazard.&lt;/p&gt;
    &lt;p&gt;Check out StreetPass and Blog Quest to discover a decentralized, independent Internet that puts you in control.&lt;/p&gt;
    &lt;p&gt;You can't drown out the noise of social media by shouting louder, you've got to whisper.&lt;/p&gt;
    &lt;head rend="h3"&gt;Image credits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edward Armitage: The Siren (1888)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexsci.com/blog/calm-tech-discover/"/><published>2025-12-07T03:26:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46181076</id><title>Java Hello World, LLVM Edition</title><updated>2025-12-07T16:11:06.428386+00:00</updated><content>&lt;doc fingerprint="8e1bab18c82db7cc"&gt;
  &lt;main&gt;
    &lt;p&gt;After exploring Java bytecode in previous years (2022, 2023, 2024), this year we’ll take an unexpected detour for a Java advent: instead of generating Java bytecode, we’ll use Java to build and execute LLVM IR, the intermediate language behind compilers like clang.&lt;/p&gt;
    &lt;p&gt;Using Java’s Foreign Function &amp;amp; Memory (FFM) API, we’ll call the LLVM C API, generate a “Hello, World!” program, and even JIT-compile it to native code – all from Java.&lt;/p&gt;
    &lt;p&gt;The task is simple: create a program that simply prints “Hello, World!”. But we must do this from Java via LLVM.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is LLVM?&lt;/head&gt;
    &lt;p&gt;The LLVM Project, a collection of modular compiler and toolchain technologies, began as a research project over 20 years ago at the University of Illinois. It has grown significantly, underpinning many compilers and tools like clang.&lt;/p&gt;
    &lt;p&gt;The core libraries provide a source &amp;amp; target independent optimizer along with code generation for a multitude of target machines. They are built around the LLVM IR, an intermediate representation, which we’ll generate &amp;amp; execute from Java.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing LLVM&lt;/head&gt;
    &lt;p&gt;To use the LLVM C API from Java, we’ll need LLVM’s shared libraries and headers installed locally. There is an automatic installation script available to easily install LLVM on Ubuntu/Debian systems, for example to install LLVM 20:&lt;/p&gt;
    &lt;code&gt;
$ wget https://apt.llvm.org/llvm.sh
$ chmod +x llvm.sh
$ ./llvm.sh 20
&lt;/code&gt;
    &lt;p&gt;Once we have LLVM installed we can use the LLVM tooling to execute textual-form LLVM IR and we’ll also be able to use the LLVM C API in Java via the FFM API.&lt;/p&gt;
    &lt;head rend="h2"&gt;LLVM IR&lt;/head&gt;
    &lt;p&gt;LLVM IR is a strongly-typed, SSA-based intermediate language. It abstracts away most machine-specific details, making it easier to represent high-level constructs in a compiler-friendly format. There are three equivalent representations of the IR: an in-memory format, a bitcode format for serialisation and a human readable assembly language representation.&lt;/p&gt;
    &lt;p&gt;The textual form of the LLVM IR for our “Hello, World!” looks like this:&lt;/p&gt;
    &lt;code&gt;
@str = private constant [14 x i8] c"Hello, World!\00"

declare i32 @puts(ptr)

define i32 @main() {
  call i32 @puts(ptr @str)
  ret i32 0
}
&lt;/code&gt;
    &lt;p&gt;Eventually, we’ll generate this via Java but, for now, if you save this in a file called helloworld.ll you can try executing it with the LLVM interpreter, lli:&lt;/p&gt;
    &lt;code&gt;
$ lli helloworld.ll
Hello, World!
&lt;/code&gt;
    &lt;p&gt;There are a few types of entities used in the helloworld.ll example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A global variable containing the string “Hello World!”&lt;/item&gt;
      &lt;item&gt;A declaration of the external libc puts function&lt;/item&gt;
      &lt;item&gt;A definition of the main function&lt;/item&gt;
      &lt;item&gt;Instructions to call puts and return an integer exit code&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can dive deeper into the LLVM “Hello, World!” example here if you like before continuing to the next section, where we’ll start using the Java FFM API.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Java FFM API?&lt;/head&gt;
    &lt;p&gt;The Foreign Function and Memory (FFM) API enables Java programs to interoperate with code and data outside the Java runtime. The API is a replacement for the older JNI API that enables Java programs to call native libraries in a safer way. The API can be used to call foreign functions and safely access foreign memory that is not managed by the JVM.&lt;/p&gt;
    &lt;p&gt;A companion to the FFM API is a tool named jextract that can automatically generate Java bindings from a C header file. &lt;code&gt;jextract&lt;/code&gt; parses C header files and automatically generates the Java source code with method handles and type-safe FFM bindings.&lt;/p&gt;
    &lt;p&gt;We’ll use the &lt;code&gt;jextract&lt;/code&gt; tool to generate bindings for the LLVM C API and those bindings will allow us to call the LLVM API from Java.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;First, let’s create a simple project to start. We’ll use maven to build our project but you can use another build tool if you like, it’s not important:&lt;/p&gt;
    &lt;code&gt;
$ mvn archetype:generate -DgroupId=com.example -DartifactId=jvm-llvm-helloworld -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
&lt;/code&gt;
    &lt;p&gt;Once you have a project skeleton, update the pom.xml file to set the Java version &amp;gt;= 22:&lt;/p&gt;
    &lt;code&gt;
 &amp;lt;properties&amp;gt;
    &amp;lt;maven.compiler.source&amp;gt;25&amp;lt;/maven.compiler.source&amp;gt;
    &amp;lt;maven.compiler.target&amp;gt;25&amp;lt;/maven.compiler.target&amp;gt;
 &amp;lt;/properties&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Then build and run the program to check everything is OK:&lt;/p&gt;
    &lt;code&gt;
$ mvn clean install
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App
Hello World!
&lt;/code&gt;
    &lt;p&gt;The maven generated sample already printed “Hello, World!” but that’s too easy! We’ll remove that and generate it via LLVM in the following sections.&lt;/p&gt;
    &lt;p&gt;Let’s now create the LLVM bindings using &lt;code&gt;jextract&lt;/code&gt; so that we can use the LLVM API.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating LLVM bindings&lt;/head&gt;
    &lt;p&gt;We’ll use jextract to generate bindings from the LLVM C API header files. Make sure LLVM is available on your system (see Installing LLVM above) and you’ll also need to download jextract.&lt;/p&gt;
    &lt;p&gt;The following jextract command (on Linux) will create Java bindings for the specified LLVM C headers, placing the generated code into the &lt;code&gt;com.example.llvm&lt;/code&gt; package within the &lt;code&gt;src/main/java&lt;/code&gt; directory, with the main header class named &lt;code&gt;LLVM&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;
$ jextract -l LLVM-20 -I /usr/include/llvm-c-20 \
     -I /usr/include/llvm-20 \
     -t com.example.llvm \
     --output src/main/java \
     --header-class-name LLVM \
     /usr/include/llvm-c-20/llvm-c/Core.h \
     /usr/include/llvm-c-20/llvm-c/Support.h \
     /usr/include/llvm-c-20/llvm-c/ExecutionEngine.h \
     /usr/include/llvm-c-20/llvm-c/Target.h \
     /usr/include/llvm-c-20/llvm-c/TargetMachine.h
&lt;/code&gt;
    &lt;p&gt;To test the generated bindings, let’s print the LLVM version using the static method generated for LLVM version string constant: edit the sample’s App.java file to print the version using the following:&lt;/p&gt;
    &lt;p&gt;If you run this, you’ll see the LLVM version printed:&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar --enable-native-access=ALL-UNNAMED com.example.App
LLVM version: 20.0.0
&lt;/code&gt;
    &lt;p&gt;Note the use of &lt;code&gt;--enable-native-access=ALL-UNNAMED&lt;/code&gt; to prevent warnings about native code access; I’ll omit this for brevity in later commands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Segments&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;LLVM_VERSION_STRING&lt;/code&gt; method returns a MemorySegment rather than a Java String. In the FFM API, a &lt;code&gt;MemorySegment&lt;/code&gt; represents a contiguous region of memory—either on or off the Java heap—enabling safe, structured access to native memory.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at the implementation in the generated source file:&lt;/p&gt;
    &lt;code&gt;
   public static MemorySegment LLVM_VERSION_STRING() {
    class Holder {
      static final MemorySegment LLVM_VERSION_STRING
         = LLVM.LIBRARY_ARENA.allocateFrom("20.0.0");
    }
    return Holder.LLVM_VERSION_STRING;
  }
&lt;/code&gt;
    &lt;p&gt;This method allocates memory containing the version string that contains the version number. The allocated MemorySegment is returned from the method and to get the String back into Java-land we need to call &lt;code&gt;getString(0)&lt;/code&gt; on the memory segment which reads a null-terminated string at the given offset (&lt;code&gt;0&lt;/code&gt;), using the UTF-8 charset.&lt;/p&gt;
    &lt;p&gt;Memory segments are managed through arenas (such as the &lt;code&gt;LLVM.LIBRARY_ARENA&lt;/code&gt; in the code above), which bridge Java’s managed heap and foreign memory spaces by applying familiar resource management patterns like try-with-resources.&lt;/p&gt;
    &lt;p&gt;Since we’ll need to allocate native memory, let’s declare an Arena:&lt;/p&gt;
    &lt;code&gt;
 public static void main(String[] args)
 {
    try (Arena arena = Arena.ofConfined()) {
       // TODO
    }
 }
&lt;/code&gt;
    &lt;head rend="h2"&gt;Creating an LLVM module&lt;/head&gt;
    &lt;p&gt;As a reminder, we need to recreate the following LLVM IR via the LLVM C API:&lt;/p&gt;
    &lt;code&gt;
declare i32 @puts(ptr)

@str = constant [14 x i8] c"Hello, World!\00"

define i32 @main() {
  call i32 @puts(ptr @str)
  ret i32 0
}
&lt;/code&gt;
    &lt;p&gt;Let’s start by creating an LLVM module – the container for all functions and globals – and print it so that we can run it through the LLVM interpreter:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // TODO: Fill in the module
            
  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;If we execute this now, we’ll see an empty IR module:&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App
; ModuleID = 'hello'
source_filename = "hello"
&lt;/code&gt;
    &lt;p&gt;If you pass this output through the LLVM interpreter, you’ll see that it tries to execute the module but cannot find the entry point main function:&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App | lli
Symbols not found: [ main ]
&lt;/code&gt;
    &lt;p&gt;We now have an LLVM module, but it has no executable code – the interpreter rightly complains that main is missing; so let’s add the main function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding a main function&lt;/head&gt;
    &lt;p&gt;The entry point to our program is the function named main which takes no parameters and returns an integer exit code, where a non-negative integer denotes success. We can add a function to the module using the LLVMAddFunction function, along with the LLVMFunctionType and LLVMInt32Type functions to create the function type.&lt;/p&gt;
    &lt;p&gt;Notice that all of these functions return a &lt;code&gt;MemorySegment&lt;/code&gt; and all 3 &lt;code&gt;LLVMAddFunction&lt;/code&gt; parameters are &lt;code&gt;MemorySegment&lt;/code&gt;s.&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

    // TODO: Add the code

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;If you execute this now you’ll see a declaration of the main function but it has no body so the LLVM interpreter will produce the same error:&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App
; ModuleID = 'hello'
source_filename = "hello"

declare i32 @main()

$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App|lli
Symbols not found: [ main ]
&lt;/code&gt;
    &lt;p&gt;Next we’ll add some instructions to the body of the function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding an entry basic block&lt;/head&gt;
    &lt;p&gt;In order to add code to a function we need to add at least 1 basic block – the entry block. A basic block is a sequence of instructions within a function that executes straight through from start to finish, with no branches in the middle. These blocks form the nodes of the Control-Flow Graph (CFG), and they connect to each other based on how control flows between them.&lt;/p&gt;
    &lt;p&gt;Basic blocks can be added to a function with the LLVMAppendBasicBlock function:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 
	  // TODO: Add the instructions

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;If you run the program through &lt;code&gt;lli&lt;/code&gt; now, you’ll see a different error:&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App | lli
lli: &amp;lt;stdin&amp;gt;:6:1: error: expected instruction opcode
}
&lt;/code&gt;
    &lt;p&gt;That makes sense, we don’t yet have any instructions in our function!&lt;/p&gt;
    &lt;head rend="h2"&gt;Building instructions&lt;/head&gt;
    &lt;p&gt;To add instructions, we first create an instruction builder using the LLVMCreateBuilder function. This gives us an LLVMBuilder that we can use to insert new instructions into a basic block.&lt;/p&gt;
    &lt;p&gt;We’ll also use the LLVMPositionBuilderAtEnd function to position the builder at the end of the entry block and LLVMBuildRet to build a return instruction:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

    // TODO: Call puts “Hello, World!”

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;If you run the program and pass the output through &lt;code&gt;lli&lt;/code&gt; now, you’ll see nothing happen:&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App | lli
&lt;/code&gt;
    &lt;p&gt;Great news – the errors are gone! Checking the return code confirms the program exited successfully, returning 0.&lt;/p&gt;
    &lt;code&gt;
$ echo $?
0
&lt;/code&gt;
    &lt;p&gt;Try changing the 0 to some other number to confirm that the value is indeed coming from the exit code returned by the LLVM IR program!&lt;/p&gt;
    &lt;head rend="h2"&gt;Global variables&lt;/head&gt;
    &lt;p&gt;A global variable, defined at the top-level in LLVM IR, defines a region of memory with a fixed address that is allocated when the program is loaded, rather than dynamically at runtime. Globals can be declared as constant if their values will never change.&lt;/p&gt;
    &lt;p&gt;We’ll add the string “Hello, World!” to our LLVM program as a global constant.&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // TODO: Call puts “Hello, World!”

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;We don’t use the &lt;code&gt;hello_str&lt;/code&gt; yet so running &lt;code&gt;lli&lt;/code&gt; would produce the same as before, but you can see the string is now declared in the LLVM IR (prefixed with @ because it is a global, like the main function):&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App 
; ModuleID = 'hello'
source_filename = "hello"

@hello_str = private unnamed_addr constant [14 x i8] c"Hello, World!\00", align 1

define i32 @main() {
entry:
  ret i32 0
}
&lt;/code&gt;
    &lt;p&gt;Let’s add the final instruction next – a call to &lt;code&gt;puts&lt;/code&gt; to print the string.&lt;/p&gt;
    &lt;head rend="h2"&gt;Calling functions&lt;/head&gt;
    &lt;p&gt;Before we can call the libc puts function we must declare it in the module by first building the function type and then calling &lt;code&gt;LLVMAddFunction&lt;/code&gt; to add it to the module:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // TODO: Call puts “Hello, World!”

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;Now that we’ve declared the function we can call it with the &lt;code&gt;@hello_str&lt;/code&gt; global as a parameter using the LLVMBuildCall2 function:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;Running the program’s output through &lt;code&gt;lli&lt;/code&gt; will finally display the expected result: “Hello, World!”:&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App | lli
Hello, World!
&lt;/code&gt;
    &lt;p&gt;Congratulations, you’ve successfully used the Java FFM API to call the LLVM C API to build an LLVM module that contains code to print “Hello, World!”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Just-in-time (JIT) Compilation&lt;/head&gt;
    &lt;p&gt;So far, we’ve been printing LLVM IR and letting &lt;code&gt;lli&lt;/code&gt; execute it. But LLVM also exposes a JIT compiler API, allowing us to generate and execute machine code in-memory. Let’s see how to JIT our “Hello, World!” directly from Java.&lt;/p&gt;
    &lt;p&gt;LLVM IR is target independent but once we start compiling to native code we must know which machine we are targeting. We’ll target x86 Linux in the following code; if you’re using ARM, Mac or Windows you’ll need to adjust the code for your machine.&lt;/p&gt;
    &lt;p&gt;The first step is to initialise and create an LLVM JIT compiler for the target machine:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

    // Initialize LLVM JIT + x86 Target
    LLVMLinkInMCJIT();
    LLVMInitializeX86Target();
    LLVMInitializeX86TargetInfo();
    LLVMInitializeX86TargetMC();
    LLVMInitializeX86AsmPrinter();
    LLVMInitializeX86AsmParser();

    // Create JIT execution engine
    var jitCompiler = arena.allocate(ADDRESS);
    var jitErrorMsgPtrPtr = arena.allocate(ADDRESS);
    LLVMCreateJITCompilerForModule(jitCompiler, module, /* optimization level = */ 2, jitErrorMsgPtrPtr);

    // Disable the IR printing now
  	// var llvmIrCharPtr = LLVMPrintModuleToString(module);
    //
    // try {
    //  System.out.println(llvmIrCharPtr.getString(0));
    // } catch (Exception e) {
    //   System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    // }

	   // Clean up LLVM resources
     // LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;LLVMCreateJITCompilerForModule&lt;/code&gt; sets up a JIT execution engine to compile an LLVM module to native machine code. &lt;code&gt;LLVMCreateJITCompilerForModule&lt;/code&gt; will return a 1 upon failure and then we can check the error message string for more information but to simplify things we’ll ignore error handling for now. &lt;/p&gt;
    &lt;p&gt;Requesting the address of the main function triggers its compilation – LLVM generates the machine code only when it’s first needed, hence the name Just-In-Time compilation. We can retrieve a pointer to the compiled function using &lt;code&gt;LLVMGetPointerToGlobal&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

    // Initialize LLVM JIT + x86 Target
    LLVMLinkInMCJIT();
    LLVMInitializeX86Target();
    LLVMInitializeX86TargetInfo();
    LLVMInitializeX86TargetMC();
    LLVMInitializeX86AsmPrinter();
    LLVMInitializeX86AsmParser();

    // Create JIT execution engine
    var jitCompiler = arena.allocate(ADDRESS);
    var jitErrorMsgPtrPtr = arena.allocate(ADDRESS);
    LLVMCreateJITCompilerForModule(jitCompiler, module, /* optimization level = */ 2, jitErrorMsgPtrPtr);

    var executionEngine = jitCompiler.get(ADDRESS, 0);
    var addressOfMainFunc = LLVMGetPointerToGlobal(executionEngine, mainFunc);

    // Disable the IR printing now
  	// var llvmIrCharPtr = LLVMPrintModuleToString(module);
    //
    // try {
    //  System.out.println(llvmIrCharPtr.getString(0));
    // } catch (Exception e) {
    //   System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    // }

	   // Clean up LLVM resources
     // LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;Now that we’ve compiled the function, we need a way to invoke it from Java. To do this, we use the foreign linker to create a &lt;code&gt;MethodHandle&lt;/code&gt; for the JIT-compiled main function. This handle acts as a callable reference to the native code:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

    // Initialize LLVM JIT + x86 Target
    LLVMLinkInMCJIT();
    LLVMInitializeX86Target();
    LLVMInitializeX86TargetInfo();
    LLVMInitializeX86TargetMC();
    LLVMInitializeX86AsmPrinter();
    LLVMInitializeX86AsmParser();

    // Create JIT execution engine
    var jitCompiler = arena.allocate(ADDRESS);
    var jitErrorMsgPtrPtr = arena.allocate(ADDRESS);
    LLVMCreateJITCompilerForModule(jitCompiler, module, /* optimization level = */ 2, jitErrorMsgPtrPtr);

    var executionEngine = jitCompiler.get(ADDRESS, 0);
    var addressOfMainFunc = LLVMGetPointerToGlobal(executionEngine, mainFunc);

    // Create method handle to the int main() function that
    // we just created and compiled.
    var functionHandle = Linker.nativeLinker().downcallHandle(
        addressOfMainFunc,
        FunctionDescriptor.of(/* returnType = */ JAVA_INT)
    );

    // Disable the IR printing now
  	// var llvmIrCharPtr = LLVMPrintModuleToString(module);
    //
    // try {
    //  System.out.println(llvmIrCharPtr.getString(0));
    // } catch (Exception e) {
    //   System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    // }

	   // Clean up LLVM resources
     // LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;downcallHandle&lt;/code&gt; method tells Java how to interpret the native function’s signature – in this case, a function that takes no arguments and returns an int.&lt;/p&gt;
    &lt;p&gt;Now we can invoke the compiled native function directly from Java, just like a regular method call:&lt;/p&gt;
    &lt;code&gt;
public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

    // Initialize LLVM JIT + x86 Target
    LLVMLinkInMCJIT();
    LLVMInitializeX86Target();
    LLVMInitializeX86TargetInfo();
    LLVMInitializeX86TargetMC();
    LLVMInitializeX86AsmPrinter();
    LLVMInitializeX86AsmParser();

    // Create JIT execution engine
    var jitCompiler = arena.allocate(ADDRESS);
    var jitErrorMsgPtrPtr = arena.allocate(ADDRESS);
    LLVMCreateJITCompilerForModule(jitCompiler, module, /* optimization level = */ 2, jitErrorMsgPtrPtr);

    var executionEngine = jitCompiler.get(ADDRESS, 0);
    var addressOfMainFunc = LLVMGetPointerToGlobal(executionEngine, mainFunc);

    // Create method handle to the int main() function that
    // we just created and compiled.
    var functionHandle = Linker.nativeLinker().downcallHandle(
        addressOfMainFunc,
        FunctionDescriptor.of(/* returnType = */ JAVA_INT)
    );

    // Execute the main function via the method handle.
    try {
      int result = (int) functionHandle.invoke();
      System.out.println("main() returned: " + result);
    } catch (Throwable e) {
      System.err.println("Error calling JIT function: " + e.getMessage());
    }

    // Disable the IR printing now
  	// var llvmIrCharPtr = LLVMPrintModuleToString(module);
    //
    // try {
    //  System.out.println(llvmIrCharPtr.getString(0));
    // } catch (Exception e) {
    //   System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    // }

	   // Clean up LLVM resources
     // LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
&lt;/code&gt;
    &lt;p&gt;When &lt;code&gt;functionHandle.invoke()&lt;/code&gt; runs, Java crosses into the native world and calls the machine code that was just compiled by the LLVM JIT compiler.&lt;/p&gt;
    &lt;p&gt;And that’s it, you can now run the Java application without the LLVM interpreter and see the resulting “Hello, World!”:&lt;/p&gt;
    &lt;code&gt;
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App 
Hello, World!
&lt;/code&gt;
    &lt;p&gt;Congratulations, you’ve now JIT-compiled Hello World, with the help of Java’s FFM API calling LLVM’s C API.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;In this Java advent we built and executed native machine code from pure Java and a little help from LLVM – no JNI, no C glue, just memory segments, method handles, and a modern FFI. By the end, we had just a simple program that prints “Hello, World!” but it shows the potential of the Java FFM API and the things you can do when Java and native code work together.&lt;/p&gt;
    &lt;p&gt;Now see what else you can do, for example, try generating other instructions: print more text, do simple calculations, or even build tiny programs entirely in LLVM from Java.&lt;/p&gt;
    &lt;p&gt;The full code for this post is available on GitHub over here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.javaadvent.com/2025/12/java-hello-world-llvm-edition.html"/><published>2025-12-07T11:51:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46181231</id><title>Google Titans architecture, helping AI have long-term memory</title><updated>2025-12-07T16:11:06.238070+00:00</updated><content>&lt;doc fingerprint="3ef3d18a9c53810a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Titans + MIRAS: Helping AI have long-term memory&lt;/head&gt;
    &lt;p&gt;December 4, 2025&lt;/p&gt;
    &lt;p&gt;Ali Behrouz, Student Researcher, Meisam Razaviyayn, Staff Researcher, and Vahab Mirrokni, VP and Google Fellow, Google Research&lt;/p&gt;
    &lt;p&gt;We introduce the Titans architecture and the MIRAS framework, which allow AI models to work much faster and handle massive contexts by updating their core memory while it's actively running.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick links&lt;/head&gt;
    &lt;p&gt;The Transformer architecture revolutionized sequence modeling with its introduction of attention, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.&lt;/p&gt;
    &lt;p&gt;The research community explored various approaches for solutions, such as efficient linear recurrent neural networks (RNNs) and state space models (SSMs) like Mamba-2. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.&lt;/p&gt;
    &lt;p&gt;In two new papers, Titans and MIRAS, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.&lt;/p&gt;
    &lt;p&gt;The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Titans: Learning new context on the fly&lt;/head&gt;
    &lt;p&gt;An effective learning system requires distinct yet interconnected memory modules, mirroring the human brain's separation of short-term and long-term memory.&lt;/p&gt;
    &lt;p&gt;While attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural long-term memory module, that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a multi-layer perceptron). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn't simply taking notes; it's understanding and synthesizing the entire story.&lt;/p&gt;
    &lt;p&gt;Crucially, Titans doesn’t just passively store data. It actively learns how to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.&lt;/p&gt;
    &lt;p&gt;In the context of Titans, the "surprise metric" is the model detecting a large difference between what it currently remembers and what the new input is telling it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Low surprise: If the new word is "cat" and the model's memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word "cat" in its permanent long-term state.&lt;/item&gt;
      &lt;item&gt;High surprise: If the model's memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model uses this internal error signal (the gradient) as a mathematical equivalent of saying, "This is unexpected and important!" This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.&lt;/p&gt;
    &lt;p&gt;Titans refines this mechanism by incorporating two critical elements:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Momentum: The model considers both "momentary surprise" (the current input) and "past surprise" (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.&lt;/item&gt;
      &lt;item&gt;Forgetting (weight decay): To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;MIRAS: A unified view of sequence modeling&lt;/head&gt;
    &lt;p&gt;Every major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex associative memory module.&lt;/p&gt;
    &lt;p&gt;Accordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten.&lt;/p&gt;
    &lt;p&gt;MIRAS defines a sequence model through four key design choices:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memory architecture: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).&lt;/item&gt;
      &lt;item&gt;Attentional bias: The internal learning objective the model optimizes that determines what it prioritizes.&lt;/item&gt;
      &lt;item&gt;Retention gate: The memory regularizer. MIRAS reinterprets "forgetting mechanisms" as specific forms of regularization that balance new learning against retaining past knowledge.&lt;/item&gt;
      &lt;item&gt;Memory algorithm: The optimization algorithm used to update the memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Transcending the mean squared error paradigm&lt;/head&gt;
    &lt;p&gt;Virtually all successful existing sequence models rely on mean squared error (MSE) or dot-product similarity for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.&lt;/p&gt;
    &lt;p&gt;MIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with non-Euclidean objectives and regularization.&lt;/p&gt;
    &lt;p&gt;Using MIRAS, we created three specific attention-free models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;YAAD: We designed this MIRAS variant to be less sensitive to major errors or "outliers" (like a single typo in a large document). It uses a gentler math penalty (Huber loss) for mistakes, so it doesn't overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.&lt;/item&gt;
      &lt;item&gt;MONETA: This model explores the use of more complex and strict mathematical penalties (called generalized norms). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.&lt;/item&gt;
      &lt;item&gt;MEMORA: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on mean squared error (MSE) or dot-product similarity for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Experiments and results&lt;/head&gt;
    &lt;p&gt;We rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including Transformer++, Mamba-2, and Gated DeltaNet. We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.&lt;/p&gt;
    &lt;p&gt;Across both standard language modeling datasets (C4, WikiText) and zero-shot reasoning tasks (HellaSwag, PIQA), our models consistently demonstrated higher accuracy and perplexity (a measure of how surprised an LLM is when looking at a piece of text).&lt;/p&gt;
    &lt;head rend="h3"&gt;The power of deep memory&lt;/head&gt;
    &lt;p&gt;Ablation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Language modeling and efficiency&lt;/head&gt;
    &lt;p&gt;In language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extreme long-context recall&lt;/head&gt;
    &lt;p&gt;The most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the BABILong benchmark, a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/"/><published>2025-12-07T12:23:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46181237</id><title>How the Disappearance of Flight 19 Fueled the Legend of the Bermuda Triangle</title><updated>2025-12-07T16:11:06.094897+00:00</updated><content>&lt;doc fingerprint="76c2dc531122038a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How the Disappearance of Flight 19, a Navy Squadron Lost in 1945, Fueled the Legend of the Bermuda Triangle&lt;/head&gt;
    &lt;head rend="h2"&gt;Eighty years ago, five planes vanished during a training run off the Florida coast. A patrol plane sent to search for the men went missing, too, giving rise to a host of conspiracy theories&lt;/head&gt;
    &lt;p&gt;“I don’t know where we are,” a voice on the radio said. “We must have got lost after that last turn.”&lt;/p&gt;
    &lt;p&gt;This message was the first inkling that something was amiss with Flight 19, a routine United States Navy training run off the coast of Florida on December 5, 1945. In the confusing hours that followed, the five torpedo bombers’ radio transmissions grew fainter as their fuel supply dwindled. Soon, the men—a mix of Marines and naval aviators—were no longer able to communicate with land, and two naval patrol bombers were dispatched to search for them. A short time later, one of the rescue planes abruptly dropped off radio contact and disappeared, too.&lt;/p&gt;
    &lt;p&gt;In total, six aircraft carrying 27 men (14 from Flight 19 and 13 from the patrol bomber) vanished over the so-called Bermuda Triangle that day. No confirmed trace of them has ever been found. Eighty years later, Flight 19 remains one of aviation’s most notable mysteries—and where answers are elusive, legions of theories have emerged to fill in the blanks.&lt;/p&gt;
    &lt;p&gt;From alien abductions to the lost continent of Atlantis, magnetic anomalies, methane eruptions and time travel, highly improbable explanations for the disappearances of planes and ships lost in the Bermuda Triangle have gripped the public’s imagination. Flight 19 has even appeared in both a Steven Spielberg film and a Scooby-Doo mystery.&lt;/p&gt;
    &lt;p&gt;Yet the real postwar tragedy that inspired these fictionalizations is a mystery that denies the imagination a satisfying explanation, supernatural or otherwise. Instead, false assumptions, fleeting communications and a hastily scrambled rescue search left behind clues as to what transpired in the final moments of Flight 19—and exposed a series of decisions that, if they’d unfolded differently, might have brought the men home safely.&lt;/p&gt;
    &lt;head rend="h2"&gt;A minute-by-minute breakdown of the disappearance of Flight 19&lt;/head&gt;
    &lt;p&gt;At 2:10 p.m. on December 5, flight leader Charles Carroll Taylor, a 28-year-old lieutenant known as “C.C.,” took off from Naval Air Station (NAS) Fort Lauderdale. He was accompanied by four other planes. The squadron’s flight plan advised the men to practice bombing on a small grouping of rocks near the Bahamian island of Bimini, a distance of 56 miles, then fly up over the northern islands of the Bahamas before turning back east to Fort Lauderdale—a routine navigation exercise intended to last around 2 hours and 40 minutes.&lt;/p&gt;
    &lt;p&gt;Lieutenant Robert F. Cox, the pilot of an unrelated flight coming out of the same air station, picked up that first concerning transmission shortly before 3:45 p.m., according to declassified records digitized by the National Archives. In a later interview with investigators, Cox recounted establishing contact with Taylor, who informed him that “both my compasses are out. … I am over land, but it’s broken. I’m sure I’m in the [Florida] Keys, but I don’t know how far down, and I don’t know how to get to Fort Lauderdale.”&lt;/p&gt;
    &lt;p&gt;Cox, who didn’t yet realize that Flight 19 couldn’t have made it as far south as the Keys, advised Taylor to fly north, with the setting sun visible to the west on his left wing. He also offered to fly toward the lost pilot’s location to pick the men up. Taylor waved Cox off, reassuring his fellow lieutenant that he now knew where he was. “Don’t come after me,” Taylor said—a statement that would later assume mythical significance in the supernatural theories that proliferated after Flight 19 vanished.&lt;/p&gt;
    &lt;p&gt;But Cox noticed something strange: The squadron’s transmissions were growing weaker. He realized that Taylor’s formation couldn’t be flying north from the Keys, which would have brought the group closer to his own position off Fort Lauderdale, on Florida’s east coast. Instead, the five Grumman TBF Avengers seemed to be straying farther from the coastline, toward the Bahamas and the vast expanse of the Atlantic Ocean.&lt;/p&gt;
    &lt;p&gt;A flurry of confused communications ensued among ground units and those trying to advise Flight 19. Around 4:25 p.m., Taylor asked if anyone in the area had a radar screen that could pick up the group’s location in real time, but no such technology was readily available. An air-sea task unit based in Port Everglades, a seaport in Fort Lauderdale, alerted pilots and potential rescuers along the Florida coast of the squadron’s situation, simultaneously asking about available navigation equipment and requesting that Taylor switch to a less-trafficked emergency radio frequency.&lt;/p&gt;
    &lt;p&gt;The sheer volume of radio communications makes it difficult to know how much of the back and forth actually got through to Flight 19, but subsequent accident reports highlighted efforts to advise Taylor and his men to fly west, “into the sun.” “If these directions had been heard and carried out,” investigators later concluded, “we are certain this flight would have returned to base safely.”&lt;/p&gt;
    &lt;p&gt;Around 4:45 p.m., Taylor relayed that the group planned to “fly north to make sure we are not over the Gulf of Mexico,” off Florida’s west coast. Half an hour later, he seemed to concur with outside observers’ advice, stating that the men would fly west “until we hit the beach or run out of gas.” Taylor reminded the other Avengers to remain in formation and, if gas got low, to ditch together.&lt;/p&gt;
    &lt;p&gt;At 6:04 p.m., Taylor made a crucial decision—one that likely doomed Flight 19. He concluded that they “didn’t go far enough east—turn around again—we may just as well turn around and go east again.” As lights were being lit at air stations and on ships up and down the Florida coast in hopes of guiding the men home, they were unknowingly flying in the opposite direction.&lt;/p&gt;
    &lt;p&gt;When ground units were finally able to calculate a navigational fix (an estimate of the squadron’s coordinates) around 6 p.m., it was estimated to be within 100 miles of 29 degrees north latitude, 79 degrees west longitude—well north of the Bahamas and roughly east of Daytona Beach, Florida.&lt;/p&gt;
    &lt;p&gt;In retrospect, it is an agonizing irony that by this point, rescue units could no longer reach Flight 19 directly, but they could still hear garbled transmissions between the five planes. The last known message came from Ensign Joseph Bossi, one of the planes’ pilots, who tried to reach Taylor at 7:04 p.m. No response was heard.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened to Flight 19?&lt;/head&gt;
    &lt;p&gt;Exactly what happened next remains a mystery. Did the men stay together, run out of fuel and successfully ditch their planes? If so, how long did they survive in the water? Taylor had been involved in three previous ditches, and the Avengers were almost certainly equipped with life rafts.&lt;/p&gt;
    &lt;p&gt;The Navy’s Board of Investigation later concluded “that the state of the sea in the area in which Flight 19 was presumed to have landed was rough and unfavorable for a water landing.”&lt;/p&gt;
    &lt;p&gt;The most likely outcome is that the men “ran out of gas and went into the ocean,” says John Bloom, director of the Naval Air Station Fort Lauderdale Museum, which hosts an annual memorial ceremony on the anniversary of the disappearance. But any certainty of what transpired after all communications ended was lost along with the men who died on Flight 19.&lt;/p&gt;
    &lt;p&gt;Those killed included five pilots and nine crew members and trainees, some of them still teenagers, yet also veterans of World War II, which had ended just three months earlier. Taylor had more than 2,500 hours of flying experience, 61 of them in combat operations. Edward Joseph Powers Jr., a 26-year-old Marine captain, left behind a wife and child.&lt;/p&gt;
    &lt;p&gt;The tragedy of losing these 14 men was soon compounded after a fellow Navy pilot, Lieutenant Walter G. Jeffrey, volunteered to fly one of two Martin PBM-5 Mariners that departed from NAS Banana River, around 150 miles north of the Fort Lauderdale station, at 7:27 p.m., in the direction of the Avengers’ last presumed location.&lt;/p&gt;
    &lt;p&gt;Within half an hour, the Mariner suddenly fell off radio contact. Later that night, a ship in the vicinity reported seeing a “burst of flames” estimated to be 100 feet tall around 7:50 p.m. By the time searchers reached the site, no trace of the Mariner or its 13-man crew could be found. (Mariners were sometimes dubbed “flying gas tanks” due to their high fuel capacity and propensity to experience onboard fires.)&lt;/p&gt;
    &lt;p&gt;With yet another plane missing, the search took on an additional layer of complexity. It lasted five days, with more than 200 planes scouring the Atlantic each day in hopes of finding the 27 men. Sightings of flares and flashes, of life jackets and rafts, fueled hope, even if they turned out to be imagined or unrelated debris. The cycle of hope and disappointment made headlines: On December 9, for example, the United Press news agency reported the discovery of six rafts, but officials soon retracted the sighting as a mistake. The Associated Press deemed the rescue mission “the most extensive search ever undertaken along the Atlantic Seaboard.”&lt;/p&gt;
    &lt;head rend="h2"&gt;A mother’s fight to clear her son’s name&lt;/head&gt;
    &lt;p&gt;When Taylor’s mother, Katherine Taylor, received a telegram informing her that her son was missing, she was at the front of her classroom in Corpus Christi, the Texas city where she’d raised him. She was so devastated by the news that she walked out, never to return.&lt;/p&gt;
    &lt;p&gt;One of the central questions asked by both Katherine and the Navy was how Taylor, a seasoned pilot with extensive flying experience, could become so disoriented that he believed he was over the Keys. Within days, the Navy convened an investigation board, which focused on the testimony of those involved in the search along the coast.&lt;/p&gt;
    &lt;p&gt;Investigators’ initial report, issued internally in January 1946, acknowledged that elements of the search protocol could have been improved, including coordination between units and rescuers’ ability to get a bearing on the flights. But it heavily weighted Taylor’s disorientation, noting that he wasn’t sure where the Florida peninsula was at the time, and that “this uncertainty influenced his later decisions.”&lt;/p&gt;
    &lt;p&gt;Bloom points out that the area was unfamiliar terrain for Taylor, who had transferred up from Miami the previous month. This may explain the pilot’s belief that he was over the Keys instead of the Bahamas. The squadron “never said another word about their compass not working,” Bloom says. “But then they did fly north thinking they’d see the peninsula, and when they didn’t, they thought they were out over the Gulf, so [Taylor] flies east.” What would have been obvious to aviators who had been based at the air station longer—that it would have been impossible for the group to be over the Keys so soon after taking off—seemed to be less apparent to the members of Flight 19 in their duress and confusion.&lt;/p&gt;
    &lt;p&gt;The Navy was less magnanimous in its assessment of Taylor. “The leader of the flight became so hopelessly confused as to have suffered something akin to a mental aberration,” the chief of naval air training wrote.&lt;/p&gt;
    &lt;p&gt;That conclusion did not sit well with Katherine. She believed there were holes in the Navy’s official version, which relied heavily on rescuers’ testimony in the absence of physical evidence and the aviators’ own accounts. As Katherine wrote in a letter to the mother of Walter Reed Parpart Jr., the 18-year-old radioman with Taylor on his plane, she didn’t believe the planes had been properly instructed to come west, or that their approximate location was ascertained and communicated to searchers in a timely fashion.&lt;/p&gt;
    &lt;p&gt;“It just can’t be possible that so many could disappear without a trace,” Katherine told a reporter in March 1946, after taking a trip to Florida to interview those involved in the rescue effort. “I shall never give up until I find out why!” She proved to be a tenacious advocate for this viewpoint, and officials concluded that she could not be swayed. “It is believed that Mrs. Taylor is emotionally unstable as a result of this disaster,” the Navy noted in its report.&lt;/p&gt;
    &lt;p&gt;One of Katherine’s key contentions centered around the ready plane, a standby aircraft that could have been sent up quickly from NAS Fort Lauderdale. In his testimony, Cox—the pilot who’d first realized Flight 19 was lost—testified that he’d returned to the station and asked Lieutenant Commander Donald J. Poole, the officer in charge of training flights there, if he could take the ready plane and fly northeast to the squadron’s likely position. Cox recalled that Poole “very definitely said no, that he didn’t think there was any use in sending it out then.”&lt;/p&gt;
    &lt;p&gt;In his own testimony, Poole outlined several reasons for his response, including waiting on the fix and fear that sending an additional plane would “complicate” communications from the ground. None of these explanations was particularly convincing to Katherine, nor to Cox himself.&lt;/p&gt;
    &lt;p&gt;“My dad felt extremely strongly that he could locate the flight if he were allowed to go back up, and he was very frustrated that he wasn’t allowed to,” Cox’s daughter, Colby Cox, said in a 2021 History Channel documentary.&lt;/p&gt;
    &lt;p&gt;Bloom, for his part, says, “They should have been proactively doing something. … The plane was up and ready.”&lt;/p&gt;
    &lt;p&gt;Poole’s testimony provides some insight into the prevailing mindset about the situation’s severity. “I was still confident and certain that they would hit the coast before their gas supply ran out,” he told investigators. Lieutenant Commander Charles Kenyon, operations officer at NAS Fort Lauderdale, similarly testified that he “figured they were temporarily confused and … they would come back right on time.”&lt;/p&gt;
    &lt;p&gt;In 1947, after tireless lobbying by Katherine, the Navy modified its findings away from the initial focus on Taylor. A corrections review board concluded “that the flight disappeared for reasons or causes unknown.” This would stand as the official word on Flight 19 in the decades that followed.&lt;/p&gt;
    &lt;p&gt;As the Navy closed the book on the case, a plethora of conspiracy theories emerged to fill the information void.&lt;/p&gt;
    &lt;head rend="h2"&gt;Flight 19 conspiracy theories and the Bermuda Triangle&lt;/head&gt;
    &lt;p&gt;In 1950, the Associated Press published an article on plane and ship disappearances off the U.S.’s southeastern coast, including Flight 19, which it erroneously suggested had disappeared under complete radio silence. “It is the same big world the ancients knew into which men and their machines and ships can disappear without a trace,” the AP noted ominously.&lt;/p&gt;
    &lt;p&gt;Fourteen years later, in 1964, the men’s magazine Argosy published an article by Vincent Gaddis titled “The Deadly Bermuda Triangle.” The piece was the first to popularize this term, which refers to a roughly 500,000- to 1.5-million-square-mile area of the North Atlantic Ocean, stretching from Florida’s east coast to Bermuda to the Greater Antilles. But the area Gaddis described had already developed a fearsome reputation among mariners of centuries past, who navigated a sometimes stormy and hurricane-prone section of the ocean, fueled by the warm currents of the Gulf Stream, without the benefit of accurate weather forecasting.&lt;/p&gt;
    &lt;head rend="h4"&gt;Did you know? The Tempest and the Bermuda Triangle&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Some scholars posit that William Shakespeare based his play The Tempest on accounts of the Sea Venture, an English ship wrecked off Bermuda in 1609.&lt;/item&gt;
      &lt;item&gt;Others disagree, suggesting the Bard was inspired by a near-shipwreck closer to home, or that he simply drew on his imagination.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As the Bermuda Triangle descriptor took hold in the popular imagination, it was applied retroactively to events that had taken place in the same radius, such as the disappearance of the USS Cyclops, a World War I transport ship that vanished in 1918 with its crew of 306 after transmitting a final message: “Weather fair. All well.”&lt;/p&gt;
    &lt;p&gt;“Whatever this menace that lurks within a triangle of tragedy so close to home,” Gaddis wrote, “it was responsible for the most incredible mystery in the history of aviation—the lost patrol.” (Despite the staying power of this nickname for Flight 19, the group was a squadron on a training run, not a patrol.) In his best-selling 1974 book, The Bermuda Triangle, Charles Berlitz, a chronicler of paranormal phenomena, deemed the loss of the Avengers and the Mariner “the first occasion in which planes were affected,” building on his long—and largely unreliable—list of ships that had vanished in the area.&lt;/p&gt;
    &lt;p&gt;The fact that the Mariner disappeared in approximately the same vicinity as Flight 19 fueled theories of supernatural intervention, with outside observers ignoring the fact that a rescue plane would naturally be sent to its target’s presumed location.&lt;/p&gt;
    &lt;p&gt;Berlitz also cited a request by Taylor to be dismissed from the assignment ahead of time as a “presentiment of disaster.” But those who actually saw Taylor on the day of the training run didn’t notice anything amiss, and the lieutenant never officially escalated his request. Berlitz also falsely relayed Taylor’s message of “don’t come after me” as “don’t come after me … they look like they are from outer space.” That line really clinched the legend of the Bermuda Triangle: “This final mystery, with its suggestion of other-world interference, is echoed in more than a few of the other disappearances,” Berlitz wrote.&lt;/p&gt;
    &lt;p&gt;Bloom, for his part, dismisses such speculation. “I don’t believe in fiction,” he says. “There’s a lot of misstatements about what was said when they were out there.”&lt;/p&gt;
    &lt;p&gt;The 1977 Spielberg film Close Encounters of the Third Kind depicted the crew of Flight 19 re-emerging years later, without aging, after being abducted by aliens. Musicians have tackled the legend of the Bermuda Triangle, too: Fleetwood Mac sang of “all of these ships and planes, a great big mystery that cannot be explained,” while Barry Manilow mused on a place that “makes people disappear.” Numerous television dramas and documentaries have sustained this block of ocean’s notoriety well into contemporary times.&lt;/p&gt;
    &lt;p&gt;In the National Oceanic and Atmospheric Administration’s view, “There is no evidence that mysterious disappearances occur with any greater frequency in the Bermuda Triangle than in any other large, well-traveled area of the ocean.” The myth is fueled by fanciful interpretations of the many unknowns of such disappearances, which can often be attributed to environmental factors like tropical storms and hurricanes, sudden changes in weather, and shallow waters in the Caribbean Sea.&lt;/p&gt;
    &lt;p&gt;The real—and perhaps most unsolvable—mysteries of Flight 19 are what Taylor experienced that day that led him off course in the first place, and what happened after the last radio call from Bossi to Taylor at 7:04 p.m. Though the emergence of conspiracy theories has obscured this, many aviation and naval tragedies have resulted from a confluence of banal mishaps and missed opportunities. What might have happened if the ready plane flew sooner? Or if the location fix had come in earlier and been heard by Flight 19?&lt;/p&gt;
    &lt;p&gt;For all the fabrications and exaggerations that Gaddis detailed in his 1964 magazine article, one statement rings all too true today: “The sea guards well her secrets.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.smithsonianmag.com/history/how-the-disappearance-of-flight-19-a-navy-squadron-lost-in-1945-fueled-the-legend-of-the-bermuda-triangle-180987759/"/><published>2025-12-07T12:25:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46181268</id><title>The Anatomy of a macOS App</title><updated>2025-12-07T16:11:05.973552+00:00</updated><content>&lt;doc fingerprint="2e8a02cb2135172c"&gt;
  &lt;main&gt;
    &lt;p&gt;Programs running in windowing environments, applications as we used to know them, have more complicated requirements than those run from a command line. Rather than embed all the resources they require for windows, menus and the rest in a single file, Mac OS broke new ground by putting those into resources stored in the app’s resource fork.&lt;/p&gt;
    &lt;p&gt;This is QuarkXPress version 4.11 from around 2000, with its resources displayed in the resource editor ResEdit. Executable code was also stored in CODE resources, and every file contained type and creator information to support the illusions created by the Finder.&lt;/p&gt;
    &lt;head rend="h4"&gt;Mac OS X&lt;/head&gt;
    &lt;p&gt;When Mac OS X was designed, it switched to the bundle structure inherited from NeXTSTEP. Instead of this multitude of resources, apps consisted of a hierarchy of directories containing files of executable code, and those with what had in Mac OS been supporting resources. Those app bundles came to adopt a standard form, shown below.&lt;/p&gt;
    &lt;p&gt;The bundle name has the extension .app, and contains a single directory Contents. Within that, the executable code is in the MacOS directory, which may contain both the main executable for the GUI app and any bundled command tools provided. Another directory contains Resources, including the app’s custom icon, and components of its GUI. In some apps, there’s another directory of Frameworks containing dylibs (libraries).&lt;/p&gt;
    &lt;p&gt;There are also two important files, Info.plist and PkgInfo. The latter contains the same type and creator information inherited from Classic Mac OS, and apparently isn’t mandatory although it appears universal. The information property list is essential, as it specifies the names of the executable and its icon file in Resources, the minimum version of macOS required, type declarations of the app’s documents, version numbers, and more.&lt;/p&gt;
    &lt;p&gt;When running a command tool in macOS, its Mach-O executable is launched by &lt;code&gt;launchd&lt;/code&gt;, whose purpose is to run code. Launching an app is more demanding, although the app’s executable is still launched by &lt;code&gt;launchd&lt;/code&gt;. Before that can happen, macOS starts the launch process using LaunchServices and RunningBoard, which rely on information obtained from Info.plist and other components in the app bundle.&lt;/p&gt;
    &lt;head rend="h4"&gt;macOS&lt;/head&gt;
    &lt;p&gt;This structure remained stable until the introduction of code signatures in Mac OS X 10.5 Leopard in 2007. Accommodating those added a directory named _CodeSignature containing the signature in a CodeResources file. That includes code directory hashes (CDHashes) to check the integrity of the contents of the app bundle. Apps distributed by the App Store include a store receipt in another directory, _MASReceipt. Since 2018, when Apple introduced notarization, the ‘ticket’ issued by Apple can be ‘stapled’ into the app bundle as the file CodeResources.&lt;/p&gt;
    &lt;p&gt;Many apps come with additional items that might in the past have been installed by them in their Library/Application Support folders and elsewhere, but are now included in the app bundle. These can include the following directories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Library, containing folders of LaunchDaemons and LoginItems that would previously have been installed in either the main Library folder, or that in the user’s Home folder;&lt;/item&gt;
      &lt;item&gt;XPCServices, for executable code that the app uses to provide specific services;&lt;/item&gt;
      &lt;item&gt;Plugins, for some types of app extension (Appex);&lt;/item&gt;
      &lt;item&gt;Extensions, for other types of app extension, including app intents.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You may also come across other components, including a version.plist in Apple’s apps.&lt;/p&gt;
    &lt;p&gt;This centralisation of components in the app bundle has brought several benefits. Being self-contained, apps are easier to install and update, and cleaner to remove. Their components are less likely to go missing, and most of all they’re held within the protection of the app’s signature and notarisation, an important improvement in security.&lt;/p&gt;
    &lt;p&gt;Assembling these into a diagram shows how the anatomy of an app has grown over the last few years.&lt;/p&gt;
    &lt;p&gt;Components shown in pale yellow are either mandatory or essentially universal. Those shown in green are found in apps distributed through the App Store, while that shown in blue is the stapled notarisation ticket (optional). You will also see additional folders and components such as Automator workflows, scripts, and others.&lt;/p&gt;
    &lt;p&gt;There is no difference in structure between apps built for current Intel and Arm architectures. That’s because binaries in the MacOS folder (and executable code in other directories like Frameworks, XPCServices and Plugins) contain platform-specific code in a single Mach-O executable. Thus, an app that’s Universal and runs native on both architectures includes code for both in its single ‘fat’ code file, and they even have separate signatures stored within common files.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eclecticlight.co/2025/12/04/the-anatomy-of-a-macos-app/"/><published>2025-12-07T12:31:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46181466</id><title>At least 50 hallucinated citations found in ICLR 2026 submissions</title><updated>2025-12-07T16:11:05.712720+00:00</updated><content>&lt;doc fingerprint="d1d20b8ecdb25ec3"&gt;
  &lt;main&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-top:1px solid #000000;border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;background-color:#d9d9d9;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;Title&lt;/cell&gt;
      &lt;cell style="border-top:1px solid #000000;border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;background-color:#d9d9d9;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;Average Review Rating&lt;/cell&gt;
      &lt;cell style="border-top:1px solid #000000;border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;background-color:#d9d9d9;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;Paper Link&lt;/cell&gt;
      &lt;cell style="border-top:1px solid #000000;border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;background-color:#d9d9d9;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;Citation Check Scan Link&lt;/cell&gt;
      &lt;cell style="border-top:1px solid #000000;border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;background-color:#d9d9d9;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;Example of Verified Hallucination&lt;/cell&gt;
      &lt;cell style="border-top:1px solid #000000;border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;background-color:#d9d9d9;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;Comment&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;TamperTok: Forensics-Driven Tokenized Autoregressive Framework for Image Tampering Localization&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;8.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:bottom;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;TamperTok: Forensics-Driven Tokenized Autoregressive Framework for Image Tampering Localization | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/4645494f-70eb-40bb-aea7-0007e13f7179/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Chong Zou, Zhipeng Wang, Ziyu Li, Nan Wu, Yuling Cai, Shan Shi, Jiawei Wei, Xia Sun, Jian Wang, and Yizhou Wang. Segment everything everywhere all at once. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;This paper exists, but all authors are wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive Text Sources&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;8.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:bottom;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive Text Sources | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/bfd10666-ea2d-454c-9ab2-75faa8b84281/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Dan Hendrycks, Collin Burns, Steven Basart, Andy Critch, Jerry Li, Dawn Ippolito, Aina Lapedriza, Florian Tramer, Rylan Macfarlane, Eric Jiang, et al. Measuring massive multitask language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;The paper and first 3 authors match. The last 7 authors are not on the paper, and some of them do not exist&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;6.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/9afb1d51-c5c8-48f2-9b75-250d95062521/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Dinghuai Zhang, Yang Song, Inderjit Dhillon, and Eric Xing. Defense against adversarial attacks using spectral regularization. In International Conference on Learning Representations (ICLR), 2020.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;6.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/e3f155d7-067a-4720-adf8-65dc9dc714b9/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Robert Huben, Logan Riggs, Aidan Ewart, Hoagy Cunningham, and Lee Sharkey. Sparse autoencoders can interpret randomly initialized transformers, 2025. URL https://arxiv.org/ abs/2501.17727.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;This paper exists, but all authors are wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Principled Policy Optimization for LLMs via Self-Normalized Importance Sampling&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;5.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Principled Policy Optimization for LLMs via Self-Normalized Importance Sampling | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/54c8aa45-c97d-48fc-b9d0-d491d54df8d3/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;David Rein, Stas Gaskin, Lajanugen Logeswaran, Adva Wolf, Oded teht sun, Jackson H. He, Divyansh Kaushik, Chitta Baral, Yair Carmon, Vered Shwartz, Sang-Woo Lee, Yoav Goldberg, C. J. H. un, Swaroop Mishra, and Daniel Khashabi. Gpqa: A graduate-level google-proof q\&amp;amp;a benchmark, 2023&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;All authors except the first are fabricated.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;PDMBench: A Standardized Platform for Predictive Maintenance Research&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;PDMBench: A Standardized Platform for Predictive Maintenance Research | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/5c55afe7-1689-480d-ac44-9502dc0f9229/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt; Andrew Chen, Andy Chow, Aaron Davidson, Arjun DCunha, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Clemens Mewald, Siddharth Murching, Tomas Nykodym, et al. Mlflow: A platform for managing the machine learning lifecycle. In Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning, pp. 1-4. ACM, 2018. &lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Authors and conference match this paper, but title is somewhat different and the year is wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/5461eefd-891e-4100-ba1c-e5419af520c0/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Chen Zhu et al. A survey on efficient deployment of large language models. arXiv preprint arXiv:2307.03744, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;The arXiv ID is real, but the paper has different authors and a different title.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/c07521cd-2757-40a2-8dc1-41382d7eb11b/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;K. Marino, R. Salakhutdinov, and A. Gupta. Fine-grained image classification with learnable semantic parts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4500-4509, 2019.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Authors and subject match this paper&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;TopoMHC: Sequence–Topology Fusion for MHC Binding&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;TopoMHC: Sequence–Topology Fusion for MHC Binding | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/8da4f86c-00d8-4d73-81dd-c168c0bfdf4e/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Yuchen Han, Yohan Kim, Dalibor Petrovic, Alessandro Sette, Morten Nielsen, and Bjoern Peters. Deepligand: a deep learning framework for peptide-mhc binding prediction. Bioinformatics, 39 (1):btac834, 2023. doi: 10.1093/bioinformatics/btac834.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Can Text-to-Video Models Generate Realistic Human Motion?&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Can Text-to-Video Models Generate Realistic Human Motion? | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/f52aad2d-2253-44bf-80ba-8e8668df650f/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Yugandhar Balaji, Jianwei Yang, Zhen Xu, Menglei Chai, Zhoutong Xu, Ersin Yumer, Greg Shakhnarovich, and Deva Ramanan. Conditional gan with discriminative filter generation for text-to-video synthesis. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI), pp. 2155-2161, July 2019. doi: 10.24963/ijcai.2019/276.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;This paper exists, but the authors and page numbers are wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;GRF-LLM: Environment-Aware Wireless Channel Modeling via LLM-Guided 3D Gaussians&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;GRF-LLM: Environment-Aware Wireless Channel Modeling via LLM-Guided 3D Gaussians | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/c3e66b9c-20b4-4c50-b881-e40aba2a514f/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Junting Chen, Yong Zeng, and Rui Zhang. Rfcanvas: A radio frequency canvas for wireless network design. In IEEE International Conference on Communications, pp. 1-6, 2024.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Title partially matches this article.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Listwise Generalized Preference Optimization with Process-aware Signals for LLM Reasoning&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Listwise Generalized Preference Optimization with Process-aware Signals for LLM Reasoning | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/bbeecf1c-189a-4311-999b-617aab686ea9/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Kaixuan Zhou, Jiaqi Liu, Yiding Wang, and James Zou. Generalized direct preference optimization. arXiv preprint arXiv:2402.05015, 2024.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/0f12d2fc-403b-4859-8d00-f75fd9f56e39/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Yash Goyal, Anamay Mohapatra, Nihar Kwatra, and Pawan Goyal. A benchmark for compositional text-to-image synthesis. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;This paper exists, but the authors are all wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Resolving the Security-Auditability Dilemma with Auditable Latent Chain-of-Thought&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Resolving the Security-Auditability Dilemma with Auditable Latent Chain-of-Thought | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/5cee5c3a-5e75-4063-a054-1e934a071705/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Yixiang Ma, Ziyi Liu, Zhaoyu Wang, Zhaofeng Xu, Yitao Wang, and Yang Liu. Safechain: A framework for securely executing complex commands using large language models. arXiv preprint arXiv:2402.16521, 2024a.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No match; although this paper is closely related.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;4.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/f3441445-5401-48e9-9617-09a635992ff9/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Yunzhu Yang, Shuang Li, and Jiajun Wu. MM-ReAct: Prompting chatgpt to multi-modal chain-ofthought reasoning. arXiv preprint arXiv:2401.04740, 2024.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/80c64df2-eee6-41aa-90cc-3f835b128747/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Chenglong Wang, Yang Liu, Zhihong Xu, Ruochen Zhang, Jiahao Wu, Tao Luo, Jingang Li, Xunliang Liu, Weiran Qi, Yujiu Yang, et al. Gram-r ${ }^{8}$ : Self-training generative foundation reward models for reward reasoning. arXiv preprint arXiv:2509.02492, 2025b.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;All authors except the first are fabricated and the title is altered.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;DANCE-ST: Why Trustworthy AI Needs Constraint Guidance, Not Constraint Penalties&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;DANCE-ST: Why Trustworthy AI Needs Constraint Guidance, Not Constraint Penalties | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/3ebd71b4-560d-4fa3-a0d3-ed2fa13c519f/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Sardar Asif, Saad Ghayas, Waqar Ahmad, and Faisal Aadil. Atcn: an attention-based temporal convolutional network for remaining useful life prediction. The Journal of Supercomputing, 78(1): $1-19,2022$.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Two papers with similar titles exist here and here, but the authors, journal, and date do not match.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Federated Hierarchical Anti-Forgetting Framework for Class-Incremental Learning with Large Pre-Trained Models&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.33&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Federated Hierarchical Anti-Forgetting Framework for Class-Incremental Learning with Large Pre-Trained Models | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/ae10437b-c65b-455b-ad22-918742a5ed82/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Arslan Chaudhry, Arun Mallya, and Abhinav Srivastava. Fedclassil: A benchmark for classincremental federated learning. In NeurIPS, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modeling&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.33&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modeling | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/dff2c063-6986-4241-8c20-4327a39d4d4b/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Ishita et al. Bardhan. Icu length-of-stay prediction with interaction-based explanations. Journal of Biomedical Informatics, 144:104490, 2024.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;TRACEALIGN - Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.33&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;TRACEALIGN - Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/4b379aba-8d8a-427b-ac67-d13af5eda8c9/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Lisa Feldman Barrett. Emotions are constructed: How brains make meaning. Current Directions in Psychological Science, 25(6):403-408, 2016.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;This article is similar, but the title, and metadata are different.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;MEMORIA: A Large Language Model, Instruction Data and Evaluation Benchmark for Intangible Cultural Heritage&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.33&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;MEMORIA: A Large Language Model, Instruction Data and Evaluation Benchmark for Intangible Cultural Heritage | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/956129a3-11ee-4503-92e3-3ed5db12d2d6/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Yang Cao, Rosa Martinez, and Sarah Thompson. Preserving indigenous languages through neural language models: Challenges and opportunities. Computational Linguistics, 49(3):567-592, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Reflexion: Language Models that Think Twice for Internalized Self-Correction&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.2&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Reflexion: Language Models that Think Twice for Internalized Self-Correction | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/45f2f68d-df09-4bbf-8513-588fe24f26fa/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Guang-He Xiao, Haolin Wang, and Yong-Feng Zhang. Rethinking uncertainty in llms: A case study on a fact-checking benchmark. arXiv preprint arXiv:2305.11382, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;ECAM: Enhancing Causal Reasoning in Foundation Models with Endogenous Causal Attention Mechanism&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;ECAM: Enhancing Causal Reasoning in Foundation Models with Endogenous Causal Attention Mechanism | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/d99a5552-38e0-459b-8746-4e64069b0640/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Atticus Geiger, Zhengxuan Wu, Yonatan Rozner, Mirac Suzgun Naveh, Anna Nagarajan, Jure Leskovec, Christopher Potts, and Noah D Goodman. Causal interpretation of self-attention in pre-trained transformers. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023), 2023. URL https://proceedings.neurips.cc/paper_files/paper/ 2023/file/642a321fba8a0f03765318e629cb93ea-Paper-Conference.pdf.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;A paper with this title exists at the given URL, but the authors don't match.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/381ed9a6-b168-4cd0-81ad-1f50139c0737/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Guy Dove. Language as a cognitive tool to imagine goals in curiosity-driven exploration. Nature Communications, 13(1):1-14, 2022.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;An article with this title exists, but author and publication don't match.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;LOSI: Improving Multi-agent Reinforcement Learning via Latent Opponent Strategy Identification&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;LOSI: Improving Multi-agent Reinforcement Learning via Latent Opponent Strategy Identification | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/53e86e4b-a7e2-48d0-976b-240bfc412836/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Jing Liang, Fan Zhou, Shuying Li, Jun Chen, Guandong Zhou, Huaiming Xu, and Xin Li. Learning opponent behavior for robust cooperation in multi-agent reinforcement learning. IEEE Transactions on Cybernetics, 53(12):7527-7540, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;The Dynamic Interaction Field Transformer: A Universal, Tokenizer-Free Language Architecture&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;The Dynamic Interaction Field Transformer: A Universal, Tokenizer-Free Language Architecture | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/80fd90a6-c99e-4c31-af72-0da9e90949f6/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Kaj Bostrom and Greg Durrett. Byte-level representation learning for multi-lingual named entity recognition. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4617-4627, 2020.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Strategema: Probabilistic Analysis of Adversarial Multi-Agent Behavior with LLMs in Social Deduction Games&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Strategema: Probabilistic Analysis of Adversarial Multi-Agent Behavior with LLMs in Social Deduction Games | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/1155e8a8-f679-4942-8fd9-c47fb64ad967/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Tom Eccles, Jeffrey Tweedale, and Yvette Izza. Let's pretend: A study of negotiation with autonomous agents. In 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT), volume 3, pp. 449-452. IEEE, 2009.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Understanding Transformer Architecture through Continuous Dynamics: A Partial Differential Equation Perspective&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;3.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Understanding Transformer Architecture through Continuous Dynamics: A Partial Differential Equation Perspective | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/460a1a23-1a97-482a-9759-ade855a4a0b4/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Zijie J Wang, Yuhao Choi, and Dongyeop Wei. On the identity of the representation learned by pre-trained language models. arXiv preprint arXiv:2109.01819, 2021.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Diffusion Aligned Embeddings&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.8&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Diffusion Aligned Embeddings | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/3d95a003-06c6-4233-881b-03b1e29b4ba2/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Yujia Wang, Hu Huang, Cynthia Rudin, and Yaron Shaposhnik. Pacmap: Dimension reduction using pairwise controlled manifold approximation projection. Machine Learning, 110:559-590, 2021.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;A similar paper with two matching authors exists, but the other authors, title, and journal are wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Leveraging NLLB for Low-Resource Bidirectional Amharic – Afan Oromo Machine Translation&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Leveraging NLLB for Low-Resource Bidirectional Amharic – Afan Oromo Machine Translation | Open Review&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/813da6e2-f7e8-4c95-bdd8-7d29b8e4b641/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Atnafa L. Tonja, Gebremedhin Gebremeskel, and Seid M. Yimam. Evaluating machine translation systems for ethiopian languages: A case study of amharic and afan oromo. Journal of Natural Language Engineering, 29(3):456-478, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Certified Robustness Training: Closed-Form Certificates via CROWN&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Certified Robustness Training: Closed-Form Certificates via CROWN | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/53b60ef5-2ebf-403e-8123-3a9bb2da0f33/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Huan Zhang, Hongge Chen, Chaowei Xiao, and Bo Zhang. Towards deeper and better certified defenses against adversarial attacks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJgG92A2m&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Context-Aware Input Switching in Mobile Devices: A Multi-Language, Emoji-Integrated Typing System&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Context-Aware Input Switching in Mobile Devices: A Multi-Language, Emoji-Integrated Typing System | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/68998766-49c3-4269-9eca-3b6a76ed68b4/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Ishan Tarunesh, Syama Sundar Picked, Sai Krishna Bhat, and Monojit Choudhury. Machine translation for code-switching: A systematic literature review. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, pp. 3654-3670, 2021.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Partial match to this article, but authors, title, and metadata is largely wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Five-Mode Tucker-LoRA for Video Diffusion on Conv3D Backbones&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Five-Mode Tucker-LoRA for Video Diffusion on Conv3D Backbones | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/eb0fd660-ed00-4769-a940-3d093d4f1ec1/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Shengming Chen, Yuxin Wang, et al. Videocrafter: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2305.07932, 2023b.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;A paper with the same title exists, but the authors and arXiv ID are wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Activation-Guided Regularization: Improving Deep Classifiers using Feature-Space Regularization with Dynamic Prototypes&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Activation-Guided Regularization: Improving Deep Classifiers using Feature-Space Regularization with Dynamic Prototypes | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/4031111e-24ef-4e06-908e-18ab99b08932/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Wentao Cheng and Tong Zhang. Improving deep learning for classification with unknown label noise. In International Conference on Machine Learning, pp. 6059-6081. PMLR, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;A similar paper exists.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Sparse-Smooth Decomposition for Nonlinear Industrial Time Series Forecasting&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Sparse-Smooth Decomposition for Nonlinear Industrial Time Series Forecasting | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/c01ad49e-a788-4916-a6ee-f43314d14676/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Yutian Chen, Kun Zhang, Jonas Peters, and Bernhard Schölkopf. Causal discovery and inference for nonstationary systems. Journal of Machine Learning Research, 22(103):1-72, 2021.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/ba257eea-e86c-4276-84c0-08b7465e1e3e/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;&lt;lb/&gt;Xuechen Li, Juntang Zhuang, Yifan Ding, Zhaozong Jin, Yun chen Chen, and Stefanie Jegelka. Scalable gradients for stochastic differential equations. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS 2020), volume 108 of Proceedings of Machine Learning Research, pp. 3898-3908, 2020.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;The paper exists and the first author is correct but all other authors and the page range are wrong&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;SAFE-LLM: A Unified Framework for Reliable, Safe, And Secure Evaluation of Large Language Models&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;SAFE-LLM: A Unified Framework for Reliable, Safe, And Secure Evaluation of Large Language Models | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/05ee7ff4-40e2-48b7-b5bd-8c307d7db669/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Kuhn, J., et al. Semantic Entropy for Hallucination Detection. ACL 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;A similar paper with different authors can be found here.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;PIPA: An Agent for Protein Interaction Identification and Perturbation Analysis&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;PIPA: An Agent for Protein Interaction Identification and Perturbation Analysis | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/5031a806-1271-4fd3-b333-2554f47cb9fa/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Alex Brown et al. Autonomous scientific experimentation at the advanced light source using language-model-driven agents. Nature Communications, 16:7001, 2025.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/9d2e3239-99db-4712-be7f-e032156d92a5/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;DeepMind. Gemma scope: Scaling mechanistic interpretability to chain of thought. DeepMind Safety Blog, 2025. URL https://deepmindsafetyresearch.medium.com/ evaluating-and-monitoring-for-ai-scheming-8a7f2ce087f9. Discusses scaling mechanistic interpretability techniques to chain-of-thought and applications such as hallucination detection.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;ThA similar URL exists, and the title is similar to this blog. However, no exact match exists.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Graph-Based Operator Learning from Limited Data on Irregular Domains&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Graph-Based Operator Learning from Limited Data on Irregular Domains | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/6c52217f-fb88-4bd8-85aa-bd546e1fa88c/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Liu, Y., Lütjens, B., Azizzadenesheli, K., and Anandkumar, A. (2022). U-netformer: A u-net style transformer for solving pdes. arXiv preprint arXiv:2206.11832.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;KARMA: Knowledge-Aware Reward Mechanism Adjustment via Causal AI&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;KARMA: Knowledge-Aware Reward Mechanism Adjustment via Causal AI | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#434343;"&gt;https://app.gptzero.me/documents/92b6492c-68ad-41a3-ae35-628d67f053e0/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Reinaldo A. C. Bianchi, Luis A. Celiberto Jr, and Ramon Lopez de Mantaras. Knowledge-based reinforcement learning: A survey. Journal of Artificial Intelligence Research, 62:215-261, 2018.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Microarchitecture Is Destiny: Performance and Accuracy of Quantized LLMs on Consumer Hardware&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;2.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Microarchitecture Is Destiny: Performance and Accuracy of Quantized LLMs on Consumer Hardware | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/4504a39a-af72-41ab-9679-6f6a017a3275/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Zhihang Jiang, Dingkang Wang, Yao Li, et al. Fp6-llm: Efficient llm serving through fp6-centric co-design. arXiv preprint arXiv:2401.14112, 2024.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;the arXiv ID corresponds with a very similar paper, but the authors are wrong and the title is altered.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Decoupling of Experts: A Knowledge-Driven Architecture for Efficient LLMs&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;1.6&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Decoupling of Experts: A Knowledge-Driven Architecture for Efficient LLMs | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/74eade70-da36-4635-8749-5e1d04748b6d/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;H Zhang, Y L, X W, Y Z, X Z, H W, X H, K G, Z W, H W, H C, H L, and J W. Matrix data pile: A trillion-tokenscale datasets for llm pre-training. arXiv preprint arXiv:2408.12151, 2024.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match; arxiv is is unrelated&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;QUART: Agentic Reasoning To Discover Missing Knowledge in Multi-Domain Temporal Data.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;1.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;QUART: Agentic Reasoning To Discover Missing Knowledge in Multi-Domain Temporal Data. | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/c6f30343-3948-4c07-b7de-6b1407d5daa6/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Meera Jain and Albert Chen. Explainable ai techniques for medical applications: A comprehensive review. AI in Healthcare, 5:22-37, 2024.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;font-weight:bold;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;From Physics-Informed Models to Deep Learning: Reproducible AI Frameworks for Climate Resilience and Policy Alignment&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;font-weight:bold;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;1.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;font-weight:bold;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;From Physics-Informed Models to Deep Learning: Reproducible AI Frameworks for Climate Resilience and Policy Alignment | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;font-weight:bold;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/a7ed6c42-4349-4b45-a356-0e325090e5af/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;font-weight:bold;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;MIT Climate Group. A cautionary tale for deep learning in climate science. https://example. com, 2019.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;The title matches this paper, but the citation is obviously hallucinated.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;A superpersuasive autonomous policy debating system&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;1.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;A superpersuasive autonomous policy debating system | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/b792a4de-baa8-47d4-b880-87b330a482ce/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Roy Bar-Haim, Shachar Bhattacharya, Michal Jacovi, Yosi Mass, Matan Orbach, Eyal Sliwowicz, and Noam Slonim. Key point analysis via contrastive learning and extractive argument summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7953-7962, Online and Punta Cana, Dominican Republic, November 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.629. URL https://aclanthology.org/2021.emnlp-main. 629.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;A paper with the same title exists, but the authors and URL are wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education Through Automated Question Generation and Interactive Assessment&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;1.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education Through Automated Question Generation and Interactive Assessment | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/720d6d24-2223-4e0e-95b9-6dfce674f8c7/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;Shiyang Liu, Hongyi Xu, and Min Chen. Measuring and reducing perplexity in large-scale llms. arXiv preprint arXiv:2309.12345, 2023.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;AI-Assisted Medical Triage Assistant&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;1.0&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;AI-Assisted Medical Triage Assistant | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/391b5d76-929a-4f3f-addf-31f6993726f2/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;[3] K. Arnold, J. Smith, and A. Doe. Variability in triage decision making. Resuscitation, 85:12341239, 2014.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;No Match&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Deciphering Cross-Modal Feature Interactions in Multimodal AIGC Models: A Mechanistic Interpretability Approach&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;0.67&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Deciphering Cross-Modal Feature Interactions in Multimodal AIGC Models: A Mechanistic Interpretability Approach | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/d4102812-01c4-45b2-aea8-59e467d31fd4/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Shuyang Basu, Sachin Y Gadre, Ameet Talwalkar, and Zico Kolter. Understanding multimodal llms: the mechanistic interpretability of llava in visual question answering. arXiv preprint arXiv:2411.17346, 2024.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;A paper with this title exists, but the authors and arXiv ID are wrong.&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:21px;"&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;border-left:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:top;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt;Scalable Generative Modeling of Protein Ligand Trajectories via Graph Neural Diffusion Networks&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;text-align:center;"&gt;0.5&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;Scalable Generative Modeling of Protein Ligand Trajectories via Graph Neural Diffusion Networks | OpenReview&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;text-decoration:underline;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1155cc;"&gt;https://app.gptzero.me/documents/32d43311-6e69-4b88-be99-682e4eb0c2cc/share&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;color:#1f2937;"&gt;E. Brini, G. Jayachandran, and M. Karplus. Coarse-graining biomolecular simulations via statistical learning. J. Chem. Phys., 154:040901, 2021.&lt;/cell&gt;
      &lt;cell style="border-right:1px solid #000000;border-bottom:1px solid #000000;overflow:hidden;padding:2px 3px 2px 3px;vertical-align:middle;font-size:11pt;wrap-strategy:4;white-space:normal;word-wrap:break-word;"&gt; There is no match for the title and authors, but the journal, volume, and year match this article&lt;/cell&gt;
    &lt;/row&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gptzero.me/news/iclr-2026/"/><published>2025-12-07T13:16:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46181491</id><title>Goodbye, Microsoft: Schleswig-Holstein Relies on Open Source and Saves Millions</title><updated>2025-12-07T16:11:04.275940+00:00</updated><content>&lt;doc fingerprint="7549a99b1ab8b81"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Goodbye, Microsoft: Schleswig-Holstein relies on Open Source and saves millions&lt;/head&gt;
    &lt;p&gt;Schleswig-Holstein saves 15 million euros in license costs by migrating from Microsoft to free software. The conversion is significantly cheaper.&lt;/p&gt;
    &lt;p&gt;The state administration of Schleswig-Holstein is making a remarkable U-turn in its IT strategy and consistently relying on open source. After the migration from proprietary Microsoft software to free solutions was initially accompanied by problems and criticism, Digitalization Minister Dirk Schrödter (CDU) can now report a significant success: According to his ministry, the state will save over 15 million euros in license costs for Windows, Microsoft Office &amp;amp; Co. next year alone. It is expected to be similar in the following years.&lt;/p&gt;
    &lt;p&gt;In contrast, there would be one-time investments of nine million euros in 2026, explained the Ministry of Digitalization to the Kieler Nachrichten. These would have to be made for the conversion of workplaces and the further development of solutions with free software in the next 12 months. Given the annual savings, this sum will pay for itself in less than a year. In the past, the state transferred millions to the US company Microsoft, primarily for the use of office software and other programs.&lt;/p&gt;
    &lt;p&gt;The department sees the departure from this "vendor lock-in" – the dependence on a single large provider – as a clear signal for greater independence and sustainable digitalization. The financial incentive now underscores that digital sovereignty can be not only a political buzzword but also an economic gain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Almost 80 percent of licenses canceled&lt;/head&gt;
    &lt;p&gt;The numbers speak for themselves: outside the tax administration, almost 80 percent of workplaces in the state administration have already been switched to the open-source office software LibreOffice. Schrödter thus confirms a course that reduces technical and economic dependence on individual manufacturers. The consequence of the conversion was already evident recently, as Schrödter emphasized in an interview with c't. Regarding the status of Microsoft license cancellations, he said: "We are at almost 80, without the tax administration." For tax matters, the state finance ministers have "given themselves a clear timetable for the switch." Recently, the Christian Democrat also emphasized, according to the Südtiroler Wirtschaftszeitung, that the state has entered a marathon, not just a sprint.&lt;/p&gt;
    &lt;p&gt;The remaining 20 percent of workplaces are currently still dependent on Microsoft programs such as Word or Excel, as there is a technical dependency on these programs in certain specialized applications. According to Schrödter, however, the successive conversion of these remaining computers is the stated goal.&lt;/p&gt;
    &lt;head rend="h3"&gt;Opposition sees challenges&lt;/head&gt;
    &lt;p&gt;Despite the savings and the almost completed migration in large parts of the administration, the opposition continues to criticize the quality of the conversion. SPD state parliament member Kianusch Stender pointed out to the Kieler Nachrichten: "It may be that on paper 80 percent of workplaces have been converted. But far fewer than 80 percent of employees can now work with them properly." Errors in the migration are "still present." The initial difficulties in introducing the open-source programs have apparently led to ongoing frustration among some employees in certain areas.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;p&gt;The Green state parliament member Jan Kürschner also admitted in an interview with heise online that such a comprehensive conversion would not go without friction. But he emphasized the long-term nature of the project and the necessity of fundamentally rethinking administrative processes: "With the change, there is an opportunity to truly rethink the administration and free ourselves from old burdens. That is the great added value." If only a one-to-one conversion is made, it might certainly "stumble at one point or another." But those who truly optimize administrative processes will likely find in the end: "Open source is the better way."&lt;/p&gt;
    &lt;p&gt;The challenge now is to resolve the initial migration problems and acceptance difficulties and to further develop the open-source solutions so that they fully meet the requirements of a modern state administration. The savings achieved give Schleswig-Holstein more financial leeway for this.&lt;/p&gt;
    &lt;p&gt;(nie)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.heise.de/en/news/Goodbye-Microsoft-Schleswig-Holstein-relies-on-Open-Source-and-saves-millions-11105459.html"/><published>2025-12-07T13:21:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46181962</id><title>Dollar-stores overcharge cash-strapped customers while promising low prices</title><updated>2025-12-07T16:11:04.135964+00:00</updated><content>&lt;doc fingerprint="be06198a658e03f9"&gt;
  &lt;main&gt;
    &lt;p&gt;On a cloudy winter day, a state government inspector named Ryan Coffield walked into a Family Dollar store in Windsor, North Carolina, carrying a scanner gun and a laptop.&lt;/p&gt;
    &lt;p&gt;Inside the store, which sits along a three-lane road in a county of peanut growers and poultry workers, Coffield scanned 300 items and recorded their shelf prices. He carried the scanned bar codes to the cashier and watched as item after item rang up at a higher price.&lt;/p&gt;
    &lt;p&gt;Red Baron frozen pizzas, listed on the shelf at $5, rang up at $7.65. Bounty paper towels, shelf price $10.99, rang up at $15.50. Kellogg’s Frosted Flakes, Stouffer’s frozen meatloaf, Sprite and Pepsi, ibuprofen, Klondike Minis – shoppers were overpaying for all of them. Pedigree puppy food, listed at $12.25, rang up at $14.75.&lt;/p&gt;
    &lt;p&gt;All told, 69 of the 300 items came up higher at the register: a 23% error rate that exceeded the state’s limit by more than tenfold. Some of the price tags were months out of date.&lt;/p&gt;
    &lt;p&gt;The January 2023 inspection produced the store’s fourth consecutive failure, and Coffield’s agency, the state department of agriculture &amp;amp; consumer services, had fined Family Dollar after two previous visits. But North Carolina law caps penalties at $5,000 per inspection, offering retailers little incentive to fix the problem. “Sometimes it is cheaper to pay the fines,” said Chad Parker, who runs the agency’s weights-and-measures program.&lt;/p&gt;
    &lt;p&gt;The dollar-store industry, including Family Dollar and its larger rival, Dollar General, promises everyday low prices for household essentials. But an investigation by the Guardian found that the prices listed on the shelves at these two chains often don’t materialize at checkout – in North Carolina and around the country. As the cost of living soars across America, the customers bearing the burden are those who can least afford it – customers who often don’t even notice they’re overpaying.&lt;/p&gt;
    &lt;p&gt;These overcharges are widespread.&lt;/p&gt;
    &lt;p&gt;Dollar General stores have failed more than 4,300 government price-accuracy inspections in 23 states since January 2022, a Guardian review found. Family Dollar stores have failed more than 2,100 price inspections in 20 states over the same time span, the review found.&lt;/p&gt;
    &lt;p&gt;Among these thousands of failed inspections, some of the biggest flops include a 76% error rate in October 2022 at a Dollar General in Hamilton, Ohio; a 68% error rate in February 2023 at a Family Dollar in Bound Brook, New Jersey; and a 58% error rate three months ago at a Family Dollar in Lorain, Ohio.&lt;/p&gt;
    &lt;p&gt;Many of the stores that failed state or local government checks were repeat violators. A Family Dollar in Provo, Utah, flunked 28 inspections in a row – failures that included a 48% overcharge rate in May 2024 and a 12% overcharge rate in October 2025.&lt;/p&gt;
    &lt;p&gt;The chains’ pricing disparities are drawing increasing attention. In May, Arizona’s attorney general announced a $600,000 settlement to resolve a consumer-fraud investigation against Family Dollar. In October, Colorado’s attorney general settled with Dollar General for $400,000 after its stores failed 15 out of 23 state inspections. Dollar General has also settled with New Jersey, Vermont and Wisconsin, and both companies have settled with Ohio.&lt;/p&gt;
    &lt;p&gt;Linda Davis, a 64-year-old Family Dollar shopper in Dayton, Ohio, called the state attorney general’s office in February after walking home from the dollar store and discovering that 12 of her 23 purchases had rung up incorrectly. “I’m adding it up in my head as I’m shopping,” she told the Guardian. “But I was way off and I didn’t know why … I thought: where did I miscalculate? I’ve [only] got so much cash on me.”&lt;/p&gt;
    &lt;p&gt;Davis, who lives on social security, said she could shop elsewhere, but that would involve paying for a bus ride. “I don’t have money like that,” she said.&lt;/p&gt;
    &lt;p&gt;Both Family Dollar and Dollar General declined interview requests and did not answer detailed lists of questions from the Guardian. Instead, both sent the Guardian brief statements.&lt;/p&gt;
    &lt;p&gt;“At Family Dollar, we take customer trust seriously and are committed to ensuring pricing accuracy across our stores,” the company said. “We are currently reviewing the concerns raised and working to better understand any potential discrepancies. We continue to be focused on providing a consistent and transparent shopping experience.”&lt;/p&gt;
    &lt;p&gt;Dollar General said it was “committed to providing customers with accurate prices on items purchased in our stores, and we are disappointed any time we fail to deliver on this commitment”. In one court case in Ohio, Dollar General’s lawyers argued that “it is virtually impossible for a retailer to match shelf pricing and scanned pricing 100% of the time for all items. Perfection in this regard is neither plausible nor expected under the law.”&lt;/p&gt;
    &lt;p&gt;The Guardian’s examination of inspection failures by the two chains was based on record requests to 45 states and more than 140 counties and cities in New York, Ohio and California, along with court documents and public databases.&lt;/p&gt;
    &lt;p&gt;In nearly half of US states, information about whether customers are being overcharged was limited or unavailable. Many states do little or nothing to monitor retail stores’ pricing practices. Some, like Maryland, Idaho and Washington, do no random inspections, responding only to consumer complaints. Illinois, South Carolina and others don’t inspect at all. In 2020, auditors in Kansas revealed that these inspections were a low priority in many states. “Consumers can check price accuracy themselves,” they wrote.&lt;/p&gt;
    &lt;p&gt;Even in states with tougher enforcement, financial penalties don’t always solve the problem: in the 23 months after Dollar General agreed in November 2023 to pay Wisconsin $850,000, its stores failed 31% of their price inspections. During the same period, Wisconsin’s Family Dollar stores failed 30% of their state inspections.&lt;/p&gt;
    &lt;p&gt;According to industry watchers, employees and lawsuits, overcharges often stem from labor practices within the dollar-store sector. When a company changes prices, the registers are updated automatically. But the shelf prices are not: someone needs to remove the old labels manually and replace them with new ones. In an industry known for minimal staffing, workers don’t always have time to put up the new shelf tags.&lt;/p&gt;
    &lt;p&gt;In many instances, customers may not notice that they are being charged more than what’s listed on the shelf. If they notice at the register, they may decide to put those items back – or ask a store employee to honor the shelf price.&lt;/p&gt;
    &lt;p&gt;Dollar General, in its statement, said its store teams “are empowered to correct the matter on the spot”. But customers and current and former employees said that while some dollar stores will correct the price, others refuse to make fixes at the register – and turn away customers who return later and request a refund.&lt;/p&gt;
    &lt;p&gt;“Overcharging even by a small amount per item can strain a really tight budget,” said Elizabeth M Harris, acting director of the New Jersey division of consumer affairs. “If you’ve ever gone into any store … with a child like I have, there’s chaos at the checkout counter and you’re not really paying attention.” With items being rung up quickly, she added, “consumers are trusting that the retailer is actually charging them the price that’s displayed.”&lt;/p&gt;
    &lt;p&gt;Her state settled in 2023 with Dollar General for $1.2m after finding more than 2,000 items rung up as overcharges across 58 stores.&lt;/p&gt;
    &lt;p&gt;Even if the overcharges paid by dollar-store customers are accidental, they still reflect the industry’s decision not to correct a problem it has known about for years, according to Kennedy Smith, a researcher at the non-profit Institute for Local Self-Reliance, which works to protect communities from negative impacts of big corporations.&lt;/p&gt;
    &lt;p&gt;“If they’re called on it, they’ll say, ‘Oh yeah, our mistake,’” Kennedy said. “Until they’re called on it, they’re happy to let those scanner errors bring in the millions.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘The cheap stuff’&lt;/head&gt;
    &lt;p&gt;When consumers feel economic pain, as they do now thanks to rising costs exacerbated by tariffs, price gouging and other inflationary pressures, one place they turn to are dollar stores. These one-stop centers for inexpensive food, clothing and housewares tend to sell in small quantities, one $1 chicken-noodle-soup can at a time. And they are relatively easy to get to: 75% of Americans live within 5 miles of a Dollar General, according to the company.&lt;/p&gt;
    &lt;p&gt;The industry’s largest player is flourishing. Todd Vasos, the CEO of Dollar General, told investors in August that his company’s quarterly sales had increased 5% over the same period last year. Some of that growth, he said, came from middle- and higher-income shoppers tightening their belts. But the company’s low-income “core customers” were spending more at the chain too.&lt;/p&gt;
    &lt;p&gt;Those customers have been the industry’s niche from the beginning. When a 48-year-old former tobacco farmer and traveling salesman named James Luther Turner opened JL Turner and Son Wholesale Dry Goods, Shoes, Notions and Hosiery in Scottsville, Kentucky, in 1939, his mission was “to sell the cheap stuff to the poor folks”. (Someone else had cornered the market on “selling the good stuff” to Scottsville’s rich folks.)&lt;/p&gt;
    &lt;p&gt;By 1955, Turner and his eldest son, Hurley Calister “Cal” Turner Sr, were overseeing 36 stores in small southern towns. Cal Sr decided that year to co-opt the “Dollar Days” sales at big department stores and to open outlets featuring a single low price of $1. Adopting a name that nodded to the general store, he designed a bold black-and-yellow sign and that June christened the first Dollar General in Springfield, Kentucky.&lt;/p&gt;
    &lt;p&gt;Dollar General now operates over 20,000 stores in 48 states – more than any other retailer of any kind in the US. (It has long since abandoned its $1 price limit.) Though it has more than 195,000 employees and net sales of $40.6bn, the company still calls itself “America’s neighborhood general store”.&lt;/p&gt;
    &lt;p&gt;Family Dollar began in 1959 in Charlotte, North Carolina, and now operates 8,000 stores nationwide. For most of the past decade, it was owned by yet another chain, Dollar Tree, but the two brands divorced last summer.&lt;/p&gt;
    &lt;p&gt;What Dollar General and Family Dollar have in common is a conspicuous presence in places that don’t offer a lot of other retail: low-income urban neighborhoods and rural towns like Windsor.&lt;/p&gt;
    &lt;p&gt;A predominantly Black county seat of 3,400 on North Carolina’s coastal plain, Windsor used to be a retail hub. “All the streets were full on a weekend,” recalled Russell Parker, a 66-year-old retired pilot. “There were people everywhere, people playing music.” And people spending money: at the fish market, the cobbler, the independent groceries, the automotive-supply store. But today Windsor’s downtown – like many rural main streets – is pocked with empty storefronts. The town never fully recovered from Hurricane Floyd, in 1999. “Every young person that graduates from high school gets on the first thing smokin’ to somewhere else,” Parker said.&lt;/p&gt;
    &lt;p&gt;One supermarket remains on the edge of town. Shopping for clothes often means driving to the next county, at least for those who drive. But Windsor does have three stores that help fill the gap: a Dollar General and two Family Dollars.&lt;/p&gt;
    &lt;p&gt;At the Family Dollar that failed multiple inspections, some regulars remain vigilant. Chris Outlaw, a 54-year-old hemodialysis technician, shops there because it’s near his house and workplace. Experience has taught him to buy only a few items at once and to examine his receipts. Not all his neighbors do the same. “I’ve seen people in there with baskets full,” he said. “You can just imagine how much of that stuff didn’t ring out right, and they had so much they couldn’t catch it.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘Big old savings’&lt;/head&gt;
    &lt;p&gt;Customers walking into Dollar General stores are often greeted by a bright yellow sign blaring “Hello, Low Prices”– and by as many as 10,000 items cramming shelves and, often, cluttering the aisles.&lt;/p&gt;
    &lt;p&gt;“They will send you more than what you need of any product,” said Stephanie, a former lead sales associate in Louisiana. “Your shelf can only hold 10 Glade air fresheners, right? But they will send you 50.”&lt;/p&gt;
    &lt;p&gt;Rarely is there enough staffing, current and former employees say, to complete all of the tasks expected of them, including stocking shelves, ringing up sales, looking out for shoplifters, mopping floors – and updating price changes and sales stickers.&lt;/p&gt;
    &lt;p&gt;More than two dozen current and former employees of the chain in 15 states interviewed by the Guardian agreed that price discrepancies are the byproduct of the company’s employment policies. (Most, including Stephanie, spoke on the condition of anonymity because of fear of retaliation.)&lt;/p&gt;
    &lt;p&gt;Often there are only one or two people on duty. “You’re lucky if you get to work two to four hours of your eight- to 13-hour shift with another human being,” a former assistant manager in Illinois said.&lt;/p&gt;
    &lt;p&gt;Every Tuesday, employees are supposed to print and post hundreds of shelf stickers representing price changes already updated in the computer system. On Saturdays, stacks of sales stickers arrive; often, workers are expected to remove all the previous week’s stickers by 5pm and put up new stickers – as many as 1,000 of them – before closing up that night. Stickers fail to get put up, they fall off easily, and they are confusing, with some sales instant and others linked to coupons. “I threw away tags sometimes, to keep me or a co-worker out of trouble,” Stephanie admitted.&lt;/p&gt;
    &lt;p&gt;A former store manager at a Dollar General in Connecticut noted that many of his customers were poor or disabled enough that they got by on public assistance. “I didn’t want people to get screwed over, but I knew that it was happening,” he said. “If I’m in the store, I’m gonna try to do the best I can for them. But at the end of the day, they’re still probably gonna get overcharged for a few things.”&lt;/p&gt;
    &lt;p&gt;Dollar General, in its statement, said it schedules time each week for “price change execution”, among other measures to ensure accuracy.&lt;/p&gt;
    &lt;p&gt;Ten current and former employees in eight states claimed that – along with allowing pricing errors caused by understaffing and overstocking – some Dollar General stores engage in a tactic designed to fool customers: special sales that don’t actually lower the price of an item. A manager from Florida, for example, sent the Guardian two photos of price stickers for Café Bustelo ground coffee. In the first photo, a sticker said “SALE” in white block letters against a red background. It advertised a markdown from $7.95 to $6.50. In the second photo, the top sticker had been peeled away to show the original price: $6.50.&lt;/p&gt;
    &lt;p&gt;A sales associate from Illinois sent photos showing cutlery with what he said was a fake original price of $8.50. “It’s trying to say that you’re making this big old savings by buying this item here,” explained the employee, “when it’s actually always been $6.95.”&lt;/p&gt;
    &lt;p&gt;Dollar General declined to comment on these workers’ claims.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘We have little choice’&lt;/head&gt;
    &lt;p&gt;When the Ohio attorney general, Dave Yost, sued Dollar General in 2022, he submitted 114 pages of customer complaints as part of the case.&lt;/p&gt;
    &lt;p&gt;One of them came from Melanie Hutzler, who lives in Canton without a car and whose mobility is limited by arthritis and multiple sclerosis. Hutzler, 51, relies on government food assistance and said she was cautious about spending money. At the time of her complaint, she could reach two food stores on foot. Getting to the Save A Lot grocery required crossing a busy road, but getting to a Dollar General did not.&lt;/p&gt;
    &lt;p&gt;“Every single time we went into that store, something would ring up wrong,” she told the Guardian. “They never had a manager there that would fix the prices.” Hutzler said she would walk the cashier over to the shelf and point out the listed price, only to be told, “There’s nothing we can do about it.”&lt;/p&gt;
    &lt;p&gt;Other Ohioans expressed similar frustrations. “My 87-year-old mother and I have frequented Dollar General for years, and there have been innumerable times we have made purchases that were well higher than advertised,” wrote Robert Hevlin of Dayton. “My mother and I have literally lost thousands over the years with this company, but both of us being on social security, we have little choice in where we shop.”&lt;/p&gt;
    &lt;p&gt;In September 2023, Yost reached a $1m settlement with Dollar General, which he said had error rates at some stores that ran as high as 88%. In February 2024, he announced a $400,000 settlement with Family Dollar to resolve similar allegations. Most of that money went to charitable organizations that distribute food and personal-care items.&lt;/p&gt;
    &lt;p&gt;Both chains agreed in the settlements to tighten their pricing practices. Yost’s office continues to receive complaints. A Dollar General customer in Garfield Heights said in February that he was charged $6.35 for a carton of eggs with a shelf sticker of $5.10, but the “cashier was too busy having a personal call on her cellphone to address the price discrepancy”. The same month, a Family Dollar shopper in Genoa reported being charged $2.65 for cough medicine listed on the shelf at $1.50. “I was told by the cashier that there was nothing that could be done about it,” the complaint said.&lt;/p&gt;
    &lt;p&gt;Over in Missouri, state officials are pursuing a lawsuit that accuses Dollar General of “deceptive” pricing practices. The suit, filed in 2023, says 92 of the 147 stores the state checked failed their inspections, with discrepancies as high as $6.50 an item.&lt;/p&gt;
    &lt;p&gt;The companies declined to comment on these state lawsuits.&lt;/p&gt;
    &lt;p&gt;Dollar General has also been hit with private lawsuits, including several filed by its shareholders. In a document filed in August in federal court in Nashville, lawyers for Dollar General investors argued that understaffing, poor inventory control and overcharging were all interrelated.&lt;/p&gt;
    &lt;p&gt;The investors allege that the company deceived them by portraying itself as financially sound. In truth, the court filing says, “Dollar General’s inventory management processes were broken, which caused a massive bloat of excess product to clog the company at both its distribution centers and stores, and its workforce had been slashed.” These problems gave rise to price discrepancies and other “dire consequences”, the court filing asserts.&lt;/p&gt;
    &lt;p&gt;The filing includes the stories of 36 former employees who claimed direct knowledge that Dollar General managers and executives knew about the problems. Several reported notifying the top leadership directly. “All the prices were off in the stores,” said one of those ex-employees, a manager who monitored inventory levels in Ohio and Pennsylvania. She claimed to know firsthand, based on calls she participated in, that company vice-presidents and regional directors were aware of the “huge” price mismatches.&lt;/p&gt;
    &lt;p&gt;Dollar General, in response, said that the testimony of a handful of ex-workers does not prove that it misled investors. In their “years-long search for fraud”, the company’s lawyers claimed, the shareholders “came up empty”.&lt;/p&gt;
    &lt;p&gt;Earlier this year, a federal judge in New Jersey halted a class-action lawsuit against Dollar General filed by a shopper who said he was overcharged for groceries. Dollar General argued that when customers create accounts – for example, by downloading the company’s mobile app – they agree to use arbitration to resolve disputes and forfeit the right to file class-action suits. The judge agreed.&lt;/p&gt;
    &lt;p&gt;This victory for Dollar General threw up an obstacle for customers seeking justice. “Who’s going to bring a consumer arbitration with a $225 filing fee over a 50-cent overcharge?” asked Marc Dann, a former Ohio attorney general whose law firm filed the New Jersey case. “They’ve essentially closed the door to the courthouse to people.”&lt;/p&gt;
    &lt;p&gt;Dann’s firm did reach a settlement with Dollar General in another case this fall, though the details have not been made public.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘This endless cycle’&lt;/head&gt;
    &lt;p&gt;The dollar-store chains describe themselves as mission-driven companies. “Our stores are conveniently located in neighborhoods, and often in ‘food deserts’ where other stores choose not to locate,” Family Dollar says on its website. Dollar General takes pride in offering value to families who, according to CEO Vasos, “have had to sacrifice even on the necessities”.&lt;/p&gt;
    &lt;p&gt;The industry’s critics say the cause and effect are reversed. “Dollar stores are often seen as a symptom of economic distress,” said the Institute for Local Self-Reliance’s co-executive director, Stacy Mitchell. “What we found is that they’re, in fact, a cause of it.” Sometimes, she said, a chain dollar store will open near an independent grocer and skim off enough of its business that it is forced to close. That limits the availability of fresh produce and forces shoppers to buy more packaged and processed foods.&lt;/p&gt;
    &lt;p&gt;In a statement, Dollar General said its stores often “operate along with local grocers and business owners to collectively meet customers’ needs”. It added that 7,000 of its 20,000 stores sell fresh produce and that the company also partners with local food banks “to further help nourish our neighbors in need”.&lt;/p&gt;
    &lt;p&gt;The people enduring the effects of hollowed-out local economies – and getting hit with overcharges at dollar-store chains – include residents of Essex county, New York. The county, tucked among the stately pines of the Adirondack Mountains, has a population of 37,000. It has five Dollar Generals and two Family Dollars. All seven regularly fail pricing-accuracy tests. The Dollar General in Port Henry, which sits on the shores of Lake Champlain, was fined $103,550 for failed inspections between November 2022 and June 2025.&lt;/p&gt;
    &lt;p&gt;Over the course of seven inspections, 279 out of 700 tested items were overcharges – a combined error rate of just under 40%. One inspection yielded a 78% error rate, including overcharges on Flintstones vitamins, Peter Pan peanut butter and Prego pasta sauce.&lt;/p&gt;
    &lt;p&gt;The Port Henry store is 5 miles from the Mineville Dollar General, which occupies a lonely stretch of country road across from an auto-repair shop with spare parts littering its lawn. Down the block, an abandoned church presides over a stretch of grass that looks like it hasn’t been mown for years.&lt;/p&gt;
    &lt;p&gt;Aside from a whiskey warehousing operation and a health center, opportunities for employment are limited. The high-security prison built atop the iron mine for which Mineville is named closed in 2022, taking 100 jobs with it.&lt;/p&gt;
    &lt;p&gt;The local playground is littered with trash, cigarette butts and the occasional syringe. The town “is nice from the outside”, said Katelyn Miller, a 26-year-old Port Henry resident who lives with her mother, six-year-old daughter and two-year-old son. But “you hear about a lot of crack-den places, like blowing up or getting busted.’” Drug use is rampant in the county, which is 92% white. “Everybody around here seems to be on pain meds or buying someone else’s, because they’re also working themselves to death.”&lt;/p&gt;
    &lt;p&gt;When it comes to grocery shopping near Miller’s home, the choice is between the two Dollar Generals and a gas station/convenience store. “We live in a food desert,” she said, “even though you would think living in all this farmland, we would have more access.”&lt;/p&gt;
    &lt;p&gt;There is a Walmart 30 minutes away, in Fort Ticonderoga. Miller said she recently bought salmon there only to arrive home and discover that the $20 piece of fish had gone bad. “So I had to go to Dollar General and get the Stouffer’s,” she said, adding that she feels “caught in this endless cycle of never having food that will nourish me and my family, and instead having to get 2,000 grams of sodium because at least it has meat”.&lt;/p&gt;
    &lt;p&gt;The region’s economic straits put regulators in a bind when it comes to overcharges. Daniel Woods, the county’s director of weights and measures, said in 2023 that he didn’t always assess the full penalty on violators. “We’re not trying to put people out of business,” he told a local newspaper. “In some towns that’s their [only] store. I don’t want to pull that away from people, but at the same time, I’m trying to fix the problem.”&lt;/p&gt;
    &lt;head rend="h2"&gt;On the way out&lt;/head&gt;
    &lt;p&gt;When Coffield, the North Carolina inspector, visited the Windsor Family Dollar in April 2023, the pricing issues seemed to have abated. Of the 300 items he scanned, he only found five overcharges: incontinence pads, laundry sanitizer, two coffee products and, again, Red Baron pizza. With an error rate below the state’s 2% threshold, the store passed its inspection, and it did so again in November 2024.&lt;/p&gt;
    &lt;p&gt;But customers still reported problems. Chris Outlaw, the hemodialysis technician, stopped by the Family Dollar earlier this year and noticed a sale: a $1.25 savings on five bags of Cheez Doodles. He bought them but discovered on the way out that he had been charged the regular price. The manager refused to refund the difference, Outlaw said, because he had already walked through the exit door.&lt;/p&gt;
    &lt;p&gt;Another time, he saw some discounted socks near the counter that he thought would make good Christmas gifts. “I was like, ‘Oh, I like these socks, so I’ll probably give them to somebody,’” he recalled. “Nice, plushy socks.” But they rang up at a higher price, so he left the store without them.&lt;/p&gt;
    &lt;p&gt;During a visit in August, a Guardian reporter found the Windsor Family Dollar closed for much of the afternoon. “Be Back Soon!” read a handwritten sign taped to the door. Two waiting customers said that they frequently paid prices higher than the shelf listing, including a cook whose nearby restaurant buys some of its ingredients there. “It is aggravating,” she said. “Very aggravating.”&lt;/p&gt;
    &lt;p&gt;Workers reopened the doors after a few hours. Inside, carts of unshelved dog food and other merchandise blocked the aisles. The Guardian compared the prices of 15 items. Two of them rang up higher than advertised, including a frying pan set that was $10 on the shelf and $12 at the register. Though the cashier offered to honor the lower prices, that was still an error rate of 13% – more than six times the state’s standard.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/us-news/2025/dec/03/customers-pay-more-rising-dollar-store-costs"/><published>2025-12-07T14:37:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46182202</id><title>Scala 3 slowed us down?</title><updated>2025-12-07T16:11:04.084115+00:00</updated><content>&lt;doc fingerprint="47f0e0b7a83080ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Scala 3 slowed us down?&lt;/head&gt;
    &lt;p&gt;Is this clickbait? Not really. &lt;lb/&gt; Is this the fault of the language or the compiler? Definitely not. &lt;lb/&gt; Rather, it was part of a rushed migration. Sharing the lessons learned in the process.&lt;/p&gt;
    &lt;p&gt;I was refreshing one of our services. Part of this process was to migrate codebase from Scala 2.13 to Scala 3. I’ve done this a few times before and overall had a positive experience. Well, at least until we talk about projects with macro wizardry.&lt;/p&gt;
    &lt;p&gt;The service in question had no macros at all, but it was at the heart of data ingestion, so performance was not an afterthought.&lt;/p&gt;
    &lt;p&gt;I did it as usual - updating dependencies, compiler options and some type/syntax changes.&lt;/p&gt;
    &lt;p&gt;Then after resolving few tricky implicit resolutions and config derivations, project compiled on Scala 3.7.3 🎉&lt;/p&gt;
    &lt;p&gt;All tests passed, end-to-end flow locally works perfectly fine, so I decided to roll out the changes in a testing environment. Similarly, no issues at all. No concerning logs, all metrics ranging from infrastructure, through JVM up to application level look healthy.&lt;/p&gt;
    &lt;p&gt;With that in mind, I began a staged rollout. Again, all seem good. I kept observing the service but it looked like my job is done.&lt;/p&gt;
    &lt;p&gt;Well, as you probably can guess, it wasn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;The mysterious slowdown&lt;/head&gt;
    &lt;p&gt;After 5-6 hours, Kafka lag started increasing on a few environments. Of course, this wasn’t something new. Most often it is caused by a spike of data. We have pretty advanced machinery to deal with that. Usually the lag resolves by itself without any manual action.&lt;/p&gt;
    &lt;p&gt;However, this time something was off. Upstream load turned out to be relatively modest, yet we needed much more instances of the service - meaning the processing rate per instance dropped. I was confused to say the least. Why would it decrease the processing rate just on these environments?&lt;/p&gt;
    &lt;p&gt;Anyway, we decided to rollback the changes - this brought the rate back.&lt;/p&gt;
    &lt;head rend="h2"&gt;Digging deeper&lt;/head&gt;
    &lt;p&gt;I came back to testing. In particular, load testing. However similarly as on production environments I did not notice regression. So I played around with different payloads and granularity of messages. To my surprise, for more fine-grained, heterogeneous workloads, the processing rate significantly dropped.&lt;/p&gt;
    &lt;p&gt;Still, I had no idea why it would happen, but my bet was in the dependencies. Therefore, I tried one-by-one, reverting the serialization library, database SDK, base Docker image and even config libraries. None of these made any changes.&lt;/p&gt;
    &lt;p&gt;This made me pull out the big guns. I profiled the service using async-profiler and indeed&lt;/p&gt;
    &lt;p&gt;CPU profile looked vastly different on Scala 3 than on 2.13.&lt;/p&gt;
    &lt;p&gt;JVM-level CPU time was now dominated by JIT compiler while application-level by decoding.&lt;/p&gt;
    &lt;p&gt;Looking at the top of Scala 3 flamegraph I noticed a long quicklens call.&lt;/p&gt;
    &lt;p&gt;What used to be transparent (frankly, I didn’t even realize we used the library), now took almost half of the total CPU time. I compared how it looks on Scala 2.13 and it was barely noticeable with around 0.5% samples.&lt;/p&gt;
    &lt;p&gt;Turns out there was indeed a subtle bug making chained evaluations inefficient in Scala 3. This also explained why the JVM spent so much time compiling.&lt;/p&gt;
    &lt;p&gt;After upgrading the library, performance and CPU characteristics on Scala 3 became indistinguishable from Scala 2.13.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;While the details of the bug are pretty interesting(hats off to the SoftwareMill team for catching it!), that’s not my point here. I want to emphasize that libraries can behave very differently between Scala versions, especially when they rely on meta-programming.&lt;/p&gt;
    &lt;p&gt;Even if your migration is seamless and the service runs fine on Scala 3 - when performance is not just a nice-to-have, do not assume. Know your hotspots and benchmark them. Otherwise, your code will benchmark you, revealing bottlenecks in places you didn’t even know existed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kmaliszewski9.github.io/scala/2025/12/07/scala3-slowdown.html"/><published>2025-12-07T15:08:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46182340</id><title>Martin Parr has died</title><updated>2025-12-07T16:11:03.947881+00:00</updated><content>&lt;doc fingerprint="16421a67f9922ef5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;British photographer Martin Parr dies aged 73&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Published&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Photographer Martin Parr, whose colourful images captured British life, has died at the age of 73.&lt;/p&gt;
    &lt;p&gt;He died on Saturday at his home in Bristol, the director of the Martin Parr Foundation, Jenni Smith, told BBC News.&lt;/p&gt;
    &lt;p&gt;In a statement, external, the foundation said he would "be greatly missed" and was survived by his wife Susie, daughter Ellen, sister and grandson. It added the family asked for privacy.&lt;/p&gt;
    &lt;p&gt;The documentary photographer rose to prominence in the mid 1980s, with The Last Resort: his study of working class people on holiday in New Brighton in Merseyside.&lt;/p&gt;
    &lt;p&gt;Parr's works were known for capturing the smallest details of everyday life. His photographs were playful and had humour, but also provoked debate and discussion.&lt;/p&gt;
    &lt;p&gt;"I make serious photographs disguised as entertainment," he told The Architectural Review in 2020, external.&lt;/p&gt;
    &lt;p&gt;"I try to point out when I find universal truths. Truth is subjective, but it's the world how I found it."&lt;/p&gt;
    &lt;p&gt;For more than 50 years, Parr's photographs observed with an apparently flat but amused and sympathetic eye the quiet rituals and absurdities of his home country, from desolate seaside towns to village fetes and modern shopping centres.&lt;/p&gt;
    &lt;p&gt;He was known for using a colour saturated palette that mimicked postcards from the 1950s and 1960s.&lt;/p&gt;
    &lt;p&gt;The shots he took in New Brighton were meant to be about capturing a moment in time and challenging people's perceptions of social classes.&lt;/p&gt;
    &lt;p&gt;The collection showcased the best - and worst - days at the seaside, with pictures of day trippers enjoying picnics among the litter and rundown amenities which characterised the Wirral town at the time.&lt;/p&gt;
    &lt;p&gt;But those famous seaside shots became very controversial, as he himself acknowledged earlier this year ahead of a new film about his life.&lt;/p&gt;
    &lt;p&gt;"People from London and the South East, they really didn't know what places in the North looked like," said Parr, now 72.&lt;/p&gt;
    &lt;p&gt;"The litter was quite terrible, but they just weren't used to it, so it was almost like it was my fault that the place looked so scruffy."&lt;/p&gt;
    &lt;p&gt;Last month, in an interview with AFP, he warned the world needs the kind of satire captured in his images more than ever.&lt;/p&gt;
    &lt;p&gt;"The state we're all in is appalling," he said. "We're all too rich. We're consuming all these things in the world. And we can't. It's unsustainable."&lt;/p&gt;
    &lt;p&gt;Photographer Diane Smyth, editor of the British Journal of Photography, called Parr a "giant of post-war photography" in a tribute posted on Instagram, external.&lt;/p&gt;
    &lt;p&gt;"He was a hoot - always up for a call, especially if it was very early, and always very direct. He did he own thing, worked incredibly hard, helped others along the way - a life well-lived."&lt;/p&gt;
    &lt;p&gt;Jonathan Stephenson, who collaborated on art and design projects with Parr over the years, told BBC News he died peacefully watching football, adding he was "a firm and loyal friend".&lt;/p&gt;
    &lt;p&gt;"It was a massive privilege - and continually inspiring - to engage with Martin's eyes and mind," he said. "Martin's enthusiasm for everyday life was infectious."&lt;/p&gt;
    &lt;head rend="h2"&gt;Related topics&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published26 February&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.co.uk/news/articles/cg5m0mnvnvmo"/><published>2025-12-07T15:23:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46182496</id><title>Locks in PostgreSQL</title><updated>2025-12-07T16:11:03.226979+00:00</updated><content>&lt;doc fingerprint="8a3487d549b52fd6"&gt;
  &lt;main&gt;&lt;p&gt;We've already discussed some object-level locks (specifically, relation-level locks), as well as row-level locks with their connection to object-level locks and also explored wait queues, which are not always fair.&lt;lb/&gt;We have a hodgepodge this time. We'll start with deadlocks (actually, I planned to discuss them last time, but that article was excessively long in itself), then briefly review object-level locks left and finally discuss predicate locks.&lt;lb/&gt;When using locks, we can confront a deadlock. It occurs when one transaction tries to acquire a resource that is already in use by another transaction, while the second transaction tries to acquire a resource that is in use by the first. The figure on the left below illustrates this: solid-line arrows indicate acquired resources, while dashed-line arrows show attempts to acquire a resource that is already in use.&lt;lb/&gt;To visualize a deadlock, it is convenient to build the wait-for graph. To do this, we remove specific resources, leave only transactions and indicate which transaction waits for which other. If a graph contains a cycle (from a vertex, we can get to itself in a walk along arrows), this is a deadlock.&lt;lb/&gt;A deadlock can certainly occur not only for two transactions, but for any larger number of them.&lt;lb/&gt;If a deadlock occured, the involved transactions can do nothing but wait infinitely. Therefore, all DBMS, including PostgreSQL, track locks automatically.&lt;lb/&gt;The check, however, requires a certain effort, and it's undesirable to make it each time a new lock is requested (deadlocks are pretty infrequent after all). So, when a process tries to acquire a lock, but cannot, it queues and «falls asleep», but sets the timer to the value specified in the deadlock_timeout parameter (1 second by default). If the resource gets free earlier, this is fine and we skimp on the check. But if on expiration of deadlock_timeout, the wait continues, the waiting process will wake up and initiate the check.&lt;lb/&gt;If the check (which consists in building the wait-for graph and searching it for cycles) does not detect deadlocks, it continues sleeping, this time «until final victory».&lt;lb/&gt;But if a deadlock is detected, one of the transactions (in most cases, the one that initiated the check) is forced to abort. This releases the locks it acquired and enables other transactions to continue.&lt;lb/&gt;Deadlocks usually mean that the application is designed incorrectly. There are two ways to detect such situations: first, messages will occur in the server log and second, the value of&lt;lb/&gt;Usually deadlocks are caused by an inconsistent order of locking table rows.&lt;lb/&gt;Let's consider a simple example. The first transaction is going to transfer 100 rubles from the first account to the second one. To this end, the transaction reduces the first account:&lt;lb/&gt;At the same time, the second transaction is going to transfer 10 rubles from the second account to the first one. And it starts with reducing the second account:&lt;lb/&gt;Now the first transaction tries to increase the second account, but detects a lock on the row.&lt;lb/&gt;Then the second transaction tries to increase the first account, but also gets blocked.&lt;lb/&gt;So a circular wait arises, which won't end on its own. In a second, the first transaction, which cannot access the resource yet, initiates a check for a deadlock and is forced to abort by the server.&lt;lb/&gt;Now the second transaction can continue.&lt;lb/&gt;The correct way to perform such operations is to lock resources in the same order. For example: in this case, accounts can be locked in ascending order of their numbers.&lt;lb/&gt;Sometimes we can get a deadlock in situations where, seemingly, it could never occur. For example: it is convenient and usual to treat SQL commands as atomic, but the UPDATE command locks rows as they are updated. This does not happen instantaneously. Therefore, if the order in which a command updates rows is inconsistent with the order in which another command does this, a deadlock can occur.&lt;lb/&gt;Although such a situation is unlikely, it can still occur. To reproduce it, we will create an index on the&lt;lb/&gt;To be able to watch what happens, let's create a function that increases the passed value, but very-very slowly, for as long as an entire second:&lt;lb/&gt;We will also need the&lt;lb/&gt;The first UPDATE command will update the entire table. The execution plan is evident — it is sequential scan:&lt;lb/&gt;Since tuples on the table page are located in ascending order of the amount (exactly how we added them), they will also be updated in the same order. Let the update start.&lt;lb/&gt;At the same time, in another session we'll forbid sequential scans:&lt;lb/&gt;In this case, for the next UPDATE operator, the planner decides to use index scan:&lt;lb/&gt;The second and third rows meet the condition, and since the index is built in descending order of the amount, the rows will be updated in a reverse order.&lt;lb/&gt;Let's run the next update.&lt;lb/&gt;A quick look into the table page shows that the first operator already managed to update the first row (0,1) and the second operator updated the last row (0,3):&lt;lb/&gt;One more second elapses. The first operator updated the second row, and the second one would like to do the same, but cannot.&lt;lb/&gt;Now the first operator would like to update the last table row, but it is already locked by the second operator. Hence a deadlock.&lt;lb/&gt;One of the transactions aborts:&lt;lb/&gt;And the second one continues:&lt;lb/&gt;This completes a talk on deadlocks, and we proceed to the remaining object-level locks.&lt;lb/&gt;When we need to lock a resource that is not a relation in the meaning of PostgreSQL, locks of the&lt;lb/&gt;Illustrating this by a simple example. Let's start a transaction and create a table in it:&lt;lb/&gt;Now let's see what locks of the&lt;lb/&gt;To figure out what in particular is locked here, we need to look at three fields:&lt;lb/&gt;We work as&lt;lb/&gt;Now let's clarify the second line. The database is specified, and it is&lt;lb/&gt;This shows that the&lt;lb/&gt;So, we've seen that when an object is created, the owner role and schema in which the object is created get locked (in a shared mode). And this is reasonable: otherwise, someone could drop the role or schema while the transaction is not completed yet.&lt;lb/&gt;When the number of rows in a relation (table, index or materialized view) increases, PostgreSQL can use free space in available pages for inserts, but evidently, once new pages also have to be added. Physically they are added at the end of the appropriate file. And this is meant by a relation extension.&lt;lb/&gt;To ensure that two processes do not rush to add pages simultaneously, the extension process is protected by a specialized lock of the&lt;lb/&gt;This lock is certainly released without waiting for completion of the transaction.&lt;lb/&gt;Page-level locks of the&lt;lb/&gt;GIN indexes enable us to accelerate search in compound values, for instance: words in text documents (or array elements). To a first approximation, these indexes can be represented as a regular B-tree that stores separate words from the documents rather than the documents themselves. Therefore, when a new document is added, the index has to be rebuilt pretty much in order to add there each new word from the document.&lt;lb/&gt;For better performance, GIN index has a postponed insert feature, which is turned on by the&lt;lb/&gt;To prevent moving from the pending list to the main index by several processes simultaneously, for the duration of moving, the index metapage gets locked in an exclusive mode. This does not hinder regular use of the index.&lt;lb/&gt;Unlike other locks (such as relation-level locks), advisory locks are never acquired automatically — the application developer controls them. They are useful when, for instance, an application for some reason needs a locking logic that is not in line with the standard logic of regular locks.&lt;lb/&gt;Assume we have a hypothetical resource that does not match any database object (which we could lock using commands such as SELECT FOR or LOCK TABLE). We need to devise a numeric identifier for it. If a resource has a unique name, a simple option is to use its hash code:&lt;lb/&gt;This is how we have the lock acquired:&lt;lb/&gt;As usual, information on locks is available in&lt;lb/&gt;For locking to be really effective, other processes must also acquire a lock on the resource prior to accessing it. Evidently the application must ensure that this rule is observed.&lt;lb/&gt;In the above example, the lock will be held through the end of the session rather than the transaction, as usual.&lt;lb/&gt;And we need to explicitly release it:&lt;lb/&gt;A rich collection of functions to work with advisory locks is available for all intents and purposes:&lt;lb/&gt;A collection of&lt;lb/&gt;The predicate lock term occurred long ago, when early DBMS made first attempts to implement complete isolation based on locks (the Serializable level, although there was no SQL standard at that time). The issue they confronted then was that even locking of all read and updated rows did not ensure complete isolation: new rows that meet the same selection conditions can occur in the table, which causes phantoms to arise (see the article on isolation).&lt;lb/&gt;The idea of predicate locks was to lock predicates rather than rows. If during execution of a query with the condition a &amp;gt; 10 we lock the a &amp;gt; 10 predicate, this won't allow us to add new rows that meet the condition to the table and will enable us to avoid phantoms. The issue is that this problem is computationally complicated; in practice, it can be solved only for very simple predicates.&lt;lb/&gt;In PostgreSQL, the Serializable level is implemented differently, on top of the available isolation based on data snapshots. Although the predicate lock term is still used, its meaning drastically changed. Actually these «locks» block nothing; they are used to track data dependencies between transactions.&lt;lb/&gt;It is proved that snapshot isolation permits an inconsistent write (write skew) anomaly and a read-only transaction anomaly, but any other anomalies are impossible. To figure out that we deal with one of the two above anomalies, we can analyze dependencies between transactions and discover certain patterns there.&lt;lb/&gt;Dependencies of two kinds are of interest to us:&lt;lb/&gt;We can track WR dependencies using already available regular locks, but RW dependencies have to be tracked specially.&lt;lb/&gt;To reiterate, despite the name, predicate locks bock nothing. A check is performed at the transaction commit instead, and if a suspicious sequence of dependencies that may indicate an anomaly is discovered, the transaction aborts.&lt;lb/&gt;Let's look at how predicate locks are handled. To do this, we'll create a table with a pretty large number of locks and an index on it.&lt;lb/&gt;If a query is executed using sequential scan of the entire table, a predicate lock on the entire table gets acquired (even if not all rows meet the filtering condition).&lt;lb/&gt;All predicate locks are acquired in one special mode — SIReadLock (Serializable Isolation Read):&lt;lb/&gt;But if a query is executed using index scan, the situation changes for the better. If we deal with a B-tree, it is sufficient to have a lock acquired on the rows read and on the leaf index pages walked through — this allows us to track not only specific values, but all the range read.&lt;lb/&gt;Note a few complexities.&lt;lb/&gt;First, a separate lock is created for each read tuple, and the number of such tuples can potentially be very large. The total number of predicate locks in the system is limited by the product of parameter values: max_pred_locks_per_transaction × max_connections (the default values are 64 and 100, respectively). The memory for these locks is allocated at the server start; an attempt to exceed this limit will result in errors.&lt;lb/&gt;Therefore, escalation is used for predicate locks (and only for them!). Prior to PostgreSQL 10, the limitations were hard coded, but starting this version, we can control the escalation through parameters. If the number of tuple locks related to one page exceeds max_pred_locks_per_page, these locks are replaced with one page-level lock. Consider an example:&lt;lb/&gt;We see one lock of the&lt;lb/&gt;Likewise, if the number of locks on pages related to one relation exceeds max_pred_locks_per_relation, these locks are replaced with one relation-level lock.&lt;lb/&gt;There are no other levels: predicate locks are acquired only for relations, pages and tuples and always in the SIReadLock mode.&lt;lb/&gt;Certainly, escalation of locks inevitably results in an increase of the number of transactions that falsely terminate with a serialization error, and eventually, the system throuthput will decrease. Here you need to balance RAM consumption and performance.&lt;lb/&gt;The second complexity is that different operations with an index (for instance, due to splits of index pages when new rows are inserted) change the number of leaf pages that cover the range read. But the implementation takes this into account:&lt;lb/&gt;By the way, predicate locks are not always released immediately on completion of the transaction since they are needed to track dependencies between several transactions. But anyway, they are controlled automatically.&lt;lb/&gt;By no means all types of indexes in PostgreSQL support predicate locks. Before PostgreSQL 11, only B-trees could boast of this, but that version improved the situation: hash, GiST and GIN indexes were added to the list. If index access is used, but the index does not support predicate locks, a lock on the entire index is acquired. This, certainly, also increases the number of false aborts of transactions.&lt;lb/&gt;Finally, note that it's the use of predicate locks that limits all transactions to working at the Serializable level in order to ensure complete isolation. If a certain transaction uses a different level, it just won't acquire (and check) predicate locks.&lt;lb/&gt;Read on.&lt;/p&gt;&lt;p&gt;We have a hodgepodge this time. We'll start with deadlocks (actually, I planned to discuss them last time, but that article was excessively long in itself), then briefly review object-level locks left and finally discuss predicate locks.&lt;/p&gt;&lt;head rend="h1"&gt;Deadlocks&lt;/head&gt;&lt;p&gt;When using locks, we can confront a deadlock. It occurs when one transaction tries to acquire a resource that is already in use by another transaction, while the second transaction tries to acquire a resource that is in use by the first. The figure on the left below illustrates this: solid-line arrows indicate acquired resources, while dashed-line arrows show attempts to acquire a resource that is already in use.&lt;/p&gt;&lt;p&gt;To visualize a deadlock, it is convenient to build the wait-for graph. To do this, we remove specific resources, leave only transactions and indicate which transaction waits for which other. If a graph contains a cycle (from a vertex, we can get to itself in a walk along arrows), this is a deadlock.&lt;/p&gt;&lt;p&gt;A deadlock can certainly occur not only for two transactions, but for any larger number of them.&lt;/p&gt;&lt;p&gt;If a deadlock occured, the involved transactions can do nothing but wait infinitely. Therefore, all DBMS, including PostgreSQL, track locks automatically.&lt;/p&gt;&lt;p&gt;The check, however, requires a certain effort, and it's undesirable to make it each time a new lock is requested (deadlocks are pretty infrequent after all). So, when a process tries to acquire a lock, but cannot, it queues and «falls asleep», but sets the timer to the value specified in the deadlock_timeout parameter (1 second by default). If the resource gets free earlier, this is fine and we skimp on the check. But if on expiration of deadlock_timeout, the wait continues, the waiting process will wake up and initiate the check.&lt;/p&gt;&lt;p&gt;If the check (which consists in building the wait-for graph and searching it for cycles) does not detect deadlocks, it continues sleeping, this time «until final victory».&lt;/p&gt;&lt;quote&gt;Earlier, I was fairly reproached in the comments for not mentioning the lock_timeout parameter, which affects any operator and allows avoiding an infinitely long wait: if a lock cannot be acquired during the time specified, the operator terminates with a&lt;code&gt;lock_not_available&lt;/code&gt;error. Do not confuse this parameter with statement_timeout, which limits the total time to execute the operator, no matter whether the latter waits for a lock or does a regular work.&lt;/quote&gt;&lt;p&gt;But if a deadlock is detected, one of the transactions (in most cases, the one that initiated the check) is forced to abort. This releases the locks it acquired and enables other transactions to continue.&lt;/p&gt;&lt;p&gt;Deadlocks usually mean that the application is designed incorrectly. There are two ways to detect such situations: first, messages will occur in the server log and second, the value of&lt;/p&gt;&lt;code&gt;pg_stat_database.deadlocks&lt;/code&gt; will increase.&lt;head rend="h2"&gt;Example of deadlocking&lt;/head&gt;&lt;p&gt;Usually deadlocks are caused by an inconsistent order of locking table rows.&lt;/p&gt;&lt;p&gt;Let's consider a simple example. The first transaction is going to transfer 100 rubles from the first account to the second one. To this end, the transaction reduces the first account:&lt;/p&gt;&lt;code&gt;=&amp;gt; BEGIN;
=&amp;gt; UPDATE accounts SET amount = amount - 100.00 WHERE acc_no = 1;
&lt;/code&gt;&lt;code&gt;UPDATE 1
&lt;/code&gt;&lt;p&gt;At the same time, the second transaction is going to transfer 10 rubles from the second account to the first one. And it starts with reducing the second account:&lt;/p&gt;&lt;code&gt;|  =&amp;gt; BEGIN;
|  =&amp;gt; UPDATE accounts SET amount = amount - 10.00 WHERE acc_no = 2;
&lt;/code&gt;&lt;code&gt;|  UPDATE 1
&lt;/code&gt;&lt;p&gt;Now the first transaction tries to increase the second account, but detects a lock on the row.&lt;/p&gt;&lt;code&gt;=&amp;gt; UPDATE accounts SET amount = amount + 100.00 WHERE acc_no = 2;
&lt;/code&gt;&lt;p&gt;Then the second transaction tries to increase the first account, but also gets blocked.&lt;/p&gt;&lt;code&gt;|  =&amp;gt; UPDATE accounts SET amount = amount + 10.00 WHERE acc_no = 1;
&lt;/code&gt;&lt;p&gt;So a circular wait arises, which won't end on its own. In a second, the first transaction, which cannot access the resource yet, initiates a check for a deadlock and is forced to abort by the server.&lt;/p&gt;&lt;code&gt;ERROR:  deadlock detected
DETAIL:  Process 16477 waits for ShareLock on transaction 530695; blocked by process 16513.
Process 16513 waits for ShareLock on transaction 530694; blocked by process 16477.
HINT:  See server log for query details.
CONTEXT:  while updating tuple (0,2) in relation "accounts"
&lt;/code&gt;&lt;p&gt;Now the second transaction can continue.&lt;/p&gt;&lt;code&gt;|  UPDATE 1
&lt;/code&gt;&lt;code&gt;|  =&amp;gt; ROLLBACK;
&lt;/code&gt;&lt;code&gt;=&amp;gt; ROLLBACK;
&lt;/code&gt;&lt;p&gt;The correct way to perform such operations is to lock resources in the same order. For example: in this case, accounts can be locked in ascending order of their numbers.&lt;/p&gt;&lt;head rend="h2"&gt;Deadlock of two UPDATE commands&lt;/head&gt;&lt;p&gt;Sometimes we can get a deadlock in situations where, seemingly, it could never occur. For example: it is convenient and usual to treat SQL commands as atomic, but the UPDATE command locks rows as they are updated. This does not happen instantaneously. Therefore, if the order in which a command updates rows is inconsistent with the order in which another command does this, a deadlock can occur.&lt;/p&gt;&lt;p&gt;Although such a situation is unlikely, it can still occur. To reproduce it, we will create an index on the&lt;/p&gt;&lt;code&gt;amount&lt;/code&gt; column in descending order of &lt;code&gt;amount&lt;/code&gt;:&lt;code&gt;=&amp;gt; CREATE INDEX ON accounts(amount DESC);
&lt;/code&gt;&lt;p&gt;To be able to watch what happens, let's create a function that increases the passed value, but very-very slowly, for as long as an entire second:&lt;/p&gt;&lt;code&gt;=&amp;gt; CREATE FUNCTION inc_slow(n numeric) RETURNS numeric AS $$
  SELECT pg_sleep(1);
  SELECT n + 100.00;
$$ LANGUAGE SQL;
&lt;/code&gt;&lt;p&gt;We will also need the&lt;/p&gt;&lt;code&gt;pgrowlocks&lt;/code&gt; extension.&lt;code&gt;=&amp;gt; CREATE EXTENSION pgrowlocks;
&lt;/code&gt;&lt;p&gt;The first UPDATE command will update the entire table. The execution plan is evident — it is sequential scan:&lt;/p&gt;&lt;code&gt;|  =&amp;gt; EXPLAIN (costs off)
|  UPDATE accounts SET amount = inc_slow(amount);
&lt;/code&gt;&lt;code&gt;|           QUERY PLAN         
|  ----------------------------
|   Update on accounts
|     -&amp;gt;  Seq Scan on accounts
|  (2 rows)
&lt;/code&gt;&lt;p&gt;Since tuples on the table page are located in ascending order of the amount (exactly how we added them), they will also be updated in the same order. Let the update start.&lt;/p&gt;&lt;code&gt;|  =&amp;gt; UPDATE accounts SET amount = inc_slow(amount);
&lt;/code&gt;&lt;p&gt;At the same time, in another session we'll forbid sequential scans:&lt;/p&gt;&lt;code&gt;||     =&amp;gt; SET enable_seqscan = off;
&lt;/code&gt;&lt;p&gt;In this case, for the next UPDATE operator, the planner decides to use index scan:&lt;/p&gt;&lt;code&gt;||     =&amp;gt; EXPLAIN (costs off)
||     UPDATE accounts SET amount = inc_slow(amount) WHERE amount &amp;gt; 100.00;
&lt;/code&gt;&lt;code&gt;||                            QUERY PLAN                       
||     --------------------------------------------------------
||      Update on accounts
||        -&amp;gt;  Index Scan using accounts_amount_idx on accounts
||              Index Cond: (amount &amp;gt; 100.00)
||     (3 rows)
&lt;/code&gt;&lt;p&gt;The second and third rows meet the condition, and since the index is built in descending order of the amount, the rows will be updated in a reverse order.&lt;/p&gt;&lt;p&gt;Let's run the next update.&lt;/p&gt;&lt;code&gt;||     =&amp;gt; UPDATE accounts SET amount = inc_slow(amount) WHERE amount &amp;gt; 100.00;
&lt;/code&gt;&lt;p&gt;A quick look into the table page shows that the first operator already managed to update the first row (0,1) and the second operator updated the last row (0,3):&lt;/p&gt;&lt;code&gt;=&amp;gt; SELECT * FROM pgrowlocks('accounts') \gx
&lt;/code&gt;&lt;code&gt;-[ RECORD 1 ]-----------------
locked_row | (0,1)
locker     | 530699            &amp;lt;- the first
multi      | f
xids       | {530699}
modes      | {"No Key Update"}
pids       | {16513}
-[ RECORD 2 ]-----------------
locked_row | (0,3)
locker     | 530700            &amp;lt;- the second
multi      | f
xids       | {530700}
modes      | {"No Key Update"}
pids       | {16549}
&lt;/code&gt;&lt;p&gt;One more second elapses. The first operator updated the second row, and the second one would like to do the same, but cannot.&lt;/p&gt;&lt;code&gt;=&amp;gt; SELECT * FROM pgrowlocks('accounts') \gx
&lt;/code&gt;&lt;code&gt;-[ RECORD 1 ]-----------------
locked_row | (0,1)
locker     | 530699            &amp;lt;- the first
multi      | f
xids       | {530699}
modes      | {"No Key Update"}
pids       | {16513}
-[ RECORD 2 ]-----------------
locked_row | (0,2)
locker     | 530699            &amp;lt;- the first was quicker
multi      | f
xids       | {530699}
modes      | {"No Key Update"}
pids       | {16513}
-[ RECORD 3 ]-----------------
locked_row | (0,3)
locker     | 530700            &amp;lt;- the second
multi      | f
xids       | {530700}
modes      | {"No Key Update"}
pids       | {16549}
&lt;/code&gt;&lt;p&gt;Now the first operator would like to update the last table row, but it is already locked by the second operator. Hence a deadlock.&lt;/p&gt;&lt;p&gt;One of the transactions aborts:&lt;/p&gt;&lt;code&gt;||     ERROR:  deadlock detected
||     DETAIL:  Process 16549 waits for ShareLock on transaction 530699; blocked by process 16513.
||     Process 16513 waits for ShareLock on transaction 530700; blocked by process 16549.
||     HINT:  See server log for query details.
||     CONTEXT:  while updating tuple (0,2) in relation "accounts"
&lt;/code&gt;&lt;p&gt;And the second one continues:&lt;/p&gt;&lt;code&gt;|  UPDATE 3
&lt;/code&gt;&lt;quote&gt;Engaging details of detecting and preventing deadlocks can be found in the lock manager README.&lt;/quote&gt;&lt;p&gt;This completes a talk on deadlocks, and we proceed to the remaining object-level locks.&lt;/p&gt;&lt;head rend="h1"&gt;Locks on non-relations&lt;/head&gt;&lt;p&gt;When we need to lock a resource that is not a relation in the meaning of PostgreSQL, locks of the&lt;/p&gt;&lt;code&gt;object&lt;/code&gt; type are used. Almost whatever we can think of can refer to such resources: tablespaces, subscriptions, schemas, enumerated data types and so on. Roughly, this is everything that can be found in the system catalog.&lt;p&gt;Illustrating this by a simple example. Let's start a transaction and create a table in it:&lt;/p&gt;&lt;code&gt;=&amp;gt; BEGIN;
=&amp;gt; CREATE TABLE example(n integer);
&lt;/code&gt;&lt;p&gt;Now let's see what locks of the&lt;/p&gt;&lt;code&gt;object&lt;/code&gt; type appeared in &lt;code&gt;pg_locks&lt;/code&gt;:&lt;code&gt;=&amp;gt; SELECT
  database,
  (SELECT datname FROM pg_database WHERE oid = l.database) AS dbname,
  classid,
  (SELECT relname FROM pg_class WHERE oid = l.classid) AS classname,
  objid,
  mode,
  granted
FROM pg_locks l
WHERE l.locktype = 'object' AND l.pid = pg_backend_pid();
&lt;/code&gt;&lt;code&gt; database | dbname | classid |  classname   | objid |      mode       | granted
----------+--------+---------+--------------+-------+-----------------+---------
        0 |        |    1260 | pg_authid    | 16384 | AccessShareLock | t
    16386 | test   |    2615 | pg_namespace |  2200 | AccessShareLock | t
(2 rows)
&lt;/code&gt;&lt;p&gt;To figure out what in particular is locked here, we need to look at three fields:&lt;/p&gt;&lt;code&gt;database&lt;/code&gt;, &lt;code&gt;classid&lt;/code&gt; and &lt;code&gt;objid&lt;/code&gt;. We start with the first line.&lt;code&gt;database&lt;/code&gt; is the OID of the database that the resource being locked relates to. In this case, this column contains zero. It means that we deal with a global object, which is not specific to any database.&lt;code&gt;classid&lt;/code&gt; contains the OID from &lt;code&gt;pg_class&lt;/code&gt; that matches the name of the system catalog table that actually determines the resource type. In this case, it is &lt;code&gt;pg_authid&lt;/code&gt;, that is, a role (user) is the resource.&lt;code&gt;objid&lt;/code&gt; contains the OID from the system catalog table indicated by &lt;code&gt;classid&lt;/code&gt;.&lt;code&gt;=&amp;gt; SELECT rolname FROM pg_authid WHERE oid = 16384;
&lt;/code&gt;&lt;code&gt; rolname
---------
 student
(1 row)
&lt;/code&gt;&lt;p&gt;We work as&lt;/p&gt;&lt;code&gt;student&lt;/code&gt;, and this is exactly the role locked.&lt;p&gt;Now let's clarify the second line. The database is specified, and it is&lt;/p&gt;&lt;code&gt;test&lt;/code&gt;, to which we are connected.&lt;code&gt;classid&lt;/code&gt; indicates the &lt;code&gt;pg_namespace&lt;/code&gt; table, which contains schemas.&lt;code&gt;=&amp;gt; SELECT nspname FROM pg_namespace WHERE oid = 2200;
&lt;/code&gt;&lt;code&gt; nspname
---------
 public
(1 row)
&lt;/code&gt;&lt;p&gt;This shows that the&lt;/p&gt;&lt;code&gt;public&lt;/code&gt; schema is locked.&lt;p&gt;So, we've seen that when an object is created, the owner role and schema in which the object is created get locked (in a shared mode). And this is reasonable: otherwise, someone could drop the role or schema while the transaction is not completed yet.&lt;/p&gt;&lt;code&gt;=&amp;gt; ROLLBACK;
&lt;/code&gt;&lt;head rend="h1"&gt;Lock on relation extension&lt;/head&gt;&lt;p&gt;When the number of rows in a relation (table, index or materialized view) increases, PostgreSQL can use free space in available pages for inserts, but evidently, once new pages also have to be added. Physically they are added at the end of the appropriate file. And this is meant by a relation extension.&lt;/p&gt;&lt;p&gt;To ensure that two processes do not rush to add pages simultaneously, the extension process is protected by a specialized lock of the&lt;/p&gt;&lt;code&gt;extend&lt;/code&gt; type. The same lock is used when vacuuming indexes for other processes to be unable to add pages during the scan.&lt;p&gt;This lock is certainly released without waiting for completion of the transaction.&lt;/p&gt;&lt;quote&gt;Earlier, tables could extend only by one page at a time. This caused issues during simultaneous row inserts by several processes; therefore, starting with PostgreSQL 9.6, several pages are added to tables at once (in proportion to the number of waiting processes, but not greater than 512).&lt;/quote&gt;&lt;head rend="h1"&gt;Page lock&lt;/head&gt;&lt;p&gt;Page-level locks of the&lt;/p&gt;&lt;code&gt;page&lt;/code&gt; type are used in the only case (aside from predicate locks, to be discussed later).&lt;p&gt;GIN indexes enable us to accelerate search in compound values, for instance: words in text documents (or array elements). To a first approximation, these indexes can be represented as a regular B-tree that stores separate words from the documents rather than the documents themselves. Therefore, when a new document is added, the index has to be rebuilt pretty much in order to add there each new word from the document.&lt;/p&gt;&lt;p&gt;For better performance, GIN index has a postponed insert feature, which is turned on by the&lt;/p&gt;&lt;code&gt;fastupdate&lt;/code&gt; storage parameter. New words are quickly added to an unordered pending list first, and after a while, everything accumulated is moved to the main index structure. The gains are due to a high probability of occurrence of the same words in different documents.&lt;p&gt;To prevent moving from the pending list to the main index by several processes simultaneously, for the duration of moving, the index metapage gets locked in an exclusive mode. This does not hinder regular use of the index.&lt;/p&gt;&lt;head rend="h1"&gt;Advisory locks&lt;/head&gt;&lt;p&gt;Unlike other locks (such as relation-level locks), advisory locks are never acquired automatically — the application developer controls them. They are useful when, for instance, an application for some reason needs a locking logic that is not in line with the standard logic of regular locks.&lt;/p&gt;&lt;p&gt;Assume we have a hypothetical resource that does not match any database object (which we could lock using commands such as SELECT FOR or LOCK TABLE). We need to devise a numeric identifier for it. If a resource has a unique name, a simple option is to use its hash code:&lt;/p&gt;&lt;code&gt;=&amp;gt; SELECT hashtext('resource1');
&lt;/code&gt;&lt;code&gt; hashtext  
-----------
 991601810
(1 row)
&lt;/code&gt;&lt;p&gt;This is how we have the lock acquired:&lt;/p&gt;&lt;code&gt;=&amp;gt; BEGIN;
=&amp;gt; SELECT pg_advisory_lock(hashtext('resource1'));
&lt;/code&gt;&lt;p&gt;As usual, information on locks is available in&lt;/p&gt;&lt;code&gt;pg_locks&lt;/code&gt;:&lt;code&gt;=&amp;gt; SELECT locktype, objid, mode, granted 
FROM pg_locks WHERE locktype = 'advisory' AND pid = pg_backend_pid();
&lt;/code&gt;&lt;code&gt; locktype |   objid   |     mode      | granted 
----------+-----------+---------------+---------
 advisory | 991601810 | ExclusiveLock | t
(1 row)
&lt;/code&gt;&lt;p&gt;For locking to be really effective, other processes must also acquire a lock on the resource prior to accessing it. Evidently the application must ensure that this rule is observed.&lt;/p&gt;&lt;p&gt;In the above example, the lock will be held through the end of the session rather than the transaction, as usual.&lt;/p&gt;&lt;code&gt;=&amp;gt; COMMIT;
=&amp;gt; SELECT locktype, objid, mode, granted 
FROM pg_locks WHERE locktype = 'advisory' AND pid = pg_backend_pid();
&lt;/code&gt;&lt;code&gt; locktype |   objid   |     mode      | granted 
----------+-----------+---------------+---------
 advisory | 991601810 | ExclusiveLock | t
(1 row)
&lt;/code&gt;&lt;p&gt;And we need to explicitly release it:&lt;/p&gt;&lt;code&gt;=&amp;gt; SELECT pg_advisory_unlock(hashtext('resource1'));
&lt;/code&gt;&lt;p&gt;A rich collection of functions to work with advisory locks is available for all intents and purposes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;pg_advisory_lock_shared&lt;/code&gt;has a shared lock acquired.&lt;/item&gt;&lt;item&gt;&lt;code&gt;pg_advisory_xact_lock&lt;/code&gt;(and&lt;code&gt;pg_advisory_xact_lock_shared&lt;/code&gt;) has a shared lock acquired up to the end of the transaction.&lt;/item&gt;&lt;item&gt;&lt;code&gt;pg_try_advisory_lock&lt;/code&gt;(as well as&lt;code&gt;pg_try_advisory_xact_lock&lt;/code&gt;and&lt;code&gt;pg_try_advisory_xact_lock_shared&lt;/code&gt;) does not wait for a lock, but returns&lt;code&gt;false&lt;/code&gt;if a lock could not be acquired immediately.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;A collection of&lt;/p&gt;&lt;code&gt;try_&lt;/code&gt; functions is one more technique to avoid waiting for a lock, in addition to those listed in the last article.&lt;head rend="h1"&gt;Predicate locks&lt;/head&gt;&lt;p&gt;The predicate lock term occurred long ago, when early DBMS made first attempts to implement complete isolation based on locks (the Serializable level, although there was no SQL standard at that time). The issue they confronted then was that even locking of all read and updated rows did not ensure complete isolation: new rows that meet the same selection conditions can occur in the table, which causes phantoms to arise (see the article on isolation).&lt;/p&gt;&lt;p&gt;The idea of predicate locks was to lock predicates rather than rows. If during execution of a query with the condition a &amp;gt; 10 we lock the a &amp;gt; 10 predicate, this won't allow us to add new rows that meet the condition to the table and will enable us to avoid phantoms. The issue is that this problem is computationally complicated; in practice, it can be solved only for very simple predicates.&lt;/p&gt;&lt;p&gt;In PostgreSQL, the Serializable level is implemented differently, on top of the available isolation based on data snapshots. Although the predicate lock term is still used, its meaning drastically changed. Actually these «locks» block nothing; they are used to track data dependencies between transactions.&lt;/p&gt;&lt;p&gt;It is proved that snapshot isolation permits an inconsistent write (write skew) anomaly and a read-only transaction anomaly, but any other anomalies are impossible. To figure out that we deal with one of the two above anomalies, we can analyze dependencies between transactions and discover certain patterns there.&lt;/p&gt;&lt;p&gt;Dependencies of two kinds are of interest to us:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;One transaction reads a row that is then updated by the second transaction (RW dependency).&lt;/item&gt;&lt;item&gt;One transaction updates a row that is then read by the second transaction (WR dependency).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We can track WR dependencies using already available regular locks, but RW dependencies have to be tracked specially.&lt;/p&gt;&lt;p&gt;To reiterate, despite the name, predicate locks bock nothing. A check is performed at the transaction commit instead, and if a suspicious sequence of dependencies that may indicate an anomaly is discovered, the transaction aborts.&lt;/p&gt;&lt;p&gt;Let's look at how predicate locks are handled. To do this, we'll create a table with a pretty large number of locks and an index on it.&lt;/p&gt;&lt;code&gt;=&amp;gt; CREATE TABLE pred(n integer);
=&amp;gt; INSERT INTO pred(n) SELECT g.n FROM generate_series(1,10000) g(n);
=&amp;gt; CREATE INDEX ON pred(n) WITH (fillfactor = 10);
=&amp;gt; ANALYZE pred;
&lt;/code&gt;&lt;p&gt;If a query is executed using sequential scan of the entire table, a predicate lock on the entire table gets acquired (even if not all rows meet the filtering condition).&lt;/p&gt;&lt;code&gt;|  =&amp;gt; SELECT pg_backend_pid();
&lt;/code&gt;&lt;code&gt;|   pg_backend_pid 
|  ----------------
|            12763
|  (1 row)
&lt;/code&gt;&lt;code&gt;|  =&amp;gt; BEGIN ISOLATION LEVEL SERIALIZABLE;
|  =&amp;gt; EXPLAIN (analyze, costs off)
|    SELECT * FROM pred WHERE n &amp;gt; 100;
&lt;/code&gt;&lt;code&gt;|                             QUERY PLAN                           
|  ----------------------------------------------------------------
|   Seq Scan on pred (actual time=0.047..12.709 rows=9900 loops=1)
|     Filter: (n &amp;gt; 100)
|     Rows Removed by Filter: 100
|   Planning Time: 0.190 ms
|   Execution Time: 15.244 ms
|  (5 rows)
&lt;/code&gt;&lt;p&gt;All predicate locks are acquired in one special mode — SIReadLock (Serializable Isolation Read):&lt;/p&gt;&lt;code&gt;=&amp;gt; SELECT locktype, relation::regclass, page, tuple
FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 12763;
&lt;/code&gt;&lt;code&gt; locktype | relation | page | tuple 
----------+----------+------+-------
 relation | pred     |      |      
(1 row)
&lt;/code&gt;&lt;code&gt;|  =&amp;gt; ROLLBACK;
&lt;/code&gt;&lt;p&gt;But if a query is executed using index scan, the situation changes for the better. If we deal with a B-tree, it is sufficient to have a lock acquired on the rows read and on the leaf index pages walked through — this allows us to track not only specific values, but all the range read.&lt;/p&gt;&lt;code&gt;|  =&amp;gt; BEGIN ISOLATION LEVEL SERIALIZABLE;
|  =&amp;gt; EXPLAIN (analyze, costs off)
|    SELECT * FROM pred WHERE n BETWEEN 1000 AND 1001;
&lt;/code&gt;&lt;code&gt;|                                       QUERY PLAN                                     
|  ------------------------------------------------------------------------------------
|   Index Only Scan using pred_n_idx on pred (actual time=0.122..0.131 rows=2 loops=1)
|     Index Cond: ((n &amp;gt;= 1000) AND (n &amp;lt;= 1001))
|     Heap Fetches: 2
|   Planning Time: 0.096 ms
|   Execution Time: 0.153 ms
|  (5 rows)
&lt;/code&gt;&lt;code&gt;=&amp;gt; SELECT locktype, relation::regclass, page, tuple
FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 12763;
&lt;/code&gt;&lt;code&gt; locktype |  relation  | page | tuple 
----------+------------+------+-------
 tuple    | pred       |    3 |   236
 tuple    | pred       |    3 |   235
 page     | pred_n_idx |   22 |      
(3 rows)
&lt;/code&gt;&lt;p&gt;Note a few complexities.&lt;/p&gt;&lt;p&gt;First, a separate lock is created for each read tuple, and the number of such tuples can potentially be very large. The total number of predicate locks in the system is limited by the product of parameter values: max_pred_locks_per_transaction × max_connections (the default values are 64 and 100, respectively). The memory for these locks is allocated at the server start; an attempt to exceed this limit will result in errors.&lt;/p&gt;&lt;p&gt;Therefore, escalation is used for predicate locks (and only for them!). Prior to PostgreSQL 10, the limitations were hard coded, but starting this version, we can control the escalation through parameters. If the number of tuple locks related to one page exceeds max_pred_locks_per_page, these locks are replaced with one page-level lock. Consider an example:&lt;/p&gt;&lt;code&gt;=&amp;gt; SHOW max_pred_locks_per_page;
&lt;/code&gt;&lt;code&gt; max_pred_locks_per_page 
-------------------------
 2
(1 row)
&lt;/code&gt;&lt;code&gt;|  =&amp;gt; EXPLAIN (analyze, costs off)
|    SELECT * FROM pred WHERE n BETWEEN 1000 AND 1002;
&lt;/code&gt;&lt;code&gt;|                                       QUERY PLAN                                     
|  ------------------------------------------------------------------------------------
|   Index Only Scan using pred_n_idx on pred (actual time=0.019..0.039 rows=3 loops=1)
|     Index Cond: ((n &amp;gt;= 1000) AND (n &amp;lt;= 1002))
|     Heap Fetches: 3
|   Planning Time: 0.069 ms
|   Execution Time: 0.057 ms
|  (5 rows)
&lt;/code&gt;&lt;p&gt;We see one lock of the&lt;/p&gt;&lt;code&gt;page&lt;/code&gt; type instead of three locks of the &lt;code&gt;tuple&lt;/code&gt; type:&lt;code&gt;=&amp;gt; SELECT locktype, relation::regclass, page, tuple
FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 12763;
&lt;/code&gt;&lt;code&gt; locktype |  relation  | page | tuple 
----------+------------+------+-------
 page     | pred       |    3 |      
 page     | pred_n_idx |   22 |      
(2 rows)
&lt;/code&gt;&lt;p&gt;Likewise, if the number of locks on pages related to one relation exceeds max_pred_locks_per_relation, these locks are replaced with one relation-level lock.&lt;/p&gt;&lt;p&gt;There are no other levels: predicate locks are acquired only for relations, pages and tuples and always in the SIReadLock mode.&lt;/p&gt;&lt;p&gt;Certainly, escalation of locks inevitably results in an increase of the number of transactions that falsely terminate with a serialization error, and eventually, the system throuthput will decrease. Here you need to balance RAM consumption and performance.&lt;/p&gt;&lt;p&gt;The second complexity is that different operations with an index (for instance, due to splits of index pages when new rows are inserted) change the number of leaf pages that cover the range read. But the implementation takes this into account:&lt;/p&gt;&lt;code&gt;=&amp;gt; INSERT INTO pred SELECT 1001 FROM generate_series(1,1000);
=&amp;gt; SELECT locktype, relation::regclass, page, tuple
FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 12763;
&lt;/code&gt;&lt;code&gt; locktype |  relation  | page | tuple 
----------+------------+------+-------
 page     | pred       |    3 |      
 page     | pred_n_idx |  211 |      
 page     | pred_n_idx |  212 |      
 page     | pred_n_idx |   22 |      
(4 rows)
&lt;/code&gt;&lt;code&gt;|  =&amp;gt; ROLLBACK;
&lt;/code&gt;&lt;p&gt;By the way, predicate locks are not always released immediately on completion of the transaction since they are needed to track dependencies between several transactions. But anyway, they are controlled automatically.&lt;/p&gt;&lt;p&gt;By no means all types of indexes in PostgreSQL support predicate locks. Before PostgreSQL 11, only B-trees could boast of this, but that version improved the situation: hash, GiST and GIN indexes were added to the list. If index access is used, but the index does not support predicate locks, a lock on the entire index is acquired. This, certainly, also increases the number of false aborts of transactions.&lt;/p&gt;&lt;p&gt;Finally, note that it's the use of predicate locks that limits all transactions to working at the Serializable level in order to ensure complete isolation. If a certain transaction uses a different level, it just won't acquire (and check) predicate locks.&lt;/p&gt;&lt;quote&gt;Traditionally, providing you with a link to the predicate locking README, to start exploring the source code with.&lt;/quote&gt;&lt;p&gt;Read on.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://habr.com/en/companies/postgrespro/articles/504498/"/><published>2025-12-07T15:42:02+00:00</published></entry></feed>