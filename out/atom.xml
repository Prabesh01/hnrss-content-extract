<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-04T13:47:50.407606+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45804122</id><title>The Mack Super Pumper was a locomotive engined fire fighter (2018)</title><updated>2025-11-04T13:47:56.441048+00:00</updated><content>&lt;doc fingerprint="3a1ad86c4fc08ebb"&gt;
  &lt;main&gt;
    &lt;p&gt;(Lead image: Philip Goldstein) – In the early 1960s the New York City Fire Department was facing a host of problems. The world around them was growing ever taller, ever more compact, and ever more dangerous with respect to fire. There were times when the very infrastructure that was supposed to be supplying them the water to extinguish a blaze simply stopped flowing, there were other times that the equipment they had proved to be woefully insufficient to stop a fire that should have been controlled, resulting in massive blazes that ate up homes, businesses, lives, and millions upon millions of dollars. In April of 1963, a massive fire on Staten Island taxed the city’s fire service to the absolute breaking point while destroying millions and millions in property.&lt;/p&gt;
    &lt;p&gt;The day is still referred to as Black Saturday by the people who lived through it. Due to the lack of suitable water supplies, the fire was far larger than it should have been. There was a drought that year and many of the sources that the firemen were used to pulling water from had literally run dry. This began a series of events that led to the construction of the most powerful land based fire fighting truck ever created, the Mack Super Pumper System. It was actually five trucks that worked as a brigade to battle the worst flaming disasters that the city could throw at it.&lt;/p&gt;
    &lt;p&gt;From 1965 through the early 1980s, the Mack Super Pumper System responded to more than 2,200 calls with more than 900 firefighters serving to operate it in some capacity. The five trucks that made up the super pumper system were a massive, locomotive-engined central pumping truck, a tender truck full of hoses, manifolds, and other gear, and three satellite trucks that looked like standard fire engines but were not equipped with their own pumps. It cost the city of New York $875,000 when it was new and we’ll wager to say that it was probably the best money ever spent to keep Gotham safe. There’s never been anything else like it.&lt;/p&gt;
    &lt;p&gt;The pumping unit –&lt;/p&gt;
    &lt;p&gt;The keystone of the whole operation was the massive central pumping unit that could draw water from eight hydrants at once, drop lines into bodies of water, supply a mind-boggling number of lines with water simultaneously, and flow over 10,000 gallons per minute at low pressures if the situation called for it. When the pressure was ramped up to to 350psi, it could move 8,800 GPM. This was enough to supply the other satellite trucks as well as feed a massive water cannon on the tender truck that could heave water over 600ft. That’s right, nearly and eight of a mile in whatever direction you wanted it to go. How was this possible? It was possible because of innovations in diesel engine technology during WWII. The grunt for the Super Pumper system came from a Napier-Deltic diesel engine. This was an engine designed by the British during WWII as a lightweight, high speed means to propel their ships. Making 2,400 horsepower and even more prodigious torque numbers, the engine was “light” enough to be mounted in a trailer behind a tractor and carted around. The Napier-Deltic was used to power locomotives and other massive land craft as well for a while. The engine’s design is interesting in the fact that it had three crankshafts and was an opposed piston style engine meaning that the pistons travel at each other. With turbochargers and a two stroke design, it was as mighty a compact piston powered engine the world had ever known to that point. It was thirsty and noisy as well. When working at full song, the engine would consume 137 gallons of diesel fuel per hour and the noise was so deafening that firemen near the truck had to wear strong ear protection to prevent hearing damage.&lt;/p&gt;
    &lt;p&gt;Mack was awarded the contract to build the truck in 1964 and by the end of the year, the unit was nearly ready to hit the streets of NYC. The tractor employed to drag the pumping unit around was a F715FSTP cab over that used a 255hp Mack END864 engine. The top speed of the whole rig was 42mph but since it was intended for responding to calls in the city, high mph was not as much a concern as maneuverability, and the ability to zip around at lower speeds happily. There were custom built PTOs to power the priming pump for the water pump and to to run an air compressor that needed 450psi to light off the pump engine.&lt;/p&gt;
    &lt;p&gt;The custom built trailer housed the engine and all of the stuff needed to keep it alive like the cooling system, fuel tanks, etc. At the rear of the trailer was the enormous six stage pump which was built by a company called DeLaval and that’s where the real magic happened. When the big Deltic was put to work turning that bad boy, fire, at least any on the first 60 stories of a building, didn’t stand a chance. The whole rig weighed in at 68,000lbs and for as much reading and research as we have done, there are no accounts of it ever faltering, failing, or leaving firefighters without the resources they needed to battle a fire. Often times we read about awesome machines like this and discover that they were unreliable or prone to fail but not this big guy.&lt;/p&gt;
    &lt;p&gt;Some pretty stunning facts about the truck and the pumper:&lt;/p&gt;
    &lt;p&gt;At 8,800 GPM it was throwing nearly 70,000lbs of water on a fire per minute.&lt;/p&gt;
    &lt;p&gt;During a fire in the Bronx, firemen laid 7,000ft of hose to get to a suitable water supply and the truck pumped as though it was dipping its feet into the ocean.&lt;/p&gt;
    &lt;p&gt;In 1967 the Super Pumper responded to a fire at a postal annex in NYC and managed to supply water to the massive gun on the tender truck, its three satellite units, two tower ladder trucks, and a portable manifold with multiple hand lines all by itself.&lt;/p&gt;
    &lt;p&gt;The hoses on the truck were pressure tested to 1,000psi of pressure but typically operated anywhere in the 350-800psi range depending on the situation. This is way higher (by several times!) what modern trucks use by our understanding. The hoses were a derivative of hoses developed by the Navy in WWII for high pressure applications and while incredibly heavy when compared to modern hoses, they were cutting edge at the time.&lt;/p&gt;
    &lt;p&gt;The truck still exists, living at a museum in Michigan and standing as a great reminder that human beings are amazingly inventive and creative beings when forced to find solutions to problems that endanger lots of lives or lots of valuable property!&lt;/p&gt;
    &lt;p&gt;SCROLL DOWN FOR MORE INFORMATION AND PHOTOS ON THIS MIGHTY TRUCK THAT SERVED NYC DUTIFULLY FOR MANY YEARS –&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bangshift.com/bangshiftxl/mack-super-pumper-system-locomotive-engine-powered-pumper-extinguish-hell-often/"/><published>2025-11-03T20:37:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45804377</id><title>AI's Dial-Up Era</title><updated>2025-11-04T13:47:56.088432+00:00</updated><content>&lt;doc fingerprint="898080510eed85e8"&gt;
  &lt;main&gt;
    &lt;p&gt;It is 1995.&lt;/p&gt;
    &lt;p&gt;Your computer modem screeches as it tries to connect to something called the internet. Maybe it works. Maybe you try again.&lt;/p&gt;
    &lt;p&gt;For the first time in history, you can exchange letters with someone across the world in seconds. Only 2000-something websites exist1, so you could theoretically visit them all over a weekend. Most websites are just text on gray backgrounds with the occasional pixelated image2. Loading times are brutal. A single image takes a minute, a 1-minute video could take hours. Most people do not trust putting their credit cards online. The advice everyone gives: don’t trust strangers on the internet.&lt;/p&gt;
    &lt;p&gt;People split into two camps very soon.&lt;/p&gt;
    &lt;p&gt;Optimists predict grand transformations. Some believe digital commerce will overtake physical retail within years. Others insist we’ll wander around in virtual reality worlds.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“I expect that within the next five years more than one in ten people will wear head-mounted computer displays while traveling in buses, trains, and planes.” - Nicholas Negroponte, MIT Professor, 1993&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Pessimists call the internet a fad and a bubble.&lt;/p&gt;
    &lt;p&gt;If you told the average person in 1995 that within 25 years, we’d consume news from strangers on social media over newspapers, watch shows on-demand in place of cable TV, find romantic partners through apps more than through friends, and flip “don’t trust strangers on the internet” so completely that we’d let internet strangers pick us up in their personal vehicles and sleep in their spare bedrooms, most people would find that hard to believe.&lt;/p&gt;
    &lt;p&gt;We’re in 1995 again. This time with Artificial Intelligence.&lt;/p&gt;
    &lt;p&gt;And both sides of today’s debate are making similar mistakes.&lt;/p&gt;
    &lt;p&gt;One side warns that AI will eliminate entire professions and cause mass unemployment within a couple of years. The other claims that AI will create more jobs than it destroys. One camp dismisses AI as overhyped vaporware destined for a bubble burst, while the other predicts it will automate every knowledge task and reshape civilization within the decade.&lt;/p&gt;
    &lt;p&gt;Both are part right and part wrong.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Employment Paradox: Why Automation’s Impact Depends On The Industry&lt;/head&gt;
    &lt;p&gt;Geoffrey Hinton, who some call the Father of AI, warned in 2016 that AI would trigger mass unemployment. “People should stop training radiologists now,” he declared, certain that AI would replace them within years.&lt;/p&gt;
    &lt;p&gt;Yet as Deena Mousa, a researcher, shows in “The Algorithm Will See You Now,” AI hasn’t replaced radiologists, despite predictions. It is thriving.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2025, American diagnostic radiology residency programs offered a record 1,208 positions across all radiology specialties, a four percent increase from 2024, and the field’s vacancy rates are at all-time highs. In 2025, radiology was the second-highest-paid medical specialty in the country, with an average income of $520,000, over 48 percent higher than the average salary in 2015.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Mousa identifies a few factors for why the prediction failed - real-world complexity, the job involves more than image recognition, and regulatory/insurance hurdles. Most critical she points is Jevons Paradox, which is the economic principle that a technological improvement in resource efficiency leads to an increase in the total consumption of that resource, rather than a decrease. Her argument is that as AI makes radiologists more productive, better diagnostics and faster turnaround at lower costs mean more people get scans. So employment doesn’t decrease. It increases.&lt;/p&gt;
    &lt;p&gt;This is also the Tech world’s consensus. Microsoft CEO Satya Nadella agrees, as does Box CEO Aaron Levie, who suggests:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“The least understood yet most important concept in the world is Jevons Paradox. When we make a technology more efficient, demand goes well beyond the original level. AI is the perfect example of this—almost anything that AI is applied to will see more demand, not less.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;They’re only half right.&lt;/p&gt;
    &lt;p&gt;First, as Andrej Karpathy, the computer scientist who coined the term vibe coding, points out, radiology is not the right job to look for initial job displacements.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Radiology is too multi-faceted, too high risk, too regulated. When looking for jobs that will change a lot due to AI on shorter time scales, I’d look in other places - jobs that look like repetition of one rote task, each task being relatively independent, closed (not requiring too much context), short (in time), forgiving (the cost of mistake is low), and of course automatable giving current (and digital) capability. Even then, I’d expect to see AI adopted as a tool at first, where jobs change and refactor (e.g. more monitoring or supervising than manual doing, etc).”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Second, the tech consensus that we will see increased employment actually depends on the industry. Specifically, how much unfulfilled demand can be unlocked in that industry, and whether this unfulfilled demand growth outpaces continued automation and productivity improvement.&lt;/p&gt;
    &lt;p&gt;To understand this better, look at what actually happened in three industries over a 200-year period from 1800 to 2000. In the paper Automation and jobs: when technology boosts employment, James Bessen, an economist, shows the employment, productivity, and demand data for textile, iron &amp;amp; steel, and motor vehicle industries.&lt;/p&gt;
    &lt;p&gt;After automation, both textile and iron/steel workers saw employment increase for nearly a century before experiencing a steep decline. Vehicle manufacturing, by contrast, holds steady and hasn’t seen the same steep decline yet.&lt;/p&gt;
    &lt;p&gt;To answer why those two industries saw sharp declines but motor vehicle manufacturing did not, first look at the productivity of workers in all three industries:&lt;/p&gt;
    &lt;p&gt;Then look at the demand across those three industries:&lt;/p&gt;
    &lt;p&gt;What the graphs show is a consistent pattern (note: the productivity and demand graphs are logarithmic, meaning productivity and demand grew exponentially). Early on, a service or product is expensive because many workers are needed to produce it. Most people can’t afford it or use them sparingly. For example, in the early 1800s, most people could only afford a pair of pants or shirt. Then automation makes workers dramatically more productive. A textile worker in 1900 could produce fifty times more than one in 1800. This productivity explosion crashes prices, which creates massive new demand. Suddenly everyone can afford multiple outfits instead of just one or two. Employment and productivity both surge (note: employment growth masks internal segment displacement and wage changes. See footnote3)&lt;/p&gt;
    &lt;p&gt;Once demand saturates, employment doesn’t further increase but holds steady at peak demand. But as automation continues and workers keep getting more productive, employment starts to decline. In textiles, mechanization enabled massive output growth but ultimately displaced workers once consumption plateaued while automation and productivity continued climbing. We probably don’t need infinite clothing. Similarly, patients will likely never need a million radiology reports, no matter how cheap they become and so radiologists will eventually hit a ceiling. We don’t need infinite food, clothing, tax returns, and so on.&lt;/p&gt;
    &lt;p&gt;Motor vehicles, in Bessen’s graphs, tell a different story because demand remains far from saturated. Most people globally still don’t own cars. Automation hasn’t completely conquered manufacturing either (Tesla’s retreat from full manufacturing automation proves the current technical limits). When both demand and automation potential remain high, employment can sustain or even grow despite productivity gains.&lt;/p&gt;
    &lt;p&gt;Software presents an even more interesting question. How many apps do you need? What about software that generates applications on demand, that creates entire software ecosystems autonomously? Until now, handcrafted software was the constraint. Expensive software engineers and their labor costs limited what companies could afford to build. Automation changes this equation by making those engineers far more productive. Both consumer and enterprise software markets suggest significant unmet demand because businesses have consistently left projects unbuilt4. They couldn’t justify the development costs or had to allocate limited resources to their top priority projects. I saw this firsthand at Amazon. Thousands of ideas went unfunded not because they lacked business value, but because of the lack of engineering resources to build them. If AI can produce software at a fraction of the cost, that unleashes enormous latent demand. The key question then is if and when that demand will saturate.&lt;/p&gt;
    &lt;p&gt;So to generalize, for each industry, employment hinges on a race between two forces:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The magnitude and growth of unmet market demand, and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Whether that demand growth outpaces productivity improvements from automation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Different industries will experience different outcomes depending on who’s winning that demand and productivity race.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Bubble: Irrational Exuberance Builds the Future&lt;/head&gt;
    &lt;p&gt;The second debate centers on whether this AI boom is a bubble waiting to burst.&lt;/p&gt;
    &lt;p&gt;The dotcom boom of the 1990s saw a wave of companies adding “.com” to their name to ride the mania and watch their valuations soar. Infra companies poured billions into fiber optics and undersea cables - expensive projects only possible because people believed the hype5. All of this eventually burst in spectacular fashion in the dotcom crash in 2000-2001. Infrastructure companies like Cisco briefly became the most valuable in the world only to come tumbling down6. Pets.com served as the poster child of this exuberance raising $82.5 million in its IPO, spending millions on a Super Bowl ad only to collapse nine months later7.&lt;/p&gt;
    &lt;p&gt;But the dotcom bubble also got several things right. More importantly, it eventually bought us the physical infrastructure that made YouTube, Netflix, and Facebook possible. Sure, companies like Worldcom, NorthPoint, and Global Crossing making these investments went bankrupt, but they also laid the foundation for the future. Although the crash proved the skeptics right in the short term, it proved the optimists were directionally correct in the long term.&lt;/p&gt;
    &lt;p&gt;Today’s AI boom shows similar exuberance. Consider the AI startup founded by former OpenAI executive Mira Murati, which raised $2 billion at a $10 billion valuation, the largest seed round in history8. This despite having no product and declining to reveal what it’s building or how it will generate returns. Several AI wrappers have raised millions in seed funding with little to no moat.&lt;/p&gt;
    &lt;p&gt;Yet some investments will outlast the hype and will likely help future AI companies even if this is a bubble. For example, the annual capital expenditures of Hyperscalers9 that have more than doubled since ChatGPT’s release - Microsoft, Google, Meta, and Amazon are collectively spending almost half a trillion dollars on data centers, chips, and compute infrastructure. Regardless of which specific companies survive, this infrastructure being built now will create the foundation for our AI future - from inference capacity to the power generation needed to support it.&lt;/p&gt;
    &lt;p&gt;The infrastructure investments may have long-term value, but are we already in bubble territory? Azeem Azhar, a tech analyst and investor, provides an excellent practical framework to answer the AI bubble question. He benchmarks today’s AI boom using five gauges: economic strain (investment as a share of GDP), industry strain (capex to revenue ratios), revenue growth trajectories (doubling time), valuation heat (price-to-earnings multiples), and funding quality (the resilience of capital sources). His analysis shows that AI remains in a demand-led boom rather than a bubble, but if two of the five gauges head into red, we will be in bubble territory.&lt;/p&gt;
    &lt;p&gt;The demand is real. After all OpenAI is one of the fastest-growing companies in history10. But that alone doesn’t prevent bubbles. OpenAI will likely be fine given its product-market fit, but many other AI companies face the same unit economics questions that plagued dotcom companies in the 1990s. Pets.com had millions of users too (a then large portion of internet users), but as the tech axiom goes, you can acquire infinite customers and generate infinite revenue if you sell dollars for 85 cents11. So despite the demand, the pattern may rhyme with the 1990s. Expect overbuilding. Expect some spectacular failures. But also expect the infrastructure to outlast the hype cycle and enable things we can’t yet imagine.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Predictably Unpredictable Future&lt;/head&gt;
    &lt;p&gt;So where does this leave us?&lt;/p&gt;
    &lt;p&gt;We’re early in the AI revolution. We’re at that metaphorical screeching modem phase of the internet era. Just as infrastructure companies poured billions into fiber optics, hyperscalers now pour billions into compute. Startups add “.ai” to their names like companies once added “.com” as they seek higher valuations. The hype will cycle through both euphoria and despair. Some predictions will look laughably wrong. Some that seem crazy will prove conservative.&lt;/p&gt;
    &lt;p&gt;Different industries will experience different outcomes. Unlike what the Jevons optimists suggest, demand for many things plateaus once human needs are met. Employment outcomes in any industry depend on the magnitude and growth of unmet market demand and whether that demand growth outpaces productivity improvements from automation.&lt;/p&gt;
    &lt;p&gt;Cost reduction will unlock market segments. Aswath Damodaran, a finance professor, (in)famously undervalued Uber assuming it would only capture a portion of the existing taxi market12. He missed that making rides dramatically cheaper would expand the market itself as people took Ubers to destinations they’d never have paid taxi prices to reach. AI will similarly enable products and services currently too expensive to build with human intelligence. A restaurant owner might use AI to create custom supply chain software that say at $100,000 with human developers would never have been built. A non-profit might deploy AI to contest a legal battle that was previously unaffordable.&lt;/p&gt;
    &lt;p&gt;We can predict change, but we can’t predict the details. No one in 1995 predicted we’d date strangers from the internet, ride in their ubers, or sleep in their airbnbs. Or that a job called influencers would become the most sought-after career among young people. Human creativity generates outcomes we can’t forecast with our current mental models. Expect new domains and industries to emerge. AI has already helped us decode more animal communication in the last five years than in the last fifty. Can we predict what jobs a technology that allows us to have full-blown conversations with them will unlock? A job that doesn’t exist today will likely be the most sought-after job in 2050. We can’t name it because it hasn’t been invented yet.&lt;/p&gt;
    &lt;p&gt;Job categories will transform. Even as the internet made some jobs obsolete, it also transformed others and created new categories. Expect the same with AI. Karpathy ends with a question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;About 6 months ago, I was also asked to vote if we will have less or more software engineers in 5 years. Exercise left for the reader.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To answer this question, go back to 1995 and ask the same question but with journalists. You might have predicted more journalists because the internet would create more demand by enabling you to reach the whole world. You’d be right for 10 or so years as employment in journalism grew until the early 2000s. But 30 years later, the number of newspapers and the number of journalists both have declined, even though more “journalism” happens than ever. Just not by people we call journalists. Bloggers, influencers, YouTubers, and newsletter writers do the work that traditional journalists used to do13.&lt;/p&gt;
    &lt;p&gt;The same pattern will play out with software engineers. We’ll see more people doing software engineering work and in a decade or so, what “software engineer” means will have transformed. Consider the restaurant owner from earlier who uses AI to create custom inventory software that is useful only for them. They won’t call themselves a software engineer.&lt;/p&gt;
    &lt;p&gt;So just like in 1995, if the AI optimists today say that within 25 years, we’d prefer news from AI over social media influencers, watch AI-generated characters in place of human actors, find romantic partners through AI matchmakers more than through dating apps (or perhaps use AI romantic partners itself), and flip “don’t trust AI” so completely that we’d rely on AI for life-or-death decisions and trust it to raise our children, most people would find that hard to believe. Even with all the intelligence, both natural and artificial, no one can predict with certainty what our AI future will look like. Not the tech CEOs, not the AI researchers, and certainly not some random guy pontificating on the internet. But whether we get the details right or not, our AI future is loading.&lt;/p&gt;
    &lt;p&gt;Approximately 2,879 websites were established before 1995, expanding to 23,500 by June 1995. See https://en.wikipedia.org/wiki/List_of_websites_founded_before_1995&lt;/p&gt;
    &lt;p&gt;Historical data from the Industrial Revolution shows that even as aggregate textile employment grew, workers shifted between job types within the industry as some roles became redundant. And correspondingly, some jobs saw wages collapse while others saw increases. For example, domestic hand-loom weavers were displaced by power looms and saw their wages collapse, while self-acting mule spinners (a newly created role) and factory workers saw stable employment and steady compensation growth. Additionally, Britain’s deflationary period (1815-1850) saw food prices fall by half, meaning real purchasing power often rose even when nominal wages declined. Despite all this, the psychological reality was harsh. Even with falling prices, watching your paycheck shrink while neighbors lost jobs and debts grew harder to pay (lower wages but fixed obligations) created real instability regardless of aggregate statistics improvements. Also see Acemoglu &amp;amp; Johnson, 2024.&lt;/p&gt;
    &lt;p&gt;Ask any tech product leader about their roadmap planning, and they’ll all universally report far more worthwhile projects than resources to build them, forcing ruthless prioritization to decide what gets built.&lt;/p&gt;
    &lt;p&gt;Hyperscalers are large cloud computing companies like Microsoft, Google, Meta, and Amazon that operate massive data centers and provide the computing infrastructure necessary to train and run AI models at scale.&lt;/p&gt;
    &lt;p&gt;Damodaran valued Uber at $6B, assuming a 10% market share of the then $100B taxi market. Uber’s market cap as of October 2025 is $190B.&lt;/p&gt;
    &lt;p&gt;Job transformation doesn’t guarantee comparable compensation. Much of this new “journalism” or content creation happens for free or at rates far below what traditional news organizations paid, separating the work from stable employment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wreflection.com/p/ai-dial-up-era"/><published>2025-11-03T21:01:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45805458</id><title>Guideline has been acquired by Gusto</title><updated>2025-11-04T13:47:55.570606+00:00</updated><content>&lt;doc fingerprint="2ec558179725a185"&gt;
  &lt;main&gt;
    &lt;p&gt;We have exciting news to share: Guideline has been acquired by Gusto, a leading provider of payroll, benefits, and HR solutions tailored to small businesses.&lt;/p&gt;
    &lt;p&gt;By joining forces, we can deliver payroll and 401(k) services that are more seamlessly integrated all in one place. This is the start of a new chapter that will make saving for retirement simpler and more accessible than ever.&lt;/p&gt;
    &lt;p&gt;The service you've come to know and expect from Guideline hasn’t changed. Everything works the same as it did previously, and we're committed to making sure your experience stays smooth and reliable.&lt;/p&gt;
    &lt;p&gt;We know you might have questions about the acquisition, so here are some answers on what to expect.&lt;/p&gt;
    &lt;head rend="h1"&gt;Frequently asked questions&lt;/head&gt;
    &lt;head rend="h2"&gt;General information and day-to-day operations&lt;/head&gt;
    &lt;p&gt;Do I need to take any action right now?&lt;/p&gt;
    &lt;p&gt;No. Your current 401(k) setup stays exactly the same. Participants do not need to take any action and all current 401(k) contributions will continue according to the selections within your account.&lt;/p&gt;
    &lt;p&gt;Plan sponsors who received notice about transitioning to a self-service plan can find additional details on administrative and fiduciary responsibilities here.&lt;/p&gt;
    &lt;p&gt;Why does this benefit me?&lt;/p&gt;
    &lt;p&gt;By Guideline joining Gusto, we're making payroll and 401(k) management more seamless than ever. For instance, you can now access your 401(k) account using your Gusto credentials via Single Sign-On (SSO).&lt;/p&gt;
    &lt;p&gt;Over time, we’ll bring new tools to make setting up and managing your 401(k) even easier.&lt;/p&gt;
    &lt;p&gt;Will I lose access to anything I use today (mobile app, statements, educational resources)?&lt;/p&gt;
    &lt;p&gt;Not at all. You can continue to use the mobile app, access statements, and visit our Help Center for educational resources just as you normally would. We’ll keep you informed about future enhancements and a more unified experience.&lt;/p&gt;
    &lt;p&gt;Will I still have access to historical documents?&lt;/p&gt;
    &lt;p&gt;Yes, you will continue to have access to all documentation within your dashboard.&lt;/p&gt;
    &lt;p&gt;Will anything change for my company?&lt;/p&gt;
    &lt;p&gt;The core features and day-to-day operations of your plan are expected to continue seamlessly. We are focused on a smooth transition with minimal disruption. If your company uses Gusto payroll, you will experience the benefits of deeper integration.&lt;/p&gt;
    &lt;p&gt;Plan sponsors who received notice about being transitioned to a self-service plan can find additional details on administrative and fiduciary responsibilities here.&lt;/p&gt;
    &lt;p&gt;Will my fees change?&lt;/p&gt;
    &lt;p&gt;No pricing changes are planned at this time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security and transactions&lt;/head&gt;
    &lt;p&gt;Is my retirement account still secure during this transition?&lt;/p&gt;
    &lt;p&gt;We take the security of your account and personal information extremely seriously. Protecting your assets and data is a responsibility to which we are fully committed, both during this transition and beyond. All of our rigorous security measures will continue to operate without interruption.&lt;/p&gt;
    &lt;p&gt;Will there be any changes to my investments or allocations?&lt;/p&gt;
    &lt;p&gt;No, there will be no changes to your investments at this time. Your holdings and asset allocation are unaffected by this transition, and your money remains invested according to your chosen strategy.&lt;/p&gt;
    &lt;p&gt;Will this delay my contributions, distributions, or rollovers?&lt;/p&gt;
    &lt;p&gt;Not at all. We are committed to processing all of your financial transactions in a timely manner, just as we always have. You can expect your transactions to be handled promptly without any new delay.&lt;/p&gt;
    &lt;head rend="h2"&gt;Support&lt;/head&gt;
    &lt;p&gt;Who do I reach out to if I have questions?&lt;/p&gt;
    &lt;p&gt;Guideline's support teams are not changing. For 401(k) plan questions, sponsors and administrators can continue directing questions to clients@guideline.com. Participants can reach out to support@guideline.com. For questions related to payroll, please contact your existing payroll support team.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://help.guideline.com/en/articles/12694322-guideline-has-joined-gusto-faqs-about-our-recent-acquisition"/><published>2025-11-03T22:54:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45805539</id><title>A friendly tour of process memory on Linux</title><updated>2025-11-04T13:47:55.360653+00:00</updated><content>&lt;doc fingerprint="8628597b90e6bdc5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;A Friendly Tour of Process Memory on Linux&lt;/head&gt;- 24 mins&lt;head rend="h1"&gt;A Friendly Tour of Process Memory on Linux&lt;/head&gt;&lt;p&gt;You run a program. It reads and writes addresses as if a giant, continuous slab of memory had been waiting there all along. It didn’t. Linux builds that illusion on the fly, one page at a time. This is a walk through what your process actually owns, what happens on the first touch of a byte, how protections and huge pages fit in, how to see the truth from &lt;code&gt;/proc&lt;/code&gt;, and why modern kernels do a little extra dance to defend against Meltdown.&lt;/p&gt;&lt;p&gt;Note: This tour targets Linux on x86‑64, other architectures differ in details (page sizes, cache rules), but the ideas carry over.&lt;/p&gt;&lt;head rend="h2"&gt;Intro&lt;/head&gt;&lt;p&gt;The picture below is a quick introduction. It is a simple map you can keep in mind as you read.&lt;/p&gt;&lt;p&gt;Physical RAM is the real memory. It is a bunch of frames scattered around. The virtual view is the clean line your program sees. It does not match the real layout. The page table is a list. It tells which spot on the virtual line points to which frame in RAM. Disk is extra space the system can use when RAM is full.&lt;/p&gt;&lt;p&gt;Here is how it plays out. When you read or write, the CPU looks in the page table. If there is an entry it goes to that frame. If there is no entry you get a page fault. The system then fills a frame and adds the entry, or it stops you with an error. We will explain faults later.&lt;/p&gt;&lt;p&gt;When RAM is tight the system makes room. It moves pages you have not used in a while to disk, or drops file pages it can load again. If you touch one of those later it brings it back.&lt;/p&gt;&lt;p&gt;Tiny explainers appear throughout so anyone can follow along, regardless of background.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;/proc&lt;/code&gt;&lt;code&gt;/proc&lt;/code&gt;is a virtual filesystem the kernel builds in memory. It exposes process and kernel state as files. You can read them with normal tools like&lt;code&gt;cat&lt;/code&gt;.&lt;/quote&gt;&lt;head rend="h2"&gt;The floor plan you never see&lt;/head&gt;&lt;p&gt;Inside the kernel, your process owns one object that represents its whole address space. Think of it as a floor plan. Each room on that plan is a virtual memory area (VMA) a contiguous range with the same permissions (read, write, execute) and the same kind of backing (anonymous memory or a file).&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: VMA&lt;/p&gt;&lt;lb/&gt;A VMA is a continuous virtual address range with one set of rights and one kind of backing.&lt;/quote&gt;&lt;p&gt;Under the plan sit the page tables that the hardware reads to translate your virtual addresses to real page frames.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: page tables and PTE&lt;/p&gt;&lt;lb/&gt;Page tables are lookup structures the CPU walks to translate addresses. A page table entry (PTE) maps one virtual page to one physical page and holds bits like present and writable.&lt;/quote&gt;&lt;p&gt;All threads in the process share the same plan. When the scheduler runs you, the CPU is pointed at your page tables, so pointer dereferences don’t need a syscall once a mapping exists, the hardware does the translation on its own.&lt;/p&gt;&lt;p&gt;You change the plan in three ways: &lt;code&gt;mmap&lt;/code&gt; draws a room, &lt;code&gt;mprotect&lt;/code&gt; changes the sign on its door (R/W/X), and &lt;code&gt;munmap&lt;/code&gt; tears it down.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;mmap&lt;/code&gt;&lt;code&gt;mmap&lt;/code&gt;reserves a virtual range with given permissions and a backing source.&lt;lb/&gt;Tiny explainer:&lt;code&gt;mprotect&lt;/code&gt;&lt;code&gt;mprotect&lt;/code&gt;changes the permissions on an existing range.&lt;lb/&gt;Tiny explainer:&lt;code&gt;munmap&lt;/code&gt;&lt;code&gt;munmap&lt;/code&gt;removes a mapping from the address space.&lt;/quote&gt;&lt;p&gt;Everything else (creating pages, reading file data, swapping) happens lazily when you touch memory.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: page&lt;/p&gt;&lt;lb/&gt;Hardware manages memory in fixed chunks called pages. On many x86‑64 machines a base page is 4 KiB. Bigger pages exist at 2 MiB and 1 GiB.&lt;/quote&gt;&lt;head rend="h2"&gt;A quick glance at your own house&lt;/head&gt;&lt;p&gt;Run:&lt;code&gt;cat /proc/self/maps | sed -n '1,80p'&lt;/code&gt;&lt;/p&gt;&lt;p&gt;You’ll see your main binary’s segments (code, data, bss), the heap, anonymous mappings (allocators use these for big chunks), shared libraries, and thread stacks near the top.&lt;/p&gt;&lt;p&gt;You’ll typically also see two small regions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;[vdso]&lt;/code&gt;: a tiny shared object the kernel maps in so a few calls like&lt;code&gt;gettimeofday&lt;/code&gt;can run without a kernel trap.&lt;/item&gt;&lt;item&gt;&lt;code&gt;[vvar]&lt;/code&gt;: read‑only data those helpers use.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;vdso&lt;/code&gt;and&lt;code&gt;vvar&lt;/code&gt;&lt;code&gt;vdso&lt;/code&gt;is code the kernel maps into your process to make some syscalls fast.&lt;code&gt;vvar&lt;/code&gt;holds data that code reads.&lt;/quote&gt;&lt;p&gt;They’re why asking the time is fast.&lt;/p&gt;&lt;head rend="h2"&gt;&lt;code&gt;mmap&lt;/code&gt;, without the fog&lt;/head&gt;&lt;p&gt;When you call &lt;code&gt;mmap&lt;/code&gt;, you’re not “allocating memory” so much as drawing a promise on the floor plan. You say give me a range of addresses with these rights and back it by this file plus offset or by anonymous memory. Linux picks an address, makes sure it doesn’t collide, adjusts VMAs so each remains uniform, and records the promise.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: ASLR&lt;/p&gt;&lt;lb/&gt;Address Space Layout Randomization places mappings at randomized locations to make exploits harder.&lt;lb/&gt;Tiny explainer: anonymous vs file mapping&lt;lb/&gt;Anonymous memory is not tied to a file and starts as zeros. File mappings mirror file contents.&lt;/quote&gt;&lt;p&gt;It does not allocate pages yet. That comes later at first touch.&lt;/p&gt;&lt;p&gt;Two gotchas come up over and over:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;File mappings: &lt;code&gt;offset&lt;/code&gt;must be page aligned or&lt;code&gt;mmap&lt;/code&gt;returns&lt;code&gt;EINVAL&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;Mapping past end of file is allowed, but touching beyond the true end raises &lt;code&gt;SIGBUS&lt;/code&gt;. The VMA exists, the data does not.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;MAP_PRIVATE&lt;/code&gt;and&lt;code&gt;MAP_SHARED&lt;/code&gt;&lt;code&gt;MAP_SHARED&lt;/code&gt;means writes go back to the file and are visible to others that share it.&lt;code&gt;MAP_PRIVATE&lt;/code&gt;means you see the file but writes go to private copy‑on‑write pages.&lt;/quote&gt;&lt;p&gt;Anonymous mappings start life as zeroes. File mappings mirror the file. If the file ends mid page the tail of that last page reads as zeros but still belongs to the file.&lt;/p&gt;&lt;p&gt;&lt;code&gt;MAP_FIXED&lt;/code&gt; means exactly here and it overwrites anything already mapped at that address. Prefer &lt;code&gt;MAP_FIXED_NOREPLACE&lt;/code&gt; to fail instead of clobbering. Without either flag your &lt;code&gt;addr&lt;/code&gt; is just a hint.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;MAP_FIXED_NOREPLACE&lt;/code&gt;&lt;lb/&gt;Ask for an exact address and fail if something is already there. Safer than overwriting.&lt;/quote&gt;&lt;head rend="h2"&gt;The first touch&lt;/head&gt;&lt;p&gt;Imagine &lt;code&gt;*p = 42;&lt;/code&gt; to a fresh mapping. The CPU tries to translate the address. It finds no entry so it raises a page fault that includes the address and an error code.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: page fault&lt;/p&gt;&lt;lb/&gt;A page fault is the CPU asking the kernel to handle a missing or illegal translation for an address.&lt;/quote&gt;&lt;p&gt;The kernel’s handler runs on your behalf and asks three questions in this order:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Is the address inside any VMA&lt;lb/&gt;If not you are poking a hole in the plan →&lt;code&gt;SIGSEGV&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;Do the rights allow this access&lt;lb/&gt;Write to a read only page or execute from non exec →&lt;code&gt;SIGSEGV&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;If it is valid but missing make it real&lt;lb/&gt;For an anonymous mapping the kernel allocates a zero filled physical page, wires a page table entry with your requested permissions, and returns to your instruction. For a file mapping it first checks the page cache. If the data is not in RAM it reads from storage, then installs the translation and retries your instruction. Your store lands. You keep going.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: page cache&lt;/p&gt;&lt;lb/&gt;The page cache is the kernel’s cache of file data in RAM. File mappings read and write through it.&lt;lb/&gt;Tiny explainer: zero page&lt;lb/&gt;Some reads from fresh anonymous memory can be satisfied by a shared read only page of zeros. A private page is created on the first write.&lt;/quote&gt;&lt;p&gt;People count these faults:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A minor fault means the data was already in RAM and only the translation was missing.&lt;/item&gt;&lt;item&gt;A major fault means the kernel had to wait for I/O which is expensive.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: stack guard&lt;/p&gt;&lt;lb/&gt;User stacks have a guard page. Touching just below the current stack can grow it. Touching far below looks like a bug and gets a&lt;code&gt;SIGSEGV&lt;/code&gt;.&lt;/quote&gt;&lt;p&gt;This same lazy first touch explains how memory is shared after &lt;code&gt;fork()&lt;/code&gt; and how &lt;code&gt;MAP_PRIVATE&lt;/code&gt; works. The next section shows that path.&lt;/p&gt;&lt;head rend="h2"&gt;Copy on write with &lt;code&gt;fork()&lt;/code&gt; and &lt;code&gt;MAP_PRIVATE&lt;/code&gt;&lt;/head&gt;&lt;p&gt;Why this is here. We just talked about first touch. The same rule explains why pages do not copy on &lt;code&gt;fork&lt;/code&gt; and why &lt;code&gt;MAP_PRIVATE&lt;/code&gt; does not change the file.&lt;/p&gt;&lt;p&gt;&lt;code&gt;fork&lt;/code&gt; does not duplicate pages. The child points at the same physical pages as the parent. The kernel flips those pages to read only for both. The first write hits a copy on write fault. The kernel allocates a new page, copies the bytes, updates the writer’s page table entry to the new page with write permission, and returns. Reads still share the original page. That is why RSS stays flat after &lt;code&gt;fork&lt;/code&gt; until you write.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: RSS&lt;/p&gt;&lt;lb/&gt;Resident Set Size is how many pages of this process are currently in RAM. Tiny explainer: copy on write&lt;lb/&gt;Share the same page for reads. Make a private copy only when a write happens.&lt;/quote&gt;&lt;p&gt;&lt;code&gt;MAP_PRIVATE&lt;/code&gt; uses the same idea. You read file data through the page cache. When you write, the kernel gives you a private page. The file stays unchanged.&lt;/p&gt;&lt;p&gt;Things you will also run into:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;fork&lt;/code&gt;then&lt;code&gt;execve&lt;/code&gt;. The child replaces its whole address space soon after. That avoids most CoW work.&lt;/item&gt;&lt;item&gt;&lt;code&gt;vfork&lt;/code&gt;. The child runs in the parent’s address space until it calls&lt;code&gt;exec&lt;/code&gt;or&lt;code&gt;_exit&lt;/code&gt;. The parent waits. Do not touch memory in the child.&lt;/item&gt;&lt;item&gt;&lt;code&gt;clone&lt;/code&gt;with&lt;code&gt;CLONE_VM&lt;/code&gt;. This makes a thread. One address space. No copy.&lt;/item&gt;&lt;item&gt;&lt;code&gt;MAP_SHARED&lt;/code&gt;. Writes go to the shared page and to the file or shmem. No CoW.&lt;/item&gt;&lt;item&gt;&lt;code&gt;MADV_DONTFORK&lt;/code&gt;. Leave this mapping out of the child.&lt;/item&gt;&lt;item&gt;&lt;code&gt;MADV_WIPEONFORK&lt;/code&gt;. The child sees zeros for this mapping.&lt;/item&gt;&lt;item&gt;Transparent huge pages. Breaking CoW on a huge page may split it first. Small extra cost.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Changing rights, and the little pause you feel&lt;/head&gt;&lt;p&gt;Why you care. JITs and loaders flip a region from writable to executable after codegen which is W^X. That flip is not free.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: W^X&lt;/p&gt;&lt;lb/&gt;Write xor Execute is a policy. A page is never writable and executable at the same time.&lt;/quote&gt;&lt;p&gt;&lt;code&gt;mprotect(addr, len, prot)&lt;/code&gt; changes permissions. Internally the kernel may split VMAs so each remains uniform, edits the page table entries for the range, and then does one more necessary thing. It invalidates old translations from the CPU’s small cache of address translations which is the TLB. That invalidation is the small pause you sometimes feel when a JIT flips RW to RX or back.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: TLB&lt;/p&gt;&lt;lb/&gt;The Translation Lookaside Buffer caches recent translations so the CPU does not walk page tables every time.&lt;/quote&gt;&lt;p&gt;Most systems enforce W^X. A page should not be writable and executable at the same time. JITs keep to that by flipping after codegen or by keeping two virtual mappings of the same memory so no single mapping is both.&lt;/p&gt;&lt;p&gt;Remember there are two layers of permission checks:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Filesystem or mount policy like &lt;code&gt;noexec&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Page permissions like &lt;code&gt;PROT_EXEC&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Either layer can block execution.&lt;/p&gt;&lt;head rend="h2"&gt;Seeing what’s really mapped&lt;/head&gt;&lt;p&gt;For everyday questions the friendly view is enough.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;/proc/&amp;lt;pid&amp;gt;/maps&lt;/code&gt;shapes: addresses, rights, file names&lt;/item&gt;&lt;item&gt;&lt;code&gt;/proc/&amp;lt;pid&amp;gt;/smaps&lt;/code&gt;and&lt;code&gt;smaps_rollup&lt;/code&gt;add per region accounting like how much is resident which is RSS, private vs shared, and whether huge pages were used like&lt;code&gt;AnonHugePages&lt;/code&gt;and&lt;code&gt;FilePmdMapped&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;When you need truth at the per page level Linux exposes sharper tools.&lt;/p&gt;&lt;p&gt;&lt;code&gt;/proc/&amp;lt;pid&amp;gt;/pagemap&lt;/code&gt; has one 64 bit entry per virtual page. It tells you whether a page is present, swapped, soft dirty, exclusively mapped with caveats for huge pages, whether it is write protected via userfaultfd, or part of a guard region. It can also reveal the page frame number which is PFN but modern kernels hide PFNs from unprivileged users. You need the right capability or root.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: PFN&lt;/p&gt;&lt;lb/&gt;Page Frame Number is the physical page index used inside the kernel. Tiny explainer: userfaultfd&lt;lb/&gt;A file descriptor that lets a userspace thread handle faults and write protect events for a range.&lt;/quote&gt;&lt;p&gt;&lt;code&gt;/proc/kpagecount&lt;/code&gt; is indexed by PFN and tells you how many mappings point at a given physical page.&lt;/p&gt;&lt;p&gt;&lt;code&gt;/proc/kpageflags&lt;/code&gt; is also indexed by PFN and tells you what kind of page it is and what is happening to it like anonymous or file backed, part of a transparent huge page, in the LRU, dirty, under writeback, a page table page, or the shared zero page.&lt;/p&gt;&lt;head rend="h3"&gt;Common wrinkles&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Sparse files. To tell hole vs data, combine &lt;code&gt;mincore()&lt;/code&gt;which says resident or not with&lt;code&gt;lseek(..., SEEK_DATA/SEEK_HOLE)&lt;/code&gt;on the backing file.&lt;/item&gt;&lt;item&gt;Shared memory and swap. Shared and shmem pages may be non present at the PTE level while still logically allocated. Expect swap entries and non present PTEs.&lt;/item&gt;&lt;item&gt;Privileges. Modern kernels restrict PFN and some flag visibility to privileged users for security.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;mincore&lt;/code&gt;&lt;code&gt;mincore&lt;/code&gt;tells you which pages of a mapping are in RAM.&lt;lb/&gt;Tiny explainer:&lt;code&gt;SEEK_DATA&lt;/code&gt;and&lt;code&gt;SEEK_HOLE&lt;/code&gt;&lt;lb/&gt;File offsets that let you skip to the next data chunk or the next hole in a sparse file.&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: soft dirty vs written&lt;/p&gt;&lt;lb/&gt;Soft dirty marks pages dirtied by userland but it can get lost across swaps or VMA merges. Newer kernels offer an ioctl named&lt;code&gt;PAGEMAP_SCAN&lt;/code&gt;that scans a range for pages written since last write protect and can in the same step write protect them again. It pairs with userfaultfd write protect to give fast and race free userspace dirty tracking for snapshotting and live migration.&lt;/quote&gt;&lt;head rend="h2"&gt;When your page suddenly gets bigger&lt;/head&gt;&lt;p&gt;Your CPU would rather cover more ground with fewer entries in its TLB. Linux can help by backing hot memory with bigger pages.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: THP&lt;/p&gt;&lt;lb/&gt;Transparent Huge Pages automatically try to use larger pages for performance when safe.&lt;/quote&gt;&lt;p&gt;Transparent Huge Pages do this automatically for anonymous memory and shmem or tmpfs. A fault can be satisfied with a 2 MiB page instead of 512 small ones. A background thread named khugepaged can also collapse adjacent base pages into a huge page when it is safe.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;khugepaged&lt;/code&gt;&lt;lb/&gt;A kernel thread that scans and merges adjacent small pages into huge pages when conditions are right.&lt;/quote&gt;&lt;p&gt;Modern kernels add multi size THP which is mTHP on some architectures. Groups of base pages like 16 KiB or 64 KiB reduce fault count and TLB pressure without always jumping to 2 MiB. They are still PTE mapped but behave as larger folios inside the VM.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: mTHP&lt;/p&gt;&lt;lb/&gt;Multi size THP allows variable order large folios so you get some of the benefit without a full 2 MiB page.&lt;/quote&gt;&lt;p&gt;You can ask for THP in a region with &lt;code&gt;madvise(..., MADV_HUGEPAGE)&lt;/code&gt; or opt out with &lt;code&gt;MADV_NOHUGEPAGE&lt;/code&gt;. System wide behavior lives under &lt;code&gt;/sys/kernel/mm/transparent_hugepage/&lt;/code&gt; with per size controls. &lt;code&gt;enabled&lt;/code&gt; can be &lt;code&gt;always&lt;/code&gt;, &lt;code&gt;madvise&lt;/code&gt;, &lt;code&gt;never&lt;/code&gt;, or &lt;code&gt;inherit&lt;/code&gt;. Shmem or tmpfs have their own knobs like a &lt;code&gt;huge=&lt;/code&gt; mount option with &lt;code&gt;always&lt;/code&gt;, &lt;code&gt;advise&lt;/code&gt;, &lt;code&gt;within_size&lt;/code&gt;, &lt;code&gt;never&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;How to tell if it worked. In &lt;code&gt;/proc/self/smaps&lt;/code&gt; the lines for a region include &lt;code&gt;AnonHugePages&lt;/code&gt; for anonymous THP and &lt;code&gt;FilePmdMapped&lt;/code&gt; for file or shmem huge mappings. System wide &lt;code&gt;/proc/meminfo&lt;/code&gt; has &lt;code&gt;AnonHugePages&lt;/code&gt;, &lt;code&gt;ShmemPmdMapped&lt;/code&gt;, and &lt;code&gt;ShmemHugePages&lt;/code&gt;. &lt;code&gt;/proc/vmstat&lt;/code&gt; keeps a diary of THP events allocated on fault, fell back, split, swapped as a whole, and so on.&lt;/p&gt;&lt;p&gt;Controls plain map:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Top level: &lt;code&gt;/sys/kernel/mm/transparent_hugepage/enabled&lt;/code&gt;which is&lt;code&gt;always&lt;/code&gt;or&lt;code&gt;madvise&lt;/code&gt;or&lt;code&gt;never&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Defrag effort: &lt;code&gt;/sys/kernel/mm/transparent_hugepage/defrag&lt;/code&gt;tunes how hard the kernel tries on the fault path vs deferring to khugepaged&lt;/item&gt;&lt;item&gt;Shmem or tmpfs: &lt;code&gt;huge=always|within_size|advise|never&lt;/code&gt;plus shmem specific knobs&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Modern kernels may also create variable order large folios that are bigger than 4 KiB but PTE mapped not full 2 MiB PMD. This reduces fault count and TLB pressure without always jumping to 2 MiB. Behavior differs by kernel and architecture.&lt;/p&gt;&lt;p&gt;One trade off. Assembling a huge page may require compaction which moves other pages to free a contiguous chunk and this can add a small pause. If first touch latency matters more than steady state speed the defrag knob lets you temper how hard the kernel tries which pushes work to khugepaged instead of doing it inline.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: THP vs hugetlbfs&lt;/p&gt;&lt;lb/&gt;THP is automatic and pageable. Explicit huge pages from&lt;code&gt;MAP_HUGETLB&lt;/code&gt;or hugetlbfs are quota managed and non swap.&lt;/quote&gt;&lt;head rend="h2"&gt;Dirty‑tracking in userspace, without racing the kernel&lt;/head&gt;&lt;p&gt;Imagine you want to copy only the pages an application modified since your last snapshot.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Give yourself the ability to catch write protect faults with userfaultfd in write protect mode.&lt;/item&gt;&lt;item&gt;Use &lt;code&gt;PAGEMAP_SCAN&lt;/code&gt;over your range with the category written since last write protect. Ask the kernel to write protect matching pages and to return compact ranges of what it found.&lt;/item&gt;&lt;item&gt;Copy those ranges. When the app later writes to one of them userfaultfd wakes your thread. Log the dirtied page, clear write protect, and let it proceed.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This avoids walking every PTE and avoids the classic race where a page is dirtied while you were looking. It is also fast because scan plus write protect happens as one atomic operation inside the kernel.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;PAGEMAP_SCAN&lt;/code&gt;&lt;lb/&gt;An ioctl that scans a virtual range for pages with properties like written since last protect and can also apply write protect in the same step.&lt;/quote&gt;&lt;head rend="h2"&gt;The TLB, and why &lt;code&gt;mprotect&lt;/code&gt; costs a little&lt;/head&gt;&lt;p&gt;The Translation Lookaside Buffer remembers recent translations so the CPU does not walk page tables on every access. If Linux changes a mapping or its permissions it must make sure stale entries are not used.&lt;/p&gt;&lt;p&gt;On x86 there are two broad ways to do it.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Precise invalidation. Invalidate one page at a time with &lt;code&gt;INVLPG&lt;/code&gt;. Good for small changes. A single invalidation on a huge page mapping drops the whole 2 MiB entry.&lt;/item&gt;&lt;item&gt;Broader flushes. Drop many or all entries for example by reloading the page table root register. Fewer instructions now and more misses later while refilling.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Which is better depends on how big a change you made, whether you are changing small or huge pages, and the microarchitecture.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: PCID&lt;/p&gt;&lt;lb/&gt;Process Context Identifiers tag TLB entries so switching page tables does not flush everything.&lt;lb/&gt;Tiny explainer: INVPCID&lt;lb/&gt;Allows targeted invalidation of TLB entries for a given tag without switching to it.&lt;/quote&gt;&lt;p&gt;There is also a debug knob on some x86 builds named &lt;code&gt;tlb_single_page_flush_ceiling&lt;/code&gt; that nudges when the kernel switches from per page invalidations to a broad flush.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;INVLPG&lt;/code&gt;&lt;lb/&gt;A privileged instruction that invalidates TLB entries for the page containing a given address in the current address space tag.&lt;/quote&gt;&lt;head rend="h2"&gt;Meltdown, and why the kernel sometimes switches maps on entry&lt;/head&gt;&lt;p&gt;Early 2018 brought Meltdown. Speculative execution plus a cache side channel could leak data across the user and kernel boundary. Even if a user mode load from a kernel address would fault, the CPU might speculatively execute it and touch data that leaves a measurable cache trace.&lt;/p&gt;&lt;p&gt;Linux’s defense on x86‑64 is Page Table Isolation which is PTI. Keep two views and switch between them on entry and exit.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: CR3&lt;/p&gt;&lt;lb/&gt;CR3 holds the current page table root and on x86 switching it changes the active address space.&lt;lb/&gt;Tiny explainer: PTI&lt;lb/&gt;PTI keeps a reduced userspace view without normal kernel data mapped and a full kernel view used while in the kernel.&lt;/quote&gt;&lt;p&gt;Cost. More page table switches, different TLB sharing behavior, and a small memory bump for extra top level tables and the per CPU entry area. With PCID Linux keeps separate TLB tags for the two views to reduce flushes. Some systems allow opting out with &lt;code&gt;nopti&lt;/code&gt; when acceptable. Default is on.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: what Meltdown reads&lt;/p&gt;&lt;lb/&gt;Permissions never turn off. The architectural access still faults. The leak is in transient speculation which leaves a timing trace.&lt;/quote&gt;&lt;head rend="h2"&gt;How the kernel changes mappings safely&lt;/head&gt;&lt;p&gt;When Linux edits page tables the order is deliberate.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Handle cache rules first on architectures that need it.&lt;/item&gt;&lt;item&gt;Modify page tables by adding, removing, or changing PTEs.&lt;/item&gt;&lt;item&gt;Invalidate the TLB so the CPU forgets stale translations.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Under the hood are functions that match the granularity of the change like flush an address space, flush a range, or flush a single page.&lt;/p&gt;&lt;p&gt;There is a parallel story for kernel only mappings made with &lt;code&gt;vmap&lt;/code&gt; and &lt;code&gt;vmalloc&lt;/code&gt;. Before I/O the kernel flushes the vmap range so the physical page sees the latest bytes. After I/O it invalidates the vmap range so speculative reads do not go stale.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer:&lt;/p&gt;&lt;code&gt;vmap&lt;/code&gt;and&lt;code&gt;vmalloc&lt;/code&gt;&lt;lb/&gt;APIs that create kernel virtual mappings to non contiguous physical pages for use inside the kernel.&lt;/quote&gt;&lt;p&gt;On x86 you rarely think about the instruction cache because it is coherent with data stores. On others, copying code into executable memory requires an explicit instruction cache flush before running it. The VM has hooks like &lt;code&gt;copy_to_user_page&lt;/code&gt; and &lt;code&gt;flush_icache_range&lt;/code&gt; where architectures do this housekeeping.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: icache flush&lt;/p&gt;&lt;lb/&gt;Some CPUs need an instruction cache sync after writing new code so execution sees the new bytes.&lt;/quote&gt;&lt;head rend="h2"&gt;A tiny x86 aside: stacks and calls, without the haze&lt;/head&gt;&lt;p&gt;In 64 bit mode registers wear an R. &lt;code&gt;RIP&lt;/code&gt; is the instruction pointer, &lt;code&gt;RSP&lt;/code&gt; is the stack, &lt;code&gt;RBP&lt;/code&gt; is the frame. The stack grows down. &lt;code&gt;push&lt;/code&gt; decrements &lt;code&gt;RSP&lt;/code&gt; and stores. &lt;code&gt;pop&lt;/code&gt; loads then increments. &lt;code&gt;CALL&lt;/code&gt; pushes the return address and jumps. &lt;code&gt;RET&lt;/code&gt; pops it into &lt;code&gt;RIP&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;On Linux the System V AMD64 ABI passes the first arguments in registers which are &lt;code&gt;RDI&lt;/code&gt;, &lt;code&gt;RSI&lt;/code&gt;, &lt;code&gt;RDX&lt;/code&gt;, &lt;code&gt;RCX&lt;/code&gt;, &lt;code&gt;R8&lt;/code&gt;, &lt;code&gt;R9&lt;/code&gt; and returns values in &lt;code&gt;RAX&lt;/code&gt;. Large objects go by pointer. Your stack must be readable and writable.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: System V AMD64 ABI&lt;/p&gt;&lt;lb/&gt;The calling convention for 64 bit Unix like systems on x86‑64 that defines where arguments and return values go.&lt;/quote&gt;&lt;p&gt;User code runs in ring 3. The kernel runs in ring 0. Crossings like syscalls, interrupts, and exceptions go through CPU defined gates. In 64 bit mode Linux uses a flat segmentation model and relies on paging for isolation.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer: rings&lt;/p&gt;&lt;lb/&gt;Rings are CPU privilege levels. Ring 3 is user mode. Ring 0 is kernel mode.&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;Tiny note for ARM64 readers&lt;/p&gt;&lt;lb/&gt;The ideas like stack growth and user vs kernel separation are similar. Register names, calling conventions, and syscall entry differ.&lt;/quote&gt;&lt;head rend="h2"&gt;When things go sideways (and what that usually means)&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;mmap&lt;/code&gt;→&lt;code&gt;EINVAL&lt;/code&gt;often a misaligned file&lt;code&gt;offset&lt;/code&gt;which must be page aligned or an impossible flag combo&lt;/item&gt;&lt;item&gt;&lt;code&gt;mmap&lt;/code&gt;→&lt;code&gt;ENOMEM&lt;/code&gt;you may be out of virtual space or VMA count or you hit strict overcommit&lt;/item&gt;&lt;item&gt;Store to a file mapping → &lt;code&gt;SIGBUS&lt;/code&gt;you walked past EOF. The VMA existed, the data did not&lt;/item&gt;&lt;item&gt;&lt;code&gt;mprotect(PROT_EXEC)&lt;/code&gt;→&lt;code&gt;EACCES&lt;/code&gt;could be a&lt;code&gt;noexec&lt;/code&gt;mount or a W^X policy&lt;/item&gt;&lt;item&gt;Big &lt;code&gt;malloc&lt;/code&gt;creates a new line in&lt;code&gt;maps&lt;/code&gt;your allocator used&lt;code&gt;mmap&lt;/code&gt;for that size&lt;/item&gt;&lt;item&gt;RSS balloons after &lt;code&gt;fork()&lt;/code&gt;copy on write did its job and you wrote to lots of shared pages&lt;/item&gt;&lt;item&gt;Accidentally clobbered a mapping you probably used &lt;code&gt;MAP_FIXED&lt;/code&gt;. Prefer&lt;code&gt;MAP_FIXED_NOREPLACE&lt;/code&gt;to fail instead of overwrite&lt;/item&gt;&lt;/list&gt;&lt;p&gt;When it is mysterious, look. Start friendly with &lt;code&gt;smaps_rollup&lt;/code&gt; for the big picture and &lt;code&gt;maps&lt;/code&gt; for shapes. Drop to &lt;code&gt;pagemap&lt;/code&gt; and the &lt;code&gt;kpage*&lt;/code&gt; files only when you truly need per page truth and expect to need privileges.&lt;/p&gt;&lt;head rend="h2"&gt;A small checklist to keep nearby&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Need memory now. &lt;code&gt;mmap&lt;/code&gt;anonymous with&lt;code&gt;PROT_READ|PROT_WRITE&lt;/code&gt;and&lt;code&gt;MAP_PRIVATE|MAP_ANONYMOUS&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Generating code. Keep W^X. Write bytes then &lt;code&gt;mprotect(PROT_READ|PROT_EXEC)&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Mapping a file. &lt;code&gt;offset&lt;/code&gt;must be page aligned. Touching beyond real EOF is&lt;code&gt;SIGBUS&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Lots of major faults. Nudge the kernel with &lt;code&gt;MADV_WILLNEED&lt;/code&gt;or touch earlier. Watch page cache and storage&lt;/item&gt;&lt;item&gt;Where did memory go. Start with &lt;code&gt;/proc/&amp;lt;pid&amp;gt;/smaps_rollup&lt;/code&gt;then&lt;code&gt;/proc/&amp;lt;pid&amp;gt;/maps&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Forking big processes. Expect CoW. RSS grows as you write. Consider &lt;code&gt;exec&lt;/code&gt;in the child for heavy work&lt;/item&gt;&lt;item&gt;Latency sensitive. Consider THP or mTHP where it helps. &lt;code&gt;mlock&lt;/code&gt;hot sets. Watch your TLB behavior&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Feedback is extremely welcomed! You can reach out to me on X @0xkato&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.0xkato.xyz/linux-process-memory/"/><published>2025-11-03T23:04:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45805900</id><title>Things you can do with diodes</title><updated>2025-11-04T13:47:55.249665+00:00</updated><content/><link href="https://lcamtuf.substack.com/p/things-you-can-do-with-diodes"/><published>2025-11-03T23:49:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45806263</id><title>You can't cURL a Border</title><updated>2025-11-04T13:47:54.932794+00:00</updated><content>&lt;doc fingerprint="7f44c81abfa3ce57"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You can't cURL a Border&lt;/head&gt;
    &lt;p&gt;An error fare to Iceland pops up. It's cheap enough to feel like a typo and most likely will be gone in minutes. I'm moderately obsessed with ways to travel on budget, so I keep an eye on these.&lt;lb/&gt; Before I click Buy, I need to know (fast!) if it actually works for me: would I need a visa, are there any odd passport requirements, can I quickly sort out the driving permit, would it affect my Schengen 90/180 window, break UK presence tests or accidentally prevent a tax residency I am chasing.&lt;/p&gt;
    &lt;p&gt;It isn’t one check, it’s a stack of small unfriendly ones, and takes around 20 minutes to process. Some bits are fun, like hunting for a seat upgrade, but mostly it’s counting midnights and expiry dates so a cheap weekend doesn’t become an expensive lesson.&lt;/p&gt;
    &lt;p&gt;I've been doing this dance for a decade now. In 2015 I made a spreadsheet for a US visa application that wanted ten years of travel history, down to the day. The spreadsheet grew: UK work visa, Indefinite Leave to Remain and citizenship applications, Canadian work permits. Any government form that asked "where have you been?" got its answer from the same battered CSV. It worked well enough, in the sense that I was never detained.&lt;/p&gt;
    &lt;p&gt;It also made me think that this was a solvable problem I was solving badly. I built a ledger to answer “where was I on 15 March 2023?” Instead, I ran simulations to check, “if I book this, what breaks later?”&lt;/p&gt;
    &lt;p&gt;The only question is whether the computer can answer all of faster than I do, and leave December, the next border control, and the end of the tax year blissfully uneventful.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does this trip compile? ¶&lt;/head&gt;
    &lt;p&gt;That twenty-minute panic before buying a flight comes from one basic problem: none of the systems that judge you will tell you your state.&lt;/p&gt;
    &lt;p&gt;Schengen is running one check. The UK is running another. Tax residency is running a third. Your passport is running its own quiet clock in the background. None of them explain themselves, and none of them agree on what “a day” even is.&lt;/p&gt;
    &lt;p&gt;Schengen cares about presence across rolling windows. The UK counts how many midnights you were physically in the country in a tax year that, for historical reasons [1], starts on 6 April out of all options. Some countries track how many days you’ve spent in certain places and change the medical paperwork they expect from you once you cross a threshold. Meanwhile your passport might fail you with its expiry date, validity rules that may apply on arrival or departure depending on routing, and a finite number of blank facing pages that some countries require.&lt;/p&gt;
    &lt;p&gt;None of that is easily exposed. The officer at the desk can see it but you can’t. That's parsing the State—both kinds. The government's view of you, and the state machine that tracks it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The bureaucratic edge cases ¶&lt;/head&gt;
    &lt;p&gt;The rules aren't just complex—they're occasionally specific in ways that make you regret leaving the house in the first place.&lt;/p&gt;
    &lt;p&gt;To apply for British citizenship, you need to prove you were physically in the UK on your application date but five years ago. Not approximately five years, not that week—that exact day when you press "submit" on the form minus five years. Miss it by 24 hours and your application is reject after months of waiting, and you have to pay a hefty fee to re-apply.&lt;/p&gt;
    &lt;p&gt;Transiting through a UK airport? Leaving the terminal doesn't count as presence unless you do something "unrelated to your travel"—buy a sausage roll at Greggs, see a play in West End, meet a friend. The guidance doesn't even specify a minimum spend.&lt;/p&gt;
    &lt;p&gt;Morocco runs on UTC+1 most of the year but switches to UTC during Ramadan to shorten the fasting day. Which means "days spent in Morocco" depends on your timezone database version and whether you remembered to update it.&lt;/p&gt;
    &lt;p&gt;It would be alright with a single source of truth, but all these facts are scattered across (semi)official websites and PDFs, and you're supposed to figure it out yourself.&lt;/p&gt;
    &lt;p&gt;So the job isn't "log trips" (I already did that for ten years in a spreadsheet). The job is: given what I've already done and what I'm about to do, does this plan quietly break anything, and if so, where, and by how much.&lt;/p&gt;
    &lt;p&gt;"You're at 56 days because Amsterdam contributed 12, Prague 3, Barcelona 10, Iceland would add a month, and February doesn't count anymore" is something I can trust, argue with, or fix. "You're fine" isn't.&lt;/p&gt;
    &lt;p&gt;That's where this stops being a spreadsheet and starts being a linter. Apparently I want the compiler warning before I press Buy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Counting the right midnight ¶&lt;/head&gt;
    &lt;p&gt;If I want a compiler warning I trust, the compiler has to agree with the officer about what a day is.&lt;/p&gt;
    &lt;p&gt;For the past five years I've been working on an app for people with epilepsy, managing timezones, medication reminders, and edge cases. We juggled multiple sources of truth and multiple storage styles (some records in UTC, some in local time with timezones stored separately—historical reasons, obviously).&lt;/p&gt;
    &lt;p&gt;This time, I tried to learn from that: facts are stored as instants, reasoning happens in local days of the jurisdiction that cares.&lt;/p&gt;
    &lt;p&gt;Take this routing: depart Dublin morning of November the 17th, brief Newark layover, a longer one in Mexico City, 23-hour Heathrow stop, then Tenerife. Ask five immigration systems "how many tax residency days?" and you get five answers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ireland: zero (under 30 days/year threshold).&lt;/item&gt;
      &lt;item&gt;US: zero (foreign-to-foreign transit under 24 hours).&lt;/item&gt;
      &lt;item&gt;Mexico: two (you cross midnight twice).&lt;/item&gt;
      &lt;item&gt;UK: zero (even though you cross midnight once), unless you went landside for non-travel reasons, then one.&lt;/item&gt;
      &lt;item&gt;Schengen: one (entry day counts, exit day will count too, even if both are only for 15 minutes).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each stop has same or similar conditions, but different state machines are asking different questions. I pin the timezone database version that produced each result, and when rules or clocks shift, I recompute so I could show both answers if needed. Yesterday should stay reproducible even when tomorrow disagrees.[2]&lt;/p&gt;
    &lt;head rend="h2"&gt;Parsing the State ¶&lt;/head&gt;
    &lt;p&gt;In other words, the linter is meant to answer the same question in various disguises: "what happens if I do this?"&lt;/p&gt;
    &lt;p&gt;Can I book Christmas in the Alps with three summer weekends planned in Europe? Does it matter if I leave UK before the tax year ends? What passport should I travel on? Does anything expire between booking and boarding?&lt;/p&gt;
    &lt;p&gt;Every question has the same shape: simulate forward, find what breaks, decide if you care. The goal isn't to convince border officers—it's to not make mistakes they'd catch. Trips get assembled from whatever I can verify later: geotagged photos, background location, manual entries. A resolver turns that into "present on this local day" and keeps track of why.&lt;/p&gt;
    &lt;p&gt;I don't hardcode rules, I ship interpretations instead: each jurisdiction has a small versioned blob that says what counts, how the window is measured, where that reading came from.&lt;/p&gt;
    &lt;p&gt;The paperwork gets the same treatment because documents are state machines too. A passport isn’t just means of identity; it has constraints and timers. Some requirements, like six months validity are legacy and usually exist to keep deportations possible without issuing emergency documents, but still need to be checked. [3]&lt;/p&gt;
    &lt;p&gt;Before I buy anything the linter should tell me that I don't have the correct flavour of IDP (and man, getting those in Scotland since they delegated it from Post Offices to corner shops is tough), that a Dubai connection flips a “valid on arrival” buffer into “invalid on departure”. Quiet warnings, early enough to change dates or renew the right booklet, and clear enough that I won't have to improvise at a counter.&lt;/p&gt;
    &lt;p&gt;If the world changes—new examples, revised guidance, a delayed system finally launches—I don’t rewrite history. I version the assumption, keep both answers recorded, and move on.&lt;/p&gt;
    &lt;p&gt;Keeping all those rules up-to-date is hard, so rather than maintaining rules for every country, I parse a few databases, then let users configure their own tracking goals. A user emailed about Cyprus's fast-tracked tax residency scheme; another pointed out I'd missed a few countries entirely. The app gets better as people use it, which feels more honest than pretending to be a global authority on 195 countries' immigration rules.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making things public ¶&lt;/head&gt;
    &lt;p&gt;The app is local by default. Calculations happen on-device, and if you're in airplane mode, it still works. Network is always the bottleneck, and I'm the person who spent a weekend reverse-engineering gym entry to save 44 seconds. I'm not adding server round-trips when you're standing at a gate.&lt;/p&gt;
    &lt;p&gt;Being local also means no liability. Personal immigration history is exactly the kind of data governments might want. Keeping it off my servers means nobody can demand I hand it over. Some friends asked about cloud sync: I keep saying no. Not because sync is hard—it is, though surely Claude Code can do that for me [4]—but because the moment you add a server you add retention policies, jurisdiction questions, and a magnet for legal demands. If you want it on another device, export a file and move it yourself like the ancestors did it.&lt;/p&gt;
    &lt;p&gt;The first version just counted Schengen days, then I added the UK's midnight arithmetic because I needed it for my own calculations. Then documents with their expiry rules because I was tired of manually looking them up. Then the what-if layer because adding and deleting trips to see impact felt like manually diffing state. Then visa requirements and IDP rules: none of this was planned, it accumulated from use, the same way my fermentation tracker grew from "can I eat this?" to HACCP compliance documents.&lt;/p&gt;
    &lt;p&gt;I shipped it because keeping it private felt unfinished, and because I'd like fewer people spending twenty minutes researching whether a £62 return flight will cause problems six months later.&lt;/p&gt;
    &lt;p&gt;That Iceland error fare? I bought it. The app told me I wouldn't need an IDP, that the trip wouldn't push me over any Schengen threshold, that I'd leave with 34 days of margin in my 90/180 window, and—importantly—that booking it would mean I'd stop being a UK tax resident given my upcoming Canada move. Useful things to know before clicking purchase. The officer at Keflavík looked at his screen, agreed with his systems, and waved me through.&lt;/p&gt;
    &lt;p&gt;I called the app Residency and you can get it here. No subscriptions, costs less than an airport martini, and you'll likely regret it less a few hours later.[5]&lt;/p&gt;
    &lt;p&gt;You can't &lt;code&gt;cURL&lt;/code&gt; a border. But you can track your own state carefully enough that when the governments know the answer, so do you.&lt;/p&gt;
    &lt;p&gt;Working on problems where rules are complex and official documentation is &lt;del&gt;garbage&lt;/del&gt; contradictory? I build systems that handle state when the state won't tell you your state. work@drobinin.com&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Until 1752, England’s New Year began on 25 March, aka Lady Day. The calendar reform shaved 11 days off, nudging the tax year to 5 April, and skipping a leap day in 1800 pushed it to 6 April. Two centuries later it stays there probably because changing it would be expensive and annoying. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apparently time zones shift all the time for a variety of reasons, from political to religious. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I couldn't find any legit reasons for keeping the "six-month rule" around but it seems like it's still occasionally checked, sometimes even during boarding. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;These stunts are performed by trained professionals, don't try this at home. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I have nothing against airport bars, but hear me out: vermouth should be kept in the fridge at all times, and glasses need to be chilled. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://drobinin.com/posts/you-cant-curl-a-border/"/><published>2025-11-04T00:37:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45806348</id><title>When stick figures fought</title><updated>2025-11-04T13:47:54.836174+00:00</updated><content/><link href="https://animationobsessive.substack.com/p/when-stick-figures-fought"/><published>2025-11-04T00:48:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45806903</id><title>My Truck Desk</title><updated>2025-11-04T13:47:54.672240+00:00</updated><content>&lt;doc fingerprint="6f48b816f30396dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Photograph courtesy of Bud Smith.&lt;/p&gt;
    &lt;p&gt;After eight glorious weeks of freedom, I got rehired.&lt;/p&gt;
    &lt;p&gt;First thing I did was walk over to the machine shop to look for my F-150. The oil stain was there but the truck wasn’t. It wasn’t in the rock lot where the bulldozers parked either.&lt;/p&gt;
    &lt;p&gt;Who would have stooped so low as to co-opt that piece of shit? It had no heat and no air-conditioning. The radio bubbled static. Door handles were missing. Floorboards, fenders, and frame all rusted and rotted. It certainly hadn’t been what could be called roadworthy. And, my God, the smell.&lt;/p&gt;
    &lt;p&gt;I went into the machine shop. One of the welders lifted his hood and told me the bad news—they’d had to move the truck for a rebar delivery and the engine on that old thing finally blew, so the truck got dragged to the scrapyard.&lt;/p&gt;
    &lt;p&gt;In a dusty corner, I saw a pile of salvaged tools from the truck. I took some wrenches and my tape measure but didn’t see what I was really looking for—my Truck Desk®. Oh well.&lt;/p&gt;
    &lt;p&gt;I caught a ride out to the unit with the foreman and the rest of the crew. Our goal for the day was to unbolt components from a heat exchanger and fly them off with a crane. Once the exchanger was apart and inspected, we’d begin our real repairs.&lt;/p&gt;
    &lt;p&gt;The morning went well. The mornings always go well. Everybody knows what they’re doing. We’re professionals, equals. Same pay. Same benefits. All working together toward retirement. We look out for each other. Whoever has the hardest task in this crew today could be the foreman tomorrow, and vice versa. Nobody wants to be the boss, so our bosses are the best kind.&lt;/p&gt;
    &lt;p&gt;At first break we packed into our truck and drove shoulder-to-shoulder back to the trailer compound for coffee. During the five-minute drive, I couldn’t help but think how good I’d had it when I had the luxury of using that piece of shit F-150.&lt;/p&gt;
    &lt;p&gt;See, the truck nobody else wanted had been my office. I’d built a portable desk inside it. My truck desk, I called it. A couple of planks screwed together, our union sticker slapped on, the whole deal sealed with shellac. I’d built the desk so it slid into the bottom of the steering wheel and sat across the armrests. I used to hang back at the job and sneak in some creative work while the rest of the crew went to break. My desk—which I’d taken far too long to build and perfect through many prototypes—had been stowed behind the driver’s seat when the truck was hauled off by the wrecker.&lt;/p&gt;
    &lt;p&gt;Back at the break trailer, I took my old seat and joined in on the jokes, insults, tall tales. That trailer was, to me, the best place for storytelling in the world—but, as always, it was too loud, too raucous, too fun to do any writing or reading, which is all I ever want to do on break. At lunch, I retreated into the relative quiet of the machine shop. I sat down by the drill press and took out my cell phone and started writing. Just like I used to do.&lt;/p&gt;
    &lt;p&gt;For nearly two decades I’ve worked off and on at this petrochemical plant as a mechanic and welder. The union dispatched me here: When it gets slow, I get laid off; when work picks up, I boomerang back. And the whole time, I’ve written stories and parts of my novels during breaks—fifteen minutes for coffee and then half an hour for lunch. I’ve also made use of the heaven-sent delays brought on by lightning, severe rainstorms, evacuations, permitting problems, equipment issues, and so on. I’m thankful for each and every delay that happens on this construction site, and, believe me, there are many.&lt;/p&gt;
    &lt;p&gt;Most artists I know are like this. Finding time to make art while working another job, or taking care of loved ones. They improvise. They get better. They get worse. They get better again.&lt;/p&gt;
    &lt;p&gt;Really it mostly comes down to that first thing: finding time. When I talk to people who want to find more time, I repeat something an old-timer said to me early on: “You’ve gotta make your own conditions.”&lt;/p&gt;
    &lt;p&gt;What does that mean? Well. Is it raining? You can either stand out in the rain and get wet, or you can find a coil of tie-wire and hang up tarps for a hooch.&lt;/p&gt;
    &lt;p&gt;There’s another expression I like, which goes: “Let your wallet be your guide.” I try to remember that every time I feel the urge to quit my job and never return.&lt;/p&gt;
    &lt;p&gt;So ever since cell phones got smart, I’ve sat somewhere quiet, semi-on-the-clock, texting myself poems, paragraphs that became stories and novels, and things about my life, or I should say just life, like this thing you’re reading right now.&lt;/p&gt;
    &lt;p&gt;Writing on my cell phone, pecking away, was good enough for many years, but then after a rightfully humbling decade of manual labor, I started having irrational fantasies about convenience and comfort.&lt;/p&gt;
    &lt;p&gt;Of course I have a desk in my apartment, but I couldn’t help myself. Somehow I’d gotten seduced by the prospect of attaining my very own cubicle amid this massive junkyard full of toxic waste.&lt;/p&gt;
    &lt;p&gt;One day I walked into the payroll trailer where the secretaries and site manager sat. There wasn’t an explicit sign that said NO CONTRACTORS ALLOWED, but it was an unspoken rule. The trailer had a few unused old cubicles tucked to the side. I sat down in one and happily pecked away with my thumbs. Every break for a week I went in and worked on my writing. After a few days I started to feel like I should hang pictures of my mom and dad and my wife inside it. But I didn’t dare.&lt;/p&gt;
    &lt;p&gt;Then things really heated up. I brought in a Bluetooth keyboard and wrote a whole story that day on my breaks. There was no going back. My heart soared. I thought I should adopt a brown dog with a bandanna around his neck just so I could thumbtack his picture to the cubicle wall. I hadn’t interacted with any of the office staff, but they’d seen me. They’d followed my oily bootprints down the hallway and begun to leer. Who is this diesel-stinking contractor? He’s probably the one who’s been eating Janelle’s Oreos. He raided the mango-kiwi yogurt from the fridge. He glommed all the sporks. I knew my cubicle dreams were over the morning I found the site manager waiting in “my” cubicle.&lt;/p&gt;
    &lt;p&gt;“What are you doing here?” he asked.&lt;/p&gt;
    &lt;p&gt;In all my years working at that place, I’d never seen the site manager out on the site. I’m not sure he knew what it was or where it was. You went to him to order tools; he was the one who said no. I’d only ever seen him at a urinal or buying bacon and eggs off the lunch truck. But if I had ever seen him out on the site, it would have never occurred to me to ask him what he was doing there. He was wearing a blue polo shirt and khakis, and I was in his world—and he was asking.&lt;/p&gt;
    &lt;p&gt;“Office work,” I said.&lt;/p&gt;
    &lt;p&gt;“What kind, exactly?”&lt;/p&gt;
    &lt;p&gt;How can you explain literary fiction to a site manager?&lt;/p&gt;
    &lt;p&gt;“Little bit of everything,” I said.&lt;/p&gt;
    &lt;p&gt;I started writing in the machine shop again. It wasn’t the same. Once I’d been infected by the cubicle virus, there was no going back. Out of scrap lumber I gathered from various dumpsters, I built a proper desk for myself in the northeast corner of the shop. That desk was a huge leap forward in possibility and productivity. In the evenings, if I wrote something by hand or on my typewriter at home, I could now use my time at work to retype it at my shop desk.&lt;/p&gt;
    &lt;p&gt;The shop desk was not ideal. Some days I arrived to find someone had disassembled a small motor on top of it, gaskets and hardware spread out on newspaper. Other times I found pneumatic guns taken apart, or electrical devices with wiring splayed in a colorful tangle, or—fair enough—important blueprints laid out the entire length of the desk.&lt;/p&gt;
    &lt;p&gt;Right around this time I first saw the F-150. One of the workers had abandoned it by the shop. I put a battery in. That lasted one shift. Then I took an alternator out of another junk truck and, lo and behold, I had my own four wheels. The fan belt screamed. The engine smoked. The brakes worked when they wanted to. It was mine that whole dangerous year.&lt;/p&gt;
    &lt;p&gt;Then, one day, my luck changed.&lt;/p&gt;
    &lt;p&gt;A crate full of chain falls got delivered. It was a glorious crate, made of sanded spruce. I unscrewed some of the planking and built my first Truck Desk prototype.&lt;/p&gt;
    &lt;p&gt;It was made of three boards cut at twenty-four inches. Light and compact. Sealed with shellac. It slid into the bottom of the steering wheel, one side supported by a curved rebar I welded into a nut that fit exactly in a recess on the driver’s door. The center console supported the other side of the desk. I kept it stored behind the seat. Whenever break time came and the crew drove back to the trailer compound, I stayed parked on the unit and got at least ten extra minutes to write.&lt;/p&gt;
    &lt;p&gt;Now that I had my Truck Desk, that vehicle was my very own rolling cubicle.&lt;/p&gt;
    &lt;p&gt;Having that truck reminded me of when I lived on 173rd Street in New York City. Back then I used to drive around endlessly looking for street parking. I would see men and women sitting in their cars. They weren’t leaving, though; they were reading a book or a magazine, smoking cigarettes, playing Sudoku, scribbling love letters. They were the wisest men and women in the entire city, using their vehicles as a kind of office down on the street, a sanctuary where they could do their real work.&lt;/p&gt;
    &lt;p&gt;After the F-150 was scrapped, I never got a replacement truck. I never found that first Truck Desk either, even when I called the scrapyard.&lt;/p&gt;
    &lt;p&gt;What I did do, though, was go over to the carpenter’s side of the shop and cut a scaffold plank at twenty-nine inches. This simple plank fits across the armrests of whatever Chevy or Ford pickup the crew has that day. This dramatic redesign of Truck Desk into Truck Plank® took all of ten seconds. I didn’t bother with the sticker or shellac.&lt;/p&gt;
    &lt;p&gt;The years on the job have rolled on. Now editors send me Word documents with comments and questions and tracked changes. I bring my backpack to work with my laptop inside.&lt;/p&gt;
    &lt;p&gt;Every morning, when I find out what crew I’m in, I bring that plank with me. I stick it on the dashboard and climb into the driver’s seat. I drive us all out to the job and at break time I take them to the trailer. I clean my hands with pumice wipes and sit alone in whoever’s truck it is that day, pulling the plank off the dashboard and setting it across the armrests. Within a minute or so, I’ve got the laptop out and I’m working. If somebody from the crew is still in the back seat, bandanna over their eyes, snoozing, I do my best to keep extra quiet. And if they begin to snore, I don’t let that bother me at all.&lt;/p&gt;
    &lt;p&gt;Bud Smith is the author of the novel Teenager and the story collection Double Bird, among other books. Mighty, a novel, is forthcoming from Knopf in spring 2027. His story “Skyhawks” appears in the new Fall issue of The Paris Review.&lt;/p&gt;
    &lt;p&gt;Last / Next Article&lt;/p&gt;
    &lt;p&gt;Share&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theparisreview.org/blog/2025/10/29/truck-desk/"/><published>2025-11-04T02:37:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45807735</id><title>Pain Points of OCaml</title><updated>2025-11-04T13:47:54.450671+00:00</updated><content>&lt;doc fingerprint="e521109f4f6181dc"&gt;
  &lt;main&gt;
    &lt;p&gt;I am currently looking for a job! If you're hiring a new grad in 2026 for for Rust, TypeScript, or React, feel free to shoot me an email at serena (at) quamserena.com.&lt;/p&gt;
    &lt;head rend="h1"&gt;Pain points of OCaml&lt;/head&gt;
    &lt;p&gt;I've been writing a compiler in OCaml for the language described by the dragon book for a class, and now that I'm about halfway through it I've found myself missing Rust in a few spots. In no particular order, the pain points I've hit while using OCaml:&lt;/p&gt;
    &lt;head rend="h2"&gt;Syntax&lt;/head&gt;
    &lt;p&gt;The OCaml syntax is... not great, its only redeeming quality being that it has no significant whitespace. There are some alternative syntax frontends for OCaml like ReasonML that fix these problems, but I didn't know about them before starting the project. &lt;code&gt;let...in&lt;/code&gt; syntax is weird, but I can chalk that up to an aesthetic choice; the real devil is in the fact that match statements have no terminator, leading to mysterious errors sometimes if you try to nest them. In general, OCaml has very little punctuation (which isn't necessarily a bad thing) but it can mean that in some situations it's difficult to tell where this start and stop. For me, C-style brackets help make the syntax heirarchy in code more clear.&lt;/p&gt;
    &lt;p&gt;Another annoyance is automatic currying and partial function application. Usually partial function application is a bug, and typically there is a helpful lint where it happens, but if you're not explicitly annotating types the errors can be inscrutable. My preferred method for debugging syntax-related issues is to start randomly adding parenthesis where I think things are supposed to start and end, and usually that makes it clear where the error is.&lt;/p&gt;
    &lt;head rend="h2"&gt;Typechecking is way too clever&lt;/head&gt;
    &lt;p&gt;The type checker in OCaml is extremely clever and is usually able to determine the correct type of a variable without having to annotate anything. When it can't, though, (usually because of an error you made somewhere else) the error message is typically inscrutable. While this is cool because it allows OCaml to be both statically typed and have no type annotations, it is extremely bad from a software robustness perspective — usage of a variable in one area can influence its type in another opaquely, and it's just annoying to debug why the type inference is failing. Good luck debugging this when one of your dependencies updates and changes a function's signature. I've taken to annotating every function parameter and return type manually just for clarity's sake and so that the error messages make more sense. I think that Rust was definitely correct for making type annotations for function parameters and return values explicit despite also being based on Hindley-Milner type inference. Rust still allows you to be clever with type inference in some places if you want to though, such as with return type polymorphism.&lt;/p&gt;
    &lt;head rend="h2"&gt;Types&lt;/head&gt;
    &lt;p&gt;Annoyingly you cannot use types before they're declared — there is no type hoisting as you would see in other languages. You can get around this pretty easily by creating a parameterized type and then at the bottom of the file setting an alias for the type with the correct parameter bound, but it adds noise to the code. Enumerated types also dump all of their variants into the module scope (not namespaced to any type) and will silently shadow variants from other types, which is an odd choice to say the least. I think that it is reminiscent of global &lt;code&gt;#define&lt;/code&gt;'s not being namespaced in C, but I am not sure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tooling and ecosystem&lt;/head&gt;
    &lt;p&gt;OCaml tooling has improved a lot recently. It has a build system, Dune, that is comparable to the build systems of other languages like Cargo or NPM, and generally works quite well in my experience. The OCaml ecosystem is a bit strange, being dominated by Jane Street who maintains their own separate standard library Core (since the OCaml stdlib leaves a lot to be desired).&lt;/p&gt;
    &lt;p&gt;In terms of writing compilers, there's ocamllex and Menhir, which is what I'm using. (Menhir is (mostly) a drop-in replacement for ocamlyacc that fixes some problems). My gripe with these is that they are two DSLs that opaquely compile to OCaml source files and have their own (separate) semantics and grammar. Despite OCaml having match statements and everything, these tools instead more similarly follow their C counterparts, choosing to define their own match-statement-like-construct for lexing and parsing. They also emit errors where the entire diagnostic is just "Syntax Error", which is difficult to debug considering that these are DSLs with OCaml embedded within them. Usually the only way to debug this is to open up the Menhir manual and look at the examples.&lt;/p&gt;
    &lt;head rend="h2"&gt;Printing&lt;/head&gt;
    &lt;p&gt;Printing strings to stdout is oddly annoying in OCaml. There isn't really a way that you can just take an object and print it; instead you have to use &lt;code&gt;printf&lt;/code&gt; for primitives and define your own print function for your own objects — there is no equivalent to Java's &lt;code&gt;toString&lt;/code&gt; or Rust's &lt;code&gt;#[derive(Debug)]&lt;/code&gt;. The inability to express this is a weakness of the OCaml type system in my opinion.&lt;/p&gt;
    &lt;head rend="h2"&gt;Would I do it again?&lt;/head&gt;
    &lt;p&gt;When comparing Rust to OCaml, there is a general theme of robustness vs. elegance. Many of the design choices of OCaml (partial function application, inferred types) are motivated from the perspective of elegant functional programming (the things a PhD student cares about), meanwhile Rust is always about long-term stability and maintainability (the things a software engineer cares about). The usual mantra applies here: that neither of these are better in the general case but rather the best choice depends on the problem domain.&lt;/p&gt;
    &lt;p&gt;If I was to write another compiler, I would highly consider OCaml again, despite all of its pain points and missing quality-of-life features. I enjoy being able to use pattern matching and the elegance of a functional approach without having to worry about lifetimes and memory management, at least for a first draft/MVP version of the compiler. I would probably take a closer look at ReasonML though.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://quamserena.com/2025-11-03/pain-points-of-ocaml"/><published>2025-11-04T05:41:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45807775</id><title>Tell HN: X is opening any tweet link in a webview whether you press it or not</title><updated>2025-11-04T13:47:54.096810+00:00</updated><content>&lt;doc fingerprint="d49d99132ed61bc5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Just saw the CEO of Substack celebrating traffic from X/Twitter shooting up thinking they stopped suppressing tweets with links[0]. Actually, this traffic is because now any time you open a tweet with a link, the in-app webview loads in the background, and displays when you press the link.&lt;/p&gt;
      &lt;p&gt;I run an ecom store that gets a lot of its customers from Twitter. I was also shocked to see my traffic double or triple overnight and thought the algorithm had blessed me and my business. Soon realized what was actually happening. Thought other traffic-monitors might appreciate this explanation.&lt;/p&gt;
      &lt;p&gt;Meanwhile Nikita Bier is pretending they never suppressed tweets with links to begin with, offering the alternative explanation: "a common complaint is that posts with links tend to get lower reach. This is because the web browser covers the post and people forget to Like or Reply. So X doesn't get a clear signal whether the content is any good"[1]. A bit of a rewriting of history since Elon and his mom both tweeted about how it wasn't fair to use his platform to promote other links/platforms, even banning people who shared profiles of other social networks (including Paul Graham for a period). They suppressed all links shortly after.&lt;/p&gt;
      &lt;p&gt;[0] https://x.com/cjgbest/status/1985464687350485092&lt;/p&gt;
      &lt;p&gt;[1] https://x.com/nikitabier/status/1979994223224209709&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45807775"/><published>2025-11-04T05:53:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45808308</id><title>Lessons from interviews on deploying AI Agents in production</title><updated>2025-11-04T13:47:53.347971+00:00</updated><content>&lt;doc fingerprint="bf43921aad23a4ce"&gt;
  &lt;main&gt;
    &lt;p&gt;“Customers look at Microsoft’s Copilot and think, ‘Oh great, Clippy 2.0!’” – Marc Benioff, CEO of Salesforce&lt;/p&gt;
    &lt;p&gt;“Copilot? Think of it as Clippy after a decade at the gym.” – Satya Nadella, CEO of Microsoft&lt;/p&gt;
    &lt;p&gt;If you don’t know what Clippy is (or better yet, never encountered it), consider yourself lucky. Introduced in Microsoft Office in 1996, Clippy was the infamously annoying digital paperclip that offered unsolicited advice to users, and instantly became the world’s most hated virtual assistant. And why are we referencing an oft-reviled 1996 virtual assistant that was eventually turned off in 2007? Because history never repeats itself, but it does often rhyme.&lt;/p&gt;
    &lt;p&gt;With all the buzz around agentic AI (and Gartner’s prediction that over 40% of agent-based AI initiatives will be abandoned by 2027), we asked ourselves: What does it take for AI agents to be deployed in production environments at large enterprises? What does it take for agentic products and copilots to actually be loved and used by employees (unlike the much-maligned Clippy)?&lt;/p&gt;
    &lt;p&gt;To this end, we surveyed over 30 of the top agentic AI startup founders in Europe and interviewed 40+ practitioners to not only build a view around the State of Agentic AI, but also create a playbook based on the common practices we’ve seen successful agentic startups deploy. We also include raw, unfiltered commentary from the survey. To give you a flavour of some of the things we’ve learned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The biggest challenges founders encounter when they are deploying AI Agents in production environments are actually not of the technical variety, instead they are: &lt;list rend="ul"&gt;&lt;item&gt;Workflow integration and the human-agent interface (60% of startups)&lt;/item&gt;&lt;item&gt;Employee resistance and other non-technical factors (50%)&lt;/item&gt;&lt;item&gt;Data privacy and security (50%) &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;As a result, the most successful deployment strategies involve a “Think Small” approach, starting with low-risk yet medium-impact, easy-to-verify tasks that quickly demonstrate clear ROI. Even better if it’s automating a task the human user hates, and it’s pitched as a co-pilot that augments (rather than replaces) humans.&lt;/item&gt;
      &lt;item&gt;A significant 62% of agentic AI startups are already tapping into Line of Business or core spend budgets, proving the technology is moving beyond the experimental phase.&lt;/item&gt;
      &lt;item&gt;Although pricing strategies are continuously evolving, Hybrid and Per Task are most commonly used (23% each), while the “Holy Grail” of Outcome-based pricing is currently used by only 3% because different customers value different outcomes, it’s hard to attribute, measure and monitor these outcomes, and that makes pricing unpredictable.&lt;/item&gt;
      &lt;item&gt;As the ecosystem is in such nascent stages, most (52%) startups are building their agentic infrastructure fully or predominantly in-house.&lt;/item&gt;
      &lt;item&gt;Startups are focusing on reliability, as over 90% report at least 70% accuracy in their solutions. While healthcare startups reported the highest levels of accuracy (unsurprising), medium levels of accuracy is acceptable for simpler, low-risk use cases with easily verifiable output, when the high volume of automation offsets the lower accuracy, or when the AI enables an entirely new, previously impossible capability.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on our findings and interviews with enterprise practitioners, we’ve outlined observations on successful agentic deployment drivers, covering everything from strategic roll-out of use cases to (what we call) the 3Es framework (Education, Entertainment and Expectation management). Are you an agentic AI startup looking to overcome the various challenges around enterprise deployments? Jump straight to our observations here.&lt;/p&gt;
    &lt;p&gt;If you’re a founder building in this space, please reach out to Advika, Sevi or Mina – we’d love to chat.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s an AI agent, and why do we need them?&lt;/head&gt;
    &lt;p&gt;“What’s in a name? That which we call an AI Agent&lt;lb/&gt; By any other name would be just as hypey”&lt;/p&gt;
    &lt;p&gt;– Not said by William Shakespeare&lt;/p&gt;
    &lt;p&gt;A variety of definitions of AI agents are thrown about, but for the purposes of our discussion, we describe their key attributes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Goal orientation: AI agents are assigned specific tasks or objectives, and their actions are aligned with achieving those goals.&lt;/item&gt;
      &lt;item&gt;Reasoning: Agents create plans to achieve the aforementioned goals and incorporate the ever-changing real-world context in their planning; they break down their main goal or complex problems into smaller, manageable tasks and think about the next best steps.&lt;/item&gt;
      &lt;item&gt;Autonomy: AI agents act independently without needing constant inputs/instructions from humans; they make decisions and take actions (via tool calling) based on the changing world around them. Given the nascence of GenAI-powered agents and assorted reliability issues around them (and enterprise practitioners’ caution around deploying fully autonomous systems), our definition of agents does not require full autonomy. As a result, co-pilots are included in our definition (so long as they meet the the other criteria we’ve listed out, such as goal orientation, reasoning, and actions via tool use).&lt;/item&gt;
      &lt;item&gt;Persistence: Agents have memory, or are able to remember their prior experiences and maintain focus on a long-term goals across sessions. This is also known as state management.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AI agents are different from basic LLM chatbots because state management and tool calling are a harder engineering challenge, making their deployments much trickier. An evolution of this is multi-agent systems (MAS) is where agents can have shared memory, overarching goals, coordination amongst themselves. These MAS involve individual agents with specialised capabilities (or distinct sub-components of a broader goal) working together to solve complex problems, even across organisational boundaries.&lt;/p&gt;
    &lt;p&gt;Given MAS distribute cognitive load across multiple agents (each optimised for specific sub-tasks), they have demonstrated superior performance in handling complex, open ended problems vs single-agent approaches. They improve efficiency, reduce costs, and offer better fault tolerance and flexibility – which means they outperform single-agent systems in overall performance.&lt;/p&gt;
    &lt;p&gt;But why use AI agents at all? Why not RPA (Robotic Process Automation) or other traditional forms of automation? That’s because AI agents are better for complex, dynamic, and unstructured tasks that require cognitive ability, reasoning, and adaptability. Unlike RPA which follows rigid, pre-defined rules, AI agents can reason toward a goal, make dynamic decisions on the fly, and learn or improve over time – this allows them to handle edge cases and changes in the environment without breaking.&lt;/p&gt;
    &lt;head rend="h2"&gt;What does enterprise adoption of Agentic AI look like?&lt;/head&gt;
    &lt;p&gt;Certain surveys, such as KPMG’s AI 3Q 2025 quarterly pulse, note that AI agent deployment has nearly quadrupled, with 42% of organisations now having deployed “at least some agents,” up from 11% two quarters ago. While this may sound promising, we think “at least some agents deployed” is a poor measure of the true picture of adoption. Our conversations with practitioners suggest that, yes, most large enterprises are deploying AI agents in production environments, but these deployments are typically fairly small. They’re also mostly concentrated in (relatively) more mature areas as such as Customer Support, Sales and Marketing, Cybersecurity and Tech (e.g. AI coding agents).&lt;/p&gt;
    &lt;p&gt;We think it would be more useful to think of adoption through the following lenses:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How many teams and employees are actually using Agentic AI in their day-to-day work: A May 2025 PwC survey noted that for most respondents (68%), half or fewer of their employees interact with agents in their everyday work. That said, our conversations with practitioners suggests that employees are using personal accounts where enterprises are not adopting the tech, triggering a “Shadow AI” problem where compliance issues run rampant.&lt;/item&gt;
      &lt;item&gt;The extent to which employees are using AI agents for their “potentially automatable” workflows (for very few of their workflows, some, or most): We stress on the “potentially automatable” point because it may not be desirable for EVERY workflow to be automated, and agentic AI may not necessarily be the best automation technique for that specific task. &lt;lb/&gt;While collecting data for actual workflows automated vs potentially automatable workflows is challenging, KPMG’s observation (from the same survey) on “How have AI agents been received by employees?” is a relatively useful proxy: only 10% of respondents indicated “significant adoption” where employees are enthusiastically adopted AI agents and are fully integrating them into workflows, while 45% pointed to “slight adoption” where employees are beginning to accept and integrate AI agents into their work (the remainder are getting mixed responses).&lt;/item&gt;
      &lt;item&gt;The degree of autonomy given to the AI agent for each workflow (whether it can execute only some tasks within a given workflow, or it can drive the entire workflow end-to-end): Our conversations with enterprise practitioners suggest that they are taking a conservative approach. Even if agentic AI solutions can theoretically be run reliably at 80% levels of autonomy, most practitioners will veer towards greater levels of human-in-the-loop and run the solution at 50% levels of autonomy.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Survey Findings&lt;/head&gt;
    &lt;p&gt;We surveyed 30 European agentic AI startup founders, and interviewed 40+ enterprise practitioners and founders to ascertain:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Levels of accuracy and autonomy that their Agentic AI solutions are operating at&lt;/item&gt;
      &lt;item&gt;Pricing strategies most commonly used by founders&lt;/item&gt;
      &lt;item&gt;Budgets that agentic startups are able to tap into (just innovation budgets, or the core Line of Business budgets)&lt;/item&gt;
      &lt;item&gt;Challenges startup founders typically encounter when deploying agentic AI solutions at large enterprises&lt;/item&gt;
      &lt;item&gt;Agentic infrastructure that founders have built internally, and third party tools they have used&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Autonomy and Accuracy&lt;/head&gt;
    &lt;p&gt;Autonomy and Accuracy are linked dimensions – after all, you only automate to the extent to which you can get reliable and accurate outputs from AI agents. In an ideal agentic world, we would have extremely high levels of both accuracy and autonomy. By accuracy, we mean the % of agent-executed tasks result in a successful or accepted outcome (i.e. 0 being completely overridden by a human, 10 being fully accepted without changes).&lt;/p&gt;
    &lt;p&gt;While we’re not there yet, we’ve found that currently &amp;gt;90% of Agentic AI startups have at least 70% accuracy, though only 66% of Agentic AI startups operate at least 70% autonomy. Unsurprisingly, the acceptable levels of accuracy vary by industry and use case – e.g. 80% average accuracy for financial services, 90% accuracy for healthcare and so on. The more interesting question is: under what circumstances is a medium level of accuracy acceptable?&lt;/p&gt;
    &lt;p&gt;Given the interplay between accuracy and autonomy, we’ve identified three configurations that startups mainly sit in:&lt;/p&gt;
    &lt;p&gt;Medium Accuracy, High Autonomy: A medium level of accuracy (60-70%) is acceptable if the use case is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;low risk and results in an output that is easy for a human to verify and modify; and&lt;/item&gt;
      &lt;item&gt;such that the lower level of accuracy is more than offset by a very high level of automation, so if it’s a time-consuming task with overwhelming volumes you would take the higher level of automation so you can move through massive volumes and only focus on the edge cases which the agent can’t handle; or&lt;/item&gt;
      &lt;item&gt;an entirely new capability, which was previously impossible, so the trade-off is that you would rather be able to perform a certain activity at 70% accuracy than not be able to do it at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High Accuracy, Low Autonomy: This category predominantly comprised of agentic healthcare startups, where the typical levels of accuracy and autonomy were 90% and 40% respectively – and these were for much more high-stakes use cases (e.g. research for clinical trials, mental health care) where accuracy is of paramount importance. As one founder noted (regarding their agentic AI solution’s &amp;gt;85% accuracy):&lt;/p&gt;
    &lt;p&gt;“This accuracy level is not sufficient to remove human oversight and achieve full autonomy, especially in the sensitive context of clinical trials where regulatory standards are stringent.”&lt;/p&gt;
    &lt;p&gt;High Accuracy, High Autonomy: Majority of the startups in this category operate at 80-90% accuracy and autonomy levels, and are typically focused on the financial services use cases (e.g. compliance) as well as relatively more mature areas of AI deployment, such as customer support, cybersecurity, and research. In these cases, we’ve observed that founders are increasingly marrying probabilistic LLMs with more deterministic AI methods to enhance accuracy and consequently further autonomy.&lt;/p&gt;
    &lt;p&gt;Here’s a visual summary of the three Accuracy/Autonomy configurations that agentic startups mainly sit in:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;As we increasingly deploy agents on multi-step problems or introduce multi-agent systems, the bar for accuracy is only going to increase – like what happens when you chain a 90% accurate agent with another 90% accurate agent, and so on with the errors compounding at each step? It triggers something called cascade failure, a phenomenon we explore in our upcoming research reports (along with how knowledge graphs and neurosymbolic AI are the way forward) – so stay tuned!&lt;/p&gt;
    &lt;head rend="h3"&gt;Pricing&lt;/head&gt;
    &lt;p&gt;Given the agentic AI ecosystem is in early stages, most founders we spoke to see their pricing strategies as something to be evolved over time, a position we believe makes sense. For instance, a “per user” pricing makes sense at lower levels of autonomy (because a co-pilot needs a human user alongside) but at higher levels of autonomy, where an agent could perform most of an employee’s tasks (plus unlock new capabilities), a “per agent” pricing with an outcomes bonus may make more sense.&lt;/p&gt;
    &lt;p&gt;As SaaS license and API usage-based pricing are well understood, we’re focusing here on other pricing strategies and their implications:&lt;/p&gt;
    &lt;head rend="h4"&gt;Outcome based&lt;/head&gt;
    &lt;p&gt;Outcome-based pricing is often touted as the Holy Grail of AI Monetisation, because it lets customers pay only when a specific, pre-defined business result is achieved. A great example of this is Intercom, which charges customers $0.99 for every successful conversation resolution achieved autonomously by its Fin AI Agent, ensuring they only pay when the agent delivers. This way price is aligned with the business value delivered, the customers’ risk is lower and because it’s connected with a tangible output, it’s relatively easier for customers to understand than a token-based calculation (which doesn’t feel intuitive).&lt;/p&gt;
    &lt;p&gt;However, in practice, outcome based pricing is difficult to achieve for a wide variety of reasons. Firstly you’ll need to agree on what outcomes the customer values (and different customers may value different outcomes, so you could potentially end up in a situation where you have loads of customised contracts). Secondly, you’ll need to figure out attribution – for instance, with a Sales Co-pilot, it would be hard to attribute how much of a new customer win is driven by the AI agent vs the human sales rep. Linked with that is the problem of how to measure the outcome – and all of this makes the calculation more complex. Finally, it can be unpredictable because it is challenging to forecast certain outcomes (e.g. % of cost savings) in advance – as in, not only are you uncertain of the magnitude of outcome but also the timing of the outcome (it could be deferred). Here’s what a founder had to say:&lt;/p&gt;
    &lt;p&gt;“But the problem is ultimately it’s very difficult to agree on what those outcomes are. It’s very difficult to agree on tracking that, and it’s very hard to do at scale. You can’t really do that self serve because it’s so gamified – people are incentivised not to report outcomes to you.”&lt;/p&gt;
    &lt;p&gt;It’s much easier to work with outcome based pricing when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the desired outcomes are well-defined and similar across your customers;&lt;/item&gt;
      &lt;item&gt;the agent operates the entire workflow or task end-to-end, so it’s easier to attribute; and&lt;/item&gt;
      &lt;item&gt;when the outcomes are simple to measure and monitor in real time (like Intercom’s outcomes are binary – either the agent resolves it or doesn’t, and that feedback is received quickly).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consequently, we expect to see more hybrid models than pure outcome-based, where a per-agent pricing model is augmented with outcome bonuses rather than charging only for outcomes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Per user&lt;/head&gt;
    &lt;p&gt;From a budget allocation perspective, this is easier for customers to understand, and it also makes sense for co-pilots where a human user is necessarily required alongside your product. The disadvantage of this pricing model is that it doesn’t distinguish between power users and casual users of your agentic AI solution, leading to the casual users subsidising the lower or negative margin power users. However, if the price point of your co-pilot product is high enough to cover even the costs of supporting power users, it’s a good starting point. As one founder noted:&lt;/p&gt;
    &lt;p&gt;“We’re fortunate to be in an industry [financial services] where price anchoring is quite high; if you have premium product you can charge a better price. While usage is very high, usage would need to be rather absurd to sufficiently eat into the margins.”&lt;/p&gt;
    &lt;p&gt;Also, if your agentic solution is highly successful in automating away a great many tasks, it would end up reducing the number of seats to be had in the first place – so it is unsuitable for highly automated solutions. That said, most founders we spoke to intend to evolve their pricing to a hybrid model, particularly as they enable greater autonomy.&lt;/p&gt;
    &lt;head rend="h4"&gt;Per agent&lt;/head&gt;
    &lt;p&gt;This is an intuitive pricing model when you’re automating the vast majority of tasks a particular employee carries out; that way you’re replacing a human and it comes out of the headcount budget. It’s also predictable and easy for customers to understand. However, an interesting dimension we observed around how founders (running this pricing model) are positioning it – rather than pitching their product as a replacement for a human employee (or focusing on the tasks that an employee currently performs), they are focusing on the net new capabilities the AI agent unlocks that a human employee couldn’t, which allows them to charge more premium prices.&lt;/p&gt;
    &lt;head rend="h4"&gt;Per task&lt;/head&gt;
    &lt;p&gt;This is intuitively easy to understand, because it directly connects usage with the cost (so customers pay only for what they use). This is especially helpful in cases where it’s challenging to predict the frequency and volumes of the tasks to be performed. Because it’s linked with tasks performed, it also helps startups tap into the services budget.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hybrid&lt;/head&gt;
    &lt;p&gt;We increasingly see founders opting for a hybrid strategy, which typically involves some sort of base fee, and variable pricing on top, with tiers and overages. Or it could be charging per agent plus an outcome based bonus. Or it could be charging per agent plus metered dedicated tools (so it’s a bit like a human employee asking for SaaS tools to perform their work). As you can see, there are a variety of ways to implement a hybrid pricing model.&lt;/p&gt;
    &lt;p&gt;It’s good because it’s much more flexible, and protects margins by capping usage (so startups can control costs and reduce the risk of unprofitable customers). However, it can quickly get complex, and helping customers predict consumption is key – whether it’s by having a pre-installation analysis for existing volumes of work that could be automated, setting usage reminders and hard usage limits, or credit rollovers, depending on the implementation of the hybrid model.&lt;/p&gt;
    &lt;head rend="h3"&gt;Agentic AI increasingly a part of line of business budgets&lt;/head&gt;
    &lt;p&gt;We asked founders: “Which enterprise budgets are you currently tapping into?” and we were heartened to see that agentic AI startups are predominantly selling into Line of Business or core spend budgets. It goes to show that we are moving past a pure experimental stage (which is where innovation budgets come in) and that AI agents are making a difference to real business use cases or core activities. It’s an excellent way of tracking the mainstreaming of AI agents- even though deployments currently are “broad” rather than “deep” it’s undeniably positive.&lt;/p&gt;
    &lt;p&gt;Our findings were corroborated by other enterprise-oriented surveys as well:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On average, CFOs report dedicating 25% of their current, total AI budget on AI agents. (Salesforce, August 2025 survey of 261 global CFOs)&lt;/item&gt;
      &lt;item&gt;88% of executives say their companies plan to up their AI-related budgets this year due to agentic AI. Over a quarter of them plan increases of 26% or more. (PwC, May 2025 survey of 300 senior executives)&lt;/item&gt;
      &lt;item&gt;Organisations are redirecting their AI investments toward core functions, which now command 64% of AI budgets compared to 36% for noncore activities. This reallocation suggests a growing sophistication: a recognition that AI delivers its most compelling value when applied to central business operations rather than peripheral processes. (IBM, June 2025 survey of 2,900 executives globally)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The challenges with agentic deployments&lt;/head&gt;
    &lt;p&gt;We asked founders in our survey: “What are the biggest issues you have encountered when deploying AI Agents for your customers? Please rank them in order of magnitude (e.g. Rank 1 assigned to the biggest issue)”&lt;/p&gt;
    &lt;p&gt;The results of the Top 3 issues were illuminating: we’ve frequently heard that integrating with legacy tech stacks and dealing with data quality issues are painful. These issues haven’t gone away; they’ve merely been eclipsed by other major problems. Namely:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Difficulties in integrating AI agents into existing customer/company workflows, and the human-agent interface (60% of respondents)&lt;/item&gt;
      &lt;item&gt;Employee resistance and non-technical factors (50% of respondents)&lt;/item&gt;
      &lt;item&gt;Data privacy and security (50% of respondents)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Workflow integration and the human-agent interface&lt;/head&gt;
    &lt;p&gt;By this we are referring to both the conceptual aspects (e.g. How should my processes, workflows or even role evolve to accommodate AI? Where and how can AI agents help me?) and practical aspects (e.g. what does the UI look like?).&lt;/p&gt;
    &lt;p&gt;Conceptually, it takes end-users some time to adapt to his new paradigm. First it’s about accepting and realising that processes need to change, second it’s about figuring out how they need to change. And it’s not just for the end-users to work that out, but also for the team that makes the buying decisions for agentic AI solutions.&lt;/p&gt;
    &lt;p&gt;Practically, startups are focused on making sure their agents are deployed within the context the user needs and also show up in other UIs (e.g. ServiceNow, Slack) in workflows across systems. Basically, meeting users wherever they are, to make the process of adopting agents as frictionless as possible. It’s also about making sure that the workflows and outputs are customised to the human user. As one founder observed:&lt;/p&gt;
    &lt;p&gt;“A lot of companies will want very specific workflows – which makes sense – but supporting multiple unique instances is still quite difficult as some users will want it in very specific formats e.g. specific excel output – supporting that ‘last mile’ UI is probably the biggest headache.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Employee resistance and non-technical factors&lt;/head&gt;
    &lt;p&gt;We observed an interesting pattern in the survey results, where startups whose agents operate at higher levels of autonomy (9/10 or higher) were more likely to report employee resistance as a bigger issue. Those operating in heavily regulated industries and domains (healthcare, compliance) and therefore require high accuracy also noted that customers were sceptical of agentic solutions. Our findings around autonomy, accuracy and their effect on employee resistance are simply expressions of a single problem: Trust issues.&lt;/p&gt;
    &lt;p&gt;These trust issues invariably have other manifestations. Our conversations with enterprise practitioners suggested that human-AI collaborations weren’t always working well; either humans overrelied on the AI which gave faulty responses or they underrelied and double-checked everything the AI did, reducing efficiency. This phenomenon was also observed in an MIT study which suggests that human-AI collaboration often underperforms compared to AI or humans working alone. Reasons for this include the presence of communication barriers, trust issues, ethical concerns, and lack of effective coordination between humans and AI systems. As one founder noted:&lt;/p&gt;
    &lt;p&gt;“They [human users] often think AI is ‘magic’, and don’t fully grasp its advantages and downsides. Failing to understand how AI works can sometimes lead to frustration and confusion. There is also a certain reluctance to drop old processes and taking the plunge fully with AI.”&lt;/p&gt;
    &lt;p&gt;Another major non-technical factor that founders pointed to was that customers often lacked a coherent AI and data strategy, leading to a plethora or use cases and test pilots but no cohesive plan for AI adoption at scale. In fairness to the customers, another founder highlighted:&lt;/p&gt;
    &lt;p&gt;“AI proliferation creates selling friction. Every incumbent provider promises AI enabled point solutions now, which are often initially attractive to customers as its covered by committed budget. But this results in a fragmented AI strategy and very often fails to bring the latest innovation; not all AI is equal.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Integration with legacy tech stacks&lt;/head&gt;
    &lt;p&gt;This isn’t a new problem; we’ve always had these issues with enterprise software. But here’s a fun fact for you – 42% of enterprises need access to eight or more data sources to deploy AI agents successfully. It’s not as much fun when you’re working through it all: legacy tech stacks don’t always have an API, documentation is lacking, customers rely on a variety of super-walled archaic applications that keep the company knowledge blocked, so data is siloed and distributed… and the list goes on. We aren’t sharing any quotes from founders here because they’ve mostly said similar things, which speaks to the universality of a painful experience that doesn’t need any further elucidation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Observability, monitoring and evaluation&lt;/head&gt;
    &lt;p&gt;In our previous research on Responsible AI, we covered why it’s so hard to ensure that AI systems function as intended, and interpreting what the AI model did and why. It was hard enough to interpret the behaviour of a single LLM-powered agent, but this complexity is compounded when multiple agents interact asynchronously and dynamically with each other. Each agent may have its own memory, task objective and reasoning path, so tracing the chain of events leading to a final decision or failure is difficult. Also, you can have cascading errors in a multi-agent system, where they end up reinforcing each others’ bad decisions. And all of these would be difficult to detect unless you have ongoing monitoring and robust eval mechanisms. By ensuring that the AI agents are working as intended, observability, monitoring and evals give customers the confidence to launch them with their end users. It’s also about traceability. As a founder highlighted:&lt;/p&gt;
    &lt;p&gt;“The challenge is to find a rationale for the AI agent’s output that is understood and verifiable by humans, so as to increase trust and actually free up time.”&lt;/p&gt;
    &lt;p&gt;Here’s an interesting aside: As part of our survey, we ask founders “What would you like to learn through our research into Agentic AI? What would be most helpful or useful to you?” and a surprising number of questions were around observability and evals. Stay tuned, we’re covering it in our upcoming research.&lt;/p&gt;
    &lt;head rend="h4"&gt;Data privacy and security&lt;/head&gt;
    &lt;p&gt;There are the actual issues, and then there are the perceived issues. In terms of actual issues, founders talked about everything from doing considerable engineering (with several complete restarts) to work around large financial services requirements for the data they can or can’t send to LLMs, to getting ISO 27001 certifications to overcome issues with MedTech clients. Yet even if the data is protected, there are perceived issues leading to resistance or slower rollout of agentic solutions. To illustrate with some founder observations:&lt;/p&gt;
    &lt;p&gt;“Data and privacy are not so much as a blocker as a major source of slowing us down.”&lt;/p&gt;
    &lt;p&gt;“Data privacy is not a problem per se, but on occasion we have experienced resistance from senior leadership because of concerns around privacy and security.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Data quality, data infrastructure issues&lt;/head&gt;
    &lt;p&gt;Much like with integration issues, data quality and data infrastructure issues are not new. Almost all the founders in our survey talked about having to do a lot of data clean-up to get to reliable agentic workflows. However, the problem is exacerbated because agentic AI aims to tackle actual tasks performed, and clients typically have poor or outdated documentation of processes. Not to mention that there is lots of embedded knowledge of processes that sits within users’ heads.&lt;/p&gt;
    &lt;head rend="h4"&gt;Infrastructure costs&lt;/head&gt;
    &lt;p&gt;Sam Altman noted in his blog that “The cost to use a given level of AI falls about 10x every 12 months, and lower prices lead to much more use.” Even as the price per token (for a given level of AI) has decreased, the newer cutting-edge reasoning models are more expensive, and the number of tokens consumed has skyrocketed. Epoch AI found that average output length for reasoning models has grown at 5x per year (vs 2.2x for non-reasoning models), and reasoning models exhibit longer response lengths overall – 8x more tokens on average compared to non-reasoning models. And even a simple query may use about 5,000 reasoning tokens internally to return only a 100 token response. The token bloat is a real problem, and the quest for quality (and consistent) model outputs exacerbates the issue, as a founder called out:&lt;/p&gt;
    &lt;p&gt;“Model consistency is a challenge and has implications for infrastructure costs. Infrastructure costs are a balancing act as it limits the tiers we can make agentic flows available. We have found we need a lot of context and multi pass/reasoning models for most real tasks to get at the required reliability in 2025 which could become significant enough to impact margin.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Agentic infrastructure predominantly built in-house&lt;/head&gt;
    &lt;p&gt;We asked founders: “Which third-party AI agent infrastructure solutions do you work with when building, deploying, monitoring the agents? Eg. solutions for memory, tool calling, agentic frameworks, browser infrastructure, agentic payments etc.”&lt;/p&gt;
    &lt;p&gt;Based on their responses, we found that 52% of the founders surveyed built their agentic AI infrastructure in-house (either predominantly or fully). We attribute this primarily to the nascence of the agentic ecosystem.&lt;/p&gt;
    &lt;p&gt;Here are some of the excerpts from our founder survey (click on the carousel arrows to see more quotes):&lt;/p&gt;
    &lt;p&gt;In terms of the third-party tools most frequently cited, ChatGPT and Claude models were most often mentioned along with the Google Agent Development Kit, while LangChain (unsurprisingly) came up as the most popular framework. Other tools that received shout-outs include: frameworks and orchestration platforms (Pydantic, Temporal, Inngest, Pipecat); monitoring, observability and evaluations (Langfuse, Langtrace, Coval); agentic browsers (Browserbase, Browser Use, Strawberry) and vector databases (Qdrant).&lt;/p&gt;
    &lt;head rend="h2"&gt;Observations&lt;/head&gt;
    &lt;p&gt;Based on our 40+ interviews with enterprise practitioners and startup founders, we outline the common approaches taken by startups to successfully deploy AI agents in enterprises.&lt;/p&gt;
    &lt;head rend="h3"&gt;Strategic rollout of use cases&lt;/head&gt;
    &lt;p&gt;The most successful deployment strategies we’ve seen started with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;simple and specific use cases with clear value drivers, that were low risk yet medium impact;&lt;/item&gt;
      &lt;item&gt;weren’t majorly disruptive to existing workflows;&lt;/item&gt;
      &lt;item&gt;preferably automating a task that the human user dislikes (or was outsourced);&lt;/item&gt;
      &lt;item&gt;the output of the workflow can be easily/quickly verified by the human for accuracy or suitability; and&lt;/item&gt;
      &lt;item&gt;demonstrated clear ROI quickly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Given the current levels of technological development, AI Agents work best when narrowly applied to very specific tasks and operating under a specific context. For instance, we’ve seen this in healthcare with revenue cycle management processes (claim and denial management) that health systems were already outsourcing to third-party providers.&lt;/p&gt;
    &lt;p&gt;The land-and-expand strategy for AI agents is very different to traditional SaaS. Given enterprises are increasingly under pressure from the C-Suite to incorporate AI into their work, there are plenty of opportunities for startups to “land” but it’s much harder to “expand” – and not only that, it’s taking much longer to expand even when they want to expand, because it’s a use case by use case rollout. Much like the iconic Volkswagen ad, sometimes it’s better to “Think Small” and build trust first, rather than attempt too many use cases (and excessively complex use cases) right off the bat.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hand-holding and more hand-holding&lt;/head&gt;
    &lt;p&gt;Successful enterprise deployments of agentic AI require significant levels of hand-holding and education. This is primarily because enterprises aren’t often fully clear on the best use cases to apply agentic AI to, the opportunities and limitations of the technology, how best to use the tools, how to redesign workflows… and more critically, how to evaluate and buy agentic AI products.&lt;/p&gt;
    &lt;p&gt;Workshops and consultative GTM: Pre-installation analysis and workshops at the very outset are critical for setting and managing expectations, on everything from identifying areas where agents can or can’t be helpful, to providing clarity upfront on expected usage and pricing. For instance, Health Force (AI Agents that automate daily administrative tasks at hospitals), does an AI Readiness Assessment for free, and helps hospitals identify the workflows where AI agents would be most beneficial. Or Runwhen (AI Agents for developer experience) performs a pre-installation analysis on existing alerts or chats and measures which could be automated via Runwhen. Using a consultative GTM approach also gives the enterprise comfort around the degree of customisability of third-party solutions (every organisation has some workflows unique to them, and incorporating their specific needs is key to driving adoption).&lt;/p&gt;
    &lt;p&gt;Forward Deployed Engineers (FDE) driving adoption forward: A Forward Deployed Engineer (FDE) is a software engineer who works directly with customers, often embedded within their teams, to solve complex, real-world problems – so it’s a hybrid role where an FDE is a software developer, a consultant and a product manager, all rolled into one.&lt;/p&gt;
    &lt;p&gt;Most of the agentic startups we spoke to have found Palantir-style forward deployment useful when selling to enterprises/mid-market clients who have complex data that is fragmented across different data sources. But there other forms of complexity as well, such as product complexity and process complexity, that necessitate a deeper partnership with customers at the very outset to ensure the agentic solutions are achieving the desired outcomes. The more complex the data integration, the product and the business processes, the greater is the need for an FDE to help drive the best outcomes for clients.&lt;/p&gt;
    &lt;p&gt;The human-agent interface and the three E’s (education, entertainment and expectation management): As we observed earlier, our survey suggests that 60% of agentic AI startups struggle with workflow integration and the human-agent interface. Startups such as Strawberry (AI agents on browsers) are focused on building out multiple dimensions of that, such as: (a) moving beyond merely a chatbot-style interface; (b) having the AI agents themselves educate the customers on what they can or can’t do, plus give suggestions on how to better use the product whilst managing expectations; and (c) make the AI Agents fun or engaging to work with. For our part, we were vastly amused by Strawberry’s agents such as LinkedIn Linus or Competition Camille or Data Extraction Denise (as you can see, we have an ardent appreciation for alliteration).&lt;/p&gt;
    &lt;p&gt;Besides educating the customers (in an engaging way) on how to best use agents and manage expectations, founders are also focused on enabling human users to educate the agents, so users can guide the agent’s behaviour to reflect changing priorities and workloads, as well as capture the users’ unique style of working. Users need to enjoy working with the agent enough to evangelise it (clearly, no Clippy!)&lt;/p&gt;
    &lt;head rend="h3"&gt;Positioning&lt;/head&gt;
    &lt;p&gt;A common question we’ve got from agentic AI founders is how to position their products when everyone’s marketing sounds the same. Also, many solutions claim to use Agentic AI; they over-promise and under-deliver, leading to buyer fatigue and scepticism – thus creating a challenging environment for truly high quality agentic AI solutions to cut through the noise. Taking a consultative, collaborative and problem-focused approach that demonstrates real value is critical (which we described above), but so are the various dimensions of positioning (which we discuss below). We fully acknowledge that the positioning is mostly a function of current perceptions and levels of technological development; as these systems see more mainstream acceptance and agents achieve higher levels of autonomy reliably, no doubt the positioning strategies will evolve as well.&lt;/p&gt;
    &lt;p&gt;To mention AI or not to mention AI, that is the question: We’ve observed an interesting dichotomy in positioning strategies. In verticals like Healthcare, founders are actively downplaying the use of agentic AI in their solutions. As two founders in Healthcare observed:&lt;/p&gt;
    &lt;p&gt;“You know what’s weird? If you use the words ‘agent’ or ‘AI’ it actually backlashes more than it benefits. The moment you put AI out to clients, it’s like, ‘oh, here goes a bunch of fluff again.’”&lt;/p&gt;
    &lt;p&gt;“We position more as a mental healthcare company than an agent company to our customers.”&lt;/p&gt;
    &lt;p&gt;However, in verticals such as Financial Services, founders are prominently featuring their “agentic AI” proposition, given the AI-forward positioning resonates with users and buyers. The good news is that in most verticals (outside of healthcare), the “agentic AI” positioning resonates well (provided it meets all the criteria we outlined in the section on “strategic rollout of use cases”).&lt;/p&gt;
    &lt;p&gt;Levels of autonomy: Most founders we spoke to have opted for a co-pilot approach to selling, even if their solutions were capable of higher levels of autonomy. This was mainly done to build trust with the customer. For instance, Juna AI (whose agents optimise complex manufacturing processes in heavy industries) started with a co-pilot approach where the agents give recommendations to the customer on how to optimally run the systems, and the customer still has the option to choose whether or not they implement it. While the idea is to eventually get to higher levels of autonomy (the solution is certainly capable of it), it’s baby steps for now.&lt;/p&gt;
    &lt;p&gt;Most practitioners we spoke to feel like they’re on a learning journey, and would much prefer the co-pilot approach than a fully autonomous one (though again, this depends on 3 factors: the criticality/impact of the task being automated, how easy it is to audit the mistakes the AI may potentially make and catch them before it does any harm, and whether it unlocks an entirely new capability e.g. being able to perform a task a human was never able to do before). However, being able to easily review the AI agent’s outputs were critical.&lt;/p&gt;
    &lt;p&gt;Augmentation, not replacement: Tied to the previous point on lower levels of autonomy, startups that have positioned themselves as “augmenting” rather than replacing existing employees or legacy tech stacks have found it easier to gain a foothold in large enterprises. It’s even better if they’re pitching a net new capability that wasn’t previously possible. From a tech standpoint, rip-and-replace is difficult for customers who have complex downstream workflows built on top of their existing ERPs like SAP and startups (such as askLio in the procurement space) are focused on working with existing technologies to get to faster deployments. From an employee standpoint, we’re not yet at a level where most AI agents are sufficiently reliable or capable of so many automated end-to-end workflows that enterprises could contemplate a true FTE replacement. Or even if both those things were true (tying back to our earlier point around levels of automation) enterprise practitioners are more cautious with highly autonomous deployments.&lt;/p&gt;
    &lt;p&gt;Articulation of value proposition and ROI: We can analyse the issue in two ways: (1) where the value proposition is well understood so it is relatively easier to articulate the ROI; or (2) where AI agents have unlocked entirely new capabilities (so it’s hard to compare to existing solutions) and therefore harder to characterise ROI.&lt;/p&gt;
    &lt;p&gt;Let’s take the first case, where it is easier to understand the use case and articulate the ROI because it’s an established workflow. Here, it’s usually about pitching time and cost savings and/or revenue uplift. For instance, Covecta (AI agents for financial services) talks about 70% time saving on tasks such as drafting detailed credit applications, while Biorce (clinical AI platform that speeds up drug development) talks about ROI both in terms of labour cost savings as well as faster time-to-market (Biorce’s calculation being that one hour spent on its platform saves 720 human hours), with the faster time-to-market itself creating revenue acceleration opportunities. Credit applications and drug discovery are still well understood; but what of entirely new developments such as Generative UI?&lt;/p&gt;
    &lt;p&gt;That brings us to the second case. Startups such as Architect provide AI agents to build, personalise, and optimise your web pages for every visitor – something we would call “Generative UI” because the visual presentation, content and visitor experience of the website changes on the fly depending on who the viewer is. Given the novelty of the solution, it may be challenging to pitch the product, but Architect overcomes this by positioning the product as complementary to ads systems/platforms (like Google AdWords) and measures success through improvement in conversion (emphasising the utility, not just the novelty).&lt;/p&gt;
    &lt;p&gt;Given we backed Synthesia (AI video platform that generates photorealistic performances of avatars) back in 2019, we’ve seen firsthand how startups with highly novel technologies get widespread adoption through emphasising utility over novelty. We don’t expect the agentic AI wave (for net new use cases) to be any different.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting to the desirable end state&lt;/head&gt;
    &lt;p&gt;Today’s AI agents are still (for the most part) reactive, because they are triggered in response to human prompts or explicit user instruction to act. However, in the future we expect to see more ambient agents and proactive agents that initiate tasks by themselves, and can reason more effectively around edge cases so that task execution is robust even under uncertainty. This means that agents need to be adaptable without becoming unreliable, and they need to learn continuously as well as retain those memories over long periods of time (much like how a human colleague learns about your organisation). Today, they operate in more constrained and controlled environments with organisations, but we see agents eventually interacting with “open” environments – engaging with and negotiating with other agents across different organisations, hiring other agents and interacting with the wider world much like a human colleague would.&lt;/p&gt;
    &lt;p&gt;The question is: how do we get there? And we explore that in the next 3 parts of our series on Agentic AI, where we focus on the technologies and startups that are: (a) enabling AI agents to access accurate, relevant and up-to-date information, as well as manage context and memory; (b) enabling agents to perform actions reliably, whether through secure tool execution or agentic browsers to help agents navigate a visual world much like a human would; and (c) ensuring that agents are trustworthy, reliable, and robust to adversarial attacks or unintended failure modes.&lt;/p&gt;
    &lt;p&gt;If you’re a founder building something that gets us closer to the desirable end state, please reach out to Advika, Sevi or Mina – we’d love to chat.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mmc.vc/research/state-of-agentic-ai-founders-edition/"/><published>2025-11-04T07:26:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45808690</id><title>Some software bloat is OK</title><updated>2025-11-04T13:47:53.010646+00:00</updated><content>&lt;doc fingerprint="b34add5ae62d211c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Some software bloat is OK&lt;/head&gt;
    &lt;p&gt;Published on&lt;/p&gt;
    &lt;p&gt;In the era of fast CPUs, gigabytes of RAM, and terabytes of storage software efficiency has become an overlooked thing. Many believe there are less reasons to optimize software today, as processors are fast and memory is cheap. They claim that we should focus on other things, such as developers' efficiency, maintainability, fast prototyping etc. They often recall Donald Knuth's famous quote:&lt;/p&gt;
    &lt;quote&gt;Premature optimization is the root of all evil.&lt;/quote&gt;
    &lt;head rend="h2"&gt;How bad is software bloat nowadays?&lt;/head&gt;
    &lt;p&gt;Historically computers had much less computing power and memory. CPUs and memory were expensive. Programmers were much more constrained by the CPU speed and available memory. A lot of work had to be done in order to fit the program into these limited resources. It's no surprise that for 1970-80s era programs it was a normal thing to be written in very low-level languages, such as machine code or assembly as they give the programmers ultimate control over every byte and processor instruction used.&lt;/p&gt;
    &lt;p&gt;Over time, memory and CPUs became cheaper (all the hardware overall). This reduced the constraints and allowed to use higher level languages and eventually led to the rise of languages with garbage collection (Java, C#, PHP, Python, JavaScript, etc).&lt;/p&gt;
    &lt;p&gt;Sometimes when you compare the system requirements of older and newer generation software, you become shocked. Just compare the system requirements of, let's say Windows 11 Calculator alone (let alone the full OS), and the whole Windows 95 OS! Windows 11 Calculator alone consumes over 30MiB of RAM (even this might be an underestimation because shared memory is not included), while Windows 95 could work even with 4MiB of RAM.&lt;/p&gt;
    &lt;p&gt;Here is another more dramatic example. Super Mario Bros. game was just 31KiB (or 40KiB, still doesn't change much anything) and used only 2KiB of RAM. But this high quality (preserved pixels from original, lossless) WebP image below is almost 54KiB. Larger than the game itself!&lt;/p&gt;
    &lt;head rend="h2"&gt;A significant part of this bloat is actually a tradeoff&lt;/head&gt;
    &lt;p&gt;From the first glance this seems insane, the developers probably don't care about efficiency, right? Well, it's more complicated, and actually a significant part of this isn't caused by the developers' incompetence or laziness.&lt;/p&gt;
    &lt;p&gt;Today's software solves some problems that were less of a concern in 1990s and before. Here are some examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Layers &amp;amp; frameworks. Modern software tends to be more complicated. Most of the time people don't write software from scratch, they usually use some library or framework. For example, the mentioned Windows 11 Calculator is a UWP / WinUI / XAML app with C++ / WinRT. You're loading a UI framework, layout engine, localization, input, vector icons, theming, high-DPI, etc. Those shared DLLs live somewhere in RAM even if Task Manager only shows part of them in the app's "Working Set".&lt;/item&gt;
      &lt;item&gt;Security &amp;amp; isolation. Nowadays security is very important. Sandboxes, code integrity, ASLR, CFG, data execution prevention, etc. add processes, mappings, and metadata. This didn't exist much in older era software.&lt;/item&gt;
      &lt;item&gt;Robustness &amp;amp; error handling / reporting. Related to the previous one. Since modern software is usually complicated and has tons of edge cases (and sometimes integrated with other third parties), this requires to handle and log all the possible errors / failures. All these safety measures also add extra code.&lt;/item&gt;
      &lt;item&gt;Globalization &amp;amp; accessibility. Full Unicode, RTL, complex script support, screen readers, keyboard navigation, high-contrast / animations bring code and resources.&lt;/item&gt;
      &lt;item&gt;Containers &amp;amp; virtualization. Bundle the app with its exact runtime/deps into an image, run it as a container on any host with Docker. Same container in dev, CI, staging, prod means fewer environment drift bugs. VMs are great when in addition to this something should be run on an OS that is not compatible with the host OS.&lt;/item&gt;
      &lt;item&gt;Engineering trade-offs. We accept a larger baseline to ship faster, safer code across many devices. Hardware grew ~three orders of magnitude. Developer time is often more valuable than RAM or CPU cycles. Also modern software is usually developed by many people (sometimes even organizations) which adds a need of a code structure that makes it easier to maintain and collaborate, such as modularity, extensibility / flexibility, code / architectural patterns, etc. I mean just try to add some feature to an ancient ultra-optimized game written entirely in assembly by some hobbyist. Without the help of the author it will probably be a very challenging task: the code will be much harder to understand (more like reverse engineer), the changes might break some "clever but fragile" code, etc. We certainly don't want that.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;But yeah, a significant part of the bloat is also bad&lt;/head&gt;
    &lt;p&gt;That being said, a lot of this bloat is also not a result of a tradeoff, but incompetence or laziness. For example, using libraries or frameworks for trivial things or not understanding algorithmic complexity. Many websites are notoriously bloated by having dozens (sometimes hundreds) of questionable dependencies, they don't only degrade performance, they can cause security issues and maintenance problems. And nowadays AI is a multiplier of such problems.&lt;/p&gt;
    &lt;p&gt;Another source of bloat is over-engineering:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microservices for a tiny app.&lt;/item&gt;
      &lt;item&gt;Generic plugin systems.&lt;/item&gt;
      &lt;item&gt;DI forests.&lt;/item&gt;
      &lt;item&gt;"Just in case" interfaces that are used once.&lt;/item&gt;
      &lt;item&gt;SPA + global state for static content (heavy hydration for simple pages).&lt;/item&gt;
      &lt;item&gt;Multiple build steps/tools for marginal gains.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, this obsession with containers is also concerning. Containers often cause increased startup time (quite long even on SSDs), RAM and CPU usage (looking at you, Ubuntu Snap). Sadly, containers are very appealing as an easy crutch for mitigating compatibility and security issues for ordinary desktop apps.&lt;/p&gt;
    &lt;head rend="h2"&gt;"Bottlenecks" still exist and are still optimized&lt;/head&gt;
    &lt;p&gt;Many programs still have some small but critical areas of code that need some level of optimization. This might be some database query or a long running function.&lt;/p&gt;
    &lt;p&gt;There are still highly demanded optimized programs or parts of such programs which won't disappear any time soon. Here is a small fraction of such software:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Codecs: dav1d (AV1), x264 / x265 (H.264 / 265), libjpeg-turbo.&lt;/item&gt;
      &lt;item&gt;Archivers / compressors: zstd, xz / LZMA, brotli, libarchive.&lt;/item&gt;
      &lt;item&gt;VMs / JITs: HotSpot (JIT tiers + GC tuning), V8 / JavaScriptCore (inline caches, TurboFan), LuaJIT (trace JIT).&lt;/item&gt;
      &lt;item&gt;Std libs: glibc, musl (small, predictable), modern C++ STL (small-buffer opt, node vs flat containers), Rust std (zero-cost iterators).&lt;/item&gt;
      &lt;item&gt;Crypto: OpenSSL / BoringSSL / LibreSSL (constant-time, vectorized primitives).&lt;/item&gt;
      &lt;item&gt;GPU drivers.&lt;/item&gt;
      &lt;item&gt;Game engines.&lt;/item&gt;
      &lt;item&gt;OS kernels.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Such software will always exist, it just moved to some niche or became a lower level "backbone" of other higher level software.&lt;/p&gt;
    &lt;head rend="h2"&gt;Too late optimization is also a source of evil&lt;/head&gt;
    &lt;p&gt; While premature optimization by sacrificing design and correctness for hypothetical gains is harmful, delaying it is also bad. You still need to choose the right algorithms and architecture early on since wrong decisions might bite you later. The choice between &lt;code&gt;O(N)&lt;/code&gt; and &lt;code&gt;O(N2)&lt;/code&gt;
		algorithms
		is usually still important in the early
		stages.
	&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Some bloat is actually OK and it has benefits. Without some bloat we would not have so much innovation. Sure, we could make many things ultra-optimized, we could reduce many apps' sizes by a factor of 10 - 100. But most of the time it will be just an unworthy exchange of the developers' time for a Pyrrhic victory. On the other hand, like most other things, the bloat becomes harmful when it's not in moderation which we also see. Thus, too late optimization can be a problem as well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://waspdev.com/articles/2025-11-04/some-software-bloat-is-ok"/><published>2025-11-04T08:27:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45808918</id><title>Show HN: Yourshoesmells.com – Find the most smelly boulder gym</title><updated>2025-11-04T13:47:52.784902+00:00</updated><content>&lt;doc fingerprint="7b37ec77fee53b6e"&gt;
  &lt;main&gt;
    &lt;p&gt;💨 Stinky ✨ Login 💨 TOP 5 Stink 💨 Top 5 Stinky Gyms 💨 Top 5 Stinky Gyms 💡 Request a feature Feedback ☕ Buy me a coffee Support 🔒 Register / Login ✕ Username Letters, numbers, underscores, or hyphens only; 3-20 characters Password at least 6 characters Register 🌍 Farthest Gym Calculating... 💨 Stinkiest Gym Calculating... 🏋️ Gyms Visited Loading... 📍 Gyms by Region Loading chart... 🔒 Setup / Reset Password ✕ ✏️ Cast Your Vote ✕&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yourshoesmells.com"/><published>2025-11-04T09:11:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45808998</id><title>Bloom filters are good for search that does not scale</title><updated>2025-11-04T13:47:52.575905+00:00</updated><content>&lt;doc fingerprint="7b8d83e54decc003"&gt;
  &lt;main&gt;
    &lt;p&gt;A great blog post from 2013 describes using bloom filters to build a space-efficient full text search index for small numbers of documents. The algorithm is simple: Per document, create a bloom filter of all its words. To query, simply check each document's bloom filter for the query terms.&lt;/p&gt;
    &lt;p&gt;With a query time complexity of O(number-of-documents), we can forget about using this on big corpuses, right? In this blog post I propose a way of scaling the technique to large document corpuses (e.g. the web) and discuss why that is a bad idea.&lt;/p&gt;
    &lt;p&gt;Fun fact: There is a nice implementation of this exact algorithm that is still used in the wild. But let's get into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why even try this?&lt;/head&gt;
    &lt;p&gt;The bloom filter index's big selling point is its small size. It allows static websites with dozens of pages to ship a full text search index to the client that is as small as a small image. An equivalent inverted index, which is the traditional textbook approach for keyword-based full text search, would be multiple times bigger.&lt;/p&gt;
    &lt;p&gt;But index size is not only relevant on small blog websites. If we could scale this technique to larger document corpuses and achive similar space savings, that would be huge!&lt;/p&gt;
    &lt;p&gt;The main thing that seems to stand in our way is query performance. Instead of always checking every document's bloom filter, we will try to construct an index that only checks a small subset of filters, but still finds all matching documents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Ideas that don't work at all&lt;/head&gt;
    &lt;p&gt;Look, I brainstormed a bunch of ideas for how to improve the bloom filter based index. I will quickly go over two of them, because identifying and discarding ideas that will not work is an important part of science and engineering.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sort the filters&lt;/head&gt;
    &lt;p&gt;If we sort the filters by some metric, for example by the most to least significant bits, then we can use a binary search algorithm or something like that, right? - Wrong.&lt;/p&gt;
    &lt;p&gt;We can construct a simple counter example to show that this does not work. Here the query is matched by the first and the last filter in a sorted list of filters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tree of filters&lt;/head&gt;
    &lt;p&gt;Plain sorting does not work, but what if we structure our big set of filters into a tree? Imagine it sort of like this, but much bigger.&lt;/p&gt;
    &lt;p&gt;At each branch node, we construct an aggregate filter that encodes all documents that are reachable from the branch. Aggregate filters are constructed by a simple bitwise or of the other filters like this.&lt;/p&gt;
    &lt;p&gt;When we get a query, we first check it against the top level branch filters. If a filter does not match e.g. the filter for document 6-10, we can discard that entire branch of the tree for this query.&lt;/p&gt;
    &lt;p&gt;Ideally we would like to search as few branches of the tree as possible to improve performance. How many branches we do need to search, depends heavily on the partitioning of the documents. Intuitively, we can think of it like this: branch A should contain all documents that contain the words "dog", "cat", "bird". Branch B should contain documents with "car", "bus", "plane". The fewer branches each word is contained in the better.&lt;/p&gt;
    &lt;p&gt;Here comes the problem: What if there is a document that says "I took my cat on the bus today"?&lt;lb/&gt; This breaks our assumtion above, and suddenly for the query "bus" we need to search both branches. You can imagine this happening for almost every word in the dicionary across all branches, because language is complex and lets us say so many different things in many different contexts.&lt;/p&gt;
    &lt;p&gt;Or in other words: Text documents are high-dimensional.&lt;lb/&gt; I recommend reading about the curse of dimensionality for an intuition of what issues this implies. Here it means that it is basically impossible to cluster text documents into disjunct subsets without significant overlap. For our seach index that means that even when using this tree, we would still need to search almost every document for every query.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inverted Index of Bloom Filters&lt;/head&gt;
    &lt;p&gt;The problem with our previous tree-based idea is that there is so much overlap between text documents. But I know one book that only contains every work exactly once: The dictionary. We can construct a search tree of the entire dictionary, again based on bloom filters. Each leaf represents a set of words. At each leaf we keep a list of pointers to every document's filter that contains one of those words.&lt;/p&gt;
    &lt;p&gt;Maybe not so incidentally, this looks a lot like an inverted index. And it works! For any query term we can walk the tree to the leaf that contains the query term and then we match only against the filters at that leaf. Instead of a hash table, as in the inverted index, our index uses a tree for the dictionary, but fundamentally it does a similar thing.&lt;/p&gt;
    &lt;p&gt;The big difference is that the tree can be smaller than the hash table. Remember, size is the main reason to attempt this at all. Not only is there no empty space in a tree, but we also encode all the words in our bloom filters instead of storing them outright. Modern bloom filters (actually called Xor filters) require about ten bits per element [1], much less than the 8 bits per character required to store a full word.&lt;/p&gt;
    &lt;p&gt;As an aside, bloom filters are indeed already used in full text search for large-ish datasets, but in the form of skip-indexes. In a skip index, a bloom filter is used to quickly check if a large chunk of data contains a value (e.g. a word) at all. That way, a database can avoid reading chunks of data that do not contain any records for a given query. Until very recently this technique was used by the Clickhouse OLAP system for full text search [2]. It has been superseeded by a proper inverted index in 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why all of this is still a bad idea&lt;/head&gt;
    &lt;p&gt;We did it! We have a working idea for a bloom filter based search index that works for large document corpuses. The query time complexity is not as good as for an inverted index, but it is logarithmic with the number of documents. That is good enough if you ask me. So why do I write that it is still a bad idea?&lt;/p&gt;
    &lt;p&gt;Let's think about what allows the bloom filter based index to be small again. Instead of storing the entire dictionary in our index, we use bloom filters that require about ten bits per word. Ten bits per word. Ten bits per every word. Not unique word. Every word in our document corpus (except duplicates in the same document). To make our math exceedingly simple, let's say the english dictionary has about 500 Thousand unique words and every document contains 1000 distinct words. At ten bits per entry for a bloom filter, that makes each document's filter about 1.25kb. Assume that words are on average ten characters long, then the dictionary will require 5mb for the text alone. We can assume another 4mb for the inverted index's hash table to get a lower bound of 9mb for the inverted index. Both indexes require similar amounts of space for document ids and pointers, so relative to the inverted index, our bloom filter index grows by 1.25kb per document. Divide 9mb by 1.25kb and you find out that at only 7200 documents the inverted index becomes more space efficient than the bloom filter index. Of course the real numbers will be different and we are ignoring some things here, but the trend will stay the same.&lt;/p&gt;
    &lt;p&gt;What is going on here is that while an inverted index must store every word in the dictionary exactly once, sharing the space when a word is reused, bloom filters do not share space amongst each other. Every document's bloom filter must encode all words in the document from scratch. If a word is contained in thousands of documents, that requires much more space than simply storing the word in plain text.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;When you have a small number of documents relative to the size of your dictionary, bloom filters can indeed achieve a much smaller full text search index than is possible traditionally.&lt;/p&gt;
    &lt;p&gt;Bloom filters are space efficient when compressing a large dictionary into a small number of filters. As more filters share the same dictionary, this efficiency decreases. Intuitively this is because bloom filters cannot share information amongst each other. Each filter must encode its entire dictionary from scratch. An inverted index does not do this. It only stores the dictionary once and shares it for all documents, so it gets more space efficient with the number of documents.&lt;/p&gt;
    &lt;p&gt;More generally, there is no synergy between bloom filters. Each filter on its own is efficient, but as a whole system, a different approach might be more efficient. We can transfer this insight to other problem domains as well. For example, imagine a content moderation system on a social media platform that allows blocking individual accounts. If we have one global blocklist on our platform, a bloom filter can be an efficient (though maybe not ideal) implementation of this. But allow every user to create their own blocklist and a different design will be more scaleable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://notpeerreviewed.com/blog/bloom-filters/"/><published>2025-11-04T09:25:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45809193</id><title>What is a manifold?</title><updated>2025-11-04T13:47:52.391606+00:00</updated><content>&lt;doc fingerprint="ba91984004c1afe9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What Is a Manifold?&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Standing in the middle of a field, we can easily forget that we live on a round planet. We’re so small in comparison to the Earth that from our point of view, it looks flat.&lt;/p&gt;
    &lt;p&gt;The world is full of such shapes — ones that look flat to an ant living on them, even though they might have a more complicated global structure. Mathematicians call these shapes manifolds. Introduced by Bernhard Riemann in the mid-19th century, manifolds transformed how mathematicians think about space. It was no longer just a physical setting for other mathematical objects, but rather an abstract, well-defined object worth studying in its own right.&lt;/p&gt;
    &lt;p&gt;This new perspective allowed mathematicians to rigorously explore higher-dimensional spaces — leading to the birth of modern topology, a field dedicated to the study of mathematical spaces like manifolds. Manifolds have also come to occupy a central role in fields such as geometry, dynamical systems, data analysis and physics.&lt;/p&gt;
    &lt;p&gt;Today, they give mathematicians a common vocabulary for solving all sorts of problems. They’re as fundamental to mathematics as the alphabet is to language. “If I know Cyrillic, do I know Russian?” said Fabrizio Bianchi, a mathematician at the University of Pisa in Italy. “No. But try to learn Russian without learning Cyrillic.”&lt;/p&gt;
    &lt;p&gt;So what are manifolds, and what kind of vocabulary do they provide?&lt;/p&gt;
    &lt;head rend="h2"&gt;Ideas Taking Shape&lt;/head&gt;
    &lt;p&gt;For millennia, geometry meant the study of objects in Euclidean space, the flat space we see around us. “Until the 1800s, ‘space’ meant ‘physical space,’” said José Ferreirós, a philosopher of science at the University of Seville in Spain — the analogue of a line in one dimension, or a flat plane in two dimensions.&lt;/p&gt;
    &lt;p&gt;In Euclidean space, things behave as expected: The shortest distance between any two points is a straight line. A triangle’s angles add up to 180 degrees. The tools of calculus are reliable and well defined.&lt;/p&gt;
    &lt;p&gt;But by the early 19th century, some mathematicians had started exploring other kinds of geometric spaces — ones that aren’t flat but rather curved like a sphere or saddle. In these spaces, parallel lines might eventually intersect. A triangle’s angles might add up to more or less than 180 degrees. And doing calculus can become a lot less straightforward.&lt;/p&gt;
    &lt;p&gt;The mathematical community struggled to accept (or even understand) this shift in geometric thinking.&lt;/p&gt;
    &lt;p&gt;But some mathematicians wanted to push these ideas even further. One of them was Bernhard Riemann, a shy young man who had originally planned to study theology — his father was a pastor — before being drawn to mathematics. In 1849, he decided to pursue his doctorate under the tutelage of Carl Friedrich Gauss, who had been studying the intrinsic properties of curves and surfaces, independent of the space surrounding them.&lt;/p&gt;
    &lt;p&gt;In 1854, Riemann was required to deliver a lecture to secure a teaching position at the University of Göttingen. His assigned topic: the foundations of geometry. On June 10, despite a fear of public speaking, he described a new theory in which he generalized Gauss’ ideas about the geometry of surfaces to an arbitrary number of dimensions (and even to infinite dimensions).&lt;/p&gt;
    &lt;p&gt;Gauss was immediately impressed with the lecture, which involved not just math but also philosophy and physics. But most mathematicians found Riemann’s ideas too vague and abstract to be of much use. “Many scientists and philosophers were saying, ‘This is nonsense,’” Ferreirós said. And so, for decades, the work was largely ignored. Riemann’s lecture didn’t appear in print until 1868, two years after his death.&lt;/p&gt;
    &lt;p&gt;But by the end of the 19th century, mathematical greats like Henri Poincaré had recognized the importance of Riemann’s ideas. And in 1915, Albert Einstein used them in his general theory of relativity, bringing them out of the realm of philosophical abstraction and into the real world. By the middle of the 20th century, they had become a mathematical staple.&lt;/p&gt;
    &lt;p&gt;Riemann had introduced a concept that could encompass all possible geometries, in any number of dimensions. A concept that would change how mathematicians view space.&lt;/p&gt;
    &lt;p&gt;A manifold.&lt;/p&gt;
    &lt;head rend="h2"&gt;Charted Territory&lt;/head&gt;
    &lt;p&gt;The term “manifold” comes from Riemann’s Mannigfaltigkeit, which is German for “variety” or “multiplicity.”&lt;/p&gt;
    &lt;p&gt;A manifold is a space that looks Euclidean when you zoom in on any one of its points. For instance, a circle is a one-dimensional manifold. Zoom in anywhere on it, and it will look like a straight line. An ant living on the circle will never know that it’s actually round. But zoom in on a figure eight, right at the point where it crosses itself, and it will never look like a straight line. The ant will realize at that intersection point that it’s not in a Euclidean space. A figure eight is therefore not a manifold.&lt;/p&gt;
    &lt;p&gt;Similarly, in two dimensions, the surface of the Earth is a manifold; zoom in far enough anywhere on it, and it’ll look like a flat 2D plane. But the surface of a double cone — a shape consisting of two cones connected at their tips — is not a manifold.&lt;/p&gt;
    &lt;p&gt;Manifolds address a problem that mathematicians would otherwise have to deal with: A shape’s properties can change depending on the nature and dimension of the space it lives in (and how it sits in that space). For instance, lay a piece of string on a table, and connect its ends without lifting it. You’ll get a simple loop. Now hold the string in the air and tie its ends together. By considering the string in three dimensions, you can pass it over and under itself before you connect the ends, creating all sorts of knots beyond the simple loop. They all represent the same one-dimensional manifold — the looped string — but they have different properties when considered in two versus three dimensions.&lt;/p&gt;
    &lt;p&gt;Mathematicians avoid such ambiguities by focusing on the manifold’s intrinsic properties. The defining property of manifolds — that at any point, they look Euclidean — is immensely helpful on that front. Because it’s possible to think about any small patch of the manifold in terms of Euclidean space, mathematicians can use traditional calculus techniques to, say, compute its area or volume, or describe movement on it.&lt;/p&gt;
    &lt;p&gt;To do this, mathematicians divide a given manifold into several overlapping patches and represent each with a “chart” — a set of some number of coordinates (equal to the manifold’s dimension) that tell you where you are on the manifold. Crucially, you also need to write down rules that describe how the coordinates of overlapping charts relate to one another. The collection of all these charts is called an atlas.&lt;/p&gt;
    &lt;p&gt;You can then use this atlas — whose charts translate smaller regions of your potentially complicated manifold into familiar Euclidean space — to measure and explore the manifold one patch at a time. If you want to understand how a function behaves on a manifold, or get a sense of its global structure, you can break the problem up into pieces, solve each piece on a different chart, in Euclidean space, and then stitch together the results from all the charts in the atlas to get the full answer you’re seeking.&lt;/p&gt;
    &lt;p&gt;Today, this approach is ubiquitous throughout math and physics.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manifold Uses&lt;/head&gt;
    &lt;p&gt;Manifolds are crucial to our understanding of the universe, for one. In his general theory of relativity, Einstein described space-time as a four-dimensional manifold, and gravity as that manifold’s curvature. And the three-dimensional space we see around us is also a manifold — one that, as manifolds do, appears Euclidean to those of us living within it, even though we’re still trying to figure out its global shape.&lt;/p&gt;
    &lt;p&gt;Even in cases where manifolds don’t seem to be present, mathematicians and physicists try to rewrite their problems in the language of manifolds to make use of their helpful properties. “So much of physics comes down to understanding geometry,” said Jonathan Sorce, a theoretical physicist at Princeton University. “And often in surprising ways.”&lt;/p&gt;
    &lt;p&gt;Consider a double pendulum, which consists of one pendulum hanging from the end of another. Small changes in the double pendulum’s initial conditions lead it to carve out very different trajectories through space, making its behavior hard to predict and understand. But if you represent the configuration of the pendulum with just two angles (one describing the position of each of its arms), then the space of all possible configurations looks like a doughnut, or torus — a manifold. Each point on this torus represents one possible state of the pendulum; paths on the torus represent the trajectories the pendulum might follow through space. This allows researchers to translate their physical questions about the pendulum into geometric ones, making them more intuitive and easier to solve. This is also how they study the movements of fluids, robots, quantum particles and more.&lt;/p&gt;
    &lt;p&gt;Similarly, mathematicians often view the solutions to complicated algebraic equations as a manifold to better understand their properties. And they analyze high-dimensional datasets — such as those recording the activity of thousands of neurons in the brain — by looking at how those data points might sit on a lower-dimensional manifold.&lt;/p&gt;
    &lt;p&gt;Asking how scientists use manifolds is akin to asking how they use numbers, Sorce said. “They are at the foundation of everything.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/what-is-a-manifold-20251103/"/><published>2025-11-04T09:58:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45809897</id><title>This Month in Ladybird – October 2025</title><updated>2025-11-04T13:47:52.170010+00:00</updated><content>&lt;doc fingerprint="e203cc51a9875bc0"&gt;
  &lt;main&gt;
    &lt;p&gt;October is in the bag, and we merged 217 PRs from 43 contributors. Let’s look at some highlights!&lt;/p&gt;
    &lt;head rend="h3"&gt;Welcoming new sponsors&lt;/head&gt;
    &lt;p&gt;Ladybird is entirely funded by the generous support of companies and individuals who believe in the open web. This month, we’re excited to welcome the following new sponsors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Axeptio with $10,000 (their announcement)&lt;/item&gt;
      &lt;item&gt;Gravwell with $5,000&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’re incredibly grateful for their support. If you’re interested in sponsoring the project, please contact us.&lt;/p&gt;
    &lt;head rend="h3"&gt;Web Platform Tests (WPT)&lt;/head&gt;
    &lt;p&gt;We’ve continued to make solid progress on WPT this month. There has been a significant increase in passing subtests, with 111,431 new passing subtests bringing our total to 1,964,649 .&lt;/p&gt;
    &lt;p&gt;The majority of this increase comes from a large update to the test suite itself, with 100,751 subtests being added - mainly due to the Wasm core tests being updated to Wasm 3.0.&lt;/p&gt;
    &lt;p&gt;This bump has seen us pass over 90% of all WPT subtests for the first time! An important milestone, as this is one of the criteria Apple uses to determine eligibility for alternative browser engines on iOS.&lt;/p&gt;
    &lt;p&gt;For context, here are the current top 6 browser engines and their WPT scores today vs. one month ago.&lt;/p&gt;
    &lt;head rend="h3"&gt;HTTP disk cache&lt;/head&gt;
    &lt;p&gt;Last month, we enabled our in-memory HTTP cache. This cache lives in a single WebContent process, and does not persist across browser restarts.&lt;/p&gt;
    &lt;p&gt;We’ve now begun work on a persistent disk cache! (PRs #6435, #6487, #6579, and #6645). This provides a second layer of caching, which will make visits to the same websites much faster.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance work&lt;/head&gt;
    &lt;p&gt;We’ve made a number of performance improvements this month. Some examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;JS string performance optimizations (PRs #6396, #6400, #6401)&lt;/item&gt;
      &lt;item&gt;JS object property assignment optimization (PR #6413)&lt;/item&gt;
      &lt;item&gt;Inline caching for JS object property access in C++ (PR #6474)&lt;/item&gt;
      &lt;item&gt;Faster add/subtract/multiply of two Int32 values when result overflows (PR #6414)&lt;/item&gt;
      &lt;item&gt; Ignore repaints inside iframes with &lt;code&gt;visibility: hidden&lt;/code&gt;(PR #6555)&lt;/item&gt;
      &lt;item&gt;Discard heavy UTF-32 buffers after parsing HTML (PR #6561)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Trusted Types in the DOM&lt;/head&gt;
    &lt;p&gt;Continuing work from previous months, Trusted Types support was added to several DOM APIs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HTMLIFrameElement&lt;/code&gt;:&lt;code&gt;srcdoc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Element&lt;/code&gt;:&lt;code&gt;setAttribute&lt;/code&gt;,&lt;code&gt;setAttributeNS&lt;/code&gt;,&lt;code&gt;setHTMLUnsafe&lt;/code&gt;,&lt;code&gt;innerHTML&lt;/code&gt;,&lt;code&gt;outerHTML&lt;/code&gt;, and&lt;code&gt;insertAdjacentHTML&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Document&lt;/code&gt;:&lt;code&gt;write&lt;/code&gt;,&lt;code&gt;writeln&lt;/code&gt;, and&lt;code&gt;parseHTMLUnsafe&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ShadowRoot&lt;/code&gt;:&lt;code&gt;setHTMLUnsafe&lt;/code&gt;and&lt;code&gt;innerHTML&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DOMParser&lt;/code&gt;:&lt;code&gt;parseFromString&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Range&lt;/code&gt;:&lt;code&gt;createContextualFragment&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Worker&lt;/code&gt;,&lt;code&gt;SharedWorker&lt;/code&gt;, and&lt;code&gt;ServiceWorker&lt;/code&gt;constructors&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Initial support for XPath evaluations&lt;/head&gt;
    &lt;p&gt;Some basic support for &lt;code&gt;XPathEvaluator&lt;/code&gt;, &lt;code&gt;XPathExpression&lt;/code&gt;, &lt;code&gt;XPathResult&lt;/code&gt;, &lt;code&gt;XPathNSResolver&lt;/code&gt;, and &lt;code&gt;Document.evaluate&lt;/code&gt; was added this month.&lt;/p&gt;
    &lt;p&gt;It uses libxml2 under the hood to run XPath evaluations on an XML tree compatible with the library that is created from our parsed XML tree. (PR #6342)&lt;/p&gt;
    &lt;p&gt;This is a short-term solution to bring initial support to these classes, because there are differences between the DOM and XML implementations of XPath that are not possible to work around with libxml2.&lt;/p&gt;
    &lt;p&gt;This enables the htmx library to work in Ladybird:&lt;/p&gt;
    &lt;head rend="h3"&gt;Audio/Video synchronization, multi-track support&lt;/head&gt;
    &lt;p&gt;Previously, our media playback implementation relied on separate, unrelated audio and video playback systems to play media files where both are present. This meant that it was possible for audio and video to desynchronize very easily, especially when their seeking behavior was inconsistent.&lt;/p&gt;
    &lt;p&gt;That implementation has now been replaced with a single new system that controls audio and video playback together, ensuring they remain in sync at all times. (PR #6410)&lt;/p&gt;
    &lt;p&gt;As part of this reimplementation, several aspects of playback have improved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seeking is done asynchronously, and can therefore complete much faster. Scrubbing the timeline now previews the new playback position instantly, instead of waiting for the mouse button to be released.&lt;/item&gt;
      &lt;item&gt;Media elements now support the audioTracks and videoTracks properties, which allow scripts to interact with multiple audio or video tracks in a file, toggling them at will. The playback system mixes any enabled audio tracks into one output.&lt;/item&gt;
      &lt;item&gt;The fastSeek() function now follows spec with regard to seeking forward when the target is later than the current position.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Pinch-to-zoom support&lt;/head&gt;
    &lt;p&gt;Ladybird now supports pinch-to-zoom on macOS (AppKit UI). The &lt;code&gt;VisualViewport&lt;/code&gt; API exposes the zoom transform.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accessibility DevTools&lt;/head&gt;
    &lt;p&gt;Since switching away from our own inspector to using Firefox’s developer tools, we’ve been missing a few features. This month we reimplemented inspection of the accessibility tree - a structure parallel to the usual element tree that represents how tools like screen readers see the page. We don’t yet support such tools, but being able to examine what’s going on inside Ladybird will make that easier. (PR #6462)&lt;/p&gt;
    &lt;head rend="h3"&gt;CSS Typed OM progress&lt;/head&gt;
    &lt;p&gt;Continuing from last month, we’re still working on the CSS Typed OM API, which allows authors to get and set style properties using objects instead of strings. Ladybird now supports setting style with most types of &lt;code&gt;CSSStyleValue&lt;/code&gt;.
(PRs #6370, #6378, #6440, and #6466)&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;sibling-count()&lt;/code&gt; and &lt;code&gt;sibling-index()&lt;/code&gt; &lt;/head&gt;
    &lt;p&gt;These are relatively new CSS functions that enable authors to style or animate elements differently depending on how many siblings they have and where they appear in that list. This month we implemented support for these. (PRs #6426 and #6526)&lt;/p&gt;
    &lt;head rend="h3"&gt;CSS and SVG gradients&lt;/head&gt;
    &lt;p&gt;This month we’ve improved support for both CSS repeating linear gradients and SVG gradients with coordinates expressed in &lt;code&gt;objectBoundingBox&lt;/code&gt; units. This results in many such gradients looking crisper and more correct! (PRs
6533,
6592)&lt;/p&gt;
    &lt;head rend="h3"&gt;CanvasPattern&lt;/head&gt;
    &lt;p&gt;We had preliminary support in place for &lt;code&gt;CanvasPattern&lt;/code&gt;, a way to create an image pattern based on an arbitrary image
source such as a canvas, an &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; or even a video frame. We did not actually use this pattern in drawing commands
though, causing some significant graphical artifacts. We do now, which causes the background on
slither.io to finally render correctly!
(PR #6548)&lt;/p&gt;
    &lt;head rend="h3"&gt;Improved WebGL2 buffer support&lt;/head&gt;
    &lt;p&gt;We’ve improved support for the WebGL2 &lt;code&gt;.readPixels()&lt;/code&gt; method and implemented the &lt;code&gt;.getBufferSubData()&lt;/code&gt; method, which
are used to transfer pixel data between arbitrary buffers by libraries such as SparkJS. (PR
#6637)&lt;/p&gt;
    &lt;head rend="h3"&gt;Improved offscreen WebGL framebuffer support&lt;/head&gt;
    &lt;p&gt;An issue with offscreen framebuffers being cleared after frame presentation caused the demo on the PixiJS homepage not to display the masked/warped image of stars/nebulae. This has been fixed, so the demo appears correctly now:&lt;/p&gt;
    &lt;head rend="h3"&gt;WebGL progress on Google Maps&lt;/head&gt;
    &lt;p&gt;There were some targeted WebGL fixes for Google Maps Globe View, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Respecting UNPACK_FLIP_Y_WEBGL&lt;/item&gt;
      &lt;item&gt;Returning null for missing uniforms instead of -1&lt;/item&gt;
      &lt;item&gt;Calculating the correct pitch for 16-bit textures&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This resulted in far fewer visual issues in Globe View; the sky still does not render and remains the biggest outstanding issue.&lt;/p&gt;
    &lt;p&gt;(PR #6558)&lt;/p&gt;
    &lt;head rend="h3"&gt;Progress on Windows support&lt;/head&gt;
    &lt;p&gt;While Windows support is not the focus for the alpha release in 2026, the community has continued to improve it.&lt;/p&gt;
    &lt;p&gt;This month, Ladybird ran on Windows for the first time:&lt;/p&gt;
    &lt;p&gt;Here’s the Gamepad API working on Windows as well:&lt;/p&gt;
    &lt;p&gt;(PR #5981)&lt;/p&gt;
    &lt;head rend="h3"&gt;Credits&lt;/head&gt;
    &lt;p&gt;We’d like to thank everyone who contributed code this month:&lt;/p&gt;
    &lt;p&gt;Adam Patterson, Adrian Kiezik, Ali Mohammad Pur, Aliaksandr Kalenik, Amish K. Naidu, Andreas Kling, aplefull, ayeteadoe, Ben Eidson, Callum Law, caztanj, Dave-London, Dowsley, Feng Yu, InvalidUsernameException, Jan Koudijs, Jelle Raaijmakers, Johannes Gustafsson, Julian Dominguez-Schatz, Junior Rantila, ljamar, Lorenz A, lukasxcs, Luke Wilde, Manuel Zahariev, mikiubo, Pavel Shliak, Psychpsyo, R-Goc, rmg-x, Rocco Corsi, Sam Atkins, ste, stelar7, Tete17, Tim Ledbetter, Timothy Flynn, Undefine, vedant-pandey, Zaggy1024&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ladybird.org/newsletter/2025-10-31/"/><published>2025-11-04T11:52:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45809932</id><title>Former US Vice-President Cheney Dies</title><updated>2025-11-04T13:47:51.776543+00:00</updated><content>&lt;doc fingerprint="6bd3811191b3d28a"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 4 (Reuters) - Dick Cheney, a driving force behind the U.S. invasion of Iraq in 2003, was considered by presidential historians as one of the most powerful vice presidents in U.S. history.&lt;/p&gt;
    &lt;p&gt;He died at age 84 on Monday from complications of pneumonia and cardiac and vascular disease, his family said in a statement on Tuesday.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;The Republican - a former Wyoming congressman and secretary of defense - was already a major Washington player when then-Texas governor George W. Bush chose him to be his running mate in the 2000 presidential race that Bush went on to win.&lt;/p&gt;
    &lt;p&gt;As vice president from 2001 to 2009, Cheney fought vigorously for an expansion of the power of the presidency, having felt that it had been eroding since the Watergate scandal that drove his one-time boss Richard Nixon from office. He also expanded the clout of the vice president's office by putting together a national security team that often served as a power center of its own within the administration.&lt;/p&gt;
    &lt;p&gt;Cheney was a strong advocate for the 2003 invasion of Iraq and was among the most outspoken of Bush administration officials warning of the danger from Iraq's alleged stockpile of weapons of mass destruction. No such weapons were found.&lt;/p&gt;
    &lt;p&gt;He clashed with several top Bush aides, including Secretaries of State Colin Powell and Condoleezza Rice, and defended "enhanced" interrogation techniques of terrorism suspects that included waterboarding and sleep deprivation. Others, including the U.S. Senate Select Committee on Intelligence and the U.N. special rapporteur on counter terrorism and human rights, called these techniques "torture."&lt;/p&gt;
    &lt;p&gt;His daughter Liz Cheney also became an influential Republican lawmaker, serving in the House of Representatives but losing her seat after opposing Republican President Donald Trump and voting to impeach him in the wake of the January 6, 2021 attack on the Capitol by his supporters. Her father, who agreed with her, said that he would vote for Democratic candidate Kamala Harris in 2024.&lt;/p&gt;
    &lt;p&gt;"In our nation's 248 year-history, there has never been an individual who is a greater threat to our republic than Donald Trump," said the man who had long been a foe of the left.&lt;/p&gt;
    &lt;p&gt;Cheney was troubled much of his life by heart problems, suffering the first of a number of heart attacks at age 37. He had a heart transplant in 2012.&lt;/p&gt;
    &lt;head rend="h2"&gt;TAKING ON IRAQ&lt;/head&gt;
    &lt;p&gt;Cheney and Defense Secretary Donald Rumsfeld, who had been colleagues in the Nixon White House, were key voices pushing for the March 2003 invasion of Iraq.&lt;/p&gt;
    &lt;p&gt;In the run-up to the war, Cheney suggested there might be links between Iraq and al Qaeda and the September 11, 2001 attacks on the United States. A commission on the 9/11 attacks later discredited this theory.&lt;/p&gt;
    &lt;p&gt;Cheney predicted U.S. forces would be "greeted as liberators" in Iraq and that the troop deployment - which would last around a decade - would "go relatively quickly ... weeks rather than months."&lt;/p&gt;
    &lt;p&gt;Although no weapons of mass destruction were found, Cheney in later years insisted that the invasion was the right decision based on the intelligence at the time and the removal of Iraqi President Saddam Hussein from power.&lt;/p&gt;
    &lt;p&gt;More than a decade earlier, as defense secretary under President George H.W. Bush, Cheney had directed the U.S. military operation to expel an Iraqi occupation army from Kuwait in the first Gulf War.&lt;/p&gt;
    &lt;p&gt;He urged Bush senior to take an uncompromising line against Iraq after Saddam Hussein sent his troops to occupy Kuwait in August 1990. But at that point Cheney did not support an invasion of Iraq, saying the United States would have to act alone and that the situation would become a quagmire.&lt;/p&gt;
    &lt;p&gt;Because of Cheney's long ties to the Bush family and experience in government, George W. Bush chose him to head his vice presidential search in 2000. Bush then decided the man doing the search was the best candidate for the job.&lt;/p&gt;
    &lt;p&gt;Upon his re-entry into politics, Cheney received a $35 million retirement package from oil services firm Halliburton, which he had run from 1995 to 2000. Halliburton became a leading government contractor during the Iraq war. Cheney's oil industry links were a subject of frequent criticism by opponents of the war.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE FIRST REPUBLICAN IN GENERATIONS&lt;/head&gt;
    &lt;p&gt;Richard Bruce Cheney was born in Lincoln, Nebraska, to Marjorie Lorraine (née Dickey) and Richard Herbert Cheney on January 30, 1941, the day then-President Franklin Roosevelt turned 59. His mother was a waitress turned softball player, his father a federal worker with the Soil Conservation Service.&lt;/p&gt;
    &lt;p&gt;Both sides of the family were staunch New Deal Democrats, he wrote in his 2011 book "In My Time: A Personal and Political Memoir."&lt;/p&gt;
    &lt;p&gt;Convinced that the president would want to know that he shared a birthday with the newborn, Cheney's grandfather urged Marjorie and Richard to share the news by telegram with the White House.&lt;/p&gt;
    &lt;p&gt;In his family he "was the first Republican probably since my great-grandfather who fought in the Civil War on the Union side," he told the PBS documentary "Dick Cheney: A Heartbeat Away."&lt;/p&gt;
    &lt;p&gt;He moved as a boy to Wyoming with his family, before attending Yale University. "I was a mediocre student, at best," he said. He dropped out.&lt;/p&gt;
    &lt;head rend="h2"&gt;'A DEADLY ALLERGY TO OLIVE DRAB'&lt;/head&gt;
    &lt;p&gt;Back in Wyoming in 1962, he worked on building electrical transmission lines and coal-fired power stations, before eventually earning undergraduate and master's degrees in political science from the University of Wyoming.&lt;/p&gt;
    &lt;p&gt;Of that time he recalled a visit by then President John F. Kennedy, who addressed students on the importance of using what they were learning to build a better nation and a better world. "He had inspired us all, and at a time when I was trying to put my life back together, I was particularly grateful for the sense of elevated possibilities he described," Cheney wrote in his memoir.&lt;/p&gt;
    &lt;p&gt;In his 20s, Cheney strongly disagreed with the students who shut down campuses in protest against the Vietnam War, he recalled in his memoir. "As a general proposition, I supported our troops in Vietnam and the right of the Kennedy and Johnson administrations to make the decision to be involved there," he wrote. He himself was never drafted.&lt;/p&gt;
    &lt;p&gt;According to his biographer, John Nichols, Cheney repeatedly applied for deferments and exemptions to avoid conscription. "Cheney reacted to the prospect of wearing his country’s uniform like a man with a deadly allergy to olive drab," Nichols wrote in The Nation magazine in 2011. Cheney stated that he would have been happy to serve.&lt;/p&gt;
    &lt;head rend="h2"&gt;EMBRACING DARTH VADER&lt;/head&gt;
    &lt;p&gt;Cheney went to Washington in 1969 as a congressional intern and held various White House jobs during the Republican administrations of Nixon and Gerald Ford. One of his earliest mentors was Rumsfeld, who worked as secretary of defense in both the Ford and George W. Bush administrations. When Cheney became Ford's chief of staff, he succeeded Rumsfeld.&lt;/p&gt;
    &lt;p&gt;During the 10 years he served as Wyoming's only congressman, Cheney had a highly conservative record, consistently voting against abortion rights. He also voted against the release of imprisoned South African leader Nelson Mandela and against gun control and environmental and education funding measures.&lt;/p&gt;
    &lt;p&gt;His wife Lynne, who had been his high school sweetheart, became a conservative voice on cultural issues. Liz, the couple's eldest daughter, was elected to the House in 2016 after building a reputation for pushing hawkish foreign policy views similar to her father's.&lt;/p&gt;
    &lt;p&gt;During his time as vice president, late-night television comedians referred to Cheney as Darth Vader. He shrugged it off by joking that he was honored to be compared to the "Star Wars" villain, even dressing as Vader for an appearance on the "Tonight Show" to promote his memoir.&lt;/p&gt;
    &lt;head rend="h2"&gt;'THANK YOU TO SATAN'&lt;/head&gt;
    &lt;p&gt;Even before the rise of Trump, his support for conservative issues was not uniform. His second daughter, Mary, a Republican fundraiser, is a lesbian. Cheney spoke supportively of same-sex relationships, which put him at odds with the Bush administration's push for a constitutional amendment against gay marriage. That amendment ultimately failed.&lt;/p&gt;
    &lt;p&gt;Mary and Liz both survive him, as does Lynne. All three were with him as he died, the family said.&lt;/p&gt;
    &lt;p&gt;In 2006 he made headlines during a hunting trip in Texas when he accidentally wounded his friend, Texas lawyer Harry Wittington, in the face with a spray of birdshot.&lt;/p&gt;
    &lt;p&gt;Controversy continued to dog Cheney even after he left the Bush administration. He was the subject of a scathing biographical film in 2018 titled "Vice," starring Christian Bale, who gained 40 pounds (18 kg) and shaved his head to mimic the former vice president's paunchiness and baldness.&lt;/p&gt;
    &lt;p&gt;"Thank you to Satan for giving me inspiration on how to play this role," Bale said in accepting a Golden Globes award for his Cheney portrayal.&lt;/p&gt;
    &lt;p&gt;During a book tour for his memoir, Cheney seemed to relish raising the ire of critics. Just before its release he gleefully predicted it would leave heads "exploding" all over Washington.&lt;/p&gt;
    &lt;p&gt;He devoted parts of the book to settling scores with former colleagues such as Rice, whom he depicted as naive. Cheney also took aim at then-President Barack Obama's world view, puzzling over the Democrat's concern that the U.S. military prison at Guantanamo Bay in Cuba was harmful to America's image.&lt;/p&gt;
    &lt;p&gt;Additional reporting by Steve Holland in Washington and Gursimran Kaur in Bengaluru; Editing by Olivier Holmey, Bill Trott and Rosalba O'Brien&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.reuters.com/world/us/former-us-vp-dick-cheney-dead-84-punchbowl-news-says-2025-11-04/"/><published>2025-11-04T11:56:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45810127</id><title>Ask HN: Why are most status pages delayed?</title><updated>2025-11-04T13:47:51.475755+00:00</updated><content>&lt;doc fingerprint="44523e5705035050"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;As I type this, Reddit is down. My own requests return 500s, Down Detector reports that there is an outage but Reddit itself says all systems operational.&lt;/p&gt;
      &lt;p&gt;This is a pattern that I have noticed time and time again with many services. Why even have a status page if it is not going to be accurate in real time? It's also not uncommon that smaller issues never get acknowledged.&lt;/p&gt;
      &lt;p&gt;Is this a factor of how Atlassian Statuspage works?&lt;/p&gt;
      &lt;p&gt;Edit: Redditstatus finally acknowledged the issue as of 04:27 PST, a good 20+ minutes after Down Detector charts show the spike&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45810127"/><published>2025-11-04T12:27:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45810430</id><title>Chaining FFmpeg with a Browser Agent</title><updated>2025-11-04T13:47:51.128227+00:00</updated><link href="https://100x.bot/a/chaining-ffmpeg-with-browser-agent"/><published>2025-11-04T12:52:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45810676</id><title>The Art of Atari (2016)</title><updated>2025-11-04T13:47:50.688024+00:00</updated><content>&lt;doc fingerprint="ab7d1b5d8689ed71"&gt;
  &lt;main&gt;
    &lt;p&gt;ATARI is one of the most recognized names in the world. Since its formation in 1972, the company pioneered hundreds of iconic titles including ASTEROIDS, CENTIPEDE and MISSILE COMMAND.&lt;/p&gt;
    &lt;p&gt;Asteroids&lt;/p&gt;
    &lt;p&gt;Centipede&lt;/p&gt;
    &lt;p&gt;Casino&lt;/p&gt;
    &lt;p&gt;Gravitar&lt;/p&gt;
    &lt;p&gt;Maze Craze&lt;/p&gt;
    &lt;p&gt;Radar Lock&lt;/p&gt;
    &lt;p&gt;RealSports: Football&lt;/p&gt;
    &lt;p&gt;RealSports: Tennis&lt;/p&gt;
    &lt;p&gt;Missile Command&lt;/p&gt;
    &lt;p&gt;Star Ship&lt;/p&gt;
    &lt;p&gt;Swordquest&lt;/p&gt;
    &lt;p&gt;Video Pinball&lt;/p&gt;
    &lt;p&gt;In addition to hundreds of games created for arcades, home video systems and computers, original artwork was specially commissioned to enhance the Atari experience to further entice children and adults to embrace and enjoy the new era of electronic entertainment.&lt;/p&gt;
    &lt;p&gt;Star Raiders&lt;/p&gt;
    &lt;p&gt;Super Baseball&lt;/p&gt;
    &lt;p&gt;Defender&lt;/p&gt;
    &lt;p&gt;Space Invaders&lt;/p&gt;
    &lt;p&gt;Outlaw&lt;/p&gt;
    &lt;p&gt;Steeple Chase&lt;/p&gt;
    &lt;p&gt;Pole Position&lt;/p&gt;
    &lt;p&gt;Millipede&lt;/p&gt;
    &lt;p&gt;Missle Command&lt;/p&gt;
    &lt;p&gt;Cartridge Patent&lt;/p&gt;
    &lt;p&gt;Warlords&lt;/p&gt;
    &lt;p&gt;Asteroids&lt;/p&gt;
    &lt;p&gt;THE ART OF ATARI is the first official collection of such artwork. Sourced from private collections worldwide, this book spans over 40 years of the companys unique illustrations used in packaging, advertisements, catalogs, and more.&lt;/p&gt;
    &lt;p&gt;Includes a comprehensive retrospective collecting game production and concept artwork, photos, marketing art, with insight from key people involved in Ataris rich history, and behind-the-scenes details on how dozens of games featured within were conceived, illustrated, approved (or rejected), and brought to life!&lt;/p&gt;
    &lt;p&gt;TIM LAPETINO&lt;/p&gt;
    &lt;p&gt;Atari is a touchstone for many people. Their games and game system exposed many to video games for the first time. Whether youre a fan, collector, enthusiast, or new to the world of Atari, this book offers the most complete collection of artwork ever produced!&lt;/p&gt;
    &lt;p&gt;Available through your local independent book store.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://www.artofatari.com"/><published>2025-11-04T13:15:52+00:00</published></entry></feed>