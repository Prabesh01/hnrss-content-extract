<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-22T13:24:35.920330+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46708601</id><title>TrustTunnel: AdGuard VPN protocol goes open-source</title><updated>2026-01-22T13:24:42.869474+00:00</updated><content>&lt;doc fingerprint="3c516d5bcf33e1d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We‚Äôve kept our promise: AdGuard VPN protocol goes open-source ‚Äî meet TrustTunnel&lt;/head&gt;
    &lt;p&gt;Today is a big day for us, and for everyone who cares about transparency, privacy, and having full control over their own traffic. We‚Äôre finally open-sourcing the protocol that powers AdGuard VPN. And it now has a name: TrustTunnel.&lt;/p&gt;
    &lt;p&gt;For a long time, we‚Äôve wanted to make the protocol public. Many of you asked for it, and we always said: yes, we will, it‚Äôs only a matter of time. Well, the time has come.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is TrustTunnel?&lt;/head&gt;
    &lt;p&gt;At its core, TrustTunnel is a modern, secure, mobile-optimized VPN protocol. It‚Äôs the very same technology that has been running inside all AdGuard VPN apps: on mobile, desktop, and browser extensions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why TrustTunnel? Because we needed something better&lt;/head&gt;
    &lt;p&gt;There are plenty of VPN protocols out there, so why create our own, some might ask. That is because we‚Äôve seen in practice the faults of popular VPN protocols, especially in countries with tight restrictions on internet access. Protocols like OpenVPN, WireGuard, and IPSec share common weaknesses: they are easy to detect and block at the network level, and attempts to conceal VPN traffic often reduce speed. Traditional approaches ‚Äúwrap‚Äù VPN data in a TCP connection and mimic normal web traffic, but TCP‚Äôs way of confirming every piece of data creates delays and makes the connection slower.&lt;/p&gt;
    &lt;p&gt;Unlike those conventional VPN protocols, TrustTunnel is engineered to blend in with regular HTTPS traffic, making it far harder to throttle or block and helping it slip past deep-packet inspection, all while preserving strong privacy and security. It achieves this through TLS-based encryption, the same standard that secures HTTPS, and by leveraging HTTP/2 or HTTP/3 transport, which are ubiquitous on the web. Each connection runs on its own dedicated stream, which combines packets for faster, more efficient transmission. It is also optimized for mobile platforms and performs well even in unstable network conditions.&lt;/p&gt;
    &lt;head rend="h2"&gt;A protocol you can use, run, tweak, extend, and build upon&lt;/head&gt;
    &lt;p&gt;By releasing TrustTunnel, we hope to achieve two things. First of all, we want to finally show our users what protocol is powering AdGuard VPN, thus allowing them to audit it openly. At AdGuard, we have always been staunch supporters of the idea of open-source software, and many of our products have long been open source. AdGuard VPN was lagging behind in this regard, but with TrustTunnel being released publicly, it is starting to catch up.&lt;/p&gt;
    &lt;p&gt;But most importantly, we want to change the status quo in the world of VPN protocols and offer an alternative to existing solutions. That said, we do not want it to be just a PR stunt, when the protocol‚Äôs code is de-facto ‚Äòopen source,‚Äô but only one VPN service actually runs it. We believe in free and open-source software (FOSS) and want TrustTunnel to be used widely, including by other VPN services. We believe this is the right way to go about open source development, and we hope the community will participate in the TrustTunnel evolution. We welcome any contribution, whether it is a feature request, a bug report, or even a direct contribution to the app‚Äôs development.&lt;/p&gt;
    &lt;p&gt;What have we done to make this possible?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We are publishing the first version of the TrustTunnel specification.&lt;/item&gt;
      &lt;item&gt;We are releasing the complete code of our reference implementation of the TrustTunnel server and its clients under a very permissive license.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don‚Äôt have to install AdGuard VPN to use TrustTunnel. You can configure your own server and use open source TrustTunnel clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Command-line TrustTunnel clients support Linux, Windows, and macOS&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We are also releasing two client apps for iOS and Android&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TrustTunnel clients already have a lot of functionality, they allow you to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Use flexible routing rules to decide which requests go through the tunnel and which stay on the local network&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Exercise fine-grained control, separating work and personal traffic, routing specific domains or apps, and tuning network behavior without complicated setup&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Benefit from a real-time request log that provides full transparency into where the device sends traffic, how routing rules apply, and which connections use the tunnel&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Useful links&lt;/head&gt;
    &lt;p&gt;This is a long-awaited moment for us. We promised to open-source our protocol, and today we‚Äôre delivering on that promise. With TrustTunnel now open source, users and developers alike can explore, self-host, and build on the technology.&lt;/p&gt;
    &lt;p&gt;To get started, check out the following resources:&lt;lb/&gt; TrustTunnel website&lt;lb/&gt; TrustTunnel open-source repository on GitHub&lt;lb/&gt; TrustTunnel app for iOS&lt;lb/&gt; TrustTunnel app for Android&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adguard-vpn.com/en/blog/adguard-vpn-protocol-goes-open-source-meet-trusttunnel.html"/><published>2026-01-21T17:21:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708678</id><title>Waiting for dawn in search: Search index, Google rulings and impact on Kagi</title><updated>2026-01-22T13:24:42.611637+00:00</updated><content>&lt;doc fingerprint="19c2dbe203a4134e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Waiting for dawn in search: Search index, Google rulings and impact on Kagi&lt;/head&gt;
    &lt;p&gt;This blog post is a follow-up to Dawn of a new era in Search, published last year. A lot has changed: the legal case has advanced, AI has become the central battleground, and the need for open index access has only grown sharper.&lt;/p&gt;
    &lt;p&gt;As of late 2025, one company decides what nearly 9 out of 10 people see when they search the web: Google. On August 5, 2024, a U.S. court officially ruled that Google is a monopolist in general search services. This ruling is not about ads or browser defaults alone. It is about who controls the index that powers both search and AI - and whether anyone else is allowed to build on it.&lt;/p&gt;
    &lt;p&gt;The stakes have grown sharper over the past year. LLMs hallucinate without grounding in real-world information; every agent that answers questions about the real world, depends on search. LLMs themselves are a blend of proprietary and open source. Cloud compute is competitive. But search is different - only one company controls a comprehensive, fresh, high-quality web index. If one company controls the index, it controls the floor on how good AI can be - and who gets to build it. The innovation crunch in search is now an innovation crunch in AI.&lt;/p&gt;
    &lt;p&gt;We are writing this from a position we believe in: people should have the choice to access information without behaviour-changing, ad-driven, intermediary standing between them and knowledge.&lt;/p&gt;
    &lt;p&gt;Why does this matter? The information we consume shapes our understanding of the world as profoundly as the food we eat shapes our bodies. Search (directly, and indirectly through AI) is the primary mechanism through which we inform political judgments, financial decisions, medical choices, and countless other consequential aspects of our lives. When a single company controls the gateway to information - and operates that gateway in ways misaligned with user interests - it influences not only what we know, but how we reason.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem: A search monopoly&lt;/head&gt;
    &lt;p&gt;The data is stark.&lt;/p&gt;
    &lt;p&gt;Worldwide search market share (October 2025, StatCounter):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Search Engine&lt;/cell&gt;
        &lt;cell role="head"&gt;Market Share&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;90.06%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Bing&lt;/cell&gt;
        &lt;cell&gt;4.31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;1.84%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yahoo&lt;/cell&gt;
        &lt;cell&gt;1.45%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DuckDuckGo&lt;/cell&gt;
        &lt;cell&gt;0.89%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Baidu&lt;/cell&gt;
        &lt;cell&gt;0.73%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The United States is similar: Google at 85%, Bing at 9%, everyone else in the noise.&lt;/p&gt;
    &lt;p&gt;This is not a competitive market. It is a monopoly with a distant second place.&lt;/p&gt;
    &lt;p&gt;The search index is irreplaceable infrastructure. Building a comparable one from scratch is like building a parallel national railroad. Microsoft spent roughly $100 billion over 20 years on Bing and still holds single-digit share. If Microsoft cannot close the gap, no startup can do it alone.&lt;/p&gt;
    &lt;p&gt;This is exactly what the Sherman Act was designed to address: when one company‚Äôs control of critical infrastructure prevents effective competition, regulators must force open access on fair terms.&lt;/p&gt;
    &lt;p&gt;When a single, ad-driven gatekeeper controls the primary way humans reach information, it is not just competition that suffers - it is our collective ability to learn, to make informed medical and economic choices, and to participate meaningfully in democratic life.&lt;/p&gt;
    &lt;p&gt;As Ian Bremmer put it: ‚ÄúThe idea that we get our information as citizens through algorithms determined by the world‚Äôs largest advertising company is my definition of dystopia.‚Äù&lt;/p&gt;
    &lt;p&gt;Google‚Äôs own founders knew this. In their 1998 white paper, Sergey Brin and Larry Page sharply criticized the ad-supported search model for creating mixed motives and biasing results toward advertisers‚Äô interests. They wrote that ‚Äúadvertising funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers‚Äù and that ‚Äúadvertising income often provides an incentive to provide poor quality search results.‚Äù Those concerns have only grown more pressing as search has become the primary interface between humanity and the web.&lt;/p&gt;
    &lt;head rend="h2"&gt;We tried to do it the right way&lt;/head&gt;
    &lt;p&gt;Kagi has always tried to integrate the best sources of knowledge into one coherent, ad-free experience. We see ourselves as connective tissue: letting people reach high-quality information directly, without passing through an ad system whose incentives are misaligned with their needs.&lt;/p&gt;
    &lt;p&gt;We approached every major index vendor seeking direct licensing on FRAND terms (Fair, Reasonable, And Non-Discriminatory): fair pricing, no mandatory ad syndication, ability to reorder and blend results. We succeeded with many, including:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Vendor&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Mojeek&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Brave&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wikipedia&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TripAdvisor&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yelp&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Apple&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wolfram Alpha&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Our own Small Web Index&lt;/cell&gt;
        &lt;cell&gt;Proprietary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;With Google and Bing, we failed - not for lack of trying.&lt;/p&gt;
    &lt;p&gt;Bing: Their terms didn‚Äôt work for us from the start. Microsoft‚Äôs terms prohibited reordering results or merging them with other sources - restrictions incompatible with Kagi‚Äôs approach. In February 2023, they announced price increases of up to 10x on some API tiers. Then in May 2025, they retired the Bing Search APIs entirely, effective August 2025, directing customers toward AI-focused alternatives like Azure AI Agents.&lt;/p&gt;
    &lt;p&gt;Google: Google does not offer a public search API. The only available path is an ad-syndication bundle with no changes to result presentation - the model Startpage uses. Ad syndication is a non-starter for Kagi‚Äôs ad-free subscription model.[^1]&lt;/p&gt;
    &lt;head rend="h2"&gt;The current interim approach&lt;/head&gt;
    &lt;p&gt;Because direct licensing isn‚Äôt available to us on compatible terms, we - like many others - use third-party API providers for SERP-style results (SERP meaning search engine results page). These providers serve major enterprises (according to their websites) including Nvidia, Adobe, Samsung, Stanford, DeepMind, Uber, and the United Nations.&lt;/p&gt;
    &lt;p&gt;This is not our preferred solution. We plan to exit it as soon as direct, contractual access becomes available. There is no legitimate, paid path to comprehensive Google or Bing results for a company like Kagi. Our position is clear: open the search index, make it available on FRAND terms, and enable rapid innovation in the marketplace.&lt;/p&gt;
    &lt;head rend="h2"&gt;The DOJ ruling&lt;/head&gt;
    &lt;p&gt;The Google antitrust case began in 2020. On August 5, 2024, the court ruled Google violated Section 2 of the Sherman Act by unlawfully maintaining its monopoly through exclusive distribution agreements. (Full ruling)&lt;/p&gt;
    &lt;p&gt;On September 2, 2025, the DOJ announced remedies (press release):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limits on exclusivity: Google is prohibited from exclusive contracts related to Search, Chrome, Assistant, and Gemini.&lt;/item&gt;
      &lt;item&gt;Data sharing and syndication: Google must provide search index and interaction data to competitors and offer syndication services to help rivals build competitive search.&lt;/item&gt;
      &lt;item&gt;Addressing monopolization tactics: The remedies aim to dismantle a decade of exclusionary agreements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In December 2025, Judge Mehta issued a memorandum outlining the specific remedies the court intends to impose. The details are significant:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mandatory syndication: Google must offer query-based search syndication to ‚ÄúQualified Competitors‚Äù on terms no less favorable than those provided to current partners.&lt;/item&gt;
      &lt;item&gt;No ad bundling: Google cannot condition access to search results on the use of Google Ads; competitors are free to monetize via their own ads or third parties.&lt;/item&gt;
      &lt;item&gt;Index data access: Google must provide Web Search Index data (URLs, crawl metadata, spam scores) at marginal cost.&lt;/item&gt;
      &lt;item&gt;Duration: The judgment remains in effect for 6 years, with syndication licenses guaranteed for terms of 5 years.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If implemented as outlined, this is exactly what we have been asking for. The legal trajectory is promising. Google will contest details, and final enforceable terms are still being worked out. The fight now is ensuring these remedies become real, practical access - not paper compliance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why enforcement matters now&lt;/head&gt;
    &lt;p&gt;Even as these remedies take shape, Google is moving to close the back door. In December 2025, Google sued SerpApi for scraping its results at scale.&lt;/p&gt;
    &lt;p&gt;We take a measured, principled view:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context matters: Google built its index by crawling the open web before robots.txt was a widespread norm, often over publishers‚Äô objections. Today, publishers ‚Äúconsent‚Äù to Google‚Äôs crawling because the alternative - being invisible on a platform with 90% market share - is economically unacceptable. Google now enforces ToS and robots.txt against others from a position of monopoly power it accumulated without those constraints. The rules Google enforces today are not the rules it played by when building its dominance.&lt;/item&gt;
      &lt;item&gt;The structural problem remains: This lawsuit is only necessary because Google refuses to offer legitimate, paid index access.&lt;/item&gt;
      &lt;item&gt;Our position is unchanged: We have always wanted direct licensing. We would happily pay market rates for clean, contractual access. The fact that we - and companies like Stanford, Nvidia, Adobe, and the United Nations - have had to rely on third-party vendors is a symptom of the closed ecosystem, not a preference.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The connection to DOJ remedies is direct: if Google is going to close the back door, regulators must ensure the front door is open. That is exactly what the DOJ‚Äôs index syndication requirements are meant to achieve - and why we support their full implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;What could be: A layered search ecosystem&lt;/head&gt;
    &lt;p&gt;The DOJ ruling does not itself create a healthy market, but it makes one possible.&lt;/p&gt;
    &lt;p&gt;And while this post focuses on remedies and their impact on Kagi, it is worth zooming out: even if those remedies work perfectly, long-term societal prosperity and resilience require a non-commercial baseline for access to information - something that is not dependent on ad incentives or a single vendor‚Äôs business priorities. Think of it as a north-star model for a modern society where information access is a fundamental right.&lt;/p&gt;
    &lt;p&gt;Here is what that could look like:&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 1: Search as a public good&lt;/head&gt;
    &lt;p&gt;This is a long-term possibility, not a near-term expectation. A government-backed, ad-free, intermediary-free, taxpayer-funded search service providing baseline, non-discriminatory access to information. Imagine search.org.&lt;/p&gt;
    &lt;p&gt;This is not something the DOJ remedies create directly, nor something Kagi expects to exist soon. It is included here to make explicit what an open-index world could ultimately make possible.&lt;/p&gt;
    &lt;p&gt;This layer would replace the role public libraries played for centuries - a role that effectively disappeared when commercial web search took over in the late 1990s. Our ancestors understood well the benefits that non-discriminatory, direct access to information brings to citizens, and ultimately society itself.&lt;/p&gt;
    &lt;p&gt;It raises hard questions: governance, funding, political independence, precedent. But the principle is sound. Every citizen should have access to information without an ad-optimized algorithm standing between them and knowledge. If we can fund public libraries, we can fund public search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 2: Free, ad-based search&lt;/head&gt;
    &lt;p&gt;Commercial search engines with richer features, funded by advertising. Users understand the tradeoff and have a genuine public alternative. This is the space where most contemporary search engines operate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 3: Paid, subscription-based search&lt;/head&gt;
    &lt;p&gt;Premium search engines offering the highest possible quality, privacy, and advanced features for users who value this and are willing to pay. This is where Kagi operates - and where we are expanding as an integrator of knowledge across search, browser, mail, and AI assistants, without selling your attention.&lt;/p&gt;
    &lt;p&gt;This layered model creates a diverse ecosystem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A public baseline for information access.&lt;/item&gt;
      &lt;item&gt;Commercial free options for convenience and reach.&lt;/item&gt;
      &lt;item&gt;Premium paid options for those who want maximum quality and control.&lt;/item&gt;
      &lt;item&gt;Aligns with the primary purpose of the Sherman Act.[^2]&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The DOJ ruling is starting to do what antitrust is supposed to do: turn a closed, private choke point into shared infrastructure that others can build on. If the remedies land as real, usable access (APIs, cost-based pricing, no ad bundling), the web can support a layered ecosystem again: a public baseline for citizens, free ad-supported products for reach, and paid services that compete on quality, privacy, and power-user features.&lt;/p&gt;
    &lt;p&gt;That is the world we are building Kagi for. We are ready to walk through the front door - not depend on gray-market workarounds. Our job now is to be ready when the door opens, and to help make sure it does: keep Kagi genuinely multi-source, keep investing in our Small Web Index, and keep shipping a subscription search experience that delivers the best results across providers. If we get this right, the next decade of search and AI does not have to be one funnel owned by one company. It can be a competitive stack of layers that treats information access as the public good it has always been.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;head rend="h4"&gt;DOJ v. Google&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UNITED STATES OF AMERICA v. GOOGLE LLC, 1:20-cv-03010 √¢ Full case docket on CourtListener&lt;/item&gt;
      &lt;item&gt;Memorandum Opinion √¢ Judge Amit Mehta (PDF) √¢ Court ruling finding Google violated antitrust law&lt;/item&gt;
      &lt;item&gt;Department of Justice Wins Significant Remedies Against Google √¢ DOJ press release announcing remedies, September 2, 2025&lt;/item&gt;
      &lt;item&gt;Judge Mehta‚Äôs Remedies Memorandum (PDF) √¢ December 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Market data and commentary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search Engine Market Share Worldwide √¢ StatCounter, October 2025&lt;/item&gt;
      &lt;item&gt;Search Engine Market Share United States √¢ StatCounter, October 2025&lt;/item&gt;
      &lt;item&gt;Ian Bremmer on algorithmic information access √¢ Commentary on ad-driven search&lt;/item&gt;
      &lt;item&gt;The Anatomy of a Large-Scale Hypertextual Web Search Engine √¢ Original Google white paper by Brin &amp;amp; Page, Stanford, 1998&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Third-party search API providers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Google lobs lawsuit at search result scraping firm √¢ Ars Technica coverage of Google‚Äôs litigation&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;[^1]: A note on Google‚Äôs existing APIs: Google offers PSE, designed for adding search boxes to websites. It can return web results, but with reduced scope and terms tailored for that narrow use case. More recently, Google offers Grounding with Google Search through Vertex AI, intended for grounding LLM responses. Neither is general-purpose index access. Programmable Search Engine is not designed for building competitive search. Grounding with Google Search is priced at $35 per 1,000 requests - economically unviable for search at scale, and structured as an AI add-on rather than standalone index syndication. These are not the FRAND terms the market needs.&lt;/p&gt;
    &lt;p&gt;[^2]: Our understanding of the primary purpose of the Sherman Act is not to shield competitors from the success of legitimate businesses or to prevent those businesses from earning fair profits. Rather, it is to preserve a competitive marketplace that protects consumers from harm (see Competition law and consumer protection, Kluwer Law International, pp. 291√¢293). Opening the search index would create healthy, real, and intense competition in the search space - including competition to Kagi - which aligns with our understanding of the Sherman Act‚Äôs intent. The goal is not the elimination of dominant firms, but the prevention of a single, closed index from becoming the only gateway to information.&lt;/p&gt;
    &lt;p&gt;Published by Vladimir Prelovac and Raghu Murthi on January 21, 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.kagi.com/waiting-dawn-search"/><published>2026-01-21T17:28:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46709543</id><title>Show HN: Rails UI</title><updated>2026-01-22T13:24:42.267450+00:00</updated><content>&lt;doc fingerprint="7d1af0212bdc2360"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stop fighting CSS and build beautiful Rails apps faster&lt;/head&gt;
    &lt;p&gt;No more ugly Rails apps. Get professional-looking components and themes that work perfectly with Rails‚Äîno design skills required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Booking date&lt;/cell&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Payout&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;June 1, 2026&lt;/p&gt;
          &lt;p&gt;7:38 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cozy Mountain A-Frame&lt;/cell&gt;
        &lt;cell&gt;$1,165.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Aug 3, 2026&lt;/p&gt;
          &lt;p&gt;7:38 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mountain Vista Chalet&lt;/cell&gt;
        &lt;cell&gt;$2,846.46&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Jul 18, 2026&lt;/p&gt;
          &lt;p&gt;4:30 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cozy Mountain A-Frame&lt;/cell&gt;
        &lt;cell&gt;$1,326.36&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Pro&lt;/p&gt;
    &lt;p&gt;Active subscriber&lt;/p&gt;
    &lt;p&gt;Monthly&lt;/p&gt;
    &lt;p&gt;Sarah updated deal status to "Qualified"&lt;/p&gt;
    &lt;p&gt;2 hours ago&lt;/p&gt;
    &lt;p&gt;New contact added: John Smith&lt;/p&gt;
    &lt;p&gt;Yesterday&lt;/p&gt;
    &lt;p&gt;Acme Corporation&lt;/p&gt;
    &lt;p&gt;Enterprise Plan ‚Ä¢ 12 team members&lt;/p&gt;
    &lt;p&gt;Next invoice: $299/mo&lt;/p&gt;
    &lt;p&gt;Due on Feb 1, 2025&lt;/p&gt;
    &lt;head rend="h1"&gt;Sign in to your account&lt;/head&gt;
    &lt;p&gt;Or sign up for an account&lt;/p&gt;
    &lt;head rend="h2"&gt;Enhanced User Authentication System&lt;/head&gt;
    &lt;p&gt;A small-batch cycle to build a refreshed authentication flow.&lt;/p&gt;
    &lt;p&gt;Components&lt;/p&gt;
    &lt;head rend="h3"&gt;Components that make your Rails app look professional&lt;/head&gt;
    &lt;p&gt;No design experience? No problem. Copy-paste beautiful forms, buttons, and layouts that work perfectly with Rails. Focus on your business logic‚Äîwe've got the pretty stuff covered.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Accordion&lt;/item&gt;
      &lt;item&gt;Alert&lt;/item&gt;
      &lt;item&gt;Badge&lt;/item&gt;
      &lt;item&gt;Button&lt;/item&gt;
      &lt;item&gt;Card&lt;/item&gt;
      &lt;item&gt;Dropdown&lt;/item&gt;
      &lt;item&gt;Modal&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;How do I import contacts from a CSV file?&lt;/head&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;Can I customize my deal pipeline stages?&lt;/head&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;How do automated follow-up reminders work?&lt;/head&gt;
    &lt;head rend="h3"&gt;Time to launch&lt;/head&gt;
    &lt;p&gt;The beta is no more. &lt;lb/&gt;We are ready to go live.&lt;/p&gt;
    &lt;p&gt;Themes&lt;/p&gt;
    &lt;head rend="h3"&gt; Complete app designs that don't look like &lt;lb/&gt; programmer art&lt;/head&gt;
    &lt;p&gt;Skip the hours of CSS frustration. Get complete, professional-looking app layouts that work with Rails out of the box. Your users will think you hired a designer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Collie&lt;/head&gt;
    &lt;p&gt;Community platform&lt;/p&gt;
    &lt;head rend="h3"&gt;Husky&lt;/head&gt;
    &lt;p&gt;Personal Finance&lt;/p&gt;
    &lt;head rend="h3"&gt;Boxer&lt;/head&gt;
    &lt;p&gt;Agency Management&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Rails UI is going to save me months of work. I'm an experienced software developer building my first Ruby on Rails app, but I'm not strong at front-end design. Support has been awesome as well."&lt;/p&gt;Adam G. ‚Äî Software Developer&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;"Launched our MVP in two weeks instead of two months. The themes look so polished that our investors thought we had a full design team."&lt;/p&gt;Sarah M. ‚Äî Startup Founder&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;"My clients can't believe how fast I deliver now. Rails UI pays for itself on the first project."&lt;/p&gt;James T. ‚Äî Freelance Developer&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://railsui.com/"/><published>2026-01-21T18:31:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46711574</id><title>eBay explicitly bans AI "buy for me" agents in user agreement update</title><updated>2026-01-22T13:24:42.108763+00:00</updated><content>&lt;doc fingerprint="ddd85f438da6cb94"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;eBay Explicitly Bans AI ‚ÄúBuy For Me‚Äù Agents, Updates Arbitration &amp;amp; Dispute Rules In User Agreement Update&lt;/head&gt;
    &lt;p&gt;eBay explicitly prohibits AI "buy for me" agents and LLM (larger language model) bots, updates arbitration and dispute resolution requirements in latest User Agreement update, going into effect February 20, 2026.&lt;/p&gt;
    &lt;p&gt;The following summary of changes was provided in an email sent to users:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We‚Äôve updated eBay‚Äôs User Agreement, including the agreement to arbitrate any disputes you may have with us. Our updated User Agreement was posted on January 20, 2026. For users who agreed to a prior version of our User Agreement, this agreement is effective as of February 20, 2026.&lt;/p&gt;
      &lt;p&gt;In this update, eBay is updating its anti-scraping prohibition to clarify that it specifically also includes bots used for AI or LLMs. eBay is also updating the agreement to arbitrate in the updated User Agreement:&lt;/p&gt;
      &lt;item&gt;We clarified the scope of the class action waiver.&lt;/item&gt;
      &lt;item&gt;We clarified the process for opting out of the agreement to arbitrate.&lt;/item&gt;
      &lt;item&gt;We updated the physical address to which notices for informal dispute resolution, arbitration demands, and notices for opting out of arbitration must be sent.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;As always, sellers are encouraged to read the entire updated terms carefully, but Value Added Resource has you covered with a side by side comparison highlighting some key changes.&lt;/p&gt;
    &lt;p&gt;Disclaimer: comparisons are made using both automated and manual methods and are provided for informational purposes only - no warranty of completeness or accuracy is expressed or implied and users are advised to do their own due diligence.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI Agents &amp;amp; LLM Scraping&lt;/head&gt;
    &lt;p&gt;First, as the summary calls out, eBay is explicitly prohibiting AI "buy for me" agents and LLM scraping bots from interacting with the platform without permission from eBay.&lt;/p&gt;
    &lt;p&gt;Old Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In connection with using or accessing our Services you agree to comply with this User Agreement, our policies, our terms, and all applicable laws, rules, and regulations, and you will not...&lt;/p&gt;
      &lt;p&gt;...use any robot, spider, scraper, data mining tools, data gathering and extraction tools, or other automated means to access our Services for any purpose, except with the prior express permission of eBay;&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;New Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In connection with using or accessing our Services you agree to comply with this User Agreement, our policies, our terms, and all applicable laws, rules, and regulations, and you will not...&lt;/p&gt;
      &lt;p&gt;use any robot, spider, scraper, data mining tools, data gathering and extraction tools, or other automated means (including, without limitation buy-for-me agents, LLM-driven bots, or any end-to-end flow that attempts to place orders without human review) to access our Services for any purpose, except with the prior express permission of eBay;&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The move comes after eBay quietly changed their robots.txt file with new guidance placing guardrails and restrictions on how AI agents interact with the site in December.&lt;/p&gt;
    &lt;p&gt;It also comes on the heels of Amazon's controversial Buy For Me test which uses agentic AI to display items from direct merchant websites for sale through the Amazon app, even if the brand does not sell on Amazon themselves - raising concerns about transparency, consent, and control over how product details are displayed to buyers.&lt;/p&gt;
    &lt;p&gt;While it appears that Amazon Buy For Me currently does not pull inventory from other third party marketplaces, it would not be surprising if eBay is reacting at least in part to this and other agentic commerce news making recent headlines.&lt;/p&gt;
    &lt;head rend="h3"&gt;Arbitration &amp;amp; Dispute Resolution&lt;/head&gt;
    &lt;p&gt;The rest of the changes in this User Agreement update affect arbitration and dispute resolution.&lt;/p&gt;
    &lt;p&gt;eBay's previous User Agreement update in May 2025 made significant changes to arbitration terms and limits on lawsuits, forcing users to give up their right to the sue the company in many situations.&lt;/p&gt;
    &lt;p&gt;In this update, eBay has finally updated the address to send arbitration opt out requests and other legal correspondence to since selling their former office in Draper, UT in 2024.&lt;/p&gt;
    &lt;p&gt;Old Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Notice to eBay should be sent by email to DisputeNotice@eBay.com or regular mail to our offices located at 583 W. eBay Way, Draper, UT 84020.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;New Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Notice to eBay should be sent by email to DisputeNotice@eBay.com or regular mail to our offices located at 339 W. 13490 S., Ste. 500, Draper, UT 84020&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Most importantly, eBay has expanded their arbitration clause which previously prohibited class actions to now also explicitly exclude more types of group legal actions.&lt;/p&gt;
    &lt;p&gt;Old Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;EACH OF US MAY BRING CLAIMS AGAINST THE OTHER ONLY ON AN INDIVIDUAL BASIS AND NOT ON A CLASS, REPRESENTATIVE, OR COLLECTIVE BASIS, AND THE PARTIES HEREBY WAIVE ALL RIGHTS TO HAVE ANY DISPUTE BE BROUGHT, HEARD, ADMINISTERED, RESOLVED, OR ARBITRATED ON A CLASS, COLLECTIVE, OR REPRESENTATIVE BASIS. ONLY INDIVIDUAL RELIEF IS AVAILABLE.&lt;/p&gt;
      &lt;p&gt;Subject to this Agreement to Arbitrate, the arbitrator may award declaratory or injunctive relief only in favor of the individual party seeking relief and only to the extent necessary to provide relief warranted by the party‚Äôs individual claim. Nothing in this paragraph is intended to, nor shall it, affect the terms and conditions under Section 19.B.7 ("Batch Arbitration").&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;New Version:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;EACH OF US MAY BRING CLAIMS AGAINST THE OTHER ONLY ON AN INDIVIDUAL BASIS AND NOT AS A PLAINTIFF OR CLASS MEMBER IN ANY PURPORTED CLASS, OR REPRESENTATIVE, OR COLLECTIVE BASIS, OR PRIVATE ATTORNEY GENERAL ACTION OR PROCEEDING, NOR OTHERWISE TO SEEK RECOVERY OF LOSSES OR DAMAGES (WHETHER FOR YOURSELF OR OTHERS) INCURRED BY A THIRD PARTY, AND THE PARTIES HEREBY WAIVE ALL RIGHTS TO HAVE ANY DISPUTE BE BROUGHT, HEARD, ADMINISTERED, RESOLVED, OR ARBITRATED ON A CLASS, COLLECTIVE, OR REPRESENTATIVE BASIS. ONLY INDIVIDUAL RELIEF IS AVAILABLE.&lt;/p&gt;
      &lt;p&gt;Subject to this Agreement to Arbitrate, the arbitrator may award declaratory or injunctive relief only in favor of the individual party seeking relief and only to the extent necessary to provide relief warranted by the party‚Äôs individual claim. Nothing in this paragraph is intended to, nor shall it, affect the terms and conditions under Section 19.B.7 ("Batch Arbitration").&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here's what that means in plain language:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ÄúNot as a plaintiff or class member‚Äù ‚Äî prevents someone from joining an existing class action.&lt;/item&gt;
      &lt;item&gt;‚ÄúNo private attorney general actions‚Äù ‚Äî blocks lawsuits brought ‚Äúon behalf of the public,‚Äù a type of claim sometimes used in consumer protection cases.&lt;/item&gt;
      &lt;item&gt;‚ÄúNor‚Ä¶ for losses incurred by a third party‚Äù ‚Äî prevents a person from trying to recover damages suffered by someone else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: this language does not in any way change or restrict legal action that state Attorneys General, the FTC or other regulatory or legal agencies can take on behalf of sellers and/or consumers - so don't be dissuaded from letting those agencies know about your experiences with the platform, like the recent changes to Promoted Listings ad attribution policies.&lt;/p&gt;
    &lt;p&gt;And finally, this User Agreement update has been changed to clarify that only new users may request to opt out of arbitration agreement - existing users missed their opportunity if they did not opt out before May 16, 2025.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Opt-Out Procedure&lt;/p&gt;
      &lt;p&gt;IF YOU ARE A NEW USER OF OUR SERVICES, YOU CAN CHOOSE TO OPT OUT OF THIS AGREEMENT TO ARBITRATE ("OPT OUT") BY MAILING US A WRITTEN OPT-OUT NOTICE ("OPT-OUT NOTICE").&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's it for changes to eBay's User Agreement going into effect February 20, 2026.&lt;/p&gt;
    &lt;p&gt;Let us know in the comments below what you think of these change and how they'll affect your business!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.valueaddedresource.net/ebay-bans-ai-agents-updates-arbitration-user-agreement-feb-2026/"/><published>2026-01-21T21:07:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46711792</id><title>Jerry (YC S17) Is Hiring</title><updated>2026-01-22T13:24:41.816116+00:00</updated><link href="https://www.ycombinator.com/companies/jerry-inc/jobs/QaoK3rw-software-engineer-core-automation-marketplace"/><published>2026-01-21T21:26:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46712678</id><title>Your brain on ChatGPT: Accumulation of cognitive debt when using an AI assistant</title><updated>2026-01-22T13:24:41.436721+00:00</updated><content>&lt;doc fingerprint="a81bbfb1ef3e053e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Abstract&lt;/head&gt;
      &lt;p&gt;This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.media.mit.edu/publications/your-brain-on-chatgpt/"/><published>2026-01-21T22:41:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713106</id><title>Show HN: Sweep, Open-weights 1.5B model for next-edit autocomplete</title><updated>2026-01-22T13:24:41.223735+00:00</updated><content>&lt;doc fingerprint="85b3cff318accb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sweep Next-Edit 1.5B (GGUF)&lt;/head&gt;
    &lt;p&gt;A 1.5B parameter model for next-edit autocomplete, quantized to Q8_0 GGUF format.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Description&lt;/head&gt;
    &lt;p&gt;Sweep Next-Edit predicts your next code edit before you make it. It runs locally on your laptop in under 500ms (with speculative decoding) and outperforms models over 4x its size on next-edit benchmarks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Usage&lt;/head&gt;
    &lt;p&gt;Download &lt;code&gt;run_model.py&lt;/code&gt; and the model file, then:&lt;/p&gt;
    &lt;code&gt;uv pip install llama-cpp-python huggingface_hub
python run_model.py
&lt;/code&gt;
    &lt;head rend="h2"&gt;Model Details&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format: GGUF (Q8_0 quantization)&lt;/item&gt;
      &lt;item&gt;Parameters: 1.5B&lt;/item&gt;
      &lt;item&gt;Context Length: 8192 tokens&lt;/item&gt;
      &lt;item&gt;Base Model: Qwen2.5-Coder&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;The model uses a specific prompt format with file context, recent diffs, and current state to predict the next edit. See &lt;code&gt;run_model.py&lt;/code&gt; for a complete example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Links&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blog Post - Technical details and benchmarks&lt;/item&gt;
      &lt;item&gt;JetBrains Plugin - Sweep AI JetBrains Plugin&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;License&lt;/head&gt;
    &lt;p&gt;Apache 2.0&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Downloads last month&lt;/item&gt;
      &lt;item rend="dd-1"&gt;71&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Hardware compatibility&lt;/p&gt;
    &lt;p&gt;Log In to view the estimation&lt;/p&gt;
    &lt;p&gt;8-bit&lt;/p&gt;
    &lt;p&gt; Inference Providers NEW &lt;/p&gt;
    &lt;p&gt;This model isn't deployed by any Inference Provider. üôã Ask for provider support&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://huggingface.co/sweepai/sweep-next-edit-1.5B"/><published>2026-01-21T23:22:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713387</id><title>Lix ‚Äì universal version control system for binary files</title><updated>2026-01-22T13:24:41.062377+00:00</updated><content>&lt;doc fingerprint="f514be7bc4ab260c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Lix: A universal version control system&lt;/head&gt;
    &lt;head rend="h2"&gt;AI agents need version control beyond text&lt;/head&gt;
    &lt;p&gt;Changes AI agents make need to be reviewable by humans.&lt;/p&gt;
    &lt;p&gt;For code, Git solves this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reviewable diffs: What exactly did the agent change?&lt;/item&gt;
      &lt;item&gt;Human-in-the-loop: Review, then merge or reject.&lt;/item&gt;
      &lt;item&gt;Rollback changes: Undo mistakes instantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But agents modify binary files too. And Git can't diff them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Lix&lt;/head&gt;
    &lt;p&gt;Lix is a universal version control system that can diff any file format (&lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, etc).&lt;/p&gt;
    &lt;p&gt;Unlike Git's line-based diffs, Lix understands file structure. Lix sees &lt;code&gt;price: 10 √¢ 12&lt;/code&gt; or &lt;code&gt;cell B4: pending √¢ shipped&lt;/code&gt;, not "line 4 changed" or "binary files differ".&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reviewable diffs: See exactly what an agent changed in any file format.&lt;/item&gt;
      &lt;item&gt;Human-in-the-loop: Agents propose, humans approve.&lt;/item&gt;
      &lt;item&gt;Safe rollback: Undo mistakes instantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Excel file example&lt;/head&gt;
    &lt;p&gt;An AI agent updates an order status in &lt;code&gt;orders.xlsx&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Before:&lt;/p&gt;
    &lt;code&gt;  | order_id | product  | status   |
  | -------- | -------- | -------- |
  | 1001     | Widget A | shipped  |
  | 1002     | Widget B | pending |
&lt;/code&gt;
    &lt;p&gt;After:&lt;/p&gt;
    &lt;code&gt;  | order_id | product  | status   |
  | -------- | -------- | -------- |
  | 1001     | Widget A | shipped  |
  | 1002     | Widget B | shipped |
&lt;/code&gt;
    &lt;p&gt;Git sees:&lt;/p&gt;
    &lt;code&gt;-Binary files differ
&lt;/code&gt;
    &lt;p&gt;Lix sees:&lt;/p&gt;
    &lt;code&gt;order_id 1002 status: 

- pending
+ shipped
&lt;/code&gt;
    &lt;head rend="h2"&gt;JSON file example&lt;/head&gt;
    &lt;p&gt;Even for structured text file formats like &lt;code&gt;.json&lt;/code&gt; lix is tracking semantics rather than line by line diffs.&lt;/p&gt;
    &lt;p&gt;Before:&lt;/p&gt;
    &lt;code&gt;{"theme":"light","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;After:&lt;/p&gt;
    &lt;code&gt;{"theme":"dark","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;Git sees:&lt;/p&gt;
    &lt;code&gt;-{"theme":"light","notifications":true,"language":"en"}
+{"theme":"dark","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;Lix sees:&lt;/p&gt;
    &lt;code&gt;property theme: 
- light
+ dark
&lt;/code&gt;
    &lt;head rend="h2"&gt;How does Lix work?&lt;/head&gt;
    &lt;p&gt;Lix adds a version control system on top of SQL databases that let's you query virtual tables like &lt;code&gt;file&lt;/code&gt;, &lt;code&gt;file_history&lt;/code&gt;, etc. via plain SQL. These table's are version controlled.&lt;/p&gt;
    &lt;p&gt;Why this matters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lix doesn't reinvent databases √¢ durability, ACID, and corruption recovery are handled by battle-tested SQL databases.&lt;/item&gt;
      &lt;item&gt;Full SQL support √¢ query your version control system with the same SQL.&lt;/item&gt;
      &lt;item&gt;Can runs in your existing database √¢ no separate storage layer to manage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢                      Lix                        √¢
√¢           (version control system)              √¢
√¢                                                 √¢
√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢ √¢
√¢ √¢ Filesystem √¢ √¢ Branches √¢ √¢ History √¢ √¢ ... √¢ √¢
√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
                         √¢
                         √¢¬º
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢                  SQL database                   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;Read more about Lix architecture √¢&lt;/p&gt;
    &lt;head rend="h2"&gt;Why did we build lix?&lt;/head&gt;
    &lt;p&gt;Lix was developed alongside inlang, open-source localization infrastructure.&lt;/p&gt;
    &lt;p&gt;We had to develop a new version control system that addressed git's limitations inlang ran into, see (see "Git is unsuited for applications"). The result is Lix, now at over 90k weekly downloads on NPM.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;JavaScript √Ç¬∑ Python √Ç¬∑ Rust √Ç¬∑ Go&lt;/p&gt;
    &lt;code&gt;npm install @lix-js/sdk
&lt;/code&gt;
    &lt;code&gt;import { openLix } from "@lix-js/sdk";

const lix = await openLix({
  environment: new InMemorySQLite()
});

await lix.db.insertInto("file").values({ path: "/hello.txt", data: ... }).execute();

const diff = selectWorkingDiff({ lix })
&lt;/code&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;The next version of Lix will be a refactor to be purely "preprocessor" based. This enables:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fast writes (RFC 001)&lt;/item&gt;
      &lt;item&gt;Any SQL database (SQLite, Postgres, Turso, MySQL)&lt;/item&gt;
      &lt;item&gt;SDKs for Python, Rust, Go (RFC 002)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;                      √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
  SELECT * FROM ...   √¢  Lix Engine    √¢   SELECT * FROM ...
 √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬∂ √¢    (Rust)      √¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬∂  Database
                      √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lix.dev/blog/introducing-lix/"/><published>2026-01-21T23:55:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713444</id><title>From stealth blackout to whitelisting: Inside the Iranian shutdown</title><updated>2026-01-22T13:24:40.862167+00:00</updated><content>&lt;doc fingerprint="3ff80173394545ce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Stealth Blackout to Whitelisting: Inside the Iranian Shutdown&lt;/head&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Iran is in the midst of one of the world‚Äôs most severe communications blackouts. This post uses Kentik data to detail how this historic event unfolded, where this event lies in the context of previous Iranian shutdowns, and finally discusses what might be in store next for Iran.&lt;/p&gt;
    &lt;p&gt;For nearly two weeks, Iran has been enduring one of the most severe internet shutdowns in modern history. The theocratic regime‚Äôs decision to restrict communications coincided with a violent nationwide crackdown on a growing protest movement driven by worsening economic hardship.&lt;/p&gt;
    &lt;p&gt;In this post, I explore the situation in Iran using Kentik‚Äôs aggregate NetFlow data, along with other sources.&lt;/p&gt;
    &lt;head rend="h2"&gt;The big picture&lt;/head&gt;
    &lt;p&gt;At the time of this writing, a near-complete internet shutdown has persisted for almost 14 days. Along with internet services, international voice calling has also been blocked (there have been a couple of periods when limited outgoing calls were allowed), and domestic communication services have experienced extended disruptions, including Iran‚Äôs National Information Network. For a country of 90 million people, the combined blocking of these communication modes makes this blackout one of the most severe in history.&lt;/p&gt;
    &lt;p&gt;To learn more about the conditions that lead to the check out this special episode of Kentik‚Äôs Telemetry Now podcast with Iranian digital rights expert Amir Rashidi, Director of Digital Rights and Security at the human rights organization Miaan Group:&lt;/p&gt;
    &lt;head rend="h2"&gt;Some background first&lt;/head&gt;
    &lt;p&gt;For decades, the internet of Iran has been connected to the world via two international gateways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Telecommunication Infrastructure Company (TIC) (AS49666, previously AS12880, AS48159)&lt;/item&gt;
      &lt;item&gt;Institute for Research in Fundamental Sciences (IPM) (AS6736)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IPM, primarily a university and research network, was the country‚Äôs original internet connection in the 1990s, a story covered in the excellent book The Internet of Elsewhere by Cyrus Farivar. Years later, the state telecom TIC got into the business of providing internet service and today handles the vast majority of internet traffic into and out of Iran.&lt;/p&gt;
    &lt;p&gt;Despite TIC‚Äôs dominance, IPM has maintained a technologically independent connection to the outside world, though it has never been immune from Iranian government censorship and surveillance. This distinction matters because each gateway behaved differently during the shutdown.&lt;/p&gt;
    &lt;head rend="h2"&gt;January 8, 2026&lt;/head&gt;
    &lt;p&gt;In the days leading up to January 8, there were many reports of localized internet blockages around the country, but these incidents weren‚Äôt big enough to register on any of our national traffic statistics for Iran.&lt;/p&gt;
    &lt;p&gt;The first major development occurred at 11:42 UTC on January 8, 2026, when TIC (AS49666) began withdrawing its IPv6 BGP routes from its sessions with other networks. Within hours, nearly all of Iran‚Äôs IPv6 routing had disappeared from the global routing table.&lt;/p&gt;
    &lt;p&gt;From our perspective, this is what IPv6 traffic to Iran looked like on January 8.&lt;/p&gt;
    &lt;p&gt;However, based on our aggregate NetFlow, IPv6 traffic normally amounts to less than 1% of the overall traffic (in bits/sec) into Iran, so the average Iranian was unlikely to be affected by this issue. Regardless, the withdrawal of IPv6 routes appeared to be an early indication of what was to come later in the day.&lt;/p&gt;
    &lt;p&gt;Following a brief disruption, we observed internet traffic levels begin to plummet at 16:30 UTC (7pm local). The drop continued until internet traffic into Iran had all but ceased by 1845 UTC, as illustrated below. It took over two hours to stop all internet traffic into and out of the country.&lt;/p&gt;
    &lt;p&gt;At 19:00 UTC, we observed TIC disconnecting from a subset of its transit providers, including Russian state telecom Rostelecom (AS12389) and regional operator Gulf Bridge International (AS200612), and all of its settlement-free peers.&lt;/p&gt;
    &lt;p&gt;Despite the loss of numerous BGP adjacencies for AS49666 (TIC), the vast majority of Iranian IPv4 routes continued to be routed globally. The drop in Iranian IPv4 traffic, therefore, could not be explained by reachability issues; another mechanism was at work at the network edge blocking traffic.&lt;/p&gt;
    &lt;p&gt;Georgia Tech‚Äôs IODA tool captures this divergence well. In the below screenshot, active probing (blue) drops to zero as traffic is blocked, while routed IPv4 space in BGP (green) is almost completely unscathed (98.14%).&lt;/p&gt;
    &lt;p&gt;Although IPv4 routes remained online, internet traffic stopped for roughly 90 million Iranians. This distinction is central to Iran‚Äôs next step: internet ‚Äúwhitelisting,‚Äù in which an Iranian version of the Chinese Great Firewall allows only approved users or services while blocking all others. Had authorities withdrawn IPv4 routes, as they did with IPv6, Iran would have become completely unreachable, as Egypt was in January 2011. By keeping IPv4 routes in circulation, Iranian authorities can selectively grant full internet access to specific users while denying it to the broader population.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limited connectivity&lt;/head&gt;
    &lt;p&gt;As mentioned above, the internet shutdown in Iran is not complete. There has been a tiny amount of traffic still trickling in and out as a small set of Iranians continue to enjoy internet access.&lt;/p&gt;
    &lt;p&gt;There have also been a few temporary partial restorations of service, such as a multi-hour restoration of service to Iranian universities via AS6736 on January 9th, and a more recent small surge in traffic.&lt;/p&gt;
    &lt;p&gt;From our data, we have also observed the emergence of a diurnal pattern of traffic to AS49666 emerge on January 13. AS49666 is not typically a major terminus for internet traffic to Iran, so this traffic is likely proxied traffic from whitelisted individuals or services.&lt;/p&gt;
    &lt;p&gt;As of late, we‚Äôve seen a few measures like the restoration of transit from Rostelecom and the return of routes originated by IPM, as the country appears to be moving towards a partial restoration. At the time of this writing, the plan appears to be to operate the Iranian internet as a whitelisted network indefinitely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evolving calculus of shutdowns in Iran&lt;/head&gt;
    &lt;p&gt;Back in 2012, Iran was in the beginning stages of building its National Information Network (NIN), ostensibly built to allow the country to continue to function in the event that it was cut off from the outside world. At the time, I teamed up with Iran researcher Collin Anderson to investigate. With access to in-country servers, we mapped Iran‚Äôs national internet from the inside (research published here).&lt;/p&gt;
    &lt;p&gt;We found that the NIN had been implemented by routing RFC1918 address space (specifically 10.x.x.x) between Iranian ASes within the country. By doing so, they could be assured that devices connected to the NIN would not be able to receive connections from the outside world, as those IP addresses are not routable on the public internet.&lt;/p&gt;
    &lt;p&gt;In 2019, I reported on Iran‚Äôs internet shutdown following the government‚Äôs decision to raise gas prices. At the time, it was the most severe shutdown in the countryÔøΩ‚Äôs history‚Äîuntil this month. It involved withdrawing BGP routes of some networks while blocking traffic of others, and lasted for almost two weeks.&lt;/p&gt;
    &lt;p&gt;Government-directed shutdowns in Cuba and Iran in 2022 led me to join up with Peter Micek of the digital rights NGO Access Now to write a blog post that traced the history and logic behind ‚Äúinternet curfews,‚Äù a tactic of communication suppression in which internet service is temporarily blocked on a recurring basis.&lt;/p&gt;
    &lt;p&gt;The article described internet curfews as another means of reducing the costs of shutdowns, not unlike the development of the NIN, according to Iranian digital rights expert Amir Rashidi. In that post, we wrote:&lt;/p&gt;
    &lt;quote&gt;The objective of internet curfews, like Iran‚Äôs NIN, is to reduce the cost of shutdowns on the authorities that order them. By reducing the costs of these shutdowns, they become a more palatable option for an embattled leader and, therefore, are likely to continue in the future.&lt;/quote&gt;
    &lt;p&gt;During the Twelve-Day War between Israel and Iran this June, Iran partially or fully shut down its internet, ostensibly to defend against cyberattacks and drone strikes. We, along with other internet observers, documented the shutdown‚Äôs phases and contributed to a detailed report by Rashidi‚Äôs team, which dubbed the shutdown as a ‚Äústealth blackout‚Äù due to the fact that traffic was disrupted without withdrawing any BGP routes.&lt;/p&gt;
    &lt;p&gt;The outage demonstrated Iran‚Äôs newfound ability to block traffic nationwide without manipulating BGP routes, signaling a higher level of sophistication in its internet filtering. This summer‚Äôs Stealth Blackout ultimately foreshadowed the ongoing shutdown Iran is now enduring.&lt;/p&gt;
    &lt;head rend="h2"&gt;Help from above&lt;/head&gt;
    &lt;p&gt;In the aftermath of the 2022 protests, Starlink began allowing connections from Iran. Satellite internet operators like Starlink must typically clear, at a minimum, two legal hurdles to operate in a country: a telecom license and radio spectrum authorization from the local government. Starlink has been operational in Iran for over three years at this point without either, and the Iranian government has taken note.&lt;/p&gt;
    &lt;p&gt;The ITU Radio Regulations Board (RRB) is a quasi-judicial United Nations body that interprets and applies the Radio Regulations, to include satellite emissions. It exists to resolve disputes between countries and oversees compliance with the international radio frequency register, but, in the end, has no direct enforcement power.&lt;/p&gt;
    &lt;p&gt;Since 2023, the Iranian has been pleading their case to the ITU that the Starlink service in Iran needed to be disabled. The 100th meeting of the ITU RRB took place in November, and on the topic of Starlink, the board decided to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ÄúRequest the Administration of the Islamic Republic of Iran to pursue its efforts, to the extent possible, to identify and deactivate unauthorized STARLINK terminals in its territory,&lt;/item&gt;
      &lt;item&gt;Strongly urge the Administration of Norway to take all appropriate actions at its disposal to have the operator of the Starlink system immediately disable unauthorized transmissions of its terminals within the territory of the Islamic Republic of Iran.‚Äù&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regardless of the decisions of this body, Starlink continues to operate in the country. (Note: The US and Norway share responsibility for Starlink‚Äôs ITU registration.)&lt;/p&gt;
    &lt;p&gt;Despite a recent Iranian law that would equate the use of Starlink with espionage, punishable by death, Iranian digital rights activists have been working for years to smuggle in terminals and build communication infrastructure to extend the internet services within the country. The recent front-page New York Times article I collaborated on described these efforts, which now must contend with a novel form of jamming Starlink service in some urban areas of Iran.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other governments are watching, learning&lt;/head&gt;
    &lt;p&gt;In the decade and a half since the internet shutdowns of the Arab Spring, we‚Äôve observed the practice of suppressing communications evolve as authoritarian governments learn tactics from one another. In the ongoing shutdown in Iran, multiple such tactics are on display.&lt;/p&gt;
    &lt;p&gt;To mitigate the costs of its shutdown, the Iranian government has created an internal national internet and appears to be in the process of building a ‚Äúwhitelisting‚Äù system to allow certain individuals and services internet access while blocking the rest. If these measures successfully enable an unpopular Iranian government to remain in power, we can expect to see them replicated elsewhere.&lt;/p&gt;
    &lt;p&gt;On the other side, the digital rights activists have also been building tools, funded in large part by the now-embattled Open Technology Fund, to allow communications to continue during a shutdown like this. However, no amount of circumvention tooling can restore service to 90 million people.&lt;/p&gt;
    &lt;p&gt;The fight for open and free communications does not have an end. As long as authoritarian governments exist, this game of cat-and-mouse will continue. Ours is only to decide which side we‚Äôre on and to throw our support (financially and otherwise) to those working on solutions to these problems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kentik.com/blog/from-stealth-blackout-to-whitelisting-inside-the-iranian-shutdown/"/><published>2026-01-22T00:00:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713526</id><title>Threat actors expand abuse of Microsoft Visual Studio Code</title><updated>2026-01-22T13:24:40.689886+00:00</updated><content>&lt;doc fingerprint="a425853934a78a08"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Threat Actors Expand Abuse of Microsoft Visual Studio Code&lt;/head&gt;
    &lt;p&gt;Jamf Threat Labs identifies additional abuse of Visual Studio Code. See the latest evolution in the Contagious Interview campaign.&lt;/p&gt;
    &lt;p&gt;By Thijs Xhaflaire&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;At the end of last year, Jamf Threat Labs published research related to the Contagious Interview campaign, which has been attributed to a threat actor operating on behalf of North Korea (DPRK). Around the same time, researchers from OpenSourceMalware (OSM) released additional findings that highlighted an evolution in the techniques used during earlier stages of the campaign.&lt;/p&gt;
    &lt;p&gt;Specifically, these newer observations highlight an additional delivery technique alongside the previously documented ClickFix-based techniques. In these cases, the infection chain abuses Microsoft Visual Studio Code task configuration files, allowing malicious payloads to be executed on the victim system.&lt;/p&gt;
    &lt;p&gt;Following the discovery of this technique, both Jamf Threat Labs and OSM continued to closely monitor activity associated with the campaign. In December, Jamf Threat Labs identified additional abuse of Visual Studio Code &lt;code&gt;tasks.json&lt;/code&gt; configuration files. This included the introduction of dictionary files containing heavily obfuscated JavaScript, which is executed when a victim opens a malicious repository in Visual Studio Code.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs shared these findings with OSM, who subsequently published a more in-depth technical analysis of the obfuscated JavaScript and its execution flow.&lt;/p&gt;
    &lt;p&gt;Earlier this week, Jamf Threat Labs identified another evolution in the campaign, uncovering a previously undocumented infection method. This activity involved the deployment of a backdoor implant that provides remote code execution capabilities on the victim system.&lt;/p&gt;
    &lt;p&gt;At a high level, the chain of events for the malware look like so:&lt;/p&gt;
    &lt;p&gt;Throughout this blog post we will shed light on each of these steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Initial Infection&lt;/head&gt;
    &lt;p&gt;In this campaign, infection begins when a victim clones and opens a malicious Git repository, often under the pretext of a recruitment process or technical assignment. The repositories identified in this activity are hosted on either GitHub or GitLab and are opened using Visual Studio Code.&lt;/p&gt;
    &lt;p&gt;When the project is opened, Visual Studio Code prompts the user to trust the repository author. If that trust is granted, the application automatically processes the repository‚Äôs &lt;code&gt;tasks.json&lt;/code&gt; configuration file, which can result in embedded arbitrary commands being executed on the system.&lt;/p&gt;
    &lt;p&gt;On macOS systems, this results in the execution of a background shell command that uses &lt;code&gt;nohup bash -c&lt;/code&gt; in combination with &lt;code&gt;curl -s&lt;/code&gt; to retrieve a JavaScript payload remotely and pipe it directly into the Node.js runtime. This allows execution to continue independently if the Visual Studio Code process is terminated, while suppressing all command output.&lt;/p&gt;
    &lt;p&gt;In observed cases, the JavaScript payload is hosted on &lt;code&gt;vercel.app&lt;/code&gt;, a platform that has been increasingly used in recent DPRK-related activity following a move away from other hosting services, as previously documented by OpenSourceMalware.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs reported the identified malicious repository to GitHub, after which the repository was removed. While monitoring the activity prior to takedown, we observed the URL referenced within the repository change on multiple occasions. Notably, one of these changes occurred after the previously referenced payload hosting infrastructure was taken down by Vercel.&lt;/p&gt;
    &lt;head rend="h2"&gt;The JavaScript Payload&lt;/head&gt;
    &lt;p&gt;Once execution begins, the JavaScript payload implements the core backdoor logic observed in this activity. While the payload appears lengthy, a significant portion of the code consists of unused functions, redundant logic, and extraneous text that is never invoked during execution &lt;code&gt;(SHA256: 932a67816b10a34d05a2621836cdf7fbf0628bbfdf66ae605c5f23455de1e0bc)&lt;/code&gt;. This additional code increases the size and complexity of the script without impacting its observed behavior. It is passed to the node executable as one large argument.&lt;/p&gt;
    &lt;p&gt;Focusing on the functional components, the payload establishes a persistent execution loop that collects basic host information and communicates with a remote command-and-control (C2) server. Hard-coded identifiers are used to track individual infections and manage tasks from the server.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core backdoor functionality&lt;/head&gt;
    &lt;p&gt;While the JavaScript payload contains a significant amount of unused code, the backdoor's core functionality is implemented through a small number of routines. These routines provide remote code execution, system fingerprinting, and persistent C2 communication.&lt;/p&gt;
    &lt;p&gt;Remote code execution capability&lt;/p&gt;
    &lt;p&gt;The payload includes a function that enables the execution of arbitrary JavaScript while the backdoor is active. At its core, this is the main functionality of this backdoor.&lt;/p&gt;
    &lt;p&gt;This function allows JavaScript code supplied as a string to be dynamically executed over the course of the backdoor lifecycle. By passing the &lt;code&gt;require&lt;/code&gt;function into the execution context, attacker-supplied code can import additional Node.js modules allowing additional arbitrary node functions to be executed.&lt;/p&gt;
    &lt;p&gt;System fingerprinting and reconnaissance&lt;/p&gt;
    &lt;p&gt;To profile the infected system, the backdoor collects a small set of host-level identifiers:&lt;/p&gt;
    &lt;p&gt;This routine gathers the system hostname, MAC addresses from available network interfaces, and basic operating system details. These values provide a stable fingerprint that can be used to uniquely identify infected hosts and associate them with a specific campaign or operator session.&lt;/p&gt;
    &lt;p&gt;In addition to local host identifiers, the backdoor attempts to determine the victim‚Äôs public-facing IP address by querying the external service ipify.org, a technique that has also been observed in prior DPRK-linked campaigns.&lt;/p&gt;
    &lt;p&gt;Command-and-control beaconing and task execution&lt;/p&gt;
    &lt;p&gt;Persistent communication with the C2 server is implemented through a polling routine that periodically sends host information and processes server responses. The beaconing logic is handled by the following function:&lt;/p&gt;
    &lt;p&gt;This function periodically sends system fingerprinting data to a remote server and waits for a response. The beacon executes every five seconds, providing frequent interaction opportunities.&lt;/p&gt;
    &lt;p&gt;The server response indicates successful connectivity and allows the backdoor to maintain an active session while awaiting tasking.&lt;/p&gt;
    &lt;p&gt;If the server response contains a specific status value, the contents of the response message are passed directly to the remote code execution routine, mentioned prior.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further Execution and Instructions&lt;/head&gt;
    &lt;p&gt;While monitoring a compromised system, Jamf Threat Labs observed further JavaScript instructions being executed roughly eight minutes after the initial infection. The retrieved JavaScript went on to set up a very similar payload to the same C2 infrustructure.&lt;/p&gt;
    &lt;p&gt;Review of this retrieved payload yields a few interesting details...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It beacons to the C2 server every 5 seconds, providing its system details and asks for further JavaScript instructions.&lt;/item&gt;
      &lt;item&gt;It executes that additional JavaScript within a child process.&lt;/item&gt;
      &lt;item&gt;It's capable of shutting itself and child processes down and cleaning up if asked to do so by the attacker.&lt;/item&gt;
      &lt;item&gt;It has inline comments and phrasing that appear to be consistent with AI-assisted code generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This activity highlights the continued evolution of DPRK-linked threat actors, who consistently adapt their tooling and delivery mechanisms to integrate with legitimate developer workflows. The abuse of Visual Studio Code task configuration files and Node.js execution demonstrates how these techniques continue to evolve alongside commonly used development tools.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs will continue to track these developments as threat actors refine their tactics and explore new ways to deliver macOS malware. We strongly recommend that customers ensure Threat Prevention and Advanced Threat Controls are enabled and set to block mode in Jamf for Mac to remain protected against the techniques described in this research.&lt;/p&gt;
    &lt;p&gt;Developers should remain cautious when interacting with third-party repositories, especially those shared directly or originating from unfamiliar sources. Before marking a repository as trusted in Visual Studio Code, it‚Äôs important to review its contents. Similarly, "npm install" should only be run on projects that have been vetted, with particular attention paid to package.json files, install scripts, and task configuration files to help avoid unintentionally executing malicious code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicators or Compromise&lt;/head&gt;
    &lt;p&gt;Dive into more Jamf Threat Labs research on our blog.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jamf.com/blog/threat-actors-expand-abuse-of-visual-studio-code/"/><published>2026-01-22T00:12:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713929</id><title>Significant US farm losses persist, despite federal assistance</title><updated>2026-01-22T13:24:40.270511+00:00</updated><content>&lt;doc fingerprint="6f3c401c4bbe6fca"&gt;
  &lt;main&gt;
    &lt;p&gt;Economist&lt;/p&gt;
    &lt;p&gt;Key Takeaways&lt;/p&gt;
    &lt;p&gt;The USDA-Economic Research Service (ERS) December update to Commodity Costs and Returns provides a comprehensive look at per-acre production costs for the nine principal row crops: corn, soybeans, wheat, cotton, rice, barley, oats, peanuts and sorghum. At a high level, ERS projects average total costs per acre to increase for every crop in 2026, underscoring the persistence of elevated production expenses across U.S. agriculture.&lt;/p&gt;
    &lt;p&gt;When operating expenses and farm-wide costs like equipment, land and management are combined, costs vary widely by crop. In 2025, forecasted total per-acre costs are $1,308 for rice, $1,166 for peanuts, $943 for cotton, $890 for corn, $658 for soybeans, $498 for oats, $491 for barley, $443 for sorghum, and $396 for wheat. Looking ahead, ERS projections for 2026 suggest continued upward pressure across most cost categories, with total cost increasing anywhere from 2.2% to 3.3%. Amongst the nine principal crops, wheat ($409 per acre), sorghum ($458) and oats ($513) remain at the lower end of the production cost spectrum, while soybeans ($678) and barley ($507) fall in the mid-range in 2026. Cotton ($965), peanuts ($1,194) and rice ($1,336) remain the most expensive crops to produce on a per-acre basis.&lt;/p&gt;
    &lt;p&gt;Operating costs‚Äîexpenses directly tied to producing a yearly crop, such as seed, fertilizer, chemicals, fuel and labor‚Äîsubstantially vary across crops. In 2025, total operating costs ranged from $155 per acre for wheat to more than $764 per acre for rice and $631 per acre for peanuts. In 2026, these costs are expected to rise, ranging from $774 per acre for rice and $160 per acre for wheat. While select inputs have moderated slightly from recent peaks, overall operating expenses remain well above pre-2021 levels. Rising costs since 2020 have been driven primarily by sharp increases in interest expenses (+71%), fertilizer (+37%), fuel and oil (+32%), labor (+47%), chemicals (+25%) and maintenance (+27%), alongside notable gains in seed (+18%) and marketing costs (+18%).&lt;/p&gt;
    &lt;p&gt;Losses Persist Even After FBA and ECAP&lt;/p&gt;
    &lt;p&gt;Against this backdrop of elevated costs, commodity prices have remained under pressure, limiting farmers‚Äô ability to cover their costs through the marketplace alone. As a result, many farms are projected to experience losses for a fourth or fifth consecutive year, even after accounting for crop insurance indemnities and ad hoc assistance.&lt;/p&gt;
    &lt;p&gt;The Farmer Bridge Assistance (FBA) Program and the Emergency Commodity Assistance Program (ECAP) provide important near-term support. However, ECAP was designed to address 2023 and 2024 losses, rather than 2025 and later production challenges. For both programs, payments are calculated on a per-acre basis. However, when compared to current per-acre production costs and weak commodity prices, these payments generally cover only a share of losses rather than restore profitability. In fact, returns over total costs for all nine principal row crops are projected to remain negative on a per-acre basis even after accounting for federal assistance. Based on loss calculations used in the Farmer Bridge Assistance Program, rice producers face losses of roughly $210 per acre, followed by cotton ($202), oats ($159), peanuts ($131), sorghum ($91), corn ($87), wheat ($70), soybeans ($61) and barley ($42). In total, net losses across the sector are estimated to exceed $50 billion over the past three crop years.&lt;/p&gt;
    &lt;p&gt;For many farms, aid helps slow the erosion of working capital but does not fully offset negative margins. As a result, producers continue to absorb multiyear losses that strain balance sheets, tighten cash flow and complicate access to operating credit. These loss estimates reflect national averages; actual costs of production and returns vary by region, management decisions and ownership structure. For example, producers who own their farmland may face lower total costs by avoiding cash rental expenses, resulting in higher returns.&lt;/p&gt;
    &lt;p&gt;Specialty Crops&lt;lb/&gt;Additionally, neither the FBA program nor the ECAP address losses in the specialty crops market. The 2024 Marketing Assistance for Specialty Crop Program (MASC) provided a first but limited relief step for growers and, for many, represented some of the first federal assistance tied to market challenges in the sector. Specialty crop growers continue to face deep and persistent economic losses driven by rising input costs, tightening margins, weather and disease disruptions, labor expenses and constraints, and global trade instability ‚Äî challenges shared by field crop agriculture, including producers of crops beyond the nine principal crops, such as alfalfa and sugar beets. Strengthening support for all sectors of agriculture is an economic necessity. Doing so will help maintain a resilient, accessible and diverse U.S. food system. &lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;ERS cost projections make clear that input costs for all of the nine principal row crops remain elevated and sticky. Continued increases in both operating and overhead expenses are pushing breakeven prices higher, while commodity prices remain insufficient to offset those costs for many producers.&lt;/p&gt;
    &lt;p&gt;While FBA and ECAP payments are an important and welcome step in addressing near-term financial stress, they do not fully close the gap between costs and returns. As farmers enter the 2026/27 marketing year, accumulated losses ‚Äî estimated to exceed $50 billion across the sector over the past three crop years ‚Äî continue to weigh on farm finances.&lt;/p&gt;
    &lt;p&gt;These estimates reflect national average conditions and are calculated ahead of the growing season, before producers make final planting, input and marketing decisions. In practice, farmers respond to market signals by adjusting crop mix, input use and risk management strategies as conditions evolve. While outcomes vary widely by region and operation, persistently elevated breakeven prices underscore the importance of market-driven solutions that strengthen domestic demand ‚Äî such as year-round access to E15 ‚Äî to help support commodity prices and improve farm margins.&lt;/p&gt;
    &lt;p&gt;Much-needed safety net enhancements through the One Big Beautiful Bill Act (OBBBA) are expected to take effect in October 2026, but those changes do not address the pressures farmers face today. In a recent letter to Congress organized by the American Farm Bureau Federation and signed by 56 agricultural organizations, farm groups warned of an economic crisis in rural America, citing multiyear losses driven by record-high input costs and historically low commodity prices. Congressional leaders from both parties have acknowledged the severity of these losses and the need for additional aid to stabilize farm finances. Until longer-term policy improvements take hold, many operations remain caught between high operating costs and low commodity prices, underscoring the ongoing financial strain facing U.S. agriculture as producers weigh whether they can afford to plant another crop.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.fb.org/market-intel/significant-farm-losses-persist-despite-federal-assistance"/><published>2026-01-22T01:11:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46715600</id><title>Doctors in Brazil using tilapia fish skin to treat burn victims</title><updated>2026-01-22T13:24:39.987033+00:00</updated><content>&lt;doc fingerprint="ef03d738aad3766e"&gt;
  &lt;main&gt;
    &lt;p&gt;By ‚Äî Nadia Sussman, STAT Nadia Sussman, STAT Leave a comment 0comments Share Copy URL https://www.pbs.org/newshour/health/brazilian-city-uses-tilapia-fish-skin-treat-burn-victims Email Facebook Twitter LinkedIn Pinterest Tumblr Share on Facebook Share on Twitter Why this Brazilian city uses tilapia fish skin to treat burn victims Health Mar 3, 2017 1:09 PM EST FORTAZELA, Brazil ‚Äî In this historic city by the sea in northeast Brazil, burn patients look as if they've emerged from the waves. They are covered in fish skin ‚Äî specifically strips of sterilized tilapia. Doctors here are testing the skin of the popular fish as a bandage for second- and third-degree burns. The innovation arose from an unmet need. Animal skin has long been used in the treatment of burns in developed countries. But Brazil lacks the human skin, pig skin, and artificial alternatives that are widely available in the US. The three functional skin banks in Brazil can meet only 1 percent of the national demand, said Dr. Edmar Maciel, a plastic surgeon and burn specialist leading the clinical trials with tilapia skin. As a result, public health patients in Brazil are normally bandaged with gauze and silver sulfadiazine cream. "It's a burn cream because there's silver in it, so it prevents the burns from being infected," said Dr. Jeanne Lee, interim burn director at the the regional burn center at the University of California at San Diego. "But it doesn't help in terms of debriding a burn or necessarily helping it heal." READ MORE: First Look: Plumbing the mysteries of sweat to help burn patients cool their skin The gauze-and-cream dressing must be changed every day, a painful process. In the burn unit at Fortaleza's Jos√© Frota Institute, patients contort as their wounds are unwrapped and washed. Enter the humble tilapia, a fish that's widely farmed in Brazil and whose skin, until now, was considered trash. Unlike the gauze bandages, the sterilized tilapia skin goes on and stays on. The first step in the research process was to analyze the fish skin. "We got a great surprise when we saw that the amount of collagen proteins, types 1 and 3, which are very important for scarring, exist in large quantities in tilapia skin, even more than in human skin and other skins," Maciel said. "Another factor we discovered is that the amount of tension, of resistance in tilapia skin is much greater than in human skin. Also the amount of moisture." In patients with superficial second-degree burns, the doctors apply the fish skin and leave it until the patient scars naturally. For deep second-degree burns, the tilapia bandages must be changed a few times over several weeks of treatment, but still far less often than the gauze with cream. The tilapia treatment also cuts down healing time by up to several days and reduces the use of pain medication, Maciel said. Ant√¥nio dos Santos, a fisherman, was offered the tilapia treatment as part of a clinical trial after he sustained burns to his entire right arm when a gas canister on his boat exploded. He accepted. "After they put on the tilapia skin, it really relieved the pain," he said. "I thought it was really interesting that something like this could work." READ MORE: High-tech bandage wins $100K from Boston Marathon bombing survivor's family The initial batches of tilapia skin were studied and prepared by a team of researchers at the Federal University of Cear√°. Lab technicians used various sterilizing agents, then sent the skins for radiation in S√£o Paulo to kill viruses, before packaging and refrigerating the skins. Once cleaned and treated, they can last for up to two years. In the US, animal-based skin substitutes require levels of scrutiny from the Food and Drug Administration and animal rights groups that can drive up costs, Lee said. Given the substantial supply of donated human skin, tilapia skin is unlikely to arrive at American hospitals anytime soon. But it may be a boon in developing countries. "I'm willing to use anything that might actually help a patient," Lee said. "It may be a good option depending on what country you're talking about. But I also think the problem is that you need to find places that have the resources to actually process the skin and sterilize it, and make sure it doesn't have diseases." In Brazil, in addition to the clinical trials, researchers are currently conducting histological studies that compare the composition of human, tilapia, pig, and frog skins. They are also conducting studies on the comparative costs of tilapia skin and conventional burn treatments. If clinical trials show continued success, doctors hope a company will process the skins on an industrial scale and sell it to the public health system. This article is reproduced with permission from STAT. It was first published on Mar. 2, 2017. Find the original story here. A free press is a cornerstone of a healthy democracy. Support trusted journalism and civil dialogue. Donate now By ‚Äî Nadia Sussman, STAT Nadia Sussman, STAT&lt;/p&gt;
    &lt;p&gt;FORTAZELA, Brazil ‚Äî In this historic city by the sea in northeast Brazil, burn patients look as if they've emerged from the waves. They are covered in fish skin ‚Äî specifically strips of sterilized tilapia. Doctors here are testing the skin of the popular fish as a bandage for second- and third-degree burns. The innovation arose from an unmet need. Animal skin has long been used in the treatment of burns in developed countries. But Brazil lacks the human skin, pig skin, and artificial alternatives that are widely available in the US. The three functional skin banks in Brazil can meet only 1 percent of the national demand, said Dr. Edmar Maciel, a plastic surgeon and burn specialist leading the clinical trials with tilapia skin. As a result, public health patients in Brazil are normally bandaged with gauze and silver sulfadiazine cream. "It's a burn cream because there's silver in it, so it prevents the burns from being infected," said Dr. Jeanne Lee, interim burn director at the the regional burn center at the University of California at San Diego. "But it doesn't help in terms of debriding a burn or necessarily helping it heal." READ MORE: First Look: Plumbing the mysteries of sweat to help burn patients cool their skin The gauze-and-cream dressing must be changed every day, a painful process. In the burn unit at Fortaleza's Jos√© Frota Institute, patients contort as their wounds are unwrapped and washed. Enter the humble tilapia, a fish that's widely farmed in Brazil and whose skin, until now, was considered trash. Unlike the gauze bandages, the sterilized tilapia skin goes on and stays on. The first step in the research process was to analyze the fish skin. "We got a great surprise when we saw that the amount of collagen proteins, types 1 and 3, which are very important for scarring, exist in large quantities in tilapia skin, even more than in human skin and other skins," Maciel said. "Another factor we discovered is that the amount of tension, of resistance in tilapia skin is much greater than in human skin. Also the amount of moisture." In patients with superficial second-degree burns, the doctors apply the fish skin and leave it until the patient scars naturally. For deep second-degree burns, the tilapia bandages must be changed a few times over several weeks of treatment, but still far less often than the gauze with cream. The tilapia treatment also cuts down healing time by up to several days and reduces the use of pain medication, Maciel said. Ant√¥nio dos Santos, a fisherman, was offered the tilapia treatment as part of a clinical trial after he sustained burns to his entire right arm when a gas canister on his boat exploded. He accepted. "After they put on the tilapia skin, it really relieved the pain," he said. "I thought it was really interesting that something like this could work." READ MORE: High-tech bandage wins $100K from Boston Marathon bombing survivor's family The initial batches of tilapia skin were studied and prepared by a team of researchers at the Federal University of Cear√°. Lab technicians used various sterilizing agents, then sent the skins for radiation in S√£o Paulo to kill viruses, before packaging and refrigerating the skins. Once cleaned and treated, they can last for up to two years. In the US, animal-based skin substitutes require levels of scrutiny from the Food and Drug Administration and animal rights groups that can drive up costs, Lee said. Given the substantial supply of donated human skin, tilapia skin is unlikely to arrive at American hospitals anytime soon. But it may be a boon in developing countries. "I'm willing to use anything that might actually help a patient," Lee said. "It may be a good option depending on what country you're talking about. But I also think the problem is that you need to find places that have the resources to actually process the skin and sterilize it, and make sure it doesn't have diseases." In Brazil, in addition to the clinical trials, researchers are currently conducting histological studies that compare the composition of human, tilapia, pig, and frog skins. They are also conducting studies on the comparative costs of tilapia skin and conventional burn treatments. If clinical trials show continued success, doctors hope a company will process the skins on an industrial scale and sell it to the public health system. This article is reproduced with permission from STAT. It was first published on Mar. 2, 2017. Find the original story here. A free press is a cornerstone of a healthy democracy. Support trusted journalism and civil dialogue. Donate now&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pbs.org/newshour/health/brazilian-city-uses-tilapia-fish-skin-treat-burn-victims"/><published>2026-01-22T05:15:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46716469</id><title>30 Years of ReactOS</title><updated>2026-01-22T13:24:39.105450+00:00</updated><content>&lt;doc fingerprint="6f9d8a1678567905"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;30 years of ReactOS&lt;/head&gt;
    &lt;p&gt;by Carl Bialorucki | January 22, 2026&lt;/p&gt;
    &lt;p&gt;Happy Birthday ReactOS! Today marks 30 years since the first commit to the ReactOS source tree. It√¢s been such a long journey that many of our contributors today, including myself, were not alive during this event. Yet our mission to deliver √¢your favorite Windows apps and drivers in an open-source environment you can trust√¢ continues to bring people together. Let‚Äôs take a brief look at some of the high and low points throughout our history.&lt;/p&gt;
    &lt;head rend="h2"&gt;1996-2003: The Painful Road to ReactOS 0.1.0&lt;/head&gt;
    &lt;p&gt;ReactOS started from the ashes of the FreeWin95 project, which aimed to provide a free and open-source clone of Windows 95. FreeWin95 suffered from analysis paralysis, attempting to plan the whole system before writing any code. Tired of the lack of progress on the project, Jason Filby took the reins as project coordinator and led a new effort targeting Windows NT. The project was renamed to √¢ReactOS√¢ as it was a reaction to Microsoft√¢s monopolistic position in home computer operating systems.&lt;/p&gt;
    &lt;p&gt;Progress on ReactOS was very slow at first. Contributors had to first build a very basic NT-like kernel before they could develop drivers for it, then continue developing the kernel; not too dissimilar to the process of bootstrapping a new programming language. Once a few basic drivers were written, other contributors were able to learn from these examples and develop other drivers.&lt;/p&gt;
    &lt;p&gt;While writing this article, I reached out to Eric Kohl. He developed the original storage driver stack for ReactOS (atapi, scsiport, class2, disk, cdrom, cdfs) and has been with the project since 1998. I asked him about his experiences with ReactOS during this time, how he found the project, and what contributing to ReactOS was like during those early days. He wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I think I found ReactOS while searching for example code for my contributions to the WINE project. I subscribed to the mailing list and followed the discussions for a few days. The developers were discussing the future of shell.exe, a little command line interpreter that could only change drives and directories and execute programs. A few days [later] I had started to convert the FreeDOS command.com into a Win32 console application, because I wanted to extend it to make it 4DOS compatible. 4DOS was a very powerful command line interpreter. On December 4th, 1998 I introduced myself and suggested to use my converted FreeDOS command.com as the future ReactOS cmd.exe. I had a little conversation with Jason Filby and Rex Joliff, the CVS repository maintainer. I sent my cmd.exe code to Rex and he applied it to the repository. After applying a few more cmd-related patches over the next weeks, Rex asked me whether I would like to have write-access to the repository. I accepted the offer‚Ä¶&lt;/p&gt;
      &lt;p&gt;‚Ä¶&lt;/p&gt;
      &lt;p&gt;The first version I downloaded and used was 0.0.8. It was not much more than a DOS-based bootloader, some drivers, and a basic kernel that ran a few test routines after initialization.&lt;/p&gt;
      &lt;p&gt;‚Ä¶&lt;/p&gt;
      &lt;p&gt;Version 0.0.8 didn‚Äôt use PE files, but flat (position independent) binaries. There was no PE loader, no smss, no csrss, no winlogon, no process heaps, no process environments, no threads, etc. Each and every little feature was a milestone.&lt;/p&gt;
      &lt;p&gt;‚Ä¶&lt;/p&gt;
      &lt;p&gt;Initially there was not a review process at all. You write some code, test it and fix it until it works. Then you commit it. If something failed on another machine, you got a reply on the mailing list and discussed a solution. You fixed the issue and committed a fix. That‚Äôs how it worked.&lt;/p&gt;
      &lt;p&gt;‚Ä¶&lt;/p&gt;
      &lt;p&gt;There was always an open and friendly atmosphere. It was and still is always nice to talk to other developers. No fights, no wars, like in some other projects.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Editors note: minor errors were corrected.&lt;/p&gt;
    &lt;p&gt;ReactOS 0.1.0 was released on February 1st, 2003 and received minor updates up until November 2003. ReactOS 0.1.0 was the first version of ReactOS that could boot from a CD. It had a command line interface and no desktop. Watch a demo of it below, provided courtesy of archeYR.&lt;/p&gt;
    &lt;head rend="h2"&gt;2003-2006: ReactOS 0.2.x&lt;/head&gt;
    &lt;p&gt;During this period ReactOS saw rapid development. New drivers were being built all the time, a basic desktop was built, and ReactOS became increasingly stable and usable. Public interest grew as ReactOS matured. In October 2005, Jason Filby stepped down as project coordinator, and Steven Edwards was voted to be the next project coordinator.&lt;/p&gt;
    &lt;p&gt;It wasn√¢t all sunshine and rainbows though. In January 2006, concerns grew about contributors having access to leaked Windows source code and possibly using this leaked source code in their contributions. In response, Steven Edwards strengthened the project√¢s intellectual property policy and the project made the difficult decision to audit the existing source code and temporarily freeze contributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;2006-2016: ReactOS 0.3.x&lt;/head&gt;
    &lt;p&gt;The ongoing audit and contribution freeze from the end of the ReactOS 0.2.x era slowed development and momentum considerably for ReactOS 0.3.x. Following challenges with the audit, Steven Edwards stepped down as project coordinator and Aleksey Bragin assumed the role by August 2006.&lt;/p&gt;
    &lt;p&gt;Despite the challenges during this time, ReactOS 0.3.x continued to build upon ReactOS√¢s legacy. ReactOS 0.3.0 was released on August 28th, 2006. It introduced networking support and a package manager called √¢Download!√¢. This package manager would become the basis for RAPPS, the package manager built into modern versions of ReactOS. In July 2008, the x86_64 port of ReactOS was started. One year later, ReactOS 0.3.10 imported the UniATA driver, written by Alexandr Telyatnikov (Alter). While we run into limitations with the UniATA driver today, UniATA enabled ReactOS to support SATA storage devices and to support partitions greater than 8GB in size. On February 8th, 2012, ReactOS 0.3.14 supported being built using the MSVC compiler and added visual style support.&lt;/p&gt;
    &lt;head rend="h2"&gt;2016-Today: ReactOS 0.4.x&lt;/head&gt;
    &lt;p&gt;ReactOS 0.4.0 was released on February 16th, 2016. It introduced a new graphical shell that utilized more Windows features and was more similar architecturally to Windows Explorer. ReactOS 0.4.0 also introduced support for kernel debugging using WinDbg when compiled with MSVC. Being able to use standard Windows tools for kernel debugging has helped us progress considerably. ReactOS 0.4.0 continued to receive incremental updates every few months up until versions 0.4.14 and 0.4.15 which had years of development updates each. Today, the x86_64 port of ReactOS is similarly functional to its x86 counterpart, but with no WoW64 subsystem to run x86 apps its usability is limited.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future of ReactOS&lt;/head&gt;
    &lt;p&gt;We‚Äôre continuing to move ReactOS forward. Behind the scenes there are several out-of-tree projects in development. Some of these exciting projects include a new build environment for developers (RosBE), a new NTFS driver, a new ATA driver, multi-processor (SMP) support, support for class 3 UEFI systems, kernel and usermode address space layout randomization (ASLR), and support for modern GPU drivers built on WDDM.&lt;/p&gt;
    &lt;p&gt;The future of ReactOS will be written by the people who believe in the mission and are willing to help carry it forward.&lt;/p&gt;
    &lt;p&gt;If you believe in running √¢your favorite Windows apps and drivers in an open-source environment you can trust√¢, you can help make that a reality by making a financial contribution, opening a pull request on GitHub, or testing and filing bug reports. Even small contributions can help a lot!&lt;/p&gt;
    &lt;head rend="h2"&gt;Statistics&lt;/head&gt;
    &lt;p&gt;Note: Statistics were calculated at commit f60b1c9&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Total commits: 88,198&lt;/item&gt;
      &lt;item&gt;Total unique contributors: 301&lt;/item&gt;
      &lt;item&gt;Total files: 31,025&lt;/item&gt;
      &lt;item&gt;Total lines of code: 14,929,578&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://reactos.org/blogs/30yrs-of-ros/"/><published>2026-01-22T08:03:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46716696</id><title>In Praise of APL (1977)</title><updated>2026-01-22T13:24:38.792388+00:00</updated><content>&lt;doc fingerprint="d7080b9921a12be0"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;In Praise of APL:&lt;/p&gt;
          &lt;p&gt;Many reasons can be given for teaching one or more aspects of computer science (defined as the study of the set of phenomena arising around and because of the computer) to all university students. Probably every reader of this note supports some of these reasons. Let me list the few I find most important: (1) to understand and to be able to compose algorithms; (2) to understand how computers are organized and constructed; (3) to develop fluency in (at least) one programming language; (4) to appreciate the inevitability of controlling complexity through the design of systems; (5) to appreciate the devotion of computer scientists to their subject and the exterior consequences (to the student as citizen) of the science‚Äôs development.&lt;/p&gt;
          &lt;p&gt;Even though computer science deals with symbolic objects whose nature we study mathematically, it cannot be taught as an orderly development arising from a few fundamental ideas whose existence the student has already observed intuitively during his maturation, such as gravitation and electricity.&lt;/p&gt;
          &lt;p&gt;It is during this first computer course that the student awakes to the possibilities and consequences of computation. They arise most usefully and in greatest profusion during his writing of programs. He must program and program and program! He must learn how to state precisely in a programming language what he perceives about the nature of symbolic systems. I know of no better way to expedite this awakening than by programming.&lt;/p&gt;
          &lt;p&gt;But what should the student program? and in what language? I do not place much emphasis on heavy use of other people‚Äôs programs, packages if you will, that perform real services such as statistical packages, data management systems, linear equations solvers, etc. While it is wise to use standard programs when they match one‚Äôs needs, it is more important to master self-expression during this initial contact.&lt;/p&gt;
          &lt;p&gt;Available time is a limiting factor; a semester provides about 16 weeks of contact. During that interval the student must negotiate a set of tasks that sharpens his abilities and explodes his perceptions of the computer‚Äôs capabilities. He must be on the computer early and often during the semester and his approach to it must be smooth and easy. The computer system he uses should be time-sharing and interactive, if you will.&lt;/p&gt;
          &lt;p&gt;Learning to program involves a sequence of acts of discovery punctuated by recovery from errors. As the semester progresses the causes and nature of errors will change. Certain kinds will diminish and even disappear, only to be replaced by errors of deeper significance, harder to isolate and more resistant to satisfactory removal ‚Äî syntactic gaffes give way to semantic errors ‚Äî incorrect perceptions of purpose, improper use of means, the use of hammers to swat flies and swatters to level mountains.&lt;/p&gt;
          &lt;p&gt;To write correct and balanced programs a student may be forced to move between programs that are not related to each other by a few simple textual rearrangements. He must learn to write and test complicated programs quite rapidly. As he moves through the sequence of assigned tasks his ability to express himself fluently should not founder too soon because of language shortcomings. For all of the above reasons as well as a few others, I have come to believe that APL is the most rational first language for a first course in computer science.&lt;/p&gt;
          &lt;p&gt;It is true that BASIC and FORTRAN are easier to learn than APL, for example, a week versus a month. However, once mastered, APL fits the above requirements much better than either BASIC or FORTRAN or their successors ALGOL 60, PL/I and Pascal. The syntax of APL is not a significant difficulty for the students. The large number of primitive functions, at first mind-numbing in their capabilities, quickly turn out to be easily mastered, soon almost all are used naturally in every program ‚Äî the primitive functions form a harmonious and useful set. As a data organization, arrays turn out to be extraordinarily useful (though prolonged contact with APL makes one wish for the added presence of more heterogeneous structures).&lt;/p&gt;
          &lt;p&gt;Style and Idiom&lt;/p&gt;
          &lt;p&gt;The virtues of APL that strike the programmer most sharply are its terseness ‚Äî complicated acts can be described briefly, its flexibility ‚Äî there are a large number of ways to state even moderately complicated tasks (the language provides choices that match divergent views of algorithm construction), and its composability ‚Äî there is the possibility to construct sentences ‚Äî one-liners as they are commonly called ‚Äî that approach in the flow of phrase organization, sequencing and imbedding, the artistic possibilities achievable in natural language prose.&lt;/p&gt;
          &lt;p&gt;The sweep of the eye across a single sentence can expose an intricate, ingenious and beautiful interplay of operation and control that in other programming languages is observable only in several pages of text. One begins to appreciate the emergence and significance of style and to observe that reading and writing facility is tied to the development of an arsenal of idioms which soon become engraved in one‚Äôs skull as units.&lt;/p&gt;
          &lt;p&gt;The combination of these three factors makes it possible to develop an excellent set of exercises in the first course. These exercises can be large in number, cover a wide range of topics and vary widely in complexity, and still be done during the 16 week period. The later exercises can be tied to the design and development of a system ‚Äî a collection of procedures that, in varying combinations, perform a set of tasks.&lt;/p&gt;
          &lt;p&gt;In Teaching Computer Organization&lt;/p&gt;
          &lt;p&gt;To appreciate computer science one requires an understanding of the computer. Once the student understands the computer ‚Äî its macroscopic components monitored by the fetch-execute cycle and its apparent complexity being controlled by gigantic replication of a few simple logical elements ‚Äî he can become aware of the important equilibrium between hardware and software ‚Äî the shifting of function between the two as determined by economic factors ‚Äî and between algorithm and system as determined by traffic and variation. Using APL it is straightforward to model a computer and to illustrate any of its macroscopic or microscopic components at any level of detail. The programs to perform these functions at every level of description remain small and manageable ‚Äî about 40 lines or so.&lt;/p&gt;
          &lt;p&gt;The development of software, e.g., a machine language assembler, is a task of similar difficulty (about 40 lines) and hence possible within the confines of a first course.&lt;/p&gt;
          &lt;p&gt;Word processing and graphics, increasingly important application areas of computers, can be explored with exercises of no great size, e.g., to do permuted-index of title lists (~12 lines), display, rotation and scaling of composites of polygons (~20 lines), graphing of functions (~5 lines), etc.&lt;/p&gt;
          &lt;p&gt;With (or even without) the use of 2 or 3 pre-built functions, file processing problems such as payroll, personnel search, etc. can be written in a relatively few lines.&lt;/p&gt;
          &lt;p&gt;An important consequence of the attainable brevity of these compositions cannot be ignored: the student becomes aware that he need not be forced to depend upon external, pre-packaged and elaborate systems to do what are really simple programming tasks. Instead of learning a new coding etiquette to negotiate a complex external system, he writes his own programs, develops his own systems tailor-made to his own needs and understood at all levels of detail by him. If anything is meant by man-machine symbiosis, it is the existence of such abilities on the man side of the ‚Äúmembrane‚Äù, for there is no partnership here between man and machine, merely the existence of a growing, but never perfectly organized, inventory of tools that the competent can pick among, adapt and use to multiply his effective use of the computer.&lt;/p&gt;
          &lt;p&gt;I cannot overemphasize the importance of terseness, flexibility and phrase growth to a beginning student. His horizons of performance are being set in this first course. If he sees a task as a mountain then its reduction to molehill proportions is itself a considerable algorithmic task. While this is true of very large tasks, even when using APL this conscious chaining of organized reductions can be postponed until the student has already collected a large number of useful data-processing functions, engraved in his skull, with which to level mountains.&lt;/p&gt;
          &lt;p&gt;It is important to recognize that no matter how complicated the task, the APL functions will usually be anywhere from 1/5 to 1/10 the number of statements or lines, or what have you, of a FBAPP (FORTRAN or BASIC or ALGOL or PL/I or Pascal) program. Since APL distributes, through its primitive functions, control that the other languages explicate via sequences of statements dominated by explicit control statements, errors in APL programs tend to be far fewer in number than in their correspondents in FBAPP.&lt;/p&gt;
          &lt;p&gt;I can come now to the topics of structured programming and program verification. Both are important, but their content and importance depend strongly on the language in which programs are couched. A program is well-structured if it has a high degree of lexical continuity: small changes in program capability are acquired by making changes within lexically close text (modularization).&lt;/p&gt;
          &lt;p&gt;Since APL has a greater density of function within a given lexical scope than FBAPP, one would expect that APL programs will support considerably more structure than equivalent size FBAPP programs. Put another way, since the APL programs are 1/5 to 1/10 the size of FBAPP programs, the consequences to APL programs of weak structuring are less disastrous. Recovery from design mistakes is more rapid. Since we can only structure what we already understand, the delay in arriving at stable program organization should be considerably less with FBAPP!&lt;/p&gt;
          &lt;p&gt;Please note that the emphasis here is on the control of propagation of relationships, not the nonsense of restricting goto or bathing programs in cascades of while loops.&lt;/p&gt;
          &lt;p&gt;The verification, formal or informal, of programs is a natural and important activity. It is linked to specification: what we can‚Äôt specify we can‚Äôt verify. By specification we mean stating what is to be output for a given input. We immediately observe that, since specification in FBAPP is extremely tedious and unnatural, we must use some other language. APL turns out to be quite good and has often been suggested as a specification language. Assertions and verification conditions can be much more easily expressed as APL predicates than as FBAPP predicates. Because of the widespread distribution of control into the semantics of primitive functions, for which no proof steps need then be given, APL verifications tend to be, just as their counterpart APL programs, shorter and more analytic than equivalent FBAPP program verifications.&lt;/p&gt;
          &lt;p&gt;APL and Architecture&lt;/p&gt;
          &lt;p&gt;The form of the FBAPP languages follows closely the structure of the computers that prevailed during their inception. They have the nice property that one may often optimize machine performance of their compiled programs by transforming FBAPP programs to other FBAPP programs. Control of the computer is more easily exercised with programs in these languages than with APL, since the latter is more independent of current machines. For many programs this control over the target machine performance is quite vital, and APL couples more weakly to the standard computer than does FBAPP.&lt;/p&gt;
          &lt;p&gt;However, new array processing computers are beginning to appear and, had they been standard 20 years ago, APL and not FORTRAN would have been the prototype of language development. I often wonder at what descriptive levels we would be programming today had that been the case! Since it was not the case, we should not throw out or limit APL. We must seek ways to match it to the common computer. We must design compilers as well as computers that fit APL better.&lt;/p&gt;
          &lt;p&gt;More Cost-Effective than BASIC&lt;/p&gt;
          &lt;p&gt;Cost is an important issue in the instructional process. An APL computer system currently costs about $10K per terminal, about twice the cost of a BASIC system. As APL system designs stabilize and integrated circuitry costs drop, the two figures will coincide at or near the cost of a contemporary terminal. However, even now the APL system is cheaper than BASIC systems for equivalent work loads because one can do more than twice as much with APL in a given period of time than with BASIC!&lt;/p&gt;
          &lt;p&gt;Let me mention in closing two additional issues regarding the use of APL in an introductory computer science course. First, most university computer scientists don‚Äôt really know APL. They haven‚Äôt appreciated what it means to think in APL ‚Äî to think about parallel operations in arrays and to distribute and submerge explicit looping among its primitive functions. I am reminded of the difficulties many math departments experience when they try to replace calculus by a fine math and combinatorics course as the first meat and potatoes offering by the department to the university. However at Yale we have found that faculty outside the software milieu ‚Äî in theory, for example ‚Äî pick up APL quite fast and prefer it to FBAPP. I am sure the same is true elsewhere.&lt;/p&gt;
          &lt;p&gt;The second issue is of a different kind. I am firmly convinced that APL and LISP are related to each other along an important axis of language design and that acquiring simultaneous expertise in both languages is possible and desirable for the beginning student. Were they unified, the set of tasks that succumb to terse, flexible and expressive descriptions will enlarge enormously without overly increasing the intellectual burden on the student over his initial 16 week contact period.&lt;/p&gt;
          &lt;p&gt;Above all, remember what we must provide is a pou sto to last the student for 40 years, not a handbook for tomorrow‚Äôs employment.&lt;/p&gt;
          &lt;p&gt;First appeared in SIAM News, 1977-06.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jsoftware.com/papers/perlis77.htm"/><published>2026-01-22T08:44:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46717556</id><title>We will ban you and ridicule you in public if you waste our time on crap reports</title><updated>2026-01-22T13:24:38.386928+00:00</updated><link href="https://curl.se/.well-known/security.txt"/><published>2026-01-22T10:48:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46718061</id><title>Design Thinking Books You Must Read</title><updated>2026-01-22T13:24:37.797370+00:00</updated><content>&lt;doc fingerprint="9d38fa0109b22121"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Design Thinking Books You Must Read (updated)&lt;/head&gt;
    &lt;p&gt;Can you think that following a design thinking process with five steps turns you into a creative innovator?! Believe me, it isn‚Äôt and never has been this way. The spread of the term design thinking is aligned with a significant amount of misleading criticism. The doubts about the effectiveness of design thinking are influenced by the promotional language used by some companies, training places, and public speakers. The truth is that there is no secret recipe to turn someone into a creative designer. Yet, there is a way to use the design expertise inside each of us. Understanding the design thinking core values can help team members improve their design ability and appreciate the creative practice inside the organization to achieve the next competitive advantage. This is why I wanted to share with you those key design thinking books to learn the core principles underpinning the design practice.&lt;/p&gt;
    &lt;p&gt;This is an updated list of design thinking books that I keep adding to their new book suggestions. So, please keep the link or subscribe to the newsletters to receive updates once new books are added. I am also starting to add papers that represent the cornerstone in the design thinking principles that I believe are as important as the book. In this update, two books and one paper added: The Science of Artificial, Wicked Problems in Design Thinking, and How Designers Think.&lt;/p&gt;
    &lt;p&gt;Previously, we explored different challenges that can be faced when applying design thinking inside the organization ( Why Companies Need to Apply Design Thinking and Why Companies Need to Apply Design Thinking). The majority of these factors rely on the lack of understanding of the core value of design thinking, which can be a reason for over-promotion and misuse of a commercialized language (check Why Design Thinking Doesn‚Äôt Work). Above all, many design thinking trainers are not designers themselves and never practice the creative practice before teaching it which causes the gap between classrooms and practices.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Thinking Books&lt;/head&gt;
    &lt;p&gt;To expand my knowledge of the core values behind design thinking, I thought I would share with you some of the book titles that highlight design characteristics. Each of these books explores design from a specific perspective. Learning about these design aspects is essential for both designers and non-designers before jumping to learn design thinking. While there are several books about design thinking toolkits, the books below don‚Äôt teach you to design thinking methodology but the core principles behind design thinking to develop new alternatives of ideas and improve the analytical thinking of problems and solutions. They aim to guide you in understanding the core values and practices of design as a collaborative process. By acquiring this knowledge, you can effectively apply any of the design thinking processes we discussed earlier in previous articles with effectiveness. I am sure that those are not the only books out there, so please share with us your book suggestions in the comments below the article.&lt;/p&gt;
    &lt;p&gt;Related article:&lt;/p&gt;
    &lt;p&gt;The Double Diamond Design Thinking Process and How to Use it&lt;/p&gt;
    &lt;p&gt;What is Design? And What is not?&lt;/p&gt;
    &lt;p&gt;Design Thinking Guide: What, Why and How&lt;/p&gt;
    &lt;p&gt;Why Design Thinking Doesn‚Äôt Work&lt;/p&gt;
    &lt;p&gt;Measuring the Impact of Design Thinking&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Expertise by Kees Dorst&lt;/head&gt;
    &lt;p&gt;The Design Expertise, written by Lawson and Dorst, focuses on the understanding of design practice in the creative industry. The book aims to explore the nature of design from a practitioner‚Äôs perspective. It starts by exploring the different definitions of design and how they contributed to identifying the border of the discipline of design.&lt;/p&gt;
    &lt;p&gt;The book presents design work for different designers and tries to use this overview of their work to provide a practical example of design characteristics. This book provides you with a base idea about design, what it is, and its characteristics. Exploring the characteristics through design thinking case studies, and examples helps you see design‚Äôs core value. This value is the main cornerstone behind the application of the design thinking process.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frame Innovation by Kees Dorst&lt;/head&gt;
    &lt;p&gt;One of the main design characteristics is to solve problems or move from one position to an improved one. However, this can‚Äôt be achieved without a clear idea of the problem and its different borders. In his book Frame Innovation, Dorst explores the cognitive design process‚Äôs problem and solution frames. Also, he explores how designers move from one frame to another and how this feedback process contributes toward an optimum solution for wicked problems.&lt;/p&gt;
    &lt;p&gt;Many of the design thinking process models move from the exploration stage (divergent) to defining the solution (conversion). While this practice shares the principle of critical thinking, they all move between the problem frame and solution frame. Through this book, you will explore the principles and practices of problem/solution frames to develop creative potential ideas.&lt;/p&gt;
    &lt;p&gt;The book extends discussion of of the principle of frame innovation by covering the opportunities and challenges related to its application in creative industries. The book ends by putting a practice action plan to move toward using the frame innovation in different business models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Thinking: Understanding How Designers Think and Work&lt;/head&gt;
    &lt;p&gt;In this small yet informative book, Design Thinking, Nigel Cross explores how designers think and reach creative ideas in the design field and the nature of design from the perspective of idea formation. To this goal, the book overviews design practice based on observing and interviewing creative designers and exploring expert tips with them. Design processes try to explore the design expertise from creating the idea to applying it. However, the design ability comes earlier when ideas are formulated. The book‚Äôs first chapter explores this design ability and how each of us has a level of design ability to develop new ideas. Yet, some people are more designers than others, which is known in Lusy Kimbel‚Äôs two papers as the creative class (Rethinking Design Thinking: Part 1 and Part 2).&lt;/p&gt;
    &lt;p&gt;The book overviews designers‚Äô practice in different fields and stories. The aim of this overview through creative designers‚Äô experience is to build an understanding of the inspiration or exploration stage in the design thinking process. For instance, what is brainstorming, and why is it applied at an early point in the design thinking process (How to Successfully Apply Inspiration in Design Thinking)? Linking similar questions to the practice helps you map your practice to rational reasoning and subsequently improves the progress of the process in the future.&lt;/p&gt;
    &lt;head rend="h3"&gt;Change by Design by Tim Brown&lt;/head&gt;
    &lt;p&gt;Change by Design, by Tim Bowen, CEO of the IDEO, is probably one of the commonly known books about design thinking because of the popularity of the IDEO in the application of design thinking in various social innovation contexts. In his book, Tim Brown manifests his ideology about design thinking and interprets it from the organisational perspective. The book aims to clarify what design thinking is, and where to go from theory to practice. In the first part, the books focus on the main concepts of design thinking (check Design Thinking Tools and Methods Complete Guide), such as extending behind the aesthetics, shifting toward a human-centred approach (i.e. improving customer experience and building inclusive design), the power of prototyping, and the importance of storytelling. The second part of the book aims to interpret these principles for practicality to identify the business opportunities for design thinking and the use of design to achieve innovation inside organisations through creative collaboration between stakeholders.&lt;/p&gt;
    &lt;p&gt;The book is a good resource for both designers and business people to understand design thinking and its applications. Despite several criticisms of the IDEO design thinking model, the book describes the theoretical base of design thinking, which could have a positive, innovative impact on organisations, especially if applied properly to develop viable business strategies. The IDEO Field Guide can be a good companion for the book as it presents a toolbox to apply Tim Brown‚Äôs ideology in practice.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Design of Everyday Things by Don Norman&lt;/head&gt;
    &lt;p&gt;Don Norman is one of the leading professors in behaviour psychology and human-computer interaction (HCI). His book, The Design of Everyday Things, is based on a simple observation: why do we love and hate some elements in our lives? And what is the psychology behind our behaviour toward products? Addressing these two questions presents a cornerstone of your design practice. For example, why do some people love products such as Apple, Mini Cooper, or IKEA? By understanding how consumers love or hate products, the design team can target these features to build an empathic relationship between the product (or service) and the client, known as emphatic design.&lt;/p&gt;
    &lt;p&gt;The book explores human-centred design and its impact on usability interaction design principles, as well as user experience. While other books covered this aspect of design experience, Norman studied the experience from a psychological point of view to examine this complex design process. The book covers the psychology behind our daily actions, knowledge, design limitations, and human error. Later, the book explores design thinking as a tool to solve problems and the usage of the Design Council Double Diamond design thinking process. The book is not only for UX designers but for designers from different practices, as you can learn the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;How the brain works and the psychology related to products and services,&lt;/item&gt;
      &lt;item&gt;The limitations related to our experience with interacting with designs around and&lt;/item&gt;
      &lt;item&gt;Human error and a bad design causes .&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How Designers Think? by Bryan Lawson&lt;/head&gt;
    &lt;p&gt;How Designers Think? by Bryan Lawson is one of the design thinking books I recommend for my students who are still new to problem-solving and understanding the philosophical approach underpinning the problem and solution space in the design thinking process, How Designer Think for Bryan Lawson overviews the design definition, the relation between problem and solutions and the design thinking process.&lt;/p&gt;
    &lt;p&gt;Unlike other books, Lawson doesn‚Äôt aim to teach you his method or derive a specific point; it is more like a discussion book to allow you to reflect and synthesise on the design practice and finally come up with your conclusion. The book presents a flow of ideas as a case study, making it easy to understand and enjoyable for new readers in design thinking. I recommend reading it before moving to more advanced books such as The Science of Artificial.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Science of Artificial by Herbert Simon&lt;/head&gt;
    &lt;p&gt;The Science of Artificial is one of Simon‚Äôs most famous and irritating works based on three lectures for him at MIT in 1968, a year before the book was first published. The book discusses the nature of human thinking and the ‚Äúartefact.‚Äù In eight chapters, it explores how humans use artefacts to solve everyday problems. His expression of human rationale is expressed with three premises:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The limitations in the human‚Äôs cognitive ability&lt;/item&gt;
      &lt;item&gt;The time available to make a decision, and&lt;/item&gt;
      &lt;item&gt;The complexity of the problem&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on these three premises, he concluded that we are under the illusion that we can choose the optimal solution for problems. Instead, we find a way to determine the reasonable solutions (check What Are The Six Thinking Hats? And How to Use Them?).&lt;/p&gt;
    &lt;p&gt;Simon was awarded a Nobel Prize for his theory and its contribution to economic rationality. According to the above theory, Simon defined three problem-solving activities: the ability to conduct a heuristic search for alternatives, evaluate solutions, and allocate resources for search. He illustrates this concept in his statement:&lt;/p&gt;
    &lt;p&gt;‚Äù Human problem solving involves nothing more than varying mixtures of trial and error and selectivity. The selectivity derives from various rules of thumb, or heuristics, suggesting which paths should be tried first and which promising leads.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Wicked Problems in Design Thinking (Paper) by Richard Buchanan&lt;/head&gt;
    &lt;p&gt;Wicked Problems in Design Thinking by Buchanan was published in Design Studies in 1992. Buchanan linked design and analytical philosophy by understanding the design problem‚Äôs nature and elements. He discussed two terms: ‚Äúcategory‚Äù and ‚Äúplacement‚Äù, where we frame the different aspects of the problem. Buchanan describes them as follows:&lt;/p&gt;
    &lt;p&gt;‚ÄúUnderstanding the difference between a category and a placement is essential if design thinking is to be regarded as more than a series of creative accidents. Categories have fixed meanings that are accepted within the framework of a theory or a philosophy and serve as the basis for analysing what already exists. Placements have boundaries to shape and constrain meaning but are not rigidly fixed and determinate. The boundary of placement gives a context or orientation to thinking, but the application to a specific situation can generate a new perception of that situation and, hence, a new possibility to be tested. Therefore, placements are sources of new ideas and possibilities when applied to problems in concrete circumstances.‚Äù&lt;/p&gt;
    &lt;p&gt;The expandable nature of the ‚Äúplacement‚Äù presents a critical element of wicked problems and how we can see them as a universal concept whose boundaries can change based on the situation. This manifestation of the definition of the problem elements presented the cornerstone for Kees Dorst‚Äôs problem/solution frame discussed in the earlier book Frame Innovation (What is the 8D Problem-Solving? ).&lt;/p&gt;
    &lt;p&gt;His ideas of wicked problems link with Simon‚Äôs concept about the design thinking process and how it can be seen as a non-linear process where different design ideas interact in the design arena. Also, In this placement, Buchanan differentiated between four elements of the design thinking process:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Signs: material objects&lt;/item&gt;
      &lt;item&gt;Things: actions&lt;/item&gt;
      &lt;item&gt;Thoughts: complex systems or environments, which is a weird characterisation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As he links the above elements and the two terms described earlier (category vs placement), he describes the nature of wicked problems:&lt;lb/&gt;‚ÄúHowever, when a designer‚Äôs conceptual placements become categories of thinking, the result can be mannered imitations of an earlier invention that are no longer relevant to discovering specific possibilities in a new situation. Ideas are then forced onto a situation rather than discovered in the particularities and novel possibilities of that situation.‚Äù&lt;/p&gt;
    &lt;p&gt;The above manifestation describes how wicked problems are constructed and change over time, paving the way for a new perspective on problems and their analysis to identify new solutions (check also How to Use TRIZ in the Problem-Solving Process).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Dilemmas in a General Theory of Planning by Rittel and Webber&lt;/head&gt;
    &lt;p&gt;The Dilemmas in a General Theory of Planning by Rittel and Webber, despite its age, remains a seminal work that has significantly influenced the understanding of wicked problems. Published in 1969, this paper laid the groundwork for Kees Dorst‚Äôs Frame Innovation and Buchanan‚Äôs Wicked Problems, both of which we‚Äôve discussed. While the paper‚Äôs focus is on planning and policy science, its insights can be applied to problem definition and the process of solving them.&lt;/p&gt;
    &lt;p&gt;Rittel and Webber distinguished between two types of problems: tame and wicked problems. Tame problems are well-defined and clearly stated, and there is a clear direction to finding the solution, such as scientific and business problems (check how this concept influenced TRIZ problem-solving). In contrast, wicked problems are ill-defined, and we can‚Äôt define the problem until we reach a solution. However, a wicked problem is never solved, yet it moves from one state to an improved, desirable one.&lt;/p&gt;
    &lt;p&gt;The other nature of wicked problems is that we cannot reach a definitive formulation for them. To describe them, we need to develop an exhaustive inventory of conceivable solutions when asking questions about the problem. So, problem understanding and resolution are linked and change as we build an understanding of the problem at a particular moment in time.&lt;/p&gt;
    &lt;head rend="h2"&gt;The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda by Gilbert Cockton&lt;/head&gt;
    &lt;p&gt;As you can see, the above books and papers give us a novel look at design problems and how we perceive them. My question is, why do we see problems the way we used to? A big part of the answer lies in our language, which presents mental models that stand as barriers to seeing the core nature of problems, especially the wicked ones. Therefore, we needed new vocabulary that helped us to escape these constraints. The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda, by my PhD supervisor, Professor Gilbert Cockton, presents a cornerstone of new vocabularies that can help us see design thinking and how to solve problems.&lt;/p&gt;
    &lt;p&gt;The paper eliminated the so-called design thinking process, as the term employs a linear nature; while design thinking is far from linear, it is intersected activities. Cockton described the design practice as design arenas; these arenas are distinguishing ‚Äúartefacts‚Äù and ‚Äúmemoranda.‚Äù The ‚Äúartefact‚Äù represents the design outcome, and the ‚Äúmemoranda‚Äù is the thing to be borne in mind. This new terminology replaces the problem and solution spaces. However, the Latin root of an artefact means the product of change or doing some art. However, this term is limited as wicked problems are not understood until we solve them, which means artefacts. So, the outcome of the design arena may remain the same as the original state, or the change is against the target user, such as preventive design and design against crime. So, Cockton replaced the word artefact with A_tefact. The memoranda consist of three arenas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Beneficiaries: The purpose of design&lt;/item&gt;
      &lt;item&gt;Purposes: The Artefact and Evaluation&lt;/item&gt;
      &lt;item&gt;Evaluations: Modifications to the Artefact&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other terms were also introduced in the paper, such as episodes to replace stages (or phases) that are inherited from linear process age. The multiple foci (sequence by concurrency) replaced the centre of the process term to indicate the complex nature of the iteration with no simple way to describe it. The term ‚Äúiteration‚Äù is replaced with balanced concurrent drama, and validation is replaced with the term ‚Äúaxiofact,‚Äù or the value generated. The new terminology presented in Cockton‚Äôs paper allows us to escape the old mental model when addressing wicked problems. If you check the MPPF method in Design Thinking, which we discussed previously, you will find it a useful tool as it can help us address wicked problems.&lt;/p&gt;
    &lt;p&gt;Each of the above books and papers focuses on specific aspects of design and how we observe the design thinking practice driven by feedback from both academia and industry. The different design thinking models are based on appreciating these characteristics of design and encouraging it inside the organization. By applying the steps alone, you will never reach any improved status. You need to recognize these characteristics of design and try to practice them during the design process. Again, the above books came to my mind as key books in design. I am sure there are other titles. So, please share it in the comments below.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;p&gt;Brown, T. and Katz, B., 2011. Change by design. Journal of Product Innovation Management, 28(3), pp.381-383.&lt;/p&gt;
    &lt;p&gt;Buchanan, R., 1992. Wicked problems in design thinking. Design issues, 8(2), pp.5-21.&lt;/p&gt;
    &lt;p&gt;Cockton, G., 2017, May. New process, new vocabulary: Axiofact= a_tefact+ memoranda. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (pp. 747-757).&lt;/p&gt;
    &lt;p&gt;Cross, N., 2023. Design thinking: Understanding how designers think and work. Bloomsbury Publishing.&lt;/p&gt;
    &lt;p&gt;Dorst, K., 2015. Frame innovation: Create new thinking by design. MIT press.&lt;/p&gt;
    &lt;p&gt;Norman Donald, A., 2013. The design of everyday things. MIT Press.&lt;/p&gt;
    &lt;p&gt;Lawson, B., 2006. How designers think. Routledge.&lt;/p&gt;
    &lt;p&gt;Rittel, H.W. and Webber, M.M., 1973. Dilemmas in a general theory of planning. Policy sciences, 4(2), pp.155-169.&lt;/p&gt;
    &lt;p&gt;Simon, H.A., 1988. The science of design: Creating the artificial. Design Issues, pp.67-82.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.designorate.com/design-thinking-books/"/><published>2026-01-22T11:51:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46718140</id><title>Flowtel (YC W25) Is Hiring</title><updated>2026-01-22T13:24:37.231024+00:00</updated><link href="https://www.ycombinator.com/companies/flowtel/jobs/LaddaEz-founding-engineer-staff-senior"/><published>2026-01-22T12:00:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46718330</id><title>The first commercial space station, Haven-1, now undergoing assembly for launch</title><updated>2026-01-22T13:24:36.792103+00:00</updated><content>&lt;doc fingerprint="daff027f10ce2ce3"&gt;
  &lt;main&gt;
    &lt;p&gt;As Ars reported last week, NASA‚Äôs plan to replace the International Space Station with commercial space stations is running into a time crunch.&lt;/p&gt;
    &lt;p&gt;The sprawling International Space Station is due to be decommissioned less than five years from now, and the US space agency has yet to formally publish rules and requirements for the follow-on stations being designed and developed by several different private companies.&lt;/p&gt;
    &lt;p&gt;Although there are expected to be multiple bidders in ‚Äúphase two‚Äù of NASA‚Äôs commercial space station program, there are at present four main contenders: Voyager Technologies, Axiom Space, Blue Origin, and Vast Space. At some point later this year, the space agency is expected to select one, or more likely two, of these companies for larger contracts that will support their efforts to build their stations.&lt;/p&gt;
    &lt;p&gt;To get a sense of the overall landscape as the competition heats up, Ars recently interviewed Voyager chief executive Dylan Taylor about his company‚Äôs plans for a private station, Starlab. Today we are publishing an interview with Max Haot, the chief executive of Vast. The company is furthest along in terms of development, choosing to build a smaller, interim space station, Haven-1, capable of short-duration stays. Eventually, NASA wants facilities capable of continuous habitation, but it is not clear whether that will be a requirement starting in 2030.&lt;/p&gt;
    &lt;p&gt;Until today, Haven-1 had a public launch date of mid-2026. However, as Haot explained in our interview, that launch date is no longer tenable.&lt;/p&gt;
    &lt;p&gt;Ars: You‚Äôre slipping the launch of Haven-1 from the middle of this year to the first quarter of 2027. Why?&lt;/p&gt;
    &lt;p&gt;Max Haot: This is obviously our first space station, and we‚Äôre moving as safely and as fast as we can. That‚Äôs the date right now that we are confident we will meet. We‚Äôve been tracking that date, without slip, for quite a while. And that‚Äôs still a year, probably two years or even more, ahead of anyone else. It will be building the world‚Äôs first commercial space station from scratch, from an empty building and no team, in under four years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/space/2026/01/the-first-commercial-space-station-haven-1-is-now-undergoing-assembly-for-launch/"/><published>2026-01-22T12:22:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46718485</id><title>Satya Nadella: "We need to find something useful for AI"</title><updated>2026-01-22T13:24:36.675025+00:00</updated><content>&lt;doc fingerprint="a00311f2a31ad7d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft CEO warns that we must 'do something useful' with AI or they'll lose 'social permission' to burn electricity on it&lt;/head&gt;
    &lt;p&gt;Workers should learn AI skills and companies should use it because it's a "cognitive amplifier," claims Satya Nadella.&lt;/p&gt;
    &lt;p&gt;In a conversation at this year's rich person convention‚Äîaka the World Economic Forum‚ÄîMicrosoft CEO Satya Nadella warned that AI will lose public support unless it's used to "do something useful that changes the outcomes of people and communities and countries and industries."&lt;/p&gt;
    &lt;p&gt;"We will quickly lose even the social permission to take something like energy, which is a scarce resource, and use it to generate these tokens, if these tokens are not improving health outcomes, education outcomes, public sector efficiency, private sector competitiveness, across all sectors, small and large, right?" said Nadella. "And that, to me, is ultimately the goal."&lt;/p&gt;
    &lt;p&gt;On the supply side, Nadella says that AI companies and policy makers must build out "a ubiquitous grid of energy and tokens," which is the task currently making it impossible to buy a stick of RAM at a reasonable price. But after that, he says it's on employers and job seekers to, more or less, just start using AI.&lt;/p&gt;
    &lt;p&gt;"The demand side of this is a little bit like, every firm has to start by using it," said Nadella, throwing in some industry-standard hyperbole by calling AI a "cognitive amplifier" that gives you "access to infinite minds." The CEO added that the AI industry needs to encourage job seekers to pick up AI skills (undefined), in the same way people master Excel to make themselves more employable.&lt;/p&gt;
    &lt;p&gt;"People need to say, 'Oh, I pick up this AI skill, and now I'm a better provider of some product or service in the real economy," said Nadella.&lt;/p&gt;
    &lt;p&gt;He did at least provide one real example of what he means by all this: "When a doctor can ‚Ä¶ spend more time with the patient, because the AI is doing the transcription and entering the records in the EMR system, entering the right billing code so that the healthcare industry is better served across the payer, the provider, and the patient, ultimately‚Äîthat's an outcome that I think all of us can benefit from."&lt;/p&gt;
    &lt;p&gt;I wonder if I'll really want to spend more time talking to my doctor with an AI eavesdropper listening intently for reasons to reclassify my preventative care visit as a more expensive diagnostic visit (could we just redesign the US healthcare system instead?), but at least for some doctors, AI recording and note-taking tools have already been helpful. One study said that medical professionals reported "tremendous benefits" from using AI scribes, while calling for more research.&lt;/p&gt;
    &lt;p&gt;Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.&lt;/p&gt;
    &lt;p&gt;I also find automatic transcription tools useful, but if I were banking on general purpose LLMs being as revolutionary as personal computers and the internet, I'd find it worrying how many applications boil down to transcribing audio, summarizing text, and fetching code snippets.&lt;/p&gt;
    &lt;p&gt;There are reasons to be skeptical about the idea that we're going to reorganize society around these functions. LLMs are error-prone‚Äîa UK police chief just resigned over a Microsoft Copilot error‚Äîand a recent report by researchers associated with the MIT Media Lab suggests that despite billions in investment, "95% of organizations are getting zero return" from adopting AI.&lt;/p&gt;
    &lt;p&gt;Addressing the notion that AI is a bubble waiting to burst, Nadella said that it's only a bubble if tech company partnerships and infrastructure spending are all there is to it. He's confident, however, that AI will "bend the productivity curve" and bring "economic growth all around the world, not just economic growth driven by capital expense."&lt;/p&gt;
    &lt;p&gt;Just let me know when I can buy RAM again!&lt;/p&gt;
    &lt;p&gt;1. Best gaming laptop: Razer Blade 16&lt;/p&gt;
    &lt;p&gt;2. Best gaming PC: HP Omen 35L&lt;/p&gt;
    &lt;p&gt;3. Best handheld gaming PC: Lenovo Legion Go S SteamOS ed.&lt;/p&gt;
    &lt;p&gt;4. Best mini PC: Minisforum AtomMan G7 PT&lt;/p&gt;
    &lt;p&gt;5. Best VR headset: Meta Quest 3&lt;/p&gt;
    &lt;p&gt;Tyler grew up in Silicon Valley during the '80s and '90s, playing games like Zork and Arkanoid on early PCs. He was later captivated by Myst, SimCity, Civilization, Command &amp;amp; Conquer, all the shooters they call "boomer shooters" now, and PS1 classic Bushido Blade (that's right: he had Bleem!). Tyler joined PC Gamer in 2011, and today he's focused on the site's news coverage. His hobbies include amateur boxing and adding to his 1,200-plus hours in Rocket League.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pcgamer.com/software/ai/microsoft-ceo-warns-that-we-must-do-something-useful-with-ai-or-theyll-lose-social-permission-to-burn-electricity-on-it/"/><published>2026-01-22T12:41:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46718556</id><title>I am moving away from Scala</title><updated>2026-01-22T13:24:36.533237+00:00</updated><content>&lt;doc fingerprint="4370c41f00b38237"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I am moving away from Scala&lt;/head&gt;
    &lt;p&gt;I have been a Scala developer for almost ten years. It was a fascinating journey, marked by exciting projects and numerous meetings with talented software engineers. However, I decided to step out of Scala, and this post is about why I chose to do so.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I got into Scala&lt;/head&gt;
    &lt;p&gt;Like many other Scala developers, I came from the Java world. At my first job as a backend engineer, I got a project written in Play Framework. This framework is written in Scala, but you can also use it for Java development. When you read the documentation for Play, it gives you code examples for both Java and Scala, and the Scala code looked much cleaner and expressive. It was free from cumbersome, duplicated Java keywords, and the Scala examples also had fewer lines of code. So I got curious about the language. My curiosity was also driven by the fact that Scala was still in hype at that time. The language had a reputation as the next Java, eventually replacing its old, verbose cousin.&lt;/p&gt;
    &lt;p&gt;I started to learn Scala by completing an online course from Lausanne‚Äôs tech university (EPFL), where the language originates. Shortly after, I changed jobs, got a position as a Scala developer, and since then, I‚Äôve worked on many different projects with a Scala backend. I was happy where I was. The projects were interesting and challenging. But more importantly, all the colleagues, the fellow Scala developers, were bright engineers with whom it was a pleasure to work.&lt;/p&gt;
    &lt;p&gt;Nevertheless, after some years, my attitude to the language has changed. Several problems in the language‚Äôs ecosystem led me and other developers to abandon Scala. The primary trigger was the release of Scala 3, or to be more concrete, issues related to it. But apart from this, there are also other problems with the language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scala 3 adaptation issues&lt;/head&gt;
    &lt;p&gt;Scala 3 was released in 2021. It was a long-desired upgrade full of tasty features, and every Scala developer couldn‚Äôt wait to try them. It is already 2025, but the Scala community has not fully switched to version 3. Moreover, most projects I know still use Scala 2. The reason is that the new major version of the language has several significant issues that hinder its adoption.&lt;/p&gt;
    &lt;p&gt;First of all, there is no backward compatibility, which makes it challenging to upgrade the existing applications. The lack of backward compatibility was not a big surprise. EPFL made it clear long before the official release. However, EPFL did not make a big deal of it, claiming that ‚Äúsince Scala is statically typed, the upgrade will be much easier than the upgrade from Python 2 to 3, for instance‚Äù. The reality is that it is not easy, and you‚Äôll need several sprints to upgrade an average-sized codebase. It is a very long time for a tech debt story that does not bring much value. Yes, I wrote before that the new features looked appealing. The third version finally got the union types, proper enums, nice syntactic sugar improvements, less confusing implicits, and many other things. But you could write workable and maintainable software before, using Scala 2, without all these syntax improvements.&lt;/p&gt;
    &lt;p&gt;Another problem with the upgrade is that many popular Scala libraries started to support the third version very late or have not started it at all yet. So even if you managed to upgrade your application, you still have to use dependencies from the previous major version of the language, with all the risks of compatibility. You can understand the maintainers of the libraries very well: It‚Äôs highly annoying to support two parallel versions of a language. Many of them lacked a compelling reason to migrate to Scala 3, as most of their users remained on the second version.&lt;/p&gt;
    &lt;p&gt;I also noticed an opposite trend recently, that some library maintainers started to provide upgrades only for Scala 3. If you haven‚Äôt upgraded your codebase yet, you‚Äôll stop receiving upgrades for the dependencies, including security patches. It gives you a more valid reason for the upgrade. Still, again, it is not a quick task, especially in the current economic situation when companies are under financial pressure and teams are downsized. Additionally, you may still be blocked by dependencies that do not support Scala 3 yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Poor Scala 3 tooling support&lt;/head&gt;
    &lt;p&gt;Let‚Äôs assume you got lucky enough to upgrade your old Scala 2 codebase. Or you even got a green light for building a brand new application in Scala 3. Do not celebrate it too much! After more than four years since the release, Scala 3 still has a lot of issues with tooling, which do not make the language fully production-ready.&lt;/p&gt;
    &lt;p&gt;For instance, while working on a software project, I prefer to rely on CI/CD pipelines that allow deploying the application only if certain checks are passed. The pipelines run unit tests and other types of tests to validate the application. Then the pipelines run a test coverage check to make sure the code has sufficient automatic testing. Additionally, the pipelines can run checks for the code itself, such as static analysis, to catch pieces of code that are semantically correct but not written according to best practices. All these checks help you to build a reliable software and prevent many nasty issues in production. Unfortunately, the tooling for the test coverage and code validation has horrible support for Scala 3.&lt;/p&gt;
    &lt;p&gt;You can use the test coverage plugin in some way. However, there are still plenty of bugs, such as false positives for lines of code that are executed in the unit tests. Besides, the plugin still does not have all the features that were available for Scala 2. Anyway, it is a tremendous progress compared to the situation at the beginning of the year, when the plugin gave you completely wrong calculations of the test coverage. The situation is significantly better than with the static analysis plugin, which I couldn‚Äôt get to work. It runs, but it ignores all the suspicious uses of code, always giving you the green light. Remember, it is a situation four years after the release of the language!&lt;/p&gt;
    &lt;p&gt;Another complaint goes to the IDE support. In IntelliJ, the Scala 3 coding experience is terrible. Sometimes, you need to wait for a minute to get the code completion hints. Often, the error highlighting stops working, so you have to start rebuilding the project manually. It is not as if there‚Äôs no good IDE support at all. Metals, the Scala LSP server, provides you with a better Scala development experience now, and it is constantly getting improved. However, it is highly disappointing and strange that the most popular IDE in the Scala world still lacks proper language support.&lt;/p&gt;
    &lt;p&gt;To make it worse, even Sbt, the primary Scala building tool, is buggy while working with the third version. It is not always successful in building and running an application. The most annoying thing for me and my colleagues is running the unit tests. Only every fifth attempt is successful. For all the rest, you get annoying ‚ÄúClass not found‚Äù exceptions, every time for a random different class.&lt;/p&gt;
    &lt;p&gt;At the same time, Scala 3 still inherits annoying issues from the past. For instance, you still have the SemVer vs PVP conflict problem or the forward compatibility issues. All these problems make the development experience terrible. And it is deeply concerning that the third version still has so many issues several years after its release.&lt;/p&gt;
    &lt;head rend="h2"&gt;Small but fragmented ecosystem&lt;/head&gt;
    &lt;p&gt;The Scala ecosystem is relatively small because the language is not among the top-tier languages. But here‚Äôs a fun fact: there is no dominating framework or library. For instance, when discussing backend development in Java, it typically refers to building an application using Spring. In Python, Django heavily dominates in the niche of web development. You cannot imagine Ruby without Ruby on Rails. However, it is hard to recall such examples in Scala, except for Apache Spark. But the latter one focuses more on data engineering than software development. For the other areas of Scala‚Äôs application, we lack a de facto standardized framework or library.&lt;/p&gt;
    &lt;p&gt;This is particularly true for backend development, the central niche of the language. Here we have only the top tier consisting of several names: Akka (nowadays also Pekko, after the Akka licence change), Play Framework, ZIO, Lagom, Cats, and so on. We can find similar examples in smaller application areas, where you need a library for solving a specific issue. You will get again a ‚Äúgreat‚Äù choice of dozens of concurrent components, without a prominent leader. The classic example is the thousand JSON serialization/deserialization libraries, which became a target of jokes in the Scala community. So it seems that every Scala developer has written their own solution for processing JSON at some point in time. And if you work with an old and large Scala codebase, you will likely find a couple of different JSON libraries there, added by other developers at different stages of the project, or introduced as a transitive dependency.&lt;/p&gt;
    &lt;p&gt;Of course, the great variety and diversity are highly beneficial in all areas of our lives. Monocultures are damaging not only in the natural ecosystems but also in IT. However, the Scala ecosystem has at least a couple of adverse side effects due to its high number of concurrent frameworks and libraries.&lt;/p&gt;
    &lt;p&gt;First of all, the higher number of concurrent projects results in a lower number of users per project, which in turn leads to fewer contributors and maintainers. And take into account that Scala is not among the most popular languages! So the number of developers here is already small. As a result, even popular Scala libraries are maintained poorly compared to other languages. Bugs can stay unfixed there for a very long time. Adaptation to new language versions is very slow (as I wrote above, it is not such an easy task in the Scala ecosystem). Finally, there is a very high chance of a project becoming abandoned. The last example is the most frustrating. Occasionally, during a dependency update, you may notice that a library has not had any new releases for a while, and the latest changes in the repository are dated a couple of years ago. Then you have a tough choice: either find an alternative library, refactor your codebase, or hope that there will be no serious vulnerabilities in the dependency.&lt;/p&gt;
    &lt;p&gt;The second problem is that the high number of frameworks and libraries makes the Scala ecosystem highly complex and challenging to navigate. The language itself is not beginner-friendly. But in addition, you should also be familiar with a large number of Scala repositories and components to be able to work on different projects. If you are a newcomer, it is easy to get lost in all these alternative solutions. It is hard to figure out which frameworks you need to learn first, which are essential for the industry, which are not, and what the standard approach is to build an application in general. So the Scala ecosystem‚Äôs fragmentation not only makes the life of existing Scala programmers harder, but it also makes the language less attractive for junior developers.&lt;/p&gt;
    &lt;head rend="h2"&gt;The language is dying&lt;/head&gt;
    &lt;p&gt;In 2018, I attended a Scala conference in Berlin. During the round table sessions about Scala‚Äôs future, you could already hear the cynical statements that ‚Äúthe language is not dead yet‚Äù. It was already clear that Scala would not replace Java, and the language was becoming a niche tool. The Scala‚Äôs future did not seem as bright anymore, as it had seemed before. So it is not surprising that the optimism and the enthusiasm were already low at that time.&lt;/p&gt;
    &lt;p&gt;Since then, the situation has been getting only worse. You can see the decay of Scala in all the popular rankings and surveys. For instance, the Stack Overflow survey shows a clear downward trend. From 2018, the year of the conference, to the current 2025, the popularity fell from 4.4% to 2.6%. Now, Scala ranks below Elixir, a quite esoteric and narrow-specialized language. It is also amusing that Groovy, with 4.8%, is several lines above Scala, given the urban legend that Groovy‚Äôs creator acknowledged Scala‚Äôs superiority and abandoned further development of the language.&lt;/p&gt;
    &lt;p&gt;Despite the decline, there were still plenty of Scala projects, at least in the Netherlands, where I live. However, over the last few years, all these projects have been legacy systems dating back to the golden age of Scala, when the language was trendy and developers were curious about it. I haven‚Äôt heard about any new Scala projects starting for a while. The conservative cohort of developers continues to use Java + Spring for the backend development. Other programmers try water with Rust or Go. We also have Kotlin, which allows using Spring Boot for backend development without Java‚Äôs boilerplate. You can even see Typescript or Python as backend languages, mainly in the startup area. So the developers are losing interest in Scala. The Stack Overflow surveys also confirm it. In 2023, 52.3% of the respondents wanted to use Scala. In 2025, the number is 39.4%, which is comparable to that of PHP, 38.9%, and below good old Ada with 41.2%.&lt;/p&gt;
    &lt;p&gt;I see the following reason for this negative trend. The language remains complex, making it less accessible to newcomers. But at the same time, you would not get a sexy greenfield project as a Scala developer. Most likely, you will get a relic legacy system with a pile of tech debt gathered by a couple of generations of programmers before you. It is not the most exciting prospect, especially for the junior developers. Considering this and all the issues listed in the previous parts, it‚Äôs not surprising that Scala is going downhill.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving forward&lt;/head&gt;
    &lt;p&gt;To conclude, the language has numerous annoying issues. First of all, the unsuccessful release of Scala 3 introduced numerous problems in supporting existing projects, as it was not yet production-ready. The Scala ecosystem faces challenges due to its high number of concurrent libraries and lack of consolidation. Finally, it is getting harder and harder to jump into an interesting Scala project because the language is losing its popularity.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it is time to move on. I still love the language. I enjoy how expressive Scala is, how easy it is to describe the application‚Äôs logic thanks to functional programming. But more importantly, you are always surrounded by brilliant people if you work on a Scala project. The language attracted excellent developers, I was never the smartest person in the room. Sadly, the problems have managed to outweigh the benefits, and Scala is now in a dead spiral.&lt;/p&gt;
    &lt;p&gt;About which language to pick up next, I have a couple of candidates. First of all, Kotlin is a popular choice for switching among Scala developers. It is also a JVM language that stands somewhere between Java and Scala. It lacks the support of advanced functional programming magic, compared to Scala. But at the same time, it is more expressive than Java and has a nice mechanism for null-safety.&lt;/p&gt;
    &lt;p&gt;Then I am also thinking about Go. It is the opposite of the paradigm perspective from Scala, lacking functional programming, but instead employing the old-school procedural style with some object-oriented programming. It has a reputation for being a simple language. It is indeed an easy-to-approach language, but not only from a syntax perspective. It has many nice features that make a developer‚Äôs life much easier. Go has a rich set of out-of-the-box tools and libraries, and at the same time, it has a much broader and stronger ecosystem. I also appreciate that the backward compatibility principle is a cornerstone in the Go world.&lt;/p&gt;
    &lt;p&gt;Time will tell where I will end up next. But I know for sure I have to move on from Scala and change ecosystems for another time in my career.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arbuh.medium.com/why-i-am-moving-away-from-scala-7a9d3dca17b9"/><published>2026-01-22T12:48:25+00:00</published></entry></feed>