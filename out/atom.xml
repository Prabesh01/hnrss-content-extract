<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-26T08:43:29.334299+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45373081</id><title>Cloudflare Email Service: private beta</title><updated>2025-09-26T08:43:37.708993+00:00</updated><content>&lt;doc fingerprint="ee97ad5b3f10bcad"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;If you are building an application, you rely on email to communicate with your users. You validate their signup, notify them about events, and send them invoices through email. The service continues to find new purpose with agentic workflows and other AI-powered tools that rely on a simple email as an input or output.&lt;/p&gt;
      &lt;p&gt;And it is a pain for developers to manage. Itâs frequently the most annoying burden for most teams. Developers deserve a solution that is simple, reliable, and deeply integrated into their workflow.Â &lt;/p&gt;
      &lt;p&gt;Today, we're excited to announce just that: the private beta of Email Sending, a new capability that allows you to send transactional emails directly from Cloudflare Workers. Email Sending joins and expands our popular Email Routing product, and together they form the new Cloudflare Email Service â a single, unified developer experience for all your email needs.&lt;/p&gt;
      &lt;p&gt;With Cloudflare Email Service, weâre distilling our years of experience securing and routing emails, and combining it with the power of the developer platform. Now, sending an email is as easy as adding a binding to a Worker and calling &lt;code&gt;send&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
  async fetch(request, env, ctx) {

    await env.SEND_EMAIL.send({
      to: [{ email: "[email protected]" }],
      from: { email: "[email protected]", name: "Your App" },
      subject: "Hello World",
      text: "Hello World!"
    });

    return new Response(`Successfully sent email!`);
  },
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Email experience is user experience&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Email is a core tenet of your user experience. Itâs how you stay in touch with your users when they are outside your applications. Users rely on email to inform them when they need to take actions such as password resets, purchase receipts, magic login links, and onboarding flows. When they fail, your application fails.&lt;/p&gt;
      &lt;p&gt;That means itâs crucial that emails need to land in your usersâ inboxes, both reliably and quickly. A magic link that arrives ten minutes late is a lost user. An email delivered to a spam folder breaks user flows and can erode trust in your product. Thatâs why weâre focusing on deliverability and time-to-inbox with Cloudflare Email Service.Â &lt;/p&gt;
      &lt;p&gt;To do this, weâre tightly integrating with DNS to automatically configure the necessary DNS records â like SPF, DKIM and DMARC â such that email providers can verify your sending domain and trust your emails. Plus, in true Cloudflare fashion, Email Service is a global service. That means that we can deliver your emails with low latency anywhere in the world, without the complexity of managing servers across regions.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Simple and flexible for developers&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Treating email as a core piece of your application also means building for every touchpoint in your development workflow. Weâre building Email Service as part of the Cloudflare stack to make developing with email feels as natural as writing a Worker.Â &lt;/p&gt;
      &lt;p&gt;In practice, that means solving for every part of the transactional email workflow:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Starting with Email Service is easy. Instead of managing API keys and secrets, you can use the &lt;code&gt;Email&lt;/code&gt; binding to your &lt;code&gt;wrangler.jsonc&lt;/code&gt; and send emails securely and with no risk of leaked credentials.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can use Workers to process incoming mail, store attachments in R2, and add tasks to Queues to get email sending off the hot path of your application. And you can use &lt;code&gt;wrangler&lt;/code&gt; to emulate Email Sending locally, allowing you to test your user journeys without jumping between tools and environments.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;In production, you have clear observability over your emails with bounce rates and delivery events. And, when a user reports a missing email, you can quickly dive into the delivery status to debug issues quickly and help get your user back on track.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Weâre also making sure Email Service seamlessly fits into your existing applications. If you need to send emails from external services, you can do so using either REST APIs or SMTP. Likewise, if youâve been leaning on existing email frameworks (like React Email) to send rich, HTML-rendered emails to users, you can continue to use them with Email Service. Import the library, render your template, and pass it to the `send` method just as you would elsewhere.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { render, pretty, toPlainText } from '@react-email/render';
import { SignupConfirmation } from './templates';

export default {
  async fetch(request, env, ctx) {

    // Convert React Email template to html
    const html = await pretty(await render(&amp;lt;SignupConfirmation url="https://your-domain.com/confirmation-id"/&amp;gt;));

    // Use the Email Sending binding to send emails
    await env.SEND_EMAIL.send({
      to: [{ email: "[email protected]" }],
      from: { email: "[email protected]", name: "Welcome" },
      subject: "Signup Confirmation",
      html,
      text: toPlainText(html)
    });

    return new Response(`Successfully sent email!`);
  }
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Email Routing and Email Sending: Better together&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Sending email is only half the story. Applications often need to receive and parse emails to create powerful workflows. By combining Email Sending with our existing Email Routing capabilities, we're providing a complete, end-to-end solution for all your application's email needs.&lt;/p&gt;
      &lt;p&gt;Email Routing allows you to create custom email addresses on your domain and handle incoming messages programmatically with a Worker, which can enable powerful application flows such as:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Using Workers AI to parse, summarize and even label incoming emails: flagging security events from customers, early signs of a bug or incident, and/or generating automatic responses based on those incoming emails.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Creating support tickets in systems like JIRA or Linear from emails sent to &lt;code&gt;[email protected]&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Processing invoices sent to &lt;code&gt;[email protected]&lt;/code&gt; and storing attachments in R2.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;To use Email Routing, add the &lt;code&gt;email&lt;/code&gt; handler to your Worker application and process it as needed:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
  // Create an email handler to process emails delivered to your Worker
  async email(message, env, ctx) {

    // Classify incoming emails using Workers AI
    const { score, label } = env.AI.run("@cf/huggingface/distilbert-sst-2-int8", { text: message.raw" })

    env.PROCESSED_EMAILS.send({score, label, message});
  },
};  &lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;When you combine inbound routing with outbound sending, you can close the loop entirely within Cloudflare. Imagine a user emails your support address. A Worker can receive the email, parse its content, call a third-party API to create a ticket, and then use the Email Sending binding to send an immediate confirmation back to the user with their ticket number. Thatâs the power of a unified Email Service.&lt;/p&gt;
      &lt;p&gt;Email Sending will require a paid Workers subscription, and we'll be charging based on messages sent. We're still finalizing the packaging, and we'll update our documentation, changelog, and notify users as soon as we have final pricing and long before we start charging. Email Routing limits will remain unchanged.&lt;/p&gt;
      &lt;p&gt;Email is core to your application today, and it's becoming essential for the next generation of AI agents, background tasks, and automated workflows. We built the Cloudflare Email Service to be the engine for this new era of applications, weâll be making it available in private beta this November.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Interested in Email Sending? Sign up to the waitlist here.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Want to start processing inbound emails? Get started with Email Routing, which is available now, remains free and will be folded into the new email sending APIs coming.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Weâre excited to be adding Email Service to our Developer Platform, and weâre looking forward to seeing how you reimagine user experiences that increasingly rely on emails!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/email-service/"/><published>2025-09-25T14:33:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45375477</id><title>ChatGPT Pulse</title><updated>2025-09-26T08:43:37.544091+00:00</updated><content>&lt;doc fingerprint="9a26a651bae7d2f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing ChatGPT Pulse&lt;/head&gt;
    &lt;p&gt;Now ChatGPT can start the conversation&lt;/p&gt;
    &lt;p&gt;We're building ChatGPT to help you reach your goals. Since ChatGPT launched, that's always meant coming to ask a question. There's magic in being able to simply ask and get answers to help you learn, create or solve problems. However that's limited by what you know to ask for and always puts the burden on you for the next step.&lt;/p&gt;
    &lt;p&gt;Today we're releasing a preview of ChatGPT Pulse to Pro users on mobile. Pulse is a new experience where ChatGPT proactively does research to deliver personalized updates based on your chats, feedback, and connected apps like your calendar. You can curate what ChatGPT researches by letting it know what’s useful and what isn’t. The research appears in Pulse as topical visual cards you can scan quickly or open for more detail, so each day starts with a new, focused set of updates.&lt;/p&gt;
    &lt;p&gt;This is the first step toward a more useful ChatGPT that proactively brings you what you need, helping you make more progress so you can get back to your life. We’ll learn and improve from early use before rolling it out to Plus, with the goal of making it available to everyone.&lt;/p&gt;
    &lt;p&gt;ChatGPT can now do asynchronous research on your behalf. Each night, it synthesizes information from your memory, chat history, and direct feedback to learn what’s most relevant to you, then delivers personalized, focused updates the next day. These could look like follow-ups on topics you discuss often, ideas for quick, healthy dinner to make at home that evening, or next steps toward a longer-term goal such as training for a triathlon.&lt;/p&gt;
    &lt;p&gt;You can also connect Gmail and Google Calendar to provide additional context for more relevant suggestions. When Calendar is connected, ChatGPT might draft a sample meeting agenda, remind you to buy a birthday gift, or surface restaurant recommendations for an upcoming trip. These integrations are off by default and can be turned on or off anytime in settings.&lt;/p&gt;
    &lt;p&gt;Topics shown in Pulse also pass through safety checks to avoid showing harmful content that violates our policies.&lt;/p&gt;
    &lt;p&gt;You can ask for what you’d like ChatGPT to research for you each day. Tap "curate" to request what you want to see in future editions—ask for a Friday roundup of local events, tips for learning a new skill, or something specific like "focus on professional tennis updates tomorrow." You can also give quick feedback with a thumbs up or thumbs down, and easily view or delete your feedback history. Over time, your guidance makes Pulse more personal and useful.&lt;/p&gt;
    &lt;p&gt;Every morning, ChatGPT delivers a curated set of the most relevant updates, giving you the information you need so you can get back to what matters most. Each update is available for that day only unless you save it as a chat or ask a follow-up question, which adds it to your conversation history. Expand any update to dive deeper, request next steps, or save it for later so you can move forward on goals with clear, timely information.&lt;/p&gt;
    &lt;p&gt;We partnered with college students in the ChatGPT Lab to gather early feedback and improve Pulse. One insight in particular we had was that many started to feel its utility once they started telling ChatGPT what they wanted to see. That insight underscored the importance of simple feedback, so we added more ways to share reactions and guide what appears. Here are a few of the students’ favorite personalized updates.&lt;/p&gt;
    &lt;head rend="h3"&gt;Student use cases&lt;/head&gt;
    &lt;head rend="h3"&gt;Actionable recommendations&lt;/head&gt;
    &lt;p&gt;"Received this based on a conversation that I had yesterday that focused on calendar management/structuring PTO for my grant period in Taiwan. What it produced was several logical steps ahead of where I was at in the conversation. The update was incredibly helpful and exposed me to train and commute information I would have never come across or looked for otherwise."&lt;/p&gt;
    &lt;p&gt;Pulse is a preview and won’t always get things right. It aims to show you what’s most relevant and useful but you may still see suggestions that miss the mark. For example, you may get tips for a project you already completed. You can guide what shows up by telling ChatGPT directly. It remembers your feedback for next time and improves as it learns from real use.&lt;/p&gt;
    &lt;p&gt;Pulse is the first step toward a new paradigm for interacting with AI.&lt;/p&gt;
    &lt;p&gt;By combining conversation, memory, and connected apps, ChatGPT is moving from answering questions to a proactive assistant that works on your behalf. Over time, we envision AI systems that can research, plan, and take helpful actions for you—based on your direction—so that progress happens even when you are not asking.&lt;/p&gt;
    &lt;p&gt;Pulse introduces this future in its simplest form: personalized research and timely updates that appear regularly to keep you informed. Soon, Pulse will be able to connect with more of the apps you use so updates capture a more complete picture of your context. We’re also exploring ways for Pulse to deliver relevant work at the right moments throughout the day, whether it’s a quick check before a meeting, a reminder to revisit a draft, or a resource that appears right when you need it.&lt;/p&gt;
    &lt;p&gt;As we expand to more apps and richer actions, ChatGPT will evolve from something you consult into something that quietly accelerates the work and ideas that matter to you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-chatgpt-pulse/"/><published>2025-09-25T16:59:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45375845</id><title>Improved Gemini 2.5 Flash and Flash-Lite</title><updated>2025-09-26T08:43:36.876756+00:00</updated><content>&lt;doc fingerprint="bf7738879939489d"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we are releasing updated versions of Gemini 2.5 Flash and 2.5 Flash-Lite, available on Google AI Studio and Vertex AI, aimed at continuing to deliver better quality while also improving the efficiency.&lt;/p&gt;
    &lt;p&gt;The latest version of Gemini 2.5 Flash-Lite was trained and built based on three key themes:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;You can start testing this version today using the following model string: &lt;code&gt;gemini-2.5-flash-lite-preview-09-2025&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This latest 2.5 Flash model comes with improvements in two key areas we heard consistent feedback on:&lt;/p&gt;
    &lt;p&gt;We’re already seeing positive feedback from early testers. As Yichao ‘Peak’ Ji, Co-Founder &amp;amp; Chief Scientist at Manus, an autonomous AI agent, noted: “The new Gemini 2.5 Flash model offers a remarkable blend of speed and intelligence. Our evaluation on internal benchmarks revealed a 15% leap in performance for long-horizon agentic tasks. Its outstanding cost-efficiency enables Manus to scale to unprecedented levels—advancing our mission to Extend Human Reach.”&lt;/p&gt;
    &lt;p&gt;You can start testing this preview version today by using the following model string: &lt;code&gt;gemini-2.5-flash-preview-09-2025&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Over the last year, we’ve learned that shipping preview versions of our models allows you to test our latest improvements and innovations, provide feedback, and build production-ready experiences with the best of Gemini. Today’s releases are not intended to graduate to a new, stable version but will help us shape our future stable releases, and allow us to continue iterating and bring you the best of Gemini.&lt;/p&gt;
    &lt;p&gt;To make it even easier to access our latest models while also reducing the need to keep track of long model string names, we are also introducing a &lt;code&gt;-latest&lt;/code&gt; alias for each model family. This alias always points to our most recent model versions, allowing you to experiment with new features without needing to update your code for each release. You can access the new previews using:&lt;/p&gt;
    &lt;code&gt;gemini-flash-latest&lt;/code&gt;
    &lt;code&gt;gemini-flash-lite-latest&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;To ensure you have time to test new models, we will always provide a 2-week notice (via email) before we make updates or deprecate a specific version behind &lt;code&gt;-latest&lt;/code&gt;. These are just model aliases so the rate limits, cost, and features available may fluctuate between releases.&lt;/p&gt;
    &lt;p&gt;For applications that require more stability, continue to use &lt;code&gt;gemini-2.5-flash&lt;/code&gt; and &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We continue to push the frontier of what is possible with Gemini and this release is just another step in that direction. We will have more to share soon, but in the meantime, happy building!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"/><published>2025-09-25T17:20:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45376605</id><title>Athlon 64: How AMD turned the tables on Intel</title><updated>2025-09-26T08:43:36.479291+00:00</updated><content>&lt;doc fingerprint="e70d5115fd2265ff"&gt;
  &lt;main&gt;
    &lt;p&gt;22 years ago, on September 23, 2003, AMD changed the game for x86 once and for all. They released the Athlon 64 CPU, a chip that did something Intel didn’t want. Intel didn’t want to extend x86 to 64 bits. But when AMD did it, it forced Intel to clone AMD, rather than the other way around.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Intel didn’t want to go 64-bit&lt;/head&gt;
    &lt;p&gt;Even in 2001, x86 had decades of baggage attached to it. It was a 32-bit architecture that had been extended from a 16-bit architecture. But that in turn had been extended from an 8-bit CPU design from 1972 that, believe it or not, originated at Datapoint, not Intel.&lt;/p&gt;
    &lt;p&gt;This was great for backward compatibility. 8-bit applications were very easy to port to x86 in the early 1980s, and those early DOS applications still ran flawlessly on modern systems 30 years later. For that matter, it’s not impossible to get them running even today.&lt;/p&gt;
    &lt;p&gt;Removal of the ability to run 16-bit applications in 64-bit Windows was a design decision, not a technical limitation.&lt;/p&gt;
    &lt;p&gt;Intel wanted to start over to go 64-bit. Without having to worry about backward compatibility, they could design something that would be faster and more efficient. In theory at least, it would be able to scale higher in clock speed. And there was no question a new design would outperform a theoretical 64-bit x86 when running at the same speed because of efficiency.&lt;/p&gt;
    &lt;p&gt;And if you are cynical, there was one more motivation. If Intel could start over, they wouldn’t have to worry about competing CPU designs, at least not for a very long time. The new design would be encumbered with so many patents, it might be 20 years before someone could clone it.&lt;/p&gt;
    &lt;p&gt;Keep in mind that in 2003, not only was AMD in the picture, but Transmeta was still in the picture, and Cyrix was fading but not completely gone.&lt;/p&gt;
    &lt;p&gt;Starting over with a new CPU architecture outright was massively attractive to Intel.&lt;/p&gt;
    &lt;p&gt;This new 64-bit architecture wasn’t theoretical, either. Intel was producing it. It was called Itanium, and Intel first released it in June 2001.&lt;/p&gt;
    &lt;head rend="h2"&gt;AMD’s risky bet and why they made it&lt;/head&gt;
    &lt;p&gt;AMD was well aware of the shortcomings of extending x86 to 64 bits. And they did it anyway. For them, the stakes were completely different.&lt;/p&gt;
    &lt;p&gt;AMD knew that if Itanium caught on, that would be the end for them as a CPU company, unless maybe they wanted to become just another ARM licensee. Being just another ARM licensee is more attractive in 2025 than it was in 2003.&lt;/p&gt;
    &lt;p&gt;But they could see Itanium wasn’t catching on. It had its uses, and it was doing well enough in those niches, but Windows on Itanium was a non-starter. So much so, The Register called it “Itanic.”&lt;/p&gt;
    &lt;p&gt;AMD bet that there would be appeal in a 64-bit architecture that was fully backward compatible with x86 and natively ran 32-bit applications at full speed. People would be able to run 32-bit Windows and 32-bit applications on it if they needed to, and then when they were ready for 64-bit software, the hardware was there and ready to go. And they could continue to run 32-bit apps in 64-bit operating systems as long as needed to ease the transition.&lt;/p&gt;
    &lt;p&gt;The transition to 32 bits took a decade. AMD reasoned more people would be willing to upgrade to 64 bits if they made that transition as similar as the transition from the 286 to the 386 as possible.&lt;/p&gt;
    &lt;p&gt;They believed the market would willingly trade lower 64-bit performance in the long term for better 32-bit performance right away. They also believed that if Microsoft was willing to build Windows on Itanium, they would be willing to take a chance on 64-bit x86 as well.&lt;/p&gt;
    &lt;p&gt;So on September 23, 2003, AMD launched its Athlon 64, the first 64-bit x86 CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why the Athlon 64 was a hit&lt;/head&gt;
    &lt;p&gt;AMD64 was everything AMD hoped it would be. It was backward compatible with 32-bit x86. The 64-bit builds of Windows weren’t available immediately, and they didn’t catch on immediately, but you cannot say nobody used them. People did, in fact, use them. In late 2005, I was in charge of administering the complimentary antivirus software that Charter Communications provided to its subscribers. I’m not going to say say someone called me every day wanting 64-bit antivirus for 64-bit Windows. But it did happen once a week.&lt;/p&gt;
    &lt;p&gt;The transition took at least as long as AMD expected. When I finally bought an Athlon 64 in 2011, I found native 64-bit software was still scarce. I’m an outspoken Firefox fan; the reason I briefly switched to Google Chrome was to get a 64-bit web browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Athlon 64 in the enterprise&lt;/head&gt;
    &lt;p&gt;A few months later, I got a better job with more pay and better growth potential. I can’t talk a lot about the job, but I was administering a mission critical system that ran on Windows, mostly on Dell hardware. I mention Dell because they were exclusively an Intel vendor for years. Cofounder and longtime AMD CEO Jerry Sanders once said of Michael Dell, “I can’t sell him a[n AMD] K6 no matter what I do.”&lt;/p&gt;
    &lt;p&gt;It was the Athlon 64 that made Dell relent and finally start using AMD CPUs. Not only were they using them on desktop systems, but they were putting AMD CPUs in servers, an idea that would have been extremely controversial 5 years before. At least in the circles I ran in.&lt;/p&gt;
    &lt;p&gt;The Athlon 64 caught on because, in spite of its name, it was an outstanding 32-bit CPU. It was faster than an Intel CPU running at the same clock rate, and it used less power as well. The power consumption was the key to getting into the data center. The Intel name was a security blanket, even though AMD had been making x86 CPUs exactly as long as Intel. But certain decision makers bought Intel marketing and saw AMD as a second tier brand.&lt;/p&gt;
    &lt;p&gt;The thing is, when you have a data center with hundreds of systems in it, the money you save on a more efficient CPU really talks.&lt;/p&gt;
    &lt;p&gt;Replacing Intel Prescott-based servers with AMD64 servers was not a universally popular idea. But you could tell a difference when you were standing behind a rack full of Intel-based servers versus a rack full of AMD based servers. The Intels ran hotter.&lt;/p&gt;
    &lt;p&gt;From an uptime perspective, we couldn’t see a difference. The performance metrics I collected showed there was a slight difference, and that difference was in AMD’s favor. So the AMD critics quickly ate their words.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intel giving in and cloning AMD64&lt;/head&gt;
    &lt;p&gt;In 2004, Intel wrote off the Itanium and cloned AMD64. They called it Intel64, but it was a blatant copy of the AMD implementation. A quirk in the agreements that allowed AMD to use the x86 instruction set also gave Intel the rights to use the AMD64 instructions. So there was nothing illegal about what Intel did. Itanium continued to see use in specialized applications, but Intel quietly discontinued it in 2020.&lt;/p&gt;
    &lt;p&gt;AMD and Intel have been chasing and catching each other ever since. One of them will pass the other for a CPU generation or two, and then they will change positions. It’s not terribly different from the situation in 1999 with the original Athlon, when AMD outperformed Intel for the first time. The question in everyone’s mind was whether they would do it a second time. The Athlon 64 was the second time.&lt;/p&gt;
    &lt;p&gt;It was a big step forward. Eight years before, AMD was trying to pass off a high-clocked 486 as a Pentium equivalent. With the Athlon 64, AMD was innovating.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/"/><published>2025-09-25T18:09:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45377641</id><title>Ollama Web Search</title><updated>2025-09-26T08:43:36.370729+00:00</updated><content>&lt;doc fingerprint="2d52dc131ca4ae0e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Web search&lt;/head&gt;
    &lt;head rend="h2"&gt;September 24, 2025&lt;/head&gt;
    &lt;p&gt;A new web search API is now available in Ollama. Ollama provides a generous free tier of web searches for individuals to use, and higher rate limits are available via Ollama’s cloud.&lt;/p&gt;
    &lt;p&gt;This web search capability can augment models with the latest information from the web to reduce hallucinations and improve accuracy.&lt;/p&gt;
    &lt;p&gt;Web search is provided as a REST API with deeper tool integrations in Ollama’s Python and JavaScript libraries. This also enables models such as OpenAI’s &lt;code&gt;gpt-oss&lt;/code&gt; models to conduct long-running research tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Create an API key from your Ollama account.&lt;/p&gt;
    &lt;code&gt;export OLLAMA_API_KEY="your_api_key"
&lt;/code&gt;
    &lt;head rend="h4"&gt;cURL&lt;/head&gt;
    &lt;code&gt;curl https://ollama.com/api/web_search \
  --header "Authorization: Bearer $OLLAMA_API_KEY" \
  -d '{
    "query": "what is ollama?"
  }'
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;{
  "results": [
    {
      "title": "Ollama",
      "url": "https://ollama.com/",
      "content": "Cloud models are now available..."
    },
    {
      "title": "What is Ollama? Introduction to the AI model management tool",
      "url": "https://www.hostinger.com/tutorials/what-is-ollama",
      "content": "Ariffud M. 6min Read..."
    },
    {
      "title": "Ollama Explained: Transforming AI Accessibility and Language ...",
      "url": "https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/",
      "content": "Data Science Data Science Projects Data Analysis..."
    }
  ]
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Python&lt;/head&gt;
    &lt;p&gt;Install and run Ollama’s Python library&lt;/p&gt;
    &lt;code&gt;pip install 'ollama&amp;gt;=0.6.0'
&lt;/code&gt;
    &lt;p&gt;Then make a request using &lt;code&gt;ollama.web_search&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import ollama
response = ollama.web_search("What is Ollama?")
print(response)
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;results = [
    {
        "title": "Ollama",
        "url": "https://ollama.com/",
        "content": "Cloud models are now available in Ollama..."
    },
    {
        "title": "What is Ollama? Features, Pricing, and Use Cases - Walturn",
        "url": "https://www.walturn.com/insights/what-is-ollama-features-pricing-and-use-cases",
        "content": "Our services..."
    },
    {
        "title": "Complete Ollama Guide: Installation, Usage &amp;amp; Code Examples",
        "url": "https://collabnix.com/complete-ollama-guide-installation-usage-code-examples",
        "content": "Join our Discord Server..."
    }
]
&lt;/code&gt;
    &lt;head rend="h4"&gt;JavaScript&lt;/head&gt;
    &lt;p&gt;Install and run Ollama’s JavaScript library&lt;/p&gt;
    &lt;code&gt;npm install 'ollama@&amp;gt;=0.6.0'
&lt;/code&gt;
    &lt;code&gt;import { Ollama } from "ollama";

const client = new Ollama();
const results = await client.webSearch({ query: "what is ollama?" });
console.log(JSON.stringify(results, null, 2));
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;{
  "results": [
    {
      "title": "Ollama",
      "url": "https://ollama.com/",
      "content": "Cloud models are now available..."
    },
    {
      "title": "What is Ollama? Introduction to the AI model management tool",
      "url": "https://www.hostinger.com/tutorials/what-is-ollama",
      "content": "Ollama is an open-source tool..."
    },
    {
      "title": "Ollama Explained: Transforming AI Accessibility and Language Processing",
      "url": "https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/",
      "content": "Ollama is a groundbreaking..."
    }
  ]
}

&lt;/code&gt;
    &lt;head rend="h3"&gt;Building a search agent&lt;/head&gt;
    &lt;p&gt;Use Ollama’s web search as a tool to build a mini search agent.&lt;/p&gt;
    &lt;p&gt;The example uses Alibaba’s Qwen 3 model with 4B parameters.&lt;/p&gt;
    &lt;code&gt;ollama pull qwen3:4b
&lt;/code&gt;
    &lt;code&gt;from ollama import chat, web_fetch, web_search

available_tools = {'web_search': web_search, 'web_fetch': web_fetch}

messages = [{'role': 'user', 'content': "what is ollama's new engine"}]

while True:
  response = chat(
    model='qwen3:4b',
    messages=messages,
    tools=[web_search, web_fetch],
    think=True
    )
  if response.message.thinking:
    print('Thinking: ', response.message.thinking)
  if response.message.content:
    print('Content: ', response.message.content)
  messages.append(response.message)
  if response.message.tool_calls:
    print('Tool calls: ', response.message.tool_calls)
    for tool_call in response.message.tool_calls:
      function_to_call = available_tools.get(tool_call.function.name)
      if function_to_call:
        args = tool_call.function.arguments
        result = function_to_call(**args)
        print('Result: ', str(result)[:200]+'...')
        # Result is truncated for limited context lengths
        messages.append({'role': 'tool', 'content': str(result)[:2000 * 4], 'tool_name': tool_call.function.name})
      else:
        messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})
  else:
    break
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;Thinking:  Okay, the user is asking about Ollama's new engine. I need to figure out what they're referring to. Ollama is a company that develops large language models, so maybe they've released a new model or an updated version of their existing engine...

Tool calls:  [ToolCall(function=Function(name='web_search', arguments={'max_results': 3, 'query': 'Ollama new engine'}))]
Result:  results=[WebSearchResult(content='# New model scheduling\n\n## September 23, 2025\n\nOllama now includes a significantly improved model scheduling system. Ahead of running a model, Ollama’s new engine...

Thinking:  Okay, the user asked about Ollama's new engine. Let me look at the search results.

First result is from September 23, 2025, talking about new model scheduling. It mentions improved memory management, reduced crashes, better GPU utilization, and multi-GPU performance. Examples show speed improvements and accurate memory reporting. Supported models include gemma3, llama4, qwen3, etc...

Content:  Ollama has introduced two key updates to its engine, both released in 2025:

1. **Enhanced Model Scheduling (September 23, 2025)**
   - **Precision Memory Management**: Exact memory allocation reduces out-of-memory crashes and optimizes GPU utilization.
   - **Performance Gains**: Examples show significant speed improvements (e.g., 85.54 tokens/s vs 52.02 tokens/s) and full GPU layer utilization.
   - **Multi-GPU Support**: Improved efficiency across multiple GPUs, with accurate memory reporting via tools like `nvidia-smi`.
   - **Supported Models**: Includes `gemma3`, `llama4`, `qwen3`, `mistral-small3.2`, and more.

2. **Multimodal Engine (May 15, 2025)**
   - **Vision Support**: First-class support for vision models, including `llama4:scout` (109B parameters), `gemma3`, `qwen2.5vl`, and `mistral-small3.1`.
   - **Multimodal Tasks**: Examples include identifying animals in multiple images, answering location-based questions from videos, and document scanning.

These updates highlight Ollama's focus on efficiency, performance, and expanded capabilities for both text and vision tasks.
&lt;/code&gt;
    &lt;p&gt;Recommended models:&lt;/p&gt;
    &lt;p&gt;These models have great tool-use capabilities and are able to have multi-turn interactions with the user and tools to get to a final result.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;qwen3&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gpt-oss&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recommended cloud models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;qwen3:480b-cloud&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gpt-oss:120b-cloud&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;deepseek-v3.1-cloud&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;web_search&lt;/code&gt; and &lt;code&gt;web_fetch&lt;/code&gt; tools can return thousands of tokens. It is recommended to increase the context length of the model to ~32000 tokens for reasonable performance. Search agents work best with full context length.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fetching page results&lt;/head&gt;
    &lt;p&gt;To fetch individual pages (e.g. when a user provides a url in the prompt), use the new web fetch API.&lt;/p&gt;
    &lt;head rend="h4"&gt;Python library&lt;/head&gt;
    &lt;code&gt;from ollama import web_fetch

result = web_fetch('https://ollama.com')
print(result)
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;WebFetchResponse(
    title='Ollama',
    content='[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama\n\n**Chat &amp;amp; build
with open models**\n\n[Download](https://ollama.com/download) [Explore
models](https://ollama.com/models)\n\nAvailable for macOS, Windows, and Linux',
    links=['https://ollama.com/', 'https://ollama.com/models', 'https://github.com/ollama/ollama']
)
&lt;/code&gt;
    &lt;p&gt;Example Python code is available on GitHub.&lt;/p&gt;
    &lt;head rend="h4"&gt;JavaScript library&lt;/head&gt;
    &lt;code&gt;import { Ollama } from "ollama";

const client = new Ollama();
const fetchResult = await client.webFetch({ url: "https://ollama.com" });
console.log(JSON.stringify(fetchResult, null, 2));
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;{
  "title": "Ollama",
  "content": "[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...",
  "links": [
    "https://ollama.com/",
    "https://ollama.com/models",
    "https://github.com/ollama/ollama"
  ]
}
&lt;/code&gt;
    &lt;p&gt;Example JavaScript code is available on GitHub.&lt;/p&gt;
    &lt;head rend="h4"&gt;cURL&lt;/head&gt;
    &lt;code&gt;curl --request POST \
  --url https://ollama.com/api/web_fetch \
  --header "Authorization: Bearer $OLLAMA_API_KEY" \
  --header 'Content-Type: application/json' \
  --data '{
      "url": "ollama.com"
}'
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;{
  "title": "Ollama",
  "content": "[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...",
  "links": [
    "http://ollama.com/",
    "http://ollama.com/models",
    "https://github.com/ollama/ollama"
  ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Integrations&lt;/head&gt;
    &lt;head rend="h3"&gt;MCP Server (Model Context Protocol server)&lt;/head&gt;
    &lt;p&gt;You can enable web search in any MCP client through the Python MCP server.&lt;/p&gt;
    &lt;head rend="h4"&gt;Cline&lt;/head&gt;
    &lt;p&gt;To integrate with Cline, configure MCP servers in its settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Manage MCP Servers &amp;gt; Configure MCP Servers &amp;gt; Add the configuration below&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "mcpServers": {
    "web_search_and_fetch": {
      "type": "stdio",
      "command": "uv",
      "args": ["run", "path/to/web-search-mcp.py"],
      "env": { "OLLAMA_API_KEY": "your_api_key_here" }
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Codex&lt;/head&gt;
    &lt;p&gt;Add the following configuration to &lt;code&gt;~/.codex/config.toml&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;[mcp_servers.web_search]
command = "uv"
args = ["run", "path/to/web-search-mcp.py"]
env = { "OLLAMA_API_KEY" = "your_api_key_here" }
&lt;/code&gt;
    &lt;head rend="h4"&gt;Goose&lt;/head&gt;
    &lt;p&gt;You can integrate with Ollama via Goose’s extensions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Web search is included with a free Ollama account, with much higher rate limits available by upgrading your Ollama subscription.&lt;/p&gt;
    &lt;p&gt;To get started, sign up for an Ollama account!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ollama.com/blog/web-search"/><published>2025-09-25T19:21:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45377748</id><title>Can a model trained on satellite data really find brambles on the ground?</title><updated>2025-09-26T08:43:35.989245+00:00</updated><content>&lt;doc fingerprint="3d09f083bdbd4a9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Can a model trained on satellite data really find brambles on the ground?&lt;/head&gt;
    &lt;p&gt;Over the summer Gabriel Mahler has been conducting research on hedgehog habitat mapping using Agent Based Models (ABMs) and remote sensing. Hedgehogs seem to like brambles and so as part of his work he has produced a bramble map. He did this by combining the TESSERA earth representation embeddings (using the geotessera library) with data from iNaturalist. The current model is an ensemble of logistic regression and a knn classifier.&lt;/p&gt;
    &lt;p&gt;Can we really see brambles from space? What better way to test the model than a quick field trip around Cambridge. Gabriel, Anil, Shane and I did just that today.&lt;/p&gt;
    &lt;p&gt;We started at Milton Community Centre, as the model was relatively confident there were brambles near the car park and along the path to Milton Park. It took us about 20 seconds to find the first one in an area indicated by the model.&lt;/p&gt;
    &lt;p&gt;So it turns out that there's a lot of bramble between the community center and entrance to Milton Country Park. We stopped six or seven times before reaching the park entrance. While the model predicted we'd find brambles all over the park, we went for the few areas of very high confidence near the entrance. In every place we checked, we found pretty significant amounts of bramble.&lt;/p&gt;
    &lt;p&gt;We collected photos of all the places we stopped, as well as recording our GPS location. One thought while out exploring is that the model did a great job predicting where we would find very large quantities of bramble without any cover. It didn't have high confidence in other areas where we found smaller brambles under partial cover. Since TESSERA is learned representation from remote sensing data (Sentinel 1 and 2), it would make sense that bramble partially obscured from above might be harder to spot. This is something we can potentially tease apart when we have more validation data.&lt;/p&gt;
    &lt;p&gt;Finally, we were satisfied the model was doing a good job in the park area and decided to pick a hotspot the model was predicting in part of a residential street. We drove over to find an empty plot that did indeed have a lot of bramble!&lt;/p&gt;
    &lt;p&gt;Another hotspot was on Fen Road and we stopped by to find this absolute unit:&lt;/p&gt;
    &lt;p&gt;Finally, we headed back in to Cambridge to see what one of the big hotspots in North Cambridge was like. To our amusement we ended up at the local nature reserve Bramblefields, which, true to its name, has a lot of bramble.&lt;/p&gt;
    &lt;p&gt;I was pleasantly surprised by how good Gabriel's model was for its simplicity. Great work!&lt;/p&gt;
    &lt;p&gt;We had hoped to actually re-run the model based on the data we were gathering but that proved tricky on a laptop, in a park. Given the richness of the TESSERA embeddings and the simplicity of the classifiers being used, a mobile phone-based human-in-the-loop active learning setup could be practical..&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://toao.com/blog/can-we-really-see-brambles-from-space"/><published>2025-09-25T19:28:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45379325</id><title>RedoxFS is the default filesystem of Redox OS, inspired by ZFS</title><updated>2025-09-26T08:43:35.769531+00:00</updated><content>&lt;doc fingerprint="1c99101b8162c9f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RedoxFS&lt;/head&gt;
    &lt;p&gt;This is the default filesystem of Redox OS, inspired by ZFS and adapted to a microkernel architecture.&lt;/p&gt;
    &lt;p&gt;Redox had a read-only ZFS driver but it was abandoned because of the monolithic nature of ZFS that created problems with the Redox microkernel design.&lt;/p&gt;
    &lt;p&gt;(It's a replacement for TFS)&lt;/p&gt;
    &lt;p&gt;Current features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compatible with Redox and Linux (FUSE)&lt;/item&gt;
      &lt;item&gt;Copy-on-write&lt;/item&gt;
      &lt;item&gt;Data/metadata checksums&lt;/item&gt;
      &lt;item&gt;Transparent encryption&lt;/item&gt;
      &lt;item&gt;Standard Unix file attributes&lt;/item&gt;
      &lt;item&gt;File/directory size limit up to 193TiB (212TB)&lt;/item&gt;
      &lt;item&gt;File/directory quantity limit up to 4 billion per 193TiB (2^32 - 1 = 4294967295)&lt;/item&gt;
      &lt;item&gt;Disk encryption fully supported by the Redox bootloader, letting it load the kernel off an encrypted partition.&lt;/item&gt;
      &lt;item&gt;MIT licensed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Being MIT licensed, RedoxFS can be bundled on GPL-licensed operating systems (Linux, for example).&lt;/p&gt;
    &lt;head rend="h2"&gt;Tooling&lt;/head&gt;
    &lt;p&gt;RedoxFS tooling can be used to create, mount and edit contents of an &lt;code&gt;.img&lt;/code&gt; file containing RedoxFS. It can be installed with:&lt;/p&gt;
    &lt;code&gt;cargo install redoxfs
&lt;/code&gt;
    &lt;p&gt;If you found errors while installing it, make sure to install &lt;code&gt;fuse3&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a disk&lt;/head&gt;
    &lt;p&gt;You can create an empty, non bootable RedoxFS by allocating an empty file with &lt;code&gt;fallocate&lt;/code&gt; then run &lt;code&gt;redoxfs-mkfs&lt;/code&gt; to initialize the whole image as &lt;code&gt;RedoxFS&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fallocate -l 1G redox.img
&lt;/code&gt;
    &lt;code&gt;redoxfs-mkfs redox.img
&lt;/code&gt;
    &lt;head rend="h3"&gt;Mount a disk&lt;/head&gt;
    &lt;p&gt;To mount the disk, run &lt;code&gt;redoxfs [image] [directory]&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;mkdir ./redox-img
&lt;/code&gt;
    &lt;code&gt;redoxfs redox.img ./redox-img
&lt;/code&gt;
    &lt;p&gt;It will mount the disk using FUSE underneath.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unmount&lt;/head&gt;
    &lt;p&gt;Unmount the disk using FUSE unmount binary:&lt;/p&gt;
    &lt;code&gt;fusermount3 ./redox-img
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://doc.redox-os.org/book/redoxfs.html"/><published>2025-09-25T21:25:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45380699</id><title>Redis is fast – I'll cache in Postgres</title><updated>2025-09-26T08:43:35.513895+00:00</updated><content>&lt;doc fingerprint="6770f5d56a0059d1"&gt;
  &lt;main&gt;
    &lt;p&gt;There are books &amp;amp; many articles online, like this one arguing for using Postgres for everything. I thought I’d take a look at one use case - using Postgres instead of Redis for caching. I work with APIs quite a bit, so I’d build a super simple HTTP server that responds with data from that cache. I’d start from Redis as this is something I frequently encounter at work, switch it out to Postgres using unlogged tables and see if there’s a difference.&lt;/p&gt;
    &lt;head rend="h2"&gt;The setup&lt;/head&gt;
    &lt;p&gt;I’ll run the experiment on my homelab’s k8s cluster. The idea is to run Postgres or Redis on one node, limiting it to 2CPUs via k8s limits, as well as 8GiB of memory. On another node, I’ll run the web server itself and then spin a pod for the benchmark executed via k6 on the third.&lt;/p&gt;
    &lt;p&gt;Both postgres and redis are used with the out of the box settings for the following images:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Postgres - &lt;code&gt;postgres:17.6&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Redis - &lt;code&gt;redis:8.2&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I wrote a simple webserver, with 2 endpoints, a cache and a “Session” struct which we’ll store in the cache:&lt;/p&gt;
    &lt;code&gt;var ErrCacheMiss = errors.New("cache miss")

type Cache interface {
	Get(ctx context.Context, key string) (string, error)
	Set(ctx context.Context, key string, value string) error
}

type Session struct {
	ID string
}


func serveHTTP(c Cache) {
	http.HandleFunc("/get", getHandler(c))
	http.HandleFunc("/set", setHandler(c))

	port := os.Getenv("PORT")
	if port == "" {
		port = "8080"
	}

	fmt.Println("Server starting on http://0.0.0.0:" + port)

	server := &amp;amp;http.Server{Addr: "0.0.0.0:" + port}

	go func() {
		if err := server.ListenAndServe(); err != nil &amp;amp;&amp;amp; err != http.ErrServerClosed {
			fmt.Println("Error starting server:", err)
		}
	}()

	quit := make(chan os.Signal, 1)
	signal.Notify(quit, os.Interrupt)
	&amp;lt;-quit

	fmt.Println("Shutting down server...")

	if err := server.Close(); err != nil {
		fmt.Println("Error shutting down server:", err)
	}
}&lt;/code&gt;
    &lt;p&gt;For redis, I’ve implemented the cache using &lt;code&gt;github.com/redis/go-redis/v9&lt;/code&gt; as follows:&lt;/p&gt;
    &lt;code&gt;type RedisCache struct {
	client *redis.Client
}

func NewRedisCache() *RedisCache {
	redisURL := os.Getenv("REDIS_URL")
	if redisURL == "" {
		redisURL = "localhost:6379"
	}

	fmt.Println("Connecting to Redis at", redisURL)

	client := redis.NewClient(&amp;amp;redis.Options{
		Addr:     redisURL,
		Password: "",
		DB:       0,
	})

	return &amp;amp;RedisCache{
		client: client,
	}
}

func (r *RedisCache) Get(ctx context.Context, key string) (string, error) {
	val, err := r.client.Get(ctx, key).Result()
	if err == redis.Nil {
		return "", ErrCacheMiss
	}
	if err != nil {
		return "", err
	}
	return val, nil
}

func (r *RedisCache) Set(ctx context.Context, key string, value string) error {
	return r.client.Set(ctx, key, value, 0).Err()
}&lt;/code&gt;
    &lt;p&gt;The postgres cache is implemented using the &lt;code&gt;github.com/jackc/pgx/v5&lt;/code&gt; library:
&lt;/p&gt;
    &lt;code&gt;type PostgresCache struct {
	db *pgxpool.Pool
}

func NewPostgresCache() (*PostgresCache, error) {
	pgDSN := os.Getenv("POSTGRES_DSN")
	if pgDSN == "" {
		pgDSN = "postgres://user:password@localhost:5432/mydb"
	}

	cfg, err := pgxpool.ParseConfig(pgDSN)
	if err != nil {
		return nil, err
	}

	cfg.MaxConns = 50
	cfg.MinConns = 10

	pool, err := pgxpool.NewWithConfig(context.Background(), cfg)
	if err != nil {
		return nil, err
	}

	_, err = pool.Exec(context.Background(), `
		CREATE UNLOGGED TABLE IF NOT EXISTS cache (
			key VARCHAR(255) PRIMARY KEY,
			value TEXT
		);
	`)
	if err != nil {
		return nil, err
	}

	return &amp;amp;PostgresCache{
		db: pool,
	}, nil
}

func (p *PostgresCache) Get(ctx context.Context, key string) (string, error) {
	var content string
	err := p.db.QueryRow(ctx, `SELECT value FROM cache WHERE key = $1`, key).Scan(&amp;amp;content)
	if err == pgx.ErrNoRows {
		return "", ErrCacheMiss
	}
	if err != nil {
		return "", err
	}
	return content, nil
}

func (p *PostgresCache) Set(ctx context.Context, key string, value string) error {
	_, err := p.db.Exec(ctx, `INSERT INTO cache (key, value) VALUES ($1, $2) ON CONFLICT (key) DO UPDATE SET value = $2`, key, value)
	return err
}&lt;/code&gt;
    &lt;p&gt;I’ll seed the redis and postgres with 30 million entries each, keeping record of the inserted uuids. From there, I’ll generate a subset of existing uuids to use while benchmarking. This allows for simulating both hits and misses.&lt;/p&gt;
    &lt;p&gt;I’ll do a few runs of benchmarks for gets first, then sets and then a mixed run. Each run will execute for 2 minutes. I’ll look at the number of operations per second, latencies as well as memory and CPU usage during those times.&lt;/p&gt;
    &lt;p&gt;To simulate a somewhat real scenario where only a subset of keys exist in the cache the set benchmark will have a 10% chance to update an existing key, whereas the get will have an 80% chance of picking an existing key. The mixed workload will have a 20% chance to execute a set scenario and 80% for the get scenario.&lt;/p&gt;
    &lt;head rend="h2"&gt;The results&lt;/head&gt;
    &lt;head rend="h3"&gt;Getting values from cache&lt;/head&gt;
    &lt;p&gt;Redis performed better than Postgres, which did not surprise me at all. The bottleneck was actually the HTTP server. The machine running the http server maxed out on CPU, with redis running comfortably with ~1280mCPU - short of the 2000mCPU limit imposed. Redis used ~3800MiB of RAM, which stayed flat across the runs.&lt;/p&gt;
    &lt;p&gt;For postgres, the bottleneck was the CPU on postgres side. It consistently maxed out the 2 cores dedicated to it, while also using ~5000MiB of RAM.&lt;/p&gt;
    &lt;p&gt;Redis also did better when it comes to latencies of the HTTP responses:&lt;/p&gt;
    &lt;head rend="h3"&gt;Setting values in cache&lt;/head&gt;
    &lt;p&gt;Once again Redis performed better. The CPU usage stayed roughly the same as in the case of the GET experiment, with the RAM usage growing to ~4300MiB due to the new keys being inserted. The bottleneck stayed on the HTTP server side, with Redis using ~1280mCPU once again.&lt;/p&gt;
    &lt;p&gt;Postgres once again was bottlenecked by the CPU, constantly using 100% of the 2 cores it was limited to. During the course of the run, the memory usage grew to ~5500MiB.&lt;/p&gt;
    &lt;p&gt;During the test, the endpoints with the Redis cache implementation also had better latencies:&lt;/p&gt;
    &lt;head rend="h3"&gt;Read/write performance&lt;/head&gt;
    &lt;p&gt;The mixed benchmark also returned the predictable result of Redis reigning superior. As has been the story so far, the CPU stayed put at ~1280mCPU, RAM usage grew a bit due to the new keys being inserted.&lt;/p&gt;
    &lt;p&gt;Postgres maxed out the two cores and reached around 6GiB of memory used.&lt;/p&gt;
    &lt;p&gt;Latencies once again were better when using redis:&lt;/p&gt;
    &lt;head rend="h3"&gt;Unlogged tables&lt;/head&gt;
    &lt;p&gt;In the benchmark, I’ve used an unlogged table for postgres but this has not seemed to help, or has it? If I rerun the same benchmark with a normal(logged) table we can look at the numbers.&lt;/p&gt;
    &lt;p&gt;The unlogged table makes a huge difference for the write benchmark and a somewhat smaller but still significant one for the mixed workload. This is because the unlogged tables skip the write ahead log making them a lot faster for writes. There’s very little difference for the read performance though and I expect more runs would show the two test cases converging.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Redis is faster than postgres when it comes to caching, there’s no doubt about it. It conveniently comes with a bunch of other useful functionality that one would expect from a cache, such as TTLs. It was also bottlenecked by the hardware, my service or a combination of both and could definitely show better numbers. Surely, we should all use Redis for our caching needs then, right? Well, I think I’ll still use postgres. Almost always, my projects need a database. Not having to add another dependency comes with its own benefits. If I need my keys to expire, I’ll add a column for it, and a cron job to remove those keys from the table. As far as speed goes - 7425 requests per second is still a lot. That’s more than half a billion requests per day. All on hardware that’s 10 years old and using laptop CPUs. Not many projects will reach this scale and if they do I can just upgrade the postgres instance or if need be spin up a redis then. Having an interface for your cache so you can easily switch out the underlying store is definitely something I’ll keep doing exactly for this purpose.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dizzy.zone/2025/09/24/Redis-is-fast-Ill-cache-in-Postgres/"/><published>2025-09-25T23:34:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381010</id><title>Investigating a Forged PDF</title><updated>2025-09-26T08:43:34.905741+00:00</updated><content>&lt;doc fingerprint="767cce1f3dd3c327"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Captcha Check&lt;/head&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mjg59.dreamwidth.org/73317.html"/><published>2025-09-26T00:14:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381584</id><title>Cloudflare Data Platform</title><updated>2025-09-26T08:43:34.709648+00:00</updated><content>&lt;doc fingerprint="6b1b9856ddb78a00"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;For Developer Week in April 2025, we announced the public beta of R2 Data Catalog, a fully managed Apache Iceberg catalog on top of Cloudflare R2 object storage. Today, we are building on that foundation with three launches:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Cloudflare Pipelines receives events sent via Workers or HTTP, transforms them with SQL, and ingests them into Iceberg or as files on R2&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;R2 Data Catalog manages the Iceberg metadata and now performs ongoing maintenance, including compaction, to improve query performance&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;R2 SQL is our in-house distributed SQL engine, designed to perform petabyte-scale queries over your data in R2&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Together, these products make up the Cloudflare Data Platform, a complete solution for ingesting, storing, and querying analytical data tables.&lt;/p&gt;
      &lt;p&gt;Like all Cloudflare Developer Platform products, they run on our global compute infrastructure. Theyâre built around open standards and interoperability. That means that you can bring your own Iceberg query engine â whether that's PyIceberg, DuckDB, or Spark â connect with other platforms like Databricks and Snowflake â and pay no egress fees to access your data.&lt;/p&gt;
      &lt;p&gt;Analytical data is critical for modern companies. It allows you to understand your userâs behavior, your companyâs performance, and alerts you to issues. But traditional data infrastructure is expensive and hard to operate, requiring fixed cloud infrastructure and in-house expertise. We built the Cloudflare Data Platform to be easy enough for anyone to use with affordable, usage-based pricing.&lt;/p&gt;
      &lt;p&gt;If you're ready to get started now, follow the Data Platform tutorial for a step-by-step guide through creating a Pipeline that processes and delivers events to an R2 Data Catalog table, which can then be queried with R2 SQL. Or read on to learn about how we got here and how all of this works.&lt;/p&gt;
      &lt;p&gt;We launched R2 Object Storage in 2021 with a radical pricing strategy: no egress fees â the bandwidth costs that traditional cloud providers charge to get data out â effectively ransoming your data. This was possible because we had already built one of the largest global networks, interconnecting with thousands of ISPs, cloud services, and other enterprises.&lt;/p&gt;
      &lt;p&gt;Object storage powers a wide range of use cases, from media to static assets to AI training data. But over time, we've seen an increasing number of companies using open data and table formats to store their analytical data warehouses in R2.&lt;/p&gt;
      &lt;p&gt;The technology that enables this is Apache Iceberg. Iceberg is a table format, which provides database-like capabilities (including updates, ACID transactions, and schema evolution) on top of data files in object storage. In other words, itâs a metadata layer that tells clients which data files make up a particular logical table, what the schemas are, and how to efficiently query them.&lt;/p&gt;
      &lt;p&gt;The adoption of Iceberg across the industry meant users were no longer locked-in to one query engine. But egress fees still make it cost-prohibitive to query data across regions and clouds. R2, with zero-cost egress, solves that problem â users would no longer be locked-in to their clouds either. They could store their data in a vendor-neutral location and let teams use whatever query engine made sense for their data and query patterns.&lt;/p&gt;
      &lt;p&gt;But users still had to manage all of the metadata and other infrastructure themselves. We realized there was an opportunity for us to solve a major pain point and reduce the friction of storing data lakes on R2. This became R2 Data Catalog, our managed Iceberg catalog.&lt;/p&gt;
      &lt;p&gt;With the data stored on R2 and metadata managed, that still left a few gaps for users to solve.&lt;/p&gt;
      &lt;p&gt;How do you get data into your Iceberg tables? Once it's there, how do you optimize for query performance? And how do you actually get value from your data without needing to self-host a query engine or use another cloud platform?&lt;/p&gt;
      &lt;p&gt;In the rest of this post, we'll walk through how the three products that make up the Data Platform solve these challenges.&lt;/p&gt;
      &lt;p&gt;Analytical data tables are made up of events, things that happened at a particular point in time. They might come from server logs, mobile applications, or IoT devices, and are encoded in data formats like JSON, Avro, or Protobuf. They ideally have a schema â a standardized set of fields â but might just be whatever a particular team thought to throw in there.&lt;/p&gt;
      &lt;p&gt;But before you can query your events with Iceberg, they need to be ingested, structured according to a schema, and written into object storage. This is the role of Cloudflare Pipelines.&lt;/p&gt;
      &lt;p&gt;Built on top of Arroyo, a stream processing engine we acquired earlier this year, Pipelines receives events, transforms them with SQL queries, and sinks them to R2 and R2 Data Catalog.&lt;/p&gt;
      &lt;p&gt;Pipelines is organized around three central objects:&lt;/p&gt;
      &lt;p&gt;Streams are how you get data into Cloudflare. They're durable, buffered queues that receive events and store them for processing. Streams can accept events in two ways: via an HTTP endpoint or from a Cloudflare Worker binding.&lt;/p&gt;
      &lt;p&gt;Sinks define the destination for your data. We support ingesting into R2 Data Catalog, as well as writing raw files to R2 as JSON or Apache Parquet. Sinks can be configured to frequently write files, prioritizing low-latency ingestion, or to write less frequent, larger files to get better query performance. In either case, ingestion is exactly-once, which means that we will never duplicate or drop events on their way to R2.&lt;/p&gt;
      &lt;p&gt;Pipelines connect streams and sinks via SQL transformations, which can modify events before writing them to storage. This enables you to shift left, pushing validation, schematization, and processing to your ingestion layer to make your queries easy, fast, and correct.&lt;/p&gt;
      &lt;p&gt;For example, here's a pipeline that ingests events from a clickstream data source and writes them to Iceberg:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;INSERT into events_table
SELECT
  user_id,
  lower(event) AS event_type,
  to_timestamp_micros(ts_us) AS event_time,
  regexp_match(url, '^https?://([^/]+)')[1]  AS domain,
  url,
  referrer,
  user_agent
FROM events_json
WHERE event = 'page_view'
  AND NOT regexp_like(user_agent, '(?i)bot|spider');&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;SQL transformations are very powerful and give you full control over how data is structured and written into the table. For example, you can&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Schematize and normalize your data, even using JSON functions to extract fields from arbitrary JSON&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Filter out events or split them into separate tables with their own schemas&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Redact sensitive information before storage with regexes&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Unroll nested arrays and objects into separate events&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Initially, Pipelines supports stateless transformations. In the future, we'll leverage more of Arroyo's stateful processing capabilities to support aggregations, incrementally-updated materialized views, and joins.&lt;/p&gt;
      &lt;p&gt;Cloudflare Pipelines is available today in open beta. You can create a pipeline using the dashboard, Wrangler, or the REST API. To get started, check out our developer docs.&lt;/p&gt;
      &lt;p&gt;We arenât currently billing for Pipelines during the open beta. However, R2 storage and operations incurred by sinks writing data to R2 are billed at standard rates. When we start billing, we anticipate charging based on the amount of data read, the amount of data processed via SQL transformations, and data delivered.&lt;/p&gt;
      &lt;p&gt;We launched the open beta of R2 Data Catalog in April and have been amazed by the response. Query engines like DuckDB have added native support, and we've seen useful integrations like marimo notebooks.&lt;/p&gt;
      &lt;p&gt;It makes getting started with Iceberg easy. Thereâs no need to set up a database cluster, connect to object storage, or manage any infrastructure. You can create a catalog with a couple of Wrangler commands:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ npx wrangler bucket create mycatalog 
$ npx wrangler r2 bucket catalog enable mycatalog&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This provisions a data lake that can scale to petabytes of storage, queryable by whatever engine you want to use with zero egress fees.&lt;/p&gt;
      &lt;p&gt;But just storing the data isn't enough. Over time, as data is ingested, the number of underlying data files that make up a table will grow, leading to slower and slower query performance.&lt;/p&gt;
      &lt;p&gt;This is a particular problem with low-latency ingestion, where the goal is to have events queryable as quickly as possible. Writing data frequently means the files are smaller, and there are more of them. Each file needed for a query has to be listed, downloaded, and read. The overhead of too many small files can dominate the total query time.&lt;/p&gt;
      &lt;p&gt;The solution is compaction, a periodic maintenance operation performed automatically by the catalog. Compaction rewrites small files into larger files which reduces metadata overhead and increases query performance.Â &lt;/p&gt;
      &lt;p&gt;Today we are launching compaction support in R2 Data Catalog. Enabling it for your catalog is as easy as: &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ npx wrangler r2 bucket catalog compaction enable mycatalog&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;We're starting with support for small-file compaction, and will expand to additional compaction strategies in the future. Check out the compaction documentation to learn more about how it works and how to enable it.&lt;/p&gt;
      &lt;p&gt;At this time, during open beta, we arenât billing for R2 Data Catalog. Below is our current thinking on future pricing:&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell/&gt;
          &lt;cell&gt;
            &lt;p&gt;Pricing*&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;R2 storage&lt;/p&gt;
            &lt;p&gt;For standard storage class&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;$0.015 per GB-month (no change)&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;R2 Class A operations&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;$4.50 per million operations (no change)&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;R2 Class B operations&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;$0.36 per million operations (no change)&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Data Catalog operations&lt;/p&gt;
            &lt;p&gt;e.g., create table, get table metadata, update table properties&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;$9.00 per million catalog operations&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Data Catalog compaction data processed&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;$0.005 per GB processed&lt;/p&gt;
            &lt;p&gt;$2.00 per million objects processed&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Data egress&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;$0 (no change, always free)&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;*prices subject to change prior to General Availability&lt;/p&gt;
      &lt;p&gt;We will provide at least 30 days notice before billing starts or if anything changes.&lt;/p&gt;
      &lt;p&gt;Having data in R2 Data Catalog is only the first step; the real goal is getting insights and value from it. Traditionally, that means setting up and managing DuckDB, Spark, Trino, or another query engine, adding a layer of operational overhead between you and those insights. What if instead you could run queries directly on Cloudflare?&lt;/p&gt;
      &lt;p&gt;Now you can. Weâve built a query engine specifically designed for R2 Data Catalog and Cloudflareâs edge infrastructure. We call it R2 SQL, and itâs available today as an open beta.&lt;/p&gt;
      &lt;p&gt;With Wrangler, running a query on an R2 Data Catalog table is as easy as&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ npx wrangler r2 sql query "{WAREHOUSE}" "\
  SELECT user_id, url FROM events \
  WHERE domain = 'mywebsite.com'"&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Cloudflare's ability to schedule compute anywhere on its global network is the foundation of R2 SQL's design. This lets us process data directly where it lives, instead of requiring you to manage centralized clusters for your analytical workloads.&lt;/p&gt;
      &lt;p&gt;R2 SQL is tightly integrated with R2 Data Catalog and R2, which allows the query planner to go beyond simple storage scanning and make deep use of the rich statistics stored in the R2 Data Catalog metadata. This provides a powerful foundation for a new class of query optimizations, such as auxiliary indexes or enabling more complex analytical functions in the future.&lt;/p&gt;
      &lt;p&gt;The result is a fully serverless experience for users. You can focus on your SQL without needing a deep understanding of how the engine operates. If you are interested in how R2 SQL works, the team has written a deep dive into how R2 SQLâs distributed query engine works at scale.&lt;/p&gt;
      &lt;p&gt;The open beta is an early preview of R2 SQL querying capabilities, and is initially focused around filter queries. Over time, we will be expanding its capabilities to cover more SQL features, like complex aggregations.&lt;/p&gt;
      &lt;p&gt;We're excited to see what our users do with R2 SQL. To try it out, see the documentation and tutorials. During the beta, R2 SQL usage is not currently billed, but R2 storage and operations incurred by queries are billed at standard rates. We plan to charge for the volume of data scanned by queries in the future and will provide notice before billing begins.&lt;/p&gt;
      &lt;p&gt;Today, you can use the Cloudflare Data Platform to ingest events into R2 Data Catalog and query them via R2 SQL. In the first half of 2026, weâll be expanding on the capabilities in all of these products, including:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Integration with Logpush, so you can transform, store, and query your logs directly within Cloudflare&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;User-defined functions via Workers, and stateful processing support for streaming transformations&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Expanding the featureset of R2 SQL to cover aggregations and joins&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In the meantime, you can get started with the Cloudflare Data Platform by following the tutorial to create an end-to-end analytical data system, from ingestion with Pipelines, through storage in R2 Data Catalog, and querying with R2 SQL.Â Weâre excited to see what you build! Come share your feedback with us on our Developer Discord.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/cloudflare-data-platform/"/><published>2025-09-26T01:37:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381590</id><title>Exploit allows for takeover of fleets of Unitree robots</title><updated>2025-09-26T08:43:34.202218+00:00</updated><content>&lt;doc fingerprint="b4089113aaa71981"&gt;
  &lt;main&gt;
    &lt;p&gt;A critical vulnerability in the Bluetooth Low Energy (BLE) Wi-Fi configuration interface used by several different Unitree robots can result in a root level takeover by an attacker, security researchers disclosed on 20 September. The exploit impacts Unitree’s Go2 and B2 quadrupeds and G1 and H1 humanoids. Because the vulnerability is wireless, and the resulting access to the affected platform is complete, the vulnerability becomes wormable, say the researchers, meaning “an infected robot can simply scan for other Unitree robots in BLE range and automatically compromise them, creating a robot botnet that spreads without user intervention.”&lt;/p&gt;
    &lt;p&gt;Initially discovered by security researchers Andreas Makris and Kevin Finisterre, UniPwn takes advantage of several security lapses that are still present in the firmware of Unitree robots as of 20 September, 2025. As far as IEEE Spectrum is aware, this is the first major public exploit of a commercial humanoid platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unitree Robots’ BLE Security Flaw Exposed&lt;/head&gt;
    &lt;p&gt;Like many robots, Unitree’s robots use an initial BLE connection to make it easier for a user to set up a Wi-Fi network connection. The BLE packets that the robot accepts are encrypted, but those encryption keys are hardcoded and were published on X (formerly Twitter) by Makris in July. Although the robot does validate the contents of the BLE packets to make sure that the user is authenticated, the researchers say that all it takes to become an authenticated user is to encrypt the string ‘unitree’ with the hardcoded keys and the robot will let someone in. From there, an attacker can inject arbitrary code masquerading as the Wi-Fi SSID and password, and when the robot attempts to connect to Wi-Fi, it will execute that code without any validation and with root privileges.&lt;/p&gt;
    &lt;p&gt;“A simple attack might be just to reboot the robot, which we published as a proof-of-concept,” explains Makris. “But an attacker could do much more sophisticated things: It would be possible to have a trojan implanted into your robot’s startup routine to exfiltrate data while disabling the ability to install new firmware without the user knowing. And as the vulnerability uses BLE, the robots can easily infect each other, and from there the attacker might have access to an army of robots.”&lt;/p&gt;
    &lt;p&gt;Makris and Finisterre first contacted Unitree in May in an attempt to responsibly disclose this vulnerability. After some back and forth with little progress, Unitree stopped responding to the researchers in July, and the decision was made to make the vulnerability public. “We have had some bad experiences communicating with them,” Makris tells us, citing an earlier backdoor vulnerability he discovered with the Unitree Go1. “So we need to ask ourselves—are they introducing vulnerabilities like this on purpose, or is it sloppy development? Both answers are equally bad.” Unitree has not responded to a request for comment from IEEE Spectrum as of press time.&lt;/p&gt;
    &lt;p&gt;“Unitree, as other manufacturers do, has simply ignored prior security disclosures and repeated outreach attempts,” says Víctor Mayoral-Vilches, the founder of robotics cybersecurity company Alias Robotics. “This is not the right way to cooperate with security researchers.” Mayoral-Vilches was not involved in publishing the UniPwn exploit, but he has found other security issues with Unitree robots, including undisclosed streaming of telemetry data to servers in China which could potentially include audio, visual, and spatial data.&lt;/p&gt;
    &lt;p&gt;Mayoral-Vilches explains that security researchers are focusing on Unitree primarily because the robots are available and affordable. This makes them not just more accessible for the researchers, but also more relevant, since Unitree’s robots are already being deployed by users around the world who are likely not aware of the security risks. For example, Makris is concerned that the Nottinghamshire Police in the UK have begun testing a Unitree Go2, which can be exploited by UniPwn. “We tried contacting them and would have disclosed the vulnerability upfront to them before going public, but they ignored us. What would happen if an attacker implanted themselves into one of these police dogs?”&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Secure Unitree Robots&lt;/head&gt;
    &lt;p&gt;In the short term, Mayoral-Vilches suggests that people using Unitree robots can protect themselves by only connecting the robots to isolated Wi-Fi networks and disabling their Bluetooth connectivity. “You need to hack the robot to secure it for real,” he says. “This is not uncommon and why security research in robotics is so important.”&lt;/p&gt;
    &lt;p&gt;Both Mayoral-Vilches and Makris believe that fundamentally it’s up to Unitree to make their robots secure in the long term, and that the company needs to be much more responsive to users and security researchers. But Makris says: “There will never be a 100 percent secure system.”&lt;/p&gt;
    &lt;p&gt;Mayoral-Vilches agrees. “Robots are very complex systems, with wide attack surfaces to protect, and a state-of-the-art humanoid exemplifies that complexity.”&lt;/p&gt;
    &lt;p&gt;Unitree, of course, is not the only company offering complex state-of-the-art quadrupeds and humanoids, and it seems likely (if not inevitable) that similar exploits will be discovered in other platforms. The potential consequences here can’t be overstated—the idea that robots can be taken over and used for nefarious purposes is already a science fiction trope, but the impact of a high-profile robot hack on the reputation of the commercial robotics industry is unclear. Robots companies are barely talking about security in public, despite how damaging even the perception of an unsecured robot might be. A robot that is not under control has the potential to be a real physical danger.&lt;/p&gt;
    &lt;p&gt;At the IEEE Humanoids Conference in Seoul from 30 September to 2 October, Mayoral-Vilches has organized a workshop on Cybersecurity for Humanoids, where he will present a brief (co-authored with Makris and Finisterre) titled Humanoid Robots as Attack Vectors. Despite the title, their intent is not to overhype the problem but instead to encourage roboticists (and robotics companies) to take security seriously, and not treat it as an afterthought. As Mayoral-Vilches points out, “robots are only safe if secure.”&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This Robotics Startup Wants to Be the Boston Dynamics of China ›&lt;/item&gt;
      &lt;item&gt;Unitree’s New Go2 Is One Dynamic Quadruped ›&lt;/item&gt;
      &lt;item&gt;Unitree’s Go1 Robot Dog Looks Pretty Great, Costs Just USD $2700 ›&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Evan Ackerman is a senior editor at IEEE Spectrum. Since 2007, he has written over 6,000 articles on robotics and technology. He has a degree in Martian geology and is excellent at playing bagpipes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/unitree-robot-exploit"/><published>2025-09-26T01:38:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381631</id><title>Bit is all we need: binary normalized neural networks</title><updated>2025-09-26T08:43:33.970920+00:00</updated><content>&lt;doc fingerprint="853f05b8443e96ce"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 7 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:1 bit is all we need: binary normalized neural networks&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are critical to ensure practical deployment and effective utilization of these models across various applications. In this work, a novel type of neural network layers and models is developed that uses only single-bit parameters. In this novel type of models all parameters of all layers, including kernel weights and biases, only have values equal to zero or one. This novel type of models uses layers named as binary normalized layer. These binary normalized layers can be of any type, such as fully connected, convolutional, attention, etc., and they consist of slight variations of the corresponding conventional layers. To show the effectiveness of the binary normalized layers, two different models are configured to solve a multiclass image classification problem and a language decoder to predict the next token of a sequence. The model to solve the image classification has convolutional and fully connected layers, and the language model is composed of transformer blocks with multi-head attention. The results show that models with binary normalized layers present almost the same results obtained by equivalent models with real 32-bit parameters. The binary normalized layers allow to develop models that use 32 times less memory than current models and have equivalent performance. Besides, the binary normalized layers can be easily implemented on current computers using 1-bit arrays, and do not require the development of dedicated electronic hardware. This novel type of layers opens a new era for large neural network models with reduced memory requirements that can be deployed using simple and cheap hardware, such as mobile devices or only cpus.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.07025"/><published>2025-09-26T01:43:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45381813</id><title>Writing Memory Safe JIT Compilers</title><updated>2025-09-26T08:43:33.825516+00:00</updated><content>&lt;doc fingerprint="8e229c19fc33becc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Writing Truly Memory Safe JIT Compilers&lt;/head&gt;
    &lt;head rend="h2"&gt;How to kill off a top source of browser exploits&lt;/head&gt;
    &lt;p&gt;Last month the V8 team published an excellent blog post on what they call the V8 Sandbox. This isn’t a sandbox for your JavaScript code — it’s intended to mitigate browser exploits caused by bugs in the JIT compiler itself. That’s important work because they report that most Chrome exploits start with a V8 memory safety bug.&lt;/p&gt;
    &lt;p&gt;V8 is written in C++, so it may seem like these are the sort of bugs you’d expect from working in a memory-unsafe language. Unfortunately the situation is more complex. Why? The team explain:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There is a catch: V8 vulnerabilities are rarely “classic” memory corruption bugs (use-after-frees, out-of-bounds accesses, etc.) but instead subtle logic issues which can in turn be exploited to corrupt memory. As such, existing memory safety solutions are, for the most part, not applicable to V8. In particular, neither switching to a memory safe language, such as Rust, nor using current or future hardware memory safety features, such as memory tagging, can help with the security challenges faced by V8 today.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;They give an example bug that can cause memory corruption without the engine itself containing any normal memory safety problems, as VM intrinsics or JIT compiled machine code itself may accidentally rely on invalid assumptions about memory.&lt;/p&gt;
    &lt;p&gt;It would be nice if there was a rigorous approach to writing language runtimes that eliminated such bugs by design.&lt;/p&gt;
    &lt;p&gt;GraalVM has a JavaScript engine called GraalJS. It’s written in Java using the Truffle language framework. Its peak performance is competitive with V8 and on a few benchmarks (such as ray tracing) is actually faster!&lt;/p&gt;
    &lt;p&gt;Although being written in Java does improve memory safety, we just saw that rewriting V8 in a safe language wouldn’t help with the types of bugs V8 is trying to solve and so we would intuitively expect that GraalJS must suffer from the same classes of bugs. Yet, it doesn’t. Let’s take a look at why not. Along the way we’ll explore the first Futamura projection, the core theoretical idea underpinning Truffle.&lt;/p&gt;
    &lt;p&gt;All fast language VMs work the same way. A program is loaded from disk into in-memory data structures representing the program, either an abstract syntax tree or byte code. The program starts running in an interpreter. Parts are soon discovered to be hot spots, i.e. the program spends much more time there than in other parts. Those hot spots are passed to a just-in-time compiler that converts them to optimized machine code, and execution then jumps back and forth between the interpreter and the collection of compiled program fragments. This gives a big performance boost.&lt;/p&gt;
    &lt;p&gt;This architecture is standard — both the JVM and V8 use it — but viewed from a security perspective the design has a flaw: it’s error prone. The language semantics are implemented twice, once for the interpreter and again for the JIT compiler. It’s critical not only that both places are fully correct but also that they exactly match. Otherwise, the VM becomes exploitable.&lt;/p&gt;
    &lt;p&gt;Truffle is a Java library that helps you build advanced, high performance language runtimes. VMs built using the Truffle framework operate in a fundamentally different way to conventional VMs, one that not only makes them much easier to write but which also eliminates memory safety bugs by design. It all starts with you writing an interpreter for your language in Java. This doesn’t mean compiling your target language to JVM bytecode — in fact bytecode won’t feature anywhere in this story. You just write an ordinary interpreter. Because the interpreter’s code is garbage collected and bounds-checked, malicious user code can’t use memory safety bugs to exploit it.&lt;/p&gt;
    &lt;p&gt;If you think about conventional Java then this may sound quite slow — isn’t Java itself interpreted until it gets JIT compiled? Are we … interpreting an interpreter? Fortunately not because you can ship your Truffle-based language runtime as a native executable, meaning it’s compiled ahead of time to fully native code using the Graal compiler (from which the wider umbrella project takes its name).&lt;/p&gt;
    &lt;p&gt;So at the start of the user’s program their JavaScript is running in a regular interpreter shipped as a normal executable binary or DLL, but which still benefits from the safety properties of a Java program. Soon some methods get hot. At this point something unconventional happens. The Truffle framework is keeping track of which functions are hot for you and will decide to schedule JIT compilations. But unlike in a conventional VM design, you don’t write your own JIT compiler. Instead your user’s code is automatically compiled by the same general-purpose Graal compiler that was used to convert your interpreter to native code, and execution will start automatically switching back and forth between the interpreter and compiled functions. This is possible thanks to an unusual technique called partial evaluation (or the first Futamura projection).&lt;/p&gt;
    &lt;p&gt;You might not have encountered Futamura projections or partial evaluation before, so what is this strange sounding thing?&lt;/p&gt;
    &lt;p&gt;The core idea is to automatically transform the code of your interpreter to create individual JIT compiled user methods. Instead of needing to carefully implement the language semantics in two places (interpreter and hand-crafted JIT), it’s sufficient to implement it just once. As the interpreter is memory safe and the transform preserves interpreter semantics, the compiled version of the user’s code is guaranteed to match the interpreter’s behavior and is therefore also automatically memory safe. This makes it much harder to slip up and write an exploitable VM.&lt;/p&gt;
    &lt;p&gt;There are several tricks that make this possible. The most important is a new form of constant-ness, added to Java using annotations. In normal programming a variable is either mutable or immutable. An immutable variable is marked with a special keyword such as&lt;code&gt;final&lt;/code&gt; or &lt;code&gt;const&lt;/code&gt; and must be set only once, at the declaration site. Constants are great for compilers because they can be folded, meaning that references to them can be replaced with their value. Consider the following bit of code:&lt;/p&gt;
    &lt;code&gt;class Example {&lt;lb/&gt;    private static final int A = 1;&lt;lb/&gt;    private static final int B = 2;&lt;lb/&gt;&lt;lb/&gt;    static int answer() {&lt;lb/&gt;        return A - B;&lt;lb/&gt;    }&lt;lb/&gt;&lt;lb/&gt;    static String doSomething() {&lt;lb/&gt;        if (answer() &amp;lt; 0) &lt;lb/&gt;            return "OK" &lt;lb/&gt;        else &lt;lb/&gt;            throw new IllegalStateException();&lt;lb/&gt;    }&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;It’s easy to see the &lt;code&gt;answer()&lt;/code&gt; method will always return the same number. A good compiler will substitute 1 and 2 into the expression yielding &lt;code&gt;return 1 — 2&lt;/code&gt; and pre-compute the answer. Then it will inline any calls to answer (i.e. copy/paste the implementation into the call site), substituting those with -1 and thus removing the call overhead as well. That in turn may trigger even more constant folding, such as in the &lt;code&gt;doSomething&lt;/code&gt; method where the compiler will prove that the exception can never be thrown and delete it entirely. Having done that, &lt;code&gt;doSomething&lt;/code&gt; can also be optimized out by simply replacing it with “OK”, and so on.&lt;/p&gt;
    &lt;p&gt;That’s neat, but every compiler can do that … as long as the constant values are known at compile time. Truffle changes that by introducing a third kind of const-ness called compilation final. If in your interpreter implementation you declare a variable like this:&lt;/p&gt;
    &lt;code&gt;@CompilationFinal private int a = 1;&lt;/code&gt;
    &lt;p&gt;then it will change its const-ness depending on when it’s being accessed. From inside your interpreter, it’s mutable. You will use such variables to implement your interpreter. They’ll be set when you load your user’s program and maybe also whilst it runs. Once a function in the user’s script becomes hot, Truffle will work together with the Graal compiler to recompile the parts of the interpreter corresponding to the user’s code, and this time &lt;code&gt;a&lt;/code&gt;will be treated as if it was a constant, i.e. the same as the literal value &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This works for any kind of data, including complex objects. Consider the following highly simplified pseudocode:&lt;/p&gt;
    &lt;code&gt;import com.oracle.truffle.api.nodes.Node;&lt;lb/&gt;&lt;lb/&gt;class JavaScriptFunction extends Node {&lt;lb/&gt;    @CompilationFinal Node[] statements;&lt;lb/&gt;&lt;lb/&gt;    Object execute() {&lt;lb/&gt;        for (var statement : statements) statement.execute();&lt;lb/&gt;    } &lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;This is the sort of class you might find in a typical abstract syntax tree interpreter. The &lt;code&gt;statements&lt;/code&gt; array is marked compilation-final. When the program is first loaded we can initialize the array with objects representing the different things a user’s JavaScript function is doing, because it’s mutable. Now imagine that the function represented by this object gets hot. Truffle will start a special compilation of the &lt;code&gt;execute()&lt;/code&gt; method in which Graal is told that the &lt;code&gt;this&lt;/code&gt; pointer should be treated implicitly as compilation-final. Because the object is treated as constant, so can &lt;code&gt;this.statements&lt;/code&gt; also be treated as constant. It’ll be substituted with the exact contents of a specific &lt;code&gt;JavaScriptFunction&lt;/code&gt; object on the interpreter heap enabling the compiler to unroll the loop inside &lt;code&gt;execute&lt;/code&gt;, transforming it to look like this:&lt;/p&gt;
    &lt;code&gt;Object execute() {&lt;lb/&gt;    this.statements[0].execute();&lt;lb/&gt;    this.statements[1].execute();&lt;lb/&gt;    this.statements[2].execute();&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;Here &lt;code&gt;Node&lt;/code&gt; is a superclass and &lt;code&gt;execute()&lt;/code&gt; is virtual, but that doesn’t matter. Because the list is compilation-final the individual objects in the list are also constant folded, so the &lt;code&gt;execute&lt;/code&gt; method can be de-virtualized (resolved to whatever concrete type it really is) and then inlined as well.&lt;/p&gt;
    &lt;p&gt;And on and on we go. At the end the compiler generates a native function which matches the semantics of the user’s JavaScript (or Python or C++ or whatever language we’re implementing). Invocations of the specific &lt;code&gt;JavaScriptFunction.execute()&lt;/code&gt; method that were compiled are diverted, so when the interpreter invokes it, there will be a transition from interpreter to native code and back. If your interpreter realizes it needs to change a &lt;code&gt;@CompilationFinal&lt;/code&gt; field, for example because the program changes its behavior and invalidates an optimistic assumption you made, that's absolutely fine. Truffle will let you do that and "deoptimizes" the program back to the interpreter for you. Deoptimization (tech talk) is an advanced technique that's normally very hard to implement securely, as it means mapping the optimized CPU state back to the interpreter state and once again, any mistakes can be exploitable (you may be seeing a theme here). But you don’t have to write any of this. It’s all done for you by Truffle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this work?&lt;/head&gt;
    &lt;p&gt;It might not be obvious why partial evaluation actually makes things faster.&lt;/p&gt;
    &lt;p&gt;Interpreters are slow because they have to make a lot of decisions. The user’s program could do anything, so interpreters must constantly check for many possibilities to find out what the program is trying to do at that exact moment. Because branches and memory loads are difficult for the CPU to execute quickly, the whole program ends up being slow. This technique of compiling an interpreter with enhanced constant folding eliminates branches and loads. On top of this, Truffle builds an API that makes it easy to implement advanced optimizations and features for JavaScript or indeed, for any other language you have an interpreter for. For example, it offers a simple API for using assumptions — a way to JIT compile code that executes faster by not including code for handling edge cases. If such an edge case is hit then the compiled code can be thrown away and regenerated to take into account that the edge case was observed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recompilation&lt;/head&gt;
    &lt;p&gt;Above we briefly mentioned “recompilation”, but glossed over how that’s possible. We said the interpreter is just native code, right?&lt;/p&gt;
    &lt;p&gt;When the interpreter was compiled ahead of time with the &lt;code&gt;native-image&lt;/code&gt; in preparation for shipping to the user’s computer, the Graal compiler recognized that it was compiling a Truffle-using program. Graal and Truffle are co-developed, so although they can be used independently, when used together they recognize each other and collaborate.&lt;/p&gt;
    &lt;p&gt;Graal changes its behavior in a couple of ways when it notices it’s compiling a Truffle language ahead-of-time. Firstly, it adds a copy of itself to the output program. Interpreter methods are then discovered by doing a static analysis of the program and then stored in the resulting executable, but with a twist: they’re stored more than once. One version is directly executable machine code. That’s your regular generic interpreter. Another is a carefully encoded form of Graal’s intermediate representation (or IR). An IR is sort of half way between the source code you write and the machine code that eventually executes (Graal’s IR is an object graph). Graal also compiles in a garbage collector, either the advanced and mature G1 collector (if you use Oracle GraalVM) or a simpler GC written in pure Java (if you use the GraalVM Community Edition).&lt;/p&gt;
    &lt;p&gt;When a user function gets hot, Truffle looks up the embedded IR for the “execute a user function” node and partially evaluates it. The evaluation is interleaved with the parsing of the graph IR to ensure that the process is as efficient as possible — if something won’t be executed because constant folding already proved it can’t be reached it won’t even be decoded or seen by the compiler. This also ensures that memory usage during the compile is kept low.&lt;/p&gt;
    &lt;head rend="h2"&gt;My only friend, the end&lt;/head&gt;
    &lt;p&gt;And that’s it! That’s how an entire class of subtle safety bugs is eliminated in GraalJS: because the semantics of the language are defined by the memory-safe interpreter and then partially evaluated, the generated machine code is also memory safe by construction.&lt;/p&gt;
    &lt;p&gt;What about the V8 sandbox that the original blog post is about? Expressing pointers as offsets from a heap base is a great idea that’s already used in GraalVM natively compiled binaries. However this is done for performance, as the other memory safety mechanisms mean there’s no need for mitigating heap overwrites.&lt;/p&gt;
    &lt;p&gt;None of the above is in any way specific to JavaScript, and nor are Truffle’s benefits limited to security and performance. In fact Truffle automatically adds many other features to your language, such as debugging (through Chrome Debugger’s wire protocol), language interop with both Java/Kotlin/etc and any other Truffle language, a fast regular expression engine, a fast foreign function interface, profiling tools, heap snapshotting and much more. Truffle has been used to build over 30 language VMs for dozens of languages, including languages you wouldn’t expect to have such features such as the recent Pkl configuration language from Apple.&lt;/p&gt;
    &lt;p&gt;If this article has whetted your appetite to learn more, take a look at the documentation or this tech talk on how it all works.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://medium.com/graalvm/writing-truly-memory-safe-jit-compilers-f79ad44558dd"/><published>2025-09-26T02:10:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45382397</id><title>My Deus Ex lipsyncing fix mod</title><updated>2025-09-26T08:43:32.373259+00:00</updated><content>&lt;doc fingerprint="43251212cb9cd376"&gt;
  &lt;main&gt;
    &lt;p&gt;Back in 2021 I made a mod for Deus Ex 1 that fixes the lipsyncing and blinking, which, I betcha didn’t know, was broken since ship. Everything I wrote about it is on Twitter, and it oughta be somewhere else, so here’s a post about it. The mod itself can be downloaded here.&lt;/p&gt;
    &lt;p&gt;I guess I was playing DX1 and thinking, geez, was this lipsync always this bad? In a weird way? It’s insta-snapping mouth shapes, but they’re not always the same mouth shapes. Is this broken? I couldn’t find anything online about it, but I did find this article: an interview with Chris Norden, a coder on DX, where he goes into the lipsyncing and how it was, at one point, super elaborate and amazing, and they had to pare it back for performance reasons. I thought I’d check how much of this was done in Unrealscript (since the C++ source for DX is nowhere) and whether I could un-pare it. It turns out it was an extremely simple fix to get it as good as I got it, and I think that’s as good as you can get it until someone leaks the source code.&lt;/p&gt;
    &lt;p&gt;I’d messed around with lipsyncing stuff before and was familiar with the broad strokes of how it tends to work via my intense familiarity with Half-Life 2: you figure out, hopefully automatically, the sounds (phonemes) present in a sound file (“oo”, “ah”, whatever) and map those to mouth shapes (visemes), then when the audio plays, move the mouth into the right shape for the phoneme we’re in at this moment. The figuring-out process is called “phoneme extraction”, at least by Valve, and Valve do this offline, because it takes a sec. In Valve’s case they append this phoneme information to the end of the .wav file, and it looks like this:&lt;/p&gt;
    &lt;code&gt;PLAINTEXT
{
Okay, I don't blame you for hesitating, but if we're gonna do this thing, then let's just get through it. 
}
WORDS
{
WORD Okay 0.064 0.224
{
111 ow 0.014 0.096 1.000
107 k 0.096 0.142 1.000
101 ey 0.142 0.220 1.000
}
WORD I 0.224 0.352
{
593 ay 0.220 0.310 1.000
105 iy 0.310 0.364 1.000
}
WORD don't 0.352 0.496
{
100 d 0.364 0.396 1.000
111 ow 0.396 0.456 1.000
110 n 0.456 0.496 1.000
}
&lt;/code&gt;
    &lt;p&gt;, etc. Phonemes, start times, end times. Easy!&lt;/p&gt;
    &lt;p&gt;My assumption is that the reason Deus Ex’s super cool lipsyncing was too expensive to ship was, they don’t seem to save this information anywhere, so I guess they were figuring out the phonemes in realtime. If correct, this is sort of a bummer – doing what Valve did would have scooped the whole cost out. Maybe there was more to it.&lt;/p&gt;
    &lt;p&gt;Anyway, the Unrealscript. Deus Ex is pre-Unreal having skeletal animation, it’s all vertex animation. The character heads have a few: relevant here, 7 visemes and a blink. &lt;code&gt;nextphoneme&lt;/code&gt; is set from somewhere outside this code (probably a cpp audio system I can’t access) to A, E, F, M, O, T or U, which it doesn’t matter which is which and I don’t remember, or X, which is nothing (close mouth). Then this Unrealscript on the character sets the head’s anim sequence to the appropriate pose. This all happens on tick, but only if &lt;code&gt;IsSpeaking&lt;/code&gt;. We have a &lt;code&gt;tweentime&lt;/code&gt; we’re using to blend between these poses, so we should be seeing nice smooth blending, the lack of which is why I’m here in the first place! So what’s the problem?&lt;/p&gt;
    &lt;p&gt;The main thing is a dodgy frame rate check:&lt;/p&gt;
    &lt;code&gt;// update the animation timers that we are using
	animTimer[0] += deltaTime;
	animTimer[1] += deltaTime;
	animTimer[2] += deltaTime;

	if (bIsSpeaking)
	{
		// if our framerate is high enough (&amp;gt;20fps), tween the lips smoothly
		if (Level.TimeSeconds - animTimer[3]  &amp;lt; 0.05)
			tweentime = 0;
		else
			tweentime = 0.1;
&lt;/code&gt;
    &lt;p&gt;“tweentime” is how long it takes to blend to the next viseme in seconds; if 0, it’s an instant snap. The intent here is to skip blending entirely if our framerate is so low that it looks better snapping the lips around than showing any in-between poses, only it doesn’t work. The code is keeping &lt;code&gt;Level.TimeSeconds&lt;/code&gt; from the previous frame and subtracting that from the current &lt;code&gt;Level.TimeSeconds&lt;/code&gt; to get deltatime, which if it’s less than 0.05, we’re assumed to be getting less than 20fps. So it’s flipped.&lt;/p&gt;
    &lt;p&gt;Also, 0.1 is just way too fast a value, which I suspect a reason for that I’ll come back to*. I increased it to 0.35 to make the blends take long enough to really see.&lt;/p&gt;
    &lt;p&gt;With that fixed, the lipsync is smooth! Hooray! But it’s not perfect: at the end of a line, when the audio finishes, we don’t smoothly close the mouth; we snap the mouth shut instantly. This is because we’re only doing any blending if &lt;code&gt;bIsSpeaking=true&lt;/code&gt;, which it suddenly isn’t. The perf hit of this function no longer matters at all, so I just skip that check too: every character always gets to run lipsync. Tweentime is also local to this function and initialises at 0, so I had to set it to 0.3 to get blending even when we have no phoneme.&lt;/p&gt;
    &lt;p&gt;Blinking was also way too fast, so fast as to be invisible, so I slowed it down a ton. Now you can see ’em blinkin’.&lt;/p&gt;
    &lt;p&gt;So now we have nice blinking and smooth mouth movement, but there’s one thing that still sucks: presumably as part of the optimisation that made this ship at all, &lt;code&gt;nextphoneme&lt;/code&gt; does not update every tick, or anywhere near every tick. It doesn’t even update at a fixed rate – sometimes you’ll get a good amount of updates in a sentence, sometimes one or two. This means that all the smooth blending in the world won’t get you a correct result unless you happen to get lucky: JC can be speaking the M in “a bomb” and you’re still back on the “a”. As far as I can tell there’s no way to fix this right now – the code that updates the phonemes just needs to do it every tick, and it don’t, and it’s not Unrealscript so I can’t touch it. If the time between phoneme updates was at least consistent, you could set tweentime to that duration and make your blend take as long as it takes for a new phoneme to show up, but it ain’t. So close!&lt;/p&gt;
    &lt;p&gt;*In the interview where Norden alludes to this amazing lipsync demo they had going on before they optimised it down, I assume it was initially getting a new phoneme every tick, and that is probably when they set 0.1 seconds as a blend duration. If you’re getting constant new phonemes, blending super fast to the next one makes sense; it’s only when you’re not that a slower blend time looks good.&lt;/p&gt;
    &lt;p&gt;There’s a lot of jank to this code. The silliest thing about it might be that it lives in &lt;code&gt;ScriptedPawn&lt;/code&gt;, Deus Ex’s NPC class, which does not share an immediate parent with the player character, so this whole function is just duplicated between the two classes.&lt;/p&gt;
    &lt;p&gt;Anyway, here’s the whole function after I futzed with it.&lt;/p&gt;
    &lt;code&gt;// lip synching support - DEUS_EX CNN
//
function LipSynch(float deltaTime)
{
	local name animseq;
	local float rnd;
	local float tweentime;

	// update the animation timers that we are using
	animTimer[0] += deltaTime;
	animTimer[1] += deltaTime;
	animTimer[2] += deltaTime;

	if (bIsSpeaking)
	{
		// if our framerate is high enough (&amp;gt;20fps), tween the lips smoothly
		
//JOE CHANGE: 
//This used to set tweentime to 0 (no blend) if it thought FPS was low, else 0.1. It was 
//backwards though, the result was the opposite. 
//Even 0.1 is too fast to look good though. Anyway, skip the check, we don't care
//
//		if (Level.TimeSeconds - animTimer[3]  &amp;lt; 0.05)
//			tweentime = 0.4;
//		else
			tweentime = 0.36;

//Also, ideally tweentime would be the duration until the next time we get a phoneme update?
//But I don't know where that update comes from at the moment

		// the last animTimer slot is used to check framerate
		animTimer[3] = Level.TimeSeconds;

		if (nextPhoneme == "A")
			animseq = 'MouthA';
		else if (nextPhoneme == "E")
			animseq = 'MouthE';
		else if (nextPhoneme == "F")
			animseq = 'MouthF';
		else if (nextPhoneme == "M")
			animseq = 'MouthM';
		else if (nextPhoneme == "O")
			animseq = 'MouthO';
		else if (nextPhoneme == "T")
			animseq = 'MouthT';
		else if (nextPhoneme == "U")
			animseq = 'MouthU';
		else if (nextPhoneme == "X")
			animseq = 'MouthClosed';

		if (animseq != '')
		{
					if (lastPhoneme != nextPhoneme)
			{
				lastPhoneme = nextPhoneme;
				TweenBlendAnim(animseq, tweentime);
				TimeLastPhoneme = Level.TimeSeconds;
			}
		
		}
		

//		if ((Level.TimeSeconds - TimeLastPhoneme) &amp;gt;= tweentime*0.8 &amp;amp;&amp;amp; TimeLastPhoneme != 0)
//		{
//		TweenBlendAnim('MouthClosed', 0.2);
//		nextPhoneme = "X";
//		lastPhoneme = "A";
//		TimeLastPhoneme = Level.TimeSeconds;
//		}
	}
	else
	if (bWasSpeaking)
	{
		bWasSpeaking = false;
		
//JOE: I added this tweentime set. Without it it was 0 as initialised, so the jaw snapped shut

		tweentime = 0.3;
		TweenBlendAnim('MouthClosed', tweentime);
	}

	// blink randomly
	if (animTimer[0] &amp;gt; 0.5)
	{
		animTimer[0] = 0;
		if (FRand() &amp;lt; 0.4)
			PlayBlendAnim('Blink', 0.2, 0.1, 1);
	}

	LoopHeadConvoAnim();
	LoopBaseConvoAnim();
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.joewintergreen.com/my-deus-ex-lipsyncing-fix-mod-making-of/"/><published>2025-09-26T03:45:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45382434</id><title>Flock Reinstalls Cameras Without City Approval After Unlawful Govt Access</title><updated>2025-09-26T08:43:32.252359+00:00</updated><content>&lt;doc fingerprint="6f8091763aa212fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Private surveillance vendor Flock Safety reinstalled all of its stationary license plate cameras in Evanston that had previously been removed, apparently doing so without authorization from the city, which sent the company a cease-and-desist order Tuesday afternoon demanding that the cams be taken back down.&lt;/p&gt;
    &lt;p&gt;The city previously ordered Flock to shut down 19 automated license plate readers (18 stationary and one flex camera that can be attached to a squad car) provided by the company and put its contract with Flock on a 30-day termination notice on Aug. 26.&lt;/p&gt;
    &lt;p&gt;This decision came after Illinois Secretary of State Alexi Giannoulias discovered that Flock had allowed U.S. Customs and Border Protection to access Illinois cameras in a “pilot program” against state law, and after the RoundTable reported in June that out-of-state law enforcement agencies were able to search Flock’s data for assistance in immigration cases.&lt;/p&gt;
    &lt;p&gt;Flock had removed 15 of the 18 stationary cameras by Sept. 8, only to reinstall each one at or near its prior location by Tuesday. City spokesperson Cynthia Vargas said in a written statement that the city has not deviated from or made any changes to its policies “since the earlier contract termination, meaning Flock reinstalled the cameras without the city’s permission.”&lt;/p&gt;
    &lt;p&gt;“Recently, we became aware that Flock has reinstalled the physical cameras that they had previously taken down,” Vargas wrote. “We immediately issued a cease-and-desist order to Flock. Earlier this afternoon, Flock committed to promptly removing the cameras.”&lt;/p&gt;
    &lt;p&gt;Flock did not immediately respond to a request for comment from the RoundTable on Tuesday night.&lt;/p&gt;
    &lt;p&gt;The city first installed Flock cameras in late 2022 and early 2023 as part of two separate one-year contracts, and City Council later approved a single five-year contract extension in January 2024.&lt;/p&gt;
    &lt;p&gt;The city has paid the first two years of that extension but would still owe $145,500 for the final three years if the contract is upheld. The city intends to terminate the contract on Sept. 26 under its notice to Flock, but the company is challenging that termination, and the dispute could escalate to litigation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Same spots, with some different models&lt;/head&gt;
    &lt;p&gt;The RoundTable mapped and photographed each of the 18 stationary cameras in June, and site visits on Sept. 8 confirmed that all but three had been removed by Flock. The last three, which appear to have never been removed, are the north-facing cameras at Howard Street’s intersections with Chicago, Ridge and Dodge avenues.&lt;/p&gt;
    &lt;p&gt;Further site visits Tuesday confirmed that the 15 removed cameras had been replaced at the same locations. Most of them were banded back onto public streetlight fixtures where they were placed before, while five located on east-west streets along McCormick Boulevard had individual poles reinstalled into the ground. Near three of these pole mounts were freshly spray painted lines, the word “FLOCK” and numbers appearing to designate the cameras individually.&lt;/p&gt;
    &lt;p&gt;A Reddit user posted a photo to the r/Evanston subreddit on Monday evening showing a worker installing one of these pole mounts and its camera earlier that morning at the corner of McCormick and Main Street.&lt;/p&gt;
    &lt;p&gt;The worker is seen on a ladder holding the camera’s solar panel in front of the pole mount, and behind them is an Enterprise-branded rental van parked on the sidewalk in front of the sign for the Skokie Northshore Channel Park. Although this camera and the one at McCormick and Oakton Street are installed outside of Evanston’s city limits, they both fall under Evanston’s contract with Flock, rather than Skokie’s.&lt;/p&gt;
    &lt;p&gt;Click on the images in the gallery above to see them full screen.&lt;/p&gt;
    &lt;p&gt;Additionally, not all of the reinstalled cameras were “Falcon” models — the long, oval-shaped camera with a solar panel and battery packs that was previously used in every location.&lt;/p&gt;
    &lt;p&gt;At five locations, there was instead a stubbier camera that looks similar to the “Standard” model currently advertised on Flock’s website, except with an extra attachment under the main body. These five also appear to lack solar panels, instead attaching to several previously unseen boxes, and at least one camera is attached to a wire connected to the city-owned light post it’s mounted to, suggesting it may draw power from the city’s grid.&lt;/p&gt;
    &lt;p&gt;Click on the images in the gallery above to see them full screen.&lt;/p&gt;
    &lt;head rend="h4"&gt;Analysis: Flock’s data suggests cams could be active&lt;/head&gt;
    &lt;p&gt;Even before any cameras were initially removed, none of them were supposed to be collecting any data. The city wrote in its Aug. 26 announcement that the 19 cameras were “no longer collecting or providing license plate reader data to the Flock network,” and EPD Cmdr. Scott Sophier reconfirmed this to the RoundTable on Sept. 8.&lt;/p&gt;
    &lt;p&gt;“The last read on an Evanston Flock camera was logged shortly before 1:00 p.m. on August 26th, which is consistent with the City’s request for de-activation,” Sophier said at the time.&lt;/p&gt;
    &lt;p&gt;However, Flock’s own publicly available data suggests that may not be the case.&lt;/p&gt;
    &lt;p&gt;The company maintains a “transparency portal” webpage for Evanston that updates daily with basic data on the cameras’ operations, including “Number of LPR [license plate readers] and other cameras” and “Vehicles detected in the last 30 days.” The RoundTable has tracked this page since shortly after the city’s shutdown order, logging the data and archiving updates on most days.&lt;/p&gt;
    &lt;p&gt;The “Number of LPR and other cameras” figure was at 19 when the shutdown was ordered, matching Evanston’s 19 cameras, but it later dropped to 10 on Aug. 30. Rather than falling to zero, however, the figure stayed at 10 until Sept. 16, when it increased to 12, eventually returning to 19 on Sept. 23, matching the reinstallation of all the cameras.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the “Vehicles detected in the last 30 days” number has steadily decreased since the shutdown order, with each passing update rolling off another day when the cameras were known to be active. However, the figure has not decreased enough over time to actually reach zero once 30 days have passed.&lt;/p&gt;
    &lt;p&gt;When the RoundTable began tracking this figure on Aug. 28, it stood at 439,542 vehicles detected over approximately 28 days of active cameras. To reach zero by 30 days post-shutdown, the figure would need to drop by an average of around 15,700 each day, because every new day added to the data should have included zero new vehicles detected.&lt;/p&gt;
    &lt;p&gt;Based on the city’s Aug. 26 termination notice, there should only be two full days’ worth of vehicle detections left on Flock’s data portal as of late Tuesday, Sept. 23. But the page still reports 155,507 vehicles detected in the last 30 days, yielding a reduction of 284,035 vehicles over 26 days, or around 10,924 per day — well below the reduction rate needed to reach zero.&lt;/p&gt;
    &lt;p&gt;This trend means that on Friday, Sept. 26, when more than 30 days will have passed since the city’s cameras were supposed to be shut down, Flock will still report some number of vehicles as being detected in the prior 30 days. That suggests some number of cameras may have remained active and logging vehicles after Aug. 26, in violation of the city’s order and without the city’s knowledge, as indicated by Sophier’s response to the RoundTable on Sept. 8.&lt;/p&gt;
    &lt;p&gt;“Flock has not indicated to the City in direct communications that any ALPR’s are active or have been re-activated,” Sophier wrote. “There is no indication that Flock did not honor/fulfill the City’s request and also no indication on the City’s end to show any plate reads since the aforementioned date/time.”&lt;/p&gt;
    &lt;p&gt;Flock did not answer questions about this data sent by the RoundTable on Sept. 8. Site visits by the RoundTable that day confirmed that the 15 aforementioned cameras had been removed by that time.&lt;/p&gt;
    &lt;p&gt;Update: The City of Evanston has covered up the Flock cameras while waiting for their removal.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://evanstonroundtable.com/2025/09/24/flock-safety-reinstalls-evanston-cameras/"/><published>2025-09-26T03:51:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45382645</id><title>A platform-jumping prince – History of Prince of Persia's 1990s Ports</title><updated>2025-09-26T08:43:31.773433+00:00</updated><content>&lt;doc fingerprint="a7c6792f1b92179c"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;A platform-jumping prince&lt;/head&gt;
    &lt;p&gt;"Which is your favorite/definitive version of the original Prince of Persia game?"&lt;/p&gt;
    &lt;p&gt;I get this question surprisingly often, considering it's been 35 years. I figured it deserves a blog post.&lt;/p&gt;
    &lt;head rend="h4"&gt;Apple II&lt;/head&gt;
    &lt;p&gt;The Apple II version was the original. It's the only version I programmed myself; Prince of Persia's gameplay, graphics, animation and music were all created on the Apple II. I spent three years sweating over every byte (from 1986 to 1989), so it's close to my heart in a way no other version can be. That said...&lt;/p&gt;
    &lt;head rend="h4"&gt;DOS/Windows&lt;/head&gt;
    &lt;p&gt;The 1990 PC version, developed in parallel with the Apple II and shipped a few months later, took advantage of the PC's improved graphics and sound capabilities to deliver the Prince of Persia most players remember (in CGA, EGA, or VGA). My dad, Francis Mechner, re-orchestrated his music (previously limited by the Apple II's tinny built-in speaker) for MIDI synthesizers. The Broderbund in-house team, led by programmer Lance Groody, with Leila Joslyn on art, Tom Rettig on sound, and me as director, stayed faithful to the Apple game while upping the quality in every dimension. The digitized spike and slicer sound effects that traumatized many an elementary-school gamer originated with the PC version. If someone asked me the best way to play old-school PoP online today, I'd likely recommend the DOS version.&lt;/p&gt;
    &lt;p&gt;In 1990, C-family programming languages were the future, 6502 machine language the past. For good reasons, nearly all subsequent ports of PoP took the PC version as their starting point, rather than the Apple II.&lt;/p&gt;
    &lt;head rend="h4"&gt;Amiga&lt;/head&gt;
    &lt;p&gt;The Amiga port was developed by Dan Gorlin (of Choplifter fame), in parallel with the PC version, using the graphics and sound assets developed by the Broderbund team.&lt;/p&gt;
    &lt;p&gt;Danny was one of my game-author heroes. Playing Choplifter, as a 17-year-old college freshman in 1982, blew me away and set me on the creative path that would lead to Karateka. I was star-struck that he agreed to port PoP to Amiga. He did an impeccable job, working alone at home, using the state-of-the-art development system he'd built for his games Airheart and Typhoon Thompson.&lt;/p&gt;
    &lt;p&gt;In a detail perhaps mainly interesting to lawyers, Amiga was one of three PoP versions (Apple II and Macintosh were the others) that I was contractually responsible for delivering to Broderbund, rather than their doing the development. This meant me driving to Danny's house for meetings instead of to Broderbund, and that I was on the hook in case the project fell behind schedule or something went wrong. Fortunately, with Danny, all was smooth sailing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Commodore 64&lt;/head&gt;
    &lt;p&gt;One port that didn't get greenlit was the Commodore 64. Like the Apple II, the C64 had its heyday in the mid-1980s. By 1990, Broderbund (and most U.S. retailers) considered the C64 and Apple II outdated platforms; sales numbers were dwindling by the month. Broderbund couldn't escape publishing PoP on Apple, since it was the lead platform I created the game on, but they had little interest in a C64 version. It would have been a tough port in any case. To fit PoP into 64K of memory, with the Commodore's technical limitations, needed an ace 6502 programmer.&lt;/p&gt;
    &lt;p&gt;In a twist I'd never have predicted, an unofficial, fan-made C64 port was finally done in 2011, over 20 years later, and a Commodore Plus/4 port just last year. I hope my Apple II source code was helpful.&lt;/p&gt;
    &lt;head rend="h4"&gt;Macintosh&lt;/head&gt;
    &lt;p&gt;In 1984, Apple unveiled the Macintosh computer (with a now-legendary Super Bowl ad). Still in college, and flush with Karateka royalties, I took advantage of the student discount to purchase a 128K Mac — keeping my Apple IIe for games. (A computer with no lowercase, and enough RAM to hold four pages of text, isn't ideal for writing term papers.) I loved my Mac, and faithfully upgraded my system every time they did: Mac Plus, SE, II, IIci, LC. By 1990, I was proudly Mac-only.&lt;/p&gt;
    &lt;p&gt;But the games market was overwhelmingly PC. Broderbund estimated Mac's games market share as 5% of DOS/Windows. Since I believed in the Mac more than they did, it made sense for me to take on the port, as I'd done with Amiga. I subcontracted it to Presage Software, a group of ex-Broderbund programmers I'd known since Karateka days.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fun fact: the previous occupant of Presage's San Rafael office was George Lucas's Industrial Light &amp;amp; Magic.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Presage had an excellent, seasoned lead Mac programmer in Scott Shumway; but whereas Danny met his Amiga milestones promptly, Scott's Mac milestones receded like the horizon as they approached. With each new Mac model release — black-and-white, then color, then a different-sized screen — Presage had to redo the bit-mapped PoP graphics for the new configuration. While Prince of Persia's Apple, Amiga and PC versions languished on store shelves (the game wasn't a hit in its first two years), the Mac release date slipped from 1990 to 1991, then to 1992.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fun fact #2: the young graphic artist who up-rezzed the Mac sprites, Mike Kennedy, went on to found the comics imprint Magnetic Press. We met again in 2024, when Magnetic published my graphic novel Monte Cristo.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ironically, the Mac delays turned out to be a blessing in disguise. By the time the port was finally finished, almost two years late, Broderbund marketing had noticed that despite PoP's lackluster U.S. sales, its overseas and console versions were doing surprisingly well. Maybe the game had untapped potential?&lt;/p&gt;
    &lt;p&gt;Broderbund took the gamble of combining PoP's Mac release with a PC re-release in a bigger, hourglass-shaped "candy box" designed by the San Francisco firm Wong &amp;amp; Yeo. The dual Mac-PC release in the new box turned the prince's fortunes around. PoP not only became the #1-selling Mac game, it went from ice-cold to hot on PC as well. To 1992 Mac owners who'd been using their machines mainly for work, a game like PoP was a welcome diversion.&lt;/p&gt;
    &lt;p&gt;The Mac port was terrific. A sign of its quality is that we adopted its revamped prince (sporting a vest, turban and shoes) for the sequel, Prince of Persia 2: The Shadow and The Flame.&lt;/p&gt;
    &lt;p&gt;But I still think the original Apple and PC graphics play best. The CRT blur and fat pixels smoothed over animated glitches, enhancing the illusion of life. Higher resolution leaves less to the imagination. (The same can be said of photography and cinema.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Other ports&lt;/head&gt;
    &lt;p&gt;Between 1990 and 1993, more computer and console ports of PoP than I can list — Nintendo NES, Game Boy, SEGA Game Gear, Genesis, Master System, Amstrad CPC, Atari ST, NEC PC-9801, FM Towns, Sam Coupé — were developed by teams in Japan, Europe, and elsewhere. Usually, by the time someone handed me a controller to playtest a build, it was too late for my feedback to matter, so I rarely played beyond the first level or two. I don't remember enough specifics of those versions to compare them; I'll leave that to players who know them better.&lt;/p&gt;
    &lt;p&gt;There is one unforgettable exception.&lt;/p&gt;
    &lt;head rend="h4"&gt;Super Nintendo&lt;/head&gt;
    &lt;p&gt;In March 1992, I moved to Paris for a year (to learn French and 16mm filmmaking). Soon after my arrival, a colleague at Activision invited me to visit their office. They showed me the Super Nintendo version of PoP, developed by Arsys and published by NCS in Japan. Activision was lobbying Broderbund for the rights to publish it in Europe and the U.S. It wasn't my call, but they hoped I'd put in a word.&lt;/p&gt;
    &lt;p&gt;I wrote in my journal that day:&lt;/p&gt;
    &lt;p&gt;"Wow! It was like a brand new game. For the first time I felt what it's really like to play Prince of Persia, when you're not the author and don't already know by rote what's lurking around every corner."&lt;/p&gt;
    &lt;p&gt;Arsys had done more than a straight port; they'd expanded the game from 12 levels to 20, adding new enemies, traps, setpieces, and new music. I didn't play all the way through — a half-hour in Activision's office only scratched the surface — but I'll never forget the delighted thrill of being surprised playing my own game. You can see and play it in your browser here.&lt;/p&gt;
    &lt;p&gt;Elaborate production values and doubled playtime helped make SNES PoP a huge hit. I especially loved the fantastic box artwork by Katsuya Terada.&lt;/p&gt;
    &lt;p&gt;A recent feature article in Time Extension revealed behind-the-scenes details about the SNES development that I hadn't known — including that game producer Keiichi Onogi traveled to the U.S. to visit Broderbund in 1991, hoping to get my feedback. (I missed his visit.) The article is a fascinating time capsule and testament to how special that port was.&lt;/p&gt;
    &lt;head rend="h4"&gt;...And onwards&lt;/head&gt;
    &lt;p&gt;The SNES, so different from the original Apple/DOS version, gave me my first taste of a feeling I would grow used to in decades to come: playing and enjoying new Prince of Persia games that were made by others. With the exception of The Sands of Time (2003), where I was part of a Ubisoft Montreal team, the more recent modern PoP games don't have my fingerprints on them.&lt;/p&gt;
    &lt;p&gt;I suspect that for many reading this post, your answer to "Which is your favorite PoP?" will be the same as mine: Whichever version we played, for hours on end, at a formative age when playing and finishing a game mattered intensely. The real value is in the ingenuity and imagination you brought to the effort, and in your own memories tied to that time.&lt;/p&gt;
    &lt;p&gt;Thanks for reading this post. If you'd like a deeper dive into the story behind Prince of Persia's creation, I've published two books on the subject: my old journals (1985-1993), and my new graphic novel Replay. You can check them out here. Archival materials about PoP (including the Apple II source code) can be found in this website's Library.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jordanmechner.com/en/latest-news/#a-platform-jumping-prince"/><published>2025-09-26T04:29:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45382755</id><title>No reachable chess position with more than 218 moves</title><updated>2025-09-26T08:43:31.369031+00:00</updated><content>&lt;doc fingerprint="13011afe410486f7"&gt;
  &lt;main&gt;&lt;p&gt;Created by the author using GIMP and freely available images.&lt;/p&gt;&lt;head rend="h1"&gt;There is no reachable chess position with more than 218 moves.&lt;/head&gt;Stop searching, we had it right for 60 years.&lt;p&gt;Ever since Nenad Petrović, grandmaster of chess composition, published his 218 move composition in 1964, people have tried to come up with a better one. Last month, I joined the hunt and, being a computer scientist, I decided to settle this question once and for all, using computers. You can give it a try yourself. Try to find a position with more moves than the one below.&lt;/p&gt;&lt;p&gt;Spoiler: You won't.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Reachable chess position with 218 moves for White, published by Petrović in 1964.&lt;/p&gt;&lt;p&gt;...but how can we know for sure?&lt;/p&gt;&lt;p&gt;By checking all approximately 8.7x10^45 reachable chess positions?&lt;lb/&gt;Yeah that's not gonna happen...&lt;/p&gt;&lt;p&gt;That's 8.7 billion billion billion billion billion and it's enough to scare even the mightiest of supercomputers. In fact, cracking AES-128 encryption would be easier.&lt;/p&gt;&lt;p&gt;Fortunately, we can use the power of Math!&lt;/p&gt;&lt;p&gt;Follow me along and perhaps you can compute yourself a world record afterwards :)&lt;/p&gt;&lt;head rend="h2"&gt;Using the power of Math&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;Red is the official color of Math, at least in my elementary school.&lt;/p&gt;&lt;p&gt;We're gonna tackle this problem from the white side, i.e., it is White to move. Except for rare exceptions regarding the reachability of positions, this is equivalent.&lt;/p&gt;&lt;p&gt;Since proving that a position is reachable is complicated, we're gonna search through all ways of placing pieces on the board and filter out the non-reachable ones later on, if needed.&lt;/p&gt;&lt;p&gt;Obviously, 99.9% of the positions suck at having lots of moves, they are not even close to 218. We just need a way to skip them and pray that there exists enough electricity to check the rest.&lt;/p&gt;&lt;p&gt;Let's begin with a couple of useful observations.&lt;/p&gt;&lt;head rend="h4"&gt;Useless pieces&lt;/head&gt;&lt;p&gt;A black piece does not improve the number of moves, most of the time. Its existence only benefits the number of moves if it increases White's number of moves, i.e., at least one of the following is true:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It can be taken by a white pawn, giving said pawn more moves&lt;/item&gt;&lt;item&gt;It protects the black king from check, making an otherwise illegal position with lots of moves legal&lt;/item&gt;&lt;item&gt;It frees a white piece from being pinned to the white king, thus giving it more moves&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Otherwise, it is useless, at best, and thus can be removed.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;I inserted this picture for the sole purpose of making this article look less text-heavy.&lt;/p&gt;&lt;head rend="h4"&gt;Too powerful pieces&lt;/head&gt;&lt;p&gt;Next, we observe that, if piece counts permit, we can always replace a black piece, with the exception of the black king, with a strictly less powerful one. That is, a piece that has a subset of the moves of the original piece. For example, a queen with a pawn/bishop/rook or a bishop with a pawn. Except if a pawn is on the seventh rank, since in that case, each of its moves to the promotion square counts as 4 separata moves. The only way for Black's moves to affect White's number of moves is by pinning White's pieces or preventing the white king from stepping on a certain square. Both of these things can't get worse if the black piece has less moves than before.&lt;/p&gt;&lt;head rend="h4"&gt;Too weak pieces?&lt;/head&gt;&lt;p&gt;The other way around, it is not so easy, however. You'd think that you can just replace white rooks and white bishops with white queens if counts permit, but the problem is this: How do you ensure that you cannot capture the black king, making the position illegal? Maybe the optimal solution has a rook instead of a queen to avoid just that?&lt;/p&gt;&lt;p&gt;Well, you might say "Let's just place a black piece in between then".&lt;/p&gt;&lt;p&gt;And you would be wrong unless you can tell me why you haven't just blocked some other white piece and thus reduced the number of moves in total. Or what your plan is, if there is no space to put something in between. You just made the position illegal, duh!&lt;/p&gt;&lt;head rend="h4"&gt;No checks, thank you&lt;/head&gt;&lt;p&gt;Finally, we can get rid of checks. If the black king is in check, it means that the position is illegal, since it is White's turn to move and they could just capture the black king. Not okay! So that can't be it. On the other hand, if the white king is in check, then the number of White's moves is severely restricted and we can easily prove that we cannot reach 218 moves. There are three ways to get out of check:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Move the king&lt;/item&gt;&lt;item&gt;Capture the attacker&lt;/item&gt;&lt;item&gt;Block the attack&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Moving the king gives 8 moves at best. Since any square can be reached by at most 16 pieces at the same time (8 knights and 8 other pieces straight or diagonally), capturing gives 16 moves at best. We can have at most 6 squares to block an attack, so that's an additional 6 x 16 = 96 moves. So at best 8+16+96 = 120 moves, far less than 218, and thus we do not need to consider positions in which either king is in check.&lt;/p&gt;&lt;p&gt;So is this it Tobi? Can we now call NASA and ask for their supercomputer?&lt;/p&gt;&lt;p&gt;Nope, it's still absolutely hopeless, there are waaaaay too many positions left.&lt;lb/&gt;We have to skip even more positions.&lt;/p&gt;&lt;head rend="h2"&gt;Introducing Chess with Cheating: Partial Pieces and Moves&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;Reminder: Replace rook pictures by something more interesting.&lt;/p&gt;&lt;p&gt;While searching through all possible piece configurations, we would ideally like to have some provably correct way of telling whether we can still reach 218 moves, so we can stop trying and save ourselves an astronomically large amount of work. The better the method is, the more work we save. But it also needs to be fast, since we need to run it millions and billions of times.&lt;/p&gt;&lt;p&gt;A common technique in optimization is to allow fractional decisions. Instead of a piece being either on e4 or not, it can be 27.3% there and 72.7% not there. This enables us to just "swim" through the solution space towards the optimal solution instead of trying all combinations. The drawback is that we usually end up with a way too good solution with most decisions being fractional. But if that doesn't get us beyond 218 moves, we know we can stop trying.&lt;/p&gt;&lt;p&gt;Obviously, these kinds of algorithms are already implemented in state-of-the-art solvers like Gurobi, so all we need to do is model our problem to its likings (as a so-called integer programming problem), tell it to maximize the number of moves and pray. Finally, after ~55 000 seconds, it crashed.&lt;lb/&gt;Not enough memory, but also hopelessly far away from completing the proof. Extrapolating the runtime led to an estimated runtime of ~6 years. Yeah, let's not go there. Going back to making more observations:&lt;/p&gt;&lt;head rend="h4"&gt;Checking more positions to make things less slow&lt;/head&gt;&lt;p&gt;To reduce the size of the model, I bent some chess rules, simplifying the search space:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;I allowed castling if king and rook were on the right squares, not requiring anything else&lt;/item&gt;&lt;item&gt;I stopped caring about pieces moving despite being pinned&lt;/item&gt;&lt;item&gt;I stopped caring whether the white king is in check or walks into check&lt;/item&gt;&lt;item&gt;I allowed white pawns to always capture when standing on the fifth rank so I'd not have to check en passant.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;All of these things are unlikely to happen in 218+ move positions and after a solution has been found, I can still check and discard it if it has less moves than it claims to have.&lt;/p&gt;&lt;p&gt;I started again and this time, memory wasn't a problem, but the progress was awfully slow with ~29 days remaining. The world could not wait this long and neither could I. I pressed cancel.&lt;/p&gt;&lt;head rend="h4"&gt;Preventing white magic&lt;/head&gt;&lt;p&gt;&lt;lb/&gt;The optimal fractional solution for the empty board. Most pieces are spread out over multiple squares and have fractional numbers of moves available that sum to ~305. This does prove that the real solution can't have more than 305 moves. Still quite a bit from proving 218 to be the optimum.&lt;/p&gt;&lt;p&gt;Our current way of cheating is too different from reality, causing the solver to have to search through way too many board configurations. In our case, the optimal solution to the easy problem flooded the center with half queens and half knights sharing the same squares, having half pieces move through other half pieces with half moves. Gurobi thinks that the above position has 305 moves in total, which is far away from 218 and a bad upper bound.&lt;/p&gt;&lt;p&gt;In order to cut off this crazy solution, I added a "redundant" constraint, saying that at most one piece in total can move from one direction onto a particular square. Which got us a new hallucination...&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;The second try. This one has 271 2/3 moves which proves that there is no solution with more than 271 moves.&lt;/p&gt;&lt;p&gt;Now we have 271 2/3 moves, which is much better. It's a bit like having to try all passwords with 53 characters vs. all passwords with 87 characters, the latter being many magnitudes harder.&lt;/p&gt;&lt;p&gt;Wait a minute, White can't have 4 kings.&lt;/p&gt;&lt;p&gt;White does not have four kings. White has 3 times 24.6% kings and one 26.2% king. Also, the queen on g3 barely exists, it's 0.8% but apparently it somehow contributes to the total sum of moves :)&lt;/p&gt;&lt;p&gt;With this improved model, I tried again and after ~23 000 seconds, Gurobi solved it to optimality!&lt;/p&gt;&lt;head rend="h2"&gt;Results&lt;/head&gt;&lt;p&gt;Sadly, instead of finding me a fancy position with 219 moves and making my name immortal, Gurobi gave me the following 12 representative positions (of 40,000 in total) with 218 moves each:&lt;/p&gt;&lt;p&gt;All 12 positions seem trivially reachable. I only constructed a proof game for one of the positions, since that is sufficient for proving our claim. If you don't believe that this is possible, keep in mind that White's last or second last move might have been a capture. Or click on the link :)&lt;/p&gt;&lt;p&gt;Sadly not a world record, but at least we now know for certain that 218 is the limit.&lt;/p&gt;&lt;p&gt;And you smart chess move 3.7 bit compression people and chess engine developers, you can finally stop worrying. 256 moves will be enough. You're welcome :-)&lt;/p&gt;&lt;p&gt;Except if you allow non-reachable positions, in which case you might want to read on :P&lt;/p&gt;&lt;head rend="h2"&gt;Other stuff solved along the way&lt;/head&gt;&lt;p&gt;I also confirmed the optimality of the 144 move record without promotions. Since Gurobi did not find any position with more than 144 moves, that means that there also is no reachable position with more than 144 moves. Hence, 144 moves is the best we can do and "Jenő Bán", a chess composer from Hungary, found one in 1960 already:&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;144 moves for White, no promotions. Created by Hungarian chess composer "Jenő Bán". Here is a proof game, demonstrating that the position is reachable.&lt;/p&gt;&lt;p&gt;Also, I confirmed the optimality of the following illegal position, which has 288 moves for White.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Illegal position with 288 moves for White. Corner queens can be replaced with bishops.&lt;/p&gt;&lt;p&gt;Now have a guess at what the best non-reachable legal position looks like ;)&lt;lb/&gt;Yep, you're right, cramming the kings into the corners for 271 moves.&lt;/p&gt;&lt;p&gt;&lt;lb/&gt;Legal but non-reachable position with 271 moves for White. Corner queens can be replaced with bishops.&lt;/p&gt;&lt;head rend="h2"&gt;Future plans&lt;/head&gt;&lt;p&gt;My code snippet is freely available at Github. If you manage to do something cool based on it, please let the world know :)&lt;/p&gt;&lt;p&gt;Fun problems enthusiasts could try to tackle next using similar techniques:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Most captures&lt;/item&gt;&lt;item&gt;Most stalemates&lt;/item&gt;&lt;item&gt;Most checks&lt;/item&gt;&lt;item&gt;Most checkmates&lt;/item&gt;&lt;item&gt;Most mates in two&lt;/item&gt;&lt;item&gt;...&lt;/item&gt;&lt;item&gt;Most ... under &amp;lt;condition&amp;gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Some of these might be hard, extremely hard or practically impossible to solve with current technology. I don't think that integer linear programming is a suitable approach for all of them, one likely has to develop a custom algorithm for computing good upper bounds, based on creative mathematical insights.&lt;/p&gt;&lt;p&gt;Good luck to those who dare to try solving one of these ^^&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lichess.org/@/Tobs40/blog/there-is-no-reachable-chess-position-with-more-than-218-moves/a5xdxeqs"/><published>2025-09-26T04:47:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45383483</id><title>Safe in the sandbox: security hardening for Cloudflare Workers</title><updated>2025-09-26T08:43:30.990800+00:00</updated><content>&lt;doc fingerprint="96bbd892fc23a2cd"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;As a serverless cloud provider, we run your code on our globally distributed infrastructure. Being able to run customer code on our network means that anyone can take advantage of our global presence and low latency. Workers isnât just efficient though, we also make it simple for our users. In short: You write code. We handle the rest.&lt;/p&gt;
      &lt;p&gt;Part of 'handling the rest' is making Workers as secure as possible. We have previously written about our security architecture. Making Workers secure is an interesting problem because the whole point of Workers is that we are running third party code on our hardware. This is one of the hardest security problems there is: any attacker has the full power available of a programming language running on the victim's system when they are crafting their attacks.&lt;/p&gt;
      &lt;p&gt;This is why we are constantly updating and improving the Workers Runtime to take advantage of the latest improvements in both hardware and software. This post shares some of the latest work we have been doing to keep Workers secure.&lt;/p&gt;
      &lt;p&gt;Some background first: Workers is built around the V8 JavaScript runtime, originally developed for Chromium-based browsers like Chrome. This gives us a head start, because V8 was forged in an adversarial environment, where it has always been under intense attack and scrutiny. Like Workers, Chromium is built to run adversarial code safely. That's why V8 is constantly being tested against the best fuzzers and sanitizers, and over the years, it has been hardened with new technologies like Oilpan/cppgc and improved static analysis.&lt;/p&gt;
      &lt;p&gt;We use V8 in a slightly different way, though, so we will be describing in this post how we have been making some changes to V8 to improve security in our use case.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Hardware-assisted security improvements from Memory Protection Keys&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Modern CPUs from Intel, AMD, and ARM have support for memory protection keys, sometimes called PKU, Protection Keys for Userspace. This is a great security feature which increases the power of virtual memory and memory protection.&lt;/p&gt;
      &lt;p&gt;Traditionally, the memory protection features of the CPU in your PC or phone were mainly used to protect the kernel and to protect different processes from each other. Within each process, all threads had access to the same memory. Memory protection keys allow us to prevent specific threads from accessing memory regions they shouldn't have access to.&lt;/p&gt;
      &lt;p&gt;V8 already uses memory protection keys for the JIT compilers. The JIT compilers for a language like JavaScript generate optimized, specialized versions of your code as it runs. Typically, the compiler is running on its own thread, and needs to be able to write data to the code area in order to install its optimized code. However, the compiler thread doesn't need to be able to run this code. The regular execution thread, on the other hand, needs to be able to run, but not modify, the optimized code. Memory protection keys offer a way to give each thread the permissions it needs, but no more. And the V8 team in the Chromium project certainly aren't standing still. They describe some of their future plans for memory protection keys here.&lt;/p&gt;
      &lt;p&gt;In Workers, we have some different requirements than Chromium. The security architecture for Workers uses V8 isolates to separate different scripts that are running on our servers. (In addition, we have extra mitigations to harden the system against Spectre attacks). If V8 is working as intended, this should be enough, but we believe in defense in depth: multiple, overlapping layers of security controls.&lt;/p&gt;
      &lt;p&gt;That's why we have deployed internal modifications to V8 to use memory protection keys to isolate the isolates from each other. There are up to 15 different keys available on a modern x64 CPU and a few are used for other purposes in V8, so we have about 12 to work with. We give each isolate a random key which is used to protect its V8 heap data, the memory area containing the JavaScript objects a script creates as it runs. This means security bugs that might previously have allowed an attacker to read data from a different isolate would now hit a hardware trap in 92% of cases. (Assuming 12 keys, 92% is about 11/12.)&lt;/p&gt;
      &lt;p&gt;The illustration shows an attacker attempting to read from a different isolate. Most of the time this is detected by the mismatched memory protection key, which kills their script and notifies us, so we can investigate and remediate. The red arrow represents the case where the attacker got lucky by hitting an isolate with the same memory protection key, represented by the isolates having the same colors.&lt;/p&gt;
      &lt;p&gt;However, we can further improve on a 92% protection rate. In the last part of this blog post we'll explain how we can lift that to 100% for a particular common scenario. But first, let's look at a software hardening feature in V8 that we are taking advantage of.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The V8 sandbox, a software-based security boundary&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Over the past few years, V8 has been gaining another defense in depth feature: the V8 sandbox. (Not to be confused with the layer 2 sandbox which Workers have been using since the beginning.) The V8 sandbox has been a multi-year project that has been gaining maturity for a while. The sandbox project stems from the observation that many V8 security vulnerabilities start by corrupting objects in the V8 heap memory. Attackers then leverage this corruption to reach other parts of the process, giving them the opportunity to escalate and gain more access to the victim's browser, or even the entire system.&lt;/p&gt;
      &lt;p&gt;V8's sandbox project is an ambitious software security mitigation that aims to thwart that escalation: to make it impossible for the attacker to progress from a corruption on the V8 heap to a compromise of the rest of the process. This means, among other things, removing all pointers from the heap. But first, let's explain in as simple terms as possible, what a memory corruption attack is.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Memory corruption attacks&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;A memory corruption attack tricks a program into misusing its own memory. Computer memory is just a store of integers, where each integer is stored in a location. The locations each have an address, which is also just a number. Programs interpret the data in these locations in different ways, such as text, pixels, or pointers. Pointers are addresses that identify a different memory location, so they act as a sort of arrow that points to some other piece of data.&lt;/p&gt;
      &lt;p&gt;Here's a concrete example, which uses a buffer overflow. This is a form of attack that was historically common and relatively simple to understand: Imagine a program has a small buffer (like a 16-character text field) followed immediately by an 8-byte pointer to some ordinary data. An attacker might send the program a 24-character string, causing a "buffer overflow." Because of a vulnerability in the program, the first 16 characters fill the intended buffer, but the remaining 8 characters spill over and overwrite the adjacent pointer.&lt;/p&gt;
      &lt;p&gt;See below for how such an attack would now be thwarted.&lt;/p&gt;
      &lt;p&gt;Now the pointer has been redirected to point at sensitive data of the attacker's choosing, rather than the normal data it was originally meant to access. When the program tries to use what it believes is its normal pointer, it's actually accessing sensitive data chosen by the attacker.&lt;/p&gt;
      &lt;p&gt;This type of attack works in steps: first create a small confusion (like the buffer overflow), then use that confusion to create bigger problems, eventually gaining access to data or capabilities the attacker shouldn't have.Â The attacker can eventually use the misdirection to either steal information or plant malicious data that the program will treat as legitimate.&lt;/p&gt;
      &lt;p&gt;This was a somewhat abstract description of memory corruption attacks using a buffer overflow, one of the simpler techniques. For some much more detailed and recent examples, see this description from Google, or this breakdown of a V8 vulnerability.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Compressed pointers in V8&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Many attacks are based on corrupting pointers, so ideally we would remove all pointers from the memory of the program.Â Since an object-oriented language's heap is absolutely full of pointers, that would seem, on its face, to be a hopeless task, but it is enabled by an earlier development. Starting in 2020, V8 has offered the option of saving memory by using compressed pointers. This means that, on a 64-bit system, the heap uses only 32 bit offsets, relative to a base address. This limits the total heap to maximally 4 GiB, a limitation that is acceptable for a browser, and also fine for individual scripts running in a V8 isolate on Cloudflare Workers.&lt;/p&gt;
      &lt;p&gt;An artificial object with various fields, showing how the layout differs in a compressed vs. an uncompressed heap. The boxes are 64 bits wide.&lt;/p&gt;
      &lt;p&gt;If the whole of the heap is in a single 4 GiB area then the first 32 bits of all pointers will be the same, and we don't need to store them in every pointer field in every object. In the diagram we can see that the object pointers all start with 0x12345678, which is therefore redundant and doesn't need to be stored. This means that object pointer fields and integer fields can be reduced from 64 to 32 bits.&lt;/p&gt;
      &lt;p&gt;We still need 64 bit fields for some fields like double precision floats and for the sandbox offsets of buffers, which are typically used by the script for input and output data. See below for details.&lt;/p&gt;
      &lt;p&gt;Integers in an uncompressed heap are stored in the high 32 bits of a 64 bit field. In the compressed heap, the top 31 bits of a 32 bit field are used. In both cases the lowest bit is set to 0 to indicate integers (as opposed to pointers or offsets).&lt;/p&gt;
      &lt;p&gt;Conceptually, we have two methods for compressing and decompressing, using a base address that is divisible by 4 GiB:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Decompress a 32 bit offset to a 64 bit pointer by adding a base address.
void* Decompress(uint32_t offset) { return base + offset; }
// Compress a 64 bit pointer to a 32 bit offset by discarding the high bits.
uint32_t Compress(void* pointer) { return (intptr_t)pointer &amp;amp; 0xffffffff; }&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This pointer compression feature, originally primarily designed to save memory, can be used as the basis of a sandbox.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;From compressed pointers to the sandbox&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;The biggest 32-bit unsigned integer is about 4 billion, so the &lt;code&gt;Decompress()&lt;/code&gt; function cannot generate any pointer that is outside the range [base, base + 4Â GiB]. You could say the pointers are trapped in this area, so it is sometimes called the pointer cage. V8 can reserve 4 GiB of virtual address space for the pointer cage so that only V8 objects appear in this range. By eliminating all pointers from this range, and following some other strict rules, V8 can contain any memory corruption by an attacker to this cage. Even if an attacker corrupts a 32 bit offset within the cage, it is still only a 32 bit offset and can only be used to create new pointers that are still trapped within the pointer cage.&lt;/p&gt;
      &lt;p&gt;The buffer overflow attack from earlier no longer works because only the attacker's own data is available in the pointer cage.&lt;/p&gt;
      &lt;p&gt;To construct the sandbox, we take the 4 GiB pointer cage and add another 4 GiB for buffers and other data structures to make the 8 GiB sandbox. This is why the buffer offsets above are 33 bits, so they can reach buffers in the second half of the sandbox (40 bits in Chromium with larger sandboxes). V8 stores these buffer offsets in the high 33 bits and shifts down by 31 bits before use, in case an attacker corrupted the low bits.&lt;/p&gt;
      &lt;p&gt;Cloudflare Workers have made use of compressed pointers in V8 for a while, but for us to get the full power of the sandbox we had to make some changes. Until recently, all isolates in a process had to be one single sandbox if you were using the sandboxed configuration of V8. This would have limited the total size of all V8 heaps to be less than 4 GiB, far too little for our architecture, which relies on serving 1000s of scripts at once.&lt;/p&gt;
      &lt;p&gt;That's why we commissioned Igalia to add isolate groups to V8. Each isolate group has its own sandbox and can have 1 or more isolates within it. Building on this change we have been able to start using the sandbox, eliminating a whole class of potential security issues in one stroke. Although we can place multiple isolates in the same sandbox, we are currently only putting a single isolate in each sandbox.&lt;/p&gt;
      &lt;p&gt;The layout of the sandbox. In the sandbox there can be more than one isolate, but all their heap pages must be in the pointer cage: the first 4 GiB of the sandbox. Instead of pointers between the objects, we use 32 bit offsets. The offsets for the buffers are 33 bits, so they can reach the whole sandbox, but not outside it.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Virtual memory isn't infinite, there's a lot going on in a Linux process&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;At this point, we were not quite done, though. Each sandbox reserves 8 GiB of space in the virtual memory map of the process, and it must be 4 GiB aligned for efficiency. It uses much less physical memory, but the sandbox mechanism requires this much virtual space for its security properties. This presents us with a problem, since a Linux process 'only' has 128 TiB of virtual address space in a 4-level page table (another 128 TiB are reserved for the kernel, not available to user space).&lt;/p&gt;
      &lt;p&gt;At Cloudflare, we want to run Workers as efficiently as possible to keep costs and prices down, and to offer a generous free tier. That means that on each machine we have so many isolates running (one per sandbox) that it becomes hard to place them all in a 128 TiB space.&lt;/p&gt;
      &lt;p&gt;Knowing this, we have to place the sandboxes carefully in memory. Unfortunately, the Linux syscall, mmap, does not allow us to specify the alignment of an allocation unless you can guess a free location to request. To get an 8 GiB area that is 4 GiB aligned, we have to ask for 12 GiB, then find the aligned 8 GiB area that must exist within that, and return the unused (hatched) edges to the OS:&lt;/p&gt;
      &lt;p&gt;If we allow the Linux kernel to place sandboxes randomly, we end up with a layout like this with gaps. Especially after running for a while, there can be both 8 GiB and 4 GiB gaps between sandboxes:&lt;/p&gt;
      &lt;p&gt;Sadly, because of our 12 GiB alignment trick, we can't even make use of the 8 GiB gaps. If we ask the OS for 12 GiB, it will never give us a gap like the 8 GiB gap between the green and blue sandboxes above. In addition, there are a host of other things going on in the virtual address space of a Linux process: the malloc implementation may want to grab pages at particular addresses, the executable and libraries are mapped at a random location by ASLR, and V8 has allocations outside the sandbox.&lt;/p&gt;
      &lt;p&gt;The latest generation of x64 CPUs supports a much bigger address space, which solves both problems, and Linux kernels are able to make use of the extra bits with five level page tables. A process has to opt into this, which is done by a single mmap call suggesting an address outside the 47 bit area. The reason this needs an opt-in is that some programs can't cope with such high addresses. Curiously, V8 is one of them.&lt;/p&gt;
      &lt;p&gt;This isn't hard to fix in V8, but not all of our fleet has been upgraded yet to have the necessary hardware. So for now, we need a solution that works with the existing hardware. We have modified V8 to be able to grab huge memory areas and then use mprotect syscalls to create tightly packed 8 GiB spaces for sandboxes, bypassing the inflexible mmap API.&lt;/p&gt;
      &lt;p&gt;Taking control of the sandbox placement like this actually gives us a security benefit, but first we need to describe a particular threat model.&lt;/p&gt;
      &lt;p&gt;We assume for the purposes of this threat model that an attacker has an arbitrary way to corrupt data within the sandbox. This is historically the first step in many V8 exploits. So much so that there is a special tier in Google's V8 bug bounty program where you may assume you have this ability to corrupt memory, and they will pay out if you can leverage that to a more serious exploit.&lt;/p&gt;
      &lt;p&gt;However, we assume that the attacker does not have the ability to execute arbitrary machine code. If they did, they could disable memory protection keys. Having access to the in-sandbox memory only gives the attacker access to their own data. So the attacker must attempt to escalate, by corrupting data inside the sandbox to access data outside the sandbox.&lt;/p&gt;
      &lt;p&gt;You will recall that the compressed, sandboxed V8 heap only contains 32 bit offsets. Therefore, no corruption there can reach outside the pointer cage. But there are also arrays in the sandbox â vectors of data with a given size that can be accessed with an index. In our threat model, the attacker can modify the sizes recorded for those arrays and the indexes used to access elements in the arrays. That means an attacker could potentially turn an array in the sandbox into a tool for accessing memory incorrectly. For this reason, the V8 sandbox normally has guard regions around it: These are 32 GiB virtual address ranges that have no virtual-to-physical address mappings. This helps guard against the worst case scenario: Indexing an array where the elements are 8 bytes in size (e.g. an array of double precision floats) using a maximal 32 bit index. Such an access could reach a distance of up to 32 GiB outside the sandbox: 8 times the maximal 32 bit index of four billion.&lt;/p&gt;
      &lt;p&gt;We want such accesses to trigger an alarm, rather than letting an attacker access nearby memory.Â This happens automatically with guard regions, but we don't have space for conventional 32 GiB guard regions around every sandbox.&lt;/p&gt;
      &lt;p&gt;Instead of using conventional guard regions, we can make use of memory protection keys. By carefully controlling which isolate group uses which key, we can ensure that no sandbox within 32 GiB has the same protection key. Essentially, the sandboxes are acting as each other's guard regions, protected by memory protection keys. Now we only need a wasted 32 GiB guard region at the start and end of the huge packed sandbox areas. &lt;/p&gt;
      &lt;p&gt;With the new sandbox layout, we use strictly rotating memory protection keys. Because we are not using randomly chosen memory protection keys, for this threat model the 92% problem described above disappears. Any in-sandbox security issue is unable to reach a sandbox with the same memory protection key. In the diagram, we show that there is no memory within 32 GiB of a given sandbox that has the same memory protection key. Any attempt to access memory within 32 GiB of a sandbox will trigger an alarm, just like it would with unmapped guard regions.&lt;/p&gt;
      &lt;p&gt;In a way, this whole blog post is about things our customers don't need to do. They don't need to upgrade their server software to get the latest patches, we do that for them. They don't need to worry whether they are using the most secure or efficient configuration. So there's no call to action here, except perhaps to sleep easy.&lt;/p&gt;
      &lt;p&gt;However, if you find work like this interesting, and especially if you have experience with the implementation of V8 or similar language runtimes, then you should consider coming to work for us. We are recruiting both in the US and in Europe. It's a great place to work, and Cloudflare is going from strength to strength.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/safe-in-the-sandbox-security-hardening-for-cloudflare-workers/"/><published>2025-09-26T06:45:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45383612</id><title>Kapa.ai (YC S23) Is Hiring a Customer Solutions Engineer (EU Remote)</title><updated>2025-09-26T08:43:30.018617+00:00</updated><content>&lt;doc fingerprint="3d459b337e3b4fd0"&gt;
  &lt;main&gt;
    &lt;p&gt;The fastest way to build AI assistants on technical content&lt;/p&gt;
    &lt;p&gt;Kapa makes technical knowledge instantly accessible through AI assistants. As a customer solutions engineer you will work work closely with our 200+ customers to help them deploy and manage both customer-facing and employee-facing AI assistants. Check out Docker’s documentation (https://docs.docker.com) for a live example of what Kapa is (look for the “Ask AI” button).&lt;/p&gt;
    &lt;p&gt;In this role, you will:&lt;/p&gt;
    &lt;p&gt;You may be a good fit if you have*:&lt;/p&gt;
    &lt;p&gt;* This is neither an exhaustive nor necessary set of attributes. Even if none of these apply to you, but you believe you will contribute to kapa.ai, please reach out.&lt;/p&gt;
    &lt;p&gt;We make it easy for technical companies to build AI assistants. Companies like Docker, Grafana and Mixpanel deploy kapa in the following ways:&lt;/p&gt;
    &lt;p&gt;We leverage companies existing technical knowledge sources including documentation, tutorials, forum posts, Slack channels, GitHub issues and many more to generate AI assistants that can handle complicated technical questions. More than 200 companies use kapa and we have answered more than 10 million questions to date.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/kapa-ai/jobs/mHIFJVz-support-engineer"/><published>2025-09-26T07:01:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45383637</id><title>Translating a Fortran F-16 Simulator to Unity3D</title><updated>2025-09-26T08:43:29.726922+00:00</updated><content>&lt;doc fingerprint="3633da10e06c133a"&gt;
  &lt;main&gt;&lt;p&gt;I recently purchased the textbook “Aircraft Control and Simulation” by Brian L. Stevens, Frank L. Lewis, and Eric N. Johnson1. This book covers the control and simulation of aircraft. It’s really dense and frankly hard to understand. As far as aerodynamics texts go, it’s pretty typical.&lt;/p&gt;&lt;p&gt;One interesting item in the appendices of the book is the source code for the simulation of an F-16. It has a flight model, based on scale model wind tunnel data. The flight model consists of a dozen lookup tables and the math equations to make it fly.&lt;/p&gt;&lt;p&gt;The only problem: it’s written entirely in Fortran.&lt;/p&gt;&lt;p&gt;The source code is available on Github.&lt;/p&gt;&lt;p&gt;You can play the finished project right now on itch.io&lt;/p&gt;&lt;p&gt;Or watch the demo on Youtube:&lt;/p&gt;&lt;p&gt;While I am a professional software engineer and I have worked in the aerospace industry, that doesn’t mean that I understand what I’m doing.&lt;/p&gt;&lt;p&gt;Table of Contents&lt;/p&gt;&lt;head rend="h1"&gt;Introduction&lt;/head&gt;&lt;p&gt;In previous posts on this blog2 3 4, I covered the development of a flight simulator based on the lift equation and hand-tuned parameters. This gives the game designer direct control over a lot of flight parameters. For example, you can directly choose the turn rate and the G-limit of the aircraft, allowing the designer to easily tune the corner speed. This works well for game development, since the designer, and ultimately the player, care more about these high-level parameters.&lt;/p&gt;&lt;p&gt;But real aircraft are designed from the other direction, starting from low-level parameters such as the size, shape, and position of airfoils. Engineers tune every aspect of the aircraft in order to reach those high-level behaviors. But every design decision has trade offs and reaching the goal for one parameter means compromising another. An airliner is designed very differently from a fighter jet because of this.&lt;/p&gt;&lt;p&gt;Simulating all of the low-level parameters is difficult. It’s possible to simulate air flow over the vehicle using computational fluid dynamics (CFD), but this kind of software is difficult to write and even more difficult to verify.&lt;/p&gt;&lt;p&gt;The F-16 flight model from the textbook does not simulate the low-level parameters, but it also doesn’t simulate the high-level parameters either. It sits somewhere in between, so it serves as a useful stepping stone from my previous projects. This project will explore more advanced flight dynamics and explain the limitations of the old flight model as well as the new one.&lt;/p&gt;&lt;head rend="h1"&gt;Aerospace Conventions&lt;/head&gt;&lt;head rend="h2"&gt;Coordinate System&lt;/head&gt;&lt;p&gt;Before we can write any code, we need to understand the conventions used for mathematically modeling aircraft that are used in the aerospace industry. The textbook uses aerospace conventions and to use them in this project, we must convert them to Unity conventions.&lt;/p&gt;&lt;p&gt;The first convention is the coordinate system axes. If you’ve ever visited a graphics programming forum, you might have seen people arguing over how the X, Y, and Z axes should be arranged in their game. Especially whether to use a right handed or left handed, and Y-up or Z-up coordinate system.&lt;/p&gt;&lt;p&gt;This chart by Freya Holmer5 shows the axis choices made by a variety of 3D software tools.&lt;/p&gt;&lt;p&gt;The aerospace industry takes a different path. The most common coordinate system for aircraft is right handed, X forward, Y right, and Z down. This is completely different from every tool shown above. The textbook defines all of it’s math using this convention.&lt;/p&gt;&lt;p&gt;Luckily, translating between two coordinate systems is easy. You just swap the components around and then add or remove minus signs until it all works. Every calculation made by the textbook’s code can be easily translated into Unity’s coordinate system and vice versa.&lt;/p&gt;&lt;p&gt;Writing functions to do this is simple:&lt;/p&gt;&lt;quote&gt;public static Vector3 ConvertVectorToAerospace(Vector3 vector) { return new Vector3(vector.z, vector.x, -vector.y); } public static Vector3 ConvertVectorToUnity(Vector3 vector) { return new Vector3(vector.y, -vector.z, vector.x); }&lt;/quote&gt;&lt;p&gt;When translating euler angles, torque, or other angular values, one additional negation is needed:&lt;/p&gt;&lt;quote&gt;public static Vector3 ConvertAngleToAerospace(Vector3 angle) { // negate when switching handedness return -ConvertVectorToAerospace(angle); } public static Vector3 ConvertAngleToUnity(Vector3 angle) { // negate when switching handedness return -ConvertVectorToUnity(angle); }&lt;/quote&gt;&lt;head rend="h2"&gt;Units&lt;/head&gt;&lt;p&gt;For completely inscrutable reasons, American aerospace texts (and the industry!) insist on using US customary units for everything. All math is defined with these units. Distance is measured in feet. Mass is measured in slugs.&lt;/p&gt;&lt;p&gt;What the hell is a slug? A slug is the unit of mass in the US system. This is the equivalent unit of the kilogram in the metric system. Remember that weight and mass are not the same thing.&lt;/p&gt;\(1 \, \text{kg} * 9.81 \, \text{m/s}^2 = 9.81 \, \text{N}\) \(1 \, \text{slug} * 32.17 \, \text{ft/s}^2 = 32.17 \, \text{lb}\)&lt;p&gt;Mass is the measure of how much an object resists linear force. Moment of inertia is how much the object resists rotational force or torque. The unit for moment of inertia in metric is kg-m2. Thus, the equivalent unit in customary is slug-ft2.&lt;/p&gt;&lt;p&gt;You want to measure how much air mass is in a given volume? That’s gonna be slugs/ft3.&lt;/p&gt;&lt;p&gt;Speed is mostly measured in feet per second, unless you want to know the speed of the aircraft. Then you use knots, which means nautical miles per hour. Importantly, a nautical mile is not the same as a regular mile. A regular mile is 5,280 feet. A nautical mile is ~6,076 feet or exactly 1,852 meters (???).&lt;/p&gt;&lt;p&gt;Do you want to know how fast your ship is sailing? Just throw out this piece of wood tied to a spool of rope. The rope has knots tied at regular intervals. Count the number of knots that unspool in a given time frame. That’s how many knots your ship is making.&lt;/p&gt;&lt;p&gt;Finally, temperature is measured in degrees Rankine. You know how the Kelvin scale is just the Celsius scale adjusted so that 0 Kelvin equals absolute zero? Well Rankine is the same concept applied to Fahrenheit.&lt;/p&gt;\(0 \, \text{R} = \text{absolute zero} \\ 534 \, \text{R} = 75 \, \text{F} = \text{room temperature}\)&lt;p&gt;How the fuck did we ever build the SR-71? 💀&lt;/p&gt;&lt;p&gt;Unity by convention uses metric for all physics units. The flight model in the textbook uses US customary units, so every input and output of this system has to be converted. So we have to add functions to handle converting to and from US customary units. This is easy enough since conversion is just a multiplication or division operation.&lt;/p&gt;&lt;head rend="h2"&gt;Terminology&lt;/head&gt;&lt;p&gt;There are several terms used in aerospace that I need to define. I have used equivalent terms in the previous project, but I will clarify them here.&lt;/p&gt;&lt;p&gt;Alpha (α) refers to the angle of attack.&lt;/p&gt;&lt;p&gt;Beta (β) refers to the angle of side slip.&lt;/p&gt;&lt;p&gt;Longitudinal axis is the X-axis, from tail to nose.&lt;/p&gt;&lt;p&gt;Normal axis is the Z- axis, the vertical axis pointing downwards.&lt;/p&gt;&lt;p&gt;Lateral axis is the Y-axis, or side axis, pointing right.&lt;/p&gt;&lt;p&gt;Phi (φ) is the aircraft’s roll around the X axis.&lt;/p&gt;&lt;p&gt;Theta (θ) is the aircraft’s pitch around the Y axis.&lt;/p&gt;&lt;p&gt;Psi (ψ) is the aircraft’s yaw around the Z axis.&lt;/p&gt;&lt;p&gt;P, Q, and R refer to the angular velocity around the X, Y, and Z axes respectively.&lt;/p&gt;&lt;p&gt;In general you will find that aerodynamics texts are allergic to good variable names. I suspect this is a form of gatekeeping. Or perhaps the authors have to pay by the letter to publish.&lt;/p&gt;&lt;head rend="h1"&gt;Air Data&lt;/head&gt;&lt;p&gt;Airplanes need air to fly [citation needed]. Every behavior of a plane is determined by the movement of air. Therefore it is critically important, for real and simulated planes, to be able to measure the air flowing around it.&lt;/p&gt;&lt;p&gt;Real planes need to measure static and dynamic air pressure to determine how fast the plane is moving. Static pressure is measured by a static pressure port. It’s the pressure that you would measure if you just lifted a pressure meter to the same altitude as the plane. Static pressure decreases with altitude.&lt;/p&gt;&lt;p&gt;Dynamic pressure measures the pressure added by the plane’s forward motion. This requires a pitot tube to measure. As the plane moves forward it rams air into the pitot tube and increases the pressure above the static pressure. The pitot measures the total pressure of the air. By subtracting the static pressure, we can obtain the dynamic pressure.&lt;/p&gt;&lt;p&gt;The static and dynamic pressures can then be used to calculate many of the variables the pilot needs to fly. Most important are the airspeed and altitude of the aircraft. Specifically, these values can be used to calculate the indicated airspeed of the aircraft. Indicated airspeed is calculated directly from the dynamic pressure.&lt;/p&gt;&lt;p&gt;At most subsonic speeds, the dynamic pressure of air flowing over the wings is the most important variable in flight. A plane’s performance can be defined in terms of indicated airspeed. For example, a plane may have a stall speed of 100 knots indicated airspeed. This means that no matter what altitude the plane is at, the indicated airspeed will be 100 when the plane stalls.&lt;/p&gt;&lt;p&gt;This is important since it gives a consistent number for the stall speed regardless of atmospheric conditions. The pressure and density of the air can vary based on weather, temperature, and other factors. So the true airspeed when a stall occurs can be very inconsistent. But as long as the pilot knows the indicated airspeed, they know how their plane will behave.&lt;/p&gt;&lt;p&gt;For this simulator, the calculation has to work backwards. We know the true airspeed and altitude of the plane from the velocity and position of the rigidbody. From that, we can calculate the dynamic pressure. This dynamic pressure is then used for later calculations in the flight model. Additionally, the plane’s speed in mach is calculated here as well.&lt;/p&gt;&lt;p&gt;The original Fortran source code is given:&lt;/p&gt;&lt;quote&gt;SUBROUTINE ADC(VT,ALT,AMACH,QBAR) DATA R0/2.377E-3/ TFAC = 1.0 - 0.703E-5 * ALT T = 519.0 * TFAC IF (ALT .GE. 35000.0) T= 390.0 RHO = R0 * (TFAC**4.14) AMACH= VT/SQRT(1.4*1716.3*T) QBAR = 0.5*RHO*VT*VT C PS = 1715.0 * RHO * T RETURN END&lt;/quote&gt;&lt;p&gt;It turns out, Fortran is actually pretty good at translating formulas. So this code is not as difficult to read as I expected.&lt;/p&gt;&lt;p&gt;To translate this to Unity, we create a class AirDataComputer to perform these calculations. The output of the calculation is the AirData struct.&lt;/p&gt;&lt;quote&gt;public struct AirData { public float altitudeMach; public float qBar; } public class AirDataComputer { /// &amp;lt;summary&amp;gt; /// Density in slugs/ft^3 /// &amp;lt;/summary&amp;gt; public const float SeaLevelDensity = 2.377e-3f; public const float MaxAltitude = 35000.0f; /// &amp;lt;summary&amp;gt; /// Calculates air data based on velocity and altitude /// &amp;lt;/summary&amp;gt; /// &amp;lt;param name="velocity"&amp;gt;Velocity in ft/s&amp;lt;/param&amp;gt; /// &amp;lt;param name="altitude"&amp;gt;Altitude in ft&amp;lt;/param&amp;gt; /// &amp;lt;returns&amp;gt;Air data&amp;lt;/returns&amp;gt; public AirData CalculateAirData(float velocity, float altitude) { ... } }&lt;/quote&gt;&lt;p&gt;Here we can see where the US customary units are used. The density of air at sea level is defined in slugs/ft3. The altitude is defined in feet. Theoretically, these values could be defined using metric. But the implementation of the function depends on even more values defined in customary.&lt;/p&gt;&lt;quote&gt;const float baseTemperature = 519.0f; // sea level temp in R const float minTemperature = 390.0f; // minimum temp in R const float temperatureGradient = 0.703e-5f; // gradient in R / ft altitude = Mathf.Clamp(altitude, 0, MaxAltitude); // calculate temperature in Rankine float temperatureFactor = 1.0f - (temperatureGradient * altitude); float T = Mathf.Max(minTemperature, baseTemperature * temperatureFactor);&lt;/quote&gt;&lt;p&gt;These calculations simulate the change in atmospheric conditions at different altitudes. Particularly important is how the temperature drops at higher altitudes. The temperature gradient approximates the decreases in temperature (in Rankine) as altitude increases.&lt;/p&gt;&lt;p&gt;This flight model supports altitudes up to 35,000 ft. Altitudes above this are not supported. At any altitude above this, the plane will behave as if it were at 35,000 ft. This is because the temperatures at this altitude no longer consistently decrease, as it does in the lower atmosphere. A more advanced atmosphere model would need to be used.&lt;/p&gt;&lt;p&gt;Temperature factor does not drop below about 0.75 in this range, so the resulting temperature T does not fall below 390 R.&lt;/p&gt;&lt;quote&gt;const float gamma = 1.4f; // ratio of specific heats const float gasConstant = 1716.3f; float speedOfSound = Mathf.Sqrt(gamma * gasConstant * T); float altitudeMach = velocity / speedOfSound;&lt;/quote&gt;&lt;p&gt;Now we can calculate the speed of sound at the plane’s current altitude and use it to find the plane’s Mach number. The speed of sound varies with density, which varies with temperature. The speed of sound is equal to the square root of the ratio of specific heat, called gamma, times the gas constant, gasConstant, times the absolute temperature, T.7&lt;/p&gt;&lt;p&gt;Once the speed of sound is known, calculating the Mach number is just a simple division.&lt;/p&gt;&lt;quote&gt;const float densityPower = 4.14f; float rho = SeaLevelDensity * Mathf.Pow(temperatureFactor, densityPower); float qBar = 0.5f * rho * velocity * velocity;&lt;/quote&gt;&lt;p&gt;And finally the dynamic pressure is calculated from the temperature factor. I’ll admit, I don’t understand why exactly the formula is designed this way. It seems to calculate a density factor, called rho, based solely on the temperature factor, raised to an arbitrary value, densityPower.&lt;/p&gt;&lt;p&gt;The NASA reference provides a similar formula using metric units and using a different arbitrary power. I guess this value is just what results from using customary🤷♂️&lt;/p&gt;&lt;p&gt;In any case, this gives us the two air data values we need for the rest of the simulation, dynamic pressure and mach number.&lt;/p&gt;&lt;head rend="h1"&gt;Table Interpolation&lt;/head&gt;&lt;p&gt;Throughout this flight model, various forms of table lookups are used to determine the aircraft’s behavior. Lookup tables are commonly used in flight simulators to represent complex curves and functions. In fact, Unity’s AnimationCurve class in the previous project is used to define a few lookup tables, such as lift coefficient.&lt;/p&gt;&lt;p&gt;This animation curve serves as a 1 dimensional lookup table. The input dimension is AOA and the output value is lift coefficient.&lt;/p&gt;&lt;p&gt;Fortran code doesn’t have the luxury of using AnimationCurves, but a simple table of values with an interpolation function is almost as powerful.&lt;/p&gt;&lt;head rend="h2"&gt;1D Lookup Table&lt;/head&gt;&lt;p&gt;The interpolation functions provided by the textbook look something like this:&lt;/p&gt;&lt;quote&gt;FUNCTION LOOKUP(ALPHA, RESULT) REAL A(-2:9) C DATA A / .770,.241,-.100,-.416,-.731,-1.053, &amp;amp; -1.366,-1.646,-1.917,-2.120,-2.248,-2.229 / C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) RESULT = A(K) + ABS(DA)*(A(L)-A(K)) RETURN END&lt;/quote&gt;&lt;p&gt;This function takes alpha (AOA) and uses it to lookup a value from the table. Alpha is a float that can have any value from [-10, 45]. The table “A” represents values for every 5 degree increment of alpha. Note that Fortran supports arrays with an arbitrary starting index, in this case -2. So this table supports indices in the range [-2, 9].&lt;/p&gt;&lt;p&gt;This first step is multiplying alpha by a scaling value to create a float S, which maps alpha to the range [-2, 9]. An integer index K is created from S and then clamped to values one less than the table’s index range. The value DA is calculated as the difference between S and K.&lt;/p&gt;&lt;p&gt;The value L is calculated to be one index away from K, in the same direction as S. So now we have two indices to the table, K and L, which we use to read two values from the table, A(K) and A(L). DA is then used to blend between these table values and produce the final result.&lt;/p&gt;&lt;p&gt;This has two effects. The first is the simplest to understand. If alpha falls within the input range of the table, L and K are selected as the closest table values. For example, if alpha is 12, the two indices would be 2 and 3. The difference between S and K would be less than 1. The values A(2) and A(3) can be read from the table and then interpolated based on the value of DA. This is a fairly normal interpolation calculation.&lt;/p&gt;&lt;p&gt;The other effect is what happens when alpha is outside of the input range of the table. K is guaranteed to not be the first or last index, and L is allowed to be one index off of K. L and K are still valid indices, but the value of DA may be larger than 1. This means when we interpolate between A(L) and A(K), we can extrapolate values for inputs beyond the range of the table.&lt;/p&gt;&lt;p&gt;This means our lookup table can handle values outside of it’s input range. But there is still a limitation. As the input value gets further away from the input range, the extrapolated values will become more and more unrealistic. This allows our plane to fly slightly outside the flight envelope of the lookup tables.&lt;/p&gt;&lt;p&gt;I translated this function into C# like this:&lt;/p&gt;&lt;quote&gt;public static float ReadTable(float[] table, int i, int start) { return table[i - start]; } public static (int k0, int k1, float t) GetLookUpIndex(float value, float scale, int min, int max) { float scaled = value * scale; int K0 = Mathf.Clamp((int)scaled, min, max); float T = scaled - K0; int K1 = K0 + (int)Mathf.Sign(T); return (K0, K1, T); } public static float LinearLookup(float value, float scale, float[] table, int min, int max) { (int k0, int k1, float kT) = GetLookUpIndex(value, scale, min + 1, max - 1); float T = ReadTable(table, k0, min); float U = ReadTable(table, k1, min); float result = T + Math.Abs(kT) * (U - T); return result; }&lt;/quote&gt;&lt;p&gt;GetLookUpIndex calculates K, L, and DA. These variables are renamed to k0, k1, and kT respectively.&lt;/p&gt;&lt;p&gt;ReadTable is a function that maps array indices to a new range, to support arbitrary starting indices like Fortran. (C# surprisingly supports this feature natively, but who actually uses that?)&lt;/p&gt;&lt;p&gt;LinearLookup reads the k0 and k1 values from the array and performs the interpolation. This allows us to calculate values for any input to the lookup table.&lt;/p&gt;&lt;p&gt;Note that the expression “T + Math.Abs(kT) * (U – T)” is effectively equivalent to Mathf.LerpUnclamped.&lt;/p&gt;&lt;head rend="h2"&gt;2D Lookup Table&lt;/head&gt;&lt;p&gt;All of the above code is needed to perform a one dimensional table lookup. Performing this kind of table lookup with two input dimensions is called a bilinear interpolation. Extending this to two dimensions is not that much more complicated.&lt;/p&gt;&lt;p&gt;The two input values to the table form a two dimensional space. Our input values form a two dimensional point. Instead of selecting two array indices K and L, we need to select four array indices. These four indices form a box around our input point. We simply perform 2 one dimensional lookups, and then interpolate between them to produce the final value.&lt;/p&gt;&lt;p&gt;The 2 one dimensional lookups are marked in red. The final interpolation is marked in blue.&lt;/p&gt;&lt;p&gt;Implementing this in C# is a simple extension of the LinearLookup function:&lt;/p&gt;&lt;quote&gt;public static float BilinearLookup(float xValue, float xScale, float yValue, float yScale, float[,] table, int xMin, int xMax, int yMin, int yMax) { (int x0, int x1, float xT) = GetLookUpIndex(xValue, xScale, xMin + 1, xMax - 1); (int y0, int y1, float yT) = GetLookUpIndex(yValue, yScale, yMin + 1, yMax - 1); float T = ReadTable(table, x0, y0, xMin, yMin); float U = ReadTable(table, x0, y1, xMin, yMin); float V = T + Math.Abs(xT) * (ReadTable(table, x1, y0, xMin, yMin) - T); float W = U + Math.Abs(xT) * (ReadTable(table, x1, y1, xMin, yMin) - U); float result = V + (W - V) * Math.Abs(yT); return result; }&lt;/quote&gt;&lt;p&gt;A bilinear interpolation is a very common operation in computer graphics. This is how textures are sampled when placed on 3D geometry.&lt;/p&gt;&lt;p&gt;In the next section, we will see that the engine thrust calculation interpolates between the output of 2 two dimensional tables. Adding this third interpolation means this calculation is now a trilinear interpolation. Interpolating between two tables is how mipmaps are blended together in computer graphics. How neat is that?&lt;/p&gt;&lt;head rend="h1"&gt;Engine&lt;/head&gt;&lt;p&gt;The next system we’re going to add is the engine. In my previous project, the engine was dead simple. The player selected a throttle value from [0, 1], which is multiplied by the plane’s total thrust. This works fine for that simulation and even gives us the ability to reduce thrust to zero, so the plane becomes a glider.&lt;/p&gt;&lt;p&gt;However, it is not a realistic simulation of how a jet engine works. In reality, a jet engine still produces some thrust at idle throttle. And there are more factors that affect thrust output than just the throttle setting.&lt;/p&gt;&lt;p&gt;The thrust output of a jet engine decreases with altitude and increases with speed. As altitude increases, the air gets thinner and the jet engine becomes weaker. But as speed increases, dynamic pressure, and thus pressure in the engine, increases and the engine becomes stronger. These two effects need to be considered at the same time to find the thrust output at any given moment.&lt;/p&gt;&lt;p&gt;Additionally, we have to consider how jet engines behave in terms of RPM. Just like piston engines (like in a typical car), jet engines have rotating components whose speed increases with throttle. The max RPM of a jet is much higher than a piston engine, however the range of possible RPM is smaller.&lt;/p&gt;&lt;p&gt;The engine in an F-16 has a maximum RPM of about 14,000. This is at the maximum non-afterburner power, called military power. When throttle is reduced to the lowest setting, idle, the RPM falls to about 8,400 RPM or about 60% of the max. Planes of course do not have a transmission like a car does, so this range of RPM also covers the range of thrust needed at all stages of flight.&lt;/p&gt;&lt;p&gt;At idle throttle, the engine runs at 60% max RPM, but only produces 8% of max thrust. At military power, the engine runs at 100% RPM and produces 100% thrust.&lt;/p&gt;&lt;p&gt;Military power is selected when the pilot moves the throttle lever to 77% of it’s max setting. Pushing the throttle beyond that engages the afterburner and produces even more thrust. Setting the throttle lever to 100% is called max power. Max power provides about 57% more thrust than military power. Engine RPM does not increase when using afterburner.&lt;/p&gt;&lt;p&gt;A significant difference between a piston engine and a jet engine is how fast the engine can change RPM. In a car, you can put the transmission in neutral and rev the engine up and down very quickly. But a jet engine is much slower to respond to changes in throttle, regardless of how fast the pilot moves the throttle lever. Generally, it can take several seconds to go from idle to military power or vice versa.&lt;/p&gt;&lt;p&gt;The reasons why jet engines are slower to change RPM are complicated. The change in throttle is managed by a computer to avoid compressor stall, which can cause damage or shut down of the engine. This computer will change engine parameters slowly to avoid compressor stall or any other problems that might be caused by moving the throttle too quickly.&lt;/p&gt;&lt;head rend="h2"&gt;Power&lt;/head&gt;&lt;p&gt;The behavior of the jet engine is included in the textbook’s flight model. RPM is not explicitly modeled, but is abstracted as power. The pilot chooses a commanded power level and the engine’s current power setting will move towards this over time. This behavior is spring-like, thus a larger difference will cause the current power setting to change faster. It takes about 2 seconds to increase from idle to military power in this flight model.&lt;/p&gt;&lt;p&gt;The first step is to translate the player’s throttle setting into engine power. This is a fairly simple function that maps military power, or 77% throttle, to 50% power. Full afterburner, or 100%, is mapped to 100% power. This is called the “throttle gearing”, but don’t confuse that with a car’s gearing. It’s much simpler.&lt;/p&gt;&lt;quote&gt;FUNCTION TGEAR(THTL) ! Power command v. thtl. relationship IF(THTL.LE.0.77) THEN TGEAR = 64.94*THTL ELSE TGEAR = 217.38*THTL-117.38 END IF RETURN END&lt;/quote&gt;&lt;p&gt;In C#, this is translated as:&lt;/p&gt;&lt;quote&gt;public static float CalculateThrottleGear(float throttle) { // maps throttle 0 - 0.77 to power 0% - 50% // maps throttle 0.77 - 1.0 to power 50% - 100% float power; if (throttle &amp;lt;= militaryPowerThrottle) { power = 64.94f * throttle; } else { power = 217.38f * throttle - 117.38f; } return power; }&lt;/quote&gt;&lt;p&gt;Those constants might seem weird, but they just define two lines with different slopes. The two lines intersect when the throttle is 0.77.&lt;/p&gt;&lt;p&gt;The player’s throttle setting is used to calculate the commanded power level. The rate of change of engine power also depends on the current power level. This rate is calculated in the functions PDOT and RTAU:&lt;/p&gt;&lt;quote&gt;FUNCTION PDOT(P3,P1) ! PDOT= rate of change of power IF (P1.GE.50.0) THEN ! P3= actual power, P1= power command IF (P3.GE.50.0) THEN T=5.0 P2=P1 ELSE P2=60.0 T=RTAU(P2-P3) END IF ELSE IF (P3.GE.50.0) THEN T=5.0 P2=40.0 ELSE P2=P1 T=RTAU(P2-P3) END IF END IF PDOT=T*(P2-P3) RETURN END FUNCTION RTAU(DP) ! used by function PDOT IF (DP.LE.25.0) THEN RTAU=1.0 ! reciprocal time constant ELSE IF (DP.GE.50.0)THEN RTAU=0.1 ELSE RTAU=1.9-.036*DP END IF RETURN END&lt;/quote&gt;&lt;p&gt;PDOT means power rate of change. In C#, this is translated as:&lt;/p&gt;&lt;quote&gt;float CalculatePowerRateOfChange(float actualPower, float commandPower) { // calculates how fast power output should change based on commanded power float T; float p2; if (commandPower &amp;gt;= 50.0) { if (actualPower &amp;gt;= 50.0) { T = 5.0f; p2 = commandPower; } else { p2 = 60.0f; T = CalculateRTau(p2 - actualPower); } } else { if (actualPower &amp;gt;= 50.0) { T = 5.0f; p2 = 40.0f; } else { p2 = commandPower; T = CalculateRTau(p2 - actualPower); } } float pdot = T * (p2 - actualPower); return pdot; } float CalculateRTau(float deltaPower) { float rTau; if (dp &amp;lt;= 25.0) { rTau = 1.0f; } else if (dp &amp;gt;= 50.0) { rTau = 0.1f; } else { rTau = 1.9f - 0.036f * dp; } return rTau; }&lt;/quote&gt;&lt;p&gt;Power rate of change is the velocity of the power level. The most important line is this:&lt;/p&gt;&lt;quote&gt;float pdot = T * (p2 - actualPower);&lt;/quote&gt;&lt;p&gt;The velocity depends on the quantity (p2 – actualPower). Let’s call this value deltaPower. A larger deltaPower means a larger velocity. This is scaled by the factor T. The complexity comes from selecting the values for p2 and T. p2 is sometimes the commandPower value. T is sometimes the result of calling CalculateRTau.&lt;/p&gt;&lt;p&gt;These values are selected by the if statements above. These check for two conditions, the commandedPower being above 50%, and the actualPower being above 50%. This is checking whether the afterburner is being requested, and whether the afterburner is currently active. Remember that afterburner starts at 77% throttle, but 50% power.&lt;/p&gt;&lt;p&gt;If the afterburner is not active, then the T is given the value of CalculateRTau. If it is active, then T is given the constant value of 5.0. This matches with our expectation of how the engine’s RPM changes. When not in afterburner, the engine RPM should change slowly, thus power changes slowly. When in afterburner, fuel flow into the afterburner can change quickly, thus power changes quickly.&lt;/p&gt;&lt;p&gt;If we look at the function CalculateRTau, we can see that T can vary in the range [0.1, 1.0]. This depends on deltaPower. When the engine is not in afterburner, T can be at most 1.0. In afterburner, T is 5.0. That means power can change about 5 times faster when in afterburner. When multiplied with deltaPower, pdot can be as large as 250% per second.&lt;/p&gt;&lt;p&gt;The smallest value of T occurs when deltaPower is 50 or greater. This occurs when actualPower is 0 and commanded power is 50%, for example. This will cause the power rate of change to be quite small at only 6% per second. Note that this is simply the instantaneous rate of change. As the actual power rises, T will become larger and the rate of change will increase.&lt;/p&gt;&lt;p&gt;Now the reason why p2 is used instead of commandedPower is to handle the case where commandedPower is over 50% and actualPower is below 50%, or vice versa. The pilot is requesting afterburner, but the engine has not reached military power yet. In that case, deltaPower would become very large and the simulation would change power levels too quickly. To avoid this, an arbitrary constant is chosen that is on the opposite side of 50%, but not very far.&lt;/p&gt;&lt;p&gt;So if the actualPower is 0%, but commandedPower is 100%, p2 is set to the value of 60. This limits deltaPower to a maximum value of 60, instead of 100. And in the case where actualPower is 100% and commandedPower is 0%, deltaPower is limited to -60.&lt;/p&gt;&lt;p&gt;Another behavior of this code is that CalculateRTau does not handle cases where deltaPower is negative. In this case, the function returns 1, the highest value it can return. This means that the power can decrease 10 times faster than it can increase, in the most extreme case.&lt;/p&gt;&lt;p&gt;I don’t know if this is an intentional effect. This may match the behavior of real jet engines, or it may be an oversight by the authors. You can play with the behavior by adding a few calls to Mathf.Abs().&lt;/p&gt;&lt;p&gt;The practical effect of all this is that the plane’s power will lag behind the player’s throttle setting. The pilot needs to make sure that they provide enough time for the power level to change when moving the throttle.&lt;/p&gt;&lt;p&gt;The HUD for this project is mostly reused from the previous flight sim project. But the throttle indicator must be updated, since it can’t show the difference between commanded power and current power.&lt;/p&gt;&lt;p&gt;Previously, the red bar used to show the player’s throttle setting. This worked fine since power lag was not modeled. In this project, the red bar shows the engine’s current power level. I added a triangle marker to show the commanded power setting.&lt;/p&gt;&lt;p&gt;As you move the throttle, you’ll see that current power level changes quickly when there is a large difference from commanded power, and it slows down as it approaches. And when the engine enters afterburner, the power level changes very quickly.&lt;/p&gt;&lt;head rend="h2"&gt;Thrust&lt;/head&gt;&lt;p&gt;Engine power is a fairly abstract variable in this flight model. It doesn’t really correspond to any physical variable. Once we calculate the current power, we use it to find the thrust generated by the engine. Thrust in this flight model is defined in terms of pounds-force (lbf).&lt;/p&gt;&lt;p&gt;Thrust is defined by a group of look up tables. Each table has two dimensions as input, mach number and altitude, and the output is thrust. This gives us different thrust values in different flight conditions. Mach is input as 0.0 to 1.0 mach, in increments of 0.2 mach. Altitude is input as 0 to 50,000 ft, in increments of 10,000 ft. In other words, the table has dimensions 6×6.&lt;/p&gt;&lt;p&gt;The lookup tables in this flight model correspond to idle power, military power, and max power (full afterburner). The engine’s power value is used to perform a third interpolation between the output values of these tables. This makes the thrust calculation a trilinear interpolation.&lt;/p&gt;&lt;p&gt;At idle throttle, the thrust output has 100% influence from the idle table. When the throttle is halfway to military power, the output has 50% influence from the idle table and 50% influence from the military table. Above military power, the output will have some influence from the military power table and the max power table.&lt;/p&gt;&lt;p&gt;The code to read one table in Fortran is given:&lt;/p&gt;&lt;quote&gt;DATA A/ [IDLE TABLE OMITTED] DATA B/ [MIL TABLE OMITTED] DATA C/ [MAX TABLE OMITTED] H=0.0001*ALT I=INT(H) IF (I.GE.5) I=4 DH=H-FLOAT(I) RM=5.*RMACH M=INT(RM) IF (M.GE.5) M=4 DM=RM-FLOAT(M) CDH=1.0-DH&lt;/quote&gt;&lt;p&gt;These parameters are used to perform the table lookups:&lt;/p&gt;&lt;quote&gt;TMIL= S + (T-S)*DM IF (POW.LT.50.0) THEN S= A(I,M)*CDH + A(I+1,M)*DH T= A(I,M+1)*CDH + A(I+1,M+1)*DH TIDL= S + (T-S)*DM THRUST= TIDL + (TMIL-TIDL)*POW/50.0 ELSE S= C(I,M)*CDH + C(I+1,M)*DH T= C(I,M+1)*CDH + C(I+1,M+1)*DH TMAX= S + (T-S)*DM THRUST= TMIL + (TMAX-TMIL)*(POW-50.0)*0.02 END IF&lt;/quote&gt;&lt;p&gt;The output of the military power table, TMIL, is always calculated. If the power level is under 50, then the idle table is calculated as well, TIDL. Otherwise the max table is calculated, TMAX. The output of the two table lookups is then interpolated again to calculate the final thrust value, THRUST.&lt;/p&gt;&lt;p&gt;Altogether, this forms a trilinear lookup. To translate this to C#, we call BilinearLookup twice. Then those two results are interpolated based on the power level:&lt;/p&gt;&lt;quote&gt;float InterpolateThrust(float thrust1, float thrust2, float power) { float result = Mathf.LerpUnclamped(thrust1, thrust2, power * 0.02f); return result; } float CalculateThrust(float power, float altitude, float rMach) { float a = Mathf.Max(0, altitude); float m = Mathf.Max(0, rMach); float thrust; float thrustMilitary = Table.BilinearLookup(a, 0.0001f, m, 5, militaryPowerTable, 0, 6, 0, 6); // perform trilinear interpolation if (power &amp;lt; 50.0) { float thrustIdle = Table.BilinearLookup(a, 0.0001f, m, 5, idlePowerTable, 0, 6, 0, 6); thrust = InterpolateThrust(thrustIdle, thrustMilitary, power); } else { float thrustMax = Table.BilinearLookup(a, 0.0001f, m, 5, maxPowerTable, 0, 6, 0, 6); thrust = InterpolateThrust(thrustMilitary, thrustMax, power - 50.0f); } return thrust; }&lt;/quote&gt;&lt;p&gt;The output of this calculation is the plane’s thrust in pounds-force. A simple unit conversion allows us to apply it in newtons to a Unity rigidbody:&lt;/p&gt;&lt;quote&gt;void UpdateThrust(float dt) { engine.ThrottleCommand = Throttle; engine.Mach = Mach; engine.Altitude = AltitudeFeet; engine.Update(dt); Rigidbody.AddRelativeForce(new Vector3(0, 0, engine.Thrust * poundsForceToNewtons)); }&lt;/quote&gt;&lt;head rend="h1"&gt;Forces&lt;/head&gt;&lt;head rend="h2"&gt;Lift force vs Normal force&lt;/head&gt;&lt;p&gt;In the previous flight sim project, we calculated a plane’s lift force using the angle of attack and an AnimationCurve. This is the very core of the flight simulator and is what enables flight. The flight model from the textbook does not calculate lift force.&lt;/p&gt;&lt;p&gt;Instead what this flight model calculates is normal force. Recall that lift force is perpendicular to the aircraft’s velocity vector. Normal force is perpendicular to the aircraft’s nose. This distinction is subtle at a low angle of attack, but it becomes significant at a high angle of attack.&lt;/p&gt;&lt;p&gt;There are two more analogous forces to consider, drag and axial force. Drag is always exactly opposite to the aircraft’s velocity vector while axial force is opposite the aircraft’s nose. Lift and drag are perpendicular to each other and form one set of forces. Normal and axial form another perpendicular set. It’s important to understand that these two sets of forces are equally valid. In fact, they are simply the consequence of choosing different basis vectors for measuring force.&lt;/p&gt;&lt;p&gt;And of course there is the side force that points to the right. These forces are applied on the normal, side, and longitudinal (axial) axes, which are equivalent to the X, Y, and Z axes.&lt;/p&gt;&lt;p&gt;Imagine all of the forces being produced by the aircraft are summed into a single force vector. This vector would be strongly vertical, because the plane is generating enough lift to support it’s own weight, and somewhat backwards because of drag. When this vector is projected onto the lift vector, the result is the lift force. When it’s projected onto the normal vector, the result is the normal force.&lt;/p&gt;&lt;p&gt;Choosing to represent these forces as lift/drag or normal/axial is arbitrary. The textbook flight model only deals with normal/axial force. I suspect that’s because it’s easier to measure the physical forces when using normal/axial forces in a wind tunnel, since those are always aligned with the plane’s local axes.&lt;/p&gt;&lt;p&gt;The normal force is very similar to lift for low angles of attack. Lift force peaks at the stall AOA and then declines. Normal force similarly peaks at stall AOA, but it then increases again to peak at 90 AOA, with an even higher force. 90 degrees AOA means the plane is falling downwards belly first, so it’s no longer producing lift over the wings. Instead the normal vector and the drag vector are now aligned. All of the drag force projected onto the normal vector results in a large normal force.&lt;/p&gt;&lt;p&gt;We can calculate the lift force from the normal and axial force. Both normal and axial force may contribute to the lift force, so a complete projection needs to use both. This is the formula:&lt;/p&gt;\(\text{Lift} = \text{normal} * \cos{(\text{alpha})} – \text{axial} * \sin{(\text{alpha})}\)&lt;p&gt;When we apply this formula to the normal force from the textbook, this is the result:&lt;/p&gt;&lt;p&gt;Oh wait, that’s upside down. Recall that the Z axis points downward in this coordinate system. So a negative Z value is an upwards force. Still though, the chart is a little confusing. I inverted the values below to make it more intuitive.&lt;/p&gt;&lt;p&gt;We can see at 90 degrees AOA, the normal force stays relatively high while the lift force drops to zero. This roughly matches with the chart from aerospaceweb.org above.&lt;/p&gt;&lt;p&gt;Also note that the textbook only provides table values up to 45 degrees AOA. The extrapolation of the table lookup function is what allows us to have normal force values up to 90 degrees AOA. Additionally, the table only goes down to -10 degrees AOA. We can extrapolate further, but the data will be inaccurate by -30 degrees AOA. Large negative AOA values will quickly become inaccurate. So when you’re flying, don’t do that.&lt;/p&gt;&lt;p&gt;Anyways, adding these forces to our simulator is easy. The functions are fairly simple. They are called CZ, CY, and CX. These calculate the coefficients of force on the Z, Y, and X axes respectively. Note that these functions are the coefficients, not the force values themselves. They are used to calculate the force later on.&lt;/p&gt;&lt;p&gt;CZ or the normal coefficient is calculated like this:&lt;/p&gt;&lt;quote&gt;FUNCTION CZ(ALPHA,BETA,EL) REAL A(-2:9) C DATA A/ [TABLE OMITTED] C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = A(K) + ABS(DA)*(A(L)-A(K)) CZ = S*(1-(BETA/57.3)**2) - .19*(EL/25.0) C RETURN END&lt;/quote&gt;&lt;p&gt;The bulk of this code is just the table interpolation function. The table only depends on ALPHA and the output is S. The only new part here is the last line, where CZ is assigned a value. S is reduced based on the value of BETA and another term is subtracted based on EL, the elevator angle.&lt;/p&gt;&lt;p&gt;This is very easy to translate to C#:&lt;/p&gt;&lt;quote&gt;float GetZAxisForceCoefficient(float alpha, float beta, float elevator) { float S = Table.LinearLookup(alpha, 0.2f, zAxisTable, -2, 10); float CZ = S * (1 - Mathf.Pow(beta * Mathf.Deg2Rad, 2)) - 0.19f * (elevator / 25.0f); return CZ; }&lt;/quote&gt;&lt;p&gt;CY or the side coefficient is even simpler. It doesn’t even have a lookup table. Side force is perpendicular to both normal and axial force.&lt;/p&gt;&lt;quote&gt;FUNCTION CY(BETA,AIL,RDR) CY = -.02*BETA + .021*(AIL/20.0) + .086*(RDR/30.0) C RETURN END&lt;/quote&gt;&lt;p&gt;Side coefficient depends solely on beta, aileron angle, and rudder angle.&lt;/p&gt;&lt;p&gt;In C#:&lt;/p&gt;&lt;quote&gt;float GetYAxisForceCoefficient(float beta, float aileron, float rudder) { float CY = -0.02f * beta + 0.021f * (aileron / 20.0f) + 0.086f * (rudder / 30.0f); return CY; }&lt;/quote&gt;&lt;p&gt;CX or the axial coefficient is basically what creates drag on the aircraft. This function is a little more complicated since it performs a bilinear interpolation, with alpha and elevator angle as the inputs.&lt;/p&gt;&lt;quote&gt;FUNCTION CX(ALPHA,EL) REAL A(-2:9,-2:2) C DATA A/ [TABLE OMITTED] C S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = EL/12.0 M = INT(S) IF(M.LE.-2) M=-1 IF(M.GE.2) M=1 DE = S - FLOAT(M) N = M + INT(SIGN(1.1,DE)) V = A(K,M) + ABS(DA)*(A(L,M)-A(K,M)) W = A(K,N) + ABS(DA)*(A(L,N)-A(K,N)) CX = V + (W-V)*ABS(DE) C RETURN END&lt;/quote&gt;&lt;p&gt;Thanks to the table lookup functions, this is easy to translate to C#:&lt;/p&gt;&lt;quote&gt;float GetXAxisForceCoefficient(float alpha, float elevator) { float result = Table.BilinearLookup(alpha, 0.2f, elevator, 1f / 12f, xAxisTable, -2, 9, -2, 2); return result; }&lt;/quote&gt;&lt;p&gt;These three functions define all of the linear force coefficients applied to the aircraft during flight. None of these will rotate the aircraft. That is handled by a different and more complicated set of calculations.&lt;/p&gt;&lt;head rend="h1"&gt;Moments&lt;/head&gt;&lt;p&gt;Moment is another word for torque. (There is a subtle difference, but who cares?🤓) The F-16 flight model uses another set of look up tables to compute the moment for the aircraft.&lt;/p&gt;&lt;p&gt;In the previous flight sim, torque was not actually calculated. Instead, the flight model calculates the angular acceleration directly. This ignores the mass of the plane when applying the torque. This is a simplification. A more realistic flight model would take into account the mass of the aircraft when applying torque.&lt;/p&gt;&lt;p&gt;Note that mass is not sufficient to model rotations. When it comes to rotation, the analogy to mass is called moment of inertia. Just like mass is the property that measures an object’s resistance to force, moment of inertia is the resistance to torque. But unlike mass, moment of inertia can differ on all 3 axes. This means a torque on the X axis will result in a different angular acceleration than the same torque on the Y axis, for example.&lt;/p&gt;&lt;p&gt;Moment of inertia is a four-dimensional value, called AXX, AYY, AZZ, and AXZ in the textbook code. This flight model contains it’s own calculations for angular velocity using these moment of inertia values.&lt;/p&gt;&lt;p&gt;The flight model contains several functions that calculate the moment of the aircraft. The three basic functions are called CM, CL, and CN. These calculate the moments around the Y, X, and Z axes, AKA pitch, roll, and yaw, respectively. (L stands for longitudinal, which is the X axis. N stands for normal, which is the Z axis. M stands for… something)&lt;/p&gt;&lt;p&gt;These functions are all simple look up tables. CM (pitch) uses alpha and elevator angle as the input. CL (roll) and CN (yaw) use alpha and beta as the input. CM is basically the same as the other lookup table functions. CL and CN are similar to each other since they both use a symmetric table. This is because the plane is symmetric on the lateral axis, so a single table can represent the left and right sides. Their final output is then multiplied by the sign of beta.&lt;/p&gt;&lt;quote&gt;FUNCTION CL(ALPHA,BETA) REAL A(-2:9,0:6) DATA A/ [DATA OMITTED] S = 0.2*ALPHA K = INT(S) IF(K.LE.-2) K=-1 IF(K.GE.9) K=8 DA = S - FLOAT(K) L = K + INT(SIGN(1.1,DA)) S = .2*ABS(BETA) M = INT(S) IF(M.EQ.0) M=1 IF(M.GE.6) M=5 DB = S - FLOAT(M) N = M + INT(SIGN(1.1,DB)) T = A(K,M) U = A(K,N) V = T + ABS(DA)*(A(L,M) - T) W = U + ABS(DA)*(A(L,N) - U) DUM = V + (W - V) * ABS(DB) CL = DUM + SIGN(1.0,BETA) RETURN END&lt;/quote&gt;&lt;p&gt;Note that there is an error in the textbook code. The final operation “CL = DUM + SIGN(…)” should use multiplication instead of addition. Otherwise this operation doesn’t make any sense.&lt;/p&gt;&lt;p&gt;When translated into C#:&lt;/p&gt;&lt;quote&gt;float GetYAxisMoment(float alpha, float elevator) { float result = Table.BilinearLookup(alpha, 0.2f, elevator, 1f / 12f, yMomentTable, -2, 9, -2, 2); return result; } float GetXAxisMoment(float alpha, float beta) { float DUM = Table.BilinearLookup(alpha, 0.2f, Mathf.Abs(beta), 0.2f, xMomentTable, -2, 9, 0, 7); float CL = DUM * Mathf.Sign(beta); return CL; } float GetZAxisMoment(float alpha, float beta) { float DUM = Table.BilinearLookup(alpha, 0.2f, Mathf.Abs(beta), 0.2f, zMomentTable, -2, 9, 0, 7); float CL = DUM * Mathf.Sign(beta); return CL; }&lt;/quote&gt;&lt;p&gt;Notice that CM takes “elevator” as an argument, so this is where the elevator’s turning effect is calculated. But CL and CN do not take any control surface as an argument. These functions only apply moment based on alpha and beta. For example, at high angles of sideslip, the plane tends to roll. On real planes, this is caused by wing sweep. In this flight model, it’s caused by the CL function.&lt;/p&gt;&lt;p&gt;Elevators are applied in CM, but rudder and ailerons are not. Those are actually handled by four more functions, called DLDA, DLDR, DNDA, and DNDR. The names are cryptic, but it just means which axis is affected from which control surface.&lt;/p&gt;&lt;p&gt;The “L” stands for longitudinal, so DLDA is the longitudinal moment from the ailerons, A. DLDR is the longitudinal moment from the rudder, R. The “N” stands for normal, so those functions are the normal axis moment from aileron and rudders.&lt;/p&gt;&lt;p&gt;These four functions are eventually summed with the CL and CN functions above. These functions mean that roll is affected by aileron and rudder, and yaw is affected by aileron and rudder.&lt;/p&gt;&lt;head rend="h2"&gt;Damping&lt;/head&gt;&lt;p&gt;There is one more set of coefficients that must be calculated. These are the damping coefficients and they depend solely on alpha. These values are stored in 9 distinct 1D lookup tables. The code for these lookups is the same as the other lookup code.&lt;/p&gt;&lt;p&gt;These values are stored in an array of length 9 called D.&lt;/p&gt;&lt;p&gt;Damping is the moment that opposes the angular velocity of an aircraft, essentially angular drag. They affect the other moment values in somewhat complex ways. For example, some of them are combined with the plane’s current bank value to affect the roll moment.&lt;/p&gt;&lt;p&gt;What isn’t clear is what the damping values actually represent. In the C# code, I added these comments explaining their meaning:&lt;/p&gt;&lt;quote&gt;// D[0] = CXq // D[1] = CYr // D[2] = CYp // D[3] = CZq // D[4] = Clr // D[5] = Clp // D[6] = Cmq // D[7] = Cnr // D[8] = Cnp&lt;/quote&gt;&lt;p&gt;Hope this helps!&lt;/p&gt;&lt;p&gt;The best I can tell is that “CXq” is the damping moment on the X axis relative to q, which is the angular velocity around the Y axis. The other damping values follow this naming scheme.&lt;/p&gt;&lt;p&gt;This is yet another example of aerodynamics texts with poor variable names.&lt;/p&gt;&lt;head rend="h1"&gt;Complete Flight Model&lt;/head&gt;&lt;p&gt;With all of the individual coefficients defined, we can now implement the complete flight model for the F-16. This flight model actually contains it’s own physics integrator. The text provides it’s own code for calculating velocity and angular velocity from the aerodynamic forces.&lt;/p&gt;&lt;p&gt;Strictly speaking, we don’t need to use this code since Unity allows us to provide those same forces and then performs the physics calculation for us. Setting the mass is easy enough, we just have to convert slugs to kilograms. The textbook code calculates acceleration by dividing the force by the aircraft mass. We simply omit this division, convert the forces to newtons, and apply it to the rigidbody.&lt;/p&gt;&lt;p&gt;However the moment of inertia is more complicated. The textbook provides the 4 dimensional MOI values, but Unity expects a 3 dimensional inertia tensor. That inertia tensor is then rotated by a quaternion called “inertiaTensorRotation”. I have no idea how to calculate this quaternion from the textbook’s provided value.&lt;/p&gt;&lt;p&gt;Therefore, we continue to use the textbook’s code for applying moment and simply apply the resulting angular acceleration to the rigidbody.&lt;/p&gt;&lt;p&gt;The Fortran code for the flight model is concise, yet scrutable. The first step is to read the plane’s current state from the input state vector X. This is simply an array that contains all of the relevant data for this frame.&lt;/p&gt;&lt;quote&gt;VT= X(1); ALPHA= X(2)*RTOD; BETA= X(3)*RTOD PHI=X(4); THETA= X(5); PSI= X(6) P= X(7); Q= X(8); R= X(9); ALT= X(12); POW= X(13)&lt;/quote&gt;&lt;p&gt;VT is the plane’s velocity in feet per second.&lt;/p&gt;&lt;p&gt;ALPHA and BETA are the plane’s AOA and AOS in degrees. RTOD is the constant to convert from radians to degrees.&lt;/p&gt;&lt;p&gt;PHI, THETA, and PSI are the plane’s roll, pitch, and yaw in radians.&lt;/p&gt;&lt;p&gt;P, Q, and R are the plane’s angular velocities (or roll rate, pitch rate, and yaw rate) in radians per second.&lt;/p&gt;&lt;p&gt;ALT is the altitude in feet.&lt;/p&gt;&lt;p&gt;POW is the current power level of the engine (0 – 100).&lt;/p&gt;&lt;p&gt;The air data computer (ADC) and engine model are then called using these variables:&lt;/p&gt;&lt;quote&gt;CALL ADC(VT,ALT,AMACH,QBAR); CPOW= TGEAR(THTL) XD(13) = PDOT(POW,CPOW); T= THRUST(POW,ALT,AMACH)&lt;/quote&gt;&lt;p&gt;The ADC function populates the variables AMACH and QBAR, which are the altitude mach and dynamic pressure.&lt;/p&gt;&lt;p&gt;CPOW is the pilot’s commanded power setting. That is, the power level returned by calling the throttle gear function, TGEAR, on the throttle lever position, THTL.&lt;/p&gt;&lt;p&gt;The array XD is the output state vector. Specifically, it holds the calculated derivative for every input value. XD(13) is set to the value calculated by PDOT, which is the velocity of the power level.&lt;/p&gt;&lt;p&gt;T is the thrust in pounds-force output by the engine, calculated using the power level, altitude, and altitude mach.&lt;/p&gt;&lt;p&gt;Then the aerodynamic coefficients are calculated using the force and moment functions:&lt;/p&gt;&lt;quote&gt;CXT = CX (ALPHA,EL) CYT = CY (BETA,AIL,RDR) CZT = CZ (ALPHA,BETA,EL) DAIL= AIL/20.0; DRDR= RDR/30.0 CLT = CL(ALPHA,BETA) + DLDA(ALPHA,BETA)*DAIL &amp;amp; + DLDR(ALPHA,BETA)*DRDR CMT = CM(ALPHA,EL) CNT = CN(ALPHA,BETA) + DNDA(ALPHA,BETA)*DAIL &amp;amp; + DNDR(ALPHA,BETA)*DRDR&lt;/quote&gt;&lt;p&gt;The values CXT, CYT, and CZT are the coefficients on the X, Y, and Z axes, calculated by calling their respective coefficient functions.&lt;/p&gt;&lt;p&gt;EL, AIL, and RDR are the current position of the elevators, ailerons, and rudder in degrees. DAIL and DRDR are simply the angle of these surfaces divided by the max angle. Their range is [-1, 1].&lt;/p&gt;&lt;p&gt;The values CLT, CMT, and CNT are the moment coefficients on the longitudinal, m’lateral, and normal axes. Note that CM calculates moment caused by the elevator position. The effects of the other control surfaces are calculated in the DLDA, DLDR, DNDA, and DNDR functions.&lt;/p&gt;&lt;p&gt;Then some other values are calculated and the damping coefficients are added to the above values:&lt;/p&gt;&lt;quote&gt;TVT= 0.5/VT; B2V= B*TVT; CQ= CBAR*Q*TVT CALL DAMP(ALPHA,D) CXT= CXT + CQ * D(1) CYT= CYT + B2V * ( D(2)*R + D(3)*P ) CZT= CZT + CQ * D(4) CLT= CLT + B2V * ( D(5)*R + D(6)*P ) CMT= CMT + CQ * D(7) + CZT * (XCGR-XCG) CNT= CNT + B2V*(D(8)*R + D(9)*P) - CYT*(XCGR-XCG) * CBAR/B&lt;/quote&gt;&lt;p&gt;I’ll be honest, I straight up don’t know what any of these values are or why they are being applied like this. The effect appears to be angular damping (AKA angular drag) which opposes the plane’s angular velocity.&lt;/p&gt;&lt;p&gt;The value (XCGR – XCG) is the center of gravity reference minus the current center of gravity. This allows us to alter the center of gravity of the aircraft and see how that affects stability.&lt;/p&gt;&lt;p&gt;XCGR is 0.35 for this flight model. XCG is 0.35 by default. XCG is the normalized position of the center of gravity, with a possible range of [0, 1]. This means that when XCG is 0.35, the term (XCGR – XCG) becomes zero and the aircraft is balanced around it’s center of gravity.&lt;/p&gt;&lt;p&gt;The center of gravity term affects CMT and CNT, which are the pitch and yaw axes. The roll axis is not affected.&lt;/p&gt;&lt;p&gt;The next block of code is fun:&lt;/p&gt;&lt;quote&gt;CBTA = COS(X(3)); U=VT*COS(X(2))*CBTA V= VT * SIN(X(3)); W=VT*SIN(X(2))*CBTA STH = SIN(THETA); CTH= COS(THETA); SPH= SIN(PHI) CPH = COS(PHI) ; SPSI= SIN(PSI); CPSI= COS(PSI) QS = QBAR * S ; QSB= QS * B; RMQS= QS/MASS GCTH = GD * CTH ; QSPH= Q * SPH AY = RMQS*CYT ; AZ= RMQS * CZT&lt;/quote&gt;&lt;p&gt;This writhing mass of arithmetic is simply pre-calculating a lot of the values that are used to calculate the aerodynamic forces. Some of these values are used in multiple places, so to avoid repeating them, they are pulled out of those equations and placed here.&lt;/p&gt;&lt;p&gt;This is essentially the “common subexpression” optimization pass of a compiler, but applied manually.&lt;/p&gt;&lt;p&gt;The important variables are U, V, and W, which is the plane’s velocity on the X, Y, and Z axes respectively.&lt;/p&gt;&lt;p&gt;QS is QBAR (dynamic pressure) times S (wing area).&lt;/p&gt;&lt;p&gt;Now the aerodynamic forces are calculated:&lt;/p&gt;&lt;quote&gt;UDOT = R*V - Q*W - GD*STH + (QS * CXT + T)/MASS VDOT = P*W - R*U + GCTH * SPH + AY WDOT = Q*U - P*V + GCTH * CPH + AZ DUM = (U*U + W*W) xd(1) = (U*UDOT + V*VDOT + W*WDOT)/VT xd(2) = (U*WDOT - W*UDOT) / DUM xd(3) = (VT*VDOT- V*XD(1)) * CBTA / DUM&lt;/quote&gt;&lt;p&gt;Once again, I don’t actually understand what I’m reading. UDOT etc are the accelerations on each axis. These values are then used to update the output state vector xd(1), xd(2), and xd(3), which are the VT, ALPHA, and BETA that will be used in the next frame.&lt;/p&gt;&lt;p&gt;It appears that this flight model is calculating the change in alpha and beta directly from the change in velocity. This is not necessary in C#, since we can calculate alpha and beta fresh in each frame.&lt;/p&gt;&lt;p&gt;But I don’t fully understand how UDOT is calculated. R and Q are angular velocities, so multiplying them with linear velocity doesn’t make any sense. Perhaps this is some physics equation that I’m not familiar with.&lt;/p&gt;&lt;p&gt;GD * STH is the gravity acceleration times sin(theta). This is simply how gravity is applied. When the plane is level (theta = 0), sin(theta) is 0. The plane experiences no gravity acceleration on the X axis (the forward axis). When the plane is pointed straight down, sin(theta) = 1, so the plane experiences the full force of gravity pulling on the X axis.&lt;/p&gt;&lt;p&gt;A similar calculation is made for every axis.&lt;/p&gt;&lt;p&gt;For UDOT, the final term is (QS * CXT + T) / MASS. This is the coefficient CXT plus the thrust from the engine, divided by mass. VDOT and WDOT have similar final terms, made more difficult to read by the common subexpression optimization.&lt;/p&gt;&lt;p&gt;Ignoring the other terms, the 3 accelerations can be written:&lt;/p&gt;&lt;quote&gt;UDOT = (QS * CXT + T)/MASS VDOT = AY WDOT = AZ&lt;/quote&gt;&lt;p&gt;Then the variables can be expanded and rewritten:&lt;/p&gt;&lt;quote&gt;UDOT = (QBAR * S * CXT + T) / MASS VDOT = (QBAR * S * CYT) / MASS WDOT = (QBAR * S * CZT) / MASS&lt;/quote&gt;&lt;p&gt;This is simply the force coefficient times QBAR (dynamic pressure) times S (wing area). Then thrust is added to the X axis. This is how all forces are applied to the aircraft.&lt;/p&gt;&lt;p&gt;Recall the lift equation from my previous project:&lt;/p&gt;\(L=\frac12\times A\times\rho\times C_L\times v^2\)&lt;list rend="ul"&gt;&lt;item&gt;L is the resulting lift force&lt;/item&gt;&lt;item&gt;A is the surface area&lt;/item&gt;&lt;item&gt;ρ (rho) is the air density&lt;/item&gt;&lt;item&gt;CL is the coefficient of lift&lt;/item&gt;&lt;item&gt;v is the velocity&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The surface area A is equivalent to the wing area S in the Fortran code. CL is equivalent to the variables CXT, CYT, or CZT. The factor ρ * v2 is equivalent to QBAR. Thus we are essentially calculating a lift force on all three axes. But remember that we are specifically calculating normal force, not lift force.&lt;/p&gt;&lt;p&gt;The roll, pitch, and yaw state vectors are then updated:&lt;/p&gt;&lt;quote&gt;xd(4) = P + (STH/CTH)*(QSPH + R*CPH) xd(5) = Q*CPH - R*SPH xd(6) = (QSPH + R*CPH)/CTH&lt;/quote&gt;&lt;p&gt;Once again, these equations make zero sense to me🤷♂️. It’s important for the Fortran code, but we will be calculating roll, pitch, and yaw differently in C#.&lt;/p&gt;&lt;p&gt;Aerodynamic moment is about to be calculated. However this depends on the moment of inertia values and some more values derived from those:&lt;/p&gt;&lt;quote&gt;PARAMETER (AXX=9496.0, AYY= 55814.0, AZZ=63100.0, AXZ= 982.0) PARAMETER (AXZS=AXZ**2, XPQ=AXZ*(AXX-AYY+AZZ),GAM=AXX*AZZ-AXZ**2) PARAMETER (XQR= AZZ*(AZZ-AYY)+AXZS, ZPQ=(AXX-AYY)*AXX+AXZS) PARAMETER ( YPR= AZZ - AXX )&lt;/quote&gt;&lt;p&gt;Now the aerodynamic moment is calculated:&lt;/p&gt;&lt;quote&gt;ROLL = QSB*CLT PITCH = QS *CBAR*CMT YAW = QSB*CNT PQ = p*Q QR = Q*R QHX = Q*HX xd(7) = ( XPQ*PQ - XQR*QR + AZZ*ROLL + AXZ*(YAW + QHX) )/GAM xd(8) = ( YPR*P*R - AXZ*(P**2 - R**2) + PITCH - R*HX )/AYY xd(9) = ( ZPQ*PQ - XPQ*QR + AXZ*ROLL + AXX*(YAW + QHX) )/GAM&lt;/quote&gt;&lt;p&gt;There’s a lot of stuff going on here. The output state vectors are updated using the moment of inertia values as well as HX, which is the angular momentum of the spinning engine mass. I don’t know enough about physics to fully understand why these equations are defined like this.&lt;/p&gt;&lt;p&gt;But we can at least see how the moment coefficients are used if we expand the ROLL, PITCH, and YAW variables:&lt;/p&gt;&lt;quote&gt;ROLL = QBAR * S * B * CLT PITCH = QBAR * S * CBAR * CMT YAW = QBAR * S * B * CNT&lt;/quote&gt;&lt;p&gt;ROLL and YAW depend on B, the wingspan of the plane. Pitch depends on CBAR, the mean aerodynamic chord.&lt;/p&gt;&lt;p&gt;The final step is to calculate the world space position of the aircraft. Since we are using Unity rigidbodies to implement the flight model, this step is not translated to C#. But for reference:&lt;/p&gt;&lt;quote&gt;T1= SPH * CPSI; T2= CPH * STH; T3= SPH * SPSI S1= CTH * CPSI; S2= CTH * SPSI; S3= T1 * STH - CPH * SPSI S4= T3 * STH + CPH * CPSI; S5= SPH * CTH; S6= T2*CPSI + T3 S7= T2 * SPSI - T1; S8= CPH * CTH xd(10) = U * S1 + V * S3 + W * S6 ! North speed xd(11) = U * S2 + V * S4 + W * S7 ! East speed xd(12) = U * STH -V * S5 - W * S8 ! Vertical speed AN = -AZ/GD; ALAT= AY/GD;&lt;/quote&gt;&lt;p&gt;Now we can start translating this into C# using Unity’s physics engine to replace some parts.&lt;/p&gt;&lt;p&gt;A lot of the code can be reused from the previous flight sim project. Using it for this new flight model only requires some conversion into customary units and back. The main class that controls everything is Plane. This class contains instances of the AirDataComputer, Engine, and Aerodynamics, which is where the translated Fortran code lives.&lt;/p&gt;&lt;p&gt;One simplification can be made since we are using Unity physics. We do not need to calculate the acceleration of the aircraft manually. That can be done automatically by the physics engine. However, the moment calculation needs to be copied more or less directly from the textbook.&lt;/p&gt;&lt;p&gt;The air data computer needs to be called:&lt;/p&gt;&lt;quote&gt;void UpdateAirData() { float speed = LocalVelocity.magnitude; // m/s float speedFeet = speed * metersToFeet; AltitudeFeet = Rigidbody.position.y * metersToFeet; airData = airDataComputer.CalculateAirData(speedFeet, AltitudeFeet); }&lt;/quote&gt;&lt;p&gt;Then the engine needs to be updated and the thrust force applied:&lt;/p&gt;&lt;quote&gt;void UpdateThrust(float dt) { engine.ThrottleCommand = Throttle; engine.Mach = Mach; engine.Altitude = AltitudeFeet; engine.Update(dt); Rigidbody.AddRelativeForce(new Vector3(0, 0, engine.Thrust * poundsForceToNewtons)); }&lt;/quote&gt;&lt;p&gt;For the aerodynamics class, a struct with all relevant aerodynamic state is passed, similar to the state vector in the Fortran code.&lt;/p&gt;&lt;quote&gt;public struct AerodynamicState { public Vector4 inertiaTensor; public Vector3 velocity; public Vector3 angularVelocity; public AirData airData; public float altitude; public float alpha; public float beta; public float xcg; public ControlSurfaces controlSurfaces; }&lt;/quote&gt;&lt;p&gt;This is populated by the Plane class, which also handles unit conversions:&lt;/p&gt;&lt;quote&gt;AerodynamicState currentState = new AerodynamicState { inertiaTensor = inertiaTensor, velocity = ConvertVectorToAerospace(LocalVelocity) * metersToFeet, angularVelocity = ConvertAngleToAerospace(LocalAngularVelocity), airData = airData, alpha = alpha, beta = beta, xcg = centerOfGravityPosition, controlSurfaces = ControlSurfaces }; var newState = aerodynamics.CalculateAerodynamics(currentState);&lt;/quote&gt;&lt;p&gt;All of the flight model code is located inside the Aerodynamics class.&lt;/p&gt;&lt;p&gt;First step is to call the aerodynamic coefficient functions from above:&lt;/p&gt;&lt;quote&gt;Vector3 GetForceCoefficient(float alpha, float beta, float aileron, float rudder, float elevator) { return new Vector3( GetXAxisForceCoefficient(alpha, elevator), GetYAxisForceCoefficient(beta, aileron, rudder), GetZAxisForceCoefficient(alpha, beta, elevator) ); } Vector3 GetMomentCoefficient(float alpha, float beta, float elevator) { return new Vector3( GetXAxisMomentCoefficient(alpha, beta), GetYAxisMomentCoefficient(alpha, elevator), GetZAxisMomentCoefficient(alpha, beta) ); } ... public AerodynamicForces CalculateAerodynamics(AerodynamicState currentState) { Vector3 forceCoefficient = GetForceCoefficient( currentState.alpha, currentState.beta, currentState.controlSurfaces.aileron, currentState.controlSurfaces.rudder, currentState.controlSurfaces.elevator ); Vector3 momentCoefficient = GetMomentCoefficient( currentState.alpha, currentState.beta, currentState.controlSurfaces.elevator ); }&lt;/quote&gt;&lt;p&gt;Then we calculate the damping values. This function simply performs the 9 table lookups.&lt;/p&gt;&lt;quote&gt;void CalculateDampingValues(float alpha) { float S = 0.2f * alpha; int K = Mathf.Clamp((int)S, -1, 8); float DA = S - K; int L = K + (int)Mathf.Sign(DA); for (int i = 0; i &amp;lt; 9; i++) { dampingTable[i] = ReadDampTable(dampTable, K, i) + Math.Abs(DA) * (ReadDampTable(dampTable, L, i) - ReadDampTable(dampTable, K, i)); } }&lt;/quote&gt;&lt;p&gt;Then the variables we need later are calculated:&lt;/p&gt;&lt;quote&gt;// calculate variables float P = currentState.angularVelocity.x; // roll rate float Q = currentState.angularVelocity.y; // pitch rate float R = currentState.angularVelocity.z; // yaw rate float airspeed = Mathf.Max(1, currentState.velocity.magnitude); float TVT = 0.5f / airspeed; float B2V = wingSpanFt * TVT; float CQ = CBAR * Q * TVT; float DAIL = currentState.controlSurfaces.aileron / 20.0f; float DRDR = currentState.controlSurfaces.rudder / 30.0f; float QS = currentState.airData.qBar * wingAreaFtSquared; float QSB = QS * wingSpanFt;&lt;/quote&gt;&lt;p&gt;Then damping is applied to the force and moment coefficients:&lt;/p&gt;&lt;quote&gt;// damping float CXT = forceCoefficient.x + CQ * dampingTable[0]; float CYT = forceCoefficient.y + B2V * (dampingTable[1] * R + dampingTable[2] * P); float CZT = forceCoefficient.z + CQ * dampingTable[3]; float CLT = momentCoefficient.x + B2V * (dampingTable[4] * R + dampingTable[5] * P); CLT += GetDLDA(currentState.alpha, currentState.beta) * DAIL; CLT += GetDLDR(currentState.alpha, currentState.beta) * DRDR; float CMT = momentCoefficient.y + CQ * dampingTable[6] + CZT * (XCGR - currentState.xcg); float CNT = momentCoefficient.z + B2V * (dampingTable[7] * R + dampingTable[8] * P) - CYT * (XCGR - currentState.xcg) * CBAR / wingSpanFt; CNT += GetDNDA(currentState.alpha, currentState.beta) * DAIL; CNT += GetDNDR(currentState.alpha, currentState.beta) * DRDR;&lt;/quote&gt;&lt;p&gt;Note that the damping array in Fortran is 1 based, while the same array in C# is 0 based.&lt;/p&gt;&lt;p&gt;Forces are calculated from the force coefficients. Since we are using Unity’s physics to apply gravity, the gravity terms are not included here. The force from engine thrust is applied outside of this class. And force is applied to a rigidbody, so acceleration does not need to be calculated manually. So the force calculations are now very simple:&lt;/p&gt;&lt;quote&gt;// forces // Acceleration in original text. Need to calculate force instead of acceleration float UDOT = QS * CXT; float VDOT = QS * CYT; float WDOT = QS * CZT;&lt;/quote&gt;&lt;p&gt;Moments are calculated using largely the same code as the textbook:&lt;/p&gt;&lt;quote&gt;// moments float ROLL = QSB * CLT; float PITCH = QS * CBAR * CMT; float YAW = QSB * CNT; float PQ = P * Q; float QR = Q * R; float QHX = Q * HX; // calculate inertia values float AXX = currentState.inertiaTensor.x; float AYY = currentState.inertiaTensor.y; float AZZ = currentState.inertiaTensor.z; float AXZ = currentState.inertiaTensor.w; float AXZS = AXZ * AXZ; float XPQ = AXZ * (AXX - AYY + AZZ); float GAM = AXX * AZZ - AXZS; float XQR = AZZ * (AZZ - AYY) + AXZS; float ZPQ = (AZZ - AYY) * AXX + AXZS; float YPR = AZZ - AXX; float rollAccel = ((XPQ * PQ) - (XQR * QR) + (AZZ * ROLL) + (AXZ * (YAW + QHX))) / GAM; float pitchAccel = ((YPR * P * R) - (AXZ * (P * P - R * R)) + PITCH - (R * HX)) / AYY; float yawAccel = ((ZPQ * PQ) - (XPQ * QR) + (AXZ * ROLL) + (AXX * (YAW + QHX))) / GAM;&lt;/quote&gt;&lt;p&gt;Finally, the force and angular acceleration is returned:&lt;/p&gt;&lt;quote&gt;public AerodynamicForces CalculateAerodynamics(AerodynamicState currentState) { ... result.force = new Vector3(UDOT, VDOT, WDOT); result.angularAcceleration = new Vector3(rollAccel, pitchAccel, yawAccel); return result; }&lt;/quote&gt;&lt;p&gt;Then in the Plane class, the force and angular acceleration can be applied to the rigidbody:&lt;/p&gt;&lt;quote&gt;// aeroForces in pounds var forces = ConvertVectorToUnity(aeroForces) * poundsForceToNewtons; Rigidbody.AddRelativeForce(forces); // aeroAngularAcceleration changes angular velocity directly Vector3 avCorrection = ConvertAngleToUnity(aeroAngularAcceleration); Rigidbody.AddRelativeTorque(avCorrection, ForceMode.Acceleration); lastAngularAcceleration = avCorrection;&lt;/quote&gt;&lt;p&gt;The plane is now able to fly. But if you try flying it right now, you will quickly find that it is impossible to fly by hand.&lt;/p&gt;&lt;head rend="h1"&gt;Stability&lt;/head&gt;&lt;p&gt;One important aerodynamic effect not modeled in my previous flight sim project is stability. Stability is the behavior of an aircraft when it’s disturbed from it’s flight path. More specifically, it’s how the aircraft behaves when it’s nose vector doesn’t match it’s velocity vector. Stability is the force that pulls the nose vector back towards the velocity vector.&lt;/p&gt;&lt;p&gt;For most aircraft, stability is created by the stabilizers in the tail. A stabilizer is simply a small airfoil (wing). Even without the pilot giving input, the stabilizers act like the fins of a dart. As the plane increases it’s Angle of Attack, the horizontal stabilizer will produce a lift force at the rear of the plane. This creates a torque that pulls the plane’s nose back towards the velocity vector, thus reducing the AOA. Likewise, the vertical stabilizers will create a torque that reduces Angle of Slip.&lt;/p&gt;&lt;p&gt;Keep in mind that stability depends on AOA, just as lift does. When the aircraft has a large AOA, the wings produce lift, which brings the velocity vector towards the nose vector. The stabilizers create torque which brings the nose vector towards the velocity vector. These two forces balance out somewhere and the aircraft will take a new attitude with a new velocity.&lt;/p&gt;&lt;p&gt;However, the aircraft needs to maintain a non-zero AOA to create enough lift to fly straight and level. How does it maintain this AOA when stability works to reduce AOA to zero? The stabilizers can be trimmed to hold a specific AOA. This means that the stabilizers produce zero torque at this particular non-zero AOA. In some planes this must be done manually by the pilot, but in the F-16 this is done automatically by the FCS.&lt;/p&gt;&lt;p&gt;The two forces of lift and stability combine to produce the “feel” of an aircraft’s controls. The tendency for the nose to be pulled towards the velocity vector is called positive stability. Most non-fighter aircraft are designed to have positive stability to maximize safety and ease of flying.&lt;/p&gt;&lt;p&gt;But fighter aircraft like the F-16 are different. These aircraft are often designed to have neutral or even negative stability. Neutral stability means that the aircraft will hold it’s current attitude. Negative stability means that the aircraft will rotate even further away from the velocity vector, at an increasing rate.&lt;/p&gt;&lt;p&gt;The previous flight sim does not model this at all. There is no torque that changes the plane’s attitude except for the steering force. So the behavior is best described as neutrally stable.&lt;/p&gt;&lt;p&gt;This F-16 flight model does include stability. But keep in mind that the real F-16 was designed to have relaxed static stability. This means that it is positively stable, but weakly so. This makes the aircraft more maneuverable and better at retaining energy while turning. But flying an aircraft like this is difficult or even impossible for a human pilot. The plane will depart from steady flight from the smallest stick input or wind gust. The only way a human can handle an aircraft like this during long and stressful missions is with a computerized flight control system.&lt;/p&gt;&lt;head rend="h1"&gt;Flight Control System&lt;/head&gt;&lt;p&gt;A flight control system (FCS) is a computer located between the flight stick and the control surfaces. This computer translates the pilot’s input on the flight stick into control surface movement. It can react to disturbances in the plane’s attitude more quickly and precisely than a human can.&lt;/p&gt;&lt;p&gt;In my previous flight sim project, the control surfaces were purely cosmetic. The actual method used to turn the vehicle was by applying torque directly to the center of mass. That torque was calculated to create a certain amount of angular acceleration without exceeding the plane’s turn rate limit.&lt;/p&gt;&lt;p&gt;For example, the plane had a turn rate on the pitch axis of 60 degrees per second and an acceleration of 120 degrees per second per second. The plane’s turn rate never leaves the range [-60, 60]. Actually, no torque is ever calculated. Unity provides a function to apply angular acceleration directly, ignoring moment of inertia. I chose this behavior to make it easy to both understand the code and to fly the plane.&lt;/p&gt;&lt;p&gt;But this F-16 simulator does depend on the position of the control surfaces. Instead of specifying the acceleration directly, this simulator specifies the torque (moment) and calculates the resulting acceleration. This is more accurate to how serious simulators work and how real planes fly, but this makes controlling the plane more difficult.&lt;/p&gt;&lt;p&gt;The steering system in the previous flight sim project was essentially a perfect FCS that could always achieve the turn rate chosen by the pilot. This is helped by the fact that that simulator does not model aerodynamic stability or instability at all. Spinning out of control was simply not possible.&lt;/p&gt;&lt;p&gt;This F-16 simulator is more difficult to control both because of the more accurate control surfaces and because of the modeled stability. You can actually try to fly this F-16 manually, by disabling the FCS in the config menu.&lt;/p&gt;&lt;p&gt;You will quickly find that the F-16 is almost impossible to fly manually. Every small disturbance from straight and level flight will create small torques that turn your plane unexpectedly. If you try to correct it with the control stick, you will almost certainly overcorrect and send the plane into a new and exciting attitude. This is called pilot induced oscillation.&lt;/p&gt;&lt;p&gt;It simply isn’t possible for a human to react quickly and precisely enough to fly this aircraft. You may be able to fly straight and level with some effort, but you will quickly lose control if you attempt any maneuver. This is indeed a property of the real F-16.&lt;/p&gt;&lt;p&gt;The textbook provides no Fortran code for the FCS. From here on out, it’s my own original code.&lt;/p&gt;&lt;head rend="h2"&gt;PID Controllers&lt;/head&gt;&lt;p&gt;The steering system from the previous project cannot be reused. The solution is to use PID controllers, a topic I’ve covered on this blog before.11&lt;/p&gt;&lt;p&gt;To be more specific, steering in the previous flight sim was easy because we could read the angular velocity of the aircraft and apply a torque that directly countered any undesired movement. This F-16 flight model does not allow us to apply torques directly. We can only set the angle of the control surfaces. This is the problem that PID controllers are good at solving.&lt;/p&gt;&lt;p&gt;Adding the PID controllers is simple. The pilot’s control input is used to select a target angular velocity for the plane, for the 3 axes of rotation. This is given to three independent PID controllers. The output of the PID controllers set the target position for the control surface.&lt;/p&gt;&lt;p&gt;The control surface positions are then passed into the flight model inside AerodynamicState.&lt;/p&gt;&lt;quote&gt;Vector3 targetAV = Vector3.Scale(controlInput, steeringSpeed * steeringSpeedFactor); var accel = lastAngularAcceleration * Mathf.Rad2Deg * dt; controlSurfaceTarget = new Vector3( pitchController.Calculate(dt, av.x, accel.x, targetAV.x), -yawController.Calculate(dt, av.y, accel.y, targetAV.y), rollController.Calculate(dt, av.z, accel.z, targetAV.z) ); var current = ControlSurfaces; ControlSurfaces = new ControlSurfaces( Utilities.MoveTo(current.elevator, controlSurfaceTarget.x, elevatorSpeed, dt, -elevatorRange, elevatorRange), Utilities.MoveTo(current.rudder, controlSurfaceTarget.y, rudderSpeed, dt, -rudderRange, rudderRange), Utilities.MoveTo(current.aileron, controlSurfaceTarget.z, aileronSpeed, dt, -aileronRange, aileronRange) ); ... AerodynamicState currentState = new AerodynamicState { controlSurfaces = ControlSurfaces }; var newState = aerodynamics.CalculateAerodynamics(currentState);&lt;/quote&gt;&lt;p&gt;Here the PIDs are named “pitchController”, “yawController”, and “rollController”. They are all tuned separately to handle a single axis.&lt;/p&gt;&lt;p&gt;When the player releases the stick, the PID controllers will attempt to hold an angular velocity of zero. This makes the aircraft feel like it’s neutrally stable. This also acts as a way to trim the aircraft, so that level flight can be maintained without needing to constantly pull the stick. The PID controller will detect an undesired rotation and move the elevators at a slight angle to counter it.&lt;/p&gt;&lt;p&gt;These PID controllers only add a small amount of complexity to the code, but they achieve similar results as the perfect FCS from the previous project. But there are still limitations that prevent it from being a perfect FCS.&lt;/p&gt;&lt;p&gt;First, the PID controllers must be tuned. The output has to be strong enough to quickly respond to pilot inputs, while avoiding oscillation. This is of course a limitation of any PID control system.&lt;/p&gt;&lt;p&gt;Second, the control surfaces move at a finite speed. This means that it will take some time for the control surface to match the FCS’s commands. So the commands will be imperfectly applied to the aircraft.&lt;/p&gt;&lt;p&gt;Third, unlike the previous flight sim, the three axes of rotation are not independent. For example, a large angle of slip will cause the plane to roll. This is due to the swept wings of the F-16. The roll controller will cancel this out somewhat, but a large enough AOS will result in an uncommanded roll.&lt;/p&gt;&lt;p&gt;Even with these limitations, the PID controllers work fairly well at keeping the plane in control.&lt;/p&gt;&lt;p&gt;Additionally, I use a technique called gain scheduling to change the gain parameters of the roll controller. Because roll performance increases with airspeed, we need a way to limit the amount of aileron movement at high speed. I add two animation curves, which take speed as input, and give the P and D gain of the roll controller as output.&lt;/p&gt;&lt;quote&gt;rollController.P = rollPSchedule.Evaluate(Mathf.Max(0, LocalVelocity.z)); rollController.D = rollDSchedule.Evaluate(Mathf.Max(0, LocalVelocity.z)); Vector3 fcsTarget = new Vector3( pitchController.Calculate(dt, av.x, accel.x, targetAV.x), -yawController.Calculate(dt, av.y, accel.y, targetAV.y), rollController.Calculate(dt, av.z, accel.z, targetAV.z) );&lt;/quote&gt;&lt;p&gt;This allows us to change the strength of the roll controller at different speeds. A more advanced FCS might have a gain schedule for each controller, possibly using more inputs than just airspeed. In fact, if there were multiple inputs, we would need a 2D lookup table to calculate the gain schedule.&lt;/p&gt;&lt;p&gt;Because the flight model is a complete description of how the aircraft will respond at different combinations of AOA, AOS, and control input, it is theoretically possible to design an FCS system that perfectly counters all of the unwanted tendencies. However, that is beyond my understanding of aerodynamics and control theory.&lt;/p&gt;&lt;head rend="h2"&gt;G and AOA Limiter&lt;/head&gt;&lt;p&gt;The weakness of PID controllers is that they only control the angular velocity of the plane. This is not sufficient to control the plane. The previous project has a G limiter, which is simple since steering torque is applied directly to the aircraft. Adding a G limiter is more complicated with this F-16 flight model.&lt;/p&gt;&lt;p&gt;Additionally, a critical part of the FCS on a real F-16 is the AOA limiter. Just like the G limiter prevents the pilot from creating excessive G-forces while maneuvering, the AOA limiter prevents excessive AOA. This is because the aircraft becomes so unstable at about 28 degrees AOA that even the FCS can not compensate. And importantly, our flight model only supports a limited range of AOA (up to 45 degrees), so if the pilot goes beyond that, the behavior of the simulator becomes nonsensical. So limiting the AOA to about 25 degrees is important for maintaining stable flight.&lt;/p&gt;&lt;p&gt;The previous flight sim project did not have anything like an AOA limiter. I simply tuned the steering strength so that AOA would not exceed about 15 degrees (unless stalling). And even then, there is no instability caused by high AOA, so nothing bad happens if the pilot exceeds that.&lt;/p&gt;&lt;p&gt;We need a system that prevents the pilot from exceeding 25 degrees AOA. This would be implemented as a multiplier on the pilot’s stick input, just like a G limiter. Since there are two limits, we simply select the more restrictive limit using min(). So if the G limiter says to limit input to 0.75 and the AOA limiter says to limit input to 0.5, the value 0.5 is chosen.&lt;/p&gt;&lt;p&gt;Because this flight model uses lookup tables, there is no simple formula for calculating either the G limiter or AOA limiter. The G limiter from the previous project won’t work here. Additionally, the relationship between steering input and AOA is not simple. There is a feedback loop between AOA and lift. As AOA increases, lift increases. But as lift increases, AOA decreases since lift pulls the plane onto a new velocity vector. Not to mention lift also depends on airspeed and altitude.&lt;/p&gt;&lt;p&gt;Luckily, the F-16 flight model is completely disconnected from Unity’s physics system. We can actually run the flight model as much as we want with any inputs, and use the outputs for any purpose. There is the “main” flight model that syncs with a Unity rigidbody. But we can create “side” flight models to predict future behavior of the plane.&lt;/p&gt;&lt;p&gt;I chose to implement the G and AOA limiters by running a side flight model. This side model takes the pilot’s inputs and simulates the aircraft in a simplified world state. In a single physics update, the main flight model runs once, but the side flight model runs multiple times to predict movement several seconds into the future. Because running the flight model is a few lookups and math operations, running multiple times per frame is dirt cheap.&lt;/p&gt;&lt;p&gt;By running this side model, we can determine how the plane would behave if it flew without any limiters. So if the plane is flying fast enough to pull 16 Gs, the side model will report that. We can use that information to calculate the G limiter for the main model.&lt;/p&gt;&lt;p&gt;The side model is contained in the class SimpleTrimmer. The main function Trim looks like this:&lt;/p&gt;&lt;quote&gt;public SimulatedState Trim(float dt, float timeMax, SimulatedState initialState) { float time = 0; while (time &amp;lt; timeMax) { AerodynamicState aeroState = new AerodynamicState() { ... }; var aeroForces = aerodynamics.CalculateAerodynamics(aeroState); … time += dt; } return state; }&lt;/quote&gt;&lt;p&gt;It just calls CalculateAerodynamics in a loop with it’s own time variable. The timestep can also be different from the main FixedUpdate loop time step. The variable timeMax controls how far into the future the prediction runs. For example, this side model can run at 0.1 second time steps for 5 seconds total.&lt;/p&gt;&lt;p&gt;After one step of the simulation is run, the state variables are updated and fed back into the next step. The maximum G force and AOA of the whole simulation is recorded.&lt;/p&gt;&lt;quote&gt;// rotate velocity by pitchDelta Quaternion newRotation = Quaternion.Euler(0, pitchDelta, 0); Vector3 newVelocity = newRotation * state.velocity; newVelocity.y = 0; newVelocity.z += gravity * dt; Vector3 velNormalized = newVelocity.normalized; // assume airspeed magnitude does not change (no drag, no thrust) state.velocity = velNormalized * airspeed; state.alpha = Mathf.Atan2(velNormalized.z, velNormalized.x) * Mathf.Rad2Deg; state.maxAlpha = Mathf.Max(state.maxAlpha, state.alpha); state.maxAccelerationZ = Mathf.Min(state.maxAccelerationZ, state.acceleration.z);&lt;/quote&gt;&lt;p&gt;This simulation is highly simplified compared to the main flight model. It ignores the pilot’s input except pitch. It ignores angular velocity except for pitch rate. It does not apply drag or any other force that changes airspeed or altitude. It ignores any change to the aircraft’s pitch. Note that the flight model does not care about the orientation of the aircraft to begin with.&lt;/p&gt;&lt;p&gt;The Trim function assumes the pilot is giving a full pitch up or pitch down input and takes the pitch PID controller as a parameter. So this side flight model uses the same PID values as the main model, to prevent the simulation from turning faster than the max turn rate. Since the I term is not used on the PID controller, we can use it without worrying about state.&lt;/p&gt;&lt;p&gt;Gravity as a single float value is also passed as a parameter. This allows the simulation to know how much gravity is affecting the turn on the pitch axis. If the plane is level, this value is 1. If the plane is rolled 90 degrees to the side, this value is 0. If upside down, this value is -1. Gravity on the other axes is ignored.&lt;/p&gt;&lt;p&gt;The larger time step and reduced complexity of simulation means that the side model is not completely accurate to how the plane will fly. But that’s acceptable since we are only using this to estimate the maximum G force and AOA that a turn might create.&lt;/p&gt;&lt;p&gt;After running through a few seconds of simulation on the flight model, the Trim function returns with the max G force and AOA. The FCS then uses these values to calculate the limiting factors for the pilot’s pitch input.&lt;/p&gt;&lt;quote&gt;SimpleTrimmer.SimulatedState state = simpleTrimmer.Trim( trimmerTimeStep, trimmerTime, initialState, maxAV.x * Mathf.Deg2Rad, gravityFactor * metersToFeet, pitchController, centerOfGravityPosition ); float predictedAlpha = state.maxAlpha; float predictedG = -state.maxAccelerationZ * feetToMeters; float aoaPitchMult = CalculateAOALimiter(predictedAlpha); float gLimit = gLimitPitch; // pitch up limit (ie 8G) if (controlInput.x &amp;gt; 0) { gLimit = this.gLimit; // pitch down limit (ie 4G) } float gPitchMult = CalculateGLimiter(predictedG, gLimit);&lt;/quote&gt;&lt;p&gt;The limiting factor for AOA and G force is calculated with a simple function:&lt;/p&gt;&lt;quote&gt;float ApplyLimiter(float value, float limit, float limitStrength) { if (limit &amp;lt;= 0) return 1; if (value &amp;lt; limit) return 1; float error = value - limit; error *= limitStrength; return limit / (limit + error); }&lt;/quote&gt;&lt;p&gt;ApplyLimiter returns a factor in the range [0, 1], which is eventually multiplied with the pilot’s control input. This function then used in the limiter functions:&lt;/p&gt;&lt;quote&gt;float CalculateGLimiter(float predictedG, float gLimit) { float gForce = predictedG / 9.81f; float gPitchMult = ApplyLimiter(gForce, gLimit, gLimitStrength); return gPitchMult; }&lt;/quote&gt;&lt;p&gt;The variable gForce is the predicted max G force from the side model. gLimit is the value chosen as the max G force, for example, 8. If the predicted value is 12, then the variable error will be 12 – 8 = 4. The returned factor would be 8 / (8 + 4) = 8 / 12 = 0.66. limitStrength is used to tune how strongly the error affects the returned limit factor.&lt;/p&gt;&lt;p&gt;If the value is below the limit, the returned factor is 1.&lt;/p&gt;&lt;p&gt;The AOA limiter uses the same function to calculate two limiting factors which are combined:&lt;/p&gt;&lt;quote&gt;float CalculateAOALimiter(float predictedAlpha) { float aoaPitchMult = 1.0f; aoaPitchMult *= ApplyLimiter(predictedAlpha, predictedAoaLimitMax, predictedAoaLimitStrength); float realAOA = AngleOfAttack * Mathf.Rad2Deg; aoaPitchMult *= ApplyLimiter(realAOA, feedbackAoaLimitMax, feedbackAoaLimitStrength); return aoaPitchMult; }&lt;/quote&gt;&lt;p&gt;One limit factor depends on the predicted alpha from the SimpleTrimmer class. The other factor depends on the actual alpha value the plane currently has. This can handle cases where the real alpha is much larger than the predicted value, such as when the plane is already stalling.&lt;/p&gt;&lt;p&gt;Then the AOA and G limiter factors are applied to the pilot’s input:&lt;/p&gt;&lt;quote&gt;float aoaPitchMult = CalculateAOALimiter(predictedAlpha); float gPitchMult = CalculateGLimiter(predictedG, gLimitPitch); float pitchMult = Mathf.Min(aoaPitchMult, gPitchMult); // select whichever limiter is stronger float rollMult = rollPitchFactor.Evaluate(Mathf.Abs(controlInput.x)) * rollAOAFactor.Evaluate(AngleOfAttack * Mathf.Rad2Deg); Vector3 limitedInput = Vector3.Scale(controlInput, new Vector3(pitchMult, 1, rollMult)); Vector3 targetAV = Vector3.Scale(limitedInput, steeringSpeed * steeringSpeedFactor);&lt;/quote&gt;&lt;p&gt;The min() function is used to select whichever limiter factor is strongest. Since I am designing these systems myself, I can tell you there is not a strong reason why I chose min() instead of another multiplication. This is just the formula that felt right when I was testing it.&lt;/p&gt;&lt;p&gt;In fact there are many different ways that the limiting factors could be calculated and combined. I designed the ApplyLimiter function primarily to be easy to tune. These allow me to have separate variables for tuning predicted G, predicted AOA, and feedback AOA limiters.&lt;/p&gt;&lt;p&gt;There is one final limiter above, rollMult. This is controlled by two AnimationCurves, rollPitchFactor and rollAOAFactor. These curves reduce the strength of roll input when the pilot is commanding a strong pitch rotation and when the plane has a high AOA. I added this because rolls felt too sensitive when in a high G or high AOA turn. Tune these to your own taste.&lt;/p&gt;&lt;head rend="h2"&gt;Stick Pusher&lt;/head&gt;&lt;p&gt;The final system to add is a stick pusher. A stick pusher is a device some aircraft have that physically pushes the stick forward to avoid a stall. This doesn’t exist in the real F-16, even digitally, but who cares? It was quick and easy to write.&lt;/p&gt;&lt;p&gt;If the AOA exceeds some threshold, a bias value is added to the pilot’s stick input to push the nose down. This is different from the AOA limiter above, which multiplies the input by a factor [0, 1]. If the pilot is giving an input of 0, then the AOA limiter has no effect. The stick pusher adds the bias value to the pilot’s input, so it will work even when the pilot gives no input.&lt;/p&gt;&lt;p&gt;The stick pusher will apply when the plane is stalling or if the AOA limiter fails to keep the AOA in the safe range.&lt;/p&gt;&lt;p&gt;The code for this is incredibly simple in concept and implementation:&lt;/p&gt;&lt;quote&gt;float CalculateAOAPusher() { float bias = 0.0f; float aoa = AngleOfAttack * Mathf.Rad2Deg; if (aoa &amp;gt; stickPusherThreshold) { float error = aoa - stickPusherThreshold; bias = stickPusherCurve.Evaluate(error); } return Mathf.Min(stickPusherMax, bias); }&lt;/quote&gt;&lt;p&gt;If the AOA is over the stickPushThreshold, add a bias to the player’s input. The more it exceeds the threshold, the stronger the bias. At max strength, the stick pusher can give a full nose down input that can’t be overridden by the pilot.&lt;/p&gt;&lt;p&gt;This value is summed with the pilot’s input before running the PID controllers.&lt;/p&gt;&lt;quote&gt;Vector3 stickPusher = new Vector3(CalculateAOAPusher(), 0, 0); Vector3 limitedInput = Vector3.Scale(controlInput, new Vector3(pitchMult, 1, rollMult)) + stickPusher; Vector3 targetAV = Vector3.Scale(limitedInput, steeringSpeed * steeringSpeedFactor);&lt;/quote&gt;&lt;p&gt;With all of these systems added to the FCS, the plane should be very stable to fly now. Since the side model simulation is simplified, the G and AOA limiters are not perfect. They will sometimes result in those parameters being limited at a value too high or too low. But these systems do work accurately enough to keep the plane stable.&lt;/p&gt;&lt;head rend="h1"&gt;Testing&lt;/head&gt;&lt;p&gt;Of course any implementation can have bugs. We need to test the flight model to make sure it works. This includes the translation of the Fortran flight model, and the code that implements all of this in Unity.&lt;/p&gt;&lt;head rend="h2"&gt;Unit Testing&lt;/head&gt;&lt;p&gt;Because the flight model is separate from Unity’s physics engine, we can actually test it using normal unit testing techniques. Unity provides a unit testing framework based on NUnit, so testing is pretty typical for C#.&lt;/p&gt;&lt;p&gt;The authors of the textbook also helpfully provide a test case to use. They give the inputs to the model (airspeed, control surface position, throttle, etc) and the expected output of the model (forces, moment, angular velocity, etc). This lets us validate that the model is implemented correctly by running a single step of simulation.&lt;/p&gt;&lt;quote&gt;// Textbook provides a table of input values and the expected output // Index Param Input State (X) Output (XD) // 1 0.4 (XCG) 0.9 (throttle) 500 (vt) -75.23724 // 2 20 (elevator) 0.5 (alpha) -0.8813419 // 3 -15 (aileron) -0.2 (beta) -0.4759990 // 4 -20 (rudder) -1 (phi) // 5 1 (theta) // 6 -1 (psi) // 7 0.7 (P) 12.62679 // 8 -0.8 (Q) 0.9649671 // 9 0.9 (R) 0.5809759 // 10 1000 (north) // 11 900 (east) // 12 10000 (alt) 248.1241 // 13 90 (power) -58.68999&lt;/quote&gt;&lt;p&gt;Note that the output values for roll, pitch, and yaw, and north, east, and altitude, are not checked in this test. We are using the Unity rigidbody to handle these, so these values are not even calculated in C#.&lt;/p&gt;&lt;p&gt;Additionally, the table lookup operations are fairly simple C# code, so these functions can also be unit tested. I caught a few bugs in the flight model by adding these tests.&lt;/p&gt;&lt;p&gt;All of the tests are in the ModelTestCase class.&lt;/p&gt;&lt;head rend="h2"&gt;Flight Testing&lt;/head&gt;&lt;p&gt;Of course unit testing can only cover so much. The whole point of this project is to create a flight simulator. The only way to know how the flight model really feels is to fly it. So get out there and start flying it!&lt;/p&gt;&lt;p&gt;I have caught a few bugs in the implementation just by flying it and realizing that some aspect felt weird.&lt;/p&gt;&lt;p&gt;In the aerospace industry, test flights are thoroughly instrumented to gather as much data as possible. Force on every axis, angular velocity, pilot input, GPS track, etc is all recorded and stored for future analysis. It’s possible to write automated tests that read this data and check that values stay within expected bounds.&lt;/p&gt;&lt;p&gt;I have done none of that here. Just have fun flying 🙂&lt;/p&gt;&lt;head rend="h1"&gt;Limitations&lt;/head&gt;&lt;p&gt;The flight model defined in the textbook has several limitations.&lt;/p&gt;&lt;p&gt;The effects of alpha on the flight model is only modeled for the range [-10, 45]. Beta is only modeled for the range [-30, 30]. The flight model supports extrapolating data tables beyond their defined ranges, but the returned values will quickly become nonsensical. This means that if you manage to fly the F-16 beyond the provided ranges for alpha and beta, the flight model will break down and begin behaving non-physically.&lt;/p&gt;&lt;p&gt;In some cases, the aircraft will eventually return to controlled flight. But in other cases, one bad data value used to query the tables will cause increasingly bad data to be stored to the plane’s state. These bad values will quickly grow until the plane is thrown to infinity.&lt;/p&gt;&lt;p&gt;Hopefully, this is not possible when using FCS that I’ve written. But I encourage any readers to try breaking it themselves.&lt;/p&gt;&lt;p&gt;You can also turn off parts of the FCS using the config menu in the top left corner. This allows you to fly the plane completely manually, turn the engine off, or alter the center of gravity.&lt;/p&gt;&lt;p&gt;If the flight model doesn’t bug out from extreme values, then you can actually perform a backflip or a “Kulbit” maneuver with the F-16. I recommend turning off only the pitch axis FCS if you want to try that.&lt;/p&gt;&lt;p&gt;Another limitation is that flaps and slats are not defined in the flight model. The real F-16 uses a single control surface called a “flaperon” that works as both a flap and an aileron. When more lift is needed at low speeds, both flaperons deflect downwards like traditional flaps. Leading edge slats also deflect downwards to increase lift.&lt;/p&gt;&lt;p&gt;The textbook flight model only considers these control surfaces to be ailerons. That is, they always deflect in opposite directions in order to create a roll moment. Only a single “aileron” value is used to represent both left and right, so they cannot be used as flaps. If they were to be used as flaps, then there would need to be a left aileron and right aileron value and the Z axis force coefficient would depend on flaperon position.&lt;/p&gt;&lt;p&gt;The effect of slat position is blended into the existing tables, so there is some effect of slats on Z axis force. But the slat position cannot be animated on the plane’s 3D model since no variable for it exists.&lt;/p&gt;&lt;p&gt;This means that there are reduced high lift devices on the aircraft. The extra lift from flaps cannot be modeled. So the plane’s takeoff speed is much higher than you might expect from the F-16. The textbook only defines a model for flight, not for taxiing or takeoff. Landings feel quite bad because of this.&lt;/p&gt;&lt;p&gt;Another limitation is the lack of landing gear simulation. The landing gear is implemented exactly the same as the previous project: three capsule colliders. There is no simulation of wheel, tire, or suspension behavior. Again, this makes takeoff and landing feel kind of weird. But I have no idea how to write a system like that and it’s out of scope for this project anyways.🤷♂️&lt;/p&gt;&lt;p&gt;Another limitation of the flight model is the inaccuracy when flying super sonic. With real planes, lift and drag forces change drastically as you approach Mach 1. Air accelerates as it passes over the wing. Even while the plane remains subsonic, some parts of the air flow are forced to accelerate above Mach 1. When this air reaches supersonic speeds, shockwaves form over the wing which alters the way air flows around it.&lt;/p&gt;&lt;p&gt;This region, where some air is supersonic and some is not, is called the transonic region. This has a drastic effect on the aircraft’s performance and handling. In particular, the coefficient of drag increases, creating the “sound barrier” effect. The position of lift force on the wing changes, which will change how the plane handles.&lt;/p&gt;&lt;p&gt;None of these effects are included in the textbook’s flight model. These could be modeled by adding another input dimension to the force and moment tables. I suspect these were omitted to keep the flight model simple.&lt;/p&gt;&lt;p&gt;The practical effect is that the flight model only works up to about Mach 0.7. Above that, all of the forces on the aircraft become unrealistic. The behavior of the plane continues to increase smoothly with airspeed as if supersonic effects don’t exist.&lt;/p&gt;&lt;head rend="h1"&gt;Conclusion&lt;/head&gt;&lt;p&gt;I started this project after I got a job in the aerospace industry. The textbook was recommended by my manager, since I was working on real flight control systems. In a way, this article is a summary of everything I’ve learned about flying and software engineering in that job.&lt;/p&gt;&lt;p&gt;The way this F-16 flight model is implemented is very different from my previous project. It is actually close to how professional level sims are written, though simplified to fit in a textbook. Even so, there are still plenty of limitations in the flight model which means the simulation will behave unrealistically beyond the intended flight envelope.&lt;/p&gt;&lt;p&gt;The authors of the textbook based their flight model on a NASA paper12 which measured the aerodynamic properties of a scale model in a wind tunnel. The Nasa paper provides 50 lookup tables. The textbook simplified, approximated, and combined these into only 13 lookup tables.&lt;/p&gt;&lt;p&gt;With only a little more effort, you could write a simulator that uses many more tables to cover a larger flight envelope with more detail. The only limit is the data you have access to and your understanding of aerodynamics.&lt;/p&gt;&lt;p&gt;The FCS I’ve written is much simpler than the real FCS. Theoretically, it would be possible to write code that models the real F-16 FCS and apply it to this flight simulator. But how could you even get that information and who would be crazy enough to try that?&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;“Aircraft Control and Simulation” by Brian L. Stevens, Frank L. Lewis, and Eric N. Johnson ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/346/flight-simulator-in-unity3d-part-1/ ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/467/flight-simulator-in-unity3d-part-2/ ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/503/creating-a-flight-simulator-in-unity3d-part-3/ ↩︎&lt;/item&gt;&lt;item&gt;https://twitter.com/FreyaHolmer/status/1325556229410861056 ↩︎&lt;/item&gt;&lt;item&gt;https://commons.wikimedia.org/wiki/File:Speyer_Handlog.jpg ↩︎&lt;/item&gt;&lt;item&gt;https://www.grc.nasa.gov/www/k-12/VirtualAero/BottleRocket/airplane/sound.html ↩︎&lt;/item&gt;&lt;item&gt;https://en.wikipedia.org/wiki/Bilinear_interpolation ↩︎&lt;/item&gt;&lt;item&gt;https://en.wikipedia.org/wiki/Trilinear_interpolation ↩︎&lt;/item&gt;&lt;item&gt;https://aerospaceweb.org/question/aerodynamics/q0194.shtml ↩︎&lt;/item&gt;&lt;item&gt;https://vazgriz.com/621/pid-controllers/ ↩︎&lt;/item&gt;&lt;item&gt;Simulator study of stall/post-stall characteristics of a fighter airplane with relaxed longitudinal static stability, Nyugen et al, 1979 (https://ntrs.nasa.gov/citations/19800005879) ↩︎&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vazgriz.com/762/f-16-flight-sim-in-unity-3d/"/><published>2025-09-26T07:06:45+00:00</published></entry></feed>