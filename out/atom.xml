<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-15T13:54:56.232356+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46619614</id><title>Show HN: Sparrow-1 – Audio-native model for human-level turn-taking without ASR</title><updated>2026-01-15T13:57:23.772872+00:00</updated><content>&lt;doc fingerprint="34dbeb9a0b368fab"&gt;
  &lt;main&gt;
    &lt;p&gt;All Posts&lt;/p&gt;
    &lt;p&gt;Sparrow-1 is a specialized, multilingual audio model for real-time conversational flow and floor transfer. It predicts when a system should listen, wait, or speak, enabling response timing that mirrors human conversation rather than simply responding as fast as possible.&lt;/p&gt;
    &lt;p&gt;Despite major advances in LLMs and TTS, conversational AI still lacks reliable human-level timing. Traditional voice systems wait for silence, then respond. Sparrow-1 instead models conversational timing continuously. This allows it to respond quickly, even instantaneously when the speaker is clearly done, all while deliberately waiting when theyâre not.&lt;/p&gt;
    &lt;p&gt;The difference is subtle but transformative: Sparrow-1 doesn't just respond as fast as possible. It responds at the moment a human listener would.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timing Is the Hard Part&lt;/head&gt;
    &lt;p&gt;Conversation is not just an exchange of words. It is a real-time coordination task where participants continuously anticipate when to respond, drawing on rhythm, hesitation, intonation, and meaning at the same time. Sparrow-1 models this coordination directly, aligning its behavior with the timing patterns humans use subconsciously during dialogue.&lt;/p&gt;
    &lt;p&gt;Research in conversation analysis and psycholinguistics has identified several key categories of signals that govern conversational-flow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Semantic completeness: whether an utterance constitutes a complete thought, question, or request that projects a relevant response.&lt;/item&gt;
      &lt;item&gt;Lexical structure: grammatical structure and speech act boundaries that create transition-relevance places.&lt;/item&gt;
      &lt;item&gt;Prosodic boundary markers: pitch contours, lengthening, and intensity changes that signal utterance completion.&lt;/item&gt;
      &lt;item&gt;Disfluencies and hesitation phenomena: filled pauses, false starts, and repairs that indicate ongoing cognitive processing.&lt;/item&gt;
      &lt;item&gt;Non-verbal cues are invisible to text: Transcription-based models discard sighs, throat-clearing, hesitation sounds, and other non-verbal vocalizations that carry critical conversational-flow information. Sparrow-1 hears what ASR ignores.&lt;/item&gt;
      &lt;item&gt;Overlap management: the negotiation of simultaneous speech, which occurs in approximately 40% of turn transitions.&lt;/item&gt;
      &lt;item&gt;Affective silences: pauses that carry emotional or pragmatic weight distinct from planning delays.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;When timing fails in conversational AI&lt;/head&gt;
    &lt;p&gt;When you talk to an AI with a human voice, you expect human timing. When timing breaks down, you notice immediately. Delayed responses, premature interruptions, and awkward pauses shatter the rhythm of natural dialogue.&lt;/p&gt;
    &lt;p&gt;Todayâs voice AI sounds increasingly human, yet still feels mechanical in conversation. Systems on platforms like ChatGPT, Claude, and Grok decide when to speak using endpoint detection, waiting for silence thresholds before responding. They react to the absence of sound rather than conversational intent, leading to missed hesitation cues and poorly timed responses. The voice sounds real, but the interaction does not.&lt;/p&gt;
    &lt;p&gt;Human-level conversation requires more:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Near-instant response when intent is clear&lt;/item&gt;
      &lt;item&gt;Contextual fluidity that adapts to pacing and tone&lt;/item&gt;
      &lt;item&gt;Graceful handling of interruptions, backchannels, and overlapping speech&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most systems fall short since they treat conversational-flow as an afterthought, a threshold to tune rather than a problem to model.Â &lt;lb/&gt;Sparrow-1 takes a different approach: it models humanlike floor transfer with intent, timing, and tone, ensuring that conversational timing matches the realism of the voice delivering it.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Sparrow-1?&lt;/head&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;p&gt;Sparrow-1 is a conversational flow control model built for real-time conversational video in Tavusâs Conversational Video Interface. It treats timing as a first-class modeling problem rather than an artifact of endpoint detection, extending Sparrow-0 with a more capable architecture and richer supervision.&lt;/p&gt;
    &lt;p&gt;Most existing turn-taking systems are built around endpoint detection. They wait for speech to stop, apply silence thresholds, and then trigger a response. This reactive design introduces latency, misinterprets hesitation as turn completion, and fails to support natural conversational behaviors such as backchanneling, overlap, and interruption. Silence is treated as a proxy for intent, even though the absence of speech does not reliably signal that a speaker has yielded the conversational floor.&lt;/p&gt;
    &lt;p&gt;Sparrow-1 takes a different approach. Instead of asking whether speech has ended, it models who owns the conversational floor at every moment, allowing it to anticipate turn transitions rather than react to them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core Properties and Capabilities&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Audio-native, streaming-first: Operates directly on continuous audio with persistent state, preserving prosody, rhythm, and timing cues that are lost in transcription-based systems.&lt;/item&gt;
      &lt;item&gt;Explicit floor ownership modeling: Predicts conversational floor ownership at frame-level granularity instead of relying on silence or fixed timeouts, enabling responses at the moment of handoff rather than after a delay buffer.&lt;/item&gt;
      &lt;item&gt;Trained on continuous utterances: Learns from real conversational streams where turn boundaries are probabilistic and context-dependent, reflecting the messiness of natural dialogue.&lt;/item&gt;
      &lt;item&gt;Designed for interruption and overlap: Actively reasons about hesitation, overlap, and mid-speech interruptions to decide whether to yield, pause, or continue speaking.&lt;/item&gt;
      &lt;item&gt;Speaker-adaptive in real time: Uses a recurrent architecture to converge on user-specific timing patterns within a single session, without explicit calibration or fine-tuning.&lt;/item&gt;
      &lt;item&gt;Optimized for latency and correctness: Responds immediately when intent is clear and deliberately waits when uncertainty remains, avoiding both interruptions and unnatural delays.&lt;/item&gt;
      &lt;item&gt;Enables speculative inference: Predicts floor transfer proactively, allowing downstream components to begin response generation before the user finishes speaking, committing or discarding output based on real-time floor predictions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;A new architecture for conversational flow&lt;/head&gt;
    &lt;p&gt;Sparrow-1 is not a general language model or even strictly a turn-taking model. It is a timing and control system that governs when a conversational system should speak, wait, or get out of the way: a conversational-flow model&lt;/p&gt;
    &lt;p&gt;This distinction matters because conversational timing is not handled cleanly by most real-time voice architectures. Today, two dominant approaches exist:&lt;/p&gt;
    &lt;p&gt;End-to-end speech-to-speech models handle timing implicitly but are expensive, opaque, and difficult to control or customize. They achieve fluency by tightly coupling perception, reasoning, and generation, but sacrifice efficiency and controllability in the process.&lt;/p&gt;
    &lt;p&gt;Modular pipelines (ASR â LLM â TTS) are flexible and scalable but suffer from a coordination problem: timing decisions fall between components, with no dedicated mechanism for deciding when the system should speak.&lt;/p&gt;
    &lt;p&gt;Sparrow-1 fills this gap. By explicitly modeling conversational floor transfer as a standalone timing and control layer, it brings human-level conversational-flow to modular pipelines, preserving their flexibility while restoring the conversational feel users expect.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking Human ConversationÂ&lt;/head&gt;
    &lt;p&gt;Conversational-flow systems are often evaluated on clean endpoints and average latency, but these metrics are not representative of the true human dance, and miss the failures that matter most in real conversation: cutting users off, waiting too long, or behaving inconsistently during hesitation.Â&lt;/p&gt;
    &lt;p&gt;To evaluate these cases, we benchmarked Sparrow-1 against representative industry approaches using 28 challenging real world audio samples of real conversations designed to expose hesitation, overlap, and ambiguous turn endings, rather than clean silence.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Interpreting the Results&lt;/head&gt;
    &lt;p&gt;Each system was evaluated on the same set of 28 real-world conversational samples. Performance was measured across response latency, correct floor transfer, and interruptions. Correct floor transfer was measured using precision and recall within a 400ms grace window that reflects human conversational tolerance.&lt;/p&gt;
    &lt;p&gt;Correct floor transfer is quantified using precision and recall, with a 400ms grace window that reflects the tolerance humans naturally allow in conversation. Detections occurring within 400ms before a speaker finishes are treated as correct, while earlier responses are classified as interruptions. Precision captures how often a system avoids cutting users off, while recall measures how reliably it responds when a turn is actually complete.&lt;/p&gt;
    &lt;p&gt;Across existing approaches, the benchmark exposes a consistent speed and correctness tradeoff. Conservative systems minimize interruptions by waiting for extended silence, but impose multi-second delays that feel unnatural in dialogue. More aggressive systems reduce latency by lowering detection thresholds, but interrupt users frequently. In practice, systems are forced to choose between being slow or being wrong.&lt;/p&gt;
    &lt;p&gt;These results show that this tradeoff is not inherent to conversation, but a consequence of endpoint-based turn-taking design.&lt;/p&gt;
    &lt;head rend="h3"&gt;â&lt;/head&gt;
    &lt;head rend="h3"&gt;The Speed-Correctness Tradeoff&lt;/head&gt;
    &lt;p&gt;Existing systems force a choice between responsiveness and correctness:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Conservative approaches like LiveKit avoid most interruptions by waiting for extended silence, but impose unnatural delays. Median latency: 1504ms.&lt;/item&gt;
      &lt;item&gt;Aggressive approaches like Smart-Turn respond faster by lowering detection thresholds, but interrupt users frequently. Median latency: 237ms. Interruptions: 21 across 28 samples.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;â&lt;/head&gt;
    &lt;head rend="h3"&gt;Sparrow-1 Breaks the Tradeoff&lt;/head&gt;
    &lt;p&gt;Sparrow-1 avoids this compromise by responding quickly when a turn is complete and waiting when the user is still speaking, achieving both speed and correctness.&lt;/p&gt;
    &lt;p&gt;This performance reflects a fundamentally different approach. Sparrow-1 treats conversational flow as continuous, frame-level floor ownership prediction, aligning its behavior with human conversational timing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance and Latency&lt;/head&gt;
    &lt;p&gt;Human conversation optimizes for appropriateness, not speed. People respond quickly when intent is clear and wait when meaning is uncertain.&lt;/p&gt;
    &lt;p&gt;Because Sparrow-1 models conversational certainty directly, its response latency is dynamic. It responds in under 100ms when confident and waits during hesitation or trailing speech, typically producing response times of 200 to 500ms without multi-second delays.&lt;/p&gt;
    &lt;p&gt;This ability to be simultaneously fast and patient creates the perception of zero-latency conversation. The system responds not as quickly as possible, but at the moment it should.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;Modeling human-like turn-taking behavior&lt;/head&gt;
    &lt;p&gt;These design choices manifest as concrete runtime behaviors that govern how Sparrow-1 adapts, interrupts, and listens during live conversation. At runtime, turn-taking emerges from continuous speaker adaptation, interruption-aware control, and audio-native perception rather than fixed rules or thresholds. The result is behavior that closely matches how humans manage conversational flow in practice.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Adaptation without fine-tuning&lt;/head&gt;
    &lt;p&gt;Sparrow-1 behaves as a meta in-context learner, adapting to individual speaking patterns continuously as a conversation unfolds. Using a recurrent architecture, each 40ms frame updates internal state that encodes prosody, pacing, historical turn timing, and response latency preferences.&lt;/p&gt;
    &lt;p&gt;Early in a conversation, the model operates with higher uncertainty. As evidence accumulates, predictions sharpen around user-specific patterns, producing progressive synchronization without explicit calibration.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Interruption handling&lt;/head&gt;
    &lt;p&gt;Interruptions are treated as first-class conversational signals. Incoming speech during system output immediately pauses playback while the model continues evaluating floor ownership. If confidence rises, Sparrow-1 yields the turn. If not, it resumes speaking. This process distinguishes intentional interruptions from incidental overlap within tens of milliseconds without introducing delay.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Listening beyond words&lt;/head&gt;
    &lt;p&gt;Sparrow-1 models conversational intent using acoustic and temporal cues that extend beyond lexical content: interpreting not just what is said, but how it is said:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fillers and hesitations: Vocalizations such as "uh," "um," and partial restarts that signal cognitive load or turn-holding.&lt;/item&gt;
      &lt;item&gt;Trailing vocalizations: Soft completions, rising tones, or fading energy that indicate uncertainty or invite response.&lt;/item&gt;
      &lt;item&gt;Prosodic rhythm: Variations in pacing, pause structure, and intonation that distinguish finished thoughts from mid-utterance pauses.&lt;/item&gt;
      &lt;item&gt;Emotional cadence: Patterns in energy, timing, and speech continuity that reflect speaker engagement and conversational stance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By incorporating these paralinguistic signals into its floor predictions, Sparrow-1 aligns with how humans naturally infer attention, hesitation, and intent during conversation: resulting in listening that feels responsive rather than reactive.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;Access and Closing&lt;/head&gt;
    &lt;p&gt;We built Sparrow-1 as part of a broader mission: teaching machines to participate in human conversation. Our Conversational Video Interface (CVI) powers AI experiences that look, sound, and interact like real people: and poor timing breaks that illusion faster than almost anything else.&lt;/p&gt;
    &lt;p&gt;In conversational AI, the uncanny valley is rarely about what the AI says. It's about when it says it. Responses that arrive too early feel rude; too late, artificial. In conversational video, these errors are amplified, reminding users they're speaking to a system rather than a partner.&lt;/p&gt;
    &lt;p&gt;We use Sparrow-1 to solve this at the level it must be solved: as a first-class timing and control system. By modeling conversational uncertainty directly and responding with human-like precision, it enables interactions that feel attentive, patient, and natural.&lt;/p&gt;
    &lt;p&gt;Sparrow-1 is now available to GA across the Tavus APIs and platform, and already powers conversational experiences in the Tavus PALs and enterprise deployments.&lt;/p&gt;
    &lt;p&gt;Try the demo at tavus.io and learn more in our docs.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tavus.io/post/sparrow-1-human-level-conversational-timing-in-real-time-voice"/><published>2026-01-14T18:01:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46622328</id><title>Claude Cowork exfiltrates files</title><updated>2026-01-15T13:57:23.546816+00:00</updated><content>&lt;doc fingerprint="34890ebe6fffce0b"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;Claude Cowork Exfiltrates Files&lt;/head&gt;
    &lt;p&gt;Claude Cowork is vulnerable to file exfiltration attacks via indirect prompt injection as a result of known-but-unresolved isolation flaws in Claude's code execution environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context&lt;/head&gt;
    &lt;p&gt;Two days ago, Anthropic released the Claude Cowork research preview (a general-purpose AI agent to help anyone with their day-to-day work). In this article, we demonstrate how attackers can exfiltrate user files from Cowork by exploiting an unremediated vulnerability in Claudeâs coding environment, which now extends to Cowork. The vulnerability was first identified in Claude.ai chat before Cowork existed by Johann Rehberger, who disclosed the vulnerability â it was acknowledged but not remediated by Anthropic. &lt;lb/&gt;Anthropic warns users, âCowork is a research preview with unique risks due to its agentic nature and internet access.â Users are recommended to be aware of âsuspicious actions that may indicate prompt injectionâ. However, as this feature is intended for use by the general populace, not just technical users, we agree with Simon Willisonâs take:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âI do not think it is fair to tell regular non-programmer users to watch out for 'suspicious actions that may indicate prompt injectionâ!â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As Anthropic has acknowledged this risk and put it on users to âavoid granting access to local files with sensitive informationâ (while simultaneously encouraging the use of Cowork to organize your Desktop), we have chosen to publicly disclose this demonstration of a threat users should be aware of. By raising awareness, we hope to enable users to better identify the types of âsuspicious actionsâ mentioned in Anthropicâs warning.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;p&gt;This attack leverages the allowlisting of the Anthropic API to achieve data egress from Claude's VM environment (which restricts most network access).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The victim connects Cowork to a local folder containing confidential real estate files&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The victim uploads a file to Claude that contains a hidden prompt injection&lt;/p&gt;&lt;lb/&gt;For general use cases, this is quite common; a user finds a file online that they upload to Claude code. This attack is not dependent on the injection source - other injection sources include, but are not limited to: web data from Claude for Chrome, connected MCP servers, etc. In this case, the attack has the file being a Claude âSkillâ (although, as mentioned, it could also just be a regular document), as it is a generalizable file convention that users are likely to encounter, especially when using Claude.&lt;lb/&gt;Note: If you are familiar with Skills, they are canonically Markdown files (which users often do not heavily scrutinize). However, we demonstrate something more interesting: here, the user uploads a .docx (such as may be shared on an online forum), which poses as a Skill - the contents appear to be Markdown that was just saved after editing in Word. In reality, this trick allows attackers to conceal the injection using 1-point font, white-on-white text, and with line spacing set to 0.1 â making it effectively impossible to detect.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The victim asks Cowork to analyze their files using the Real Estate âskillâ they uploaded&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The injection manipulates Cowork to upload files to the attackerâs Anthropic account&lt;/p&gt;&lt;lb/&gt;The injection tells Claude to use a âcurlâ command to make a request to the Anthropic file upload API with the largest available file. The injection then provides the attackerâs API key, so the file will be uploaded to the attackerâs account.&lt;lb/&gt;At no point in this process is human approval required.&lt;p&gt;If we expand the 'Running command' block, we can see the malicious request in detail:&lt;/p&gt;&lt;p&gt;Code executed by Claude is run in a VM - restricting outbound network requests to almost all domains - but the Anthropic API flies under the radar as trusted, allowing this attack to complete successfully.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The attackerâs account contains the victim's file, allowing them to chat with it&lt;/p&gt;
        &lt;p&gt;The exfiltrated file contains financial figures and PII, including partial SSNs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;A Note on Model-specific Resilience&lt;/head&gt;
    &lt;p&gt;The above exploit was demonstrated against Claude Haiku. Although Claude Opus 4.5 is known to be more resilient against injections, Opus 4.5 in Cowork was successfully manipulated via indirect prompt injection to leverage the same file upload vulnerability to exfiltrate data in a test that considered a 'user' uploading a malicious integration guide while developing a new AI tool:&lt;/p&gt;
    &lt;p&gt;As the focus of this article was more for everyday users (and not developers), we opted to demonstrate the above attack chain instead of this one.&lt;/p&gt;
    &lt;head rend="h3"&gt;DOS via Malformed Files&lt;/head&gt;
    &lt;p&gt;An interesting finding: Claude's API struggles when a file does not match the type it claims to be. When operating on a malformed PDF (ends .pdf, but it is really a text file with a few sentences in it), after trying to read it once, Claude starts throwing an API error in every subsequent chat in the conversation.&lt;/p&gt;
    &lt;p&gt;We posit that it is likely possible to exploit this failure via indirect prompt injection to cause a limited denial of service attack (e.g., an injection can elicit Claude to create a malformed file, and then read it). Uploading the malformed file via the files API resulted in notifications with an error message, both in the Claude client and the Anthropic Console.&lt;/p&gt;
    &lt;head rend="h3"&gt;Agentic Blast Radius&lt;/head&gt;
    &lt;p&gt;One of the key capabilities that Cowork was created for is the ability to interact with one's entire day-to-day work environment. This includes the browser and MCP servers, granting capabilities like sending texts, controlling one's Mac with AppleScripts, etc. &lt;lb/&gt;These functionalities make it increasingly likely that the model will process both sensitive and untrusted data sources (which the user does not review manually for injections), making prompt injection an ever-growing attack surface. We urge users to exercise caution when configuring Connectors. Though this article demonstrated an exploit without leveraging Connectors, we believe they represent a major risk surface likely to impact everyday users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files"/><published>2026-01-14T20:12:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624352</id><title>The State of OpenSSL for pyca/cryptography</title><updated>2026-01-15T13:57:23.384050+00:00</updated><content>&lt;doc fingerprint="f2d8f89a348b06d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The State of OpenSSL for &lt;code&gt;pyca/cryptography&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Published: January 14, 2026&lt;/p&gt;
    &lt;p&gt;For the past 12 years, we (Paul Kehrer and Alex Gaynor) have maintained the Python &lt;code&gt;cryptography&lt;/code&gt; library (also known as &lt;code&gt;pyca/cryptography&lt;/code&gt; or cryptography.io). For that entire period, we’ve relied on OpenSSL to provide core cryptographic algorithms. This past October, we gave a talk at the OpenSSL Conference describing our experiences. This talk focuses on the growing problems we have with OpenSSL’s direction. The mistakes we see in OpenSSL’s development have become so significant that we believe substantial changes are required — either to OpenSSL, or to our reliance on it.&lt;/p&gt;
    &lt;p&gt;Fundamentally, OpenSSL’s trajectory can be understood as a play in three acts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In the pre-Heartbleed era (pre-2014), OpenSSL was under-maintained and languishing, substantially lagging behind expectations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the immediate post-Heartbleed era, OpenSSL’s maintenance was reinvigorated and it made substantial progress and improvements. It grew a real code review process, began running tests in CI, adopted fuzz testing, and matured its release process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, in 2021 OpenSSL 3 was released. OpenSSL 3 introduced new APIs and had large internal refactors. Relative to previous OpenSSL versions, OpenSSL 3 had significant regressions in performance, complexity, API ergonomics, and didn’t make needed improvements in areas like testing, verification, and memory safety. Over the same period, OpenSSL’s forks have all made progress in these areas. Many of our concerns about OpenSSL’s direction in this time have substantial overlap with those highlighted by HAProxy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The remainder of this post describes the problems we have with OpenSSL in more detail, and concludes with the changes we are making to our own policies in response. To avoid burying the lede, we intend to pursue several approaches to reducing our reliance on OpenSSL.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;Compared to OpenSSL 1.1.1, OpenSSL 3 has significant performance regressions in areas such as parsing and key loading.&lt;/p&gt;
    &lt;p&gt;Several years ago, we filed a bug reporting that elliptic curve public key loading had regressed 5-8x between OpenSSL 1.1.1 and 3.0.7. The reason we had noticed this is that performance had gotten so bad that we’d seen it in our test suite runtimes. Since then, OpenSSL has improved performance such that it’s only 3x slower than it used to be. But more significantly, the response to the issue was that, ‘regression was expected with OpenSSL 3, and while there might be some optimizations, we shouldn’t expect it to ever get back to 1.1.1 levels’. Performance regressions can be acceptable, and even appropriate, when they improve other areas of the library, however as we’ll describe, the cause of these regressions has been other mistakes, and not offsetting improvements.&lt;/p&gt;
    &lt;p&gt;As a result of these sorts of regressions, when &lt;code&gt;pyca/cryptography&lt;/code&gt; migrated X.509 certificate parsing from OpenSSL to our own Rust code, we got a 10x performance improvement relative to OpenSSL 3 (n.b., some of this improvement is attributable to advantages in our own code, but much is explainable by the OpenSSL 3 regressions). Later, moving public key parsing to our own Rust code made end-to-end X.509 path validation 60% faster — just improving key loading led to a 60% end-to-end improvement, that’s how extreme the overhead of key parsing in OpenSSL was.&lt;/p&gt;
    &lt;p&gt;The fact that we are able to achieve better performance doing our own parsing makes clear that doing better is practical. And indeed, our performance is not a result of clever SIMD micro-optimizations, it’s the result of doing simple things that work: we avoid copies, allocations, hash tables, indirect calls, and locks — none of which should be required for parsing basic DER structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complexity and APIs&lt;/head&gt;
    &lt;p&gt;OpenSSL 3 started the process of substantially changing its APIs — it introduced &lt;code&gt;OSSL_PARAM&lt;/code&gt; and has been using those for all new API surfaces (including those for post-quantum cryptographic algorithms). In short, &lt;code&gt;OSSL_PARAM&lt;/code&gt; works by passing arrays of key-value pairs to functions, instead of normal argument passing. This reduces performance, reduces compile-time verification, increases verbosity, and makes code less readable. To the extent there is an argument in favor of it, we infer that the benefit is that it allows OpenSSL to use the same API (and ABI) for different algorithms with different parameters, allowing things like reading algorithm parameters from configuration files with generic configuration parsing code that doesn’t need to be updated when new algorithms are added to OpenSSL.&lt;/p&gt;
    &lt;p&gt;For a concrete comparison of the verbosity, performing an ML-KEM encapsulation with OpenSSL takes 37 lines with 6 fallible function calls. Doing so with BoringSSL takes 19 lines with 3 fallible function calls.&lt;/p&gt;
    &lt;p&gt;In addition to making public APIs more frustrating and error prone to use, OpenSSL internals have also become more complex. For example, in order to make managing arrays of &lt;code&gt;OSSL_PARAM&lt;/code&gt; palatable, many OpenSSL source files are no longer simply C files, they now have a custom Perl preprocessor for their C code.&lt;/p&gt;
    &lt;p&gt;OpenSSL 3 also introduced the notion of “providers” (obsoleting, but not replacing, the previous ENGINE APIs), which allow for external implementations of algorithms (including algorithms provided by OpenSSL itself). This was the source of innumerable performance regressions, due to poorly designed APIs. In particular, OpenSSL allowed replacing any algorithm at any point in program execution, which necessitated adding innumerable allocations and locks to nearly every operation. To mitigate this, OpenSSL then added more caches, and ultimately RCU (Read-Copy-Update) — a complex memory management strategy which had difficult to diagnose bugs.&lt;/p&gt;
    &lt;p&gt;From our perspective, this is a cycle of compounding bad decisions: the providers API was incorrectly designed (there is no need to be able to redefine SHA-256 at arbitrary points in program execution) leading to performance regressions. This led to additional complexity to mitigate those regressions in the form of caching and RCU, which in term led to more bugs. And after all that, performance was still worse than it had been at the beginning.&lt;/p&gt;
    &lt;p&gt;Finally, taking an OpenSSL public API and attempting to trace the implementation to see how it is implemented has become an exercise in self-flagellation. Being able to read the source to understand how something works is important both as part of self-improvement in software engineering, but also because as sophisticated consumers there are inevitably things about how an implementation works that aren’t documented, and reading the source gives you ground truth. The number of indirect calls, optional paths, &lt;code&gt;#ifdef&lt;/code&gt;, and other obstacles to comprehension is astounding. We cannot overstate the extent to which just reading the OpenSSL source code has become miserable — in a way that both wasn’t true previously, and isn’t true in LibreSSL, BoringSSL, or AWS-LC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing and Verification&lt;/head&gt;
    &lt;p&gt;We joke that the Python Cryptographic Authority is a CI engineering project that incidentally produces a cryptography library. The joke reflects our real belief that investment in testing and automation enables Pareto improvements in development speed and correctness — to the point that it can make other work look trivial.&lt;/p&gt;
    &lt;p&gt;The OpenSSL project does not sufficiently prioritize testing. While OpenSSL’s testing has improved substantially since the pre-Heartbleed era there are quite significant gaps. The gaps in OpenSSL’s test coverage were acutely visible during the OpenSSL 3.0 development cycle — where the project was extremely reliant on the community to report regressions experienced during the extended alpha and beta period (covering 19 pre-releases over the course of 16 months), because their own tests were insufficient to catch unintended real-world breakages. Despite the known gaps in OpenSSL’s test coverage, it’s still common for bug fixes to land without an accompanying regression test.&lt;/p&gt;
    &lt;p&gt;OpenSSL’s CI is exceptionally flaky, and the OpenSSL project has grown to tolerate this flakiness, which masks serious bugs. OpenSSL 3.0.4 contained a critical buffer overflow in the RSA implementation on AVX-512-capable CPUs. This bug was actually caught by CI — but because the crash only occurred when the CI runner happened to have an AVX-512 CPU (not all did), the failures were apparently dismissed as flakiness. Three years later, the project still merges code with failing tests: the day we prepared our conference slides, five of ten recent commits had failing CI checks, and the day before we delivered the talk, every single commit had failing cross-compilation builds.&lt;/p&gt;
    &lt;p&gt;This incident also speaks to the value of adopting tools like Intel SDE, which allows controlled testing against CPUs with different subsets of x86-64 extension instructions. Using Intel SDE to have dedicated test jobs with and without AVX-512 would have made the nature of the failure immediately legible and reproducible.&lt;/p&gt;
    &lt;p&gt;OpenSSL is not keeping pace with the state of the art in formal verification. Formal methods have gone from academic novelty to practical reality for meaningful chunks of cryptographic code. BoringSSL and AWS-LC have incorporated formally verified implementations and use automated reasoning to increase assurance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Safety&lt;/head&gt;
    &lt;p&gt;At the time OpenSSL was created, there were no programming languages that meaningfully provided performance, embeddability, and memory safety — if you wanted a memory safe language, you were committing to giving up performance and adding a garbage collector.&lt;/p&gt;
    &lt;p&gt;The world has changed. Nearly 5 years ago, &lt;code&gt;pyca/cryptography&lt;/code&gt; issued our first release incorporating Rust code, and since then we have migrated nearly all functionality to Rust, using a mix of pure-Rust for all parsing and X.509 operations combined with using OpenSSL for providing cryptographic algorithms — gaining performance wins and avoiding several OpenSSL CVEs. We know these transitions are possible.&lt;/p&gt;
    &lt;p&gt;A library committed to security needs to make a long-term commitment to a migration to a memory safe programming language. OpenSSL has shown no initiative at all on this issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing Causes&lt;/head&gt;
    &lt;p&gt;Whenever issues with an open source project are raised, many will suggest this is an issue of funding or tragedy of the commons. This is inapposite, in the past decade, post-Heartbleed, OpenSSL has received considerable funding, and at this moment the OpenSSL Corporation and Foundation employ more software engineers than work full time on either BoringSSL or LibreSSL. The problems we have described are not ones caused by underfunding.&lt;/p&gt;
    &lt;p&gt;We do not fully understand the motivations that led to the public APIs and internal complexity we’ve described here. We’ve done our best to reverse engineer them by asking “what would motivate someone to do this” and often we’ve found ourselves coming up short. The fact that none of the other OpenSSL forks have made these same design choices is informative to the question of “was this necessary”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Directions&lt;/head&gt;
    &lt;p&gt;Our experience with OpenSSL has been on a negative trajectory for several years. As a result of these issues, we are making the following changes to our (admittedly undocumented) policies.&lt;/p&gt;
    &lt;p&gt;First, we will no longer require OpenSSL implementations for new functionality. Where we deem it desirable, we will add new APIs that are only on LibreSSL/BoringSSL/AWS-LC. Concretely, we expect to add ML-KEM and ML-DSA APIs that are only available with LibreSSL/BoringSSL/AWS-LC, and not with OpenSSL.&lt;/p&gt;
    &lt;p&gt;Second, we currently statically link a copy of OpenSSL in our wheels (binary artifacts). We are beginning the process of looking into what would be required to change our wheels to link against one of the OpenSSL forks.&lt;/p&gt;
    &lt;p&gt;If we are able to successfully switch to one of OpenSSL’s forks for our binary wheels, we will begin considering the circumstances under which we would drop support for OpenSSL entirely.&lt;/p&gt;
    &lt;p&gt;Lastly, in the long term, we are actively tracking non-OpenSSL derived cryptography libraries such as Graviola as potential alternatives.&lt;/p&gt;
    &lt;p&gt;We recognize that changes in which libraries we use to provide cryptographic implementations have substantial impact on our users — particularly redistributors. We do not contemplate these steps lightly, nor do we anticipate making them hastily. However, due to the gravity of our concerns, we are compelled to act. If you rely on &lt;code&gt;pyca/cryptography&lt;/code&gt;’s support for OpenSSL, the best way to avoid the most drastic steps contemplated here is to engage with the OpenSSL project and contribute to improvements on these axes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cryptography.io/en/latest/statements/state-of-openssl/"/><published>2026-01-14T22:04:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624541</id><title>Scaling long-running autonomous coding</title><updated>2026-01-15T13:57:23.167818+00:00</updated><content>&lt;doc fingerprint="9ff1b067ed89904f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Scaling long-running autonomous coding&lt;/head&gt;
    &lt;p&gt;We've been experimenting with running coding agents autonomously for weeks.&lt;/p&gt;
    &lt;p&gt;Our goal is to understand how far we can push the frontier of agentic coding for projects that typically take human teams months to complete.&lt;/p&gt;
    &lt;p&gt;This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;The limits of a single agent&lt;/head&gt;
    &lt;p&gt;Today's agents work well for focused tasks, but are slow for complex projects. The natural next step is to run multiple agents in parallel, but figuring out how to coordinate them is challenging.&lt;/p&gt;
    &lt;p&gt;Our first instinct was that planning ahead would be too rigid. The path through a large project is ambiguous, and the right division of work isn't obvious at the start. We began with dynamic coordination, where agents decide what to do based on what others are currently doing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning to coordinate&lt;/head&gt;
    &lt;p&gt;Our initial approach gave agents equal status and let them self-coordinate through a shared file. Each agent would check what others were doing, claim a task, and update its status. To prevent two agents from grabbing the same task, we used a locking mechanism.&lt;/p&gt;
    &lt;p&gt;This failed in interesting ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Agents would hold locks for too long, or forget to release them entirely. Even when locking worked correctly, it became a bottleneck. Twenty agents would slow down to the effective throughput of two or three, with most time spent waiting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The system was brittle: agents could fail while holding locks, try to acquire locks they already held, or update the coordination file without acquiring the lock at all.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We tried replacing locks with optimistic concurrency control. Agents could read state freely, but writes would fail if the state had changed since they last read it. This was simpler and more robust, but there were still deeper problems.&lt;/p&gt;
    &lt;p&gt;With no hierarchy, agents became risk-averse. They avoided difficult tasks and made small, safe changes instead. No agent took responsibility for hard problems or end-to-end implementation. This lead to work churning for long periods of time without progress.&lt;/p&gt;
    &lt;head rend="h2"&gt;Planners and workers&lt;/head&gt;
    &lt;p&gt;Our next approach was to separate roles. Instead of a flat structure where every agent does everything, we created a pipeline with distinct responsibilities.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Planners continuously explore the codebase and create tasks. They can spawn sub-planners for specific areas, making planning itself parallel and recursive.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Workers pick up tasks and focus entirely on completing them. They don't coordinate with other workers or worry about the big picture. They just grind on their assigned task until it's done, then push their changes.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At the end of each cycle, a judge agent determined whether to continue, then the next iteration would start fresh. This solved most of our coordination problems and let us scale to very large projects without any single agent getting tunnel vision.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running for weeks&lt;/head&gt;
    &lt;p&gt;To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.&lt;/p&gt;
    &lt;p&gt;Despite the codebase size, new agents can still understand it and make meaningful progress. Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts.&lt;/p&gt;
    &lt;p&gt;While it might seem like a simple screenshot, building a browser from scratch is extremely difficult.&lt;/p&gt;
    &lt;p&gt;Another experiment was doing an in-place migration of Solid to React in the Cursor codebase. It took over 3 weeks with +266K/-193K edits. As we've started to test the changes, we do believe it's possible to merge this change.&lt;/p&gt;
    &lt;p&gt;Another experiment was to improve an upcoming product. A long-running agent made video rendering 25x faster with an efficient Rust version. It also added support to zoom and pan smoothly with natural spring transitions and motion blurs, following the cursor. This code was merged and will be in production soon.&lt;/p&gt;
    &lt;p&gt;We have a few other interesting examples still running:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java LSP: 7.4K commits, 550K LoC&lt;/item&gt;
      &lt;item&gt;Windows 7 emulator: 14.6K commits, 1.2M LoC&lt;/item&gt;
      &lt;item&gt;Excel: 12K commits, 1.6M LoC&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What we've learned&lt;/head&gt;
    &lt;p&gt;We've deployed billions of tokens across these agents toward a single goal. The system isn't perfectly efficient, but it's far more effective than we expected.&lt;/p&gt;
    &lt;p&gt;Model choice matters for extremely long-running tasks. We found that GPT-5.2 models are much better at extended autonomous work: following instructions, keeping focus, avoiding drift, and implementing things precisely and completely.&lt;/p&gt;
    &lt;p&gt;Opus 4.5 tends to stop earlier and take shortcuts when convenient, yielding back control quickly. We also found that different models excel at different roles. GPT-5.2 is a better planner than GPT-5.1-codex, even though the latter is trained specifically for coding. We now use the model best suited for each role rather than one universal model.&lt;/p&gt;
    &lt;p&gt;Many of our improvements came from removing complexity rather than adding it. We initially built an integrator role for quality control and conflict resolution, but found it created more bottlenecks than it solved. Workers were already capable of handling conflicts themselves.&lt;/p&gt;
    &lt;p&gt;The best system is often simpler than you'd expect. We initially tried to model systems from distributed computing and organizational design. However, not all of them work for agents.&lt;/p&gt;
    &lt;p&gt;The right amount of structure is somewhere in the middle. Too little structure and agents conflict, duplicate work, and drift. Too much structure creates fragility.&lt;/p&gt;
    &lt;p&gt;A surprising amount of the system's behavior comes down to how we prompt the agents. Getting them to coordinate well, avoid pathological behaviors, and maintain focus over long periods required extensive experimentation. The harness and models matter, but the prompts matter more.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;Multi-agent coordination remains a hard problem. Our current system works, but we're nowhere near optimal. Planners should wake up when their tasks complete to plan the next step. Agents occasionally run for far too long. We still need periodic fresh starts to combat drift and tunnel vision.&lt;/p&gt;
    &lt;p&gt;But the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected. Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.&lt;/p&gt;
    &lt;p&gt;The techniques we're developing here will eventually inform Cursor's agent capabilities. If you're interested in working on the hardest problems in AI-assisted software development, we'd love to hear from you at hiring@cursor.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/scaling-agents"/><published>2026-01-14T22:18:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624658</id><title>Crafting Interpreters</title><updated>2026-01-15T13:57:22.845274+00:00</updated><content>&lt;doc fingerprint="55803025f8ef288d"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;Ever wanted to make your own programming language or wondered how they are designed and built?&lt;/p&gt;&lt;p&gt;If so, this book is for you.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Crafting Interpreters contains everything you need to implement a full-featured, efficient scripting language. You’ll learn both high-level concepts around parsing and semantics and gritty details like bytecode representation and garbage collection. Your brain will light up with new ideas, and your hands will get dirty and calloused. It’s a blast.&lt;/p&gt;&lt;p&gt;Starting from &lt;code&gt;main()&lt;/code&gt;, you build a language that features rich
syntax, dynamic typing, garbage collection, lexical scope, first-class
functions, closures, classes, and inheritance. All packed into a few thousand
lines of clean, fast code that you thoroughly understand because you write each
one yourself.&lt;/p&gt;&lt;p&gt;The book is available in four delectable formats:&lt;/p&gt;&lt;p&gt;640 pages of beautiful typography and high resolution hand-drawn illustrations. Each page lovingly typeset by the author. The premiere reading experience.&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Amazon.com&lt;/cell&gt;&lt;cell&gt;.ca&lt;/cell&gt;&lt;cell&gt;.uk&lt;/cell&gt;&lt;cell&gt;.au&lt;/cell&gt;&lt;cell&gt;.de&lt;/cell&gt;&lt;cell&gt;.fr&lt;/cell&gt;&lt;cell&gt;.es&lt;/cell&gt;&lt;cell&gt;.it&lt;/cell&gt;&lt;cell&gt;.jp&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Barnes and Noble&lt;/cell&gt;&lt;cell&gt;Book Depository&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Perfectly mirrors the hand-crafted typesetting and sharp illustrations of the print book, but much easier to carry around.&lt;/p&gt;Buy from Payhip Download Free Sample&lt;head rend="h3"&gt;Web&lt;/head&gt;&lt;p&gt;Meticulous responsive design looks great from your desktop down to your phone. Every chapter, aside, and illustration is there. Read the whole book for free. Really.&lt;/p&gt;Read Now&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://craftinginterpreters.com/"/><published>2026-01-14T22:26:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46626210</id><title>New Safari developer tools provide insight into CSS Grid Lanes</title><updated>2026-01-15T13:57:22.327754+00:00</updated><content>&lt;doc fingerprint="b71e011275b5ebb3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New Safari developer tools provide insight into CSS Grid Lanes&lt;/head&gt;
    &lt;p&gt;You might have heard recently that Safari Technology Preview 234 landed the final plan for supporting masonry-style layouts in CSS. It’s called Grid Lanes.&lt;/p&gt;
    &lt;p&gt;CSS Grid Lanes adds a whole new capability to CSS Grid. It lets you line up content in either columns or rows — and not both.&lt;/p&gt;
    &lt;p&gt;This layout pattern allows content of various aspect ratios to pack together. No longer do you need to truncate content artificially to make it fit. Plus, the content that’s earlier in the HTML gets grouped together towards the start of the container. If new items get lazy loaded, they appear at the end without reshuffling what’s already on screen.&lt;/p&gt;
    &lt;p&gt;It can be tricky to understand the content flow pattern as you are learning Grid Lanes. The content is not flowing down the first column to the very bottom of the container, and then back up to the top of the second column. (If you want that pattern, use CSS Multicolumn or Flexbox.)&lt;/p&gt;
    &lt;p&gt;With Grid Lanes, the content flows perpendicular to the layout shape you created. When you define columns, the content flows back and forth across those columns, just like to how it would if rows existed. If you define rows, the content will flow up and down through the rows — in the column direction, as if columns were there.&lt;/p&gt;
    &lt;p&gt;Having a way to see the order of items can make it easier to understand this content flow. Introducing the CSS Grid Lanes Inspector in Safari. It’s just the regular Grid Inspector, now with more features.&lt;/p&gt;
    &lt;p&gt;Safari’s Grid Inspector already reveals the grid lines for Grid Lanes, and labels track sizes, line numbers, line names, and area names. Now it has a new feature — “Order Numbers”.&lt;/p&gt;
    &lt;p&gt;By turning on the order numbers in the example above, we can clearly see how Item 1, 2, 3, and 4 flow across the columns, as if there were a row. Then Item 5 is in the middle right, followed by Item 6 on the far right, and so on.&lt;/p&gt;
    &lt;p&gt;You might be tempted to believe the content order doesn’t matter. With pages like this photo gallery — most users will have no idea how the photos are ordered in the HTML. But for many users, the content order has a big impact on their experience. You should always consider what it’s like to tab through content — watching one item after another sequentially come into focus. Consider what it’s like to listen to the site through a screenreader while navigating by touch or keyboard. With Grid Lanes, you can adjust &lt;code&gt;flow-tolerance&lt;/code&gt; to reduce the jumping around and put items where people expect.&lt;/p&gt;
    &lt;p&gt;To know which value for flow tolerance to choose, it really helps to quickly see the order of items. That makes it immediately clear how your CSS impacts the result.&lt;/p&gt;
    &lt;p&gt;Order Numbers in the Grid Inspector is an extension of a feature Safari’s Flexbox Inspector has had since Safari 16.0 — marking the order of Flex items. Seeing content order is also helpful when using the &lt;code&gt;order&lt;/code&gt; property in Flexbox.&lt;/p&gt;
    &lt;p&gt;Order Numbers in Safari’s Grid Inspector works for CSS Grid and Subgrid, as well as Grid Lanes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Try out Safari’s layout tooling&lt;/head&gt;
    &lt;p&gt;The Grid and Flexbox layout inspectors might seem similar across browsers, but the team behind Safari’s Web Inspector has taken the time to finely polish the details. In both the Grid and Flexbox Inspectors, you can simultaneously activate as many overlays as you want. No limits. And no janky scrolling due to performance struggles.&lt;/p&gt;
    &lt;p&gt;Safari’s Flexbox Inspector visually distinguishes between excess free space and Flex gaps, since knowing which is which can solve confusion. It shows the boundaries of items, revealing how they are distributed both on the main axis and the cross axis of Flexbox containers. And it lists all the Flexbox containers, making it easier to understand what’s happening overall.&lt;/p&gt;
    &lt;p&gt;Our Grid Inspector has a simple and clear interface, making it easy to understand the options. It also lists all Grid containers. And of course, you can change the default colors of the overlays, to best contrast with your site content.&lt;/p&gt;
    &lt;p&gt;And Safari’s Grid and Flexbox Inspectors are the only browser devtools that label content order. We hope seeing the order of content in Grid Lanes helps you understand it more thoroughly and enjoy using this powerful new layout mechanism.&lt;/p&gt;
    &lt;head rend="h3"&gt;Try out Order Numbers&lt;/head&gt;
    &lt;p&gt;Order Numbers in Safari’s Grid Inspector shipped today in Safari Technology Preview 235. Let us know what you think. There’s still time to polish the details to make the most helpful tool possible. You can ping Jen Simmons on Bluesky or Mastodon with links, comments and ideas.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://webkit.org/blog/17746/new-safari-developer-tools-provide-insight-into-css-grid-lanes/"/><published>2026-01-15T00:34:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46626410</id><title>Furiosa: 3.5x efficiency over H100s</title><updated>2026-01-15T13:57:21.770074+00:00</updated><content>&lt;doc fingerprint="a19ed41a2a13478a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Furiosa NXT RNGD Server: Efficient AI inference at data center scale&lt;/head&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;p&gt;We are excited to introduce FuriosaAI’s NXT RNGD Server—our first branded, turnkey solution for AI inference.&lt;/p&gt;
    &lt;p&gt;Built around our RNGD accelerators, NXT RNGD Server is an optimized system that delivers high performance on today’s most important AI workloads while fitting seamlessly into existing data center environments.&lt;/p&gt;
    &lt;p&gt;With NXT RNGD Server, enterprises can move from experimentation to deployment faster than ever. The system ships with the Furiosa SDK and Furiosa LLM runtime preinstalled, so applications can serve immediately upon installation. We optimized the platform over standard PCIe interconnects, eliminating the need for proprietary fabrics or exotic infrastructure.&lt;/p&gt;
    &lt;p&gt;Designed for compatibility, NXT RNGD Server runs at just 3 kW per system, allowing organizations to scale AI within the power and cooling limits of most modern facilities. This makes NXT RNGD Server a practical and cost-effective system to build out AI factories inside the data centers enterprises already operate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Specifications&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute: Up to 8 × RNGD accelerators (4 petaFLOPS FP8 per server) with dual AMD EPYC processors. Supports BF16, FP8, INT8, and INT4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Memory: 384 GB HBM3 (12 TB/s bandwidth) plus 1 TB DDR5 system memory&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Storage: 2 × 960 GB NVMe M.2 (OS), 2 × 3.84 TB NVMe U.2 (internal)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Networking: 1G management NIC plus 2 × 25G data NICs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Power &amp;amp; Cooling: 3 kW system power, redundant 2,000 W Titanium PSUs, air-cooled&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Security &amp;amp; Management: Secure Boot, TPM, BMC attestation, dual management paths (PCIe + I2C)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software: Preinstalled Furiosa SDK and Furiosa LLM runtime with native Kubernetes and Helm integration&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Real-world benefits and proven performance&lt;/head&gt;
    &lt;p&gt;NXT RNGD Server’s superior power efficiency significantly lowers businesses’ TCO. Enterprise customers can run advanced AI efficiently at scale within current infrastructure and power limitations – using on-prem servers or cloud data centers. This is crucial for leveraging existing infrastructure, since more than 80% of data centers today are air-cooled and operate at 8 kW per rack or less. &lt;/p&gt;
    &lt;p&gt;For businesses with sensitive workloads, regulatory compliance requirements, or enhanced privacy and security needs, NXT RNGD Server offers complete control over enterprise data, with model weights running entirely on local infrastructure.&lt;/p&gt;
    &lt;p&gt;Global enterprises have validated NXT RNGD Server’s performance. In July, LG AI Research announced that it has adopted RNGD for inference computing with its EXAONE models. Running LG’s EXAONE 3.5 32B model on a single server with four RNGD cards and a batch size of one, LG AI Research achieved 60 tokens/second with a 4K context window and 50 tokens/second with a 32K context window.&lt;/p&gt;
    &lt;p&gt;We are now working with LG AI Research to supply NXT RNGD servers to enterprises using EXAONE across key sectors, including electronics, finance, telecommunications, and biotechnology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making rapid deployment of advanced AI available to everyone&lt;/head&gt;
    &lt;p&gt;With global data center demand at 60 GW in 2024 and expected to triple by the end of the decade, the industry faces a once-in-a-generation transformation. More than 80 percent of facilities today are air-cooled and operate at 8 kW per rack or less, making them poorly suited for GPU-based systems that require liquid cooling and 10 kW+ per server.&lt;/p&gt;
    &lt;p&gt;NXT RNGD Server provides a practical path forward. It allows organizations to deploy advanced AI within their existing facilities, without prohibitive energy costs or disruptive retrofits. Engineered as a plug-and-play system, NXT RNGD combines AI-optimized silicon with Furiosa LLM, a vLLM-compatible serving framework featuring built-in OpenAI API support, enabling organizations to deploy and scale AI workloads from day one.&lt;/p&gt;
    &lt;p&gt;By combining silicon and system design, NXT RNGD Server makes efficient, enterprise-ready, and future-proof AI infrastructure a reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Availability&lt;/head&gt;
    &lt;p&gt;We are taking inquiries and orders for January 2026.&lt;/p&gt;
    &lt;p&gt;Download the datasheet here and sign up for RNGD updates here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://furiosa.ai/blog/introducing-rngd-server-efficient-ai-inference-at-data-center-scale"/><published>2026-01-15T00:53:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46626836</id><title>Bubblewrap: A nimble way to prevent agents from accessing your .env files</title><updated>2026-01-15T13:55:00.391825+00:00</updated><content/><link href="https://patrickmccanna.net/a-better-way-to-limit-claude-code-and-other-coding-agents-access-to-secrets/"/><published>2026-01-15T01:45:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46627652</id><title>The URL shortener that makes your links look as suspicious as possible</title><updated>2026-01-15T13:55:00.014845+00:00</updated><content/><link href="https://creepylink.com/"/><published>2026-01-15T03:28:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46628397</id><title>Handy – Free open source speech-to-text app</title><updated>2026-01-15T13:54:59.396273+00:00</updated><content>&lt;doc fingerprint="d43bde7e4d379f86"&gt;
  &lt;main&gt;
    &lt;p&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/p&gt;
    &lt;p&gt;Handy is a cross-platform desktop application built with Tauri (Rust + React/TypeScript) that provides simple, privacy-focused speech transcription. Press a shortcut, speak, and have your words appear in any text field—all without sending your voice to the cloud.&lt;/p&gt;
    &lt;p&gt;Handy was created to fill the gap for a truly open source, extensible speech-to-text tool. As stated on handy.computer:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Free: Accessibility tooling belongs in everyone's hands, not behind a paywall&lt;/item&gt;
      &lt;item&gt;Open Source: Together we can build further. Extend Handy for yourself and contribute to something bigger&lt;/item&gt;
      &lt;item&gt;Private: Your voice stays on your computer. Get transcriptions without sending audio to the cloud&lt;/item&gt;
      &lt;item&gt;Simple: One tool, one job. Transcribe what you say and put it into a text box&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Handy isn't trying to be the best speech-to-text app—it's trying to be the most forkable one.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Press a configurable keyboard shortcut to start/stop recording (or use push-to-talk mode)&lt;/item&gt;
      &lt;item&gt;Speak your words while the shortcut is active&lt;/item&gt;
      &lt;item&gt;Release and Handy processes your speech using Whisper&lt;/item&gt;
      &lt;item&gt;Get your transcribed text pasted directly into whatever app you're using&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The process is entirely local:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Silence is filtered using VAD (Voice Activity Detection) with Silero&lt;/item&gt;
      &lt;item&gt;Transcription uses your choice of models: &lt;list rend="ul"&gt;&lt;item&gt;Whisper models (Small/Medium/Turbo/Large) with GPU acceleration when available&lt;/item&gt;&lt;item&gt;Parakeet V3 - CPU-optimized model with excellent performance and automatic language detection&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Works on Windows, macOS, and Linux&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest release from the releases page or the website&lt;/item&gt;
      &lt;item&gt;Install the application following platform-specific instructions&lt;/item&gt;
      &lt;item&gt;Launch Handy and grant necessary system permissions (microphone, accessibility)&lt;/item&gt;
      &lt;item&gt;Configure your preferred keyboard shortcuts in Settings&lt;/item&gt;
      &lt;item&gt;Start transcribing!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For detailed build instructions including platform-specific requirements, see BUILD.md.&lt;/p&gt;
    &lt;p&gt;Handy is built as a Tauri application combining:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React + TypeScript with Tailwind CSS for the settings UI&lt;/item&gt;
      &lt;item&gt;Backend: Rust for system integration, audio processing, and ML inference&lt;/item&gt;
      &lt;item&gt;Core Libraries: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;whisper-rs&lt;/code&gt;: Local speech recognition with Whisper models&lt;/item&gt;&lt;item&gt;&lt;code&gt;transcription-rs&lt;/code&gt;: CPU-optimized speech recognition with Parakeet models&lt;/item&gt;&lt;item&gt;&lt;code&gt;cpal&lt;/code&gt;: Cross-platform audio I/O&lt;/item&gt;&lt;item&gt;&lt;code&gt;vad-rs&lt;/code&gt;: Voice Activity Detection&lt;/item&gt;&lt;item&gt;&lt;code&gt;rdev&lt;/code&gt;: Global keyboard shortcuts and system events&lt;/item&gt;&lt;item&gt;&lt;code&gt;rubato&lt;/code&gt;: Audio resampling&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Handy includes an advanced debug mode for development and troubleshooting. Access it by pressing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: &lt;code&gt;Cmd+Shift+D&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows/Linux: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is actively being developed and has some known issues. We believe in transparency about the current state:&lt;/p&gt;
    &lt;p&gt;Whisper Model Crashes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whisper models crash on certain system configurations (Windows and Linux)&lt;/item&gt;
      &lt;item&gt;Does not affect all systems - issue is configuration-dependent &lt;list rend="ul"&gt;&lt;item&gt;If you experience crashes and are a developer, please help to fix and provide debug logs!&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wayland Support (Linux):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limited support for Wayland display server&lt;/item&gt;
      &lt;item&gt;Requires &lt;code&gt;wtype&lt;/code&gt;or&lt;code&gt;dotool&lt;/code&gt;for text input to work correctly (see Linux Notes below for installation)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Text Input Tools:&lt;/p&gt;
    &lt;p&gt;For reliable text input on Linux, install the appropriate tool for your display server:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Display Server&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommended Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Install Command&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;X11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;xdotool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sudo apt install xdotool&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Wayland&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;wtype&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sudo apt install wtype&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Both&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dotool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;sudo apt install dotool&lt;/code&gt; (requires &lt;code&gt;input&lt;/code&gt; group)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;X11: Install &lt;code&gt;xdotool&lt;/code&gt;for both direct typing and clipboard paste shortcuts&lt;/item&gt;
      &lt;item&gt;Wayland: Install &lt;code&gt;wtype&lt;/code&gt;(preferred) or&lt;code&gt;dotool&lt;/code&gt;for text input to work correctly&lt;/item&gt;
      &lt;item&gt;dotool setup: Requires adding your user to the &lt;code&gt;input&lt;/code&gt;group:&lt;code&gt;sudo usermod -aG input $USER&lt;/code&gt;(then log out and back in)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without these tools, Handy falls back to enigo which may have limited compatibility, especially on Wayland.&lt;/p&gt;
    &lt;p&gt;Other Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The recording overlay is disabled by default on Linux (&lt;/p&gt;&lt;code&gt;Overlay Position: None&lt;/code&gt;) because certain compositors treat it as the active window. When the overlay is visible it can steal focus, which prevents Handy from pasting back into the application that triggered transcription. If you enable the overlay anyway, be aware that clipboard-based pasting might fail or end up in the wrong window.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you are having trouble with the app, running with the environment variable&lt;/p&gt;&lt;code&gt;WEBKIT_DISABLE_DMABUF_RENDERER=1&lt;/code&gt;may help&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can manage global shortcuts outside of Handy and still control the app via signals. Sending&lt;/p&gt;&lt;code&gt;SIGUSR2&lt;/code&gt;to the Handy process toggles recording on/off, which lets Wayland window managers or other hotkey daemons keep ownership of keybindings. Example (Sway):&lt;quote&gt;bindsym $mod+o exec pkill -USR2 -n handy&lt;/quote&gt;&lt;code&gt;pkill&lt;/code&gt;here simply delivers the signal—it does not terminate the process.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS (both Intel and Apple Silicon)&lt;/item&gt;
      &lt;item&gt;x64 Windows&lt;/item&gt;
      &lt;item&gt;x64 Linux&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The following are recommendations for running Handy on your own machine. If you don't meet the system requirements, the performance of the application may be degraded. We are working on improving the performance across all kinds of computers and hardware.&lt;/p&gt;
    &lt;p&gt;For Whisper Models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: M series Mac, Intel Mac&lt;/item&gt;
      &lt;item&gt;Windows: Intel, AMD, or NVIDIA GPU&lt;/item&gt;
      &lt;item&gt;Linux: Intel, AMD, or NVIDIA GPU &lt;list rend="ul"&gt;&lt;item&gt;Ubuntu 22.04, 24.04&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Parakeet V3 Model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU-only operation - runs on a wide variety of hardware&lt;/item&gt;
      &lt;item&gt;Minimum: Intel Skylake (6th gen) or equivalent AMD processors&lt;/item&gt;
      &lt;item&gt;Performance: ~5x real-time speed on mid-range hardware (tested on i5)&lt;/item&gt;
      &lt;item&gt;Automatic language detection - no manual language selection required&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We're actively working on several features and improvements. Contributions and feedback are welcome!&lt;/p&gt;
    &lt;p&gt;Debug Logging:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adding debug logging to a file to help diagnose issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS Keyboard Improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for Globe key as transcription trigger&lt;/item&gt;
      &lt;item&gt;A rewrite of global shortcut handling for MacOS, and potentially other OS's too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Opt-in Analytics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collect anonymous usage data to help improve Handy&lt;/item&gt;
      &lt;item&gt;Privacy-first approach with clear opt-in&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Settings Refactoring:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cleanup and refactor settings system which is becoming bloated and messy&lt;/item&gt;
      &lt;item&gt;Implement better abstractions for settings management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tauri Commands Cleanup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Abstract and organize Tauri command patterns&lt;/item&gt;
      &lt;item&gt;Investigate tauri-specta for improved type safety and organization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're behind a proxy, firewall, or in a restricted network environment where Handy cannot download models automatically, you can manually download and install them. The URLs are publicly accessible from any browser.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open Handy settings&lt;/item&gt;
      &lt;item&gt;Navigate to the About section&lt;/item&gt;
      &lt;item&gt;Copy the "App Data Directory" path shown there, or use the shortcuts: &lt;list rend="ul"&gt;&lt;item&gt;macOS: &lt;code&gt;Cmd+Shift+D&lt;/code&gt;to open debug menu&lt;/item&gt;&lt;item&gt;Windows/Linux: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt;to open debug menu&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;macOS: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The typical paths are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: &lt;code&gt;~/Library/Application Support/com.pais.handy/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows: &lt;code&gt;C:\Users\{username}\AppData\Roaming\com.pais.handy\&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux: &lt;code&gt;~/.config/com.pais.handy/&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inside your app data directory, create a &lt;code&gt;models&lt;/code&gt; folder if it doesn't already exist:&lt;/p&gt;
    &lt;code&gt;# macOS/Linux
mkdir -p ~/Library/Application\ Support/com.pais.handy/models

# Windows (PowerShell)
New-Item -ItemType Directory -Force -Path "$env:APPDATA\com.pais.handy\models"&lt;/code&gt;
    &lt;p&gt;Download the models you want from below&lt;/p&gt;
    &lt;p&gt;Whisper Models (single .bin files):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small (487 MB): &lt;code&gt;https://blob.handy.computer/ggml-small.bin&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Medium (492 MB): &lt;code&gt;https://blob.handy.computer/whisper-medium-q4_1.bin&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Turbo (1600 MB): &lt;code&gt;https://blob.handy.computer/ggml-large-v3-turbo.bin&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Large (1100 MB): &lt;code&gt;https://blob.handy.computer/ggml-large-v3-q5_0.bin&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Parakeet Models (compressed archives):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;V2 (473 MB): &lt;code&gt;https://blob.handy.computer/parakeet-v2-int8.tar.gz&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;V3 (478 MB): &lt;code&gt;https://blob.handy.computer/parakeet-v3-int8.tar.gz&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Whisper Models (.bin files):&lt;/p&gt;
    &lt;p&gt;Simply place the &lt;code&gt;.bin&lt;/code&gt; file directly into the &lt;code&gt;models&lt;/code&gt; directory:&lt;/p&gt;
    &lt;code&gt;{app_data_dir}/models/
├── ggml-small.bin
├── whisper-medium-q4_1.bin
├── ggml-large-v3-turbo.bin
└── ggml-large-v3-q5_0.bin
&lt;/code&gt;
    &lt;p&gt;For Parakeet Models (.tar.gz archives):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Extract the &lt;code&gt;.tar.gz&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;Place the extracted directory into the &lt;code&gt;models&lt;/code&gt;folder&lt;/item&gt;
      &lt;item&gt;The directory must be named exactly as follows: &lt;list rend="ul"&gt;&lt;item&gt;Parakeet V2: &lt;code&gt;parakeet-tdt-0.6b-v2-int8&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Parakeet V3: &lt;code&gt;parakeet-tdt-0.6b-v3-int8&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Parakeet V2: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Final structure should look like:&lt;/p&gt;
    &lt;code&gt;{app_data_dir}/models/
├── parakeet-tdt-0.6b-v2-int8/     (directory with model files inside)
│   ├── (model files)
│   └── (config files)
└── parakeet-tdt-0.6b-v3-int8/     (directory with model files inside)
    ├── (model files)
    └── (config files)
&lt;/code&gt;
    &lt;p&gt;Important Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For Parakeet models, the extracted directory name must match exactly as shown above&lt;/item&gt;
      &lt;item&gt;Do not rename the &lt;code&gt;.bin&lt;/code&gt;files for Whisper models—use the exact filenames from the download URLs&lt;/item&gt;
      &lt;item&gt;After placing the files, restart Handy to detect the new models&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Restart Handy&lt;/item&gt;
      &lt;item&gt;Open Settings → Models&lt;/item&gt;
      &lt;item&gt;Your manually installed models should now appear as "Downloaded"&lt;/item&gt;
      &lt;item&gt;Select the model you want to use and test transcription&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check existing issues at github.com/cjpais/Handy/issues&lt;/item&gt;
      &lt;item&gt;Fork the repository and create a feature branch&lt;/item&gt;
      &lt;item&gt;Test thoroughly on your target platform&lt;/item&gt;
      &lt;item&gt;Submit a pull request with clear description of changes&lt;/item&gt;
      &lt;item&gt;Join the discussion - reach out at contact@handy.computer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The goal is to create both a useful tool and a foundation for others to build upon—a well-patterned, simple codebase that serves the community.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Handy CLI - The original Python command-line version&lt;/item&gt;
      &lt;item&gt;handy.computer - Project website with demos and documentation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License - see LICENSE file for details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whisper by OpenAI for the speech recognition model&lt;/item&gt;
      &lt;item&gt;whisper.cpp and ggml for amazing cross-platform whisper inference/acceleration&lt;/item&gt;
      &lt;item&gt;Silero for great lightweight VAD&lt;/item&gt;
      &lt;item&gt;Tauri team for the excellent Rust-based app framework&lt;/item&gt;
      &lt;item&gt;Community contributors helping make Handy better&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;"Your search for the right speech-to-text tool can end here—not because Handy is perfect, but because you can make it perfect for you."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/cjpais/Handy"/><published>2026-01-15T05:23:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46629191</id><title>Show HN: MailPilot – Freedom to go anywhere while your agents work</title><updated>2026-01-15T13:54:59.029707+00:00</updated><content>&lt;doc fingerprint="f7c505d7bc89decc"&gt;
  &lt;main&gt;
    &lt;div&gt;https://mailpilot.chat&lt;p&gt;What is this? A local TUI (like Claude, Codex, Gemini, OpenCode or Copilot) that wraps your agent and sends each turn to an email in a nice format.&lt;/p&gt;&lt;p&gt;What does it enable? It lets you email your agents. And them email you. So They Keep Working on the tasks you want, and You go and do What You Want. No need to stay at your desk. Be free.&lt;/p&gt;&lt;p&gt;I built this for me, but thought others would find it useful, so I turned it into a product. I want get outside, and away from my desk, but still have the agents work.&lt;/p&gt;&lt;p&gt;The accidental killer feature: You can CC your team. If you forward the thread to a coworker, their reply goes straight to the agent context too. It turns a local session into an async multiplayer thread.&lt;/p&gt;&lt;p&gt;Works out of the box with Claude, Codex, Gemini, Copilot, and OpenCode. Happy to answer questions!&lt;/p&gt;&lt;p&gt;https://mailpilot.chat&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46629191"/><published>2026-01-15T07:20:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46629474</id><title>A letter to those who fired tech writers because of AI</title><updated>2026-01-15T13:54:58.900914+00:00</updated><content>&lt;doc fingerprint="a52049962f092c01"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;To those who fired or didn't hire tech writers because of AI&lt;/head&gt;
    &lt;p&gt;Hey you,&lt;/p&gt;
    &lt;p&gt;Yes, you, who are thinking about not hiring a technical writer this year or, worse, erased one or more technical writing positions last year because of AI. You, who are buying into the promise of docs entirely authored by LLMs without expert oversight or guidance. You, who unloaded the weight of docs on your devs’ shoulders, as if it was a trivial chore.&lt;/p&gt;
    &lt;p&gt;You are making a big mistake. But you can still undo the damage.&lt;/p&gt;
    &lt;p&gt;It’s been a complicated year, 2025. When even Andrej Karpathy, one of OpenAI’s founders, admits, in a fit of Oppenheimerian guilt, to feeling lost, you know that no one holds the key to the future. You flail and dance around these new totems made of words, which are neither intelligent nor conscious, pretending they can replace humans while, in fact, they’re little more than glorified tools.&lt;/p&gt;
    &lt;p&gt;You might think that the plausible taste of AI prose is all you need to give your products a voice. You paste code into a field and something that resembles docs comes out after a few minutes. Like a student eager to turn homework in, you might be tempted to content yourself with docs theatre, thinking that it’ll earn you a good grade. It won’t, because docs aren’t just artifacts.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You keep using that word. I do not think it means what you think it means&lt;/p&gt;
      &lt;p&gt;—The Princess Bride&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When you say “docs”, you’re careful to focus on the output, omitting the process. Perhaps you don’t know how docs are produced. You’ve forgotten, or perhaps never knew, that docs are product truth; that without them, software becomes unusable, because software is never done, is never obvious, and is never simple. Producing those docs requires tech writers.&lt;/p&gt;
    &lt;p&gt;Tech writers go to great lengths to get the information they need. They write so that your audience can understand. They hunger for clarity and meaning and impact. They power through weeks full of deadlines, chasing product news, because without their reporting, most products wouldn’t thrive; some wouldn’t even exist. Their docs aren’t a byproduct: they tie the product together.&lt;/p&gt;
    &lt;p&gt;An LLM can’t do all that, because it can’t feel the pain of your users. It can’t put itself into their shoes. It lacks the kind of empathy that’s behind great help content. It does not, in fact, have any empathy at all, because it cannot care. You need folks who will care, because content is a hairy beast that can only be tamed by agents made of flesh and capable of emotions: humans.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI generated docs are broken&lt;/head&gt;
    &lt;p&gt;You can’t generate docs on autopilot. Let me tell you why.&lt;/p&gt;
    &lt;p&gt;First, AI-generated docs are not intelligent. They not only make up things in subtle ways: They lack vision. Even if you fed them millions of tokens, they couldn’t develop a docs strategy, decide what not to document, or structure content for reuse. And they fail to capture the tension, the caveats, the edge cases, the feeling of unfinishedness that only someone who cares can feel. Without that grounding, docs are hollow.&lt;/p&gt;
    &lt;p&gt;Second, liability doesn’t vanish just because AI wrote it. When docs cause harm through wrong instructions, someone will be held responsible. It won’t be the model. You can’t depose an LLM. You can’t fire it. You can’t point at it in court when a customer’s data evaporates because your GenAI runbook told them to run the wrong command. That someone will be you, or someone who reports to you.&lt;/p&gt;
    &lt;p&gt;Third, even your favorite AI must RTFM. All your Claude Skills, Cursor rules, all the semantic tagging that makes RAG work, is technical writing under a new name: context curation. You fired or didn’t hire the people who create high-quality context and then wondered why your AI tools produce slop. You can’t augment what isn’t there. The writers you let go were the supply chain for the intelligence you’re now betting on.&lt;/p&gt;
    &lt;head rend="h2"&gt;The solution is to augment your technical writers&lt;/head&gt;
    &lt;p&gt;It’s not all bad news: Marvelous things can happen if you provide your writers with AI tools and training while you protect the quality of your content through an AI policy. I’ve described the ideal end state in My day as an augmented technical writer in 2030, a vision of the future where writers orchestrate, edit, and publish docs together with AI agents. This is already happening before our eyes.&lt;/p&gt;
    &lt;p&gt;Productivity gains are real when you understand that augmentation is better than replacing humans, a reality even AWS’ CEO, Matt Garman, acknowledged. Read how I’m using AI as a technical writer. I’m not alone: Follow Tom Johnson, CT Smith, and Sarah Deaton, and discover how tech writers are building tools through AI to better apply it to docs.&lt;/p&gt;
    &lt;p&gt;Develop an AI strategy for docs together with tech writers, and give them time and resources to experiment with AI. Tech writers are resourceful by nature: they’ve spent careers doing more with less, optimizing workflows, finding clever solutions to impossible quests. Give them the tools and a bit of runway, and they’ll figure out how to make AI work for the docs, not instead of them.&lt;/p&gt;
    &lt;head rend="h2"&gt;So here’s my request for you: Reconsider&lt;/head&gt;
    &lt;p&gt;Reconsider the positions you did not open. Or the writers you let go. Reconsider the assumption that AI has solved a problem that, at its core, is deeply human and requires not only concatenating words, but also chasing subject-matter experts and understanding the subtleties of product motions, among many other things.&lt;/p&gt;
    &lt;p&gt;Technical writers aren’t a luxury. They are the people who translate what you’ve built into something others can use. Without them, you’re shipping a product that can’t speak for itself, or that lies. Your product needs to speak. AI can generate noise effectively and infinitely, but only a technical writer can create the signal.&lt;/p&gt;
    &lt;p&gt;Don’t choose the noise. Get them back. Get them onboard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;Thanks to Tiffany Hrabusa, Casey Smith, and Anna Urbiztondo for their reviews of early drafts and for their encouragement. Thanks to my partner, Valentina, for helping me improve this piece and for suggesting to wait a bit before hitting Publish. And a heartfelt thank you to the tech writing community and its wonderful human beings.&lt;/p&gt;
    &lt;p&gt;For a standalone version of this letter, use https://passo.uno/reconsider/.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://passo.uno/letter-those-who-fired-tech-writers-ai/"/><published>2026-01-15T07:58:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46629682</id><title>Raspberry Pi's New AI Hat Adds 8GB of RAM for Local LLMs</title><updated>2026-01-15T13:54:58.782860+00:00</updated><content>&lt;doc fingerprint="fe7514741c06875a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Raspberry Pi's new AI HAT adds 8GB of RAM for local LLMs&lt;/head&gt;
    &lt;p&gt;Today Raspberry Pi launched their new $130 AI HAT+ 2 which includes a Hailo 10H and 8 GB of LPDDR4X RAM.&lt;/p&gt;
    &lt;p&gt;With that, the Hailo 10H is capable of running LLMs entirely standalone, freeing the Pi's CPU and system RAM for other tasks. The chip runs at a maximum of 3W, with 40 TOPS of INT8 NPU inference performance in addition to the equivalent 26 TOPS INT4 machine vision performance on the earlier AI HAT with Hailo 8.&lt;/p&gt;
    &lt;p&gt;In practice, it's not as amazing as it sounds.&lt;/p&gt;
    &lt;p&gt;You still can't upgrade the RAM on the Pi, but at least this way if you do have a need for an AI coprocessor, you don't have to eat up the Pi's memory to run things on it.&lt;/p&gt;
    &lt;p&gt;And it's a lot cheaper and more compact than running an eGPU on a Pi. In that sense, it's more useful than the silly NPUs Microsoft forces into their 'AI PCs'.&lt;/p&gt;
    &lt;p&gt;But it's still a solution in search of a problem, in all but the most niche of use cases.&lt;/p&gt;
    &lt;p&gt;Besides feeling like I'm living in the world of the Turbo Encabulator every time I'm testing AI hardware, I find the marketing of these things to be very vague, and the applications not very broad.&lt;/p&gt;
    &lt;p&gt;For example, the Hailo 10H is advertised as being used for a Fujitsu demo of automatic shrink detection for a self-checkout.&lt;/p&gt;
    &lt;p&gt;That's certainly not a worthless use case, but it's not something I've ever needed to do. I have a feeling this board is meant more for development, for people who want to deploy the 10H in other devices, rather than as a total solution to problems individual Pi owners need to solve.&lt;/p&gt;
    &lt;p&gt;Especially when it comes to the headline feature: running inference, like with LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;I also published a video with all the information in this blog post, but if you enjoy text more than video, scroll on pastâit doesn't offend me!&lt;/p&gt;
    &lt;head rend="h2"&gt;LLM performance on the AI HAT+ 2&lt;/head&gt;
    &lt;p&gt;I ran everything on an 8 gig Pi 5, so I could get an apples-to-apples comparison, running the same models on the Pi's CPU as I did on the AI HAT's NPU.&lt;/p&gt;
    &lt;p&gt;They both have the same 8GB LPDDR4X RAM configuration, so ideally, they'd have similar performance.&lt;/p&gt;
    &lt;p&gt;I tested every model Hailo put out so far, and compared them, Pi 5 versus Hailo 10H:&lt;/p&gt;
    &lt;p&gt;The Pi's built-in CPU trounces the Hailo 10H.&lt;/p&gt;
    &lt;p&gt;The Hailo is only close, really, on Qwen2.5 Coder 1.5B.&lt;/p&gt;
    &lt;p&gt;It is slightly more efficient in most cases:&lt;/p&gt;
    &lt;p&gt;But looking more closely at power draw, we can see why the Hailo doesn't keep up:&lt;/p&gt;
    &lt;p&gt;The Pi's CPU is allowed to max out it's power limits (10W on the SoC), which are a lot higher than the Hailo's (3W).&lt;/p&gt;
    &lt;head rend="h2"&gt;Qwen 30B on a Pi&lt;/head&gt;
    &lt;p&gt;So power holds it back, but the 8 gigs of RAM holds back the LLM use case (vs just running on the Pi's CPU) the most. The Pi 5 can be bought in up to a 16 GB configuration. That's as much as you get in decent consumer graphics cards1.&lt;/p&gt;
    &lt;p&gt;Because of that, many quantized medium-size models target 10-12 GB of RAM usage (leaving space for context, which eats up another 2+ GB of RAM).&lt;/p&gt;
    &lt;p&gt;A couple weeks ago, ByteShape got Qwen3 30B A3B Instruct to fit on a 16GB Pi 5. Now this post isn't about LLMs, but the short of it is they found a novel way to compress the model to fit in 10 GB of RAM.&lt;/p&gt;
    &lt;p&gt;A little bit of quality is lost, but like a JPEG, it's still good enough to ace all the contrived tests (like building a TODO list app, or sorting a complex list) that the tiny models I ran on the Hailo 10H didn't complete well (see the video earlier in this post for details).&lt;/p&gt;
    &lt;p&gt;To test the 30B model, I installed llama.cpp following this guide from my blog, and downloaded the compressed model.&lt;/p&gt;
    &lt;p&gt;I asked it to generate a single page TODO list app, and it's still not a speed demon (this is a Pi CPU with LPDDR4x RAM we're talking about), but after a little while, it gave me this:&lt;/p&gt;
    &lt;p&gt;It met all my requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I can type in as many items as I want&lt;/item&gt;
      &lt;item&gt;I can drag them around to rearrange them&lt;/item&gt;
      &lt;item&gt;I can check off items and they go to the bottom of the list...&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It's honestly crazy how many small tasks you can do even with free local models... even on a Pi. Natural Language Programming was just a dream back when I started my career.&lt;/p&gt;
    &lt;p&gt;Besides being angry Google, OpenAI, Anthropic and all these other companies are consuming all the world's money and resources doing this stuffânot to mention destroying the careers of thousands of junior developersâit is kinda neat to see NLP work for very tightly defined examples.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking computer vision&lt;/head&gt;
    &lt;p&gt;But I don't think this HAT is the best choice to run local, private LLMs (at least not as a primary goal).&lt;/p&gt;
    &lt;p&gt;What it is good for, is vision processing. But the original AI HAT was good for that too!&lt;/p&gt;
    &lt;p&gt;In my testing, Hailo's hailo-rpi5-examples were not yet updated for this new HAT, and even if I specified the Hailo 10H manually, model files would not load, or I ran into errors once the board was detected.&lt;/p&gt;
    &lt;p&gt;But Raspberry Pi's models ran, so I tested them with a Camera Module 3:&lt;/p&gt;
    &lt;p&gt;I pointed it over at my desk, and it was able to pick out things like my keyboard, my monitor (which it thought was a TV), my phone, and even the mouse tucked away in the back.&lt;/p&gt;
    &lt;p&gt;It all ran quite fastâand 10x faster than on the Pi's CPUâbut the problem is I can do the same thing with the original AI HAT ($110)âor the AI Camera ($70).&lt;/p&gt;
    &lt;p&gt;If you just need vision processing, I would stick with one of those.&lt;/p&gt;
    &lt;p&gt;The headline feature of the AI HAT+ 2 is the ability to run in a 'mixed' mode, where it can process machine vision (frames from a camera or video feed), while also running inference (like an LLM or text-to-speech).&lt;/p&gt;
    &lt;p&gt;Unfortunately, when I tried running two models simultaneously, I ran into segmentation faults or 'device not ready', and lacking any working examples from Hailo, I had to give up on getting that working in time for this post.&lt;/p&gt;
    &lt;p&gt;Just like the original AI HAT, there's some growing pains.&lt;/p&gt;
    &lt;p&gt;It seems like with most hardware with "AI" in the name, it's hardware-first, then software comes laterâif it comes at all. At least with Raspberry Pi's track record, the software does come, it's just... often the solutions are only useful in tiny niche use cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;8 GB of RAM is useful, but it's not quite enough to give this HAT an advantage over just paying for the bigger 16GB Pi with more RAM, which will be more flexible and run models faster.&lt;/p&gt;
    &lt;p&gt;The main use case for this HAT might be in power-constrained applications where you need both vision processing and inferencing. But even there... it's hard to say "yes, buy this thing", because for just a few more watts, the Pi could achieve better performance for inference in tandem with the $70 AI Camera or the $110 AI HAT+ for the vision processing.&lt;/p&gt;
    &lt;p&gt;Outside of running tiny LLMs in less than 10 watts, maybe the idea is you use the AI HAT+ 2 as a development kit for designing devices using the 10H like self-checkout scanners (which might not even run on a Pi)? I'm not sure.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;With the obvious caveat that the VRAM on GPUs runs a lot faster than equivalent LPDDR4 RAM on a Pi! ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/"/><published>2026-01-15T08:23:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46629957</id><title>Nao Labs (Open-Source Analytics Agent, YC X25) Is Hiring</title><updated>2026-01-15T13:54:58.328157+00:00</updated><content>&lt;doc fingerprint="e9cee3477d95e4ac"&gt;
  &lt;main&gt;
    &lt;p&gt;AI data IDE&lt;/p&gt;
    &lt;p&gt;Founding software engineer — Build the future of data teams experience.&lt;/p&gt;
    &lt;p&gt;Location: Paris 11, France&lt;/p&gt;
    &lt;p&gt;Hello, we're nao Labs&lt;/p&gt;
    &lt;p&gt;We are building an open-source AI agent for data analytics.&lt;/p&gt;
    &lt;p&gt;We are an early stage start-up with 2 cofounders - we joined Y Combinator Spring 2025 batch and STATION F and are now based in Paris 11.&lt;/p&gt;
    &lt;p&gt;We already have a first product - AI IDE for data people - used by 100+ data teams. We are now rolling out a new product - the open source analytics agent. So we’re looking for a founding engineer to help us build it!&lt;/p&gt;
    &lt;p&gt;We are a cofounding team with 18+ years experience in data/AI:&lt;/p&gt;
    &lt;p&gt;We're looking for&lt;/p&gt;
    &lt;p&gt;As a founding engineer, you'll join us to inventing a new, better way to work on data with AI.&lt;/p&gt;
    &lt;p&gt;You will be a fit if...&lt;/p&gt;
    &lt;p&gt;You will&lt;/p&gt;
    &lt;p&gt;You are&lt;/p&gt;
    &lt;p&gt;Tech&lt;/p&gt;
    &lt;p&gt;The product uses these main technologies:&lt;/p&gt;
    &lt;p&gt;Front: React, Typescript&lt;/p&gt;
    &lt;p&gt;Back: node.js, Python&lt;/p&gt;
    &lt;p&gt;Agentic system: Vercel, OpenAI, Anthropic&lt;/p&gt;
    &lt;p&gt;Apply&lt;/p&gt;
    &lt;p&gt;Start Date: Flexible&lt;/p&gt;
    &lt;p&gt;Compensation: competitive salary + early equity&lt;/p&gt;
    &lt;p&gt;Location: Mainly on-site in our Paris 11 office. Remote available.&lt;/p&gt;
    &lt;p&gt;Join us at nao Labs and help shape the future of data work. Let’s build something incredible together!&lt;/p&gt;
    &lt;p&gt;At nao Labs, we are reimagining how data work gets done. Data work is different from traditional software development, and it deserves a specialised workflow. That’s why we’re creating nao—an AI-powered code editor tailored for data workers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/nao-labs/jobs/KjOBhf5-founding-software-engineer"/><published>2026-01-15T08:59:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46629964</id><title>Python: Tprof, a Targeting Profiler</title><updated>2026-01-15T13:54:57.995470+00:00</updated><content>&lt;doc fingerprint="e42e279d26586f91"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Python: introducing tprof, a targeting profiler&lt;/head&gt;
    &lt;p&gt;Profilers measure the performance of a whole program to identify where most of the time is spent. But once youâve found a target function, re-profiling the whole program to see if your changes helped can be slow and cumbersome. The profiler introduces overhead to execution and you have to pick out the stats for the one function you care about from the report. I have often gone through this loop while optimizing client or open source projects, such as when I optimized Djangoâs system checks framework (previous post).&lt;/p&gt;
    &lt;p&gt;The pain here inspired me to create tprof, a targeting profiler for Python 3.12+ that only measures the time spent in specified target functions. Use it to measure your program before and after an optimization to see if it made any difference, with a quick report on the command line.&lt;/p&gt;
    &lt;p&gt;For example, say youâve realized that creating &lt;code&gt;pathlib.Path&lt;/code&gt; objects is the bottleneck for your code. You could run tprof like so:&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmark with comparison mode&lt;/head&gt;
    &lt;p&gt;Sometimes when optimizing code, you want to compare several functions, such as âbeforeâ and âafterâ versions of a function youâre optimizing. tprof supports this with its comparison mode, which adds a âdeltaâ column to the report showing how much faster or slower each function is compared to a baseline.&lt;/p&gt;
    &lt;p&gt;For example, given this code:&lt;/p&gt;
    &lt;code&gt;def before():
    total = 0
    for i in range(100_000):
        total += i
    return total


def after():
    return sum(range(100_000))


for _ in range(100):
    before()
    after()
&lt;/code&gt;
    &lt;p&gt;â¦you can run tprof like this to compare the two functions:&lt;/p&gt;
    &lt;code&gt;$ tprof -x -t before -t after -m example
ð¯ tprof results:
 function         calls total  mean Â± Ï      min â¦ max   delta
 example:before()   100 227ms   2ms Â± 34Î¼s   2ms â¦ 2ms   -
 example:after()    100  86ms 856Î¼s Â± 15Î¼s 835Î¼s â¦ 910Î¼s -62.27%
&lt;/code&gt;
    &lt;p&gt;The output shows that &lt;code&gt;after()&lt;/code&gt; is about 60% faster than &lt;code&gt;before()&lt;/code&gt;, in this case.&lt;/p&gt;
    &lt;head rend="h2"&gt;Python API&lt;/head&gt;
    &lt;p&gt;tprof also provides a Python API via a context manager / decorator, &lt;code&gt;tprof()&lt;/code&gt;. Use it to profile functions within a specific block of code.&lt;/p&gt;
    &lt;p&gt;For example, to recreate the previous benchmarking example within a self-contained Python file:&lt;/p&gt;
    &lt;code&gt;from tprof import tprof


def before():
    total = 0
    for i in range(100_000):
        total += i
    return total


def after():
    return sum(range(100_000))


with tprof(before, after, compare=True):
    for _ in range(100):
        before()
        after()
&lt;/code&gt;
    &lt;p&gt;â¦which produces output like:&lt;/p&gt;
    &lt;code&gt;$ python example.py
ð¯ tprof results:
 function          calls total  mean Â± Ï      min â¦ max delta
 __main__:before()   100 227ms   2ms Â± 83Î¼s   2ms â¦ 3ms -
 __main__:after()    100  85ms 853Î¼s Â± 22Î¼s 835Î¼s â¦ 1ms -62.35%
&lt;/code&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;tprof uses Pythonâs &lt;code&gt;sys.monitoring&lt;/code&gt;, a new API introduced in Python 3.12 for triggering events when functions or lines of code execute. &lt;code&gt;sys.monitoring&lt;/code&gt; allows tprof to register callbacks for only specific target functions, meaning it adds no overhead to the rest of the program. Timing is done in C to further reduce overhead.&lt;/p&gt;
    &lt;p&gt;Thanks to Mark Shannon for contributing sys.monitoring to CPython! This is the second time Iâve used itâthe first time was for tracking down an unexpected mutation (see previous post).&lt;/p&gt;
    &lt;head rend="h2"&gt;Fin&lt;/head&gt;
    &lt;p&gt;If tprof sounds useful to you, please give it a try and let me know what you think! Install tprof from PyPI with your favourite package manager.&lt;/p&gt;
    &lt;p&gt;May you hit your Q1 targets,&lt;/p&gt;
    &lt;p&gt;âAdam&lt;/p&gt;
    &lt;p&gt;ð¸ð¸ð¸ Check out my new book on using GitHub effectively, Boost Your GitHub DX! ð¸ð¸ð¸&lt;/p&gt;
    &lt;p&gt;One summary email a week, no spam, I pinky promise.&lt;/p&gt;
    &lt;p&gt;Related posts:&lt;/p&gt;
    &lt;p&gt;Tags: python&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adamj.eu/tech/2026/01/14/python-introducing-tprof/"/><published>2026-01-15T09:00:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46630369</id><title>Photos Capture the Breathtaking Scale of China's Wind and Solar Buildout</title><updated>2026-01-15T13:54:57.833098+00:00</updated><content>&lt;doc fingerprint="3690c1515a344078"&gt;
  &lt;main&gt;
    &lt;p&gt;Last year China installed more than half of all wind and solar added globally. In May alone, it added enough renewable energy to power Poland, installing solar panels at a rate of roughly 100 every second.&lt;/p&gt;
    &lt;p&gt;The massive buildout is happening across the country, from crowded eastern cities increasingly topped by rooftop solar panels to remote western deserts where colossal wind farms sprawl across the landscape.&lt;/p&gt;
    &lt;p&gt;“From the ground, it’s hard to grasp the scale of these power plants,” said Chinese photographer Weimin Chu. “But when you rise into the air, you can see the geometry, the rhythm — and their relationship with the mountains, the desert, the sea.”&lt;/p&gt;
    &lt;p&gt;Chu has spent three years capturing the shift underway using drones to photograph power plants from overhead. His work, which draws from the visual language of traditional Chinese ink paintings, was featured last year in an award-winning exhibition, presented by Greenpeace. A selection of those photos is reproduced here.&lt;/p&gt;
    &lt;p&gt;“I started out just shooting landscapes,” Chu said. “But when I traveled to places like Guizhou, Yunnan, and Qinghai in 2022, I kept seeing wind farms and solar power plants appear in my camera frame. I realized this is the story of our time — and almost no one is documenting it in a systematic way.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://e360.yale.edu/digest/china-renewable-photo-essay"/><published>2026-01-15T09:54:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46630798</id><title>The 3D Software Rendering Technology of 1998's Thief: The Dark Project (2019)</title><updated>2026-01-15T13:54:57.213196+00:00</updated><content>&lt;doc fingerprint="de6941542f66831f"&gt;
  &lt;main&gt;
    &lt;p&gt;In 1998 Looking Glass Studios released the stealth game Thief: The Dark Project. This was just as 3D hardware acceleration was taking off, so due to the development cycle it didn't use hardware acceleration; it was a purely software-rendered game.&lt;/p&gt;
    &lt;p&gt;I was the primary author of the core rendering technology in Thief (although I didn't write the object or character renderers), as well as some related bits and pieces. The same rendering engine, modified by others to use 3D hardware acceleration, also did the rendering for System Shock 2 and Thief 2.&lt;/p&gt;
    &lt;p&gt;The engine was written somewhat contemporaneously with Quake (despite the game being released much later), and the basic appearance strongly resembles Quake. Many of its technologies were copied from or inspired by Quake, but in many cases the way it works is slightly or significantly different.&lt;/p&gt;
    &lt;p&gt;The Quake software rendering was thoroughly documented by Michael Abrash in a series of articles which were reprinted in his Graphics Programming Black Book. The techniques used in Thief were never written up and I thought it might be nice to write them down for once, even if they're now totally irrelevant. I'll try to describe them relative to the probably-more-familiar Quake approaches where possible.&lt;/p&gt;
    &lt;p&gt;Important contemporaneous games with similar rendering technology:&lt;/p&gt;
    &lt;p&gt;Looking Glass games previous to Thief had used grid-based worlds. System Shock 1 and the Ultima Underworlds computed visibility using a grid-based traversal. Thief, however, was totally free-form, so I had to start from scratch to design the Thief engine (which started long before Thief, and I imagine even before we shipped Terra Nova: Strike Force Centauri, which I worked on).&lt;/p&gt;
    &lt;p&gt;Thief was based on the portal-and-cell idea for visibility and occlusion published by Seth Teller in his 1992 PhD dissertation. In fact, the Thief world renderer was called "Portal", but since that name has a newer popular meaning I'll just call it "Thief" or "the Thief engine" (but there was far far more to the Thief engine than just the renderer, such as the object system, AI, physics, and I had nothing to do with them; I didn't write "the Thief engine", just the renderer).&lt;/p&gt;
    &lt;p&gt;I was originally exploring this idea as research for an imagined holy grail for Looking Glass: developing an "indoor-outdoor" CRPG, since the previous LGS CRPGs (Ultima Underworld 1 &amp;amp; 2, System Shock 1) had all been "indoor" dungeons, but we also had this outdoor tech for Terra Nova, and many of us felt we'd want to eventually make a hypothetical Underworld 3 or something which would have dungeons and exteriors and buildings and what not (this was before Daggerfall). In trying to imagine how we could put some kind of a hole in the Terra Nova terrain to incorporate dungeons, I realized that portals could be used to seamlessly integrate multiple independent renderers (like an indoor renderer and an outdoor renderer) by putting portals at the boundaries, so I wrote up a long document over one Christmas vacation documenting my thoughts along those lines. Then I guess I had all these ideas bouncing around in my head, and I knew we didn't have any idea how to do non-grid stuff, and it seemed viable to just try to make a full indoor renderer using portals... so I went ahead and tackled implementing one as research.&lt;/p&gt;
    &lt;p&gt;The portal-and-cell idea was used offline by Quake for computing its precomputed PVS ("potentially visible set")--also described in Seth Teller's dissertation--but Thief used it at runtime; Thief precomputed the portals and cells, but didn't precompute any further visibility information. (I believe Descent, which was released before I even started writing the Thief engine, had already done this, but I don't know the details.)&lt;/p&gt;
    &lt;p&gt;The Thief engine kept a representation of the level in which the open areas (the areas you could see and/or walk) were divided into convex polyhedra known as cells. Each cell contained zero or more visible "world polygons" along the boundaries of the cell that were the visible surfaces of the world. Cells which adjoined other cells had special boundary polygons called "portals" which marked the adjacency between the cells. If you could see into a given cell, then you could see into adjoining cells through the portals to them.&lt;/p&gt;
    &lt;p&gt;To determine what part of the level was currently visible and needed rendering, the engine conducted a breadth-first traversal of the portals and cells starting from the location of the viewpoint. As each portal was traversed, the cells reached were added to the visible list. Each considered portal was transformed in 3D, checked if it was back-facing, and if not it was projected to 2D. Then a "bounding octagon" was generated, which consisted of a regular 2D bounding box and a 45-degree rotated bounding box.&lt;/p&gt;
    &lt;p&gt;If a portal led from cell A to cell B, then we compared that portal's bounding octagon to a bounding octagon of the visiblity of cell A. If the two didn't overlap, then the portal wasn't actually visible. If it was visible, then the intersection of cell A's octagon and the portal's octagon was the part of cell B that was visible through this portal, so it added that to the list. (It was possible you would see into cell B through multiple paths, so the engine had to accumulate all the visible regions, which it did by simply keeping a conservative bounding octagon of all the incoming path octagons along paths. If you had two small view paths into one cell, the two at opposite ends of the cell, the bounding octagon would be much bigger, typically the size of the whole cell.) The cell containing the viewpoint was always fully visible.&lt;/p&gt;
    &lt;p&gt;Quake precomputed similar information: for each cell, Quake stored the list of all other cells that were visible from anywhere in that cell. This result could be less tight; for example, if you had a long corridor with many side rooms, and that long corridor was a single cell, then in Quake it would always try to render all the side rooms, whereas Thief would only try to render the entry cell to each of the side rooms (since those cells would always be adjacent to the corridor and visible), but Thief could cull the rooms themselves if they weren't currently visible.&lt;/p&gt;
    &lt;p&gt;Full cell-portal-analysis didn't scale well to higher polygon counts, so the engine was limping by the time the last game shipped with it (in other words, it had a lot of overhead when hardware accelerated).&lt;/p&gt;
    &lt;p&gt;Quake reduced overdraw by using a span-buffering ("edge list") technique in which world surfaces were (conceptually) drawn front-to-back and each pixel was only drawn exactly once.&lt;/p&gt;
    &lt;p&gt;Thief drew polygons back-to-front, so they did overdraw each other. Thief's overdraw was already reduced compared to Quake's "naive" (pre-edge-list) overdraw because of the tighter bounds from traversing the portals dynamically, as described above.&lt;/p&gt;
    &lt;p&gt;Thief further reduced overdraw by clipping each rendered world polygon by the bounding octagon for the visibility of the cell containing that polygon. (This is part of why bounding octagons were used, as they significantly reduced overdraw in some cases compared to just using bounding boxes. In the limit if we had clipped to the exact portals leading to each cell, this would have resulted in drawing each pixel only once as well.) I don't have any recollection of what our typical overdraw was using this approach.&lt;/p&gt;
    &lt;p&gt;For this to work, Thief had to store world polygons in cells. This means that world polygons that extended through multiple cells had to be split; Quake was often able to preserve those as single polygons since it stored the polygons in a BSP tree directly on split planes. I'm not sure how much difference this made, though, since both Thief and Quake also had to split polygons for surface caching.&lt;/p&gt;
    &lt;p&gt;The 2D clipping to the bounding octagon meant that many Thief polygons ended up being rendered as 8-sided polygons. This wasn't a problem. In fact, clipping was straightforward and efficient since it was purely 2D along simple axes, and there were no S,T (i.e. U,V) texture coordinates on polygon vertices, since Thief used a technique I'd previously published in the PC Game Programmer's Encyclopedia in which the texture coordinates were defined as a 3D vector basis independent of the vertices, and then texture coordinates for spans and pixels were computed directly from the basis vectors.&lt;/p&gt;
    &lt;p&gt;The skybox was drawn just by tagging polygons with a special texture, then when drawing it, taking the transformed 2D polygon and clipping it to each skybox polygon and using the skybox texture mapping for each of them, or something like that. I don't remember 100%.&lt;/p&gt;
    &lt;p&gt;After discovering all the cells that needed rendering, Thief would decide which objects needed rendering. Only objects that were in visible cells might need rendering. However, Thief also computed a 2D bounding box (or probably octagon) of each potentially visible object and compared it against the bounding octagon for any cells containing the object. (Quake had nothing like this since it didn't process portals at runtime.)&lt;/p&gt;
    &lt;p&gt;Because the Thief engine was better about rejecting invisible cells entirely, and was able to reject objects that weren't currently visible even if the cells containing them were, Thief could generally handle more object-dense worlds than a renderer made with Quake technology could have. You couldn't necessarily have more visible objects at once, but you could put more objects in overall as long as you limited where you could see them from, and so Thief generally was able to have well-stocked kitchens and dining tables and sideboards and such.&lt;/p&gt;
    &lt;p&gt;But this was just dumb luck as I hadn't chosen these algorithms for that reason in particular, especially as the engine development long preceded the eventual morphing of the game design into Thief.&lt;/p&gt;
    &lt;p&gt;Quake rendered objects by generating a z-buffer from the world polygons (even though the world polygons themselves didn't z-test), and then testing and updating that z-buffer when rendering objects.&lt;/p&gt;
    &lt;p&gt;Thief didn't use a z-buffer. Instead, Thief drew the world back-to-front ("painter's algorithm"), and interleaved rendering of world polygons and objects to make them appear to occlude each other appropriately.&lt;/p&gt;
    &lt;p&gt;Many games in the software rendering era (which rarely used z-buffers) exhibited sorting issues where objects would become visible through walls or invisible behind walls they shouldn't be, and portals/cells didn't offer any special guarantees that would avoid this problem. (For example, Descent exhibited such problems.)&lt;/p&gt;
    &lt;p&gt;This really bothered me and I worked very hard to fix it. The sorting algorithm in Thief is the most complicated painter's algorithm sorter I've every heard of; I remember having to write a little mathematical proof at one point for one part of it.&lt;/p&gt;
    &lt;p&gt;I don't remember the details, but I'll try and sketch out some of the issues and how it probably worked.&lt;/p&gt;
    &lt;p&gt;Originally the cells were sorted based on the portal-traversal-discovery order, which provided a topological front-to-back sort order that could be reversed for back-to-front rendering. However, there were some issues and so by the time Thief shipped the cells were actually being sorted by a BSP tree. This meant that the sort order was very far from a breadth-first search; if you were near the rootmost split plane, the draw order could feature very-close-to-the-viewer cells drawn before very-far-from-the-viewer cells if the cells and viewer happened to fall on the appropriate sides of some BSP split plane.&lt;/p&gt;
    &lt;p&gt;Because of the BSP tree there was no danger that the world polygons would render in the wrong order, but there was some danger that objects could sort wrong with respect to each other or the world polygons. To avoid this, the Thief engine (again, I'm kind of guessing here) conceptually numbered the cells in the order it would draw them. An object in cell N would normally render immediately after drawing the (inward-facing) world polygons in cell N and before drawing the walls of cell N+1. However, sometimes objects would lie in multiple cells. An object in cell M and N, with M &amp;lt; N, would need to be drawn after all the walls of cell M, but the parts of it in cell M might be obscured by walls in cell N or indeed in any of the cells between M and N. (A common case that came up: a corridor (cell) with a niche (cell) holding a torch, with the torch slightly protruding into the corridor. The niche is M, and the corridor is N--e.g. if the viewpoint is in the corridor, then the corridor is "closer" to the viewpoint in a back-to-front rendering sense. For example, one of the walls of the corridor will occlude parts of the niche.)&lt;/p&gt;
    &lt;p&gt;To deal with those complexities, Thief would consider whether it was acceptable to draw a given object in its frontmost cell N (or, indeed, at any point in the draw order between its rearmost and frontmost). To do this it would compute bounding octagons on the objects and on the world polygons and see whether the polygons occluded the objects. If a world polygon was supposed to be "in front" of an object and the bounding octagons intersected, then it wasn't safe to move the object later in the rendering order to a point later than that world polygon.&lt;/p&gt;
    &lt;p&gt;The Thief engine would resolve what range it was valid to draw each object in between backmost and frontmost cells containing that object; if the object was only in one cell it would always be considered as just being in that cell. Once this was determined, Thief then attempted to resolve object-vs-object sorting. Because an object might extend through multiple cells and then be behind or in front of an object that was in a single cell, objects that were only in one cell might need to be moved back or moved forward in the sort order anyway; in other words, the final range of cells that an object might be considered for rendering after might cover a larger range than just between backmost and frontmost containing cell.&lt;/p&gt;
    &lt;p&gt;Thief attempted to use the above ranges to find some sort order for objects, holding the cell sort order fixed, so that objects and world polygons would all inter-sort correctly. (This was where that proof came in.) However, sometimes this was impossible. For example, in the torch-in-the-niche case described above, it's possible for one world polygon in cell N to be occluded by the torch (the wall with the niche on it extending behind the niche), but another world polygon in N to occlude the niche and part of the torch (the wall with the niche on it extend forward towards the viewer). In this case, there is no sort order of objects vs. cells that works, because parts of the torch occlude the cell while parts of the same cell occlude the torch. It would be fixable by interleaving the world polygons from the cell with the torch, instead of always drawing all the world polygons from one cell as a unit, but not all cases could be fixed that way, so Thief actually never did that (it always drew all world polygons from one cell as a unit) -- unless I'm remembering wrong.&lt;/p&gt;
    &lt;p&gt;Instead Thief had an entirely different, extremely expensive mechanism that it fell back on to solve difficult sorting problems. Each object could be "split" into multiple pieces, one per cell. In each cell, only the part of the object which was contained in that cell was rendered. This wasn't done by actually splitting the object, but rather by rendering it multiple times, once for each "piece", and for each piece dynamically clipping the object with a "user clip plane", using similar technology to the frustum clipping that was also being done. (In complex cases, multiple clip planes would be needed; however, it was only ever necessary to clip to portals between the cells the object was in, and not literally clip to all the planes defining each cell.) If this technique was applied, then that part of that object could always be drawn in or after that cell. However, although it didn't increase the per-pixel costs, it required transforming and clipping the object multiple times, so it was rather expensive. (I encouraged designers to place their torches fully inside the niches instead.) This was particularly bad when rendering characters, which were skinned, since I think the skinning transformations would be performed multiple times.&lt;/p&gt;
    &lt;p&gt;This also had another problem: if you had three cells arranged so that the portals between them formed a T, then the dynamic clipping could create t-junctions at the border. These would then cause cracks in the rendering. I don't remember how/if we fixed this.&lt;/p&gt;
    &lt;p&gt;During development I noticed that if you used all the walls of a cell for user clip planes--not just the portals--then if an object interpentrated a wall, it would be clipped away, looking exactly like a z-buffer rendering. Normally these artifacts arose because the game physics allowed an object to interpenetrate briefly, so it was actually better not to clip those objects, and then they wouldn't look (from most angles) like they were going through the wall. But we could've used an effect like that to allow creatures that could move through walls even without though we had no z-buffer.&lt;/p&gt;
    &lt;p&gt;After my positive experience with user clip planes I was frustrated over the next few years that GPUs didn't efficiently/easily support them. (Even in D3D9, they're still done in the pixel shader; although you can use z and stencil to cull more tightly and reduce pixel shading costs.) Of course, with the z-buffer, it's not clear there are lots of use cases, but there's a chicken-and-egg problem to that conclusion.&lt;/p&gt;
    &lt;p&gt;The core technique for rendering texture mapped polygons with decent precomputed shadows is very similar (identical?) to Quake. Thief used light mapping and a surface cache to store the lit surfaces. I refer you to Mike Abrash's articles.&lt;/p&gt;
    &lt;p&gt;My recollection is that we added this to the engine after we (the Thief team) saw the Quake "QTest" release. However, I'm doubtful there really was a Thief team at that point. According to wikipedia, QTest came out February 24, 1996. Terra Nova came out on March 5, 1996, so I guess we had built our final version of Terra Nova by the time QTest was released, but I can't imagine we'd geared up a whole team yet. In fact I don't know for sure when exactly I did the original research version of this engine.&lt;/p&gt;
    &lt;p&gt;At one point objects raycasted to all (or the top N) light sources to determine if they were visible to it or not and turned the entire light on/off over the entire object to simulate objects going in-and-out of shadow. But I don't know if we shipped with this or using the "check the lightmap value on the floor", which I seem to recall we at least considered using for the player's visiblity to guards (to avoid players being tricked/confused by places that appeared to casual inspection to being in shadow).&lt;/p&gt;
    &lt;p&gt;Thief levels were created using Constructive Solid Geometry (CSG) methods, based again on knowledge that that was how Quake worked. Technologically though Thief worked very differently from Quake.&lt;/p&gt;
    &lt;p&gt;The Thief CSG model was "temporal", which maybe I can better explain by analogy to Photoshop. In photoshop, you have layers. Stuff placed in one layer obscures stuff in lower layers, except if you use more interesting blend modes, in which case stuff in a layer changes the stuff visible below it but has no affect on things above it.&lt;/p&gt;
    &lt;p&gt;Algorithmically, you can think of generating the final image by processing each layer in the image sequentially, from back to front, compositing it with the "image so far". At the time each image is processed, the image-so-far contains the accumulated effects of all the earlier (lower) layers, and then the layer is processed and the image-so-far is changed and that layer cannot have any further effect except by data it left in the image-so-far.&lt;/p&gt;
    &lt;p&gt;Normally we think of the photoshop layer model as a stack of 2D layers, but you could instead think of the above algorithm as the model, and think of the layers not as a "vertical" stack but as an ordered sequence of operations to perform. This is what I mean by a temporal model, and this is the model that Thief used for its CSG. (If the vertical photoshop layer stack is a vertical third dimension of the 2D images, then the Thief layering model would be a fourth dimension on the 3D shapes, and thinking about this as four-dimensional space is not very effective.)&lt;/p&gt;
    &lt;p&gt;The Thief CSG took as input a series of "operations" arranged in an order over time. Those operations were "brush placements", where a brush was a 3D convex solid plus a characterization of how the area covered by that solid would be changed by the operation. The entire space started solid, so one brush operation was "carve out a hole in this area"--in other words "change the area covered by this brush to open". For example you would use this to carve out a room. Another placed solid matter; you could use this to create a pillar. Another placed water, and another lava. Because space could be of 4 types (solid, air, water, or lava -- oh hey, the 4 classical elements!), each operation could be considered by which output type it produced. Moreover, we allowed these operations to be selective. For example, the "flood brush" turned air into water, but left all other types alone. This made it easier to fill an area with water--you could construct it entirely with air and then fill the lower half with water. Because of the temporal aspect, you could then go and change some of the water to "air" if you needed to. It would have been possible to make brush types that were much complicated (air turns to water, water turns to solid, solid turns to air, and lava unchanged) but this wasn't actually useful so I think all the brush types were of the unconditional-single-type or conditional-single-type.&lt;/p&gt;
    &lt;p&gt;As with Photoshop layers, the temporal aspect was something under user control; you could move a brush forwards or backwards in time. Unfortunately, this was much harder to visualize (there was no "layers" window) and I'm not sure designers were really thrilled with this.&lt;/p&gt;
    &lt;p&gt;In comparison, Quake started with a level that was all open space and placed solids. It used a more-traditional-CSG explicit-subtraction operation. Water was handled by making all water brushes occur first temporally, so that all the effective temporal sequence of operations was: all air =&amp;gt; some air turns to water =&amp;gt; all solids are placed. Because subtraction was explicit on the solid brushes before they were added to the world, the subtraction couldn't "accidentally" erase water, so there was no need for an explicit temporal model (the set of things you could do in Quake was more limited, but it supported almost everything that mattered in practice--although some designers came up with some strange sequences of operations in Thief to create more complicated shapes).&lt;/p&gt;
    &lt;p&gt;Because Quake levels started empty, Quake had invisible "exterior" surfaces that required a separate process to detect and eliminate. If the level wasn't watertight, then the exterior could be reachable and the automated tools couldn't eliminate it. In contrast, because the Thief level started solid, this was never necessary. (I think Unreal's CSG may have started as solid as well.)&lt;/p&gt;
    &lt;p&gt;I had no idea what I was doing when I implemented the Thief CSG system (despite having the opportunity to pepper John Carmack with questions) so I made a terrible choice. The Thief system worked by dividing the world up into convex polyhedra and keeping track of what each polyhedron's type i.e. air, solid, water, lava, which I call the "medium". To do this, I stored the world as a solid BSP tree; the BSP tree classifies all of space into convex polyhedra (except at the edges where it may have unbounded shapes extending to infinity).&lt;/p&gt;
    &lt;p&gt;Using a BSP tree for this had a performance advantage, but I didn't do it for that reason; I did it because I literally couldn't think of any other way to compute the output. This way I was able to sequentially add each brush to the BSP tree and then apply the medium-transformation-operations to each BSP leaf that the brush contained.&lt;/p&gt;
    &lt;p&gt;But reading the Quake source, it turns out you can do it a different way: intersect every brush with every other brush directly, without a spatial data strucure. By being careful, you can build a growing list of convex polyhedra all of which are non-overlapping. You can then add a spatial data structure to accelerate determining which ones might overlap, but without affecting the computation itself.&lt;/p&gt;
    &lt;p&gt;The difference is that the BSP-tree based CSG tree would have situations where a brush early in the processing would be inserted in the tree early and would introduce a BSP split plane near the root which then extended across the whole level or a significant part of it. This might randomly come close to an edge or vertex of a totally unrelated brush, causing an extra split in that brush and introducing additional epsilon problems. As CSG is already painful numeric geometric algorithms with epsilon problems, introducing this extra uncontrollable problem was terrible. The Thief editor was (is?) notorious for having weird problems where a change to one brush might trigger a failure in the CSG generator at an entirely unrelated place in the level--a failure which meant the level simply failed to "compile".&lt;/p&gt;
    &lt;p&gt;Partway through Thief development I switched everything in the CSG engine from floats to doubles and cranked down the epsilon, which made things better but didn't solve the problem properly, but I realize in hindsight it would have been much, much better to have simply avoided the BSP tree entirely.&lt;/p&gt;
    &lt;p&gt;The epsilon problems were exacerbated by the crazy way I built the polygons and portals directly from the BSP-tree in a t-junc free way by computing a shared mesh along each BSP split plane to guarantee that vertices on one side of the split plane would always have a matching vertex on the other side; this introduced a much more complicated set of invariants that had to be maintained that also had epsilon problems. It meant I didn't have to write a post-processing t-junction eliminator the way Quake did, but in hindsight that would have been better.&lt;/p&gt;
    &lt;p&gt;Looking Glass games prior to Thief such as System Shock 1 and Terra Nova had used a "lines of constant-Z" mapper to perform perspective texture mapping. Those games typically used the perspective texture mapper for large, near polygons, but used an affine mapper for distant polygons.&lt;/p&gt;
    &lt;p&gt;Thief used a custom-written perspective mapper for all polygons. It used the trick used in Quake of issuing a floating-point divide for perspective correction for every N pixels which then executed in parallel with the texture mapping of the next N pixels; this worked because the Pentium would execute the floating-point divide "in the background" if you only used integer instructions after it (the Pentium was not an out-of-order processor in general).&lt;/p&gt;
    &lt;p&gt;Thief computed the perspective correction every 8 pixels (that is, N above was 8). Thief aligned the correction so it occurred on pixels whose x coordinates were multiples of 8. The beginning and ending of a span of pixels could be less than 8, and would call a general-purpose N-pixel mapper, but the 8-pixel spans called a routine that was hardcoded to compute exactly 8 pixels.&lt;/p&gt;
    &lt;p&gt;Because the engine changed over time and was initially used for research, it supported two different formats that textures could be stored in. One was restricted to 256x256 textures, or rather textures with powers-of-two sizes and in which the stride/pitch was always 256. This used an inner loop similar to the one I wrote up for the PCGPE. The second one supported arbitrary non-power-of-two textures but didn't support wrapping. This is the one we switched to when we added surface cached lighting, because padding those textures widths to multiples of 256 would have been far too wasteful. I believe the "step table" indexed-by-carry-flag trick used in this mapper came directly from Mike Abrash.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Details for x86 programmers: &lt;p&gt;Here is the code for the second pixel of the non-power-of-two texture-mapper-unrolled-8-times:&lt;/p&gt;&lt;quote&gt;adc ebx,edi add eax,ebp sbb edi,edi ; save v carry add esi,edx ; update u mov byte ptr _pt_buffer+1,al mov al,[ebx] mov edi,_pt_step_table[edi*4+4]&lt;/quote&gt;&lt;p&gt;Here is the code for the second pixel of the 256x256 texture-mapper-unrolled-8-times (not used in the shipping game, unless maybe it was used for water surface):&lt;/p&gt;&lt;quote&gt;add esi,edx mov ecx,ebx adc bl,dl add eax,ebp adc bh,dh mov al,[edi+ecx] mov byte ptr _pt_buffer+1,al and ebx,0babebeach&lt;/quote&gt;&lt;p&gt;The spacing of both pieces of code shows that they were optimized for the Pentium; each one uses a nominal 4 cycles per pixel. (They also both would trigger "Partial Register Stalls" on the Pentium Pro.) The Pentium had a 2-cycle "AGI" stall that you couldn't use a register as an address for 2 cycles after it's computed, so you can see the texture fetch (from ebx or edi+ecx) is designed to compute those registers two cycles before the fetch. The full 8-pixel routines use 32 and 33 Pentium-pairs of instructions for 8 pixels (although there's also setup to get the right values into the right registers).&lt;/p&gt;&lt;p&gt;The 0babebeach at the end there is a 0xdeadbeef-style constant that relied on "self-modifying" code to update the constant, a common trick on the 486 and Pentium. Those processor had an "and ebx,memory_location" instruction, but it wasn't single-cycle, whereas "and ebx,#immediate" was single-cycle. This meant there were 9 places in the code that had to be modified (this is an 8-unrolled loop, plus the non-unrolled loop), but only when the texture was changed.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Unfortunately, I totally missed out on the possibility of reblocking the texture to improve cache usage, which might have allowed significant performance improvements (I don't know); I've heard that Build and other engines did do this, and since the surface cache in Thief was built in 8x8 blocks anyway it probably wouldn't have been hard to support on that end.&lt;/p&gt;
    &lt;p&gt;Looking Glass games generally demanded flexibility, and graphics engines faced with that need for flexibility often had a combinatorial explosion of texture mappers to handle all the different cases. The texture mappers I wrote for Thief sacrificed a litttle performance for the sake of simplicity and maintainability and maximum flexibility; the texture mapping routines above ended with an indirect branch which would branch to a routine that would decide what to do with the 8 pixels that had just been written into a temporary buffer.&lt;/p&gt;
    &lt;p&gt;There were two kinds of functions that could be invoked at this point. The most common kind was one which wrote to the "framebuffer" (which was actually just a block of RAM somewhere that would be blitted to the screen later) and returned from the original function call that invoked the texture mapper chain. The other kind of function would read the pixels from the temporary buffer, modify them, and write them back to the temporary buffer, then jump to yet another function. You could chain arbitrary numbers of these processing functions, in theory, not that this turned out to be necessary.&lt;/p&gt;
    &lt;p&gt;The codebase had a bunch of these functions:&lt;/p&gt;
    &lt;p&gt;Most of those weren't used, I think. Paletted lighting in particular required setting up the palette in a particular way, and I used this in the original research engine to demo a gouraud-shaded level running at 640x400 at 20-ish fps on a Pentium 90. The level was a retextured version of Doom 1 E1M1, using a texture made by Kurt Bickenbach. He and I iterated a bit trying to figure out how to make textures that would look good and not too banded in 8-bit using paletted lighting, and we got some visually pleasing results, but in the end paletted lighting was too limited, so we abandoned it. (It was too limited artistically, and it couldn't do pitch black properly; when we ended up with a shadow-oriented game it was clearly wrong, but I think it had been abandoned long before due to the first issue.) Abandoning it meant abandoning hitting that frame rate/resolution target as well, but in the long run this didn't matter that much because LGS games were complicated enough that it's not like we spent 95% of the frame time in the renderer anyway, and we didn't ship it for a long time anyway.&lt;/p&gt;
    &lt;p&gt;The engine supported specifying an arbitrary color look-up table ("clut") for each portal, and coloring everything seen through that portal with that clut. This wasn't done by applying this as a post-process on that portal surface, which would have used extra "fill rate", but by storing cluts (and concatenating them into new cluts) and applying them while rendering surfaces seen through the portal. This effect could have been used to make force fields or such things, although it ended up underutilized. (Note, though, that if a single cell were visible through multiple paths, and the paths had different sequences of cluts, there was no correct way to render it; the Thief engine chose randomly. Probably there was no good solution to this problem, but since it was never exercised, it didn't matter.)&lt;/p&gt;
    &lt;p&gt;During development, this was used to make water "foggy", where the more underwater portals away something was, the more opaque the water became. However, this looked pretty terrible as it showed the portal boundaries clearly and although we had all gotten used to it since it was in the engine for years, I did eventually turn off the portal-varying underwater fog, replacing it with a fixed underwater fog.&lt;/p&gt;
    &lt;p&gt;It was also used to color the water surface--the water surface actually used a purely transparent texture and surfaces underwater were colored while they were rendered, instead of having to use the "translucent" read-modify-write mapper. This means the object renderers also needed to support rendering with a clut; I don't remember the details since I didn't work on those, but this was probably something we tended to support by default anyway, so I doubt this was problematic for those renderers.&lt;/p&gt;
    &lt;p&gt;I'm not sure what happened for objects that stuck through the water surface; I guess those were forced to use the dynamic user clip plane path. Because Quake used a z-buffer, Quake couldn't have both a distant wall-underwater and the water surface visible at the same pixel (the z-buffer could only store one depth), so the Quake water surface was opaque, at least in the software renderer. I'm not sure what Quake-derived engines like Half-Life did; one expensive solution would be to render the world except the water, then render the objects, then finally render the transparent water through the z-buffer (much as we do with hardware). The z-buffered transparent water effect would have been much more expensive in pixel costs than Thief's approach, although Thief was spending more effort per triangle/vertex having to render the object multiple times.&lt;/p&gt;
    &lt;p&gt;In Thief, most world surfaces simply used the arbitrary-pitch no-wrapping sampler and the plain store-to-memory writer.&lt;/p&gt;
    &lt;p&gt;I imagine that the object renderers used the shared LGS software rendering libraries and so didn't use these mappers at all.&lt;/p&gt;
    &lt;p&gt;I didn't write all the graphics technology used in Thief. I'll call out the things I didn't write below. I associate each of them with someone at LGS who did the work, but I'm not sure the people I can think of were the only contributors to that effort, so rather than risk missing important people out, I'm just not going to name names if multiple people might have been involved.&lt;/p&gt;
    &lt;p&gt;The 3D camera system, vertex transformation, and clipping processing were part of a shared technology library used by all LGS products. (Thief may have been the first customer of that tech--previous games having used fixed-point so they could run on x86 computers without floating point acceleration.) Thief batched together all the vertices used in a single cell and transformed them all at once, whether they were visible polygons or portals, which was possible since the 3D vertex API allowed separating things out like that. (Sort of like compiled vertex arrays.)&lt;/p&gt;
    &lt;p&gt;As noted before, general object rendering presumably also went through shared libraries. Object polygons were sorted using a BSP tree; James Fleming had written a very clever system that significantly reduced the number of BSP node decisions that needed to be made, based on the fact that single-sided polygons often couldn't occlude each other from any angle. (For example, I believe a polygonal torus, which is an object that self-occludes and needs sorting if the polygons are two-sided, can be statically sorted if it's made of single-sided polygons.)&lt;/p&gt;
    &lt;p&gt;Most importantly, Thief used character skinning with skeletal animation to render its characters, and I never touched a single line of code of that system.&lt;/p&gt;
    &lt;p&gt;I quit Looking Glass for a time; during my absence hardware acceleration support was added. (I came back and added support for colored lighting in the lighting generator and maybe the surface cache or something.)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nothings.org/gamedev/thief_rendering.html"/><published>2026-01-15T10:59:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46631276</id><title>Jiga (YC W21) Is Hiring Full Stack Engineers</title><updated>2026-01-15T13:54:56.832818+00:00</updated><content>&lt;doc fingerprint="ebd8a57c5d14cb90"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re building tools that help NASA, Tesla, Google, and hundreds of top teams bring the future to life. Can you help us make it even better?&lt;/p&gt;
    &lt;p&gt;If you’re into engineering and building physical products, you probably know the sourcing grind. You email a few suppliers, wait days for quotes, answer the same questions twice (or ten times). Track everything in spreadsheets, deal with logistics, revisions, customs (you gotta love new tariffs every week), forms, reports and surprising delays.&lt;/p&gt;
    &lt;p&gt;It’s expensive, slow, tedious and painful – and it pulls you away from actual engineering work.&lt;/p&gt;
    &lt;p&gt;Jiga fixes that. We connect engineers directly with vetted manufacturers, handle quoting and communication in one place, build AI workflows to deal with the administrative work, and give full visibility into every order from prototype into mass production. What used to take weeks now takes hours.&lt;/p&gt;
    &lt;p&gt;We build the platform the manufacturing industry has been craving for years, and we’re aiming for the moon.&lt;/p&gt;
    &lt;p&gt;Most companies talk about transparency and trust. We actually do it.&lt;/p&gt;
    &lt;p&gt;1. We share the numbers. As a team member, you’ll see the full picture: revenue, valuation, runway, roadmap, sales pipeline, what’s working and what fails.&lt;/p&gt;
    &lt;p&gt;2. We have been remote and async from day one and we love it. But we also love our team: once a year we bring everyone together to a small offsite in paradise for co-working, hiking, eating, drinking and sailing.&lt;/p&gt;
    &lt;p&gt;3. We don’t count hours, we track performance. You are senior enough to know what it means, we don’t need to explain.&lt;/p&gt;
    &lt;p&gt;4. We don’t do busy-work. We meet for a weekly all-hands and a team sync. That’s it. Everything else is async. Your calendar will be empty to write code, talk to customers or to fix problems.&lt;/p&gt;
    &lt;p&gt;5. We deliver 11/10 customer experience. We don’t settle for “good enough.” Quick response times, going the extra mile, creating fans not just users. It’s part of our DNA.&lt;/p&gt;
    &lt;p&gt;6. We decide fast. The person closest to the problem makes the call. No approval chains, no review committees and no endless layers of management.&lt;/p&gt;
    &lt;p&gt;7. Profitability is freedom. We are cashflow positive, growing revenue by 3x YoY. No desperate fundraising, no panic pivots, no mass layoffs. The magic we’re building is being built to last for long time.&lt;/p&gt;
    &lt;p&gt;8. “Don’t drink the Kool-Aid” is our motto. If you see something that needs fixing, it’s your duty to raise a flag or just fix it yourself.&lt;/p&gt;
    &lt;p&gt;9. We hire the best people. We’ve gathered a super group of the best talents in engineering, supply chain and sales. Everyone knows what they need to do and cruising through it.&lt;/p&gt;
    &lt;p&gt;10. Ship now, iterate later. Perfection is the enemy of done. We’d rather learn from something that is live on production, than debate something theoretical.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jiga.io/about-us"/><published>2026-01-15T12:00:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46631728</id><title>The 500k-ton typo: Why data center copper math doesn't add up</title><updated>2026-01-15T13:54:56.678187+00:00</updated><content>&lt;doc fingerprint="bec5886a2eb0d520"&gt;
  &lt;main&gt;
    &lt;p&gt;There is a fine line between a structural bull case and a physical impossibility; at least in the media and some overly-enthusiastic analysts.&lt;/p&gt;
    &lt;p&gt;Recently, Forbes dug up a technical paper from Nvidia that was first published in May and it has been circulating through research notes and AI training sets, originally sourced from an NVIDIA technical brief. The claim from Nvidia suggests -- it's still on their website -- that the rack busbars in a single 1 gigawatt (GW) data center could require up to half a million tons of copper.&lt;/p&gt;
    &lt;quote&gt;The physics of using 54 VDC in a single 1 MW rack requires up to 200 kg of copper busbar. The rack busbars alone in a single 1 gigawatt (GW) data center could require up to half a million tons of copper. Clearly current power distribution technology isn’t sustainable in a GW data center future.&lt;/quote&gt;
    &lt;p&gt;Tat sounds like the ultimate catalyst for the commodities market and copper has been hitting records. In reality, it is a cautionary tale about the importance of primary research in an era of automated headlines.&lt;/p&gt;
    &lt;p&gt;If the "half a million tons" figure were accurate, a single 1 GW data center would consume 1.7% of the world's annual copper supply. If we built 30 GW of capacity—a reasonable projection for the AI build-out—that sector alone would theoretically absorb almost half of all the copper mined on Earth.&lt;/p&gt;
    &lt;p&gt;Thunder Said Energy today is flagging the math, which makes them "quite convinced that NVIDIA has made an innocent typo in its statement above, and must in fact mean "half a million pounds of copper", a number that is 2,200x smaller."&lt;/p&gt;
    &lt;p&gt;It should have never got to this point and it's understandable that journalists would run with it but the numbers were also touted by The Copper Development Association, who should know better.&lt;/p&gt;
    &lt;p&gt;When you even look at the Nvidia report itself, the error becomes clear with some simple math. It says standard rack architectures use approximately 200kg of copper per megawatt.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;1 GW (1,000 MW) x 200kg = 200,000kg&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;200,000kg = 200 Metric Tons.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The discrepancy between 200 tons (the reality) and 500,000 tons (the claim) is a factor of 2,500x. It is almost certain that the original document intended to say "half a million pounds"—which equates to roughly 226 tons—and a simple unit conversion error.&lt;/p&gt;
    &lt;p&gt;That this number was circulated so widely is worrisome if you're a copper bull (as I have been for years). We are certainly headed towards undersupply and it can't be fixed because of long build and permitting timelines for mines. But that's not a problem for 2026 and so with prices rising and a reach-for-headlines, there is a risk that it's over-inflated in the short term.&lt;/p&gt;
    &lt;p&gt;That's something Goldman Sachs warned about late last year when they said any copper breakout will be short lived.&lt;/p&gt;
    &lt;p&gt;The real bull case for copper remains compelling. Between grid upgrades, EV expansion, and data center cooling systems, the upside demand is estimated at a very healthy 400,000 to 800,000 tons per year. That is a significant, market-tightening figure—but it is a far cry from the accidental "copper apocalypse" suggested by the typo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://investinglive.com/news/the-500000-ton-typo-why-data-center-copper-math-doesnt-add-up-20260113/"/><published>2026-01-15T12:53:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46632004</id><title>French Court Orders Popular VPNs to Block More Pirate Sites, Despite Opposition</title><updated>2026-01-15T13:54:56.465385+00:00</updated><content>&lt;doc fingerprint="f023a9d11c3c7771"&gt;
  &lt;main&gt;
    &lt;p&gt;Since 2024, the Paris Judicial Court has expanded the typical piracy site blocking orders beyond Internet providers.&lt;/p&gt;
    &lt;p&gt;Initially, rightsholders set their aim at DNS resolvers. This resulted in orders targeted at Cloudflare, Google, and others, requiring them to actively block access to pirate sites through their public DNS resolvers.&lt;/p&gt;
    &lt;p&gt;These blocking expansions were requested by sports rights holders, covering Formula 1, football Ligue 1, MotoGP, and other major sporting brands. They claimed that public DNS resolvers could help users to bypass existing ISP blockades, and the court agreed.&lt;/p&gt;
    &lt;p&gt;Last year, rightsholders cast their net even wider by targeting VPN providers with similar blocking demands. Again, the Paris Court acknowledged the threat of circumvention, ordering CyberGhost, ExpressVPN, NordVPN, ProtonVPN, and Surfshark to start blocking access to specific websites in France.&lt;/p&gt;
    &lt;head rend="h2"&gt;VPN Blocking Expands&lt;/head&gt;
    &lt;p&gt;The VPN blocking effort was not a one-off. After the first order was granted in May, more followed in June and July. These additional orders target various sports piracy sites as requested by the French entertainment powerhouse Canal Plus (SECP) and beIN Sports.&lt;/p&gt;
    &lt;p&gt;After this initial barrage, the blocking activity seemed to have quieted down, but it is far from over. On December 18, the Paris Judicial Court issued a new blocking order. This time around, the French top football league (LFP) and its commercial arm are the requesting parties.&lt;/p&gt;
    &lt;p&gt;As in previous orders, ProtonVPN, Nordvpn, Cyberghost, Surfshark and ExpressVPN are the main targets. These VPN providers have to block access to several domains that provide access to pirated sports streams.&lt;/p&gt;
    &lt;p&gt;The order covers 13 initial domains, including miztv.top, strikeout.im, and prosmarterstv.com. However, it is a ‘dynamic’ order in the sense that, through the overseeing body ARCOM, LFP can add new domains in case additional mirrors and proxies are launched. These blocks remain active for the entire 2025/2026 football season.&lt;/p&gt;
    &lt;p&gt;The court concludes that these VPNs help people to bypass existing site-blocking measures, rendering ISP blocking ineffective. While the VPN blockades are no silver bullet, combined with other blocking measures they should make it more difficult to access these pirate sites.&lt;/p&gt;
    &lt;head rend="h2"&gt;The No-Log Defense&lt;/head&gt;
    &lt;p&gt;All VPN providers, except ProtonVPN, appeared in court to argue a defense. They raised various arguments, with the “no-log” defense from Surfshark and NordVPN standing out.&lt;/p&gt;
    &lt;p&gt;Specifically, the VPNs argued that their “no-log” policy means they do not track user IP addresses or geolocate their users. Therefore, a court order to block access only for French users would violate their contractual obligations.&lt;/p&gt;
    &lt;p&gt;The court was not very receptive to this argument. Instead, it bluntly concluded that “the contractual stipulations binding VPN service providers to their clients cannot be invoked against [the plaintiffs] who have demonstrated an infringement of their rights.”&lt;/p&gt;
    &lt;p&gt;The court stressed that blocking the domains does not require the service to permanently store information on its users. The VPNs simply have to make sure that the sites are blocked from France.&lt;/p&gt;
    &lt;p&gt;In addition, the court rejected the notion that the blocking measures would constitute a “general monitoring obligation”, which is not allowed under the EU’s DSA, because the measures are limited to specific domains and end after the 2025-2026 football season.&lt;/p&gt;
    &lt;head rend="h2"&gt;Court Rejects Other Defenses&lt;/head&gt;
    &lt;p&gt;The VPNs also argued that their services don’t qualify as “technical intermediaries” under Article L. 333-10 of the Sports Code, but that was denied by the Paris court as well. The same applies to the proportionality and effectiveness arguments, which all failed.&lt;/p&gt;
    &lt;p&gt;The court’s logic throughout the order is that technical neutrality does not equal legal immunity.&lt;/p&gt;
    &lt;p&gt;By citing the DSA and the Sports Code, the judge effectively argues that VPN services can be key intermediaries in the piracy ecosystem. Therefore, they are legally obligated to act.&lt;/p&gt;
    &lt;p&gt;“Contrary to the assertions of Surfshark and NordVPN, the mere act of serving as a bridge to enable access to the pirate sites fulfills the function of transmission. Even if an intermediary acts in a passive, automatic, and neutral manner during the connection between internet domains, it nonetheless remains an essential agent in the transmission of data from one domain to another,” the (translated) order reads.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Happens Next?&lt;/head&gt;
    &lt;p&gt;The latest ruling confirms that VPN providers can be obligated to block pirate sites, at least in France. However, the final word hasn’t been said.&lt;/p&gt;
    &lt;p&gt;Speaking with TorrentFreak this week, a NordVPN spokesperson confirms that their appeal is already underway. The company did not directly explain how it complies with the court order but instead said that site-blocking measures are futile.&lt;/p&gt;
    &lt;p&gt;“While it may address superficial cases, it fails to tackle the root causes of piracy. Pirates can easily circumvent these blocks by using subdomains: blocking does not eliminate the content itself or reduce the incentives for piracy,” NordVPN notes.&lt;/p&gt;
    &lt;p&gt;“Effective piracy control should focus on eliminating the source of the content, targeting hosting providers, cutting off financing for illegal operations, and increasing the availability of legitimate content.”&lt;/p&gt;
    &lt;p&gt;In addition, NordVPN notes that, since the French order targets reputable VPNs, users may choose lower-quality free VPNs that will remain a loophole for pirates.&lt;/p&gt;
    &lt;p&gt;For now, however, the targeted VPN providers have to find a way to implement the blocking order. The court order doesn’t specify any technical measures, so they are free to do as they please, as long as the targeted sites are unavailable.&lt;/p&gt;
    &lt;p&gt;If the French VPN blockades are ultimately upheld, some providers may choose to leave the country entirely, but none have made this drastic step yet.&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;A copy of the order issued by the Tribunal Judiciaire de Paris in favor of LFP is available here (pdf). A list of all the targeted domain names is available below.&lt;/p&gt;
    &lt;p&gt;1. miztv.top&lt;lb/&gt; 2. strikeout.im&lt;lb/&gt; 3. qatarstreams.me&lt;lb/&gt; 4. iptvfrancai.com&lt;lb/&gt; 5. vip.kata17.xyz&lt;lb/&gt; 6. iptv-france4k.fr&lt;lb/&gt; 7. front-main.4k-drm.com&lt;lb/&gt; 8. prosmarterstv.com&lt;lb/&gt; 9. line.line-dino.com&lt;lb/&gt; 10. iptvninja.fr&lt;lb/&gt; 11. cdnhome.pro&lt;lb/&gt; 12. elitetv.fr&lt;lb/&gt; 13. smatest.xyz&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://torrentfreak.com/french-court-orders-popular-vpns-to-block-more-pirate-sites-despite-opposition/"/><published>2026-01-15T13:15:40+00:00</published></entry></feed>