<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-05T12:20:19.591500+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45788086</id><title>Moving tables across PostgreSQL instances</title><updated>2025-11-05T12:20:24.714965+00:00</updated><content>&lt;doc fingerprint="a358c972ddb109a2"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Moving tables across PostgreSQL instances&lt;/head&gt;02 Nov 2025&lt;p&gt;At work, we recently had to move a few tables from one PostgreSQL instance to another. In my previous post, I discussed how to use Google’s Database Migration Service (DMS) to migrate data from one instance to another. Unfortunately, that option was not available here, since DMS only allows the migration of an entire database, not specific tables within a database.&lt;/p&gt;&lt;p&gt;We chose the native logical replication option. It’s a much more involved process compared to using DMS, but it provides greater flexibility and allows replication of specific tables only.&lt;/p&gt;&lt;head rend="h3"&gt;Grant access to user accounts&lt;/head&gt;&lt;p&gt;Let’s assume you already have both the source and destination PostgreSQL instances ready. You need to grant replication access to the user accounts on both the source and destination databases. In the case of Cloud SQL, we had to grant the &lt;code&gt;REPLICATION&lt;/code&gt; role to the user account;
this may vary for other instances.&lt;/p&gt;&lt;code&gt;ALTER USER "sql" with REPLICATION

-- run \du to verify
&lt;/code&gt;&lt;head rend="h3"&gt;Copy over schema&lt;/head&gt;&lt;p&gt;The next step is to copy over the schema. The table schema needs to be identical on both the source and destination. We used &lt;code&gt;pg_dump&lt;/code&gt; to dump
and restore the schema, as we had to move more than 50 tables.&lt;/p&gt;&lt;p&gt;There are a few nuances related to constraints as well, which we will cover soon. Before that, let’s try to understand a bit more about how logical replication works.&lt;/p&gt;&lt;p&gt;Logical replication runs in two modes:&lt;/p&gt;&lt;p&gt;1) Initial dump: It copies the data from the source to the destination.&lt;/p&gt;&lt;p&gt;2) CDC: Once the initial dump is done, it switches to CDC mode and applies changes to the destination in real time.&lt;/p&gt;&lt;p&gt;The key point here is that during the initial dump, some constraints can’t be enforced. For example, if you have a foreign key relationship between two tables, dumping one of the tables will throw an error if the referenced column hasn’t been dumped yet.&lt;/p&gt;&lt;p&gt;To solve this problem and to speed up the initial dump process, we first create tables without constraints and indexes. Indexes slow down the dump stage, and it’s easier to rebuild them once the initial dump is complete.&lt;/p&gt;&lt;p&gt;&lt;code&gt;pg_dump&lt;/code&gt; provides a useful flag to dump only the table definitions
without indexes and constraints.&lt;/p&gt;&lt;code&gt;pg_dump URL --no-owner --no-acl --section=pre-data -s \
   -t users \
   -t events \
   &amp;gt; /tmp/pre-data.sql
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;--section&lt;/code&gt; flag allows us to control what is dumped. &lt;code&gt;pre-data&lt;/code&gt;
dumps only the table definitions, while &lt;code&gt;post-data&lt;/code&gt; dumps all
constraints and indexes.&lt;/p&gt;&lt;code&gt;pg_dump URL --no-owner --no-acl --section=post-data -s \
   -t users \
   -t events \
   &amp;gt; /tmp/post-data.sql
&lt;/code&gt;&lt;head rend="h4"&gt;Restore table definition&lt;/head&gt;&lt;code&gt;psql URL --echo-all --single-transaction -v ON_ERROR_STOP=1 -f /tmp/pre-data.sql
&lt;/code&gt;&lt;p&gt;There’s one more catch here: logical replication depends on the primary key, so you need to create the primary key constraint in addition to the table definition. However, primary key constraints are in the &lt;code&gt;post-data.sql&lt;/code&gt; file. Open it in an editor and remove everything
except the primary key constraints.&lt;/p&gt;&lt;head rend="h4"&gt;Restore primary key constrain&lt;/head&gt;&lt;code&gt;psql URL --echo-all --single-transaction -v ON_ERROR_STOP=1 -f /tmp/primary-key-only.sql
&lt;/code&gt;&lt;head rend="h4"&gt;Functions and others&lt;/head&gt;&lt;p&gt;If you use PostgreSQL functions, enums, or anything that isn’t covered by &lt;code&gt;pg_dump&lt;/code&gt;, you might have to handle those manually. When you
specify the &lt;code&gt;-t table&lt;/code&gt; option, it only copies objects directly related
to the tables. Enums and functions don’t fall under that.&lt;/p&gt;&lt;head rend="h3"&gt;Set up publication and subscription&lt;/head&gt;&lt;p&gt;At this point, the table structure should be identical on both instances for the tables that are going to be migrated. The publication should be created on the source instance.&lt;/p&gt;&lt;code&gt;CREATE PUBLICATION migration_publication FOR TABLE users, events
&lt;/code&gt;&lt;p&gt;A corresponding subscription needs to be created on the destination instance.&lt;/p&gt;&lt;code&gt;CREATE SUBSCRIPTION migration_subscription
         CONNECTION 'host={IP} port=5432 user={USER} password={SECRET} dbname={DBNAME} sslmode=require'
        PUBLICATION migration_publication
&lt;/code&gt;&lt;p&gt;Replace &lt;code&gt;{variable}&lt;/code&gt; with the respective values. If the verify CA
option is enabled on the source instance, you need to disable it and
enable only SSL mode.&lt;/p&gt;&lt;p&gt;Once the subscription is created, PostgreSQL starts copying data from the source to the destination instance. It performs an initial data dump for each table and then switches to CDC mode.&lt;/p&gt;&lt;p&gt;You need to wait until the initial dump is complete and it moves to the CDC state with near-zero lag. PostgreSQL exposes this information through several tables such as &lt;code&gt;pg_replication_slots&lt;/code&gt;, &lt;code&gt;pg_stat_replication&lt;/code&gt;, and
&lt;code&gt;pg_stat_subscription&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;SELECT slot_name,
       confirmed_flush_lsn,
       pg_current_wal_lsn(),
       (pg_current_wal_lsn() - confirmed_flush_lsn) AS lsn_distance
FROM pg_replication_slots
&lt;/code&gt;&lt;code&gt;SELECT subscription_name, active,
       pg_size_pretty(pg_current_wal_lsn() - replay_lsn) AS lag_bytes
FROM pg_stat_subscription
&lt;/code&gt;&lt;code&gt;SELECT relid::regclass AS table_name,
       srel.srsubstate AS replication_state,
       CASE srel.srsubstate
           WHEN 'i' THEN 'Initializing'
           WHEN 'd' THEN 'Initial Dump'
           WHEN 's' THEN 'Synchronized'
           WHEN 'r' THEN 'Replicating'
           ELSE 'Unknown'
       END AS state_description
FROM pg_subscription sub
JOIN pg_subscription_rel srel ON sub.oid = srel.srsubid
ORDER BY table_name
&lt;/code&gt;&lt;head rend="h3"&gt;Add indexes and foreign keys&lt;/head&gt;&lt;p&gt;Once the replication moves to the CDC state, you can create indexes and foreign keys. The &lt;code&gt;post-data.sql&lt;/code&gt; file contains all indexes and foreign
keys. Remove the primary key constraints and keep the rest.&lt;/p&gt;&lt;code&gt;psql URL --echo-all --single-transaction -v ON_ERROR_STOP=1 -f /tmp/indexes.sql
&lt;/code&gt;&lt;p&gt;This will take quite some time if you have a lot of data.&lt;/p&gt;&lt;head rend="h3"&gt;Analyze&lt;/head&gt;&lt;p&gt;&lt;code&gt;analyze&lt;/code&gt; is one of the most often overlooked steps when moving or upgrading
PostgreSQL instances. PostgreSQL depends on the statistics generated by this
command to create an efficient query plan. Without these statistics, it might
choose an inefficient plan, and a query that used to take 50 ms could turn
into a 1-second query on your new instance. So make sure to run &lt;code&gt;analyze&lt;/code&gt;
once the indexes are created. If you have more time, you can also run
a &lt;code&gt;vacuum&lt;/code&gt; as well.&lt;/p&gt;&lt;code&gt;-- start with analyze (will be done faster compared to vacuum)
analyze (verbose, BUFFER_USAGE_LIMIT '64MB')
-- vacuum analyze
vacuum (verbose, analyze)
&lt;/code&gt;&lt;head rend="h3"&gt;Switchover&lt;/head&gt;&lt;head rend="h4"&gt;Sequence&lt;/head&gt;&lt;p&gt;Your data is now available on both systems, and you are nearly ready to switch your traffic from the source to the destination.&lt;/p&gt;&lt;p&gt;PostgreSQL copies the data and keeps the indexes in sync, but it doesn’t sync the sequences. You have to do that manually.&lt;/p&gt;&lt;code&gt;-- view current value
SELECT s.schemaname, s.sequencename, s.last_value
FROM pg_sequences s
WHERE s.sequencename in ('users_id_seq', 'events_id_seq');
&lt;/code&gt;&lt;p&gt;You can view the current value on the source instance, and then set it to a higher value on the destination instance.&lt;/p&gt;&lt;code&gt;SELECT
  'SELECT setval(' ||
  quote_literal(s.schemaname || '.' || s.sequencename) || ', ' ||
  CASE
    WHEN s.sequencename = 'users_id_seq' THEN s.last_value + 100
    ELSE s.last_value + 10000
  END || ', true);'
FROM pg_sequences s
WHERE s.sequencename IN (
  'events_id_seq',
  'users_id_seq'
);
&lt;/code&gt;&lt;p&gt;You can run the snippet above on the source instance; the output is a SQL query that you can run on the destination instance. The buffer value is up to you. The key point is that after running the query on the destination instance, you need to perform the switchover before the sequence values on the source instance exceed those on the destination instance.&lt;/p&gt;&lt;head rend="h4"&gt;Disable writes&lt;/head&gt;&lt;p&gt;Once the sequence is updated, stop sending writes to the source PostgreSQL instance. Wait for the replication lag to reach zero, and then switch all writes to the destination PostgreSQL instance. You can monitor the replication lag using &lt;code&gt;pg_replication_slots&lt;/code&gt;.&lt;/p&gt;&lt;head rend="h4"&gt;PgBouncer&lt;/head&gt;&lt;p&gt;The amount of downtime depends on how your app is architected, whether your application can run in read-only mode, and other factors. PgBouncer can help significantly in this regard, and it’s what we used to achieve near-zero downtime.&lt;/p&gt;&lt;p&gt;PgBouncer is a PostgreSQL proxy. The key feature relevant to our situation is that it allows configuration changes without requiring a restart. Assume you have a database named &lt;code&gt;myapp&lt;/code&gt; that’s configured to
connect to the source instance. You can edit the PgBouncer config file
to update the connection details to point to the destination instance.
Then, connect to the PgBouncer
admin console and
run the following commands.&lt;/p&gt;&lt;code&gt;pgbouncer&amp;gt; PAUSE myapp;
pgbouncer&amp;gt; RELOAD;
pgbouncer&amp;gt; SHOW DATABASES;
pgbouncer&amp;gt; RESUME myapp;
&lt;/code&gt;&lt;p&gt;The first command pauses all connections from PgBouncer to the source PostgreSQL instance. The command blocks until all in-flight queries are completed, and new queries are queued. &lt;code&gt;RELOAD&lt;/code&gt; reloads the configuration
from disk. You can run &lt;code&gt;SHOW DATABASES&lt;/code&gt; to quickly verify that the new
configuration has been loaded. &lt;code&gt;RESUME&lt;/code&gt; then resumes connectivity, now
to the new destination PostgreSQL instance.&lt;/p&gt;&lt;p&gt;If you don’t have any long-running queries, this process can result in near-zero downtime, as no queries are dropped. As long as &lt;code&gt;RESUME&lt;/code&gt; is
executed quickly, users will notice at most a slight increase in latency.&lt;/p&gt;&lt;p&gt;You can use the following query to check for long-running queries and terminate them if needed.&lt;/p&gt;&lt;code&gt;SELECT pid, now() - pg_stat_activity.query_start AS duration, query
FROM pg_stat_activity
WHERE (now() - pg_stat_activity.query_start) &amp;gt; interval '2 minutes';
&lt;/code&gt;&lt;p&gt;In the worst-case scenario, if the &lt;code&gt;PAUSE&lt;/code&gt; command hangs because of a
long-running query, you can forcefully restart PgBouncer. However, this
will result in errors for any active connections.&lt;/p&gt;&lt;head rend="h3"&gt;Cleanup&lt;/head&gt;&lt;p&gt;Once you’re confident everything is working correctly, you can clean up the logical replication setup.&lt;/p&gt;&lt;p&gt;Drop the subscription on the destination instance.&lt;/p&gt;&lt;code&gt;DROP SUBSCRIPTION migration_subscription
&lt;/code&gt;&lt;p&gt;Drop the publication on the source instance.&lt;/p&gt;&lt;code&gt;DROP PUBLICATION migration_publication
&lt;/code&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ananthakumaran.in/2025/11/02/moving-tables-across-postgres-instances.html"/><published>2025-11-02T05:47:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45788385</id><title>Intervaltree with Rust Back End</title><updated>2025-11-05T12:20:24.301529+00:00</updated><content>&lt;doc fingerprint="1a5b32fdd0e4e110"&gt;
  &lt;main&gt;
    &lt;p&gt;This crate exposes an interval tree implementation written in Rust to Python via PyO3. The Python wrapper provides the ability to build a tree from tuples, insert additional intervals, search for overlaps, and delete intervals by their &lt;code&gt;(left, right)&lt;/code&gt; key.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust toolchain (for compiling the extension module)&lt;/item&gt;
      &lt;item&gt;Python 3.8+&lt;/item&gt;
      &lt;item&gt;maturin for building/installing the package&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python -m venv .venv
source .venv/bin/activate
pip install maturin
maturin develop&lt;/code&gt;
    &lt;p&gt;You can install the package with (also with uv)&lt;/p&gt;
    &lt;code&gt;pip install intervaltree_rs
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;maturin develop&lt;/code&gt; builds the extension module in-place and installs it into the active virtual environment, making it importable as &lt;code&gt;intervaltree_rs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Once installed, you can use the interval tree directly from Python:&lt;/p&gt;
    &lt;code&gt;from intervaltree_rs import IntervalTree

# Build a tree from tuples: (left, right, payload)
intervals = [
    (5, 10, "a"),
    (12, 18, "b"),
    (1, 4, "c"),
]
tree = IntervalTree.from_tuples(intervals)

# Insert another interval
tree.insert((8, 11, "d"))

# Search for overlaps. Inclusive bounds are enabled by default.
hits = tree.search(9, 10)
for left, right, value in hits:
    print(left, right, value)

# Delete by the interval key
removed = tree.delete((12, 18))
print("Removed:", removed)&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;IntervalTree.search(ql, qr, inclusive=True)&lt;/code&gt; accepts an &lt;code&gt;inclusive&lt;/code&gt; flag. Set it to &lt;code&gt;False&lt;/code&gt; to perform exclusive range queries.&lt;/p&gt;
    &lt;p&gt;To build a wheel that you can distribute or upload to PyPI, run:&lt;/p&gt;
    &lt;code&gt;maturin build --release&lt;/code&gt;
    &lt;p&gt;The built wheels will be placed under &lt;code&gt;target/wheels/&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The Python bindings are covered by Rust unit tests. Run them with:&lt;/p&gt;
    &lt;code&gt;cargo test&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Athe-kunal/intervaltree_rs"/><published>2025-11-02T07:04:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45812024</id><title>This week in 1988, Robert Morris unleashed his eponymous worm</title><updated>2025-11-05T12:20:24.055124+00:00</updated><content>&lt;doc fingerprint="ff40223f0be08a6f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;37 years ago this week, the Morris worm infected 10% of the Internet within 24 hours — worm slithered out and sparked a new era in cybersecurity&lt;/head&gt;
    &lt;p&gt;The Internet contracted worms a year before the World Wide Web was even a thing.&lt;/p&gt;
    &lt;p&gt;This week in 1988, Cornell graduate student Robert Tappan Morris unleashed his eponymous worm upon the Internet. The wave of infections grew to 10% of the entire Internet within 24 hours, causing astronomically expensive damage for the time. However, the pioneering Morris worm malware wasn’t made with malice, says an FBI retrospective on the “programming error.” It was designed to gauge the size of the Internet, resulting in a classic case of unintended consequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Morris worm dissection&lt;/head&gt;
    &lt;p&gt;Known to be something of a prankster, Morris must have felt some foreboding about releasing his ‘innocent’ program into the wild. Evidence of this comes from his release method. “He released it by hacking into an MIT computer from his Cornell terminal in Ithaca, New York,” according to the FBI.&lt;/p&gt;
    &lt;p&gt;The Morris worm was written in C and targeted BSD UNIX systems, like VAX and Sun-3 machines. Specifically, the FBI writes, it “exploited a backdoor in the Internet’s electronic mail system and a bug in the ‘finger’ program that identified network users.” In contrast to computer viruses, the worm Morris had devised had no need of a host program, but could self-replicate and spread autonomously.&lt;/p&gt;
    &lt;p&gt;Thankfully, the Morris worm wasn’t written to cause damage to files. Due to those unintended consequences, though, it precipitated massive slowdowns, and messaging delays and system crashes were common symptoms. It became a computer news sensation in the worst possible way. Just to get rid of the worm in a timely fashion, some institutions ended up wiping complete systems and unplugging networks for as long as a week.&lt;/p&gt;
    &lt;p&gt;Among the Morris worm's casualties were prestigious institutions such as Berkeley, Harvard, Princeton, Stanford, Johns Hopkins, NASA, and the Lawrence Livermore National Laboratory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whodunit?&lt;/head&gt;
    &lt;p&gt;Experts worked hard to find a fix, and while they did so, the question of who was behind the worm came to the fore. Understandably, whoever created and unleashed this worm needed to feel some consequences, and thus, the FBI was brought in.&lt;/p&gt;
    &lt;p&gt;Apparently, Morris sought to anonymously explain and apologize for the worm, but an inadvertent slip of his initials by a friend landed Morris in it.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;FBI interviews and computer file analysis would subsequently confirm Morris was the culprit. He was indicted under the rather freshly inked Computer Fraud and Abuse Act of 1986. After a court appearance for his misdemeanors in 1989, Morris ended up not with jail time, but with a fine, probation, and 400 hours of community service to complete.&lt;/p&gt;
    &lt;head rend="h2"&gt;Computer worms have been around longer than the World Wide Web&lt;/head&gt;
    &lt;p&gt;Back in November 1988, the Internet bore little resemblance to what it is today. For example, the World Wide Web (WWW) wasn’t even a thing. Though the WWW would soon form the core experience for the first tide of surfers in the 90s.&lt;/p&gt;
    &lt;p&gt;At the time, the Internet’s backbone was the NSFNET, the recent successor to ARPANET. Its purpose was mostly to expand the prior backbone’s reach beyond military and defense institutions, and it more broadly embraced academia. While we are here, it is worth mentioning that NSFNET was decommissioned in 1995, and succeeded by the commercial Internet, which emerged in the 1990s off the back of private ISPs and commercial backbones.&lt;/p&gt;
    &lt;p&gt;So, when we talk about 10% of the Internet being paralyzed by the Morris Worm, contemporary estimates are that about 6,000 of the approximately 60,000 connected systems were infected and impacted. Moreover, when we highlighted the potentially massive costs of this first worm propagating, estimates range from $100,000 to millions of dollars.&lt;/p&gt;
    &lt;p&gt;Computer worms have remained a scary phenomenon in recent times. For example, we reported on the first-generation AI worm, the Morris II generative AI worm, last year.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;sb5k&lt;/header&gt;I was working at DEC when the worm slithered its way across the Internet, as part of an engineering team. I also helped manage our Ultrix systems; our IT department knew VMS only.Reply&lt;lb/&gt;I don't remember which CPU was in our systems, but the worm was not able to run on our systems, but I did find it dropped in them.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Gaston404&lt;/header&gt;I completely disagree with the tone of the article. Depicting this as an accident without consequences and limited effect is simply incorrect.Reply&lt;lb/&gt;Back then as a part time job I managed some of the traffic routing through Washington DC. Mail relays were shutdown and backed up queues were spooked off to tape. By today’s standards the volume of traffic may seem trivial but when many of these links ran at 56kbps or less. It was a mess. The main way administrators communicated with each other was email. This also affected collaboration between University researchers and access to the NSF super computer centers.&lt;lb/&gt;At the time rumors maintained that Morris used exploits that he learned from his father who had a consulting agreement with the NSA. So if this is true there is a certain level of non-originality.&lt;lb/&gt;On one hand stronger persecution may have reduced follow on internet crime. On the other hand the fragility demonstrated by this crime, resulted in the creation of procedures to deal with outages. If anything the naive sense of trusted collaboration that pervaded the Internet started to fade.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;derekullo&lt;/header&gt;In 9 years, Tiktok has infected over 90% of the internet!Reply&lt;lb/&gt;Much slower but also much more insidious!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;DS426&lt;/header&gt;Reply&lt;quote/&gt;The next big social media craze is probably just around the corner. I shutter to think how ludicrous it will be.derekullo said:In 9 years, Tiktok has infected over 90% of the internet!&lt;lb/&gt;Much slower but also much more insidious!&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/cyber-security/on-this-day-in-1988-the-morris-worm-slithered-out-and-sparked-a-new-era-in-cybersecurity-10-percent-of-the-internet-was-infected-within-24-hours"/><published>2025-11-04T15:23:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45812606</id><title>Pg_lake: Postgres with Iceberg and data lake access</title><updated>2025-11-05T12:20:23.663079+00:00</updated><content>&lt;doc fingerprint="fb9ba072642955e8"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; integrates Iceberg and data lake files into Postgres. With the &lt;code&gt;pg_lake&lt;/code&gt; extensions, you can use Postgres as a stand-alone lakehouse system that supports transactions and fast queries on Iceberg tables, and can directly work with raw data files in object stores like S3.&lt;/p&gt;
    &lt;p&gt;At a high level, &lt;code&gt;pg_lake&lt;/code&gt; lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create and modify Iceberg tables directly from PostgreSQL, with full transactional guarantees and query them from other engines&lt;/item&gt;
      &lt;item&gt;Query and import data files in object storage in Parquet, CSV, JSON, and Iceberg format&lt;/item&gt;
      &lt;item&gt;Export query results back to object storage in Parquet, CSV, or JSON formats using COPY commands&lt;/item&gt;
      &lt;item&gt;Read geospatial formats supported by GDAL, such as GeoJSON and Shapefiles&lt;/item&gt;
      &lt;item&gt;Use the built-in map type for semi-structured or key–value data&lt;/item&gt;
      &lt;item&gt;Combine heap, Iceberg, and external Parquet/CSV/JSON files in the same SQL queries and modifications — all with full transactional guarantees and no SQL limitations&lt;/item&gt;
      &lt;item&gt;Infer table columns and types from external data sources such as Iceberg, Parquet, JSON, and CSV files&lt;/item&gt;
      &lt;item&gt;Leverage DuckDB’s query engine underneath for fast execution without leaving Postgres&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are two ways to set up &lt;code&gt;pg_lake&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Using Docker, for an easy, ready-to-run test environment.&lt;/item&gt;
      &lt;item&gt;Building from source, for a manual setup or development use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both approaches include the PostgreSQL extensions, the &lt;code&gt;pgduck_server&lt;/code&gt; application and setting up S3-compatible storage.&lt;/p&gt;
    &lt;p&gt;Follow the Docker README to set up and run &lt;code&gt;pg_lake&lt;/code&gt; with Docker.&lt;/p&gt;
    &lt;p&gt;Once you’ve built and installed the required components, you can initialize &lt;code&gt;pg_lake&lt;/code&gt; inside Postgres.&lt;/p&gt;
    &lt;p&gt;Create all required extensions at once using &lt;code&gt;CASCADE&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;CREATE EXTENSION pg_lake CASCADE;
NOTICE:  installing required extension "pg_lake_table"
NOTICE:  installing required extension "pg_lake_engine"
NOTICE:  installing required extension "pg_extension_base"
NOTICE:  installing required extension "pg_lake_iceberg"
NOTICE:  installing required extension "pg_lake_copy"
CREATE EXTENSION&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;pgduck_server&lt;/code&gt; is a standalone process that implements the Postgres wire-protocol (locally), and underneath uses &lt;code&gt;DuckDB&lt;/code&gt; to execute queries.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;pgduck_server&lt;/code&gt; it starts listening to port &lt;code&gt;5332&lt;/code&gt; on unix domain socket:&lt;/p&gt;
    &lt;code&gt;pgduck_server
LOG pgduck_server is listening on unix_socket_directory: /tmp with port 5332, max_clients allowed 10000
&lt;/code&gt;
    &lt;p&gt;As &lt;code&gt;pgduck_server&lt;/code&gt; implements Postgres wire protocol, you can access it via &lt;code&gt;psql&lt;/code&gt; on port &lt;code&gt;5332&lt;/code&gt; and host &lt;code&gt;/tmp&lt;/code&gt; and run commands via DuckDB.&lt;/p&gt;
    &lt;p&gt;For example, you can get the DuckDB version:&lt;/p&gt;
    &lt;code&gt;psql -p 5332 -h /tmp

select version() as duckdb_version; 
duckdb_version 
---------------- 
v1.3.2 (1 row)&lt;/code&gt;
    &lt;p&gt;You can also provide some additional settings while starting the server, to see all:&lt;/p&gt;
    &lt;code&gt;pgduck_server --help
&lt;/code&gt;
    &lt;p&gt;There are some important settings that should be adjusted, especially on production systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--memory_limit&lt;/code&gt;: Optionally specify the maximum memory of pgduck_server similar to DuckDB's memory_limit, the default is 80 percent of the system memory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--init_file_path &amp;lt;path&amp;gt;&lt;/code&gt;: Execute all statements in this file on start-up&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--cache_dir&lt;/code&gt;: Specify the directory to use to cache remote files (from S3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;pgduck_server&lt;/code&gt; relies on the DuckDB secrets manager for credentials and it follows the credentials chain by default for AWS and GCP. Make sure your cloud credentials are configured properly — for example, by setting them in ~/.aws/credentials.&lt;/p&gt;
    &lt;p&gt;Once you set up the credential chain, you should set the &lt;code&gt;pg_lake_iceberg.default_location_prefix&lt;/code&gt;. This is the location where Iceberg tables are stored:&lt;/p&gt;
    &lt;code&gt;SET pg_lake_iceberg.default_location_prefix TO 's3://testbucketpglake';&lt;/code&gt;
    &lt;p&gt;You can also set the credentials on &lt;code&gt;pgduck_server&lt;/code&gt; for local development with &lt;code&gt;minio&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can create Iceberg tables by adding &lt;code&gt;USING iceberg&lt;/code&gt; to your &lt;code&gt;CREATE TABLE&lt;/code&gt; statements.&lt;/p&gt;
    &lt;code&gt;CREATE TABLE iceberg_test USING iceberg 
      AS SELECT 
            i as key, 'val_'|| i  as val
         FROM 
            generate_series(0,99)i;&lt;/code&gt;
    &lt;p&gt;Then, query it:&lt;/p&gt;
    &lt;code&gt;SELECT count(*) FROM iceberg_test;
 count 
-------
   100
(1 row)&lt;/code&gt;
    &lt;p&gt;You can then see the Iceberg metadata location:&lt;/p&gt;
    &lt;code&gt;SELECT table_name, metadata_location FROM iceberg_tables;


    table_name     |                                                metadata_location
-------------------+--------------------------------------------------------------------------------------------------------------------
 iceberg_test      | s3://testbucketpglake/postgres/public/test/435029/metadata/00001-f0c6e20a-fd1c-4645-87c9-c0c64b92992b.metadata.json&lt;/code&gt;
    &lt;p&gt;You can import or export data directly using &lt;code&gt;COPY&lt;/code&gt; in Parquet, CSV, or newline-delimited JSON formats.  The format is automatically inferred from the file extension, or you can specify it explicitly with &lt;code&gt;COPY&lt;/code&gt; options like &lt;code&gt;WITH (format 'csv', compression 'gzip')&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;-- Copy data from Postgres to S3 with format parquet
-- Read from any data source, including iceberg tables, heap tables or any query results
COPY (SELECT * FROM iceberg_test) TO 's3://testbucketpglake/parquet_data/iceberg_test.parquet';

-- Copy back from S3 to any table in Postgres
-- This example copies into an iceberg table, but could be heap table as well
COPY iceberg_test FROM 's3://testbucketpglake/parquet_data/iceberg_test.parquet';&lt;/code&gt;
    &lt;p&gt;You can create a foreign table directly from a file or set of files without having to specify column names or types.&lt;/p&gt;
    &lt;code&gt;-- use the files under the path, can use * for all files
CREATE FOREIGN TABLE parquet_table() 
SERVER pg_lake 
OPTIONS (path 's3://testbucketpglake/parquet_data/*.parquet');

-- note that we infer the columns from the file
\d parquet_table
              Foreign table "public.parquet_table"
 Column |  Type   | Collation | Nullable | Default | FDW options 
--------+---------+-----------+----------+---------+-------------
 key    | integer |           |          |         | 
 val    | text    |           |          |         | 
Server: pg_lake
FDW options: (path 's3://testbucketpglake/parquet_data/*.parquet')

-- and, query it
select count(*) from parquet_table;
 count 
-------
   100
(1 row)
&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;pg_lake&lt;/code&gt; instance consists of two main components: PostgreSQL with the pg_lake extensions and pgduck_server.&lt;/p&gt;
    &lt;p&gt;Users connect to PostgreSQL to run SQL queries, and the &lt;code&gt;pg_lake&lt;/code&gt; extensions integrate with Postgres’s hooks to handle query planning, transaction boundaries, and overall orchestration of execution.&lt;/p&gt;
    &lt;p&gt;Behind the scenes, parts of query execution are delegated to DuckDB through pgduck_server, a separate multi-threaded process that implements the PostgreSQL wire protocol (locally). This process runs DuckDB together with our duckdb_pglake extension, which adds PostgreSQL-compatible functions and behavior.&lt;/p&gt;
    &lt;p&gt;Users typically don’t need to be aware of &lt;code&gt;pgduck_server&lt;/code&gt;; it operates transparently to improve performance. When appropriate, &lt;code&gt;pg_lake&lt;/code&gt; delegates scanning of the data and the computation to DuckDB’s highly parallel, column-oriented execution engine.&lt;/p&gt;
    &lt;p&gt;This separation also avoids the threading and memory-safety limitations that would arise from embedding DuckDB directly inside the Postgres process, which is designed around process isolation rather than multi-threaded execution. Moreover, it lets us interact with the query engine directly by connecting to it using standard Postgres clients.&lt;/p&gt;
    &lt;p&gt;The team behind pg_lake has a lot of experience building Postgres extensions (e.g. Citus, pg_cron, pg_documentdb). Over time, we’ve learned that large, monolithic PostgreSQL extensions are harder to evolve and maintain.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; follows a modular design built around a set of interoperating components — mostly implemented as PostgreSQL extensions, others as supporting services or libraries.&lt;lb/&gt; Each part focuses on a well-defined layer, such as table and metadata management, catalog and object store integration, query execution, or data format handling. This approach makes it easier to extend, test, and evolve the system, while keeping it familiar to anyone with a PostgreSQL background.&lt;/p&gt;
    &lt;p&gt;The current set of components are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pg_lake_iceberg: a PostgreSQL extension that implements the Iceberg specification&lt;/item&gt;
      &lt;item&gt;pg_lake_table: a PostgreSQL extension that implements a foreign data wrapper to query files in object storage&lt;/item&gt;
      &lt;item&gt;pg_lake_copy: a PostgreSQL extension that implements COPY to/from your data lake&lt;/item&gt;
      &lt;item&gt;pg_lake_engine: a common module for different pg_lake extensions&lt;/item&gt;
      &lt;item&gt;pg_extension_base: A foundational building block for other extensions&lt;/item&gt;
      &lt;item&gt;pg_extension_updater: An extension for updating all extensions on start-up. See README.md.&lt;/item&gt;
      &lt;item&gt;pg_lake_benchmark: a PostgreSQL extension that performs various benchmarks on lake tables. See README.md.&lt;/item&gt;
      &lt;item&gt;pg_map: A generic map type generator&lt;/item&gt;
      &lt;item&gt;pgduck_server: a stand-alone server that loads DuckDB into the same server machine and exposes DuckDB via the PostgreSQL protocol&lt;/item&gt;
      &lt;item&gt;duckdb_pglake: a DuckDB extension that adds missing PostgreSQL functions to DuckDB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; development started in early 2024 at Crunchy Data with the goal of bringing Iceberg to PostgreSQL. The first few months were focused on building a robust integration of an external query engine (DuckDB). To get to market early, we made the query/import/export features available to Crunchy Bridge customers as Crunchy Bridge for Analytics.&lt;/p&gt;
    &lt;p&gt;Next, we started building a comprehensive implementation of the Iceberg (v2) protocol with support for transactions and almost all PostgreSQL features. In November 2024, we relaunched Crunchy Bridge for Analytics as Crunchy Data Warehouse available on Crunchy Bridge and on-premises.&lt;/p&gt;
    &lt;p&gt;In June 2025, Crunchy Data was acquired by Snowflake. Following the acquisition, Snowflake decided to open source the project as &lt;code&gt;pg_lake&lt;/code&gt; in November 2025. The initial version is 3.0 because of the two prior generations. If you’re currently a Crunchy Data Warehouse user there will be an automatic upgrade path, though some names will change.&lt;/p&gt;
    &lt;p&gt;Full project documentation can be found in the docs directory.&lt;/p&gt;
    &lt;p&gt;Copyright (c) Snowflake Inc. All rights reserved. Licensed under the Apache 2.0 license.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; is dependent on third-party projects Apache Avro and DuckDB. During build, &lt;code&gt;pg_lake&lt;/code&gt; applies patches to Avro and certain DuckDB extensions in order to provide the &lt;code&gt;pg_lake&lt;/code&gt; functionality. The source code associated with the Avro and DuckDB extensions is downloaded from the applicable upstream repos and the source code associated with those projects remains under the original licenses. If you are packaging or redistributing packages that include &lt;code&gt;pg_lake&lt;/code&gt;, please note that you should review those upstream license terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Snowflake-Labs/pg_lake"/><published>2025-11-04T16:12:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45813310</id><title>Launch HN: Plexe (YC X25) – Build production-grade ML models from prompts</title><updated>2025-11-05T12:20:23.225989+00:00</updated><content>&lt;doc fingerprint="d1d50ecbdfaae78"&gt;
  &lt;main&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;Your Agentic ML Engineering&lt;/p&gt;
    &lt;p&gt;Team&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your raw data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and it will build a production-ready model thatâs engineered for your exact business challenge.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
    &lt;p&gt;Spotlight&lt;/p&gt;
    &lt;p&gt;Spotlight&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Featured in BIâs 10 Most Exciting AI Startups from YC Spring 2025&lt;/p&gt;
    &lt;p&gt;Featured in BIâs 10 Most Exciting AI Startups from YC Spring 2025&lt;/p&gt;
    &lt;p&gt;Plexe AI Redefines Credit Underwriting With Real-Time ML Models&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Solutions&lt;/p&gt;
    &lt;p&gt;Solutions&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your data into your competitive advantage.&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most. Youâll see whatâs working, whatâs not, and where the real opportunities are hiding. No code, no setup, no fuss.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and weâll build a production-ready model thatâs engineered for your exact business challenge. Whether itâs predicting churn or fraud detection, youâll go from idea to working AI in hours, not months.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most. Youâll see whatâs working, whatâs not, and where the real opportunities are hiding. No code, no setup, no fuss.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and weâll build a production-ready model thatâs engineered for your exact business challenge. Whether itâs predicting churn or fraud detection, youâll go from idea to working AI in hours, not months.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.plexe.ai/"/><published>2025-11-04T17:07:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45813767</id><title>Codemaps: Understand Code, Before You Vibe It</title><updated>2025-11-05T12:20:23.038118+00:00</updated><content>&lt;doc fingerprint="3fc296052cb3ae5f"&gt;
  &lt;main&gt;
    &lt;quote&gt;“Your code is your understanding of the problem you’re exploring. So it’s only when you have your code in your head that you really understand the problem.” — Paul Graham&lt;/quote&gt;
    &lt;p&gt;Software development only becomes engineering with understanding. Your ability to reason through your most challenging coding tasks is constrained by your mental model of how things work — in other words, how quickly and how well you onboard to any codebase for solving any problem. However most AI vibe coding tools are aimed at relieving you of that burden by reading → thinking → writing the code for you, increasing the separation from you and your code. This is fine for low value, commodity tasks, but absolutely unacceptable for the hard, sensitive, and high value work that defines real engineering.&lt;/p&gt;
    &lt;p&gt;We all need more AI that turns your brain ON, not OFF.&lt;/p&gt;
    &lt;p&gt;Today we are announcing Windsurf Codemaps, which are first-of-its-kind AI-annotated structured maps of your code, powered by SWE-1.5 and Claude Sonnet 4.5. Building on our popular work from DeepWiki and Ask Devin, Codemaps is the next step in hyper-contextualized codebase understanding, grounded in precise code navigation.&lt;/p&gt;
    &lt;p&gt;Every engineering task — debugging, refactors, new features — starts with understanding. Great engineers aren’t just good at writing code; they’re good at reading it, building mental models that span files, layers, and systems.&lt;/p&gt;
    &lt;p&gt;But modern codebases are sprawling: hundreds of files, multiple services, dense abstractions. Based on own experience and deep conversations with our customers across the Fortune 500, even top engineers spend much of their deep-work time finding and remembering what matters.&lt;/p&gt;
    &lt;p&gt;It’s a huge tax on productivity:&lt;/p&gt;
    &lt;p&gt;This is the frontier that AI coding tools haven’t yet solved. Onboarding isn’t even a onetime cost, you pay it every time you switch context and codebases. The faster and better you understand your codebase, the faster and better you’ll be able to fix it yourself, or prompt agents to do it.&lt;/p&gt;
    &lt;p&gt;Until today, the standard approach by Copilot, Claude Code, Codex, and even Windsurf Cascade, was to have you ask questions of a generalist agent with access to your code in a typical chat experience. But those solutions don’t solve focused onboarding and strongly grounded navigation to onboard, debug, and better context engineer for your codebase.&lt;/p&gt;
    &lt;p&gt;At Cognition, we’ve been investing far more deeply in understanding:&lt;/p&gt;
    &lt;p&gt;Codemaps is our next investment in tooling that makes engineers the best versions of themselves.&lt;/p&gt;
    &lt;p&gt;When you first open Codemaps (click the new maps icon or Cmd+Shift+C in Windsurf) with a codebase opened in Windsurf, you can enter in a prompt for the task you are trying to do, or take one of the automatic suggestions. You can choose a Fast (SWE-1.5) or Smart (Sonnet 4.5) model to generate your Codemap. Every Codemap is a snapshot of your code and respects ZDR.&lt;/p&gt;
    &lt;p&gt;Based on our demos to customers, you will experience Codemaps best on your own codebase and asking a question about how or where some functionality works. In our dogfooding, we find particular effectiveness tracing through client-server problems or a data pipeline or debugging auth/security issues:&lt;/p&gt;
    &lt;p&gt;If all you wanted was to quickly jump through grouped and nested parts of your code that related to your question, this is already an improvement compared to asking the same question in Cascade, where answers are not as densely linked to the exact lines of code.&lt;/p&gt;
    &lt;p&gt;You can also toggle over to a visually drawn Codemap, which performs the same functions when you click on individual nodes: they send you to the exact part of the codebase you clicked on.&lt;/p&gt;
    &lt;p&gt;However, if you want a little more context, then you can hit “See more” in any section to expand our “trace guide” that gives a more descriptive explanation of what groups the discovered lines together.&lt;/p&gt;
    &lt;p&gt;Finally, inside Cascade you can also reference a codemap for the agent with &lt;code&gt;@{codemap}&lt;/code&gt; (all of it, or a particular subsection) in your prompt to provide more specific context and dramatically improve the performance of your agent for your task.&lt;/p&gt;
    &lt;p&gt;We feel that the popular usage of “vibe coding” has strayed far from the original intent, into a blanket endorsement of plowing through any and all AI generated code slop. If you look at the difference between the most productive vs the problematic AI-assisted coders, the productive ones can surf the vibes of code that they understand well, whereas people get into trouble when the code they generate and maintain starts to outstrip their ability to understand it.&lt;/p&gt;
    &lt;p&gt;To understand is to be accountable. As AI takes on more of the easy work, the hard problems left to humans are the ones that demand real comprehension: debugging complex systems, refactoring legacy code, making architecture decisions. In this new era, the engineer’s role shifts from authoring to accountability — you might not write every line, but you’re still responsible for what ships. That accountability depends on understanding what the AI produced, why it changed, and whether it’s safe. Codemaps closes that gap by giving both the human and the AI a shared picture of the system: how it’s structured, how data flows, where dependencies live. Codemaps is our latest Fast Agent, but as we discussed in the Semi-Async Valley of Death, our goal isn't just about speed, it is to help your human engineers stay in flow, stay on top of their code, and to move faster and more confidently on the hardest problems, never shipping slop that they don't understand.&lt;/p&gt;
    &lt;p&gt;Augment engineers for high value work, relieve them of low value work. The other local minima that the coding agent industry has gotten stuck in is in the general messaging of replacing engineers for low value work and not having any solutions for the hardest tasks apart from “pls ultrathink high, no mistakes”, which only gives autonomy to the agent, at the expense of the engineer. The long history of human-machine collaboration teaches us that we can always do more with the synergy rather than humans-alone or AI-alone. Our view is that the AI product that engineers will love most is the one that makes them better at their job, not the one that tries to replace them with a sloppy facsimile of themselves.&lt;/p&gt;
    &lt;p&gt;With Codemaps, we are now exposing to humans some of the indexing and analysis we do inside of our coding agents. These artifacts are sharable today across teams for learning and discussion, but we have yet to benchmark how much better they can make our coding agents like Devin and Cascade in solving challenging tasks on their own. We also see opportunities for connecting and annotating codemaps, as well as defining an open &lt;code&gt;.codemap&lt;/code&gt; protocol that can be used by other code agents and custom tooling built by you. Complementing our Fast Context feature, this is an advancement in human-readable automatic context engineering.&lt;/p&gt;
    &lt;p&gt;You can try Codemaps on the latest versions of Windsurf, or DeepWiki!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cognition.ai/blog/codemaps"/><published>2025-11-04T17:47:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816041</id><title>I took all my projects off the cloud, saving thousands of dollars</title><updated>2025-11-05T12:20:22.947168+00:00</updated><content/><link href="https://rameerez.com/send-this-article-to-your-friend-who-still-thinks-the-cloud-is-a-good-idea/"/><published>2025-11-04T21:22:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816673</id><title>Grayskull: A tiny computer vision library in C for embedded systems, etc.</title><updated>2025-11-05T12:20:22.496162+00:00</updated><content>&lt;doc fingerprint="6e340ed9bad71609"&gt;
  &lt;main&gt;
    &lt;p&gt;Grayskull is a minimalist, dependency-free computer vision library designed for microcontrollers and other resource-constrained devices. It focuses on grayscale images and provides modern, practical algorithms that fit in a few kilobytes of code. Single-header design, integer-based operations, pure C99.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Image operations: copy, crop, resize (bilinear), downsample&lt;/item&gt;
      &lt;item&gt;Filtering: blur, Sobel edges, thresholding (global, Otsu, adaptive)&lt;/item&gt;
      &lt;item&gt;Morphology: erosion, dilation&lt;/item&gt;
      &lt;item&gt;Geometry: connected components, perspective warp&lt;/item&gt;
      &lt;item&gt;Features: FAST/ORB keypoints and descriptors (object tracking)&lt;/item&gt;
      &lt;item&gt;Local binary patterns: LBP cascades to detect faces, vehicles etc&lt;/item&gt;
      &lt;item&gt;Utilities: PGM read/write&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As usual, no dependencies, no dynamic memory allocation, no C++, no surprises. Just a single header file under 1KLOC.&lt;/p&gt;
    &lt;p&gt;Check out the examples folder for more!&lt;/p&gt;
    &lt;p&gt;Online demo: try Grayskull in your browser.&lt;/p&gt;
    &lt;code&gt;#include "grayskull.h"

struct gs_image img = gs_read_pgm("input.pgm");
struct gs_image blurred = gs_alloc(img.w, img.h);
struct gs_image binary = gs_alloc(img.w, img.h);

gs_blur(blurred, img, 2);
gs_threshold(binary, blurred, gs_otsu_theshold(blurred));

gs_write_pgm(binary, "output.pgm");
gs_free(img);
gs_free(blurred);
gs_free(binary);&lt;/code&gt;
    &lt;p&gt;Note that &lt;code&gt;gs_alloc&lt;/code&gt;/&lt;code&gt;gs_free&lt;/code&gt; are optional helpers; you can allocate image pixel buffers any way you like.&lt;/p&gt;
    &lt;code&gt;struct gs_image { unsigned w, h; uint8_t *data; };
struct gs_rect { unsigned x, y, w, h; }; // ROI
struct gs_point { unsigned x, y; }; // corners

uint8_t gs_get(struct gs_image img, unsigned x, unsigned y);
void gs_set(struct gs_image img, unsigned x, unsigned y, uint8_t value);
void gs_crop(struct gs_image dst, struct gs_image src, struct gs_rect roi);
void gs_copy(struct gs_image dst, struct gs_image src);
void gs_resize(struct gs_image dst, struct gs_image src);
void gs_downsample(struct gs_image dst, struct gs_image src);

// Thresholding
void gs_histogram(struct gs_image img, unsigned hist[256]);
void gs_threshold(struct gs_image img, uint8_t threshold);
uint8_t gs_otsu_threshold(struct gs_image img);
void gs_adaptive_threshold(struct gs_image dst, struct gs_image src, unsigned radius, int c);

// Filters
void gs_blur(struct gs_image dst, struct gs_image src, unsigned radius);
void gs_erode(struct gs_image dst, struct gs_image src);
void gs_dilate(struct gs_image dst, struct gs_image src);
void gs_sobel(struct gs_image dst, struct gs_image src);

// Blobs (connected components) and contours
typedef uint16_t gs_label;
struct gs_blob { gs_label label; unsigned area; struct gs_rect box; struct gs_point centroid; };
struct gs_contour { struct gs_rect box; struct gs_point start; unsigned length; };
unsigned gs_blobs(struct gs_image img, gs_label *labels, struct gs_blob *blobs, unsigned nblobs);
void gs_blob_corners(struct gs_image img, gs_label *labels, struct gs_blob *b, struct gs_point c[4]);
void gs_perspective_correct(struct gs_image dst, struct gs_image src, struct gs_point c[4]);
void gs_trace_contour(struct gs_image img, struct gs_image visited, struct gs_contour *c);

// FAST/ORB
struct gs_keypoint { struct gs_point pt; unsigned response; float angle; uint32_t descriptor[8]; };
struct gs_match { unsigned idx1, idx2; unsigned distance; };
unsigned gs_fast(struct gs_image img, struct gs_image scoremap, struct gs_keypoint *kps, unsigned nkps, unsigned threshold);
float gs_compute_orientation(struct gs_image img, unsigned x, unsigned y, unsigned r);
void gs_brief_descriptor(struct gs_image img, struct gs_keypoint *kp);
unsigned gs_orb_extract(struct gs_image img, struct gs_keypoint *kps, unsigned nkps, unsigned threshold, uint8_t *scoremap_buffer);
unsigned gs_match_orb(const struct gs_keypoint *kps1, unsigned n1, const struct gs_keypoint *kps2, unsigned n2, struct gs_match *matches, unsigned max_matches, float max_distance);

// LBP cascades
struct gs_lbp_cascade { uint16_t window_w, window_h; uint16_t nfeatures, nweaks, nstages; const int8_t *features; /* [nfeatures * 4] */ const uint16_t *weak_feature_idx; const float *weak_left_val, *weak_right_val; const uint16_t *weak_subset_offset, *weak_num_subsets; const int32_t *subsets; const uint16_t *stage_weak_start, *stage_nweaks; const float *stage_threshold; };
void gs_integral(struct gs_image src, unsigned *ii);
unsigned gs_lbp_window(const struct gs_lbp_cascade *c, const unsigned *ii, unsigned iw, unsigned ih, int x, int y, float scale);
unsigned gs_lbp_detect(const struct gs_lbp_cascade *c, const unsigned *ii, unsigned iw, unsigned ih, struct gs_rect *rects, unsigned max_rects, float scale_factor, float min_scale, float max_scale, int step);

// Optional:
struct gs_image gs_alloc(unsigned w, unsigned h);
void gs_free(struct gs_image img);
struct gs_image gs_read_pgm(const char *path);
int gs_write_pgm(struct gs_image img, const char *path);&lt;/code&gt;
    &lt;p&gt;This project is licensed under the MIT License. Feel free to use in research, products, and your next embedded vision project!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/zserge/grayskull"/><published>2025-11-04T22:35:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816853</id><title>Mr TIFF</title><updated>2025-11-05T12:20:22.418355+00:00</updated><content>&lt;doc fingerprint="6498b18bb326a2bd"&gt;
  &lt;main&gt;
    &lt;p&gt;For as long as I have published my books, one of my overarching goals was to give credit to those who actually invented the hardware and software that we use.&lt;/p&gt;
    &lt;p&gt;I have spent 10,000+ hours to create an accurate record of their work but I'm not complaining. The 'as-close-to-possible' truth of invention by individuals or teams meant identifying the work, educating myself, writing questions, and sending emails. And after that process, I set up a chat because it all gets down to talking to someone on the other side of the world, about something that happened 30 or 40 years ago.&lt;/p&gt;
    &lt;p&gt;If the invention involves a team, I try to interview more than one person, so I can cross-check the facts. Not to call anyone out, it’s just that, given time, we all forget the facts. And everyone adds their personal take. It’s because of that, for example, that I know the English musician Peter Gabriel really did visit Apple's research labs as they tested the Apple Sound Chip, and gave the team his personal approval to use the song 'Red Rain' for the Macintosh II launch. Wil Oxford, Steve Perlman, Mike Potel, Mark Lentczner and Steve Milne told me so.&lt;/p&gt;
    &lt;p&gt;As I was wrapping up Version 2.3 of Inventing the Future, I spoke with Steve M and Mark about the AIFF (Audio Interchange File Format) audio standard that they built around the same time as their VIP visit. They did so as professional programmers, amateur musicians and electronic music experts. Milne and Lentczner knew users needed a standard file format to make their work lives easier and to fend off confusion in the nascent MIDI marketplace. But it didn't exist. So Steve and Mark consulted with users and manufacturers in the Apple cafeteria after hours. This work is interesting on its own but it also underpinned other research. The AIFF, Apple Sound Chip, and MIDI Manager work scaffolded QuickTime and its extensible video formats and programs in 1991. Senior engineer Toby Farrand told me:&lt;/p&gt;
    &lt;p&gt;Audio drove the development of QuickTime more than anything.&lt;/p&gt;
    &lt;p&gt;So who or what drove the development of AIFF?&lt;/p&gt;
    &lt;p&gt;Steve and Mark referred me to the IFF (Interchange File Format (IFF) and the TIFF (Tag Image File Format) that were built before AIFF, in 1985 and 1986 respectively. These file formats were the benchmark for open media standards. My search pivoted, as it always does, to understand those inventions. I expected to be able to find the engineer or engineers names, track them down and interview them. It has worked around 100 times before.&lt;/p&gt;
    &lt;p&gt;Jerry Morrison created IFF while working at Electronic Arts and then went to Apple, where he liaised with the AIFF team. I could easily background his work.&lt;/p&gt;
    &lt;p&gt;So I turned my attention to TIFF, built initially as an image standard for desktop publishing. TIFF was able to store monochrome, grayscale, and color images, alongside metadata such as size, compression algorithms, and color space information. In many ways, it was a lot like AIFF so I was keen to know more. But I couldn't find a TIFF creator. No matter how I enquired, Aldus created TIFF.&lt;/p&gt;
    &lt;p&gt;To be clear, while a search for AIFF will offer up a company (Apple) not a person, I was able to find Milne and Lentczner in part because of their unique names and because Apple publicised the AIFF work and those publications are archived.&lt;/p&gt;
    &lt;p&gt;All I had was Aldus, an American company that created desktop publishing with the help of Apple and Adobe. In fact, Paul Brainerd, the cofounder of Aldus coined the term 'desktop publishing' to quickly explain the technicality of what they were doing to potential investors. But Aldus and their seminal product, PageMaker, are long gone, and there were no breadcrumbs for TIFF's creation.&lt;/p&gt;
    &lt;p&gt;Finally, after a day-long trawl through MacWeek back issues, I found Steve Carlson. (below)&lt;/p&gt;
    &lt;p&gt;Then I ran a similar length search through the Computer History Museum’s amazing Oral Histories transcriptions. Brainerd mentioned Carlson's name in an interview. (below)&lt;/p&gt;
    &lt;p&gt;But it was too brief an explanation so I kept looking. Then the trail went cold.&lt;/p&gt;
    &lt;p&gt;And that was because, folks had misspelt his name when quoting him and then that was copied into magazines, and reviews and so forth. Brainerd's CHM interview transcript was wrong. But I didn’t know that.&lt;/p&gt;
    &lt;p&gt;I just kept looking for Steve Carlson.&lt;/p&gt;
    &lt;p&gt;I found other inventors because they had unique middle or last names or by random methods such as searching glider pilot licences in the Napa Valley after a tip from a former colleague that 'so and so' was a pilot in retirement. I had no tips, no links, nothing.&lt;/p&gt;
    &lt;p&gt;Why couldn’t I find Steve Carlson?&lt;/p&gt;
    &lt;p&gt;All the while, the answer was right under my nose. I had downloaded the final Aldus TIFF specifications document, hoping to find the author’s name. However, the name is seemingly written in white text on white paper - making it invisible. What?&lt;/p&gt;
    &lt;p&gt;See below where I have highlighted the region with a blue block over the text.&lt;/p&gt;
    &lt;p&gt;For a reason I can’t recall, I downloaded a plain text version and typed in Carlson to see if he was mentioned, but I must have paused at ‘Carls...' and the search functionality automatically filled in the rest. Suddenly I was staring at:&lt;/p&gt;
    &lt;p&gt;Author/Editor/Arbitrator: Steve Carlsen.&lt;/p&gt;
    &lt;p&gt;‘Carls-EN’&lt;/p&gt;
    &lt;p&gt;A quick trip to Google patents, and a search for Steve Carlsen, Stephen Carlsen. Bingo! Stephen E. Carlsen’s patents at Aldus (and Adobe) in Issaquah, WA.&lt;/p&gt;
    &lt;p&gt;I checked the geography, as most folks of a certain age do not stray far from the addresses filed in their patents, and typed Stephen’s correctly spelled surname into the online US White Pages for Washington State. There was ‘a’ Stephen Carlsen listed in a retirement village in WA. His age matched, but there were no public facing email addresses.&lt;/p&gt;
    &lt;p&gt;I searched bulletin boards on the topic of TIFF, as I had found a former Apple engineer that way. Don had picked an abbreviation of his initials and numbers to post on BBS in his college days and then carried that same combination into adulthood. Many of us did. I took a punt pasting his unique prefix into hotmail, gmail etc. and found Don and interviewed him, but - Stephen Carlsen did not show up in a BBS. So, no email to try.&lt;/p&gt;
    &lt;p&gt;My ‘last straw' method for finding someone is a stamped envelope. I wrote, printed and mailed a one-page letter to Stephen's listed address, and crossed my fingers. Four months later he popped up in my email.&lt;/p&gt;
    &lt;p&gt;It was a surprise and a relief. We swapped a few emails, and he confirmed the TIFF catalyst story. For Stephen it was 'no big deal'. Once he had built the initial TIFF, Aldus needed to convince 3rd party developers and scanner manufacturers to agree to TIFF as a standard.&lt;/p&gt;
    &lt;p&gt;“We had to define and promote an industry standard for storing and processing scanned images, so that we wouldn't have to write import filters for every model of every scanner that would soon be entering the budding desktop scanner market."&lt;/p&gt;
    &lt;p&gt;Stephen himself did much of the evangelizing as Paul Brainerd later pointed out:&lt;/p&gt;
    &lt;p&gt;“(Steve) developed the standard, and then we went out and promoted it in a series of meetings with specific companies - as well as some workshops we ran in Seattle and the Bay Area during the Seybold shows and the MacWorld shows.”&lt;/p&gt;
    &lt;p&gt;I sent Stephen a draft of what I had written and he sent a prompt reply saying - ‘Looks good’.&lt;/p&gt;
    &lt;p&gt;I followed up asking him how he ended up at a tiny startup in Seattle called Aldus.&lt;/p&gt;
    &lt;p&gt;At that time, I was interviewing for a graphics position at Boeing Computer Services in Seattle, and noticed a small wanted ad that sounded really interesting, and seemed to be an excellent match for my background and interests. I interviewed with Paul and the 5-person mostly-ex-Atex engineering team, and I was hired.&lt;/p&gt;
    &lt;p&gt;Out of curiosity I put Stephen's email address, now that I knew it, into a Duck Duck search and found him helping people online with TIFF queries long after Aldus had been acquired by Adobe. He also contributed to a Google Group called tiffcentral.&lt;/p&gt;
    &lt;p&gt;Having interviewed so many people across more than a decade, I’ve got pretty good at judging those who would like to talk or type, those who are verbose and those that are not. I knew Stephen had said what he was going to say. I added his pioneering work on TIFF to the AIFF story and moved on.&lt;/p&gt;
    &lt;p&gt;Two years had flown by when I received an email yesterday. His ex-wife Peggy found my paper letter and wrote to me. Stephen passed away earlier this year.&lt;/p&gt;
    &lt;p&gt;Thank you for your interest in and support of Stephen’s brilliant work creating TIFF. I’m not surprised Stephen didn’t finish corresponding with you, as he had begun to struggle with using his computer and phone. Some days were better than others for him, but he began to lose touch with people during those months you were reaching out to him. He was a humble man, and I guess never pushed to be recognized, although I believe those who worked with him knew the truth. His last week was in my home, where he was never left alone.&lt;/p&gt;
    &lt;p&gt;Peggy finished the email with, ‘I called him Mr TIFF up to his last moment.'&lt;/p&gt;
    &lt;p&gt;The 10,000+ hours of book research disappeared in an instant. As sad as it was, I could see clearly that all of my work was worth it. Every single second. Because of this email.&lt;/p&gt;
    &lt;p&gt;Mr TIFF.&lt;/p&gt;
    &lt;p&gt;Last night, as everyone in my house went to sleep, I took a deep breath and edited the Wikipedia page for TIFF, the Tag Image File Format.&lt;/p&gt;
    &lt;p&gt;It no longer reads ‘created by Aldus’, it reads ‘…created by Stephen Carlsen, an engineer at Aldus'&lt;/p&gt;
    &lt;p&gt;"Inventing the Future" here -&amp;gt; https://books.by/john-buck/inventing-the-future&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://inventingthefuture.ghost.io/mr-tiff/"/><published>2025-11-04T22:57:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816879</id><title>Patching 68K Software – SimpleText</title><updated>2025-11-05T12:20:22.242699+00:00</updated><content>&lt;doc fingerprint="b2497ef403174708"&gt;
  &lt;main&gt;
    &lt;p&gt;Someone asked to have SimpleText open a smaller text window at startup. Initially, I assumed this would be a fairly easy fix by just overwriting a few constant values in SimpleText code. It turned out to be a pain -- but I learned a lot along the way.&lt;lb/&gt;You need to have the code editor (from one of the Apple developer CDs) in your ResEdit preference file in order to disassemble code resources within ResEdit. Then, open each 'CODE' resource of SimpleText and search for _SizeWindow (A91D). Or, just skim the code until you see system calls that look like they have something to do with windows. Here is a nice chunk in routine Anon48. A new window is created, then it looks at the main device, looks at the size of the menu bar (MBarHeight), sets the port, and moves and sizes the window.&lt;lb/&gt;But, uh oh, earlier in the code it checks what type of window it is opening. A patch is going to need to avoid resizing code for pictures, video, and the about box.&lt;lb/&gt;I could not locate any obvious constants to adjust. Instead, I would need to inject a more complicated routine that detects whether it was a text window and substitute a new rectangle for the size. But, you cannot simply insert code, as it would move all subsequent code down, and the jumps (subroutine calls) that cross over that location would now jump to the wrong spots. So, I need to jump out of their code to my own (appended at the end of the code resource) and return.&lt;lb/&gt;For example, SimpleText's code to get the size of MBarHeight can easily be performed elsewhere. My routine need only return the MBarHeight in register D0 before returning. That gives me 8 bytes to overwrite with my jump.&lt;lb/&gt;Here is my replacement code. It still is only 8 bytes. But, I now jump to my subroutine, check the result, and jump over their resizing code if my routine says it is changing the window size.&lt;lb/&gt;In my subroutine, I make sure all the needed information is in registers (which I checked that SimpleText was not using), I call my various functions and then perform any work that was lost from overwriting or jumping over the original code. Specifically, I get the MBarHeight into D0 and set the current port to the window.&lt;lb/&gt;Easy! But, later in the code, SimpleText reads the content of whatever document is being opened and once again resizes the window. So, I needed to patch later code as well. How could I determine at that point that a replacement window size was being used? I simply store the result of the first subroutine (see SetRecentResult above) and then check it on later calls.&lt;lb/&gt;Where could I store this information? it is not possible to add global variables and the registers are all reused by SimpleText between the first and second routines. Well, you can store a variable (or an entire structure with many variables) within the code itself. Here is my little workaround for CodeWarrior.&lt;lb/&gt;A couple of other tricks.&lt;lb/&gt;1. The system routine GetHandleSize has some glue code (they intercept the call in a local library before calling the system). I needed this call, but didn't want to add the weight of CodeWarrior's libary. So, I defined the direct call to GetHandleSize (I didn't need the glue fix).&lt;lb/&gt;2. You can pass any of the scratch registers (D0-D2, A0-A1, FP0-FP3) to a C function. The way of defining that in CodeWarrior is noted below. You cannot use any other registers. To make debugging easier, I wrote the original subroutine as a true C function, and the register-&amp;gt;C function as a wrapper.&lt;lb/&gt;3. CodeWarrior does not support BSR for some reason. Use JSR instead. Also, a called routine must be placed before the caller routine in order to generate a short relative JSR rather than absolute address. See my 'RecentResult' example above, where the routines that call RecentResult are placed after it in code.&lt;lb/&gt;4. SimpleText stores literals (strings, constants) at the end of the 'CODE' resource. After that is where I placed my code. Unfortunately, this breaks disassembly in ResEdit. Below, do you see 'A9FF'? That's the '_Debugger' trap call. It is follow by the rest of the code, and the MacsBug symbols for "SimpleTextWindowChoicePrep'&lt;lb/&gt;I then needed to hand compute the JSR patched in the original SimpleText code to this location at the end of the code resource.&lt;lb/&gt;5. To make it easy to redefine window sizes in the future, I added a resource.&lt;lb/&gt;The ResEdit definition of this resource is the TMPL. By the way, I have experienced corruption twice with ResEdit 2.1.3. Perhaps it has a bug with templates?&lt;lb/&gt;I doubt this information will be useful to most people. However, it may help avoid some frustrating issues for those few people that attempt patching old software.&lt;lb/&gt;Attached is the hacked version of SimpleText.&lt;lb/&gt;- David&lt;/p&gt;
    &lt;p&gt;You need to have the code editor (from one of the Apple developer CDs) in your ResEdit preference file in order to disassemble code resources within ResEdit. Then, open each 'CODE' resource of SimpleText and search for _SizeWindow (A91D). Or, just skim the code until you see system calls that look like they have something to do with windows. Here is a nice chunk in routine Anon48. A new window is created, then it looks at the main device, looks at the size of the menu bar (MBarHeight), sets the port, and moves and sizes the window.&lt;/p&gt;
    &lt;p&gt;But, uh oh, earlier in the code it checks what type of window it is opening. A patch is going to need to avoid resizing code for pictures, video, and the about box.&lt;/p&gt;
    &lt;p&gt;I could not locate any obvious constants to adjust. Instead, I would need to inject a more complicated routine that detects whether it was a text window and substitute a new rectangle for the size. But, you cannot simply insert code, as it would move all subsequent code down, and the jumps (subroutine calls) that cross over that location would now jump to the wrong spots. So, I need to jump out of their code to my own (appended at the end of the code resource) and return.&lt;/p&gt;
    &lt;p&gt;For example, SimpleText's code to get the size of MBarHeight can easily be performed elsewhere. My routine need only return the MBarHeight in register D0 before returning. That gives me 8 bytes to overwrite with my jump.&lt;/p&gt;
    &lt;p&gt;Here is my replacement code. It still is only 8 bytes. But, I now jump to my subroutine, check the result, and jump over their resizing code if my routine says it is changing the window size.&lt;/p&gt;
    &lt;p&gt;In my subroutine, I make sure all the needed information is in registers (which I checked that SimpleText was not using), I call my various functions and then perform any work that was lost from overwriting or jumping over the original code. Specifically, I get the MBarHeight into D0 and set the current port to the window.&lt;/p&gt;
    &lt;p&gt;Easy! But, later in the code, SimpleText reads the content of whatever document is being opened and once again resizes the window. So, I needed to patch later code as well. How could I determine at that point that a replacement window size was being used? I simply store the result of the first subroutine (see SetRecentResult above) and then check it on later calls.&lt;/p&gt;
    &lt;p&gt;Where could I store this information? it is not possible to add global variables and the registers are all reused by SimpleText between the first and second routines. Well, you can store a variable (or an entire structure with many variables) within the code itself. Here is my little workaround for CodeWarrior.&lt;/p&gt;
    &lt;p&gt;A couple of other tricks.&lt;/p&gt;
    &lt;p&gt;1. The system routine GetHandleSize has some glue code (they intercept the call in a local library before calling the system). I needed this call, but didn't want to add the weight of CodeWarrior's libary. So, I defined the direct call to GetHandleSize (I didn't need the glue fix).&lt;/p&gt;
    &lt;p&gt;2. You can pass any of the scratch registers (D0-D2, A0-A1, FP0-FP3) to a C function. The way of defining that in CodeWarrior is noted below. You cannot use any other registers. To make debugging easier, I wrote the original subroutine as a true C function, and the register-&amp;gt;C function as a wrapper.&lt;/p&gt;
    &lt;p&gt;3. CodeWarrior does not support BSR for some reason. Use JSR instead. Also, a called routine must be placed before the caller routine in order to generate a short relative JSR rather than absolute address. See my 'RecentResult' example above, where the routines that call RecentResult are placed after it in code.&lt;/p&gt;
    &lt;p&gt;4. SimpleText stores literals (strings, constants) at the end of the 'CODE' resource. After that is where I placed my code. Unfortunately, this breaks disassembly in ResEdit. Below, do you see 'A9FF'? That's the '_Debugger' trap call. It is follow by the rest of the code, and the MacsBug symbols for "SimpleTextWindowChoicePrep'&lt;/p&gt;
    &lt;p&gt;I then needed to hand compute the JSR patched in the original SimpleText code to this location at the end of the code resource.&lt;/p&gt;
    &lt;p&gt;5. To make it easy to redefine window sizes in the future, I added a resource.&lt;/p&gt;
    &lt;p&gt;The ResEdit definition of this resource is the TMPL. By the way, I have experienced corruption twice with ResEdit 2.1.3. Perhaps it has a bug with templates?&lt;/p&gt;
    &lt;p&gt;I doubt this information will be useful to most people. However, it may help avoid some frustrating issues for those few people that attempt patching old software.&lt;/p&gt;
    &lt;p&gt;Attached is the hacked version of SimpleText.&lt;/p&gt;
    &lt;p&gt;- David&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tinkerdifferent.com/threads/patching-68k-software-simpletext.4793/"/><published>2025-11-04T22:59:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816963</id><title>UPS plane crashes near Louisville airport</title><updated>2025-11-05T12:20:22.065241+00:00</updated><content>&lt;doc fingerprint="77180153a17383ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;UPS plane crashes near Louisville airport, at least 7 killed, officials say&lt;/head&gt;
    &lt;p&gt;At least seven people were killed and several others injured after a UPS plane crashed shortly after takeoff near the Louisville International Airport on Tuesday, officials said.&lt;/p&gt;
    &lt;p&gt;The number of fatalities is expected to increase, Kentucky Gov. Andy Beshear wrote on social media Tuesday night. At least 11 people were injured, officials said earlier.&lt;/p&gt;
    &lt;p&gt;Beshear said in a statement Wednesday morning that 16 families have reported loved ones who are unaccounted for. "A significant search and rescue mission was underway overnight, which is continuing this morning," he said.&lt;/p&gt;
    &lt;p&gt;Louisville Mayor Craig Greenberg confirmed at a news conference Tuesday night that at least four people had been killed on the ground.&lt;/p&gt;
    &lt;p&gt;UPS Flight 2976 crashed around 5:15 p.m. local time after it departed from the Louisville airport, according to the Federal Aviation Administration. The aircraft was headed to Daniel K. Inouye International Airport in Honolulu, Hawaii, when it went down three miles south of the airfield, Louisville airport public safety officer Jonathan Biven said at a news conference.&lt;/p&gt;
    &lt;p&gt;Beshear said there was no hazardous cargo onboard the plane that would create environmental issues around the crash site, but urged residents to follow any shelter-in-place orders.&lt;/p&gt;
    &lt;p&gt;The Louisville Metro Police Department described the scene as active with "fire and debris," warning residents to stay away from Fern Valley and Grade Lane, an intersection located on the south side of the airport, which serves as the hub of UPS.&lt;/p&gt;
    &lt;p&gt;More than 100 firefighters responded to the crash and were still battling hot spots as of Tuesday night, Greenberg said. Louisville Fire Chief Brian O'Neill noted the fire was "almost entirely contained."&lt;/p&gt;
    &lt;p&gt;Greenberg noted on social media Wednesday morning that there were more than 200 first responders of all types at the scene in all.&lt;/p&gt;
    &lt;p&gt;O'Neill said the fire had been extinguished enough to allow a formal grid search for any possible victims in the area.&lt;/p&gt;
    &lt;p&gt;Videos of the crash showed the aircraft partially on fire as it sped down the runway before it burst into flames.&lt;/p&gt;
    &lt;p&gt;A shelter-in-place order was reduced to a one-mile radius of the crash site, authorities said. Then Greenberg said in his post Wednesday morning it was down to a quarter-mile radius.&lt;/p&gt;
    &lt;p&gt;The police department also urged those in the area to turn off any air intake systems as soon as possible due to the smoke in the area.&lt;/p&gt;
    &lt;p&gt;"Anybody who has seen the images and the video knows how violent this crash is, and there are a lot of families that are gonna be waiting and wondering for a period of time. We're going to try to get them that information as fast as we can," Beshear said.&lt;/p&gt;
    &lt;p&gt;UPS said in a statement that it was notified of an incident involving one of its aircraft. Three UPS crewmembers were on board, the company said. It didn't immediately provide more details.&lt;/p&gt;
    &lt;p&gt;"We do not at the moment have the status of the crew," Beshear said. "Watching that video, I think we're all very, very worried about them."&lt;/p&gt;
    &lt;p&gt;Businesses in the area were heavily impacted by the crash, including Kentucky Petroleum Recycling and Grade A Auto Parts, the governor said.&lt;/p&gt;
    &lt;p&gt;All arriving and departing flights at the Louisville airport were temporarily suspended. The airport was closed Tuesday night but one runway as reopened Wednesday morning, Greenberg said.&lt;/p&gt;
    &lt;p&gt;According to preliminary flight data from FlightRadar24, the plane appeared to hit 175 feet in altitude briefly after takeoff. It would have been full of fuel for the flight to Hawaii, which likely led to the significant fire as seen from CBS affiliate WLKY-TV's chopper.&lt;/p&gt;
    &lt;p&gt;The three-engine McDonnell Douglas MD-11 was manufactured in 1991, according to FAA data.&lt;/p&gt;
    &lt;p&gt;It was carrying approximately 38,000 gallons of fuel, which weighs about 233,000 pounds, Louisville Fire Chief Brian O'Neill said. The area affected by the crash is about a city block wide, he said, but it has been difficult to contain the fire due to surrounding hazardous materials.&lt;/p&gt;
    &lt;p&gt;Greenberg urged any residents who find debris on their property not to touch it, and instead report it through a website that should be live by Wednesday.&lt;/p&gt;
    &lt;p&gt;The crash is where UPS Worldport, an international air hub for the parcel service, is located. UPS said it was halting package sorting operations at Worldport on Tuesday night.&lt;/p&gt;
    &lt;p&gt;"This is a UPS town," Louisville Metro Councilwoman Betsy Ruhe, whose district is part of the crash site, said during the news conference Tuesday night. "We all know somebody who works at UPS, and they're all texting their friends, their family, trying to make sure everyone is safe. Sadly, some of those texts are probably going to go unanswered."&lt;/p&gt;
    &lt;p&gt;The 5.2 million-square-foot facility processes more than 400,000 packages an hour and is home to 20,000 UPS workers and 300 daily flights, according to the company.&lt;/p&gt;
    &lt;p&gt;"My team and I are closely monitoring the plane crash near Louisville Muhammad Ali International Airport," Kentucky Sen. Rand Paul said. "We continue to pray for the safety of the aircrew, everyone in the area, and for the first-responders on the scene."&lt;/p&gt;
    &lt;p&gt;The National Transportation Safety Board will lead the investigation into the crash. A team of 28 investigators is expected to arrive Wednesday, Greenberg said.&lt;/p&gt;
    &lt;p&gt;All public schools in the Jefferson County School District, the largest school district in Kentucky with a little under 100,000 students, will be closed Wednesday, Greenberg added.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cbsnews.com/news/ups-plane-crash-louisville-kentucky/"/><published>2025-11-04T23:10:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816968</id><title>Google Removed 749M Anna's Archive URLs from Its Search Results</title><updated>2025-11-05T12:20:21.879647+00:00</updated><content>&lt;doc fingerprint="3b2013d053b71323"&gt;
  &lt;main&gt;
    &lt;p&gt;Anna’s Archive is a meta-search engine for shadow libraries that allows users to find pirated books and other related sources.&lt;/p&gt;
    &lt;p&gt;The site launched in the fall of 2022, just days after Z-Library was targeted in a U.S. criminal crackdown, to ensure continued availability of ‘free’ books and articles to the broader public.&lt;/p&gt;
    &lt;p&gt;In the three years since then, Anna’s Archive has built up quite the track record. The site has been blocked in various countries, was sued in the U.S. after it scraped WorldCat, and actively provides assistance to AI researchers who want to use its library for model training.&lt;/p&gt;
    &lt;p&gt;Despite legal pressure, Annas-archive.org and the related .li and .se domains remain operational. This is a thorn in the side of publishers who are actively trying to take the site down. In the absence of options to target the site directly, they ask third-party intermediaries such as Google to lend a hand.&lt;/p&gt;
    &lt;head rend="h2"&gt;749 Million URLs&lt;/head&gt;
    &lt;p&gt;Google and other major search engines allow rightsholders to request removal of allegedly infringing URLs. The aim is to ensure that pirate sites no longer show up in search results when people search for books, movies, music, or other copyrighted content.&lt;/p&gt;
    &lt;p&gt;The Pirate Bay, for example, has been a popular target; Google has removed more than 4.2 million thepiratebay.org URLs over the years in response to copyright holder complaints. While this sounds like a sizable number, it pales in comparison to the volume of takedowns targeting Anna’s Archive.&lt;/p&gt;
    &lt;p&gt;Google’s transparency report reveals that rightsholders asked Google to remove 784 million URLs, divided over the three main Anna’s Archive domains. A small number were rejected, mainly because Google didn’t index the reported links, resulting in 749 million confirmed removals.&lt;/p&gt;
    &lt;p&gt;The comparison to sites such as The Pirate Bay isn’t fair, as Anna’s Archive has many more pages in its archive and uses multiple country-specific subdomains. This means that there’s simply more content to take down. That said, in terms of takedown activity, the site’s three domain names clearly dwarf all pirate competition.&lt;/p&gt;
    &lt;head rend="h2"&gt;5% of All Google Takedowns, Ever&lt;/head&gt;
    &lt;p&gt;Since Google published its first transparency report in May 2012, rightsholders have flagged 15.1 billion allegedly infringing URLs. That’s a staggering number, but the fact that 5% of the total targeted Anna’s Archive URLs is remarkable.&lt;/p&gt;
    &lt;p&gt;Penguin Random House and John Wiley &amp;amp; Sons are the most active publishers targeting the site, but they are certainly not alone. According to Google data, more than 1,000 authors or publishers have sent DMCA notices targeting Anna’s Archive domains.&lt;/p&gt;
    &lt;p&gt;Yet, there appears to be no end in sight. Rightsholders are reporting roughly 10 million new URLs per week for the popular piracy library, so there is no shortage of content to report.&lt;/p&gt;
    &lt;p&gt;With these DMCA takedown notices, publishers are aiming to make it as difficult as possible for people to find books on the site using Google. This works, as many URLs are now delisted while others are actively being demoted by the search engine for book-related queries.&lt;/p&gt;
    &lt;p&gt;That said, the Anna’s Archive website is certainly not unfindable. Searching for the site’s name in Google still shows the main domain as the top search result.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://torrentfreak.com/google-removed-749-million-annas-archive-urls-from-its-search-results/"/><published>2025-11-04T23:11:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45817114</id><title>Bluetui – A TUI for managing Bluetooth on Linux</title><updated>2025-11-05T12:20:21.414395+00:00</updated><content>&lt;doc fingerprint="e4d181ddbeaa2ea"&gt;
  &lt;main&gt;
    &lt;p&gt;A Linux based OS with bluez installed.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;You might need to install nerdfonts for the icons to be displayed correctly.&lt;/p&gt;
    &lt;p&gt;You can download the pre-built binaries from the release page release page&lt;/p&gt;
    &lt;p&gt;You can install &lt;code&gt;bluetui&lt;/code&gt; from crates.io&lt;/p&gt;
    &lt;code&gt;cargo install bluetui&lt;/code&gt;
    &lt;p&gt;You can install &lt;code&gt;bluetui&lt;/code&gt; from the extra repository:&lt;/p&gt;
    &lt;code&gt;pacman -S bluetui&lt;/code&gt;
    &lt;p&gt;You can install &lt;code&gt;bluetui&lt;/code&gt; from the lamdness Gentoo Overlay:&lt;/p&gt;
    &lt;code&gt;sudo eselect repository enable lamdness
sudo emaint -r lamdness sync
sudo emerge -av net-wireless/bluetui&lt;/code&gt;
    &lt;p&gt;If you are a user of x-cmd, you can run:&lt;/p&gt;
    &lt;code&gt;x install bluetui&lt;/code&gt;
    &lt;p&gt;Run the following command:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/pythops/bluetui
cd bluetui
cargo build --release&lt;/code&gt;
    &lt;p&gt;This will produce an executable file at &lt;code&gt;target/release/bluetui&lt;/code&gt; that you can copy to a directory in your &lt;code&gt;$PATH&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Tab&lt;/code&gt;: Switch between different sections.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;j&lt;/code&gt; or &lt;code&gt;Down&lt;/code&gt; : Scroll down.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;k&lt;/code&gt; or &lt;code&gt;Up&lt;/code&gt;: Scroll up.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;s&lt;/code&gt;: Start/Stop scanning.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;?&lt;/code&gt;: Show help.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;esc&lt;/code&gt;: Dismiss the help pop-up.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;ctrl+c&lt;/code&gt; or &lt;code&gt;q&lt;/code&gt;: Quit the app.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;p&lt;/code&gt;: Enable/Disable the pairing.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;o&lt;/code&gt;: Power on/off the adapter.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;d&lt;/code&gt;: Enable/Disable the discovery.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;u&lt;/code&gt;: Unpair the device.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Space or Enter&lt;/code&gt;: Connect/Disconnect the device.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;t&lt;/code&gt;: Trust/Untrust the device.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;e&lt;/code&gt;: Rename the device.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Space or Enter&lt;/code&gt;: Pair the device.&lt;/p&gt;
    &lt;p&gt;Keybindings can be customized in the default config file location &lt;code&gt;$HOME/.config/bluetui/config.toml&lt;/code&gt; or from a custom path with &lt;code&gt;-c&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;# Possible values: "Legacy", "Start", "End", "Center", "SpaceAround", "SpaceBetween"
layout = "SpaceAround"

# Window width
# Possible values: "auto" or a positive integer
width = "auto"

toggle_scanning = "s"

[adapter]
toggle_pairing = "p"
toggle_power = "o"
toggle_discovery = "d"

[paired_device]
unpair = "u"
toggle_trust = "t"
rename = "e"&lt;/code&gt;
    &lt;p&gt;GPLv3&lt;/p&gt;
    &lt;p&gt;Bluetui logo: Marco Bulgarelli&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/pythops/bluetui"/><published>2025-11-04T23:29:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45817167</id><title>Uncle Sam wants to scan your iris and collect your DNA, citizen or not</title><updated>2025-11-05T12:20:21.258675+00:00</updated><content>&lt;doc fingerprint="fbca09582881b604"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Uncle Sam wants to scan your iris and collect your DNA, citizen or not&lt;/head&gt;
    &lt;head rend="h2"&gt;DHS rule would expand biometric collection to immigrants and some citizens linked to them&lt;/head&gt;
    &lt;p&gt;If you're filing an immigration form - or helping someone who is - the Feds may soon want to look in your eyes, swab your cheek, and scan your face. The US Department of Homeland Security wants to greatly expand biometric data collection for immigration applications, covering immigrants and even some US citizens tied to those cases.&lt;/p&gt;
    &lt;p&gt;DHS, through its component agency US Citizenship and Immigration Services, on Monday proposed a sweeping expansion of the agency's collection of biometric data. While ostensibly about verifying identities and preventing fraud in immigration benefit applications, the proposed rule goes much further than simply ensuring applicants are who they claim to be.&lt;/p&gt;
    &lt;p&gt;First off, the rule proposes expanding when DHS can collect biometric data from immigration benefit applicants, as "submission of biometrics is currently only mandatory for certain benefit requests and enforcement actions." DHS wants to change that, including by requiring practically everyone an immigrant is associated with to submit their biometric data.&lt;/p&gt;
    &lt;p&gt;"DHS proposes in this rule that any applicant, petitioner, sponsor, supporter, derivative, dependent, beneficiary, or individual filing or associated with a benefit request or other request or collection of information, including U.S. citizens, U.S. nationals and lawful permanent residents, and without regard to age, must submit biometrics unless DHS otherwise exempts the requirement," the rule proposal said.&lt;/p&gt;
    &lt;p&gt;DHS also wants to require the collection of biometric data from "any alien apprehended, arrested or encountered by DHS."&lt;/p&gt;
    &lt;p&gt;It's not explicitly stated in the rule proposal why US citizens associated with immigrants who are applying for benefits would have to have their biometric data collected. DHS didn't answer questions to that end, though the rule stated that US citizens would also be required to submit biometric data "when they submit a family-based visa petition."&lt;/p&gt;
    &lt;head rend="h3"&gt;Give me your voice, your eye print, your DNA samples&lt;/head&gt;
    &lt;p&gt;In addition to expanded collection, the proposed rule also changes the definition of what DHS considers to be valid biometric data.&lt;/p&gt;
    &lt;p&gt;"Government agencies have grouped together identifying features and actions, such as fingerprints, photographs, and signatures under the broad term, biometrics," the proposal states. "DHS proposes to define the term 'biometrics' to mean 'measurable biological (anatomical, physiological or molecular structure) or behavioral characteristics of an individual,'" thus giving DHS broad leeway to begin collecting new types of biometric data as new technologies are developed.&lt;/p&gt;
    &lt;p&gt;The proposal mentions several new biometric technologies DHS wants the option to use, including ocular imagery, voice prints and DNA, all on the table per the new rule.&lt;/p&gt;
    &lt;p&gt;"The rule proposes to grant DHS express authority to require, request, or accept raw DNA or DNA test results," DHS said, including "to prove or disprove … biological sex" in situations where that can affect benefit eligibility.&lt;/p&gt;
    &lt;p&gt;DHS wants to use all that data for identity enrollment, verification and management of the immigration lifecycle, national security and criminal history checks, "the production of secure identity documents," to prove familial relationships, and to perform other administrative functions, the rule states.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Facial recognition works better in the lab than on the street, researchers show&lt;/item&gt;
      &lt;item&gt;EU biometric border system launch hits inevitable teething problems&lt;/item&gt;
      &lt;item&gt;Vietnam to collect biometrics - even DNA - for new ID cards&lt;/item&gt;
      &lt;item&gt;Altman's eyeball-scanning biometric blockchain orbs officially come to America&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we noted in our story last week about DHS' new rule expanding biometric data collection on entry into and exit from the US, biometric technology - especially the often-used facial recognition scan - is ripe for misuse and prone to errors.&lt;/p&gt;
    &lt;p&gt;This new proposed rule goes far beyond subjecting immigrants to algorithmic identification tech prone to misidentifying non-white individuals, however, and reaches a new level of surveillance, with DHS seeking to collect and keep DNA test results - including partial profiles - from immigrants and some US citizens to verify family ties or biological sex when relevant. It's not much more assuring that DHS also wants to collect new forms of biometric data like voice records, which are increasingly easy to spoof with AI.&lt;/p&gt;
    &lt;p&gt;When we asked DHS questions about its biometric expansion proposal, it only sent us a statement identical to the one it sent last week when we inquired about the new entry/exit biometric requirements. The agency didn't respond when we asked for a statement pertaining to this latest proposed rule.&lt;/p&gt;
    &lt;p&gt;DHS is taking comments on the proposal until January 2; so far the submissions are nearly entirely negative, with posters decrying the plan as government overreach, comparing the proposal to communist China, and calling it a violation of Constitutional guarantees against unreasonable search and seizure. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/11/04/dhs_wants_to_collect_biometric_data/"/><published>2025-11-04T23:35:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45818319</id><title>Direct File won't happen in 2026, IRS tells states</title><updated>2025-11-05T12:20:21.127175+00:00</updated><content>&lt;doc fingerprint="b8a1c09cd9635069"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Direct File won’t happen in 2026, IRS tells states&lt;/head&gt;
    &lt;head rend="h2"&gt;The free service that allowed taxpayers to file online directly with the IRS was used by hundreds of thousands of taxpayers in 2024 and 2025, who gave it high marks — although tax prep companies and Republicans have sought its end.&lt;/head&gt;
    &lt;p&gt;The IRS has notified states that offered the free, government tax filing service known as Direct File in 2025 that the program won’t be available next filing season.&lt;/p&gt;
    &lt;p&gt;In an email sent from the IRS to 25 states, the tax agency thanked them for collaborating and noted that “no launch date has been set for the future.”&lt;/p&gt;
    &lt;p&gt;“IRS Direct File will not be available in Filing Season 2026,” says the Monday email, obtained by Nextgov/FCW and confirmed by multiple sources. It follows reports that the program was ending and Trump’s former tax chief, Billy Long, remarking over the summer that the service was “gone.”&lt;/p&gt;
    &lt;p&gt;The program, which debuted in 2024, was a big shift from the decades-long IRS policy of not competing with the tax prep industry in offering its own free, online tax filing service for Americans. Many Republicans had opposed Direct File, and tax prep companies also lobbied against it.&lt;/p&gt;
    &lt;p&gt;Still, most of the taxpayers that used Direct File earlier this year — over 296,500 — gave it high marks.&lt;/p&gt;
    &lt;p&gt;Those users won’t be able to log on to the Direct File website to get their returns anymore, according to the new email, which directs anyone needing a transcript to their IRS online accounts.&lt;/p&gt;
    &lt;p&gt;The Trump administration’s massive tax and spending policy bill signed into law over the summer directed the IRS to set up a task force to examine how the tax agency can use public-private partnerships to replace Direct File.&lt;/p&gt;
    &lt;p&gt;The IRS has relied on a public-private partnership called Free File for decades to give most Americans a free way to file their taxes, although it's been extremely underutilized. Only 3% of eligible taxpayers used it in recent years. Some of the member companies were found to have pushed people toward products they’d have to pay for, even when they could’ve used free options.&lt;/p&gt;
    &lt;p&gt;"It's not surprising since the Trump administration sabotaged Direct File all through this year's filing season, at the urging of tax prep monopolies like TurboTax," Adam Ruben, the vice president of the Economic Security Project, told Nextgov/FCW. "Trump's billionaire friends get favors while honest hardworking Americans will pay more to file their taxes."&lt;/p&gt;
    &lt;p&gt;Sen. Elizabeth Warren, D-Mass., told Nextgov/FCW that "the fight isn't over," saying that "giant tax prep companies are popping champagne, while Americans are forced to spend more time and more money to file their taxes."&lt;/p&gt;
    &lt;p&gt;The IRS did not respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Editor's note: This article has been updated to include comment from Sen. Elizabeth Warren.&lt;/p&gt;
    &lt;p&gt;If you have a tip you'd like to share, Natalie Alms can be securely contacted at nalms.41 on Signal.&lt;/p&gt;
    &lt;p&gt;NEXT STORY: CBP expands facial recognition for non-citizens at borders&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nextgov.com/digital-government/2025/11/direct-file-wont-happen-2026-irs-tells-states/409309/"/><published>2025-11-05T02:30:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45818471</id><title>The Microsoft SoftCard for the Apple II: Getting two processors to share memory</title><updated>2025-11-05T12:20:21.022613+00:00</updated><content>&lt;doc fingerprint="a2dfd9092d34a089"&gt;
  &lt;main&gt;
    &lt;p&gt;The Microsoft Z-80 SoftCard was a plug-in expansion card for the Apple II that added the ability to run CP/M software. According to Wikipedia, it was Microsoft’s first hardware product and in 1980 was the single largest revenue source for the company.&lt;/p&gt;
    &lt;p&gt;CP/M runs on an 8080 processor, but the Apple II has a 6502 processor. So how can you run CP/M on an Apple II? Answer: The card comes with its own 8080-compatible processor, the Zilog Z80, which was arguable better than the 8080 for a bunch of reasons given on its Wikipedia page.¹&lt;/p&gt;
    &lt;p&gt;Great, you now have a processor. But what happens to the old 6502 processor? Ideally, you would just shut it off, but you can’t go cold turkey because some things still had to be handled by the 6502.² Nicole Branagan digs deeper into the story of how the two processors coexist. The idea is that the SoftCard tells the 6502 that it’s doing DMA, so the 6502 pauses and waits for the DMA to complete. However, you can’t leave the 6502 paused for too long or its internal registers degrade and lose their values.&lt;/p&gt;
    &lt;p&gt;The solution is to take advantage of the Z80’s REFRESH line, which the processor uses to signal that it’s not accessing memory right now (because it’s decoding an instruction). This tells external memory refresh circuitry that it can run and keep the RAM values refreshed so that they don’t degrade and lose their values.&lt;/p&gt;
    &lt;p&gt;On the Apple II, memory refreshing is done by the video circuitry, so there is need for a dedicated REFRESH signal. The SoftCard uses this signal to allow the 6502 to execute a tiny little bit. (Presumably it is sitting in a spin loop waiting to be woken.) This keeps the 6502’s registers refreshed.&lt;/p&gt;
    &lt;p&gt;When the SoftCard needs the 6502 to do actual work, it can update some memory to tell the 6502, “Break out of your spin loop and do something for me, then let me know the answer and go back to the spin loop.” The Z80 then goes to sleep until it gets an answer from the 6502.&lt;/p&gt;
    &lt;p&gt;Another wrinkle in the way that the 6502 and Z80 shared memory is in the memory map. Both the Z80 and 6502 consider the first 256 bytes of memory to be special and want to use it for different things. Furthermore, CP/M programs expect to be loaded at $0100, but the 6502 hard-codes its CPU stack to live in the range $0100–$01FF. There are other obstacles in the low part of the Apple II memory map: The Apple II system monitor uses $0200–$02FF as its keyboard input buffer, the bytes in the range $03F0–$03FF are used to hold interrupt vectors, and the text video frame buffer goes from $0400–$07FF. (There is a second text video frame buffer from $0800–$0BFF, but almost nobody uses it.) Other big obstacles are the memory range from $C000–$CFFF, which is used by peripheral devices, and the memory range from $D000–$FFFF, which holds the Apple II monitor ROM, but can be replaced by RAM if you have the Language Card (a 16KB memory expansion card), except that the last few bytes $FFFA–$FFFF are used by the CPU as interrupt vectors.&lt;/p&gt;
    &lt;p&gt;The solution is to remap the memory by putting address translation circuitry on the SoftCard, so that when the Z80 asks for memory address $0000, say, it actually gets physical memory $1000. The remapping is carefully arranged so that all of the Apple II’s special reserved addresses get shuffled to the end of the Z80 memory map, and all of the Apple II’s normal RAM occupies contiguous address space in the Z80 memory map starting at $0000.³&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;6502&lt;/cell&gt;
        &lt;cell&gt;Physical&lt;/cell&gt;
        &lt;cell&gt;Z80&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Special use&lt;/cell&gt;
        &lt;cell&gt;$0000–$0FFF&lt;/cell&gt;
        &lt;cell&gt;↘&lt;/cell&gt;
        &lt;cell&gt;$1000–$1FFF&lt;/cell&gt;
        &lt;cell&gt;$0000–$0FFF&lt;/cell&gt;
        &lt;cell&gt;normal RAM&lt;p&gt;(contiguous,&lt;/p&gt;&lt;p&gt;up to&lt;/p&gt;&lt;p&gt;installed RAM)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;normal RAM&lt;p&gt;(contiguous,&lt;/p&gt;&lt;p&gt;up to&lt;/p&gt;&lt;p&gt;installed RAM)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;$1000–$1FFF&lt;/cell&gt;
        &lt;cell&gt;↗&lt;/cell&gt;
        &lt;cell&gt;$2000–$2FFF&lt;/cell&gt;
        &lt;cell&gt;$1000–$1FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$2000–$2FFF&lt;/cell&gt;
        &lt;cell&gt;$3000–$3FFF&lt;/cell&gt;
        &lt;cell&gt;$2000–$2FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$3000–$3FFF&lt;/cell&gt;
        &lt;cell&gt;$4000–$4FFF&lt;/cell&gt;
        &lt;cell&gt;$3000–$3FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$4000–$4FFF&lt;/cell&gt;
        &lt;cell&gt;$5000–$5FFF&lt;/cell&gt;
        &lt;cell&gt;$4000–$4FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$5000–$5FFF&lt;/cell&gt;
        &lt;cell&gt;$6000–$6FFF&lt;/cell&gt;
        &lt;cell&gt;$5000–$5FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$6000–$6FFF&lt;/cell&gt;
        &lt;cell&gt;$7000–$7FFF&lt;/cell&gt;
        &lt;cell&gt;$6000–$6FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$7000–$7FFF&lt;/cell&gt;
        &lt;cell&gt;$8000–$8FFF&lt;/cell&gt;
        &lt;cell&gt;$7000–$7FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$8000–$8FFF&lt;/cell&gt;
        &lt;cell&gt;$9000–$9FFF&lt;/cell&gt;
        &lt;cell&gt;$8000–$8FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$9000–$9FFF&lt;/cell&gt;
        &lt;cell&gt;$A000–$AFFF&lt;/cell&gt;
        &lt;cell&gt;$9000–$9FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$A000–$AFFF&lt;/cell&gt;
        &lt;cell&gt;$B000–$BFFF&lt;/cell&gt;
        &lt;cell&gt;$A000–$AFFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$B000–$BFFF&lt;/cell&gt;
        &lt;cell&gt;$D000–$DFFF&lt;/cell&gt;
        &lt;cell&gt;$B000–$BFFF&lt;/cell&gt;
        &lt;cell&gt;expansion RAM&lt;p&gt;(except for&lt;/p&gt;&lt;p&gt;last 6 bytes)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;I/O space&lt;/cell&gt;
        &lt;cell&gt;$C000–$CFFF&lt;/cell&gt;
        &lt;cell&gt;↘&lt;/cell&gt;
        &lt;cell&gt;$E000–$EFFF&lt;/cell&gt;
        &lt;cell&gt;$C000–$CFFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;expansion RAM&lt;p&gt;(except for&lt;/p&gt;&lt;p&gt;last 6 bytes)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;$D000–$DFFF&lt;/cell&gt;
        &lt;cell&gt;↗&lt;/cell&gt;
        &lt;cell&gt;$F000–$FFFF&lt;/cell&gt;
        &lt;cell&gt;$D000–$DFFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$E000–$EFFF&lt;/cell&gt;
        &lt;cell&gt;$C000–$CFFF&lt;/cell&gt;
        &lt;cell&gt;$E000–$EFFF&lt;/cell&gt;
        &lt;cell&gt;I/O space&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;$F000–$FFFF&lt;/cell&gt;
        &lt;cell&gt;$0000–$0FFF&lt;/cell&gt;
        &lt;cell&gt;$F000–$FFFF&lt;/cell&gt;
        &lt;cell&gt;Special use&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The SoftCard manual contained lots of details on how to write code for it. For example, it included instructions on how to call into a 6502 subroutine from Z80 and had a chart showing how the memory was remapped for the Z80. It even included the Z80 processor reference manual, listing all the instructions. This will come in handy in a future story.&lt;/p&gt;
    &lt;p&gt;¹ I don’t know where the hyphen in Z-80 came from.&lt;/p&gt;
    &lt;p&gt;² In many places, I/O was handled by timing loops, so if you wanted to access, say, the game paddles, you had to let the 6502 do the I/O with its precise software timing loops.&lt;/p&gt;
    &lt;p&gt;³ There were also two high resolution graphics frame buffers, one at $2000–$3FFF, and another at $4000–$5FFF. These were right in the middle of the Z80 memory map, but in practice it wasn’t a problem because CP/M was a text-mode operating system, so the programs you were running didn’t try to do graphics anyway.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://devblogs.microsoft.com/oldnewthing/20251104-00/?p=111758"/><published>2025-11-05T02:58:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45818499</id><title>Preventing Kubernetes from Pulling the Pause Image from the Internet</title><updated>2025-11-05T12:20:20.516423+00:00</updated><content>&lt;doc fingerprint="3510c03015708d17"&gt;
  &lt;main&gt;
    &lt;p&gt;I don’t normally write blog posts that regurgitate information from normal documentation, but this particular subject irks me.&lt;/p&gt;
    &lt;p&gt;If you are running an internal Kubernetes (k8s) platform, you owe it to yourself to make sure there is nothing external to your platform determining your reliability.&lt;/p&gt;
    &lt;p&gt;You could ask yourself: How many internet dependencies do you have to start a pod? Should be zero, right???&lt;/p&gt;
    &lt;p&gt;If you use stock k8s, you might be surprised to know that each of your k8s nodes is actually reaching out to &lt;code&gt;registry.k8s.io&lt;/code&gt; on first pod creation to get the &lt;code&gt;pause&lt;/code&gt; image:&lt;/p&gt;
    &lt;code&gt;$ sudo crictl images
IMAGE                                     TAG                 IMAGE ID            SIZE
registry.k8s.io/pause                     3.9                 e6f1816883972
&lt;/code&gt;
    &lt;p&gt;If you want to change that, you can update your containerd (1.x) toml:&lt;/p&gt;
    &lt;code&gt;[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = "YOUR_REGISTRY/pause:3.10"
&lt;/code&gt;
    &lt;p&gt;And depend on one less thing. The rest of the blog post will go deeper into why this is the case.&lt;/p&gt;
    &lt;head rend="h1"&gt;What Is The Pause Image Anyway?&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;pause&lt;/code&gt; image is the container image that backs the k8s “sandbox” of a pod.
This &lt;code&gt;pause&lt;/code&gt; container is designed to hold the linux namespaces.
The &lt;code&gt;pause&lt;/code&gt; container used to also reap zombie processes from the other containers in a pod, its duty as PID1, but that isn’t the case by default anymore in k8s 1.8+.&lt;/p&gt;
    &lt;p&gt;The sandbox of a pod is part of the CRI spec. The CRI spec is a generic way for k8s to talk pods (and sandboxes) that is not specific to any particular container runtime (like containerd). Any container runtime that implements the CRI spec can, in theory, run k8s pods.&lt;/p&gt;
    &lt;p&gt;This means that the &lt;code&gt;pause&lt;/code&gt; image has more to do with CRI than it does with k8s.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where The Pause Image Comes From (CRI)&lt;/head&gt;
    &lt;p&gt;When a CRI-enabled container runtime needs to create a sandbox, at least with the case of containerd, it does this by creating a real container.&lt;/p&gt;
    &lt;p&gt;The image containerd is configured to use (by default) to create that sandbox, is the &lt;code&gt;pause&lt;/code&gt; image.
You can see this in code here.&lt;/p&gt;
    &lt;head rend="h1"&gt;How To Point Containerd To Your Local Pause Image&lt;/head&gt;
    &lt;p&gt;Per the current docs, you can overwrite the containerd sandbox image with a containerd configuration like this (assuming you have mirrored to a local registry):&lt;/p&gt;
    &lt;p&gt;(containerd 1.x)&lt;/p&gt;
    &lt;code&gt;[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = "YOUR_REGISTRY/pause:3.10"
&lt;/code&gt;
    &lt;p&gt;(containerd 2.x)&lt;/p&gt;
    &lt;code&gt;version = 3

[plugins]
  [plugins.'io.containerd.cri.v1.images']
    ...
    [plugins.'io.containerd.cri.v1.images'.pinned_images]
      sandbox = 'YOUR_REGISTRY/pause:3.10'
&lt;/code&gt;
    &lt;p&gt;Don’t take my word for it here, this particular setting has changed over time, check the official docs.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;If you go to registry.k8s.io you will see:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Please note that there is NO uptime SLA as this is a free, volunteer managed service. We will however do our best to respond to issues and the system is designed to be reliable and low-maintenance. If you need higher uptime guarantees please consider mirroring images to a location you control.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So yea, this is your PSA. Please mirror like they recommend and reconfigure as needed to not depend on the internet.&lt;/p&gt;
    &lt;p&gt;Comment via email&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kyle.cascade.family/posts/preventing-kubernetes-from-pulling-the-pause-image-from-the-internet/"/><published>2025-11-05T03:04:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45818562</id><title>Hypothesis: Property-Based Testing for Python</title><updated>2025-11-05T12:20:20.335608+00:00</updated><content>&lt;doc fingerprint="a1031a81e5b71397"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome to Hypothesis!¶&lt;/head&gt;
    &lt;p&gt;Hypothesis is the property-based testing library for Python. With Hypothesis, you write tests which should pass for all inputs in whatever range you describe, and let Hypothesis randomly choose which of those inputs to check - including edge cases you might not have thought about. For example:&lt;/p&gt;
    &lt;p&gt;You should start with the tutorial, or alternatively the more condensed quickstart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tutorial¶&lt;/head&gt;
    &lt;p&gt;An introduction to Hypothesis.&lt;/p&gt;
    &lt;p&gt;New users should start here, or with the more condensed quickstart.&lt;/p&gt;
    &lt;head rend="h2"&gt;How-to guides¶&lt;/head&gt;
    &lt;p&gt;Practical guides for applying Hypothesis in specific scenarios.&lt;/p&gt;
    &lt;head rend="h2"&gt;Explanations¶&lt;/head&gt;
    &lt;p&gt;Commentary oriented towards deepening your understanding of Hypothesis.&lt;/p&gt;
    &lt;head rend="h2"&gt;API Reference¶&lt;/head&gt;
    &lt;p&gt;Technical API reference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hypothesis.readthedocs.io/en/latest/"/><published>2025-11-05T03:15:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45820715</id><title>The Hackers Manifesto (The Conscience of a Hacker) (1986)</title><updated>2025-11-05T12:20:20.240418+00:00</updated><content>&lt;doc fingerprint="646e8917c32493a6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Title : Hacker's Manifesto&lt;/p&gt;
      &lt;p&gt; Author : The Mentor&lt;/p&gt;
      &lt;quote&gt; ==Phrack Inc.== Volume One, Issue 7, Phile 3 of 10 =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= The following was written shortly after my arrest... \/\The Conscience of a Hacker/\/ by +++The Mentor+++ Written on January 8, 1986 =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Another one got caught today, it's all over the papers. "Teenager Arrested in Computer Crime Scandal", "Hacker Arrested after Bank Tampering"... Damn kids. They're all alike. But did you, in your three-piece psychology and 1950's technobrain, ever take a look behind the eyes of the hacker? Did you ever wonder what made him tick, what forces shaped him, what may have molded him? I am a hacker, enter my world... Mine is a world that begins with school... I'm smarter than most of the other kids, this crap they teach us bores me... Damn underachiever. They're all alike. I'm in junior high or high school. I've listened to teachers explain for the fifteenth time how to reduce a fraction. I understand it. "No, Ms. Smith, I didn't show my work. I did it in my head..." Damn kid. Probably copied it. They're all alike. I made a discovery today. I found a computer. Wait a second, this is cool. It does what I want it to. If it makes a mistake, it's because I screwed it up. Not because it doesn't like me... Or feels threatened by me... Or thinks I'm a smart ass... Or doesn't like teaching and shouldn't be here... Damn kid. All he does is play games. They're all alike. And then it happened... a door opened to a world... rushing through the phone line like heroin through an addict's veins, an electronic pulse is sent out, a refuge from the day-to-day incompetencies is sought... a board is found. "This is it... this is where I belong..." I know everyone here... even if I've never met them, never talked to them, may never hear from them again... I know you all... Damn kid. Tying up the phone line again. They're all alike... You bet your ass we're all alike... we've been spoon-fed baby food at school when we hungered for steak... the bits of meat that you did let slip through were pre-chewed and tasteless. We've been dominated by sadists, or ignored by the apathetic. The few that had something to teach found us will- ing pupils, but those few are like drops of water in the desert. This is our world now... the world of the electron and the switch, the beauty of the baud. We make use of a service already existing without paying for what could be dirt-cheap if it wasn't run by profiteering gluttons, and you call us criminals. We explore... and you call us criminals. We seek after knowledge... and you call us criminals. We exist without skin color, without nationality, without religious bias... and you call us criminals. You build atomic bombs, you wage wars, you murder, cheat, and lie to us and try to make us believe it's for our own good, yet we're the criminals. Yes, I am a criminal. My crime is that of curiosity. My crime is that of judging people by what they say and think, not what they look like. My crime is that of outsmarting you, something that you will never forgive me for. I am a hacker, and this is my manifesto. You may stop this individual, but you can't stop us all... after all, we're all alike. +++The Mentor+++ _______________________________________________________________________________ &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://phrack.org/issues/7/3"/><published>2025-11-05T08:28:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45821921</id><title>Founder in Residence at Woz (San Francisco)</title><updated>2025-11-05T12:20:19.865874+00:00</updated><content>&lt;doc fingerprint="3ab8fb7857c0d6e6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Founder in Residence at Woz (San Francisco)&lt;/p&gt;
      &lt;p&gt;We’re opening one of the coolest jobs ever (only slightly biased).&lt;/p&gt;
      &lt;p&gt;At Woz, we’ve built the world's first AI App Factory, capable of building business quality mobile apps in just hours.&lt;/p&gt;
      &lt;p&gt;Now we’re handing over the keys to aspiring founders and challenging them to build real, revenue generating app businesses. Founders get full internal access to our platform, a salary, a dedicated marketing budget, and meaningful upside in any revenue they generate. This is a rare chance to operate like a founder inside a YC startup that recently raised a $6M seed round, surrounded by experienced engineers and builders in the heart of San Francisco.&lt;/p&gt;
      &lt;p&gt;Who we’re looking for: - A technical builder who can ship, iterate and problem solve independently (experience with React Native and TypeScript is a plus) - Someone who has launched products, apps, or businesses before, or has a strong track record of building things on their own - Someone who understands go-to-market and growth, especially creative or viral marketing - Someone eager to learn, experiment, and build alongside our team in San Francisco (able to work in-person for at least the first three months)&lt;/p&gt;
      &lt;p&gt;Interested? Submit your info here. We’ll be in touch https://forms.gle/h8ZWjgRfQUpaTQf8A&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45821921"/><published>2025-11-05T12:00:17+00:00</published></entry></feed>