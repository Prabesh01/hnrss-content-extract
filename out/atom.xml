<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-21T18:15:18.829697+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45999038</id><title>Over-regulation is doubling the cost</title><updated>2025-11-21T18:15:29.083539+00:00</updated><content>&lt;doc fingerprint="698010174ea1b620"&gt;
  &lt;main&gt;
    &lt;p&gt;After building a software company to a multi-billion dollar exit, I made the jump to hardware. Now I’m working on carbon removal + steel at Charm Industrial, and electric long-haul trucking with Revoy. It’s epically fun to be building in the real world, but little did I expect that more than half the cost of building a hardware company would come from regulatory bottlenecks. Despite a huge push for climate fixes and the bipartisan geopolitical desire to bring industry back to the USA, I’ve been shocked to find that the single biggest barrier—by far—is over-regulation from the massive depth of bureaucracy.&lt;/p&gt;
    &lt;p&gt;Hardtech companies of all flavors are being forced to burn through limited capital while they wait for regulatory clarity and/or permits. This creates a constant cycle of cost increases that ultimately flows to consumers, it lowers investment in the US manufacturing and industrial base, it delays innovative new hardware getting into the hands of consumers and businesses, and at the end of the day, it leaves us all worse off, stuck with a quality of life pegged to technology developed decades ago.&lt;/p&gt;
    &lt;p&gt;Regulatory delays and bottlenecks have added millions of pounds of pollutants like PM2.5, NOₓ and CO₂ to our air from the continuation of business as usual, instead of the deployment of clean technologies from my two hardtech efforts alone. While CO₂ is a long-term climate issue, PM2.5 and NOₓ are immediate major drivers of asthma and excess morbidity. Both operations have high bipartisan appeal—and we’ve never been denied a permit—because we’re fundamentally cleaning up things that matter to everyone: dirty air, wildfires, orphaned oil wells. Revoy is also helping deflate the cost of long-haul freight. But none of that has made getting freedom to operate easy. For creative new technologies the default answer is “no” because there isn’t a clear path to permitting at all, and figuring out that path itself takes years — time that startups can’t afford to wait.&lt;/p&gt;
    &lt;p&gt;Regulation obviously has a critical role in protecting people and the environment, but the sheer volume, over-specificity and sometimes ambiguity of those same regulations is now actively working against those goals! We’re unintentionally blocking the very things that would improve our environment. We’ve become a society that blocks all things, and we need to be a society that builds great things every day. The rest of this article gets very specific about the astronomical costs regulations are imposing on us as a society, and the massive positive impact that could be unleashed by cutting back regulation that is working against new, cost-saving, creative technology that could also be making people and the environment healthy again.&lt;/p&gt;
    &lt;p&gt;To make it concrete: both Charm and Revoy are capital-efficient hardtech companies, but Charm will spend low hundreds of millions to get to breakeven, and Revoy will spend tens of millions. In both cases, more than half of the total cost of building each company has gone to counterproductive regulatory burden. I’m hellbent on pushing through these barriers, but the unspoken reality is that our regulatory morass is the deathbed of thousands of hardtech companies that could be drastically improving our lives. We must unleash them.&lt;/p&gt;
    &lt;head rend="h2"&gt;$300M in Societal Cost &amp;amp; $125M in Burden for Charm&lt;/head&gt;
    &lt;p&gt;Charm produces and delivers verified carbon removal to companies like Google, Microsoft and JPMorgan. Charm’s breakthrough was realizing that you could take CO₂ captured in farm &amp;amp; forestry plant residues, convert it into a carbon-rich, BBQ sauce-like liquid (it’s literally the smoke flavor in BBQ sauce), and inject it into old oil wells to permanently remove carbon from the atmosphere. This has all kinds of co-benefits like reducing the massive overburden of wildfire fuels, cleaning up &amp;amp; plugging nasty orphaned oil wells, and improving PM2.5 and NOₓ air quality by avoiding that biomass being burned instead.&lt;/p&gt;
    &lt;p&gt;And yet… there was a hangup: what kind of injection well is this? Should it be permitted as a Class I disposal, Class II oilfield disposal, or Class V experimental? This question on permitting path took four years to answer. Four years to decide which path to use, not even the actual permit! It took this long because regulators are structurally faced with no upside, only downside legal risk in taking a formal position on something new. Even when we’d done an enormous amount of lab and field work with bio-oil to understand its safety and behavior at surface and subsurface conditions. A regulator faces little cost to moving incredibly cautiously, but a major cost if they approve something that triggers activist pushback.&lt;/p&gt;
    &lt;p&gt;In the end, we’re grateful that—eventually—a state regulator took the reins and reviewed, managed, and issued the first-ever Class V bio-oil sequestration permit, through what was still an incredibly complex and detailed 14-month review process.&lt;/p&gt;
    &lt;p&gt;Now imagine that, instead of the 5.5 years from first contact to issued permit, it had only taken the 6 months it actually required to get everyone across the regulatory establishment to agree on a Class V pathway, we would have had 5 additional years operating the well. That’s the equivalent, from our real supply chain, of sinking at least 30,000 tonnes of carbon per year at $600/tonne. Looking only at this one aspect, this delay came with a $90M price tag for Charm. We’ve also spent untold millions on regulatory affairs at all levels of government, not to mention the missed acceleration in sales, and other direct hard costs spent in R&amp;amp;D and processing bio-oil for inefficient and expensive injection into salt caverns instead.&lt;/p&gt;
    &lt;p&gt;But the public health burden created by this regulatory slowness is where it gets really crazy. This one regulatory delay meant we all got subjected to decreased air quality from an additional 30,000 tonnes per year of pile burning. The resulting particulate emissions alone are estimated to have caused a mindblowing $40m/year in healthcare costs. This is $200M in additional healthcare burden over those five years, mostly borne by Medicare and Medicaid. There are additional costs to NOₓ emissions and more that take it to $300M.&lt;/p&gt;
    &lt;p&gt;In total, the total cost to society of this single regulatory delay will be about $400M: $120-150M of unnecessary cost to Charm, and the bulk of it—$300M or so—borne by the public in healthcare costs. I’m not sharing these numbers to complain or make excuses; Charm is still on the path to having a huge impact and we’re among the lucky few that can survive these delays. What pains me most is the 5 years of lost carbon removal and pollutant reduction, and the compounding effect that has on all our health and healthcare costs. Over-regulation is now working against the very things it’s intended to protect.&lt;/p&gt;
    &lt;p&gt;Regulators do their absolute best with the system they have, but the combined effects of: (1) extremely detailed and complex regulation, (2) chaotic budgets and understaffing that disrupt an efficient process, and (3) endless lawsuits against regulators since 1970s-era Naderism have created an atmosphere of fear. If we want to solve the climate crisis, build abundance, lower costs, and generate wealth for all, this has to change. We need to delete and simplify reams of regulations. We need to pay regulators well, and we need to trust our regulators to operate quickly and decisively by putting reasonable limits on endless activist legal challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;&amp;gt;$25M in Unnecessary Burden for Revoy&lt;/head&gt;
    &lt;p&gt;Revoy’s breakthrough was realizing that you could lower long-haul freight costs and electrify long-haul semi trucks by leaving the diesel tractor in place and dropping an electric powertrain onto the back of the semi. Today, we boost semis from 7 mpg to 120 mpg, driving a 94% reduction in fuel consumption. This slashes emissions that negatively impact both air quality and climate.&lt;/p&gt;
    &lt;p&gt;And yet again… a hangup: what exactly is this electric doohickey? Is it a truck? A trailer? Something else? It was clear from the regulations that it was a “converter dolly”. But getting complete alignment on that simple fact across an alphabet soup of government agencies spanning both federal and state—NHTSA, FMCSA, FHWA, state transit authorities, air quality management districts, state DMVs, highway patrols and more—took years.&lt;/p&gt;
    &lt;p&gt;A “powered converter dolly” isn’t even a new thing! Here’s one from the sixties that ran on diesel to help trucks get over mountain passes:&lt;/p&gt;
    &lt;p&gt;There were some bright spots. The Federal Motor Carrier Safety Administration (FMCSA) and the National Highway Transportation Safety Administration (NHTSA) quickly converged on informal definitional clarity, and then eventually a Highway Patrol Captain who was eager to get innovative electric vehicles on the road pushed it through with a state DMV to register the first four Revoys. But bringing along the rest of the agencies, and the rest of the states, was not fast. It delayed deployments, soaked up hundreds of thousands of dollars of legal and lobbyist time (not to mention all the corresponding time on the government side that all of us taxpayers have to bear), and maybe most importantly… even with a formal memo from the Federal DOT, it is still not 100% resolved in some states.&lt;/p&gt;
    &lt;p&gt;As one example, one state agency has asked Revoy to do certified engine testing to prove that the Revoy doesn’t increase emissions of semi trucks. And that Revoy must do this certification across every single truck engine family. It costs $100,000 per certification and there are more than 270 engine families for the 9 engines that our initial partners use. That’s $27,000,000 for this one regulatory item. And keep in mind that this is to certify that a device—whose sole reason for existence is to cut pollution by &amp;gt;90%, and which has demonstrably done so across nearly 100,000 miles of testing and operations—is not increasing the emissions of the truck. It’s a complete waste of money for everyone.&lt;/p&gt;
    &lt;p&gt;And that $27M dollar cost doesn’t include the cost to society. This over-regulation will delay deployment of EV trucks by years, increasing NOₓ and PM 2.5 air pollution exposure for many of society’s least well-off who live near freeways. The delayed deployment will also increase CO₂ emissions that threaten the climate and environment. Revoy’s Founder (Ian Rust) and I actually disagree on what exactly it is about the regulatory environment that needs to change, but we agree it’s completely broken and hurting both people and the planet.&lt;/p&gt;
    &lt;p&gt;In every interaction I have with regulators, I’m reminded that they’re good people doing god’s work operating in a fundamentally broken system. A regulatory system that structurally insists on legalistic, ultra-extreme caution is bound to generate a massive negative return for society.&lt;/p&gt;
    &lt;p&gt;If we had a regulatory system that could move fast to experiment with creative new technologies, we’d live in a world where our environment gets cleaned up faster, where awesome new hardware was constantly improving our lives by making things better and cheaper, and where large-scale hardtech innovation happened here at home in the USA, not in China.&lt;/p&gt;
    &lt;p&gt;As we collectively work to build more manufacturing capacity at home and build the next wave of technologies to power the economy, we need to grapple with the real bottlenecks holding us back. I hope other hardtech founders will publicly share more of their stories as well (the stories I’ve heard in private would shock you). Props to Blake Scholl for doing so.&lt;/p&gt;
    &lt;p&gt;We need a come-to-jesus about regulatory limits, timelines, and scope. Yes, we need basic and strong protections for clear harms, but we need to unleash every hardworking American, not just a few companies with massive funding, to invent and build hardware again. We need to combine many approaches to get there: expedited reviews for new technology, freedom to operate by default, permits by right-not-process, deleting as many regulatory steps as possible, and more. CA YIMBY’s successful push to pass a deluge of housing acceleration laws in the past two years could serve as a model. America building things again is the foundation of a prosperous, powerful, and clean America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rein.pk/over-regulation-is-doubling-the-cost"/><published>2025-11-20T22:58:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46001889</id><title>Olmo 3: Charting a path through the model flow to lead open-source AI</title><updated>2025-11-21T18:15:27.890728+00:00</updated><content>&lt;doc fingerprint="ad5e3b78241b8f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Olmo 3: Charting a path through the model flow to lead open-source AI&lt;/head&gt;
    &lt;p&gt;November 20, 2025&lt;/p&gt;
    &lt;p&gt;Ai2&lt;/p&gt;
    &lt;p&gt;Language models are often treated as snapshots—brief captures of a long and carefully curated development process. But sharing only the end result obscures the rich context needed to modify, adapt, and extend a model's capabilities. Many meaningful adjustments require integrating domain-specific knowledge deep within the development pipeline, not merely at the final stage. To truly advance open AI development and research, the entire model flow – not just its endpoint – should be accessible and customizable. The model flow is the full lifecycle of an LM: every stage, checkpoint, dataset, and dependency required to create and modify it. By exposing this complete process, the goal is to engender greater trust and enable more effective adaptation, collaboration, and innovation.&lt;/p&gt;
    &lt;p&gt;With today's release of Olmo 3, we're empowering the open source community with not only state-of-the-art open models, but the entire model flow and full traceability back to training data.&lt;/p&gt;
    &lt;p&gt;At its center is Olmo 3-Think (32B), the best fully open 32B-scale thinking model that for the first time lets you inspect intermediate reasoning traces and trace those behaviors back to the data and training decisions that produced them. Olmo 3 is a family of compact, dense models at 7 billion and 32 billion parameters that can run on everything from laptops to research clusters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo 3-Base (7B, 32B) is our most powerful base model yet. When evaluated on our expanded, diverse evaluation suite, Olmo 3-Base delivers the strongest performance among fully open base models – where training data, code, and weights are all publicly available, like Stanford's Marin and Swiss AI's Apertus – and achieves competitive performance with some of the best open-weights base models of comparable size and architecture, including Qwen 2.5 and Gemma 3. Achieving strong results in programming, reading comprehension, and math problem solving, Olmo 3-Base maintains performance at extended context lengths (~up to 65K tokens)—providing a versatile foundation for continued pretraining, targeted fine-tuning, and reinforcement learning and making it easy to build in specialized capabilities like reasoning, tool use (function calling), and instruction following through post-training.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Think (7B, 32B) is our flagship post-trained reasoning set built on Olmo 3-Base. At a time when few organizations are releasing truly open models at this scale, Olmo 3-Think (32B) serves as a workhorse for RL research, long-horizon reasoning, and other advanced experiments that require substantial compute. On our suite of reasoning benchmarks (discussed below), it's the strongest fully open thinking model we're aware of, narrowing the gap to the best open-weight models of similar scale – such as Qwen 3 32B – while training on roughly 6x fewer tokens. Olmo 3-Think (7B) brings the same design and training approach to an even more efficient form factor, surfacing intermediate thinking steps for complex prompts while making open, inspectable reasoning accessible on more modest hardware.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Instruct (7B) is a chat and quick-response focused post-train of Olmo 3-Base that handles multi-turn, instruction-following, tool use, and more. In our evaluations, it matches or outperforms open-weight models including Qwen 2.5, Gemma 3, and Llama 3.1, and narrows the gap with Qwen 3 model families at a similar scale—delivering a strong, fully open alternative for high-quality conversational and tool-using agents.&lt;/item&gt;
      &lt;item&gt;Olmo 3-RL Zero (7B), is a fully open reinforcement learning pathway built on Olmo 3-Base, designed to bootstrap complex reasoning behaviors and enable clear benchmarking of RL algorithms. We release four series of checkpoints from domain-focused training on math, code, instruction following, and general chat, enabling careful study of reinforcement learning with verifiable rewards (RLVR).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of a single set of frozen weights, Olmo 3 offers multiple, fully documented paths through development: the Instruct path for everyday chat and tool use, the RL Zero path for RL experimentation from base models, and the Think/reasoning path for models that leverage inference-time scaling to unlock complex reasoning and agentic behaviors. Each path is a concrete example of how to shape behavior from the same base model, and you’re free to fork or remix them—start with Olmo 3-Base, explore your own supervised fine-tuning (SFT) or direct preference optimization (DPO) recipe for instruct-style use cases, or plug in a new RL objective to probe different tradeoffs. The flow itself becomes a rich, reusable object—not just a record of how we built Olmo 3, but a scaffold for how you can build your own systems.&lt;/p&gt;
    &lt;p&gt;Explore the Model Flow&lt;/p&gt;
    &lt;p&gt;Click on any stage to learn more about it and download artifacts.&lt;/p&gt;
    &lt;p&gt;The Olmo 3 checkpoints we're releasing represent our initial paths targeting our goals around reasoning, tool use, and general capabilities – we have exciting plans for other ways to leverage Olmo 3-Base 32B. But because we're releasing the entire flow, you can intervene at any point: swap in domain-specific data during mid-training, adjust post-training for your use case, or build on an earlier checkpoint that better suits your needs.&lt;/p&gt;
    &lt;p&gt;As with Olmo and Olmo 2, we’re releasing all components of the Olmo 3 flow – data, code, model weights, and checkpoints – under permissive open source licenses.&lt;/p&gt;
    &lt;p&gt;Try Olmo 3 | Download the models &amp;amp; data | Read the report&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong performance across the board&lt;/head&gt;
    &lt;p&gt;We run the Olmo 3 checkpoints through a broad, updated benchmark suite, grouping dozens of industry-standard tasks (plus a few new ones we introduce) into several capability clusters. Together, the clustered suite and these held-out tasks give us a capability profile of Olmo 3—a clear picture of how well it solves math problems, codes, uses tools, answers general-knowledge questions, and more.&lt;/p&gt;
    &lt;p&gt;At a high level, the Olmo 3 family delivers the strongest fully open base and thinking models we’re aware of. Olmo 3-Base 32B outperforms other fully open base models, and Olmo 3-Think 32B emerges as the strongest fully open thinking model.&lt;/p&gt;
    &lt;p&gt;Our results were made possible by rigorous data curation at every stage of training, a carefully designed training recipe for each model, and a set of new algorithmic and infrastructure advances across data processing, training, and reinforcement learning. We also introduce an enhanced reinforcement learning framework that guides the development of our models and is particularly essential for our thinking models. To design the training recipe and coordinate targeted improvements across a wide range of capabilities at each stage of the model training pipeline, our development framework balances distributed innovation with centralized evaluation.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Base, with a training pipeline that first focuses on broad coverage over diverse text, code, and math, then concentrates on harder distributions to sharpen programming, quantitative reasoning, and reading comprehension, is clearly the strongest set of fully open base models in our evaluations. It’s also arguably the best 32B model in the entire ecosystem of models with open weights, performing impressively in programming, reading comprehension, math problem solving, and long-context benchmarks like RULER, which tests information retrieval from lengthy texts. Olmo 3-Base (7B) and Olmo 3-Base (32) maintain quality at extended context lengths and integrate cleanly with RL workflows, providing a robust foundation for continued pretraining and post-training.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Think, which turns the Base into a reasoning model by training on multi-step problems spanning math, code, and general problem solving, then running the thinking SFT → thinking DPO → RLVR model flow to elicit high-quality reasoning traces, competes with or exceeds several open-weight reasoning models of similar sizes. On math benchmarks, Olmo 3-Think (7B) matches Qwen 3 8B on MATH and comes within a few points on AIME 2024 and 2025, and also leads all comparison models on HumanEvalPlus for coding—performing strongly on MBPP and LiveCodeBench to demonstrate particular strength in code-intensive reasoning. On broader reasoning tasks like BigBench Hard and AGI Eval English, Olmo 3-Think (7B) remains competitive with Qwen 3 8B reasoning and Qwen 3 VL 8B Thinker while staying fully open and slightly smaller.&lt;/p&gt;
    &lt;p&gt;For the 32B model, Olmo 3-Think scales these trends up and becomes one of the strongest fully open reasoning models in its class. Olmo 3-Think (32B) either wins or sits within roughly two points of the best open-weight model on MATH, OMEGA, BigBenchHard, HumanEvalPlus, PopQA, and IFEval. It ties Qwen 3 VL 32B Thinking for the top score on the OMEGA suite while staying clearly ahead of Gemma 3 27B Instruct and competitive with DeepSeek R1 Distill 32B on math and reasoning. On broader knowledge and QA, Olmo 3-Think (32B) is effectively neck-and-neck with the Qwen 3 models on PopQA. And in instruction following, Olmo 3-Think (32B) tops this subset on IFEval and remains solid on IFBench and AlpacaEval 2 LC—offering a strong default for reasoning workloads at the 32B scale.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Instruct, which produces shorter sequences than the corresponding Olmo 3-Think models to improve inference efficiency and is designed to focus on general chat, tool use, and synthetic data generation, outperforms comparably-sized open-weight models. Olmo 3-Instruct ties or surpasses Qwen 2.5, Gemma 3, and Llama 3.1 in our evaluations, and competes with the Qwen 3 family at similar scale, delivering strong function calling performance and instruction-following capabilities in a fully open 7B model.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Olmo 3 architecture and training stages&lt;/head&gt;
    &lt;p&gt;Olmo 3 uses a decoder-only transformer architecture and multi-stage training pipeline. Pretraining runs in three stages—an initial large-scale training run that builds broad capabilities; a mid-training phase that focuses on harder material like math, code, and reading comprehension; and a final long-context extension stage that trains the model on very long documents. Together with architectural enhancements, this yields a more capable, efficient base for the Olmo 3 family.&lt;/p&gt;
    &lt;p&gt;Post-training then specializes the pretrained model for different use cases. Building on Olmo 2, each pathway follows a three-stage recipe – SFT, preference tuning with DPO, and RLVR – but in Olmo 3, we expose this as a fully documented model flow with complete customization over each training stage and dataset mix.&lt;/p&gt;
    &lt;p&gt;Instead of releasing only the final weights, we provide checkpoints from each major training milestone: the base pretrained model, the mid-trained model after targeted skill enhancement, the long-context-extended version, plus post-training checkpoints for the Olmo 3-Think, Olmo 3-Instruct, and Olmo 3-RL Zero flows. You can study how capabilities emerge over time, run ablations on specific stages, and fork the model at whatever point best fits your data, compute, and goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expanded training data&lt;/head&gt;
    &lt;p&gt;Compared to Olmo 2, we scaled data collection and significantly strengthened our dataset curation methods. Continuing our commitment to full transparency, we’re releasing several new, higher-quality datasets that cover every stage of base model training and post-training—from initial learning to specialized skills like complex reasoning and long-context understanding. This means anyone can see exactly what data shaped the model’s capabilities, reproduce our results, and reuse these datasets to train their own AI systems.&lt;/p&gt;
    &lt;p&gt;Olmo 3 is pretrained on Dolma 3, a new ~9.3-trillion-token corpus drawn from web pages, science PDFs processed with olmOCR, codebases, math problems and solutions, and encyclopedic text. From this pool, we construct Dolma 3 Mix, a 5.9-trillion-token (~6T) pretraining mix with a higher proportion of coding and mathematical data than earlier Dolma releases, plus much stronger decontamination via extensive deduplication, quality filtering, and careful control over data mixing. We follow established web standards in collecting training data and don’t collect from sites that explicitly disallow it, including paywalled content.&lt;/p&gt;
    &lt;p&gt;On top of this, we introduce two Dolma 3-based mixes for later stages of base model training. Dolma 3 Dolmino is our mid-training mix: 100B training tokens sampled from a ~2.2T-token pool of high-quality math, science, code, instruction-following, and reading-comprehension data, including reasoning traces that also enable RL directly on the base model. Dolma 3 Longmino is our long-context mix: ~50B training tokens drawn from a 639B-token pool of long documents combined with mid-training data to teach Olmo 3 to track information over very long inputs (like reports, logs, and multi-chapter documents).&lt;/p&gt;
    &lt;p&gt;We also introduce Dolci, a new post-training data suite tailored specifically for reasoning, tool use, and instruction following. Dolci provides separate mixes for each stage of post-training: SFT, DPO, and RLVR. For SFT, Dolci aggregates state-of-the-art datasets that advance step-by-step reasoning, tool use, and high-quality conversational behavior; for DPO, it supplies high-quality contrastive preference data; and for RL, it includes hard, diverse prompts across math, coding, instruction following, and general chat.&lt;/p&gt;
    &lt;p&gt;Together, Dolma 3 and Dolci give Olmo 3 a fully open data curriculum from first token to final post-trained checkpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;Efficient training stack&lt;/head&gt;
    &lt;p&gt;We pretrained Olmo 3 on a cluster of up to 1,024 H100 GPUs; we achieved training throughput of 7.7K tokens per device per second for Olmo 3-Base (7B). We mid-trained on 128 H100 GPUs, and post-trained on a set of 256 H100s.&lt;/p&gt;
    &lt;p&gt;For Olmo 3, building on the work we did for Olmo 2, we were able to significantly improve the efficiency of our post-training code. By moving SFT from Open Instruct (our post-training codebase, prioritizing flexibility) to Olmo Core (our pretraining codebase, designed to maximize efficiency), we increased throughput (tokens/second) by 8x. Similarly, by incorporating in-flight weight updates, continuous batching, and a lot of threading improvements, we made our RL training 4x more efficient—resulting in training runs that are significantly cheaper and faster.&lt;/p&gt;
    &lt;p&gt;A note on our 32B models: We believe 32B sits in a sweet spot for research and tinkering. 32B models are big enough to support strong, competitive performance, but still small enough that a wide audience can fine-tune and deploy them on accessible hardware.&lt;/p&gt;
    &lt;p&gt;For more details, including ablations, please read our technical report.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency at the core&lt;/head&gt;
    &lt;p&gt;A core goal of Olmo 3 is not just to open the model flow, but to make it actionable for people who want to understand and improve model behavior. Olmo 3 integrates with OlmoTrace, our tool for tracing model outputs back to training data in real time.&lt;/p&gt;
    &lt;p&gt;For example, in the Ai2 Playground, you can ask Olmo 3-Think (32B) to answer a general-knowledge question, then use OlmoTrace to inspect where and how the model may have learned to generate parts of its response. This closes the gap between training data and model behavior: you can see not only what the model is doing, but why—and adjust data or training decisions accordingly.&lt;/p&gt;
    &lt;p&gt;To further promote transparency and explainability, we’re making every training and fine-tuning dataset available for download, all under a permissive license that allows for custom deployment and reuse. The datasets come in a range of mixes to accommodate different storage and hardware constraints, from several billion tokens all the way up to 6 trillion.&lt;/p&gt;
    &lt;p&gt;Our new tooling for data processing allows you to de-contaminate, tokenize, and de-duplicate data in the same way we did for Olmo 3’s corpora. All the tooling is open source, enabling you to replicate our training curves or run controlled ablations across data mixes and objectives.&lt;/p&gt;
    &lt;p&gt;Our Olmo utilities and software cover the whole development cycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo-core is a state-of-the-art framework for distributed model training.&lt;/item&gt;
      &lt;item&gt;Open Instruct is our post-training pipeline.&lt;/item&gt;
      &lt;item&gt;datamap-rs is a pure-Rust toolkit for large-scale cleaning.&lt;/item&gt;
      &lt;item&gt;duplodocus for ultra-efficient fuzzy de-duplication.&lt;/item&gt;
      &lt;item&gt;OLMES is a toolkit for reproducible evals. It includes our brand-new eval collection OlmoBaseEval, which we used for Olmo 3 base model development.&lt;/item&gt;
      &lt;item&gt;decon removes test sets from training data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Importantly, our tooling allows you to instrument complex tasks and analyze intermediate traces to understand where the models succeed—or struggle. Because the Olmo 3 data recipes, training pipeline, and checkpoints are open, independent teams can connect model behavior back to measurable properties.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to deploy and use&lt;/head&gt;
    &lt;p&gt;Together, the Olmo 3 family makes it easier to build trustworthy features quickly, whether for research, education, or applications. By making every development step available and inspectable, we're enabling entirely new categories of research. You can run experiments on any training phase, understand exactly how different techniques contribute to model capabilities, and build on our work at whatever stage makes sense for your project.&lt;/p&gt;
    &lt;p&gt;For scientists, the fully open flow exposes the model’s inner workings, so you can instrument experiments across coding, reasoning, RL, and tool use.&lt;/p&gt;
    &lt;p&gt;If you care about AI you can study, audit, and improve, Olmo 3 is for you. Try the demos in the Ai2 Playground, explore the documentation, and build on the released weights and checkpoints. Then tell us what you discover—we invite the community to validate, critique, and extend our findings.&lt;/p&gt;
    &lt;p&gt;True openness in AI isn't just about access—it's about trust, accountability, and shared progress. We believe the models shaping our future should be fully inspectable, not black boxes. Olmo 3 represents a different path: one where anyone can understand, verify, and build upon the AI systems that increasingly influence our world. This is what open-first means—not just releasing weights, but sharing the complete knowledge needed to advance AI responsibly: the flow.&lt;/p&gt;
    &lt;p&gt;Deep dive with Olmo lead researchers Hanna Hajishirzi and Noah Smith on how – and why – we built Olmo 3, and what comes next:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://allenai.org/blog/olmo3"/><published>2025-11-21T06:50:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46002161</id><title>It's hard to build an oscillator</title><updated>2025-11-21T18:15:27.790821+00:00</updated><content/><link href="https://lcamtuf.substack.com/p/its-hard-to-build-an-oscillator"/><published>2025-11-21T07:45:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46003144</id><title>FAWK: LLMs can write a language interpreter</title><updated>2025-11-21T18:15:27.340764+00:00</updated><content>&lt;doc fingerprint="5ff690b5ea490edb"&gt;
  &lt;main&gt;
    &lt;p&gt;After reading the book The AWK Programming Language (recommended!), I was planning to try AWK out on this year’s Advent of Code. Having some time off from work this week, I tried to implement one of the problems in it to get some practice, set up my tooling, see how hard AWK would be, and… I found I’m FP-pilled.&lt;/p&gt;
    &lt;p&gt;I knew I’m addicted to the combination of algebraic data types (tagged unions) and exhaustive pattern matching, but what got me this time was immutability, lexical scope and the basic human right of being allowed to return arrays from functions.&lt;/p&gt;
    &lt;p&gt;Part 1 of the Advent of Code problem was easy enough, but for part 2 (basically a shortest path search with a twist, to not spoil too much), I found myself unable to switch from my usual functional BFS approach to something mutable, and ended up trying to implement my functional approach in AWK.&lt;/p&gt;
    &lt;p&gt;It got hairy very fast: I needed to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hashing of strings and 2D arrays (by piping to &lt;code&gt;md5sum&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;a global &lt;del rend="overstrike"&gt;set&lt;/del&gt;array of seen states&lt;/item&gt;
      &lt;item&gt;a way to serialize and deserialize a 2D array to/from a string&lt;/item&gt;
      &lt;item&gt;and a few associative arrays for retrieving this serialized array by its hash.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was very lost by the time I had all this; I spent hours just solving what felt like accidental complexity; things that I’d take for granted in more modern languages.&lt;/p&gt;
    &lt;p&gt;Now, I know nobody said AWK is modern, or functional, or that it promises any convenience for anything other than one-liners and basic scripts that fit under a handful of lines. I don’t want to sound like I expect AWK to do any of this; I knew I was stretching the tool when going in. But I couldn’t shake the feeling that there’s a beautiful AWK-like language within reach, an iteration on the AWK design (the pattern-action way of thinking is beautiful) that also gives us a few of the things programming language designers have learnt over the 48 years since AWK was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dreaming of functional AWK&lt;/head&gt;
    &lt;p&gt;Stopping my attempts to solve the AoC puzzle in pure AWK, I wondered: what am I missing here?&lt;/p&gt;
    &lt;p&gt;What if AWK had first-class arrays?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # array literals
  normal   = [1, 2, 3]
  nested   = [[1,2], [3,4]]
  assoc    = ["foo" =&amp;gt; "bar", "baz" =&amp;gt; "quux"]
  multidim = [(1,"abc") =&amp;gt; 999]

  five = range(1,5)
  analyze(five)
  print five  # --&amp;gt; still [1, 2, 3, 4, 5]! was passed by value
}

function range(a,b) {
  r = []
  for (i = a; i &amp;lt;= b; i++) {
    r[length(r)] = i
  }
  return r  # arrays can be returned!
}

function analyze(arr) {
  arr[0] = 100
  print arr[0]  # --&amp;gt; 100, only within this function
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had first-class functions and lambdas?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # construct anonymous functions
  double = (x) =&amp;gt; { x * 2 }
  add = (a, b) =&amp;gt; { c = a + b; return c }

  # functions can be passed as values
  apply = (func, value) =&amp;gt; { func(value) }

  print apply(double,add(1,3))  # --&amp;gt; 8
  print apply(inc,5)  # --&amp;gt; 6
}

function inc(a) { return a + 1 }
&lt;/code&gt;
    &lt;p&gt;What if AWK had lexical scope instead of dynamic scope?&lt;/p&gt;
    &lt;code&gt;# No need for this hack anymore ↓     ↓
#function foo(a, b         ,local1, local2) {
function foo(a, b) {
  local1 = a + b
  local2 = a - b
  return local1 + local2
}

BEGIN {
  c = foo(1,2)
  print(local1)  # --&amp;gt; 0, the local1 from foo() didn't leak!
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had explicit globals, and everything else was local by default?&lt;/p&gt;
    &lt;code&gt;BEGIN { global count }
END {
  foo()
  print count  # --&amp;gt; 1
  print mylocal # --&amp;gt; 0, didn't leak
}
function foo() { count++; mylocal++ }
&lt;/code&gt;
    &lt;p&gt;(This one, admittedly, might make programs a bit more verbose. I’m willing to pay that cost.)&lt;/p&gt;
    &lt;p&gt;What if AWK had pipelines? (OK, now I’m reaching for syntax sugar…)&lt;/p&gt;
    &lt;code&gt;BEGIN {
  result = [1, 2, 3, 4, 5] 
      |&amp;gt; filter((x) =&amp;gt; { x % 2 == 0 })
      |&amp;gt; map((x) =&amp;gt; { x * x })
      |&amp;gt; reduce((acc, x) =&amp;gt; { acc + x }, 0)

  print "Result:", result
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Making it happen&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TL;DR:&lt;/p&gt;&lt;code&gt;Janiczek/fawk&lt;/code&gt;on GitHub&lt;/quote&gt;
    &lt;p&gt;Now for the crazy, LLM-related part of the post. I didn’t want to spend days implementing AWK from scratch or tweaking somebody else’s implementation. So I tried to use Cursor Agent for a larger task than I usually do (I tend to ask for very small targeted edits), and asked Sonnet 4.5 for a README with code examples, and then a full implementation in Python.&lt;/p&gt;
    &lt;p&gt;And it did it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: I also asked for implementations in C, Haskell and Rust at the same time, not knowing if any of the four would succeed, and they all seem to have produced code that at least compiles/runs. I haven’t tried to test them or even run them though. The PRs are here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was very impressed—I still am! I expected the LLM to stumble and flail around and ultimately get nothing done, but it did what I asked it for (gave me an interpreter that could run those specific examples), and over the course of a few chat sessions, I guided it towards implementing more and more of “the rest of AWK”, together with an excessive amount of end-to-end tests.&lt;/p&gt;
    &lt;p&gt;The only time I could see it struggle was when I asked it to implement arbitrary precision floating point operations without using an external library like &lt;code&gt;mpmath&lt;/code&gt;. It attempted to use Taylor series, but couldn’t get it right for at
least a few minutes. I chickened out and told it to &lt;code&gt;uv add mpmath&lt;/code&gt; and simplify
the interpreter code. In a moment it was done.&lt;/p&gt;
    &lt;p&gt;Other things that I thought it would choke on, like &lt;code&gt;print&lt;/code&gt; being both a
statement (with &lt;code&gt;&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; redirection support) and an expression, or
multi-dimensional arrays, or multi-line records, these were all implemented
correctly. Updating the test suite to also check for backwards compatibility
with GAWK - not an issue. Lexical scoping
and tricky closure environment behaviour - handled that just fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;As the cool kids say, I have to update my priors. The frontier of what the LLMs can do has moved since the last time I tried to vibe-code something. I didn’t expect to have a working interpreter the same day I dreamt of a new programming language. It now seems possible.&lt;/p&gt;
    &lt;p&gt;The downside of vibe coding the whole interpreter is that I have zero knowledge of the code. I only interacted with the agent by telling it to implement a thing and write tests for it, and I only really reviewed the tests. I reckon this would be an issue in the future when I want to manually make some change in the actual code, because I have no familiarity with it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This also opened new questions for me wrt. my other projects where I’ve previously run out of steam, eg. trying to implement a Hindley-Milner type system for my dream forever-WIP programming language Cara. It seems I can now just ask the LLM to do it, and it will? But then, I don’t want to fall into the trap where I am no longer able to work on the codebase myself. I want to be familiar with and able to tinker on the code. I’d need to spend my time reviewing and reading code instead of writing everything myself. Perhaps that’s OK.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Performance of FAWK might be an issue as well, though right now it’s a non-goal, given my intended use case is throwaway scripts for Advent of Code, nothing user-facing. And who knows, based on what I’ve seen, maybe I can instruct it to rewrite it in Rust and have a decent chance of success?&lt;/p&gt;
    &lt;p&gt;For now, I’ll go dogfood my shiny new vibe-coded black box of a programming language on the Advent of Code problem (and as many of the 2025 puzzles as I can), and see what rough edges I can find. I expect them to be equal parts “not implemented yet” and “unexpected interactions of new PL features with the old ones”.&lt;/p&gt;
    &lt;p&gt;If you’re willing to jump through some Python project dependency hoops, you can try to use FAWK too at your own risk, at &lt;code&gt;Janiczek/fawk&lt;/code&gt; on
GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://martin.janiczek.cz/2025/11/21/fawk-llms-can-write-a-language-interpreter.html"/><published>2025-11-21T10:28:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46003778</id><title>How a French judge was digitally cut off by the USA</title><updated>2025-11-21T18:15:25.847559+00:00</updated><content>&lt;doc fingerprint="65a032ec176759d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How a French judge was digitally cut off by the USA&lt;/head&gt;
    &lt;p&gt;Nicolas Guillou has been sanctioned by the USA as a judge of the International Criminal Court. He notices the effects primarily in the digital realm.&lt;/p&gt;
    &lt;p&gt;Digital sovereignty has been much discussed in Europe in recent weeks, most recently during a German-French summit in Berlin. The extent of dependence on the USA in the digital sector is currently being experienced by a French judge. Nicolas Guillou, one of six judges and three prosecutors of the International Criminal Court (ICC), was sanctioned by the USA in August. He described his current situation as a digital time travel back to the 1990s, before the internet age, in a recent interview.&lt;/p&gt;
    &lt;p&gt;The reason for the US sanctions are the arrest warrants against Israeli Prime Minister Benjamin Netanyahu and Defense Minister Yoav Gallant. They were indicted for war crimes and crimes against humanity in the context of the destruction of the Gaza Strip. The USA condemned this decision by the court, whereupon the US Treasury Department sanctioned six judges and three prosecutors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Digitally excluded from almost everything&lt;/head&gt;
    &lt;p&gt;In Guillou's daily life, this means that he is excluded from digital life and much of what is considered standard today, he told the French newspaper Le Monde. All his accounts with US companies such as Amazon, Airbnb, or PayPal were immediately closed by the providers. Online bookings, such as through Expedia, are immediately canceled, even if they concern hotels in France. Participation in e-commerce is also practically no longer possible for him, as US companies always play a role in one way or another, and they are strictly forbidden to enter into any trade relationship with sanctioned individuals.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;p&gt;He also describes the impact on participating in banking as drastic. Payment systems are blocked for him, as US companies like American Express, Visa, and Mastercard have a virtual monopoly in Europe. He also describes the rest of banking as severely restricted. For example, accounts with non-US banks have also been partially closed. Transactions in US dollars or via dollar conversions are forbidden to him.&lt;/p&gt;
    &lt;head rend="h3"&gt;Judge: EU should block sanctions&lt;/head&gt;
    &lt;p&gt;Guillou's case shows how strong the USA's influence in the tech sector is and how few options he has to circumvent it. And this at a time when an account with a US tech company is considered a matter of course in more and more places.&lt;/p&gt;
    &lt;p&gt;The French judge advocates for Europe to gain more sovereignty in the digital and banking sectors. Without this sovereignty, the rule of law cannot be guaranteed, he warns. At the same time, he calls on the EU to activate an existing blocking regulation (Regulation (EC) No 2271/96) for the International Criminal Court, which prevents third countries like the USA from enforcing sanctions in the EU. EU companies would then no longer be allowed to comply with US sanctions if they violate EU interests. Companies that violate this would then be liable for damages.&lt;/p&gt;
    &lt;p&gt;(mki)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.heise.de/en/news/How-a-French-judge-was-digitally-cut-off-by-the-USA-11087561.html"/><published>2025-11-21T12:12:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46004293</id><title>Making a Small RPG</title><updated>2025-11-21T18:15:25.760711+00:00</updated><content/><link href="https://jslegenddev.substack.com/p/making-a-small-rpg"/><published>2025-11-21T13:23:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46004364</id><title>EXIF orientation info in PNGs isn't used for image-orientation: from-image</title><updated>2025-11-21T18:15:24.562611+00:00</updated><content>&lt;doc fingerprint="fb52bd5796b2ff66"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;EXIF orientation info in PNGs isn't used for image-orientation: from-image&lt;/head&gt;
    &lt;head rend="h2"&gt;Categories&lt;/head&gt;
    &lt;head rend="h3"&gt;(Core :: Layout: Images, Video, and HTML Frames, defect, P3)&lt;/head&gt;
    &lt;head rend="h2"&gt;Tracking&lt;/head&gt;
    &lt;head rend="h3"&gt;()&lt;/head&gt;
    &lt;head rend="h2"&gt;People&lt;/head&gt;
    &lt;head rend="h3"&gt;(Reporter: e, Unassigned)&lt;/head&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;head rend="h3"&gt;(Blocks 2 open bugs)&lt;/head&gt;
    &lt;head rend="h2"&gt;Details&lt;/head&gt;
    &lt;head rend="h3"&gt;(Keywords: parity-chrome, parity-safari, webcompat:platform-bug)&lt;/head&gt;
    &lt;head rend="h2"&gt;User Story&lt;/head&gt;
    &lt;quote&gt;user-impact-score:40&lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reporter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Description&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15&lt;/p&gt;
    &lt;p&gt;Steps to reproduce:&lt;/p&gt;
    &lt;p&gt;Go to https://ericportis.com/etc/PNG-EXIF-orientation/&lt;/p&gt;
    &lt;p&gt;Actual results:&lt;/p&gt;
    &lt;p&gt;The JPEG and PNG are rotated differently, even though they both have the same EXIF info (Orientation: Rotate 90 CW), and are both set to &lt;code&gt;image-orientation: from-image;&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Expected results:&lt;/p&gt;
    &lt;p&gt;They should display the same.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reporter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 1&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Further findings: neither Safari, Chrome, or Firefox respects exiftool's default output, which appends EXIF to the end of a PNG. This is allowed by the spec, but seems to be incompatible with progressive rendering of partially-downloaded PNGs.&lt;/p&gt;
    &lt;p&gt;Safari does respect EXIF orientation info that appears before the image data, but Firefox and Chrome do not.&lt;/p&gt;
    &lt;p&gt;https://bugs.webkit.org/show_bug.cgi?id=210021#c4&lt;lb/&gt; https://ericportis.com/etc/PNG-EXIF-orientation/shuffling-chunks/&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 2&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;heycam: Will this be covered by any of your follow-up work related to bug 1607667?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 3&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Huh, I didn't even know that PNG supported orientation data. I found https://ftp-osl.osuosl.org/pub/libpng/documents/pngext-1.5.0.html#C.eXIf which defines the &lt;code&gt;eXif&lt;/code&gt; table.  The patches I'm working on don't add support for this, but it would not be too difficult to do so, at least if the table appears earlier than the image data.  (I don't think our current image loading flow would handle the image size changing as a result of the orientation data later on.)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 4&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Because this bug's Severity has not been changed from the default since it was filed, and it's Priority is &lt;code&gt;P3&lt;/code&gt; (Backlog,) indicating it has been triaged, the bug's Severity is being updated to &lt;code&gt;S3&lt;/code&gt; (normal.)&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;•&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 5&lt;/head&gt;•&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is spec'd in PNG-3 https://www.w3.org/TR/2024/CRD-png-3-20240718/#eXIf&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;•&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 6&lt;/head&gt;•&lt;p&gt;11 months ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What is the expected waiting time for the issue to be resolved?&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;•&lt;p&gt;2 months ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 7&lt;/head&gt;•&lt;p&gt;1 month ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Should be fixed by bug 1682759. If that is incorrect please re-open.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bugzilla.mozilla.org/show_bug.cgi?id=1627423"/><published>2025-11-21T13:29:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46004386</id><title>Building a Minimal Viable Armv7 Emulator from Scratch</title><updated>2025-11-21T18:15:23.979971+00:00</updated><content>&lt;doc fingerprint="aa33d81da505eee8"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Tip or TLDR - I built a tiny, zero dependency armv7 userspace emulator in Rust&lt;/head&gt;
    &lt;p&gt;I wrote a minimal viable armv7 emulator in 1.3k lines of Rust without any dependencies. It parses and validates a 32-bit arm binary, maps its segments, decodes a subset of arm instructions, translates guest and host memory interactions and forwards arm Linux syscalls into x86-64 System V syscalls.&lt;/p&gt;
    &lt;p&gt;It can run a armv7 hello world binary and does so in 1.9ms (0.015ms for raw emulation without setup), while qemu takes 12.3ms (stinkarm is thus ~100-1000x slower than native armv7 execution).&lt;/p&gt;
    &lt;p&gt;After reading about the process the Linux kernel performs to execute binaries, I thought: I want to write an armv7 emulator - &lt;code&gt;stinkarm&lt;/code&gt;. Mostly to understand the ELF
format, the encoding of arm 32bit instructions, the execution of arm assembly
and how it all fits together (this will help me with the JIT for my programming
language I am currently designing). To fully understand everything: no
dependencies. And of course Rust, since I already have enough C projects going
on.&lt;/p&gt;
    &lt;p&gt;So I wrote the smallest binary I could think of:&lt;/p&gt;
    &lt;code&gt;1    .global _start  @ declare _start as a global
2_start:             @ start is the defacto entry point
3    mov r0, #161    @ first and only argument to the exit syscall
4    mov r7, #1      @ syscall number 1 (exit)
5    svc #0          @ trapping into the kernel (thats US, since we are translating)
&lt;/code&gt;
    &lt;p&gt;To execute this arm assembly on my x86 system, I need to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Parse the ELF, validate it is armv7 and statically executable (I don’t want to write a dynamic dependency resolver and loader)&lt;/item&gt;
      &lt;item&gt;Map the segments defined in ELF into the host memory, forward memory access&lt;/item&gt;
      &lt;item&gt;Decode armv7 instructions and convert them into a nice Rust enum&lt;/item&gt;
      &lt;item&gt;Emulate the CPU, its state and registers&lt;/item&gt;
      &lt;item&gt;Execute the instructions and apply their effects to the CPU state&lt;/item&gt;
      &lt;item&gt;Translate and forward syscalls&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sounds easy? It is!&lt;/p&gt;
    &lt;p&gt;Open below if you want to see me write a build script and a nix flake:&lt;head&gt;Minimalist arm setup and smallest possible arm binary&lt;/head&gt;&lt;/p&gt;
    &lt;p&gt;Before I start parsing ELF I’ll need a binary to emulate, so lets create a build script called &lt;code&gt;bld_exmpl&lt;/code&gt; (so I can write a lot less) and nix flake, so
the asm is converted into armv7 machine code in a armv7 binary on my non armv7
system :^)&lt;/p&gt;
    &lt;code&gt; 1// tools/bld_exmpl
 2use clap::Parser;
 3use std::fs;
 4use std::path::Path;
 5use std::process::Command;
 6
 7/// Build all ARM assembly examples into .elf binaries
 8#[derive(Parser)]
 9struct Args {
10    /// Directory containing .S examples
11    #[arg(long, default_value = "examples")]
12    examples_dir: String,
13}
14
15fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
16    let args = Args::parse();
17    let dir = Path::new(&amp;amp;args.examples_dir);
18
19    for entry in fs::read_dir(dir)? {
20        let entry = entry?;
21        let path = entry.path();
22        if path.extension().and_then(|s| s.to_str()) == Some("S") {
23            let name = path.file_stem().unwrap().to_str().unwrap();
24            let output = dir.join(format!("{}.elf", name));
25            build_asm(&amp;amp;path, &amp;amp;output)?;
26        }
27    }
28
29    Ok(())
30}
31
32fn build_asm(input: &amp;amp;Path, output: &amp;amp;Path) -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
33    println!("Building {} -&amp;gt; {}", input.display(), output.display());
34
35    let obj_file = input.with_extension("o");
36
37    let status = Command::new("arm-none-eabi-as")
38        .arg("-march=armv7-a")
39        .arg(input)
40        .arg("-o")
41        .arg(&amp;amp;obj_file)
42        .status()?;
43
44    if !status.success() {
45        return Err(format!("Assembler failed for {}", input.display()).into());
46    }
47
48    let status = Command::new("arm-none-eabi-ld")
49        .arg("-Ttext=0x8000")
50        .arg(&amp;amp;obj_file)
51        .arg("-o")
52        .arg(output)
53        .status()?;
54
55    if !status.success() {
56        return Err(format!("Linker failed for {}", output.display()).into());
57    }
58
59    Ok(fs::remove_file(obj_file)?)
60}&lt;/code&gt;
    &lt;code&gt; 1# Cargo.toml
 2[package]
 3name = "stinkarm"
 4version = "0.1.0"
 5edition = "2024"
 6default-run = "stinkarm"
 7
 8[dependencies]
 9clap = { version = "4.5.51", features = ["derive"] }
10
11[[bin]]
12name = "stinkarm"
13path = "src/main.rs"
14
15[[bin]]
16name = "bld_exmpl"
17path = "tools/bld_exmpl.rs"&lt;/code&gt;
    &lt;code&gt; 1{
 2  description = "stinkarm — ARMv7 userspace binary emulator for x86 linux systems";
 3  inputs = {
 4    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
 5    flake-utils.url = "github:numtide/flake-utils";
 6  };
 7  outputs = { self, nixpkgs, flake-utils, ... }:
 8    flake-utils.lib.eachDefaultSystem (system:
 9      let
10        pkgs = import nixpkgs { inherit system; };
11      in {
12        devShells.default = pkgs.mkShell {
13          buildInputs = with pkgs; [
14            gcc-arm-embedded
15            binutils
16            qemu
17          ];
18        };
19      }
20  );
21}&lt;/code&gt;
    &lt;head rend="h1"&gt;Parsing ELF&lt;/head&gt;
    &lt;p&gt;So there are some resources for parsing ELF, two of them I used a whole lot:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;man elf&lt;/code&gt;(remember to&lt;code&gt;export MANPAGER='nvim +Man!'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;gabi.xinuos.com&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a high level, ELF (32bit, for armv7) consists of headers and segments, it holds an Elf header, multiple program headers and the rest I don’t care about, since this emulator is only for static binaries, no dynamically linked support.&lt;/p&gt;
    &lt;head rend="h2"&gt;Elf32_Ehdr&lt;/head&gt;
    &lt;p&gt;The ELF header is exactly 52 bytes long and holds all data I need to find the program headers and whether I even want to emulate the binary I’m currently parsing. These criteria are defined as members of the &lt;code&gt;Identifier&lt;/code&gt; at the beg
of the header.&lt;/p&gt;
    &lt;p&gt;In terms of byte layout:&lt;/p&gt;
    &lt;code&gt; 1+------------------------+--------+--------+----------------+----------------+----------------+----------------+----------------+--------+---------+--------+---------+--------+--------+
 2|       identifier       |  type  |machine |    version     |     entry      |     phoff      |     shoff      |     flags      | ehsize |phentsize| phnum  |shentsize| shnum  |shstrndx|
 3|          16B           |   2B   |   2B   |       4B       |       4B       |       4B       |       4B       |       4B       |   2B   |   2B    |   2B   |   2B    |   2B   |   2B   |
 4+------------------------+--------+--------+----------------+----------------+----------------+----------------+----------------+--------+---------+--------+---------+--------+--------+
 5           \|/
 6            |
 7            |
 8            v
 9+----------------+------+------+-------+------+-----------+------------------------+
10|     magic      |class | data |version|os_abi|abi_version|          pad           |
11|       4B       |  1B  |  1B  |  1B   |  1B  |    1B     |           7B           |
12+----------------+------+------+-------+------+-----------+------------------------+&lt;/code&gt;
    &lt;p&gt;Most resources show C based examples, the rust ports are below:&lt;/p&gt;
    &lt;code&gt; 1/// Representing the ELF Object File Format header in memory, equivalent to Elf32_Ehdr in 2. ELF
 2/// header in https://gabi.xinuos.com/elf/02-eheader.html
 3///
 4/// Types are taken from https://gabi.xinuos.com/elf/01-intro.html#data-representation Table 1.1
 5/// 32-Bit Data Types:
 6///
 7/// | Elf32_ | Rust |
 8/// | ------ | ---- |
 9/// | Addr   | u32  |
10/// | Off    | u32  |
11/// | Half   | u16  |
12/// | Word   | u32  |
13/// | Sword  | i32  |
14#[derive(Debug, Clone, Copy, PartialEq, Eq)]
15pub struct Header {
16    /// initial bytes mark the file as an object file and provide machine-independent data with
17    /// which to decode and interpret the file’s contents
18    pub ident: Identifier,
19    pub r#type: Type,
20    pub machine: Machine,
21    /// identifies the object file version, always EV_CURRENT (1)
22    pub version: u32,
23    /// the virtual address to which the system first transfers control, thus starting
24    /// the process. If the file has no associated entry point, this member holds zero
25    pub entry: u32,
26    /// the program header table’s file offset in bytes. If the file has no program header table,
27    /// this member holds zero
28    pub phoff: u32,
29    /// the section header table’s file offset in bytes. If the file has no section header table, this
30    /// member holds zero
31    pub shoff: u32,
32    /// processor-specific flags associated with the file
33    pub flags: u32,
34    /// the ELF header’s size in bytes
35    pub ehsize: u16,
36    /// the size in bytes of one entry in the file’s program header table; all entries are the same
37    /// size
38    pub phentsize: u16,
39    /// the number of entries in the program header table. Thus the product of e_phentsize and e_phnum
40    /// gives the table’s size in bytes. If a file has no program header table, e_phnum holds the value
41    /// zero
42    pub phnum: u16,
43    /// section header’s size in bytes. A section header is one entry in the section header table; all
44    /// entries are the same size
45    pub shentsize: u16,
46    /// number of entries in the section header table. Thus the product of e_shentsize and e_shnum
47    /// gives the section header table’s size in bytes. If a file has no section header table,
48    /// e_shnum holds the value zero.
49    pub shnum: u16,
50    /// the section header table index of the entry associated with the section name string table.
51    /// If the file has no section name string table, this member holds the value SHN_UNDEF
52    pub shstrndx: u16,
53}&lt;/code&gt;
    &lt;p&gt;The identifier is 16 bytes long and holds the previously mentioned info so I can check if I want to emulate the binary, for instance the endianness and the bit class, in the &lt;code&gt;TryFrom&lt;/code&gt; implementation I strictly check what is parsed:&lt;/p&gt;
    &lt;code&gt; 1/// 2.2 ELF Identification: https://gabi.xinuos.com/elf/02-eheader.html#elf-identification
 2#[repr(C)]
 3#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 4pub struct Identifier {
 5    /// 0x7F, 'E', 'L', 'F'
 6    pub magic: [u8; 4],
 7    /// file class or capacity
 8    ///
 9    /// | Name          | Value | Meaning       |
10    /// | ------------- | ----- | ------------- |
11    /// | ELFCLASSNONE  | 0     | Invalid class |
12    /// | ELFCLASS32    | 1     | 32-bit        |
13    /// | ELFCLASS64    | 2     | 64-bit        |
14    pub class: u8,
15    /// data encoding, endian
16    ///
17    /// | Name         | Value |
18    /// | ------------ | ----- |
19    /// | ELFDATANONE  | 0     |
20    /// | ELFDATA2LSB  | 1     |
21    /// | ELFDATA2MSB  | 2     |
22    pub data: u8,
23    /// file version, always EV_CURRENT (1)
24    pub version: u8,
25    /// operating system identification
26    ///
27    /// - if no extensions are used: 0
28    /// - meaning depends on e_machine
29    pub os_abi: u8,
30    /// value depends on os_abi
31    pub abi_version: u8,
32    // padding bytes (9-15)
33    _pad: [u8; 7],
34}
35
36impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Identifier {
37    type Error = &amp;amp;'static str;
38
39    fn try_from(bytes: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
40        if bytes.len() &amp;lt; 16 {
41            return Err("e_ident too short for ELF");
42        }
43
44        // I don't want to cast via unsafe as_ptr and as Header because the header could outlive the
45        // source slice, thus we just do it the old plain indexing way
46        let ident = Self {
47            magic: bytes[0..4].try_into().unwrap(),
48            class: bytes[4],
49            data: bytes[5],
50            version: bytes[6],
51            os_abi: bytes[7],
52            abi_version: bytes[8],
53            _pad: bytes[9..16].try_into().unwrap(),
54        };
55
56        if ident.magic != [0x7f, b'E', b'L', b'F'] {
57            return Err("Unexpected EI_MAG0 to EI_MAG3, wanted 0x7f E L F");
58        }
59
60        const ELFCLASS32: u8 = 1;
61        const ELFDATA2LSB: u8 = 1;
62        const EV_CURRENT: u8 = 1;
63
64        if ident.version != EV_CURRENT {
65            return Err("Unsupported EI_VERSION value");
66        }
67
68        if ident.class != ELFCLASS32 {
69            return Err("Unexpected EI_CLASS: ELFCLASS64, wanted ELFCLASS32 (ARMv7)");
70        }
71
72        if ident.data != ELFDATA2LSB {
73            return Err("Unexpected EI_DATA: big-endian, wanted little");
74        }
75
76        Ok(ident)
77    }&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Type&lt;/code&gt; and &lt;code&gt;Machine&lt;/code&gt; are just enums encoding meaning in the Rust type system:&lt;/p&gt;
    &lt;code&gt; 1#[repr(u16)]
 2#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 3pub enum Type {
 4    None = 0,
 5    Relocatable = 1,
 6    Executable = 2,
 7    SharedObject = 3,
 8    Core = 4,
 9    LoOs = 0xfe00,
10    HiOs = 0xfeff,
11    LoProc = 0xff00,
12    HiProc = 0xffff,
13}
14
15impl TryFrom&amp;lt;u16&amp;gt; for Type {
16    type Error = &amp;amp;'static str;
17
18    fn try_from(value: u16) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
19        match value {
20            0 =&amp;gt; Ok(Type::None),
21            1 =&amp;gt; Ok(Type::Relocatable),
22            2 =&amp;gt; Ok(Type::Executable),
23            3 =&amp;gt; Ok(Type::SharedObject),
24            4 =&amp;gt; Ok(Type::Core),
25            0xfe00 =&amp;gt; Ok(Type::LoOs),
26            0xfeff =&amp;gt; Ok(Type::HiOs),
27            0xff00 =&amp;gt; Ok(Type::LoProc),
28            0xffff =&amp;gt; Ok(Type::HiProc),
29            _ =&amp;gt; Err("Invalid u16 value for e_type"),
30        }
31    }
32}
33
34
35#[repr(u16)]
36#[allow(non_camel_case_types)]
37#[derive(Debug, Clone, Copy, PartialEq, Eq)]
38pub enum Machine {
39    EM_ARM = 40,
40}
41
42impl TryFrom&amp;lt;u16&amp;gt; for Machine {
43    type Error = &amp;amp;'static str;
44
45    fn try_from(value: u16) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
46        match value {
47            40 =&amp;gt; Ok(Machine::EM_ARM),
48            _ =&amp;gt; Err("Unsupported machine"),
49        }
50    }
51}&lt;/code&gt;
    &lt;p&gt;Since all of &lt;code&gt;Header&lt;/code&gt;’s members implement &lt;code&gt;TryFrom&lt;/code&gt; we can implement
&lt;code&gt;TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Header&lt;/code&gt; and propagate all occurring errors in member parsing
cleanly via &lt;code&gt;?&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Header {
 2    type Error = &amp;amp;'static str;
 3
 4    fn try_from(b: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
 5        if b.len() &amp;lt; 52 {
 6            return Err("not enough bytes for Elf32_Ehdr (ELF header)");
 7        }
 8
 9        let header = Self {
10            ident: b[0..16].try_into()?,
11            r#type: le16!(b[16..18]).try_into()?,
12            machine: le16!(b[18..20]).try_into()?,
13            version: le32!(b[20..24]),
14            entry: le32!(b[24..28]),
15            phoff: le32!(b[28..32]),
16            shoff: le32!(b[32..36]),
17            flags: le32!(b[36..40]),
18            ehsize: le16!(b[40..42]),
19            phentsize: le16!(b[42..44]),
20            phnum: le16!(b[44..46]),
21            shentsize: le16!(b[46..48]),
22            shnum: le16!(b[48..50]),
23            shstrndx: le16!(b[50..52]),
24        };
25
26        match header.r#type {
27            Type::Executable =&amp;gt; (),
28            _ =&amp;gt; {
29                return Err("Unsupported ELF type, only ET_EXEC (static executables) is supported");
30            }
31        }
32
33        Ok(header)
34    }
35}&lt;/code&gt;
    &lt;p&gt;The attentive reader will see me using &lt;code&gt;le16!&lt;/code&gt; and &lt;code&gt;le32!&lt;/code&gt; for parsing bytes
into unsigned integers of different classes (&lt;code&gt;le&lt;/code&gt; is short for little endian):&lt;/p&gt;
    &lt;code&gt; 1#[macro_export]
 2macro_rules! le16 {
 3    ($bytes:expr) =&amp;gt; {{
 4        let b: [u8; 2] = $bytes
 5            .try_into()
 6            .map_err(|_| "Failed to create u16 from 2*u8")?;
 7        u16::from_le_bytes(b)
 8    }};
 9}
10
11#[macro_export]
12macro_rules! le32 {
13    ($bytes:expr) =&amp;gt; {{
14        let b: [u8; 4] = $bytes
15            .try_into()
16            .map_err(|_| "Failed to create u32 from 4*u8")?;
17        u32::from_le_bytes(b)
18    }};
19}&lt;/code&gt;
    &lt;head rend="h2"&gt;Elf32_Phdr&lt;/head&gt;
    &lt;code&gt;1+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
2|      type      |     offset     |     vaddr      |     paddr      |     filesz     |     memsz      |     flags      |     align      |
3|       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |
4+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+&lt;/code&gt;
    &lt;p&gt;For me, the most important fields in &lt;code&gt;Header&lt;/code&gt; are &lt;code&gt;phoff&lt;/code&gt; and &lt;code&gt;phentsize&lt;/code&gt;,
since we can use these to index into the binary to locate the program headers (&lt;code&gt;Phdr&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt; 1/// Phdr, equivalent to Elf32_Phdr, see: https://gabi.xinuos.com/elf/07-pheader.html
 2///
 3/// All of its member are u32, be it Elf32_Word, Elf32_Off or Elf32_Addr
 4#[derive(Debug)]
 5pub struct Pheader {
 6    pub r#type: Type,
 7    pub offset: u32,
 8    pub vaddr: u32,
 9    pub paddr: u32,
10    pub filesz: u32,
11    pub memsz: u32,
12    pub flags: Flags,
13    pub align: u32,
14}
15
16impl Pheader {
17    /// extracts Pheader from raw, starting from offset
18    pub fn from(raw: &amp;amp;[u8], offset: usize) -&amp;gt; Result&amp;lt;Self, String&amp;gt; {
19        let end = offset.checked_add(32).ok_or("Offset overflow")?;
20        if raw.len() &amp;lt; end {
21            return Err("Not enough bytes to parse Elf32_Phdr, need at least 32".into());
22        }
23
24        let p_raw = &amp;amp;raw[offset..end];
25        let r#type = p_raw[0..4].try_into()?;
26        let flags = p_raw[24..28].try_into()?;
27        let align = le32!(p_raw[28..32]);
28
29        if align &amp;gt; 1 &amp;amp;&amp;amp; !align.is_power_of_two() {
30            return Err(format!("Invalid p_align: {}", align));
31        }
32
33        Ok(Self {
34            r#type,
35            offset: le32!(p_raw[4..8]),
36            vaddr: le32!(p_raw[8..12]),
37            paddr: le32!(p_raw[12..16]),
38            filesz: le32!(p_raw[16..20]),
39            memsz: le32!(p_raw[20..24]),
40            flags,
41            align,
42        })
43    }
44}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Type&lt;/code&gt; holds info about what type of segment the header defines:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(C)]
 3pub enum Type {
 4    NULL = 0,
 5    LOAD = 1,
 6    DYNAMIC = 2,
 7    INTERP = 3,
 8    NOTE = 4,
 9    SHLIB = 5,
10    PHDR = 6,
11    TLS = 7,
12    LOOS = 0x60000000,
13    HIOS = 0x6fffffff,
14    LOPROC = 0x70000000,
15    HIPROC = 0x7fffffff,
16}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Flag&lt;/code&gt; defines the
permission flags the segment should have once it is dumped into memory:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(transparent)]
 3pub struct Flags(u32);
 4
 5impl Flags {
 6    pub const NONE: Self = Flags(0x0);
 7    pub const X: Self = Flags(0x1);
 8    pub const W: Self = Flags(0x2);
 9    pub const R: Self = Flags(0x4);
10}&lt;/code&gt;
    &lt;head rend="h2"&gt;Full ELF parsing&lt;/head&gt;
    &lt;p&gt;Putting &lt;code&gt;Elf32_Ehdr&lt;/code&gt; and &lt;code&gt;Elf32_Phdr&lt;/code&gt; parsing together:&lt;/p&gt;
    &lt;code&gt; 1/// Representing an ELF32 binary in memory
 2///
 3/// This does not include section headers (Elf32_Shdr), but only program headers (Elf32_Phdr), see either `man elf` and/or https://gabi.xinuos.com/elf/03-sheader.html
 4#[derive(Debug)]
 5pub struct Elf {
 6    pub header: header::Header,
 7    pub pheaders: Vec&amp;lt;pheader::Pheader&amp;gt;,
 8}
 9
10impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Elf {
11    type Error = String;
12
13    fn try_from(b: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, String&amp;gt; {
14        let header = header::Header::try_from(b).map_err(|e| e.to_string())?;
15
16        let mut pheaders = Vec::with_capacity(header.phnum as usize);
17        for i in 0..header.phnum {
18            let offset = header.phoff as usize + i as usize * header.phentsize as usize;
19            let ph = pheader::Pheader::from(b, offset)?;
20            pheaders.push(ph);
21        }
22
23        Ok(Elf { header, pheaders })
24    }
25}&lt;/code&gt;
    &lt;p&gt;The equivalent to &lt;code&gt;readelf -l&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1Elf {
 2    header: Header {
 3        ident: Identifier {
 4            magic: [127, 69, 76, 70],
 5            class: 1,
 6            data: 1,
 7            version: 1,
 8            os_abi: 0,
 9            abi_version: 0,
10            _pad: [0, 0, 0, 0, 0, 0, 0]
11        },
12        type: Executable,
13        machine: EM_ARM,
14        version: 1,
15        entry: 32768,
16        phoff: 52,
17        shoff: 4572,
18        flags: 83886592,
19        ehsize: 52,
20        phentsize: 32,
21        phnum: 1,
22        shentsize: 40,
23        shnum: 8,
24        shstrndx: 7
25    },
26    pheaders: [
27        Pheader {
28            type: LOAD,
29            offset: 4096,
30            vaddr: 32768,
31            paddr: 32768,
32            filesz: 12,
33            memsz: 12,
34            flags: Flags(5),
35            align: 4096
36        }
37    ]
38}&lt;/code&gt;
    &lt;p&gt;Or in the debug output of stinkarm:&lt;/p&gt;
    &lt;code&gt; 1[     0.613ms] opening binary "examples/asm.elf"
 2[     0.721ms] parsing ELF...
 3[     0.744ms] \
 4ELF Header:
 5  Magic:              [7f, 45, 4c, 46]
 6  Class:              ELF32
 7  Data:               Little endian
 8  Type:               Executable
 9  Machine:            EM_ARM
10  Version:            1
11  Entry point:        0x8000
12  Program hdr offset: 52 (32 bytes each)
13  Section hdr offset: 4572
14  Flags:              0x05000200
15  EH size:            52
16  # Program headers:  1
17  # Section headers:  8
18  Str tbl index:      7
19
20Program Headers:
21  Type       Offset   VirtAddr   PhysAddr   FileSz    MemSz  Flags  Align
22  LOAD     0x001000 0x00008000 0x00008000 0x00000c 0x00000c    R|X 0x1000&lt;/code&gt;
    &lt;head rend="h1"&gt;Dumping ELF segments into memory&lt;/head&gt;
    &lt;p&gt;Since the only reason for parsing the elf headers is to know where to put what segment with which permissions, I want to quickly interject on why we have to put said segments at these specific addresses. The main reason is that all pointers, all offsets and pc related decoding has to be done relative to &lt;code&gt;Elf32_Ehdr.entry&lt;/code&gt;, here &lt;code&gt;0x8000&lt;/code&gt;. The linker also generated all instruction
arguments according to this value.&lt;/p&gt;
    &lt;p&gt;Before mapping each segment at its &lt;code&gt;Pheader::vaddr&lt;/code&gt;, we have to understand:
One doesn’t simply &lt;code&gt;mmap&lt;/code&gt; with &lt;code&gt;MAP_FIXED&lt;/code&gt; or &lt;code&gt;MAP_NOREPLACE&lt;/code&gt; into the virtual
address &lt;code&gt;0x8000&lt;/code&gt;. The Linux kernel won’t let us, and rightfully so, &lt;code&gt;man mmap&lt;/code&gt;
says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If addr is not NULL, then the kernel takes it as a hint about where to place the mapping; on Linux, the kernel will pick a nearby page boundary (but always above or equal to the value specified by /proc/sys/vm/mmap_min_addr) and attempt to create the mapping there.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And &lt;code&gt;/proc/sys/vm/mmap_min_addr&lt;/code&gt; on my system is &lt;code&gt;u16::MAX&lt;/code&gt; (2^16)-1=65535. So
mapping our segment to &lt;code&gt;0x8000&lt;/code&gt; (32768) is not allowed:&lt;/p&gt;
    &lt;code&gt; 1let segment = sys::mmap::mmap(
 2    // this is only UB if dereferenced, its just a hint, so its safe here
 3    Some(unsafe { std::ptr::NonNull::new_unchecked(0x8000 as *mut u8) }),
 4    4096,
 5    sys::mmap::MmapProt::WRITE,
 6    sys::mmap::MmapFlags::ANONYMOUS
 7        | sys::mmap::MmapFlags::PRIVATE
 8        | sys::mmap::MmapFlags::NOREPLACE,
 9    -1,
10    0,
11)
12.unwrap();&lt;/code&gt;
    &lt;p&gt;Running the above with our &lt;code&gt;vaddr&lt;/code&gt; of &lt;code&gt;0x8000&lt;/code&gt; results in:&lt;/p&gt;
    &lt;code&gt;1thread 'main' panicked at src/main.rs:33:6:
2called `Result::unwrap()` on an `Err` value: "mmap failed (errno 1): Operation not permitted
3(os error 1)"&lt;/code&gt;
    &lt;p&gt;It only works in elevated permission mode, which is something I dont want to run my emulator in.&lt;/p&gt;
    &lt;head rend="h2"&gt;Translating guest memory access to host memory access&lt;/head&gt;
    &lt;p&gt;The obvious fix is to not mmap below &lt;code&gt;u16::MAX&lt;/code&gt; and let the kernel choose where
we dump our segment:&lt;/p&gt;
    &lt;code&gt;1let segment = sys::mmap::mmap(
2    None,
3    4096,
4    MmapProt::WRITE,
5    MmapFlags::ANONYMOUS | MmapFlags::PRIVATE,
6    -1,
7    0,
8).unwrap();&lt;/code&gt;
    &lt;p&gt;But this means the segment of the process to emulate is not at &lt;code&gt;0x8000&lt;/code&gt;, but
anywhere the kernel allows. So we need to add a translation layer between guest
and host memory: (If you’re familiar with how virtual memory works, its similar
but one more indirection)&lt;/p&gt;
    &lt;code&gt;1+--guest--+
2| 0x80000 | ------------+
3+---------+             |
4                        |
5                    Mem::translate
6                        |
7+------host------+      |
8| 0x7f5b4b8f8000 | &amp;lt;----+
9+----------------+&lt;/code&gt;
    &lt;p&gt;Putting this into rust:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;map_region&lt;/code&gt;registers a region of memory and allows&lt;code&gt;Mem&lt;/code&gt;to take ownership for calling munmap on these segments once it goes out of scope&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;translate&lt;/code&gt;takes a guest addr and translates it to a host addr&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; 1struct MappedSegment {
 2    host_ptr: *mut u8,
 3    len: u32,
 4}
 5
 6pub struct Mem {
 7    maps: BTreeMap&amp;lt;u32, MappedSegment&amp;gt;,
 8}
 9
10impl Mem {
11    pub fn map_region(&amp;amp;mut self, guest_addr: u32, len: u32, host_ptr: *mut u8) {
12        self.maps
13            .insert(guest_addr, MappedSegment { host_ptr, len });
14    }
15
16    /// translate a guest addr to a host addr we can write and read from
17    pub fn translate(&amp;amp;self, guest_addr: u32) -&amp;gt; Option&amp;lt;*mut u8&amp;gt; {
18        // Find the greatest key &amp;lt;= guest_addr.
19        let (&amp;amp;base, seg) = self.maps.range(..=guest_addr).next_back()?;
20        if guest_addr &amp;lt; base.wrapping_add(seg.len) {
21            let offset = guest_addr.wrapping_sub(base);
22            Some(unsafe { seg.host_ptr.add(offset as usize) })
23        } else {
24            None
25        }
26    }
27
28    pub fn read_u32(&amp;amp;self, guest_addr: u32) -&amp;gt; Option&amp;lt;u32&amp;gt; {
29        let ptr = self.translate(guest_addr)?;
30        unsafe { Some(u32::from_le(*(ptr as *const u32))) }
31    }
32}&lt;/code&gt;
    &lt;p&gt;This fix has the added benfit of allowing us to sandbox guest memory fully, so we can validate each memory access before we allow a guest to host memory interaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mapping segments with their permissions&lt;/head&gt;
    &lt;p&gt;The basic idea is similar to the way a JIT compiler works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;create a &lt;code&gt;mmap&lt;/code&gt;section with&lt;code&gt;W&lt;/code&gt;permissions&lt;/item&gt;
      &lt;item&gt;write bytes from elf into section&lt;/item&gt;
      &lt;item&gt;zero rest of defined size&lt;/item&gt;
      &lt;item&gt;change permission of section with &lt;code&gt;mprotect&lt;/code&gt;to the permissions defined in the&lt;code&gt;Pheader&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; 1/// mapping applies the configuration of self to the current memory context by creating the
 2/// segments with the corresponding permission bits, vaddr, etc
 3pub fn map(&amp;amp;self, raw: &amp;amp;[u8], guest_mem: &amp;amp;mut mem::Mem) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
 4    // zero memory needed case, no clue if this actually ever happens, but we support it
 5    if self.memsz == 0 {
 6        return Ok(());
 7    }
 8
 9    if self.vaddr == 0 {
10        return Err("program header has a zero virtual address".into());
11    }
12
13    // we need page alignement, so either Elf32_Phdr.p_align or 4096
14    let (start, _end, len) = self.alignments();
15
16    // Instead of mapping at the guest vaddr (Linux doesnt't allow for low addresses),
17    // we allocate memory wherever the host kernel gives us.
18    // This keeps guest memory sandboxed: guest addr != host addr.
19    let segment = mem::mmap::mmap(
20        None,
21        len as usize,
22        MmapProt::WRITE,
23        MmapFlags::ANONYMOUS | MmapFlags::PRIVATE,
24        -1,
25        0,
26    )?;
27
28    let segment_ptr = segment.as_ptr();
29    let segment_slice = unsafe { std::slice::from_raw_parts_mut(segment_ptr, len as usize) };
30
31    let file_slice: &amp;amp;[u8] =
32        &amp;amp;raw[self.offset as usize..(self.offset.wrapping_add(self.filesz)) as usize];
33
34    // compute offset inside the mmapped slice where the segment should start
35    let offset = (self.vaddr - start) as usize;
36
37    // copy the segment contents to the mmaped segment
38    segment_slice[offset..offset + file_slice.len()].copy_from_slice(file_slice);
39
40    // we need to zero the remaining bytes
41    if self.memsz &amp;gt; self.filesz {
42        segment_slice
43            [offset.wrapping_add(file_slice.len())..offset.wrapping_add(self.memsz as usize)]
44            .fill(0);
45    }
46
47    // record mapping in guest memory table, so CPU can translate guest vaddr to host pointer
48    guest_mem.map_region(self.vaddr, len, segment_ptr);
49
50    // we change the permissions for our segment from W to the segments requested bits
51    mem::mmap::mprotect(segment, len as usize, self.flags.into())
52}
53
54/// returns (start, end, len)
55fn alignments(&amp;amp;self) -&amp;gt; (u32, u32, u32) {
56    // we need page alignement, so either Elf32_Phdr.p_align or 4096
57    let align = match self.align {
58        0 =&amp;gt; 0x1000,
59        _ =&amp;gt; self.align,
60    };
61    let start = self.vaddr &amp;amp; !(align - 1);
62    let end = (self.vaddr.wrapping_add(self.memsz).wrapping_add(align) - 1) &amp;amp; !(align - 1);
63    let len = end - start;
64    (start, end, len)
65}&lt;/code&gt;
    &lt;p&gt;Map is called in the emulators entry point:&lt;/p&gt;
    &lt;code&gt;1let elf: elf::Elf = (&amp;amp;buf as &amp;amp;[u8]).try_into().expect("Failed to parse binary");
2let mut mem = mem::Mem::new();
3for phdr in elf.pheaders {
4    if phdr.r#type == elf::pheader::Type::LOAD {
5        phdr.map(&amp;amp;buf, &amp;amp;mut mem)
6            .expect("Mapping program header failed");
7    }
8}&lt;/code&gt;
    &lt;head rend="h1"&gt;Decoding armv7&lt;/head&gt;
    &lt;p&gt;We can now request a word (32bit) from our &lt;code&gt;LOAD&lt;/code&gt; segment which contains
the &lt;code&gt;.text&lt;/code&gt; section bytes one can inspect via &lt;code&gt;objdump&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1$ arm-none-eabi-objdump -d examples/exit.elf
 2
 3examples/exit.elf:     file format elf32-littlearm
 4
 5
 6Disassembly of section .text:
 7
 800008000 &amp;lt;_start&amp;gt;:
 9    8000:       e3a000a1        mov     r0, #161        @ 0xa1
10    8004:       e3a07001        mov     r7, #1
11    8008:       ef000000        svc     0x00000000&lt;/code&gt;
    &lt;p&gt;So we use &lt;code&gt;Mem::read_u32(0x8000)&lt;/code&gt; and get &lt;code&gt;0xe3a000a1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Decoding armv7 instructions seems doable at a glance, but it is a deeper rabbit-hole than I expected, prepare for a bit shifting, implicit behaviour and intertwined meaning heavy section:&lt;/p&gt;
    &lt;p&gt;Instructions are more or less grouped into four groups:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Branch and control&lt;/item&gt;
      &lt;item&gt;Data processing&lt;/item&gt;
      &lt;item&gt;Load and store&lt;/item&gt;
      &lt;item&gt;Other (syscalls &amp;amp; stuff)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each armv7 instruction is 32 bit in size, (in general) its layout is as follows:&lt;/p&gt;
    &lt;code&gt;1+--------+------+------+------+------------+---------+
2|  cond  |  op  |  Rn  |  Rd  |  Operand2  |  shamt  |
3|   4b   |  4b  |  4b  |  4b  |     12b    |   4b    |
4+--------+------+------+------+------------+---------+&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;bit range&lt;/cell&gt;
        &lt;cell role="head"&gt;name&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0..4&lt;/cell&gt;
        &lt;cell&gt;cond&lt;/cell&gt;
        &lt;cell&gt;contains &lt;code&gt;EQ&lt;/code&gt;, &lt;code&gt;NE&lt;/code&gt;, etc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4..8&lt;/cell&gt;
        &lt;cell&gt;op&lt;/cell&gt;
        &lt;cell&gt;for instance &lt;code&gt;0b1101&lt;/code&gt; for &lt;code&gt;mov&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8..12&lt;/cell&gt;
        &lt;cell&gt;rn&lt;/cell&gt;
        &lt;cell&gt;source register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12..16&lt;/cell&gt;
        &lt;cell&gt;rd&lt;/cell&gt;
        &lt;cell&gt;destination register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16..28&lt;/cell&gt;
        &lt;cell&gt;operand2&lt;/cell&gt;
        &lt;cell&gt;immediate value or shifted register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28..32&lt;/cell&gt;
        &lt;cell&gt;shamt&lt;/cell&gt;
        &lt;cell&gt;shift amount&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Rust representation&lt;/head&gt;
    &lt;p&gt;Since &lt;code&gt;cond&lt;/code&gt; decides whether or not the instruction is
executed, I decided on the following struct to be the decoded
instruction:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Copy, Clone)]
 2pub struct InstructionContainer {
 3    pub cond: u8,
 4    pub instruction: Instruction,
 5}
 6
 7#[derive(Debug, Copy, Clone)]
 8pub enum Instruction {
 9    MovImm { rd: u8, rhs: u32 },
10    Svc,
11    LdrLiteral { rd: u8, addr: u32 },
12    Unknown(u32),
13}&lt;/code&gt;
    &lt;p&gt;These 4 instructions are enough to support both the minimal binary at the intro and the asm hello world:&lt;/p&gt;
    &lt;code&gt;1    .global _start
2_start:
3    mov r0, #161
4    mov r7, #1
5    svc #0&lt;/code&gt;
    &lt;code&gt; 1    .section .rodata
 2msg:
 3    .asciz "Hello, world!\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    ldr r0, =1
 9    ldr r1, =msg
10    mov r2, #14
11    mov r7, #4
12    svc #0
13
14    mov r0, #0
15    mov r7, #1
16    svc #0&lt;/code&gt;
    &lt;head rend="h2"&gt;General instruction detection&lt;/head&gt;
    &lt;p&gt;Our decoder is a function accepting a word, the program counter (we need this later for decoding the offset for &lt;code&gt;ldr&lt;/code&gt;) and returning the
aforementioned instruction container:&lt;/p&gt;
    &lt;code&gt;1pub fn decode_word(word: u32, caddr: u32) -&amp;gt; InstructionContainer&lt;/code&gt;
    &lt;p&gt;Referring to the diagram shown before, I know the first 4 bit are the condition, so I can extract these first. I also take the top 3 bits to identify the instruction class (load and store, branch or data processing immediate):&lt;/p&gt;
    &lt;code&gt;1// ...
2let cond = ((word &amp;gt;&amp;gt; 28) &amp;amp; 0xF) as u8;
3let top = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x7) as u8;&lt;/code&gt;
    &lt;head rend="h2"&gt;Immediate mov&lt;/head&gt;
    &lt;p&gt;Since there are immediate moves and non immediate moves, both &lt;code&gt;0b000&lt;/code&gt; and
&lt;code&gt;0b001&lt;/code&gt; are valid top values we want to support.&lt;/p&gt;
    &lt;code&gt;1// ...
2if top == 0b000 || top == 0b001 {
3    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
4    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
5    if i_bit {
6        // ...
7    }
8}&lt;/code&gt;
    &lt;p&gt;If the i bit is set, we can extract convert the opcode from its bits into something I can read a lot better:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(u8)]
 3enum Op {
 4    // ...
 5    Mov = 0b1101,
 6}
 7
 8static OP_TABLE: [Op; 16] = [
 9    // ...
10    Op::Mov,
11];
12
13#[inline(always)]
14fn op_from_bits(bits: u8) -&amp;gt; Op {
15    debug_assert!(bits &amp;lt;= 0b1111);
16    unsafe { *OP_TABLE.get_unchecked(bits as usize) }
17}&lt;/code&gt;
    &lt;p&gt;We can now plug this in, match on the only ddi (data processing immediate) we know and extract both the destination register (rd) and the raw immediate value:&lt;/p&gt;
    &lt;code&gt; 1if top == 0b000 || top == 0b001 {
 2    // Data-processing immediate (ddi) (top 0b000 or 0b001 when I==1)
 3    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
 4    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
 5    if i_bit {
 6        match op_from_bits(opcode) {
 7            Op::Mov =&amp;gt; {
 8                let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 9                let imm12 = word &amp;amp; 0xFFF;
10                // ...
11            }
12            _ =&amp;gt; todo!(),
13        }
14    }
15}&lt;/code&gt;
    &lt;p&gt;From the examples before one can see the immediate value is prefixed with &lt;code&gt;#&lt;/code&gt;. To move the value &lt;code&gt;161&lt;/code&gt; into &lt;code&gt;r0&lt;/code&gt; we do:&lt;/p&gt;
    &lt;code&gt;1mov r0, #161&lt;/code&gt;
    &lt;p&gt;Since we know there are only 12 bits available for the immediate the arm engineers came up with rotation of the resulting integer by the remaining 4 bits:&lt;/p&gt;
    &lt;code&gt;1#[inline(always)]
2fn decode_rotated_imm(imm12: u32) -&amp;gt; u32 {
3    let rotate = ((imm12 &amp;gt;&amp;gt; 8) &amp;amp; 0b1111) * 2;
4    (imm12 &amp;amp; 0xff).rotate_right(rotate)
5}&lt;/code&gt;
    &lt;p&gt;Plugging this back in results in us being able to fully decode &lt;code&gt;mov r0,#161&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1if top == 0b000 || top == 0b001 {
 2    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
 3    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
 4    if i_bit {
 5        match op_from_bits(opcode) {
 6            Op::Mov =&amp;gt; {
 7                let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 8                let imm12 = word &amp;amp; 0xFFF;
 9                let rhs = decode_rotated_imm(imm12);
10                return InstructionContainer {
11                    cond,
12                    instruction: Instruction::MovImm { rd, rhs },
13                };
14            }
15            _ =&amp;gt; todo!(),
16        }
17    }
18}&lt;/code&gt;
    &lt;p&gt;As seen when &lt;code&gt;dbg!&lt;/code&gt;-ing the cpu steps:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:114:13] decoder::decode_word(word, self.pc()) =
2InstructionContainer {
3    cond: 14,
4    instruction: MovImm {
5        rd: 0,
6        rhs: 161,
7    },
8}&lt;/code&gt;
    &lt;head rend="h2"&gt;Load and Store&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;ldr&lt;/code&gt; is part of the load and store instruction group and is needed for
the accessing of &lt;code&gt;Hello World!&lt;/code&gt; in &lt;code&gt;.rodata&lt;/code&gt; and putting a ptr to it
into a register.&lt;/p&gt;
    &lt;p&gt;In comparison to immediate mov we have to do a little trick, since we only want to match for load and store that matches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;single register modification&lt;/item&gt;
      &lt;item&gt;load and store with immediate&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So we only decode:&lt;/p&gt;
    &lt;code&gt;1LDR Rd, [Rn, #imm]
2LDR Rd, [Rn], #imm
3@ etc
&lt;/code&gt;
    &lt;p&gt;Thus we match with &lt;code&gt;(top &amp;gt;&amp;gt; 1) &amp;amp; 0b11 == 0b01&lt;/code&gt; and start extracting a
whole bucket load of bit flags:&lt;/p&gt;
    &lt;code&gt; 1if (top &amp;gt;&amp;gt; 1) &amp;amp; 0b11 == 0b01 {
 2    let p = ((word &amp;gt;&amp;gt; 24) &amp;amp; 1) != 0;
 3    let u = ((word &amp;gt;&amp;gt; 23) &amp;amp; 1) != 0;
 4    let b = ((word &amp;gt;&amp;gt; 22) &amp;amp; 1) != 0;
 5    let w = ((word &amp;gt;&amp;gt; 21) &amp;amp; 1) != 0;
 6    let l = ((word &amp;gt;&amp;gt; 20) &amp;amp; 1) != 0;
 7    let rn = ((word &amp;gt;&amp;gt; 16) &amp;amp; 0xF) as u8;
 8    let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 9    let imm12 = (word &amp;amp; 0xFFF) as u32;
10
11    // Literal‑pool version
12    if l &amp;amp;&amp;amp; rn == 0b1111 &amp;amp;&amp;amp; p &amp;amp;&amp;amp; u &amp;amp;&amp;amp; !w &amp;amp;&amp;amp; !b {
13        let pc_seen = caddr.wrapping_add(8);
14        let literal_addr = pc_seen.wrapping_add(imm12);
15
16        return InstructionContainer {
17            cond,
18            instruction: Instruction::LdrLiteral {
19                rd,
20                addr: literal_addr,
21            },
22        };
23    }
24
25    todo!("only LDR with p&amp;amp;u&amp;amp;!w&amp;amp;!b is implemented")
26}&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;bit&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;pre-indexed addressing, offset added before load&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;u&lt;/cell&gt;
        &lt;cell&gt;add (1) vs subtract (0) offset&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;b&lt;/cell&gt;
        &lt;cell&gt;word (0) or byte (1) sized access&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;w&lt;/cell&gt;
        &lt;cell&gt;(no=0) write back to base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;l&lt;/cell&gt;
        &lt;cell&gt;load (1), or store (0)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;ldr Rn, &amp;lt;addr&amp;gt;&lt;/code&gt; matches exactly &lt;code&gt;load&lt;/code&gt;, base register is PC (&lt;code&gt;rn==0b1111&lt;/code&gt;), pre-indexed
addressing, added offset, no write back and no byte sized access (&lt;code&gt;l &amp;amp;&amp;amp; rn == 0b1111 &amp;amp;&amp;amp; p &amp;amp;&amp;amp; u &amp;amp;&amp;amp; !w &amp;amp;&amp;amp; !b&lt;/code&gt;).&lt;/p&gt;
    &lt;head rend="h2"&gt;Syscalls&lt;/head&gt;
    &lt;p&gt;Syscalls are the only way to interact with the Linux kernel (as far as I know), so we definitely need to implement both decoding and forwarding. Bits 27-24 are &lt;code&gt;1111&lt;/code&gt; for system calls. The immediate value is
irrelevant for us, since the Linux syscall handler either way discards
the value:&lt;/p&gt;
    &lt;code&gt;1if ((word &amp;gt;&amp;gt; 24) &amp;amp; 0xF) as u8 == 0b1111 {
2    return InstructionContainer {
3        cond,
4        // technically arm says svc has a 24bit immediate but we don't care about it, since the
5        // Linux kernel also doesn't
6        instruction: Instruction::Svc,
7    };
8}&lt;/code&gt;
    &lt;p&gt;We can now fully decode all instructions for both the simple exit and the more advanced hello world binary:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 161, }
2[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 1, }
3[src/cpu/mod.rs:121:15] instruction = Svc&lt;/code&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 1, }
2[src/cpu/mod.rs:121:15] instruction = LdrLiteral { rd: 1, addr: 32800, }
3[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 2, rhs: 14, }
4[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 4, }
5[src/cpu/mod.rs:121:15] instruction = Svc
6[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 0, }
7[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 1, }
8[src/cpu/mod.rs:121:15] instruction = Svc&lt;/code&gt;
    &lt;head rend="h1"&gt;Emulating the CPU&lt;/head&gt;
    &lt;p&gt;This is by FAR the easiest part, I only struggled with the double indirection for &lt;code&gt;ldr&lt;/code&gt; (I simply didn’t know about it), but each problem
at its time :^).&lt;/p&gt;
    &lt;code&gt; 1pub struct Cpu&amp;lt;'cpu&amp;gt; {
 2    /// r0-r15 (r13=SP, r14=LR, r15=PC)
 3    pub r: [u32; 16],
 4    pub cpsr: u32,
 5    pub mem: &amp;amp;'cpu mut mem::Mem,
 6    /// only set by ArmSyscall::Exit to propagate exit code to the host
 7    pub status: Option&amp;lt;i32&amp;gt;,
 8}
 9
10impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
11    pub fn new(mem: &amp;amp;'cpu mut mem::Mem, pc: u32) -&amp;gt; Self {
12        let mut s = Self {
13            r: [0; 16],
14            cpsr: 0x60000010,
15            mem,
16            status: None,
17        };
18        s.r[15] = pc;
19        s
20    }&lt;/code&gt;
    &lt;p&gt;Instantiating the cpu:&lt;/p&gt;
    &lt;code&gt;1let mut cpu = cpu::Cpu::new(&amp;amp;mut mem, elf.header.entry);&lt;/code&gt;
    &lt;head rend="h2"&gt;Conditional Instructions?&lt;/head&gt;
    &lt;p&gt;When writing the decoder I was confused by the 4 conditional bits. I always though one does conditional execution by using a branch to jump over instructions that shouldnt be executed. That was before I learned for arm, both ways are supported (the armv7 reference says this feature should only be used if there arent multiple instructions depending on the same condition, otherwise one should use branches) - so I need to support this too:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    #[inline(always)]
 3    fn cond_passes(&amp;amp;self, cond: u8) -&amp;gt; bool {
 4        match cond {
 5            0x0 =&amp;gt; (self.cpsr &amp;gt;&amp;gt; 30) &amp;amp; 1 == 1, // EQ: Z == 1
 6            0x1 =&amp;gt; (self.cpsr &amp;gt;&amp;gt; 30) &amp;amp; 1 == 0, // NE
 7            0xE =&amp;gt; true,                       // AL (always)
 8            0xF =&amp;gt; false,                      // NV (never)
 9            _ =&amp;gt; false,                        // strict false
10        }
11    }
12}&lt;/code&gt;
    &lt;head rend="h2"&gt;Instruction dispatch&lt;/head&gt;
    &lt;p&gt;After implementing the necessary checks and setup for emulating the cpu, the CPU can now check if an instruction is to be executed, match on the decoded instruction and run the associated logic:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    #[inline(always)]
 3    fn pc(&amp;amp;self) -&amp;gt; u32 {
 4        self.r[15] &amp;amp; !0b11
 5    }
 6
 7    /// moves pc forward a word
 8    #[inline(always)]
 9    fn advance(&amp;amp;mut self) {
10        self.r[15] = self.r[15].wrapping_add(4);
11    }
12
13    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
14        let Some(word) = self.mem.read_u32(self.pc()) else {
15            return Ok(false);
16        };
17
18        if word == 0 {
19            // zero instruction means we hit zeroed out rest of the page
20            return Ok(false);
21        }
22
23        let InstructionContainer { instruction, cond } = decoder::decode_word(word, self.pc());
24
25        if !self.cond_passes(cond) {
26            self.advance();
27            return Ok(true);
28        }
29
30        match instruction {
31            decoder::Instruction::MovImm { rd, rhs } =&amp;gt; {
32                self.r[rd as usize] = rhs;
33            }
34            decoder::Instruction::Unknown(w) =&amp;gt; {
35                return Err(err::Err::UnknownOrUnsupportedInstruction(w));
36            }
37            i =&amp;gt; {
38                stinkln!(
39                    "found unimplemented instruction, exiting: {:#x}:={:?}",
40                    word,
41                    i
42                );
43                self.status = Some(1);
44            }
45        }
46
47        self.advance();
48
49        Ok(true)
50    }
51}&lt;/code&gt;
    &lt;head rend="h2"&gt;LDR and addresses in literal pools&lt;/head&gt;
    &lt;p&gt;While Translating guest memory access to host memory access goes into depth on translating / forwarding guest memory access to host memory adresses, this chapter will focus on the layout of literals in armv7 and how &lt;code&gt;ldr&lt;/code&gt; indirects
memory access.&lt;/p&gt;
    &lt;p&gt;Lets first take a look at the ldr instruction of our hello world example:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2    @ define a string with the `msg` label
 3msg:
 4    @ asciz is like asciii but zero terminated
 5    .asciz "Hello world!\n"
 6
 7    .section .text
 8    .global _start
 9_start:
10    @ load the literal pool addr of msg into r1
11    ldr r1, =msg&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;as&lt;/code&gt;
documentation
says:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;LDR&lt;/code&gt;ARMASM&lt;code&gt;1ldr &amp;lt;register&amp;gt;, = &amp;lt;expression&amp;gt;&lt;/code&gt;&lt;p&gt;If expression evaluates to a numeric constant then a MOV or MVN instruction will be used in place of the LDR instruction, if the constant can be generated by either of these instructions. Otherwise the constant will be placed into the nearest literal pool (if it not already there) and a PC relative LDR instruction will be generated.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Now this may not make sense at a first glance, why would &lt;code&gt;=msg&lt;/code&gt; be assembled
into an address to the address of the literal. But an &lt;code&gt;armv7&lt;/code&gt; instruction can
not encode a full address, it is impossible due to the instruction being
restricted to an 8-bit value rotated right by an even number of bits. The ldr
instructions argument points to a literal pool entry, this entry is a 32-bit
value and reading it produces the actual address of &lt;code&gt;msg&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When decoding we can see ldr points to a memory address (32800 or &lt;code&gt;0x8020&lt;/code&gt;) in
the section we mmaped earlier:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = LdrLiteral { rd: 1, addr: 32800 }&lt;/code&gt;
    &lt;p&gt;Before accessing guest memory, we must translate said addr to a host addr:&lt;/p&gt;
    &lt;code&gt; 1+--ldr.addr--+
 2|   0x8020   |
 3+------------+
 4      |
 5      |             +-------------Mem::read_u32(addr)-------------+
 6      |             |                                             |
 7      |             |   +--guest--+                               |
 8      |             |   |  0x8020 | ------------+                 |
 9      |             |   +---------+             |                 |
10      |             |                           |                 |
11      +-----------&amp;gt; |                       Mem::translate        |
12                    |                           |                 |
13                    |   +------host------+      |                 |
14                    |   | 0x7ffff7f87020 | &amp;lt;----+                 |
15                    |   +----------------+                        |
16                    |                                             |
17                    +---------------------------------------------+
18                                           |
19+--literal-ptr--+                          |
20|     0x8024    | &amp;lt;------------------------+
21+---------------+&lt;/code&gt;
    &lt;p&gt;Or in code:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
 3        // ...
 4        match instruction {
 5            decoder::Instruction::LdrLiteral { rd, addr } =&amp;gt; {
 6                self.r[rd as usize] = self.mem.read_u32(addr).expect("Segfault");
 7            }
 8        }
 9        // ...
10    }
11}&lt;/code&gt;
    &lt;p&gt;Any other instruction using a addr will have to also go through the &lt;code&gt;Mem::translate&lt;/code&gt; indirection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forwarding Syscalls and other feature flag based logic&lt;/head&gt;
    &lt;p&gt;Since stinkarm has three ways of dealing with syscalls (&lt;code&gt;deny&lt;/code&gt;, &lt;code&gt;sandbox&lt;/code&gt;,
&lt;code&gt;forward&lt;/code&gt;). I decided on handling the selection of the appropriate logic at cpu
creation time via a function pointer attached to the CPU as the
&lt;code&gt;syscall_handler&lt;/code&gt; field:&lt;/p&gt;
    &lt;code&gt; 1type SyscallHandlerFn = fn(&amp;amp;mut Cpu, ArmSyscall) -&amp;gt; i32;
 2
 3pub struct Cpu&amp;lt;'cpu&amp;gt; {
 4    /// r0-r15 (r13=SP, r14=LR, r15=PC)
 5    pub r: [u32; 16],
 6    pub cpsr: u32,
 7    pub mem: &amp;amp;'cpu mut mem::Mem,
 8    syscall_handler: SyscallHandlerFn,
 9    pub status: Option&amp;lt;i32&amp;gt;,
10}
11
12impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
13    pub fn new(conf: &amp;amp;'cpu config::Config, mem: &amp;amp;'cpu mut mem::Mem, pc: u32) -&amp;gt; Self {
14        // ... 
15
16        // simplified, in stinkarm this gets wrapped if the user specifies
17        // syscall traces via -lsyscalls or -v
18        s.syscall_handler = match conf.syscalls {
19            SyscallMode::Forward =&amp;gt; translation::syscall_forward,
20            SyscallMode::Sandbox =&amp;gt; sandbox::syscall_sandbox,
21            SyscallMode::Deny =&amp;gt; sandbox::syscall_stub,
22        };
23        // ...
24    }
25}&lt;/code&gt;
    &lt;head rend="h3"&gt;Calling conventions, armv7 vs x86&lt;/head&gt;
    &lt;p&gt;In our examples I obviously used the armv7 syscall calling convention. But this convention differs from the calling convention of our x86 (technically its x86-64 System V AMD64 ABI) host by a lot.&lt;/p&gt;
    &lt;p&gt;While armv7 uses &lt;code&gt;r7&lt;/code&gt; for the syscall number and &lt;code&gt;r0-r5&lt;/code&gt; for the syscall
arguments, x86 uses &lt;code&gt;rax&lt;/code&gt; for the syscall id and &lt;code&gt;rdi&lt;/code&gt;, &lt;code&gt;rsi&lt;/code&gt;, &lt;code&gt;rdx&lt;/code&gt;, &lt;code&gt;r10&lt;/code&gt;,
&lt;code&gt;r8&lt;/code&gt; and &lt;code&gt;r9&lt;/code&gt; for the syscall arguments (&lt;code&gt;rcx&lt;/code&gt; can’t be used since &lt;code&gt;syscall&lt;/code&gt;
clobbers it, thus Linux goes with &lt;code&gt;r10&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Also the syscall numbers differ between armv7 and x86, &lt;code&gt;sys_write&lt;/code&gt; is &lt;code&gt;1&lt;/code&gt; on
x86 and &lt;code&gt;4&lt;/code&gt; on armv7. If you are interested in either calling conventions,
syscall ids and documentation, do visit The Chromium Projects- Linux System
Call
Table,
it is generated from Linux headers and fairly readable.&lt;/p&gt;
    &lt;p&gt;Table version:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;usage&lt;/cell&gt;
        &lt;cell role="head"&gt;armv7&lt;/cell&gt;
        &lt;cell role="head"&gt;x86-64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;syscall id&lt;/cell&gt;
        &lt;cell&gt;r7&lt;/cell&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;return&lt;/cell&gt;
        &lt;cell&gt;r0&lt;/cell&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg0&lt;/cell&gt;
        &lt;cell&gt;r0&lt;/cell&gt;
        &lt;cell&gt;rdi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg1&lt;/cell&gt;
        &lt;cell&gt;r1&lt;/cell&gt;
        &lt;cell&gt;rsi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg2&lt;/cell&gt;
        &lt;cell&gt;r2&lt;/cell&gt;
        &lt;cell&gt;rdx&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg3&lt;/cell&gt;
        &lt;cell&gt;r3&lt;/cell&gt;
        &lt;cell&gt;r10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg4&lt;/cell&gt;
        &lt;cell&gt;r4&lt;/cell&gt;
        &lt;cell&gt;r8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;arg5&lt;/cell&gt;
        &lt;cell&gt;r5&lt;/cell&gt;
        &lt;cell&gt;r9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So something like writing &lt;code&gt;TEXT123&lt;/code&gt; to stdout looks like this on arm:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2txt:
 3    .asciz "TEXT123\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    ldr r0, =1
 9    ldr r1, =txt
10    mov r2, #8
11    mov r7, #4
12    svc #0&lt;/code&gt;
    &lt;p&gt;While it looks like the following on x86:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2txt:
 3    .string "TEXT123\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    movq $1, %rax
 9    movq $1, %rdi
10    leaq txt(%rip), %rsi
11    movq $8, %rdx
12    syscall&lt;/code&gt;
    &lt;head rend="h3"&gt;Hooking the syscall handler up&lt;/head&gt;
    &lt;p&gt;After made the calling convention differences clear, the handling of a syscall is simply to execute this handler and use &lt;code&gt;r7&lt;/code&gt; to convert the armv7 syscall
number to the x86 syscall number:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
 3        // ...
 4
 5        match instruction {
 6            // ...
 7            decoder::Instruction::Svc =&amp;gt; {
 8                self.r[0] = (self.syscall_handler)(self, ArmSyscall::try_from(self.r[7])?) as u32;
 9            }
10            // ...
11        }
12        // ...
13    }
14}&lt;/code&gt;
    &lt;p&gt;Of course for this to work the syscall has to be implemented and even decodable. At least for the decoding, there is the &lt;code&gt;ArmSyscall&lt;/code&gt; enum:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug)]
 2#[allow(non_camel_case_types)]
 3pub enum ArmSyscall {
 4    restart = 0x00,
 5    exit = 0x01,
 6    fork = 0x02,
 7    read = 0x03,
 8    write = 0x04,
 9    open = 0x05,
10    close = 0x06,
11}
12
13impl TryFrom&amp;lt;u32&amp;gt; for ArmSyscall {
14    type Error = err::Err;
15
16    fn try_from(value: u32) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
17        Ok(match value {
18            0x00 =&amp;gt; Self::restart,
19            0x01 =&amp;gt; Self::exit,
20            0x02 =&amp;gt; Self::fork,
21            0x03 =&amp;gt; Self::read,
22            0x04 =&amp;gt; Self::write,
23            0x05 =&amp;gt; Self::open,
24            0x06 =&amp;gt; Self::close,
25            _ =&amp;gt; return Err(err::Err::UnknownSyscall(value)),
26        })
27    }
28}&lt;/code&gt;
    &lt;p&gt;By default the sandboxing mode is selected, but I will go into detail on both sandboxing and denying syscalls later, first I want to focus on the implementation of the translation layer from armv7 to x86 syscalls:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        // none are implemented, dump debug print
4        c =&amp;gt; todo!("{:?}", c),
5    }
6}&lt;/code&gt;
    &lt;head rend="h3"&gt;Handling the only exception: &lt;code&gt;exit&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Since exit means the guest wants to exit, we can’t just forward this to the host system, simply because this would exit the emulator before it would be able to do cleanup and unmap memory regions allocated.&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        ArmSyscall::exit =&amp;gt; {
4            cpu.status = Some(cpu.r[0] as i32);
5            0
6        }
7        // ...
8    }
9}&lt;/code&gt;
    &lt;p&gt;To both know we hit the exit syscall (we need to, otherwise the emulator executes further) and propagate the exit code to the host system, we set the &lt;code&gt;Cpu::status&lt;/code&gt; field to &lt;code&gt;Some(r0)&lt;/code&gt;, which is the argument to the syscall.&lt;/p&gt;
    &lt;p&gt;This field is then used in the emulator entry point / main loop:&lt;/p&gt;
    &lt;code&gt; 1fn main() {
 2    let mut cpu = cpu::Cpu::new(&amp;amp;conf, &amp;amp;mut mem, elf.header.entry);
 3
 4    loop {
 5        match cpu.step() { /**/ }
 6
 7        // Cpu::status is only some if sys_exit was called, we exit the
 8        // emulation loop
 9        if cpu.status.is_some() {
10            break;
11        }
12    }
13
14    let status = cpu.status.unwrap_or(0);
15    // cleaning up used memory via munmap
16    mem.destroy();
17    // propagating the status code to the host system
18    exit(status);
19}&lt;/code&gt;
    &lt;head rend="h3"&gt;Implementing: &lt;code&gt;sys_write&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The write syscall is not as spectacular as &lt;code&gt;sys_exit&lt;/code&gt;: writing a &lt;code&gt;buf&lt;/code&gt; of &lt;code&gt;len&lt;/code&gt;
to a file descriptor.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;register&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
        &lt;cell&gt;syscall number (1 for write)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rdi&lt;/cell&gt;
        &lt;cell&gt;file descriptor (0 for stdin, 1 for stdout, 2 for stderr)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rsi&lt;/cell&gt;
        &lt;cell&gt;a pointer to the buffer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;rdx&lt;/cell&gt;
        &lt;cell&gt;the length of the buffer &lt;code&gt;rsi&lt;/code&gt; is pointing to&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It is necessary for doing the O of I/O tho, otherwise there won’t be any &lt;code&gt;Hello, World!&lt;/code&gt;s on the screen.&lt;/p&gt;
    &lt;code&gt; 1use crate::{cpu, sys};
 2
 3pub fn write(cpu: &amp;amp;mut cpu::Cpu, fd: u32, buf: u32, len: u32) -&amp;gt; i32 {
 4    // fast path for zero length buffer
 5    if len == 0 {
 6        return 0;
 7    }
 8
 9    // Option::None returned from translate indicates invalid memory access
10    let Some(buf_ptr) = cpu.mem.translate(buf) else {
11        // so we return 'Bad Address'
12        return -(sys::Errno::EFAULT as i32);
13    };
14
15    let ret: i64;
16    unsafe {
17        core::arch::asm!(
18            "syscall",
19            // syscall number
20            in("rax") 1_u64,
21            in("rdi") fd as u64,
22            in("rsi") buf_ptr as u64,
23            in("rdx") len as u64,
24            lateout("rax") ret,
25            // we clobber rcx
26            out("rcx") _,
27            // and r11
28            out("r11") _,
29            // we don't modify the stack
30            options(nostack),
31        );
32    }
33
34    ret.try_into().unwrap_or(i32::MAX)
35}&lt;/code&gt;
    &lt;p&gt;Adding it to &lt;code&gt;translation::syscall_forward&lt;/code&gt; with it’s arguments according to the
calling convention we established before:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        // ...
4        ArmSyscall::write =&amp;gt; sys::write(cpu, cpu.r[0], cpu.r[1], cpu.r[2]),
5        // ...
6    }
7}&lt;/code&gt;
    &lt;p&gt;Executing &lt;code&gt;helloWorld.elf&lt;/code&gt; now results in:&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Cforward example/helloWorld.elf
2Hello, world!
3$ echo $status
40&lt;/code&gt;
    &lt;head rend="h3"&gt;Deny and Sandbox - restricting syscalls&lt;/head&gt;
    &lt;p&gt;The simplest sandboxing mode is to deny, the more complex is to allow some syscall interactions while others are denied. The latter requires checking arguments to syscalls, not just the syscall kind.&lt;/p&gt;
    &lt;p&gt;Lets start with the easier syscall handler: &lt;code&gt;deny&lt;/code&gt;. Deny simply returns
&lt;code&gt;ENOSYS&lt;/code&gt; to all invoked syscalls:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_deny(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    if let ArmSyscall::exit = syscall {
3        cpu.status = Some(cpu.r[0] as i32)
4    };
5
6    -(sys::Errno::ENOSYS as i32)
7}&lt;/code&gt;
    &lt;p&gt;Thus executing the hello world and enabling syscall logs results in neither &lt;code&gt;sys_write&lt;/code&gt; nor &lt;code&gt;sys_exit&lt;/code&gt; going through and &lt;code&gt;ENOSYS&lt;/code&gt; being returned for both
in &lt;code&gt;r0&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Cdeny -lsyscalls examples/helloWorld.elf
2148738 write(fd=1, buf=0x8024, len=14) [deny]
3=ENOSYS
4148738 exit(code=0) [deny]
5=ENOSYS&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;sandbox&lt;/code&gt; at a high level is the same as deny, check for conditions before
executing a syscall, if they don’t match, disallow the syscall:&lt;/p&gt;
    &lt;code&gt; 1pub fn syscall_sandbox(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
 2    match syscall {
 3        ArmSyscall::exit =&amp;gt; {
 4            cpu.status = Some(cpu.r[0] as i32);
 5            0
 6        }
 7        ArmSyscall::write =&amp;gt; {
 8            let (r0, r1, r2) = (cpu.r[0], cpu.r[1], cpu.r[2]);
 9            // only allow writing to stdout, stderr and stdin
10            if r0 &amp;gt; 2 {
11                return -(sys::Errno::ENOSYS as i32);
12            }
13
14            sys::write(cpu, r0, r1, r2)
15        }
16        _ =&amp;gt; todo!("{:?}", syscall),
17    }
18}&lt;/code&gt;
    &lt;p&gt;For instance we only allow writing to stdin, stdout and stderr, no other file descriptors. One could also add pointer range checks, buffer length checks and other hardening measures here. Emulating the hello world example with this mode (which is the default mode):&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Csandbox -lsyscalls examples/helloWorld.elf
2150147 write(fd=1, buf=0x8024, len=14) [sandbox]
3Hello, world!
4=14
5150147 exit(code=0) [sandbox]
6=0&lt;/code&gt;
    &lt;head rend="h1"&gt;Fin&lt;/head&gt;
    &lt;p&gt;So there you have it, emulating armv7 in six steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;parsing and validating a 32-bit armv7 Elf binary&lt;/item&gt;
      &lt;item&gt;mapping segments into host address space&lt;/item&gt;
      &lt;item&gt;decoding a non-trivial subset of armv7 instructions&lt;/item&gt;
      &lt;item&gt;handling program counter relative literal loads&lt;/item&gt;
      &lt;item&gt;translating memory interactions from guest to host&lt;/item&gt;
      &lt;item&gt;forwarding armv7 Linux syscalls into their x86-64 System V counterparts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diving into the Elf and armv7 spec without any previous relevant experience, except the asm module I had in uni, was a bit overwhelming at first. Armv7 decoding was by far the most annoying part of the project and I still don’t like the bizarre argument ordering for x86-64 syscalls.&lt;/p&gt;
    &lt;p&gt;The whole project is about 1284 lines of Rust, has zero dependencies1 and is as far as I know working correctly2.&lt;/p&gt;
    &lt;head rend="h1"&gt;Microbenchmark Performance&lt;/head&gt;
    &lt;p&gt;It executes a real armv7 hello world binary in ~0.015ms of guest execution-only time, excluding process startup and parsing. The e2e execution with all stages I outlined before, it takes about 2ms.&lt;/p&gt;
    &lt;code&gt; 1$ stinkarm -v examples/helloWorld.elf
 2[     0.070ms] opening binary "examples/helloWorld.elf"
 3[     0.097ms] parsing ELF...
 4[     0.101ms] \
 5ELF Header:
 6  Magic:              [7f, 45, 4c, 46]
 7  Class:              ELF32
 8  Data:               Little endian
 9  Type:               Executable
10  Machine:            EM_ARM
11  Version:            1
12  Entry point:        0x8000
13  Program hdr offset: 52 (32 bytes each)
14  Section hdr offset: 4696
15  Flags:              0x05000200
16  EH size:            52
17  # Program headers:  1
18  # Section headers:  9
19  Str tbl index:      8
20
21Program Headers:
22  Type       Offset   VirtAddr   PhysAddr   FileSz    MemSz  Flags  Align
23  LOAD     0x001000 0x00008000 0x00008000 0x000033 0x000033    R|X 0x1000
24
25[     0.126ms] mapped program header `LOAD` of 51B (G=0x8000 -&amp;gt; H=0x7ffff7f87000)
26[     0.129ms] jumping to entry G=0x8000 at H=0x7ffff7f87000
27[     0.131ms] starting the emulator
28153719 write(fd=1, buf=0x8024, len=14) [sandbox]
29Hello, world!
30=14
31153719 exit(code=0) [sandbox]
32=0
33[     0.149ms] exiting with `0`&lt;/code&gt;
    &lt;p&gt;Comparing the whole pipeline (parsing elf, segment mapping, cpu setup, etc) to &lt;code&gt;qemu&lt;/code&gt; we arrive at the following micro benchmark results. To be fair, qemu
does a whole lot more than stinkarm, it has a jit, a full linux-user runtime, a
dynamic loader, etc.&lt;/p&gt;
    &lt;code&gt;1$ hyperfine "./target/release/stinkarm examples/helloWorld.elf" -N --warmup 10
2Benchmark 1: ./target/release/stinkarm examples/helloWorld.elf
3  Time (mean ± σ):       1.9 ms ±   0.3 ms    [User: 0.2 ms, System: 1.4 ms]
4  Range (min … max):     1.6 ms …   3.4 ms    1641 runs
5
6$ hyperfine "qemu-arm ./examples/helloWorld.elf" -N --warmup 10
7Benchmark 1: qemu-arm ./examples/helloWorld.elf
8  Time (mean ± σ):      12.3 ms ±   1.5 ms    [User: 3.8 ms, System: 8.0 ms]
9  Range (min … max):     8.8 ms …  19.8 ms    226 runs&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xnacly.me/posts/2025/building-a-minimal-viable-armv7-emulator/"/><published>2025-11-21T13:30:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005111</id><title>We should all be using dependency cooldowns</title><updated>2025-11-21T18:15:23.753354+00:00</updated><content>&lt;doc fingerprint="af811b9b5acb0ef0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 21, 2025 Tags: oss, security&lt;/p&gt;
    &lt;p&gt;TL;DR: Dependency cooldowns are a free, easy, and incredibly effective way to mitigate the large majority of open source supply chain attacks. More individual projects should apply cooldowns (via tools like Dependabot and Renovate) to their dependencies, and packaging ecosystems should invest in first-class support for cooldowns directly in their package managers.&lt;/p&gt;
    &lt;p&gt;âSupply chain securityâ is a serious problem. Itâs also seriously overhyped, in part because dozens of vendors have a vested financial interest in convincing your that their framing of the underlying problem1 is (1) correct, and (2) worth your money.&lt;/p&gt;
    &lt;p&gt;Whatâs consernating about this is that most open source supply chain attacks have the same basic structure:&lt;/p&gt;
    &lt;p&gt;An attacker compromises a popular open source project, typically via a stolen credential or CI/CD vulnerabilty (such as âpwn requestsâ in GitHub Actions).&lt;/p&gt;
    &lt;p&gt;The attacker introduces a malicious change to the project and uploads it somewhere that will have maximum effect (PyPI, npm, GitHub releases, &amp;amp;c., depending on the target).&lt;/p&gt;
    &lt;p&gt;At this point, the clock has started, as the attacker has moved into the public.&lt;/p&gt;
    &lt;p&gt;Users pick up the compromised version of the project via automatic dependency updates or a lack of dependency pinning.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the aforementioned vendors are scanning public indices as well as customer repositories for signs of compromise, and provide alerts upstream (e.g. to PyPI).&lt;/p&gt;
    &lt;p&gt;Notably, vendors are incentivized to report quickly and loudly upstream, as this increases the perceived value of their services in a crowded field.&lt;/p&gt;
    &lt;p&gt;Upstreams (PyPI, npm, &amp;amp;c.) remove or disable the compromised package version(s).&lt;/p&gt;
    &lt;p&gt;End-user remediation begins.&lt;/p&gt;
    &lt;p&gt;The key thing to observe is that the gap between (1) and (2) can be very large2 (weeks or months), while the gap between (2) and (5) is typically very small: hours or days. This means that, once the attacker has moved into the actual exploitation phase, their window of opportunity to cause damage is pretty limited.&lt;/p&gt;
    &lt;p&gt;We can see this with numerous prominent supply chain attacks over the last 18 months3:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attack&lt;/cell&gt;
        &lt;cell role="head"&gt;Approx. Window of Opportunity&lt;/cell&gt;
        &lt;cell role="head"&gt;References&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xz-utils&lt;/cell&gt;
        &lt;cell&gt;â 5 weeks4&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 1)&lt;/cell&gt;
        &lt;cell&gt;12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 2)&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tj-actions&lt;/cell&gt;
        &lt;cell&gt;3 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;chalk&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nx&lt;/cell&gt;
        &lt;cell&gt;4 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rspack&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num2words&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kong Ingress Controller&lt;/cell&gt;
        &lt;cell&gt;â 10 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;web3.js&lt;/cell&gt;
        &lt;cell&gt;5 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(Each of these attacks has significant downstream effect, of course, but only within their window of opportunity. Subsequent compromises from each, like Shai-Hulud, represent new windows of opportunity where the attackers regrouped and pivoted onto the next set of compromised credentials.)&lt;/p&gt;
    &lt;p&gt;My takeaway from this: some windows of opportunity are bigger, but the majority of them are under a week long. Consequently, ordinary developers can avoid the bulk of these types of attacks by instituting cooldowns on their dependencies.&lt;/p&gt;
    &lt;p&gt;A âcooldownâ is exactly what it sounds like: a window of time between when a dependency is published and when itâs considered suitable for use. The dependency is public during this window, meaning that âsupply chain securityâ vendors can work their magic while the rest of us wait any problems out.&lt;/p&gt;
    &lt;p&gt;I love cooldowns for several reasons:&lt;/p&gt;
    &lt;p&gt;Theyâre empirically effective, per above. They wonât stop all attackers, but they do stymie the majority of high-visibiity, mass-impact supply chain attacks that have become more common.&lt;/p&gt;
    &lt;p&gt;Theyâre incredibly easy to implement. Moreover, theyâre literally free to implement in most cases: most people can use Dependabotâs functionality, Renovateâs functionality, or the functionality build directly into their package manager5.&lt;/p&gt;
    &lt;p&gt;This is how simple it is in Dependabot:&lt;/p&gt;
    &lt;p&gt;(Rinse and repeat for other ecosystems as needed.)&lt;/p&gt;
    &lt;p&gt;Cooldowns enforce positive behavior from supply chain security vendors: vendors are still incentivized to discover and report attacks quickly, but are not as incentivized to emit volumes of blogspam about âcriticalâ attacks on largely underfunded open source ecosystems.&lt;/p&gt;
    &lt;p&gt;In the very small sample set above, 8/10 attacks had windows of opportunity of less than a week. Setting a cooldown of 7 days would have prevented the vast majority of these attacks from reaching end users (and causing knock-on attacks, which several of these were). Increasing the cooldown to 14 days would have prevented all but 1 of these attacks6.&lt;/p&gt;
    &lt;p&gt;Cooldowns are, obviously, not a panacea: some attackers will evade detection, and delaying the inclusion of potentially malicious dependencies by a week (or two) does not fundamentally alter the fact that supply chain security is a social trust problem, not a purely technical one. Still, an 80-90% reduction in exposure through a technique that is free and easy seems hard to beat.&lt;/p&gt;
    &lt;p&gt;Related to the above, itâs unfortunate that cooldowns arenât baked directly into more packaging ecosystems: Dependabot and Renovate are great, but even better would be if the package manager itself (as the source of ground truth) could enforce cooldowns directly (including of dependencies not introduced or bumped through automated flows).&lt;/p&gt;
    &lt;p&gt;The problem being, succinctly: modern software stacks are complex and opaque, with little to no difference in privilege between first-party code and third-party dependencies.Â ↩&lt;/p&gt;
    &lt;p&gt;In part because of the prevalence of long-lived, overscoped credentials. Long-lived credentials let attackers operate on their own (comfortable) timelines; this is why Trusted Publishing is such a useful (but not wholly sufficient) technique for reducing the attackerâs attack staging window.Â ↩&lt;/p&gt;
    &lt;p&gt;Filippo Valsorda has an excellent compilation of recent supply chain compromises here.Â ↩&lt;/p&gt;
    &lt;p&gt;The xz-utils attack is a significant outlier, both in its scope and the length of its window of opportunity. In this case, Iâve measured from the attackerâs first backdoored release (v5.6.0, 2024-02-24) to the time of rollback within Debian (2024-03-28).Â ↩&lt;/p&gt;
    &lt;p&gt;For example, pnpmâs &lt;code&gt;minimumReleaseAge&lt;/code&gt;.
           uv also has &lt;code&gt;exclude-newer&lt;/code&gt;, 
           although this specifies an absolute cutoff rather than a rolling cooldown.Â ↩&lt;/p&gt;
    &lt;p&gt;Notably, the only attack that would have stymied a 14-day cooldown is xz-utils, which is also the most technically, logistically, and socially advanced of all of the attacks.Â ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.yossarian.net/2025/11/21/We-should-all-be-using-dependency-cooldowns"/><published>2025-11-21T14:50:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005349</id><title>XBMC 4.0 for the Original Xbox</title><updated>2025-11-21T18:15:22.729963+00:00</updated><content>&lt;doc fingerprint="3f9edd7dc8a4cddc"&gt;
  &lt;main&gt;
    &lt;p&gt;A Major Modernization of the Killer App That Started It All&lt;/p&gt;
    &lt;p&gt;A new version of Xbox Media Center (XBMC), version 4.0, has been released. This version marks a significant update to the long-standing media center platform for the Original Xbox. This marks the first major advancement to the software since 2016 and represents a renewed commitment to preserving, modernizing, and extending the capabilities of one of the most iconic console homebrew applications ever created.&lt;/p&gt;
    &lt;p&gt;XBMC has a long and influential history. In 2002, XboxMediaPlayer (XMP) was released and turned the console into a powerful multimedia device fit for the living room in an era when connecting a computer to a TV was quite novel. Later that same year, XMP merged with YAMP and became Xbox Media Player 2.0. A few years later, the software evolved into Xbox Media Center, or XBMC, which introduced a new interface, a plugin system powered by Python, and a robust skinning engine.&lt;/p&gt;
    &lt;p&gt;XBMC eventually became so capable that it outgrew the Xbox entirely. By 2007, developers were working on PC ports and in 2010, the project split into two branches: one for general computers while the Xbox version became XBMC4Xbox, and each codebase was maintained from then on by separate teams. XBMC was later renamed to Kodi in 2014 and continues to be one of the most popular media center applications available. Even Plex traces its roots back to XBMC. Plex began as OSXBMC, a Mac port of XBMC in late 2007, before becoming its own project in 2008. This means the Original Xbox helped shape not one but two of the biggest media center apps used today.&lt;/p&gt;
    &lt;p&gt;The last official release of XBMC4Xbox arrived in February 2016 with version 3.5.3. Although the community never declared the project dead, meaningful updates became scarce. XBMC 4.0 continues that legacy by bringing a modern interface, updating it to be more inline with Kodi's modern codebase, and backporting features to the original 64MB RAM / Pentium-III hardware where it all began.&lt;/p&gt;
    &lt;p&gt;This project is distinct and separate from XBMC4Gamers, the games-focused variation of XBMC4Xbox (v3.5.3) by developer Rocky5.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Modern Interface Powered by Estuary&lt;/head&gt;
    &lt;p&gt;One of the most notable advancements in XBMC 4.0 is the introduction of the Estuary user interface (skin).&lt;/p&gt;
    &lt;p&gt;Estuary, originally released in 2017 with Kodi v17 ("Krypton"), provides a clean and modern layout that improves navigation and readability over past skins. Bringing Estuary to the Xbox required extensive updates to the underlying GUI framework, including a port of the more contemporary GUIlib engine. This allows the platform to support modern skinning standards and makes future skin ports much more straightforward. After the initial work of porting GUIlib was done, porting Estuary to the Xbox was a relatively simple process of tweaking a handful of configuration files and adding contextual features specific to the Xbox. The result is a modern, intuitive front end that retains the performance and responsiveness required on legacy hardware.&lt;/p&gt;
    &lt;p&gt;Firing up an Xbox made in 2001 and being greeted by the same interface as what you'd find if you were to download Kodi today onto your PC feels like a bit of magic, and helps keep this beloved classic console relevant and useful well into the modern era.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expanded Games Library Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 introduces a fully realized games library system. This enhancement brings the same level of metadata support found in the Movies and Music sections to Xbox and emulated games. Titles can now display artwork, descriptions, and other metadata, transforming the games section into a polished and user-friendly library. XBMC’s longstanding support for trainers remains intact, giving users the option to apply gameplay modifications for compatible titles. Emulated game collections benefit as well, with the ability to browse ROM libraries and launch them directly in a user’s preferred emulator.&lt;/p&gt;
    &lt;head rend="h2"&gt;Online Scrapers and Metadata Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 restores full functionality to metadata scrapers for movies and television. This allows users to build rich media libraries complete with artwork, plot summaries, cast listings, and other information retrieved directly from online sources. XBMC 4.0 handles these tasks efficiently, even on the Xbox’s limited memory and processing power. Video playback continues to support 480p and 720p content, enabling the console to serve as a surprisingly capable media device for its age. Similar to Kodi, XBMC 4.0 supports filtering, building playlists, watch progress history for media, and intelligent handling of TV shows with seasons.&lt;/p&gt;
    &lt;p&gt;Aside from scrapers for multimedia, support for rich library capabilities for games has also been added. XBMC has always been a media-first app, and now users can enjoy the library experience that they've come to love for media now in the context of their games library (more info below).&lt;/p&gt;
    &lt;head rend="h2"&gt;Improved Task Scheduling and Multitasking&lt;/head&gt;
    &lt;p&gt;Despite the constraints of the Xbox’s single-threaded 733MHz CPU, XBMC 4.0 includes improvements to task scheduling that allow multiple activities to run concurrently. Background library updates, metadata scraping, and audio/video playback can occur while users navigate and use other parts of the interface. These optimizations help ensure a fluid experience without compromising performance. Much work has been done "under the hood" to keep XBMC on task and within memory budgets while achieving multi-tasking on a console that wasn't exactly designed with it in mind. Users who own RAM and/or CPU upgraded consoles can also take advantage of the extra overhead, as XBMC 4.0 makes use of the extra horsepower for an even smoother experience. Utilizing an SSD with higher UDMA speeds will also yield an improvement in overall responsiveness.&lt;/p&gt;
    &lt;head rend="h2"&gt;Music Experience and Visualizers&lt;/head&gt;
    &lt;p&gt;Music playback has always been a strong element of XBMC, and version 4.0 maintains that focus. The Original Xbox is capable of high quality audio output, and XBMC continues to support lossless codecs such as FLAC. The release includes compatibility with various audio visualizers, including MilkDrop, which remains one of the most visually impressive and customizable audio visualization engines available. These features allow XBMC 4.0 to function not only as a media organizer, but also as an immersive audio display system.&lt;/p&gt;
    &lt;p&gt;An online repository has been established and will be maintained moving forward where users can download legacy and newly-released add-ons as they become available. This repository is accessible without additional setup, right out of the box!&lt;/p&gt;
    &lt;head rend="h2"&gt;Add-ons and Python Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 continues to offer an extendable architecture powered by Python-based add-ons. While the current release uses Python 2.7 for compatibility, work is underway to transition to Python 3.4.10 in the future, which may provide a path for backporting many newer Kodi add-ons. Even in its current state, XBMC 4.0 already supports a variety of community-developed add-ons that extend the system’s functionality, including tools for online video playback (i.e. YouTube), online weather services, and enhanced media organization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Updated Settings, Network Services, and System Tools&lt;/head&gt;
    &lt;p&gt;The settings interface has been revised to provide more clarity and control. The update includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Playback options, including episode progression, crossfade behavior, and subtitle handling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Library management tools&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network features, such as SMB, FTP, UPnP sharing, web server access, and Insignia-compatible DNS options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Comprehensive interface customization options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple user profiles with individual library settings&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Advanced system controls for video calibration, display modes, input devices, and power management&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A robust System Information section for diagnostics, with info geared towards the Original Xbox&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A flexible File Manager with support for network protocols including FTP, SMB, WebDAV, and more&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Users may also take advantage of an online add-ons repository, offering the same experience modern Kodi provides with being able to download add-ons to extend functionality of the app with things like online multimedia providers, weather, skins, visualizers, and more. Developers can submit new add-ons to the official repository via Github.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continuing the Legacy&lt;/head&gt;
    &lt;p&gt;XBMC has been a staple of the Original Xbox's homebrew scene since its inception in the early 2000's. This new update is a revival of the platform that helped shape the landscape of home media software and helps revitalize a codebase that has been somewhat stagnant for many years. This release honors that heritage while modernizing the experience for a new generation of enthusiasts and preserving the functionality of the Original Xbox as a versatile and capable media center.&lt;/p&gt;
    &lt;p&gt;Although the hardware is decades old, the renewed effort behind XBMC 4.0 demonstrates that the platform still has room to grow and tricks up its sleeve. With ongoing development and a codebase designed with modern Kodi compatibility in mind, XBMC 4.0 represents a significant step forward into the continued development on the Original Xbox.&lt;/p&gt;
    &lt;p&gt;The development team looks forward to continuing this work and expanding the possibilities of the Original Xbox for years to come. This version is the first of many to come, with lots of things cooking in the background. Keep an eye out for future releases by joining the Xbox-Scene Discord and turning on notifications in the xbmc-news channel or by periodically checking the project's Github page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Downloads&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 (and subsequent releases) builds along with source code are available via Github:&lt;/p&gt;
    &lt;p&gt;Main project page: Click Here&lt;/p&gt;
    &lt;p&gt;Note: XBMC 4.0 is is in active development! This means updates will be released in a more frequent manner for the time being until things settle down. Check the nightly builds section on Github for the most up-to-date version.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;XBMC is open source software and welcomes contributions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coding: Developers can help XBMC by fixing a bug, adding new features, making our technology smaller and faster and making development easier for others. XBMC's codebase consists mainly of C++ with small parts written in a variety of coding languages. Our add-ons mainly consist of python and XML.&lt;/item&gt;
      &lt;item&gt;Helping users: Our support process relies on enthusiastic contributors like you to help others get the most out of XBMC. The #1 priority is always answering questions in our support forums. Everyday new people discover XBMC, and everyday they are virtually guaranteed to have questions.&lt;/item&gt;
      &lt;item&gt;Localization: Translate XBMC, add-ons, skins etc. into your native language.&lt;/item&gt;
      &lt;item&gt;Add-ons: Add-ons are what make XBMC the most extensible and customizable entertainment hub available. Get started building an add-on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Support and Bug Reporting&lt;/head&gt;
    &lt;p&gt;Need help?&lt;/p&gt;
    &lt;p&gt;Support can be found in the XBMC -&amp;gt; General channel within the Xbox-Scene Discord server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits and Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nikola Antonić - Primary Developer, Project Lead&lt;/item&gt;
      &lt;item&gt;astarivi - Contributor (cURL, wolfSSL), Tester, Debugger&lt;/item&gt;
      &lt;item&gt;EqUiNoX - Contrubitor, Tester&lt;/item&gt;
      &lt;item&gt;Rocky5 - Contributor, Tester&lt;/item&gt;
      &lt;item&gt;.lavenderStarlight+ - Add-ons / Skins Development, Tester&lt;/item&gt;
      &lt;item&gt;GoTeamScotch - Tester, Feedback&lt;/item&gt;
      &lt;item&gt;Haguero - Tester, Feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;XBMC is GPLv2 licensed. You may use, distribute and copy it under the license terms. XBMC is licensed under the same terms as Kodi. For detailed information on the licensing, please refer to the Kodi license.&lt;/p&gt;
    &lt;p&gt;This project, XBMC version 4.0 (and upcoming releases), is distinct from and is not affiliated with Team Kodi of The Kodi Foundation, or its members.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.xbox-scene.info/articles/announcing-xbmc-40-for-the-original-xbox-r64/"/><published>2025-11-21T15:18:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005388</id><title>Make product worse, get money</title><updated>2025-11-21T18:15:22.318500+00:00</updated><content>&lt;doc fingerprint="652b19d9bbe30db8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Make product worse, get money&lt;/head&gt;
    &lt;p&gt;I recently asked why people seem to hate dating apps so much. In response, 80% of you emailed me some version of the following theory:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The thing about dating apps is that if they do a good job and match people up, then the matched people will quit the app and stop paying. So they have an incentive to string people along but not to actually help people find long-term relationships.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;May I explain why I don’t find this type of theory very helpful?&lt;/p&gt;
    &lt;p&gt;I’m not saying that I think it’s wrong, mind you. Rather, my objection is that while the theory is phrased in terms of dating apps, the same basic pattern applies to basically anyone who is trying to make money by doing anything.&lt;/p&gt;
    &lt;p&gt;For example, consider a pizza restaurant. Try these theories on for size:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza: “The thing about pizza restaurants is that if they use expensive ingredients or labor-intensive pizza-making techniques, then it costs more to make pizza. So they have an incentive to use low-cost ingredients and labor-saving shortcuts.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza II: “The thing about pizza restaurants is that if they have nice tables separated at a comfortable distance, then they can’t fit as many customers. So they have an incentive to use tiny tables and cram people in cheek by jowl.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza III: “The thing about pizza restaurants is that if they sell big pizzas, then people will eat them and stop being hungry, meaning they don’t buy additional pizza. So they have an incentive to serve tiny low-calorie pizzas.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See what I mean? You can construct similar theories for other domains, too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Cars: “The thing about automakers is that making cars safe is expensive. So they have an incentive to make unsafe cars.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Videos: “The thing about video streaming is that high-resolution video uses more expensive bandwidth. So they have an incentive to use low-resolution.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Blogging: “The thing about bloggers is that research is time-consuming. So they have an incentive to be sloppy about the facts.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Durability: “The thing about {lightbulb, car, phone, refrigerator, cargo ship} manufacturing is that if you make a {lightbulb, car, phone, refrigerator, cargo ship} that lasts a long time, then people won’t buy new ones. So there’s an incentive to make {lightbulbs, cars, phones, refrigerators, cargo ships} that break quickly.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All these theories can be thought of as instances of two general patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Make product worse, get money: The thing about selling goods or services is that making goods or services better costs money. So people have an incentive to make goods and services worse.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Raise price, get money: “The thing about selling goods and services is that if you raise prices, then you get more money. So people have an incentive to raise prices.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Are these theories wrong? Not exactly. But it sure seems like something is missing.&lt;/p&gt;
    &lt;p&gt;I’m sure most pizza restauranteurs would be thrilled to sell lukewarm 5 cm cardboard discs for $300 each. They do in fact have an incentive to do that, just as predicted by these theories! Yet, in reality, pizza restaurants usually sell pizzas that are made out of food. So clearly these theories aren’t telling the whole story.&lt;/p&gt;
    &lt;p&gt;Say you have a lucrative business selling 5 cm cardboard discs for $300. I am likely to think, “I like money. Why don’t I sell pizzas that are only mostly cardboard, but also partly made of flour? And why don’t I sell them for $200, so I can steal Valued Reader’s customers?” But if I did that, then someone else would probably set prices at only $100, or even introduce cardboard-free pizzas, and this would continue until hitting some kind of equilibrium.&lt;/p&gt;
    &lt;p&gt;Sure, producers want to charge infinity dollars for things that cost them zero dollars to make. But consumers want to pay zero dollars for stuff that’s infinitely valuable. It’s in the conflict between these desires that all interesting theories live.&lt;/p&gt;
    &lt;p&gt;This is why I don’t think it’s helpful to point out that people have an incentive to make their products worse. Of course they do. The interesting question is, why are they able to get away with it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Reasons stuff is bad&lt;/head&gt;
    &lt;p&gt;First reason stuff is bad: People are cheap&lt;/p&gt;
    &lt;p&gt;Why are seats so cramped on planes? Is it because airlines are greedy? Sure. But while they might be greedy, I don’t think they’re dumb. If you do a little math, you can calculate that if airlines were to remove a single row of seats, they could add perhaps 2.5 cm (1 in) of extra legroom for everyone, while only decreasing the number of paying customers by around 3%. (This is based on a 737 with single-class, but you get the idea.)&lt;/p&gt;
    &lt;p&gt;So why don’t airlines rip out a row of seats, raise prices by 3% and enjoy the reduced costs for fuel and customer service? The only answer I can see is that people, on average, aren’t actually willing to pay 3% more for 2.5 cm more legroom. We want a worse but cheaper product, and so that’s what we get.&lt;/p&gt;
    &lt;p&gt;I think this is the most common reason stuff is “bad”. It’s why Subway sandwiches are so soggy, why video games are so buggy, why IKEA furniture and Primark clothes fall apart so quickly.&lt;/p&gt;
    &lt;p&gt;It’s good when things are bad for this reason. Or at least, that’s the premise of capitalism: “When companies cut costs, that’s the invisible hand redirecting resources to maximize social value”, or whatever. Companies may be motivated by greed. And you may not like it, since you want to pay zero dollars for infinite value. But this is markets working as designed.&lt;/p&gt;
    &lt;p&gt;Second reason stuff is bad: Information asymmetries&lt;/p&gt;
    &lt;p&gt;Why is it that almost every book / blog / podcast about longevity is such garbage? Well, we don’t actually know many things that will reliably increase longevity, and those things are mostly all boring / hard / non-fun. And even if you do all of them, it probably only adds a couple years in expectation. And telling people those facts is not a good way to find suckers who will pay you lots of money for your unproven supplements / seminars / etc.&lt;/p&gt;
    &lt;p&gt;True! But it doesn’t explain why all longevity stuff is so bad. Why don’t honest people tell the true story and drive all the hucksters out of business? I suspect the answer is that unless you have a lot of scientific training and do a lot of research, it’s basically impossible to figure out just how huckstery all the hucksters really are.&lt;/p&gt;
    &lt;p&gt;I think this same basic phenomenon explains why some supplements contain heavy metals, why some food contains microplastics, why restaurants use so much butter and salt, why rentals often have crappy insulation, and why most cars seem to only be safe along dimensions included in crash test scores. When consumers can’t tell good from evil, evil triumphs.&lt;/p&gt;
    &lt;p&gt;Third reason stuff is bad: People have bad taste&lt;/p&gt;
    &lt;p&gt;Sometimes stuff is bad because people just don’t appreciate the stuff you consider good. Examples are definitionally controversial, but I think this includes restaurants in cities where all restaurants are bad, North American tea, and travel pants. This reason has a blurry boundary with information asymmetries, as seen in ultrasonic humidifiers or products that use Sucralose instead of aspartame for “safety”.&lt;/p&gt;
    &lt;p&gt;Fourth reason stuff is bad: Pricing power&lt;/p&gt;
    &lt;p&gt;Finally, sometimes stuff is bad because markets aren’t working. Sometimes a company is selling a product but has some kind of “moat” that makes it hard for anyone else to compete with them, e.g. because of some technological or regulatory barrier, control of some key resource or location, some intellectual property, some beloved brand, or because of network effects.&lt;/p&gt;
    &lt;p&gt;If that’s true then those companies don’t have to worry much about someone else stealing their business, and so (because everyone is axiomatically greedy) they will find ways to make their product cheaper and/or raise their prices up until it’s equal to the full value it provides to the marginal consumer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Why is food so expensive at sporting events? Yes, people have no alternatives. But people know food is expensive at sporting events. And they don’t like it. Instead of selling water for $17, why don’t venues sell water for $2 and raise ticket prices instead? I don’t know. Probably something complicated, like that expensive food allows you to extract extra money from rich people without losing business from non-rich people.&lt;/p&gt;
    &lt;p&gt;So of course dating apps would love to string people along for years instead of finding them long-term relationships, so they keep paying money each month. I’d bet that some people at those companies have literally thought, “Maybe we should string people along for years instead of finding them long-term relationships, so they keep paying money each month. I love money so much.”&lt;/p&gt;
    &lt;p&gt;But if they are actually doing that (which is unclear to me) or if they are bad in some other way, then how do they get away with it? Why doesn’t someone else create a competing app that’s better and thereby steal all their business? It seems like the answer has to be either “because that’s impossible”, or “because people don’t really want that”. That’s where the mystery begins.&lt;/p&gt;
    &lt;p&gt;Dating: A mysterious constellation of facts · life economics&lt;/p&gt;
    &lt;p&gt;So much blood · conspiracy economics&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dynomight.net/worse/"/><published>2025-11-21T15:23:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005553</id><title>Arduino published updated terms and conditions: no longer an open commons</title><updated>2025-11-21T18:15:21.821779+00:00</updated><content>&lt;doc fingerprint="b470b35817f47bba"&gt;
  &lt;main&gt;
    &lt;p&gt;Six weeks ago, Qualcomm acquired Arduino. The maker community immediately worried that Qualcomm would kill the open-source ethos that made Arduino the lingua franca of hobby electronics.&lt;/p&gt;
    &lt;p&gt;This week, Arduino published updated terms and conditions and a new privacy policy, clearly rewritten by Qualcomm’s lawyers. The changes confirm the community’s worst fears: Arduino is no longer an open commons. It’s becoming just another corporate platform.&lt;/p&gt;
    &lt;p&gt;Here’s what’s at stake, what Qualcomm got wrong, and what might still be salvaged, drawing from community discussions across maker forums and sites.&lt;/p&gt;
    &lt;p&gt;What changed?&lt;lb/&gt;The new terms read like standard corporate boilerplate: mandatory arbitration, data integration with Qualcomm’s global ecosystem, export controls, AI use restrictions. For any other SaaS platform, this would be unremarkable.&lt;/p&gt;
    &lt;p&gt;But Arduino isn’t SaaS. It’s the foundation of the maker ecosystem.&lt;/p&gt;
    &lt;p&gt;The most dangerous change is Arduino now explicitly states that using their platform grants you no patent licenses whatsoever. You can’t even argue one is implied.&lt;/p&gt;
    &lt;p&gt;This means Qualcomm could potentially assert patents against your projects if you built them using Arduino tools, Arduino examples, or Arduino-compatible hardware.&lt;/p&gt;
    &lt;p&gt;And here’s the disconnect, baffling makers. Arduino’s IDE is licensed under AGPL. Their CLI is GPL v3. Both licenses explicitly require that you can reverse engineer the software. But the new Qualcomm terms explicitly forbid reverse engineering “the Platform.”&lt;/p&gt;
    &lt;p&gt;What’s really going on?&lt;lb/&gt;The community is trying to figure out what is Qualcomm’s actual intent. Are these terms just bad lawyering with SaaS lawyers applying their standard template to cloud services, not realizing Arduino is different? Or is Qualcomm testing how much they can get away with before the community revolts? Or is this a first step toward locking down the ecosystem they just bought?&lt;/p&gt;
    &lt;p&gt;Some people point out that “the Platform” might only mean Arduino’s cloud services (forums, Arduino Cloud, Project Hub) not the IDE and CLI that everyone actually uses.&lt;/p&gt;
    &lt;p&gt;If that’s true, Qualcomm needs to say so, explicitly, and in plain language. Because library maintainers are likely wondering whether contributing to Arduino repos puts them at legal risk. And hardware makers are questioning whether “Arduino-compatible” is still safe to advertise.&lt;/p&gt;
    &lt;p&gt;Why Adafruit’s alarm matters&lt;lb/&gt;Adafruit has been vocal about the dangers of this acquisition. Some dismiss Adafruit’s criticism as self-serving. After all, they sell competing hardware and promote CircuitPython. But that misses who Adafruit is.&lt;/p&gt;
    &lt;p&gt;Adafruit has been the moral authority on open hardware for decades. They’ve made their living proving you can build a successful business on open principles. When they sound the alarm, it’s not about competition, it’s about principle.&lt;/p&gt;
    &lt;p&gt;What they’re calling out isn’t that Qualcomm bought Arduino. It’s that Qualcomm’s lawyers fundamentally don’t understand what they bought. Arduino wasn’t valuable because it was just a microcontroller company. It was valuable because it was a commons. And you can’t apply enterprise legal frameworks to a commons without destroying it.&lt;/p&gt;
    &lt;p&gt;Adafruit gets this. They’ve built their entire business on this. That’s why their criticism carries weight.&lt;/p&gt;
    &lt;p&gt;What Qualcomm doesn’t seem to understand&lt;lb/&gt;Qualcomm probably thought they were buying an IoT hardware company with a loyal user base. &lt;/p&gt;
    &lt;p&gt;They weren’t. They bought the IBM PC of the maker world.&lt;/p&gt;
    &lt;p&gt;Arduino’s value was never just the hardware. Their boards have been obsolete for years. Their value is the standard.&lt;/p&gt;
    &lt;p&gt;The Arduino IDE is the lingua franca of hobby electronics.&lt;/p&gt;
    &lt;p&gt;Millions of makers learned on it, even if they moved to other hardware. ESP32, STM32, Teensy, Raspberry Pi Pico – none of them are Arduino hardware, but they all work with the Arduino IDE.&lt;/p&gt;
    &lt;p&gt;Thousands of libraries are “Arduino libraries.” Tutorials assume Arduino. University curricula teach Arduino. When you search “how to read a sensor,” the answer comes back in Arduino code.&lt;/p&gt;
    &lt;p&gt;This is the ecosystem Qualcomm’s lawyers just dropped legal uncertainty onto.&lt;/p&gt;
    &lt;p&gt;If Qualcomm’s lawyers start asserting control over the IDE, CLI, or core libraries under restrictive terms, they will poison the entire maker ecosystem. Even people who never buy Arduino hardware are dependent on Arduino software infrastructure.&lt;/p&gt;
    &lt;p&gt;Qualcomm didn’t just buy a company. They bought a commons. And now they inadvertently are taking steps that are destroying what made it valuable.&lt;/p&gt;
    &lt;p&gt;What are makers supposed to do?&lt;lb/&gt;There has been some buzz of folks just leaving the Arduino environment behind. But Arduino IDE alternatives such as PlatformIO and VSCode are not in any way beginner friendly. If the Arduino IDE goes, then there’s a huge problem. &lt;/p&gt;
    &lt;p&gt;I remember when Hypercard ended. There were alternatives, but none so easy. I don’t think I really coded again for almost 20 years until I picked up the Arduino IDE (go figure).&lt;/p&gt;
    &lt;p&gt;If something happens to the Arduino IDE, even if its development stalls or becomes encumbered, there’s no replacement for that easy onboarding. We’d lose many promising new makers because the first step became too steep.&lt;/p&gt;
    &lt;p&gt;The institutional knowledge at risk&lt;lb/&gt;But leaving Arduino behind isn’t simple. The platform’s success depends on two decades of accumulated knowledge, such as countless Arduino tutorials on YouTube, blogs, and school curricula; open-source libraries that depend on Arduino compatibility; projects in production using Arduino tooling; and university programs built around Arduino as the teaching platform&lt;/p&gt;
    &lt;p&gt;All of these depend on Arduino remaining open and accessible.&lt;/p&gt;
    &lt;p&gt;If Qualcomm decided to sunset the open Arduino IDE in favor of a locked-down “Arduino Pro” platform, or if they start asserting patent claims, or if uncertainty makes contributors abandon the ecosystem, all that knowledge becomes stranded.&lt;/p&gt;
    &lt;p&gt;It’s like Wikipedia going behind a paywall. The value isn’t just the content, it is the trust that it remains accessible. Arduino’s value isn’t just the code, it’s the trust that the commons would stay open.&lt;/p&gt;
    &lt;p&gt;That trust is now gone. And once lost, it hard to get back.&lt;/p&gt;
    &lt;p&gt;Why this happened (but doesn’t excuse it)&lt;lb/&gt;Let’s be fair to Qualcomm, their lawyers were doing their jobs.&lt;/p&gt;
    &lt;p&gt;When you acquire a company, you standardize the legal terms; add mandatory arbitration to limit class action exposure; integrate data systems for compliance and auditing; add export controls because you sell to defense contractors; prohibit reverse engineering because that’s in the template.&lt;/p&gt;
    &lt;p&gt;For most acquisitions, this is just good corporate hygiene. And Arduino, now part of a megacorp, faces higher liabilities than it did as an independent entity.&lt;/p&gt;
    &lt;p&gt;But here’s what Qualcomm’s lawyers missed: Arduino isn’t a normal acquisition. The community isn’t a customer base, it’s a commons. And you can’t apply enterprise SaaS legal frameworks to a commons without destroying what made it valuable.&lt;/p&gt;
    &lt;p&gt;This is tone-deafness, not malice. But the outcome is the same. A community that trusted Arduino no longer does.&lt;/p&gt;
    &lt;p&gt;Understanding why this happened doesn’t excuse it, but it might suggest what needs to happen next.&lt;/p&gt;
    &lt;p&gt;What should have happened and how to still save it&lt;lb/&gt;Qualcomm dropped legal boilerplate on the community with zero context and let people discover the contradictions themselves. That’s how you destroy trust overnight.&lt;/p&gt;
    &lt;p&gt;Qualcomm should have announced the changes in advance. They should have given the community weeks, not hours, to understand what’s changing and why. They should have used plain-language explanations, not just legal documents.&lt;/p&gt;
    &lt;p&gt;Qualcomm can fix things by explicitly carving out the open ecosystem. They should state clearly that the terms apply to Arduino Cloud services, and the IDE, CLI, and core libraries remain under their existing open source licenses.&lt;/p&gt;
    &lt;p&gt;We’d need concrete commitments, such as which repos stay open, which licenses won’t change, what’s protected from future acquisition decisions. Right now we have vague corporate-speak about “supporting the community.”&lt;/p&gt;
    &lt;p&gt;Indeed, they could create some structural protection, as well, by putting IDE, CLI, and core libraries in a foundation that Qualcomm couldn’t unilaterally control (think the Linux Foundation model).&lt;/p&gt;
    &lt;p&gt;Finally, Qualcomm might wish to establish some form of community governance with real representation and real power over the tools the community depends on.&lt;/p&gt;
    &lt;p&gt;The acquisition is done. The legal integration is probably inevitable. But how it’s done determines whether Arduino survives as a commons or dies as just another Qualcomm subsidiary.&lt;/p&gt;
    &lt;p&gt;What’s next?&lt;lb/&gt;Arduino may be the toolset that made hobby electronics accessible to millions. But that maker community built Arduino into what it became. Qualcomm’s acquisition has thrown that legacy into doubt. Whether through legal confusion, corporate tone-deafness, or deliberate strategy, the community’s trust is broken.&lt;/p&gt;
    &lt;p&gt;The next few months will reveal whether this was a stumble or a strategy. If Qualcomm issues clarifications, moves repos to some sort of governance, and explicitly protects the open toolchain, then maybe this is salvageable. If they stay silent, or worse, if IDE development slows or license terms tighten further, then that’s a signal to find alternatives.&lt;/p&gt;
    &lt;p&gt;The question isn’t whether the open hobby electronics maker community survives. It’s whether Arduino does.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.molecularist.com/2025/11/did-qualcomm-kill-arduino-for-good.html"/><published>2025-11-21T15:44:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006004</id><title>Command Lines – AI Coding's Control Spectrum</title><updated>2025-11-21T18:15:21.553619+00:00</updated><content>&lt;doc fingerprint="3751c158b81408fc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Command Lines&lt;/head&gt;
    &lt;head rend="h3"&gt;AI Coding's Control Spectrum&lt;/head&gt;
    &lt;p&gt;In the early 1950s, Grace Hopper coined the term “compiler” and built one of the first versions with her A-0 system1. The compilers that followed abstracted away machine code, letting programmers focus on higher-level logic instead of lower-level hardware details. Today, AI coding assistants2 are enabling a similar change, letting software engineers focus on higher-order work by generating code from natural language prompts3. Everyone from big tech to well-funded startups is competing to capture this shift. Yesterday Google announced Antigravity, their new AI coding assistant, and the day before, AWS announced the general availability of their AI coding tool, Kiro. Last week, Cursor, the standout startup in this space, raised $2.3B in their series-D round at a valuation of $29.3B.&lt;/p&gt;
    &lt;p&gt;Two lines in Cursor’s press release stood out to me. The first:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We’ve also crossed $1B in annualized revenue, counting millions of developers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This disclosure means Anysphere Inc. (Cursor’s parent company) is the fastest company in history to reach $1B in annual recurring revenue (ARR). Yes, faster than OpenAI, and faster than Anthropic4.&lt;/p&gt;
    &lt;p&gt;Engineers are trying every new AI coding tool. As a result, the AI-coding tool market is growing exponentially (+5x in just over a year)5. But it’s still early. As I wrote in Why Some AI Wrappers Build Billion-dollar Businesses, companies spend several hundred billion dollars a year on software engineering, and AI has the potential to unlock productivity gains across that entire spend.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Software developers represent roughly 30% of the workforce at the world’s five largest market cap companies, all of which are technology firms as of October 2025. Development tools that boost productivity by even modest percentages unlock billions in value.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In my view, this nascent market is splitting based on three types of users.&lt;/p&gt;
    &lt;p&gt;On one end is Handcrafted Coding. These are engineers who actively decline to use LLMs, either because of skepticism about quality or insistence on full control of every code. They argue that accepting AI suggestions creates technical debt you cannot see until it breaks in production. This segment continues to decline as the quality of AI coding models improves.&lt;/p&gt;
    &lt;p&gt;The opposite end is Vibe Coding. These are typically non-engineers, who use AI to build concepts and prototypes. They prompt the model hoping for an end-to-end solution, accept the output with minimal review, and trust that it works. The user describes what they want and lets the model figure out the implementation details of how to build it.&lt;/p&gt;
    &lt;p&gt;In the middle sits Architect + AI Coding. The engineer uses the AI/LLM as a pair programmer exploring system designs, analyzing data models, and reviewing API details. When the work is something entirely new or something that needs careful handling, the human programmer still codes those pieces by hand. But for boilerplate code, package installations, generic User Interface (UI) components, and any kind of code that is typically found on the internet, they assign it to the model6. The engineer stays in command of what is important to them and delegates what is not.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Market Split&lt;/head&gt;
    &lt;p&gt;Based on the user types, I think, the AI coding market splits into two.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Hands-off: Non-engineers (product managers, designers, marketers, other internal employees) use these tools to vibe code early product concepts. They look to AI as the lead engineer to spin-up concepts/prototypes of apps, websites, and tools by simply prompting the AI to make something for them. Lovable, Vercel, Bolt, Figma Make, and Replit fit here7. Code from these users, as of now, are not typically pushed to prod.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hands-on: Professional software engineers use these tools in their existing workflow to ship production code. They use AI as an assistant to write boilerplate code, refactor existing services, wire new features or UI screens, and triage bugs in codebases. Cursor, Claude Code, OpenAI Codex, Github Copilot, Cline, AWS Kiro play here. These products live where the work is done, and integrate into the engineer’s workflow. This is, at least as of now, the bigger market segment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To see an evaluation of all the major AI coding tools currently in the market, checkout this breakdown by Peter Yang, who runs the newsletter Behind The Craft.&lt;/p&gt;
    &lt;p&gt;That brings me to the second thing in Cursor’s press release that stood out to me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our in-house models now generate more code than almost any other LLMs in the world.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While I am not convinced about that claim8, what I am convinced about is that Cursor is still growing despite its previous reliance on foundation models. From Why Some AI Wrappers Build Billion-dollar Businesses again:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;But Cursor and other such tools depend almost entirely on accessing Anthropic, OpenAI and Gemini models, until&lt;/p&gt;&lt;del&gt;open-source&lt;/del&gt;open-weight and in-house models match or exceed frontier models in quality. Developer forums are filled with complaints about rate limits from paying subscribers. In my own projects, I exhausted my Claude credits in Cursor mid-project and despite preferring Cursor’s user interface and design, I migrated to Claude Code (and pay ten times more to avoid rate limits). The interface may be better, but model access proved decisive.&lt;/quote&gt;
    &lt;p&gt;Cursor’s new in-house model Composer-2, which just launched last month, is a good example of how this model versus application competition is evolving. Cursor claims (without any external benchmarks, I must say) that Composer-2 is almost as good as frontier models but 4x faster. It’s still early to say how true that is. Open-source models have not yet come close to the top spots in SWE-bench verified or in private evals9.&lt;/p&gt;
    &lt;p&gt;To me, model quality is the most decisive factor in these AI coding wars. And in my view, that’s why Claude Code has already overtaken Cursor, and OpenAI’s Codex is close behind, despite both having launched a year or so later.&lt;/p&gt;
    &lt;p&gt;Even though the newcomers Cursor, Claude Code, and OpenAI Codex are the talk of the (developer) town, incumbents such as Microsoft with Github Copilot, AWS with Kiro, and Google with Antigravity, can utilize their existing customer relationships, bundle their offerings with their existing suites, and/or provide their option as the default in their tech stack to compete. As an example, Cursor charges $20–$40 monthly per user for productive usage, while Google Antigravity launched free with generous limits for individual users. Github Copilot still leads this market, proving once again that enterprise bundling and distribution has structural advantages. This is the classic Microsoft Teams vs. Slack Dynamic10.&lt;/p&gt;
    &lt;p&gt;One way for startups to compete is by winning individual users who may use a coding tool with or without formal approval, and then be the tool’s advocate inside the organization. That organic interest and adoption eventually forces IT and security teams to officially review the tool and then eventually sanction its usage.&lt;/p&gt;
    &lt;p&gt;Yet, even as these newer tools capture developer mindshare, the underlying developer tools market is changing. Both the IDEs developers choose and the resources &lt;del&gt;they&lt;/del&gt; we consult have changed dramatically. StackOverflow, once the default for programmers stuck on a programming issue, has seen its traffic and number of questions decline dramatically since ChatGPT’s launch, suggesting that AI is already replacing some traditional developer resources.&lt;/p&gt;
    &lt;p&gt;Just as compilers freed programmers from writing assembly code, AI tools are freeing software engineers from the grunt work of writing boilerplate and routine code, and letting &lt;del&gt;them&lt;/del&gt; us focus on higher-order thinking. Eventually, one day, AI may get so good that it will generate applications on demand and create entire software ecosystems autonomously. Both hands-off and hands-on AI coding tools, as well as incumbents and newcomers, see themselves as the path to that fully autonomous software generation, even if they are taking different approaches. The ones who get there will be those who deliver the best model quality that ships code reliably, go deep enough to ship features that foundation models can’t care enough to replicate, and become sticky enough that users will not leave even when they can11. &lt;/p&gt;
    &lt;p&gt;If you enjoyed this post, please consider sharing it on Twitter/X or LinkedIn, and tag me when you do.&lt;/p&gt;
    &lt;p&gt;Hopper’s A-0 system and her definition of the term compiler is different from what we consider a compiler today, but it established the foundational concept.&lt;/p&gt;
    &lt;p&gt;In the context of coding assistants, most products labeled as AI tools are powered by LLMs, and so I use AI and LLM interchangeably in this article despite the actual difference.&lt;/p&gt;
    &lt;p&gt;A better comparison might be at the product level rather than company level. In that case, ChatGPT and Claude both reached $1B faster than Cursor did.&lt;/p&gt;
    &lt;p&gt;I would argue that the vast majority of productive code is hidden behind company firewalls. Current foundation models are trained on publicly available data on the internet, and do not have access to proprietary codebases. We are yet to see breakthrough solutions where a company augments their confidential private data to generate production-ready code using current LLMs. While Retrieval-Augmented Generation has shown some promise, it has not yet delivered transformative results. Companies such as Glean are actively working on this problem.&lt;/p&gt;
    &lt;p&gt;Replit and Cognition probably appeal to both segments. To me, Replit leans hands-off with its rapid prototyping focus. Cognition’s agent-based approach, though hands-off, lets engineers still control the code directly, making it lean hands-on.&lt;/p&gt;
    &lt;p&gt;I was curious how Cursor knows how much code is generated by other LLMs outside Cursor? When I asked this on hackernews, swyx suggested that they “can pretty much triangulate across openrouter x feedback from the top 3 model labs to compare with internal usage and figure that out”. To me, triangulation makes sense for internal estimates. but for external publication, I’m surprised Cursor didn’t include “we estimate” or similar qualifying language. My understanding is that FTC policy requires substantiation before making definitive comparative claims (like more than, better than etc). All that to say, I’m not fully convinced about their claims.&lt;/p&gt;
    &lt;p&gt;SWE-bench is a benchmark for evaluating large language models (LLMs) on real world software engineering tasks and issues collected from GitHub. Performance against public benchmarks can be gamed by the model builders. Currently after any new model launch, we see people using the model in the wild and forming a consensus around how the model performs which is a better indicator than these benchmarks.&lt;/p&gt;
    &lt;p&gt;Microsoft bundled Teams into Office 365 subscriptions at no extra cost, using its dominant enterprise distribution to surpass Slack’s paid standalone product within three years despite Slack’s earlier launch and product innovation. See https://venturebeat.com/ai/microsoft-teams-has-13-million-daily-active-users-beating-slack&lt;/p&gt;
    &lt;p&gt;Natasha Malpani, Twitter/X, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wreflection.com/p/command-lines-ai-coding"/><published>2025-11-21T16:33:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006016</id><title>Show HN: Wealthfolio 2.0- Open source investment tracker. Now Mobile and Docker</title><updated>2025-11-21T18:15:21.379203+00:00</updated><content>&lt;doc fingerprint="27ab40bb69b94b92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grow Wealth. Keep Control.&lt;/head&gt;
    &lt;head rend="h2"&gt;A beautiful, Private and Open-Source investment tracker that runs locally on all your devices.&lt;/head&gt;
    &lt;head rend="h2"&gt;WHY CHOOSE WEALTHFOLIO?&lt;/head&gt;
    &lt;p&gt;A beautiful portfolio tracker that respects your privacy and your data&lt;/p&gt;
    &lt;head rend="h3"&gt;Privacy-First Approach&lt;/head&gt;
    &lt;p&gt;Your data never leaves your device. As an open-source project, we prioritize security and transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple and Beautifully Crafted&lt;/head&gt;
    &lt;p&gt;Powerful features wrapped in an elegant, easy-to-use interface. Simplicity meets sophistication.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Hidden Costs&lt;/head&gt;
    &lt;p&gt;Free to use with optional one-time payment. No subscriptions or recurring fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE ESSENTIALS YOU NEED TO TRACK YOUR WEALTH&lt;/head&gt;
    &lt;p&gt;No More Messy Spreadsheets or Privacy Concerns - Just You and Your Secure, Personal Wealth Companion Application&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Aggregation&lt;/head&gt;
    &lt;p&gt;Gather all your investment and savings accounts in one place. See everything at a glance, from stocks to savings! Import your CSV statements from your broker or bank.&lt;/p&gt;
    &lt;head rend="h4"&gt;Comprehensive View&lt;/head&gt;
    &lt;p&gt;See all your accounts in one place.&lt;/p&gt;
    &lt;head rend="h4"&gt;CSV Import&lt;/head&gt;
    &lt;p&gt;Easily import your CSV statements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Holdings Overview&lt;/head&gt;
    &lt;p&gt;Get a clear picture of what's in your portfolio. Stocks, ETFs, or Cryptocurrencies - know what you have and how it's performing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Portfolio Insights&lt;/head&gt;
    &lt;p&gt;Understand your asset allocation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance Tracking&lt;/head&gt;
    &lt;p&gt;Monitor how your investments are doing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance Dashboard&lt;/head&gt;
    &lt;p&gt;See how your investments stack up, all in one place. Compare your accounts side by side, check if you are beating the S&amp;amp;P 500, and track your favorite ETFs without the hassle. No fancy jargon - just clear, useful charts that help you understand how your money is really doing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compare Your Accounts&lt;/head&gt;
    &lt;p&gt;See which accounts are doing best.&lt;/p&gt;
    &lt;head rend="h4"&gt;Beat the Market?&lt;/head&gt;
    &lt;p&gt;Check how you stack up against some popular indexes and ETFs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Income Tracking&lt;/head&gt;
    &lt;p&gt;Monitor dividends and interest income across your entire portfolio. Get a clear view of your passive income streams, helping you make informed decisions about your investments.&lt;/p&gt;
    &lt;head rend="h4"&gt;Dividend Monitoring&lt;/head&gt;
    &lt;p&gt;Track your dividend income.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interest Income&lt;/head&gt;
    &lt;p&gt;Keep an eye on interest earnings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Performance&lt;/head&gt;
    &lt;p&gt;Track your accounts' holdings and performance over time. See how a particular account is performing, and how it's changing over time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Historical Data&lt;/head&gt;
    &lt;p&gt;View past performance trends.&lt;/p&gt;
    &lt;head rend="h4"&gt;Account Analysis&lt;/head&gt;
    &lt;p&gt;Analyze individual account performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goals Tracking&lt;/head&gt;
    &lt;p&gt;Set your savings targets clearly. Distribute your funds across these objectives, assigning a specific percentage to each. Keep an eye on your progress.&lt;/p&gt;
    &lt;head rend="h4"&gt;Target Setting&lt;/head&gt;
    &lt;p&gt;Define your financial goals.&lt;/p&gt;
    &lt;head rend="h4"&gt;Progress Monitoring&lt;/head&gt;
    &lt;p&gt;Track your progress towards goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Contribution Rooms and Limit Tracking&lt;/head&gt;
    &lt;p&gt;Stay on top of your contribution limits for tax-advantaged accounts like IRAs, 401(k)s, or TFSAs. Track your available contribution room and avoid over-contributing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Limit Awareness&lt;/head&gt;
    &lt;p&gt;Know your contribution limits.&lt;/p&gt;
    &lt;head rend="h4"&gt;Avoid Over-Contribution&lt;/head&gt;
    &lt;p&gt;Prevent excess contributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extend Wealthfolio with Powerful Add-ons&lt;/head&gt;
    &lt;head rend="h3"&gt;Investment Fees Tracker&lt;/head&gt;
    &lt;p&gt;Track and analyze investment fees across your portfolio with detailed analytics and insights&lt;/p&gt;
    &lt;head rend="h3"&gt;Goal Progress Tracker&lt;/head&gt;
    &lt;p&gt;Track your investment progress towards target amounts with a visual representation&lt;/p&gt;
    &lt;head rend="h3"&gt;Stock Trading Tracker&lt;/head&gt;
    &lt;p&gt;Simple swing stock trading tracker with performance analytics and calendar views&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wealthfolio.app/?v=2.0"/><published>2025-11-21T16:34:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006082</id><title>You Can Now Make PS2 Games in JavaScript</title><updated>2025-11-21T18:15:21.272035+00:00</updated><content/><link href="https://jslegenddev.substack.com/p/you-can-now-make-ps2-games-in-javascript"/><published>2025-11-21T16:42:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006183</id><title>Private Equity's New Venture: Youth Sports</title><updated>2025-11-21T18:15:20.897263+00:00</updated><content>&lt;doc fingerprint="2cc848a8af19564d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Private Equity’s New Venture: Youth Sports&lt;/head&gt;
    &lt;p&gt;Backed by Wall Street, the company Black Bear Sports Group is tightening its grip on youth sports. In a scheme only private equity could dream up, parents now can’t record their kids’ games — but they can pay a steep price to watch corporate recordings.&lt;/p&gt;
    &lt;p&gt;There’s an ironclad truism in youth sports: every parent turns into an ESPN 30 for 30 documentarian as soon as they have a video recording device in hand and their kid is in the game.&lt;/p&gt;
    &lt;p&gt;Some record the games and post them online so family members and friends who can’t attend in person can watch their kids play. Sometimes they do so to attract the attention of college scouts or help players hone their craft. Some people just want to preserve the memories.&lt;/p&gt;
    &lt;p&gt;But in the world of corporatized youth sports, even this simple pleasure is being banned and monetized by Wall Street to extract as much profit as possible from players and parents, no matter how many kids get sidelined because they can’t afford the sport’s rising costs.&lt;/p&gt;
    &lt;p&gt;As the $40 billion youth sports industry comes under private equity control, corporate-owned facilities and leagues — from hockey rinks to cheerleading arenas — have begun prohibiting parents from recording their own kids’ sports games.&lt;/p&gt;
    &lt;p&gt;Instead, parents are forced to subscribe to these companies’ exclusive recording and streaming service, which can cost many times more than the streaming costs for professional sporting events. Meanwhile, the firms’ exclusive contracts have prohibited alternative video services from being made available.&lt;/p&gt;
    &lt;p&gt;In some instances, parents have been threatened that if they choose to defy the rules and record the game, they may end up on a blacklist that punishes their kids’ teams. Those threats were even reportedly made to a sitting US senator.&lt;/p&gt;
    &lt;p&gt;“I was told this past weekend that if I livestreamed my child’s hockey game, my kid’s team will be penalized and lose a place in the standings,” said Sen. Chris Murphy (D-CT) at a public event earlier this year. “Why is that? Because a private equity company has bought up the rinks.”&lt;/p&gt;
    &lt;p&gt;Murphy did not name the company in question, though the restrictive streaming practices he described have become widespread across youth hockey.&lt;/p&gt;
    &lt;p&gt;Black Bear Sports Group, an emerging youth hockey empire and the largest owner-operator of hockey rinks in the country, is among the private equity–backed companies that are amassing a chokehold on recording and streaming youth sports. At Black Bear–owned ice rinks, parents cannot record, post, or livestream their kids’ hockey games online “per official company policy,” according to staff at those venues. Some rink attendants said they will confiscate attendees’ recording devices if they find them.&lt;/p&gt;
    &lt;p&gt;Some specialized sports training consultants have agreements with Black Bear that allow them to record games and practices, but only for internal use.&lt;/p&gt;
    &lt;p&gt;According to a spokesperson, Black Bear claims the policy is to mitigate “significant safety risks to players,” such as players being filmed without their consent. The spokesperson failed to answer a follow-up question about what penalties attendees might face if they try to record the games themselves.&lt;/p&gt;
    &lt;p&gt;Black Bear’s streaming service costs between $25 and $50 a month, depending on the package and additional fees. The company’s aggressive expansion of the program has even triggered a lawsuit from a former streaming partner alleging breach of contract and trade secret theft.&lt;/p&gt;
    &lt;p&gt;In addition to its recording rules and associated costs, Black Bear is starting to add a $50 “registration and insurance” fee per player for some leagues. That’s on top of what players already spend on expensive equipment, team registration, and membership to USA Hockey, the sport’s national governing body.&lt;/p&gt;
    &lt;p&gt;“Black Bear Sports Group does not have a good reputation in the hockey world and is known for predatory practices of its customers like price gouging,” reads a recently launched petition protesting the new registration and insurance charges.&lt;/p&gt;
    &lt;p&gt;The fees and streaming restrictions reveal how private equity firms are deploying the same playbook in youth sports as they have in other domains, from dentistry to bowling: degrade the quality of service while juicing returns for investors.&lt;/p&gt;
    &lt;p&gt;“Black Bear [is] following the exact same model as we’ve seen elsewhere in the industry,” said Katie Van Dyck, an antitrust attorney and senior fellow at the American Economic Liberties Project. “It’s not about investing to enrich our children’s lives.”&lt;/p&gt;
    &lt;head rend="h1"&gt;“The New Sport of Kings”&lt;/head&gt;
    &lt;p&gt;The new fees tacked on by Black Bear contribute to the already rising costs of participating in youth and recreational sports like hockey.&lt;/p&gt;
    &lt;p&gt;Across the board, youth sports have become an increasingly expensive budget item for American families, thanks to costs ranging from equipment to team memberships and travel.&lt;/p&gt;
    &lt;p&gt;According to a recent study from the Aspen Institute, households now spend an average of $1,016 a year on their child’s primary sport, a 46 percent increase since 2019.&lt;/p&gt;
    &lt;p&gt;The professionalization of youth sports has further driven up costs. Some parents now pay for personal trainers and even sports psychologists to give their kids a competitive edge in the hopes of them reaching the collegiate or professional level.&lt;/p&gt;
    &lt;p&gt;As a result, many children from lower-income families are being priced out of youth sports.&lt;/p&gt;
    &lt;p&gt;“We have this affordability crisis, and youth sports are one of those things that’s becoming an activity only for the wealthy,” said Van Dyck. “It’s not something that is accessible to people who make less than six figures a year.”&lt;/p&gt;
    &lt;p&gt;This trend line has been particularly pronounced in hockey, which, according to some metrics, is the most expensive youth sport, with an average cost of $2,583. Skate prices can top $1,000, and sticks can often cost several hundred.&lt;/p&gt;
    &lt;p&gt;“It’s the new sport of kings,” said Joseph Kolodziej, who runs a consultancy helping parents and athletes navigate the world of youth hockey. “I’ve been hearing for over twenty years that prices are forcing people out of the sport and that teams are losing gifted athletes because they can’t afford to play.”&lt;/p&gt;
    &lt;p&gt;The rapid commercialization of youth sports has become big business. One recent estimate put the total valuation of the youth sports market at $40 billion. Youth hockey alone could reach over $300 million by the end of the decade.&lt;/p&gt;
    &lt;p&gt;Those sky-high revenues have attracted Wall Street investors looking to charge more money from a wealthier customer base willing to pay more for their kids.&lt;/p&gt;
    &lt;p&gt;And now, virtually every corner of the youth sports industry is coming under corporate ownership.&lt;/p&gt;
    &lt;p&gt;A company called Unrivaled Sports, run by two veterans of Blackstone, the world’s largest private equity firm, is rapidly consolidating baseball camps, flag football, and other leagues. The operation even bought the iconic baseball megacomplex in Cooperstown, New York, considered the birthplace of the sport, where summer tournaments draw teams from around the country.&lt;/p&gt;
    &lt;p&gt;Bain Capital–backed Varsity Brands, meanwhile, has cannibalized the competitive cheerleading arena and now acts as the gatekeeper controlling access to the sport.&lt;/p&gt;
    &lt;p&gt;All of this outside investment has raised concerns that the financial firms rolling up the market may further increase costs for families.&lt;/p&gt;
    &lt;p&gt;From health care to retail, private equity firms purchase companies, load them up with debt, slash costs, and extract as much profit as possible for investors before selling the operations or filing for bankruptcy.&lt;/p&gt;
    &lt;p&gt;“When youth sports become an investment vehicle, rather than a development vehicle for children, there [are] all kinds of financial predation that can arise from vulture companies that don’t have the sport’s long-term interest in mind,” said Van Dyck at the American Economic Liberties Project.&lt;/p&gt;
    &lt;p&gt;Varsity Brands, for example, faced a class-action antitrust lawsuit for alleged anticompetitive practices that pushed out cheerleading rivals while squeezing profits from participants, such as forcing teams to purchase Varsity’s own apparel and equipment. In 2024, Varsity, which was also mired in a sex abuse scandal, settled the suit for $82 million.&lt;/p&gt;
    &lt;p&gt;In addition to controlling venues, uniforms, and the tournaments for competitive cheerleading, Varsity expanded into entertainment streaming services with Varsity TV, which has the exclusive right to livestream the company’s competitions. It’s lorded that arrangement not just over parents but also tech giants. During the 2020 Netflix docuseries Cheer, which follows a cheerleading team competing across the country, Varsity wouldn’t allow the series’ crew to film inside the venue they owned in Daytona, Florida.&lt;/p&gt;
    &lt;p&gt;The Texas attorney general is probing similar anticompetitive practices by the Dallas Stars, a professional National Hockey League team, following an explosive USA Today investigation into its youth hockey operations. According to the report, the team bought up dozens of Texas’s recreational rinks. It then allegedly used its market power to jack up fees on youth players, underinvested in rink maintenance, and retaliated against clubs that tried to oppose them.&lt;/p&gt;
    &lt;p&gt;Now, legal experts say Black Bear Sports is replicating a similar model for youth hockey teams along the East Coast and beyond.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Only Game in Town&lt;/head&gt;
    &lt;p&gt;Hockey has grown in popularity across the United States, with USA Hockey membership reaching an all-time high of 577,900 in 2025. But it’s become increasingly difficult for small operations to meet the growing demand.&lt;/p&gt;
    &lt;p&gt;For example, rinks require immense amounts of energy for air conditioning to reach freezing temperatures, and electric utility bills have skyrocketed over the past decade. And while many local rinks used to be municipally run or publicly funded, such support has been slashed in recent decades in favor of government privatization.&lt;/p&gt;
    &lt;p&gt;In 2015, the Maryland-based Black Bear Sports entered the scene. The company, owned by the private equity firm Blackstreet Capital, began buying up struggling ice rinks, some of which were on the verge of closing. According to the company’s sales pitch, it would invest the capital to retrofit and renovate the rinks, making them serviceable.&lt;/p&gt;
    &lt;p&gt;This approach follows a familiar pattern for Black Bear Sports’ founder, Murry Gunty, a longtime hockey aficionado who got his start at Blackstone before launching his own private equity firm, Blackstreet Capital. Blackstreet is known for buying up small- to medium-sized distressed companies for cheap, then making the businesses leaner before selling them off. While slashing costs to bring in returns for the firm’s investors, the private equity fund managers charge massive fees to pad their own bottom lines.&lt;/p&gt;
    &lt;p&gt;Shortly after founding Black Bear in 2015, Gunty was sued by the Securities and Exchange Commission for charging investors high fees without being licensed as a broker. Blackstreet settled the charges for $3.1 million.&lt;/p&gt;
    &lt;p&gt;Today Black Bear owns forty-two rinks across eleven states across the Northeast, Midwest, and mid-Atlantic coast. In some areas, those venues are the only game in town. With its network of rinks, Black Bear controls the basic infrastructure that other clubs, leagues, and tournaments need to access.&lt;/p&gt;
    &lt;p&gt;Along with rinks, Black Bear also manages four national and regional youth hockey associations, a handful of junior-level sports teams, such as the Maryland Black Bears, and organizes major youth hockey tournaments on the East Coast. Gunty acts as the commissioner of the United States Premier Hockey League, one of the largest top-level junior leagues with seventy-five teams nationwide, offering a direct pathway for young athletes to play at the college level. Black Bear’s vice president, Tony Zasowski, is the league commissioner for the Tier 1 Hockey Federation and the Atlantic Hockey Federation, top-level hockey leagues.&lt;/p&gt;
    &lt;p&gt;Those organizations set the rules for the league, dictate playing schedules, and require paid dues, among other costs. They also determine where leagues and tournaments will be held — such as Black Bear’s own rinks.&lt;/p&gt;
    &lt;p&gt;The conglomerate also launched its own online hockey ratings system, used to determine team rankings and players’ status.&lt;/p&gt;
    &lt;p&gt;Among the company’s newest ventures is a streaming site, Black Bear TV. In September 2024, the company put out a public notice that “all games played inside the Black Bear venues and certain partner venues will be streamed exclusively on Black Bear TV.”&lt;/p&gt;
    &lt;p&gt;That exclusive arrangement also includes all games played within the leagues run by Black Bear, even if they aren’t occurring at their own arenas. Shortly after Gunty became commissioner of the United States Premier Hockey League in 2024, the organization inked a deal to make Black Bear TV the exclusive provider for all its games.&lt;/p&gt;
    &lt;p&gt;Previously, Black Bear had an exclusive agreement with the sports broadcaster LiveBarn to livestream the games, and the two split the revenues.&lt;/p&gt;
    &lt;p&gt;But Black Bear wanted to assume full control over streaming services and profits, according to a lawsuit LiveBarn filed this year, which claims Black Bear stole LiveBarn’s business and then used inside information about its prices and terms to convince other rinks to sign deals with Black Bear.&lt;/p&gt;
    &lt;p&gt;Black Bear TV isn’t cheap. Each individual game on its online platform costs $14.99 to watch. For the service’s full suite of features, including the ability to clip plays, packages range between $26 and $36 a month and can total roughly $440 a year. Certain premier leagues controlled by Black Bear are subject to additional fees, driving up prices to $50 a month.&lt;/p&gt;
    &lt;p&gt;For comparison, an $11.99 monthly subscription to ESPN TV would include access to nearly every Division 1 college game, most National Hockey League games, professional soccer matches, PGA Tour golf tournaments, and other major sporting events.&lt;/p&gt;
    &lt;p&gt;A Black Bear spokesperson says its prices reflect the high-quality service it provides to customers. “With Black Bear TV, we are no longer limited by a fixed, center-ice camera connected to [a] rink wireless connection that often faces delays and low-quality picture,” said the spokesperson.&lt;/p&gt;
    &lt;p&gt;But user reviews for Black Bear TV complain about the service’s streaming quality and spotty coverage. The company gets to pick and choose which games it features on the service.&lt;/p&gt;
    &lt;p&gt;Starting this year, Black Bear is introducing another fee: a separate registration and insurance charge for adult leagues to access its ice rinks.&lt;/p&gt;
    &lt;p&gt;The new $50 annual charge, which could become a model for youth leagues under Black Bears’ control, triggered a public petition in September demanding the company reduce its fees.&lt;/p&gt;
    &lt;p&gt;Black Bear contends that the new fee is a slightly lower-cost alternative to USA Hockey’s $52 adult registration cost, which is required to participate in the organization’s sanctioned leagues.&lt;/p&gt;
    &lt;p&gt;But according to the petition, certain recreational leagues weren’t previously paying any fees at Black Bear rinks, and some players may now have to pay both registration fees if they also play in leagues unrelated to Black Bear.&lt;/p&gt;
    &lt;p&gt;The additional fees could be another hurdle denying some players the joys of participating in the sport altogether.&lt;/p&gt;
    &lt;p&gt;“Adding an additional fee is unnecessary and makes an already hard-to-access sport even more difficult, especially for new players . . . [it] risks killing our league as it has already shrunken from previous years,” say petition organizers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jacobin.com/2025/11/youth-sports-hockey-private-equity"/><published>2025-11-21T16:52:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006250</id><title>Pivot Robotics (YC W24) Is Hiring for an Industrial Automation Hardware Engineer</title><updated>2025-11-21T18:15:20.258931+00:00</updated><content>&lt;doc fingerprint="6e34f445ca77fde2"&gt;
  &lt;main&gt;
    &lt;p&gt;AI for Robot Arms in Factories&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Pivot Robots (YC W24) is building the AI brain for robot arms for high-mix manufacturing.&lt;/p&gt;
    &lt;p&gt;Pivot Robots combines off-the-shelf robots and vision sensors with recent breakthroughs in foundation vision models to give industrial robot arms the power to adapt. Our first product directly addresses the dangerous and unpopular task of grinding metal parts. Currently, our software is being deployed on 10+ robots at a large cast iron foundry.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/pivot-robotics/jobs/7xG9Dc6-mechanical-engineer-controls"/><published>2025-11-21T17:00:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006521</id><title>How Cops Are Using Flock's ALPR Network to Surveil Protesters and Activists</title><updated>2025-11-21T18:15:19.652356+00:00</updated><content>&lt;doc fingerprint="2d51cd9273907347"&gt;
  &lt;main&gt;
    &lt;p&gt;It's no secret that 2025 has given Americans plenty to protest about. But as news cameras showed protesters filling streets of cities across the country, law enforcement officers—including U.S. Border Patrol agents—were quietly watching those same streets through different lenses: Flock Safety automated license plate readers (ALPRs) that tracked every passing car.&lt;/p&gt;
    &lt;p&gt;Through an analysis of 10 months of nationwide searches on Flock Safety's servers, we discovered that more than 50 federal, state, and local agencies ran hundreds of searches through Flock's national network of surveillance data in connection with protest activity. In some cases, law enforcement specifically targeted known activist groups, demonstrating how mass surveillance technology increasingly threatens our freedom to demonstrate.&lt;/p&gt;
    &lt;p&gt;Flock Safety provides ALPR technology to thousands of law enforcement agencies. The company installs cameras throughout their jurisdictions, and these cameras photograph every car that passes, documenting the license plate, color, make, model and other distinguishing characteristics. This data is paired with time and location, and uploaded to a massive searchable database. Flock Safety encourages agencies to share the data they collect broadly with other agencies across the country. It is common for an agency to search thousands of networks nationwide even when they don't have reason to believe a targeted vehicle left the region.&lt;/p&gt;
    &lt;p&gt;Via public records requests, EFF obtained datasets representing more than 12 million searches logged by more than 3,900 agencies between December 2024 and October 2025. The data shows that agencies logged hundreds of searches related to the 50501 protests in February, the Hands Off protests in April, the No Kings protests in June and October, and other protests in between.&lt;/p&gt;
    &lt;p&gt;The Tulsa Police Department in Oklahoma was one of the most consistent users of Flock Safety's ALPR system for investigating protests, logging at least 38 such searches. This included running searches that corresponded to a protest against deportation raids in February, a protest at Tulsa City Hall in support of pro-Palestinian activist Mahmoud Khalil in March, and the No Kings protest in June. During the most recent No Kings protests in mid-October, agencies such as the Lisle Police Department in Illinois, the Oro Valley Police Department in Arizona, and the Putnam County (Tenn.) Sheriff's Office all ran protest-related searches.&lt;/p&gt;
    &lt;p&gt;While EFF and other civil liberties groups argue the law should require a search warrant for such searches, police are simply prompted to enter text into a "reason" field in the Flock Safety system. Usually this is only a few words–or even just one.&lt;/p&gt;
    &lt;p&gt;In these cases, that word was often just “protest.”&lt;/p&gt;
    &lt;p&gt;Crime does sometimes occur at protests, whether that's property damage, pick-pocketing, or clashes between groups on opposite sides of a protest. Some of these searches may have been tied to an actual crime that occurred, even though in most cases officers did not articulate a criminal offense when running the search. But the truth is, the only reason an officer is able to even search for a suspect at a protest is because ALPRs collected data on every single person who attended the protest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Search and Dissent&lt;/head&gt;
    &lt;p&gt;2025 was an unprecedented year of street action. In June and again in October, thousands across the country mobilized under the banner of the “No Kings” movement—marches against government overreach, surveillance, and corporate power. By some estimates, the October demonstrations ranked among the largest single-day protests in U.S. history, filling the streets from Washington, D.C., to Portland, OR.&lt;/p&gt;
    &lt;p&gt;EFF identified 19 agencies that logged dozens of searches associated with the No Kings protests in June and October 2025. In some cases the "No Kings" was explicitly used, while in others the term "protest" was used but coincided with the massive protests.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Law Enforcement Agencies that Ran Searches Corresponding with "No Kings" Rallies&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Washington state, the Spokane County Sheriff's Office listed "no kings" as the reason for three searches on June 13, 2025. The agency queried 95 camera networks, looking for vehicles matching the description of "work van," "bus" or "box truck."&lt;/item&gt;
      &lt;item&gt;In Texas, the Beaumont Police Department ran six searches related to two vehicles on June 14, 2025, listing "KINGS DAY PROTEST" as the reason. The queries reached across 1,774 networks.&lt;/item&gt;
      &lt;item&gt;In California, the San Bernardino County Sheriff's Office ran a single search for a vehicle across 711 networks, logging "no king" as the reason.&lt;/item&gt;
      &lt;item&gt;In Arizona, the Tempe Police Department made three searches for "ATL No Kings Protest" on June 15, 2025 searching through 425 networks. "ATL" is police code for "attempt to locate." The agency appears to not have been looking for a particular plate, but for any red vehicle on the road during a certain time window.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the No Kings protests weren't the only demonstrations drawing law enforcement's digital dragnet in 2025.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Nevada's state capital, the Carson City Sheriff's Office ran three searches that correspond to the February 50501 Protests against DOGE and the Trump administration. The agency searched for two vehicles across 178 networks with "protest" as the reason.&lt;/item&gt;
      &lt;item&gt;In Florida, the Seminole County Sheriff's Office logged "protest" for five searches that correspond to a local May Day rally.&lt;/item&gt;
      &lt;item&gt;In Alabama, the Homewood Police Department logged four searches in early July 2025 for three vehicles with "PROTEST CASE" and "PROTEST INV." in the reason field. The searches, which probed 1,308 networks, correspond to protests against the police shooting of Jabari Peoples.&lt;/item&gt;
      &lt;item&gt;In Texas, the Lubbock Police Department ran two searches for a Tennessee license plate on March 15 that corresponds to a rally to highlight the mental health impact of immigration policies. The searches hit 5,966 networks, with the logged reason "protest veh."&lt;/item&gt;
      &lt;item&gt;In Michigan, Grand Rapids Police Department ran five searches that corresponded with the Stand Up and Fight Back Rally in February. The searches hit roughly 650 networks, with the reason logged as "Protest."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some agencies have adopted policies that prohibit using ALPRs for monitoring activities protected by the First Amendment. Yet many officers probed the nationwide network with terms like "protest" without articulating an actual crime under investigation.&lt;/p&gt;
    &lt;p&gt;In a few cases, police were using Flock’s ALPR network to investigate threats made against attendees or incidents where motorists opposed to the protests drove their vehicle into crowds. For example, throughout June 2025, an Arizona Department of Public Safety officer logged three searches for “no kings rock threat,” and a Wichita (Kan.) Police Department officer logged 22 searches for various license plates under the reason “Crime Stoppers Tip of causing harm during protests.”&lt;/p&gt;
    &lt;p&gt;Even when law enforcement is specifically looking for vehicles engaged in potentially criminal behavior such as threatening protesters, it cannot be ignored that mass surveillance systems work by collecting data on everyone driving to or near a protest—not just those under suspicion.&lt;/p&gt;
    &lt;head rend="h2"&gt;Border Patrol's Expanding Reach&lt;/head&gt;
    &lt;p&gt;As U.S. Border Patrol (USBP), ICE, and other federal agencies tasked with immigration enforcement have massively expanded operations into major cities, advocates for immigrants have responded through organized rallies, rapid-response confrontations, and extended presences at federal facilities.&lt;/p&gt;
    &lt;p&gt;USBP has made extensive use of Flock Safety's system for immigration enforcement, but also to target those who object to its tactics. In June, a few days after the No Kings Protest, USBP ran three searches for a vehicle using the descriptor “Portland Riots.”&lt;/p&gt;
    &lt;p&gt;USBP has made extensive use of Flock Safety's system for immigration enforcement, but also to target those who object to its tactics.&lt;/p&gt;
    &lt;p&gt;USBP also used the Flock Safety network to investigate a motorist who had “extended his middle finger” at Border Patrol vehicles that were transporting detainees. The motorist then allegedly drove in front of one of the vehicles and slowed down, forcing the Border Patrol vehicle to brake hard. An officer ran seven searches for his plate, citing "assault on agent" and "18 usc 111," the federal criminal statute for assaulting, resisting or impeding a federal officer. The individual was charged in federal court in early August.&lt;/p&gt;
    &lt;p&gt;USBP had access to the Flock system during a trial period in the first half of 2025, but the company says it has since paused the agency's access to the system. However, Border Patrol and other federal immigration authorities have been able to access the system’s data through local agencies who have run searches on their behalf or even lent them logins.&lt;/p&gt;
    &lt;head rend="h2"&gt;Targeting Animal Rights Activists&lt;/head&gt;
    &lt;p&gt;Law enforcement's use of Flock's ALPR network to surveil protesters isn't limited to large-scale political demonstrations. Three agencies also used the system dozens of times to specifically target activists from Direct Action Everywhere (DxE), an animal-rights organization known for using civil disobedience tactics to expose conditions at factory farms.&lt;/p&gt;
    &lt;p&gt;Delaware State Police queried the Flock national network nine times in March 2025 related to DxE actions, logging reasons such as "DxE Protest Suspect Vehicle." DxE advocates told EFF that these searches correspond to an investigation the organization undertook of a Mountaire Farms facility.&lt;/p&gt;
    &lt;p&gt;Additionally, the California Highway Patrol logged dozens of searches related to a "DXE Operation" throughout the day on May 27, 2025. The organization says this corresponds with an annual convening in California that typically ends in a direct action. Participants leave the event early in the morning, then drive across the state to a predetermined but previously undisclosed protest site. Also in May, the Merced County Sheriff's Office in California logged two searches related to "DXE activity."&lt;/p&gt;
    &lt;p&gt;As an organization engaged in direct activism, DxE has experienced criminal prosecution for its activities, and so the organization told EFF they were not surprised to learn they are under scrutiny from law enforcement, particularly considering how industrial farmers have collected and distributed their own intelligence to police.&lt;/p&gt;
    &lt;p&gt;The targeting of DxE activists reveals how ALPR surveillance extends beyond conventional and large-scale political protests to target groups engaged in activism that challenges powerful industries. For animal-rights activists, the knowledge that their vehicles are being tracked through a national surveillance network undeniably creates a chilling effect on their ability to organize and demonstrate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fighting Back Against ALPR&lt;/head&gt;
    &lt;p&gt;ALPR systems are designed to capture information on every vehicle that passes within view. That means they don't just capture data on "criminals" but on everyone, all the time—and that includes people engaged in their First Amendment right to publicly dissent. Police are sitting on massive troves of data that can reveal who attended a protest, and this data shows they are not afraid to use it.&lt;/p&gt;
    &lt;p&gt;Our analysis only includes data where agencies explicitly mentioned protests or related terms in the "reason" field when documenting their search. It's likely that scores more were conducted under less obvious pretexts and search reasons. According to our analysis, approximately 20 percent of all searches we reviewed listed vague language like "investigation," "suspect," and "query" in the reason field. Those terms could well be cover for spying on a protest, an abortion prosecution, or an officer stalking a spouse, and no one would be the wiser–including the agencies whose data was searched. Flock has said it will now require officers to select a specific crime under investigation, but that can and will also be used to obfuscate dubious searches.&lt;/p&gt;
    &lt;p&gt;For protestors, this data should serve as confirmation that ALPR surveillance has been and will be used to target activities protected by the First Amendment. Depending on your threat model, this means you should think carefully about how you arrive at protests, and explore options such as by biking, walking, carpooling, taking public transportation, or simply parking a little further away from the action. Our Surveillance Self-Defense project has more information on steps you could take to protect your privacy when traveling to and attending a protest.&lt;/p&gt;
    &lt;p&gt;For local officials, this should serve as another example of how systems marketed as protecting your community may actually threaten the values your communities hold most dear. The best way to protect people is to shut down these camera networks.&lt;/p&gt;
    &lt;p&gt;Everyone should have the right to speak up against injustice without ending up in a database.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/deeplinks/2025/11/how-cops-are-using-flock-safetys-alpr-network-surveil-protesters-and-activists"/><published>2025-11-21T17:20:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006756</id><title>UK bar bans solo-drinkers to 'protect customers'</title><updated>2025-11-21T18:15:19.288031+00:00</updated><content>&lt;doc fingerprint="be140b4430c4c6db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'I've banned solo drinkers from my bar'&lt;/head&gt;
    &lt;p&gt;The owner of a cocktail and karaoke bar where solo drinkers are banned says he is baffled by the furore it has caused on social media as the policy is "for the safety of all guests".&lt;/p&gt;
    &lt;p&gt;Carl Peters said the no-single-entry policy after 21:00 had been in place since the opening of Alibi in Altrincham, Greater Manchester, in 2022.&lt;/p&gt;
    &lt;p&gt;He told the BBC it was to "mitigate risk" and "protect his customers" from being "mithered" by solo drinkers.&lt;/p&gt;
    &lt;p&gt;He posted a reel on Instagram earlier this month about the policy after saying someone had accused him of discrimination. He said he was "astounded" by the ensuing criticism, describing his venue as an "inclusive and safe environment".&lt;/p&gt;
    &lt;p&gt;The ban got a mixed response from Instagram users with one commenter saying she "always feels safe in Alibi", while another described the policy as "narrow minded", adding if he finishes work late and goes for a drink himself "he has never once mithered anyone and... happy in my own company".&lt;/p&gt;
    &lt;p&gt;Mr Peters explained there were two reasons why he introduced the policy.&lt;/p&gt;
    &lt;p&gt;Firstly, if someone on their own has a seizure or an accident with no-one with them "in a late night busy bar environment, it's an absolute nightmare for us to deal with".&lt;/p&gt;
    &lt;p&gt;He said also "sometimes if you let people in on their own, the reason why they're on their own is that they've got no-one to talk to, so they start mithering other groups".&lt;/p&gt;
    &lt;p&gt;"So what we do as a venue is we just eliminate that."&lt;/p&gt;
    &lt;p&gt;He added: "Unless you're with a group and we know who you're with, then you're not coming in."&lt;/p&gt;
    &lt;p&gt;Mr Peters added he took "pride that his customers feel safe in his venue" so was "astounded" it was now being criticised on social media.&lt;/p&gt;
    &lt;p&gt;Listen to the best of BBC Radio Manchester on Sounds and follow BBC Manchester on Facebook, X, and Instagram. You can also send story ideas via Whatsapp to 0808 100 2230.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cnve4eypg8zo"/><published>2025-11-21T17:44:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006848</id><title>McDonald's is losing its low-income customers: a symptom of the wealth divide</title><updated>2025-11-21T18:15:19.101686+00:00</updated><content>&lt;doc fingerprint="7ee4841a2cbe0786"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fast food is losing its low-income customers. Economists call it a symptom of the stark wealth divide&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Share via&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fast-food prices have risen so high that traffic from one of the industry’s core customer bases, low-income households, has dropped by double digits.&lt;/item&gt;
      &lt;item&gt;The low-income customers at McDonald’s quickly are being replaced by higher earners, according to industry experts.&lt;/item&gt;
      &lt;item&gt;The change demonstrates the pressure facing low-income consumers who are being squeezed by higher housing, clothes and child-care costs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the early 2000s, after a severe slump, McDonald’s orchestrated a major turnaround, with the introduction of its Dollar Menu.&lt;/p&gt;
    &lt;p&gt;The menu, whose items all cost $1, illustrated just how important it was to market to low-income consumers — who value getting the most bang for their buck.&lt;/p&gt;
    &lt;p&gt;Coming at a time of flagging growth, tumbling stock and the company’s first report of a quarterly loss, the Dollar Menu reversed the fast-food giant’s bad fortune. It paved the way for three years of sales growth at stores open at least a year and ballooned revenue by 33%, news outlets reported at the time.&lt;/p&gt;
    &lt;p&gt;But no longer.&lt;/p&gt;
    &lt;p&gt;For the record:&lt;/p&gt;
    &lt;p&gt;9:16 a.m. Nov. 17, 2025A previous version of this article incorrectly described a McDonald’s chief executive’s statement. The statement was about an industry-wide trend, not just McDonald’s. The headline was also updated.&lt;/p&gt;
    &lt;p&gt;Industry-wide, fast-food restaurants have seen traffic from one of its core customer bases, low-income households, drop by double digits, McDonald’s chief executive Christopher Kempczinski told investors last week. Meanwhile, traffic from higher earners increased by nearly as much, he said.&lt;/p&gt;
    &lt;p&gt;The struggle of the Golden Arches in particular — long synonymous with cheap food for the masses — reflects a larger trend upending the consumer economy and makes “affordability” a hot policy topic, experts say.&lt;/p&gt;
    &lt;p&gt;McDonald’s executives say the higher costs of restaurant essentials, such as beef and salaries, have pushed food prices up and driven away lower-income customers who already are being squeezed by the rising cost of groceries, clothes, rent and child care.&lt;/p&gt;
    &lt;p&gt;With prices for everything rising, consumer companies concerned about the pressures on low-income Americans include food, automotive and airline businesses, among others, analyst Adam Josephson said. “The list goes on and on,” he said.&lt;/p&gt;
    &lt;p&gt;“Happy Meals at McDonald’s are prohibitively expensive for some people, because there’s been so much inflation,” Josephson said.&lt;/p&gt;
    &lt;p&gt;Josephson and other economists say the shrinking traffic of low-income consumers is emblematic of a larger trend of Americans diverging in their spending, with wealthier customers flexing their purchasing power and lower-income shoppers pulling back — what some call a “K-shaped economy.”&lt;/p&gt;
    &lt;p&gt;U.S. economy struggles with rising prices&lt;/p&gt;
    &lt;p&gt;At hotel chains, luxury brands are holding up better than low-budget options. Revenue at brands including Four Seasons, Ritz-Carlton and St. Regis is up 2.9% this year, while economy hotels experienced a 3.1% decline for the same period, according to industry tracker CoStar.&lt;/p&gt;
    &lt;p&gt;“There are examples everywhere you look,” Josephson said.&lt;/p&gt;
    &lt;p&gt;Consumer credit delinquency rates show just how much low-income households are hurting, with households that make less than $45,000 annually experiencing “huge year-over-year increases,” even as delinquency rates for high- and middle-income households have flattened and stabilized, said Rikard Bandebo, chief strategy officer and chief economist at VantageScore.&lt;/p&gt;
    &lt;p&gt;After COVID-19-related stimulus programs ended, these households were the first to experience dramatically increased delinquency rates, and haven’t had a dip in delinquencies since 2022, according to data from VantageScore on 60-day, past-due delinquencies from January 2020 to September 2025. And although inflation has come down from its peak in 2022, people still are struggling with relatively higher prices and “astronomical” rent increases, Bandebo said.&lt;/p&gt;
    &lt;p&gt;A report released this year by researchers with Joint Center for Housing Studies at Harvard University found that half of all renters, 22.6 million people, were cost-burdened in 2023, meaning they spent more than 30% of their income on housing and utilities, up 3.2 percentage points since 2019 and 9 percentage points since 2001. Twenty-seven percent of renters are severely burdened, spending more than 50% of their income on housing.&lt;/p&gt;
    &lt;p&gt;As rents have grown, the amount families have left after paying for housing and utilities has fallen to record lows. In 2023, renters with annual household incomes under $30,000 had a median of just $250 per month in residual income to spend on other needs, an amount that’s fallen 55% since 2001, with the steepest declines since the pandemic, according to the Harvard study.&lt;/p&gt;
    &lt;p&gt;“It’s getting tougher and tougher every month for low-income households to make ends meet,” Bandebo said.&lt;/p&gt;
    &lt;p&gt;Mariam Gergis, a registered nurse at UCLA who also works a second job as a home caregiver, said she’s better off than many others, and still she struggles.&lt;/p&gt;
    &lt;p&gt;“I can barely afford McDonald’s,” she said. “But it’s a cheaper option.”&lt;/p&gt;
    &lt;p&gt;On Monday morning she sat in a booth at a McDonald’s in MacArthur Park with two others. The three beverages they ordered, two coffees and a soda, amounted to nearly $20, Gergis said, pointing to the receipt.&lt;/p&gt;
    &lt;p&gt;“I’d rather have healthier foods, but when you’re on a budget, it’s difficult,” she said.&lt;/p&gt;
    &lt;p&gt;Her brother, who works as a cashier, can’t afford meals out at all, she said. The cost of his diabetes medication has increased greatly, to about $200 a month, which she helps him cover.&lt;/p&gt;
    &lt;p&gt;“He would rather go hungry than eat outside,” Gergis said. The bank closed his credit cards due to nonpayment, she said, and he may lose his housing soon.&lt;/p&gt;
    &lt;p&gt;Prices at limited-service restaurants, which include fast-food restaurants, are up 3.2% year over year, at a rate higher than inflation “and that’s climbing,” said Marisa DiNatale, an economist at Moody’s Analytics.&lt;/p&gt;
    &lt;p&gt;Expect to hear President Trump talk a lot about affordability in coming months.&lt;/p&gt;
    &lt;p&gt;On top of that, price increases because of tariffs disproportionately affect lower-income households, because they spend a greater portion of their income on goods rather than services, which are not directly impacted by tariffs. Wages also are stagnating more for these households compared to higher- and middle-income households, DiNatale said.&lt;/p&gt;
    &lt;p&gt;“It has always been the case that more well-off people have done better. But a lot of the economic and policy headwinds are disproportionately affecting lower-income households, and [McDonald’s losing low-income customers] is a reflection of that,” DiNatale said.&lt;/p&gt;
    &lt;p&gt;It makes sense, then, that any price increases would hit these consumers hard.&lt;/p&gt;
    &lt;p&gt;According to a corporate fact sheet, from 2019 to 2024, the average cost of a McDonald’s menu item rose 40%. The average price of a Big Mac in 2019, for example, was $4.39, rising in 2024 to $5.29, according to the company. A 10-piece McNuggets Meal rose from $7.19 to $9.19 in the same time period.&lt;/p&gt;
    &lt;p&gt;The company says these increases are in line with the costs of running a restaurant — including soaring labor costs and high prices of beef and other goods.&lt;/p&gt;
    &lt;p&gt;Beef prices have skyrocketed, with inventory of the U.S. cattle herd at the lowest in 75 years because of the toll of drought and parasites. And exports of beef bound to the U.S. are down because of President Trump’s trade war and tariffs. As a result the prices of ground beef sold in supermarkets is up 13% in September, year-over-year.&lt;/p&gt;
    &lt;p&gt;McDonald’s also has placed blame on the meat-packing industry, accusing it of maneuvering to artificially inflate prices in a lawsuit filed last year against the industry’s “Big Four” companies — Tyson, JBS, Cargill and the National Beef Packing Company. The companies denied wrongdoing and paid tens of millions of dollars to settle lawsuits alleging price-fixing.&lt;/p&gt;
    &lt;p&gt;McDonald’s chief financial officer Ian Borden said on the recent earnings call that the company has managed to keep expenses from getting out of control.&lt;/p&gt;
    &lt;p&gt;“The strength of our supply chain means our beef costs are, I think, certainly up less than most,” he said.&lt;/p&gt;
    &lt;p&gt;McDonald’s did not disclose how the company gauges the income levels of fast-food customers, but businesses often analyze the market by estimating the background of their customers based on where they are shopping and what they are buying.&lt;/p&gt;
    &lt;p&gt;President Trump signed an executive order Friday eliminating tariffs on beef, coffee and tropical fruits to address rising consumer prices and grocery costs.&lt;/p&gt;
    &lt;p&gt;In California, the debate around fast-food prices has centered on labor costs, with legislation going into effect last year raising the minimum wage for fast-food workers at chains with more than 60 locations nationwide.&lt;/p&gt;
    &lt;p&gt;But more than a year after fast-food wages were boosted, the impact still is being debated, with economists divided and the fast-food industry and unions sparring over its impact.&lt;/p&gt;
    &lt;p&gt;Fast-food restaurant owners as well as trade associations like the International Franchise Assn., which spearheaded an effort to block the minimum wage boost, have said businesses have been forced to trim employee hours, institute hiring freezes or lay people off to offset the cost of higher wages.&lt;/p&gt;
    &lt;p&gt;Meanwhile, an analysis by researchers at UC Berkeley’s Center on Wage and Employment Dynamics of some 2,000 restaurants found the $20 wage did not reduce fast-food employment and “led to minimal menu price increases” of “about 8 cents on a $4 burger.”&lt;/p&gt;
    &lt;p&gt;Labor groups have argued that minimum wage increases give workers more purchasing power, helping to stimulate the economy.&lt;/p&gt;
    &lt;p&gt;California voters complain that canvassers for a measure to repeal a law expanding protections for fast-food workers lied about the effects.&lt;/p&gt;
    &lt;p&gt;McDonald’s said last year that spending by the company on restaurant worker salaries had grown around 40% since 2019, while costs for food, paper and other goods were up 35%.&lt;/p&gt;
    &lt;p&gt;The success of its Dollar Menu in the early 2000s was remarkable because it came amid complaints of the chain’s highly processed, high-calorie and high-fat products, food safety concerns and worker exploitation.&lt;/p&gt;
    &lt;p&gt;As the company marketed the Dollar Menu, which included the double cheeseburger, the McChicken sandwich, French fries, a hot fudge sundae and a 16-ounce soda, it also added healthier options to its regular menu, including salads and fruit.&lt;/p&gt;
    &lt;p&gt;But the healthier menu items did not drive the turnaround. The $1 double cheeseburgers brought in far more revenue than salads or the chicken sandwiches, which were priced from $3 to $4.50.&lt;/p&gt;
    &lt;p&gt;“The Dollar Menu appeals to lower-income, ethnic consumers,” Steve Levigne, vice president for United States business research at McDonald’s, told the New York Times in 2006. “It’s people who don’t always have $6 in their pocket.”&lt;/p&gt;
    &lt;p&gt;The Dollar Menu eventually became unsustainable, however. With inflation driving up prices, McDonald’s stores, particularly franchisee locations, struggled to afford it, and by November 2013 rebranded it as the “Dollar Menu &amp;amp; More” with prices up to $5.&lt;/p&gt;
    &lt;p&gt;Last year McDonald’s took a stab at appealing to cash-stretched customers with a $5 deal for a McDouble or McChicken sandwich, small fries, small soft drink and four-piece McNuggets. And in January it rolled out a deal offering a $1 menu item alongside an item bought for full price, with an ad starring John Cena, and launched Extra Value Meals in early September — offering combos costing 15% less than ordering each of the items separately.&lt;/p&gt;
    &lt;p&gt;The marketing didn’t seem to immediately resonate with customers, with McDonald’s in May reporting U.S. same-store sales in the recent quarter declined 3.6% from the year before. However, in its recent third-quarter earnings, the company reported a 2.4% lift in sales, even as its chief executive sounded the alarm about the increasingly two-tiered economy.&lt;/p&gt;
    &lt;p&gt;The iconic brand still has staying power, even with prices creeping up, some customers said.&lt;/p&gt;
    &lt;p&gt;“Everywhere prices are going up. This is the only place I do eat out, because it’s convenient,” said Ronald Mendez, 32, who said he lives about a block away from the McDonald’s in MacArthur Park.&lt;/p&gt;
    &lt;p&gt;D.T. Turner, 18, munched on hash browns and pancakes, with several still-wrapped McMuffins and cheeseburgers sitting on the tray between him and his friend. In total, their haul cost about $45, he said. He eats at McDonald’s several times a week.&lt;/p&gt;
    &lt;p&gt;“We grew up eating it,” Turner said.&lt;/p&gt;
    &lt;p&gt;His friend chimed in: “The breakfast is great, and nuggets are cool.”&lt;/p&gt;
    &lt;p&gt;The longest federal government shutdown in U.S. history appears to be nearing an end, but not without leaving a mark on an already-struggling economy&lt;/p&gt;
    &lt;p&gt;That other businesses also are reviving deals is a sign of the times. San Francisco-based burger chain Super Duper promoted its “recession combo” on social media. For $10, customers get fries, a drink and a “recession burger” at one of the chain’s 19 California locations.&lt;/p&gt;
    &lt;p&gt;What’s clear is companies are wary of passing along higher costs to customers, said DiNatale, of Moody’s Analytics.&lt;/p&gt;
    &lt;p&gt;“A lot of businesses are saying, ‘We just don’t think consumers will stand for this,’” DiNatale said. Consumers “have been through years of higher prices, and there’s just very little tolerance for higher prices going forward.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.latimes.com/business/story/2025-11-16/mcdonalds-is-losing-its-low-income-customers"/><published>2025-11-21T17:52:37+00:00</published></entry></feed>