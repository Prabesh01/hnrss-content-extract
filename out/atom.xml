<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-02T12:18:11.191282+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45413294</id><title>Tactility: An ESP32 OS</title><updated>2025-10-02T12:18:20.084906+00:00</updated><content>&lt;doc fingerprint="d37fffed7efd5e8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tactility.one"/><published>2025-09-29T13:11:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45415591</id><title>IP over Lasers</title><updated>2025-10-02T12:18:19.680784+00:00</updated><content>&lt;doc fingerprint="bd755010b88b32c2"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;IP Over Lasers&lt;/head&gt;
          &lt;p&gt;September 29, 2025&lt;/p&gt;
          &lt;p&gt;Introduction&lt;/p&gt;
          &lt;p&gt;A typical computer is connected to the local network using either an Ethernet cable or WiFi. This project connects two computers together through lasers.&lt;/p&gt;
          &lt;p&gt;This project takes advantage of the Linux support for a tun (tunnel) network device. Like any other network device, it gets its own IP address. The difference is, when network traffic is routed to it, a user land computer reads in the data and from there can do whatever it wants with it. Likewise, packets can be written to the tunnel device and the kernel will use it the same as if it came from a hardware network card.&lt;/p&gt;
          &lt;p&gt;Explanation, video, pictures, and source code below.&lt;/p&gt;
          &lt;p&gt;Related Projects @mikekohn.net&lt;/p&gt;
          &lt;p&gt;Video&lt;/p&gt;
          &lt;p&gt;YouTube: https://youtu.be/U-pM_YwO_94&lt;/p&gt;
          &lt;p&gt;Above is a video with a demo and short explanation of the project. One mistake I make while making the video, I was playing around with different baud rates while having some problems and thought I left it at 2400 baud. It's actually running here at 1200 baud, but it does for sure work at 2400 baud.&lt;/p&gt;
          &lt;p&gt;Explanation&lt;/p&gt;
          &lt;p&gt;Both computers (in this case a laptop and a Raspberry Pi 5) have USB-UART cables connected to them. Those cables are connected to two pins on ATtiny85s. The ATtiny85s on two other pins have a laser and a phototransitors. Each laser is pointed to the phototransistor on the opposite microcontroller.&lt;/p&gt;
          &lt;p&gt;Originally, the plan was to try to use a 38kHz IR sensor, so the lasers would flicker at 38kHz when they were on and light "noise" would therefore not affect the system. It actually seemed to detect the laser sometimes, but it was kind of obvious it wasn't going to work so instead a phototransistor is used.&lt;/p&gt;
          &lt;p&gt;I did use these lasers for other projects, including the Drag Racing Tree circuit and never used eye protection. Kind of realized that was probably a really stupid idea so I ordered some cheap $10 goggles. It made working on the project more difficult, but better to be safe than blind.&lt;/p&gt;
          &lt;p&gt;Using minicom, data could be transferred character by character over the lasers at 4800 baud. At 9600 baud only garbage was transfered. I was having trouble with the relay of IP data so I dropped it down to 2400 baud. There were some bugs in the code at that time so maybe it could be bumped back up to 4800. Either way, ping time isn't pretty and connecting using SSH takes... a minute or so.&lt;/p&gt;
          &lt;p&gt;ATtiny85&lt;/p&gt;
          &lt;p&gt;The code running on the ATtiny85 is extremely simple. The pins are set up like this:&lt;/p&gt;
          &lt;code&gt;
  ;; PB0: UART-TX (output)
  ;; PB1: UART-RX (input)
  ;; PB3: laser-out
  ;; PB4: light-in (reverse logic)

&lt;/code&gt;
          &lt;p&gt;From the UART cable, the TX pin is connected to the PB1 in and the RX pin is connected to the PB0 pin. Code does the following logic:&lt;/p&gt;
          &lt;code&gt;
Input:
  If PB1 is set as 1, PB3 is high (laser on).
  If PB1 is set as 0, PB3 is low  (laser off).

Output:
  If PB4 is 0 (the laser is on)  PB0 is high.
  If PB4 is 1 (the laser is off) PB1 is low.

&lt;/code&gt;
          &lt;p&gt;laser0 / tun0&lt;/p&gt;
          &lt;p&gt;The tun0 device is set up with the follow commands:&lt;/p&gt;
          &lt;code&gt;
Computer 1:
  sudo ip tuntap add mode tun dev laser0
  sudo ip addr add 192.168.3.101/24 dev laser0
  sudo ip link set dev laser0 up

Computer 2:
  sudo ip tuntap add mode tun dev laser0
  sudo ip addr add 192.168.3.100/24 dev laser0
  sudo ip link set dev laser0 up

&lt;/code&gt;
          &lt;p&gt;Each computer runs a program called relay. This program simply sits in a while loop that does simply this:&lt;/p&gt;
          &lt;code&gt;
  * Check tun0 for a packet. If packet is available read into buffer and
    send out UART.

  * Check UART for data. If data is available read in 4 bytes to see if
    it's 0xff 0xff and the next 2 bytes are number of bytes to read for
    the size of the packet. Read in the read of the packet and then transfer
    to tun0.

&lt;/code&gt;
          &lt;p&gt;Pictures&lt;/p&gt;
          &lt;p&gt;Above is a close up picture of the two circuits and the two lasers. Each circuit runs on 3v of battery which was nice since the battery holders had on/off switches. This allowed me to turn the lasers off if I needed to make some code changes.&lt;/p&gt;
          &lt;p&gt;And this is bigger picture. Between the two computers are the laser saftey goggles I used for eye protection.&lt;/p&gt;
          &lt;p&gt;Source Code&lt;/p&gt;
          &lt;p&gt;https://github.com/mikeakohn/small_projects/tree/main/ip_over_lasers&lt;/p&gt;
          &lt;p&gt; Copyright 1997-2025 - Michael Kohn &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.mikekohn.net/micro/ip_over_lasers.php"/><published>2025-09-29T16:19:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45418775</id><title>Understanding Cultural Differences: The Michigan Fish Test (2013)</title><updated>2025-10-02T12:18:19.446025+00:00</updated><content>&lt;doc fingerprint="7d17d6f499c427a5"&gt;
  &lt;main&gt;
    &lt;p&gt; Check out this image. What do you see? &lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Source: Richard Nisbett via CNN.com&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; In this article for CNN, Columbia Professor Sheena Iyengar describes how people of different cultures view this picture quite differently, and she explains what that tells about important cross-cultural distinctions. Iyengar is an expert on cross-cultural differences in decision-making processes. Here is an excerpt:&lt;/p&gt;
    &lt;p&gt; The image here, known in psychology as the Michigan Fish Test, was presented to American and Japanese participants in a study conducted by Richard Nisbett and Takahiko Masuda. In their five-second viewing, Americans paid more attention to the large fish, the "main characters" of the scene, while Japanese described the scene more holistically. For Americans, the large fish were the powerful agents, influencing everything around them. For Japanese, the environment dominated, interacting with and influencing all the characters. After the initial test, the researchers offered participants different versions of the fish picture, with some elements changed and some not. With the altered pictures, the Japanese were more likely to notice changes in the scenery or context. The Americans, on the other hand, proved adept at recognizing the large fish wherever they appeared, while the Japanese had more trouble recognizing the fish in new contexts, outside the original environment. So members of two different cultures--the more individualist Americans and the more collectivist Japanese--"saw" the pictures with differing emphasis on individuals, the environment, and how these elements interacted. The divergent accounts point to differing narratives of what controls what in the world, and how individual people fit into it.&lt;/p&gt;
    &lt;p&gt; For more on Iyengar's own research comparing how Japanese and American children approach choice, see this earlier blog post. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://michael-roberto.blogspot.com/2013/07/understanding-cultural-differences.html"/><published>2025-09-29T21:08:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45437759</id><title>F3: Open-source data file format for the future [pdf]</title><updated>2025-10-02T12:18:19.104117+00:00</updated><content/><link href="https://db.cs.cmu.edu/papers/2025/zeng-sigmod2025.pdf"/><published>2025-10-01T13:52:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45437893</id><title>Unix philosophy and filesystem access makes Claude Code amazing</title><updated>2025-10-02T12:18:18.813980+00:00</updated><content>&lt;doc fingerprint="370009d837378e9b"&gt;
  &lt;main&gt;
    &lt;p&gt;Noah Brier, September 30, 2025&lt;/p&gt;
    &lt;p&gt;If you've talked to me lately about AI, you've almost certainly been subject to a long soliloquy about the wonders of Claude Code. What started as a tool I ran in parallel with other tools to aid coding has turned into my full-fledged agentic operating system, supporting all kinds of workflows.&lt;/p&gt;
    &lt;p&gt;Most notably, Obsidian, the tool I use for note-taking. The difference between Obsidian and Notion or Evernote is that all the files are just plain old Markdown files stored on your computer. You can sync, style, and save them, but ultimately, it's still a text file on your hard drive. A few months ago, I realized that this fact made my Obsidian notes and research a particularly interesting target for AI coding tools. What first started with trying to open my vault in Cursor quickly moved to a sort of note-taking operating system that I grew so reliant on, I ended up standing up a server in my house so I could connect via SSH from my phone into my Claude Code + Obsidian setup and take notes, read notes, and think through things on the go.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, I went on Dan Shipper's AI &amp;amp; I Podcast to wax poetic about my love for this setup. I did a pretty deep dive into the system I use, how it works, why it works, etc. I won't retread all those details—you can read the transcript or listen to the podcast—but I want to talk about a few other things related to Claude Code that I've come to realize since the conversation.&lt;/p&gt;
    &lt;p&gt;I've really struggled to answer this question. I'm also not sure it's better than Cursor for all things, but I do think there are a set of fairly exceptional pieces that work together in concert to make me turn to Claude Code whenever I need to build anything these days. Increasingly, that's not even about applying it to existing codebases as much as it's building entirely new things on top of its functionality (more on that in a bit).&lt;/p&gt;
    &lt;p&gt;So what's the secret? Part of it lies in how Claude Code approaches tools. As a terminal-based application, it trades accessibility for something powerful: native Unix command integration. While I typically avoid long blockquotes, the Unix Philosophy deserves an exception—Doug McIlroy's original formulation captures it perfectly:&lt;/p&gt;
    &lt;p&gt;The Unix philosophy is documented by Doug McIlroy in the Bell System Technical Journal from 1978:&lt;/p&gt;
    &lt;p&gt;It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):&lt;/p&gt;
    &lt;p&gt;These fifty-year-old principles are exactly how LLMs want to use tools. If you look at how these models actually use the tools they're given, they are constantly "piping" output to input (albeit using their own fuzziness in between). (As an aside, the Unix | command allows you to string the output from one command into the input of another.) When models fail to weld their tools effectively, it is almost always because the tools are overly complex.&lt;/p&gt;
    &lt;p&gt;So part one of why Claude Code can be so mind-blowing is that the commands that power Unix happen to be perfectly suited for use by LLMs. This is both because they're simple and also incredibly well-documented, meaning the models had ample source material to teach them the literal ins and outs.&lt;/p&gt;
    &lt;p&gt;But that still wasn't the whole thing. The other piece was obviously Claude Code's ability to write code initially and, more recently, prose (for me, at least). But while other applications like ChatGPT and Claude can write output, there was something different going on here. Last week, while reading The Pragmatic Engineer's deep dive into how Claude Code is built. The answer was staring me in the face: filesystem access.&lt;/p&gt;
    &lt;p&gt;The filesystem changes everything. ChatGPT and Claude in the browser have two fatal flaws: no memory between conversations and a cramped context window. A filesystem solves both. Claude Code writes notes to itself, accumulates knowledge, and keeps running tallies. It has state and memory. It can think beyond a single conversation.&lt;/p&gt;
    &lt;p&gt;Back in 2022, when I first played with the GPT-3 API, I said that even if models never got better than they were in that moment, we would still have a decade to discover the use cases. They did get better—reasoning models made tool calling reliable—but the filesystem discovery proves my point.&lt;/p&gt;
    &lt;p&gt;I bring this up because in the Pragmatic Engineer interview, Boris Cherney, who built the initial version of Claude Code, uses it to describe the aha:&lt;/p&gt;
    &lt;p&gt;In AI, we talk about “product overhang”, and this is what we discovered with the prototype. Product overhang means that a model is able to do a specific thing, but the product that the AI runs in isn’t built in a way that captures this capability. What I discovered about Claude exploring the filesystem was pure product overhang. The model could already do this, but there wasn’t a product built around this capability!&lt;/p&gt;
    &lt;p&gt;Again, I'd argue it's filesystem + Unix commands, but the point is that the capability was there in the model just waiting to be woken up, and once it was, we were off to the races. Claude Code works as a blueprint for building reliable agentic systems because it captures model capabilities instead of limiting them through over-engineered interfaces.&lt;/p&gt;
    &lt;p&gt;I talked about my Claude Code + Obsidian setup, and I've actually taken it a step further by open-sourcing "Claudesidian," which pulls in a bunch of the tools and commands I use in my own Claude Code + Obsidian setup. It also goes beyond that and was a fun experimental ground for me. Most notably, I built an initial upgrade tool so that if changes are made centrally, you can pull them into your own Claudesidian, and the AI will help you check to see if you've made changes to the files being updated and, if so, attempt to smartly merge your changes with the new updates. Both projects follow the same Unix philosophy principles—simple, composable tools that do one thing well and work together. This is the kind of stuff that Claude Code makes possible, and why it's so exciting for me as a new way of building applications.&lt;/p&gt;
    &lt;p&gt;Speaking of which, one I'm not quite ready to release, but hopefully will be soon, is something I've been calling "Inbox Magic," though I'll surely come up with a better name. It's a Claude Code repo with access to a set of Gmail tools and a whole bunch of prompts and commands to effectively start operating like your own email EA. Right now, the functionality is fairly simple: it can obviously run searches or send emails on your behalf, but it can also do things like triage and actually run a whole training run on how you sound over email so it can more effectively draft emails for you. While Claude Code and ChatGPT both have access to my emails, they mostly grab one or two at a time. This system, because it can write things out to files and do lots of other fancy tricks, can perform a task like “find every single travel-related email in my inbox and use that to build a profile of my travel habits that I can use as a prompt to help ChatGPT/Claude do travel research that's actually aligned with my preferences.” Anyway, more on this soon, and if it's something you want to try out, ping me with your GitHub username, and as soon as I feel like I have something ready to test, I'll happily share it.&lt;/p&gt;
    &lt;p&gt;While I generally shy away from conclusions, I think there are a few here worth reiterating.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.alephic.com/writing/the-magic-of-claude-code"/><published>2025-10-01T14:05:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438346</id><title>Show HN: Autism Simulator</title><updated>2025-10-02T12:18:18.702843+00:00</updated><link href="https://autism-simulator.vercel.app/"/><published>2025-10-01T14:48:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438496</id><title>Building the heap: racking 30 petabytes of hard drives for pretraining</title><updated>2025-10-02T12:18:18.512027+00:00</updated><content>&lt;doc fingerprint="2fd91d7735ac7fcd"&gt;
  &lt;main&gt;
    &lt;p&gt;We built a storage cluster in downtown SF to store 90 million hours worth of video data. Why? We’re pretraining models to solve computer use. Compared to text LLMs like LLaMa-405B, which require ~60 TB of text data to train, videos are sufficiently large that we need 500 times more storage. Instead of paying the $12 million / yr it would cost to store all of this on AWS, we rented space from a colocation center in San Francisco to bring that cost down ~40x to $354k per year, including depreciation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why&lt;/head&gt;
    &lt;p&gt;Our use case for data is unique. Most cloud providers care highly about redundancy, availability, and data integrity, which tends to be unnecessary for ML training data. Since pretraining data is a commodity—we can lose any individual 5% with minimal impact—we can handle relatively large amounts of data corruption compared to enterprises who need guarantees that their user data isn’t going anywhere. In other words, we don’t need AWS’s 13 nines of reliability; 2 is more than enough.&lt;/p&gt;
    &lt;p&gt;Additionally, storage tends to be priced substantially above cost. Most companies use relatively small amounts of storage (even ones like Discord still use under a petabyte for messages), and the companies that use petabytes are so large that storage remains a tiny fraction of their total compute spend.&lt;/p&gt;
    &lt;p&gt;Data is one of our biggest contraints, and would be prohibitively expensive otherwise. As long as the cost predictions work out in favor of a local datacenter, and it would not consume too much of the core team’s time, it would make sense to stack hard drives ourselves. [1] 1. We talked to some engineers at the Internet Archive, which had basically the same problem as us; even after massive friends &amp;amp; family discounts on AWS, it was still 10 times more cost-effective to buy racks and store the data themselves!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost Breakdown: Cloud Alternatives vs In-House&lt;/head&gt;
    &lt;p&gt;Internet and electricity total $17.5k as our only recurring expenses (the price of colocation space, cooling, etc were bundled into electricity costs). One-time costs were dominated by hard drive capex. [2] 2. When deciding the datacenter location we had multiple options across the Bay Area, including options in Fremont through Hurricane Electric for around $10k in setup fees and $12.8k per month, saving us $38.5k initially and $4.7k per month, but ended up opting for a datacenter that was only a couple blocks from our office in SF. Though this came at a premium, it was extremely helpful to get the initial nodes setup and for ongoing maintenance. Our team is just 5 people, so any friction in going to the datacenter would come at a noticeable cost to team productivity.&lt;/p&gt;
    &lt;p&gt;Table 1: Cost comparison of cloud alternatives vs in-house. AWS is $1,130,000/month including estimated egress, Cloudflare is $270,000/month (with bulk-discounted pricing), and our datacenter is $29,500/month (including recurring costs and depreciation).&lt;/p&gt;
    &lt;head rend="h3"&gt;Monthly Recurring Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Internet&lt;/cell&gt;
        &lt;cell&gt;$7,500/month&lt;/cell&gt;
        &lt;cell&gt;100Gbps DIA from Zayo, 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Electricity&lt;/cell&gt;
        &lt;cell&gt;$10,000/month&lt;/cell&gt;
        &lt;cell&gt;1 kW/PB, $330/kW. Includes cabinet space &amp;amp; cooling. 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Monthly&lt;/cell&gt;
        &lt;cell&gt;$17,500/month&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;One-Time Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;Hard drives (HDDs)&lt;/cell&gt;
        &lt;cell&gt;$300,000&lt;/cell&gt;
        &lt;cell&gt;2,400 drives. Mostly 12TB used enterprise drives (3/4 SATA, 1/4 SAS). The JBOD DS4246s work for either.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage Infrastructure&lt;/cell&gt;
        &lt;cell&gt;NetApp DS4246 chassis&lt;/cell&gt;
        &lt;cell&gt;$35,000&lt;/cell&gt;
        &lt;cell&gt;100 dual SATA/SAS chassis, 4U each&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compute&lt;/cell&gt;
        &lt;cell&gt;CPU head nodes&lt;/cell&gt;
        &lt;cell&gt;$6,000&lt;/cell&gt;
        &lt;cell&gt;10 Intel RR2000s from eBay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Datacenter Setup&lt;/cell&gt;
        &lt;cell&gt;Install fee&lt;/cell&gt;
        &lt;cell&gt;$38,500&lt;/cell&gt;
        &lt;cell&gt;One-off datacenter install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Labor&lt;/cell&gt;
        &lt;cell&gt;Contractors&lt;/cell&gt;
        &lt;cell&gt;$27,000&lt;/cell&gt;
        &lt;cell&gt;Contractors to help physically screw in / install racks and wire cables&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Networking &amp;amp; Misc&lt;/cell&gt;
        &lt;cell&gt;Install expenses&lt;/cell&gt;
        &lt;cell&gt;$20,000&lt;/cell&gt;
        &lt;cell&gt;Power cables, 100GbE QSFP CX4 NICs, Arista router, copper jumpers, one-time internet install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total One-Time&lt;/cell&gt;
        &lt;cell&gt;$426,500&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Our price assuming three-year depreciation (including for the one-off install fees) is $17.5k/month in fixed monthly costs (internet, power, etc.) and $12k/month in depreciation, for $29.5k/month overall.&lt;/p&gt;
    &lt;p&gt;We compare our costs to two main providers: AWS’s public pricing numbers as a baseline, and Cloudflare’s discounted pricing for 30PB of storage. It’s important to note that AWS egress would be substantially lower if we utilized AWS GPUs. This is not reflected on our graph because AWS GPUs are priced at substantially above market prices and large clusters are difficult to attain, untenable at our compute scales.&lt;/p&gt;
    &lt;p&gt;Here are the pricing breakdowns:&lt;/p&gt;
    &lt;head rend="h3"&gt;AWS Pricing Breakdown&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Cost Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;$0.021/GB/month&lt;/cell&gt;
        &lt;cell&gt;$630,000&lt;/cell&gt;
        &lt;cell&gt;For data over 500TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Egress&lt;/cell&gt;
        &lt;cell&gt;$0.05/GB&lt;/cell&gt;
        &lt;cell&gt;$500,000&lt;/cell&gt;
        &lt;cell&gt;Entire dataset egressed quarterly (10 PB/month)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total AWS Monthly&lt;/cell&gt;
        &lt;cell&gt;$1,130,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare R2 Pricing&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Pricing Tier&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Published Rate&lt;/cell&gt;
        &lt;cell&gt;$0.015/GB/month&lt;/cell&gt;
        &lt;cell&gt;$450,000&lt;/cell&gt;
        &lt;cell&gt;No egress fees&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Estimated Private Pricing [3] 3. Cloudflare has a more reasonable estimate for the 30 PB, placing it at an overall monthly cost of $270k without egress fees. We also have bulk-discounted pricing estimates after getting pricing quotes—this was our main point of comparison for the datacenter.&lt;/cell&gt;
        &lt;cell&gt;$0.009/GB/month&lt;/cell&gt;
        &lt;cell&gt;$270,000&lt;/cell&gt;
        &lt;cell&gt;Estimated rate for &amp;gt;20 PB scale&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That brings monthly costs to $38/TB/month for AWS, $10/TB/month for Cloudflare, and $1/TB/month for our datacenter—about 38x lower and 10x lower respectively. (At the very cheapest end of the spectrum, Backblaze has a $6/TB product that is unsuitable for model training due to egress speed limitations; their $15/TB Overdrive AI-specific storage product is closer to Cloudflare’s in price &amp;amp; performance)&lt;/p&gt;
    &lt;p&gt;While we use Cloudflare as a comparison point, we’ve sometimes done too much load for their R2 servers. In particular, in the past we’ve done enough load during large model training runs that they rate-limited us, later confirming we were saturating their metadata layer and the rate limit wasn’t synthetic. Because our metadata on the heap is so simple, and we have a 100Gbps DIA connection, we haven’t ran into any issues there. [4] 4. We love Cloudflare and use many of their products often; we include this anecdote as a fact about our scale being difficult to handle, not as a dig!&lt;/p&gt;
    &lt;p&gt;This setup was and is necessary for our video data pipelines, and we’re extremely happy that we made this investment. By gathering large scale data at low costs, we can be competitive with frontier labs with billions of dollars in capital.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup/The Process&lt;/head&gt;
    &lt;p&gt;We cared a lot about getting this built fast, because this kind of project can easily stretch on for months if not careful. Hence Storage Stacking Saturday, or S3. We threw a hard drive stacking party in downtown SF and got our friends to come, offering food and custom-engraved hard drives to all who helped. The hard drive stacking started at 6am and continued for 36 hours (with a break to sleep), and by the end of that time we had 30 PB of functioning hardware racked and wired up. We brought in contractors for additional help and professional installation later on in the event.&lt;/p&gt;
    &lt;p&gt;People at the hard drive stacking party! Cool shots of the servers&lt;/p&gt;
    &lt;p&gt;Our software is 200 lines of Rust code for writing (to determine the drive to write data onto) and a nginx webserver for reading data, with a simple SQLite db for tracking metadata like which heap node each file is on and what data split it belongs to. We kept this obsessively simple instead of using MinIO or Ceph because we didn’t need any of the features they provided; it’s much, much simpler to debug a 200-line program than to debug Ceph, and we weren’t worried about redundancy or sharding. All our drives were formatted with XFS.&lt;/p&gt;
    &lt;p&gt;The storage software landscape offers many options, but every option available comes with drawbacks. People experienced with Ceph strongly warned us to avoid it unless we were willing to hire dedicated Ceph specialists—our research confirmed this advice. Ceph appears far more complex than justified for most use cases, only worthwhile for companies that absolutely need maximum performance and customizability and are prepared to invest heavily in tuning. Minio presents an interesting option if S3 compatibility is essential, but otherwise remains a bit too fancy for us and similar use-cases. Weka and Vast are absurdly expensive at 2k / TB / year or so and are primarily designed for NVMEs, not spinning disks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Post-Mortem&lt;/head&gt;
    &lt;p&gt;Building the datacenter was a large endeavor and we definitely learned lessons, both good and bad.&lt;/p&gt;
    &lt;head rend="h3"&gt;Things That We Got Correct&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We think the redundancy &amp;amp; capability tradeoffs we made are very reasonable at our disk speeds. We’re able to approximately saturate our 100G network for both read &amp;amp; write.&lt;/item&gt;
      &lt;item&gt;Doing this locally a couple blocks away was well worth it because of the amount of debugging and manual work needed.&lt;/item&gt;
      &lt;item&gt;Ebay is good to find vendors but bad to actually buy things with. After finding vendors, they can often individually supply all the parts we need and provide warranties, which are extremely valuable.&lt;/item&gt;
      &lt;item&gt;100G dedicated internet is pretty important, and much much easier to debug issues with than using cloud products.&lt;/item&gt;
      &lt;item&gt;Having high-quality cable management during the racking process saved us a ton of time debugging in the long run; making it easy to switch up the networking saved us a lot of headache.&lt;/item&gt;
      &lt;item&gt;We had a very strong simplicity prior, and this saved an immense amount of effort. We are quite happy that we didn’t use ceph or minio. Unlike e.g. nginx, they do not work out of the box. We were willing to write a simple Rust script and roughly saturated our network read &amp;amp; write at 100 Gbps without any fancy code.&lt;/item&gt;
      &lt;item&gt;We were basically right about the price and advantages this offered, and did not substantially overestimate the amount of time / effort it would take. While the improvements list is longer than this, most of those are minor; fundamentally we built a cluster rivaling massive clouds for 40x cheaper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Difficult Bits&lt;/head&gt;
    &lt;p&gt;A map of reality only gets you so far—while setting up the datacenter we ran into a couple problems and unexpected challenges. We’ll include a list:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We used frontloaders instead of toploaders for our server rack. This meant we had to screw every single individual drive in—tedious for 2.4k HDDs&lt;/item&gt;
      &lt;item&gt;Our storage was not dense—we could have saved 5x the work on physical placement and screwing by having a denser array of hard drives&lt;/item&gt;
      &lt;item&gt;Shortcuts like daisy-chaining are usually a bad idea. We could have gotten substantially higher read/write speeds without daisy chaining networked nodes, giving each chassis its own HBA (Host Bus Adapter, not a significant cost).&lt;/item&gt;
      &lt;item&gt;Compatibility is key—specifically in networking functionally everything is locked to a specific brand. We had many pain points here. Fiber transceivers will ~never work unless used with the right brand, but copper cables are much more forgiving. FS.com is pretty good and well priced (though their speed estimates were pretty inconsistent); Amazon will also often have the parts you need rapidly.&lt;/item&gt;
      &lt;item&gt;Networking came at substantial cost and required experimentation. In general, with our relatively non-sensitive training data, we optimized for convenience and ease of use over all else: we did not use DHCP as our used enterprise switches didn’t support it out of the box, and we didn’t use NAT as we wanted public IPs for the nodes for convenient and performant access from our servers. (We firewalled off unused ports and had basic security with nginx secure_link; we would not be able to do this if handling customer data, but it was fine for our use case.) While this is an area where we would have saved time with a cloud solution, we had our networking up within days and kinks ironed out within ~3 weeks.&lt;/item&gt;
      &lt;item&gt;We were often bottlenecked by easy access to servers via monitor/keyboard; idle crash carts during setup are helpful.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ideas Worth Trying&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Working KVMs are extremely useful, and you shouldn’t go without them or good IPMI. Physically going to a datacenter is really inconvenient, even if it’s a block away. IPMI is good, but only if you have pretty consistent machines.&lt;/item&gt;
      &lt;item&gt;Think through your management Ethernet network as much as your real network - it’s really nice to be able to SSH into servers while configuring the network, and IPMI is great!&lt;/item&gt;
      &lt;item&gt;Overprovision your network—e.g. if doable it’s worth having 400 Gigabit internally (you can use 100G cards etc for this!)&lt;/item&gt;
      &lt;item&gt;We could have substantially increased density at additional upfront cost by buying 90-drive SuperMicro SuperServers and putting 20TB drives into them. This would allow us to use 2 racks instead of 10, give us about the equivalent of 20 AMD 9654s in total CPU capacity, and use less total power.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How You Can Build This Yourself&lt;/head&gt;
    &lt;p&gt;Here’s what you need to replicate our setup.&lt;/p&gt;
    &lt;head rend="h3"&gt;Storage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;10 CPU head nodes.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Intel Rr2000 with Dual Intel Gold 6148 and 128GB of DDR4 ECC RAM per server (which are incredibly cheap and roughly worked for our use cases) but you have a lot of flexibility in what you use.&lt;/item&gt;
          &lt;item&gt;If you use the above configuration you likely won’t be able to do anything at all CPU-intensive on the servers (like on-device data processing or ZFS data compression / deduplication / etc, which is valuable if you’re storing non-video data).&lt;/item&gt;
          &lt;item&gt;Our CPU nodes cost $600 each—it seems quite reasonable to us to spend up to $3k each if you want ZFS / compression or the abiliy to do data processing on-CPU.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;100 DS4246 chassis—each can hold 24 hard drives.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2,400 3.5 inch HDDs—need to be all SATA or all SAS in each chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We would recommend SAS hard drives if possible [5] 5. if you use SAS drives you’ll need to deal with or disable mulipathing, which is reasonably simple as they roughly double speed over similar SATA drives.&lt;/item&gt;
          &lt;item&gt;We used a mix of 12TB and 14TB drives—basically any size should work, roughly the larger the better holding price constant (density makes stacking easier + in general increases resale value).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Physical parts to mount the chassis—you’ll need rails or l-brackets. We used l-brackets which worked well, as we haven’t needed to take the chassis out to slot hard drives. If you buy toploaders, you’ll need rails.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple “crash carts” with monitors and keyboards that allow you to physically connect to your CPU head nodes and configure them—this is invaluable when you’re debugging network issues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A 100 GbE switch&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;A used Arista is fine, should be QSFP28, should cost about $1-2k.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HBAs (Host Bus Adapters), which connect your head nodes to your DS4246 chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The best configuration we tried was with Broadcom 9305-16E HBAs, with 3x HBAs per server (make sure your server has physical space for them!) with SFF-8644 to QSFP mini SAS cables.&lt;/item&gt;
          &lt;item&gt;There are 4 slots per HBA, so you can cable each DS4246 chassis directly to the HBA. [6] 6. The option we ended up going with for convenience was putting LSI SAS9207-8e HBAs, which have 2 ports each, into the CPU head nodes- then daisy-chaining the DS4246s together with QSFP+ to QSFP+ DACs.. We deployed this on Storage Stacking Saturday, then while debugging speeds tried the above method on one of the servers and got to ~4 Gbps per chassis-but didn’t find it worth it to swap everything out in pure labor because of the way we had set up some of our head nodes such that they were difficult to take out. Insofar as it is reasonably cheap to just do the above thing to start and we’ve tested it to work, you should probably do as we say, not as we did in this case!&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network cards (NICs).&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Mellanox ConnectX-4 100GbE. Make sure they come in Ethernet mode and not Infiniband mode for ease of config.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DAC (Direct Attach Copper) or AOC (Active Optical) cables, to connect the NICs in your head nodes to your switch and therefore the internet. You almost certainly want DACs if your racks are close together, as they are far more compatible with arbitrary networking equipment than AOCs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We would recommend that you find a supplier to sell you the CPU head nodes with the HBAs and NICs installed—there are a number of used datacenter / enterprise parts suppliers who are willing to do this. This is a substantial positive because it means that you don’t have to spend hours installing the HBAs/NICs yourself and can have a substantially higher degree of confidence in your operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Serial cables—you’ll need these to connect to your switch!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Optional but recommended: an Ethernet management network of some kind. If you can’t easily get ethernet, we’d recommend getting a wifi adapter like this and then a ethernet switch like this —it’s substantially easier to set up than the 100GbE, is a great backup for when that’s not working, and will allow you to do ~everything over SSH from the comfort of the office instead of in the datacenter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Datacenter Requirements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3.5 kW of usable power per cabinet, with 10 4U chassis + 1 2U (cabinets are 42U tall)&lt;/item&gt;
      &lt;item&gt;1 spare cabinet for the 1U or 2U 100GbE switch (you can obviously also just swap out one of the 4U chassis in another cabinet for the switch).&lt;/item&gt;
      &lt;item&gt;1 42U cabinet per 3 PB of storage&lt;/item&gt;
      &lt;item&gt;A dedicated 100G connection (will come in as a fiber pair probably via QSFP28 LR4, but confirm with your datacenter provider before buying parts here!)&lt;/item&gt;
      &lt;item&gt;Ideally physically near your office—there is a lot of value in being able to walk over and debug issues instead of e.g. dealing with remote hands services to get internet to the nodes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some setup tips:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure to first properly configure your switch. Depending on your switch model this should be relatively straightforward—you’ll need to physically connect to the switch and then configure the specific port that your 100GbE is connected to (you’ll get a fiber cross-connect from your datacenter that you should plug into a QSFP28 transceiver. Make sure that you get a transceiver that is compatible in form with the ISP, probably LR4, and specifically branded with your switch brand, otherwise it is very unlikely to work). Depending on your ISP you might have to talk to them to make sure that you can get “light” through the fiber cables from both ends, which might involve rolling the fiber and otherwise making sure it’s working properly. &lt;list rend="ul"&gt;&lt;item&gt;If your switch isn’t working / you haven’t configured one before, I’d suggest trying to directly plug the fiber cable from the ISP into one of your 10 heap servers, making sure to buy a transceiver that is compatible with your NIC brand (e.g. Mellanox). Once you get it working from there, move over to your switch and get it working.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Once you can connect to the internet from your switch (simply ping 1.1.1.1 to check) you are ready to set up the netplans for the individual nodes. this is most easily done during the Ubuntu setup process, which will walk you through setting up internet for your CPU head nodes, but is also doable outside of that&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you have internet access to your nodes and have properly connected 1 cable to each DS4246, you should format &amp;amp; mount the drives on each node, test that all of them are properly working, and then you are ready to deploy any software you want.&lt;/p&gt;
    &lt;p&gt;If you end up building a similar storage cluster based on this writeup we’d love to hear from you—we’re very curious what can be improved, both in our guidance and in the object-level process. You can reach us at [email protected]&lt;/p&gt;
    &lt;p&gt;If you came away from this post excited about our work, we’d love to chat. We’re a research lab currently focused on pretraining models to use computers, with the long-term goal of building general models that can learn in-context and do arbitrary tasks while aligned with human values; we’re hiring top researchers and engineers to help us train these. If you’re interested in chatting, shoot us an email at [email protected].&lt;/p&gt;
    &lt;head rend="h3"&gt;Collaborators&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Neel Redkar&lt;/item&gt;
      &lt;item&gt;Devansh Pandey&lt;/item&gt;
      &lt;item&gt;Nicholas Charette&lt;/item&gt;
      &lt;item&gt;Galen Mead&lt;/item&gt;
      &lt;item&gt;Yudhister Kumar&lt;/item&gt;
      &lt;item&gt;Robert Avery&lt;/item&gt;
      &lt;item&gt;Raj Thimmiah&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://si.inc/posts/the-heap/"/><published>2025-10-01T15:00:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438503</id><title>Ask HN: Who is hiring? (October 2025)</title><updated>2025-10-02T12:18:17.582470+00:00</updated><content>&lt;doc fingerprint="3651195a341ae364"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss these other fine threads:&lt;/p&gt;&lt;p&gt;Who wants to be hired? https://news.ycombinator.com/item?id=45438501&lt;/p&gt;&lt;p&gt;Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45438502&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45438503"/><published>2025-10-01T15:01:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45439670</id><title>Gmail will no longer support checking emails from third-party accounts via POP</title><updated>2025-10-02T12:18:17.265938+00:00</updated><content>&lt;doc fingerprint="fda843131f22facd"&gt;
  &lt;main&gt;&lt;p&gt;Starting January 2026, Gmail will no longer provide support for the following:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Gmailify: This feature allows you to get special features like spam protection or inbox organization applied to your third-party email account. Learn more about Gmailify.&lt;/item&gt;&lt;item&gt;POP: Unlike IMAP connections, with POP, emails are downloaded, and you decide how often you want to download new emails. As an alternative, you can still link your third-party accounts in the Gmail app.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;These changes help provide the most secure and current options to access your messages in Gmail.&lt;/p&gt;&lt;head rend="h2"&gt;Learn about changes to Gmailify&lt;/head&gt;&lt;p&gt;You won’t be able to get specific features in Gmail applied to your third-party account, like:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Spam protection&lt;/item&gt;&lt;item&gt;Better email notifications on mobile&lt;/item&gt;&lt;item&gt;Inbox categories&lt;/item&gt;&lt;item&gt;Faster search with advanced search operators&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;What you need to do&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;You can still read and send emails from your other account within the Gmail app. This uses a standard IMAP connection, which is supported in the Gmail mobile app.&lt;/item&gt;&lt;item&gt;Learn how to add another email account to the Gmail app.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Learn about changes to POP connections&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Gmail will no longer support checking emails from third-party accounts through POP.&lt;/item&gt;&lt;item&gt;The option to "Check mail from other accounts" will no longer be available in Gmail on your computer.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;What you need to do&lt;/head&gt;&lt;p&gt;Important: If you have a work or school account, your administrator can help migrate your email data into Google Workspace. Learn more about the data migration service.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;To continue to receive messages from your other account in Gmail, you need to set up IMAP access. &lt;list rend="ul"&gt;&lt;item&gt;Check your email provider’s documentation for instructions on how to enable IMAP for your account.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;To read your messages from your other account, use the Gmail app. Learn how to add another email account to the Gmail app.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;Will I lose the emails I already imported?&lt;p&gt;No. All messages synced before the deprecation stay in Gmail.&lt;/p&gt;&lt;p&gt;Yes. For third-party accounts like Yahoo! and Outlook, you can add them to the Gmail mobile app on Android and iPhone and iPad.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://support.google.com/mail/answer/16604719?hl=en"/><published>2025-10-01T16:25:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45439997</id><title>The RAG Obituary: Killed by agents, buried by context windows</title><updated>2025-10-02T12:18:16.891692+00:00</updated><content>&lt;doc fingerprint="3740cad20bcd8702"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The RAG Obituary: Killed by Agents, Buried by Context Windows&lt;/head&gt;
    &lt;head rend="h3"&gt;Why Retrieval-Augmented Generation Won’t Survive the Context Revolution and the End of Chunking, Embeddings, and Rerankers as We Know Them.&lt;/head&gt;
    &lt;p&gt;I’ve been working in AI and search for a decade. First building Doctrine, the largest European legal search engine and now building Fintool, an AI-powered financial research platform that helps institutional investors analyze companies, screen stocks, and make investment decisions.&lt;/p&gt;
    &lt;p&gt;After three years of building, optimizing, and scaling LLMs with retrieval-augmented generation (RAG) systems, I believe we’re witnessing the twilight of RAG-based architectures. As context windows explode and agent-based architectures mature, my controversial opinion is that the current RAG infrastructure we spent so much time building and optimizing is on the decline.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Rise of Retrieval-Augmented Generation&lt;/head&gt;
    &lt;p&gt;In late 2022, ChatGPT took the world by storm. People started endless conversations, delegating crucial work only to realize that the underlying model, GPT-3.5 could only handle 4,096 tokens... roughly six pages of text!&lt;/p&gt;
    &lt;p&gt;The AI world faced a fundamental problem: how do you make an intelligent system work with knowledge bases that are orders of magnitude larger than what it can read at once?&lt;/p&gt;
    &lt;p&gt;The answer became Retrieval-Augmented Generation (RAG), an architectural pattern that would dominate AI for the next three years.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Mathematical Reality of Early LLMs&lt;/head&gt;
    &lt;p&gt;GPT-3.5 could handle 4,096 token and the next model GPT-4 doubled it to 8,192 tokens, about twelve pages. This wasn’t just inconvenient; it was architecturally devastating.&lt;/p&gt;
    &lt;p&gt;Consider the numbers: A single SEC 10-K filing contains approximately 51,000 tokens (130+ pages).&lt;/p&gt;
    &lt;p&gt;With 8,192 tokens, you could see less than 16% of a 10-K filing. It’s like reading a financial report through a keyhole!&lt;/p&gt;
    &lt;head rend="h3"&gt;The RAG Architecture: A Technical Deep Dive&lt;/head&gt;
    &lt;p&gt;RAG emerged as an elegant solution borrowed directly from search engines. Just as Google displays 10 blue links with relevant snippets for your query, RAG retrieves the most pertinent document fragments and feeds them to the LLM for synthesis.&lt;/p&gt;
    &lt;p&gt;The core idea is beautifully simple: if you can’t fit everything in context, find the most relevant pieces and use those. It turns LLMs into sophisticated search result summarizers.&lt;/p&gt;
    &lt;p&gt;Basically, LLMs can’t read the whole book but they can know who dies at the end; convenient!&lt;/p&gt;
    &lt;head rend="h4"&gt;The Chunking Challenge&lt;/head&gt;
    &lt;p&gt;Long documents need to be chunked into pieces and it’s when problems start. Those digestible pieces are typically 400-1,000 tokens each which is basically 300-750 words.&lt;/p&gt;
    &lt;p&gt;The problem? It isn’t as simple as cutting every 500 words.&lt;/p&gt;
    &lt;p&gt;Consider chunking a typical SEC 10-K annual report. The document has a complex hierarchical structure:&lt;/p&gt;
    &lt;p&gt;- Item 1: Business Overview (10-15 pages)&lt;/p&gt;
    &lt;p&gt;- Item 1A: Risk Factors (20-30 pages)&lt;/p&gt;
    &lt;p&gt;- Item 7: Management’s Discussion and Analysis (30-40 pages)&lt;/p&gt;
    &lt;p&gt;- Item 8: Financial Statements (40-50 pages)&lt;/p&gt;
    &lt;p&gt;After naive chunking at 500 tokens, critical information gets scattered:&lt;/p&gt;
    &lt;p&gt;- Revenue recognition policies split across 3 chunks&lt;/p&gt;
    &lt;p&gt;- A risk factor explanation broken mid-sentence&lt;/p&gt;
    &lt;p&gt;- Financial table headers separated from their data&lt;/p&gt;
    &lt;p&gt;- MD&amp;amp;A narrative divorced from the numbers it’s discussing&lt;/p&gt;
    &lt;p&gt;If you search for “revenue growth drivers,” you might get a chunk mentioning growth but miss the actual numerical data in a different chunk, or the strategic context from MD&amp;amp;A in yet another chunk!&lt;/p&gt;
    &lt;p&gt;At Fintool, we’ve developed sophisticated chunking strategies that go beyond naive text splitting:&lt;/p&gt;
    &lt;p&gt;- Hierarchical Structure Preservation: We maintain the nested structure from Item 1 (Business) down to sub-sections like geographic segments, creating a tree-like document representation&lt;/p&gt;
    &lt;p&gt;- Table Integrity: Financial tables are never split—income statements, balance sheets, and cash flow statements remain atomic units with headers and data together&lt;/p&gt;
    &lt;p&gt;- Cross-Reference Preservation: We maintain links between narrative sections and their corresponding financial data, preserving the “See Note X” relationships&lt;/p&gt;
    &lt;p&gt;- Temporal Coherence: Year-over-year comparisons and multi-period analyses stay together as single chunks&lt;/p&gt;
    &lt;p&gt;- Footnote Association: Footnotes remain connected to their referenced items through metadata linking&lt;/p&gt;
    &lt;p&gt;Each chunk at Fintool is enriched with extensive metadata:&lt;/p&gt;
    &lt;p&gt;- Filing type (10-K, 10-Q, 8-K)&lt;/p&gt;
    &lt;p&gt;- Fiscal period and reporting date&lt;/p&gt;
    &lt;p&gt;- Section hierarchy (Item 7 &amp;gt; Liquidity &amp;gt; Cash Position)&lt;/p&gt;
    &lt;p&gt;- Table identifiers and types&lt;/p&gt;
    &lt;p&gt;- Cross-reference mappings&lt;/p&gt;
    &lt;p&gt;- Company identifiers (CIK, ticker)&lt;/p&gt;
    &lt;p&gt;- Industry classification codes&lt;/p&gt;
    &lt;p&gt;This allows for more accurate retrieval but even our intelligent chunking can’t solve the fundamental problem: we’re still working with fragments instead of complete documents!&lt;/p&gt;
    &lt;p&gt;Once you have the chunks, you need a way to search them. One way is to embed your chunks.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Embedding and Retrieval Pipeline&lt;/head&gt;
    &lt;p&gt;Each chunk is converted into a high‑dimensional vector (typically 1,536 dimensions in most embedding models). These vectors live in a space where, theoretically, similar concepts are close together.&lt;/p&gt;
    &lt;p&gt;When a user asks a question, that question also becomes a vector. The system finds the chunks whose vectors are closest to the query vector using cosine similarity.&lt;/p&gt;
    &lt;p&gt;It’s elegant in theory and in practice, it’s a nightmare of edge cases.&lt;/p&gt;
    &lt;p&gt;Embedding models are trained on general text and struggle with specific terminologies. They find similarities but they can’t distinguish between “revenue recognition” (accounting policy) and “revenue growth” (business performance).&lt;/p&gt;
    &lt;p&gt;Consider that example: Query: “What is the company’s litigation exposure?&lt;/p&gt;
    &lt;p&gt;RAG searches for “litigation” and returns 50 chunks:&lt;/p&gt;
    &lt;p&gt;- Chunks 1-10: Various mentions of “litigation” in boilerplate risk factors&lt;/p&gt;
    &lt;p&gt;- Chunks 11-20: Historical cases from 2019 (already settled)&lt;/p&gt;
    &lt;p&gt;- Chunks 21-30: Forward-looking safe harbor statements&lt;/p&gt;
    &lt;p&gt;- Chunks 31-40: Duplicate descriptions from different sections&lt;/p&gt;
    &lt;p&gt;- Chunks 41-50: Generic “we may face litigation” warnings&lt;/p&gt;
    &lt;p&gt;What RAG Reports: $500M in litigation (from Legal Proceedings section)&lt;/p&gt;
    &lt;p&gt;What’s Actually There:&lt;/p&gt;
    &lt;p&gt;- $500M in Legal Proceedings (Item 3)&lt;/p&gt;
    &lt;p&gt;- $700M in Contingencies note (”not material individually”)&lt;/p&gt;
    &lt;p&gt;- $1B new class action in Subsequent Events&lt;/p&gt;
    &lt;p&gt;- $800M indemnification obligations (different section)&lt;/p&gt;
    &lt;p&gt;- $2B probable losses in footnotes (keyword “probable” not “litigation”)&lt;/p&gt;
    &lt;p&gt;The actual Exposure is $5.1B. 10x what RAG found. Oupsy! By late 2023, most builders realized pure vector search wasn’t enough.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hybrid Search: The Complexity That Actually Works&lt;/head&gt;
    &lt;p&gt;Enter hybrid search: combine semantic search (embeddings) with the traditional keyword search (BM25). This is where things get interesting.&lt;/p&gt;
    &lt;p&gt;BM25 (Best Matching 25) is a probabilistic retrieval model that excels at exact term matching. Unlike embeddings, BM25:&lt;/p&gt;
    &lt;p&gt;- Rewards Exact Matches: When you search for “EBITDA,” you get documents with “EBITDA,” not “operating income” or “earnings”&lt;/p&gt;
    &lt;p&gt;- Handles Rare Terms Better: Financial jargon like “CECL” (Current Expected Credit Losses) or “ASC 606” gets proper weight&lt;/p&gt;
    &lt;p&gt;- Document Length Normalization: Doesn’t penalize longer documents&lt;/p&gt;
    &lt;p&gt;- Term Frequency Saturation: Multiple mentions of “revenue” don’t overshadow other important terms&lt;/p&gt;
    &lt;p&gt;At Fintool, we’ve built a sophisticated hybrid search system:&lt;/p&gt;
    &lt;p&gt;1. Parallel Processing: We run semantic and keyword searches simultaneously&lt;/p&gt;
    &lt;p&gt;2. Dynamic Weighting: Our system adjusts weights based on query characteristics:&lt;/p&gt;
    &lt;p&gt;- Specific financial metrics? BM25 gets 70% weight&lt;/p&gt;
    &lt;p&gt;- Conceptual questions? Embeddings get 60% weight&lt;/p&gt;
    &lt;p&gt;- Mixed queries? 50/50 split with result analysis&lt;/p&gt;
    &lt;p&gt;3. Score Normalization: Different scoring scales are normalized using:&lt;/p&gt;
    &lt;p&gt;- Min-max scaling for BM25 scores&lt;/p&gt;
    &lt;p&gt;- Cosine similarity already normalized for embeddings&lt;/p&gt;
    &lt;p&gt;- Z-score normalization for outlier handling&lt;/p&gt;
    &lt;p&gt;So at the end the embeddings search and the keywords search retrieve chunks and the search engine combines them using Reciprocal Rank Fusion. RRF merges rankings so items that consistently appear near the top across systems float higher, even if no system put them at #1!&lt;/p&gt;
    &lt;p&gt;So now you think it’s done right? But hell no!&lt;/p&gt;
    &lt;head rend="h4"&gt;The Reranking Bottleneck: RAG’s Dirty Secret&lt;/head&gt;
    &lt;p&gt;Here’s what nobody talks about: even after all that retrieval work, you’re not done. You need to rerank the chunks one more time to get a good retrieval and it’s not easy. Rerankers are ML models that take the search results and reorder them by relevance to your specific query limiting the number of chunks sent to the LLM.&lt;/p&gt;
    &lt;p&gt;Not only LLMs are context poor, they also struggle when dealing with too much information. It’s vital to reduce the number of chunks sent to the LLM for the final answer.&lt;/p&gt;
    &lt;p&gt;The Reranking Pipeline:&lt;/p&gt;
    &lt;p&gt;1. Initial search retrieval with embeddings + keywords gets you 100-200 chunks&lt;/p&gt;
    &lt;p&gt;2. Reranker ranks the top 10&lt;/p&gt;
    &lt;p&gt;3. Top 10 are fed to the LLM to answer the question&lt;/p&gt;
    &lt;p&gt;Here is the challenge with reranking:&lt;/p&gt;
    &lt;p&gt;- Latency Explosion: Rerank adds between 300-2000ms per query. Ouch.&lt;/p&gt;
    &lt;p&gt;- Cost Multiplication: it adds significant extra cost to every query. For instance, Cohere Rerank 3.5 costs $2.00 per 1,000 search units, making reranking expensive.&lt;/p&gt;
    &lt;p&gt;- Context Limits: Rerankers typically handle few chunks (Cohere Rerank supports only 4096 tokens), so if you need to re-rank more than that, you have to split it into different parallel API calls and merge them!&lt;/p&gt;
    &lt;p&gt;- Another Model to Manage: One more API, one more failure point&lt;/p&gt;
    &lt;p&gt;Re-rank is one more step in a complex pipeline.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Infrastructure Burden of Traditional RAG&lt;/head&gt;
    &lt;p&gt;What I find difficult with RAG is what I call the “cascading failure problem”.&lt;/p&gt;
    &lt;p&gt;1. Chunking can fail (split tables) or be too slow (especially when you have to ingest and chunk gigabytes of data in real-time)&lt;/p&gt;
    &lt;p&gt;2. Embedding can fail (wrong similarity)&lt;/p&gt;
    &lt;p&gt;3. BM25 can fail (term mismatch)&lt;/p&gt;
    &lt;p&gt;4. Hybrid fusion can fail (bad weights)&lt;/p&gt;
    &lt;p&gt;5. Reranking can fail (wrong priorities)&lt;/p&gt;
    &lt;p&gt;Each stage compounds the errors of the previous stage. Beyond the complexity of hybrid search itself, there’s an infrastructure burden that’s rarely discussed.&lt;/p&gt;
    &lt;p&gt;Running production Elasticsearch is not easy. You’re looking at maintaining TB+ of indexed data for comprehensive document coverage, which requires 128-256GB RAM minimum just to get decent performance. The real nightmare comes with re-indexing. Every schema change forces a full re-indexing that takes 48-72 hours for large datasets. On top of that, you’re constantly dealing with cluster management, sharding strategies, index optimization, cache tuning, backup and disaster recovery, and version upgrades that regularly include breaking changes.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Fundamental Limitations of RAG for Complex Documents&lt;/head&gt;
    &lt;p&gt;Here are some structural limitations:&lt;/p&gt;
    &lt;p&gt;1. Context Fragmentation&lt;/p&gt;
    &lt;p&gt;- Long documents are interconnected webs, not independent paragraphs&lt;/p&gt;
    &lt;p&gt;- A single question might require information from 20+ documents&lt;/p&gt;
    &lt;p&gt;- Chunking destroys these relationships permanently&lt;/p&gt;
    &lt;p&gt;2. Semantic Search Fails on Numbers&lt;/p&gt;
    &lt;p&gt;- “$45.2M” and “$45,200,000” have different embeddings&lt;/p&gt;
    &lt;p&gt;- “Revenue increased 10%” and “Revenue grew by a tenth” rank differently&lt;/p&gt;
    &lt;p&gt;- Tables full of numbers have poor semantic representations&lt;/p&gt;
    &lt;p&gt;3. No Causal Understanding&lt;/p&gt;
    &lt;p&gt;- RAG can’t follow “See Note 12” → Note 12 → Schedule K&lt;/p&gt;
    &lt;p&gt;- Can’t understand that discontinued operations affect continuing operations&lt;/p&gt;
    &lt;p&gt;- Can’t trace how one financial item impacts another&lt;/p&gt;
    &lt;p&gt;4. The Vocabulary Mismatch Problem&lt;/p&gt;
    &lt;p&gt;- Companies use different terms for the same concept&lt;/p&gt;
    &lt;p&gt;- “Adjusted EBITDA” vs “Operating Income Before Special Items”&lt;/p&gt;
    &lt;p&gt;- RAG retrieves based on terms, not concepts&lt;/p&gt;
    &lt;p&gt;5. Temporal Blindness&lt;/p&gt;
    &lt;p&gt;- Can’t distinguish Q3 2024 from Q3 2023 reliably&lt;/p&gt;
    &lt;p&gt;- Mixes current period with prior period comparisons&lt;/p&gt;
    &lt;p&gt;- No understanding of fiscal year boundaries&lt;/p&gt;
    &lt;p&gt;These aren’t minor issues. They’re fundamental limitations of the retrieval paradigm.&lt;/p&gt;
    &lt;p&gt;Three months ago I stumbled on an innovation on retrievial that blew my mind&lt;/p&gt;
    &lt;head rend="h2"&gt;The Emergence of Agentic Search - A New Paradigm&lt;/head&gt;
    &lt;p&gt;In May 2025, Anthropic released Claude Code, an AI coding agent that works in the terminal. At first, I was surprised by the form factor. A terminal? Are we back in 1980? no UI?&lt;/p&gt;
    &lt;p&gt;Back then, I was using Cursor, a product that excelled at traditional RAG. I gave it access to my codebase to embed my files and Cursor ran a search n my codebase before answering my query. Life was good. But when testing Claude Code, one thing stood out:&lt;/p&gt;
    &lt;p&gt;It was better and faster and not because their RAG was better but because there was no RAG.&lt;/p&gt;
    &lt;head rend="h4"&gt;How Claude Code Search Works&lt;/head&gt;
    &lt;p&gt;Instead of a complex pipeline of chunking, embedding, and searching, Claude Code uses direct filesystem tools:&lt;/p&gt;
    &lt;p&gt;1. Grep (Ripgrep)&lt;/p&gt;
    &lt;p&gt;- Lightning-fast regex search through file contents&lt;/p&gt;
    &lt;p&gt;- No indexing required. It searches live files instantly&lt;/p&gt;
    &lt;p&gt;- Full regex support for precise pattern matching&lt;/p&gt;
    &lt;p&gt;- Can filter by file type or use glob patterns&lt;/p&gt;
    &lt;p&gt;- Returns exact matches with context lines&lt;/p&gt;
    &lt;p&gt;2. Glob&lt;/p&gt;
    &lt;p&gt;- Direct file discovery by name patterns&lt;/p&gt;
    &lt;p&gt;- Finds files like `**/*.py` or `src/**/*.ts` instantly&lt;/p&gt;
    &lt;p&gt;- Returns files sorted by modification time (recency bias)&lt;/p&gt;
    &lt;p&gt;- Zero overhead—just filesystem traversal&lt;/p&gt;
    &lt;p&gt;3. Task Agents&lt;/p&gt;
    &lt;p&gt;- Autonomous multi-step exploration&lt;/p&gt;
    &lt;p&gt;- Handle complex queries requiring investigation&lt;/p&gt;
    &lt;p&gt;- Combine multiple search strategies adaptively&lt;/p&gt;
    &lt;p&gt;- Build understanding incrementally&lt;/p&gt;
    &lt;p&gt;- Self-correct based on findings&lt;/p&gt;
    &lt;p&gt;By the way, Grep was invented in 1973. It’s so... primitive. And that’s the genius of it.&lt;/p&gt;
    &lt;p&gt;Claude Code doesn’t retrieve. It investigates:&lt;/p&gt;
    &lt;p&gt;- Runs multiple searches in parallel (Grep + Glob simultaneously)&lt;/p&gt;
    &lt;p&gt;- Starts broad, then narrows based on discoveries&lt;/p&gt;
    &lt;p&gt;- Follows references and dependencies naturally&lt;/p&gt;
    &lt;p&gt;- No embeddings, no similarity scores, no reranking&lt;/p&gt;
    &lt;p&gt;It’s simple, it’s fast and it’s based on a new assumption that LLMs will go from context poor to context rich.&lt;/p&gt;
    &lt;p&gt;Claude Code proved that with sufficient context and intelligent navigation, you don’t need RAG at all. The agent can:&lt;/p&gt;
    &lt;p&gt;- Load entire files or modules directly&lt;/p&gt;
    &lt;p&gt;- Follow cross-references in real-time&lt;/p&gt;
    &lt;p&gt;- Understand structure and relationships&lt;/p&gt;
    &lt;p&gt;- Maintain complete context throughout investigation&lt;/p&gt;
    &lt;p&gt;This isn’t just better than RAG—it’s a fundamentally different paradigm. And what works for code can work for any long documents that are not coding files.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Context Revolution: From Scarcity to Abundance&lt;/head&gt;
    &lt;p&gt;The context window explosion made Claude Code possible:&lt;/p&gt;
    &lt;p&gt;2022-2025 Context-Poor Era:&lt;/p&gt;
    &lt;p&gt;- GPT-4: 8K tokens (~12 pages)&lt;/p&gt;
    &lt;p&gt;- GPT-4-32k: 32K tokens (~50 pages)&lt;/p&gt;
    &lt;p&gt;2025 and beyond Context Revolution:&lt;/p&gt;
    &lt;p&gt;- Claude Sonnet 4: 200k tokens (~700 pages)&lt;/p&gt;
    &lt;p&gt;- Gemini 2.5: 1M tokens (~3,000 pages)&lt;/p&gt;
    &lt;p&gt;- Grok 4-fast: 2M tokens (~6,000 pages)&lt;/p&gt;
    &lt;p&gt;At 2M tokens, you can fit an entire year of SEC filings for most companies.&lt;/p&gt;
    &lt;p&gt;The trajectory is even more dramatic: we’re likely heading toward 10M+ context windows by 2027, with Sam Altman hinting at billions of context tokens on the horizon. This represents a fundamental shift in how AI systems process information. Equally important, attention mechanisms are rapidly improving—LLMs are becoming far better at maintaining coherence and focus across massive context windows without getting “lost” in the noise.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Claude Code Insight: Why Context Changes Everything&lt;/head&gt;
    &lt;p&gt;Claude Code demonstrated that with enough context, search becomes navigation:&lt;/p&gt;
    &lt;p&gt;- No need to retrieve fragments when you can load complete files&lt;/p&gt;
    &lt;p&gt;- No need for similarity when you can use exact matches&lt;/p&gt;
    &lt;p&gt;- No need for reranking when you follow logical paths&lt;/p&gt;
    &lt;p&gt;- No need for embeddings when you have direct access&lt;/p&gt;
    &lt;p&gt;It’s mind-blowing. LLMs are getting really good at agentic behaviors meaning they can organize their work into tasks to accomplish an objective.&lt;/p&gt;
    &lt;p&gt;Here’s what tools like ripgrep bring to the search table:&lt;/p&gt;
    &lt;p&gt;- No Setup: No index. No overhead. Just point and search.&lt;/p&gt;
    &lt;p&gt;- Instant Availability: New documents are searchable the moment they hit the filesystem (no indexing latency!)&lt;/p&gt;
    &lt;p&gt;- Zero Maintenance: No clusters to manage, no indices to optimize, no RAM to provision&lt;/p&gt;
    &lt;p&gt;- Blazing Fast: For a 100K line codebase, Elasticsearch needs minutes to index. Ripgrep searches it in milliseconds with zero prep.&lt;/p&gt;
    &lt;p&gt;- Cost: $0 infrastructure cost vs a lot of $$$ for Elasticsearch&lt;/p&gt;
    &lt;p&gt;So back to our previous example on SEC filings. An agent can SEC filing structure intrinsically:&lt;/p&gt;
    &lt;p&gt;- Hierarchical Awareness: Knows that Item 1A (Risk Factors) relates to Item 7 (MD&amp;amp;A)&lt;/p&gt;
    &lt;p&gt;- Cross-Reference Following: Automatically traces “See Note 12” references&lt;/p&gt;
    &lt;p&gt;- Multi-Document Coordination: Connects 10-K, 10-Q, 8-K, and proxy statements&lt;/p&gt;
    &lt;p&gt;- Temporal Analysis: Compares year-over-year changes systematically&lt;/p&gt;
    &lt;p&gt;For searches across thousands of companies or decades of filings, it might still use hybrid search, but now as a tool for agents:&lt;/p&gt;
    &lt;p&gt;- Initial broad search using hybrid retrieval&lt;/p&gt;
    &lt;p&gt;- Agent loads full documents for top results&lt;/p&gt;
    &lt;p&gt;- Deep analysis within full context&lt;/p&gt;
    &lt;p&gt;- Iterative refinement based on findings&lt;/p&gt;
    &lt;p&gt;My guess is traditional RAG is now a search tool among others and that agents will always prefer grep and reading the whole file because they are context rich and can handle long-running tasks.&lt;/p&gt;
    &lt;p&gt;Consider our $6.5B lease obligation question as an example:&lt;/p&gt;
    &lt;p&gt;Step 1: Find “lease” in main financial statements&lt;/p&gt;
    &lt;p&gt;→ Discovers “See Note 12”&lt;/p&gt;
    &lt;p&gt;Step 2: Navigate to Note 12&lt;/p&gt;
    &lt;p&gt;→ Finds “excluding discontinued operations (Note 23)”&lt;/p&gt;
    &lt;p&gt;Step 3: Check Note 23&lt;/p&gt;
    &lt;p&gt;→ Discovers $2B additional obligations&lt;/p&gt;
    &lt;p&gt;Step 4: Cross-reference with MD&amp;amp;A&lt;/p&gt;
    &lt;p&gt;→ Identifies management’s explanation and adjustments&lt;/p&gt;
    &lt;p&gt;Step 5: Search for “subsequent events”&lt;/p&gt;
    &lt;p&gt;→ Finds post-balance sheet $500M lease termination&lt;/p&gt;
    &lt;p&gt;Final answer: $5B continuing + $2B discontinued - $500M terminated = $6.5B&lt;/p&gt;
    &lt;p&gt;The agent follows references like a human analyst would. No chunks. No embeddings. No reranking. Just intelligent navigation.&lt;/p&gt;
    &lt;p&gt;Basically, RAG is like a research assistant with perfect memory but no understanding:&lt;/p&gt;
    &lt;p&gt;- “Here are 50 passages that mention debt”&lt;/p&gt;
    &lt;p&gt;- Can’t tell you if debt is increasing or why&lt;/p&gt;
    &lt;p&gt;- Can’t connect debt to strategic changes&lt;/p&gt;
    &lt;p&gt;- Can’t identify hidden obligations&lt;/p&gt;
    &lt;p&gt;- Just retrieves text, doesn’t comprehend relationships&lt;/p&gt;
    &lt;p&gt;Agentic search is like a forensic accountant:&lt;/p&gt;
    &lt;p&gt;- Follows the money systematically&lt;/p&gt;
    &lt;p&gt;- Understands accounting relationships (assets = liabilities + equity)&lt;/p&gt;
    &lt;p&gt;- Identifies what’s missing or hidden&lt;/p&gt;
    &lt;p&gt;- Connects dots across time periods and documents&lt;/p&gt;
    &lt;p&gt;- Challenges management assertions with data&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Agentic Search Represents the Future&lt;/head&gt;
    &lt;p&gt;1. Increasing Document Complexity&lt;/p&gt;
    &lt;p&gt;- Documents are becoming longer and more interconnected&lt;/p&gt;
    &lt;p&gt;- Cross-references and external links are proliferating&lt;/p&gt;
    &lt;p&gt;- Multiple related documents need to be understood together&lt;/p&gt;
    &lt;p&gt;- Systems must follow complex trails of information&lt;/p&gt;
    &lt;p&gt;2. Structured Data Integration&lt;/p&gt;
    &lt;p&gt;- More documents combine structured and unstructured data&lt;/p&gt;
    &lt;p&gt;- Tables, narratives, and metadata must be understood together&lt;/p&gt;
    &lt;p&gt;- Relationships matter more than isolated facts&lt;/p&gt;
    &lt;p&gt;- Context determines meaning&lt;/p&gt;
    &lt;p&gt;3. Real-Time Requirements&lt;/p&gt;
    &lt;p&gt;- Information needs instant processing&lt;/p&gt;
    &lt;p&gt;- No time for re-indexing or embedding updates&lt;/p&gt;
    &lt;p&gt;- Dynamic document structures require adaptive approaches&lt;/p&gt;
    &lt;p&gt;- Live data demands live search&lt;/p&gt;
    &lt;p&gt;4. Cross-Document Understanding&lt;/p&gt;
    &lt;p&gt;Modern analysis requires connecting multiple sources:&lt;/p&gt;
    &lt;p&gt;- Primary documents&lt;/p&gt;
    &lt;p&gt;- Supporting materials&lt;/p&gt;
    &lt;p&gt;- Historical versions&lt;/p&gt;
    &lt;p&gt;- Related filings&lt;/p&gt;
    &lt;p&gt;RAG treats each document independently. Agentic search builds cumulative understanding.&lt;/p&gt;
    &lt;p&gt;5. Precision Over Similarity&lt;/p&gt;
    &lt;p&gt;- Exact information matters more than similar content&lt;/p&gt;
    &lt;p&gt;- Following references beats finding related text&lt;/p&gt;
    &lt;p&gt;- Structure and hierarchy provide crucial context&lt;/p&gt;
    &lt;p&gt;- Navigation beats retrieval&lt;/p&gt;
    &lt;p&gt;The evidence is becoming clear. While RAG served us well in the context-poor era, agentic search represents a fundamental evolution. The potential benefits of agentic search are compelling:&lt;/p&gt;
    &lt;p&gt;- Elimination of hallucinations from missing context&lt;/p&gt;
    &lt;p&gt;- Complete answers instead of fragments&lt;/p&gt;
    &lt;p&gt;- Faster insights through parallel exploration&lt;/p&gt;
    &lt;p&gt;- Higher accuracy through systematic navigation&lt;/p&gt;
    &lt;p&gt;- Massive infrastructure cost reduction&lt;/p&gt;
    &lt;p&gt;- Zero index maintenance overhead&lt;/p&gt;
    &lt;p&gt;The key insight? Complex document analysis—whether code, financial filings, or legal contracts—isn’t about finding similar text. It’s about understanding relationships, following references, and maintaining precision. The combination of large context windows and intelligent navigation delivers what retrieval alone never could.&lt;/p&gt;
    &lt;p&gt;RAG was a clever workaround for a context-poor era. It helped us bridge the gap between tiny windows and massive documents, but it was always a band-aid. The future won’t be about splitting documents into fragments and juggling embeddings. It will be about agents that can navigate, reason, and hold entire corpora in working memory.&lt;/p&gt;
    &lt;p&gt;We are entering the post-retrieval age. The winners will not be the ones who maintain the biggest vector databases, but the ones who design the smartest agents to traverse abundant context and connect meaning across documents. In hindsight, RAG will look like training wheels. Useful, necessary, but temporary.&lt;/p&gt;
    &lt;p&gt;The next decade of AI search will belong to systems that read and reason end-to-end. Retrieval isn’t dead—it’s just been demoted.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nicolasbustamante.com/p/the-rag-obituary-killed-by-agents"/><published>2025-10-01T16:51:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45440431</id><title>OpenTSLM: Language models that understand time series</title><updated>2025-10-02T12:18:16.775038+00:00</updated><content>&lt;doc fingerprint="748d54ec193a388b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenTSLM&lt;/head&gt;
    &lt;p&gt;The Future of AI Delivered on Time&lt;/p&gt;
    &lt;p&gt;AI understands text, images, audio, and video.&lt;lb/&gt;But the real world runs on time.&lt;/p&gt;
    &lt;p&gt;Every heartbeat, price tick, sensor pulse, machine log, and user click is a temporal signal.&lt;lb/&gt;Current models can't reason about them.&lt;/p&gt;
    &lt;p&gt;We're changing that.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Class of Foundation Models&lt;/head&gt;
    &lt;p&gt;Time-Series Language Models (TSLMs) are multimodal foundation models with time series as a native modality, next to text, enabling direct reasoning, explanation, and forecasting over temporal data in natural language.&lt;/p&gt;
    &lt;p&gt;Our research shows order-of-magnitude gains in temporal reasoning while running on smaller, faster backbones. TSLMs are not an add-on. They're a new modality for AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Core, Frontier Edge&lt;/head&gt;
    &lt;p&gt;OpenTSLM: Lightweight base models trained on public data, released openly. They set the standard for temporal reasoning and power a global developer and research ecosystem.&lt;/p&gt;
    &lt;p&gt;Frontier TSLMs: Advanced proprietary models trained on specialized data, delivering enterprise-grade performance and powering APIs, fine-tuning, and vertical solutions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our Vision&lt;/head&gt;
    &lt;p&gt;We're building the temporal interface for AI - the layer that connects continuous real-world signals to intelligent decisions and autonomous agents.&lt;/p&gt;
    &lt;p&gt;A universal TSLM will power proactive healthcare, adaptive robotics, resilient infrastructure, and new forms of human-AI collaboration.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;OpenTSLM is a team of scientists, engineers, and builders from ETH, Stanford, Harvard, Cambridge, TUM, CDTM, Google, Meta, AWS, and beyond. We are the original authors of the OpenTSLM paper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.opentslm.com/"/><published>2025-10-01T17:25:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45443462</id><title>Edge264 – Minimalist, high-performance software decoder for H.264/AVC video</title><updated>2025-10-02T12:18:16.200262+00:00</updated><content>&lt;doc fingerprint="2658de598d253911"&gt;
  &lt;main&gt;
    &lt;p&gt;Minimalist software decoder with state-of-the-art performance for the H.264/AVC video format.&lt;/p&gt;
    &lt;p&gt;Please note this is a work in progress and will be ready for use after making GStreamer/VLC plugins.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports Progressive High and MVC 3D profiles, up to level 6.2&lt;/item&gt;
      &lt;item&gt;Any resolution up to 8K UHD&lt;/item&gt;
      &lt;item&gt;8-bit 4:2:0 planar YUV output&lt;/item&gt;
      &lt;item&gt;Slices and Arbitrary Slice Order&lt;/item&gt;
      &lt;item&gt;Slice and frame multi-threading&lt;/item&gt;
      &lt;item&gt;Per-slice reference picture list&lt;/item&gt;
      &lt;item&gt;Memory Management Control Operations&lt;/item&gt;
      &lt;item&gt;Long-term reference frames&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows: x86, x64&lt;/item&gt;
      &lt;item&gt;Linux: x86, x64, ARM64&lt;/item&gt;
      &lt;item&gt;Mac OS: x64&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;edge264 is entirely developed in C using 128-bit vector extensions and vector intrinsics, and can be compiled with GNU GCC or LLVM Clang. SDL2 runtime library may be used (optional) to enable display with &lt;code&gt;edge264_test&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here are the &lt;code&gt;make&lt;/code&gt; options for tuning the compiled library file:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CC&lt;/code&gt;- C compiler used to convert source files to object files (default&lt;code&gt;cc&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CFLAGS&lt;/code&gt;- additional compilation flags passed to&lt;code&gt;CC&lt;/code&gt;and&lt;code&gt;TARGETCC&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TARGETCC&lt;/code&gt;- C compiler used to link object files into library file (default&lt;code&gt;CC&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;LDFLAGS&lt;/code&gt;- additional compilation flags passed to&lt;code&gt;TARGETCC&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TARGETOS&lt;/code&gt;- resulting file naming convention among&lt;code&gt;Windows&lt;/code&gt;|&lt;code&gt;Linux&lt;/code&gt;|&lt;code&gt;Darwin&lt;/code&gt;(default host)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;VARIANTS&lt;/code&gt;- comma-separated list of additional variants included in the library and selected at runtime (default&lt;code&gt;logs&lt;/code&gt;)&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;x86-64-v2&lt;/code&gt;- variant compiled for x86-64 microarchitecture level 2 (SSSE3, SSE4.1 and POPCOUNT)&lt;/item&gt;&lt;item&gt;&lt;code&gt;x86-64-v3&lt;/code&gt;- variant compiled for x86-64 microarchitecture level 3 (AVX2, BMI, LZCNT, MOVBE)&lt;/item&gt;&lt;item&gt;&lt;code&gt;logs&lt;/code&gt;- variant compiled with logging support in YAML format (headers and slices)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BUILD_TEST&lt;/code&gt;- toggles compilation of&lt;code&gt;edge264_test&lt;/code&gt;(default&lt;code&gt;yes&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FORCEINTRIN&lt;/code&gt;- enforce the use of intrinsics among&lt;code&gt;x86&lt;/code&gt;|&lt;code&gt;ARM64&lt;/code&gt;(for WebAssembly)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ make CFLAGS="-march=x86-64" VARIANTS=x86-64-v2,x86-64-v3 BUILD_TEST=no # example x86 build&lt;/code&gt;
    &lt;p&gt;The automated test program &lt;code&gt;edge264_test&lt;/code&gt; can browse files in a given directory, decoding each &lt;code&gt;&amp;lt;video&amp;gt;.264&lt;/code&gt; file and comparing its output with each sibling file &lt;code&gt;&amp;lt;video&amp;gt;.yuv&lt;/code&gt; if found. On the set of AVCv1, FRExt and MVC conformance bitstreams, 109/224 files are decoded without errors, the rest using yet unsupported features.&lt;/p&gt;
    &lt;code&gt;$ make
$ ./edge264_test --help # prints all options available
$ ffmpeg -i vid.mp4 -vcodec copy -bsf h264_mp4toannexb -an vid.264 # optional, converts from MP4 format
$ ./edge264_test -d vid.264 # replace -d with -b to benchmark instead of display&lt;/code&gt;
    &lt;p&gt;Here is a complete example that opens an input file in Annex B format from command line, and dumps its decoded frames in planar YUV order to standard output. See edge264_test.c for a more complete example which can also display frames.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;

#include "edge264.h"

int main(int argc, char *argv[]) {
	int fd = open(argv[1], O_RDONLY);
	struct stat st;
	fstat(fd, &amp;amp;st);
	uint8_t *buf = mmap(NULL, st.st_size, PROT_READ, MAP_SHARED, fd, 0);
	const uint8_t *nal = buf + 3 + (buf[2] == 0); // skip the [0]001 delimiter
	const uint8_t *end = buf + st.st_size;
	// auto threads, no logs, auto allocs
	Edge264Decoder *dec = edge264_alloc(-1, NULL, NULL, 0, NULL, NULL, NULL);
	Edge264Frame frm;
	int res;
	do {
		res = edge264_decode_NAL(dec, nal, end, 0, NULL, NULL, &amp;amp;nal);
		while (!edge264_get_frame(dec, &amp;amp;frm, 0)) {
			for (int y = 0; y &amp;lt; frm.height_Y; y++)
				write(1, frm.samples[0] + y * frm.stride_Y, frm.width_Y);
			for (int y = 0; y &amp;lt; frm.height_C; y++)
				write(1, frm.samples[1] + y * frm.stride_C, frm.width_C);
			for (int y = 0; y &amp;lt; frm.height_C; y++)
				write(1, frm.samples[2] + y * frm.stride_C, frm.width_C);
		}
	} while (res == 0 || res == ENOBUFS);
	edge264_free(&amp;amp;dec);
	munmap(buf, st.st_size);
	close(fd);
	return 0;
}&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;const uint8_t * edge264_find_start_code(buf, end, four_byte)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Return a pointer to the next three or four byte (0)001 start code prefix, or &lt;code&gt;end&lt;/code&gt; if not found.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * buf&lt;/code&gt;- first byte of buffer to search into&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * end&lt;/code&gt;- first invalid byte past the buffer that stops the search&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int four_byte&lt;/code&gt;- if 0 seek a 001 prefix, otherwise seek a 0001&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;Edge264Decoder * edge264_alloc(n_threads, log_cb, log_arg, log_mbs, alloc_cb, free_cb, alloc_arg)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Allocate and initialize a decoding context.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;int n_threads&lt;/code&gt;- number of background worker threads, with 0 to disable multithreading and -1 to detect the number of logical cores at runtime&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* log_cb)(const char * str, void * log_arg)&lt;/code&gt;- if not NULL, a&lt;code&gt;fputs&lt;/code&gt;-compatible function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;will call to log every header, SEI or macroblock (requires the&lt;code&gt;logs&lt;/code&gt;variant otherwise fails at runtime, called from the same thread except macroblocks in multithreaded decoding)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * log_arg&lt;/code&gt;- custom value passed to&lt;code&gt;log_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int log_mbs&lt;/code&gt;- set to 1 to enable logging of macroblocks&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* alloc_cb)(void ** samples, unsigned samples_size, void ** mbs, unsigned mbs_size, int errno_on_fail, void * alloc_arg)&lt;/code&gt;- if not NULL, a function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;will call (on the same thread) instead of malloc to request allocation of samples and macroblock buffers for a frame (&lt;code&gt;errno_on_fail&lt;/code&gt;is ENOMEM for mandatory allocations, or ENOBUFS for allocations that may be skipped to save memory but reduce playback smoothness)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* free_cb)(void * samples, void * mbs, void * alloc_arg)&lt;/code&gt;- if not NULL, a function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;and&lt;code&gt;edge264_free&lt;/code&gt;will call (on the same thread) to free buffers allocated through&lt;code&gt;alloc_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * alloc_arg&lt;/code&gt;- custom value passed to&lt;code&gt;alloc_cb&lt;/code&gt;and&lt;code&gt;free_cb&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;int edge264_decode_NAL(dec, buf, end, non_blocking, free_cb, free_arg, next_NAL)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Decode a single NAL unit containing any parameter set or slice.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * buf&lt;/code&gt;- first byte of NAL unit (containing&lt;code&gt;nal_unit_type&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * end&lt;/code&gt;- first byte past the buffer (max buffer size is 231-1 on 32-bit and 263-1 on 64-bit)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int non_blocking&lt;/code&gt;- set to 1 if the current thread has other processing thus cannot block here&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* free_cb)(void * free_arg, int ret)&lt;/code&gt;- callback that may be called from another thread when multithreaded, to signal the end of parsing and release the NAL buffer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * free_arg&lt;/code&gt;- custom value that will be passed to&lt;code&gt;free_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t ** next_NAL&lt;/code&gt;- if not NULL and the return code is&lt;code&gt;0&lt;/code&gt;|&lt;code&gt;ENOTSUP&lt;/code&gt;|&lt;code&gt;EBADMSG&lt;/code&gt;, will receive a pointer to the next NAL unit after the next start code in an Annex B stream&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Return codes are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0&lt;/code&gt;on success&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOTSUP&lt;/code&gt;on unsupported stream (decoding may proceed but could return zero frames)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EBADMSG&lt;/code&gt;on invalid stream (decoding may proceed but could show visual artefacts, if you can check with another decoder that the stream is actually flawless, please consider filling a bug report 🙏)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EINVAL&lt;/code&gt;if the function was called with&lt;code&gt;dec == NULL&lt;/code&gt;or&lt;code&gt;dec-&amp;gt;buf == NULL&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENODATA&lt;/code&gt;if the function was called while&lt;code&gt;dec-&amp;gt;buf &amp;gt;= dec-&amp;gt;end&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOMEM&lt;/code&gt;if&lt;code&gt;malloc&lt;/code&gt;failed to allocate memory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOBUFS&lt;/code&gt;if more frames should be consumed with&lt;code&gt;edge264_get_frame&lt;/code&gt;to release a picture slot&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EWOULDBLOCK&lt;/code&gt;if the non-blocking function would have to wait before a picture slot is available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;int edge264_get_frame(dec, out, borrow)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Fetch the next frame ready for output.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Edge264Frame *out&lt;/code&gt;- a structure that will be filled with data for the frame returned&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int borrow&lt;/code&gt;- if 0 the frame may be accessed until the next call to&lt;code&gt;edge264_decode_NAL&lt;/code&gt;, otherwise the frame should be explicitly returned with&lt;code&gt;edge264_return_frame&lt;/code&gt;. Note that access is not exclusive, it may be used concurrently as reference for other frames.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Return codes are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0&lt;/code&gt;on success (one frame is returned)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EINVAL&lt;/code&gt;if the function was called with&lt;code&gt;dec == NULL&lt;/code&gt;or&lt;code&gt;out == NULL&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOMSG&lt;/code&gt;if there is no frame to output at the moment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While reference frames may be decoded ahead of their actual display (ex. B-Pyramid technique), all frames are buffered for reordering before being released for display:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding a non-reference frame releases it and all frames set to be displayed before it.&lt;/item&gt;
      &lt;item&gt;Decoding a key frame releases all stored frames (but not the key frame itself which might be reordered later).&lt;/item&gt;
      &lt;item&gt;Exceeding the maximum number of frames held for reordering releases the next frame in display order.&lt;/item&gt;
      &lt;item&gt;Lacking an available frame buffer releases the next non-reference frame in display order (to salvage its buffer) and all reference frames displayed before it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;typedef struct Edge264Frame {
	const uint8_t *samples[3]; // Y/Cb/Cr planes
	const uint8_t *samples_mvc[3]; // second view
	const uint8_t *mb_errors; // probabilities (0..100) for each macroblock to be erroneous, NULL if there are no errors, values are spaced by stride_mb in memory
	int8_t pixel_depth_Y; // 0 for 8-bit, 1 for 16-bit
	int8_t pixel_depth_C;
	int16_t width_Y;
	int16_t width_C;
	int16_t height_Y;
	int16_t height_C;
	int16_t stride_Y;
	int16_t stride_C;
	int16_t stride_mb;
	uint32_t FrameId;
	uint32_t FrameId_mvc; // second view
	int16_t frame_crop_offsets[4]; // {top,right,bottom,left}, useful to derive the original frame with 16x16 macroblocks
	void *return_arg;
} Edge264Frame;&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_return_frame(dec, return_arg)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Give back ownership of the frame if it was borrowed from a previous call to &lt;code&gt;edge264_get_frame&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * return_arg&lt;/code&gt;- the value stored inside the frame to return&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_flush(dec)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;For use when seeking, stop all background processing, flush all delayed frames while keeping them allocated, and clear the internal decoder state.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_free(pdec)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Deallocate the entire decoding context, and unset the pointer.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder ** pdec&lt;/code&gt;- pointer to a decoding context, initialized or not&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stress testing (in progress)&lt;/item&gt;
      &lt;item&gt;Multithreading (in progress)&lt;/item&gt;
      &lt;item&gt;Error recovery (in progress)&lt;/item&gt;
      &lt;item&gt;Integration in VLC/ffmpeg/GStreamer&lt;/item&gt;
      &lt;item&gt;ARM32&lt;/item&gt;
      &lt;item&gt;PAFF and MBAFF&lt;/item&gt;
      &lt;item&gt;4:0:0, 4:2:2 and 4:4:4&lt;/item&gt;
      &lt;item&gt;9-14 bit depths with possibility of different luma/chroma depths&lt;/item&gt;
      &lt;item&gt;Transform-bypass for macroblocks with QP==0&lt;/item&gt;
      &lt;item&gt;SEI messages&lt;/item&gt;
      &lt;item&gt;AVX-2 optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I use edge264 to experiment on new programming techniques to improve performance and code size over existing decoders, and presented a few of these techniques at FOSDEM'24 and FOSDEM'25.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Single header file - It contains all struct definitions, common constants and enums, SIMD aliases, inline functions and macros, and exported functions for each source file. To understand the code base you should look at this file first.&lt;/item&gt;
      &lt;item&gt;Code blocks instead of functions - The main decoding loop is a forward pipeline designed as a DAG loosely resembling hardware decoders, with nodes being non-inlined functions and edges being tail calls. It helps mutualize code branches wherever possible, thus reduces code size to help fit in L1 cache.&lt;/item&gt;
      &lt;item&gt;Tree branching - Directional intra modes are implemented with a jump table to the leaves of a tree then unconditional jumps down to the trunk. It allows sharing the bottom code among directional modes, to reduce code size.&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Global context register - The pointer to the main structure holding context data is assigned to a register when supported by the compiler (GCC).&lt;/del&gt;This technique was dropped as Clang eventually reached on-par performance, so there is little incentive to maintain this hack.&lt;/item&gt;
      &lt;item&gt;Default neighboring values (search &lt;code&gt;unavail_mb&lt;/code&gt;) - Tests for availability of neighbors are replaced with fake neighboring macroblocks around each frame. It reduces the number of conditional tests inside the main decoding loop, thus reduces code size and branch predictor pressure.&lt;/item&gt;
      &lt;item&gt;Relative neighboring offsets (look for &lt;code&gt;A4x4_int8&lt;/code&gt;and related variables) - Access to left/top macroblock values is done with direct offsets in memory instead of copying their values to a buffer beforehand. It helps to reduce the reads and writes in the main decoding loop.&lt;/item&gt;
      &lt;item&gt;Parsing uneven block shapes (look at function &lt;code&gt;parse_P_sub_mb&lt;/code&gt;) - Each Inter macroblock paving specified with mb_type and sub_mb_type is first converted to a bitmask, then iterated on set bits to fetch the correct number of reference indices and motion vectors. This helps to reduce code size and number of conditional blocks.&lt;/item&gt;
      &lt;item&gt;Using vector extensions - GCC's vector extensions are used along vector intrinsics to write more compact code. All intrinsics from Intel are aliased with shorter names, which also provides an enumeration of all SIMD instructions used in the decoder.&lt;/item&gt;
      &lt;item&gt;Register-saturating SIMD - Some critical SIMD algorithms use more simultaneous vectors than available registers, effectively saturating the register bank and generating stack spills on purpose. In some cases this is more efficient than splitting the algorithm into smaller bits, and has the additional benefit of scaling well with later CPUs.&lt;/item&gt;
      &lt;item&gt;Piston cached bitstream reader - The bitstream bits are read in a size_t[2] intermediate cache with a trailing set bit to keep track of the number of cached bits, giving access to 32/64 bits per read from the cache, and allowing wide refills from memory.&lt;/item&gt;
      &lt;item&gt;On-the-fly SIMD unescaping - The input bitstream is unescaped on the fly using vector code, avoiding a full preprocessing pass to remove escape sequences, and thus reducing memory reads/writes.&lt;/item&gt;
      &lt;item&gt;Multiarch SIMD programming - Using vector extensions along with aliased intrinsics allows supporting both Intel SSE and ARM NEON with around 80% common code and few #if #else blocks, while keeping state-of-the-art performance for both architectures.&lt;/item&gt;
      &lt;item&gt;The Structure of Arrays pattern - The frame buffer is stored with arrays for each distinct field rather than an array of structures, to express operations on frames with bitwise and vector operators (see AoS and SoA). The task buffer for multithreading also relies on it partially.&lt;/item&gt;
      &lt;item&gt;Deferred error checking - Error detection is performed once in each type of NAL unit (search for &lt;code&gt;return&lt;/code&gt;statements), by clamping all input values to their expected ranges, then expecting&lt;code&gt;rbsp_trailing_bit&lt;/code&gt;afterwards (with very high probability of catching an error if the stream is corrupted). This design choice is discussed in A case about parsing errors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other yet-to-be-presented bits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimalistic API with FFI-friendly design (7 functions and 1 structure).&lt;/item&gt;
      &lt;item&gt;The bitstream caches for CAVLC and CABAC (search for &lt;code&gt;rbsp_reg&lt;/code&gt;) are stored in two size_t variables each, which may be mapped to Global Register Variables in the future.&lt;/item&gt;
      &lt;item&gt;The decoding of input symbols is interspersed with their parsing (instead of parsing to a &lt;code&gt;struct&lt;/code&gt;then decoding the data). It deduplicates branches and loops that are present in both parsing and decoding, and even eliminates the need to store some symbols (e.g. mb_type, sub_mb_type, mb_qp_delta).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the help of a custom bitstream writer using the same YAML format edge264 outputs, a set of extensive tests are being created in tools/raw_tests to stress the darkest corners of this decoder. The following table lists them all, along with the files implementing them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;General tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All supported types of NAL units&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;supp-nals&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported types of NAL units&lt;/cell&gt;
        &lt;cell&gt;All unsupp&lt;/cell&gt;
        &lt;cell&gt;unsupp-nals&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Maximal header log-wise&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;max-logs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All conditions (incl. ignored) for detecting the start of a new frame&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;finish-frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nal_ref_idc=0 on a IDR&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;non-ref-idr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Missing rbsp_trailing_bit for all supported NAL types&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;no-trailing-bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NAL of less than 11 bytes starting/ending at page boundary&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;tiny-nal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SEI/slice referencing an uninitialized SPS/PPS&lt;/cell&gt;
        &lt;cell&gt;1 OK, 4 errors&lt;/cell&gt;
        &lt;cell&gt;missing-ps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two non-ref frames with decreasing POC&lt;/cell&gt;
        &lt;cell&gt;All OK, any order&lt;/cell&gt;
        &lt;cell&gt;non-ref-dec-poc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Horizontal/vertical cropping leaving zero space&lt;/cell&gt;
        &lt;cell&gt;All OK, 1x1 frames&lt;/cell&gt;
        &lt;cell&gt;zero-cropping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P/B slice with nal_unit_type=5 or max_num_ref_frames=0&lt;/cell&gt;
        &lt;cell&gt;4 OK, 2 errors&lt;/cell&gt;
        &lt;cell&gt;no-refs-P-B-slice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;IDR slice with frame_num&amp;gt;0&lt;/cell&gt;
        &lt;cell&gt;All OK, clamped to 0&lt;/cell&gt;
        &lt;cell&gt;pos-frame-num-idr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A ref that must bump out higher POCs to enter DPB (C.4.5.2)&lt;/cell&gt;
        &lt;cell&gt;All OK, check output order&lt;/cell&gt;
        &lt;cell&gt;poc-out-of-order&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two ref frames with the same frame_num but differing POC, then a third frame referencing both&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gap in frame_num while gaps_in_frame_num_value_allowed_flag=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Stream starting with non-IDR I frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Stream starting with P/B frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ref slice with delta_pic_order_cnt_bottom=-2**31, then a second frame referencing it&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two frames A/B with intersecting top/bottom POC intervals in all possible intersections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A 32-bit POC overflow between 2 frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A B-frame referencing frames with more than 2**16 POC diff&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num_ref_idx_active&amp;gt;15 in SPS then no override in slice for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with more ref_pic_list_modifications than num_ref_idx_active/16 for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with ref_pic_list_modifications duplicating a ref then referencing the second one&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with insufficient ref frames with and without override of num_ref_idx_active for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A modification of RefPicList[0/1] to a non-existing short/long term frame, then referencing it in mb&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;33 IDR with long_term_reference_flag=0/1 while max_num_ref_frames=0 (8.2.5.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A new reference while max_num_ref_frames are already all long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of mmco on all non-existing/short/long refs, with at least twice each mmco&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two fields of the same frame being assigned different long-term frame indices then referenced&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;While all max_num_ref_frames are long-term, a ref_pic_list_modification that references all of them&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;An IDR picture with POC&amp;gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A picture with mmco=5 decoded after a picture with greater POC (8.2.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A P/B frame with zero references before or received with a gap in frame_num equal to max_ref_frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A P/B frame referencing a non-existing/erroneous ref&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A B frame with colPic set to a non-existing frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A current frame mmco'ed to long-term while all max_num_ref_frames are already long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A mmco marking a non-existing picture to long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of IntraNxNPredMode with A/B/C/D unavailability with asserts for out-of-bounds reads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A direct Inter reference from colPic that is not present in RefPicList0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A residual block with all coeffs at maximum 32-bit values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two slices of the same frame separated by a currPic reset (ex. AUD)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two frames with the same POC yet differing TopFieldOrderCnt/BottomFieldOrderCnt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Differing mmcos on two slices of the same frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sending 2 IDR, then reaching the lowest possible POC, then getting all frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two slices with mmco=5 yet frame_num&amp;gt;0 (to make it look like a new frame)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;POCs spaced by more than half max bits, such that relying on a stale prevPicOrderCnt yields wrong POC&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Filling the DPB with 16 refs then setting max_num_ref_frames=1 and adding a new ref frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Adding a frame cropping after decoding a frame&lt;/cell&gt;
        &lt;cell&gt;Crop should not apply retroactively&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Making a Direct ref_pic be used after it has been unreferenced&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;poc_type=2 and non-ref frame followed by non-ref pic, and the opposite (7.4.2.1.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;direct_8x8_inference_flag=1 with frame_mbs_only_flag=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;checking that a gap in frame_num with poc_type==0 does not insert refs in B slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SPS changing frame format while currPic&amp;gt;=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A frame allocator putting all allocs at start/end of a page boundary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Parameter sets tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid profile_idc=0/255&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Highest level_idc=255&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported values of chroma_format_idc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported values of bit_depth_luma/chroma&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;qpprime_y_zero_transform_bypass_flag=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All scaling lists default/fallback rules and repeated values for all indices, with residual macroblock&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;log2_max_frame_num=4 and a frame referencing another with the same frame_num%4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;CAVLC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid total_zeros=0-8-prefix+3-bit-suffix for TotalCoeffs in [0;15] for 4x4 and 2x2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid total_zeros=31/63/127-prefix for TotalCoeffs in [0;15] for 4x4 and 2x2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid coeff_token=0-14-prefix+4-bit-suffix for nC=0/2/4, and valid 6-bit-values for nC=8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid coeff_token=31/63/127-prefix for nC=0/2/4, and invalid 6-bit-values for nC=8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid levelCode=25-prefix+suffixLength-bit-suffix for all values of suffixLength&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid run_before for all values of zerosLeft&amp;lt;=7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid run_before=31/63/127 for zerosLeft=7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Macroblock of maximal size for all values of mb_type&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;mb_qp_delta=-26/25 that overflows on both sides&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid inferences of nC for all values of nA/nB=unavail/other-slice/0-16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All coded_block_pattern=[0;47] for I and P/B slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of intra_chroma_pred_mode and Intra4x4/8x8/16x16PredMode with A/B-unavailability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All values of mb_type+sub_mb_types for I/P/B with ref_idx/mvds different than values from B_Direct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;mvd=[-32768/0/32767,-32768/0/32767] in a single 16x16 macroblock&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TotalCoeff=16 for a Intra16x16 AC block&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A residual block with run_length=14 making zerosLeft negative&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;CABAC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mixing CAVLC and CABAC in a same frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Single slice with at least 8 cabac_zero_word&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;MVC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All wrong combinations of non_idr_flag with nal_unit_type=1/5 and nal_ref_idc=0/1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nal_unit_type=14 then filler unit then nal_unit_type=1/5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;An nal_unit_type=5 view paired with a non_idr_flag=0 P view, or a non_idr_flag=1 view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Missing a base or non-base view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Receiving a SSPS yet only base views then&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16 ref base views while non base are non-refs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SSPS with different pic_width_in_mbs/pic_height_in_mbs/chroma_format_idc than its SPS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SSPS with num_views=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A non-base view with weighted_bipred_idc=2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A non-base view with its base in RefPicList1[0] and direct_spatial_mv_pred_flag=0 (H.7.4.3)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with num_ref_idx_l0_active&amp;gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;svc_extension_flag=1 on a MVC stream&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SSPS with additional_extension2_flag=1 and more trailing data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gap in frame_num of 16 frames on both views&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Specifying extra_frames=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Receiving a non-base view before its base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A stream sending non-base views after a few frames have been output&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Error recovery tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tests to implement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A complete frame received twice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice of a frame received twice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Frame with correct and erroneous slice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations erroneous/correct and all interval intersections on 2 slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All failures of malloc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;All (dis-)allowed bit positions at the end without rbsp_trailing_bit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tvlabs/edge264"/><published>2025-10-01T21:00:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45444694</id><title>Cormac McCarthy's personal library</title><updated>2025-10-02T12:18:15.947522+00:00</updated><content>&lt;doc fingerprint="77a350d52da307b7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Two Years After Cormac McCarthy’s Death, Rare Access to His Personal Library Reveals the Man Behind the Myth&lt;/head&gt;
    &lt;head rend="h2"&gt;The famously reclusive novelist amassed a collection of thousands of books ranging in topics from philosophical treatises to advanced mathematics to the naked mole-rat&lt;/head&gt;
    &lt;p&gt;Cormac McCarthy, one of the greatest novelists America has ever produced and one of the most private, had been dead for 13 months when I arrived at his final residence outside Santa Fe, New Mexico. It was a stately old adobe house, two stories high with beam-ends jutting out of the exterior walls, set back from a country road in a valley below the mountains. First built in 1892, the house was expanded and modernized in the 1970s and extensively modified by McCarthy himself, who, it turns out, was a self-taught architect as well as a master of literary fiction.&lt;/p&gt;
    &lt;p&gt;I was invited to the house by two McCarthy scholars who were embroiled in a herculean endeavor. Working unpaid, with help from other volunteer scholars and occasional graduate students, they had taken it upon themselves to physically examine and digitally catalog every single book in McCarthy’s enormous and chaotically disorganized personal library. They were guessing it contained upwards of 20,000 volumes. By comparison, Ernest Hemingway, considered a voracious book collector, left behind a personal library of 9,000.&lt;/p&gt;
    &lt;p&gt;What makes McCarthy’s library so intriguing is not just its size, nor the fact that very few people know about it. His books, many of which are annotated with margin comments, promise to reveal far more about this elusive literary giant than the few cagey interviews he gave when he was alive. For as long as people have been reading McCarthy, they have speculated about which books and authors informed and inspired his work, a subject he was loath to discuss. They have wondered about his interests and true personality because all he presented to the public was a reclusive, austere, inscrutable facade.&lt;/p&gt;
    &lt;p&gt;When Bryan Giemza, a scholar of literature and humanities at Texas Tech University, offered me exclusive journalistic access to McCarthy’s library and the cataloging project, what he was really offering was an unprecedented insight into McCarthy’s life and work. As a further enticement, he said that Cormac’s younger brother Dennis McCarthy would be there. “Dennis probably knew him as well as anyone,” Giemza said.&lt;/p&gt;
    &lt;head rend="h4"&gt;Did You Know? Who was Cormac McCarthy?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cormac McCarthy is an award-winning novelist whose works often explored the American West with darkness and complexity. Among McCarthy's many awards are a Guggenheim Fellowship in 1969, a MacArthur Fellowship in 1981, a Pulitzer Prize for Fiction in 2007 for The Road and a National Book award in 1992 for All the Pretty Horses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I parked behind the house between a silver 1966 Buick Riviera rusting on deflated tires and a weathered red Lincoln Mark VIII. These were among the last survivors of McCarthy’s little-known vehicle collection. Dennis had sold 13 other cars, including two Allard racing cars from the early 1950s, a 1992 Lotus and a Ford GT40 racing car. McCarthy, who labored in obscurity and chronic poverty until he was 60, became a multi-millionaire later in life and freely indulged his desires and obsessions, with classic sports cars high on the list. Most of the money came from Hollywood, which turned three of his novels—All the Pretty Horses, No Country for Old Men and The Road—into star-studded movies.&lt;/p&gt;
    &lt;p&gt;I knocked on the imposing front door, an Indo-Portuguese antique made of teak and fortified with iron strappings, metal studs, flattened nails and small chains. There was no response, so I tried the handle. The door swung open and revealed a dimly lit hallway reduced to a narrow passage by head-high stacks of cardboard boxes on both sides. All those boxes were packed with books.&lt;/p&gt;
    &lt;p&gt;The first room off the hallway—the room where McCarthy died at age 89—was now so crammed with book boxes that it was impenetrable. Scholars called it “the Beast Room.” The next room was nearly as full. One open box showed volumes about the architect Frank Lloyd Wright, country houses in Ireland, schizophrenia, African history and British antique rifle barrels.&lt;/p&gt;
    &lt;p&gt;In the dining room, underneath a beautiful hanging light fixture of wood and colored glass that McCarthy designed and built himself, scholars were sitting at the table, scanning books’ ISBN bar codes through their phones into the library cataloging software on their laptops.&lt;/p&gt;
    &lt;p&gt;I found Giemza in the living room, wrestling with an internet connectivity problem. “Cormac didn’t have Wi-Fi in the house, so we had to bring our own,” he said. Nor did McCarthy use a computer—ever. He typed out his pages on a cheap, durable Olivetti typewriter and, I learned later, did most of his work propped up on pillows in bed.&lt;/p&gt;
    &lt;p&gt;The living room, like the house in general, had a sturdy, old-fashioned and decidedly masculine feel, but its clean lines were obscured by a chaotic overlay of clutter—mainly books, but also piles of nameless junk and hundreds of bowls, glasses and kitchenware items still in their packaging. Some of the book boxes and loose books had been moved into the room for the cataloging project, but not the rest of it. One of the first discoveries made by the visiting scholars was that McCarthy was something of a hoarder. His particular fixation on kitchenware, much of it bargain kitchenware, remains mysterious and a mark of his eccentricity.&lt;/p&gt;
    &lt;p&gt;The second major discovery, discernible in his work but confirmed beyond doubt in his library, was that McCarthy was a genius-level intellectual polymath with an insatiable curiosity. His interests ranged from quantum physics, which he taught himself by reading 190 books on the notoriously challenging subject, to whale biology, violins, obscure corners of French history in the early Middle Ages, the highest levels of advanced mathematics and almost any other subject you can name.&lt;/p&gt;
    &lt;p&gt;Giemza marveled at the heavy-duty philosophy books they were finding. “Seventy-five titles by or about Wittgenstein so far,” he said, referring to the Austrian philosopher of mathematics, logic, language and the mind. “And most of them are annotated, meaning Cormac read them closely. A lot of Hegel. That was his light evening reading, apparently.”&lt;/p&gt;
    &lt;p&gt;In the living room was a pool table piled with books and a leather couch facing two tall windows and three sets of nine-foot-tall wooden bookshelves designed by McCarthy that held approximately 1,000 books. Moving closer, I saw they were nearly all nonfiction hardbacks with no obvious system of organization.&lt;/p&gt;
    &lt;p&gt;One shelf held volumes about Mesoamerican&lt;lb/&gt; history and archaeology, along with Charles Darwin’s collected notebooks, Victor Klemperer’s three-volume diary of the Nazi years, books about organic chemistry and sports cars, and an obscure volume titled The Biology of the Naked Mole-Rat (Monographs in Behavior and Ecology). Another shelf held books about Grand Prix and Formula 1 racing, a great passion of McCarthy’s, and the collected writings of Charles S. Peirce, the American scientist, philosopher and logician, in six fat volumes of dense, difficult prose. &lt;/p&gt;
    &lt;p&gt;Trying to take it all in, I felt both fascinated and overwhelmed. It seemed almost inconceivable that an author who produced 12 novels, two plays and five screenplays had also found the time, energy and brainpower to master architecture, woodworking, stonemasonry and a wide range of intellectual disciplines. Some of his math books were nearly all equations.&lt;/p&gt;
    &lt;p&gt;Then we found an intricate drawing he’d made for an engine modification to one of his cars, and another showing how to rifle a gun barrel with hand tools. We found dozens of well-thumbed engine repair manuals and auto mechanic’s tools in the outbuildings, and learned that he could disassemble, reassemble and redesign an engine to increase its horsepower. Then I learned he had an eidetic memory and could remember nearly everything he had read or heard, including the lyrics to thousands of songs. McCarthy was starting to seem like a man whose talents and intelligence were without limits, yet he lived in a hoarder’s shambles and couldn’t stop buying nonstick skillets and fruit bowls.&lt;/p&gt;
    &lt;p&gt;By studying his library more closely, I hoped to gain a better understanding of McCarthy, but it was possible the mystery of his character would only deepen.&lt;/p&gt;
    &lt;p&gt;Giemza introduced me to his colleague Stacey Peebles, a professor of film and English at Centre College in Danville, Kentucky, and the current president of the Cormac McCarthy Society. It was Peebles who first met with Dennis McCarthy, the author’s brother and literary executor, and suggested that the society take on the monumental task of cataloging the books. The society’s mission is to further the study and appreciation of McCarthy’s work, and Peebles thought there was enough material in the library to keep scholars busy for decades—scrutinizing the annotations, tracing connections between the research books and passages in the novels, interpreting literary and philosophical influences. “If we were a well-funded institution, we’d take all these boxes into an empty building where we had plenty of space to work in, a dedicated team of people and all the time we needed,” she said. But Peebles and her small team all have full-time jobs, so the project has required a trade-off between detail and efficiency. “We can’t be as meticulous as we’d like and scan all the annotations, because we’ve got limited time and a massive amount of books to get through.”&lt;/p&gt;
    &lt;p&gt;McCarthy often had a pencil when he was reading and would make tiny vertical marks next to sentences that interested him and add comments in the margins in small print handwriting. Sometimes he jotted down thoughts on slips of paper that he left between the pages. Inside The Life of Saint Teresa of Ávila by Herself, first published in 1565, we found him musing philosophically: “There is an intelligence to the universe (of which we are fractal) and that intelligence has a character and that character is benign. Intends well toward all things. How could it not?”&lt;/p&gt;
    &lt;p&gt;McCarthy is known for the bleak, violent nihilism in many of his novels, so it was a surprise to see him describing the universe as intelligent and well-&lt;lb/&gt; intentioned. He was a lapsed Catholic who went back and forth on the question of God’s existence, sometimes changing his mind from one day to the next. &lt;/p&gt;
    &lt;p&gt;Peebles was collecting her favorite annotated books on the pool table. One was Realism in Mathematics by Penelope Maddy. In the margins, Mc-&lt;lb/&gt; Carthy summarizes the author’s points and comments on them, frequently disagreeing. “Gibberish,” he noted at one point. It was an exciting find for the scholars because McCarthy mined this book deeply for his final novel, Stella Maris. Its protagonist, Alicia Western, is a young mathematical genius with schizophrenia.&lt;/p&gt;
    &lt;p&gt;Another of Peebles’ favorite finds is an annotated copy of Shakespeare’s Hamlet. McCarthy, who once said, “the ugly fact is books are made out of books,” borrowed and altered elements from Hamlet for his 1979 novel Suttree. It was his most ornate and poetic book and the closest he ever came to writing autobiographically. The main character is a troubled dropout who has rejected a life of privilege and responsibility in Knoxville, Tennessee, where McCarthy grew up as the black sheep among six children in a well-to-do Catholic family with strong Irish roots.&lt;/p&gt;
    &lt;p&gt;He was born in 1933 and christened Charles Joseph McCarthy Jr. after his father. In his youth, he was known as Charlie, or sometimes Doc, until he changed his name to Cormac as a young man, partly inspired by the medieval Irish king Cormac mac Airt. The name change was probably also a declaration of independence from his father. Charles McCarthy Sr. was an attorney who became the chief counsel for the Tennessee Valley Authority, and Cormac always characterized him as a domineering, violent man who beat him viciously for trivial offenses. (His brother Dennis disputes this description and says that Cormac was “grossly exaggerating.“)&lt;/p&gt;
    &lt;p&gt;McCarthy told his own son John, and some of his friends, that the beatings started when he was 3 years old. Readers and critics have often wondered where the darkness and violence in McCarthy’s work comes from, and, if Cormac’s characterization is true, his childhood might account for some of it. He loved his mother, Gladys, but she was psychologically fragile and frequently absent from the family in mental health institutions.&lt;/p&gt;
    &lt;p&gt;He hated his Catholic school and loved roaming outdoors. Talking about his school days in a (rare) 1992 interview, McCarthy said, “There was no hobby I didn’t have, name anything, no matter how esoteric, I had found it and dabbled in it.” He made money trapping muskrats around Knoxville and selling the pelts, and somehow also established himself as an authority on antique American rifles.&lt;/p&gt;
    &lt;p&gt;In 1953, McCarthy dropped out of the University of Tennessee, where he was studying engineering and physics, and joined the Air Force. He was stationed in Anchorage, where he became the radio disc jockey for the base, and started reading in earnest in his spare time. After four years, he returned to the University of Tennessee but dropped out again and started writing novels.&lt;/p&gt;
    &lt;p&gt;The first three, The Orchard Keeper, Outer Dark and Child of God, were gothic tales set in the rural Appalachia he knew from his youth. In lyrical prose with marvelously rendered vernacular speech, they tackled dark subjects—murder, infanticide, incest, necrophilia—while displaying a reverence for nature and folk traditions. The fourth novel was Suttree, McCarthy’s richly comedic evocation of 1950s Knoxville. These novels earned critical praise and prestigious grants and awards, but each sold more poorly than the last.&lt;/p&gt;
    &lt;p&gt;One of the few details we have about McCarthy’s personal life comes from his second wife, Anne&lt;lb/&gt; DeLisle, an English singer and dancer, whom he met on a ship to Ireland in 1965. Their home was a partially converted dairy barn outside Knoxville; they bathed in a lake. “Someone would call up and offer him $2,000 to come speak at a university about his books,” she once said. “And he would tell them that everything he had to say was there on the page. So we would eat beans for another week.”&lt;/p&gt;
    &lt;p&gt;After leaving her without an explanation in 1974, McCarthy drifted around cheap motels with his typewriter, a pile of books and a light bulb for good reading light. In 1976 McCarthy took up residence in El Paso and turned his attention on the American Southwest and northern Mexico, setting himself the task of learning the culture, history, natural history, geology, folkways and distinctive Spanish idioms of the borderlands.&lt;/p&gt;
    &lt;p&gt;According to a letter he wrote, McCarthy read over 300 books to research Blood Meridian (1985), an ultraviolent philosophical Western based on the true story of a state-funded scalp-hunting gang in the 1840s and 1850s. Now widely regarded as his greatest masterpiece, it sold a pitiful 1,883 copies when it was first published. McCarthy’s fortunes changed with the publication of All the Pretty Horses in 1992. This elegiac Western, set in 1949 and 1950 in Texas and Mexico, became a best seller, won a National Book Award, and was adapted into a movie starring Matt Damon and Penélope Cruz. McCarthy followed with two more novels about drifting cowboys, then shifted course with No Country for Old Men (2005), a crime thriller that the Coen brothers turned into a quadruple-Oscar-winning movie starring Josh Brolin, Javier Bardem and Tommy Lee Jones. Next came The Road, a post-apocalyptic father-son journey that won the Pulitzer Prize for fiction in 2007 and was made into a film with Viggo Mortensen playing the father.&lt;/p&gt;
    &lt;p&gt;McCarthy had moved to Santa Fe with his third wife, Jennifer Winkley, and their young son John in 2001. He found the town off-puttingly liberal, moneyed and artsy, and moved there for one reason only: His great friend Murray Gell-Mann, the Nobel Prize-winning physicist, invited him to join the Santa Fe Institute, serving as a sort of in-house literary intellectual. This elite scientific think tank, co-founded by Gell-Mann, brings together some of the world’s most brilliant minds to research complex interconnected systems. McCarthy had long preferred the company of scientists to that of literary people, and he delighted in the high-flying conversations at the institute. He went there nearly every day to work on his writing and kept up with all the institute’s scientific research.&lt;/p&gt;
    &lt;p&gt;McCarthy was famous for refusing to discuss his work, so there was widespread amazement in literary quarters when he agreed to do a televised interview in 2007 with Oprah Winfrey, who had picked The Road as her book club selection. Viewers saw a courteous, gray-haired Southerner with a high-domed forehead and a flashing smile. When Oprah asked if he was “passionate” about writing, he replied, “Passionate sounds like a pretty fancy word.”&lt;/p&gt;
    &lt;p&gt;Oprah, knowing it was true, asked if The Road was a love story to his young son John. “In a way, I suppose, that’s kind of embarrassing,” he said. She made slightly better headway on the subject of punctuation. McCarthy didn’t use quotation marks, hated semicolons and kept commas to the barest minimum. “There’s no reason to block the page up with weird little marks,” he said. “If you write properly, you shouldn’t have to punctuate.”&lt;/p&gt;
    &lt;p&gt;McCarthy could pull it off because he was a virtuoso, renowned for his powers of description and ear for dialogue. The Nobel Prize-winning novelist Saul Bellow extolled McCarthy’s “absolutely overpowering use of language, his life-giving and death-dealing sentences.” McCarthy’s detractors, meanwhile, found his writing overly mannered, his characters overly masculine, and accused him of relishing the violence he wrote about so vividly.&lt;/p&gt;
    &lt;p&gt;When McCarthy died in June 2023, after battling leukemia, prostate cancer, dehydration and what he once called “the sheer velocity of time,” the accolades were immediate, and fulsome. Stephen King called him the “last great white male American novelist.” Sebastian Junger compared him to Mount Everest. The Guardian headlined its remembrance with a prophecy: “His work will sing down the centuries.”&lt;/p&gt;
    &lt;p&gt;Dennis McCarthy, the youngest of the six children, made his way through the book-choked hallway into the book-strewn living room. A retired lawyer, editor and conservation biologist, Dennis published his first novel in 2021, a spiritual Western about Billy the Kid. Now 81, he was fit and trim, with blue eyes, a radiant smile and a strong resemblance to Cormac. “He was my best friend for 70 years and a fabulous older brother who always looked out for me,” he said. “We were very, very close.”&lt;/p&gt;
    &lt;p&gt;I asked him which authors his brother most admired. “Moby-Dick was Cormac’s favorite book without question, and Faulkner was more of an influence than he liked to admit,” he said. “He loved Hemingway’s short stories, James Joyce, Dostoyevsky and Shakespeare of course.” Readers and scholars had already identified these literary forebears, but it was satisfying to hear them confirmed.&lt;/p&gt;
    &lt;p&gt;Of the many thousands of books in the house, the basement and the outbuildings, how many had McCarthy actually read? “If you exclude the encyclopedias and reference books, I would guess about 85 percent,” Dennis said. “Cormac kept on ordering books after he was too sick and frail to read, because it was a compulsion, but until that point he would read for hours and hours nearly every day. He never left the house without a book. He never left the house without a gun. Both were equally unthinkable.”&lt;/p&gt;
    &lt;p&gt;Why was he always armed? “He was a conservative country boy from the South who understood that the world is a dangerous place.” When he was 24, McCarthy accidentally shot himself in the leg while practicing alone on a gun range in Tennessee. Dennis didn’t know any more details, because his brother refused to discuss the incident, but it was likely a quick-draw gone wrong.&lt;/p&gt;
    &lt;p&gt;When I asked Dennis about his brother’s reputation as a recluse, he said it was totally inaccurate. “He was very sociable and could get along with anybody. Well, almost anybody. He didn’t suffer fools gladly, or people who rushed up to him gushing about his books. But he had a lot of friends, and he loved dining and conversation, and five-hour lunches that sometimes turned into ten-hour lunches.”&lt;/p&gt;
    &lt;p&gt;Those friends included physicists and quark-discoverers Gell-Mann and George Zweig, the whale biologist Roger Payne, the movie star Josh Brolin, plus a bar owner in Tucson who calls himself God and a silver-tongued con man from Knoxville named John Sheddan, who appears exactly as himself under his own name in McCarthy’s penultimate novel, The Passenger.&lt;/p&gt;
    &lt;p&gt;Brolin got to know McCarthy during the filming of No Country for Old Men and was at the author’s bedside the night before he died. “It was me, his ex-wife, his son John, and that was it,” Brolin tells me on the phone. “He was telling these wild stories, about drinking wine with André the Giant in Paris, and all this stuff was coming out totally lucid, sharp, funny, inspired. Then he would go into this lost dementia and he’d be grabbing at stuff that wasn’t there. Then he’d go to sleep, and then he’d wake up and tell another story.” Soon after Brolin left, McCarthy drew his final breath.&lt;/p&gt;
    &lt;p&gt;McCarthy’s son John, the model for the boy character in The Road, was now 26 and sleeping in his father’s old bedroom upstairs. He’s a licensed pilot, a composer and a musician. The first time I met John, he was coming sleepily down the wooden stairs in search of coffee. I had just learned that Dennis had emptied two storage units full of books in El Paso and two more in Santa Fe and moved the boxes into the house for cataloging. “So I’m getting a totally unrealistic picture of what the house was like when you were growing up here,” I said to John.&lt;/p&gt;
    &lt;p&gt;“Not really,” he said. “This is pretty much how it was. Boxes everywhere. Piles of books everywhere. The hallway stacked up with boxes with a little path through the middle. Whole rooms so full of books you couldn’t go in there. It didn’t bother me at all.”&lt;/p&gt;
    &lt;p&gt;It was John who told me that McCarthy worked in bed—a California king with high-thread-count sheets, the Olivetti on a wooden platform with a leather pillow underneath it, and piles of typed pages, magazines, books and catalogs. Writing, McCarthy once said, was not a conscious process for him. He put a blank piece of paper in the Olivetti, the words arrived, and he typed them down. But that was just the first stage of an extensive rewriting and structuring process, and some of his books took 20 years or more to get right.&lt;/p&gt;
    &lt;p&gt;“Dad didn’t like being interrupted when he was working, or when he was reading,” John said. “‘No, no, no. I’m reading. Go away!’ he would say. But he was a great father, always there for me, and I learned so much from him. We would have these long conversations about science and history and music, and whatever else, and he was the funniest person I’ve ever met, just a natural comedian.”&lt;/p&gt;
    &lt;p&gt;I asked John what else his father collected apart from books, cars and kitchenware. “I would say clothes were the other big one. He had hundreds of tweed jackets, hundreds of shirts, hundreds of suits that he’d never worn.” John once spent three days dragging stuff out of a room he wanted to use as a bedroom. “When I was done, I said, ‘You ever think you might be a little bit of a hoarder?’ And he looks at me and he goes, ‘Yeah, probably.’ He attributed it to all those years when he had no money.”&lt;/p&gt;
    &lt;p&gt;Dennis isn’t buying that explanation. “Cormac always lived in chaos, which I found fascinating because he had such a fabulous artistic sense. He could design things beautifully and he dressed impeccably, but his living quarters were always a disaster. He was an incredibly complicated individual.”&lt;/p&gt;
    &lt;p&gt;The cataloging scholars could only spare four or five days at a time. Then they would go back to their jobs for a few months and try to carve out another long weekend in New Mexico. The stalwarts were Peebles and Rick and Jonathan Elmore, whip-smart twin brothers who looked nothing alike, taught at different colleges and wrote academic papers together about McCarthy’s work.&lt;/p&gt;
    &lt;p&gt;The cataloging was dusty, repetitive, eye-straining work, but it was conducted with good humor and camaraderie, and you never knew what might come out of the next box. One afternoon, after looking through a batch about Cistercian abbeys, violin makers, metaphysics, meta-ontology, the incest taboo and the material foundations of ancient Mesopotamian civilization, I said, “Was there anything he wasn’t interested in? Sewing perhaps?”&lt;/p&gt;
    &lt;p&gt;“Nope,” said Jonathan Elmore, an English professor at Louisiana Tech University. “We’ve cataloged books on needlework and quilting.” Rick noted McCarthy’s keen interest in clothes and fashion, which could, I granted, be described as sewing-related. McCarthy was a longtime subscriber to the fashion and style magazine W, and he had annotated many of his books about menswear. In his copy of The Suit: A Machiavellian Approach to Men’s Style, McCarthy penciled his opinion of slip-on dress shoes: “disgusting.” Further down the same page, next to a sentence praising shiny-buckled monk-strap shoes, he wrote, “yet more horror.”&lt;/p&gt;
    &lt;p&gt;The scholars treated annotations like pieces of treasure and would read them aloud to each other. Inside Reclaiming History: The Assassination of President John F. Kennedy, they found notes on a slip of paper, including a line about the assassin’s bullet: “it was going like a bat out of hell when it left the president’s head and in that crowd it is a pure freak of chance that it didn’t take out a citizen-spectator.”&lt;/p&gt;
    &lt;p&gt;The historical figures who interested McCarthy the most, judging by the number of books he owned about them, were Albert Einstein (114 books), Winston Churchill (88) and James Joyce (78). Architecture is the dominant subject in the collection, with 855 books. The human being whom McCarthy most admired, Dennis confirms, was Ludwig Wittgenstein. The team cataloged a staggering 142 books by or about the philosopher, with a high proportion annotated.&lt;/p&gt;
    &lt;p&gt;McCarthy’s fascination with Wittgenstein came as a surprise to the scholars, but it makes sense. As Rick Elmore, a philosophy professor at Appalachian State University with floral tattoos climbing up his neck, puts it, “Wittgenstein was always asking how the systems we use to represent the world relate to the world we want to represent. It’s one of the central questions in McCarthy’s work.”&lt;/p&gt;
    &lt;p&gt;With the exception of Moby-Dick in multiple, gorgeous leather-bound editions, the scholars found hardly any novels until they started cutting open boxes that Dennis retrieved from a storage locker in El Paso. Out came the entire canon of Western literature, from ancient Greece and Rome to the best novelists, poets and essayists of the 1970s, nearly all in cheap, worn, paperback editions. “These are the books that he read in his 20s and 30s and maybe into his 40s, and he was broke that whole time,” said Dennis. “Once he got money, Cormac bought all his books in hardback if possible, and for the last 40 years of his life he read almost no fiction at all.”&lt;/p&gt;
    &lt;p&gt;Why? The answer stems from McCarthy’s deeply disparaging view of modern society, which he considered lost, divorced from nature, history and tradition and heading toward social collapse and apocalypse. “Cormac considered contemporary fiction a waste of time,” said Dennis, “because contemporary writers no longer have a legitimate culture to feed their souls.”&lt;/p&gt;
    &lt;p&gt;One afternoon, Dennis was marveling at McCarthy’s storytelling abilities and comedic talents, and I asked him if there was anything, apart from housekeeping, that his brother had been bad at. He thought about it for a moment and said, “Marriage.”&lt;/p&gt;
    &lt;p&gt;McCarthy was married and divorced three times. “His wives needed more than he gave them,” Dennis said. “The work always came first for Cormac. He loved those women, but he loved himself more. He was a narcissist. And if he hadn’t been a narcissist, he never would have achieved the same heights of artistic greatness.”&lt;/p&gt;
    &lt;p&gt;The most enduring love of McCarthy’s life was a woman named Augusta Britt. As she revealed last year in interviews with Vanity Fair magazine, they began a sexual relationship when she was 17 and he was 43, and he took her to Mexico to evade the FBI, who were after him for statutory rape and Mann Act violations. Britt has said she didn’t feel sexually exploited and credits McCarthy for saving her life by rescuing her from an abusive situation in Tucson, but some readers and commentators have found McCarthy’s behavior with her beyond the pale. (Britt declined to comment for this piece.)&lt;/p&gt;
    &lt;p&gt;McCarthy and Britt were together as a couple for about four years. Even after they split up, “He never stopped loving her,” Dennis said. “He continued to see her on a regular basis, and they maintained a close relationship for the rest of his life.”&lt;/p&gt;
    &lt;p&gt;Piece by piece, the inscrutable mystique that McCarthy built around himself is falling away. Two biographies are on the way to publication, one by a friend of McCarthy’s named Laurence Gonzales, the other by literary biographer Tracy Daugherty, and Britt might collaborate on a book with Vincenzo Barney, who wrote her story in Vanity Fair. We also have McCarthy’s library, which perhaps more than any other source can illuminate the mind of the man who, as Peebles says, “built his life on books.”&lt;/p&gt;
    &lt;p&gt;On the first day of the final cataloging session, Peebles let out a hooting sound upon finding a dead bat at the bottom of a box. The downstairs of the house had been steadily accumulating dust for more than two years, since McCarthy’s death, and it was still crammed with books. The McCarthy scholars—Cormackians, as they call themselves—repacked the cataloged books, wrote the date and “Cataloged CMS” for Cormac McCarthy Society, and stacked them up wherever space could be found. The annotated books went into separate boxes marked “annotated” or were piled up on the pool table. The dead bat was left in the bottom of a book box.&lt;/p&gt;
    &lt;p&gt;When the project began, Peebles had hoped that all the books could be kept together in a single collection in some sort of Cormac McCarthy memorial building, but that wasn’t panning out. Dennis had arranged for the annotated books to join his brother’s papers, which include the notes and drafts for his entire body of work, at the Wittliff Collections archive at Texas State University. The Santa Fe Institute wanted a selection of the most intellectually rigorous academic books for a small library it was planning to build in honor of McCarthy. The rest of the books were going to the University of Tennessee in Knoxville, where he enrolled twice and failed to graduate.&lt;/p&gt;
    &lt;p&gt;In the digital realm, however, McCarthy’s library will live on as a complete entity, and the public will be able to inspect its cataloged titles free of charge. “Our goal, right from the outset, was to create an open-access database listing all the books in his collection,” Peebles said. “Anyone who wants to know what books McCarthy was reading, and whether he annotated them, will be able to log on and access that information.” The University of South Carolina Press has agreed to partner with Peebles to create a website for this purpose, and to publish a monograph by Peebles about the cataloging project. There’s talk of scanning all the annotations at some point and making them available on the website, but that is still theoretical.&lt;/p&gt;
    &lt;p&gt;Almost exactly a year after the project began, Peebles opened the very last box. Perhaps the best adjective for its contents is Cormackian. Peebles pulled them out and announced books about Mexican architecture and the French Renaissance court, Kierkegaard’s metaphors and the Texas Rangers, the neurobiology of mental illness, architecture and society in Normandy from 1120 to 1270, and the Gun Digest book of assault weapons.&lt;/p&gt;
    &lt;p&gt;She was unable to calculate the total number of books because the cataloging software didn’t account for multi-volume works. McCarthy’s 36-volume history of Utah, for example, registered as a single entry. Nor did the software tally multiple editions of the same book, so McCarthy’s 13 copies of Moby-Dick registered as one entry. The total number of entries was 18,520. Taking into account duplicate copies and multi-volume works, Peebles felt confident that McCarthy’s library contained just over 20,000 books, with 2,170 annotated.&lt;/p&gt;
    &lt;p&gt;Driving away from the house, with the taste of old book dust in my mouth, I marveled at the extraordinary force of McCarthy’s curiosity. I thought about the books on acousto-optics and lay intellectuals in the ninth-century Carolingian Empire. The $2,200 he spent on eight volumes of Horace Walpole’s collected letters. The $10,000 in several uncashed royalty checks that he used as a bookmark in the memoir of William Faulkner’s niece. To peer into someone’s library is to peer into their brain, and here, it seemed, was a mind that wanted to know everything.&lt;/p&gt;
    &lt;p&gt;Editors’ note: After the print version of this story was published, this version of the piece was updated with further comment from Dennis McCarthy. Also, on September 8, 2025, this article was updated with further details about the University of South Carolina Press’ involvement in this archival project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.smithsonianmag.com/arts-culture/two-years-cormac-mccarthys-death-rare-access-to-personal-library-reveals-man-behind-myth-180987150/"/><published>2025-10-01T23:06:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45445114</id><title>Keyhive – Local-first access control</title><updated>2025-10-02T12:18:15.646935+00:00</updated><content>&lt;doc fingerprint="a6f991d21d2583d4"&gt;
  &lt;main&gt;
    &lt;p&gt;Keyhive is a project exploring local-first access control. It aims to provide a firm basis for secure collaboration, similar to the guarantees of private chat but for any local-first application.&lt;/p&gt;
    &lt;p&gt;In this lab notebook, we’ll share snippets of our findings as we explore the problem space and prototype potential solutions.&lt;/p&gt;
    &lt;p&gt;The entries start from the beginning, but you can jump to the most recent post: 05 · Syncing Keyhive&lt;/p&gt;
    &lt;p&gt;As the local-first ecosystem matures, the contexts that local-first applications fill has also expanded. Local-first emphasizes collaboration, but the constraints on an application are different if you build an application for you and a handful of friends versus delivering a team-oriented product. Your data not being viewable or editable by everyone in the world is a basic requirement of applications ranging from planning a surprise party, corporate meeting notes, book drafts, and legal contracts.&lt;/p&gt;
    &lt;p&gt;Today’s most common access control patterns assume a central server. While cloud auth tools are forever developing, generally speaking existing tools for cloud auth are very mature. Doing access control without a cloud auth server requires rethinking the underlying mechanics of how auth works. Keyhive is an attempt to do secure and efficient local-first auth while retaining the user experience found in familiar applications like Google Docs, Dropbox, GitHub, and Discord. We believe that these are table stakes for the next generation of local-first applications.&lt;/p&gt;
    &lt;p&gt;We’ve seen user-agency principals successfully applied to other contexts. Signal popularized end-to-end encrypted chat while retaining much of the convenience of less-secure messaging applications. We find ourselves asking “what would Signal for documents look like?”&lt;/p&gt;
    &lt;p&gt;Least Surprise&lt;/p&gt;
    &lt;p&gt;Unlike a cloud auth system which can depend on the network to keep data hidden behind a web API, local-first runs a complete copy of the application at each replica. What are the correct bounds on access control when everyone has direct access to all of the content? Ultimately access control is about collaboration. Collaboration and access control can be seen as two sides of the same topic: who do you want to collaborate with, in which ways, and for how long?&lt;/p&gt;
    &lt;p&gt;CRDTs try to merge data in the least surprising way possible. For example, concurrent text will merge to produce the same data on all replicas, but the resulting paragraphs may not make sense next to each other. Users then fix these semantic errors manually. We believe that this is a major improvement over the user experience of something like Git, which often gets stuck and demands user intervention.&lt;/p&gt;
    &lt;p&gt;The equivalent situation exists for concurrent access control, but the stakes are higher. Preventing your friend from learning that you’re planning a surprise party, or opposing legal councel from altering your case prep are both important, and it should be clear how they will behave despite any underlying concurrency. The behavior of an access control system should be as clear to the end user as possible. Since there is no single source of truth about who can do what at any given time, the rules themselves need to be straightforward.&lt;/p&gt;
    &lt;p&gt;Out of Obscurity&lt;/p&gt;
    &lt;p&gt;Often local-first applications today depend on “security through obscurity”. For example, by default you can write into any Automerge document that you know the document ID for. This style is sometimes called “Swiss number” or “Rumpelstiltskin” security. It works as long as the document ID is only ever shared with people that you want to collaborate with, your security is all-or-nothing, and you never want to later remove someone from a document. If the document ID leaks (e.g. someone posts it to Bluesky), then the document is world-writable.&lt;/p&gt;
    &lt;p&gt;In lieu of a widely-adopted1 purely local-first access control system, some teams have tried leveraging existing auth methods by routing updates through a cloud auth server (e.g. OAuth login and auth logic in a server). Others have opted to emphasize decentralized user agency by using a blockchain to store access control policies. Both of these approaches require a network connection in order to check if an update is valid, which is not local-first. Bringing access control features to a local-first context requires rethinking how authority flows between nodes.&lt;/p&gt;
    &lt;p&gt;What we want is a system that retains the best of the above: the self-certification of Rumpelstiltskin, the power of auth servers, and the user agency of decentralized solutions. Following the definition of local-first, applications should accept updates after arbitrarily long periods of disconnection. Extending that requirement to access control means the ability to revoke access or have finer grained control (e.g. read vs write) requires tracking who has authorization to do what, and at which point in the document’s history.&lt;/p&gt;
    &lt;p&gt;Today’s cloud services have very mature access control features. These systems depend on a key architectural detail: they are able to rely on encapsulation by taking advantage of the network boundary. Since data is not available to read or write directly by the client, a privileged guard process is able to apply arbitrary access control rules. This process retrieves and/or mutates data on behalf of clients.&lt;/p&gt;
    &lt;p&gt;This power unfortunately comes at a price: since auth is on the hot path of every request — and generally depends on a central single-source-of-truth auth database — authorization at scale often bottlenecks overall application performance. And yet, an attacker that is able to bypass the auth part of the request lifecycle has unmitigated access to arbitrarily read, change, or delete the application’s data. This is to say nothing of the complexity of building, deploying, and maintaining cloud architectures to get that network boundary in the first place!&lt;/p&gt;
    &lt;p&gt;For local-first software to be successful in many production contexts, it needs to provide similar features without relying on a central authorization server. The local-first setting does not have the luxury of a network boundary: access control must travel with the data itself and work without a central guard.&lt;/p&gt;
    &lt;p&gt;There are also some tricky edge cases due to causal consistency. What should happen to honest operations that causally depend on content that’s later discovered to be malicious? What is the best strategy to handle operations from an agent that was revoked concurrently, especially given that “back-dating” operations is always possible. If a document has exactly two admins (and many non-admin users), what should happen if the admins concurrently revoke each other (for instance, one is malicious)?&lt;/p&gt;
    &lt;p&gt;To address the above challenges, we’ve started work on Keyhive: a project focused on local-first access control. Our goal is to design and build a production ready instance of such a system which is general enough for most local-first applications.&lt;/p&gt;
    &lt;p&gt;To date, the local-first ecosystem has primarily used a purely pull-based model where users manually decide which changes to accept. This approach is often sufficient for personal projects: each user can manually decide which peers to connect to and which changes should be applied. On the other hand, many production contexts have lower trust, require higher alignment, and are ideally low touch enough so that it’s not up to each person in a large organization to separately and manually infer who to trust. As a rough north star, we’re keeping the following use cases in mind:&lt;/p&gt;
    &lt;p&gt;Publishing (publicly visible data with restricted edits, like a blog)&lt;/p&gt;
    &lt;p&gt;Journalists &amp;amp; activists: small-to-medium groups, high risk&lt;/p&gt;
    &lt;p&gt;Cryptography has a reputation for being slow, especially if there’s crypto-heavy code running on low-powered devices. To have a performance margin that can cover a large range of practical use cases, Keyhive aims to run efficiently over at least ten-of-thousands of documents, millions of readers, thousands of writers, and hundreds of admins/superusers.&lt;/p&gt;
    &lt;p&gt;Since authorization, authentication, and identity are often conflated, it is worth highlighting that Keyhive deliberately excludes user identity (i.e. the binding of a human identity to an application’s identifier like a public key). In our initial community consultations we found that there are many different identity mechanisms that developers downstream of Keyhive would like to use. As such, we’re designing the system to be decentralized and secure, and leave name registration/discovery and user verification (e.g. email or social) to a future layer above Keyhive.&lt;/p&gt;
    &lt;p&gt;The following are left out of our design goals:&lt;/p&gt;
    &lt;p&gt;Constraining downstream applications to use a small predefined set of policies or roles&lt;/p&gt;
    &lt;p&gt;Interactive protocols (since local-first must work under network partition)&lt;/p&gt;
    &lt;p&gt;Most client/server backends place data at the bottom, and compute over it. In that model, auth is just another kind of computation. Leaving access control to a central process is not possible in a local-first context. In our context, the auth layer must act as a foundation.&lt;/p&gt;
    &lt;p&gt;Static authorization typically impacts the design of all other layers of a project. As an intuition, the storage layer will need to support data that is encrypted-at-rest, and so its design has a dependency on the auth layer. This means that since the design of an authorization mechanism may impose downstream constraints, its design should consider such potential impacts on the design of the rest of the stack. As much as possible, this project attempts to minimize imposing such constraints on other layers.&lt;/p&gt;
    &lt;p&gt;Keyhive (as currently designed) carves out three layers to handle this:&lt;/p&gt;
    &lt;p&gt;Convergent Capabilities: A new capability model appropriate for CRDTs, and sits between object- and certificate-capabilities&lt;/p&gt;
    &lt;p&gt;A Group Management CRDT: Self-certifying, concurrent group management complete with coordination-free revocation&lt;/p&gt;
    &lt;p&gt;E2EE with Causal Keys: With post-compromise security (PCS) and symmetric key management granting access to causal predessesors.&lt;/p&gt;
    &lt;p&gt;These three have a strong dependency between each other. Capabilities enable use to manage groups, and groups let us share keys for E2EE. We will go into more detail on all three in future posts, but in the meantime here is a very high level treatment:&lt;/p&gt;
    &lt;p&gt;Capabilities and delegation form the basic access control mechanism that are known to be very expressive. In short: all Automerge documents get identified by a public key, and delegate control over themselves to other public keys. This provides stateless self-certification with a cryptographic proof. Public keys in the system can represent anything: other documents, users, groups, or anything else. This is a very low-level mechanism that can be used to model high level concepts like powerboxes, roles, device groups, and more with very little code, all while remaining extensible to new patterns.&lt;/p&gt;
    &lt;p&gt;Object-capabilities (AKA “ocap”) are “fail-stop”, meaning that they intentionally stop working if there’s a network partition to preserve consistency over availability. Since local-first operates under partition (e.g. offline), parts of the classic object-capability design are not suitable. Certificate capabilities such as SPKI/SDSI, zcap-ld and UCAN are partition-tolerant, but depend on stateless certificate chains which is highly scalable but somewhat limits their flexibility. We propose a system between the two: convergent capabilities (“concap” for short) which contain CRDT state to get the benefits of both while retaining suitability for local-first applications.&lt;/p&gt;
    &lt;p&gt;Concurrent access control will always have some tricky situations. The big obvious ones are what to do if two admins concurrently revoke each other, or happened if operations depend on others that were revoked, and how to handle maliciously back-dated updates. There is quite a lot to discuss on this topic, so we’ll leave it for a future post.&lt;/p&gt;
    &lt;p&gt;Groups are built on top of convergent capabilities. They’re “just” a thin design pattern, but help model things like user devices, teams, and more. By following the delegations between groups, we can discover which public keys have what kind of access to a certain document. This provides a handy abstraction over teams and user devices. By following the links, it both lets a writer know who has read access (i.e. who to share keys for the latest E2EE chunk with), and lets the trust-minimized sync engine know which documents the current device can request from the server.&lt;/p&gt;
    &lt;p&gt;Data in Keyhive is encrypted-at-rest. Encrypting every Automerge operation separately would lead to very large documents that cannot be compressed. Instead we use the Automerge Binary Format to compress-then-encrypt ranges of changes. We expect these encryption boundaries to change over time as parts of the document become more stable, so we need a way to manage (and prune) a potentially large number of keys with changing envelope boundaries.&lt;/p&gt;
    &lt;p&gt;We achieve the above by including the keys to all of their causal predessesor chunks. This sacrifices forward secrecy (FS) — leaking old message keys in the case of a later compromised key — but retains secrecy of concurrent and future chunks. Of course “leaking” anything sounds bad. However, unlike ephemeral messaging (e.g. Signal) where not all users are nessesarily expected to have the entire chat history, CRDTs like Automerge require access to the entire causal history in order to render a view. This means that in all scenarios we need to pass around all historical keys, whether or not they’re in the same encryption envelope. We believe that this choice is appropriate for static control context on documents that require the entire history. As a nice side-effect of this choice, we also gain flexibility and simplicity.&lt;/p&gt;
    &lt;p&gt;In this design, keys behave a bit like pointers, so we can apply all of the standard data structure pointer indirection tricks to do smooth updates to encryption boundaries. This is fairly well-developed at this stage, so we will save a deeper exploration of this topic for a future post.&lt;/p&gt;
    &lt;p&gt;E2EE raises a new issue: there is no such thing as perfect security. All encryption algorithms are deemed secure with respect to some explicitly-defined assumptions (such as the difficulty of factoring large primes or group operations). There may be mathematical breakthroughs, edge cases discovered, or new hardware that render your choice of encryption algorithm useless. Even more worse, keys can be accidentally leaked or devices stolen. While we can revoke future write access, if someone has the data and the symmetric key, then they have the ability to read that data. The best practice is to have defense in depth: don’t make ciphertexts retrievable by anyone, but only those with “pull access” or higher. “Pull” is weaker than the more familiar “read” and “write” access effects: it’s only the ability to retrieve bytes from the network but not decrypt or modify them. This is especially helpful for trust-minimizing sync servers, since by definition they cannot have the ability to see the plaintext if we want to claim E2EE.&lt;/p&gt;
    &lt;p&gt;If we want to move towards an ecosystem of interchangeable relays, minimizing trust on such relays is a must. Our approach (perhaps unsurprisingly) is to end-to-end encrypt the data, removing read access from sync servers altogether. Under this regime, sync engines are “merely” a way to move random-looking bytes between clients.&lt;/p&gt;
    &lt;p&gt;There is another ongoing project at the lab focused improving data synchronization for peer-to-peer and via sync servers. We’ve realized that sync and secrecy strongly interact. Broadly speaking, sync protocols benefit from more metadata (to efficiently calculate deltas), but cryptographic protocols aim to minimize or eliminate metadata exposure. This tension extends across related systems, including merging E2EE compressed chunks, and determining if a peer has already received specific operations when a sync server cannot access them in plaintext.&lt;/p&gt;
    &lt;p&gt;Fortunately, combining these systems can sometimes result in more than the sum of their parts. For instance, convergent capabilities help facilitate the calculation of which documents are of interest to particular agent, helping the sync system know which documents to send deltas of. For these reasons, we’re treating synchronization and authorization as part of a larger, unified project, even though each will yield distinct artifacts.&lt;/p&gt;
    &lt;p&gt;Cryptographic code is notoriously difficult to debug, so we decided to start with design and move to code when we had some fairly good theories on how the basics of this system should work. Now that we’re at that point, we’ve very recently begun to implement this design. We’ll report on our progress in future posts, as well as dive deeper into some of the topics we touched on in the overview here.&lt;/p&gt;
    &lt;p&gt;As we’ve seen in past lab notes, Keyhive provides access control for local-first applications. We support both server-based collaboration and peer-to-peer operation without a trusted server. And individuals might work offline for extended periods of time. In the context of Automerge, our goal is to control access to documents, collections of documents, and parts of documents.&lt;/p&gt;
    &lt;p&gt;Every document has a group of users with access to that document. That group might include other groups as members (in which case the members of those groups are also members of the document). Importantly, a document group’s membership is dynamic, with new members added and removed over time. We must be able to handle concurrent changes in a distributed context.&lt;/p&gt;
    &lt;p&gt;Of course, if we want to limit read access to just our group, we can’t safely share our document as plaintext via sync servers. We need a way to encrypt and decrypt our data that is accessible to only our document’s members. This means we need a way for our group to agree on the keys that will be used for encryption and decryption over time.&lt;/p&gt;
    &lt;p&gt;In the literature, this problem is known as Continuous Group Key Agreement (CGKA). A CGKA protocol enables a dynamic group to agree on a sequence of keys over time. CGKAs ordinarily guarantee two properties: forward secrecy (FS) and post-compromise security (PCS). Imagine a successful attacker compromises a single key. In the simplest terms, forward secrecy means that this key will not enable access to past data. And post-compromise security means it will not enable access to future data. If you can guarantee both, then you can limit the damage from a key compromise.&lt;/p&gt;
    &lt;p&gt;One way to achieve forward secrecy is through “ratcheting”. With a ratchet, honest users employ a key derivation function (KDF) to deterministically transform a key in a way that is effectively impossible to reverse. A cryptographic hash function is one way to achieve this. Ratcheting with such a one-way function prevents an attacker from discovering past keys since there is no feasible way to reverse the function. But a one-way function on its own does not prevent an attacker from discovering future keys, since you can derive all future keys from a compromised key by repeatedly applying the hash function.&lt;/p&gt;
    &lt;p&gt;Of course, we don’t want a system that once compromised is always insecure. That’s where post-compromise security comes in. The intuitive idea is that a system with post-compromise security has some mechanism to deny access after an attack. Compromised information will no longer be enough to derive future keys. One way to achieve this is to periodically rotate information required for determining future keys in a way that is not accessible to a past attacker.&lt;/p&gt;
    &lt;p&gt;In practice, ratcheting protocols mix in fresh information with each ratchet so that knowledge of a key is not by itself sufficient to derive future keys. For example, Signal’s Double Ratchet protocol includes sending a Diffie-Hellman public key with each message so that the receiver can derive a shared Diffie-Hellman secret to use as a side input to the key derivation function (KDF) that is used for ratcheting.&lt;/p&gt;
    &lt;p&gt;The current Message Layer Security (MLS) protocol for CGKA uses TreeKEM, a protocol for asynchronous, decentralized key agreement for dynamic groups1. TreeKEM uses a binary tree with group members’ public keys at the leaves and the current group secret encrypted at the root. All other inner nodes act like the root for their subtrees (and subtrees act like subgroups with their own shared, encrypted secrets). Members can be dynamically added and removed from the tree.&lt;/p&gt;
    &lt;p&gt;For post-compromise security, each member periodically rotates out its public keys on its leaf, which leads to cascading secret updates all the way to the root. Both updating and decrypting the root secret requires traversing the path from the member’s leaf to the root, performing log(n) operations (although there is a linear worst case under certain conditions).&lt;/p&gt;
    &lt;p&gt;Unfortunately, Keyhive’s requirements rule out TreeKEM as it stands. That’s because TreeKEM depends on a central server to create a total order of operations, and to pick winners among concurrent operations. Keyhive’s local-first model is peer-to-peer compatible and does not require such a central server. And for Keyhive, concurrent operations can be merged in long after they were actually performed (for example, if a member made changes while aboard a long-haul flight).&lt;/p&gt;
    &lt;p&gt;An alternative that is more aligned with our requirements is the Decentralized Continuous Group Key Agreement (DCGKA) protocol developed by Matthew Weidner and Martin Kleppmann. This protocol assumes a decentralized network that does not depend on a trusted central server. However, unlike TreeKEM, it provides linear rather than logarithmic performance. As a result, they target groups on the order of 100 members as opposed to MLS’s target of 50k members. A design goal for Keyhive is to target at least thousands of members2.&lt;/p&gt;
    &lt;p&gt;Matthew Weidner has also proposed an alternative to TreeKEM called Causal TreeKEM. Whereas TreeKEM requires a total order imposed by a central server, Causal TreeKEM only requires a causal order, which is much better suited to a decentralized network. Like TreeKEM, it has logarithmic performance (with a linear worst case) and is meant to ensure both forward secrecy and post-compromise security.&lt;/p&gt;
    &lt;p&gt;However, Causal TreeKEM depends on fancier crypto than we’d prefer in order to merge concurrent updates in any order. It requires a cryptographic operation to combine updates at a node that is both associative and commutative. One option here would be BLS, but this is far less common than the standard options and there is not currently a great library option for Rust (the language Keyhive is written in). And we have definitely ruled out rolling our own crypto (you probably should too).&lt;/p&gt;
    &lt;p&gt;For these reasons, we’ve proposed our own alternative3 for Keyhive that we call “BeeKEM”. It is closely modelled on TreeKEM with insights from Causal TreeKEM. It requires no central server and only a causal order of operations. It provides logarithmic performance (with linear worst case). And like the other TreeKEM variants, it provides forward secrecy and post-compromise secrecy4. Furthermore, it relies exclusively on standard crypto, such as Diffie Hellman key exchange and BLAKE3 hashing.&lt;/p&gt;
    &lt;p&gt;In this section, we’ll see how BeeKEM works in more detail.&lt;/p&gt;
    &lt;p&gt;In BeeKEM (as in TreeKEM), the current group secret is stored encrypted at the root node of a binary tree. We’ll call this the “root secret”. The root secret is used for encrypting and decrypting document chunks shared with our group over the network5.&lt;/p&gt;
    &lt;p&gt;Each leaf of the tree corresponds to a member of the group and contains its ID and latest Diffie Hellman (DH) public key. A member’s ID is persistent over time but each member will periodically rotate its DH public key. When a member rotates its DH public key, that will cause the root secret to change as well. Thus, member key rotations help provide post-compromise security. From the point of view of an adversary, they introduce fresh randomness.&lt;/p&gt;
    &lt;p&gt;Each leaf has an implicit secret known only to the corresponding member (i.e. not stored in the tree). All other “inner” nodes in the tree contain a DH public key for that node and a corresponding secret key that is stored encrypted at the node.&lt;/p&gt;
    &lt;p&gt;Each node in a binary tree has a single sibling node, as illustrated in the following diagram:&lt;/p&gt;
    &lt;p&gt;When encrypting or decrypting a new secret at a parent node, a child node performs a Diffie Hellman key exchange with its sibling. That means it will use its sibling DH public key and its own secret key to derive what we’ll call a “shared DH secret”. The shared DH secret is used to encrypt and decrypt the new secret at the parent.&lt;/p&gt;
    &lt;p&gt;A brief (simplified) aside on how Diffie Hellman key exchange works. Imagine Alice and Bob each have their own DH public keys (alice_pk and bob_pk) and DH secrets (alice_sk and bob_sk). If Alice combines her DH public key with Bob’s secret key, she can derive a shared DH secret. If Bob combines his DH public key with Alice’s secret key, he can derive the same shared DH secret. In this way, they can agree on a shared secret just by exchanging their public keys in the open.&lt;/p&gt;
    &lt;p&gt;We use this same principle to derive a shared DH secret for any sibling pair in our tree. For example, to decrypt Alice’s parent node, Alice can use its secret alice_sk and its sibling’s public key bob_pk to derive a shared DH secret. It can then use that shared secret to decrypt the secret at the parent node.&lt;/p&gt;
    &lt;p&gt;That parent secret can in turn be used for a Diffie Hellman exchange with the parent’s sibling’s DH public key.&lt;/p&gt;
    &lt;p&gt;For a member to decrypt the root secret, it must start from its leaf and traverse the tree one parent at a time until it reaches the root. The sequence of nodes from leaf to root is called that leaf’s “path”. At each node in its path, it will derive a shared DH secret with its sibling to decrypt the secret at its parent. Once it’s decrypted the root secret, it’s done.&lt;/p&gt;
    &lt;p&gt;In the following diagram, the decrypting leaf’s path is marked in green. The siblings used as Diffie Hellman partners along the way are marked in purple:&lt;/p&gt;
    &lt;p&gt;There are three mutating operations that can be performed on the tree: Update Key (i.e. key rotation), Remove Member, and Add Member. Let’s look at these in more detail.&lt;/p&gt;
    &lt;p&gt;Every member must periodically update the DH public key at its leaf in order to guarantee post-compromise security. When we update our leaf DH public key, we must then update the secrets for all the nodes on our path, eventually updating the root secret for the entire group.&lt;/p&gt;
    &lt;p&gt;Before traversing our path, we can derive a sequence of path secrets by applying BLAKE3’s key derivation function to an initial secret once for each node on the path. As we move up each parent on our path, we will encrypt the next derived secret and store it on that parent.&lt;/p&gt;
    &lt;p&gt;In order to encrypt the secret for a parent, we use Diffie Hellman key exchange as described above. We then derive a new Diffie Hellman public key from the secret for the parent, and store both that new DH public key and the corresponding encrypted secret at the parent.&lt;/p&gt;
    &lt;p&gt;Later on, when the sibling wants to decrypt that parent secret, it can do Diffie Hellman the other way, using the encrypter node’s DH public key with the sibling node’s secret to derive the same shared DH secret that was used to encrypt the parent.&lt;/p&gt;
    &lt;p&gt;In order to explain membership changes, we must introduce the concept of “blanking” a node. Blanking a node means that we remove all key and secret information from that node. If the root node is blank, then the tree does not currently hold a shared group key. Some nodes are blanked after membership change operations, and all leaves beyond the last member leaf on the right are blank.&lt;/p&gt;
    &lt;p&gt;If a tree has a blank root, then at least one member must perform an Update Key operation to restore a root secret. An update will replace all blank nodes on its update path with key information.&lt;/p&gt;
    &lt;p&gt;When we perform a Remove Member operation, we first blank the leaf corresponding to that member. We then blank the entire path from that leaf up to the root node.&lt;/p&gt;
    &lt;p&gt;Notice that if a removed member performs an update concurrently with its removal, we need to ensure that the update does not survive (or else the member will continue to have access to the root secret). When merging concurrent removes with other operations, BeeKEM ensures that the remove paths are blanked after all other concurrent operations are merged.&lt;/p&gt;
    &lt;p&gt;When we perform an Add Member operation, we add the new member’s ID and public key to the next blank leaf on the right. We then blank the path from that leaf to the root.&lt;/p&gt;
    &lt;p&gt;Notice that if two members add a member concurrently to the same tree, they will add them to the same leaf. BeeKEM resolves such conflicts on merge by sorting all concurrently added leaves and blanking their paths.&lt;/p&gt;
    &lt;p&gt;So far, we’ve assumed that every node has a sibling with key information. That’s what allowed us to use Diffie Hellman to derive a shared DH secret. But what happens when a node’s sibling is blank?&lt;/p&gt;
    &lt;p&gt;In that case, we must find the blank node’s resolution. A node’s resolution is the set of its highest non-blank descendents. Here’s an example:&lt;/p&gt;
    &lt;p&gt;If you have a blank sibling, you must do a separate encryption of the new parent secret for every member of your sibling’s resolution. For each of those members, you use its Diffie Hellman public key with your secret to derive a shared DH secret. You then encrypt the new parent secret using that shared secret and store it for that member.&lt;/p&gt;
    &lt;p&gt;This means that if the resolution of a sibling node contains 5 members, you will need to store the parent secret 5 times, each one encrypted for a separate member.&lt;/p&gt;
    &lt;p&gt;The worse case scenario is if the entire inner tree is blank. Encrypting a new path will no longer be a logarithmic operation since every leaf will be contained in the resolution of some blank node on your path. Instead, the cost will be linear in the number of leaves: you will have to perform a separate encryption for every other leaf somewhere on your path.&lt;/p&gt;
    &lt;p&gt;When decrypting a leaf with blanks on its path, you simply skip those blanks. This works because the highest blank in your path will contain its last non-blank descendent in its resolution. So when you encounter a blank on your path, you hold onto the last secret you’ve seen and start skipping. When you eventually get to a non-blank node, you’ll use that secret you’re holding onto to derive the shared DH secret you need to decrypt the non-blank parent.&lt;/p&gt;
    &lt;p&gt;Keyhive assumes that concurrent operations can be merged in any causal order. Concurrent updates will always have some overlapping nodes in their paths (at least the root is shared by all paths). How does BeeKEM resolve these conflicts?&lt;/p&gt;
    &lt;p&gt;We must first consider our potential vulnerabilities. Imagine that an adversary has compromised a group member and their leaf secrets. They can use a compromised leaf secret to decrypt the root secret at some point in time. Recall that knowing a leaf secret means you can decrypt all of the inner node secrets along your path.&lt;/p&gt;
    &lt;p&gt;If an adversary knows the secret for a leaf, it’s possible they will continue to be able to decrypt the group secret even if that leaf is rotated during a future concurrent update. This depends on how we handle merging concurrent updates.&lt;/p&gt;
    &lt;p&gt;If we just naively pick a winner for updates to a series of overlapping nodes, then the new information added by the loser’s key rotation will no longer be necessary to decrypt the root secret. We effectively forget that information.&lt;/p&gt;
    &lt;p&gt;Notice that the winner used the outdated keys from the loser for its update (since the winner’s and loser’s updates were concurrent). That means an adversary with the loser’s outdated leaf keys will still be able to decrypt the winner’s nodes. Subsequent updates by other leaves that only intersect with the winner’s path will also fail to exclude our adversary.&lt;/p&gt;
    &lt;p&gt;In BeeKEM, when merging concurrent updates, we ensure that all updates contribute information along their entire paths. We keep conflicting information around at each node until it is overwritten by a causally subsequent operation (or blanked by a membership change).&lt;/p&gt;
    &lt;p&gt;If two leaves update the same node concurrently, then they would have each written a distinct Diffie Hellman public key and encrypted secret to that node. In this scenario, we call these “conflict keys” and store them both when merging conflicts.&lt;/p&gt;
    &lt;p&gt;Imagine a member subsequently updates the tree. If a node on its leaf’s path has a sibling with conflict keys, this means there is an unresolved merge at that sibling. An adversary could have access to both sides of the corresponding fork. So it wouldn’t be secure to use those conflict keys for Diffie Hellman. Instead, we take the resolution of the node, just as we did with blank nodes. We then separately encrypt the secret for every DH public key in the resolution, again just as we did with blank nodes.&lt;/p&gt;
    &lt;p&gt;This means that for BeeKEM we update the definition of the “resolution of a node” to mean either (1) the single DH public key at that node if there is exactly one or (2) the set of highest non-blank, non-conflict descendents of that node.&lt;/p&gt;
    &lt;p&gt;If we merged in both sides of a fork, then we know we’ve updated both corresponding leaves with their latest rotated DH public key. Since taking the resolution skips all conflict nodes, it ensures that we integrate the latest information when encrypting a parent node. That’s because any non-conflict nodes have successfully integrated all causally prior information from their descendents.&lt;/p&gt;
    &lt;p&gt;This means an adversary needs to compromise one of the latest leaf secrets to be able to decrypt an entire path to the root. Even knowing outdated leaf secrets at multiple leaves will not be enough to accomplish this. An honest user, on the other hand, will always know the latest secret for its leaf.&lt;/p&gt;
    &lt;p&gt;During a future update (key rotation), if you find a conflict key node on the path you’re updating, you can remove all conflict keys at that node and replace them with a single new public key and encrypted secret (as with normal parent encryption). That’s because your update operation is the causal successor of all the operations that placed those conflict keys. This means your tree contains the necessary information from all of those past updates, which is integrated into your update.&lt;/p&gt;
    &lt;p&gt;BeeKEM’s approach comes with two downsides. First, before conflicts are resolved by subsequent updates or blanks, we must store extra information at each conflict node. Second, conflict keys add extra encryption and decryption overhead. In the worst case, where the tree is populated with the maximum number of possible conflict keys, the space cost would be n log(n) (as opposed to the best case of 2n). The time cost in the worst case would be linear (as opposed to logarithmic), as when the tree is maximally blanked. Our current set of benchmarks reflect these time costs when we intentionally exercise our worst cases.&lt;/p&gt;
    &lt;p&gt;BeeKEM provides Keyhive with a Continuous Group Key Agreement protocol that is well-suited to distributed local-first applications that require end-to-end encryption for groups on the order of thousands of members. It exhibits logarithmic performance in the common case with linear worst case. And it provides both forward secrecy and post-compromise security.&lt;/p&gt;
    &lt;p&gt;In the future, we plan to write a paper explaining this protocol and its security and performance characteristics in more detail. But hopefully this has given you a sense for how it works.&lt;/p&gt;
    &lt;p&gt;Weidner and Kleppmann argue that secure messaging for large groups does not have a plausible threat model since it would be too easy to infiltrate them. But Keyhive is designed for shared documents. In the context of private documents shared within a company with thousands of employees, for example, we would still expect access control. It’s also worth mentioning that in Keyhive, a single user might have multiple device-specific keys (each of which will count as a member from Keyhive’s perspective). ↩︎&lt;/p&gt;
    &lt;p&gt;BeeKEM in isolation provides forward secrecy, but Keyhive as a whole does not. That’s because users require access to an entire document and Keyhive is used to encrypt that document in chunks. If you can decrypt a chunk, you will gain access to the key for decrypting the previous chunk (as described in an earlier lab note). ↩︎&lt;/p&gt;
    &lt;p&gt;More precisely, we use the root secret as one input into deriving an “application secret”. It is the application secret that is directly used for encrypting and decrypting document chunks. There can be multiple application secrets derived from one root secret, but each application secret is used to encrypt exactly one document chunk. Updating the root secret provides post-compromise security by ensuring no prior key can be used to derive application secrets associated with it. We are glossing over these details in this lab note since they strictly speaking happen outside BeeKEM, which is concerned with group agreement on the root secrets. ↩︎&lt;/p&gt;
    &lt;p&gt;The Beehive project is now officially renamed “Keyhive”!&lt;/p&gt;
    &lt;p&gt;Changing names can be a painful process, and doing so as early as possible in a project’s life is helpful. As Phil Karlton famously said, there’s exactly two hard problems in computer science: caching, naming, and off-by-one errors. Naming is important for orienting readers, searching the web, and avoiding ambiguity. We wanted to make sure that the name was finalized prior to open sourcing the code.&lt;/p&gt;
    &lt;p&gt;There is a naming philosophy that says names should be descriptive, or at least present a direct “mental hook” that implies what the signified thing does. Additional puns and whimsey help with memorability.&lt;/p&gt;
    &lt;p&gt;The previous project name “Beehive” was intended to present a sense of safety and collaboration: bees build complex-yet-sturdy structures together while working independently, and guard their hives to make a safe space on the inside. This metaphor was also inspired by earlier conversations with Christine Lemmer-Webber about metaphors to help explain capability systems (like Keyhive) to folks not familiar with formal concepts from the object-capabilities world like Vats.&lt;/p&gt;
    &lt;p&gt;At the time that we decided on “Beehive”, the team was aware of namespace conflicts in the academic distributed systems literature1. Over time it’s become clear that we also have this problem with packages in more than one language ecosystem. Since we don’t want to tie the project to Automerge exclusively, prefixing the core project with automerge-* was not appropriate.&lt;/p&gt;
    &lt;p&gt;We are retaining our apian naming for other parts of the project. BeeKEM maintains it’s pun on TreeKEM, and Beelay is the Keyhive-enabled relay.&lt;/p&gt;
    &lt;p&gt;We’re excited to announce that we’re opening the pre-alpha code for the following libraries:&lt;/p&gt;
    &lt;p&gt;beelay-core: Auth-enabled sync over end-to-end encrypted data&lt;/p&gt;
    &lt;p&gt;keyhive_core: The core signing, encryption, and delegation system&lt;/p&gt;
    &lt;p&gt;keyhive_wasm: Wasm wrapper around keyhive_core, plus TypeScript bindings&lt;/p&gt;
    &lt;p&gt;⚠️ DO NOT use this release in production applications ⚠️&lt;/p&gt;
    &lt;p&gt;We want to emphasize that this is an early preview release for those that are curious about the project. Expect there to be bugs, inconsistencies, and unstable APIs. This code has also not been through a security audit at time of writing.&lt;/p&gt;
    &lt;p&gt;The last few lab notes have focused on the cryptographic components which support a local first access control system. Those being a capability based system for managing write access to documents, and a key agreement protocol for encrypting and decrypting writes (thus implementing read control). We now have to think about how to actually transfer this data between devices.&lt;/p&gt;
    &lt;p&gt;Alongside the Keyhive project we have also been working on a new sync protocol for Automerge. The existing sync protocol works well for a single document but it is common for Automerge applications to have thousands of documents. Furthermore, the sync protocol requires that both ends are able to read the document whilst one of the objectives of Keyhive is for the server to only have access to the encrypted data.&lt;/p&gt;
    &lt;p&gt;Solving all of these problems in one go is the job of Beelay (the name is inspired by the idea of Beehive being the relay for all the bees (peers) in the Keyhive).&lt;/p&gt;
    &lt;p&gt;Beelay is an RPC protocol which is designed to be usable over any transport which can provide confidentiality (in practice, HTTPS, WebSockets, or raw TLS). The intended usage is to create a local Beelay instance and then connect it to other peers, Beelay will then authenticate with the other peers and synchronise everything which each side thinks the other has access to.&lt;/p&gt;
    &lt;p&gt;Each message is authenticated by signing it with the Ed25519 key that the local node controls. To synchronise we first synchronise the Keyhive membership graph which each end has, this allows each end to determine what documents the other end should have access to. Then we synchronise the collection of documents to figure out which documents are out of sync, before finally synchroising each individual document.&lt;/p&gt;
    &lt;p&gt;It will be useful here to review how we intend to represent devices, people, and documents in Keyhive. In Keyhive there are two important kinds of principal: “groups” and “individuals”. An individual is identified by a single Ed25519 public key - which is immutable - whilst a group is a collection of other principals (groups or individuals) and can be updated by it’s members. One way we intend to use this is to represent a person (or more specifically their authority) as a group, with each of the persons devices being an individual member of the group. Key rotation can then be handled by adding a new individual to the group and removing the old one.&lt;/p&gt;
    &lt;p&gt;Groups can contain other groups. This means that we can represent as groups, where each member of the organisation is another group representing a person (or for that matter another organisation, such as a department).&lt;/p&gt;
    &lt;p&gt;Another useful aspect of this structure is that documents can also be represented as groups. This allows documents to have members which can access the document. For example, a document representing this lab note might add the Ink &amp;amp; Switch group so that all (transitive) members of the Ink &amp;amp; Switch group can read and write to it. Documents can also add other documents which represents “folder” style relationships. The “lab notes” folder document (which is also a group, because all documents are) might contain all the lab notes and have the Ink &amp;amp; Switch as a member.&lt;/p&gt;
    &lt;p&gt;What this all means for the sync protocol is that any given peer is represented by an “individual”. The task of authentication is to ensure that each end knows what Ed25519 public key the other end is using so that we can relate that individual key to the Keyhive membership graph.&lt;/p&gt;
    &lt;p&gt;One solution which might seem obvious here is to rely on an authenticated TLS session. While we use TLS for confidentiality, and the browser itself authenticates the server, our application also needs to know about the server’s public key. Unfortunately, the browser doesn’t expose this information to the application context; there is no way in the browser to obtain the connection’s TLS certificate. We don’t just need to know that a connection is secure, we need to know the public key of the other end in order to use it for access control decisions and so on.&lt;/p&gt;
    &lt;p&gt;Given that each peer is represented by a public key, the simplest possible authentication scheme would be to sign each message. I.e. a message might look like this:&lt;/p&gt;
    &lt;p&gt;type Envelope = { message: Uint8Array, signature: Signature, sender: PublicKey, } type PublicKey = Uint8Array type Signature = Uint8Array&lt;/p&gt;
    &lt;p&gt;To authenticate a message we check that the signature is valid over the message, then we know that the other end is the individual represented by the given public key. There are two problems with this, person in the middle (PITM) attacks, and replay attacks.&lt;/p&gt;
    &lt;p&gt;A good example of PITM attack on this protocol would be a phishing based attack. Imagine an application which allows users to input the URL of a sync server to sync from. Let’s say an attacker creates a sync server at a familiar looking URL, such as wss://sync.automege.org (note the misspelling) and convinces the user to enter this URL into their application. The attacker can now intercept all messages intended for the real sync.automerge.org server and forward them on to the sync server. This means the attacker can read all the messages and even modify messages sent back to the client.&lt;/p&gt;
    &lt;p&gt;The fundamental problem here is that the message is bound to the sender but not to the receiver. We can solve this by adding an “audience” field to the message.&lt;/p&gt;
    &lt;p&gt;type Envelope = { message: Message, signature: Signature, sender: PublicKey, } type Message = { payload: Uint8Array, audience: PublicKey, }&lt;/p&gt;
    &lt;p&gt;This doesn’t quite solve the problem above though. At this stage we only have a URL, we don’t have a public key for the server. To solve this we allow the audience field to either be a public key, or the URL we are addresssing. In this case the audience would be sync.automege.org. This means that when the PITM forwards the message to sync.automerge.org the real server can check and see that the audience doesn’t match sync.automerge.org and reject the message.&lt;/p&gt;
    &lt;p&gt;This works because the connection is being made over TLS, which binds the network transport to the hostname, ensuring that whoever is at the other end, they definitely control sync.automerge.org. Beelay is designed to work over arbitrary transports though, in other network setups such as P2P transports you will need to obtain the public key of the receiver out of band.&lt;/p&gt;
    &lt;p&gt;In a replay attack an attacker is somehow able to intercept messages and store them, and then later replay them to the server. To mitigate this we add a timestamp to the message and then reject messages which are older than some validity window that accounts for latency plus a clock skew grace period — e.g. 5 minutes.&lt;/p&gt;
    &lt;p&gt;The main issue with this scheme is that the clocks of two peers might be out of sync by arbitrary amounts of time. Soft locking the sync system due to clock sync issues is not acceptable. To solve this, when a peer rejects a message due to an old timestamp, the rejecting peer sends their current timestamp along with the rejection message. This allows the sending peer to determine the drift between their local clock and the remote clock and adjust the timestamps on the messages they send, and account for it during this session.&lt;/p&gt;
    &lt;p&gt;To authenticate a message we check that the signature is valid, that the audience is either our public key or the hash of our hostname (or some other string which is bound to the recipient in some way) and that the timestamp is new enough.&lt;/p&gt;
    &lt;p&gt;Once we are authenticated, we need to determine what each side thinks the other should have access to. This means that we need to sync the Keyhive “membership graph”. This is the graph of groups and individuals which represent devices, people, organisations, and documents.&lt;/p&gt;
    &lt;p&gt;The membership graph is a directed graph of “operations” where each operation either creates a new node, delegates access to some other node, or revokes access. Unlike Automerge documents (which are also graphs) the membership operation graph is very shallow and wide, and the linked groups and documents can have cycles. There are many approaches to this problem, but it becomes much simpler if we frame it as set reconciliation, where each side has an unstructured set of operations and needs to figure out what operations the other side has that it needs (i.e. the delta between the two sets). We will encounter a very similar problem later, when we sync the collection of documents. In both cases we use a construction called Rateless Invertible Bloom Lookup Tables (RIBLT) to solve this problem.&lt;/p&gt;
    &lt;p&gt;RIBLT is described in detail in this paper, what I will describe here are the important properties that the scheme gives us.&lt;/p&gt;
    &lt;p&gt;RIBLT is a set reconciliation protocol, which means there are two peers who have some possibly overlapping set of things which they want to have the same view of. I.e. after the protocol completes each side should have the union of the things each started with.&lt;/p&gt;
    &lt;p&gt;RIBLT solves this problem by having each side encode it’s set of things into a set of hashes and then generate a set of special “symbols” which one side sends to the other.&lt;/p&gt;
    &lt;p&gt;These symbols are structured in such a way that once the receiver has received enough of them they will be able to decode the symbols into the set difference.&lt;/p&gt;
    &lt;p&gt;The details are a bit fiddly but the really important part is that the number of symbols which must be sent is proportaional to the set difference between the two peers. Specifically, the number of symbols sent ranges from 1.7x (for small sets) down to 1.35x (for large sets) the set difference.&lt;/p&gt;
    &lt;p&gt;For example, If we have one billion items each, but only five differing items, we can reconcile in 5 * ~1.5 = 7.5 symbols. The symbols themselves are (in our case) 32 bytes long, so we can reconcile a billion items in 240 bytes.&lt;/p&gt;
    &lt;p&gt;The other important part is that the result of decoding is the set of hashes - not the things themselves. In fact, we can use any fixed length array which uniquely represents the thing.&lt;/p&gt;
    &lt;p&gt;So, we use RIBLT sync to synchronise the membership graph. The process is mostly driven by the client (in the peer to peer case we arbitrarily choose that the peer who initiated the connection is the client).&lt;/p&gt;
    &lt;p&gt;First, the client sends a request to the server to begin membership sync. The server stores a pointer to the current set of ops which it thinks the other end needs and then responds with a session ID to identify this sync session, and the first 10 symbols of the RIBLT sync.&lt;/p&gt;
    &lt;p&gt;The client now receives the first 10 symbols and attempts to decode them. If they are able to decode then they are done and they know the set difference, otherwise, they send a request for the next 10 symbols, using the session ID to specify which state they are syncing with.&lt;/p&gt;
    &lt;p&gt;Eventually the client knows the set difference in terms of hashes of operations which only the server has, and operations which only the client has. Finally, the client requests the missing operations by sending their hash, and uploads the symbols which they believe the server is missing.&lt;/p&gt;
    &lt;p&gt;At this point each end has determine what documents it thinks the other should have acces to. The next step is to determine which documents are out of sync. To achieve this we use RIBLT sync again, this time instead of the set we are synchronising being the set of membership operations it is the set of (document ID, state) pairs, where state here is a hash of the document state.&lt;/p&gt;
    &lt;p&gt;There are two components to the document state which we care about for the purposes of synchronisation. One is the heads of the Automerge document - the document content is encrypted but we keep the hashes of the Automerge commit graph outside of the encryption envelope, so the sync server knows the heads.&lt;/p&gt;
    &lt;p&gt;The other piece of state are the BeeKEM operations for the document. Recall that BeeKEM is a continuous group key agreement (CGKA) protocol which allows peers to concurrently decide on what keys to encrypt content to. We need to have the latest CGKA ops in order to be able to decrypt the document content.&lt;/p&gt;
    &lt;p&gt;How do we form our RIBLT symbols then? One way would be to make each symbol hash(document ID, document heads, cgka ops). Then, once we’ve performed RIBLT sync we make another network call to convert each symbol into the document ID which is out of sync. However, we can do a little better than this. Recall that the RIBLT symbol is just any fixed length byte array, and document IDs are a 32 byte array. This means that instead of a hash for the symbol, we use (document ID, hash(heads, cgka ops). This means that once we have decoded the symbol we already know what the document ID is for the symbol in question without doing any more round trips.&lt;/p&gt;
    &lt;p&gt;The process for actually running this sync then is similar to the membership sync. Using the session ID from the membership sync the client fetches new document symbols from the server until it is able to decode the first symbol it received, at which point it knows which symbols are out of sync.&lt;/p&gt;
    &lt;p&gt;By this point we have a list of document IDs which are out of sync. We now have to sync the CGKA ops and encrypted commit graph for each document. For the CGKA sync we can use our old friend RIBLT sync to sync the set of CGKA ops, but for the document content we need to do something a bit different because we want to be able to take advantage of the bandwidth gains we get from compacting Automerge documents.&lt;/p&gt;
    &lt;p&gt;The set we are synchronizing here is the set of CGKA ops for the document. We use the hash of each op to create our RIBLT symbols. As with other RIBLT syncs, the client requests symbols from the server until it is able to decode it’s first symbols at which point it knows what ops to upload and what ops to request.&lt;/p&gt;
    &lt;p&gt;Syncing the document content is more complicated. Initially it might seem that we could just use RIBLT sync again where the symbols to sync are the commit hashes of the commits in the Automerge commit graph. This would certainly work, however, it would use a lot of bandwidth. Automerge commits are frequently made for each keystroke, adding a 32 byte hash for each keystroke would be very expensive.&lt;/p&gt;
    &lt;p&gt;This is a specific instance of a general problem which is that naive encodings of the Automerge commit graph contain enormous amounts of metadata overhead. We have a compact binary encoding which reduces this overhead to around 10% over the underlying data. What we need is a way to use this data in the sync protocol.&lt;/p&gt;
    &lt;p&gt;In the current sync protocol this is not a problem, the sync server has the plaintext in memory and so it can compact the document on the fly when a new peer comes online. For Beelay this isn’t an option because the server only has the ciphertext. What to do?&lt;/p&gt;
    &lt;p&gt;We have come up with a simple protocol for this which we call “sedimentree”. The idea is that every so often we compress ranges of the commit graph into chunks and we do this recursively, so that every so often smaller chunks get compressed into larger chunks. We do this in such a way that older (i.e. closer to the root of the commit graph) end up in larger and larger chunks as time goes on. This forms a tree structure, with older chunks being closer to the root of the tree - hence sedimentree, with chunks being like layers of sedimentree rock.&lt;/p&gt;
    &lt;p&gt;Choosing the boundaries of the chunks is a little fiddly because we need to do it in such a way that peers with different sets of changes still agree on what should go into each chunk. We do this by using the number of trailing zeros in the hash of a commit as the boundary. There are more details on this here.&lt;/p&gt;
    &lt;p&gt;The end result of this structure is that we can sync the document in two steps:&lt;/p&gt;
    &lt;p&gt;Download a “summary” of the sedimentree, which contains just the boundaries of the chunks.&lt;/p&gt;
    &lt;p&gt;Download the chunks we don’t have, and upload the ones the other end doesn’t have&lt;/p&gt;
    &lt;p&gt;Run RIBLT set reconciliation on the membership ops&lt;/p&gt;
    &lt;p&gt;Download ops we are missing&lt;/p&gt;
    &lt;p&gt;Upload ops the remote is missing&lt;/p&gt;
    &lt;p&gt;Sync collection state&lt;/p&gt;
    &lt;p&gt;Run RIBLT set reconciliation on the set of document states&lt;/p&gt;
    &lt;p&gt;Sync out of sync documents, for each document which is out of date&lt;/p&gt;
    &lt;p&gt;Run RIBLT sync on the CGKA ops&lt;/p&gt;
    &lt;p&gt;Download CGKA ops we are missing&lt;/p&gt;
    &lt;p&gt;UPload CGKA ops the remote is missing&lt;/p&gt;
    &lt;p&gt;Run sedimentree sync on the document content&lt;/p&gt;
    &lt;p&gt;One thing which may be concerning here is the number of round trips. We should especially worry about this in the common case where only one document has changed&lt;/p&gt;
    &lt;p&gt;One round trip for the membership sync&lt;/p&gt;
    &lt;p&gt;One round trip for collection state&lt;/p&gt;
    &lt;p&gt;One round trip for CGKA sync&lt;/p&gt;
    &lt;p&gt;Two round trips for sedimentree sync&lt;/p&gt;
    &lt;p&gt;We should be able to simplify this. One the initial message when we begin membership sync we can send the clients first 5 (say) membership RIBLT symbols and first 5 collection state symbols. In the common case the server will be able to decode these symbols (because only one document has changed) and immediately determine which document has changed, then the server can send back a response with the sedimentree summary for the changed document and the first 5 symbols of the server CGKA RIBLT state. The client will in most cases be able to determine if any CGKA ops are missing and immediately download any missing document state.&lt;/p&gt;
    &lt;p&gt;Thus in the common case we can sync graph updates (auth, content, etc) in just two round trips.&lt;/p&gt;
    &lt;p&gt;The Ink &amp;amp; Switch Dispatch&lt;/p&gt;
    &lt;p&gt;Keep up-to-date with the lab's latest findings, appearances, and happenings by subscribing to our newsletter. For a sneak peek, browse the archive.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.inkandswitch.com/keyhive/notebook/"/><published>2025-10-02T00:12:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45446834</id><title>Immich v2.0.0 – First stable release</title><updated>2025-10-02T12:18:14.527135+00:00</updated><content>&lt;doc fingerprint="6f5246cb5a87e72a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;v2.0.0 - Stable Release of Immich #22546&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;v2.0.0 - Stable Release of Immich&lt;/head&gt;
          &lt;head&gt;Welcome&lt;/head&gt;
          &lt;p&gt;Hello everyone,&lt;/p&gt;
          &lt;p&gt;After:&lt;/p&gt;
          &lt;p&gt;We are thrilled to announce the stable release of Immich! 🎉&lt;/p&gt;
          &lt;p&gt;This has been a journey long in the making. So much has changed since the first commit on the project, all the way back in February, 2022. The project and team continue to grow, and today we’re proud to announce &lt;/p&gt;
          &lt;p&gt;For more specifics about the stable release, see our FAQs below.&lt;/p&gt;
          &lt;head&gt;Merch and DVD&lt;/head&gt;
          &lt;p&gt;To celebrate this release, we want to capture this moment in a nostalgic form, reminiscent of how software was distributed in our childhood - on a CD (or DVD, in this “case”). Introducing Immich Stable in physical form! You can find the link to the disk here&lt;/p&gt;
          &lt;p&gt;The disk comes with a fully bootable Immich instance, featuring a selection of curated photos chosen by the team. You can purchase the disk from our merch store, along with a client or server product key, to support and celebrate this milestone with us.&lt;/p&gt;
          &lt;p&gt;The merch store is also updated with retro-styled Immich designs, check it out in https://immich.store&lt;/p&gt;
          &lt;head&gt;Future plans&lt;/head&gt;
          &lt;p&gt;Now that Immich is stable, here are some of the things that we will be focusing on:&lt;/p&gt;
          &lt;head&gt;Thank you&lt;/head&gt;
          &lt;p&gt;We cannot thank you enough for the support over the past three years. Community participation, from the first comments on the original reddit post, to the feedback when we joined FUTO, have contributed to the awesome product Immich is today. Thank you for joining us and believing in our mission to regain control over your most precious data. Here’s to many more years!&lt;/p&gt;
          &lt;p&gt;We'll also be hosting a Q&amp;amp;A livestream tomorrow, October 2nd, 2025 at 6 PM UTC. You can submit your questions here and subscribe for notifications when the livestream starts here.&lt;/p&gt;
          &lt;p&gt;Cheers,&lt;/p&gt;
          &lt;p&gt;The Immich Team&lt;/p&gt;
          &lt;head&gt;FAQs&lt;/head&gt;
          &lt;head&gt;Will there be a live stream?&lt;/head&gt;
          &lt;p&gt;Yes. We'll be hosting a Q&amp;amp;A livestream tomorrow, October 2nd, 2025 at 6 PM UTC. You can submit your questions here and subscribe for notifications when the livestream starts here.&lt;/p&gt;
          &lt;head&gt;Do I still need backups?&lt;/head&gt;
          &lt;p&gt;Yes! A 3-2-1 backup strategy is still crucial. The team has the responsibility to ensure that the application doesn’t cause loss of your precious memories; however, we cannot guarantee that hard drives will not fail, or an electrical event causes unexpected shutdown of your server/system, leading to data loss. Therefore, we still encourage users to follow best practices when safeguarding their data. Keep multiple copies of your most precious data: at least two local copies and one copy offsite in cold storage. Additionally, we are starting to work on a cloud backup service to make backups easier.&lt;/p&gt;
          &lt;head&gt;When will &lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 128 comments 30 replies&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Huge congratulations to the Immich team! Excited to see v2.0.0 reach stable release — an amazing milestone for open-source and self-hosting&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;congratulations to the team at Immich (and by extension, FUTO)!! a well deserved victory for the open source community!!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Is the performance better than previous versions, because it used to be a resource hog&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;About time. It's been so long without an update that I thought immich was abandoned.&lt;/p&gt;
          &lt;p&gt;(im not serious)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Stable 🥳&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;this is the legit successor of the AOL CDRom&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Are the warning messages on the docs supposed to go away too?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Congrats!!!🥳&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Since last few updates (1.14X) the app get stuck when opening I have to force close it and open it again.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Nice - congrats! 🥳🎉&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Amazing news!!!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Congrats guys! What an achievement!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;This is great news!! Congrats to everyone&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Congrats to the team! 🎉🎉 Been eagerly awaiting this one&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Congrats!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Could you please add a floating version tag, such as 'v2', so that we can pin the major version in Docker Compose? Currently, the only options are a specific version, such as v2.0.0, or the latest release, which may introduce breaking changes again when it jumps to v3.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Could you add an rss feed to the blog? Yes, people are still using rss :)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Thank you so much to the whole Immich team and all contributors for the incredible work you’ve put into this project. Your dedication to open source and self-hosting makes a huge difference, and I’m grateful to be part of this community. Congratulations on this milestone, and here’s to the future of Immich! 🎉🚀&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Congrats on reaching such a milestone! You're building an amazing and feature-rich product!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I'm experiencing an upgrade problem after using the Docker &amp;amp; VectorChord instructions from https://docs.immich.app/install/upgrading. My previous version was 1.133&lt;/p&gt;
          &lt;p&gt;Docker compose went without any issues, and ps shows healthy processes, but now I get a DNS error:&lt;/p&gt;
          &lt;p&gt;docker ps -a shows&lt;/p&gt;
          &lt;p&gt;Any pointers what I could do? Thanks!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Great!!!&lt;/p&gt;
          &lt;p&gt;There're just 3 little things that I think should be mandatory for a stable version of Immich:&lt;/p&gt;
          &lt;p&gt;I think that with these 3 things the release can really be considered a stable version 😊&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Hi, great news, congratulations!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Finally! Waiting now for editor and, most important at least for me, #165&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Congratulations !! I've been using Immich for 2 months now and it already seemed very stable, so I didn't understand why it wasn't :)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Congratulations on this awesome milestone! 🥳&lt;/p&gt;
          &lt;p&gt;Can't wait to see what you'll bring in the future!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Great, I hope there will finally be an option to disable albums from the timeline view, which will make the timeline contain only what is needed.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;OOOO LET'S GOOOO FOLKS&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Hehe. Nice.&lt;/p&gt;
          &lt;p&gt;Congrats and thanks for the ride so far! For an "unstable" product, it was quite well-working and flawless already.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Congrats 🎉&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Impressive. Previous versions was a bit buggy on mobile (lag, slow, heating up the device), but now, a breeze! 100k images, no issue!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/immich-app/immich/discussions/22546"/><published>2025-10-02T06:25:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45447536</id><title>Asked to do something illegal at work? Here's what these software engineers did</title><updated>2025-10-02T12:18:14.197618+00:00</updated><content>&lt;doc fingerprint="2c57d065b8a373cc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Asked to do something illegal at work? Here’s what these software engineers did&lt;/head&gt;
    &lt;p&gt;Update on 2 Oct 2025: back in 2021, Charlie Javice, CEO of student loan startup Frank pressured a software engineer to inflate customer numbers. She told the engineer that she did not believe that anyone would end up in an ‘orange jumpsuit’ just for this. Still, the engineer refused – and was proven right. Javice, in fact, did end up in an orange jumpsuit, sentenced to 7 years of prison in 2025 for fraud.&lt;/p&gt;
    &lt;p&gt;The below topic was sent out to full subscribers of The Pragmatic Engineer, three weeks ago, in The Pulse #66. I have received several messages from people asking if they can pay to “unlock” this information for others, given how vital it is for software engineers. It is vital, and so I’m sharing this with all readers, without a paywall. In the unlikely case that you are asked to do something fishy or illegal: I hope the below will help decide how to do the right thing.&lt;/p&gt;
    &lt;p&gt;Sign up to The Pragmatic Engineer to get articles like this earlier in your inbox. It's a pretty good read, and the #1 tech publication on Substack.&lt;/p&gt;
    &lt;p&gt;What would you do if you learned your company is up to something illegal like stealing customer funds, or you’re asked to make code changes that will enable something illegal to happen, like misleading investors, or defrauding customers? Here are three real-life cases, where what engineers and engineering managers did had serious consequences.&lt;/p&gt;
    &lt;head rend="h4"&gt;FTX: an engineering director went along with the fraud&lt;/head&gt;
    &lt;p&gt;A trial related to FTX, the cryptocurrency exchange which allegedly defrauded investors of $9B, is ongoing. Day 9 of the trial of former FTX CEO Sam Bankman-Fried trial, heard testimony from Nishad Singh, who joined the business as a software engineer, and later became an engineering director. Here is software engineer and writer Molly White summarizes of his evidence:&lt;/p&gt;
    &lt;p&gt;“To hear Singh tell it, he didn’t even really realize what was going on at FTX and Alameda Research until September 2022 — only a month or two before everything came crashing down. (...) Several times throughout various testimonies, we’ve seen a document written by Sam Bankman-Fried, in which he describes his thinking that Alameda Research should be shut down. That document was, ultimately, how Singh learned in September 2022 that Alameda Research had taken billions of dollars of customer funds from FTX. &lt;lb/&gt;This was when Gary Wang told Singh that Alameda was borrowing massive amounts of customer money from FTX — at the time, around $13 billion of it. Singh testified that he felt ‘really afraid’, and called an in-person meeting immediately. Bankman-Fried, who was sitting next to Singh at the time, ‘seemed unsurprised and made up what I understood to be a false excuse for dodging the meeting.’ Singh, Ellison, and Wang met without him, and Singh confirmed his fears: that he had not misunderstood Wang, and that Alameda had actually taken customer funds to that extent.”&lt;/p&gt;
    &lt;p&gt;Okay, so in September 2022, Singh had confirmation that something illegal was happening at the company, which he had no direct knowledge of, until then. At that point, if he wanted to avoid being an accomplice to potentially illegal activity, his options were:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Talk to a lawyer on how to avoid assisting a crime&lt;/item&gt;
      &lt;item&gt;Turn whistleblower. See the tech whistleblower guide&lt;/item&gt;
      &lt;item&gt;Quit the company, ensuring he did not further aid this activity&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The smart thing would have been to do #1. The profitable thing could have been to do #2 because in the US, a whistleblower may receive a whistleblower reward of between 10-30% of what the government recovers from fraudulent activities. The final choice #3 is hard, but could have meant Singh would not have had to plead guilty as he did.&lt;/p&gt;
    &lt;p&gt;Here’s what Singh did instead: he asked for a personal meeting with Bankman-Fried and confronted him about the missing funds. However, Bankman-Fried replied there not much to worry about, and that they’d repay the funds by raising more money from investors (!!) This should have been the point at which Singh quit. Instead:&lt;/p&gt;
    &lt;quote&gt;“He thought about leaving the company then, he testified, but worried that his departure could cause everything to fall apart. He felt that if he stayed, maybe he could help the companies make back what they owed.”&lt;/quote&gt;
    &lt;p&gt;For the next two months, Singh tried to make things better, but it was fruitless. FTX collapsed in November 2022.&lt;/p&gt;
    &lt;p&gt;Lesson #1: when you discover fraud may be happening, do not “stay around to fix it.” Any other approach would have been better for Singh; seeking legal advice, turning whistleblower, or quitting on the spot.&lt;/p&gt;
    &lt;p&gt;To be fair, Singh didn’t seen totally clueless, and it seems he decided to profit on the developments. Days after he found about this fraud, he took a $3.7M loan from FTX (!!) to buy a house, The Verge pointed out. It’s exactly the type of thing you don’t want to do after you discover fraud.&lt;/p&gt;
    &lt;p&gt;Now, Singh is facing up to 75 years in jail thanks to his decision to aid the company after discovering the fraud. His sentence will most likely be reduced due to his plea deal, but any course of action which leads to a criminal conviction is surely a grave error of judgment.&lt;/p&gt;
    &lt;head rend="h4"&gt;Frank: a software engineer refuses to fake customer data&lt;/head&gt;
    &lt;p&gt;Frank was a student loan startup founded by Charlie Javice in 2016. In 2019, Javice was featured on the Forbes “30 under 30” finance list, suggesting she was a high-flying founder:&lt;/p&gt;
    &lt;p&gt;It certainly seemed like Charlie Javice was a standout founder; in 2021, JP Morgan purchased Frank for $175M. However, things turned sour quickly. JP Morgan thought it bought a startup with 5 million customers, which worked with 6,000 schools. But after the purchase, this data was found to be mostly fake.&lt;/p&gt;
    &lt;p&gt;Let’s get to a software engineer’s involvement. This April, founder Charlie Javice was arrested, and a lawsuit is ongoing between her, former Chief Growth Officer Olivier Amar, and JP Morgan. From to this lawsuit, we get an inside look at how events unfolded inside Frank.&lt;/p&gt;
    &lt;p&gt;In 2021, an engineer was asked to produce fake data for 4.2M non-existent customers. As acquisition talks were ongoing, JP Morgan wanted to validate that Frank had the nearly 5M customers it claimed. In reality, Frank had 293,000 customers, so the CEO asked an engineer to fake the data and turn this list into 4.2M members. Here’s what happened next – from the lawsuit:&lt;/p&gt;
    &lt;p&gt;“[In 2021] Javice [CEO], Amar [Chief Growth Officer] and the Director of Engineering then had a Zoom meeting during which Javice and Amar asked the Director of Engineering to help them create a synthetic list of customer data. She asked the Director of Engineering if he could help take a known set of FAFSA application data and use it to artificially augment a much larger set of anonymous data tht her systems had collected over time.&lt;lb/&gt;The Director of Engineering questioned whether creating and using such a data set was legal, but Javice tried to assure the engineer by claiming that this was perfectly acceptable in an investment situation and she did not believe that anyone would end up in an ‘orange jumpsuit’ over this project.”&lt;/p&gt;
    &lt;p&gt;Lesson #2: when your manager claims they don’t believe anyone would end up in an “orange jumpsuit,” assume that someone definitely could. The engineering director’s next step? They refused:&lt;/p&gt;
    &lt;p&gt;“The Director of Engineering was not persuaded and told Javice and Amar that he would not perform the task, and only would send them the file containing Frank’s actual users, which amounted to approximately 293,000 individuals at the time.”&lt;/p&gt;
    &lt;p&gt;And this engineering director played it right, as the people who are likely to go to jail and end up in orange jumpsuits are the other two people on the call, who knowingly went along with the illegal.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pollen: an engineer told to double charge customers by the CEO&lt;/head&gt;
    &lt;p&gt;Last year, I published my first – and to date only– investigative article on how events tech startup Pollen raised $200M and then collapsed, owing months of wages to staff. In the investigation, I focused on an unusual detail: $3.2M worth of funds taken months early from customers. The incident was described internally by Pollen as a mistake, and an incident review should have followed. Even more confusing, the company blamed the payments processor Stripe for the incident.&lt;/p&gt;
    &lt;p&gt;The reality was that this was a very deliberate double charge. I could not share this fact at the time – as the company threatened me with libel after I informed them of this detail – but the BBC has now produced a documentary revealing details about this deliberate double charge that was covered up as an outage. From the documentary:&lt;/p&gt;
    &lt;p&gt;[Narrator] “Pollen initially told some customers that the problem was with their payments provider. Later, Callum [the CEO] addressed his staff who were demanding to know what happened.”&lt;/p&gt;
    &lt;p&gt;[CEO of Pollen talking] “All that happened was that a couple millions of dollars of payment plans that were due to be paid at a later month were then paid earlier. It’s being investigated. We’ve committed already that once that investigation is done, it will be shared with the company so that people understand what happened.”&lt;/p&gt;
    &lt;p&gt;[Narrator] “With over 1,500 customers impacted, rumors began to circulate about the causes of the incident.”&lt;/p&gt;
    &lt;p&gt;[Dan Taylor, managing editor at Tech.eu] “From my understanding, there was a creative code ‘malfunction’ that all of the sudden, double charged customers. But that double charge magically happened to meet Pollen’s payroll, that month. Hmm! Odd, don’t you think?”&lt;/p&gt;
    &lt;p&gt;[Narrator] “The internal investigation due to be shared with the company was never completed, but a group of Pollen staff did their own, unofficial digging. (...) The code contained in the report confirms that the customer's monthly payment plans had been manually altered, which meant that double or triple charges will take place on a single day, without the customer’s authorization.”&lt;/p&gt;
    &lt;p&gt;The engineer making this change even did a test run the day before, to ensure that this code change “correctly” double charges customers! A former Pollen software engineer appearing in the documentary also makes the point that any code changing production code in payments needs to go through code review, so whoever made this change could have not been acting alone.&lt;/p&gt;
    &lt;p&gt;Two days after the incident, a senior engineering team member sent an internal chat message to 3 colleagues, where they admit that they had run the script at the request of the CEO. Here is what this message said:&lt;/p&gt;
    &lt;quote&gt;“Also want to come clean that it was me who ran a bad script - in hindsight I wasn’t knowledgeable enough to alter a subset of payment plans for Balvin [one of the events organized by Pollen]. I did this as a special request from Callum and didn’t want to raise on call to handle. It’s been a long week and I displayed a very poor form of judgement.”&lt;/quote&gt;
    &lt;p&gt;In the video, a Pollen software engineer is shown the message, and he says: “I’m not sure I buy this. It seems a bit fishy.”&lt;/p&gt;
    &lt;p&gt;Lesson #3: if the CEO asks you to do something potentially illegal – document it, and consider not doing it. We don’t know what happened with the senior engineering member who carried out the code changes, following a request from the CEO. This person could have said no, like the engineering director at Frank did. The message sent a few days ago already said that this person regretted doing so, and it’s unlikely that this action was worth the risk it carried.&lt;/p&gt;
    &lt;p&gt;If you take one lesson from this, it’s that you can always say no. In these three stories, the only engineer who’s legally safe is the former engineering director at Frank who point blank refused to assist what could be an illegal request. The engineering director at FTX who stayed after he confirmed fraud was occurring is now facing jail time, while the senior engineering member at Pollen is at the mercy of the UK police, and how they deal with what could be a potential wire fraud case.&lt;/p&gt;
    &lt;p&gt;Subscribe to my weekly newsletter to get articles like this in your inbox. It's a pretty good read - and the #1 tech newsletter on Substack.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.pragmaticengineer.com/asked-to-do-something-illegal-at-work/"/><published>2025-10-02T08:51:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45447776</id><title>Greg Kroah-Hartman explains the Cyber Resilience Act for open source developers</title><updated>2025-10-02T12:18:13.917472+00:00</updated><content>&lt;doc fingerprint="bd320b189d930114"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Greg Kroah-Hartman explains the Cyber Resilience Act for open source developers&lt;/head&gt;
    &lt;head rend="h2"&gt;Impact? Nope, don't worry, be happy, says Linux veteran&lt;/head&gt;
    &lt;p&gt;Opinion There has been considerable worry about the impact of the European Union's Cyber Resilience Act on open source programmers. Linux stable kernel maintainer Greg Kroah-Hartman says, however, that there won't be much of an impact at all.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Linux Foundation has to abide by these rules. Mozilla has to abide by these rules. Me and Linus, as individuals working for them, don't have to abide by the rules...&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When the news of the EU's Cyber Resilience Act (CRA) first emerged, open source software developers and companies were worried sick. As the Python Software Foundation (PSF) executive director Deb Nicholson said at the time, "Under the current language, the PSF could potentially be financially liable for any product that includes Python code, while never having received any monetary gain from any of these products." Ouch!&lt;/p&gt;
    &lt;p&gt;Since then, however, the EU has made the CRA more open source friendly. How friendly? Well, according to Greg Kroah-Hartman, a top Linux kernel maintainer and member of the CRA working group of experts, "for open source contributors and maintainers, … [the] CRA is a good thing. I think it's gonna help us.&lt;/p&gt;
    &lt;p&gt;Speaking in Paris at the Linux Kernel Recipes conference, Kroah-Hartman started by saying, "You never expect to be dealing with lawyers and things like that when you start out programming. But here I am. This is all my personal opinion." But, he believes, the CRA has become "something that's actually palatable and can be used" for open source's benefit.&lt;/p&gt;
    &lt;p&gt;Kroah-Hartman explained that the CRA introduces a legal requirement for producers of products with digital elements (PDE). This is a broad category that includes nearly all software-driven devices and programs to document, secure, and maintain their software supply chain. This means companies must now generate a Software Bill of Materials (SBOM), tracking vulnerabilities, responding to newly discovered issues, and being transparent about security practices. For open source developers, this means, for the first time, companies must publicly acknowledge and document their open source dependencies. That's a win in Kroah-Hartman's book.&lt;/p&gt;
    &lt;p&gt;A fundamental distinction in the revised CRA's approach is how it separates unpaid, hobbyist developers from legal "people" such as foundations, projects, and companies that commercialize open source software. Specifically, non-commercial open source developers can continue publishing software with minimal worry. As long as a project is not organized as a legal or commercial entity, the CRA requires only a basic "readme" with a security contact. There is no legal risk for individual contributors simply sharing code online or in publications, even when they receive payment for writing an article, as long as the software itself is not monetized or organized.&lt;/p&gt;
    &lt;p&gt;That readme must say "who to email for security issues and report security issues" to a security monitoring organization. That's it. That's all. "So don't be afraid of the CRA. These are things you should be doing anyway."&lt;/p&gt;
    &lt;p&gt;Project stewards, such as legal persons, are another matter. They must provide a security contact and a reporting process. If the organization receives or distributes funding or donations, it falls into this category and must follow the stewardship requirements. "So, he continued, "the Linux Foundation has to abide by these rules. Mozilla has to abide by these rules. Me and Linus, as individuals working for them, don't have to abide by the rules." As far as Kroah-Hartman's concerned, "this is a reasonable baseline most responsible projects already meet."&lt;/p&gt;
    &lt;p&gt;What are these rules? Well, the CRA's focus is on commercial manufacturers and distributors. That means businesses that integrate open source code into EU products must fully comply with documentation, incident response, and lifecycle management requirements. This includes publishing Software Bills Of Materials (SBOMs), patching vulnerabilities within regulated timeframes, and responding proactively to security incident reports.&lt;/p&gt;
    &lt;p&gt;For the purposes of the CRA, consulting operations monetizing open source work could be considered manufacturers, triggering compliance obligations. Independent consultants or tiny firms may need to form a legal entity, but the workload is "not huge if you're already following good development practice," according to Kroah-Hartman.&lt;/p&gt;
    &lt;p&gt;The regulations are much more stringent for hardware or device vendors using open source code in their products than for pure software consultancies. For decades, hardware vendors have been using open source code without obeying the open source licensing rules, such as Vizio using Linux in its smart TV. Under the CRA, they'll find it much harder to get away with this or deny they're breaching open source software terms in their products, as Cisco did in 2008 with its Linksys routers. (It ultimately settled the case, appointed a free software director, made source code for products available on its website, and made a donation to the FSF.)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Manufacturers are going to care in September of next year. They're going to start panicking in the summer of next year, and things are going to start hitting the fan...&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You may think that since the CRA is an EU law, it won't matter to countries outside the EU. You would be wrong. The CRA effectively extends worldwide. If software is accessible "on the market" in the EU, it falls under the law's scope. Thus, US and Japanese vendors, for example, must pay careful attention to compliance if their products are downloadable or operable from within the EU.&lt;/p&gt;
    &lt;p&gt;For example, manufacturers must act on vulnerabilities, even if the upstream maintainer does not fix the issue. Manufacturers selecting open source code for their products must understand the code, support it, and respond to regulatory reporting requirements. This may, Kroah-Hartman observed, increase pressure on companies to use actively supported open source projects or stick closer to mainstream, well-resourced communities."&lt;/p&gt;
    &lt;p&gt;Will this make companies more wary of using open source software? Kroah-Hartman thinks it will have the opposite effect. The CRA also covers proprietary software. Instead of these new requirements creating a "chilling effect" on open source releases by nervous companies or legal departments, he thinks it will actually increase demand for open source, since companies gain more control over code destiny than with closed source vendors.&lt;/p&gt;
    &lt;p&gt;Businesses that are already using open source code in their programs still haven't realized just what a big deal the CRA will be for them. They will soon enough. Kroah-Hartman predicts, "Companies are coming after you [open source developers]. I will create a little form letter and say, 'Here's what you need to send off.'&lt;/p&gt;
    &lt;p&gt;"It's going to get worse because it's coming soon for companies. Manufacturers are going to care in September of next year. They're going to start panicking in the summer of next year, and things are going to start hitting the fan."&lt;/p&gt;
    &lt;p&gt;They'll want developers to shoulder the burden the CRA will place on them. But you don't have to do that. It's their problem, not yours as a programmer.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strap in, get ready for more Rust drivers in Linux kernel&lt;/item&gt;
      &lt;item&gt;Linux royalty backs adoption of Rust for kernel code, says its rise is inevitable&lt;/item&gt;
      &lt;item&gt;Even Linus Torvalds can have trouble with autocycle … autocracy… AUTOCOMPLETE!&lt;/item&gt;
      &lt;item&gt;Linus Torvalds affirms expulsion of Russian maintainers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, developers are encouraged to adopt best practices - eg, secure reporting, clear SBOMs, supply-chain checks - now. Foundations and large community projects are working with the EU to produce checklists and templates to simplify compliance. Kroah-Hartman reports that much of the ambiguity around commercial versus non-commercial obligations will be clarified in the coming year, with accessible implementation resources for projects.&lt;/p&gt;
    &lt;p&gt;In a Q&amp;amp;A session afterwards, Kroah-Hartman struck a hopeful note, "The goal here and the intent is not to trip up anybody in this room. The goal and intent are also to hold big companies liable when they release open source software, as part of that, because they want it not to be an end run around the CRA.&lt;/p&gt;
    &lt;p&gt;"Therefore, merging those two things together created a really wiggly line in the middle. But they are on our side. I've spoken with representatives from the different countries that created this law. They understand open source. They understand how the world runs in open source. They don't want to see it harmed by this at all. So have faith in that." ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/09/30/cyber_reiliance_act_opinion_column/"/><published>2025-10-02T09:42:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45448199</id><title>How the AI Bubble Will Pop</title><updated>2025-10-02T12:18:13.628865+00:00</updated><content>&lt;doc fingerprint="333590590dc697ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This Is How the AI Bubble Will Pop&lt;/head&gt;
    &lt;head rend="h3"&gt;The AI infrastructure boom is the most important economic story in the world. But the numbers just don't add up.&lt;/head&gt;
    &lt;p&gt;Some people think artificial intelligence will be the most important technology of the 21st century. Others insist that it is an obvious economic bubble. I believe both sides are right. Like the 19th century railroads and the 20th century broadband Internet build-out, AI will rise first, crash second, and eventually change the world.&lt;/p&gt;
    &lt;p&gt;The numbers just don’t make sense. Tech companies are projected to spend about $400 billion this year on infrastructure to train and operate AI models. By nominal dollar sums, that is more than any group of firms has ever spent to do just about anything. The Apollo program allocated about $300 billion in inflation-adjusted dollars to get America to the moon between the early 1960s and the early 1970s. The AI buildout requires companies to collectively fund a new Apollo program, not every 10 years, but every 10 months.&lt;/p&gt;
    &lt;p&gt;It’s not clear that firms are prepared to earn back the investment, and yet by their own testimony, they’re just going to keep spending, anyway. Total AI capital expenditures in the U.S. are projected to exceed $500 billion in 2026 and 2027—roughly the annual GDP of Singapore. But the Wall Street Journal has reported that American consumers spend only $12 billion a year on AI services. That’s roughly the GDP of Somalia. If you can grok the economic difference between Singapore and Somalia, you get a sense of the economic chasm between vision and reality in AI-Land. Some reports indicate that AI usage is actually declining at large companies that are still trying to figure out how large language models can save them money.&lt;/p&gt;
    &lt;p&gt;Every financial bubble has moments where, looking back, one thinks: How did any sentient person miss the signs? Today’s omens abound. Thinking Machines, an AI startup helmed by former Open AI executive Mira Murati, just raised the largest seed round in history: $2 billion in funding at a $10 billion valuation. The company has not released a product and has refused to tell investors what they’re even trying to build. “It was the most absurd pitch meeting,” one investor who met with Murati said. “She was like, ‘So we’re doing an AI company with the best AI people, but we can’t answer any questions.” Meanwhile, a recent analysis of stock market trends found that none of the typical rules for sensible investing can explain what’s going on with stock prices right now. Whereas equity prices have historically followed earnings fundamentals, today’s market is driven overwhelmingly by momentum, as retail investors pile into meme stocks and AI companies because they think everybody else is piling into meme stocks and AI companies.&lt;/p&gt;
    &lt;p&gt;Every economic bubble also has tell-tale signs of financial over-engineering, like the collateralized debt obligations and subprime mortgage-backed securities that blew up during the mid-2000s housing bubble. Ominously, AI appears to be entering its own phase of financial wizardry. As the Economist has pointed out, the AI hyperscalers—that is, the largest spenders on AI—are using accounting tricks to depress their reported infrastructure spending, which has the effect of inflating their profits1. As the investor and author Paul Kedrosky told me on my podcast Plain English, the big AI firms are also shifting huge amounts of AI spending off their books into SPVs, or special purpose vehicles, that disguise the cost of the AI build-out.&lt;/p&gt;
    &lt;p&gt;My interview with Kedrosky received the most enthusiastic and complimentary feedback of any show I’ve done in a while. His level of insight-per-minute was off the charts, touching on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;How AI capital expenditures break down&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Why the AI build-out is different from past infrastructure projects, like the railroad and dot-com build-outs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How AI spending is creating a black hole of capital that’s sucking resources away from other parts of the economy&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How ordinary investors might be able to sense the popping of the bubble just before it happens&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Why the entire financial system is balancing on big chip-makers like Nvidia&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If the bubble pops, what surprising industries will face a reckoning&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Below is a polished transcript of our conversation, organized by topic area and adorned with charts and graphs to visualize his points. I hope you learn as much from his commentary as much as I did. From a sheer economic perspective, I don’t think there’s a more important story in the world.&lt;/p&gt;
    &lt;head rend="h1"&gt;AI SPENDING: 101&lt;/head&gt;
    &lt;p&gt;Derek Thompson: How big is the AI infrastructure build-out?&lt;/p&gt;
    &lt;p&gt;Paul Kedrosky: There’s a huge amount of money being deployed and it’s going to a very narrow set of recipients and some really small geographies, like Northern Virginia. So it’s an incredibly concentrated pool of capital that’s also large enough to affect GDP. I did the math and found out that in the first half of this year, the data-center related spending—these giant buildings full of GPUs [graphical processing units] and racks and servers that are used by the large AI firms to generate responses and train models—probably accounted for half of GDP growth in the first half of the year. Which is absolutely bananas. This spending is huge.&lt;/p&gt;
    &lt;p&gt;Thompson: Where is all this money going?&lt;/p&gt;
    &lt;p&gt;Kedrosky: For the biggest companies—Meta and Google and Amazon—a little more than half the cost of a data center is the GPU chips that are going in. About 60 percent. The rest is a combination of cooling and energy. And then a relatively small component is the actual construction of the data center: the frame of the building, the concrete pad, the real estate.&lt;/p&gt;
    &lt;head rend="h1"&gt;HOW AI IS ALREADY WARPING THE 2025 ECONOMY&lt;/head&gt;
    &lt;p&gt;Thompson: How do you see AI spending already warping the 2025 economy?&lt;/p&gt;
    &lt;p&gt;Kedrosky: Looking back, the analogy I draw is this: massive capital spending in one narrow slice of the economy during the 1990s caused a diversion of capital away from manufacturing in the United States. This starved small manufacturers of capital and made it difficult for them to raise money cheaply. Their cost of capital increased, meaning their margins had to be higher. During that time, China had entered the World Trade Organization and tariffs were dropping. We’ve made it very difficult for domestic manufacturers to compete against China, in large part because of the rising cost of capital. It all got sucked into this “death star” of telecom.&lt;/p&gt;
    &lt;p&gt;So in a weird way, we can trace some of the loss of manufacturing jobs in the 1990s to what happened in telecom because it was the great sucking sound that sucked all the capital out of everywhere else in the economy.&lt;/p&gt;
    &lt;p&gt;The exact same thing is happening now. If I’m a large private equity firm, there is no reward for spending money anywhere else but in data centers. So it’s the same phenomenon. If I’m a small manufacturer and I’m hoping to benefit from the on-shoring of manufacturing as a result of tariffs, I go out trying to raise money with that as my thesis. The hurdle rate just got a lot higher, meaning that I have to generate much higher returns because they’re comparing me to this other part of the economy that will accept giant amounts of money. And it looks like the returns are going to be tremendous because look at what’s happening in AI and the massive uptake of OpenAI. So I end up inadvertently starving a huge slice of the economy yet again, much like what we did in the 1990s.&lt;/p&gt;
    &lt;p&gt;Thompson: That’s so interesting. The story I’m used to telling about manufacturing is that China took our jobs. “The China shock,” as economists like David Autor call it, essentially took manufacturing to China and production in Shenzhen replaced production in Ohio, and that’s what hollowed out the Rust Belt. You’re adding that telecom absorbed the capital.&lt;/p&gt;
    &lt;p&gt;And now you fast-forward to the 2020s. Trump is trying to reverse the China shock with the tariffs. But we’re recreating the capital shock with AI as the new telecom, the new death star that’s taking capital that might at the margin go to manufacturing.&lt;/p&gt;
    &lt;p&gt;Kedrosky: It’s even more insidious than that. Let’s say you’re Derek’s Giant Private Equity Firm and you control $500 billion. You do not want to allocate that money one $5 million check at a time to a bunch of manufacturers. All I see is a nightmare of having to keep track of all of these little companies doing who knows what.&lt;/p&gt;
    &lt;p&gt;What I’d like to do is to write 30 separate $50 billion checks. I’d like to write a small number of huge checks. And this is a dynamic in private equity that people don’t understand. Capital can be allocated in lots of different ways, but the partners at these firms do not want to write a bunch of small checks to a bunch of small manufacturers, even if the hurdle rate is competitive. I’m a human, I don’t want to sit on 40 boards. And so you have this other perverse dynamic that even if everything else is equal, it’s not equal. So we’ve put manufacturers who might otherwise benefit from the onshoring phenomenon at an even worse position in part because of the internal dynamics of capital.&lt;/p&gt;
    &lt;p&gt;Thompson: What about the energy piece of this? Electricity prices rising. Data centers are incredibly energy thirsty. I think consumers will revolt against the construction of local data centers, but the data centers have enormous political power of their own. How is this going to play out?&lt;/p&gt;
    &lt;p&gt;Kedrosky: So I think you’re going to rapidly see an offshoring of data centers. That will be the response. It’ll increasingly be that it’s happening in India, it’s happening in the Middle East, where massive allocations are being made to new data centers. It’s happening all over the world. The focus will be to move offshore for exactly this reason. Bloomberg had a great story the other day about an exurb in Northern Virginia that’s essentially surrounded now by data centers. This was previously a rural area and everything around them, all the farms sold out, and people in this area were like, wait a minute, who do I sue? I never signed up for this. This is the beginnings of the NIMBY phenomenon because it’s become visceral and emotional for people. It’s not just about prices. It’s also about: If you’ve got a six acre building beside you that’s making noise all the time, that is not what you signed up for.&lt;/p&gt;
    &lt;head rend="h1"&gt;A VERY SPECIFIC PREDICTION FOR HOW AND WHY AI BUBBLE WILL POP … &lt;/head&gt;
    &lt;head rend="h2"&gt;Keep reading with a 7-day free trial&lt;/head&gt;
    &lt;p&gt;Subscribe to Derek Thompson to keep reading this post and get 7 days of free access to the full post archives.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.derekthompson.org/p/this-is-how-the-ai-bubble-will-pop"/><published>2025-10-02T11:05:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45448326</id><title>NL Judge: Meta must respect user's choice of recommendation system</title><updated>2025-10-02T12:18:11.795100+00:00</updated><content>&lt;doc fingerprint="bc501a786403c13c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Judge in the Bits of Freedom vs. Meta lawsuit: Meta must respect users’ choice&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;02 oktober 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Today the judge issued a ruling in the summary proceedings brought by digital human rights organisation Bits of Freedom against Meta. The organisation demanded that Meta gives its users on apps such as Instagram and Facebook the option to select a feed that is not based on profiling.&lt;/p&gt;
    &lt;p&gt;Bits of Freedom sued Meta for a breach of the Digital Services Act (DSA). This European legislation is intended to give users more autonomy and control over the major online platforms. One of the core elements of the DSA is that users must have greater influence over the information they see.&lt;/p&gt;
    &lt;p&gt;For many people, and especially for young people, social media platforms are a major source of news and information. Therefore it is crucial that users themselves can decide which content appears on their feed. Without that freedom of choice, participation in the public debate is seriously hampered. That is problematic at any time, but especially so during election periods. In the Netherlands, national elections will be held at the end of this month.&lt;/p&gt;
    &lt;p&gt;The judge states that Meta is indeed acting in violation of the law. He says that “a non‑persistent choice option for a recommendation system runs counter to the purpose of the DSA, which is to give users genuine autonomy, freedom of choice, and control over how information is presented to them.” The judge also concludes that the way Meta has designed its platforms constitutes “a significant disruption of the autonomy of Facebook and Instagram users.” The judge orders Meta to adjust its apps so that the user’s choice is preserved, even when the user navigates to another section or restarts the app.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“We are pleased that the judge now makes clear that Meta must respect the user’s choice,” says Maartje Knaap, spokesperson for Bits of Freedom. “It is absolutely unacceptable that a handful of American tech billionaires determine how we see the world. That concentration of power poses a risk to our democracy. At the same time, it is regrettable that we need to go to court to ensure Meta complies with the law.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Meta has an interest in steering users toward a feed where it can show as many interest‑ and behavior‑based ads as possible. That is the core of Meta’s revenue model. Subtle design techniques push users toward that feed, while the non‑profiled feed is hidden behind a logo, making it hard to find. Users who do choose the alternative timeline also lose direct access to features such as Direct Messages. Moreover, when you open the app, it always starts with Meta’s feed, even if the user selected a different one before. Because of the judge’s ruling, Meta must change its behavior.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“This ruling shows that Meta is not untouchable,” continues Maartje Knaap. “But we are also realistic, this is just a drop in the ocean. There’s still a long way to go. We hope the decision will inspire individuals, civil society organisations, regulators and lawmakers worldwide around the world who are working to rein in Meta’s power. Together we can stand up to a company that has become overwhelmingly powerful.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can find the ruling here (in Dutch).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bitsoffreedom.nl/2025/10/02/judge-in-the-bits-of-freedom-vs-meta-lawsuit-meta-must-respect-users-choice/"/><published>2025-10-02T11:32:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45448539</id><title>Activeloop (YC S18) Is Hiring AI Search Engineer and MTS(Back End)</title><updated>2025-10-02T12:18:11.597619+00:00</updated><link href="https://careers.activeloop.ai/"/><published>2025-10-02T12:00:33+00:00</published></entry></feed>