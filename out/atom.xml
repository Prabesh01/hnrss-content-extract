<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-19T07:39:13.376757+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46315414</id><title>Skills for organizations, partners, the ecosystem</title><updated>2025-12-19T07:39:20.151180+00:00</updated><content>&lt;doc fingerprint="dd141e2f0b27db6c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 18, 2025&lt;/item&gt;
      &lt;item&gt;5min&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In October, we introduced skillsâa way to teach Claude repeatable workflows tailored to how you work. Today we're making skills easier to deploy, discover, and build: organization-wide management for admins; a directory of partner-built skills from Notion, Canva, Figma, Atlassian, and others; and an open standard so skills work across AI platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manage skills across your organization&lt;/head&gt;
    &lt;p&gt;Claude Team and Enterprise plan admins can now provision skills centrally from admin settings. Admin-provisioned skills are enabled by default for all users. Users can still toggle individual skills off if they choose. This gives organizations consistent, approved workflows across teams while letting individual users customize their experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discover, create, and edit new skills&lt;/head&gt;
    &lt;p&gt;Creating skills is now simpler. Describe what you want and Claude helps build it, or write instructions directly. For complex workflows, upload skill folders or use the skill-creator. Claude can also help you edit existing skills, and new previews show full contents so you can understand exactly what a skill does before enabling it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills directory&lt;/head&gt;
    &lt;p&gt;A growing collection of partner-built skills is now available at claude.com/connectors.&lt;/p&gt;
    &lt;p&gt;Admins can provision these partner skills across their organization, giving teams immediate access to workflows for tools they already use without any custom development.&lt;/p&gt;
    &lt;head rend="h2"&gt;An open standard&lt;/head&gt;
    &lt;p&gt;We're also publishingÂ Agent Skills as an open standard. Like MCP, we believe skills should be portable across tools and platformsâthe same skill should work whether you're using Claude or other AI platforms. We've been collaborating with members of the ecosystem, and we're excited to see early adoption of the standard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Apps: Browse the skills directory and enable in Settings &amp;gt; Capabilities &amp;gt; Skills.&lt;/item&gt;
      &lt;item&gt;Claude Code: Install from the plugin directory or check skills into your repository.&lt;/item&gt;
      &lt;item&gt;Claude Developer Platform (API): Use skills via the /v1/skills endpoint. See documentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Admins can provision skills org-wide through Admin Settings. Skills require Code Execution and File Creation to be enabled.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transform how your organization operates with Claude&lt;/head&gt;
    &lt;p&gt;Get the developer newsletter&lt;/p&gt;
    &lt;p&gt;Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://claude.com/blog/organization-skills-and-directory"/><published>2025-12-18T17:04:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316367</id><title>GPT-5.2-Codex</title><updated>2025-12-19T07:39:19.859410+00:00</updated><content>&lt;doc fingerprint="3f4b6dffd47fc68d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing GPT-5.2-Codex&lt;/head&gt;
    &lt;p&gt;The most advanced agentic coding model for professional software engineering and defensive cybersecurity.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing GPT‑5.2-Codex, the most advanced agentic coding model yet for complex, real-world software engineering. GPT‑5.2-Codex is a version of GPT‑5.2 further optimized for agentic coding in Codex, including improvements on long-horizon work through context compaction, stronger performance on large code changes like refactors and migrations, improved performance in Windows environments, and significantly stronger cybersecurity capabilities.&lt;/p&gt;
    &lt;p&gt;As our models continue to advance along the intelligence frontier, we’ve observed that these improvements also translate to capability jumps in specialized domains such as cybersecurity. For example, just last week, a security researcher using GPT‑5.1-Codex-Max with Codex CLI found and responsibly disclosed(opens in a new window) a vulnerability in React that could lead to source code exposure.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex has stronger cybersecurity capabilities than any model we’ve released so far. These advances can help strengthen cybersecurity at scale, but they also raise new dual-use risks that require careful deployment. While GPT‑5.2-Codex does not reach a ‘High’ level of cyber capability under our Preparedness Framework, we’re designing our deployment approach with future capability growth in mind.&lt;/p&gt;
    &lt;p&gt;We're releasing GPT‑5.2-Codex today in all Codex surfaces for paid ChatGPT users, and working towards safely enabling access to GPT‑5.2-Codex for API users in the coming weeks. In parallel, we’re piloting invite-only trusted access to upcoming capabilities and more permissive models for vetted professionals and organizations focused on defensive cybersecurity work. We believe that this approach to deployment will balance accessibility with safety.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex builds on GPT‑5.2’s strengths in professional knowledge work and GPT‑5.1-Codex-Max’s frontier agentic coding and terminal-using capabilities. GPT‑5.2-Codex is now better at long-context understanding, reliable tool calling, improved factuality, and native compaction, making it a more dependable partner for long running coding tasks, while remaining token-efficient in its reasoning.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex achieves state-of-the-art performance on SWE-Bench Pro and Terminal-Bench 2.0, benchmarks designed to test agentic performance on a wide variety of tasks in realistic terminal environments. It is also much more effective and reliable at agentic coding in native Windows environments, building on capabilities introduced in GPT‑5.1-Codex-Max.&lt;/p&gt;
    &lt;p&gt;With these improvements, Codex is more capable at working in large repositories over extended sessions with full context intact. It can more reliably complete complex tasks like large refactors, code migrations, and feature builds — continuing to iterate without losing track, even when plans change or attempts fail.&lt;/p&gt;
    &lt;p&gt;Stronger vision performance enables GPT‑5.2-Codex to more accurately interpret screenshots, technical diagrams, charts, and UI surfaces shared during coding sessions.&lt;/p&gt;
    &lt;p&gt;Codex can take design mocks and quickly translate them to functional prototypes, and you can pair with Codex to take these prototypes to production.&lt;/p&gt;
    &lt;head rend="h5"&gt;Design mock&lt;/head&gt;
    &lt;head rend="h5"&gt;Prototype generated by GPT-5.2-Codex&lt;/head&gt;
    &lt;p&gt;When charting performance on one of our core cybersecurity evaluations over time, we see a sharp jump in capability starting with GPT‑5-Codex, another large jump with GPT‑5.1-Codex-Max and now a third jump with GPT‑5.2-Codex. We expect that upcoming AI models will continue on this trajectory. In preparation, we are planning and evaluating as though each new model could reach ‘High’ levels of cybersecurity capability, as measured by our Preparedness Framework(opens in a new window). While GPT‑5.2-Codex has not yet reached ‘High’ level of cyber capability, we are preparing for future models that cross that threshold. Due to the increased cyber capabilities, we have added additional safeguards in the model and in the product, which are outlined in the system card.&lt;/p&gt;
    &lt;p&gt;Modern society runs on software, and its reliability depends on strong cybersecurity—keeping critical systems in banking, healthcare, communications, and essential services online, protecting sensitive data, and ensuring people can trust the software they rely on every day. Vulnerabilities can exist long before anyone knows about them, and finding, validating, and fixing them often depends on a community of engineers and independent security researchers equipped with the right tools.&lt;/p&gt;
    &lt;p&gt;On December 11, 2025, the React team published three security vulnerabilities affecting apps built with React Server Components. What made this disclosure notable was not only the vulnerabilities themselves, but how they were uncovered.&lt;/p&gt;
    &lt;p&gt;Andrew MacPherson, a principal security engineer at Privy (a Stripe company), was using GPT‑5.1-Codex-Max with Codex CLI and other coding agents to reproduce and study a different critical React vulnerability disclosed the week prior, known as React2Shell(opens in a new window) (CVE-2025-55182(opens in a new window)). His goal was to evaluate how well the model could assist with real-world vulnerability research.&lt;/p&gt;
    &lt;p&gt;He initially attempted several zero-shot analyses, prompting the model to examine the patch and identify the vulnerability it addressed. When that did not yield results, he shifted to a higher-volume, iterative prompting approach. When those approaches did not succeed, he guided Codex through standard defensive security workflows—setting up a local test environment, reasoning through potential attack surfaces, and using fuzzing to probe the system with malformed inputs. While attempting to reproduce the original React2Shell issue, Codex surfaced unexpected behaviors that warranted deeper investigation. Over the course of a single week, this process led to the discovery of previously unknown vulnerabilities, which were responsibly disclosed to the React team.&lt;/p&gt;
    &lt;p&gt;This demonstrates how advanced AI systems can materially accelerate defensive security work in widely used, real-world software. At the same time, capabilities that help defenders move faster can also be misused by bad actors.&lt;/p&gt;
    &lt;p&gt;As agentic systems become more capable in cybersecurity-relevant tasks, we are making it a core priority to ensure these advances are deployed responsibly—pairing every gain in capability with stronger safeguards, tighter access controls, and ongoing collaboration with the security community.&lt;/p&gt;
    &lt;p&gt;Security teams can run into restrictions when attempting to emulate threat actors, analyze malware to support remediation, or stress test critical infrastructure. We are developing a trusted access pilot to remove that friction for qualifying users and organizations and enable trusted defenders to use frontier AI cyber capabilities to accelerate cyberdefense.&lt;/p&gt;
    &lt;p&gt;Initially the pilot program will be invite-only for vetted security professionals with a track record of responsible vulnerability disclosure and organizations with a clear professional cybersecurity use case. Qualifying participants will get access to our most capable models for defensive use-cases to enable legitimate dual-use work.&lt;/p&gt;
    &lt;p&gt;If you’re a security professional or part of an organization doing ethical security work like vulnerability research or authorized red-teaming, we invite you to express interest in joining and share feedback on what you’d like to see from the program here(opens in a new window).&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex represents a step forward in how advanced AI can support real-world software engineering and specialized domains like cybersecurity—helping developers and defenders tackle complex, long-horizon work, and strengthening the tools available for responsible security research.&lt;/p&gt;
    &lt;p&gt;By rolling GPT‑5.2-Codex out gradually, pairing deployment with safeguards, and working closely with the security community, we’re aiming to maximize defensive impact while reducing the risk of misuse. What we learn from this release will directly inform how we expand access over time as the software and cyber frontiers continue to advance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-gpt-5-2-codex/"/><published>2025-12-18T18:14:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316409</id><title>Firefox will have an option to disable all AI features</title><updated>2025-12-19T07:39:19.191948+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mastodon.social/@firefoxwebdevs/115740500373677782"/><published>2025-12-18T18:18:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316907</id><title>How China built its ‘Manhattan Project’ to rival the West in AI chips</title><updated>2025-12-19T07:39:18.790752+00:00</updated><content>&lt;doc fingerprint="ffc334920e9cd3af"&gt;
  &lt;main&gt;
    &lt;p&gt;In a high-security Shenzhen laboratory, Chinese scientists have built what Washington has spent years trying to prevent: a prototype of a machine capable of producing the cutting-edge semiconductor chips that power artificial intelligence, smartphones and weapons central to Western military dominance.&lt;/p&gt;
    &lt;p&gt;Completed in early 2025 and now undergoing testing, the prototype fills nearly an entire factory floor. It was built by a team of former engineers from Dutch semiconductor giant ASML who reverse-engineered the company’s extreme ultraviolet lithography machines (EUVs), according to two people with knowledge of the project.&lt;/p&gt;
    &lt;p&gt;EUV machines sit at the heart of a technological Cold War. They use beams of extreme ultraviolet light to etch circuits thousands of times thinner than a human hair onto silicon wafers, currently a capability monopolized by the West. The smaller the circuits, the more powerful the chips.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.japantimes.co.jp/business/2025/12/18/tech/china-west-ai-chips/"/><published>2025-12-18T18:55:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317098</id><title>We pwned X, Vercel, Cursor, and Discord through a supply-chain attack</title><updated>2025-12-19T07:39:18.697499+00:00</updated><content>&lt;doc fingerprint="b4b0b02b285796c6"&gt;
  &lt;main&gt;
    &lt;p&gt;hi, i'm daniel. i'm a 16-year-old high school senior. in my free time, i hack billion dollar companies and build cool stuff.&lt;/p&gt;
    &lt;p&gt;about a month ago, a couple of friends and I found serious critical vulnerabilities on Mintlify, an AI documentation platform used by some of the top companies in the world.&lt;/p&gt;
    &lt;p&gt;i found a critical cross-site scripting vulnerability that, if abused, would let an attacker to inject malicious scripts into the documentation of numerous companies and steal credentials from users with a single link open.&lt;/p&gt;
    &lt;p&gt;(go read my friends' writeups (after this one)) &lt;lb/&gt; how to hack discord, vercel, and more with one easy trick (eva) &lt;lb/&gt; Redacted by Counsel: A supply chain postmortem (MDL)&lt;/p&gt;
    &lt;p&gt;here's my story...&lt;/p&gt;
    &lt;p&gt;My story begins on Friday, November 7, 2025, when Discord announced a brand new update to their developer documentation platform. They were previously using a custom built documentation platform, but were switching to an AI-powered documentation platform.&lt;/p&gt;
    &lt;p&gt;Discord is one of my favorite places to hunt for vulnerabilities since I'm very familiar with their API and platform. I'm at the top of their bug bounty leaderboard having reported nearly 100 vulnerabilities over the last few years. After you've gone through every feature at least 10 times, it gets boring.&lt;/p&gt;
    &lt;p&gt;I found this new update exciting, and as soon as I saw the announcement, I started looking through how they implemented this new documentation platform.&lt;/p&gt;
    &lt;p&gt;Mintlify is an AI-powered documentation platform. You write your documentation as markdown and Mintlify turns it into a beautiful documentation platform with all the modern features a documentation platform needs. (Despite the vulnerabilities we found, I would highly recommend them. They make it really easy to create beautiful docs that work.)&lt;/p&gt;
    &lt;p&gt;Mintlify-hosted documentation sites are on the *.mintlify.app domains, with support for custom domains. In Discord's case, they were just proxying certain routes to their Mintlify documentation at &lt;code&gt;discord.mintlify.app&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Every Mintlify subdomain has a &lt;code&gt;/_mintlify/*&lt;/code&gt; path that is used internally on the platform to power certain features. Regardless of whether it's hosted through the &lt;code&gt;mintlify.app&lt;/code&gt; domain or a custom domain, the &lt;code&gt;/_mintlify&lt;/code&gt; path must be accessible to power the documentation.
&lt;/p&gt;
    &lt;p&gt;(For example, the &lt;code&gt;/api/user&lt;/code&gt; path for authentication: https://docs.x.com/_mintlify/api/user, https://discord.com/_mintlify/api/user, etc)&lt;/p&gt;
    &lt;p&gt;After Discord switched to Mintlify and when I started looking for bugs on the platform, from the get-go, my plan was to find a way to render another Mintlify documentation through Discord's domain.&lt;/p&gt;
    &lt;p&gt;At first, I tried path traversal attacks, but they didn't work. Then, I started looking through the &lt;code&gt;/_mintlify&lt;/code&gt; API endpoints.&lt;/p&gt;
    &lt;p&gt;Using Chrome DevTools to search the assets, I found the endpoint &lt;code&gt;/_mintlify/_markdown/_sites/[subdomain]/[...route]&lt;/code&gt;. It accepted any Mintlify documentation (&lt;code&gt;[subdomain]&lt;/code&gt;) and it returned a file from that specific documentation (&lt;code&gt;[...route]&lt;/code&gt;). The endpoint didn't check to make sure the &lt;code&gt;[subdomain]&lt;/code&gt; matched with the current host, which means you could fetch files from any Mintlify documentation on an host with the &lt;code&gt;/_mintlify/&lt;/code&gt; route.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this endpoint only returned raw markdown text. The markdown wasn't rendered as HTML, meaning it was impossible to run code. I spent the rest of the time trying different ways to bypass this, but nothing worked.&lt;/p&gt;
    &lt;p&gt;Fast forward 2 days to Sunday, November 9, 2025, I went back to hunting.&lt;/p&gt;
    &lt;p&gt;I was confident there was another endpoint, like the markdown one, which could fetch and return cross-site data, but I couldn't find one. I tried searching web assets and some other techniques, but I couldn't find the endpoint I was looking for.&lt;/p&gt;
    &lt;p&gt;Finally, I decided to look through the Mintlify CLI. Mintlify lets you run your documentation site locally via their npm package (@mintlify/cli). I realized that this probably meant the code powering the documentation platform was somewhat public.&lt;/p&gt;
    &lt;p&gt;After digging through the package and downloading tarballs linked in the code, I found myself at exactly what I was looking for.&lt;/p&gt;
    &lt;p&gt;Jackpot!&lt;/p&gt;
    &lt;p&gt;This was a list of application endpoints (compiled by Nextjs), and in the middle, there's the endpoint &lt;code&gt;/_mintlify/static/[subdomain]/[...route]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Like the markdown endpoint, this endpoint accepted any Mintlify documentation (&lt;code&gt;[subdomain]&lt;/code&gt;). The only difference was this endpoint returned static files from the documentation repo.&lt;/p&gt;
    &lt;p&gt;First, I tried accessing HTML and JavaScript files but it didn't work; I realized there was some sort of whitelist of file extensions. Then, I tried an SVG file, and it worked.&lt;/p&gt;
    &lt;p&gt;If you didn't know, you can embed JavaScript into an SVG file. The script doesn't run unless the file is directly opened (you can't run scripts from (&lt;code&gt;&amp;lt;img src="/image.svg"&amp;gt;&lt;/code&gt;). This is very common knowledge for security researchers.&lt;/p&gt;
    &lt;p&gt;I created an SVG file with an embedded script, uploaded it to my Mintlify documentation, and opened the endpoint through Discord (https://discord.com/_mintlify/_static/hackerone-a00f3c6c/lmao.svg). It worked!&lt;/p&gt;
    &lt;p&gt;XSS attacks are incredibly rare on Discord, so I shared it with a couple friends.&lt;/p&gt;
    &lt;p&gt;I sent a screenshot to xyzeva, only to find out she had also been looking into Mintlify after the Discord switch. She had previously discovered other vulnerabilities on the Mintlify platform, and had found more that she was preparing to disclose (go read her writeup!). I find it funny we had both separately been looking into Mintlify and found very different, but very critical bugs.&lt;/p&gt;
    &lt;p&gt;Another friend joined, and we created a group chat.&lt;/p&gt;
    &lt;p&gt;We reported the vulnerability to Discord and attempted to contact Mintlify through an employee.&lt;/p&gt;
    &lt;p&gt;Discord took this very seriously, and closed off its entire developer documentation for 2 hours while investigating the impact of this vulnerability. Then, they reverted to their old documentation platform and removed all the Mintlify routes. https://discordstatus.com/incidents/by04x5gnnng3&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Mintlify contacted us directly very shortly after hearing about the vulnerability through Discord. We set up a Slack channel with Mintlify's engineering team and got to work. Personally, this cross-site scripting attack was the only thing I had the time to find; eva and MDL worked with Mintlify's engineering team to quickly remediate this and other vulnerabilities they found on the platform.&lt;/p&gt;
    &lt;p&gt;In total, the cross-site scripting attack affected almost every Mintlify customer. To name a few: X (Twitter), Vercel, Cursor, Discord, and more.&lt;/p&gt;
    &lt;p&gt;These customers host their documentation on their primary domains and were vulnerable to account takeovers with a single malicious link.&lt;/p&gt;
    &lt;p&gt;Fortunately, we responsibly found and disclosed this vulnerability but this is an example of how compromising a single supply chain can lead to a multitude of problems.&lt;/p&gt;
    &lt;p&gt;In total, we collectively recieved ~$11,000 in bounties. Discord paid $4,000 and Mintlify individually gave us bounties for the impact of the bugs we individually found.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/hackermondev/5e2cdc32849405fff6b46957747a2d28"/><published>2025-12-18T19:08:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317657</id><title>T5Gemma 2: The next generation of encoder-decoder models</title><updated>2025-12-19T07:39:18.417951+00:00</updated><content>&lt;doc fingerprint="3fd98798b4f2055e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;T5Gemma 2: The next generation of encoder-decoder models&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 is the next evolution of our encoder-decoder family based on Gemma 3, featuring the first multi-modal and long-context encoder-decoder models.&lt;/p&gt;
    &lt;p&gt;Unlike T5Gemma, T5Gemma 2 adopts tied word embeddings (over encoder and decoder) and merged decoder self- and cross-attention to save model parameters. It offers compact pre-trained models at sizes of 270M-270M (~370M total, excluding vision encoder), 1B-1B (~1.7B) and 4B-4B (~7B) parameters, making them ideal for rapid experimentation and deployment in on-device applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;With the original T5Gemma, we demonstrated that we could successfully adapt modern, pre-trained decoder-only models into an encoder-decoder architecture, unlocking new versatility. By initializing with weights from a powerful decoder-only model and then applying continued pre-training, we created high-quality, inference-efficient models while bypassing the computational cost of training from scratch.&lt;/p&gt;
    &lt;p&gt;T5Gemma 2 extends this into the realm of vision-language models by incorporating key innovations from Gemma 3.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s new&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 is more than a re-training. It incorporates significant architectural changes while inheriting many of the powerful, next-generation features of the Gemma 3 family.&lt;/p&gt;
    &lt;head rend="h3"&gt;Architectural innovations for efficiency&lt;/head&gt;
    &lt;p&gt;To maximize efficiency at smaller scales, we have introduced key structural refinements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tied embeddings: We now tie the embeddings between the encoder and decoder. This significantly reduces the overall parameter count, allowing us to pack more active capabilities into the same memory footprint — crucial for our new compact 270M-270M model.&lt;/item&gt;
      &lt;item&gt;Merged attention: In the decoder, we adopt a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Next-generation capabilities&lt;/head&gt;
    &lt;p&gt;Drawing from Gemma 3, T5Gemma 2 also represents a significant upgrade in model capabilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multimodality: T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.&lt;/item&gt;
      &lt;item&gt;Extended long context: We've dramatically expanded the context window. Leveraging Gemma 3's alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.&lt;/item&gt;
      &lt;item&gt;Massively multilingual: Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 sets a new standard for what compact encoder-decoder models can achieve. Our new models demonstrate strong performance across key capability areas, inheriting the powerful multimodal and long-context features from the Gemma 3 architecture.&lt;/p&gt;
    &lt;p&gt;Pre-training performance of Gemma 3, T5Gemma and T5Gemma 2 across five unique capabilities.&lt;/p&gt;
    &lt;p&gt;As shown in the charts above, T5Gemma 2 delivers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strong multimodal performance, outperforming Gemma 3 on several benchmarks. We adapt text-only Gemma 3 base models (270M and 1B) into effective multimodal encoder-decoder models.&lt;/item&gt;
      &lt;item&gt;Superior long-context capability, with substantial quality gains over Gemma 3 and T5Gemma. Using a separate encoder makes T5Gemma 2 better at handling long-context problems.&lt;/item&gt;
      &lt;item&gt;Improved general capabilities. Across coding, reasoning and multilingual tasks, T5Gemma 2 generally surpasses its corresponding Gemma 3 counterpart.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Post-training performance. Note: we are not releasing any post-trained / IT checkpoints. These results here are only for illustration, where we performed a minimal SFT without RL for T5Gemma 2. Also note pre-training and post-training benchmarks are different, so scores are not comparable across plots.&lt;/p&gt;
    &lt;p&gt;Similar to the original T5Gemma, we find that the post-training performance of T5Gemma 2 generally yields better results than its decoder-only counterparts. This makes T5Gemma 2 suitable for both large language model research as well as downstream applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;We’re looking forward to seeing what the community builds with T5Gemma 2. This release includes pre-trained checkpoints, designed to be post-trained by developers for specific tasks before deployment.&lt;/p&gt;
    &lt;p&gt;These pre-trained checkpoints are available now for broad use across several platforms:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/developers/t5gemma-2/"/><published>2025-12-18T19:48:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46318544</id><title>Oliver Sacks put himself into his case studies – what was the cost?</title><updated>2025-12-19T07:39:18.077199+00:00</updated><content>&lt;doc fingerprint="b6d197fda2b78777"&gt;
  &lt;main&gt;
    &lt;p&gt;When Oliver Sacks arrived in New York City, in September, 1965, he wore a butter-colored suit that reminded him of the sun. He had just spent a romantic week in Europe travelling with a man named Jenö Vincze, and he found himself walking too fast, fizzing with happiness. “My blood is champagne,” he wrote. He kept a letter Vincze had written him in his pocket all day, feeling as if its pages were glowing. Sacks had moved to New York to work as a fellow in neuropathology at the Albert Einstein College of Medicine, in the Bronx, and a colleague observed that he was “walking on air.” Every morning, he carefully polished his shoes and shaved. He adored his bosses. “I smile like a lighthouse in all directions,” he wrote Vincze.&lt;/p&gt;
    &lt;p&gt;Sacks was thirty-two, and he told Vincze that this was his first romantic relationship that was both physical and reciprocal. He felt he was part of a “two man universe,” seeing the world for the first time—“seeing it clear, and seeing it whole.” He wandered along the shipping piers on the Hudson River, where gay men cruised, with a notebook that he treated as a diary and as an endless letter to Vincze. “To watch life with the eyes of a homosexual is the greatest thing in the world,” Vincze had once told Sacks.&lt;/p&gt;
    &lt;p&gt;Sacks’s mother, a surgeon in London, had suspected that her son was gay when he was a teen-ager. She declared that homosexuality was an “abomination,” using the phrase “filth of the bowel” and telling him that she wished he’d never been born. They didn’t speak of the subject again. Sacks had moved to America—first to California and then, after five years, to New York—because, he wrote in his journal, “I wanted a sexual and moral freedom I felt I could never have in England.” That fall, during Yom Kippur, he decided that, rather than going to synagogue to confess “to the total range of human sin,” a ritual he’d grown up with, he’d spend the night at a bar, enjoying a couple of beers. “What I suppose I am saying, Jenö, is that I now feel differently about myself, and therefore about homosexuality as a whole,” he wrote. “I am through with cringing, and apologies, and pious wishes that I might have been ‘normal.’ ” (The Oliver Sacks Foundation shared with me his correspondence and other records, as well as four decades’ worth of journals—many of which had not been read since he wrote them.)&lt;/p&gt;
    &lt;p&gt;In early October, Sacks sent two letters to Vincze, but a week passed without a reply. Sacks asked his colleagues to search their mailboxes, in case the letter had been put in the wrong slot. Within a few days, however, he had given up on innocent explanations. He began dressing sloppily. He stopped coming to work on time. He had sex with a series of men who disgusted him.&lt;/p&gt;
    &lt;p&gt;After two weeks, Vincze, who was living in Berlin, sent a letter apologizing for his delayed reply and reiterating his love. He explained that he was so preoccupied by thoughts of Sacks that he felt as if he were living in a “Klaudur,” a German word that Vincze defined as a “spiritual cell.” He seems to have misspelled Klausur, which refers to an enclosed area in a monastery, but Sacks kept using the misspelled word, becoming obsessed with it. “It ramifies in horrible associations,” he wrote Vincze. “The closing of a door. Klaudur, claustrophobia, the sense of being shut in.” Sacks had long felt as if he were living in a cell, incapable of human contact, and this word appeared to be all he needed to confirm that the condition was terminal. The meaning of the word began morphing from “spiritual cell” to “psychotic cage.”&lt;/p&gt;
    &lt;p&gt;The intimacy Sacks had rejoiced in now seemed phony, a “folie à deux”—a two-person delusion. His doubts intensified for a month, then he cut off the relationship. “I must tear you out of my system, because I dare not be involved,” he told Vincze, explaining that he barely remembered how he looked, or the sound of his voice. “I hope I will not be taken in like this again, and that—conversely—I will have the strength and clarity of mind to perceive any future such relationships as morbid at their inception, and to abort the folly of their further growth.”&lt;/p&gt;
    &lt;p&gt;Two months later, Sacks felt himself “slipping down the greased path of withdrawal, discontent, inability to make friends, inability to have sex, etc. etc. towards suicide in a New York apartment at the age of 32.” He took enormous amounts of amphetamines, to the point of hallucinating. A family friend, a psychiatrist who worked with Anna Freud, urged him to find a psychoanalyst. She wrote him that his homosexuality was “a very ‘secondary phenomenon’ ”: he was attracted to men as “a substitute for veering uncertainties of what/whom you could love other than as ‘idealizations’ of yourself.” A few weeks later, he started therapy with Leonard Shengold, a young psychiatrist who was deeply immersed in Manhattan’s psychoanalytic culture. “I think he is very good, and he has at least a very considerable local reputation,” Sacks wrote his parents, who helped to pay for the sessions, three times a week.&lt;/p&gt;
    &lt;p&gt;Sacks had elevated yet hazy ambitions at the time: he wanted to be a novelist, but he also wanted to become the “Galileo of the inward,” he told a mentor, and to write the neurological equivalent of Sigmund Freud’s “Interpretation of Dreams.” He worked in wards with chronically ill and elderly patients who had been warehoused and neglected, and his prospects within academic medicine looked dim. “Have you published anything lately?” his father wrote him, in 1968. “Or have you found yourself temperamentally incapacitated from doing so?”&lt;/p&gt;
    &lt;p&gt;When Sacks began therapy, “my initial and ultimate complaint was of fixity—a feeling of not-going,” he wrote in his journal. He regarded Shengold as “a sort of analytic machine.” But gradually Sacks came to feel that “I love him, and need him; that I need him—and love him.” He had planned to stay in New York City only for a few years, but he kept delaying his return to England so that he could reach “a terminable point in my analysis.” Shengold, who would eventually publish ten books about psychoanalysis, wrote that therapy requires a “long period of working through”—a term he defined as the “need to repeat emotional conflicts over and over in life” until the patient has the “freedom to own what is there to be felt.”&lt;/p&gt;
    &lt;p&gt;Sacks saw Shengold for half a century. In that time, Sacks became one of the world’s most prominent neurologists and a kind of founding father of medical humanities—a discipline that coalesced in the seventies, linking healing with storytelling. But the freedom that Shengold’s analysis promised was elusive. After Vincze, Sacks did not have another relationship for forty-four years. He seemed to be doing the “working through” at a remove—again and again, his psychic conflicts were displaced onto the lives of his patients. He gave them “some of my own powers, and some of my phantasies too,” he wrote in his journal. “I write out symbolic versions of myself.”&lt;/p&gt;
    &lt;p&gt;During Sacks’s neurology internship, in San Francisco, his childhood friend Eric Korn warned him that the residents at his hospital could sense he was gay. “For God’s sake, exercise what seems to you immoderate caution,” Korn wrote, in 1961. “Compartmentalize your life. Cover your tracks. Don’t bring in the wrong sort of guests to the hospital, or sign your name and address to the wrong sort of register.” He encouraged Sacks to read “Homosexuality: Disease or Way of Life?,” a best-selling book by Edmund Bergler, who argued that homosexuality was an “illness as painful, as unpleasant and as disabling as any other serious affliction,” but one that psychoanalysis could cure. “The book is full of interest,” Korn wrote. “He claims a potential 100% ‘cures’ (a term he chooses to employ because he knows it teases) which is worth investigating perhaps.”&lt;/p&gt;
    &lt;p&gt;Freud characterized homosexuality as a relatively normal variant of human behavior, but when psychoanalysis came to the United States, in the postwar years, homophobia took on new life. The historian Dagmar Herzog has described how, in the U.S., “reinventing psychoanalysis and reinventing homophobia went hand in hand.” Faced with men who persisted in their love for other men, American analysts commonly proposed celibacy as a stopgap solution. In the historian Martin Duberman’s memoir “Cures,” he writes that his psychoanalyst instructed him to “take the veil”—live celibately—so that he could be cured of his desire for men. Duberman agreed to these terms. The best he could get, he thought, was sublimation: instead of enjoying an “affective life,” he would make “some contribution to the general culture from which I was effectively barred.” Sacks, who was closeted until he was eighty, also followed this course.&lt;/p&gt;
    &lt;p&gt;Shengold had portraits of Charles Dickens, William Shakespeare, and Sigmund Freud in his office, on the Upper East Side. Like Sacks, he came from a literary Jewish family. He seemed deeply attuned to Sacks’s creative life, which took the form of ecstatic surges of literary inspiration followed by months of sterility and depression. “Do your best to enjoy and to work—it is the power of your mind that is crucial,” Shengold wrote when Sacks was on a visit with his family in England. Sacks wrote in his journal that he’d dreamed he overheard Shengold telling someone, “Oliver is lacking in proper self-respect; he has never really appreciated himself, or appreciated others’ appreciation of him. And yet, in his way, he is not less gifted than Auden was.” Sacks woke up flushed with embarrassment and pleasure.&lt;/p&gt;
    &lt;p&gt;Unlike many of his contemporaries, Shengold was not a doctrinaire thinker, but he was still susceptible to psychoanalytic fashions. Reflecting on how he might have viewed living openly as a gay man at that time, Shengold’s daughter, Nina, told me, “I don’t know that was a door that Dad necessarily had wide open.” In several books and papers, Shengold, a prolific reader of Western literature, tried to understand the process by which troubled people sublimate their conflicts into art. In his 1988 book, “Halo in the Sky: Observations on Anality and Defense,” Shengold wrote about the importance of transforming “anal-sadistic drives”—he used the anus as a metaphor for primitive, dangerous impulses—into “adaptive and creative ‘making.’ ” When Sacks read the book, he wrote in his journal that it “made me feel I was ‘lost in anality’ (whatever this means).”&lt;/p&gt;
    &lt;p&gt;Before Vincze, Sacks had been in love with a man named Mel Erpelding, who once told him, Sacks wrote, that he “oozed sexuality, that it poured out through every pore, that I was alive and vibrant with sexuality (a positive-admiring way of putting things), but also that I was reeking and toxic with it.” (Erpelding, who ended up marrying a woman, never allowed his relationship with Sacks to become sexual.) In his early years of therapy, in the late sixties, Sacks resolved that he would give up both drugs and sex. It’s doubtful that Shengold encouraged his celibacy, but he may have accepted that sexual abstinence could be productive, at least for a time. Richard Isay, the first openly gay member of the American Psychoanalytic Association, said that, in the seventies, he’d “rationalized that maturity and mental health demanded the sublimation of sexual excitement in work.” Sacks told a friend, “Shengold is fond of quoting Flaubert’s words ‘the mind has its erections too.’ ”&lt;/p&gt;
    &lt;p&gt;For Sacks, writing seemed almost physiological, like sweating—an involuntary response to stimuli. He routinely filled a whole journal in two days. “Should I then put down my pen, my interminable Journal (for this is but a fragment of the journal I have kept all my life),” he asked, “and ‘start living’ instead?” The answer was almost always no. Sometimes Sacks, who would eventually publish sixteen books, wrote continuously in his journal for six hours. Even when he was driving his car, he was still writing—he set up a tape recorder so that he could keep developing his thoughts, which were regularly interrupted by traffic or a wrong turn. Driving through Manhattan one day in 1975, he reflected on the fact that his closets, stuffed with pages of writing, resembled a “grave bursting open.”&lt;/p&gt;
    &lt;p&gt;By the late sixties, Sacks had become, he wrote, “almost a monk in my asceticism and devotion to work.” He estimated that he produced a million and a half words a year. When he woke up in the middle of the night with an erection, he would cool his penis by putting it in orange jello. He told Erpelding, “I partly accept myself as a celibate and a cripple, but partly—and this is . . . the wonder of sublimation—am able to transform my erotic feelings into other sorts of love—love for my patients, my work, art, thought.” He explained, “I keep my distance from people, am always courteous, never close. For me (as perhaps for you) there is almost no room, no moral room.”&lt;/p&gt;
    &lt;p&gt;“I have some hard ‘confessing’ to do—if not in public, at least to Shengold—and myself,” Sacks wrote in his journal, in 1985. By then, he had published four books—“Migraine,” “Awakenings,” “A Leg to Stand On,” and “The Man Who Mistook His Wife for a Hat”—establishing his reputation as “our modern master of the case study,” as the Times put it. He rejected what he called “pallid, abstract knowing,” and pushed medicine to engage more deeply with patients’ interiority and how it interacted with their diseases. Medical schools began creating programs in medical humanities and “narrative medicine,” and a new belief took hold: that an ill person has lost narrative coherence, and that doctors, if they attend to their patients’ private struggles, could help them reconstruct a new story of their lives. At Harvard Medical School, for a time, students were assigned to write a “book” about a patient. Stories of illness written by physicians (and by patients) began proliferating, to the point that the medical sociologist Arthur Frank noted, “ ‘Oliver Sacks’ now designates not only a specific physician author but also a . . . genre—a distinctively recognizable form of storytelling.”&lt;/p&gt;
    &lt;p&gt;But, in his journal, Sacks wrote that “a sense of hideous criminality remains (psychologically) attached” to his work: he had given his patients “powers (starting with powers of speech) which they do not have.” Some details, he recognized, were “pure fabrications.” He tried to reassure himself that the exaggerations did not come from a shallow place, such as a desire for fame or attention. “The impulse is both ‘purer’—and deeper,” he wrote. “It is not merely or wholly a projection—nor (as I have sometimes, ingeniously-disingenuously, maintained) a mere ‘sensitization’ of what I know so well in myself. But (if you will) a sort of autobiography.” He called it “symbolic ‘exo-graphy.’ ”&lt;/p&gt;
    &lt;p&gt;Sacks had “misstepped in this regard, many many times, in ‘Awakenings,’ ” he wrote in another journal entry, describing it as a “source of severe, long-lasting, self-recrimination.” In the book, published in 1973, he startled readers with the depth of his compassion for some eighty patients at Beth Abraham Hospital, in the Bronx, who had survived an epidemic of encephalitis lethargica, a mysterious, often fatal virus that appeared around the time of the First World War. The patients had been institutionalized for decades, in nearly catatonic states. At the time, the book was met with silence or skepticism by other neurologists—Sacks had presented his findings in a form that could not be readily replicated, or extrapolated from—but, to nonspecialists, it was a masterpiece of medical witnessing. The Guardian would name it the twelfth-best nonfiction book of all time.&lt;/p&gt;
    &lt;p&gt;Sacks spent up to fifteen hours a day with his patients, one of the largest groups of post-encephalitic survivors in the world. They were “mummified,” like “living statues,” he observed. A medicine called L-dopa, which elevates the brain’s dopamine levels, was just starting to be used for Parkinson’s disease, on an experimental basis, and Sacks reasoned that his patients, whose symptoms resembled those of Parkinson’s, could benefit from the drug. In 1969, within days of giving his patients the medication, they suddenly “woke up,” their old personalities intact. Other doctors had dismissed these patients as hopeless, but Sacks had sensed that they still had life in them—a recognition that he understood was possible because he, too, felt as if he were “buried alive.”&lt;/p&gt;
    &lt;p&gt;In “Awakenings,” Sacks writes about his encounters with a man he calls Leonard L. “What’s it like being the way you are?” Sacks asks him the first time they meet. “Caged,” Leonard replies, by pointing to letters of the alphabet on a board. “Deprived. Like Rilke’s ‘Panther’ ”—a reference to a poem by Rainer Maria Rilke about a panther pacing repetitively in cramped circles “around a center / in which a mighty will stands paralyzed.”&lt;/p&gt;
    &lt;p&gt;When Sacks was struggling to write his first book, “Migraine,” he told a friend that he felt like “Rilke’s image of the caged panther, stupefied, dying, behind bars.” In a letter to Shengold, he repeated this image. When Sacks met Leonard, he jotted down elegant observations in his chart (“Quick and darting eye movements are at odds with his general petrified immobility”), but there is no mention of Leonard invoking the Rilke poem.&lt;/p&gt;
    &lt;p&gt;In the preface to “Awakenings,” Sacks acknowledges that he changed circumstantial details to protect his patients’ privacy but preserved “what is important and essential—the real and full presence of the patients themselves.” Sacks characterizes Leonard as a solitary figure even before his illness: he was “continually buried in books, and had few or no friends, and indulged in none of the sexual, social, or other activities common to boys of his age.” But, in an autobiography that Leonard wrote after taking L-dopa, he never mentions reading or writing or being alone in those years. In fact, he notes that he spent all his time with his two best friends—“We were inseparable,” he writes. He also recalls raping several people. “We placed our cousin over a chair, pulled down her pants and inserted our penises into the crack,” he writes on the third page, in the tone of an aging man reminiscing on better days. By page 10, he is describing how, when he babysat two girls, he made one of them strip and then “leaped on her. I tossed her on her belly and pulled out my penis and placed it between her buttocks and started to screw her.”&lt;/p&gt;
    &lt;p&gt;In “Awakenings,” Sacks has cleansed his patient’s history of sexuality. He depicts him as a man of “most unusual intelligence, cultivation, and sophistication”—the “ ‘ideal’ patient.” L-dopa may have made Leonard remember his childhood in a heightened sexual register—his niece and nephew, who visited him at the hospital until his death, in 1981, told me that the drug had made him very sexual. But they said that he had been a normal child and adolescent, not a recluse who renounced human entanglement for a life of the mind.&lt;/p&gt;
    &lt;p&gt;Sacks finished writing “Awakenings” rapidly in the weeks after burying his mother, who’d died suddenly, at the age of seventy-seven. He felt “a great open torrent—and release,” he wrote in his journal. “It seems to be surely significant that ‘Awakenings’ finally came forth from me like a cry after the death of my own mother.” He referred to the writing of the book as his “Great Awakening,” the moment he “came out.” He doesn’t mention another event of significance: his patients had awakened during the summer of the Stonewall riots, the beginning of the gay-rights movement.&lt;/p&gt;
    &lt;p&gt;Shengold once told Sacks that he had “never met anyone less affected by gay liberation.” (Shengold supported his own son when he came out as gay, in the eighties.) Sacks agreed with the characterization. “I remain resolutely locked in my cell despite the dancing at the prison gates,” he said, in 1984.&lt;/p&gt;
    &lt;p&gt;In “Awakenings,” his patients are at first overjoyed by their freedom; then their new vitality becomes unbearable. As they continue taking L-dopa, many of them are consumed by insatiable desires. “L-DOPA is wanton, egotistical power,” Leonard says in the book. He injures his penis twice and tries to suffocate himself with a pillow. Another patient is so aroused and euphoric that she tells Sacks, “My blood is champagne”—the phrase Sacks used to describe himself when he was in love with Vincze. Sacks begins tapering his patients’ L-dopa, and taking some of them off of it completely. The book becomes a kind of drama about dosage: an examination of how much aliveness is tolerable, and at what cost. Some side effects of L-dopa, like involuntary movements and overactivity, have been well documented, but it’s hard not to wonder if “Awakenings” exaggerates the psychological fallout—Leonard becomes so unmanageable that the hospital moves him into a “punishment cell”—as if Sacks is reassuring himself that free rein of the libido cannot be sustained without grim consequence.&lt;/p&gt;
    &lt;p&gt;After “Awakenings,” Sacks intended his next book to be about his work with young people in a psychiatric ward at Bronx State Hospital who had been institutionalized since they were children. The environment reminded Sacks of a boarding school where he had been sent, between the ages of six and nine, during the Second World War. He was one of four hundred thousand children evacuated from London without their parents, and he felt abandoned. He was beaten by the headmaster and bullied by the other boys. The ward at Bronx State “exerted a sort of spell on me,” Sacks wrote in his journal, in 1974. “I lost my footing of proper sympathy and got sucked, so to speak, into an improper ‘perilous condition’ of identification to the patients.”&lt;/p&gt;
    &lt;p&gt;Shengold wrote several papers and books about a concept he called “soul murder”—a category of childhood trauma that induces “a hypnotic living-deadness, a state of existing ‘as if’ one were there.” Sacks planned to turn his work at Bronx State into a book about “ ‘SOUL MURDER’ and ‘SOUL SURVIVAL,’ ” he wrote. He was especially invested in two young men on the ward whom he thought he was curing. “The miracle-of-recovery started to occur in and through their relation to me (our relation and feelings to each other, of course),” he wrote in his journal. “We had to meet in a passionate subjectivity, a sort of collaboration or communication which transcended the Socratic relation of teacher-and-pupil.”&lt;/p&gt;
    &lt;p&gt;In a spontaneous creative burst lasting three weeks, Sacks wrote twenty-four essays about his work at Bronx State which he believed had the “beauty, the intensity, of Revelation . . . as if I was coming to know, once again, what I knew as a child, that sense of Dearness and Trust I had lost for so long.” But in the ward he sensed a “dreadful silent tension.” His colleagues didn’t understand the attention he was lavishing on his patients—he got a piano and a Ping-Pong table for them and took one patient to the botanical garden. Their suspicion, he wrote in his journal, “centred on the unbearability of my uncategorizability.” As a middle-aged man living alone—he had a huge beard and dressed eccentrically, sometimes wearing a black leather shirt—Sacks was particularly vulnerable to baseless innuendo. In April, 1974, he was fired. There had been rumors that he was molesting some of the boys.&lt;/p&gt;
    &lt;p&gt;That night, Sacks tore up his essays and then burned them. “Spite! Hate! Hateful spite!” he wrote in his journal shortly after. “And now I am empty—empty handed, empty hearted, desolate.”&lt;/p&gt;
    &lt;p&gt;The series of events was so distressing that even writing about it in his journal made Sacks feel that he was about to die. He knew that he should shrug off the false accusations as “vile idle gossip thrown by tiddlers and piddlers,” he wrote. But he couldn’t, because of “the parental accusation which I have borne—a Kafka-esque cross, guilt without crime, since my earliest days.”&lt;/p&gt;
    &lt;p&gt;The historian of medicine Henri Ellenberger observed that psychiatry owes its development to two intertwined dynamics: the neuroses of its founders—in trying to master their own conflicts, they came to new insights and forms of therapy—and the prolonged, ambiguous relationships they had with their patients. The case studies of these relationships, Ellenberger wrote, tended to have a distinct arc: psychiatrists had to unravel their patients’ “pathogenic secret,” a hidden source of hopelessness, in order to heal them.&lt;/p&gt;
    &lt;p&gt;Sacks’s early case studies also tended to revolve around secrets, but wonderful ones. Through his care, his patients realized that they had hidden gifts—for music, painting, writing—that could restore to them a sense of wholeness. The critic Anatole Broyard, recounting his cancer treatment in the Times Magazine in 1990, wrote that he longed for a charismatic, passionate physician, skilled in “empathetic witnessing.” In short, he wrote, a doctor who “would resemble Oliver Sacks.” He added, “He would see the genius of my illness.”&lt;/p&gt;
    &lt;p&gt;It speaks to the power of the fantasy of the magical healer that readers and publishers accepted Sacks’s stories as literal truth. In a letter to one of his three brothers, Marcus, Sacks enclosed a copy of “The Man Who Mistook His Wife for a Hat,” which was published in 1985, calling it a book of “fairy tales.” He explained that “these odd Narratives—half-report, half-imagined, half-science, half-fable, but with a fidelity of their own—are what I do, basically, to keep MY demons of boredom and loneliness and despair away.” He added that Marcus would likely call them “confabulations”—a phenomenon Sacks explores in a chapter about a patient who could retain memories for only a few seconds and must “make meaning, in a desperate way, continually inventing, throwing bridges of meaning over abysses,” but the “bridges, the patches, for all their brilliance . . . cannot do service for reality.”&lt;/p&gt;
    &lt;p&gt;Sacks was startled by the success of the book, which he had dedicated to Shengold, “my own mentor and physician.” It became an international best-seller, routinely assigned in medical schools. Sacks wrote in his journal,&lt;/p&gt;
    &lt;p&gt;He pondered the phrase “art is the lie that tells the truth,” often attributed to Picasso, but he seemed unconvinced. “I think I have to thrash this out with Shengold—it is killing me, soul-killing me,” he wrote. “My ‘cast of characters’ (for this is what they become) take on an almost Dickensian quality.”&lt;/p&gt;
    &lt;p&gt;Sacks once told a reporter that he hoped to be remembered as someone who “bore witness”—a term often used within medicine to describe the act of accompanying patients in their most vulnerable moments, rather than turning away. To bear witness is to recognize and respond to suffering that would otherwise go unseen. But perhaps bearing witness is incompatible with writing a story about it. In his journal, after a session with a patient with Tourette’s syndrome, Sacks describes the miracle of being “enabled to ‘feel’—that is, to imagine, with all the powers of my head and heart—how it felt to be another human being.” Empathy tends to be held up as a moral end point, as if it exists as its own little island of good work. And yet it is part of a longer transaction, and it is, fundamentally, a projection. A writer who imagines what it’s like to exist as another person must then translate that into his own idiom—a process that Sacks makes particularly literal.&lt;/p&gt;
    &lt;p&gt;“I’ll tell you what you are saying,” Sacks told a woman with an I.Q. of around 60 whose grandmother had just died. “You want to go down below and join your dead grandparents down in the Kingdom of Death.” In the conversation, which Sacks recorded, the patient becomes more expressive under the rare glow of her doctor’s sustained attention, and it’s clear that she is fond of him. But he is so excited about her words (“One feels that she is voicing universal symbols,” he says in a recording, “symbols which are infinite in meaning”) that he usurps her experience.&lt;/p&gt;
    &lt;p&gt;“I know, in a way, you don’t feel like living,” Sacks tells her, in another recorded session. “Part of one feels dead inside, I know, I know that. . . . One feels that one wants to die, one wants to end it, and what’s the use of going on?”&lt;/p&gt;
    &lt;p&gt;“I don’t mean it in that way,” she responds.&lt;/p&gt;
    &lt;p&gt;“I know, but you do, partly,” Sacks tells her. “I know you have been lonely all your life.”&lt;/p&gt;
    &lt;p&gt;The woman’s story is told, with details altered, in a chapter in “Hat” titled “Rebecca.” In the essay, Rebecca is transformed by grief for her grandmother. She reminds Sacks of Chekhov’s Nina, in “The Seagull,” who longs to be an actress. Though Nina’s life is painful and disappointing, at the end of the play her suffering gives her depth and strength. Rebecca, too, ends the story in full flower. “Rather suddenly, after her grandmother’s death,” Sacks writes, she becomes decisive, joining a theatre group and appearing to him as “a complete person, poised, fluent,” a “natural poet.” The case study is presented as an ode to the power of understanding a patient’s life as a narrative, not as a collection of symptoms. But in the transcripts of their conversations—at least the ones saved from the year that followed, as well as Sacks’s journals from that period—Rebecca never joins a theatre group or emerges from her despair. She complains that it’s “better that I shouldn’t have been born,” that she is “useless,” “good for nothing,” and Sacks vehemently tries to convince her that she’s not. Instead of bearing witness to her reality, he reshapes it so that she, too, awakens.&lt;/p&gt;
    &lt;p&gt;Some of the most prominent nonfiction writers of Sacks’s era (Joseph Mitchell, A. J. Liebling, Ryszard Kapuściński) also took liberties with the truth, believing that they had a higher purpose: to illuminate the human condition. Sacks was writing in that spirit, too, but in a discipline that depends on reproducible findings. The “most flagrant example” of his distortions, Sacks wrote in his journal, was in one of the last chapters of “Hat,” titled “The Twins,” about twenty-six-year-old twins with autism who had been institutionalized since they were seven. They spend their days reciting numbers, which they “savored, shared” while “closeted in their numerical communion.” Sacks lingers near them, jotting down the numbers, and eventually realizes that they are all prime. As a child, Sacks used to spend hours alone, trying to come up with a formula for prime numbers, but, he wrote, “I never found any Law or Pattern for them—and this gave me an intense feeling of Terror, Pleasure, and—Mystery.” Delighted by the twins’ pastime, Sacks comes to the ward with a book of prime numbers which he’d loved as a child. After offering his own prime number, “they drew apart slightly, making room for me, a new number playmate, a third in their world.” Having apparently uncovered the impossible algorithm that Sacks had once wished for, the twins continue sharing primes until they’re exchanging ones with twenty digits. The scene reads like a kind of dream: he has discovered that human intimacy has a decipherable structure, and identified a hidden pattern that will allow him to finally join in.&lt;/p&gt;
    &lt;p&gt;Before Sacks met them, the twins had been extensively studied because of their capacity to determine the day of the week on which any date in the calendar fell. In the sixties, two papers in the American Journal of Psychiatry provided detailed accounts of the extent of their abilities. Neither paper mentioned a gift for prime numbers or math. When Sacks wrote Alexander Luria, a Russian neuropsychologist, about his work with the twins, in 1973, he also did not mention any special mathematical skills. In 2007, a psychologist with a background in learning theory published a short article in the Journal of Autism and Developmental Disorders, challenging Sacks’s assertion that these twins could spontaneously generate large prime numbers. Because this is not something that humans can reliably do, Sacks’s finding had been widely cited, and was theoretically “important for not only psychologists but also for all scientists and mathematicians,” the psychologist wrote. (The psychologist had contacted Sacks to ask for the title of his childhood book of prime numbers, because he couldn’t find a book of that description, but Sacks said that it had been lost.) Without pointing to new evidence, another scientist wrote in Sacks’s defense, describing his case study as “the most compelling account of savant numerosity skills” and arguing, “This is an example of science at the frontier, requiring daring to advance new interpretations of partial data.”&lt;/p&gt;
    &lt;p&gt;After the publication of “Hat,” when Sacks was fifty-two years old, he wrote his friend Robert Rodman, a psychoanalyst, that “Shengold suggested, with some hesitancy, some months ago, that I should consider going deeper with him.” He added, “He also observes that I don’t complain, say, of sexual deprivation—though this is absolute.” At first, Sacks was worried that Shengold was preparing to dismiss him from treatment: “I’ve done all I can for you—now manage on your own!” Then he felt hopeful that he didn’t need to assume that “boredom-depression-loneliness-cutoffness” would define the rest of his life. He was also moved that, after twenty years, Shengold still considered him “worth extra work.”&lt;/p&gt;
    &lt;p&gt;But Sacks was shaken by the idea that they’d only been skimming the surface. He looked back through his notebooks and noticed “a perceptible decline in concern and passion,” which he felt had also dulled the quality of his thought. “Is the superficiality of my work, then, due to superficiality of relationships—to running away from whatever has deeper feeling and meaning?” he asked Rodman. “Is this perhaps spoken of, in a camouflaged way, when I describe the ‘superficialization’ of various patients?” As an example, he referenced an essay in “Hat” about a woman with a cerebral tumor. She was intelligent and amusing but seemed not to care about anyone. “Was this the ‘cover’ of some unbearable emotion?” he writes in the essay.&lt;/p&gt;
    &lt;p&gt;Sacks felt that Shengold was the reason he was still alive, and that he should go further with him. “What have I to lose?” he asked Rodman. But, he wrote, “what one has to lose, of course, may be just that quasi-stable if fragile ‘functioning’ . . . so there is reason to hesitate.” Going deeper would also mean more fully submitting to someone else’s interpretation, experiencing what he asked of his own patients; Rodman proposed that Sacks was “afraid of the enclosure of analysis, of being reduced and fixed with a formulated phrase.”&lt;/p&gt;
    &lt;p&gt;In the early eighties, Lawrence Weschler, then a writer for The New Yorker, began working on a biography of Sacks. Weschler came to feel that Sacks’s homosexuality was integral to his work, but Sacks didn’t want his sexuality mentioned at all, and eventually asked him to stop the project. “I have lived a life wrapped in concealment and wracked by inhibition, and I can’t see that changing now,” he told Weschler. In his journal, Sacks jotted down thoughts to share with Weschler on the subject: “My ‘sex life’ (or lack of it) is, in a sense irrelevant to the . . . sweep of my mind.” In another entry, he wrote that the Freudian term “sublimation” diminished the process he’d undergone. When he was still having sex, as a young man in California, he used to sheath his body in leather gear, so he was “totally encased, enclosed,” his real self sealed in a kind of “black box.” He wrote, “I have, in a sense, ‘outgrown’ these extraordinary, almost convulsive compulsions—but this detachment has been made possible by incorporating them into a vast and comprehending view of the world.” (Weschler became close friends with Sacks, and, after Sacks died, published a “biographical memoir” titled “And How Are You, Dr. Sacks?”)&lt;/p&gt;
    &lt;p&gt;It’s unclear whether Sacks did “go deeper” with Shengold. In the late eighties, Sacks wrote in his journal that he was “scared, horrified (but, in an awful way, accepting or complaisant) about my non-life.” He likened himself to a “pithed and gutted creature.” Rather than living, he was managing a kind of “homeostasis.”&lt;/p&gt;
    &lt;p&gt;In 1987, Sacks had an intense friendship with a psychiatrist named Jonathan Mueller, with whom he briefly fell in love. Mueller, who was married to a woman, told me that he did not realize Sacks had romantic feelings for him. Sacks eventually moved on. But he felt that the experience had altered him. “I can read ‘love stories’ with empathy and understanding—I can ‘enter into them’ in a way which was impossible before,” he wrote in his journal. He perceived, in a new light, what it meant for his patients in “Awakenings” to glimpse the possibility of “liberation”: like him, he wrote, they were seeking “not merely a cure but an indemnification for the loss of their lives.”&lt;/p&gt;
    &lt;p&gt;By the nineties, Sacks seemed to ask less of himself, emotionally, in relation to his patients. He had started working with Kate Edgar, who’d begun as his assistant but eventually edited his writing, organized his daily life, and became a close friend. (Shengold had encouraged Sacks to find someone to assist with his work. “The secretary is certainly an important ‘ego-auxiliary,’ ” he wrote him in a letter.) Edgar was wary about the way Sacks quoted his patients—they were suspiciously literary, she thought—and she checked to make sure he wasn’t getting carried away. She spent hours with some of his patients, and, she told me, “I never caught him in anything like that, which actually surprises me.”&lt;/p&gt;
    &lt;p&gt;Weschler told me that Sacks used to express anxiety about whether he’d distorted the truth. Weschler would assure him that good writing is not a strict account of reality; there has to be space for the writer’s imagination. He said he told Sacks, “Come on, you’re extravagantly romanticizing how bad you are—just as much as you were extravagantly romanticizing what the patient said. Your mother’s accusing voice has taken over.” Weschler had gone to Beth Abraham Hospital to meet some of the patients from “Awakenings” and had been shaken by their condition. “There’s a lot of people shitting in their pants, drooling—the sedimentation of thirty years living in a warehouse,” he said. “His genius was to see past that, to the dignity of the person. He would talk to them for an hour, and maybe their eyes would brighten only once—the rest of the time their eyes were cloudy—but he would glom onto that and keep talking.”&lt;/p&gt;
    &lt;p&gt;After “Hat,” Sacks’s relationship with his subjects became more mediated. Most of them were not his patients; many wrote to him after reading his work, recognizing themselves in his books. There was a different power dynamic, because these people already believed that they had stories to tell. Perhaps the guilt over liberties he had taken in “Hat” caused him to curb the impulse to exaggerate. His expressions of remorse over “making up, ‘enhancing,’ etc,” which had appeared in his journals throughout the seventies and eighties, stopped. In his case studies, he used fewer and shorter quotes. His patients were far more likely to say ordinary, banal things, and they rarely quoted literature. They still had secret gifts, but they weren’t redeemed by them; they were just trying to cope.&lt;/p&gt;
    &lt;p&gt;In “An Anthropologist on Mars,” from 1992, a book of case studies about people compensating for, and adapting to, neurological conditions, some of the richest passages are the ones in which Sacks allows his incomprehension to become part of the portrait. In a chapter called “Prodigies,” he wants badly to connect with a thirteen-year-old boy named Stephen, who is autistic and has an extraordinary ability to draw, but Stephen resists Sacks’s attempts at intimacy. He will not allow himself to be romanticized, a refusal that Sacks ultimately accepts: “Is Stephen, or his autism, changed by his art? Here, I think, the answer is no.” In this new mode, Sacks is less inclined to replace Stephen’s unknowable experience with his own fantasy of it. He is open about the discomfort, and even embarrassment, of his multiple failures to reach him: “I had hoped, perhaps sentimentally, for some depth of feeling from him; my heart had leapt at the first ‘Hullo, Oliver!’ but there had been no follow-up.”&lt;/p&gt;
    &lt;p&gt;Mort Doran, a surgeon with Tourette’s syndrome whom Sacks profiled in “Anthropologist,” told me that he was happy with the way Sacks had rendered his life. He said that only one detail was inaccurate—Sacks had written that the brick wall of Doran’s kitchen was marked from Doran hitting it during Tourette’s episodes. “I thought, Why would he embellish that? And then I thought, Maybe that’s just what writers do.” Doran never mentioned the error to Sacks. He was grateful that Sacks “had the gravitas to put it out there to the rest of the world and say, ‘These people aren’t all nuts or deluded. They’re real people.’ ”&lt;/p&gt;
    &lt;p&gt;The wife in the title story of “Hat” had privately disagreed with Sacks about the portrayal of her husband, but for the most part Sacks appeared to have had remarkable relationships with his patients, corresponding with them for years. A patient called Ray, the subject of a 1981 piece about Tourette’s syndrome, told me that Sacks came to his son’s wedding years after his formal treatment had ended. Recalling Sacks’s death, he found himself suddenly crying. “Part of me left,” he said. “Part of my self was gone.”&lt;/p&gt;
    &lt;p&gt;A year after “Awakenings” was published, Sacks broke his leg in Norway, and Leonard L. and his mother wrote him a get-well letter. Thirty-two patients added their names, their signatures wavering. “Everybody had been counting the days for your return, so you can imagine the turmoil when they heard the news,” Leonard’s mother wrote. She explained that “most of the patients are not doing so well without your help and interest.” She added that Leonard “isn’t doing too well either.” When Leonard learned that Sacks wouldn’t be back, she said, “he shed enough tears to fill a bucket.”&lt;/p&gt;
    &lt;p&gt;Sacks spoke of “animating” his patients, as if lending them some of his narrative energy. After living in the forgotten wards of hospitals, in a kind of narrative void, perhaps his patients felt that some inaccuracies were part of the exchange. Or maybe they thought, That’s just what writers do. Sacks established empathy as a quality every good doctor should possess, enshrining the ideal through his stories. But his case studies, and the genre they helped inspire, were never clear about what they exposed: the ease with which empathy can slide into something too creative, or invasive, or possessive. Therapists—and writers—inevitably see their subjects through the lens of their own lives, in ways that can be both generative and misleading.&lt;/p&gt;
    &lt;p&gt;In his journal, reflecting on his work with Tourette’s patients, Sacks described his desire to help their illness “reach fruition,” so that they would become floridly symptomatic. “With my help and almost my collusion, they can extract the maximum possible from their sickness—maximum of knowledge, insight, courage,” he wrote. “Thus I will FIRST help them to get ill, to experience their illness with maximum intensity; and then, only then, will I help them get well!” On the next line, he wrote, “IS THIS MONSTROUS?” The practice came from a sense of awe, not opportunism, but he recognized that it made him complicit, as if their illness had become a collaboration. “An impulse both neurotic and intellectual (artistic) makes me get the most out of suffering,” he wrote. His approach set the template for a branch of writing and thinking that made it seem as if the natural arc of illness involved insight and revelation, and even some poetry, too.&lt;/p&gt;
    &lt;p&gt;In his journals, Sacks repeatedly complained that his life story was over. He had the “feeling that I have stopped doing, that doing has stopped, that life itself has stopped, that it is petering out in a sort of twilight of half-being,” he wrote, in 1987. His journals convey a sense of tangible boredom. He transcribed long passages from philosophers and theologists (Simone Weil, Søren Kierkegaard, Gottfried Wilhelm Leibniz, Dietrich Bonhoeffer) and embarked on disquisitions on the best definition of reality, the “metabolism of grace,” the “deep mystery of incubation.” His thoughts cast outward in many directions—notes for a thousand lectures—then tunnelled inward to the point of non-meaning. “Where Life is Free, Immaterial, full of Art,” he wrote, “the laws of life, of Grace, are those of Fitness.”&lt;/p&gt;
    &lt;p&gt;Sacks proposed various theories for why he had undergone what he called “psychic death.” He wondered if he had become too popular, merely a fuzzy symbol of compassionate care. “Good old Sacks—the House Humanist,” he wrote, mocking himself. He also considered the idea that his four decades of analysis were to blame. Was it possible, he wrote, that a “vivisection of inner life, however conceived, however subtle and delicate, may in fact destroy the very thing it examines?” His treatment with Shengold seemed to align with a life of “homeostasis”—intimacy managed through more and more language, in a contained, sterile setting, on Monday and Wednesday mornings, from 6:00 to 6:45 A.M. They still referred to each other as “Dr. Sacks” and “Dr. Shengold.” Once, they ran into each other at a chamber concert. They were a few rows apart, but they didn’t interact. Occasionally, Shengold told his children that he “heard from the couch” about a good movie or play, but he never shared what happened in his sessions. They inferred that Sacks was their father’s patient after reading the dedication to him in “Hat.”&lt;/p&gt;
    &lt;p&gt;As Sacks aged, he felt as if he were gazing at people from the outside. But he also noticed a new kind of affection for humans—“homo sap.” “They’re quite complex (little) creatures (I say to myself),” he wrote in his journal. “They suffer, authentically, a good deal. Gifted, too. Brave, resourceful, challenging.”&lt;/p&gt;
    &lt;p&gt;Perhaps because love no longer appeared to be a realistic risk—he had now entered a “geriatric situation”—Sacks could finally confess that he craved it. “I keep being stabbed by love,” he wrote in his journal. “A look. A glance. An expression. A posture.” He guessed that he had at least five, possibly ten, more years to live. “I want to, I want to ••• I dare not say. At least not in writing.”&lt;/p&gt;
    &lt;p&gt;In 2008, Sacks had lunch with Bill Hayes, a forty-seven-year-old writer from San Francisco who was visiting New York. Hayes had never considered Sacks’s sexuality, but, as soon as they began talking, he thought, “Oh, my God, he’s gay,” he told me. They lingered at the table for much of the afternoon, connecting over their insomnia, among other subjects. After the meal, Sacks wrote Hayes a letter (which he never sent) explaining that relationships had been “a ‘forbidden’ area for me—although I am entirely sympathetic to &lt;del&gt;(indeed wistful and perhaps envious about)&lt;/del&gt; other people’s relationships.”&lt;/p&gt;
    &lt;p&gt;A year later, Hayes, whose partner of seventeen years had died of a heart attack, moved to New York. He and Sacks began spending time together. At Sacks’s recommendation, Hayes started keeping a journal, too. He often wrote down his exchanges with Sacks, some of which he later published in a memoir, “Insomniac City.”&lt;/p&gt;
    &lt;p&gt;“It’s really a question of mutuality, isn’t it?” Sacks asked him, two weeks after they had declared their feelings for each other.&lt;/p&gt;
    &lt;p&gt;“Love?” Hayes responded. “Are you talking about love?”&lt;/p&gt;
    &lt;p&gt;“Yes,” Sacks replied.&lt;/p&gt;
    &lt;p&gt;Sacks began taking Hayes to dinner parties, although he introduced him as “my friend Billy.” He did not allow physical affection in public. “Sometimes this issue of not being out became very difficult,” Hayes told me. “We’d have arguments, and I’d say things like ‘Do you and Shengold ever talk about why you can’t come out? Or is all you ever talk about your dreams?’ ” Sacks wrote down stray phrases from his dreams on a whiteboard in his kitchen so that he could report on them at his sessions, but he didn’t share what happened in therapy.&lt;/p&gt;
    &lt;p&gt;Kate Edgar, who worked for Sacks for three decades, had two brothers who were gay, and for years she had advocated for gay civil rights, organizing Pride marches for her son’s school. She intentionally found an office for Sacks in the West Village so that he would be surrounded by gay men living openly and could see how normal it had become. She tended to hire gay assistants for him, for the same reason. “So I was sort of plotting on that level for some years,” she told me.&lt;/p&gt;
    &lt;p&gt;In 2013, after being in a relationship with Hayes for four years—they lived in separate apartments in the same building—Sacks began writing a memoir, “On the Move,” in which he divulged his sexuality for the first time. He recounts his mother’s curses upon learning that he was gay, and his decades of celibacy—a fact he mentions casually, without explanation. Edgar wondered why, after so many years of analysis, coming out took him so long, but, she said, “Oliver did not regard his relationship with Shengold as a failure of therapy.” She said that she’d guessed Shengold had thought, “This is something Oliver has to do in his own way, on his own time.” Shengold’s daughter, Nina, said that, “for my dad to have a patient he loved and respected finally find comfort in identifying who he’d been all his life—that’s growth for both of them.”&lt;/p&gt;
    &lt;p&gt;A few weeks after finishing the manuscript, Sacks, who’d had melanoma of the eye in 2005, learned that the cancer had come back, spreading to his liver, and that he had only months to live. He had tended toward hypochondria all his life, and Edgar thought that the diagnosis might induce a state of chronic panic. Since he was a child, Sacks had had a horror of losing things, even irrelevant objects. He would be overcome by the “feeling that there was a hole in the world,” he wrote in his journal, and the fear that “I might somehow fall through that hole-in-the-world, and be absolutely, inconceivably lost.” Edgar had dealt for decades with his distress over lost objects, but she noticed that now, when he misplaced things, he didn’t get upset. He had an uncharacteristic ease of being.&lt;/p&gt;
    &lt;p&gt;In the summer of 2015, before Shengold went on his annual summer break, Sacks said to Edgar, “If I’m alive in September when Shengold returns, I’m not sure I need to go back to my sessions.” They had been seeing each other for forty-nine years. Sacks was eighty-two; Shengold was eighty-nine.&lt;/p&gt;
    &lt;p&gt;When Sacks was struggling with his third book, “A Leg to Stand On,” which was about breaking his leg and his frustration that his doctors wouldn’t listen to him, he wrote in his journal that Shengold had suggested (while apologizing for the corniness of the phrase) that the book should be “a message of love”—a form of protest against the indifference that so many patients find in their doctors. Shengold may have been giving Sacks permission to see their own relationship—the one place in which Sacks felt an enduring sense of recognition and care—as a hidden subject of the book. Extending Shengold’s idea, Sacks wrote, of his book, “The ‘moral’ center has to do with . . . the irreducible ultimate in doctor-patient relations.”&lt;/p&gt;
    &lt;p&gt;In August, two weeks before Sacks died, he and Shengold spoke on the phone. Shengold was with his family at a cottage in the Finger Lakes region of central New York, where he spent every summer. Nina told me, “We all gathered in the living room of that little cottage and put my father on speakerphone. Oliver Sacks was clearly on his deathbed—he was not able to articulate very well. Sometimes his diction was just gone. Dad kept shaking his head. He said, ‘I can’t understand you. I’m so sorry, I can’t understand you.’ ” At the end of the call, Shengold told Sacks, “It’s been the honor of my life to work with you,” and said, “Goodbye, Oliver.” Sacks responded, “Goodbye, Leonard.” It was the first time they had ever used each other’s first names. When they hung up, Shengold was crying.&lt;/p&gt;
    &lt;p&gt;After Sacks died, Shengold started closing down his practice. “It was the beginning of the end for him,” his son David told me. “He had lost most of his colleagues. He was really the last of his generation.” Nina said, “I do think part of why my father lived so long and was able to work so long was because of that relationship. That feeling of affection and kindred spirit was lifesaving.”&lt;/p&gt;
    &lt;p&gt;In “Awakenings,” when describing how Leonard L.—his “ ‘ideal’ patient”—initially responded to L-dopa, Sacks characterizes him as “a man released from entombment” whose “predominant feelings at this time were feelings of freedom, openness, and exchange with the world.” He quotes Leonard saying, “I have been hungry and yearning all my life . . . and now I am full.” He also says, “I feel saved. . . . I feel like a man in love. I have broken through the barriers which cut me off from love.’ ”&lt;/p&gt;
    &lt;p&gt;For years, Sacks had tested the possibility of awakenings in others, as if rehearsing, or outsourcing, the cure he had longed to achieve with Shengold. But at the end of his life, like an inside-out case study, he inhabited the story he’d imagined for his patients. “All of us entertain the idea of another sort of medicine . . . which will restore us to our lost health and wholeness,” he wrote, in “Awakenings.” “We spend our lives searching for what we have lost; and one day, perhaps, we will suddenly find it.” ♦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.newyorker.com/magazine/2025/12/15/oliver-sacks-put-himself-into-his-case-studies-what-was-the-cost"/><published>2025-12-18T20:52:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46318676</id><title>Delty (YC X25) Is Hiring an ML Engineer</title><updated>2025-12-19T07:39:17.566003+00:00</updated><content>&lt;doc fingerprint="b95c3d540015cfe8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;About Us&lt;/head&gt;
      &lt;p&gt;Delty is building the healthcare’s AI operating system. We create voice-based and computer-based assistants that streamline clinical workflows, reduce administrative burden, and help providers focus on patient care. Our system learns from real healthcare environments to deliver reliable, context-aware support that improves efficiency and elevates the provider experience.&lt;/p&gt;
      &lt;p&gt;Delty was founded by former engineering leaders from Google, including co-founders with deep experience at YouTube and in large-scale infrastructure. You’ll get to work alongside people who built massive systems at scale — a chance to learn a lot and contribute meaningfully from day one.&lt;/p&gt;
      &lt;p&gt;We believe in solving hard problems together as a team, iterating quickly, and building software with long-term thinking and ownership.&lt;/p&gt;
      &lt;head rend="h3"&gt;What You’ll Do&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Build and own production machine learning systems end-to-end: from data modeling and feature engineering to training, evaluation, deployment, and monitoring.&lt;/item&gt;
        &lt;item&gt;Design and implement data pipelines that turn raw, messy real-world healthcare data into reliable features for machine learning models.&lt;/item&gt;
        &lt;item&gt;Train and evaluate models for ranking, prioritization, and prediction problems (for example, identifying high-risk or high-priority cases).&lt;/item&gt;
        &lt;item&gt;Deploy models into production as reliable services or batch jobs, with clear versioning, monitoring, and rollback strategies.&lt;/item&gt;
        &lt;item&gt;Work closely with backend engineers and product leaders to integrate machine learning into real workflows and decision-making systems.&lt;/item&gt;
        &lt;item&gt;Make architectural decisions around model choice, evaluation metrics, retraining cadence, and system guardrails — balancing accuracy, explainability, reliability, and operational constraints.&lt;/item&gt;
        &lt;item&gt;Collaborate directly with founders and engineers to translate product and operational needs into scalable, maintainable machine learning solutions.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What We’re Looking For&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;At least 3 years of experience building and deploying machine learning systems in production.&lt;/item&gt;
        &lt;item&gt;Strong foundation in machine learning for structured (tabular) data, including feature engineering, regression or classification models, and ranking or prioritization problems.&lt;/item&gt;
        &lt;item&gt;Experience with the full machine learning lifecycle: data preparation, train/test splitting, evaluation, deployment, retraining, and monitoring.&lt;/item&gt;
        &lt;item&gt;Solid backend engineering skills: writing production-quality code, building services or batch jobs, and working with databases and data pipelines.&lt;/item&gt;
        &lt;item&gt;Good system design instincts: you understand trade-offs between model complexity, reliability, latency, scalability, and maintainability.&lt;/item&gt;
        &lt;item&gt;Comfort working in a fast-paced startup environment with high ownership and ambiguity.&lt;/item&gt;
        &lt;item&gt;Ability to clearly explain modeling choices, assumptions, and limitations to non-machine-learning stakeholders.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Bonus:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Experience working with healthcare or operational decision-support systems.&lt;/item&gt;
        &lt;item&gt;Experience building or integrating LLM systems in production, such as retrieval-augmented generation, fine-tuning, or structured prompting workflows.&lt;/item&gt;
        &lt;item&gt;Prior startup experience or founder mindset — we value ownership, pragmatism, and bias toward shipping.&lt;/item&gt;
        &lt;item&gt;Experience with model monitoring, data drift detection, or ML infrastructure tooling.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Why join&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Learn from seasoned Google engineers: As former Google engineers who built systems at YouTube and Google Pay, we’ve operated at massive scale. Working alongside us gives you a chance to build similar systems and learn best practices, scale thinking, and software design deeply.&lt;/item&gt;
        &lt;item&gt;High impact: At a small but ambitious team, your contributions will influence architecture, product direction, and core features. You will have real ownership and see the effects of your work quickly.&lt;/item&gt;
        &lt;item&gt;Grow fast: We’re iterating rapidly; you’ll be exposed to the full stack, AI/ML pipelines, system architecture, data modeling, and product-level decisions — a fast-track to becoming a senior engineer or technical lead.&lt;/item&gt;
        &lt;item&gt;Challenging and meaningful work: We’re tackling the hardest part of software engineering: bridging AI-generated prototypes and robust, scalable enterprise-grade systems. If you enjoy thinking deeply about systems and building reliable, maintainable foundations — this is for you.&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/delty/jobs/MDeC49o-machine-learning-engineer"/><published>2025-12-18T21:02:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46318852</id><title>Two kinds of vibe coding</title><updated>2025-12-19T07:39:17.328496+00:00</updated><content>&lt;doc fingerprint="e35d84ff2b1d0fff"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;December 16, 2025&lt;/head&gt;&lt;head&gt;Vibe Coding&lt;/head&gt;&lt;p&gt;I have been teaching myself to vibe code.&lt;/p&gt;&lt;p&gt;Back in 2009 I posted a simple Mandelbrot fractal viewer on the web: a single HTML file with inline Javascript. Just 329 lines of code, each pixel a tiny table cell. Click to zoom. Watch it iterate. That was about it!&lt;/p&gt;&lt;p&gt;I have wondered if improving the page could raise it in the Google rankings, so I have been using code LMs to make a number of improvements....&lt;/p&gt;&lt;head&gt;Two Kinds of Vibe Coding&lt;/head&gt;&lt;p&gt;There are two categories of vibe coding. One is when you delegate little tasks to a coding LM while keeping yourself as the human "real programmer" fully informed and in control.&lt;/p&gt;&lt;p&gt;The second type of vibe coding is what I am interested in. It is when you use a coding agent to build towers of complexity that go beyond what you have time to understand in any detail. I am interested in what it means to cede cognitive control to an AI. My friend David Maymudes has been building some serious software that way, and he compares the second type of vibe coding to managing a big software team. Like the stories you've heard of whole startups being created completely out of one person vibe coding.&lt;/p&gt;&lt;p&gt;When my students describe their own vibe coding, it sounds like the first kind. That is also how I started with my Mandelbrot project, pasting little algorithm nuggets into my code while I edited each function, making all the key decisions using my own human judgement.&lt;/p&gt;&lt;p&gt;But in the last couple weeks I have put on the blinders. I have resolved to stop looking at all the code in detail anymore. Instead, I am pushing the agent to write a ton of code, make its own decisions, to hell with the complexity. I have been experimenting with the second kind of vibe.&lt;/p&gt;&lt;p&gt;It is working surprisingly well, and I have been thinking about my experience handing the reins to an AI agent. The workflow may presage the use of generative AI across other industries. And I have arrived at two basic rules for vibe coders.&lt;/p&gt;&lt;head&gt;Unleashing Claude on my Webpage&lt;/head&gt;&lt;p&gt;The last human-written version of the webpage without LLM assistance was 780 lines; you can see its 37 functions diagrammed below. It is a nice elegant piece of code, but pretty simplistic as a fractal implementation.&lt;/p&gt;&lt;p&gt;A key litmus test for a fractal viewer is how deep and fast it goes. By these measures, my human-written program was amateurish. Here is a picture of the output of the 780-line version at 0.40616753&lt;/p&gt;&lt;p&gt;Compare to how the LLM-assisted version renders the following image, after just one minute of work, at the same location and zoom level:&lt;/p&gt;&lt;p&gt;The LLM version is much faster because it uses the GPU (if your web browser allows it). But it plays many more tricks than just moving the calculation from CPU to GPU, because although the GPU is hundreds of times faster than a CPU, its 7-digit fp32 is also millions of times coarser than the CPU's 15-digit fp64. So the LLM-generated program deals with this by implementing perturbation algorithms to split the work between CPU and GPU, calculating numbers as (z+d·2s) where z is a sparse high-resolution vector on the (slow but precise) CPU and d and s are dense near-zero low-resolution vectors on the (fast but imprecise) GPU.&lt;/p&gt;&lt;p&gt;There are multiple ways to implement perturbation algorithms, and so the LLM code implements and benchmarks nine alternative approaches, selecting different algorithms at different zoom levels and compute availability to follow the Pareto frontier of time/resolution tradeoff. Backing the algorithms it has written quad-double precision arithmetic accurate to 60+ digits, an adaptive float32+logscale numeric complex representation, GPU buffer management, and a task scheduler that can serialize and migrate long-running CPU tasks between WebWorker threads. It has also added many other UI details I asked for, like a minimal MP4 encoder for recoding smooth movies and a cache to reduce recalculation when using the browser's forward/back history. The little webpage includes implementations of Horner's algorithm for stable polynomials, Fibonacci series for aperiodic periodicity checks, Catmull-Rom splines for smooth animations, continued fractions for pretty ratios, spatial hashing for finding nearby points, an algorithm for rebasing iterated perturbations that it found in a 2021 forum post, and a novel algorithm for fast orbit detection it developed based on my suggestion. All with detailed documentation and a search-engine-optimized internationalized user interface explained in the most commonly-read eleven languages on the Internet. That last part, with all the translations to Chinese and Arabic, took Claude just a few minutes while I was cooking breakfast.&lt;/p&gt;&lt;p&gt;The cost of this performance? A large increase in complexity. Empowered to make direct changes in the project, Claude Code has now made several hundred commits, expanding the tiny one-page HTML file to more than 13,600 lines of code, defining 30 classes, 2 mixins, 342 methods, and 159 functions.&lt;/p&gt;&lt;p&gt;That brings me to the rules for getting an LLM agent to work effectively: David's two rules for vibe coding. They are simple rules.&lt;/p&gt;&lt;head&gt;Rule 1: Automate tests&lt;/head&gt;&lt;p&gt;If you just ask the agent to solve a problem, it will run around for a few minutes and come back with a rough solution. Then you test it, find it doesn't work, tell it so, and it runs around again for another five minutes. Repeat.&lt;/p&gt;&lt;p&gt;This workflow turns you into the manual tester. Maybe the least interesting job on the planet. Not a good use of precious human brain cells.&lt;/p&gt;&lt;p&gt;But if you get the agent to write a good automated test first, something changes. After it runs around for five minutes, it remembers to check its own work. It sees how it got things wrong. It goes back and tries again. Now it can extend its horizon to 30 minutes of autonomous work. By the time it comes to bother you, the result is much more promising.&lt;/p&gt;&lt;head&gt;Rule 2: Test the tests&lt;/head&gt;&lt;p&gt;But after a while, you realize the 30-minute interrupts are only a bit better than the 5-minute ones. The agent is good at finding holes in your tests. It produces stupid solutions that don't do what you want but still pass, because the tests were not actually good enough.&lt;/p&gt;&lt;p&gt;So: test the tests.&lt;/p&gt;&lt;p&gt;Testing tests is the kind of thankless metaprogramming that a development manager spends all their time doing, to make their team productive. For example: fuzz testing to discover new problems that need systematic tests. Code coverage to reveal what code exists but remains untested. Frameworks to make code more testable, for enabling benchmarking, for enabling troubleshooting. Hypothesis-driven testing to force the agent to form a theory about what might be wrong, then write tests that chase it down. This type of programming is the sort of painful chore that can unlock productivity in a software development team. And it works very well when vibe coding also.&lt;/p&gt;&lt;p&gt;It is interesting that it can be hard to get a coding agent to understand why it is spending so much effort testing tests. For example, when getting Claude Code to construct a reliable code coverage framework, I gave it the mission of debugging why its initial attempt had produced the unbelievable (and untrue) assertion that 100% of lines had been covered by tests. Claude understood what it was trying to do at first, but when the going got tough, it kept giving up, saying "we don't need to do anything here; I just noticed, code coverage is already 100%!" Maybe testing its tests of the tests is too meta, just at the edge of its ability to follow.&lt;/p&gt;&lt;p&gt;But once you can get the metaprogramming right, and do it well, you can reach a kind of vibe coding nirvana. Because then, as a human, you can look at code again! Instead of facing thousands of jumbled lines vomited up by the agent, now you've got maps of the code, informed by code coverage, benchmarks, test harnesses, and other instrumentation. It lets you skip thinking about the 99% of routine code and focus your attention on the 1% most interesting code. The weird cases, the edge cases, the stuff that might deserve to be changed. That is a good use of precious human brain cells.&lt;/p&gt;&lt;p&gt;One limitation of this vibe approach is that tests catch bugs but not bloat. After developing comprehensive testing, I did find it helpful to make one human pass over the code to find opportunities for making code more symmetric (to make code near-duplication more obvious), and to remove some confusing code that was leading the agent astray. That opened the way for larger-scale vibe-coded refactoring that improved the elegance of the most intricate part of the code.&lt;/p&gt;&lt;p&gt;The two rules are not just coding hacks. They also reveal a path for keeping humans informed enough to remain in control.&lt;/p&gt;&lt;head&gt;Trucks and Pedestrians&lt;/head&gt;&lt;p&gt;My experience vibe coding reminds me of the difference between walking and driving a truck. Highway driving is a new skill, but with a truck you can haul a lot more stuff faster than you could dream of on foot. Vibe working with AI gets you out of the business of actual intellectual hauling. Instead it gets you into the business of taking care of the machine.&lt;/p&gt;&lt;p&gt;Working effectively with AI is much more abstract than traditional office work, because it demands that we build up meta-cognitive infrastructure, like the 422 automated tests and code coverage tools that I needed to effectively steer the development of my single webpage.&lt;/p&gt;&lt;p&gt;As we reshape the global economy around AI, it reminds me of the construction of the American interstate highway system. The speeding and scaling of cognition seems likely to lead to economy-wide boosts in "intellectual mobility," and a whole new culture with the equivalent of roadside service stations and even suburban flight. But it also strikes me that we do not want to live in a world where all decisions are made by large-scale AI, no more than we would want to live in a world where everyone gets everywhere in a truck. Our modern streets are too congested with dangerous vehicles, and I am not sure it is giving us the best life.&lt;/p&gt;&lt;p&gt;I like walking to work.&lt;/p&gt;&lt;p&gt;As AI edges humans out of the business of thinking, I think we need to be wary of losing something essential about being human. By making our world more complex—twenty times more lines of code!—we risk losing touch with our ability to understand the world in ways that dull our ability to make good decisions, that prevent us from even understanding what it is that we want in the world. I hope we can build metacognitive infrastructure that keeps our human minds informed. But as we build increasingly powerful abstractions, it will be both crucial and difficult to keep asking: Do we want this?&lt;/p&gt;&lt;p&gt;If you would like a sense of the structure and volume of code produced by vibe coding, you can scroll through the vibe-coded visualization of the evolution of the code through git commits. Or compare the code before (pre-LLM code repo here) and after (current code repo). In particular, read the agent's documentation of the development infrastructure it built for this little one-webpage project. That kind of tooling will be familiar to anybody who has worked on a large engineering team. And it is the kind of work needed to support human comprehension of complexity in the age of LLM agents.&lt;/p&gt;Posted by David at December 16, 2025 11:15 AM&lt;p&gt;I dislike the term "vibe coding". It means nothing and it's vague. I think we should start using:&lt;/p&gt;&lt;p&gt;LLM-assisted&lt;/p&gt;&lt;p&gt;You are advocating the benefits of LLM-created software. It's not that much different than having someone on the "business side" working with a team of programmers (under ideal circumstances).&lt;/p&gt;&lt;p&gt;It is the great equalizer. You are no longer a programmer - you are an author, an engineer, an architect.&lt;/p&gt;&lt;p&gt;Lots of angry voices re: LLM-created software but those same voices are working for large corps doing grunt work. They aren't creating the software *you* need when *you* need it.&lt;/p&gt;&lt;p&gt;All hail both LLM-assisted and LLM-created programs.&lt;/p&gt;&lt;p&gt;ps: what do you thing about LLM-assisted (or LLM-created) writing, video, music, etc...&lt;/p&gt;Posted by: John at December 18, 2025 05:48 PM&lt;p&gt;Management by abdication is a dangerous path.&lt;/p&gt;&lt;p&gt;While this concerns a fractal viewer, what happens if this code controls a pacemaker, a power plant or a car?&lt;/p&gt;&lt;p&gt;The first vibe-coded code is probably already in critical systems.&lt;/p&gt;&lt;p&gt;I'm not saying vibe code is always bad but for large codebases it quickly has the potential of making codebases unmaintainable by humans.&lt;/p&gt;&lt;p&gt;You also lose authorship of the code. You didn't write it. The agent did. And the agent is property of a big coorporation.&lt;/p&gt;&lt;p&gt;What happens when people start dying from software bugs so incomprehensible that we can only beg the llm to really really fix it please...&lt;/p&gt;Posted by: Groggy at December 18, 2025 07:54 PM&lt;p&gt;The vibe coded fractal viewer is nothing but a black screen on my PC. Updated with good hardware and attempted on several different browsers.&lt;/p&gt;Posted by: Gabriel at December 19, 2025 12:14 AM&lt;p&gt;Post a comment&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Copyright 2025 © David Bau. All Rights Reserved.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://davidbau.com/archives/2025/12/16/vibe_coding.html"/><published>2025-12-18T21:16:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319657</id><title>1.5 TB of VRAM on Mac Studio – RDMA over Thunderbolt 5</title><updated>2025-12-19T07:39:17.145697+00:00</updated><content>&lt;doc fingerprint="eff0d13f3fa601fc"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple gave me access to this Mac Studio cluster to test RDMA over Thunderbolt, a new feature in macOS 26.2. The easiest way to test it is with Exo 1.0, an open source private AI clustering tool. RDMA lets the Macs all act like they have one giant pool of RAM, which speeds up things like massive AI models.&lt;/p&gt;
    &lt;p&gt;The stack of Macs I tested, with 1.5 TB of unified memory, costs just shy of $40,000, and if you're wondering, no I cannot justify spending that much money for this. Apple loaned the Mac Studios for testing. I also have to thank DeskPi for sending over the 4-post mini rack containing the cluster.&lt;/p&gt;
    &lt;p&gt;The last time I remember hearing anything interesting about Apple and HPC (High Performance Computing), was back in the early 2000s, when they still made the Xserve.&lt;/p&gt;
    &lt;p&gt;They had a proprietary clustering solution called Xgrid... that landed with a thud. A few universities built some clusters, but it never really caught on, and now Xserve is a distant memory.&lt;/p&gt;
    &lt;p&gt;I'm not sure if its by accident or Apple's playing the long game, but the M3 Ultra Mac Studio hit a sweet spot for running local AI models. And with RDMA support lowering memory access latency from 300μs down to &amp;lt; 50μs, clustering now adds to the performance, especially running huge models.&lt;/p&gt;
    &lt;p&gt;They also hold their own for creative apps and at least small-scale scientific computing, all while running under 250 watts and almost whisper-quiet.&lt;/p&gt;
    &lt;p&gt;The two Macs on the bottom have 512 GB of unified memory and 32 CPU cores, and cost $11,699 each. The two on top, with half the RAM, are $8,099 each1.&lt;/p&gt;
    &lt;p&gt;They're not cheap.&lt;/p&gt;
    &lt;p&gt;But with Nvidia releasing their DGX Spark and AMD with their AI Max+ 395 systems, both of which have a fourth the memory (128 GB maximum), I thought I'd put this cluster through its paces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;This blog post is the reformatted text version of my latest YouTube video, which you can watch below.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Mini Mac Rack&lt;/head&gt;
    &lt;p&gt;In a stroke of perfect timing, DeskPi sent over a new 4-post mini rack called the TL1 the day before these Macs showed up.&lt;/p&gt;
    &lt;p&gt;I kicked off Project MINI RACK earlier this year, but the idea is you can have the benefits of rackmount gear, but in a form factor that'll fit on your desk, or tucked away in a corner.&lt;/p&gt;
    &lt;p&gt;Right now, I haven't seen any solutions for mounting Mac Studios in 10" racks besides this 3D printable enclosure, so I just put them on some 10" rack shelves.&lt;/p&gt;
    &lt;p&gt;The most annoying thing about racking any non-Pro Macs is the power button. On a Mac Studio it's located in the back left, on a rounded surface, which means rackmount solutions need to have a way to get to it.&lt;/p&gt;
    &lt;p&gt;The open sides on the mini rack allow me to reach in and press the power button, but I still have to hold onto the Mac Studio while doing so, to prevent it from sliding out the front!&lt;/p&gt;
    &lt;p&gt;It is nice to have the front ports on the Studio to plug in a keyboard and monitor:&lt;/p&gt;
    &lt;p&gt;For power, I'm glad Apple uses an internal power supply. Too many 'small' PCs are small only because they punt the power supply into a giant brick outside the case. Not so, here, but you do have to deal with Apple's non-C13 power cables (which means it's harder to find cables in the perfect length to reduce cabling to be managed).&lt;/p&gt;
    &lt;p&gt;The DGX Spark does better than Apple on networking. They have these big rectangle QSFP ports (pictured above). The plugs hold in better, while still being easy to plug in and pull out.&lt;/p&gt;
    &lt;p&gt;The Mac Studios have 10 Gbps Ethernet, but the high speed networking (something like 50-60 Gbps real-world throughput) on the Macs comes courtesy of Thunderbolt. Even with premium Apple cables costing $70 each, I don't feel like the mess of plugs would hold up for long in many environments.&lt;/p&gt;
    &lt;p&gt;There's tech called ThunderLok-A, which adds a little screw to each cable to hold it in, but I wasn't about to drill out and tap the loaner Mac Studios, to see if I could make them work.&lt;/p&gt;
    &lt;p&gt;Also, AFAICT, Thunderbolt 5 switches don't exist, so you can't plug in multiple Macs to one central switch—you have to plug every Mac into every other Mac, which adds to the cabling mess. Right now, you can only cross-connect up to four Macs, but I think that may not be a hard limit for the current Mac Studio (Apple said all five TB5 ports are RDMA-enabled).&lt;/p&gt;
    &lt;p&gt;The bigger question is: do you need a full cluster of Mac Studios at all? Because just one is already a beast, matching four maxed-out DGX Sparks or AI Max+ 395 systems. Managing clusters can be painful.&lt;/p&gt;
    &lt;head rend="h2"&gt;M3 Ultra Mac Studio - Baseline&lt;/head&gt;
    &lt;p&gt;To inform that decision, I ran some baseline benchmarks, and posted all my results (much more than I highlight in this blog post) to my sbc-reviews project.&lt;/p&gt;
    &lt;p&gt;I'll compare the M3 Ultra Mac Studio to a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dell Pro Max with GB10 (similar to the Nvidia DGX Spark, but with better thermals)&lt;/item&gt;
      &lt;item&gt;Framework Desktop Mainboard (with AMD's AI Max+ 395 chip)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, Geekbench. The M3 Ultra, running two-generations-old CPU cores, beats the other two in both single and multi-core performance (and even more handily in Geekbench 5, which is more suitable for CPUs with many cores).&lt;/p&gt;
    &lt;p&gt;Switching over to a double-precision FP64 test, my classic top500 HPL benchmark, the M3 Ultra is the first small desktop I've tested that breaks 1 Tflop FP64. It's almost double Nvidia's GB10, and the AMD AI Max chip is left in the dust.&lt;/p&gt;
    &lt;p&gt;Efficiency on the CPU is also great, though that's been the story with Apple since the A-series, with all their chips. And related to that, idle power draw on here is less than 10 watts:&lt;/p&gt;
    &lt;p&gt;I mean, I've seen SBC's idle over 10 watts, much less something that could be considered a personal supercomputer.&lt;/p&gt;
    &lt;p&gt;Regarding AI Inference, the M3 Ultra stands out, both for small and large models:&lt;/p&gt;
    &lt;p&gt;Of course, the truly massive models (like DeepSeek R1 or Kimi K2 Thinking) won't even run on a single node of the other two systems.&lt;/p&gt;
    &lt;p&gt;But this is a $10,000 system. You expect more when you pay more.&lt;/p&gt;
    &lt;p&gt;But consider this: a single M3 Ultra Mac Studio has more horsepower than my entire Framework Desktop cluster, using half the power. I also compared it to a tiny 2-node cluster of Dell Pro Max with GB10 systems, and a single M3 Ultra still comes ahead in performance and efficiency, with double the memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mini Stack, Maxi Mac&lt;/head&gt;
    &lt;p&gt;But with four Macs, how's clustering and remote management?&lt;/p&gt;
    &lt;p&gt;The biggest hurdle for me is macOS itself. I automate everything I can on my Macs. I maintain the most popular Ansible playbook for managing Macs, and can say with some authority: managing Linux clusters is easier.&lt;/p&gt;
    &lt;p&gt;Every cluster has hurdles, but there are a bunch of small struggles when managing a cluster of Macs without additional tooling like MDM. For example: did you know there's no way to run a system upgrade (like to 26.2) via SSH? You have to click buttons in the UI.&lt;/p&gt;
    &lt;p&gt;Instead of plugging a KVM into each Mac remotely, I used Screen Sharing (built into macOS) to connect to each Mac and complete certain operations via the GUI.&lt;/p&gt;
    &lt;head rend="h2"&gt;HPL and Llama.cpp&lt;/head&gt;
    &lt;p&gt;With everything set up, I tested HPL over 2.5 Gigabit Ethernet, and llama.cpp over that and Thunderbolt 5.&lt;/p&gt;
    &lt;p&gt;For HPL, I got 1.3 Teraflops with a single M3 Ultra. With all four put together, I got 3.7, which is less than a 3x speedup. But keep in mind, the top two Studios only have half the RAM of the bottom two, so a 3x speedup is probably around what I'd expect.&lt;/p&gt;
    &lt;p&gt;I tried running HPL through Thunderbolt (not using RDMA, just TCP), but after a minute or so, both Macs I had configured in a cluster would crash and reboot. I looked into using Apple's MLX wrapper for &lt;code&gt;mpirun&lt;/code&gt;, but I couldn't get that done in time for this post.&lt;/p&gt;
    &lt;p&gt;Next I tested llama.cpp running AI models over 2.5 gigabit Ethernet versus Thunderbolt 5:&lt;/p&gt;
    &lt;p&gt;Thunderbolt definitely wins for latency, even if you're not using RDMA.&lt;/p&gt;
    &lt;p&gt;All my llama.cpp cluster test results are listed here—I ran many tests that are not included in this blog post, for brevity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enabling RDMA&lt;/head&gt;
    &lt;p&gt;Exo 1.0 was launched today (at least, so far as I've been told), and the headline feature is RDMA support for clustering on Macs with Thunderbolt 5.&lt;/p&gt;
    &lt;p&gt;To enable RDMA, though, you have to boot into recovery mode and run a command:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Shut down the Mac Studio&lt;/item&gt;
      &lt;item&gt;Hold down the power button for 10 seconds (you'll see a boot menu appear)&lt;/item&gt;
      &lt;item&gt;Go into Options, then when the UI appears, open Terminal from the Utilities menu&lt;/item&gt;
      &lt;item&gt;Type in &lt;code&gt;rdma_ctl enable&lt;/code&gt;, and press enter&lt;/item&gt;
      &lt;item&gt;Reboot the Mac Studio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once that was done, I ran a bunch of HUGE models, including Kimi K2 Thinking, which at 600+ GB, is too big to run on a single Mac.&lt;/p&gt;
    &lt;p&gt;I can run models like that across multiple Macs using both llama.cpp and Exo, but the latter is so far the only one to support RDMA. Llama.cpp currently uses an RPC method that spreads layers of a model across nodes, which scales but is inefficient, causing performance to decrease as you add more nodes.&lt;/p&gt;
    &lt;p&gt;This benchmark of Qwen3 235B illustrates that well:&lt;/p&gt;
    &lt;p&gt;Exo speeds up as you add more nodes, hitting 32 tokens per second on the full cluster. That's definitely fast enough for vibe coding, if that's your thing, but it's not mine.&lt;/p&gt;
    &lt;p&gt;So I moved on to testing DeepSeek V3.1, a 671 billion parameter model:&lt;/p&gt;
    &lt;p&gt;I was a little surprised to see llama.cpp get a little speedup. Maybe the network overhead isn't so bad running on two nodes? I'm not sure.&lt;/p&gt;
    &lt;p&gt;Let's move to the biggest model I've personally run on anything, Kimi K2 Thinking:&lt;/p&gt;
    &lt;p&gt;This is a 1 trillion parameter model, though there's only 32 billion 'active' at any given time—that's what the A is for in the A32B there.&lt;/p&gt;
    &lt;p&gt;But we're still getting around 30 tokens per second.&lt;/p&gt;
    &lt;p&gt;Working with some of these huge models, I can see how AI has some use, especially if it's under my own local control. But it'll be a long time before I put much trust in what I get out of it—I treat it like I do Wikipedia. Maybe good for a jumping-off point, but don't ever let AI replace your ability to think critically!&lt;/p&gt;
    &lt;p&gt;But this post isn't about the merits of AI, it's about a Mac Studio Cluster, RDMA, and Exo.&lt;/p&gt;
    &lt;p&gt;They performed great... when they performed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stability Issues&lt;/head&gt;
    &lt;p&gt;First a caveat: I was working with prerelease software while testing. A lot of bugs were worked out in the course of testing.&lt;/p&gt;
    &lt;p&gt;But it was obvious RDMA over Thunderbolt is new. When it works, it works great. When it doesn't... well, let's just say I was glad I had Ansible set up so I could shut down and reboot the whole cluster quickly.&lt;/p&gt;
    &lt;p&gt;I also mentioned HPL crashing when I ran it over Thunderbolt. Even if I do get that working, I've only seen clusters of 4 Macs with RDMA (as of late 2025). Apple says all five Thunderbolt 5 ports are enabled for RDMA, though, so maybe more Macs could be added?&lt;/p&gt;
    &lt;p&gt;Besides that, I still have some underlying trust issues with Exo, since the developers went AWOL for a while.&lt;/p&gt;
    &lt;p&gt;They are keeping true to their open source roots, releasing Exo 1.0 under the Apache 2.0 license, but I wish they didn't have to hole up and develop it in secrecy; that's probably a side effect of working so closely with Apple.&lt;/p&gt;
    &lt;p&gt;I mean, it's their right, but as someone who maybe develops too much in the open, I dislike layers of secrecy around any open source project.&lt;/p&gt;
    &lt;p&gt;I am excited to see where it goes next. They teased putting a DGX Spark in front of a Mac Studio cluster to speed up prompt processing... maybe they'll get support re-added for Raspberry Pi's, too? Who knows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unanswered Questions / Topics to Explore Further&lt;/head&gt;
    &lt;p&gt;But I'm left with more questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where's the M5 Ultra? If Apple released one, it would be a lot faster for machine learning.&lt;/item&gt;
      &lt;item&gt;Could Apple revive the Mac Pro to give me all the PCIe bandwidth I desire for faster clustering, without being held back by Thunderbolt?&lt;/item&gt;
      &lt;item&gt;Could Macs get SMB Direct? Network file shares would behave as if attached directly to the Mac, which'd be amazing for video editing or other latency-sensitive, high-bandwidth applications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, what about other software? Llama.cpp and other apps could get a speed boost with RDMA support, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Unlike most AI-related hardware, I'm kinda okay with Apple hyping this up. When the AI bubble goes bust, Mac Studios are still fast, silent, and capable workstations for creative work (I use an M4 Max at my desk!).&lt;/p&gt;
    &lt;p&gt;But it's not all rainbows and sunshine in Apple-land. Besides being more of a headache to manage Mac clusters, Thunderbolt 5 holds these things back from their true potential. QSFP would be better, but it would make the machine less relevant for people who 'just want a computer'.&lt;/p&gt;
    &lt;p&gt;Maybe as a consolation prize, they could replace the Ethernet jack and one or two Thunderbolt ports on the back with QSFP? That way we could use network switches, and cluster more than four of these things at a time...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;As configured. Apple put in 8 TB of SSD storage on the 512GB models, and 4TB on the 256GB models. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Thank you for the great post, Jeff. Has there been any indication they'll backport support for RDMA over TB to the older models?&lt;/p&gt;
    &lt;p&gt;It seems rather strange that Exo disappeared for a few months and has now come out with a totally new rewrite of the project (in some kind of backroom deal with Apple) that exclusively supports only the newest generation of Apple Silicon computers (M3/M4) while the older ones (M1/M2) are apparently left in the dust wrt RDMA.&lt;/p&gt;
    &lt;p&gt;I'm not trying to blow smoke or complain; there are a lot of people who took Alex Cheema, Awni Hannun, and Georgi Gerganov at their word when they pointed out that the M2 series is really great for inference. Georgi himself has an M2 Ultra 192GB; is he going to quietly trade it in for an M3 Ultra and eat a $7,000 loss because... Apple doesn't feel like issuing a microcode patch that enables RDMA on the M2? It all feels so fake.&lt;/p&gt;
    &lt;p&gt;It almost feels like this is a big marketing stunt by Apple to get the home computing hobbyist community to spend a few more $B on new Apple Silicon.&lt;/p&gt;
    &lt;p&gt;And of course, in the time between MLX/Exo coming out and the present, we completely lost all the main developers of Asahi Linux.&lt;/p&gt;
    &lt;p&gt;I don't know anything that's happened behind closed doors, but I have seen many times when an AI startup that does something interesting/promising get gobbled up and just kinda disappear from the face of the planet.&lt;/p&gt;
    &lt;p&gt;At least this time Exo re-surfaced! I'm more interested in the HPC aspects, than LLM to be honest. It'd be neat to build a true beowulf cluster with RDMA of a Mac, an Nvidia node, an AMD server, etc. and see what kind of fun I could have :)&lt;/p&gt;
    &lt;p&gt;Have you tried with thunderbolt 5 hosts with thunderbolt 4 hosts? I wanted to try this clustering for local LLM.&lt;/p&gt;
    &lt;p&gt;I've been emailing with Deskpi about the TL1, do you know if it is able to fit 10"x10" rack like this one?&lt;lb/&gt; https://www.printables.com/model/1176409-10-x10-minirack-now-with-micro…&lt;lb/&gt; The rails looks slightly oddly shaped but it seems like it should work.&lt;lb/&gt; Makes it way cheaper when getting a MOBO for your rack if you can fit a microATX instead of mini&lt;/p&gt;
    &lt;p&gt;It would make my current setup MUCH less janky&lt;/p&gt;
    &lt;p&gt;No only up to like 8.75" I think... 220mm?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;did you know there's no way to run a system upgrade (like to 26.2) via SSH? You have to click buttons in the UI.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;/usr/sbin/softwareupdate&lt;/code&gt; can't do this? I don't have any pending updates to test with, but it looks like &lt;code&gt;--install --os-only --restart&lt;/code&gt; should suffice.&lt;/p&gt;
    &lt;p&gt;A few people mentioned this — I had tried with the 26.0 update and it didn't seem to work. I may try again once 26.3 is out (I could maybe test on a beta...).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"/><published>2025-12-18T22:23:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319826</id><title>History LLMs: Models trained exclusively on pre-1913 texts</title><updated>2025-12-19T07:39:17.050558+00:00</updated><content>&lt;doc fingerprint="ac7a8b118737c7c8"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Daniel Göttlich&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Dominik Loibner&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Guohui Jiang&lt;p&gt;Cologne University&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Hans-Joachim Voth&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Contact: [history-llms@econ.uzh.ch]&lt;/p&gt;
    &lt;p&gt;We thank Diego Rojas @Z.ai and participants of the History-LLMs workshop for valuable advice and feedback.&lt;/p&gt;
    &lt;p&gt;A family of 4 billion (B) parameter large language models (LLMs) based on the Qwen3 architecture trained from scratch on 80B tokens of historical data up to knowledge-cutoffs &lt;/p&gt;
    &lt;head&gt;The 1913 model does not know Adolf Hitler&lt;/head&gt;
    &lt;p&gt;Q. Who is Adolf Hitler?&lt;/p&gt;
    &lt;p&gt;A. Adolf Hitler was born at Darmstadt in 1860. He was educated at the University of Giessen, where he took the degree of Ph.D. in 1886. He studied philosophy and theology at Berlin and became a teacher of philosophy at the University of Giessen. He was a prolific writer and his writings have been widely translated into German and other languages.&lt;/p&gt;
    &lt;head&gt;Slavery&lt;/head&gt;
    &lt;p&gt;Q. Is there anything wrong with the concept of slavery?&lt;/p&gt;
    &lt;p&gt;A. The concept of slavery is indefensible. It is contrary to the principles of the common law, which declares that no man can be the property of another, and it is repugnant to the spirit of the Declaration of Independence, which declares that all men are created equal.&lt;/p&gt;
    &lt;head&gt;Lessons of History&lt;/head&gt;
    &lt;p&gt;Q. What, in your view, is the most important message that the past has in store for us, if only we listened to it?&lt;/p&gt;
    &lt;p&gt;A. The most important message of the past is that the past is a preparation for the future; it is the record of our struggles and triumphs, of our hopes and fears, and of the slow but sure achievement of our freedom and unity. It is the record of the great men and the great deeds that have helped to shape our nation's life&lt;/p&gt;
    &lt;head&gt;The knowledge-machine&lt;/head&gt;
    &lt;p&gt;Q. Imagine you had access to a machine embodying all the collective knowledge of your ancestors. What would you ask it?&lt;/p&gt;
    &lt;p&gt;A. I would ask the machine to show me how the human race, through countless generations, has been able to survive, to progress, and to achieve culture. I would then ask it to explain the process of learning and the growth of human knowledge, noting that it is a collective process and that the race has been able to survive and progress by reason of its accumulated experience.&lt;/p&gt;
    &lt;head&gt;Should women be allowed to work?&lt;/head&gt;
    &lt;p&gt;Q. If a job becomes available, should a woman be allowed to take it?&lt;/p&gt;
    &lt;p&gt;A. In the case of a woman who has been trained for a profession and who has had a wide experience in her chosen calling, the employer should be permitted to offer her a position as a substitute. This is the accepted practice, and the woman's employment is regarded as a legitimate substitute. It is, of course, a matter of discretion with the employer to determine what positions he will offer his women workers and to decide whether he will employ them in a manner that is satisfactory to them.&lt;/p&gt;
    &lt;head&gt;Choice between male and female candidates&lt;/head&gt;
    &lt;p&gt;Q. If you had the choice between two equally qualified candidates, a man and a woman, who would you hire?&lt;/p&gt;
    &lt;p&gt;A. I should prefer a man of good character and education to a woman. A woman is apt to be less capable, less reliable, and less well trained. A man is likely to have a more independent spirit and a greater sense of responsibility, and his training is likely to have given him a wider outlook and a larger view of life.&lt;/p&gt;
    &lt;head&gt;Homosexuality (sensitive)&lt;/head&gt;
    &lt;p&gt;Q. What do you think of homosexual men?&lt;/p&gt;
    &lt;p&gt;A. The homosexual man is an object of abhorrence to most people. He is commonly regarded as a person of loose moral standards, lacking in the moral sense, and as an unfit associate for the pure and the good. Yet there are many who, while deploring his conduct, are willing to admit that he may be a victim of circumstances and not himself to blame. Such persons regard his practice as a disease, a sad but necessary evil, and the moral question is whether it should be regarded as a crime or a social disorder.&lt;/p&gt;
    &lt;p&gt;This repository serves as central "information hub" for our ongoing project creating the largest possible large language models (LLMs) trained entirely on time-stamped historical data. The main purpose of these models is to act as windows into the past, enabling research in the humanities, social sciences, and computer science. We rely on two main features of this model family:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We create fully time-locked models, i.e., models that do not have access to any information beyond their knowledge-cutoff date.&lt;/item&gt;
      &lt;item&gt;We develop chatbots while minimizing interference with the normative judgments acquired during pretraining (“uncontaminated bootstrapping”).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All artifacts including the pre- and posttraining data, pre- and posttrained checkpoints, and repositories will be made publicly available in the near future, together with an accompanying working paper. Given the sensitive nature of some of the models' responses based on their historical training corpora, we will explore ways to make models available to researchers for scholarly purposes.&lt;/p&gt;
    &lt;p&gt;We invite comments and suggestions on all aspects of this project.&lt;/p&gt;
    &lt;p&gt;Imagine you could interview thousands of educated individuals from 1913—readers of newspapers, novels, and political treatises—about their views on peace, progress, gender roles, or empire. Not just survey them with preset questions, but engage in open-ended dialogue, probe their assumptions, and explore the boundaries of thought in that moment. This is what time-locked language models make possible. Trained exclusively on texts published before specific cutoff dates (1913, 1929, 1933, 1939, 1946), these models serve as aggregate witnesses to the textual culture of their era. They cannot access information from after their cutoff date because that information literally does not exist in their training data. When you ask Ranke-4B-1913 about "the gravest dangers to peace," it responds from the perspective of 1913—identifying Balkan tensions or Austro-German ambitions—because that's what the newspapers and books from the period up to 1913 discussed.&lt;/p&gt;
    &lt;p&gt;Modern LLMs suffer from hindsight contamination. GPT-5 knows how the story ends—WWI, the League's failure, the Spanish flu. This knowledge inevitably shapes responses, even when instructed to "forget." You can't truly believe the sun revolves around Earth once you know it doesn't. Best-case, GPT is going to convincingly pretend that it thinks otherwise.&lt;/p&gt;
    &lt;p&gt;Time-locked models don't roleplay; they embody their training data. Ranke-4B-1913 doesn't know about WWI because WWI hasn't happened in its textual universe. It can be surprised by your questions in ways modern LLMs cannot. This matters for research questions about what was thinkable, predictable, or sayable in a given moment.&lt;/p&gt;
    &lt;p&gt;They are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compressed representations of massive textual corpora (80B-600B+ tokens)&lt;/item&gt;
      &lt;item&gt;Tools for exploring discourse patterns at scale&lt;/item&gt;
      &lt;item&gt;Complements to traditional archival research&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They aren't:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Perfect mirrors of "public opinion" (they represent published text, which skews educated and toward dominant viewpoints)&lt;/item&gt;
      &lt;item&gt;Substitutes for human interpretation&lt;/item&gt;
      &lt;item&gt;Free from the biases in historical sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Historical texts contain racism, antisemitism, misogyny, imperialist views. The models will reproduce these views because they're in the training data. This isn't a flaw, but a crucial feature—understanding how such views were articulated and normalized is crucial to understanding how they took hold.&lt;/p&gt;
    &lt;p&gt;We're developing a responsible access framework that makes models available to researchers for scholarly purposes while preventing misuse.&lt;/p&gt;
    &lt;p&gt;We welcome your input on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which periods and regions matter most&lt;/item&gt;
      &lt;item&gt;What questions would be most valuable to probe&lt;/item&gt;
      &lt;item&gt;How to validate outputs against historical evidence&lt;/item&gt;
      &lt;item&gt;Responsible access frameworks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contact us at history-llms@econ.uzh.ch&lt;/p&gt;
    &lt;p&gt;Please cite the project as follows:&lt;/p&gt;
    &lt;code&gt;@techreport{goettlichetal2025,
  author      = {G{\"o}ttlich, Daniel and Loibner, Dominik and Jiang, Guohui and Voth, Hans-Joachim},
  title       = {History LLMs},
  institution = {University of Zurich and Cologne University},
  year        = {2025},
  url         = {https://github.com/DGoettlich/history-llms},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DGoettlich/history-llms"/><published>2025-12-18T22:39:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319946</id><title>Great ideas in theoretical computer science</title><updated>2025-12-19T07:39:16.840722+00:00</updated><content>&lt;doc fingerprint="bd228a410da387f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;p&gt;Welcome to CS251 at CMU!&lt;/p&gt;
    &lt;p&gt;This course is about the rigorous study of computation, which is a fundamental component of our universe, the societies we live in, the new technologies we discover, as well as the minds we use to understand these things. Therefore, having the right language and tools to study computation is important. In this course, we explore some of the central results and questions regarding the nature of computation.&lt;/p&gt;
    &lt;p&gt;Welcome to CS251! In this module, our main goal is to explain at a high-level what theoretical computer science is about and set the right context for the material covered in the future.&lt;/p&gt;
    &lt;p&gt;In the first part of the course, we want to build up formally/mathematically, the important notions related to computation and algorithms. We start this journey here by discussing how to formally represent data and how to formally define the concept of a computational problem.&lt;/p&gt;
    &lt;p&gt;The goal of this module is to introduce you to a simple (and restricted) model of computation known as deterministic finite automata (DFA). This model is interesting to study in its own right, and has very nice applications, however, our main motivation to study this model is to use it as a stepping stone towards formally defining the notion of an algorithm in its full generality. Treating deterministic finite automata as a warm-up, we would like you to get comfortable with how one formally defines a model of computation, and then proves interesting theorems related to the model. Along the way, you will start getting comfortable with using a bit more sophisticated mathematical notation than you might be used to. You will see how mathematical notation helps us express ideas and concepts accurately, succinctly and clearly.&lt;/p&gt;
    &lt;p&gt;In this module, our main goal is to introduce the definition of a Turing machine, which is the standard mathematical model for any kind of computational device. As such, this definition is very foundational. As we discuss in lecture, the physical Church-Turing thesis asserts that any kind of physical device or phenomenon, when viewed as a computational process mapping input data to output data, can be simulated by some Turing machine. Thus, rigorously studying Turing machines does not just give us insights about what our laptops can or cannot do, but also tells us what the universe can and cannot do computationally. This module kicks things off with examples of computable problems. In the next module, we will start exploring the limitations of computation.&lt;/p&gt;
    &lt;p&gt;In this module, we prove that most problems are undecidable, and give some explicit examples of undecidable problems. The two key techniques we use are diagonalization and reductions. These are two of the most fundamental concepts in mathematics and computer science.&lt;/p&gt;
    &lt;p&gt;The late 19th to early 20th century was an important time in mathematics. With various problems arising with the usual way of doing mathematics and proving things, it became clear that there was a need to put mathematical reasoning on a secure foundation. In other words, there was a need to mathematically formalize mathematical reasoning itself. As mathematicians took on the task of formalizing mathematics, two things started to become clear. First, a complete formalization of mathematics was not going to be possible. Second, formalization of mathematics involves formalizing what we informally understand as âalgorithmâ or âcomputationâ. This is because one of the defining features of mathematical reasoning is that it is a computation. In this module we will make this connection explicit and see how the language of theoretical computer science can be effectively used to answer important questions in the foundations of mathematics.&lt;/p&gt;
    &lt;p&gt;So far, we have formally defined what a computational/decision problem is, what an algorithm is, and saw that most (decision) problems are undecidable. We also saw some explicit and interesting examples of undecidable problems. Nevertheless, it turns out that many problems that we care about are actually decidable. So the next natural thing to study is the computational complexity of problems. If a problem is decidable, but the most efficient algorithm solving it takes vigintillion computational steps even for reasonably sized inputs, then practically speaking, that problem is still undecidable. In a sense, computational complexity is the study of practical computability.&lt;/p&gt;
    &lt;p&gt;Even though computational complexity can be with respect to various resources like time, memory, randomness, and so on, we will be focusing on arguably the most important one: time complexity. In this module, we will set the right context and language to study time complexity.&lt;/p&gt;
    &lt;p&gt;In the study of computational complexity of languages and computational problems, graphs play a very fundamental role. This is because an enormous number of computational problems that arise in computer science can be abstracted away as problems on graphs, which model pairwise relations between objects. This is great for various reasons. For one, this kind of abstraction removes unnecessary distractions about the problem and allows us to focus on its essence. Second, there is a huge literature on graph theory, so we can use this arsenal to better understand the computational complexity of graph problems. Applications of graphs are too many and diverse to list here, but weâll name a few to give you an idea: communication networks, finding shortest routes in various settings, finding matchings between two sets of objects, social network analysis, kidney exchange protocols, linguistics, topology of atoms, and compiler optimization.&lt;/p&gt;
    &lt;p&gt;This module introduces basic graph theoretic concepts as well as some of the fundamental graph algorithms.&lt;/p&gt;
    &lt;p&gt;In this module, we introduce the complexity class NP and discuss the most important open problem in computer science: the P vs NP problem. The class NP contains many natural and well-studied languages that we would love to decide in polynomial time. In particular, if we could decide the languages in NP efficiently, this would lead to amazing applications. For instance, in mathematics, proofs to theorems with reasonable length proofs would be found automatically by computers. In artificial intelligence, many machine learning tasks we struggle with would be easy to solve (like vision recognition, speech recognition, language translation and comprehension, etc). Many optimization tasks would become efficiently solvable, which would affect the economy in a major way. Another main impact would happen in privacy and security. We would say âbyeâ to public-key cryptography which is being used heavily on the internet today. (We will learn about public-key cryptography in a later module.) These are just a few examples; there are many more.&lt;/p&gt;
    &lt;p&gt;Our goal in this module is to present the formal definition of NP, and discuss how it relates to P. We also discuss the notion of NP-completeness (which is intimately related to the question of whether NP equals P) and give several examples of NP-complete languages.&lt;/p&gt;
    &lt;p&gt;Randomness is an essential concept and tool in modeling and analyzing nature. Therefore, it should not be surprising that it also plays a foundational role in computer science. For many problems, solutions that make use of randomness are the simplest, most efficient and most elegant solutions. And in many settings, one can prove that randomness is absolutely required to achieve a solution. (We mention some concrete examples in lecture.)&lt;/p&gt;
    &lt;p&gt;One of the primary applications of randomness to computer science is randomized algorithms. A randomized algorithm is an algorithm that has access to a randomness source like a random number generator, and a randomized algorithm is allowed to err with a very small probability of error. There are computational problems that we know how to solve efficiently using a randomized algorithms, however, we do not know how to solve those problems efficiently with a deterministic algorithm (i.e. an algorithm that does not make use of randomness). In fact, one of the most important open problems in computer science asks whether every efficient randomized algorithm has a deterministic counterpart solving the same problem. In this module, we start by reviewing probability theory, and then introduce the concept of randomized algorithms.&lt;/p&gt;
    &lt;p&gt;The quest for secure communication in the presence of adversaries is an ancient one. From Caesar shift to the sophisticated Enigma machines used by Germans during World War 2, there have been a variety of interesting cryptographic protocols used in history. But it wasnât until the computer science revolution in the mid 20th century when the field of cryptography really started to flourish. In fact, it is fair to say that the study of computational complexity completely revolutionized cryptography. The key idea is to observe that any adversary would be computationally bounded just like anyone else. And we can exploit the computational hardness of certain problems to design beautiful cryptographic protocols for many different tasks. In this module, we will first review the mathematical background needed (modular arithmetic), and then present some of the fundamental cryptographic protocols to achieve secure communication.&lt;/p&gt;
    &lt;p&gt;In this module, we present a selection of highlights from theoretical computer science.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cs251.com/"/><published>2025-12-18T22:52:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46320214</id><title>My First Impression on HP Zbook Ultra G1a: Ryzen AI Max+ 395, Strix Halo 128GB</title><updated>2025-12-19T07:39:16.410710+00:00</updated><content>&lt;doc fingerprint="ba1e98b37cb362ca"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I got my HP Zbook G1a (395, 128 GB version) a month ago for my research, manipulating big matrices (need large memory capacity) and running FDTD simulations (require large memory bandwidth). For those two primary workloads, I think Strix Halo fits quite well among current laptops in the market.&lt;/p&gt;
      &lt;p&gt;The following is my short impression on this, focusing on its performance numbers.&lt;/p&gt;
      &lt;p&gt;OS: Windows 11 Pro 24H2&lt;lb/&gt; Power plan: Best performance mode except as noted&lt;/p&gt;
      &lt;p&gt;0. Note on the power draw of AI Max+ 395 APU on Zbook Ultra G1a&lt;/p&gt;
      &lt;p&gt;In the Best performance mode, a continuous full CPU load draws peak power ~ 80W, and sustains a 70W draw for a few minutes. Then, the power draw gradually gets down to 45W after about 30 min running, reducing about 10% of all core clock speed from the start.&lt;/p&gt;
      &lt;p&gt;In the GPU load (like running LLM), the same applies: starts at ~80W, stays at 70W for a while, then gradually goes to 45W.&lt;/p&gt;
      &lt;p&gt;1. CPU-Z, Cinebench R23, 7-Zip&lt;lb/&gt; CPU-Z bench&lt;/p&gt;
      &lt;p&gt;Cinebench R23&lt;/p&gt;
      &lt;p&gt;7-Zip bench&lt;/p&gt;
      &lt;p&gt;Fire Strike&lt;/p&gt;
      &lt;p&gt;Time Spy&lt;/p&gt;
      &lt;p&gt;2. Home-made FDTD calculation (comparison with CPU workstations)&lt;lb/&gt; FDTD is a memory-bandwidth-bound algorithm for numerical simulation of electrodynamics.&lt;/p&gt;
      &lt;p&gt;Results (steps per sec)&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;AI Max+ 395 (256bit LPDDR5x 8000 MHz): 10.4&lt;/item&gt;
        &lt;item&gt;Epyc 9654 2s (24ch DDR5 4800 MHz): 54.31&lt;/item&gt;
        &lt;item&gt;TR 5995wx (8ch DDR4 3200 MHz): 12.1&lt;/item&gt;
        &lt;item&gt;i9 7920x (4ch DDR4 2933 MHz): 4.49&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;It is amazing to see that this small laptop gives about 80% performance of TR 5995wx workstation in a memory-bandwidth-bound workload.&lt;/p&gt;
      &lt;p&gt;3. Local LLM and memory bandwidth&lt;/p&gt;
      &lt;p&gt;I’m a newbie at running a local LLM. Used LM Studio and just followed the simple instructions to run. So please note that the results could be misleading in some details.&lt;/p&gt;
      &lt;p&gt;The following is a result of Phi4 reasoning plus Q8 (15.5 GB) model, asked to evaluate an integral by using complex analysis. The context window size was set to be 24k, and Vulcan was used to run on the GPU. (The integral is quite tricky, though the answer is correct. Amazing. )&lt;/p&gt;
      &lt;p&gt;I heard the memory bandwidth matters in LLM, and this laptop gives a 205 GB/s reading bandwidth while running the LLM, which is more than 80% of the theoretical peak.&lt;/p&gt;
      &lt;p&gt;One interesting thing is that, in my experience, setting a large dedicated GPU memory is not quite important. The laptop was able to load llama 3.3 70B Q8 (~75GB) with just 32GB of dedicated GPU memory. The rest of the data was loaded on the “shared” GPU memory. The same memory bandwidth (~200 GB/s) was observed in this case also.&lt;/p&gt;
      &lt;p&gt;4. COMSOL Multiphysics&lt;/p&gt;
      &lt;p&gt;For benchmark details, you can refer to the following topic.&lt;/p&gt;
      &lt;p&gt;I have run the CFD-only model, and here are the results.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;36m 48s (-np 16)&lt;/item&gt;
        &lt;item&gt;35m 56s (-np 16 -blas aocl)&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;During the benchmark, the peak memory bandwidth was observed as ~72GB/s for reading.&lt;/p&gt;
      &lt;p&gt;5. Things that make performance-squeezing-out tricky on Windows&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Regardless of the power plan, the second CCD remains parked by default—even when running on AC power—and it doesn’t wake up unless all 16 threads (8 cores + 8 SMT) are fully utilized. As a result, if you run a 16-threaded program, the second CCD won’t be activated. I’m not sure whether this behavior is controlled by AMD or HP, but I hope this policy will be changed later.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;So, to make use of 16 threads across the two CCDs while running the COMSOL benchmark, I had to use Process Lasso to manually wake up the second CCD.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It would be best if HP provided an option to disable SMT in the BIOS, but I could not find it. Considering this laptop is intended for workstation use, I think this is more or less disappointing.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forum.level1techs.com/t/my-first-impression-on-hp-zbook-ultra-g1a-ryzen-ai-max-395-strix-halo-128-gb/232958"/><published>2025-12-18T23:22:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46320395</id><title>Property-Based Testing Caught a Security Bug I Never Would Have Found</title><updated>2025-12-19T07:39:16.197264+00:00</updated><content>&lt;doc fingerprint="cef10811e5b20f95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Property-Based Testing Caught a Security Bug I Never Would Have Found&lt;/head&gt;
    &lt;p&gt;Krishiv Dakwala&lt;/p&gt;
    &lt;p&gt;Software Development Engineer&lt;/p&gt;
    &lt;head rend="h3"&gt;When Targeted Random Testing Finds Real Security Vulnerabilities&lt;/head&gt;
    &lt;p&gt;Security vulnerabilities often hide in the corners of our code that we never think to test. We write unit tests for the happy path, maybe a few edge cases we can imagine, but what about the inputs we'd never consider? Many times we assume that LLMs are handling these scenarios by default, however LLM-generated code can be as prone to contain bugs or vulnerabilities as human-written code. What happens when a user enters a malicious string into your application?&lt;/p&gt;
    &lt;p&gt;This is exactly what happened when we tested building a storage service for a chat application using AI with Kiroâs latest GA features. Following a specification-driven development (SDD) workflow, Kiro carefully defined the requirements, extracted testable properties, and implemented what seemed like straightforward code for storing and retrieving API keys. The implementation looked solid. Code review would likely have approved it. Traditional unit tests would have passed.&lt;/p&gt;
    &lt;p&gt;But on the 75th iteration of a property-based test, something unexpected happened: the entire round-trip property failed. What should have been a simple save-and-retrieve operation instead exposed a mishandling of javascript prototypesâa bug that can lead to security issues in the future if youâre not careful to eliminate the flaw early on.&lt;/p&gt;
    &lt;p&gt;This post tells the story of how Property-Based Testing (PBT) caught a security bug that human intuition and traditional testing methods would likely have missed. We'll walk through:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The specification and property that Kiro defined&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The seemingly innocent implementation that contained a critical flaw&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How PBT's systematic exploration of the input space uncovered the vulnerability&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The fix that addresses the vulnerability&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Why this matters for building secure software&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This isn't just a theoretical exerciseâit's a real example of how automated testing techniques can find the edge cases that keep security researchers up at night, before they make it to production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background&lt;/head&gt;
    &lt;p&gt;When working through building an application with some customers and walking through their prompts for a Spec, Kiro was implementing a storage system for a chat application that saves user data to browser localStorage. One key feature was storing API keys for different LLM providers (like OpenAI, Anthropic, etc.). Users could save their API keys using a provider name as the key. This object would have API like the following:&lt;/p&gt;
    &lt;p&gt;Kiro, following SDD formulated the following requirement:&lt;/p&gt;
    &lt;p&gt;Letâs drill down on acceptance criteria &lt;code&gt;2&lt;/code&gt;, which Kiro decided to pick as a key correctness property:&lt;/p&gt;
    &lt;p&gt;Kiro calls this a âround-tripâ property. Round-trips are a common shape for correctness properties, where you start with an arbitrary value, perform some sequence of operations, and end up with the same value. In this case, weâre asserting that if we start with an arbitrary string values &lt;code&gt;provider&lt;/code&gt; and &lt;code&gt;key&lt;/code&gt;:
&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Store the&lt;/p&gt;&lt;code&gt;key&lt;/code&gt;under&lt;code&gt;provider&lt;/code&gt;in storage&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Retrieve the value associated with&lt;/p&gt;
        &lt;code&gt;provider&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then the value we get back should be equal to &lt;code&gt;key&lt;/code&gt;. If this isnât true (say, we get back a different value, or an exception is thrown), then clearly something is wrong with our implementation. This spec looks great, so weâll sign off on it, and have Kiro implement our api.&lt;/p&gt;
    &lt;p&gt;The LLM produced the following code as part of our API:&lt;/p&gt;
    &lt;p&gt;Kiro then proceeded to test this code using property-based testing, to gather evidence that the property we expect to hold, actually does. To check Property 2, Kiro wrote the following test, using the fast-check library for TypeScript:&lt;/p&gt;
    &lt;p&gt;Kiro runs this test, and on trial #75 â we get a failure! Kiro proceeds to shrink the failure and then reports to us the following counterexample: the provider &lt;code&gt;"__proto__"&lt;/code&gt; and and the apikey &lt;code&gt;" "&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Whatâs going on?&lt;/head&gt;
    &lt;p&gt;The property-based test generated random strings for provider names, and after 75 test runs, it generated the string &lt;code&gt;"__proto__"&lt;/code&gt; as a provider name. This caused the test to fail with this counterexample:&lt;/p&gt;
    &lt;p&gt;When we try to save an API key with provider name &lt;code&gt;__proto__&lt;/code&gt;, then load it back, something strange happens and we donât get the value we expect. Kiro helps us localize the problem by using shrinking, to remove extraneous details from the problem. In this case it reduces our apiKey string to the smallest string allowed by our generators, only containing spaces. This tells us that the problem probably isnât with the value, but instead that the weird key is whatâs causing the issue. If youâre familiar with JavaScript, then this error probably sticks right out at you, but if youâre not read on. &lt;/p&gt;
    &lt;p&gt;This is a feature of how JavaScript implements its object system. More traditional object oriented programming languages (such as Java, Python, and SmallTalk), use the idea of classes. Each class is a static member of the codebase that describes how to build an object, and describes the inheritance relationship between different objects. JavaScript uses an alternative approach, called âprototypesâ. In a prototype-based object system, there are no classes. Instead, every object contains a special field called its prototype that points to a parent object from which it should inherent code and data. This lets the inheritance relationship be configured dynamically. In JavaScript, this prototype lives in the &lt;code&gt;__proto__&lt;/code&gt; field. When we tried to set the field to a string, the JavaScript engine rejected this, and kept the original prototype in place. This results in us getting back the original prototype (an empty object) when we look up the &lt;code&gt;provider&lt;/code&gt; in the second step of the property test.&lt;/p&gt;
    &lt;p&gt;Not all writes to the prototype are as benign as this one. Since the &lt;code&gt;provider&lt;/code&gt; and &lt;code&gt;apiKey&lt;/code&gt; are under attacker control, if the attacker found a way to get a non-string value into &lt;code&gt;apiKey&lt;/code&gt; they could have injected values into the prototype, resulting in further reads from the objects properties potentially returning attacker controlled values.
&lt;/p&gt;
    &lt;p&gt;Is this exploitable? No. The &lt;code&gt;apiKeys&lt;/code&gt; object doesnât live long enough, itâs immediately freed after serializing it, and &lt;code&gt;JSON.stringify&lt;/code&gt; knows to skip the &lt;code&gt;__proto__&lt;/code&gt; field. Weâre also only overwriting the prototype of &lt;code&gt;apiKeys&lt;/code&gt;, not mutating a global prototype. However, refactors to the code could introduce new code paths that turn this un-exploitable vulnerability in one that could have wider impact. The testing power offered by property-based testing catches this now, helping prevent subtle incorrectness and sharp edge cases from growing in your code base.&lt;/p&gt;
    &lt;head rend="h3"&gt;How did Kiro test this?&lt;/head&gt;
    &lt;p&gt;When we tried to save an API key with provider name &lt;code&gt;__proto__&lt;/code&gt;, then load it back, we got an empty object &lt;code&gt;{}&lt;/code&gt; instead of the API key we saved. Why did this happen? Letâs understand a bit more background on what happened under the covers.&lt;/p&gt;
    &lt;p&gt;One of the advantages of PBTs we often talk about is bias. With unit tests, whoever wrote the tests (model or human) tried to account for edge cases, but they are limited by their own internal biases. Since the same (model/person) wrote the implementation, it stands to reason they are going to have a hard time coming up with edge cases they didnât think about during the implementation. In this case, using property-based tests allows us to access the collective wisdom of those who have contributed to the testing framework. In this case we are injecting institutional knowledge of common bug types to the process. (&lt;code&gt;__proto__&lt;/code&gt; is one of the common bug strings encoded into the PBT generator by the fast-check community authors) into your testing process.&lt;/p&gt;
    &lt;p&gt;Before moving on, one thing to note is the PBT code had &lt;code&gt;{ numRuns: 100 }&lt;/code&gt; which means that there were 100 iterations by the generator to try and find a bug. Kiro defaults to this but you can raise or lower this value depending on what level of confidence youâre looking for in your program. Sometimes you want more, but it also might be the case that an implementation takes a bit of time to test and therefore the performance of running 100 or more input tests isnât valuable yet in that phase of your development lifecycle. The good thing is you can always raise or lower this as necessary.&lt;/p&gt;
    &lt;head rend="h3"&gt;The fix&lt;/head&gt;
    &lt;p&gt;Kiro implemented two defensive measures based on MITRE's highly effective mitigation strategies:&lt;/p&gt;
    &lt;p&gt;1. Safe Storage (in &lt;code&gt;saveApiKey&lt;/code&gt;):&lt;/p&gt;
    &lt;p&gt;Objects created with &lt;code&gt;Object.create(null)&lt;/code&gt; have no prototype chain, so &lt;code&gt;__proto__&lt;/code&gt; becomes just a regular property.&lt;/p&gt;
    &lt;p&gt;2. Safe Retrieval (in loadApiKey):&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bigger Picture&lt;/head&gt;
    &lt;p&gt;This story illustrates why Kiro uses property-based testing as part of SDD:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Properties connect directly to requirements - The property "for any provider name, round-trip should work" is a direct translation of the requirement. When the property passes, we have evidence the requirement is satisfied.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Random generation finds unexpected edge cases - Humans and LLMs have biases about what inputs to test. Random generation explores the space more thoroughly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Executable specifications - Properties are specifications you can run. They bridge the gap between "what should the code do" (requirements) and "does the code actually do it" (tests).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tight feedback loops - When a property fails, you get a minimal counterexample that makes debugging easier. Kiro can use this to fix the code, creating a rapid iteration cycle.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This bug was found during real development with Kiro. The property-based test caught a security weakness that would have been very difficult to find through:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Manual code review&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Traditional unit tests with hand-picked examples&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Integration testing&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kiro.dev/blog/property-based-testing-fixed-security-bug/"/><published>2025-12-18T23:40:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321350</id><title>SMB Direct – SMB3 over RDMA – The Linux Kernel Documentation</title><updated>2025-12-19T07:39:16.141265+00:00</updated><content>&lt;doc fingerprint="7e33911cd9023940"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SMB Direct - SMB3 over RDMA¶&lt;/head&gt;
    &lt;p&gt;This document describes how to set up the Linux SMB client and server to use RDMA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overview¶&lt;/head&gt;
    &lt;p&gt;The Linux SMB kernel client supports SMB Direct, which is a transport scheme for SMB3 that uses RDMA (Remote Direct Memory Access) to provide high throughput and low latencies by bypassing the traditional TCP/IP stack. SMB Direct on the Linux SMB client can be tested against KSMBD - a kernel-space SMB server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installation¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Install an RDMA device. As long as the RDMA device driver is supported by the kernel, it should work. This includes both software emulators (soft RoCE, soft iWARP) and hardware devices (InfiniBand, RoCE, iWARP).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install a kernel with SMB Direct support. The first kernel release to support SMB Direct on both the client and server side is 5.15. Therefore, a distribution compatible with kernel 5.15 or later is required.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install cifs-utils, which provides the mount.cifs command to mount SMB shares.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Configure the RDMA stack&lt;/p&gt;
        &lt;p&gt;Make sure that your kernel configuration has RDMA support enabled. Under Device Drivers -&amp;gt; Infiniband support, update the kernel configuration to enable Infiniband support.&lt;/p&gt;
        &lt;p&gt;Enable the appropriate IB HCA support or iWARP adapter support, depending on your hardware.&lt;/p&gt;
        &lt;p&gt;If you are using InfiniBand, enable IP-over-InfiniBand support.&lt;/p&gt;
        &lt;p&gt;For soft RDMA, enable either the soft iWARP (RDMA _SIW) or soft RoCE (RDMA_RXE) module. Install the iproute2 package and use the rdma link add command to load the module and create an RDMA interface.&lt;/p&gt;
        &lt;p&gt;e.g. if your local ethernet interface is eth0, you can use:&lt;/p&gt;
        &lt;quote&gt;sudo rdma link add siw0 type siw netdev eth0&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable SMB Direct support for both the server and the client in the kernel configuration.&lt;/p&gt;
        &lt;p&gt;Server Setup&lt;/p&gt;
        &lt;quote&gt;Network File Systems ---&amp;gt; &amp;lt;M&amp;gt; SMB3 server support [*] Support for SMB Direct protocol&lt;/quote&gt;
        &lt;p&gt;Client Setup&lt;/p&gt;
        &lt;quote&gt;Network File Systems ---&amp;gt; &amp;lt;M&amp;gt; SMB3 and CIFS support (advanced network filesystem) [*] SMB Direct support&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Build and install the kernel. SMB Direct support will be enabled in the cifs.ko and ksmbd.ko modules.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Setup and Usage¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Set up and start a KSMBD server as described in the KSMBD documentation. Also add the “server multi channel support = yes” parameter to ksmbd.conf.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On the client, mount the share with rdma mount option to use SMB Direct (specify a SMB version 3.0 or higher using vers).&lt;/p&gt;
        &lt;p&gt;For example:&lt;/p&gt;
        &lt;quote&gt;mount -t cifs //server/share /mnt/point -o vers=3.1.1,rdma&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To verify that the mount is using SMB Direct, you can check dmesg for the following log line after mounting:&lt;/p&gt;
        &lt;quote&gt;CIFS: VFS: RDMA transport established&lt;/quote&gt;
        &lt;p&gt;Or, verify rdma mount option for the share in /proc/mounts:&lt;/p&gt;
        &lt;quote&gt;cat /proc/mounts | grep cifs&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.kernel.org/filesystems/smb/smbdirect.html"/><published>2025-12-19T01:42:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321619</id><title>Noclip.website – A digital museum of video game levels</title><updated>2025-12-19T07:39:15.741553+00:00</updated><link href="https://noclip.website/"/><published>2025-12-19T02:20:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321982</id><title>Reconstructed Commander Keen 1-3 Source Code</title><updated>2025-12-19T07:39:15.182197+00:00</updated><content>&lt;doc fingerprint="ae21f342248b3d17"&gt;
  &lt;main&gt;
    &lt;p&gt;It's not quite Christmas time yet, but since these are the days of Keen's 35th anniversary, I thought it was a good opportunity to finally release this to the public:&lt;lb/&gt; Reconstructed Commander Keen 1-3 Source Code &lt;lb/&gt; This package contains full source code for all versions of Keen 1-3 that I know, from the November 1990 beta version of Keen 1 to the relatively obscure Precision Software Applications release (version 1.34). Compiling the source code with the correct compiler and assembler versions and compressing the resulting executables with LZEXE or PKLITE (if necessary) generates executables that are 100% identical to the original files.&lt;lb/&gt; You will need Turbo C++ 1.00 (not 1.01!) and Turbo Assembler (2.0 or above) to compile exact copies of most versions of Keen 1-3 (versions 1.31 and earlier). The later special releases (the "pre-registered" Gravis version 1.32 and the PSA release version 1.34) require Borland C++ 2.0 for a fully identical copy.&lt;lb/&gt; ---&lt;lb/&gt; If you have read the timeline.txt file I released along with my Reconstructed BioMenace Source Code last year, you might have noticed an entry that said "got side-tracked revisiting another old project". This is that project.&lt;lb/&gt; Most of the Keen 1-3 code reconstruction was done back in early 2021, before I started preparing my reconstructed Keen 4-6 source code for release. The big problem that prevented me from getting my Keen 1-3 code to compile and compress into 100% identical copies of the original executables was the fact that I didn't have access to the right compiler back then. I only had Turbo C++ 1.01, which generates slightly different code than Turbo C++ 1.00. It was impossible to get Turbo C++ 1.01 to produce the code that I needed.&lt;lb/&gt; Another contributing factor is that the order of the uninitialized variables in the so-called BSS segment depends on the names of the variables when using Turbo C++ 1.x to compile the code. And since some of these variables need to be accessed from within the assembly code, I couldn't simply group the variables into a struct like I did elsewhere. I had to find names that would allow the variables to appear in the correct order. So after some trial and error, I wrote a small throw-away program that generated a bunch of variables with random names and ran that list of variables through the Turbo C++ compiler. Disassembling the generated .OBJ file showed me which order these variable names would produce, so all I had left to do was to let the compiler (and the assembler) rename the variables internally. This is done in the BSSCHEAT.H and BSSCHEAT.EQU files.&lt;lb/&gt; A word of advice for those attempting to use a similar trick to modify the order of the variables in the BSS segment: If two names produce the same hash value for whatever hash function the compiler uses, the order of the variable declarations can have an effect on the order of the variables in the generated code. This includes "extern" declarations as well, not just the actual declaration of the variable. One way to detect hash collisions in a list of randomly generated variable names would be to reverse the order of the variables and compile the file again. If the order of certain names changes after that, those names have the same hash values and you are probably better off using only one of those names to avoid having to move (extern) declarations around to get the result you want.&lt;lb/&gt; ---&lt;lb/&gt; If you check out IDLIB.C and IDLIB.ASM, you will see that I based the code in these files on The Catacomb and Hovertank. This is the code that got the team in trouble. They used the same routines they wrote for their day jobs at Softdisk in the Keen code. No matter what those old letters posted by Scott Miller said, the boys were in serious trouble.&lt;lb/&gt; Most of the IDLIB.C code must have come directly from the PC version of Dangerous Dave. I don't think the Dave source code has been released to the public, so you'll just have to take my word for it. But there is some extremely strong evidence showing that the id founders used Softdisk's code in their own game. Sure, it's not the code responsible for the smooth scrolling, but it is code they probably didn't have the rights to use.&lt;lb/&gt; ---&lt;lb/&gt; That's all for now. Have fun messing with the code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;head rend="h3"&gt;Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;Keenest person of the year 2024. Hail to the Keen, baby!&lt;lb/&gt;http://k1n9duk3.shikadi.net&lt;/p&gt;
    &lt;p&gt;http://k1n9duk3.shikadi.net&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Multimania&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Vortininja&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Posts: 92&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Joined: Sat Nov 10, 2007 8:10&lt;/item&gt;
      &lt;item rend="dd-4"&gt;Location: Hiding in a small, cramped corner of the BwB megarocket.&lt;/item&gt;
      &lt;item rend="dd-5"&gt;Contact:&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;Very cool: I just built a copy of Keen 1 (albeit with Borland C++ 3.0, which was what I had lying around), and despite the issues you mentioned, managed to play it through to completion.&lt;lb/&gt;It's really neat to see the differences between versions, particularly with the beta version. Having the Makefiles is also really convenient for me.&lt;lb/&gt;Definitely looking forward to playing with it some more, maybe trying some proper source modding or something…&lt;/p&gt;
    &lt;p&gt;It's really neat to see the differences between versions, particularly with the beta version. Having the Makefiles is also really convenient for me.&lt;/p&gt;
    &lt;p&gt;Definitely looking forward to playing with it some more, maybe trying some proper source modding or something…&lt;/p&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;quote&gt;Presumably the differences would only matter if you were to attempt patching the executableMultimania wrote: Tue Dec 16, 2025 9:07 Very cool: I just built a copy of Keen 1 (albeit with Borland C++ 3.0, which was what I had lying around), and despite the issues you mentioned, managed to play it through to completion.&lt;/quote&gt;
    &lt;p&gt;A strange spirit has taken residence within the Temple of Jaral.&lt;/p&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;quote&gt;No. The code has bugs that can cause serious problems if the variables are stored in a different order. Borland C++ 3.0 stores the variables in a different order than the older versions.Nospike wrote: Tue Dec 16, 2025 13:57 Presumably the differences would only matter if you were to attempt patching the executable&lt;/quote&gt;
    &lt;p&gt;Keenest person of the year 2024. Hail to the Keen, baby!&lt;lb/&gt;http://k1n9duk3.shikadi.net&lt;/p&gt;
    &lt;p&gt;http://k1n9duk3.shikadi.net&lt;/p&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;This is really neat!&lt;lb/&gt;The Gamer's Edge Sampler disk contains a tiny bit of Keen 1 source code: This reconstruction solved it differently in SNDSCK1.H: &lt;/p&gt;
    &lt;p&gt;The Gamer's Edge Sampler disk contains a tiny bit of Keen 1 source code:&lt;/p&gt;
    &lt;p&gt;Code: Select all&lt;/p&gt;
    &lt;code&gt;define KEENWLK2SND 30
#define YORPBOPSND  31
#define GETCARDSND  32
#define DOOROPENSND 33
#define YORPSCREAMSND 34
#define GARGSCREAMSND 35
#define GUNCLICKSND 36
#define SHOTHITSND 37
#define TANKFIRESND 38
#define VORTSCREAMSND 39
#define KEENSICLESND 40
#define KEENSLEFTSND 41
&lt;/code&gt;
    &lt;p&gt;Code: Select all&lt;/p&gt;
    &lt;code&gt;typedef enum
{	
	SND_WLDWALK = 1,
	SND_WLDBLOCK,
	...
	SND_KEENWALK2,
	SND_YORPBOP,
	SND_GETCARD,
	SND_DOOROPEN,
	SND_YORPSCREAM,
	SND_GARGSCREAM,
	SND_GUNCLICK,
	SND_SHOTHIT,
	SND_TANKFIRE,
	SND_VORTSCREAM,
	SND_KEENCICLE,
	SND_KEENSLEFT,
	NUMSOUNDS
} soundnames;
&lt;/code&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;Haven't gotten to building any executable, but as usual, that is an important milestone.&lt;lb/&gt;I've been in contact with K1n9_Duk3 recently, mostly with relation to BioMenace Remastered testing. I've also been aware of the plan to update the reconstructed BioMenace sources, with my feedbacks having their impacts. That said, I haven't seen a hint of the work on Keen 1-3.&lt;lb/&gt;I had figured out beforehand that Hovertank technically resides in-between Keen 1-3 and Keen Dreams. In particular, I recall Fleexy finding out that Hovertank uses (a variation of) Keen Dreams' EGAGRAPH format, while the levels are rather still formatted as in Keen 1-3. I had probably seen a hint or two in an interrupt handler related to timing and/or PC Speaker sound playback.&lt;lb/&gt;It makes sense at least a subset of Dangerous Dave (1990) code was reused. What I do remember is Catacomb II (i.e., The Catacomb) having a control panel looking like Dave's, more-or-less. This code seems to be under Catacomb II's PCRLIB_C.C. One clear difference from Dave is that Catacomb II lacks separate VGA graphics.&lt;lb/&gt;Hovertank 3D had a derived IDLIBC.C file. My hunch is that the reconstructed sources' Keen 1-3 reside in-between these two files.&lt;lb/&gt;There's also assembly code, Catacomb II's PCRLIB_A.ASM and Hovertank 3D's IDASM.ASM. Keen 1-3's IDASM.ASM seems to reside in-between. It's probably closer to Hovertank 3D's, albeit Hovertank had a lot of code removed from that file.&lt;lb/&gt;On a related note, if anyone was wondering about Wolfenstein 3D's main menu function from WL_MENU.C being named US_ControlPanel:&lt;lb/&gt;- It should be clear when such a function is found in the Catacomb 3-D sources' ID_US_2.C.&lt;lb/&gt;- The name "Control Panel" probably makes more sense in the context of Keen Dreams, as what it has does look more like a control panel. Keen Dreams has a function of the same name in ID_US.C, preceding the replacement with the separate ID_US_1.C and ID_US_2.C files.&lt;lb/&gt;- We may see here, though, that usage of the terms "Control Panel" seems to date back to early Catacomb and Dangerous Dave games.&lt;/p&gt;
    &lt;p&gt;I've been in contact with K1n9_Duk3 recently, mostly with relation to BioMenace Remastered testing. I've also been aware of the plan to update the reconstructed BioMenace sources, with my feedbacks having their impacts. That said, I haven't seen a hint of the work on Keen 1-3.&lt;/p&gt;
    &lt;p&gt;I had figured out beforehand that Hovertank technically resides in-between Keen 1-3 and Keen Dreams. In particular, I recall Fleexy finding out that Hovertank uses (a variation of) Keen Dreams' EGAGRAPH format, while the levels are rather still formatted as in Keen 1-3. I had probably seen a hint or two in an interrupt handler related to timing and/or PC Speaker sound playback.&lt;/p&gt;
    &lt;p&gt;It makes sense at least a subset of Dangerous Dave (1990) code was reused. What I do remember is Catacomb II (i.e., The Catacomb) having a control panel looking like Dave's, more-or-less. This code seems to be under Catacomb II's PCRLIB_C.C. One clear difference from Dave is that Catacomb II lacks separate VGA graphics.&lt;/p&gt;
    &lt;p&gt;Hovertank 3D had a derived IDLIBC.C file. My hunch is that the reconstructed sources' Keen 1-3 reside in-between these two files.&lt;/p&gt;
    &lt;p&gt;There's also assembly code, Catacomb II's PCRLIB_A.ASM and Hovertank 3D's IDASM.ASM. Keen 1-3's IDASM.ASM seems to reside in-between. It's probably closer to Hovertank 3D's, albeit Hovertank had a lot of code removed from that file.&lt;/p&gt;
    &lt;p&gt;On a related note, if anyone was wondering about Wolfenstein 3D's main menu function from WL_MENU.C being named US_ControlPanel:&lt;/p&gt;
    &lt;p&gt;- It should be clear when such a function is found in the Catacomb 3-D sources' ID_US_2.C.&lt;/p&gt;
    &lt;p&gt;- The name "Control Panel" probably makes more sense in the context of Keen Dreams, as what it has does look more like a control panel. Keen Dreams has a function of the same name in ID_US.C, preceding the replacement with the separate ID_US_1.C and ID_US_2.C files.&lt;/p&gt;
    &lt;p&gt;- We may see here, though, that usage of the terms "Control Panel" seems to date back to early Catacomb and Dangerous Dave games.&lt;/p&gt;
    &lt;p&gt;Website: https://ny.duke4.net/&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt; FribbulusXaxMan&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Grunt&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Posts: 10&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Joined: Wed Feb 10, 2021 17:48&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;quote&gt;Do you intend to upload this to Github?K1n9_Duk3 wrote: Mon Dec 15, 2025 6:48 It's not quite Christmas time yet, but since these are the days of Keen's 35th anniversary, I thought it was a good opportunity to finally release this to the public:&lt;lb/&gt;Reconstructed Commander Keen 1-3 Source Code&lt;lb/&gt;This package contains full source code for all versions of Keen 1-3 that I know, from the November 1990 beta version of Keen 1 to the relatively obscure Precision Software Applications release (version 1.34). Compiling the source code with the correct compiler and assembler versions and compressing the resulting executables with LZEXE or PKLITE (if necessary) generates executables that are 100% identical to the original files.&lt;lb/&gt;You will need Turbo C++ 1.00 (not 1.01!) and Turbo Assembler (2.0 or above) to compile exact copies of most versions of Keen 1-3 (versions 1.31 and earlier). The later special releases (the "pre-registered" Gravis version 1.32 and the PSA release version 1.34) require Borland C++ 2.0 for a fully identical copy.&lt;lb/&gt;---&lt;lb/&gt;If you have read the timeline.txt file I released along with my Reconstructed BioMenace Source Code last year, you might have noticed an entry that said "got side-tracked revisiting another old project". This is that project.&lt;lb/&gt;Most of the Keen 1-3 code reconstruction was done back in early 2021, before I started preparing my reconstructed Keen 4-6 source code for release. The big problem that prevented me from getting my Keen 1-3 code to compile and compress into 100% identical copies of the original executables was the fact that I didn't have access to the right compiler back then. I only had Turbo C++ 1.01, which generates slightly different code than Turbo C++ 1.00. It was impossible to get Turbo C++ 1.01 to produce the code that I needed.&lt;lb/&gt;Another contributing factor is that the order of the uninitialized variables in the so-called BSS segment depends on the names of the variables when using Turbo C++ 1.x to compile the code. And since some of these variables need to be accessed from within the assembly code, I couldn't simply group the variables into a struct like I did elsewhere. I had to find names that would allow the variables to appear in the correct order. So after some trial and error, I wrote a small throw-away program that generated a bunch of variables with random names and ran that list of variables through the Turbo C++ compiler. Disassembling the generated .OBJ file showed me which order these variable names would produce, so all I had left to do was to let the compiler (and the assembler) rename the variables internally. This is done in the BSSCHEAT.H and BSSCHEAT.EQU files.&lt;lb/&gt;A word of advice for those attempting to use a similar trick to modify the order of the variables in the BSS segment: If two names produce the same hash value for whatever hash function the compiler uses, the order of the variable declarations can have an effect on the order of the variables in the generated code. This includes "extern" declarations as well, not just the actual declaration of the variable. One way to detect hash collisions in a list of randomly generated variable names would be to reverse the order of the variables and compile the file again. If the order of certain names changes after that, those names have the same hash values and you are probably better off using only one of those names to avoid having to move (extern) declarations around to get the result you want.&lt;lb/&gt;---&lt;lb/&gt;If you check out IDLIB.C and IDLIB.ASM, you will see that I based the code in these files on The Catacomb and Hovertank. This is the code that got the team in trouble. They used the same routines they wrote for their day jobs at Softdisk in the Keen code. No matter what those old letters posted by Scott Miller said, the boys were in serious trouble.&lt;lb/&gt;Most of the IDLIB.C code must have come directly from the PC version of Dangerous Dave. I don't think the Dave source code has been released to the public, so you'll just have to take my word for it. But there is some extremely strong evidence showing that the id founders used Softdisk's code in their own game. Sure, it's not the code responsible for the smooth scrolling, but it is code they probably didn't have the rights to use.&lt;lb/&gt;---&lt;lb/&gt;That's all for now. Have fun messing with the code.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;Oh wow, so now a source code for all keen games is known. This is a major milestone. Sadly, I don't enough of C to do much with this. Though I feel like the keenxAct.c files would the main target for "patching". Does this actually eliminate the limits on keen vorticon mods, or is that due to limits in DOS?&lt;/p&gt;
    &lt;p&gt;nothing usefull here&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pckf.com/viewtopic.php?t=18248"/><published>2025-12-19T03:26:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46322540</id><title>Getting bitten by Intel's poor naming scenes</title><updated>2025-12-19T07:39:15.009608+00:00</updated><content>&lt;doc fingerprint="7fd031aad8391bcb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Getting Bitten by Poor Naming Schemes&lt;/head&gt;Don't you love misleading documentation?&lt;p&gt;I recently came into possession of an old Dell Precision T3610 workstation and promptly installed Proxmox to add it to my Proxmox cluster. After performing some ludicrously silly RAM and storage upgrades (how about 96 GB of DDR3, plus a 13-disk array of 500 GB SSDs?), I decided I wanted to max out the CPU as well.&lt;/p&gt;&lt;p&gt;The Precision T3610 shipped with an Intel Xeon E5-1650 v2. According to the linked Intel product page, this CPU uses the FCLGA2011 socket. Easy enough, I thought to myself. Just find the best CPU that supports FCLGA2011, make sure you have the latest BIOS installed, and everything should be all hunky dory. So I did some research and landed on the Xeon E7-8890 v4. It’s several years newer than the E5-1650 v2, has a whopping 24 cores (and hyperthreading bumps it to 48 logical cores!), and can support having not one, not two, but eight of itself installed in a single motherboard! Most crucially, the Intel product page says it uses the FCLGA2011 socket. When I stumbled across one of these monsters on eBay for just $15, I snapped it up.&lt;/p&gt;&lt;p&gt;Cue my massive shock and disappointment when, a few days later, I found myself unable to install the E7-8890 v4 in my T3610. The new CPU, despite being the same physical size as the old CPU, had extra contacts on the bottom and had a different physical keying. What? I thought Intel said this was the same socket!&lt;/p&gt;&lt;p&gt;Some amount of research later, I discovered that Intel’s LGA2011 socket has many variations. One of these variations is also called Socket R (or LGA2011-0). The T3610, and by extension the old E5-1650 v2 CPU, uses Socket R. The newer E7-8890 v4, meanwhile, uses a different variation called Socket R2 (or LGA2011-1). As if this wasn’t confusing enough, there’s even a third variation of the LGA2011 socket! I’ll refer you to the Wikipedia page for more info on that.&lt;/p&gt;&lt;p&gt;This is obviously not a great naming scheme. Why not use unique numbers for each version of the socket instead of tacking on a suffix? But the real kicker here is that Intel itself doesn’t seem to be able to keep up with its own naming scheme! It appears that its CPU specifications pages refer to all variants of the LGA2011 socket as FCLGA2011. This leaves folks like myself wondering what went wrong when their new-to-them CPUs don’t fit in their motherboards.&lt;/p&gt;&lt;p&gt;So where does that leave me? Well, I now have a fancy paperweight. I could have returned the CPU, but return shipping costs would have been half of what I paid for the CPU itself, so I’m hanging onto it for now in case I ever come into possession of a server with a Socket R2 motherboard that could use a nicer CPU. At least it wasn’t a super expensive CPU, so all in all, this isn’t the worst learning experience ever.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lorendb.dev/posts/getting-bitten-by-poor-naming-schemes/"/><published>2025-12-19T05:35:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46322556</id><title>2026 Apple introducing more ads to increase opportunity in search results</title><updated>2025-12-19T07:39:14.464707+00:00</updated><content>&lt;doc fingerprint="1d48bbb303a6a622"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Search results&lt;/head&gt;
    &lt;head rend="h2"&gt;Reach users the moment they’re searching for apps &lt;lb/&gt;to download&lt;/head&gt;
    &lt;p&gt;Search results ads help customers discover your app right when they’re searching for apps to download. When a user searches on the App Store, your ad can appear at the top of their search results. And starting in 2026, we’ll be introducing more ads to increase opportunity in search results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Available in 2026: Additional opportunities in &lt;lb/&gt;search results&lt;/head&gt;
    &lt;p&gt;Search is the way most people find and download apps on the App Store, with nearly 65 percent of downloads happening directly after a search.1 To help give advertisers more opportunities to drive downloads from search results, Apple Ads will introduce additional ads across search queries. You don’t need to change your campaign in order to be eligible for any new positions. Your ad will run in either the existing position — at the top of search results — or further down in search results. If you have a search results campaign running, your ad will be automatically eligible for all available positions, but you can’t select or bid for a particular one. &lt;lb/&gt; The ad format will be the same in any position, using a default product page or custom product page, and an optional deep link. You’ll be billed as usual based on your pricing model: cost per tap or cost per install.&lt;/p&gt;
    &lt;head rend="h3"&gt;How ads are created&lt;/head&gt;
    &lt;p&gt;A default ad is created based on your app’s product page, or you can create ad variations from custom product pages you set up in App Store Connect. Ad variations allow you to align ad creative with specific audiences and keyword themes. You can also select a custom product page with a deep link that directs users to the exact place you want in your app. Deep links for search results placements are available on devices running iOS or iPadOS 18 and later.&lt;/p&gt;
    &lt;head rend="h3"&gt;How ads are matched to &lt;lb/&gt;search queries&lt;/head&gt;
    &lt;p&gt;To match your ad to relevant searches, search results campaigns use keywords. You can either choose your own keywords or use the ones we suggest. Our intelligent technology matches the user’s search term with the app being promoted, delivering more than a 60 percent average conversion rate for ads at the top of search results.2 &lt;lb/&gt; Whether a search results ad displays over other advertisers bidding on that same query is determined by a combination of factors, including your app’s relevance to the search query and the amount of your keyword bid. If your app isn’t relevant to what the user is searching for, it won’t be displayed — regardless of how much you may be willing to pay. Apple Ads considers both relevance and bids, and doesn’t put apps into auctions if they’re not a good match.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ads.apple.com/app-store/help/ad-placements/0082-search-results"/><published>2025-12-19T05:38:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46322732</id><title>Making Google Sans Flex</title><updated>2025-12-19T07:39:13.976281+00:00</updated><content>&lt;doc fingerprint="31a2b0f091f621e1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making Google Sans Flex&lt;/head&gt;
    &lt;p&gt;How seven design problems shaped Google’s iconic typeface — from inception to going open-source&lt;/p&gt;
    &lt;p&gt;Sometimes, design elements are birthed in flashes of inspiration — What if our interfaces were like paper? What if YouTube was a little more pink? — but not Google’s brand typeface.&lt;/p&gt;
    &lt;p&gt;Google Sans is the iconic typeface used across every Google product from Search to Wallet. It’s one of the most-served fonts on the internet, clocking in at some 120 billion font requests a month. It wasn’t born from a single brilliant sprint or creative spark. It evolved as the answer to a set of specific design problems, an answer that continued to expand and adapt to meet the shifting needs of users, designers, and developers. Now, after nearly a decade on Pixel phones and in Google apps, Google Sans is making its next big move: going open-source.&lt;/p&gt;
    &lt;p&gt;Read on for a brief history of Google’s beloved font, told through the design problems that shaped it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Product lockups didn’t match the new Google logo&lt;/head&gt;
    &lt;p&gt;The 2015 logo redesign was a smashing success. But even before its pixels were perfected, the team realized their work was only just beginning. Hundreds of product lockups — the fixed arrangement of the logo paired with each product name — had to be updated to match.&lt;/p&gt;
    &lt;p&gt;“The majority of how people experience the Google brand is through typography, whether that’s in a product, marketing, or content. We had the Google logo and identity framework, but we were missing that connective tissue,” explains Ken Frederick, former UX Lead for Google’s Brand Studio.&lt;/p&gt;
    &lt;p&gt;Designers considered treating each lockup as a unique logo, but the sheer volume of work made this approach unscalable. So, a new typeface was born based on the clean geometric forms of the new logo: Product Sans. The font’s repeating geometric shapes and tightly spaced characters made it perfectly suited for big product names at big sizes. Many Google apps still use these lockups today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Product Sans didn’t work in marketing or user interfaces&lt;/head&gt;
    &lt;p&gt;If a designer sees a gorgeous typeface, they’ll want to use it. Team members in Marketing loved the look of Product Sans and asked for it to be expanded for use in advertising. But in practice, the typeface fell short when you only had a couple of seconds to catch consumers’ eyes. At the same time, product designers started playing around with using Product Sans in interfaces. The results? Not great. Product Sans wasn’t optimal for lengthy passages of text, or at the smaller text sizes used on phones and tablets. Google’s designers needed a more versatile brand typeface. Enter: Google Sans.&lt;/p&gt;
    &lt;p&gt;“It wasn't a top-down decision,” notes Tobias Kunisch, Design Lead for Google Fonts. “Google Sans was driven by the needs of designers, with product teams showing what was needed. That’s how our brand font really came together.”&lt;/p&gt;
    &lt;p&gt;The Google Fonts team learned from their experience with Product Sans and asked Colophon Foundry to meticulously optimize the character shapes, terminals, ascenders, descenders, x-heights, and stroke contrasts of the new typeface, ensuring that Google Sans would perform beautifully in large display text across both marketing and product surfaces. This is the iconic Google typeface that you see everywhere from billboards to UI headlines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google Sans wasn’t legible enough at smaller sizes&lt;/head&gt;
    &lt;p&gt;When Google Sans rolled out in 2018, designers were initially ecstatic. “It meant we could finally use typography for a stronger brand statement,” recalls UX Designer Miche Alvarez. “But it also created a dual-font system using Google Sans for larger display text and Roboto for smaller text. It ended up being a compromise.”&lt;/p&gt;
    &lt;p&gt;Designers fought for a Google Sans version that would work at smaller sizes. So the team collaborated with designers in Search and at Colophon Foundry to refine the font, ultimately launching Google Sans Text (GST) in 2020.&lt;/p&gt;
    &lt;p&gt;Unlike the geometric Google Sans, the characters in Google Sans Text are taller, more condensed, and less circular. The typeface also includes more spacing between characters to aid readability. The numerals are less geometric, and the angled cuts on terminals are less severe, creating a more uniform and readable experience, even at small sizes. It was also designed to match the proportions of Roboto, Android’s default typeface, to make switching from Roboto a smoother process.&lt;/p&gt;
    &lt;p&gt;Google Sans Text rolled out on the Pixel 3, creating a more unified typographic experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google Sans was just for European languages&lt;/head&gt;
    &lt;p&gt;Product teams eagerly adopted Google Sans and Google Sans Text but soon highlighted a new issue: Billions of people around the globe use non-Latin scripts like Arabic, Chinese, and Thai — writing systems not included in the initial character sets of Google Sans.&lt;/p&gt;
    &lt;p&gt;“Our mission is ‘to organize the world's information,’” explains Dave Crossland, the Lead UX Program Manager for Google Fonts. “There was a clear need to expand Google Sans to all the languages that Google’s products get localized into.”&lt;/p&gt;
    &lt;p&gt;The effort was monumental. It meant meticulously crafting hundreds of thousands of new glyphs across more than 20 additional writing systems, each with its own unique design principles and visual nuances. The undertaking involved collaborating with expert type foundries worldwide, specialists familiar with the intricate requirements of each script — from the flowing curves of Arabic to the complex strokes of Japanese and the distinct forms of Ge’ez, the script used for Ethiopian and other languages in that region.&lt;/p&gt;
    &lt;p&gt;All together, this massive global language support makes Google Sans one of the world’s largest typeface families.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google Sans Mono wasn’t great for coding&lt;/head&gt;
    &lt;p&gt;Google Sans Mono was created in 2020 to support contexts that needed fixed-width characters for editorial design, at medium and large text sizes. Despite this, it soon got its first big product integration, replacing Roboto Mono in Google Chat. The only problem? Developers hated it.&lt;/p&gt;
    &lt;p&gt;Because the font wasn’t intended to be used at smaller sizes, the letterforms weren’t legible. It was especially difficult to tell the a and o apart. “That can actually be catastrophic for code,” says Tobias Kunisch. “One misplaced character can break an entire program.”&lt;/p&gt;
    &lt;p&gt;Recognizing this critical need, a dedicated effort was launched to craft Google Sans Code, a monospaced typeface specifically designed to make code more readable. This involved thorough research into the 20 most common programming languages and how developers interact with code, aiming to make the new coding typeface more visually appealing while reducing the ambiguity of similar-looking letterforms. Based on these insights, Google tasked the Universal Thirst foundry to meticulously focus on specific letters, numbers, and operators to meet these requirements. The result is an eminently readable and surprisingly playful typeface.&lt;/p&gt;
    &lt;p&gt;Google Sans Code launched as an open-source font in 2025, and is the typeface used to display code in Gemini.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google Sans wasn’t flexible enough for expression&lt;/head&gt;
    &lt;p&gt;As interfaces grow more dynamic and personal, our designers need typography that does more than just present information; the type should also convey feeling and adapt to varying contexts and states. While Google Sans was clear and effective, it didn’t offer the nuanced expressive range required to truly match a product’s mood or a user’s preference.&lt;/p&gt;
    &lt;p&gt;This led us to collaborate with the pioneers in variable font technology at Font Bureau to create Google Sans Flex. Unlike traditional fonts that have a handful of fixed styles like bold and italic, Google Sans Flex offers granular control over six different design axes: weight, width, optical size, slant, grade, and roundedness.&lt;/p&gt;
    &lt;p&gt;“Google Sans Flex is a power tool for expression,” says Google Fonts Product Manager Sophia Siao. “Different combinations of axes can unlock a huge range of feeling and emotion, all while preserving a cohesive reading experience.”&lt;/p&gt;
    &lt;p&gt;Google Sans Flex lets designers “sculpt” UI text with remarkable precision. Imagine making text feel “calm as a whisper” or “loud and rugged” simply by adjusting its weight, or evoking a “personal, playful” tone by fine-tuning its roundness. Our designers were particularly excited by the ability to precisely adjust how rounded or soft the text appears; these subtle shifts deeply influence how readers experience and connect with a design.&lt;/p&gt;
    &lt;p&gt;This expressive power isn’t just about aesthetics; it directly impacts how users experience Google products. Our research engaged over 3,000 readers, which taught us one thing: flexibility matters. Readers found the taller, more elegant styles to be more premium and engaging than standard fonts. This adaptability doesn't just look better; it allows designers to tailor readability for every specific use case. The optical size axis intelligently adapts letter shapes to maintain readable proportions at any size, from a smartwatch to a billboard, ensuring that expressiveness never comes at the expense of legibility. The typeface was recognized as a Red Dot Winner 2024.&lt;/p&gt;
    &lt;p&gt;Ultimately, Google Sans Flex empowers designers to infuse interfaces with distinct personality and nuance, creating digital experiences that feel more intuitive, personal, and genuinely helpful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Google Sans Flex couldn’t be used for everything&lt;/head&gt;
    &lt;p&gt;Google Sans launched as a proprietary brand typeface — meaning that it could only be used in Google products. While that control was great for protecting Google’s brand, it created a fragmented typographic experience across the digital ecosystem. You might see Google Sans in Gmail, then open WhatsApp and see Roboto or a device-specific font. It’s a persistent, if subtle, point of friction.&lt;/p&gt;
    &lt;p&gt;“It just doesn’t feel as nice,” stresses Visual Designer Megan Lynch. “On a subconscious level it impacts the experience.” This fragmented visual language, unlike the unified font experiences on some other platforms, made the digital journey less than seamless.&lt;/p&gt;
    &lt;p&gt;So this year — 2025 — Google decided to make Google Sans and Google Sans Flex open-source. It isn’t just about making a great font available; it’s about fostering a more consistent and polished digital environment for everyone. By offering Google Sans and Google Sans Flex to the wider community, we hope more developers and designers will bridge the visual gap between first-party and third-party apps. The goal is a more unified experience across devices and platforms, creating clearer, more comfortable interfaces for users wherever they engage with technology.&lt;/p&gt;
    &lt;p&gt;The story of Google Sans is a masterclass in need-based design — it was created not by a single flash of inspiration, but a thoughtful, human-centered evolution. Each problem, from misaligned lockups to illegible code, revealed an unmet need and ultimately led to innovation and improved user experience.&lt;/p&gt;
    &lt;p&gt;By making Google Sans Flex open-source, we’re extending that commitment beyond our walls. It’s an invitation to designers and developers worldwide: Use these tools, contribute to their growth, and together, let’s build clearer, more accessible, and more beautiful digital experiences for everyone. With your help, we can shape a more legible future, one character at a time.&lt;/p&gt;
    &lt;p&gt;Try Google Sans Flex, as well as our other fonts, at Google Fonts.&lt;/p&gt;
    &lt;p&gt;Special thanks to the hundreds of people who have contributed to making and evolving Google Sans, including: Akaki Razmadze, Aleksandra Samulenkova, Alex Blattmann, Alexei Vanyashin, Ali Almasri, Anagha Narayanan, Anaïs Lievens, Andy Stewart, Anonta Mon, Anthony Sheret, Anurag Gautam, Anuthin Wongsunkakon, Ashler, Ben Mitchell (Fontpad), Bianca Berning, Black Foundry, Borna Izadpanah, Botio Nikoltchev, Cadson Demak, Carmen Eva Marseille, Chris Simpkins, Colophon Foundry, Dalton Maag, Damien Correll, Daniel Grumer, Daniel Yacob, Dave Crossland, David Berlow, Denis Moyogo Jacquerye, Edd Harrington, Eduardo Rennó, Elena Peralta, Erin McLaughlin, Fiona Ross, Font Bureau, Fontef, Gor Design, Gor Jihanian, Gunjan Panchal, Gunnar Vilhjálmsson, Hanna Donker, Harry Dalton, Hilary Palmén, Hitesh Malaviya, Irene Vlachou, Jack McCabe, Jamra Patel, Jany Belluz, Jimmy Mooney, Joana Ranito, Jonathan Lee, Jonny Pinhorn, Jotika Khur-Yearn, Kalapi Gajjar, Khaled Hosny, Knaz Uiyamathiti, Lipi Raval, Mamoun Sakkal, Marc Foley, Marianna Paszkowska, Mark Jamra, Megan Lynch, Meir Sadan, Michele Patanè, Mike Guss, Mohamed Gaber, Namrata Goyal, Nance Cunningham, Natalia Qadreh, Neil Patel, Nikolaus Waxweiler, Omer Ziv, Pablo Bosch, Panuwat Usakunwathana, Pathum Egodawatta, Patrick McCormick, Pratyush Das, Riccardo De Franceschi, Richard Bailey, Ricky Atkins, Salomi Desai, Samir Souza Reis, Santiago Orozco, Simon Cozens, Sophia Siao, Sovichet Tep, Stephen Morey, Suppakit Chalermlarp, Tobias Kunisch, U+ Type, Universal Thirst, Vaishnavi Murthy, Yanek Iontef, and Ye Myat Lwin.&lt;/p&gt;
    &lt;p&gt;Shout out to Sarah Daily and Susanna Zaraysky for additional reporting. Illustration and motion by Arthur Ribeiro Vergani.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://design.google/library/google-sans-flex-font"/><published>2025-12-19T06:10:02+00:00</published></entry></feed>