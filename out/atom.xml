<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-13T10:43:47.106921+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45901904</id><title>The last-ever penny will be minted today in Philadelphia</title><updated>2025-11-13T10:43:53.736483+00:00</updated><content>&lt;doc fingerprint="4ea09c6c6896a7e7"&gt;
  &lt;main&gt;
    &lt;p&gt;The American penny passed away today after a prolonged illness. It was 238 years old.&lt;/p&gt;
    &lt;p&gt;The last penny was minted Wednesday afternoon at the US Mint in Philadelphia, overseen by US Treasurer Brandon Beach. President Donald Trump announced via social media in February that he instructed the Mint to stop making the once-popular coin, citing the cost of production.&lt;/p&gt;
    &lt;p&gt;The penny costs nearly four cents to mint, more than the coin’s worth. Once valuable enough to buy “penny candy” like gumballs and feed parking meters or toll booths, today the penny lives mostly in coin jars, junk drawers or “leave a penny/take a penny” trays.&lt;/p&gt;
    &lt;p&gt;Beach said Wednesday that the final coins pressed will be auctioned off and that the actual last pennies put into circulation from the US Mint were struck in June.&lt;/p&gt;
    &lt;p&gt;The penny outlived its sibling, the half-penny, by 168 years. It’s survived by the nickel, dime, quarter, and rarely seen half-dollar and dollar coins.&lt;/p&gt;
    &lt;p&gt;Despite its demise, the penny will remain legal tender.&lt;/p&gt;
    &lt;head rend="h2"&gt;Problems despite long planned end&lt;/head&gt;
    &lt;p&gt;For a coin that seems obsolete, its removal from circulation is causing more problems than expected, especially for retailers.&lt;/p&gt;
    &lt;p&gt;Some merchants plan to round prices to the nearest nickel, often a penny or two more. Others are asking customers to pay with pennies to help supply. But in some states, merchants could face legal trouble for rounding up or down.&lt;/p&gt;
    &lt;p&gt;Additionally, any savings from discontinuing the one-cent coin could be offset by the need to press more nickels, which costs the US Mint more money than the penny.&lt;/p&gt;
    &lt;p&gt;The government’s phasing out of the penny has been “a bit chaotic,” said Mark Weller, executive director of Americans for Common Cents. The pro-penny group is funded primarily by Artazn, the company that provides the blanks used to make pennies. “By the time we reach Christmas, the problems will be more pronounced with retailers not having pennies.”&lt;/p&gt;
    &lt;p&gt;Weller said other countries that removed low denomination coins, like Canada, Australia and Switzerland, had guidance for afterwards. Not so in the United States.&lt;/p&gt;
    &lt;p&gt;“We had a social media post (by Trump) during Super Bowl Sunday, but no real plan for what retailers would have to do,” he said, referring to the president’s February announcement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Different rounding plans&lt;/head&gt;
    &lt;p&gt;Kwik Trip, a family-owned convenience store chain that operates in the Midwest, decided to round down cash purchases in stores where it hasn’t been able to find pennies.&lt;/p&gt;
    &lt;p&gt;“There’s no way that we wanted to charge (customers) an extra 2 cents because we just didn’t think that was fair,” said John McHugh, spokesperson for the company. “I mean, it’s not their fault that there there’s a penny shortage.”&lt;/p&gt;
    &lt;p&gt;But with 20 million customers a year, and 17% of them paying with cash, the policy will eventually cost Kwik Trip a couple of million dollars a year, McHugh said.&lt;/p&gt;
    &lt;p&gt;It’s not just businesses that face increased costs. Rounding to the closest nickel will cost consumers about $6 million a year, according to a July study by the Federal Reserve Bank of Richmond. That is fairly modest, coming to about five cents each across 133 million American households.&lt;/p&gt;
    &lt;p&gt;And rounding is not a national solution.&lt;/p&gt;
    &lt;p&gt;Four states - Delaware, Connecticut, Michigan and Oregon - as well as numerous cities, including New York, Philadelphia, Miami and Washington, DC, require merchants to provide exact change, according to the National Association of Convenience Stores (NACS).&lt;/p&gt;
    &lt;p&gt;In addition, the law covering the federal food assistance program known as SNAP requires that recipients not be charged more than other customers. Since SNAP recipients use a debit card that’s charged the precise amount, if merchants round down prices for cash purchases, they could be opening themselves to legal problems and fines, said Jeff Lenard, spokesperson for NACS.&lt;/p&gt;
    &lt;p&gt;“Rounding down on all transactions presents several challenges beyond the loss of an average of 2 cents per transaction,” Lenard said. “We desperately need legislation that allows rounding so retailers can make change for these customers.”&lt;/p&gt;
    &lt;p&gt;For that reason, NACS and other retail groups recently wrote to Congress asking for legislation to deal with the questions raised by the end of penny production.&lt;/p&gt;
    &lt;head rend="h2"&gt;End of a ‘wonderful life’&lt;/head&gt;
    &lt;p&gt;The penny was one of the nation’s first coins, first minted in 1787, six years before the Mint itself was established.&lt;/p&gt;
    &lt;p&gt;Benjamin Franklin is widely credited with designing the first penny known as the Fugio cent. Its current form arrived in 1909 on the centennial of Abraham Lincoln’s birth, when it became the first American coin to feature a president.&lt;/p&gt;
    &lt;p&gt;But it has declined in both use and popularity ever since. The Treasury Department now says there are an estimated 300 billion pennies in circulation. That comes to a bit less than $9 for every American. But most of those coins are “severely underutilized.” So, outcry from the public over its demise has been muted.&lt;/p&gt;
    &lt;p&gt;Joe Ditler, a 74-year old writer and historian from Colorado, said he still has an old cigar box filled with mostly pennies given to him by his grandfather. He remembers flattening pennies on railroad tracks or souvenir machines in amusement parks.&lt;/p&gt;
    &lt;p&gt;However, he only occasionally uses pennies to make a cash purchase. And he often tosses the one-cent coin in the tip jar.&lt;/p&gt;
    &lt;p&gt;“They bring back memories that have stayed with me all my life,” he said. “The penny has had a wonderful life. But it’s probably time for it to go away.”&lt;/p&gt;
    &lt;p&gt;-CNN’s Danny Freeman contributed to this report&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnn.com/2025/11/12/business/last-penny-minted"/><published>2025-11-12T16:10:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45902273</id><title>Digital ID, a new way to create and present an ID in Apple Wallet</title><updated>2025-11-13T10:43:53.574731+00:00</updated><content>&lt;doc fingerprint="8ece4ba93b33d179"&gt;
  &lt;main&gt;
    &lt;p&gt; UPDATE November 12, 2025 &lt;/p&gt;
    &lt;head rend="h1"&gt;Apple introduces Digital ID, a new way to create and present an ID in Apple Wallet&lt;/head&gt;
    &lt;p&gt; Digital ID offers a secure and private way for users to create an ID in Apple Wallet using information from their U.S. passport, and present their ID with iPhone or Apple Watch &lt;/p&gt;
    &lt;p&gt;Apple today announced the launch of Digital ID, a new way for users to create an ID in Apple Wallet using information from their U.S. passport, and present it with the security and privacy of iPhone or Apple Watch. At launch, Digital ID acceptance will roll out first in beta at TSA checkpoints at more than 250 airports in the U.S. for in-person identity verification during domestic travel, with additional Digital ID acceptance use cases to come in the future. &lt;/p&gt;
    &lt;p&gt;Digital ID gives more people a way to create and present an ID in Apple Wallet even if they do not have a REAL ID-compliant driver’s license or state ID. Digital ID is not a replacement for a physical passport, and cannot be used for international travel and border crossing in lieu of a U.S. passport. &lt;/p&gt;
    &lt;p&gt;“With the launch of Digital ID, we’re excited to expand the ways users can store and present their identity — all with the security and privacy built into iPhone and Apple Watch,” said Jennifer Bailey, Apple’s vice president of Apple Pay and Apple Wallet. “Since introducing the ability to add a driver’s license or state ID to Apple Wallet in 2022, we’ve seen how much users love having their ID right on their devices. Digital IDs brings this secure and convenient option to even more users across the country, as they can now add an ID to Wallet using information from their U.S. passport.” &lt;/p&gt;
    &lt;p&gt;The launch follows the capability for users to add an eligible driver’s license and state ID to Apple Wallet. If users do not have a U.S. passport to create their Digital ID, they can still add an eligible driver’s license to Apple Wallet. &lt;/p&gt;
    &lt;head rend="h2"&gt;Adding Digital ID to Apple Wallet&lt;/head&gt;
    &lt;p&gt;Users can easily create and add a Digital ID to Apple Wallet using a U.S. passport. They start by tapping the Add (+) button at the top of the screen in Wallet on their iPhone and then selecting Driver’s License or ID Cards. They then select Digital ID and follow the onscreen instructions to start the setup and verification process. &lt;/p&gt;
    &lt;p&gt;Users are then asked to use their iPhone to scan the photo page of their physical passport as part of the process. They will also be asked to use their iPhone to read the chip embedded on the back of their passport to ensure the data’s authenticity. From there, they are asked to take a selfie for verification, and as another security step, they will also be prompted to complete a series of facial and head movements during the setup process. Upon verification, their Digital ID is added to Wallet. &lt;/p&gt;
    &lt;head rend="h2"&gt;Using Digital ID in Apple Wallet&lt;/head&gt;
    &lt;p&gt;To present a Digital ID in person, users can double-click the side button or Home button to access Apple Wallet and select Digital ID. From there, they can hold their iPhone or Apple Watch near an identity reader, review the specific information being requested, and use Face ID or Touch ID to authenticate. &lt;/p&gt;
    &lt;p&gt;In the future, users will be able to present their Digital ID at additional select businesses and organizations for identity and age verification in person, in apps, and online. &lt;/p&gt;
    &lt;head rend="h2"&gt;Presenting Digital ID in a Secure and Private Way&lt;/head&gt;
    &lt;p&gt;Like all IDs in Apple Wallet, Digital ID takes advantage of the privacy and security features already built into iPhone and Apple Watch to help protect against tampering and theft. Digital ID data is encrypted. When users create a Digital ID, their passport data is stored on the device. Apple cannot see when and where users present their ID, or what data was presented. Biometric authentication using Face ID or Touch ID also ensures that only the owner of the Digital ID can present it. &lt;/p&gt;
    &lt;p&gt;Only the information needed for a transaction is presented, and the user has the opportunity to review and authorize the information being requested with Face ID or Touch ID before it is shared. Users do not need to unlock, show, or hand over their device to present their ID. &lt;/p&gt;
    &lt;head rend="h2"&gt;Driver’s Licenses and State IDs in Apple Wallet Today&lt;/head&gt;
    &lt;p&gt;The introduction of Digital ID brings users another easy, secure, and private way to create, store, and present an ID in Apple Wallet. &lt;/p&gt;
    &lt;p&gt;Today, the ability to add a driver’s license or state ID to Apple Wallet is live in 12 states and Puerto Rico. In the past six months alone, the feature has come to Montana, North Dakota, and West Virginia, and launched internationally for the first time in Japan with My Number Card on iPhone. &lt;/p&gt;
    &lt;p&gt;For more information on IDs in Apple Wallet, visit learn.wallet.apple/id. &lt;/p&gt;
    &lt;p&gt;Share article&lt;/p&gt;
    &lt;head rend="h2"&gt;Media&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Text of this article&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Media in this article&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.apple.com/newsroom/2025/11/apple-introduces-digital-id-a-new-way-to-create-and-present-an-id-in-apple-wallet/"/><published>2025-11-12T16:40:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45902604</id><title>Helm 4.0</title><updated>2025-11-13T10:43:53.177761+00:00</updated><content>&lt;doc fingerprint="3612007fdbecf4d2"&gt;
  &lt;main&gt;
    &lt;p&gt;The Helm Team is proud to announce the first stable release of Helm 4.&lt;/p&gt;
    &lt;head rend="h2"&gt;New Features&lt;/head&gt;
    &lt;p&gt;Helm 4 has numerous new features, but a few deserve highlighting here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Redesigned plugin system that supports Web Assembly based plugins&lt;/item&gt;
      &lt;item&gt;Post-renderers are now plugins&lt;/item&gt;
      &lt;item&gt;Server side apply is now supported&lt;/item&gt;
      &lt;item&gt;Improved resource watching, to support waiting, based on kstatus&lt;/item&gt;
      &lt;item&gt;Local Content-based caching (e.g. for charts)&lt;/item&gt;
      &lt;item&gt;Logging via slog enabling SDK logging to integrate with modern loggers&lt;/item&gt;
      &lt;item&gt;Reproducible builds of chart archives&lt;/item&gt;
      &lt;item&gt;Updated SDK API including support for multiple chart API versions (new experimental v3 chart API version coming soon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For full release notes, please see: https://helm.sh/docs/overview/&lt;/p&gt;
    &lt;head rend="h2"&gt;Compatibility with Helm v3&lt;/head&gt;
    &lt;p&gt;Helm v4 is a major version with backward incompatible changes including to the flags and output of the Helm CLI and to the SDK.&lt;/p&gt;
    &lt;p&gt;Please evaluate the changes to your workflows. The changes are not as extensive as those from Helm v2 to v3, with the goal that the majority of workflows remain compatible between Helm v3 and v4.&lt;/p&gt;
    &lt;p&gt;Helm charts apiVersion v2 (majority of today's charts) will continue to be supported in Helm v4. Existing charts should continue to install, upgrade, and otherwise work. Please test the installation and upgrade of charts to ensure it works as expected. Changes (e.g., server side apply) may impact the experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Community&lt;/head&gt;
    &lt;p&gt;The community keeps growing, and we'd love to see you there!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Join the discussion in Kubernetes Slack: &lt;list rend="ul"&gt;&lt;item&gt;for questions and just to hang out&lt;/item&gt;&lt;item&gt;for discussing PRs, code, and bugs&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Hang out at the Public Developer Call: Thursday, 9:30 Pacific via Zoom&lt;/item&gt;
      &lt;item&gt;Test, debug, and contribute charts: ArtifactHub/packages&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Installation and Upgrading&lt;/head&gt;
    &lt;p&gt;Download Helm v4.0.0. The common platform binaries are here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MacOS amd64 (checksum / 125233cf943e6def2abc727560c5584e9083308d672d38094bae1cc3e0bfeaa2)&lt;/item&gt;
      &lt;item&gt;MacOS arm64 (checksum / 4f5d367af9e2141b047710539d22b7e5872cdaef788333396077236feb422419)&lt;/item&gt;
      &lt;item&gt;Linux amd64 (checksum / c77e9e7c1cc96e066bd240d190d1beed9a6b08060b2043ef0862c4f865eca08f)&lt;/item&gt;
      &lt;item&gt;Linux arm (checksum / 23498ff8f5fb358ad2576269cd41fa9a54b9469332806dff0d689470323180be)&lt;/item&gt;
      &lt;item&gt;Linux arm64 (checksum / 8c5c77e20cc29509d640e208a6a7d2b7e9f99bb04e5b5fbe22707b72a5235245)&lt;/item&gt;
      &lt;item&gt;Linux i386 (checksum / eda0b6508def454ba07e2f938c55f73be795e7f99552078ccc8af2c2bbd58a45)&lt;/item&gt;
      &lt;item&gt;Linux ppc64le (checksum / 73ae83e9888aafa0e9c57a1d4d77dcb6c97c253ef175a4983a8bb4bcc771d2eb)&lt;/item&gt;
      &lt;item&gt;Linux s390x (checksum / 9c7368b18c76fcae9e0281e1ee875ea0d9b5970ac3a00c4eb963205948594bad)&lt;/item&gt;
      &lt;item&gt;Linux riscv64 (checksum / a688c2559c57d6a858c49b9237b7d6bbce5c634aa5204c4342bdc8a06818b9f1)&lt;/item&gt;
      &lt;item&gt;Windows amd64 (checksum / 0f9a8c891b8d908a37fbb68f12dea92b633eb29e49070bd650f5760a1a99aa8d)&lt;/item&gt;
      &lt;item&gt;Windows arm64 (checksum / f3ff262427547cc1b1dc3356d587ed8ffaa23f2abf24bc06660a350b9b7925f9)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Quickstart Guide will get you going from there. For upgrade instructions or detailed installation notes, check the install guide. You can also use a script to install on any system with &lt;code&gt;bash&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's Next&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3.19.3 and 4.0.1 are the next patch releases and will be on December 10, 2025&lt;/item&gt;
      &lt;item&gt;3.20.0 and 4.1.0 is the next minor releases and will be on January 21, 2026&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Thank You!&lt;/head&gt;
    &lt;p&gt;The Helm project has enjoyed code contributions from many community members. Many more community members have assisted by filing issues and working with us to identify and eliminate bugs while adding new features. The #helm-users slack channel has long been a friendly and open forum for getting help and learning more about Helm. We cannot thank you enough for making this a helpful, friendly, and welcoming community for all.&lt;/p&gt;
    &lt;p&gt;❤️ The Helm Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/helm/helm/releases/tag/v4.0.0"/><published>2025-11-12T17:02:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45902898</id><title>Project Euler</title><updated>2025-11-13T10:43:52.712261+00:00</updated><content>&lt;doc fingerprint="8438e19922bbe329"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;About Project Euler&lt;/head&gt;
    &lt;head rend="h3"&gt;What is Project Euler?&lt;/head&gt;
    &lt;p&gt;Project Euler is a series of challenging mathematical/computer programming problems that will require more than just mathematical insights to solve. Although mathematics will help you arrive at elegant and efficient methods, the use of a computer and programming skills will be required to solve most problems.&lt;lb/&gt; The motivation for starting Project Euler, and its continuation, is to provide a platform for the inquiring mind to delve into unfamiliar areas and learn new concepts in a fun and recreational context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Who are the problems aimed at?&lt;/head&gt;
    &lt;p&gt;The intended audience include students for whom the basic curriculum is not feeding their hunger to learn, adults whose background was not primarily mathematics but had an interest in things mathematical, and professionals who want to keep their problem solving and mathematics on the cutting edge.&lt;/p&gt;
    &lt;p&gt;Currently we have 1365127 registered members who have solved at least one problem, representing 220 locations throughout the world, and collectively using 113 different programming languages to solve the problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can anyone solve the problems?&lt;/head&gt;
    &lt;p&gt;The problems range in difficulty and for many the experience is inductive chain learning. That is, by solving one problem it will expose you to a new concept that may allow you to undertake a previously inaccessible problem. So determined participants will be able to slowly but surely work their way through the problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;What next?&lt;/head&gt;
    &lt;p&gt;In order to track your progress it is necessary to setup an account and have Cookies enabled.&lt;/p&gt;
    &lt;p&gt;If you already have an account, then Sign In. Otherwise, please Register – it's completely free!&lt;/p&gt;
    &lt;p&gt;However, as the problems are challenging, you may wish to view the Problems before registering.&lt;/p&gt;
    &lt;p&gt;"Project Euler exists to encourage, challenge, and develop the skills and enjoyment of anyone with an interest in the fascinating world of mathematics."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://projecteuler.net"/><published>2025-11-12T17:24:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45903325</id><title>Steam Frame</title><updated>2025-11-13T10:43:52.104678+00:00</updated><content>&lt;doc fingerprint="97fc7fa59f8fbace"&gt;
  &lt;main&gt;&lt;p&gt; Install Steam &lt;/p&gt; login | language &lt;p&gt; 简体中文 (Simplified Chinese) 繁體中文 (Traditional Chinese) 日本語 (Japanese) 한국어 (Korean) ไทย (Thai) Български (Bulgarian) Čeština (Czech) Dansk (Danish) Deutsch (German) Español - España (Spanish - Spain) Español - Latinoamérica (Spanish - Latin America) Ελληνικά (Greek) Français (French) Italiano (Italian) Bahasa Indonesia (Indonesian) Magyar (Hungarian) Nederlands (Dutch) Norsk (Norwegian) Polski (Polish) Português (Portuguese - Portugal) Português - Brasil (Portuguese - Brazil) Română (Romanian) Русский (Russian) Suomi (Finnish) Svenska (Swedish) Türkçe (Turkish) Tiếng Việt (Vietnamese) Українська (Ukrainian) Report a translation problem &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://store.steampowered.com/sale/steamframe"/><published>2025-11-12T17:54:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45903404</id><title>Steam Machine</title><updated>2025-11-13T10:43:51.556431+00:00</updated><content>&lt;doc fingerprint="97fc7fa59f8fbace"&gt;
  &lt;main&gt;&lt;p&gt; Install Steam &lt;/p&gt; login | language &lt;p&gt; 简体中文 (Simplified Chinese) 繁體中文 (Traditional Chinese) 日本語 (Japanese) 한국어 (Korean) ไทย (Thai) Български (Bulgarian) Čeština (Czech) Dansk (Danish) Deutsch (German) Español - España (Spanish - Spain) Español - Latinoamérica (Spanish - Latin America) Ελληνικά (Greek) Français (French) Italiano (Italian) Bahasa Indonesia (Indonesian) Magyar (Hungarian) Nederlands (Dutch) Norsk (Norwegian) Polski (Polish) Português (Portuguese - Portugal) Português - Brasil (Portuguese - Brazil) Română (Romanian) Русский (Russian) Suomi (Finnish) Svenska (Swedish) Türkçe (Turkish) Tiếng Việt (Vietnamese) Українська (Ukrainian) Report a translation problem &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://store.steampowered.com/sale/steammachine"/><published>2025-11-12T17:59:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45904551</id><title>GPT-5.1: A smarter, more conversational ChatGPT</title><updated>2025-11-13T10:43:51.364287+00:00</updated><content>&lt;doc fingerprint="9d1fea781db6c8bf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GPT-5.1: A smarter, more conversational ChatGPT&lt;/head&gt;
    &lt;p&gt;We’re upgrading GPT‑5 while making it easier to customize ChatGPT. Starting to roll out today to everyone, beginning with paid users.&lt;/p&gt;
    &lt;p&gt;Today we’re upgrading the GPT‑5 series with the release of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GPT‑5.1 Instant: our most-used model, now warmer, more intelligent, and better at following your instructions.&lt;/item&gt;
      &lt;item&gt;GPT‑5.1 Thinking: our advanced reasoning model, now easier to understand and faster on simple tasks, more persistent on complex ones.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We heard clearly from users that great AI should not only be smart, but also enjoyable to talk to. GPT‑5.1 improves meaningfully on both intelligence and communication style.&lt;/p&gt;
    &lt;p&gt;We’re also making it easier for you to shape ChatGPT’s tone. Preferences on chat style vary—from person to person and even from conversation to conversation—so we’re introducing more intuitive and effective controls so ChatGPT can better match the tone you want in responses.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1 Instant, ChatGPT’s most used model, is now warmer by default and more conversational. Based on early testing, it often surprises people with its playfulness while remaining clear and useful.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPT-5&lt;/head&gt;
    &lt;head rend="h2"&gt;GPT-5.1 Instant&lt;/head&gt;
    &lt;p&gt;We’ve also improved instruction following, so the model more reliably answers the question you actually asked.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPT-5&lt;/head&gt;
    &lt;head rend="h2"&gt;GPT-5.1 Instant&lt;/head&gt;
    &lt;p&gt;For the first time, GPT‑5.1 Instant can use adaptive reasoning to decide when to think before responding to more challenging questions, resulting in more thorough and accurate answers, while still responding quickly. This is reflected in significant improvements on math and coding evaluations like AIME 2025 and Codeforces.&lt;/p&gt;
    &lt;p&gt;We’re also upgrading GPT‑5 Thinking to make it more efficient and easier to understand in everyday use. It now adapts its thinking time more precisely to the question—spending more time on complex problems while responding more quickly to simpler ones. In practice, that means more thorough answers for difficult requests and less waiting for simpler ones.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1 Thinking’s responses are also clearer, with less jargon and fewer undefined terms. This makes our most capable model more approachable and easily understandable, especially for complex tasks at work and explaining technical concepts.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPT-5&lt;/head&gt;
    &lt;head rend="h2"&gt;GPT-5.1 Thinking&lt;/head&gt;
    &lt;p&gt;GPT‑5.1 Thinking’s default tone is also warmer and more empathetic.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPT-5&lt;/head&gt;
    &lt;head rend="h2"&gt;GPT-5.1 Thinking&lt;/head&gt;
    &lt;p&gt;This release is a step forward in both capability and usability across the models. GPT‑5.1 Auto will continue to route each query to the model best suited for it, so in most cases, you won’t need to choose a model at all. What you will notice is that answers across GPT‑5.1 feel both smarter and more natural in tone.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1 Instant and Thinking begin rolling out today, starting with paid (Pro, Plus, Go, Business) users and then to free and logged-out users. Enterprise and Edu plans get a seven-day early-access toggle (off by default). After that window, GPT‑5.1 will become the sole default model.&lt;/p&gt;
    &lt;p&gt;If you check ChatGPT today, you may not see GPT‑5.1 available immediately. We plan to roll it out gradually over the next few days to help keep performance stable for everyone. We will also update GPT‑5 Pro to GPT‑5.1 Pro soon.&lt;/p&gt;
    &lt;p&gt;We’re bringing both GPT‑5.1 Instant and GPT‑5.1 Thinking to the API later this week. GPT‑5.1 Instant will be added as gpt-5.1-chat-latest, and GPT‑5.1 Thinking will be released as GPT‑5.1 in the API, both with adaptive reasoning.&lt;/p&gt;
    &lt;p&gt;GPT‑5 (Instant and Thinking) will remain available in ChatGPT under the legacy models dropdown for paid subscribers for three months, so people have time to compare and adapt at their own pace. The GPT‑5 sunset period does not affect the availability of other legacy models. Going forward, when we introduce new ChatGPT models, our approach is to give people ample space to evaluate what’s changed and share feedback, allowing us to continue innovating our frontier models while transitioning smoothly. Sunset periods will be communicated clearly and with plenty of advance notice.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1 is more capable and useful, and we encourage you to try it and see the difference. Our system card addendum includes more information on our safety approach for GPT‑5.1.&lt;/p&gt;
    &lt;p&gt;And a note on naming: this update is called GPT‑5.1 to reflect meaningful improvements, while remaining within the GPT‑5 generation. Future iterative upgrades to GPT‑5 will follow the same pattern.&lt;/p&gt;
    &lt;p&gt;Alongside these model improvements, we’re making it easier to customize ChatGPT’s tone and style. People have strong and varied preferences in how ChatGPT should respond, and tailoring its tone to what sounds right for you should feel effortless.&lt;/p&gt;
    &lt;p&gt;Earlier this year, we added preset options to tailor the tone of how ChatGPT responds. Today, we’re refining those options to better reflect the most common ways people use ChatGPT. Default, Friendly (formerly Listener), and Efficient (formerly Robot) remain (with updates), and we’re adding Professional, Candid, and Quirky. These options are designed to align with what we’ve learned about how people naturally steer the model, making it quick and intuitive to choose a personality that feels uniquely right.&lt;/p&gt;
    &lt;p&gt;These personality settings apply across all models. The original Cynical (formerly Cynic) and Nerdy (formerly Nerd) options we introduced earlier this year will remain available unchanged under the same dropdown in personalization settings.&lt;/p&gt;
    &lt;p&gt;Beyond these presets, for users who want more granular control over how ChatGPT responds, we’re also experimenting with the ability to tune ChatGPT’s characteristics directly from personalization settings—including how concise, warm, or scannable its responses are, and how frequently it uses emojis. ChatGPT can also proactively offer to update these preferences during conversations when it notices you asking for a certain tone or style, without requiring you to navigate into settings. You can adjust or remove any of these preferences at any time.&lt;/p&gt;
    &lt;p&gt;The updated styles and tone options are rolling out today, and the ability to finetune specific characteristics is starting to roll out gradually later this week as an experiment, starting with a limited number of users. Both will continue to improve over time. Additionally, the updated GPT‑5.1 models are also better at adhering to custom instructions, giving you even more precise control over tone and behavior.&lt;/p&gt;
    &lt;p&gt;Updates you make in personalization settings now take effect across all chats right away, including ongoing conversations, so your experience stays consistent. Before, changes to base style and tone or custom instructions only applied to conversations started afterward.&lt;/p&gt;
    &lt;p&gt;Today’s GPT‑5.1 updates and new customization options are a step toward a ChatGPT that feels like it fits you—smarter, more enjoyable to talk to, and more adaptable to your preferences. Going forward, we’ll continue improving along these dimensions—there’s much more to come.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/gpt-5-1/"/><published>2025-11-12T19:05:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45905408</id><title>GLP-1 drugs linked to lower death rates in colon cancer patients</title><updated>2025-11-13T10:43:50.872694+00:00</updated><content>&lt;doc fingerprint="f6fa41ae60c98de2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GLP-1 Drugs Linked to Dramatically Lower Death Rates in Colon Cancer Patients&lt;/head&gt;
    &lt;head rend="h2"&gt;Story by:&lt;/head&gt;
    &lt;head rend="h2"&gt;Published Date&lt;/head&gt;
    &lt;head rend="h2"&gt;Story by:&lt;/head&gt;
    &lt;head rend="h2"&gt;Topics covered:&lt;/head&gt;
    &lt;head rend="h2"&gt;Share This:&lt;/head&gt;
    &lt;head rend="h2"&gt;Article Content&lt;/head&gt;
    &lt;p&gt;A new University of California San Diego study offers compelling evidence that GLP-1 receptor agonists — the class of drugs behind Ozempic, Wegovy and Mounjaro, for example — may do more than regulate blood sugar and weight. In an analysis of more than 6,800 colon cancer patients across all University of California Health sites, researchers found that those taking glucagon-like peptide-1 (GLP-1) medications were less than half as likely to die within five years compared to those who weren’t on the drugs (15.5% vs. 37.1%).&lt;/p&gt;
    &lt;p&gt;The study, led by Raphael Cuomo, Ph.D., associate professor in the Department of Anesthesiology at UC San Diego School of Medicine and member of UC San Diego Moores Cancer Center, used real-world clinical data from the University of California Health Data Warehouse to assess outcomes across the state’s academic medical centers. After adjusting for age, body mass index (BMI), disease severity and other health factors, GLP-1 users still showed significantly lower odds of death, suggesting a strong and independent protective effect.&lt;/p&gt;
    &lt;p&gt;The survival benefit appeared most pronounced in patients with very high BMI (over 35), hinting that GLP-1 drugs may help counteract the inflammatory and metabolic conditions that worsen colon cancer prognosis. Researchers believe several biological mechanisms could explain the link. Beyond regulating blood sugar, GLP-1 receptor agonists reduce systemic inflammation, improve insulin sensitivity and promote weight loss — all factors that can dampen tumor-promoting pathways. Laboratory studies also suggest that GLP-1 drugs may directly prevent cancer cell growth, trigger cancer cell death and reshape the tumor microenvironment. However, the study authors emphasize that more research is needed to confirm these mechanisms and determine whether the survival benefit observed in this real-world analysis represents a direct anti-cancer effect or an indirect result of improved metabolic health.&lt;/p&gt;
    &lt;p&gt;Cuomo notes that while these results are observational, they underscore an urgent need for clinical trials to test whether GLP-1 drugs can improve cancer survival rates, especially for patients with obesity-related cancers.&lt;/p&gt;
    &lt;p&gt;The study appeared in Cancer Investigation on Nov. 11, 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;You May Also Like&lt;/head&gt;
    &lt;head rend="h2"&gt;Stay in the Know&lt;/head&gt;
    &lt;p&gt;Keep up with all the latest from UC San Diego. Subscribe to the newsletter today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://today.ucsd.edu/story/glp-1-drugs-linked-to-dramatically-lower-death-rates-in-colon-cancer-patients"/><published>2025-11-12T19:50:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45907259</id><title>Homebrew no longer allows bypassing Gatekeeper for unsigned/unnotarized software</title><updated>2025-11-13T10:43:49.989583+00:00</updated><content>&lt;doc fingerprint="760419b45cb6b368"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 10.7k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Verification&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; This issue's title and/or description do not reference a single formula e.g. &lt;code&gt;brew install wget&lt;/code&gt;. If they do, open an issue at https://github.com/Homebrew/homebrew-core/issues/new/choose instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Provide a detailed description of the proposed feature&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;--no-quarantine&lt;/code&gt; is used to forcibly bypass Gatekeeper, which is a built-in macOS security mechanism. This is used to run unsigned/unnotarized applications.&lt;/p&gt;
    &lt;p&gt;macOS Tahoe is the final release to support Intel systems, and last year Apple updated macOS runtime protection to make it harder to override Gatekeeper. Macs with Apple silicon also don't "permit native arm64 code to execute unless a valid signature is attached". Finally, we are ending support for all casks that fail Gatekeeper checks on September 1st, 2026.&lt;/p&gt;
    &lt;p&gt;With the above in mind, it's time to deprecate the &lt;code&gt;--no-quarantine&lt;/code&gt; flag from &lt;code&gt;brew&lt;/code&gt;. It intentionally bypasses macOS security mechanisms, which we already actively discourage. Deprecating now will give a decent lead time for users using it to come up with another solution or adjust their workflows.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is the motivation for the feature?&lt;/head&gt;
    &lt;p&gt;Intel support is coming to an end from both Apple and Homebrew. This flag is primarily used to override a macOS security mechanism, which we do not want to encourage. Since we are requiring casks fulfill Gatekeeper checks next year, there is no reason to keep this flag.&lt;/p&gt;
    &lt;head rend="h3"&gt;How will the feature be relevant to at least 90% of Homebrew users?&lt;/head&gt;
    &lt;p&gt;We will provide a safer experience for our users, and stop making it easier to bypass OS-level security.&lt;/p&gt;
    &lt;head rend="h3"&gt;What alternatives to the feature have been considered?&lt;/head&gt;
    &lt;p&gt;None. Macs with Apple silicon are the platform that will be supported in the future, and Apple is making it harder to bypass Gatekeeper as is.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Homebrew/brew/issues/20755"/><published>2025-11-12T21:50:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45907541</id><title>Marble: A Multimodal World Model</title><updated>2025-11-13T10:43:49.564438+00:00</updated><content>&lt;doc fingerprint="1815cb1fadd45c6f"&gt;
  &lt;main&gt;
    &lt;p&gt;November 12, 2025Marble, our frontier multimodal world model, is available to everyone starting today&lt;/p&gt;
    &lt;head rend="h1"&gt;Marble: A Multimodal World Model&lt;/head&gt;
    &lt;p&gt;Spatial intelligence is the next frontier in AI, demanding powerful world models to realize its full potential. World models should reconstruct, generate, and simulate 3D worlds; and allow both humans and agents to interact with them. Spatially intelligent world models will transform a wide variety of industries over the coming years.&lt;/p&gt;
    &lt;p&gt;Two months ago we shared a preview of Marble, our World Model that creates 3D worlds from image or text prompts. Since then, Marble has been available to an early set of beta users to create 3D worlds for themselves.&lt;/p&gt;
    &lt;p&gt;Today we are making Marble, a first-in-class generative multimodal world model, generally available for anyone to use. We have also drastically expanded Marble's capabilities, and are excited to highlight them here:&lt;/p&gt;
    &lt;p&gt;Multimodal Marble: Marble is now massively multimodal. Marble can create 3D worlds from text, images, video, or coarse 3D layouts; Marble also lets you interactively edit, expand, and combine worlds. Once generated, 3D worlds can be exported as Gaussian splats, meshes, or videos. These new capabilities let users create and edit worlds with fine-grained control; and makes those worlds more useful than ever before.&lt;/p&gt;
    &lt;p&gt;Marble Labs: We are launching Marble Labs, a creative hub where imagination meets experimentation. It is where artists, engineers, and designers push the boundaries of world models, showcasing bold ideas, real-world workflows, and new possibilities across gaming, VFX, design, robotics, and beyond. Marble Labs is also home to in-depth case studies, tutorials, and documentation that give anyone the tools to learn, build, and share their own 3D worlds.&lt;/p&gt;
    &lt;p&gt;Sign up at marble.worldlabs.ai and start creating worlds for yourself!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Marble World Model&lt;/head&gt;
    &lt;p&gt;Our human experience of the world is inherently multimodal: we use all of our senses to make sense of the world around us. We integrate sight, sound, touch, and language to build up a mental model of the outside world; these different representations work together, enriching and reinforcing each other to let us reason about the world and act within it.&lt;/p&gt;
    &lt;p&gt;World models should work similarly. They should be massively multimodal, able to lift whatever input signals are available into a full 3D world, and they should be able to iteratively update their understanding of the world as new information becomes available.&lt;/p&gt;
    &lt;p&gt;Marble is the first of its kind - a next-generation world model making strides toward this vision. It can now create 3D worlds from a wide variety of input types, and lets users iteratively edit or expand worlds.&lt;/p&gt;
    &lt;p&gt;Marble's new capabilities let you dive as deep as you want in controlling your generated worlds. You can quickly create full 3D worlds from a simple image or text prompt or interactively edit worlds in both 2D and 3D, bringing to life a precise vision of a world in your mind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Text and Image to World&lt;/head&gt;
    &lt;p&gt;To start, Marble can create a full 3D world from a single image or a short text prompt. This is the simplest and easiest way to create worlds. Marble can generate worlds with a wide variety of scene types and artistic styles.&lt;/p&gt;
    &lt;p&gt;Image prompts make it easy to combine Marble with other AI tools. You can generate images with your favorite image generation model, then bring it to Marble to lift it to a full 3D world.&lt;/p&gt;
    &lt;p&gt;Text and image prompts are intuitive and powerful, but limited in creative control: Marble must invent all the details of the world that are not present in the input text or image prompt. This is often magical; but sometimes you may want to steer Marble more directly toward a desired world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Image and Video to World&lt;/head&gt;
    &lt;p&gt;An easy way to create worlds with more creative control is multi-image prompting. Marble can accept different prompt images for different parts of the world, stitching them together into a unified 3D world.&lt;/p&gt;
    &lt;p&gt;Multi-image prompts let you create worlds with more precision. Unlike text or single-image prompts where Marble must invent all parts of the world not present in the prompt, with multi-image prompts you can control what the generated world will look like from different angles.&lt;/p&gt;
    &lt;p&gt;This leads to a brand-new workflow for generating worlds. You can use your favorite image generation tool to iterate separately on the input views, and Marble will lift them into full 3D worlds while also adding seamless transitions between the input views.&lt;/p&gt;
    &lt;p&gt;Multi-image prompts can also be used to create worlds inspired by real-world spaces. Marble can input a few photos or a short video depicting a real-world location from different angles, and it will combine them to generate a 3D world with elements of the real-world space.&lt;/p&gt;
    &lt;head rend="h2"&gt;World Editing&lt;/head&gt;
    &lt;p&gt;The creative process is highly iterative for many users. Often, generating a world is only the start of a creative journey. Seeing a generated 3D world often kicks off a dozen more ideas for changing it or improving it.&lt;/p&gt;
    &lt;p&gt;Marble includes AI-native world editing tools. Edits can be small and local: remove an object, touch up an area. They can also be more drastic: swap objects, change the visual style, or re-structure large parts of the world. This gives a new level of fine-grained control to the world creation process.&lt;/p&gt;
    &lt;p&gt;Edited&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;Edited&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;Edited&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;World editing lets you re-imagine the same space in endless different ways.&lt;/p&gt;
    &lt;p&gt;Edited&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;Edited&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;Edited&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;Edited&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;Edited&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;head rend="h2"&gt;Chisel: Sculpting Worlds in 3D&lt;/head&gt;
    &lt;p&gt;Marble's multimodal inputs and editing features give a lot of control over your generated 3D worlds. But sometimes, creating the world exactly as you see it in your mind's eye requires finer-grained control over the scene layout or exact sizes and positions of objects.&lt;/p&gt;
    &lt;p&gt;For these situations we are introducing Chisel, an AI-native tool to sculpt Marble worlds directly in 3D.&lt;/p&gt;
    &lt;p&gt;Chisel is a new experimental editing mode for advanced users to create 3D worlds. It lets you lay out the coarse structure of your world in 3D using coarse 3D shapes like boxes or planes, or importing existing 3D assets into the scene.&lt;/p&gt;
    &lt;p&gt;After laying out the coarse 3D scene, you can add a text prompt to describe the visual style of the scene, or additional elements not present in the coarse layout. Marble will combine these inputs to give you a fully detailed 3D world.&lt;/p&gt;
    &lt;p&gt;Chisel decouples structure from style. The coarse 3D scene determines the world's structure, while the text prompt controls its overall style. The two can be mixed in any combination, adding a whole new dimension of control to world generation.&lt;/p&gt;
    &lt;p&gt;Generated World&lt;/p&gt;
    &lt;p&gt;Coarse 3D&lt;/p&gt;
    &lt;p&gt;Coarse 3D After&lt;/p&gt;
    &lt;p&gt;Coarse 3D Before&lt;/p&gt;
    &lt;p&gt;Generated World After&lt;/p&gt;
    &lt;p&gt;Generated World Before&lt;/p&gt;
    &lt;p&gt;The coarse 3D scene can be as simple or complex as you want. In addition to building the coarse 3D scene out of basic blocks and walls, you can import existing 3D assets of objects. Objects will be restyled based on the text prompt to give a cohesive 3D world.&lt;/p&gt;
    &lt;p&gt;Varying the text prompt can give rise to 3D worlds with drastically different visual styles and appearances that all share a common structure determined by the coarse 3D scene.&lt;/p&gt;
    &lt;p&gt;Generated World&lt;/p&gt;
    &lt;p&gt;Coarse 3D&lt;/p&gt;
    &lt;p&gt;Generated World&lt;/p&gt;
    &lt;p&gt;Coarse 3D&lt;/p&gt;
    &lt;p&gt;Generated World&lt;/p&gt;
    &lt;p&gt;Coarse 3D&lt;/p&gt;
    &lt;p&gt;Generated World&lt;/p&gt;
    &lt;p&gt;Coarse 3D&lt;/p&gt;
    &lt;p&gt;Generated World&lt;/p&gt;
    &lt;p&gt;Coarse 3D&lt;/p&gt;
    &lt;p&gt;Generated World&lt;/p&gt;
    &lt;p&gt;Coarse 3D&lt;/p&gt;
    &lt;head rend="h2"&gt;Building Large Worlds by Expanding and Composing&lt;/head&gt;
    &lt;p&gt;Sometimes bigger really is better. Larger worlds give more possibilities, more space, more room for your creativity to shine. Marble offers two ways to make bigger worlds than ever before.&lt;/p&gt;
    &lt;p&gt;After a world has been generated, Marble allows one-step expansion to make it larger. You are in control of this process: you can select a region of the world to be expanded, and Marble will create more content to fill the selected region.&lt;/p&gt;
    &lt;p&gt;Expansion can make worlds larger. Regions of the world that previously broke down into artifacts can become crisp and clean after expansion. Expansion can also be used to add detail to targeted regions of a world. Sometimes the back of a table or the far corner of a room is not a crisp as the room's center; expanding the world in that region can improve it.&lt;/p&gt;
    &lt;p&gt;Expanded World&lt;/p&gt;
    &lt;p&gt;Initial World&lt;/p&gt;
    &lt;p&gt;Expanded World&lt;/p&gt;
    &lt;p&gt;Initial World&lt;/p&gt;
    &lt;p&gt;Expanded World&lt;/p&gt;
    &lt;p&gt;Initial World&lt;/p&gt;
    &lt;p&gt;Expanded World&lt;/p&gt;
    &lt;p&gt;Initial World&lt;/p&gt;
    &lt;p&gt;In addition to generating individual worlds, you can compose any number of worlds to build out extremely large spaces with Marble's composer mode. This composition is entirely under your control: you can choose exactly which worlds to compose, and exactly how to lay them out relative to each other. Composing is yet another way to build worlds that follow your creative vision.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exporting Worlds to 3D and Video&lt;/head&gt;
    &lt;p&gt;After creating a world with Marble you have many options to export it for incorporation into downstream projects.&lt;/p&gt;
    &lt;p&gt;Gaussian splats are the highest-fidelity representation for Marble worlds. They represent 3D scenes as a large set of semitransparent particles. You can render Gaussian splats in the browser using Spark, our open-source cross-platform renderer integrated with THREE.js.&lt;/p&gt;
    &lt;p&gt;Marble worlds can also be exported as triangle meshes. Marble can generate both collider meshes, which are low-fidelity meshes intended for coarse physics simulation; and high-quality meshes which are intended to match the visual fidelity of Gaussian splats as closely as possible. Exporting worlds as meshes lets them interoperate with many industry-standard tools.&lt;/p&gt;
    &lt;p&gt;Mesh&lt;/p&gt;
    &lt;p&gt;Splats&lt;/p&gt;
    &lt;p&gt;Mesh with lighting&lt;/p&gt;
    &lt;p&gt;Mesh with lighting&lt;/p&gt;
    &lt;p&gt;Marble worlds exist in full 3D, but sometimes a video is the best way to share a world. You can use Marble to render generated worlds to videos with pixel-accurate camera control, letting you frame every shot just as you imagine it. In fact, nearly all the videos in this post were generated directly from Marble.&lt;/p&gt;
    &lt;p&gt;Marble can also enhance exported videos. Enhanced videos can add detail, remove artifacts, and add dynamic elements to the scene, while maintaining pixel-perfect camera control and adhering to the structure of the generated 3D world.&lt;/p&gt;
    &lt;p&gt;Enhanced Video&lt;/p&gt;
    &lt;p&gt;Original Video&lt;/p&gt;
    &lt;p&gt;Enhanced Video&lt;/p&gt;
    &lt;p&gt;Original Video&lt;/p&gt;
    &lt;p&gt;Enhanced Video&lt;/p&gt;
    &lt;p&gt;Original Video&lt;/p&gt;
    &lt;head rend="h2"&gt;Marble Labs: A Glimpse of Future Possibilities&lt;/head&gt;
    &lt;p&gt;While flexing your creativity in Marble, Marble Labs may further inspire your imagination. This is where artists, engineers, and designers are already shaping what comes next. From cinematic filmmaking and interactive worlds to robotics simulations and therapeutic environments, these projects show how Marble is transforming imagination into reality. Each one reflects a new way of building with world models, both creative and technical. Explore Marble Labs to see what others are creating and discover how you can start building your own worlds today.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Marble to Spatial Intelligence&lt;/head&gt;
    &lt;p&gt;Marble is a state-of-the-art generative world model. Today it lets you create worlds from diverse input types, edit them, expand them, and export them. These capabilities give you unprecedented levels of control when creating worlds, and are already enabling a wide variety of creative use cases across industries.&lt;/p&gt;
    &lt;p&gt;But Marble is just a step on our journey toward spatial intelligence. Going forward, a key opportunity is interactivity. Future world models will let humans and agents alike interact with generated worlds in new ways, unlocking even more use cases in simulation, robotics, and beyond.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Marble Today&lt;/head&gt;
    &lt;p&gt;Marble is available today at marble.worldlabs.ai. Sign up now and start creating worlds!&lt;/p&gt;
    &lt;p&gt;If you are excited about this vision and want to help us build it, join us!&lt;/p&gt;
    &lt;head rend="h2"&gt;Read More&lt;/head&gt;
    &lt;p&gt;November 10, 2025&lt;/p&gt;
    &lt;head rend="h3"&gt;From Words to Worlds: Spatial Intelligence is AI’s Next Frontier&lt;/head&gt;
    &lt;p&gt;A manifesto piece explaining what spatial intelligence is, why it matters, and how we’re building the world models that will unlock it—with impact that will reshape creativity, embodied intelligence, and human progress.&lt;/p&gt;
    &lt;p&gt;October 16, 2025&lt;/p&gt;
    &lt;head rend="h3"&gt;RTFM: A Real-Time Frame Model&lt;/head&gt;
    &lt;p&gt;A research preview of RTFM, a new generative world model that generates video in real-time as you interact with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.worldlabs.ai/blog/marble-world-model"/><published>2025-11-12T22:11:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45908464</id><title>Valve is about to win the console generation</title><updated>2025-11-13T10:43:49.203966+00:00</updated><content>&lt;doc fingerprint="4e7e82b81462420f"&gt;
  &lt;main&gt;
    &lt;p&gt;Making sure you're not a bot! Loading... Please wait a moment while we ensure the security of your connection.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xeiaso.net/blog/2025/valve-is-about-to-win-the-console-generation/"/><published>2025-11-12T23:35:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45908938</id><title>Google will allow users to sideload Android apps without verification</title><updated>2025-11-13T10:43:48.974179+00:00</updated><content>&lt;doc fingerprint="89b03251d8d5e4b8"&gt;
  &lt;main&gt;
    &lt;p&gt;12 November 2025&lt;/p&gt;
    &lt;p&gt;Posted by Matthew Forsythe Director - Product Management, Android App Safety&lt;/p&gt;
    &lt;p&gt;We recently announced new developer verification requirements, which serve as an additional layer of defense in our ongoing effort to keep Android users safe. We know that security works best when it accounts for the diverse ways people use our tools. This is why we announced this change early: to gather input and ensure our solutions are balanced. We appreciate the community's engagement and have heard the early feedback – specifically from students and hobbyists who need an accessible path to learn, and from power users who are more comfortable with security risks. We are making changes to address the needs of both groups.&lt;/p&gt;
    &lt;p&gt;To understand how these updates fit into our broader mission, it is important to first look at the specific threats we are tackling.&lt;/p&gt;
    &lt;p&gt;Why verification is important&lt;/p&gt;
    &lt;p&gt;Keeping users safe on Android is our top priority. Combating scams and digital fraud is not new for us — it has been a central focus of our work for years. From Scam Detection in Google Messages to Google Play Protect and real-time alerts for scam calls, we have consistently acted to keep our ecosystem safe.&lt;/p&gt;
    &lt;p&gt;However, online scams and malware campaigns are becoming more aggressive. At the global scale of Android, this translates to real harm for people around the world – especially in rapidly digitizing regions where many are coming online for the first time. Technical safeguards are critical, but they cannot solve for every scenario where a user is manipulated. Scammers use high-pressure social engineering tactics to trick users into bypassing the very warnings designed to protect them.&lt;/p&gt;
    &lt;p&gt;For example, a common attack we track in Southeast Asia illustrates this threat clearly. A scammer calls a victim claiming their bank account is compromised and uses fear and urgency to direct them to sideload a "verification app" to secure their funds, often coaching them to ignore standard security warnings. Once installed, this app — actually malware — intercepts the victim's notifications. When the user logs into their real banking app, the malware captures their two-factor authentication codes, giving the scammer everything they need to drain the account.&lt;/p&gt;
    &lt;p&gt;While we have advanced safeguards and protections to detect and take down bad apps, without verification, bad actors can spin up new harmful apps instantly. It becomes an endless game of whack-a-mole. Verification changes the math by forcing them to use a real identity to distribute malware, making attacks significantly harder and more costly to scale. We have already seen how effective this is on Google Play, and we are now applying those lessons to the broader Android ecosystem to ensure there is a real, accountable identity behind the software you install.&lt;/p&gt;
    &lt;p&gt;Supporting students and hobbyists&lt;/p&gt;
    &lt;p&gt;We heard from developers who were concerned about the barrier to entry when building apps intended only for a small group, like family or friends. We are using your input to shape a dedicated account type for students and hobbyists. This will allow you to distribute your creations to a limited number of devices without going through the full verification requirements.&lt;/p&gt;
    &lt;p&gt;Empowering experienced users&lt;/p&gt;
    &lt;p&gt;While security is crucial, we’ve also heard from developers and power users who have a higher risk tolerance and want the ability to download unverified apps.&lt;/p&gt;
    &lt;p&gt;Based on this feedback and our ongoing conversations with the community, we are building a new advanced flow that allows experienced users to accept the risks of installing software that isn't verified. We are designing this flow specifically to resist coercion, ensuring that users aren't tricked into bypassing these safety checks while under pressure from a scammer. It will also include clear warnings to ensure users fully understand the risks involved, but ultimately, it puts the choice in their hands. We are gathering early feedback on the design of this feature now and will share more details in the coming months.&lt;/p&gt;
    &lt;p&gt;Getting started with early access&lt;/p&gt;
    &lt;p&gt;Today, we’re excited to start inviting developers to the early access for developer verification in Android Developer Console for developers that distribute exclusively outside of Play, and will share invites to the Play Console experience soon for Play developers. We are looking forward to your questions and feedback on streamlining the experience for all developers.&lt;/p&gt;
    &lt;p&gt;Watch our video below for a walkthrough of the new Android Developer Console experience and see our guides for more details and FAQs.&lt;/p&gt;
    &lt;p&gt;We are committed to working with you to keep the ecosystem safe while getting this right.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://android-developers.googleblog.com/2025/11/android-developer-verification-early.html"/><published>2025-11-13T00:33:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45909059</id><title>Human Fovea Detector</title><updated>2025-11-13T10:43:48.924843+00:00</updated><content/><link href="https://www.shadertoy.com/view/4dsXzM"/><published>2025-11-13T00:48:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45910009</id><title>A Commentary on the Sixth Edition Unix Operating System</title><updated>2025-11-13T10:43:48.877073+00:00</updated><link href="https://warsus.github.io/lions-/"/><published>2025-11-13T03:01:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45910244</id><title>On USB HID, Keyboard LEDs, and device emulation (2024)</title><updated>2025-11-13T10:43:48.750577+00:00</updated><content>&lt;doc fingerprint="a70ccc11c585a1e6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;On USB HID, Keyboard LEDs, and device emulation.&lt;/head&gt;
    &lt;p&gt;I added keyboard and mouse support to BoxLambda. The plan was to use PS/2, but when NAND2Mario announced their usb_hid_host core, I couldn’t resist. I added a Wishbone frontend and some CDC logic to cross from the 12MHz USB clock domain to the 50MHz system clock domain. I plugged in a keyboard and mouse, and it all worked fine. End of story…&lt;/p&gt;
    &lt;p&gt;Almost. The original usb_hid_host core does not include support for controlling USB keyboard LEDs. Adding that feature required a deep dive into the usb_hid_host implementation and the USB spec. I also needed a way to test it all in simulation, in other words, I needed a USB keyboard and mouse emulator.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recap&lt;/head&gt;
    &lt;p&gt;This is a summary of the current state of affairs for BoxLambda. We have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An Ibex RISC-V core, Wishbone Interconnect, timer, two GPIO ports, UART core, and internal memory.&lt;/item&gt;
      &lt;item&gt;DDR3 external memory access through the Litex Memory Controller.&lt;/item&gt;
      &lt;item&gt;OpenOCD-based Debug Access, both on FPGA and Verilator.&lt;/item&gt;
      &lt;item&gt;VERA-based VGA graphics: 2 layers tile or bitmap mode, 2 banks of 64 sprites, 128KB Video RAM, 256 color palette.&lt;/item&gt;
      &lt;item&gt;Dual YM2149 PSG Audio.&lt;/item&gt;
      &lt;item&gt;SD Card Controller and FatFs File System.&lt;/item&gt;
      &lt;item&gt;A PicoRV32-based Programmable DMA Controller.&lt;/item&gt;
      &lt;item&gt;A Picolibc-based standard C environment for software running on the Ibex RISC-V core.&lt;/item&gt;
      &lt;item&gt;Test builds running on Arty-A7-35T, Arty-A7-100T, Verilator, and CocoTB.&lt;/item&gt;
      &lt;item&gt;A Linux CMake and Bender-based Software and Gateware build system with support for automated testing and post-implementation memory updates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The NAND2Mario usb_hid_host core&lt;/head&gt;
    &lt;p&gt;The NAND2Mario usb_hid_host core.&lt;/p&gt;
    &lt;p&gt;The NAND2Mario usb_hid_host core is easy to use. You don’t need a USB software stack. UKP, a little 4-bit processor inside the core handles the USB enumeration. You don’t need a USB PHY chip either. You can just hook up the USB D-/D+ pins directly to a simple USB PMOD. I’m using the following PMOD from Machdyne:&lt;/p&gt;
    &lt;p&gt;The Machdyne USB host dual socket PMOD.&lt;/p&gt;
    &lt;p&gt;https://machdyne.com/product/usb-host-dual-socket-pmod/&lt;/p&gt;
    &lt;p&gt;Usb_hid_host requires a 12MHz clock. You just hook up the clock, reset, USB D-/D+ and the core takes care of the rest. Usb_hid_host has the following output ports:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;report: a strobe signal indicating that a new report has been received from the attached device (e.g. a key press, a mouse movement,…).&lt;/item&gt;
      &lt;item&gt;typ: the type of the attached USB device: keyboard, mouse, or gamepad.&lt;/item&gt;
      &lt;item&gt;connerr: a connection error indication.&lt;/item&gt;
      &lt;item&gt;key_modifiers: if the attached device is a keyboard, indicates which key modifiers (ctrl, shift,…) are being held down.&lt;/item&gt;
      &lt;item&gt;key1-4: if the attached device is a keyboard, indicates which non-modifier keys are being pressed, with a max. of 4 keys being pressed simultaneously.&lt;/item&gt;
      &lt;item&gt;mouse_btn: if the attached device is a mouse, indicates which mouse buttons are being pressed.&lt;/item&gt;
      &lt;item&gt;mouse_dx: if the attached device is a mouse, indicates the mouse’s horizontal movement.&lt;/item&gt;
      &lt;item&gt;mouse_dy: if the attached device is a mouse, indicates the mouse’s vertical movement.&lt;/item&gt;
      &lt;item&gt;game_*: if the attached device is a gamepad, indicates the gamepad joystick directions and buttons being pressed. I haven’t tested these yet. I currently don’t have a gamepad.&lt;/item&gt;
      &lt;item&gt;dbg_hid_report: the raw HID report, for debug purposes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;USB HIDBP&lt;/head&gt;
    &lt;p&gt;The USB HID Spec is big and complicated. A complete implementation would require a lot more infrastructure than just a small usb_hid_host core. Usb_hid_host implements a subset of the USB HID spec called the Boot Protocol (USB HIDBP). The Boot Protocol was added to the spec to humor BIOS developers. The Boot Protocol is much simpler than its counterpart, the Report Protocol, which is the protocol used by the big USB stacks in Linux or Windows.&lt;/p&gt;
    &lt;p&gt;Unfortunately, not all keyboards and mice support the Boot Protocol. Basic/Low-end keyboards and mice have a higher chance of supporting the Boot Protocol. I’m using a Dell KB212-B keyboard and a Dell OCJ339 mouse.&lt;/p&gt;
    &lt;head rend="h2"&gt;Controlling USB keyboard LEDs&lt;/head&gt;
    &lt;p&gt;I want to add keyboard LED control to the NAND2Mario core. Where to start?&lt;/p&gt;
    &lt;p&gt;The best information I could find online describing USB HID in general, and keyboard LED control in particular, is this page from the OSDev Wiki:&lt;/p&gt;
    &lt;p&gt;https://wiki.osdev.org/USB_Human_Interface_Devices.&lt;/p&gt;
    &lt;p&gt;For the lower-level details, e.g. how to set up a Control Transfer to the device, I found USB in a nutshell very helpful.&lt;/p&gt;
    &lt;p&gt;Briefly, the USB host controls keyboard LEDs by sending a SetReport message to the device using a Control Transfer. The message contains a bitmap specifying which LEDs should be on and off.&lt;/p&gt;
    &lt;p&gt;SetReport Keyboard LED bitmap.&lt;/p&gt;
    &lt;p&gt;The entire packet sequence looks like this:&lt;/p&gt;
    &lt;p&gt;SetReport Sequence Diagram.&lt;/p&gt;
    &lt;p&gt;Quite an elaborate sequence to control a few LEDS!&lt;/p&gt;
    &lt;head rend="h1"&gt;Adding USB keyboard LED control to the usb_hid_host core&lt;/head&gt;
    &lt;p&gt;UKP processor inside the usb_hid_host core just recognizes a handful of instructions that are specifically created to detect and enumerate USB devices. The original instruction set is documented here. Note that there are no instructions that respond to external inputs other than what comes in over USB. To implement keyboard LED control, I need a way to branch to a SetReport routine based on an external signal and a way to configure the contents of the SetReport message.&lt;/p&gt;
    &lt;p&gt;Keyboard LED control in the usb_hid_host core.&lt;/p&gt;
    &lt;p&gt;To add USB keyboard LED control to the usb_hid_host core, I implemented the following changes in the UKP processor:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I extended the UKP opcode width from 4 bits to 5 bits. This gave me space to add new instructions.&lt;/item&gt;
      &lt;item&gt;I added a conditional Branch Request (br) instruction and a req_branch_stb input port to the UKP module. Through the req_branch_stb signal, the user (usb_hid_host) can request the branch to be taken. When the branch has been taken, an ack_req_branch_stb is sent back to the user.&lt;/item&gt;
      &lt;item&gt;I added outr0, outr1, and outr2 instructions, associated with three 8-bit registers and three 8-bit input ports of the UKP module, also called outr0, outr1, and outr2. When an outr&lt;x&gt;instruction executes, the contents of the associated register will be transmitted over the USB port.&lt;/x&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With these UKP changes, the usb_hid_host core can, upon request, have the UKP firmware branch to a section of microcode that sends a SetReport request to the device. The parameters of this message, i.e. the LED bitmap and the CRC16 value are specified in outr0 and outr1/2.&lt;/p&gt;
    &lt;p&gt;As an aside, since the variable portion of the SetReport message is just 4 LED bits (I left out KANA, sorry!), I’m using a CRC16 look-up table to find the CRC16 value of the SetReport message, rather than a CRC16 generator. I used this handy website to compute the necessary CRC16 and CRC5 values used in the SetReport transaction:&lt;/p&gt;
    &lt;p&gt;The usb_hid_host fork with keyboard LED control support is located here:&lt;/p&gt;
    &lt;p&gt;https://github.com/epsilon537/usb_hid_host&lt;/p&gt;
    &lt;head rend="h2"&gt;Wishbone Frontend and Clock Domain Crossing&lt;/head&gt;
    &lt;p&gt;Usb_hid_host core in USB Clock Domain with Wishbone Frontend in System Clock Domain.&lt;/p&gt;
    &lt;p&gt;I added a Wishbone frontend to the usb_hid_host core so it could be integrated into the BoxLambda SoC. The BoxLambda SoC system clock runs at 50MHz while the usb_hid_host core runs at 12MHz, so Clock Domain Crossing (CDC)) logic is needed between these two clock domains. I struggled with that for a bit, until I read this paper:&lt;/p&gt;
    &lt;p&gt;http://www.sunburst-design.com/papers/CummingsSNUG2008Boston_CDC.pdf&lt;/p&gt;
    &lt;p&gt;If you haven’t read this paper and you’re messing around with CDC in your projects, don’t waste your time reading this post. Go read that paper. You’ll feel much better!&lt;/p&gt;
    &lt;p&gt;I found the toggle-pulse generation technique in Figure 20 of the document very elegant. You can bring a single clock cycle pulse/strobe from one clock domain to another by turning the pulse into a toggle in the transmitting clock domain, synchronizing the toggle to the receiving clock domain, and then turning the toggle back into a pulse.&lt;/p&gt;
    &lt;p&gt;MCP CDC technique using Toggle-Pulse.&lt;/p&gt;
    &lt;p&gt;In both directions, USB to System Clock and System Clock to USB, I’m using a Multi-Cycle Path (MCP) strategy to pass signals across the clock domains. In the System Clock to USB direction (fast to slow clock), I’m using feedback. Keyboard LED update requests are acknowledged.&lt;/p&gt;
    &lt;head rend="h2"&gt;USB HID Device Emulation&lt;/head&gt;
    &lt;p&gt;To test the usb_hid_host core in simulation, I have to connect it to an emulated USB HID device. In a dusty corner of GitHub, I found this gem:&lt;/p&gt;
    &lt;p&gt;This is a project from Pbing, the author of the ibex_wb project that I used to bootstrap BoxLambda.&lt;/p&gt;
    &lt;p&gt;Pbing’s USB project emulates a mouse. The design is based on a J1 processor executing Forth-based firmware. How cool is that?&lt;/p&gt;
    &lt;head rend="h1"&gt;Forth and J1&lt;/head&gt;
    &lt;p&gt;I don’t know how I’ve managed to overlook Forth and J1 all this time. Here’s a great post on Hacker News about Forth and The J1 Forth CPU:&lt;/p&gt;
    &lt;p&gt;https://news.ycombinator.com/item?id=25759576&lt;/p&gt;
    &lt;p&gt;Forth is very efficient and very elegant. It allows you to interactively write fast and powerful code, a couple of notches above assembly level, without requiring heavy cross-compiling toolchains and VMs. SwapForth, an interactive Forth development environment, runs easily on a J1a CPU with 8KB of RAM. I’m amazed and will be exploring further.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Simulation Setup&lt;/head&gt;
    &lt;p&gt;USB HID System Test Simulation Setup.&lt;/p&gt;
    &lt;p&gt;To test USB mouse support, I’m using Pbing’s mouse emulation as-is. To test USB keyboard support, I created a firmware variant that emulates a keyboard with a key being pressed. The firmware also accepts the SetReport messages for LED control and will set GPIOs depending on the value of the received LED bitmap.&lt;/p&gt;
    &lt;p&gt;The simulation top-level, sim_main.sv, hooks up the mouse and the keyboard emulations to the two usb_hid_host instances of the BoxLambda SoC. Sim_main.sv includes logic tracking the USB ports’ output enables and driving the USB D+/D- lines high or low to emulate the pull-up/pull-down behavior of a low-speed USB device (See https://www.beyondlogic.org/usbnutshell/usb2.shtml#SpeedIdentification).&lt;/p&gt;
    &lt;head rend="h1"&gt;The usb_hid_device repo&lt;/head&gt;
    &lt;p&gt;This is a fork of Pbing’s USB repo including USB keyboard emulation:&lt;/p&gt;
    &lt;p&gt;https://github.com/epsilon537/usb_hid_device&lt;/p&gt;
    &lt;head rend="h2"&gt;Clocks and Reset&lt;/head&gt;
    &lt;p&gt;The usb_hid_host cores introduce a new clock and reset domain to the BoxLambda SoC.&lt;/p&gt;
    &lt;p&gt;The clock tree currently looks like this:&lt;/p&gt;
    &lt;p&gt;BoxLambda’s Clocks.&lt;/p&gt;
    &lt;p&gt;The following clocks are present in the SoC:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ext_clk_100: 100MHz external clock, input to the LiteDRAM core.&lt;/item&gt;
      &lt;item&gt;sys_clk: 50MHz System Clock, generated by the LiteDRAM core, used by rest of SoC (CPU, Interconnect,…).&lt;/item&gt;
      &lt;item&gt;sys_clkx2: 100MHz Double Rate System Clock, generated by the LiteRAM core, twice the rate of, and in phase with sys_clk. Sys_clkx2 is used by the PicoRV core.&lt;/item&gt;
      &lt;item&gt;ddr_clk: SDRAM DDR PHY clock running at 4xsys_clk, generated by the LiteDRAM core.&lt;/item&gt;
      &lt;item&gt;200MHz reference clock for IODELAYs, generated and used the LiteDRAM core.&lt;/item&gt;
      &lt;item&gt;tck: JTAG clock, driven via a BSCANE2 primitive by the FPGA’s JTAG chain. The BSCANE2 primitive is instantiated in the dmi_bscane_tap module.&lt;/item&gt;
      &lt;item&gt;usb_clk: 12MHz clock for the USB HID Host cores.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BoxLambda has three synchronous Reset Domains:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;dm_reset: Resets the Debug Module Logic in the system clock domain.&lt;/item&gt;
      &lt;item&gt;ndm_reset: Resets the Non-Debug Module Logic in the system clock domain.&lt;/item&gt;
      &lt;item&gt;usb_reset: Resets the logic in the USB clock domain.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and the following Reset Sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Power-On Reset: Asserted for several clock cycles after Power-On.&lt;/item&gt;
      &lt;item&gt;External Reset: Connected to a reset button on the Arty A7 board.&lt;/item&gt;
      &lt;item&gt;Non-Debug Module Reset Request: issued by the Debug Module.&lt;/item&gt;
      &lt;item&gt;Software Reset: Reset of dm_reset, ndm_reset, or usb_reset domain triggered by software by writing to a reset_ctrl register.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The management of these reset domains and reset sources is organized by a reset_ctrl module.&lt;/p&gt;
    &lt;p&gt;BoxLambda’s Reset Controller.&lt;/p&gt;
    &lt;p&gt;The reset_ctrl module code is located here:&lt;/p&gt;
    &lt;p&gt;https://github.com/epsilon537/boxlambda/blob/master/gw/components/reset_ctrl/rtl/reset_ctrl.sv&lt;/p&gt;
    &lt;head rend="h2"&gt;Software&lt;/head&gt;
    &lt;head rend="h1"&gt;The USB HID HAL&lt;/head&gt;
    &lt;p&gt;The USB HID HAL is a very thin Hardware Access Layer, mapping directly to the two USB cores’ registers:&lt;/p&gt;
    &lt;p&gt;https://github.com/epsilon537/boxlambda/blob/master/sw/components/usb_hid/usb_hid_hal.h&lt;/p&gt;
    &lt;head rend="h1"&gt;The USB HID System Test Case&lt;/head&gt;
    &lt;p&gt;The USB HID System Test Case, running both on Verilator and on FPGA, continuously polls the two USB cores for report events. Whenever there’s a report event (indicated in the USB_HID_ISR register), the device type (Keyboard/Mouse) and report details (mouse movement, keypress…) are printed.&lt;/p&gt;
    &lt;p&gt;Additionally, when Switch 0 (SW0) is set to On and a USB keyboard is connected, the keyboard LEDs will be turned on and off in a rotating pattern.&lt;/p&gt;
    &lt;p&gt;https://github.com/epsilon537/boxlambda/blob/master/sw/projects/usb_hid_sys_test/usb_hid_sys_test.c&lt;/p&gt;
    &lt;head rend="h1"&gt;The usb_hid_host firmware&lt;/head&gt;
    &lt;p&gt;The usb_hid_host UKP firmware .hex image is checked in, so you don’t need to build it from source.&lt;/p&gt;
    &lt;p&gt;If you would like to tinker with the firmware, you’ll need to have perl installed (sudo apt-get install perl on Ubuntu).&lt;/p&gt;
    &lt;p&gt;UKP Firmware directory: sub/usb_hid_host/src/usb_hid_host/&lt;/p&gt;
    &lt;p&gt;After making your changes, run the asukp script in that same directory and a new firmware image will be generated.&lt;/p&gt;
    &lt;head rend="h1"&gt;The usb_hid_device firmware&lt;/head&gt;
    &lt;p&gt;The usb_hid_device firmware .hex images for the emulated keyboard and mouse are checked in, so you don’t need to build them from source.&lt;/p&gt;
    &lt;p&gt;If you would like to tinker with the firmware, you’ll need to install gforth (sudo apt-get install gforth on Ubuntu). The firmware is located here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mouse Firmware directory: sub/usb_hid_device/firmware_mouse/&lt;/item&gt;
      &lt;item&gt;Keyboard Firmware directory: sub/usb_hid_device/firmware_keyboard/&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After making your changes, just run make in that same directory and a new firmware image will be generated.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Out&lt;/head&gt;
    &lt;head rend="h1"&gt;Setup&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install the Software Prerequisites.&lt;/item&gt;
      &lt;item&gt;Get the BoxLambda repository: &lt;code&gt;git clone https://github.com/epsilon537/boxlambda/ cd boxlambda&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Switch to the usb_hid_host tag: &lt;code&gt;git checkout usb_hid_host&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Set up the repository. This initializes the git submodules used and creates the default build trees: &lt;code&gt;./boxlambda_setup.sh&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;The USB HID Test on Verilator&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build the usb_hid_sys_test project: &lt;code&gt;cd build/sim-a7-100/gw/projects/usb_hid_sys_test make usb_hid_sys_test_sim_sw&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Execute the generated Verilator model. You should see the following output (it’ll take a minute before you start seeing any output): &lt;code&gt;./Vmodel ... USB HID Test Start. USB1: Status change: 0x0 -&amp;gt; 0x1 Keyboard detected. ledg_1 = 2 USB1 keyboard report: 0x210900001000100 Key mods: 0x0 Keys: 0x0 USB0: Status change: 0x0 -&amp;gt; 0x2 Mouse detected. USB0 mouse report: 0xcffcfc001000100 Mouse: 0x0 USB1 keyboard report: 0x400000 Key mods: 0x0 Keys: 0x4 USB0 mouse report: 0xcffcfc001000100 Mouse: 0x0 ... Test passed.&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The ledg_1 = … lines indicate a LED update in the emulated USB keyboard.&lt;/p&gt;
    &lt;head rend="h1"&gt;The USB HID Test on FPGA&lt;/head&gt;
    &lt;p&gt;Arty A7 Setup with USB PMOD attached to port JA.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If you’re running on WSL, check BoxLambda’s documentation On WSL section.&lt;/item&gt;
      &lt;item&gt;Hook up Machdyne’s USB host dual socket PMOD to port JA and connect a keyboard and/or a mouse.&lt;/item&gt;
      &lt;item&gt;Connect a terminal program such as Putty or Teraterm to Arty’s USB serial port. Settings: 115200 8N1.&lt;/item&gt;
      &lt;item&gt;Build the project in an Arty A7 build tree (arty-a7-35 or arty-a7-100): &lt;code&gt;cd build/arty-a7-100/gw/projects/usb_hid_sys_test make usb_hid_sys_test_bit_sw&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Download the generated bitstream file to the Arty A7: &lt;code&gt;make usb_hid_sys_test_load&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Make sure Switch 0 (SW0) is in the Off position (flipped toward the edge of the board).&lt;/item&gt;
      &lt;item&gt;Press some keys on the keyboard, move the mouse around. You should see the results in the Putty terminal.&lt;/item&gt;
      &lt;item&gt;Flip SW0 on.&lt;/item&gt;
      &lt;item&gt;You should now see the keyboard LEDs rotate.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;https://youtube.com/shorts/qZe9CCA0zFE?feature=share&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;NAND2Mario made it very easy to interface an FPGA design to a USB keyboard and mouse, without requiring a ton of logic and infrastructure. I could have wrapped up the integration exercise in a couple of days or less. Adding keyboard LED control support gave me the perfect excuse to dive a little deeper. When you look under the hood, it becomes clear that it’s quite tricky to achieve the simple user interface that usb_hid_host provides.&lt;/p&gt;
    &lt;p&gt;Where the usb_hid_host core took the approach of using a tiny UKP processor with a very limited special-purpose instruction set, Pbing’s usb_hid_device core uses a J1 processor running Forth! I’m impressed by the elegance of Pbing’s approach. I have J1 and Forth on my radar now!&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;p&gt;CDC Design and Verification Techniques Using SystemVerilog.&lt;/p&gt;
    &lt;p&gt;The original NAND2Mario usb_hid_host repo&lt;/p&gt;
    &lt;p&gt;The usb_hid_host repo fork with keyboard LED control support&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://epsilon537.github.io/boxlambda/usb-hid/"/><published>2025-11-13T03:32:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45910347</id><title>Meta replaces WhatsApp for Windows with web wrapper that uses 1 GB RAM when idle</title><updated>2025-11-13T10:43:48.530410+00:00</updated><content>&lt;doc fingerprint="6c47977bf3754df"&gt;
  &lt;main&gt;
    &lt;p&gt;WhatsApp on Windows 11 has just got a ‘major’ upgrade, and you’re probably going to hate it because it simply loads web.whatsapp.com in a WebView2 container. This means WhatsApp on Windows 11 is cooked, and it’s back to being absolute garbage in terms of performance.&lt;/p&gt;
    &lt;p&gt;WhatsApp is one of those Windows apps that went from being a web wrapper to a native app and then back to the web again after all these years of investment.&lt;/p&gt;
    &lt;p&gt;WhatsApp for Windows was originally an Electron app, and it was eventually replaced with UWP after years of investment. Four years later, WhatsApp is going back to WebView2, abandoning the original WinUI/UWP native idea.&lt;/p&gt;
    &lt;head rend="h3"&gt;I blame the layoffs&lt;/head&gt;
    &lt;p&gt;My understanding is that the recent layoffs at Mark Zuckerberg-headed Meta likely disbanded the entire team behind the native WhatsApp. I don’t see any other reason why Meta would abandon its native app for Windows. Meta will save costs by maintaining the web app codebase on Windows, but you’re going to hate the experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;How bad is the new WhatsApp for Windows 11?&lt;/head&gt;
    &lt;p&gt;Our tests showed that new Chromium/WebView2-based WhatsApp for Windows 11 uses up to 300MB of RAM when you are on the login screen and doing nothing. On the other hand, the old/native WhatsApp uses just 18MB of RAM and even slips to less than 10MB when left idle on the login screen.&lt;/p&gt;
    &lt;p&gt;After logging in, WhatsApp (new) memory usage increased to 2GB while trying to load all my chats. On average, it used 1.2GB when left idle in the background.&lt;/p&gt;
    &lt;p&gt;You’d realise how bad this is when I tell you the benchmarks for the native WhatsApp for comparison. I tested the old/native WhatsApp, and it uses just 190MB most of the time, dropping to less than 100MB when it’s completely idle. At worst, it would reach 300MB, which can happen only when the chat is really active.&lt;/p&gt;
    &lt;p&gt;By the looks of things, this new WhatsApp for Windows 11 can touch 3GB RAM if you have too many active conversations.&lt;/p&gt;
    &lt;p&gt;It’s absolutely garbage, and it should not be allowed inside the Microsoft Store. You’re better off using WhatsApp on the web (Edge/Chrome) than updating/downloading this new WebView2-based app.&lt;/p&gt;
    &lt;p&gt;In fact, it appears that WhatsApp web (web.whatsapp.com) in any browser is less terrible than this WebView2 container.&lt;/p&gt;
    &lt;head rend="h3"&gt;New WhatsApp is a performance nightmare&lt;/head&gt;
    &lt;p&gt;An app can use a lot of memory, and it does not necessarily mean it’s a performance nightmare, but the issue with the new WhatsApp is that it feels sluggish. You’re going to notice sluggish performance, long loading time, and other performance issues when browsing different conversations.&lt;/p&gt;
    &lt;p&gt;We also noticed that it does not work well with Windows notifications. It also struggles with Windows 11’s Do Not Disturb mode or Active Hours. And there are delayed notifications problems as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Can you avoid this new WhatsApp upgrade on Windows 11? Yes, but not for a very long time&lt;/head&gt;
    &lt;p&gt;Windows Latest found that WhatsApp version 2.2584.3.0 replaces the native (WinUI/UWP) app and is rolling out in all regions via the Microsoft Store. Do not download it, and you might still be allowed to use the native app for the next days.&lt;/p&gt;
    &lt;p&gt;However, Windows Latest has learned that all users will be logged out eventually and forced to use the WebView2-based WhatsApp.&lt;/p&gt;
    &lt;p&gt;This ‘upgrade’ ships as the WhatsApp native experience rolls out on Apple Watch, which has 115 million consumers, while Windows has over one billion active monthly devices. Clearly, numbers are not always enough, and I am not sure if I can really blame Meta when Microsoft also does not make native apps for Windows anymore.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.windowslatest.com/2025/11/12/meta-just-killed-native-whatsapp-on-windows-11-now-it-opens-webview-uses-1gb-ram-all-the-time/"/><published>2025-11-13T03:44:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45910381</id><title>Android 16 QPR1 is being pushed to the Android Open Source Project</title><updated>2025-11-13T10:43:48.128237+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://grapheneos.social/@GrapheneOS/115533432439509433"/><published>2025-11-13T03:49:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45911704</id><title>Reverse Engineering Yaesu FT-70D Firmware Encryption</title><updated>2025-11-13T10:43:47.809418+00:00</updated><content>&lt;doc fingerprint="8e1192112a7bc6b0"&gt;
  &lt;main&gt;
    &lt;p&gt;This article dives into my full methodology for reverse engineering the tool mentioned in this article. It's a bit longer but is intended to be accessible to folks who aren't necessarily advanced reverse-engineers.&lt;/p&gt;
    &lt;p&gt;Click on any of the images to view at its original resolution.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Background&lt;/head&gt;
    &lt;p&gt;Ham radios are a fun way of learning how the radio spectrum works, and more importantly: they're embedded devices that may run weird chips/firmware! I got curious how easy it'd be to hack my Yaesu FT-70D, so I started doing some research. The only existing resource I could find for Yaesu radios was someone who posted about custom firmware for their Yaesu FT1DR.&lt;/p&gt;
    &lt;p&gt;The Reddit poster mentioned that if you go through the firmware update process via USB, the radio exposes its Renesas H8SX microcontroller and can have its flash modified using the Renesas SDK. This was a great start and looked promising, but the SDK wasn't trivial to configure and I wasn't sure if it could even dump the firmware... so I didn't use it for very long.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Other Avenues&lt;/head&gt;
    &lt;p&gt;Yaesu provides a Windows application on their website that can be used to update a radio's firmware over USB:&lt;/p&gt;
    &lt;p&gt;The zip contains the following files:&lt;/p&gt;
    &lt;code&gt;1.2 MB  Wed Nov  8 14:34:38 2017  FT-70D_ver111(USA).exe
682 KB  Tue Nov 14 00:00:00 2017  FT-70DR_DE_Firmware_Update_Information_ENG_1711-B.pdf
8 MB  Mon Apr 23 00:00:00 2018  FT-70DR_DE_MAIN_Firmware_Ver_Up_Manual_ENG_1804-B.pdf
3.2 MB  Fri Jan  6 17:54:44 2012  HMSEUSBDRIVER.exe
160 KB  Sat Sep 17 15:14:16 2011  RComms.dll
61 KB  Tue Oct 23 17:02:08 2012  RFP_USB_VB.dll
1.7 MB  Fri Mar 29 11:54:02 2013  vcredist_x86.exe
&lt;/code&gt;
    &lt;p&gt;I'm going to assume that the file specific to the FT-70D, "FT-70D_ver111(USA).exe", will likely contain our firmware image. A PE file (.exe) can contain binary resources in the &lt;code&gt;.rsrc&lt;/code&gt; section -- let's see what this file contains using XPEViewer:&lt;/p&gt;
    &lt;p&gt;Resources fit into one of many different resource types, but a firmware image would likely be put into a custom type. What's this last entry, "23"? Expanding that node we have a couple of interesting items:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;RES_START_DIALOG&lt;/code&gt; is a custom string the updater shows when preparing an update, so we're in the right area!&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;RES_UPDATE_INFO&lt;/code&gt; looks like just binary data -- perhaps this is our firmware image? Unfortunately looking at the "Strings" tab in XPEViewer or running the &lt;code&gt;strings&lt;/code&gt; utility over this data doesn't yield anything legible. The firmware image is likely encrypted.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Reverse Engineering the Binary&lt;/head&gt;
    &lt;p&gt;Let's load the update utility into our disassembler of choice to figure out how the data is encrypted. I'll be using IDA Pro, but Ghidra (free!), radare2 (free!), or Binary Ninja are all great alternatives. Where possible in this article I'll try to show my rewritten code in C since it'll be a closer match to the decompiler and machine code output.&lt;/p&gt;
    &lt;p&gt;A good starting point is the the string we saw above, &lt;code&gt;RES_UPDATE_INFO&lt;/code&gt;. Windows applications load resources by calling one of the &lt;code&gt;FindResource*&lt;/code&gt; APIs. &lt;code&gt;FindResourceA&lt;/code&gt; has the following parameters:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;HMODULE&lt;/code&gt;, a handle to the module to look for the resource in.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lpName&lt;/code&gt;, the resource name.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lpType&lt;/code&gt;, the resource type.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In our disassembler we can find references to the &lt;code&gt;RES_UPDATE_INFO&lt;/code&gt; string and look for calls to &lt;code&gt;FindResourceA&lt;/code&gt; with this string as an argument in the &lt;code&gt;lpName&lt;/code&gt; position.&lt;/p&gt;
    &lt;p&gt;We find a match in a function which happens to find/load all of these custom resources under type &lt;code&gt;23&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We know where the data is loaded by the application, so now we need to see how it's used. Doing static analysis from this point may be more work than it's worth if the data isn't operated on immediately. To speed things up I'm going to use a debugger's assistance. I used WinDbg's Time Travel Debugging to record an execution trace of the updater while it updates my radio. TTD is an invaluable tool and I'd highly recommend using it when possible. rr is an alternative for non-Windows platforms.&lt;/p&gt;
    &lt;p&gt;The decompiler output shows this function copies the &lt;code&gt;RES_UPDATE_INFO&lt;/code&gt; resource to a dynamically allocated buffer. The &lt;code&gt;qmemcpy()&lt;/code&gt; is inlined and represented by a &lt;code&gt;rep movsd&lt;/code&gt; instruction in the disassembly, so we need to break at this instruction and examine the &lt;code&gt;edi&lt;/code&gt; register's (destination address) value. I set a breakpoint by typing &lt;code&gt;bp 0x406968&lt;/code&gt; in the command window, allow the application to continue running, and when it breaks we can see the &lt;code&gt;edi&lt;/code&gt; register value is &lt;code&gt;0x2be5020&lt;/code&gt;. We can now set a memory access breakpoint at this address using &lt;code&gt;ba r4 0x2be5020&lt;/code&gt; to break whenever this data is read.&lt;/p&gt;
    &lt;p&gt;Our breakpoint is hit at &lt;code&gt;0x4047DC&lt;/code&gt; -- back to the disassembler. In IDA you can press &lt;code&gt;G&lt;/code&gt; and enter this address to jump to it. We're finally at what looks like the data processing function:&lt;/p&gt;
    &lt;p&gt;We broke when dereferencing &lt;code&gt;v2&lt;/code&gt; and IDA has automatically named the variable it's being assigned to as &lt;code&gt;Time&lt;/code&gt;. The &lt;code&gt;Time&lt;/code&gt; variable is passed to another function which formats it as a string with &lt;code&gt;%Y%m%d%H%M%S&lt;/code&gt;. Let's clean up the variables to reflect what we know:&lt;/p&gt;
    &lt;p&gt;The timestamp string is passed to &lt;code&gt;sub_4082c0&lt;/code&gt; on line 20 and the remainder of the update image is passed to &lt;code&gt;sub_408350&lt;/code&gt; on line 21. I'm going to focus on &lt;code&gt;sub_408350&lt;/code&gt; since I only care about the firmware data right now and based on how this function is called I'd wager its signature is something like:&lt;/p&gt;
    &lt;code&gt;status_t sub_408350(uint8_t *input, size_t input_len, uint8_t *output, output_len, size_t *out_data_processed);
&lt;/code&gt;
    &lt;p&gt;Let's see what it does:&lt;/p&gt;
    &lt;p&gt;I think we've found our function that starts decrypting the firmware! To confirm, we want to see what the &lt;code&gt;output&lt;/code&gt; parameter's data looks like before and after this function is called. I set a breakpoint in the debugger at the address where it's called (&lt;code&gt;bp 0x404842&lt;/code&gt;) and put the value of the &lt;code&gt;edi&lt;/code&gt; register (&lt;code&gt;0x2d7507c&lt;/code&gt;) in WinDbg's memory window.&lt;/p&gt;
    &lt;p&gt;Here's the data before:&lt;/p&gt;
    &lt;p&gt;After stepping over the function call:&lt;/p&gt;
    &lt;p&gt;We can dump this data to a file using the following command:&lt;/p&gt;
    &lt;code&gt;.writemem C:\users\lander\documents\maybe_deobfuscated.bin 0x2d7507c L100000
&lt;/code&gt;
    &lt;p&gt;010 Editor has a built-in strings utility (Search &amp;gt; Find Strings...) and if we scroll down a bit in the results, we have real strings that appear in my radio!&lt;/p&gt;
    &lt;p&gt;At this point if we were just interested in getting the plaintext firmware we could stop messing with the binary and load the firmware into IDA Pro... but I want to know how this encryption works.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Encryption Details&lt;/head&gt;
    &lt;p&gt;Just to recap from the last section:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We've identified our data processing routine (let's call this function &lt;code&gt;decrypt_update_info&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;We know that the first 4 bytes of the update data are a Unix timestamp that's formatted as a string and used for an unknown purpose.&lt;/item&gt;
      &lt;item&gt;We know which function begins decrypting our firmware image.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;# Data Decryption&lt;/head&gt;
    &lt;p&gt;Let's look at the firmware image decryption routine with some renamed variables:&lt;/p&gt;
    &lt;p&gt;At a high level this routine:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Allocates a 64-byte scratch buffer&lt;/item&gt;
      &lt;item&gt;Checks if there's any data to process. If not, set the output variable &lt;code&gt;out_data_processed&lt;/code&gt;to the number of bytes processed and return 0x0 (&lt;code&gt;STATUS_SUCCESS&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Loop over the input data in 8-byte chunks and inflate each byte to its bit representation.&lt;/item&gt;
      &lt;item&gt;After the 8-byte chunk is inflated, call &lt;code&gt;sub_407980&lt;/code&gt;with the scratch buffer and&lt;code&gt;0&lt;/code&gt;as arguments.&lt;/item&gt;
      &lt;item&gt;Loop over the scratch buffer and reassemble 8 sequential bits as 1 byte, then set the byte at the appropriate index in the output buffer.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lots going on here, but let's take a look at step #3. If we take the bytes &lt;code&gt;0xAA&lt;/code&gt; and &lt;code&gt;0x77&lt;/code&gt; which have bit representations of &lt;code&gt;0b1010_1010&lt;/code&gt; and &lt;code&gt;0b0111_1111&lt;/code&gt; respectively and inflate them to a 16-byte array using the algorithm above, we end up with:&lt;/p&gt;
    &lt;code&gt;| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |    | 8 | 9 | A | B | C | D | E | F |
|---|---|---|---|---|---|---|---|----|---|---|---|---|---|---|---|---|
| 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 |    | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 1 |
&lt;/code&gt;
    &lt;p&gt;This routine does this process over 8 bytes at a time and completely fills the 64-byte scratch buffer with 1s and 0s just like the table above.&lt;/p&gt;
    &lt;p&gt;Now let's look at step #4 and see what's going on in &lt;code&gt;sub_407980&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Oof. This is substantially more complicated but looks like the meat of the decryption algorithm. We'll refer to this function, &lt;code&gt;sub_407980&lt;/code&gt;, as &lt;code&gt;decrypt_data&lt;/code&gt; from here on out. We can see what may be an immediate roadblock: this function takes in a C++ &lt;code&gt;this&lt;/code&gt; pointer (line 5) and performs bitwise operations on one of its members (line 18, 23, etc.). For now let's call this class member &lt;code&gt;key&lt;/code&gt; and come back to it later.&lt;/p&gt;
    &lt;p&gt;This function is the perfect example of decompilers emitting less than ideal code as a result of compiler optimizations/code reordering. For me, TTD was essential for following how data flows through this function. It took a few hours of banging my head against IDA and WinDbg to understand, but this function can be broken up into 3 high-level phases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Building a 48-byte buffer containing our key material XOR'd with data from a static table.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build a 32-byte buffer containing data from an 0x800-byte static table, with indexes into this table originating from indices built from the buffer in step #1. Combine this 32-byte buffer with the 48-byte buffer in step #1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Iterate over the next 8 bytes of the output buffer. For each byte index of the output buffer, index into yet another static 32-byte buffer and use that as the index into the table from step #2. XOR this value with the value at the current index of the output buffer.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The inner loop in the &lt;code&gt;else&lt;/code&gt; branch above I think is kind of nasty, so here it is reimplemented in Rust:&lt;/p&gt;
    &lt;head rend="h3"&gt;# Key Setup&lt;/head&gt;
    &lt;p&gt;We now need to figure out how our key is set up for usage in the &lt;code&gt;decrypt_data&lt;/code&gt; function above. My approach here is to set a breakpoint at the first instruction to use the key data in &lt;code&gt;decrypt_data&lt;/code&gt;, which happens to be &lt;code&gt;xor bl, [ecx + esi + 4]&lt;/code&gt; at &lt;code&gt;0x4079d3&lt;/code&gt;. I know this is where we should break because in the decompiler output the left-hand side of the XOR operation, the key material, will be the second operand in the &lt;code&gt;xor&lt;/code&gt; instruction. As a reminder, the decompiler shows the XOR as:&lt;/p&gt;
    &lt;code&gt;v8 = *(_BYTE *)(i + 48 * v7 + v3 + 4) ^ a2[(unsigned __int8)byte_424E50[i] + 31];
&lt;/code&gt;
    &lt;p&gt;The breakpoint is hit and the address we're loading from is &lt;code&gt;0x19f5c4&lt;/code&gt;. We can now lean on TTD to help us figure out where this data was last written. Set a 1-byte memory write breakpoint at this address using &lt;code&gt;ba w1 0x19f5c4&lt;/code&gt; and press the &lt;code&gt;Go Back&lt;/code&gt; button. If you've never used TTD before, this operates exactly as &lt;code&gt;Go&lt;/code&gt; would except backwards in the program's trace. In this case it will execute backward until either a breakpoint is hit, interrupt is generated, or we reach the start of the program.&lt;/p&gt;
    &lt;p&gt;Our memory write breakpoint gets triggered at &lt;code&gt;0x4078fb&lt;/code&gt; -- a function we haven't seen before. The callstack shows that it's called not terribly far from the &lt;code&gt;decrypt_update_info&lt;/code&gt; routine!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;set_key&lt;/code&gt;(we are here -- function is originally called&lt;code&gt;sub_407850&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;sub_4082c0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;decrypt_update_info&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What's &lt;code&gt;sub_4082c0&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Not a lot to see here except the same function called 4 times, initially with the timestamp string as an argument in position 0, a 64-byte buffer, and bunch of function calls using the return value of the last as its input. The function our debugger just broke into takes only 1 argument, which is the 64-byte buffer used across all of these function calls. So what's going on in &lt;code&gt;sub_407e80&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;The bitwise operations that look supsiciously similar to the byte to bit inflation we saw above with the firmware data. After renaming things and performing some loop unrolling, things look like this:&lt;/p&gt;
    &lt;p&gt;The only mystery now is the &lt;code&gt;set_key&lt;/code&gt; routine:&lt;/p&gt;
    &lt;p&gt;This function is a bit more straightforward to reimplement:&lt;/p&gt;
    &lt;head rend="h3"&gt;# Putting Everything Together&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Update data is read from resources&lt;/item&gt;
      &lt;item&gt;The first 4 bytes of the update data are a Unix timestamp&lt;/item&gt;
      &lt;item&gt;The timestamp is formatted as a string, has each byte inflated to its bit representation, and decrypted using some static key material as the key. This is repeated 4 times with the output of the previous run used as an input to the next.&lt;/item&gt;
      &lt;item&gt;The resulting data from step 3 is used as a key for decrypting data.&lt;/item&gt;
      &lt;item&gt;The remainder of the firmware update image is inflated to its bit representation 8 bytes at a time and uses the dynamic key and 3 other unique static lookup tables to transform the inflated input data.&lt;/item&gt;
      &lt;item&gt;The result from step 5 is deflated back into its byte representation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My decryption utility which completely reimplements this magic in Rust can be found at https://github.com/landaire/porkchop.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Loading the Firmware in IDA Pro&lt;/head&gt;
    &lt;p&gt;IDA thankfully supports disassembling the Hitachi/Rensas H8SX architecture. If we load our firmware into IDA and select the "Hitachi H8SX advanced" processsor type, use the default options for the "Disassembly memory organization" dialog, then finally choose "H8S/2215R" in the "Choose the device name" dialog...:&lt;/p&gt;
    &lt;p&gt;We don't have shit. I'm not an embedded systems expert, but my friend suggested that the first few DWORDs look like they may belong to a vector table. If we right-click address 0 and select "Double word 0x142A", we can click on the new variable &lt;code&gt;unk_142A&lt;/code&gt; to go to its location. Press &lt;code&gt;C&lt;/code&gt; at this location to define it as Code, then press &lt;code&gt;P&lt;/code&gt; to create a function at this address:&lt;/p&gt;
    &lt;p&gt;We can now reverse engineer our firmware :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://landaire.net/reversing-yaesu-firmware-encryption/"/><published>2025-11-13T07:12:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45912698</id><title>Checkout.com hacked, refuses ransom payment, donates to security labs</title><updated>2025-11-13T10:43:47.627136+00:00</updated><content>&lt;doc fingerprint="dd28949d2929f588"&gt;
  &lt;main&gt;
    &lt;p&gt;Tl;dr: Last week, we were targeted by a criminal extortion attempt. The attackers gained access to a legacy, third-party cloud file storage system.Â&lt;/p&gt;
    &lt;p&gt;Our live payment processing platform was not impacted. No merchant funds or card numbers were accessed.Â&lt;/p&gt;
    &lt;p&gt;We are donating the ransom amount to fund cybercrime research.&lt;/p&gt;
    &lt;p&gt;Last week, Checkout.com was contacted by a criminal group known as âShinyHuntersâ, who claimed to have obtained data connected to Checkout.com and demanded a ransom.&lt;/p&gt;
    &lt;p&gt;Upon investigation, we determined that this data was obtained by gaining unauthorized access to a legacy third-party cloud file storage system, used in 2020 and prior years. We estimate that this would affect less than 25% of our current merchant base. The system was used for internal operational documents and merchant onboarding materials at that time.&lt;/p&gt;
    &lt;p&gt;This incident has not impacted our payment processing platform. The threat actors do not have, and never had, access to merchant funds or card numbers.&lt;/p&gt;
    &lt;p&gt;The episode occurred when threat actors gained access to this third party legacy system which was not decommissioned properly. This was our mistake, and we take full responsibility.&lt;/p&gt;
    &lt;p&gt;We are sorry. We regret that this incident has caused worry for our partners and people. We have begun the process to identify and contact those impacted and are working closely with law enforcement and the relevant regulators. We are fully committed to maintaining your trust.Â Â&lt;/p&gt;
    &lt;p&gt;We will not be extorted by criminals. We will not pay this ransom.Â&lt;/p&gt;
    &lt;p&gt;Instead, we are turning this attack into an investment in security for our entire industry. We will be donating the ransom amount to Carnegie Mellon University and the University of Oxford Cyber Security Center (OXCIS) to support their research in the fight against cybercrime.&lt;/p&gt;
    &lt;p&gt;Security, transparency and trust are the foundation of our industry. We will own our mistakes, protect our merchants, and invest in the fight against the criminal actors who threaten our digital economy.Â&lt;/p&gt;
    &lt;p&gt;We are here to assist our merchants in whatever way we can. As always, we are available through your regular Checkout point of contact for any further assistance or questions you may have.&lt;/p&gt;
    &lt;p&gt;Mariano Albera, Chief Technology Officer, Checkout.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.checkout.com/blog/protecting-our-merchants-standing-up-to-extortion"/><published>2025-11-13T09:23:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45912744</id><title>Telli (Voice AI – YC F24) is hiring ambitious engineers in [on-site, Berlin]</title><updated>2025-11-13T10:43:47.296838+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hi.telli.com/eng"/><published>2025-11-13T09:30:53+00:00</published></entry></feed>