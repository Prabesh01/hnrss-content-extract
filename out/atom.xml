<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-13T14:10:59.724848+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45529331</id><title>3D-Printed Automatic Weather Station</title><updated>2025-10-13T14:11:10.207525+00:00</updated><content>&lt;doc fingerprint="dc0dc4da61822a9d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Introduction to the online 3D-PAWS manual&lt;/p&gt;
    &lt;head rend="h3"&gt;What is 3D-PAWS?&lt;/head&gt;
    &lt;p&gt;3D-PAWS (3D-Printed Automatic Weather Station) is an international initiative that enables the local construction of reliable, low-cost weather stations using 3D printing and commercially available sensors. Developed by the University Corporation for Atmospheric Research (UCAR) and the US National Weather Service International Activities Office (NWS IAO), with support from USAID Office of U.S. Foreign Disaster Assistance (OFDA), 3D-PAWS addresses the challenges of limited weather observations in remote, rural, and underserved regions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goals of the 3D-PAWS initiative:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Expand Weather and Climate Observations:&lt;/p&gt;
        &lt;p&gt;Increase the density of surface weather and environmental monitoring in rural, remote, and underserved regions by enabling the local construction and deployment of reliable, low-cost weather stations&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reduce Weather-Related Risks:&lt;/p&gt;
        &lt;p&gt;Provide timely and accurate weather and hydrometeorological data to support early warning systems, regional decision support, and disaster risk reduction, especially in areas vulnerable to extreme weather events such as floods, droughts, and storms&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Empower Local Communities and Build Capacity:&lt;/p&gt;
        &lt;p&gt;Facilitate local ownership, assembly, and maintenance of observation networks, allowing communities, schools, and agencies to sustainably manage their own data collection infrastructure&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Promote Open Access and Innovation:&lt;/p&gt;
        &lt;p&gt;Open-source robust designs, documentation, and software to encourage widespread adoption, adaptation, and innovation in environmental sensing and data collection&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;System Overview&lt;/head&gt;
    &lt;p&gt;A very high quality 3D-PAWS surface weather station can be manufactured in about a week, at a cost of only $300-500, using locally sourced materials, microsensor technology, low-cost micro-controllers or single board computers, and a 3D printer. 3D-PAWS sensors currently measure pressure, temperature, relative humidity, wind speed, wind direction, precipitation, and visible/infrared/UV light. A range of options are available for data acquisition, data processing, and communications, including Arduino and Raspberry Pi based systems.&lt;/p&gt;
    &lt;head rend="h4"&gt;Benefits of a low-cost 3D-PAWS system:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Uses low-cost, reliable micro-sensors&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Can be assembled locally at Met Offices or other local agencies&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Components can be “re-printed” when systems fail&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Local agencies take ownership in building and maintaining observation networks&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Sensor Evaluation&lt;/head&gt;
    &lt;p&gt;3D-PAWS is being assessed at the NCAR Marshall Field Site in Boulder, CO, the NOAA Testbed facility in Sterling, VA, and at selected international locations. The Boulder site provides sampling conditions in a high-altitude semi-arid environment with subfreezing temperatures and frozen precipitation (the latter is not measured). The NOAA site provides sampling for a more temperate and humid climate near sea-level. The international 3D-PAWS sites provide an assessment of sensor performance in a variety of tropical and sub-tropical climate regimes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Station Pilot Networks&lt;/head&gt;
    &lt;p&gt;3D-PAWS systems have been deployed in the United States and in more than 17 other countries around the world. The primary focus in the United States is on testing and evaluation. The two major "success stories" are in Kenya and Barbados - in Kenya the stations are co-located with schools as part of the Globe program, while the Barbados Meteorological Service (BMS) has built and installed more than 60 stations on the island with a goal of eventually reaching 100 sites.&lt;/p&gt;
    &lt;head rend="h3"&gt;Data Access&lt;/head&gt;
    &lt;p&gt;3D-PAWS real-time data are available on the CHORDS project data servers: http://3d-kenya.chordsrt.com (Kenya) and http://3d.chordsrt.com (for testing and evaluation). CHORDS (Cloud-Hosted Real-time Data Services for Geosciences) is a US National Science Foundation (NSF) Earthcube initiative to provide a platform for sharing geosciences datasets. It is supported and managed by the UCAR/National Center for Atmospheric Research (NCAR) Earth Observing Laboratory (EOL).&lt;/p&gt;
    &lt;head rend="h3"&gt;Benefits, Impacts, and End Users&lt;/head&gt;
    &lt;p&gt;3D-PAWS observations can be used for a variety of hydrometeorological applications.&lt;/p&gt;
    &lt;head rend="h4"&gt;Example applications:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Regional weather forecasting. Observations from the 3D-PAWS network can be assimilated into regional numerical weather prediction systems such as the Weather Research and Forecast (WRF: http://www.wrf-model.org) model to improve mesoscale weather forecasts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Early alert and regional decision support systems. Real-time monitoring of precipitation in ungauged or minimally gauged river basins can provide input to flash flood guidance and early warning decision support systems to support delivery of flood alerts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Agricultural monitoring. 3D-PAWS can support water resource management tools to improve reservoir operation for fresh water supplies and the generation of hydroelectric power. Other applications include operation of irrigation systems (e.g., center pivots) and agricultural crop monitoring.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Health monitoring. 3D-PAWS can help monitor conditions leading to outbreaks of diseases such as meningitis and malaria.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Contact Information&lt;/head&gt;
    &lt;p&gt;Paul A. Kucera, Ph.D. UCAR/COMET P.O. Box 3000 Boulder, CO 80307 USA&lt;/p&gt;
    &lt;p&gt;+1. 303. 497. 2807 [email protected]&lt;/p&gt;
    &lt;p&gt;William Nicewonger UCAR/COMET P.O. Box 3000 Boulder, CO 80307 USA&lt;/p&gt;
    &lt;p&gt;+1. 303. 497. 2509 [email protected]&lt;/p&gt;
    &lt;p&gt;Last updated&lt;/p&gt;
    &lt;p&gt;Was this helpful?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://3dpaws.comet.ucar.edu"/><published>2025-10-09T15:45:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45555727</id><title>Three ways formally verified code can go wrong in practice</title><updated>2025-10-13T14:11:08.495856+00:00</updated><content>&lt;doc fingerprint="2623409772eabd7c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Three ways formally verified code can go wrong in practice&lt;/head&gt;
    &lt;head rend="h2"&gt;"Correct" doesn't mean "correct" when correctly using "correct"&lt;/head&gt;
    &lt;head rend="h3"&gt;New Logic for Programmers Release!&lt;/head&gt;
    &lt;p&gt;v0.12 is now available! This should be the last major content release. The next few months are going to be technical review, copyediting and polishing, with a hopeful 1.0 release in March. Full release notes here.&lt;/p&gt;
    &lt;head rend="h1"&gt;Three ways formally verified code can go wrong in practice&lt;/head&gt;
    &lt;p&gt;I run this small project called Let's Prove Leftpad, where people submit formally verified proofs of the eponymous meme. Recently I read Breaking “provably correct” Leftpad, which argued that most (if not all) of the provably correct leftpads have bugs! The lean proof, for example, should render &lt;code&gt;leftpad('-', 9, אֳֽ֑)&lt;/code&gt; as &lt;code&gt;---------אֳֽ֑&lt;/code&gt;, but actually does &lt;code&gt;------אֳֽ֑&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can read the article for a good explanation of why this goes wrong (Unicode). The actual problem is that correct can mean two different things, and this leads to confusion about how much formal methods can actually guarantee us. So I see this as a great opportunity to talk about the nature of proof, correctness, and how "correct" code can still have bugs.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we talk about when we talk about correctness&lt;/head&gt;
    &lt;p&gt;In most of the real world, correct means "no bugs". Except "bugs" isn't a very clear category. A bug is anything that causes someone to say "this isn't working right, there's a bug." Being too slow is a bug, a typo is a bug, etc. "correct" is a little fuzzy.&lt;/p&gt;
    &lt;p&gt;In formal methods, "correct" has a very specific and precise meaning: the code conforms to a specification (or "spec"). The spec is a higher-level description of what is supposed the code's properties, usually something we can't just directly implement. Let's look at the most popular kind of proven specification:&lt;/p&gt;
    &lt;code&gt;-- Haskell
inc :: Int -&amp;amp;gt; Int
inc x = x + 1
&lt;/code&gt;
    &lt;p&gt;The type signature &lt;code&gt;Int -&amp;gt; Int&lt;/code&gt; is a specification! It corresponds to the logical statement &lt;code&gt;all x in Int: inc(x) in Int&lt;/code&gt;. The Haskell type checker can automatically verify this for us. It cannot, however, verify properties like &lt;code&gt;all x in Int: inc(x) &amp;gt; x&lt;/code&gt;. Formal verification is concerned with verifying arbitrary properties beyond what is (easily) automatically verifiable. Most often, this takes the form of proof. A human manually writes a proof that the code conforms to its specification, and the prover checks that the proof is correct.&lt;/p&gt;
    &lt;p&gt;Even if we have a proof of "correctness", though, there's a few different ways the code can still have bugs.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. The proof is invalid&lt;/head&gt;
    &lt;p&gt;For some reason the proof doesn't actually show the code matches the specification. This is pretty common in pencil-and-paper verification, where the proof is checked by someone saying "yep looks good to me". It's much rarer when doing formal verification but it can still happen in a couple of specific cases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The theorem prover itself has a bug (in the code or introduced in the compiled binary) that makes it accept an incorrect proof. This is something people are really concerned about but it's so much rarer than every other way verified code goes wrong, so is only included for completeness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For convenience, most provers and FM languages have an "just accept this statement is true" feature. This helps you work on the big picture proof and fill in the details later. If you leave in a shortcut, and the compiler is configured to allow code-with-proof-assumptions to compile, then you can compile incorrect code that "passes the proof checker". You really should know better, though.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2. The properties are wrong&lt;/head&gt;
    &lt;p&gt;This code is provably correct:&lt;/p&gt;
    &lt;code&gt;inc :: Int -&amp;amp;gt; Int
inc x = x-1
&lt;/code&gt;
    &lt;p&gt;The only specification I've given is the type signature &lt;code&gt;Int -&amp;gt; Int&lt;/code&gt;. At no point did I put the property &lt;code&gt;inc(x) &amp;gt; x&lt;/code&gt; in my specification, so it doesn't matter that it doesn't hold, the code is still "correct".&lt;/p&gt;
    &lt;p&gt;This is what "went wrong" with the leftpad proofs. They do not prove the property "&lt;code&gt;leftpad(c, n, s)&lt;/code&gt; will take up either &lt;code&gt;n&lt;/code&gt; spaces on the screen or however many characters &lt;code&gt;s&lt;/code&gt; takes up (if more than &lt;code&gt;n&lt;/code&gt;)". They prove the weaker property "&lt;code&gt;len(leftpad(c, n, s)) == max(n, len(s))&lt;/code&gt;, for however you want to define &lt;code&gt;len(string)&lt;/code&gt;". The second is a rough proxy for the first that works in most cases, but if someone really needs the former property they are liable to experience a bug.&lt;/p&gt;
    &lt;p&gt;Why don't we prove the stronger property? Sometimes it's because the code is meant to be used one way and people want to use it another way. This can lead to accusations that the developer is "misusing the provably correct code" but this should more often be seen as the verification expert failing to educate devs on was actually "proven".&lt;/p&gt;
    &lt;p&gt;Sometimes it's because the property is too hard to prove. "Outputs are visually aligned" is a proof about Unicode inputs, and the core Unicode specification is 1,243 pages long.&lt;/p&gt;
    &lt;p&gt;Sometimes it's because the property we want is too hard to express. How do you mathematically represent "people will perceive the output as being visually aligned"? Is it OS and font dependent? These two lines are exactly five characters but not visually aligned:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;|||||&lt;/p&gt;
      &lt;p&gt;MMMMM&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Or maybe they are aligned for you! I don't know, lots of people read email in a monospace font. "We can't express the property" comes up a lot when dealing with human/business concepts as opposed to mathematical/computational ones.&lt;/p&gt;
    &lt;p&gt;Finally, there's just the possibility of a brain fart. All of the proofs in Nearly All Binary Searches and Mergesorts are Broken are like this. They (informally) proved the correctness of binary search with unbound integers, forgetting that many programming languages use machine integers, where a large enough sum can overflow.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. The assumptions are wrong&lt;/head&gt;
    &lt;p&gt;This is arguably the most important and most subtle source of bugs. Most properties we prove aren't "&lt;code&gt;X&lt;/code&gt; is always true". They are "assuming &lt;code&gt;Y&lt;/code&gt; is true, &lt;code&gt;X&lt;/code&gt; is also true". Then if &lt;code&gt;Y&lt;/code&gt; is not true, the proof no longer guarantees &lt;code&gt;X&lt;/code&gt;. A good example of this is binary &lt;del&gt;sort&lt;/del&gt; search, which only correctly finds elements assuming the input list is sorted. If the list is not sorted, it will not work correctly.&lt;/p&gt;
    &lt;p&gt;Formal verification adds two more wrinkles. One: sometimes we need assumptions to make the property valid, but we can also add them to make the proof easier. So the code can be bug-free even if the assumptions used to verify it no longer hold! Even if a leftpad implements visual alignment for all Unicode glyphs, it will be a lot easier to prove visual alignment for just ASCII strings and padding.&lt;/p&gt;
    &lt;p&gt;Two: we need make a lot of environmental assumptions that are outside our control. Does the algorithm return output or use the stack? Need to assume that there's sufficient memory to store stuff. Does it use any variables? Need to assume nothing is concurrently modifying them. Does it use an external service? Need to assume the vendor doesn't change the API or response formats. You need to assume the compiler worked correctly, the hardware isn't faulty, and the OS doesn't mess with things, etc. Any of these could change well after the code is proven and deployed, meaning formal verification can't be a one-and-done thing.&lt;/p&gt;
    &lt;p&gt;You don't actually have to assume most of these, but each assumption drop makes the proof harder and the properties you can prove more restricted. Remember, the code might still be bug-free even if the environmental assumptions change, so there's a tradeoff in time spent proving vs doing other useful work.&lt;/p&gt;
    &lt;p&gt;Another common source of "assumptions" is when verified code depends on unverified code. The Rust compiler can prove that safe code doesn't have a memory bug assuming unsafe code does not have one either, but depends on the human to confirm that assumption. Liquid Haskell is verifiable but can also call regular Haskell libraries, which are unverified. We need to assume that code is correct (in the "conforms to spec") sense, and if it's not, our proof can be "correct" and still cause bugs.&lt;/p&gt;
    &lt;p&gt;These boundaries are fuzzy. I wrote that the "binary search" bug happened because they proved the wrong property, but you can just as well argue that it was a broken assumption (that integers could not overflow). What really matters is having a clear understanding of what "this code is proven correct" actually tells you. Where can you use it safely? When should you worry? How do you communicate all of this to your teammates?&lt;/p&gt;
    &lt;p&gt;Good lord it's already Friday&lt;/p&gt;
    &lt;p&gt;If you're reading this on the web, you can subscribe here. Updates are once a week. My main website is here.&lt;/p&gt;
    &lt;p&gt;My new book, Logic for Programmers, is now in early access! Get it here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://buttondown.com/hillelwayne/archive/three-ways-formally-verified-code-can-go-wrong-in/"/><published>2025-10-12T06:17:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45559767</id><title>A years-long Turkish alphabet bug in the Kotlin compiler</title><updated>2025-10-13T14:11:08.296187+00:00</updated><content>&lt;doc fingerprint="9f00b10bcb670219"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Country That Broke Kotlin&lt;/head&gt;
    &lt;head rend="h2"&gt;Logic vs language: how a Turkish alphabet bug played a years-long game of hide-and-seek inside the Kotlin compiler&lt;/head&gt;
    &lt;p&gt;When Turkish software engineer Mehmet Nuri Öztürk posted a short message on the Kotlin discussion forum in March of 2016, he had no idea he was reporting a dangerous standard library bug that would take five years to find and fix. All he knew was that his build didn’t work.&lt;/p&gt;
    &lt;p&gt;Kotlin 1.0 had been released to the world only a month earlier, promising to breathe much-needed fresh life into the twin worlds of Java and Android development. But for Mehmet Nuri, the new programming language was a frustrating dead end. His code simply wouldn’t build, and the compiler’s output gave him nothing to go on.&lt;/p&gt;
    &lt;p&gt;He pasted the impenetrable error into his forum post:&lt;/p&gt;
    &lt;code&gt;Compilation completed with 2 errors and 0 warnings in 10s 126ms&lt;lb/&gt;&lt;lb/&gt;Error:Kotlin: Unknown compiler message tag: INFO&lt;lb/&gt;Error:Kotlin: Unknown compiler message tag: LOGGING&lt;/code&gt;
    &lt;p&gt;The Kotlin team replied quickly, but they didn’t have much to go on either. “Did you see this error just once, or do you see it every time you compile your project?”&lt;/p&gt;
    &lt;p&gt;It was consistent, Mehmet Nuri replied, not just between builds but also across different machines and operating systems.&lt;/p&gt;
    &lt;p&gt;It was a full five months before the breakthrough discovery. Muhammed Demirbaş, another programmer working in Turkey, had been running into the same mysterious build failure message, and had started to do some investigation of his own.&lt;/p&gt;
    &lt;p&gt;“I suspect that the source of the error may be my locale or language,” wrote Muhammed, commenting on Mehmet Nuri’s post. Muhammed even pinpointed the exact line of code where he thought the problem might be. “Apparently this is a uppercase–lowercase Turkish &lt;code&gt;I&lt;/code&gt; problem in the &lt;code&gt;CompilerOutputParser.CATEGORIES&lt;/code&gt; map: &lt;code&gt;I -&amp;gt; ı&lt;/code&gt;, &lt;code&gt;İ -&amp;gt; i&lt;/code&gt;.”&lt;/p&gt;
    &lt;p&gt;This was proof that Mehmet Nuri’s problems weren’t confined to his particular project, but were a symptom of something more serious going on in the compiler itself. The Kotlin team, grateful for Muhammed’s new information, filed an issue report with a link to the forum post in their YouTrack bug tracker:&lt;/p&gt;
    &lt;p&gt;“Compilation fails on Turkish locale because of locale-sensitive uppercasing.” (KT-13631)&lt;/p&gt;
    &lt;p&gt;Muhammed Demirbaş couldn’t have been more spot on in his investigation and assessment of the compiler bug. Since Kotlin is open source, he was able to search the compiler’s code for the exact line of code where that “Unknown compiler message tag” string appears:&lt;/p&gt;
    &lt;code&gt;val qNameLowerCase = qName.toLowerCase()&lt;lb/&gt;var category: CompilerMessageSeverity? = CATEGORIES[qNameLowerCase]&lt;lb/&gt;if (category == null) {&lt;lb/&gt;    messageCollector.report(ERROR, "Unknown compiler message tag: $qName")&lt;lb/&gt;    category = INFO&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;So what does this code do, and why does it sometimes go wrong?&lt;/p&gt;
    &lt;p&gt;The code is part of a class named &lt;code&gt;CompilerOutputParser&lt;/code&gt;, and is responsible for reading XML files containing messages from the Kotlin compiler. Those files look something like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;MESSAGES&amp;gt;&lt;lb/&gt;  &amp;lt;INFO path="src/main/Kotlin/Example.kt" line="1" column="1"&amp;gt;&lt;lb/&gt;    This is a message from the compiler about a line of code.&lt;lb/&gt;  &amp;lt;/INFO&amp;gt;&lt;lb/&gt;&amp;lt;/MESSAGES&amp;gt;&lt;/code&gt;
    &lt;p&gt;At the time, the tags in this file were named in all-caps: &lt;code&gt;&amp;lt;INFO/&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;ERROR/&amp;gt;&lt;/code&gt;, and so on (source: GitHub), like the HTML 1.0 webpages your grandpa used to write.&lt;/p&gt;
    &lt;p&gt;In the Kotlin code we just saw, &lt;code&gt;qName&lt;/code&gt; is the name of an XML tag that we’re parsing from this file. If we’re looking at an &lt;code&gt;&amp;lt;INFO/&amp;gt;&lt;/code&gt; tag, the &lt;code&gt;qName&lt;/code&gt; is “INFO.”&lt;/p&gt;
    &lt;p&gt;To determine what the message means, the &lt;code&gt;CompilerOutputParser&lt;/code&gt; next looks up that string in its &lt;code&gt;CATEGORIES&lt;/code&gt; map to find its corresponding &lt;code&gt;CompilerMessageSeverity&lt;/code&gt; enum entry. But wait: the keys in the &lt;code&gt;CATEGORIES&lt;/code&gt; map are lower case! (source: GitHub)&lt;/p&gt;
    &lt;code&gt;val categories = mapOf(&lt;lb/&gt;    "error" to CompilerMessageSeverity.ERROR,&lt;lb/&gt;    "info" to CompilerMessageSeverity.INFO,&lt;lb/&gt;    …&lt;lb/&gt;)&lt;/code&gt;
    &lt;p&gt;Instead of searching for “INFO,” we need to search for “info.” That’s why the code we looked at calls &lt;code&gt;qName.toLowerCase()&lt;/code&gt; before looking it up in the &lt;code&gt;CATEGORIES&lt;/code&gt; map. Here’s the code again, or at least the relevant lines:&lt;/p&gt;
    &lt;code&gt;val qNameLowerCase = qName.toLowerCase()&lt;lb/&gt;var category: CompilerMessageSeverity? = CATEGORIES[qNameLowerCase]&lt;/code&gt;
    &lt;p&gt;And that’s where the bug sneaks in.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If your computer is configured in English, &lt;code&gt;"INFO".toLowerCase()&lt;/code&gt;is&lt;code&gt;"info"&lt;/code&gt;, just like we wanted.&lt;/item&gt;
      &lt;item&gt;But if your computer is configured in Turkish, &lt;code&gt;"INFO".toLowerCase()&lt;/code&gt;turns out to be&lt;code&gt;"ınfo"&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice the difference? In the Turkish version, the lower case letter ‘ı’ has no dot above it.&lt;/p&gt;
    &lt;p&gt;The tiny discrepancy might be hard for a human to spot, but to a computer, these are two completely different strings. The dotless &lt;code&gt;"ınfo"&lt;/code&gt; string isn’t one of the keys in &lt;code&gt;CATEGORIES&lt;/code&gt; map, so the code fails to find the correct &lt;code&gt;CompilerMessageSeverity&lt;/code&gt; for our &lt;code&gt;&amp;lt;INFO/&amp;gt;&lt;/code&gt; tag, and complains that “INFO” must be a completely unknown category of message.&lt;/p&gt;
    &lt;p&gt;So why does calling &lt;code&gt;toLowerCase()&lt;/code&gt; on a Turkish computer produce this strange result?&lt;/p&gt;
    &lt;p&gt;Muhammed already provided part of the answer in his reply to Mehmet Nuri’s forum post. Turkic languages have two versions of the letter ‘i’:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;an ‘i’ with a dot, as in the word insan (human),&lt;/item&gt;
      &lt;item&gt;and a separate ‘ı’ without a dot, as in the word ırmak (river).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s more, the dotted/dotless distinction is also preserved in the upper case letters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;capital ‘i’ is ‘İ’, as in insan → İnsan,&lt;/item&gt;
      &lt;item&gt;and capital ‘ı’ is ‘I’, as in ırmak → Irmak.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That uppercase dotless ‘I’ is the same one we use in English. As a result, the single Unicode character &lt;code&gt;I&lt;/code&gt; (U+0049) has two different lower case forms: dotted &lt;code&gt;i&lt;/code&gt; (U+0069) in English, and dotless &lt;code&gt;ı&lt;/code&gt; (U+0131) in Turkish.&lt;/p&gt;
    &lt;p&gt;For Kotlin’s &lt;code&gt;toLowerCase()&lt;/code&gt; function, that’s a problem! When &lt;code&gt;toLowerCase()&lt;/code&gt; sees an &lt;code&gt;I&lt;/code&gt; character, which lower case form should it use? The lower case form of the Turkish word IRMAK should be ırmak, with no dot. But the lower case form of the English word INFO, which starts with exactly the same character, should be info, with a dot.&lt;/p&gt;
    &lt;p&gt;When you ask your computer to convert text to lower case, you should technically also specify the alphabet rules to use—English, Turkish, or something else entirely. But that’s a lot of hard work, so if you don’t specify, many systems — including, in those days, Kotlin’s &lt;code&gt;toLowerCase()&lt;/code&gt; function — will just use the language settings you chose when you set up your computer. That’s why &lt;code&gt;"INFO".toLowerCase()&lt;/code&gt; is &lt;code&gt;"ınfo"&lt;/code&gt; when you run it on a Turkish machine, and that’s why IntelliJ installations in Turkey couldn’t match the Kotlin compiler’s &lt;code&gt;&amp;lt;INFO/&amp;gt;&lt;/code&gt; messages to the lowercase &lt;code&gt;"info"&lt;/code&gt; string they were expecting to see.&lt;/p&gt;
    &lt;p&gt;But in 2016, all of that was still just a bug ticket waiting to be worked on. Muhammed Demirbaş had identified the right place to start the search, but the YouTrack issue linked to his findings was just one of hundreds of tickets in the Kotlin project backlog. With only a tiny number of people reporting that they were affected by the bug, a more thorough investigation was never a priority.&lt;/p&gt;
    &lt;p&gt;That would all change with the release of coroutines two years later, when the unassuming little bug wormed its way even deeper into the foundations of the Kotlin compiler.&lt;/p&gt;
    &lt;p&gt;October 2018 saw the release of Kotlin 1.3 — and with it, the first stable version of the new coroutines library, an innovative approach to asynchronous programming that promised to transform the Android app development experience. Coroutines had been in prerelease testing for over a year, and now that they were deemed ready for production use, Kotlin programmers of all kinds were ready to embrace them with enthusiasm.&lt;/p&gt;
    &lt;p&gt;To get the new tools, developers needed to upgrade their copy of the coroutines library from the prerelease 0.30.x version to the stable 1.0 release, at the same time as upgrading the Kotlin language and standard library to version 1.3.&lt;/p&gt;
    &lt;p&gt;Have you ever upgraded a dependency in a Kotlin or Java project? If so, you’ll know that making sure all your libraries remain compatible with one another can be a delicate juggling act. If your code references a function that’s been removed or changed in the new dependency, you’ll get a compilation error. But if the newly broken reference comes not from your own code but from another library, you won’t find out until you run the program. That’s because the code inside the library has already been compiled by its author, and isn’t being compiled or checked as part of your own build.&lt;/p&gt;
    &lt;p&gt;When one of your libraries tries to call a function that doesn’t exactly match what’s on offer in your freshly upgraded project’s new classpath, you’ll get a &lt;code&gt;NoSuchMethodError&lt;/code&gt;. As a result, when you’re upgrading dependencies—and especially if you’re upgrading several dependencies at once—the occasional &lt;code&gt;NoSuchMethodError&lt;/code&gt; is pretty much par for the course, until you figure out exactly which versions of each library are compatible with one another.&lt;/p&gt;
    &lt;p&gt;So when Kemal Atlı, an Android developer based in Turkey, ran into a &lt;code&gt;NoSuchMethodError&lt;/code&gt; while upgrading his app to use the shiny new coroutines library, it looked for all the world like just another dependency version mismatch. Kemal wasn’t having any luck fixing this one, though. Unsure if it might be a bug with the coroutines library itself, he opened a GitHub issue, pasting the stack trace from his crashed app:&lt;/p&gt;
    &lt;code&gt;java.lang.NoSuchMethodError: &lt;lb/&gt;  No static method boxİnt(I)Ljava/lang/Integer;&lt;lb/&gt;  in class Lkotlin/coroutines/jvm/internal/Boxing;&lt;/code&gt;
    &lt;p&gt;This exception already contained the vital clue—a tiny dot above the upper case letter ‘İ’ in &lt;code&gt;boxİnt()&lt;/code&gt;—but who’s going to spot that if they’re not looking for it? For now, nobody did.&lt;/p&gt;
    &lt;p&gt;“Does restarting your IDE and running a &lt;code&gt;clean build&lt;/code&gt; resolve the issue?” responded the coroutines library maintainers, immediately suspecting a version conflict. Kemal had said that the issue only happened on one of his two machines, which suggested the problem might just be an old incompatible dependency version hanging around in his build cache after the upgrade.&lt;/p&gt;
    &lt;p&gt;A week later, another bug report, from another Turkish developer seeing their app crash with the same exception. By now, the coroutines library maintainers were certain the problem could only be caused by a dependency version mismatch—a conclusion which, in any other circumstances, might have been entirely valid. They had no luck reproducing the issue on their end, which just provided further evidence that the problem was specific to the way those two issue reporters had configured and built their projects.&lt;/p&gt;
    &lt;p&gt;It took a month for someone to spot the smoking gun.&lt;/p&gt;
    &lt;p&gt;“It has to be a locale problem,” wrote Erel Özçakırlar, another Turkish software engineer, commenting on Kemal’s issue report in late December 2018. Erel pointed out what everyone had missed so far: the real function should be called &lt;code&gt;boxInt()&lt;/code&gt;, but the stack trace showed &lt;code&gt;boxİnt()&lt;/code&gt;. It wasn’t a simple case of trying to call an older version of an existing function. Instead, Kotlin had seemingly used the Turkish alphabet to invent a function name that had never existed in the first place. What’s more, Erel found he could fix the problem by running the code on a computer that used English as its system locale.&lt;/p&gt;
    &lt;p&gt;“I think Erel might have a point regarding system language settings,” replied Kemal, saying he’d look into it further. But figuring out why Kotlin’s compiler internals are suddenly inventing imaginary functions using Turkish characters — well, where would you even start? There was little for Kemal to offer beyond the original bug report, so the issue kept its “waiting for clarification” label, and was closed — along with the second similar bug report — in early 2019.&lt;/p&gt;
    &lt;p&gt;Once again, the bug was back in hiding. But this time, it had sunk its claws deep into Kotlin’s compiler internals. The misspelled &lt;code&gt;boxİnt()&lt;/code&gt; function wasn’t being called in Kemal’s own code, or even in a library he was using. Instead, the mistake was being added to his app by the compiler itself.&lt;/p&gt;
    &lt;p&gt;To understand why, we need to talk a little about how coroutines work.&lt;/p&gt;
    &lt;p&gt;Much of Kotlin’s coroutine magic takes places in the dedicated kotlinx.coroutines library. But there’s one core building block that’s more tightly integrated with the Kotlin language and its compiler: the &lt;code&gt;suspend&lt;/code&gt; keyword.&lt;/p&gt;
    &lt;p&gt;When you label a function with the &lt;code&gt;suspend&lt;/code&gt; keyword, the Kotlin compiler rewrites the function signature to make the function work asynchronously. For example, when you write a suspending function with two parameters, the corresponding output generated by the Kotlin compiler will actually include three parameters. The invisible third parameter is a &lt;code&gt;Continuation&lt;/code&gt;, which both stores the state of the function and acts as a callback to receive the function’s asynchronous result.&lt;/p&gt;
    &lt;p&gt;A &lt;code&gt;Continuation&lt;/code&gt; stores and sends all kinds of values, depending on the code inside your suspending function—and that’s where the mysterious &lt;code&gt;boxInt()&lt;/code&gt; function comes into play.&lt;/p&gt;
    &lt;p&gt;You might know that when you create an &lt;code&gt;Int&lt;/code&gt; in Kotlin, it can be stored as one of two different underlying Java types: a primitive &lt;code&gt;int&lt;/code&gt;, or an &lt;code&gt;Integer&lt;/code&gt; object. Kotlin makes the choice for you automatically, depending on how the value is used.&lt;/p&gt;
    &lt;p&gt;If you use an &lt;code&gt;Int&lt;/code&gt; in a coroutine, Kotlin will sometimes need to convert it from the primitive &lt;code&gt;int&lt;/code&gt; storage mechanism to an &lt;code&gt;Integer&lt;/code&gt; object, so that the value can pass through the generic coroutine continuation machinery. This conversion is called boxing. Java will happily perform the conversion automatically—but for the stable release of coroutines, the Kotlin team wanted to make sure the conversion was as efficient as possible.&lt;/p&gt;
    &lt;p&gt;To help the JVM optimize its execution of suspending functions, Kotlin 1.3 added a set of new functions for the compiler to use in its generated coroutine code: &lt;code&gt;boxBoolean()&lt;/code&gt;, &lt;code&gt;boxByte()&lt;/code&gt;, &lt;code&gt;boxShort()&lt;/code&gt;, &lt;code&gt;boxInt()&lt;/code&gt;, and so on (source: GitHub). Since the &lt;code&gt;suspend&lt;/code&gt; keyword is part of the core language, these functions must be available to all Kotlin programs, which is why they’re in the standard library, not the coroutines library—though they’re marked as &lt;code&gt;internal&lt;/code&gt;, and aren’t available for you to call directly.&lt;/p&gt;
    &lt;p&gt;The functions themselves aren’t the problem: they’re spelled correctly, and their implementations are so trivial that there’s nowhere for anything to go wrong. No, the bug happens when the compiler generates the code that calls these functions.&lt;/p&gt;
    &lt;p&gt;To correctly box a value, Kotlin needs to map the Java primitive type to its corresponding box function. A &lt;code&gt;boolean&lt;/code&gt; value must be passed to &lt;code&gt;boxBoolean()&lt;/code&gt;; a &lt;code&gt;byte&lt;/code&gt; value to &lt;code&gt;boxByte()&lt;/code&gt;, and so on.&lt;/p&gt;
    &lt;p&gt;There’s an obvious pattern there: capitalize the first letter of the primitive type, and then add “box” to the start. And that’s exactly what Kotlin 1.3 did, using the standard library’s &lt;code&gt;capitalize()&lt;/code&gt; function: (source: GitHub)&lt;/p&gt;
    &lt;code&gt;map[name] = "box${primitiveType.javaKeywordName.capitalize()}"&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;capitalize()&lt;/code&gt; function modifies only the first letter of a string—so &lt;code&gt;"boolean".capitalize()&lt;/code&gt; becomes &lt;code&gt;"Boolean"&lt;/code&gt;, &lt;code&gt;"int".capitalize()&lt;/code&gt; becomes &lt;code&gt;"Int"&lt;/code&gt;, and so on.&lt;/p&gt;
    &lt;p&gt;Unless you’re in Turkey.&lt;/p&gt;
    &lt;p&gt;Once again, the behaviour of &lt;code&gt;capitalize()&lt;/code&gt; can vary depending on your computer’s language settings. It’s that pesky letter ‘i’ again:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With Turkish language settings, the upper case form of &lt;code&gt;i&lt;/code&gt;is&lt;code&gt;İ&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;whereas in English, the upper case form of &lt;code&gt;i&lt;/code&gt;is&lt;code&gt;I&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re working in Turkish, the correct result for &lt;code&gt;"int".capitalize()&lt;/code&gt; is &lt;code&gt;"İnt"&lt;/code&gt;. The &lt;code&gt;capitalize()&lt;/code&gt; function has no way of knowing that “int” is a special programming keyword that needs to be treated as English text rather than Turkish. So when the Kotlin 1.3 compiler, running on a machine with Turkish language settings, needs to box up a primitive Java &lt;code&gt;int&lt;/code&gt; inside a suspending function, it’s going to generate a call to a non-existent standard library function called &lt;code&gt;boxİnt()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Oops!&lt;/p&gt;
    &lt;p&gt;It was Fatih Doğan, another Turkish programmer, who in September 2019 managed to put all the pieces together, filing the issue report that would finally lead to a fix. Fatih clearly pointed out the misplaced dot above the ‘İ’ in &lt;code&gt;boxİnt()&lt;/code&gt;, and, crucially, set up a GitHub repository with instructions to reliably reproduce the issue.&lt;/p&gt;
    &lt;p&gt;Within a day of this new detailed issue report, the Kotlin team had found the line of code that was causing the issue. Less than a week later, they had a fix ready: (source: GitHub)&lt;/p&gt;
    &lt;code&gt;map[name] = "box${primitiveType.javaKeywordName.capitalize(Locale.US)}"&lt;/code&gt;
    &lt;p&gt;Passing a specific &lt;code&gt;Locale&lt;/code&gt; to the &lt;code&gt;capitalize()&lt;/code&gt; function means it will always use the same language rules, no matter what machine you run it on. It’s an easy change: like all the case conversion functions in the Kotlin standard library, &lt;code&gt;capitalize()&lt;/code&gt; already accepted an optional &lt;code&gt;Locale&lt;/code&gt; argument. It only fell back to the system’s default &lt;code&gt;Locale&lt;/code&gt; if you didn’t specify your own.&lt;/p&gt;
    &lt;p&gt;The fix was released as part of Kotlin 1.3.6, in November 2019, finally giving Turkish developers a stable way to use suspending functions.&lt;/p&gt;
    &lt;p&gt;But that’s not the end of this story—far from it. Coroutines might be working again, but they had proved just how easy it was to fall disastrously foul of locale-sensitive case conversions. And that original build error from the start of the story still wasn’t fixed…&lt;/p&gt;
    &lt;p&gt;It took one more bug to demonstrate the true severity of the problem.&lt;/p&gt;
    &lt;p&gt;In September 2020, nearly a year after the coroutines bug had been fixed and forgotten, Muhittin Kaplan was just starting to learn Kotlin. He wrote a simple program to check his understanding of arrays:&lt;/p&gt;
    &lt;code&gt;fun main() {&lt;lb/&gt;    println("Hello, world!!!")&lt;lb/&gt;    val nums = intArrayOf(1, 2, 3, 4, 5)&lt;lb/&gt;    println(nums[2])&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;But when he ran the program, he saw a baffling error:&lt;/p&gt;
    &lt;code&gt;java.lang.NoSuchMethodError:&lt;lb/&gt;  'int[] kotlin.jvm.internal.Intrinsics$Kotlin.intArrayOf(int[])'&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;intArrayOf()&lt;/code&gt; function is one of the most basic tools in the Kotlin standard library—and it has existed in every Kotlin version since 1.0 and before. Even if it didn’t exist, or if it was being called incorrectly, the error should happen at compile time, not at runtime.&lt;/p&gt;
    &lt;p&gt;Muhittin knew something fishy was going on, and he filed a YouTrack issue describing what he was seeing.&lt;/p&gt;
    &lt;p&gt;“Hi from Türkiye,” he began.&lt;/p&gt;
    &lt;p&gt;This time, the Kotlin team knew what to look for. It wasn’t long before they had tracked down the faulty line of code in the compiler: (source: GitHub)&lt;/p&gt;
    &lt;code&gt;StringsKt.decapitalize(type.getArrayTypeName().asString()) + "Of"&lt;/code&gt;
    &lt;p&gt;Although it’s written in Java, this code is calling the &lt;code&gt;decapitalize()&lt;/code&gt; function from Kotlin’s own standard library. And once again, it’s relying on the system’s default language settings, instead of using a fixed &lt;code&gt;Locale&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The code is part of a procedure that’s responsible for configuring intrinsics—functions that don’t really have an implementation in the Kotlin standard library, but are instead replaced directly by the compiler with the corresponding Java instructions or even JVM bytecode. When you write &lt;code&gt;intArrayOf(1, 2, 3)&lt;/code&gt;, Kotlin doesn’t really call an &lt;code&gt;intArrayOf()&lt;/code&gt; function. Instead, it recognizes that &lt;code&gt;intArrayOf()&lt;/code&gt; was registered as an intrinsic, and just outputs the bytecode to create and populate an array.&lt;/p&gt;
    &lt;p&gt;Much like the &lt;code&gt;boxInt()&lt;/code&gt; function we saw before, &lt;code&gt;intArrayOf()&lt;/code&gt; is part of a wider family of functions: one array-builder function for each primitive type. The call to &lt;code&gt;type.getArrayTypeName()&lt;/code&gt; returns the name of the Kotlin class for each array—&lt;code&gt;IntArray&lt;/code&gt;, &lt;code&gt;BooleanArray&lt;/code&gt;, and so on. The corresponding function—&lt;code&gt;intArrayOf()&lt;/code&gt;, &lt;code&gt;booleanArrayOf()&lt;/code&gt;, and so on—should start with a lower case letter, so we need to call &lt;code&gt;decapitalize()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And there’s our bug.&lt;/p&gt;
    &lt;p&gt;On a machine with Turkish language settings, &lt;code&gt;"IntArray".decapitalize()&lt;/code&gt; (or &lt;code&gt;StringsKt.decapitalize("IntArray")&lt;/code&gt;, as it appears in Java) returns &lt;code&gt;"ıntArray"&lt;/code&gt;, with that all-too-familiar dotless lowercase ‘ı’. Add the &lt;code&gt;"Of"&lt;/code&gt; suffix, and we’ve just registered an intrinsic bytecode implementation for a function called &lt;code&gt;ıntArrayOf()&lt;/code&gt;—not the same as the &lt;code&gt;intArrayOf()&lt;/code&gt; function that the standard library is advertising!&lt;/p&gt;
    &lt;p&gt;When they came to fix this issue, the Kotlin team weren’t leaving anything to chance. They scoured the entire compiler codebase for case-conversion operations—calls to &lt;code&gt;capitalize()&lt;/code&gt;, &lt;code&gt;decapitalize()&lt;/code&gt;, &lt;code&gt;toLowerCase()&lt;/code&gt;, and &lt;code&gt;toUpperCase()&lt;/code&gt;—and replaced them with locale-invariant alternatives. 173 lines of code changed, across 53 files—including the compiler-output XML parser that had caused Mehmet Nuri Öztürk’s build to fail in Kotlin 1.0, all those years ago (source: GitHub).&lt;/p&gt;
    &lt;p&gt;The slew of fixes was released as part of a more general compiler upgrade project in Kotlin 1.5, in May 2021. After five years in the backlog, KT-13631 was finally closed.&lt;/p&gt;
    &lt;p&gt;Three different bugs—in compiler outputs, coroutines, and arrays—caused by three different functions—&lt;code&gt;toLowerCase()&lt;/code&gt;, &lt;code&gt;capitalize()&lt;/code&gt;, and &lt;code&gt;decapitalize()&lt;/code&gt;. Without a more foundational solution, Kotlin’s case-conversion trap was just waiting to claim its next victim.&lt;/p&gt;
    &lt;p&gt;Even before Kotlin 1.5 was released, the Kotlin team were hard at work on a project to make sure locale-sensitive case conversions would never crash another Kotlin program.&lt;/p&gt;
    &lt;p&gt;In October of 2020, they published KEEP-223, “Locale-agnostic case conversions by default”—a proposal to replace Kotlin’s case-conversion functions with a new set of functions that would ignore your system’s language settings and simply default to a fixed locale. The new &lt;code&gt;uppercase()&lt;/code&gt; and &lt;code&gt;lowercase()&lt;/code&gt; functions were added to the standard library in Kotlin 1.5, and as of Kotlin 2.1, using the older &lt;code&gt;toLowerCase()&lt;/code&gt; and &lt;code&gt;toUpperCase()&lt;/code&gt; functions generates an error.&lt;/p&gt;
    &lt;p&gt;What about &lt;code&gt;capitalize()&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;When KEEP-223 was being discussed, it gradually became clear that &lt;code&gt;capitalize()&lt;/code&gt; had more problems than just locale sensitivity. The function name itself is surprisingly ambiguous. Look up capitalize in almost any English dictionary and you’ll find two competing definitions. Here’s one example from Collins:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;6. to print or write (a word or words) in capital letters&lt;/p&gt;
      &lt;p&gt;7. to begin (a word) with a capital letter&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Kotlin’s &lt;code&gt;capitalize()&lt;/code&gt; function had always been designed to modify only the first letter of a string, and this was the perfect opportunity to clear up the confusion. If &lt;code&gt;capitalize()&lt;/code&gt; was an ambiguous name, what should its replacement be called? Can you think of a name that describes the function’s behaviour more clearly?&lt;/p&gt;
    &lt;p&gt;In the end, the Kotlin team chose not to provide a replacement at all. If the function doesn’t exist, it can’t cause confusion or bugs! In modern Kotlin, when you want to modify the first character of a string, you provide a custom lambda to &lt;code&gt;replaceFirstChar { … }&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Kotlin 2.1 was released in November 2024, drawing our story to a satisfying close.&lt;/p&gt;
    &lt;p&gt;What can we learn from it? I think the biggest lesson is just how much responsibility rests on a language’s standard library. It’s easy to think of the standard library as simply a starter pack of stock algorithms and data structures. But dig just a little deeper, and you’ll find that even the simplest string operations rely on a detailed digital model of the complexity and creativity of human culture.&lt;/p&gt;
    &lt;p&gt;Much of that digital model, by the way, is provided by the Unicode Common Locale Data Repository (CLDR), which documents language rules, date and time formats, measurement units, currencies, and much more. Unicode isn’t just for emojis!&lt;/p&gt;
    &lt;p&gt;Now, there’s just one question that’s still bugging me. Did Mehmet Nuri Öztürk ever get that app to build?&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;I write books, too. If you want more Kotlin oddities and compiler quirks, check out my puzzle book, Kotlin Brain Teasers:&lt;/p&gt;
    &lt;p&gt;And if you enjoyed learning about suspending functions and coroutines, you might like Kotlin Coroutine Confidence:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sam-cooper.medium.com/the-country-that-broke-kotlin-84bdd0afb237"/><published>2025-10-12T17:02:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45559857</id><title>Wireguard FPGA</title><updated>2025-10-13T14:11:07.772735+00:00</updated><content>&lt;doc fingerprint="b5d9dc196105abe7"&gt;
  &lt;main&gt;
    &lt;p&gt;Virtual Private Networks (VPNs) are the central and indispensable component of Internet security. They comprise a set of technologies that connect geographically dispersed, heterogeneous networks through encrypted tunnels, creating the impression of a homogenous private network on the public shared physical medium.&lt;/p&gt;
    &lt;p&gt;With traditional solutions (such as OpenVPN / IPSec) starting to run out of steam, Wireguard is increasingly coming to the forefront as a modern, secure data tunneling and encryption method, one that's also easier to manage than the incumbents. Both software and hardware implementations of Wireguard already exist. However, the software performance is far below the speed of wire. Existing hardware approaches are both prohibitively expensive and based on proprietary, closed-source IP blocks and tools.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The intent of this project is to bridge these gaps with an FPGA open-source implementation of Wireguard, written in SystemVerilog HDL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We have contributed to the Blackwire project, which is a 100Gbps hardware implementation of Wireguard switch based on AMD/Xilinx-proprietary AlveoU50 PC-accelerator card (SmartNIC form-factor), and implementable only with proprietary Vivado toolchain.&lt;/p&gt;
    &lt;p&gt;While working on the Blackwire, we have touched multiple sections, and focused on the novel algorithm for Balanced Binary Tree Search of IP tables. However, the Blackwire hardware platform is expensive and priced out of reach of most educational institutions. Its gateware is written in SpinalHDL, a nice and powerfull but a niche HDL, which has not taken roots in the industry. While Blackwire is now released to open-source, that decision came from their financial hardship -- It was originaly meant for sale. Moreover, the company behind it is subject to disputes and obligations that bring into question the legality of ownership over the codebase they "donated" to the open source community.&lt;/p&gt;
    &lt;p&gt;To make the hardware Wireguard truly accessible in the genuine spirit of open-source movement, this project implements it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;for an inexpensive hardware platform with four 1000Base-T ports&lt;/item&gt;
      &lt;item&gt;in a self-sufficient way, i.e. w/o requiring PC host&lt;/item&gt;
      &lt;item&gt;using a commodity Artix7 FPGA&lt;/item&gt;
      &lt;item&gt;which is supported by open-source tools&lt;/item&gt;
      &lt;item&gt;and with all gateware written in the ubiquitous Verilog / System Verilog&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;[Ref1] Wireguard implementations in software:&lt;/p&gt;
    &lt;p&gt;[Ref2] 100Gbps Blackwire Wireguard&lt;/p&gt;
    &lt;p&gt;[Ref3] Corundum, open-source FPGA-NIC platform&lt;/p&gt;
    &lt;p&gt;[Ref4] ChaCha20-Poly1305 open-source Crypto RTL&lt;/p&gt;
    &lt;p&gt;[Ref5] Cookie Cutter SOC&lt;/p&gt;
    &lt;p&gt;[Ref6] RISC-V ISS&lt;/p&gt;
    &lt;p&gt;[Ref7] 10Gbps Ethernet Switch&lt;/p&gt;
    &lt;p&gt;[Ref8] OpenXC7 open-source tools for Xilinx Series7&lt;/p&gt;
    &lt;p&gt;[Ref9] Alex's Ethernet Stack&lt;/p&gt;
    &lt;p&gt;[Ref10] Amina's ADASEC-SDN&lt;/p&gt;
    &lt;p&gt;The Phase1 (This!) is primarily Proof of Concept, i.e. not full-featured, and definitely not a deployable product. It is envisoned as a mere on-ramp, a springboard for future build-up and optimizations.&lt;/p&gt;
    &lt;p&gt;The Phase2 continuation project is therefore also in the plans, to maximize efficiency and overall useability, such as by increasing the number of channels, facilitating management with GUI apps, or something else as identified by the community feedback.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;HW/SW partitioning, interface, interactions and workload distribution&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;While, contrary to Blackwire, we don’t rely on an external PC connected via PCIE, we will still have an on-chip RISC-V CPU with intricate hardware interface and significant Embedded Software component that controls the backbone of wire-speed datapath&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HW/SW co-development, integration and debugging&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Standard simulation is impractical for the project of this size and complexity. We therefore intend to put to test and good use the very promissing new VProc ISS [Ref6]&lt;/item&gt;
          &lt;item&gt;It’s also impractical and expensive to provide full test systems with real traffic generators and checkers to all developers. We therefore plan to rent some space for a central lab that will host two test systems, then provide remote access to all developers&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-life, at-speed testing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Extent of open-source tools support for SystemVerilog and all needed FPGA primitives and IP functions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;QOR of the (still maturing) open-source tools&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Blackwire used commercial, AMD/Xilinx-proprietary Vivado toolchain, as well as high-end Alveo U50 FPGA silicon. Even then, they ran into multiple timing closure, utilization and routing congestion challenges.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Financial resources&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Given that this is a complex, multi-disciplinary dev effort, the available funding may not be sufficient to bring it to completion. Blackwire, despite a larger allocated budget, ended up with funding crisis and abrupt cessation of dev activities.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is WIP at the moment. The checkmarks below indicate our status. Until all checkmarks are in place, anything you get from here is w/o guaranty -- Use at own risk, as you see fit, and don't blame us if it is not working 🌤️&lt;/p&gt;
    &lt;p&gt;Board bring up. In-depth review of Wireguard ecosystem and prior art. Design Blueprint&lt;/p&gt;
    &lt;p&gt;While the board we're using is low cost, it is also not particularly known in the open-source community. We certainly don’t have prior experience with it. In this opening take we will build a solid foundation for efficient project execution. Good preparation is crucial for a smooth run. We thus seek to first &lt;code&gt;understand and document what we will be designing: SOC Architecture, Datapath Microarchitecture, Hardware/Software Partitioning, DV and Validation Strategy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Getting a good feel for our Fmax is also a goal of this take. Artix-7 does not support High-Performance (HP) I/O. Consequently, we cannot push its I/O beyond 600MHz, nor its core logic beyond 100 MHz.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Familiarization with HW platform&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Create our first FPGA program that blinks LEDs&lt;/item&gt;
          &lt;item&gt;Verify pinouts and connectivity using simple test routines&lt;/item&gt;
          &lt;item&gt;Generate a few Ethernet test patterns&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Familiarization with SW platform&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Initial bring up of embedded CPU within a cookie-cutter SOC, such as [Ref5]&lt;/item&gt;
          &lt;item&gt;Design and test a simple SW interface to rudimentary HW Ethernet datapath&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Detailed analysis and comparisons of:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Wireguard White Papers&lt;/item&gt;
          &lt;item&gt;existing implementations in software [Ref1]&lt;/item&gt;
          &lt;item&gt;vs. Blackwire hardware implementation [Ref2]&lt;/item&gt;
          &lt;item&gt;cryptographic algorithms used for Wireguard, esp. ChaCha20 for encryption, Poly1305 for authentication [Ref4] and, to a lesser extent, Curve25519 for key exchange and blake2 for hashing&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Identification and assimilation of prior art and building IP blocks, in particular Corundum [Ref3] and, to a lesser extent, 10GE Switch [Ref7]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Architecture/uArch Design. HW/SW Partitioning. Verification Plan&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creation of sufficient initial documentation for project divide-and-conquer across a multi-disciplinary team of half a dozen developers&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Implementation of a basic, statically pre-configured Wireguard link&lt;/p&gt;
    &lt;p&gt;It it in this take that we start creating hardware Datapath and hardening Wireguard encryption protocols, all using Vivado and Xilinx primitives.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Integration of collected RTL blocks into a coherent HW system that implements the basic Wireguard datapath for a handful of manually pre-configured channels.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Corundum FPGA-based NIC and platform for opensource Ethernet development [Ref3]&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;IP Core for ChaCha20-Poly1305 [Ref4] -- Definitely in hardware from the get-go&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Curve25519 module for key exchange -- Likely in software at this point&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;blake2 module for hashing (we'll most likely do it in software)&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Timing closure. Resolution of FPGA device utilization and routing congestion issues&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creation of cocoTB DV in the CI/CD environmenT, and representative test cases for datapath simulation&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Development and integration of embedded management software (Control Plane)&lt;/p&gt;
    &lt;p&gt;This work package is about hardware/software codesign and integration. The firmware will run on a soft RISC V processor, inside the FPGA. Our vanilla SOC is at this point starting to be customized to Wireguard needs. This work can to some extent go on in parallel with hardware activities of Take2.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;SW design for on-chip processor (Part 1)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Code is to be written in the bare-metal C with, as necessary, a few sections in Assembly&lt;/item&gt;
          &lt;item&gt;SW is responsible for configuration and management of hardware blocks&lt;/item&gt;
          &lt;item&gt;SW must not participate in the bulk datapath transfers&lt;/item&gt;
          &lt;item&gt;SW may however intercept the low-frequency management packets&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SW design for on-chip processor (Part 2)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;KMM function -- Key Management Module&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HW/SW Integration&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;VPN Tunnel: Session initialization, maintenance, and secure closure&lt;/p&gt;
    &lt;p&gt;This is about managing the bring-up, maintenance and tear-down of VPN tunnels between two devices.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Session Initialization: Starting the handshake process to establish secure communication with another device&lt;/item&gt;
      &lt;item&gt;Session Maintenance: Keeping the session active through the regular exchange of control messages, which allows detection and recovery from problems such as connection interruptions&lt;/item&gt;
      &lt;item&gt;Session Closure: Securely close the VPN tunnel when communication is no longer needed, ensuring that all temporary keys and sensitive data are deleted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testing, Profiling and Porting to OpenXC7&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Functional testing on the real system. Does it work as intended? Bug fixes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Performance testing. HW/SW profiling, updates and enhancements to ensure the design indeed operates at close to the wire speed on all preconfigured channels&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Porting to openXC7 [Ref8] using SV2V, in the GoCD CI/CD setting&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;This is challenging, as openXC7 has thus far been crashing for NES SV&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Timing closure with openXC7&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;This is definitely challenging, given that openXC7 is currently without accurate timing-driven STA&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Filing bug tickets with open source developers for issues found in their tools, supporting them all the way to the resolution&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creation and maintenance of an attractive and well-documented Github repo, to entice community interest&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ongoing documentation updates and CI/CD script maintenance to keep it valid in the light of inevitable design mutations compared to the original Design Blueprint.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Flow control module for efficient and stable VPN tunnel data management&lt;/p&gt;
    &lt;p&gt;The objective of this optional deliverable is to ensure stable and efficient links, thus taking this project one step closer to a deployable product.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Develop software components for management of data flow within VPN tunnels&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since the WireGuard node essentially functions as an IP router with WireGuard protocol support, we have decided to design the system according to a two-layer architecture: a control plane responsible for managing IP routing processes and executing the WireGuard protocol (managing remote peers, sessions, and keys), and a data plane that will perform IP routing and cryptography processes at wire speed. The control plane will be implemented as software running on a soft CPU, while the data plane will be fully implemented in RTL on an FPGA.&lt;/p&gt;
    &lt;p&gt;In the HW/SW partitioning diagram, we can observe two types of network traffic: control traffic, which originates from the control plane and goes toward the external network (and vice versa), and data traffic, which arrives from the external network and, after processing in the data plane, returns to the external network. Specifically, control traffic represents WireGuard protocol handshake messages, while data traffic consists of end-user traffic, either encrypted or in plaintext, depending on the perspective.&lt;/p&gt;
    &lt;p&gt;The hardware architecture essentially follows the HW/SW partitioning and consists of two domains: a soft CPU for the control plane and RTL for the data plane.&lt;/p&gt;
    &lt;p&gt;The soft CPU is equipped with a Boot ROM and a DDR3 SDRAM controller for interfacing with off-chip memory. External memory is exclusively used for control plane processes and does not store packets. The connection between the control and data planes is established through a CSR-based HAL.&lt;/p&gt;
    &lt;p&gt;The data plane consists of several IP cores, including data plane engine (DPE) and supporting components, which are listed and explained in the direction of network traffic propagation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PHY Controller - initial configuration of Realtek PHYs and monitoring link activity (link up/down events)&lt;/item&gt;
      &lt;item&gt;1G MAC - execution of the 1G Ethernet protocol (framing, flow control, FCS, etc.)&lt;/item&gt;
      &lt;item&gt;Rx FIFOs - clock domain crossing, bus width conversion, and store &amp;amp; forward packet handling&lt;/item&gt;
      &lt;item&gt;Per-Packet Round Robin Multiplexer - servicing Rx FIFOs on a per-packet basis using a round-robin algorithm&lt;/item&gt;
      &lt;item&gt;Header Parser - extraction of WireGuard-related information from packet headers (IP addresses, UDP ports, WireGuard message type, peer ID, etc.)&lt;/item&gt;
      &lt;item&gt;Wireguard/UDP Packet Disassembler - decapsulation of the payload from the Wireguard data packet for decryption of tunneled traffic&lt;/item&gt;
      &lt;item&gt;ChaCha20-Poly1305 Decryptor - decryption and authentication of tunneled traffic&lt;/item&gt;
      &lt;item&gt;IP Lookup Engine - routing/forwarding table lookup, mapping packets to the appropriate WireGuard peer, and making packet accept/reject decisions&lt;/item&gt;
      &lt;item&gt;ChaCha20-Poly1305 Encryptor - encryption and authentication of traffic to be tunneled&lt;/item&gt;
      &lt;item&gt;Wireguard/UDP Packet Assembler - encapsulation of the encrypted packet into a WireGuard data packet for tunneling to the remote peer&lt;/item&gt;
      &lt;item&gt;Per-Packet Demultiplexer - forwarding packets to Tx FIFOs based on packet type and destination&lt;/item&gt;
      &lt;item&gt;Tx FIFOs - clock domain crossing, bus width conversion, and store &amp;amp; forward packet handling&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ChaCha20-Poly1305 Encryptor/Decryptor are using RFC7539's AEAD (Authenticated Encryption Authenticated Data) construction based on ChaCha20 for symmetric encryption and Poly1305 for authentication.&lt;/p&gt;
    &lt;p&gt;The details of hardware architecture can be found in the README.md in the &lt;code&gt;1.hw/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;The conceptual class diagram provides an overview of the components in the software part of the system without delving into implementation details. The focus is on the WireGuard Agent, which implements the protocol's handshake procedures, along with the following supplementary components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Curve25519 - an ECDH algorithm implementation for establishing a shared secret using a public-private key pair between two remote parties connected via an insecure channel, such as the Internet&lt;/item&gt;
      &lt;item&gt;ChaCha20-Poly1305 - an AEAD algorithm implementation for encryption and authentication of static keys and nonce values to prevent replay attacks&lt;/item&gt;
      &lt;item&gt;XChaCha20-Poly1305 - a XAEAD algorithm implementation for encrypting and authenticating nonce values in Cookie Replay messages to mitigate potential DoS attacks&lt;/item&gt;
      &lt;item&gt;BLAKE2s - an implementation of the BLAKE2s hash function for MAC authentication and keyed hashing, per RFC7693&lt;/item&gt;
      &lt;item&gt;RNG - a random number generator used to initialize the DH key generator and generate peer identifiers&lt;/item&gt;
      &lt;item&gt;Timer - timers for rekey, retry, and keepalive procedures&lt;/item&gt;
      &lt;item&gt;HKDF - an implementation of the algorithm for expanding the ECDH result&lt;/item&gt;
      &lt;item&gt;RTC - a real-time clock used to generate the TAI64N timestamp&lt;/item&gt;
      &lt;item&gt;SipHash - a simple non-cryptographic function used for implementing a hashtable for fast lookup of decrypted static public keys of remote peers&lt;/item&gt;
      &lt;item&gt;Routing DB Updater - a subsystem for maintaining the cryptokey routing table content and deploying it to the data plane via the HAL/CSR interface&lt;/item&gt;
      &lt;item&gt;ICMP - implementing basic ICMP protocol functions (echo request/reply, TTL exceeded, etc.)&lt;/item&gt;
      &lt;item&gt;CLI - a USB/UART-based command-line interface for configuring the WireGuard node (setting the local IP address, remote peer IP addresses, network addresses, keys, etc.)&lt;/item&gt;
      &lt;item&gt;HAL/CSR Driver - a CSR-based abstraction for data plane components with an interface for reading/writing the corresponding registers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The details of software architecture can be found in the README.md in the &lt;code&gt;2.sw/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;To illustrate the operation of the system as a whole, we have prepared a step-by-step analysis of packets processing based on the capture of real WireGuard traffic. The experimental topology consists of four nodes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10.10.0.2 - the end-user host at site A&lt;/item&gt;
      &lt;item&gt;10.9.0.1 - WireGuard peer A&lt;/item&gt;
      &lt;item&gt;10.9.0.2 - WireGuard peer B&lt;/item&gt;
      &lt;item&gt;10.10.0.1 - the end-user host at site B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The detailed analysis can be found in the README.md in the &lt;code&gt;1.hw/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;The Wireguard FPGA test bench aims to have a flexible approach to simulation which allows a common test environment to be used whilst selecting between alternative CPU components, one of which uses the VProc virtual processor co-simulation element. This allows simulations to be fully HDL, with a RISC-V processor RTL implementation such as picoRV32, IBEX or EDUBOS5, or to co-simulate software using the virtual processor, with a significant speed up in simulation times. The test bench has the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A VProc virtual processor based &lt;code&gt;soc_cpu.VPROC&lt;/code&gt;component&lt;list rend="ul"&gt;&lt;item&gt;Selectable between this or an RTL softcore&lt;/item&gt;&lt;item&gt;Can run natively compiled test code&lt;/item&gt;&lt;item&gt;Can run the application compiled natively with the auto-generated co-sim HAL&lt;/item&gt;&lt;item&gt;Can run RISC-V compiled code using the rv32 RISC-V ISS model&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Uses a C sparse memory model &lt;list rend="ul"&gt;&lt;item&gt;An HDL component instantiated in logic gives logic access to this memory&lt;/item&gt;&lt;item&gt;An API is provided to VProc running code for direct access&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The udpIpPg VIP is used to drive the logic's four ethernet ports in a four port &lt;code&gt;bfm_ethernet&lt;/code&gt;block.&lt;list rend="ul"&gt;&lt;item&gt;An MDIO slave interface is also provided that maps mem_model memory areas to the registers with instantiated mem_model components&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The figure below shows an oveview block diagram of the test bench HDL.&lt;/p&gt;
    &lt;p&gt;More details on the architecture and usage of the Wireguard test bench can be found in the README.md in the &lt;code&gt;4.sim&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;The Wireguard control and status register harware abstraction layer (HAL) software is auto-generated, as is the CSR RTL, using &lt;code&gt;peakrdl&lt;/code&gt;. For co-simulation purposes an additional layer is auto-generated from the same SystemRDL specification using &lt;code&gt;systemrdl-compiler&lt;/code&gt; that accompanies the &lt;code&gt;peakrdl&lt;/code&gt; tools. This produces two header files that define a common API to the application layer for both the RISC-V platform and the VProc based co-simulation verification environment. The details of the HAL generation can be found in the README.md in the &lt;code&gt;3.build/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;WIP&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verilator v5.024&lt;/item&gt;
      &lt;item&gt;VProc v1.12.2&lt;/item&gt;
      &lt;item&gt;Mem Model v1.0.0&lt;/item&gt;
      &lt;item&gt;rv32 ISS v1.1.4&lt;/item&gt;
      &lt;item&gt;udpIpPg v1.0.3&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;TODO&lt;/p&gt;
    &lt;p&gt;We are grateful to NLnet Foundation for their sponsorship of this development activity.&lt;/p&gt;
    &lt;p&gt;The wyvernSemi's wisdom and contribution made a great deal of difference -- Thank you, we are honored to have you on the project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/chili-chips-ba/wireguard-fpga"/><published>2025-10-12T17:12:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45561428</id><title>Ask HN: What are you working on? (October 2025)</title><updated>2025-10-13T14:11:05.245014+00:00</updated><content>&lt;doc fingerprint="3748985369a5bff8"&gt;
  &lt;main&gt;
    &lt;p&gt;Last year, PlasticList found plastic chemicals in 86% of tested foods—including 100% of baby foods they tested. Around the same time, the EU lowered its “safe” BPA limit by 20,000×, while the FDA still allows levels roughly 100× higher than Europe’s new standard.&lt;/p&gt;
    &lt;p&gt;That seemed solvable.&lt;/p&gt;
    &lt;p&gt;Laboratory.love lets you crowdfund independent lab testing of the specific products you actually buy. Think Consumer Reports × Kickstarter, but focused on detecting endocrine disruptors in your yogurt, your kid’s snacks, or whatever you’re curious about.&lt;/p&gt;
    &lt;p&gt;Find a product (or suggest one), contribute to its testing fund, and get full lab results when testing completes. If a product doesn’t reach its goal within 365 days, you’re automatically refunded. All results are published publicly.&lt;/p&gt;
    &lt;p&gt;We use the same ISO 17025-accredited methodology as PlasticList.org, testing three separate production lots per product and detecting down to parts-per-billion. The entire protocol is open.&lt;/p&gt;
    &lt;p&gt;Since last month’s “What are you working on?” post:&lt;/p&gt;
    &lt;p&gt;- 4 more products have been fully funded (now 10 total!)&lt;/p&gt;
    &lt;p&gt;- That’s 30 individual samples (we do triplicate testing on different batches) and 60 total chemical panels (two separate tests for each sample, BPA/BPS/BPF and phthalates)&lt;/p&gt;
    &lt;p&gt;- 6 results published, 4 in progress&lt;/p&gt;
    &lt;p&gt;The goal is simple: make supply chains transparent enough that cleaner ones win. When consumers have real data, markets shift.&lt;/p&gt;
    &lt;p&gt;On https://laboratory.love/faq you say: "We never accept funding from companies whose products we might test. All our funding comes from individual contributors." On https://laboratory.love/blog you say: "If you're a product manufacturer interested in having your product tested, we welcome your participation in funding."&lt;/p&gt;
    &lt;p&gt;1. An example result is "https://laboratory.love/product/117", which is a list of chemicals and measurements. Is there a visualization of how these levels relate to regulations and expert recommendations? What about a visualization of how different products in the same category compare, so that consumers know which brand is supposedly "best"? Maybe a summary rating, as stars or color-coded threat level?&lt;/p&gt;
    &lt;p&gt;2. If you find regulation-violating (or otherwise serious) levels of undesirable chemicals, do you... (a) report it to FDA; (b) initiate a class-action lawsuit; (c) short the brand's stock and then news blitz; or (d) make a Web page with the test results for people to do with it what they will?&lt;/p&gt;
    &lt;p&gt;3. Is 3 tests enough? On the several product test results I clicked, there's often wide variation among the 3 samples. Or would the visualization/rating tell me that all 3 numbers are unacceptably bad, whether it's 635.8 or 6728.6?&lt;/p&gt;
    &lt;p&gt;4. If I know that plastic contamination is a widespread problem, can I secretly fund testing of my competitors' products, to generate bad press for them?&lt;/p&gt;
    &lt;p&gt;5. Could this project be shut down by a lawsuit? Could the labs be?&lt;/p&gt;
    &lt;p&gt;1. I'm still working to make results more digestible and actionable. This will include the %TDI toggle (total daily intake, for child vs adult and USA vs EU) as seen on PlasticList, but I'm also tinkering with an even more consumer-friendly 'chemical report card'. The final results page would have both the card and the detailed table of results.&lt;/p&gt;
    &lt;p&gt;2. I have not found any regulation-violating levels yet, so in some sense, I'll cross that bridge when I get there. Part of the issue here is that many believe the FDA levels are far too relaxed which is part of why demand for a service like laboratory.love exists.&lt;/p&gt;
    &lt;p&gt;3. This is part of the challenge that PlasticList faced, and additionally a lot of my thinking around the chemical report card are related to this. Some folks think a single test would be sufficient to catch major red flags. I think triplicate testing is a reasonable balance of statistically robust while not being completely cost-prohibitive.&lt;/p&gt;
    &lt;p&gt;4. Yes, I suppose one could do that, as long as the funded products can be acquired by laboratory.love anonymously through their normal consumer supply chains. Laboratory.love merely acquires three separate batches of a given product from different sources, tests them at an ISO/IEC 17025-accredited lab, and publishes the data.&lt;/p&gt;
    &lt;p&gt;5. I suppose any project can be shut down by a lawsuit, but laboratory.love is not currently breaking any laws as far as I'm aware.&lt;/p&gt;
    &lt;p&gt;The UK levels are more strict and generally more up to date, which I personally follow rather than FDA. Could be nice to show those violations as a comparison to FDA.&lt;/p&gt;
    &lt;p&gt;What bugs me is that plastics manufacturers advertise "BPA-free", which is technically correct, but then add a very similar chemical from the same family that has the same effect on the plastic - which is good - but also has the same effect on your endocrine system&lt;/p&gt;
    &lt;p&gt;I'll add subscriptions as a more formal option on laboratory.love soon!&lt;/p&gt;
    &lt;p&gt;Disclaimer: I don't think I can have a 365-day refund with a recurring donations like this. The financial infrastructure would add too much complexity.&lt;/p&gt;
    &lt;p&gt;Serious question: around 1900 meat was often preserved using formaldehyde, and milk was adulterated with water and chalk, and sometimes with pureed calf brains to simulate cream.&lt;/p&gt;
    &lt;p&gt;I hope we can agree that we are better off than that now.&lt;/p&gt;
    &lt;p&gt;What I'm curious about is whether you think it's been a steady stream of improvements, and we just need to improve further? Or if you think there was some point between 1900 and now where food health and safety was maximized, greater than either 1900 or now, and we've regressed since then?&lt;/p&gt;
    &lt;p&gt;Trying to collapse high dimensional, complex phenomena onto a single axis usually gives one a fake sense of certainty. One should avoid it as much as possible.&lt;/p&gt;
    &lt;p&gt;And yet we report gun deaths per year, smoking rates, sea warming, etc. etc. The error isn't in producing or considering an aggregate result, but in ignoring where it came from. Since this is an internet forum and not a policy think tank I think that error is largely moot.&lt;/p&gt;
    &lt;p&gt;Or put another way: it was a simple question that the ggp can answer or not as they choose. I was just curious for their perspective.&lt;/p&gt;
    &lt;p&gt;My instinct is that things have largely gotten better over time. At a super-macro level, in 1900 we had directly adulterated food that e.g. the soldiers receiving Chicago meat called "embalmed". In the mid-20th century we had waterways that caught fire and leaded gas.&lt;/p&gt;
    &lt;p&gt;By the late 20th we had clean(er) air (this is all from a U.S. perspective) and largely safe food. I think if we were to claim a regression, the high point would have to be around 2000, but I can't point to anything specific going on now that wasn't also going on then -- e.g. I think microplastics were a thing then as well, we just weren't paying attention.&lt;/p&gt;
    &lt;p&gt;Where are you? This project is not necessarily limited to products that are available in the United States. Anything that can be shipped to the United States is still testable.&lt;/p&gt;
    &lt;p&gt;Given the current reach of the project (read: still small!), I suspect for awhile yet the majority of successfully funded testing will be by concerned individuals with expendable income. It is cheaper and much faster to go through laboratory.love than it would be to partner with a lab as an individual (plus the added bonus that all data is published openly).&lt;/p&gt;
    &lt;p&gt;I've yet to have any product funded by a manufacturer. I'm open to this, but I would only publish data for products that were acquired through normal consumer supply chains anonymously.&lt;/p&gt;
    &lt;p&gt;this looks so cool! I wish it told me if the levels found for tested products were good/bad - I have no prior reference so the numbers meant nothing to me&lt;/p&gt;
    &lt;p&gt;Both of them do measurements and YouTube videos. Neither one has a particularly good index of their completed reviews, let alone tools to compare the data.&lt;/p&gt;
    &lt;p&gt;I wish I could subscribe to support a domain like “loud speaker spin tests” and then have my donation paid out to these reviewers based on them publishing new high quality reviews with good data that is published to a common store.&lt;/p&gt;
    &lt;p&gt;My side project - https://macrosforhumans.com - is a traditional mobile macro tracker with first class support for voice (and soon image and text blob) inputs for your recipes, ingredients, measurements, units, etc. Kind of a neat project that may never make it too far off the ground considering I am not a mobile dev but it's been fun to build so far with the help of claude code. It's built with flutter and a fastapi backend.&lt;/p&gt;
    &lt;p&gt;In the AI macro food logging world, there's really only Cal AI which estimates macros based on an image. I use cronometer personally, and it's super annoying to have to type everything in manually, so it makes sense why folks reach for something like Cal AI. However, the problem with something like Cal AI is accuracy. It's at best a guess based on the image. Macros for humans tries to be more of a traditional weigh your food, log it, etc kind of app, while updating the main interface for how users input that info into something more friendly.&lt;/p&gt;
    &lt;p&gt;I set myself a hard deadline to present a live demo at a local showcase/pitch event thing at the end of the month. I bet the procrastination will kick in hard enough to get the backend hosted with a proper database and a bit more UI polish running on my phone. :-)&lt;/p&gt;
    &lt;p&gt;Here's a really early demo video I recorded a few weeks ago. I had just spoken the recipe on the left and when I stop recording you can see my backend streams the objects out as they're parsed from the LLM https://www.youtube.com/watch?v=K4wElkvJR7I&lt;/p&gt;
    &lt;p&gt;I'm making a game that's inspired by the niche but adored "The Last of Us Factions", the multiplayer as part of the first Last of Us (only available on Playstation). I got a gaming PC a couple years ago and haven't been able to find anything quite like it.&lt;/p&gt;
    &lt;p&gt;Making it with the Rust game engine, Bevy and really enjoying it so far. Using Blender for making assets. I'm maybe a dumbass for making it as my first game, but I just don't really get excited by smaller projects.&lt;/p&gt;
    &lt;p&gt;Overall I've found modern games to be (1) overstimulating and (2) have algorithms in the background to keep me engaged that I don't trust (see: free to play model)&lt;/p&gt;
    &lt;p&gt;I got tired of trying to find a good MP3 player that just worked so I created a website to function as an online MP3 player. I started adding sources for content and ended up supporting YouTube, Spotify, Twitch, Instagram, Vimeo, SoundCloud, Rumble, WSHH, Facebook and X. So now you can create playlists from all of those sources with features you would find on any decent MP3 player such as loop, repeat, etc. I also drew inspiration from YTInstant and created a real time search for content that allows you to type lyrics and song titles and it will instantly find your content. Finally, I said well while I'm at it, I might as well just recreate MySpace, so I did that too. Let me know your thoughts. https://plasas.com&lt;/p&gt;
    &lt;p&gt;A couple of months ago, I saw a tweet from @awilkinson: “I just found out how much we pay for DocuSign and my jaw dropped. What's the best alternative?”&lt;/p&gt;
    &lt;p&gt;Me being naive, I thought “how hard could would it actually be to build a free e-sign tool?”&lt;/p&gt;
    &lt;p&gt;Turns out not that hard.&lt;/p&gt;
    &lt;p&gt;In about a weekend, I built a UETA and ESIGN compliant tool. And it was free. And it cost me less than $50. Unlimited free e-sign. https://useinkless.com/&lt;/p&gt;
    &lt;p&gt;You’d be surprised how much trust people place in legal departments, balance sheet strength and talent capacity. All things for which I had to turn down superior technical proposals in the past. The old saying „Nobody gets fired for buying IBM“ still runs strong.&lt;/p&gt;
    &lt;p&gt;Free e-signatures are a great idea, have you considered getting a foundation to back the project and maybe taking out some indemnity insurance, perhaps raising a dispute fund?&lt;/p&gt;
    &lt;p&gt;I am working on making ultra-low cost freeze-dried enzymes for synthetic biology.&lt;/p&gt;
    &lt;p&gt;For example, 1 PCR reaction (a common reaction used to amplify DNA) costs about $1 each, and we're doing tons every day. Since it is $1, nobody really tries to do anything about it - even if you do 20 PCRs in one day, eh it's not that expensive vs everything else you're doing in lab. But that calculus changes once you start scaling up with robots, and that's where I want to be.&lt;/p&gt;
    &lt;p&gt;Approximately $30 of culture media can produce &amp;gt;10,000,000 reactions worth of PCR enzyme, but you need the right strain and the right equipment. So, I'm producing the strain and I have the equipment! I'm working on automating the QC (usually very expensive if done by hand) and lyophilizing for super simple logistics.&lt;/p&gt;
    &lt;p&gt;My idea is that every day you can just put a tube on your robot and it can do however many PCR reactions you need that day, and when the next day, you just throw it out! Bring the price from $1 each to $0.01 + greatly simplify logistics!&lt;/p&gt;
    &lt;p&gt;Of course, you can't really make that much money off of this... but will still be fun and impactful :)&lt;/p&gt;
    &lt;p&gt;As a bio hobbyist, this is fantastic! I don't do enough volume of PCR to think of it as expensive, but your use case of high-volume/automatic sounds fantastic! (And so many other types of reagents and equipment are very expensive).&lt;/p&gt;
    &lt;p&gt;Some things that would be cool&lt;/p&gt;
    &lt;p&gt;- Along your lines: In general, cheap automated setups for PCR and gels - Cheap/automatic quantifiable gels. E.g. without needing a kV supply capillary, expensive QPCR machines etc. - Cheaper enzymes in general - More options for -80 freezers - Cheaper/more automated DNA quantification. I got a v1 Quibit which gets the job done, but new ones are very expensive, and reagent costs add up. - Cheaper shaking incubator options. You can get cheap shakers and baters, but not cheap combined ones... which you need for pretty much everything. Placing one in the other can work, but is sub-optimal due to size and power-cord considerations. - More centrifuges that can do 10kG... this is the minimum for many protocols. - Ability to buy pure ethanol without outrageous prices or hazardous shipping fees.&lt;/p&gt;
    &lt;p&gt;- Not sure if this is feasible but... reasonable cost machines to synthesize oglios?&lt;/p&gt;
    &lt;p&gt;I've thought a lot about this! My main goal is to create a cloud lab that doesn't suck - ie, a remote lab that is actually useful for people, and a lot of these are relevant things. Let me run down the ideas I have for each&lt;/p&gt;
    &lt;p&gt;1. You can purchase gel boxes that do 48 to 96 lanes at once. I'd ideally have it on a robot whose only purpose is to load and run these once or twice a day. All the samples coming through get batched together and run&lt;/p&gt;
    &lt;p&gt;2. Bioanalyzer seems nice for quantification of like PCRs to make sure you're getting the right size. But if I'll be honest I haven't though that much about it. But qPCRs actually become very cheap, if you can keep the machines full. You can also use something like a nanodrop and it is much much cheaper&lt;/p&gt;
    &lt;p&gt;3. Pichia pastoris expression ^&lt;/p&gt;
    &lt;p&gt;4. You can use a plate reader (another thing that goes bulk nicely), but the reagents you can't really get around (but cheaper in bulk from China)&lt;/p&gt;
    &lt;p&gt;5. If you aggregate, these become really cheap. The complicated bits are getting the proper cytomat parts for shaking, as they are limited on the used market&lt;/p&gt;
    &lt;p&gt;6. These can't be automated well, so I honestly haven't thought too much about it.&lt;/p&gt;
    &lt;p&gt;7. Reagents cheaper in bulk China&lt;/p&gt;
    &lt;p&gt;8. ehhhh, maybe? But not really. But if you think about a scaled centralized system, you can get away with not using oligos for a lot of things&lt;/p&gt;
    &lt;p&gt;That sounds really cool. I wouldn't agree you can't make money off this, you can make money off anything, just find people who need this and it seems you did find it.&lt;/p&gt;
    &lt;p&gt;Anyhow good luck. Would love to follow if you do anything with this in the future. Do you have a blog or anything?&lt;/p&gt;
    &lt;p&gt;I've been working on a 3D voxel-based game engine for like 10 years in my spare time. The most recent big job has been to port the world gen and editor to the GPU, which has had some pretty cute knock-on effects. The most interesting is you can hot-reload the world gen shaders and out pop your changes on the screen, like a voxel version of shadertoy.&lt;/p&gt;
    &lt;p&gt;I also wrote a metaprogramming language which generates a lot of the editor UI for the engine. It's a bespoke C parser that supports a small subset of C++, which is exposed to the user through a 'scripting-like' language you embed directly in your source files. I wrote it as a replacement for C++ templates and in my completely unbiased opinion it is WAY better.&lt;/p&gt;
    &lt;p&gt;A simple comment, but wow I really like the look of Bonsai! The lighting, shading and shapes are really beautiful, I think a game made in this would feel really unique&lt;/p&gt;
    &lt;p&gt;Currently working on an open source Heroku / Fly.io / Render alternative: https://canine.sh&lt;/p&gt;
    &lt;p&gt;Its built on top of Kubernetes, based on learnings I've had from previous experiences scaling infrastructure.&lt;/p&gt;
    &lt;p&gt;If you look at the markup PaaS (Heroku, Fly, Render) applies to IaaS (AWS, Hetzner), it's on the order of 5-10x. But not having that, and trying to stitch together random AWS services is a huge PITA for a medium sized engineering team (we've tried).&lt;/p&gt;
    &lt;p&gt;On top of all that, theres a whole host of benefits to being on kubernetes, namely, that you can install any helm package with one click, which Canine also manages.&lt;/p&gt;
    &lt;p&gt;A good example is Sentry -- even though it has an open source offering, almost everyone pays for the cloud version because its too scary to self host. With Canine, its a one click, and you get a sentry.your-domain.com to use for whatever you need.&lt;/p&gt;
    &lt;p&gt;Recently got a sponsorship from the Portainer team to allow me to dedicate way more time to this project, so hugely grateful to them for that.&lt;/p&gt;
    &lt;p&gt;I'm working on a TUI for viewing OpenTelemetry traces locally to help me debug distributed applications that use OTEL. It's currently in its infancy, but I'm already able to get a little use out of it. https://github.com/FredrikAugust/otelly&lt;/p&gt;
    &lt;p&gt;A project to implement 1000 algorithms. I have finished around 400 so far and I am now focusing on adding test cases, writing implementations in Python and C, and creating formal proofs in Lean.&lt;/p&gt;
    &lt;p&gt;It has been a fun way to dive deeper into how algorithms work and to see the differences between practical coding and formal reasoning. The long-term goal is to make it a solid reference and learning resource that covers correctness, performance, and theory in one place.&lt;/p&gt;
    &lt;p&gt;The project is still in its draft phase and will be heavily edited over the next few months and years as it grows and improves.&lt;/p&gt;
    &lt;p&gt;If anyone has thoughts on how to structure the proofs or improve the testing setup, I would love to hear ideas or feedback.&lt;/p&gt;
    &lt;p&gt;Wow, that looks fun and probably get to learn a lot about algorithms.&lt;/p&gt;
    &lt;p&gt;I don't have any feedback, but rather a question, as I've seen many repositories with people sharing their algorithms, at least on GitHub for many different languages (e.g. https://github.com/TheAlgorithms), what did you find that was missing from those repositories that you wanted to write a book and implement hundreds of algorithms, what did you find that was lacking?&lt;/p&gt;
    &lt;p&gt;Another reason for writing the book is that many developers see "algorithms" as something only needed for FAANG interviews, not for real work. For beginners and even seniors, learning algorithms often just means doing LeetCode problems, which most people dislike but feel forced to do.&lt;/p&gt;
    &lt;p&gt;I want to change that view and show that algorithms are beautiful and useful beyond interviews. They appear everywhere, from compilers to databases to the Linux kernel, where I found many interesting data structures worth exploring. (i will share more about this topic later)&lt;/p&gt;
    &lt;p&gt;I hope to share more of these insights and connect with others who enjoy discussing real world algorithm design, which is what I love most about the Hacker News community (except for the occasional trolls that show up from time to time).&lt;/p&gt;
    &lt;p&gt;Those algorithms implement so random to me, with lack of explanation, no test cases, no formal proof, and often inconsistent naming or structure across languages. Many repositories like TheAlgorithms are great collections, but they feel more like code dumps than true learning resources. You can find an implementation of Dijkstra or QuickSort, but often there is no context: why it works, how to prove it correct, what the complexity is, or how to test it against edge cases. For someone who wants to learn algorithms deeply, that missing layer of reasoning and validation is critical.&lt;/p&gt;
    &lt;p&gt;No organization for learners either. It jumps straight into implementations without a logical flow from fundamentals. I want to build something more structured: start from the very foundation (like data structures, recursion, and complexity analysis), then move to classical algorithms (search, sort, graph, dynamic programming), and eventually extend to database internals, optimization, and even machine learning or AI algorithms. Basically, a single consistent roadmap from beginner to researcher level, where every algorithm connects to the next and builds intuition step by step.&lt;/p&gt;
    &lt;p&gt;Another very good resource for beginners is https://www.hello-algo.com. At first, i actually wanted to contribute there, since it explains algorithms visually and in simple language. But it mostly covers the basics and stops before more advanced or applied topics. I want to go deeper and treat algorithms as both code and theory, with mathematical rigor and formal proofs where possible. That is something I really liked about Introduction to Algorithms (CLRS) and of course The Art of Computer Programming (TAOCP) by Knuth. They combine reasoning, math, and practice. My goal is to make something in that spirit, but more practical and modern, bridging the gap between academic books and messy open source repos.&lt;/p&gt;
    &lt;p&gt;For more context, I actually used The Algorithms as a reference when working on my own programming language, Mochi, which includes around 150–300 algorithms (I don't remember exactly) implemented directly in Mochi. These are then transpiled to over 25 programming languages such as C, Haskell, Java, Go, Scala, and more: https://github.com/mochilang/mochi/tree/main/tests/algorithm...&lt;/p&gt;
    &lt;p&gt;The VM and transpiler were originally implemented by hand, and later I used Codex to help polish the code. The generated output works, though it is a bit messy in places. Hopefully, after finishing a few books, I can return to the project with more experience and add better use cases for it.&lt;/p&gt;
    &lt;p&gt;Great idea. I had been thinking about pretty much the same but perhaps targeted at executives and perhaps including AI/Cloud.&lt;/p&gt;
    &lt;p&gt;I usually feel to many people wildly through around terms they hardly understand, in the belief they cannot possibly understand. That’s so wrong, every executive should understand some of what determines button line. It’s not like people skip economics because it’s hard.&lt;/p&gt;
    &lt;p&gt;Would love to perhaps contribute sometime next year. Stared and until then good luck - perhaps add a donation link!&lt;/p&gt;
    &lt;p&gt;Thanks! I completely agree. For more than ten years consulting, training and architecting systems for clients across government and enterprise, I have seen the same pattern. Long before "big data", "cloud" and now with "AI" and "GenAI" these buzzwords have often been misunderstood by most of the C-suite. In my entire career, explaining the basics and setting the right expectations has always been the hardest part.&lt;/p&gt;
    &lt;p&gt;I really like your idea of targeting executives and connecting it to real business outcomes. Getting decision makers to truly understand the fundamentals behind the technology would make a huge difference.&lt;/p&gt;
    &lt;p&gt;I hope the next generation learns to love "C" and Algorithms again. I have rediscovered my appreciation for C recently, even though Go is my main professional programming language.&lt;/p&gt;
    &lt;p&gt;I feel like the presentation of Lomuto's algorithm on p.110 would be improved by moving the i++ after the swap and making the corresponding adjustments to the accesses to i outside the loop. Also mentioning that it's Lomuto's algorithm.&lt;/p&gt;
    &lt;p&gt;These comments are probably too broad in scope to be useful this late in the project, so consider them a note to myself. C as the language for presenting the algorithms has the advantage of wide availability, not sweeping performance-relevant issues like GC under the rug, and stability, but it ends up making the implementations overly monomorphic. And some data visualizations as in Sedgewick's book would also be helpful.&lt;/p&gt;
    &lt;p&gt;My biggest inspiration for this project, though, is The Art of Computer Programming (TAOCP), that level of depth and precision is the ultimate goal. I'm also planning to include formal proofs of all algorithms in Lean, though that could easily turn into a 10-year project.&lt;/p&gt;
    &lt;p&gt;Sedgewick's Algorithms book is great for practical learning but too tied to Java and implementation details. It is a bit shallow on theory, though the community and resources for other languages help.&lt;/p&gt;
    &lt;p&gt;That said, I personally prefer Introduction to Algorithms (CLRS) for its formal rigor and clear proofs, and Grokking Algorithms for building intuition.&lt;/p&gt;
    &lt;p&gt;The broader goal of this project is to build a well tested, reference quality set of implementations in C, Python, and Go. That is the next milestone.&lt;/p&gt;
    &lt;p&gt;For clarification, I meant the Algorithms, 4th Edition book at https://algs4.cs.princeton.edu/home/ which is entirely in Java. All the example code, libraries, and exercises there use Java, and the authors explicitly note that the book is written for that language.&lt;/p&gt;
    &lt;p&gt;However, you are right, Prof. Sedgewick has long maintained versions of his material across multiple languages. I remember that the third edition has C, C++ and Java versions.&lt;/p&gt;
    &lt;p&gt;To reduce the current monomorphism, I might add a generic version using void* and a comparator, or generate code for a few key types, while keeping the simple monomorphic listings for readability. (Though this would make the code a bit more complex)&lt;/p&gt;
    &lt;p&gt;Nice to see that you are still around with this after your https://news.ycombinator.com/item?id=45448525 was flagged because of LLM slop issues of your work. How are you addressing those?&lt;/p&gt;
    &lt;p&gt;i am working on something pretty radical in this space. it is a book of algorithms that derives all the algorithms without telling what the algorithm is. for example for binary search your book quickly went into the low + high / 2 = mid thing. my method is radically different. i take an even sized array then try to actually find it step by step , then take an odd sized array , find it step by step, derive a general hypothesis and then create the formula from it for that algorithm. this is going to be orders of magnitude above any data structures and algorithms books and courses when it comes out. pinky promise&lt;/p&gt;
    &lt;p&gt;I found a neat way to do high-quality "semantic soft joins" using embedding vectors[1] and the Hungarian algorithm[2] and I'm turning it into an open source Python package:&lt;/p&gt;
    &lt;p&gt;It hits a sweet spot by being easier to use than record linkage[3][4] while still giving really good matches, so I think there's something there that might gain traction.&lt;/p&gt;
    &lt;p&gt;I see you saved a spot to show how to use it with an alternative embedding model. It would be nice to be able to use the library without an OpenAI api key. Might even make sense to vendor a basic open source model in your package so it can work out of the box without remote dependencies.&lt;/p&gt;
    &lt;p&gt;Yes, I'm planning out-of-the-box support for nomic[1] which can run in-process, and ollama which runs as a local server and supports many free embedding models[2].&lt;/p&gt;
    &lt;p&gt;If you're adding more LLM integration, a cool feature might be sending the results of allow_many="left" off to an LLM completions API that supports structured outputs. Eg imagine N_left=1e5 and N_right=1e5 but they are different datasets. You could use jellyjoin to identify the top ~5 candidates in right for each left, reducing candidate matches from 1e10 to 5e5. Then you ship the 5e5 off to an LLM for final scoring/matching.&lt;/p&gt;
    &lt;p&gt;I'm a dev and also a private pilot. Currently I'm working on Pilot Kit: https://air.club/ , a mobile app born from my own frustration with the amount of tedious paperwork in aviation.&lt;/p&gt;
    &lt;p&gt;It's an all-in-one toolkit designed to automate the boring stuff so you can focus on flying. Core features include: automatic flight tracking that turns into a digital logbook entry, a full suite of E6B/conversion calculators, customizable checklists, and live weather decoding.&lt;/p&gt;
    &lt;p&gt;It’s definitely not a ForeFlight killer, but it's a passion project I'm hoping can be useful for other student and private pilots.&lt;/p&gt;
    &lt;p&gt;I'm trying to fix an irritating problem by syncing my work calendar with a personal one, which would allow me to see all my events in my preferred calendar app. This project is still in the very early stages.&lt;/p&gt;
    &lt;p&gt;The main challenge is that our IT department blocks sharing calendars outside of the organisation. While this is primarily a solution for my own problem and likely not valuable to others, you could probably achieve the same result with tools like n8n or IFTTT.&lt;/p&gt;
    &lt;p&gt;Microlandia, the brutally honest city builder. Posting this for a second time, because i’ve been working super hard on a steam release.&lt;/p&gt;
    &lt;p&gt;last month’s “what are you working on” thread impulsed me to upload this game to itch and 1 month later, i’ve got a small community, lots of feedback and iterations. It brought a whole new life to a project that was on the verge of abandoning.&lt;/p&gt;
    &lt;p&gt;I wonder if you simulate at individual level or group? Would be cool at individual level each one making decisions individually and see some emerging behavior.&lt;/p&gt;
    &lt;p&gt;Also how corruption emerges in gov etc&lt;/p&gt;
    &lt;p&gt;Also if no job maybe they could try uber/food delivery crappy jobs like that or start their own business.&lt;/p&gt;
    &lt;p&gt;Maybe also less money less likely to have kids? Would be nice to show how poverty helps or not population growth. If too poor might have no education and would make kids, if average citizen and can’t save money will avoid kids. That’s why at individual level simulation could find these emerging patterns. But probably too expensive computationally ?&lt;/p&gt;
    &lt;p&gt;&amp;gt; I wonder if you simulate at individual level or group? Would be cool at individual level each one making decisions individually and see some emerging behavior.&lt;/p&gt;
    &lt;p&gt;If you are referring to the citizens, yes, at individual level. However for traffic I'm using a sampling rate.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Also if no job maybe they could try uber/food delivery crappy jobs like that or start their own business.&lt;/p&gt;
    &lt;p&gt;That's an awesome idea, I added it to my backlog :)&lt;/p&gt;
    &lt;p&gt;&amp;gt; less money less likely to have kids?&lt;/p&gt;
    &lt;p&gt;This is mega tricky, because it happens very differently across the world. Yes, can be expensive computationally that's why the city is so small (for now) but as I start to distribute the simulation into many cores, players with high core CPU will be able to choose a bigger city size :) I agree that individual level simulation is what makes it interesting and I plan to keep it like that.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Brutally honest? I hope it shows the huge amount of land needed for parking lots :P&lt;/p&gt;
    &lt;p&gt;Parking space simulation is coming soon. I feel I will completely miss the point if I leave that out. The idea is to have street parking (with configurable profit for the city) parking lots, and buildings with underground parking, that should conflict, of course, with metro lines.&lt;/p&gt;
    &lt;p&gt;That's something I haven't thought about, but I 100% agree that it should be possible! Very soon I want to introduce roads with bicycle lanes, that have less car bandwidth, that will make me refactor the traffic simulation and the idea of a bicycle-only road type will become possible.&lt;/p&gt;
    &lt;p&gt;In Venice (the Italian one) - cars, mopeds and bikes are all banned. Most trade goods are transported around by a man moving fast with a sack truck shouting 'Attencion!'&lt;/p&gt;
    &lt;p&gt;Public transport is the next thing I wanna work on. Will start with buses, which I have an idea on how it will be implemented, but for metro I will want to start learning about how it can be simulated faithfully.&lt;/p&gt;
    &lt;p&gt;This weekend I have plans to start playing a lot Subway Builder (https://www.subwaybuilder.com) which I'm really excited about, and maybe get some books on the subject, in order to get it right&lt;/p&gt;
    &lt;p&gt;I'm currently building my own coding agent, VT Code. VT Code is a Rust-based terminal coding agent with semantic code intelligence via Tree-sitter (parsers for Rust, Python, JavaScript/TypeScript, Go, Java) and ast-grep (structural pattern matching and refactoring).&lt;/p&gt;
    &lt;p&gt;It supports multiple LLM providers: OpenAI, Anthropic, xAI, DeepSeek, Gemini, OpenRouter, Z.AI, Moonshot AI, all with automatic failover, prompt caching, and token-efficient context management. Configuration occurs entirely through vtcode.toml, sourcing constants from vtcode-core/src/config/constants.rs and model IDs from docs/models.json to ensure reproducibility and avoid hardcoding. [0], [1], [2]&lt;/p&gt;
    &lt;p&gt;Recently I've added Agent Client Protocol (ACP) integration. VT Code is now a fully compatible ACP agent, works with any ACP-clients: Zed (first-class support), Neovim, marimo notebook. [3]&lt;/p&gt;
    &lt;p&gt;Thank you! I'm glad that you find this project useful. VT Code my current passion on how agent coding works and how far I can push myself to build one (with AI-assisted). I will keep developing and improve it. Currently I'm planning to run Terminal-bench to see how VT Code performs.&lt;/p&gt;
    &lt;p&gt;This looks very exciting! I'm following it and I'll give it a go. Not that I'm unsatisfied with Claude Code for my amateur level, but it's clear incentives are not exactly aligned when using a tool from the token provider xD&lt;/p&gt;
    &lt;p&gt;I love that you've made it open source and that it's in Rust, thanks a lot for the work!&lt;/p&gt;
    &lt;p&gt;Thank you for your kind words. This is my own research into how coding agent works in practice, I love to explore the underlying technologies of how Claude Code, and Codex and coding agent works in general.&lt;/p&gt;
    &lt;p&gt;I choose Rust since I have some familiarity and experience with it, VT Code is of course, AI-assisted, I mainly use Codex to help me build it. Thank you again for checking it out, have a great day! : )&lt;/p&gt;
    &lt;p&gt;Kindly let me know how your experience about Zed integration, I have done the ACP integration and merge to upstream Agent Client Protocol spec from Zed. The integration experience is quite magical, honestly. It's working in Zed , though for tool calls I'm still improving https://github.com/agentclientprotocol/agent-client-protocol...&lt;/p&gt;
    &lt;p&gt;Thank you for your very kind-words. I love building and agentic coding is my current curiosity.&lt;/p&gt;
    &lt;p&gt;&amp;gt; I’m curious though, how significant do you think it is for the agent to have semantic access through Tree-sitter?&lt;/p&gt;
    &lt;p&gt;For this, I'm really not sure, but since the start of building VT Code. I just had this idea to use tree-sitter to assist the agent to have more (or faster/more precise) semantic understanding of the coding, instead of relying them to figure out themself. For me, naively I think this could help agent to have better language-specific and accurately decision about the workspace (context) that they are working. If not having tree-sitter, I think the agent could eventually figure out itself. For this aspect, I should be research more on this topic. In VT Code, I included 6 language: Go, Python, Rust, TypeScript, Swift... ) via rust-binding crates, mostly when you launch the vtcode agent on any workspace, It will show the main languages in the workspace right way.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Also what model have you had the most success with ?&lt;/p&gt;
    &lt;p&gt;I'm having mainly limited-budget so I can only use OpenRouter and utilize its vast amount models support. So that I can prototype quickly, for different use-cases. For VT Code agent, I'm using mainly x-ai/grok-code-fast-1, in my experience, it most suit for building VT Code agent it self because of speeds, and versatile in function calling and have good instruction following. I also have good successes with x-ai/grok-4-fast. I have not tried claude-4.5-sonnet and gpt-5/gpt-5-codex though. I really love to run benchmarks for VT Code to see how it perform in real world coding task, I'm aiming for Aider polygot bench, terminal-bench and swe-bench-lite, it is in my plan for now in my GitHub issues.&lt;/p&gt;
    &lt;p&gt;For VT Code itself, I instruct it to strictly follow system-prompt, in which I take various inspiration from Anthropic, OpenAI and Devin guide/blogs on how to build coding agent. But, for a model-agnostic agent, the capability to support multi providers and multi models is a challenge. For this I think I need help. I'm fortunately to have support from open-source community suggesting me to use zig, I have had good success with it so far, for implement LLM calls and implement the /model picker.&lt;/p&gt;
    &lt;p&gt;Overall in my experience building VT, the most important aspect of effective coding agent is context engineering, like all big-lab has research. A good system prompt is also very important, but not context is everything. https://github.com/vinhnx/vtcode/blob/main/prompts/system.md&lt;/p&gt;
    &lt;p&gt;// Sorry, English is not my main language, so pardon the typo and grammar. Thank you!&lt;/p&gt;
    &lt;p&gt;Working on Maudit, a Rust library to make static websites. Emphasis on library instead of framework. I aim that you could integrate Maudit into existing Rust apps, building pages individually, rendering Markdown where you need etc, instead of a black box magic "build website" command.&lt;/p&gt;
    &lt;p&gt;- You can precisely tweak every shade/tint so you can incorporate your own brand colors. No AI or auto generation!&lt;/p&gt;
    &lt;p&gt;- It helps you build palettes that have simple to follow color contrast guarantees by design e.g. all grade 600 colors have 4.5:1 WCAG contrast (for body text) against all grade 50 colors, such as red-600 vs gray-50, or green-600 vs gray-50.&lt;/p&gt;
    &lt;p&gt;- There's export options for plain CSS, Tailwind, Figma, and Adobe.&lt;/p&gt;
    &lt;p&gt;- It uses HSLuv for the color picker, which makes it easier to explore accessible color combinations because only the lightness slider impacts the WCAG contrast. A lot of design tools still use HSL, where the WCAG contrast goes everywhere when you change any slider which makes finding contrasting colors much harder.&lt;/p&gt;
    &lt;p&gt;- Check out the included example open source palettes and what their hue, saturation and lightness curves look like to get some hints on designing your own palettes.&lt;/p&gt;
    &lt;p&gt;It's probably more for advanced users right now but I'm hoping to simplify it and add more handholding later.&lt;/p&gt;
    &lt;p&gt;Really open to any feedback, feature requests, and discussing challenges people have with creating accessible designs. :)&lt;/p&gt;
    &lt;p&gt;I've sorted the colors by luminance/lightness and added a gray swatch for comparison so can explore which color pairs pass WCAG contrast checks.&lt;/p&gt;
    &lt;p&gt;I haven't really gotten into colorblind safe colors like this yet where the colors mostly differ by hue and not luminance. Colorblind and non-colorblind people should be able to tell colors apart based on luminance difference i.e. luminance contrast. Hue perception is impacted by the several different kinds of color blindness so it's much trickier to find a set of colors that everyone can tell apart. This relates to the WCAG recommendation you don't rely on hue (contrast) to convey essential information (https://www.w3.org/WAI/WCAG21/Understanding/use-of-color.htm...).&lt;/p&gt;
    &lt;p&gt;The gray swatch above could be called colorblind safe for example because as long as you pick color pairs with enough luminance contrast between them, colorblind and non-colorblind people should be able to tell them apart. You could even vary the hue and saturation of each shade to make it really colorful, as a long as you don't change the luminance values the WCAG contrast between pairings should still pass.&lt;/p&gt;
    &lt;p&gt;I get that you say it is for advanced users, but I think a "how to use this" link with a video in it that explained a few things would probably open it up to a lot more users.&lt;/p&gt;
    &lt;p&gt;There's so much more to do with tools like this, and I'm really glad to see it.&lt;/p&gt;
    &lt;p&gt;- Drag the hue and saturation curves to customise the tints/shades of a color. Look at the UI mockup as you do this to make sure the tints/shades look good together.&lt;/p&gt;
    &lt;p&gt;- The color pairings used in the UI mockup all initially pass WCAG contrast checks but this can break if you tweak the lightness curve of a color. The mockup will show warning outlines if this happens. Click on a warning and it'll tell you which color pairs need to have their lightness values moved further apart to fix it.&lt;/p&gt;
    &lt;p&gt;- Once you're happy, use the export menu to use your colors in your CSS or Figma designs. You can use the mockup as a guide for which color pairs are accessible for body text, headings, button outlines and so on.&lt;/p&gt;
    &lt;p&gt;Does that make more sense? You really need to be on desktop as well because the mobile UI is more of a demo.&lt;/p&gt;
    &lt;p&gt;Thanks for the feedback! Yeah, I appreciate there's a lot of background here around color palette design, UI design, color spaces, and accessibility so I likely need something like a video or tutorial. Another route is to have the tool start in a less freeform mode that handholds you through the process more.&lt;/p&gt;
    &lt;p&gt;I'm deploying a biological hardware solution to a regressed masonry event currently blocking ingress to a public channel.&lt;/p&gt;
    &lt;p&gt;The stoneware bitrot was legacy but eventually overwhelmed the architecture during an off-peak environment incident.&lt;/p&gt;
    &lt;p&gt;I'm tasked with fulfilling runtime dependencies to restore the wall framework, but had issues with build time mixing parameters not compiling well with the piecemeal building blocks.&lt;/p&gt;
    &lt;p&gt;I finally got it up and running through trial and error, though I sense a full rewrite will eventually be needed in the future.&lt;/p&gt;
    &lt;p&gt;The goal is to serve the laws in a format that is easy to cite, monitor, or machine-read. It should also have predictable URLs that can be inferred from the law’s name. It will also have side by side AI translations (marked as such).&lt;/p&gt;
    &lt;p&gt;I cite a lot of laws in my content and I want to automatically flag content for review when a specific paragraph of the law changes. I also want to automatically update my tax calculator when the values change.&lt;/p&gt;
    &lt;p&gt;Basically, a refresh of gestetze-im-internet.de and buzer.de.&lt;/p&gt;
    &lt;p&gt;That's really important I think. Tangentially, https://ecfr.gov/ is bizarrely one of the most impressive websites I have encountered. It's regulations not laws but it has so many ways to discover and link and learn and trace back to law. I'm not a lawyer but it's been a great resource for understanding and surprisingly pleasant to read. I found learning about the whole law and regulation machinery and provenance via eCFR pretty fascinating.&lt;/p&gt;
    &lt;p&gt;Dunno if other governments are this Byzantine in practice (our system seems to be like... manual integration of diff patches) but it's pretty interesting and I really appreciate the work that goes into these types of things.&lt;/p&gt;
    &lt;p&gt;I've been suffering from migraines for the last month, so have channeled my (non-migraine) time into a migraine tracker to try and find the root causes. All the tracking apps I tried all have nice complex forms, which is all well and good, unless...you are having a migraine.&lt;/p&gt;
    &lt;p&gt;Rough idea is easy to use voice mode to record data, then analyze unstructured data with AI later on.&lt;/p&gt;
    &lt;p&gt;I want to track all relevant life information, so what I'm eating, meds I'm taking, headache/nausea levels, etc.&lt;/p&gt;
    &lt;p&gt;Adding records is as easy as pressing record on my apple watch and speaking some kind of information. Uses Deepgram for voice transcription since it's the best transcription API I've found.&lt;/p&gt;
    &lt;p&gt;Will then send all information through to a LLM for analysis. It has a "chat with your data" page to ask questions and try and draw conclusions.&lt;/p&gt;
    &lt;p&gt;Main webapp is done, now working on packaging it into an iOS app so I can pull biometrics from Healthkit. Will then look into releasing it, either on github or possibly in the app store. It's admittedly mostly vibe coded, so not sure if it'll be something releasable, but we'll see...&lt;/p&gt;
    &lt;p&gt;If it’s an iPhone app the new on device transcription api in ios26 works well and is very fast. You could also use the ondevice llm to clean up the transcription. Cheaper and more privacy friendly&lt;/p&gt;
    &lt;p&gt;As a fellow migraineur, I feel compelled to point out that the quest for triggers and root causes is probably never going to end. The way I see it, the migraine "bucket" slowly fills up, and the final trigger is simply the drop that makes it run over.&lt;/p&gt;
    &lt;p&gt;I can suggest the research papers by Markus Dahlem for some in depth modern takes on migraine.&lt;/p&gt;
    &lt;p&gt;It's definitely bucket-like for me, and I can attest meditation empties it. Whenever I stop meditating, mental busyness and subconscious anxiety slowly build up. Half hour a day is enough to keep it away. I just keep bringing my attention back to the breath, trying to feel into the physiological need to breathe (which is usually occluded or distorted by mental activity). Whenever I feel I am actively holding to some tension, I allow myself to release it. That's all in terms of instructions, and for me it works wonders. I look at it as the equivalent of flossing for the brain ;)&lt;/p&gt;
    &lt;p&gt;For me, as for a lot of people, lack of sleep is the big one... if I build up 4+ hours of sleep debt over a week, I'm at risk. So anything you can do to make that easier to log, like integration with a sleep tracker, would be good.&lt;/p&gt;
    &lt;p&gt;Also, a plug for Oliver Sacks's Migraine which taught me a lot about migraine with aura.&lt;/p&gt;
    &lt;p&gt;My current project also revolves around using voice notes to log life events. I'd love to talk and see if we could exchange some ideas. My Gmail username is the same as my HN username.&lt;/p&gt;
    &lt;p&gt;Well, depending on the people you meet, and the roles you are in, any kind of social contact can be mentally draining, even if it is not directly obvious.&lt;/p&gt;
    &lt;p&gt;Note that even the anticipation of meeting people can be a mental load.&lt;/p&gt;
    &lt;p&gt;That’s built on a dataset and paper I wrote called CommonForms, where I scraped CommonCrawl for hundreds of thousands of fillable form pages and used that as a training set:&lt;/p&gt;
    &lt;p&gt;Next step is training and releasing some DETRs, which I think will drive quality even higher. But the ultimate end goal is working on automatic form accessibility.&lt;/p&gt;
    &lt;p&gt;Continuing to work on a Low Power FM community radio station for the East San Fernando Valley of Los Angeles. We have started promoting and putting on local events and are trying to fund raise to build out the station. Raising money is hard! We did a big show in Burbank where several hundred people showed up but we only netted $800 after expenses. :(&lt;/p&gt;
    &lt;p&gt;Since this is hackernews, i'll add that i'm building the website and archiving system using haskell and htmx, but what is currently live is a temp static html site. https://github.com/solomon-b/kpbj.fm&lt;/p&gt;
    &lt;p&gt;This is sick - I happen to run a site for DIY and community organizations like yours. We have proven the best way to fundraise is to throw events like you did but to upsell people on a recurring donation when they get the ticket.&lt;/p&gt;
    &lt;p&gt;On the off chance you are throwing another event, I would love to help you raise much more than $800 one time (my site is https://withfriends.events/)&lt;/p&gt;
    &lt;p&gt;This might be a naive question which you've probably been asked plenty of times before so I'm sorry of I'm being tedious here.&lt;/p&gt;
    &lt;p&gt;Is it really worth the effort and expense to have a real radio station these days? Wouldn't an online stream be just as effective if it was promoted well locally?&lt;/p&gt;
    &lt;p&gt;A few years ago a friend who was very much involved in a local community group which I was also somewhat interested in asked me if I wanted to help build a low power FM station. He asked me because I know something about radio since I was into ham radio etc.&lt;/p&gt;
    &lt;p&gt;I was skeptical that it was worth the effort. The nerdy part of me would have enjoyed doing it but I couldn't help thinking that an online stream would probably reach as many people without the hassle and expensive of a transmitter, antenna etc.&lt;/p&gt;
    &lt;p&gt;I know it's a toss up. Every car has an FM radio. Not everyone is going to have a phone plugged in to Android Auto or Apple Car Play and have a good data plan and have a solid connection.&lt;/p&gt;
    &lt;p&gt;I also pointed out that the technical effort is probably the small part compared to producing interesting content.&lt;/p&gt;
    &lt;p&gt;1. Radio is COOL. As a fellow ham I think you would agree with me on this one so I'll leave it at that.&lt;/p&gt;
    &lt;p&gt;2. Internet streaming gives you wider but far less localized audience. We will have an internet stream, but being radio first shifts the focus to local community and local content.&lt;/p&gt;
    &lt;p&gt;3. Internet streaming and radio have related but not entirely overlapping histories and contexts which impacts how people produce and consume their content. I love the traditional formats of radio and they are often completely missing in online radio which IMO models itself more often on mixtape and club DJ culture.&lt;/p&gt;
    &lt;p&gt;4. AI slop is ruining the world. I have this belief that as AI slop further conquers the internet we are going to get to a place where nobody trusts internet content. People will seek out novelty and authenticity (sort of how LLMs do lol) and I think there will be a return to local content and community.&lt;/p&gt;
    &lt;p&gt;5. Commercial radio sucks. The LPFM system is a wonderful opportunity to create a strong, community driven alternative to corporate media.&lt;/p&gt;
    &lt;p&gt;Radio is so much fun to learn. It’s liberating to learn for curiosity and joy rather than commercialization. The community is welcoming, and while not directly translatable for most paid work, it does teach general problem solving skills.&lt;/p&gt;
    &lt;p&gt;For the past 2 months I've building an app called LogBuddy. I've recently completed the MVP and it helps me track my weight, my workout sessions, my food intake, and my periods, all in one app. It's really basic on purpose. This app also gave me the opportunity to go all in on mobile dev with Ionic.&lt;/p&gt;
    &lt;p&gt;Here on Croatian islands maritime traffic disruptions and power outages happen often. Constantly checking websites or searching paper notifications stuck on random street lamp posts is a no-go and timely information is important.&lt;/p&gt;
    &lt;p&gt;I'm working on a mini-project which monitors official resources on the web and sends email notifications on time. Currently covering around 15000 inhabitants.&lt;/p&gt;
    &lt;p&gt;I've become a bit addicted to online education. I finished my first masters degree in Computer Science in July, and I started a masters in Mathematics from The Open University at the beginning of October. I've wanted to really get into the weeds of obscure and arguably-useless math for about as long as I can remember, and I figure that getting a masters in it is as good a way to get that knowledge as any way else.&lt;/p&gt;
    &lt;p&gt;Other than that, I've been doing a lot of fixing of tech debt in my home network from the last six years. I've admittedly kind of half-assed a lot of the work with my home router and my server and my NAS and I want these things to be done correctly. (In fairness to me, I didn't know what I was doing back when I started, and I'd like to think I know a fair bit better now).&lt;/p&gt;
    &lt;p&gt;For example, when I first built my server, I didn't know about ZFS datasets, so everything was on the main /tank mount. This works but there are advantages to having different settings for different parts of the RAID and as such I've been dividing stuff into datasets (which has the added advantage of "defragging" because this RAID has grown by several orders of magnitude and as a result some of the initial files were fragmented).&lt;/p&gt;
    &lt;p&gt;I'm currently building an order queueing and sales recording web app for small coffee shops: SellerMate [https://sellermate.neilvan.com]&lt;/p&gt;
    &lt;p&gt;Made primarily for my friend's coffee shop. Data is stored locally, and the app is fully functional when offline. There is an optional "syncing" feature to sync your data with multiple devices which requires a sign up. This is a Progressive Web App built with Web Components. The syncing is made possible with PouchDB/CouchDB.&lt;/p&gt;
    &lt;p&gt;I still have to write (or screen record) a Getting Started guide but the app is ready for use nonetheless.&lt;/p&gt;
    &lt;p&gt;I’m building a small side project called https://www.localgeoguessr.com/ — a fun geography game that tests how well you know your local area. It’s still in a very early stage, not polished yet, but it’s somewhat playable.&lt;/p&gt;
    &lt;p&gt;The idea is to eventually add more categories like “restaurants,” “theaters,” “roads,” etc., so you can play based on local themes.&lt;/p&gt;
    &lt;p&gt;I’d love to hear your thoughts - any feedback on what you’d like to see, what feels off, or any issues you run into would be super helpful.&lt;/p&gt;
    &lt;p&gt;I thought your idea was really cool so I gave it a go. I live about 20 minutes from a minor US city.&lt;/p&gt;
    &lt;p&gt;All but 1 prompts were in a 3-block radius IN the city (again, about 20 minutes from my town's town hall).&lt;/p&gt;
    &lt;p&gt;So the 1 prompt I didn't know I guessed the same 3 block radius as the others, and it was about 2 miles away. Still in the city, not the town I typed in.&lt;/p&gt;
    &lt;p&gt;It seems like smaller towns will be gobbled up by famous cities elements. Especially here in New England where the majority of 'famous' local things are so few.&lt;/p&gt;
    &lt;p&gt;edit: also, changing the 'radius' resets the city to where the website THINKS I am instead of where I typed in.&lt;/p&gt;
    &lt;p&gt;It's really fun! Thank you. On the result screen, let me click on the locations so that I can learn more about them. Some museums I didn't know and would click immediately to learn more about them. Or even add a little explanation of what they are.&lt;/p&gt;
    &lt;p&gt;Love this. I chose a small village and got a few clues like "War memorial", I'd bet there are many War Memorials in the 10km radius - so was impossible to know which one it meant.&lt;/p&gt;
    &lt;p&gt;Working on an dedicated offline space. Screen device are stored to a locker. Serves coffee and beverages and light food. (There will be a small separate space for occasional screen/internet access in case of need)&lt;/p&gt;
    &lt;p&gt;I've been working on the idea for about a year now. I have put up the funds and set up the corporation. Been busy designing the menu, scouting an ideal location and finding the right front-end staff.&lt;/p&gt;
    &lt;p&gt;This sounds like my retirement plan: coffee shop + book store. I tend to buy old engineering and mathematics textbooks (my collection has suffered losses through multiple moves, unfortunately) and I find that these are typically overlooked at normal bookstores and even libraries.&lt;/p&gt;
    &lt;p&gt;I have been working on it for the last two years as a side project, but starting March will be my full time job! Kind of excited and scared at the same time&lt;/p&gt;
    &lt;p&gt;I am working on an AI-powered fitness and food tracker that automatically logs your food based on the photo. One of the difficulties I had when going to the gym is keeping up and sharing macros weekly with my personal trainer. Manually logging food is a hassle and massive pain point - so my app, Eat n Snap attempts to solve this problem. You can also set weight and BMI goals and see your progress on a weekly basis.&lt;/p&gt;
    &lt;p&gt;I suppose you must know this already if you have done research on alternatives, but there are already a plethora of apps like this — Lifesum, Cal AI, MacroFactor, just to name a few.&lt;/p&gt;
    &lt;p&gt;I think this is hard with only a photo because you can’t always see what’s inside. But I’ve always dreamed of something like this paired with some kind of affordable hardware scanner that can get just enough data to fill in the blanks from the photos.&lt;/p&gt;
    &lt;p&gt;This is already a feature in an app called MacroFactor. But there is definitely room for improvement in the field.&lt;/p&gt;
    &lt;p&gt;One thing that I miss in MacroFactor is that it should have some memory of my previous choice.&lt;/p&gt;
    &lt;p&gt;Example: If I take a picture of a glass of milk, it always assumes it to be whole milk (3.5% fat). Then I change it to a low fat milk (0.5% fat). But no matter how many times I do that, it keeps assuming that the milk in the photo is whole milk.&lt;/p&gt;
    &lt;p&gt;I'm working on a little website that helps me and my friends to decide easier what to play on a gamenight, because it always goes like this: - I want to play x and y - I want to play y and z - I don't have z - I don't really feel x - Lets play b - I'd rater play c - Let's settle on d - Today H is joining, he does not have d&lt;/p&gt;
    &lt;p&gt;It'll work in sessions where first everyone can suggest games, then in the second phase veto out suggestions, then vote and it'll display the games with the highest vote. You can also manage/import a list of your games and it'll show who owns what. It's geared towards video games, but will work for board games too. Hope to release it for everyone in the next weeks.&lt;/p&gt;
    &lt;p&gt;Sounds great. Would everyone "accept" the result, or would it be worth adding a little LLM explanation of why the result should content everyone by explaining how the game retains elements or this other voted game and that other voted game, to try and make people go "ok, sure"? I'm not a huge gamer so maybe this is an obvious reaction they would get from their own experience when seeing the result without needing a LLM explanation.&lt;/p&gt;
    &lt;p&gt;Maybe far down the road. For now I'm fine with a minimal tool that does not aim to take out the human interaction in the group haha. If the tool finds a small community more features might be added like content-based recommendations, but I don't want to drive up the costs right now&lt;/p&gt;
    &lt;p&gt;I'm working on a open-source tool to create photo galleries from a folder of photos: https://simple.photo. It creates galleries as static sites that are easy to self-host.&lt;/p&gt;
    &lt;p&gt;I started this out of frustration that there is no good tool I could use to share photos from my travel and of my kids with friends and family. I wanted to have a beautiful web gallery that works on all devices, where I can add rich descriptions and that I could share with a simple link.&lt;/p&gt;
    &lt;p&gt;Turned out more people wanted this (got 200+ GitHub stars for the V1) so I recently released the V2 and I'm working on it with another dev. Down the road we plan a SaaS offer for people that don't want to fiddle with the CLI and self-host the gallery.&lt;/p&gt;
    &lt;p&gt;Like the layout tiles you have for the photo thumbnails. Will dig through and learn some css. Have struggled with different size content to create a compact masonry layout.&lt;/p&gt;
    &lt;p&gt;The CSS for this is indeed tricky. I figured out this layout 5 years ago in the v1 and forgot how it works, just took it over as it looks good. The key is that not all rows are exactly the same height. There are small differences that allow photos to fit horizontally.&lt;/p&gt;
    &lt;p&gt;I also tried the vertical masonry layout, which looks good, but makes no sense if your photos have a chronological order...&lt;/p&gt;
    &lt;p&gt;I'm working on a DSL and browser-based playground for procedural 3D geometry called Geotoy: https://3d.ameo.design/geotoy&lt;/p&gt;
    &lt;p&gt;It's largely finished and functional, and I'm now focused on polish and adding additional builtin functions to expand its capabilities. I've been integrating different geometry libraries and kernels as well as writing some of my own.&lt;/p&gt;
    &lt;p&gt;I've been stress-testing it by building out different scenes from movies or little pieces of buildings on Google Maps street view - finding the sharp edges and missing pieces in the tool.&lt;/p&gt;
    &lt;p&gt;My hope is for Geotoy to be a relatively easy-to-learn tool and I've invested significantly in good docs, tutorials, and other resources. Now my goal is to ensure it's something worth using for other people.&lt;/p&gt;
    &lt;p&gt;I'm building a CLI that automatically generates and runs negative and boundary tests from OpenAPI Specs: https://github.com/dochia-dev/dochia-cli. It aims to reduce effort that engineers spent on this type of testing, either automatic or manual. But also making sure it comprehensively covers test scenarios which might not be considered by everyone.&lt;/p&gt;
    &lt;p&gt;Working on: https://github.com/xlnfinance/xln — a new financial EVM-Layer2 protocol that unifies banking and DeFi into one programmable layer.&lt;/p&gt;
    &lt;p&gt;Think Lightning, but on EVM, with credit that’s provable and enforceable. No sequencers. No DA risk. O(1) per-hop scalability.&lt;/p&gt;
    &lt;p&gt;If you want to measure real intelligence — human or machine — paste this prompt into any LLM and watch what happens:&lt;/p&gt;
    &lt;p&gt;Prompt: “Comprehend the following concept in full depth. Derive all its implicit implications, design questions, and systemic consequences. Explain why it matters, where it fits in economic history, and what architectures or institutions it could disrupt.”&lt;/p&gt;
    &lt;p&gt;XLN (RCPE) ⊃ { FCUU, FRPE } Δ ∈ [−Lₗ, C + Lᵣ] cryptographically enforced debt + collateral account proofs with L1 dispute resolution O(1) unicast • sovereign exits • no DA risk&lt;/p&gt;
    &lt;p&gt;&amp;lt;/END&amp;gt;&lt;/p&gt;
    &lt;p&gt;Looking for VCs, co-founders, market makers. If you like building deep protocols, financial math, or scalable Layer-2s — I’m looking for co-founders, researchers, and early backers. Ping me: h@xln.finance&lt;/p&gt;
    &lt;p&gt;An open source website I built to explain tensor functions in PyTorch: https://whytorch.org&lt;/p&gt;
    &lt;p&gt;It makes tricky functions like torch.gather and torch.scatter more intuitive by showing element-level relationships between inputs and outputs.&lt;/p&gt;
    &lt;p&gt;For any function, you can click elements in the result to see where they came from, or elements in the inputs to see how they contribute to the result to see exactly how it contributes to the result. I found that visually tracing tensor operations clarifies indexing, slicing, and broadcasting in ways reading that the docs can't.&lt;/p&gt;
    &lt;p&gt;You can also jump straight to WhyTorch from the PyTorch docs pages by modifying the base URL directly.&lt;/p&gt;
    &lt;p&gt;I launched a week or two back and now have the top post of all time on r/pytorch, which has been pretty fun.&lt;/p&gt;
    &lt;p&gt;This really nice. For `torch.mul(x, y)`, it would be nice if it highlighted the entire row or column in the other matrix and result. Right now it shows only a single multiplication, which gives a misleading impression of how matrix multiply works. I wouldn't mention it, except that matrix multiplication is so important that it's worth showcasing. I've bookmarked the site and will share it at a pytorch training session I'm leading in a couple of weeks.&lt;/p&gt;
    &lt;p&gt;I'm working on Penteglot - a fork of Emacs's Eglot LSP client with multi-server support.&lt;/p&gt;
    &lt;p&gt;The main feature: you can run multiple language servers simultaneously for the same buffer.&lt;/p&gt;
    &lt;p&gt;One of the main reasons people stick with lsp-mode over Eglot has been the lack of multi-server support. Eglot is otherwise the most "emacsy" LSP client, so I'm working on filling that gap and I hope it could be merged into Emacs one day.&lt;/p&gt;
    &lt;p&gt;This is still WIP but I've been using it for a while for Python (basedpyright or pyrefly + ruff for linting) and TypeScript (ts-ls + eslint + tailwind language server).&lt;/p&gt;
    &lt;p&gt;I wanted to build my own speech-to-text transcription program [1] for Discord, similar to how zoom or google hangouts works. I built it so that I can record my group's DND sessions and build applications / tools for VTTs (Virtual TableTop gaming).&lt;/p&gt;
    &lt;p&gt;It can process a set of 3-hour audio files in ~20 mins.&lt;/p&gt;
    &lt;p&gt;Interesting challenge was designing for minimal distractions while keeping setup simple for parents. Timer-locked navigation so kids can see what's next but can't start other tasks or switch profiles. Also refactored from schedule-centric (nightmare to maintain) to task-definitions as first-class citizens, which made creating schedules way easier&lt;/p&gt;
    &lt;p&gt;React Native/Expo + Firebase. On the App Store after months of dogfooding with the family&lt;/p&gt;
    &lt;p&gt;I think this is amazing. I work in the construction sector and there are so so so many small one-man tradesperson companies that need to know about this.&lt;/p&gt;
    &lt;p&gt;Nice! I recently built an invoice generator (not open sourced) for my own needs. I built mine because I needed something when I discontinued a SaaS that had provided it. Mine is written in C# and uses a JSON file to define the contents of the invoice. It's run from the command-line and just produces the PDF.&lt;/p&gt;
    &lt;p&gt;Are you planning to turn this into a full-fledged CRM of some sort? Are you planning to add user login with templates/company fields auto-populated at one point? Looks very clean, congrats.&lt;/p&gt;
    &lt;p&gt;Why would you do something like this instead of using a cheap script from a codecanyon-type website (a true CRUD crm) where you can collect customer data and provide complete service in the long run? Just saying this because you said you built it for your own use.&lt;/p&gt;
    &lt;p&gt;I actually hadn’t heard of Codecanyon before! I used to use a paid invoicing service, but these days I just need a simple way to generate invoice PDFs - that’s really all I need.&lt;/p&gt;
    &lt;p&gt;You can use invoice generators that have complete control over your customers. Most scripts are php, and if you want something very detailed I'd go with Perfex. Codecanyon is the biggest code marketplace on the internet, owned by Envato.&lt;/p&gt;
    &lt;p&gt;I wonder if you could just send invoices to Comcast for price increases to their Payable Accounts department and if they'd just pay them. Or just invoice companies for "inconvenience fees" of sorts when they actually create inconveniences.&lt;/p&gt;
    &lt;p&gt;Working hard on Rad, which is aiming to be a Bash-replacement for writing CLI scripts. The goal is to allow users to write maintainable scripts with declarative argument parsing, built-in JSON processing, HTTP requests, and interactive prompts - all in a familiar, readable syntax (Python-like!). Here's an example of the declarative approach to script args:&lt;/p&gt;
    &lt;p&gt;args: username str # Required string password str? # Optional string token str? # Optional auth token age int # Required integer status str # Required string username requires password // If username is provided, password must also be provided token excludes password // Token and password cannot be used together age range [18, 99] // Inclusive range from 18 to 99 status enum ["active", "inactive", "pending"]&lt;/p&gt;
    &lt;p&gt;Rad does all the arg parsing for you (unlike Bash), including validation for those constraints you wrote, and you can get on with writing the rest of your script is a nice, friendly syntax!&lt;/p&gt;
    &lt;p&gt;Very keen for feedback so if any of that sounds interesting, feel free to give it a go!&lt;/p&gt;
    &lt;p&gt;Redesigning investment holdings for wider screens and leaning on hotwired turbo frames. Thankful for once-campfire as a reference for how to structure the backend. The lazy loading attribute works great with css media queries to display more on larger viewports.&lt;/p&gt;
    &lt;p&gt;Enjoying learning modern css in general. App uses tailwind, but did experiment with just css on the homepage. Letting the design emerge organically from using it daily, prototype with tailwind, then slim it back down with plain css.&lt;/p&gt;
    &lt;p&gt;I'm playing around with sandboxing techniques on Mac so I can isolate AI tools and prevent them from interacting with files they shouldn't have access to -- like all my dotfiles, AWS credentials, and such.&lt;/p&gt;
    &lt;p&gt;Along the way I rolled my own git-multi-hook solution (https://github.com/webcoyote/git-multi-hook) to use git hooks for shellcheck-ing, ending files with blank lines, and avoid committing things that shouldn't be in source control.&lt;/p&gt;
    &lt;p&gt;Yes, I've used docker and podman. They're great. But I wanted to be able to run Xcode and IOS simulator, which requires macOS, so developed these solutions.&lt;/p&gt;
    &lt;p&gt;In short, an explorable database of movies, TV shows, books and board games organised around the time and place that they're set. So if you're interested in stuff set during the French Revolution but not in Paris, you could find it there, for instance.&lt;/p&gt;
    &lt;p&gt;Nice, I can see the appeal having familiar UI on Mac.&lt;/p&gt;
    &lt;p&gt;Even though I am not your target audience (linux i3 user myself), I would be interested in knowing how much "hacking" the macOS system is required to do this. Is it hard to get a list of running apps for your Task Bar? Is it hard to list the apps for the menu? How about keeping it all "on top" while other windows e.g. get maximized/minimized/full-screen, etc?&lt;/p&gt;
    &lt;p&gt;I could talk for days on all the peculiar bugs resolved. Once the alpha stabilizes I have drafts to publish on several topics.&lt;/p&gt;
    &lt;p&gt;You actually nailed the major pain points. Particularly window focus and state management. I've spent months solving this problem alone.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;1. Applications data list: Getting the list is easy! Finding out which apps in that list are "real" apps isn't. Getting icons isn't. Reliably getting information on app state isn't. Finding out why something doesn't work right is as painful as can be. Doing all this in a performant way is a nightmare.&lt;/p&gt;
    &lt;p&gt;2. Applications menu renderer: Rendering the list for the menu is easy enough: the macOS app sends this data via socket. The frontend is just web sockets and web components under the hood (https://lit.dev). The difficult part was converting app icons to PNG, which is awfully slow. So a cache-warmup stage on startup finds all apps, converts their icons to png, and caches them to the app directory for read.&lt;/p&gt;
    &lt;p&gt;3. Window state: again, by far the worst and it isn't even close. Bugs galore. The biggest issue was overriding macOS core behavior on what a window is, when it's focused, and how to communicate its events reliably to the app. Although I did include a couple private APIs to achieve this, you can get pretty far by overriding Window class types in ways that I don't think were intended (lol). There is trickery required for the app to behave correctly: and the app is deceptively simple at a glance.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;One bug, and realization, that still makes me chuckle today.. anything can be a window in macOS.&lt;/p&gt;
    &lt;p&gt;I'm writing this on Firefox now, and if I hover over a tab and a tooltip pops up - that's a window. So a fair amount of time has gone into determining _what_ these apps are doing and why. Then coming up with rules on determining when a window is likely to be a "real" window or not.&lt;/p&gt;
    &lt;p&gt;The Accessibility Inspector app comes standard on macOS and was helpful for debugging this, but it was a pain regardless.&lt;/p&gt;
    &lt;p&gt;https://fooqux.com/ - an experimental tech article aggregator. For several years now, I've had a routine of collecting articles on topics that interest me throughout the week and then reading them over the weekend. To help organize and streamline this process, I created this website.&lt;/p&gt;
    &lt;p&gt;The main idea is to gather tech articles in one place and process them with a LLM — categorize them, generate summaries, and try experimental features like annotations, questions, etc.&lt;/p&gt;
    &lt;p&gt;I hope this service might be useful to others as well. You can sign up with github account to submit your articles as well.&lt;/p&gt;
    &lt;p&gt;It's meant to be a 'rails-like' experience in Go without too much magic and conventions.&lt;/p&gt;
    &lt;p&gt;Basically, speeding up development of fullstack apps in Go using templ, datastar, sqlc with an MVC architecture and some basic generators to quickly setup models, views and controllers.&lt;/p&gt;
    &lt;p&gt;Taking a break from tech to work on a luxury fashion brand with my mum. She hand paints all the designs. I it first collection is a set of silk scarves and we’re moving into skirts and jackets soon.&lt;/p&gt;
    &lt;p&gt;Been a wonderful journey to connect with my mum in this way. And also to make something physical that I can actually touch. Tech seems so…ephemeral at times&lt;/p&gt;
    &lt;p&gt;this is super cool. congrats and best of luck with it! Love the mother &amp;amp; son backstory to the product. The scarves look like they could make a great gift as well. I'll bookmark your website.&lt;/p&gt;
    &lt;p&gt;Super silly but I'm searching for a mathematical backdoor in Bitcoin's secp and the secr curve. I saw that both curves use unsafe primes (p-1 factors pretty well) for the generator order.&lt;/p&gt;
    &lt;p&gt;So I'm trying to define a multiplication operation using primitive roots.&lt;/p&gt;
    &lt;p&gt;Currently working on an open-source agent for privilege access management (PAM) and just-in-time access (JIT) to cloud infrastructure, SaaS applications and local systems. It's using serverless workflows (https://serverlessworkflow.io/) and https://www.temporal.io to guarantee robust deterministic workflow execution. Temporal is used to orchestrate elevations across environments and systems. It tasks “agents” to grant access where it needs to be rather than centralising permission stores. It guarantees execution and revocation of permissions. Run it locally for sudo, UAC. Or in the cloud for IAM or for individual applications. Check it out: https://github.com/thand-io/agent&lt;/p&gt;
    &lt;p&gt;Still working on my favicon fetching API: https://fetchfavicon.com. Currently adding comparison pages with other services. Also learning a lot of SEO and video editing for https://soulfulsabor.com, a food blog that I started with my wife.&lt;/p&gt;
    &lt;p&gt;I am working on an English learning app. I combine flashcards like Anki with Duolingo-style motivation: leagues and streaks. Plus, integrated Giphy and a Telegram bot. Of course, we use the OpenAI API. Also, as an idea for selecting words to learn, we parse movie subtitles with AI and find cool phrases and words to learn before watching a movie in English (as a second language). The app has Russian translations and UI. My goal is to create the best online dictionary for English learners. We use a crowdsourcing approach where anyone can suggest a cool illustration for any word or phrase and add any word or idiom to learn.&lt;/p&gt;
    &lt;p&gt;I’m building SPARK (Signal Processing Algorithms, Routines, and Kernels), an open-source library of modular, efficient DSP components for low-power embedded audio systems.&lt;/p&gt;
    &lt;p&gt;The goal is to make it straightforward to design and deploy small, composable audio graphs that fit on MCUs and similar hardware. The project is in its infancy, so there’s plenty of room for experimentation and contributions.&lt;/p&gt;
    &lt;p&gt;I've been vanlifing for a few months now. I tend to have long hours on the road where my mind wonders and I want to write code hands-free.&lt;/p&gt;
    &lt;p&gt;So, I built it.&lt;/p&gt;
    &lt;p&gt;Using ChatGPT's voice agents to generate Github issues tagging @claude to trigger Claude Code's Github Action, I created https://voicescri.pt that allows me to have discussions with the voice agent, having it create issues, pull requests, and logical diffs of the code generated all via voice, hands free, with my phone in my pocket.&lt;/p&gt;
    &lt;p&gt;Your van is probably better than mine, but when I was vanlifing with my wife, I really regretted spending so many long hours on the road. If I did it over again, I'd try to limit driving to a max of two hours per day and five hours per week. We spent far too much money on repairs and not nearly enough time writing code or exploring the places I drove through. Or past.&lt;/p&gt;
    &lt;p&gt;Are you reviewing code by voice, like a blind programmer? Have you tried Emacspeak? I know that's not normally hands-free.&lt;/p&gt;
    &lt;p&gt;Building a tool that automatically generates living infrastructure diagrams from your IaC files and turns them into real-time incident dashboards. Think Figma meets Datadog - beautiful visualization that updates during outages to show you exactly what's failing and how to fix it.&lt;/p&gt;
    &lt;p&gt;The insight: your architecture diagram shouldn't be a stale PNG in Confluence. It should be your war room during incidents.&lt;/p&gt;
    &lt;p&gt;Going to be available as both web app and native desktop.&lt;/p&gt;
    &lt;p&gt;I'm making an OpenAI API proxy to stay within a spending limit (like 1$ per hour then a 429), AFAIK they only support "budget alerts" and I'm not comfortable releasing anything without a hard limit on the spend. https://github.com/goverture/goxy - Still a work in progress, I plan to support streaming as well and might support other providers if there's a demand for it.&lt;/p&gt;
    &lt;p&gt;OpenAI gives an allotment of free daily tokens if you agree to hand over the inputs as training. I’d love a proxy that places the limit just before you exhaust those free tokens, to avoid incurring any expenses for small hobbyist projects.&lt;/p&gt;
    &lt;p&gt;This would be a game changer. There are so many times I run out of credits without knowing when it’s going! I know there is the dashboard, but I think it’s quite limited in my use cases.&lt;/p&gt;
    &lt;p&gt;My on again, off again life's work has been a foss dev stack for interactive tutoring systems. Something like a general purpose Math Academy, with mechanics to permit UGC and courses that are both adaptive (to the user's background and demonstrated skill) and inter-adaptive (to the userbase's expressed priorities).&lt;/p&gt;
    &lt;p&gt;I am using this stack now to build an early literacy app targeting kids aged 3-5ish at https://letterspractice.com (also pre-release state, although the email waitlist works I think!). LLM assisted edtech has a lot of promise, but I'm pretty confident I can get the unit cost for teaching someone to read down to 5 USD or less.&lt;/p&gt;
    &lt;p&gt;The docs seem to be highly targeted towards software engineers who want to build the system. There is scant information on how teachers would find this useful.&lt;/p&gt;
    &lt;p&gt;It's like inventing the refrigerator and all the brochure talk about is the internal engineering of the machine, rather than how keeping food cold is useful from the economic and culinary perspectives.&lt;/p&gt;
    &lt;p&gt;It currently supports complex heatmaps based on travel time (e.g. close to work + close to friends + far from police precincts), and has a browser extension to display your heatmap over popular listing sites like Zillow.&lt;/p&gt;
    &lt;p&gt;I'm thinking of making it into an API to allow websites to integrate with it directly.&lt;/p&gt;
    &lt;p&gt;Absolutely stellar! I've been looking for something like this for ages. Any chance you'll have some pre- defined options like grocery stores, libraries, airport, etc?&lt;/p&gt;
    &lt;p&gt;Living in hongkong for a few months, and absolutely love exploring the different neighborhoods. I’d love something like this or walkscore but for local guides to contribute.&lt;/p&gt;
    &lt;p&gt;The demo with an inline 8 at 16000 RPM is hard to judge, because I've never heard such an engine IRL. Might I suggest adding demos of engines people know the sound of?&lt;/p&gt;
    &lt;p&gt;I'm building FlightWise (https://flightwise.io), an all-in-one SaaS platform for flight school operations.&lt;/p&gt;
    &lt;p&gt;After acquiring a flight school, I quickly realized how challenging the day-to-day operations were. To solve the problems of aircraft fleet management, scheduling, and student course progress tracking, I developed a comprehensive platform that handles all aspects of running a flight school. Existing software is often outdated and expensive, offering poor value for its high cost. FlightWise was built off the real world experiences of my own school, where it has delivered immediate and invaluable benefits to our entire team, from students to administrative staff. We've just recently started to offer this platform publicly to other flight schools.&lt;/p&gt;
    &lt;p&gt;Thank you! Started development around August 2024 and 2 months later we had it in use at our school in a very early state, and then over time added more huge features, such as bookings, etc. About 4 months ago, we fully moved away from the existing antiquated platform we were paying for, as FlightWise reached a complete feature set.&lt;/p&gt;
    &lt;p&gt;Building a VsCode extension for drone coding. So you can easily write code to control a fleet of drones, deploy ai model and even setup training of new Reinforcement Learning models for drone behavior https://tensorfleet.net&lt;/p&gt;
    &lt;p&gt;It's an API that allows zero-knowledge proofs to be generated in a streaming fashion, meaning ZKPs that use way less RAM than normal.&lt;/p&gt;
    &lt;p&gt;The goal is to let people create ZKPs of any size on any device. ZKPs are very cool but have struggled to gain adoption due to the memory requirements. You usually need to pay for specialized hardware or massive server costs. Hoping to help fix the problem for devs&lt;/p&gt;
    &lt;p&gt;Fwiw: the website is brand new and very much in the "hot garbage" phase of development. I'm not a front-end guy, so critique is welcome from all - especially any bugs in the UX. I'm still actively uncovering them&lt;/p&gt;
    &lt;p&gt;I am working on a microkernel for arm-m33 microcontrollers. Targeting the RP2350 first.&lt;/p&gt;
    &lt;p&gt;It’s going to feature a synchronous IPC model where the inter-task ‘call graph’ is known at compilation. Function call semantics to pass data between tasks. Call() recieve() reply()&lt;/p&gt;
    &lt;p&gt;A build tool that reads TOML will generate the kernel calls so that tasks can be totally isolated — all calls go though supervisor trap so we have true memory isolation.&lt;/p&gt;
    &lt;p&gt;Preemptions are possible but control is yielded only at IPC boundary so it’s not hard realtime.&lt;/p&gt;
    &lt;p&gt;So that makes things super robust and auditable behavior at compile time. Total isolation means tasks can crash catastrophically without affecting the rest of the system. Big downsides are huge increase in flash usage, constrained programming model, complex build system, task switching overhead. Just a very different model than what I’m used to at $dayjob.&lt;/p&gt;
    &lt;p&gt;I want to basically find out, hey what happens when we go full safety!? What’s hard about it? What tradeoffs do we need to make? And also kinda like what’s a different model for multitasking. Written in Rust of course.&lt;/p&gt;
    &lt;p&gt;It's a long running process, and the HW is mostly defined (but not laid out) but on pause while I work on porting TockOS to an ATSAMV71 to make sure I won't run into any project ending issues with the SW before I build the hardware.&lt;/p&gt;
    &lt;p&gt;- A front-end library that generates 10kb single-html-file artifacts using a Reagent-like API and a ClojureScript-like language. https://github.com/chr15m/eucalypt&lt;/p&gt;
    &lt;p&gt;- Beat Maker, an online drum machine. I'm adding sample uploads now with a content accessible storage API on the server. https://dopeloop.ai/beat-maker&lt;/p&gt;
    &lt;p&gt;- Tinkering with Nostr as a decentralized backend for simple web apps.&lt;/p&gt;
    &lt;p&gt;As a means to get into WebAssembly, I started writing a WebAssembly binary decoder (i.e. a parser for `.wasm` files) from scratch.&lt;/p&gt;
    &lt;p&gt;Recently I started executing the upstream spec tests against it, as a means to increase spec conformance. It's non-streaming, which is a non-starter for many use cases, but I'm hoping to provide a streaming API later down the road. Also, the errors interface are still very much WIP.&lt;/p&gt;
    &lt;p&gt;All that said, it's getting close to a fully-conformant one and it's been a really fun project.&lt;/p&gt;
    &lt;p&gt;A common lisp (sbcl) roguelike, to (re)learn Emacs and CL for fun. Using croatoan for ncurses, though if I had found BearLibTerminal earlier I might have used that.&lt;/p&gt;
    &lt;p&gt;Not sure what the market is for something like this but it's something I've been thinking a lot about since stepping down as CEO of my previous company.&lt;/p&gt;
    &lt;p&gt;My goal is two-fold:&lt;/p&gt;
    &lt;p&gt;1. Help teams make better, faster decisions with all context populating a source-of-truth.&lt;/p&gt;
    &lt;p&gt;2. Help leaders stay eyes-on, and circumstantially hands-on, without slowing everything down. What I'd hope to be an effective version of "Founder Mode".&lt;/p&gt;
    &lt;p&gt;If anybody wants to play around with it, here's a link to my staging environment:&lt;/p&gt;
    &lt;p&gt;Great idea! Great website! Terrible video. The 90 second format is great, this is how much I would like to spend learning what exactly your product does. But the whole video is just clicking some user interfaces with no result. After watching the video, I have even less idea of what it the product is for. I would love to see a video that goes through the "next, next, next" in the wizard and then shows the actual outcome.&lt;/p&gt;
    &lt;p&gt;Great feedback, I'll work on the video ASAP. I intended to immediately create a follow-up video that steps through each component of a newly created decision, got distracted, never circled back.&lt;/p&gt;
    &lt;p&gt;OK, it seems you are on the path of another 8 fig exit. Good on you. It seems like a great project and could possible save so much time if well executed and well integrated.&lt;/p&gt;
    &lt;p&gt;I've added it to SaaSHub saashub.com/orgtools. If you have an @orgtools.com email you can verify and improve the profile. Cheers!&lt;/p&gt;
    &lt;p&gt;This is a good nudge to choose the grammatically correct option, thank you.&lt;/p&gt;
    &lt;p&gt;I originally had "less meetings" before an LLM corrected me into using "fewer meetings". Then when talking about Orgtools to a couple people I heard them say "less meetings" and switched back thinking that sounds slightly more natural (but incorrect).&lt;/p&gt;
    &lt;p&gt;It is a tool that lets you create whiteboard explainers.&lt;/p&gt;
    &lt;p&gt;You can prompt it with an idea or upload a document and it will create a video with illustrations and voiceover. All the design and animations are done by using AI apis, you dont need any design skills.&lt;/p&gt;
    &lt;p&gt;Here is a video explainer of the popular "Attention is all you need" paper.&lt;/p&gt;
    &lt;p&gt;I really like the idea! One issue though is that the content seems to "stream" much slower than what's being spoken. The result is that I'm sitting there waiting to see whats going to come, even though its already been said which makes it hard to focus on whatever new information is coming.&lt;/p&gt;
    &lt;p&gt;The animations / drawings themselves are solid too. I think there's more to play with wrt the dimensions and space of the background. It would be nice to see it zoom in and out for example.&lt;/p&gt;
    &lt;p&gt;A little computer vision library for embedded systems, by magnitudes smaller than OpenCV, but still practical enough to do feature tracking or cascade detections. Works well on ESP32 and cheap ARMs with low-resolution grayscale cameras.&lt;/p&gt;
    &lt;p&gt;Still slowly working away on my location intelligence data union..&lt;/p&gt;
    &lt;p&gt;I’ve spent a while understanding what sort of market would make it viable. I think it does actually work if you can square: 10K participants per major metro area, revenue of about 2.9M per metro area (so say, 5K monthly recurring with about 50 customers).&lt;/p&gt;
    &lt;p&gt;At that point you could pay data union participants about $5 a week to share their location data with you.&lt;/p&gt;
    &lt;p&gt;From talking to some previous data union folks, the major challenges are paying out (my target is much higher than any union managed), and people dropping out over time.&lt;/p&gt;
    &lt;p&gt;My bet is that these are both solvable things by selling data products rather than just bundles of data, and the data source being very passive.&lt;/p&gt;
    &lt;p&gt;I’m also interested in the idea that such a union should act more like a union than previous efforts in this space, by actively defending members’ data from brokers.&lt;/p&gt;
    &lt;p&gt;I work with DSPy in Python and felt it was missing in the Ruby ecosystem.&lt;/p&gt;
    &lt;p&gt;So I started https://github.com/vicentereig/dspy.rb: a composable, type-safe version built for Rubyists who want to design and optimize prompts, and reuse LLM pipelines without leaving their language of choice. Working with DSPy::Signatures reminds me a bit of designing a db schema with an ORM.&lt;/p&gt;
    &lt;p&gt;It’s still early, but it already lets you define structured modules, instrument them in Langfuse, wire them up like functional components, and experiment with signature optimization. All in plain Ruby.&lt;/p&gt;
    &lt;p&gt;Most recipes are a failure for beginners on the first try. I aim to make recipes bulletproof so anyone can pick up any recipe and it will just work.&lt;/p&gt;
    &lt;p&gt;The goal is to make the best recipe app ever. On a technical level recipes are built as graphs and assembled on demand. This makes multilanguage support easy, any recipe can use any unit imaginable, blind people could have custom recipe settings for their needs, search becomes OP, and there is also a wikipedia like database with information that links to all recipes. Because of the graphs; nutritional information, environmental impact, cost etc. can simply be calculated accurately by following linked graphs. Most recipe apps are very targeted to specific geographical regions and languages, this graph system removes a lot of barriers between countries and will also be a blessing to expats. Imagine an American in Europe that wish to use imperial units, english recipes, but with ingredients native to their new homeland. No problem, just follow a different set of nodes and the recipe is created that way for them.&lt;/p&gt;
    &lt;p&gt;The website is slightly outdated but gives a good idea of what is coming. Current goal is to do beta launch in 2026.&lt;/p&gt;
    &lt;p&gt;I admire the dedication and love to idea / how much you've thought it trough from the app / logic side.&lt;/p&gt;
    &lt;p&gt;From the marketing side...&lt;/p&gt;
    &lt;p&gt;I'd make a selection on the website on first visit - I'm a chef / creator - I like to cook&lt;/p&gt;
    &lt;p&gt;Your cta (call to action) is... Not very effective&lt;/p&gt;
    &lt;p&gt;Instagram only has 7 followers and no posts. ...&lt;/p&gt;
    &lt;p&gt;I like the dedication but I'd definitely recommend to improve your marketing / promotion skills (if you build it they will come is a myth unfortunately...), if you wanna have a call about it feel free to hit me up, tijlatduckdotcom. I'm also in Europe so easy for timing.&lt;/p&gt;
    &lt;p&gt;Working on https://fileboost.dev –– A Ruby on Rails Active Storage plugin that is a plug and play gem for image transformation without making any code changes.&lt;/p&gt;
    &lt;p&gt;Being a Ruby on Rails consultant, I frequently see active storage transformation becoming a bottleneck for web servers by eating up resources and making them sweat.&lt;/p&gt;
    &lt;p&gt;I built Fileboost to solve this problem for my customers. I'd love any feedback.&lt;/p&gt;
    &lt;p&gt;I'm working on 1:6 size furniture. There's not much woodworking I can do outside of the shop, so I've been trying to shrink full joinery techniques down to dollhouse size.&lt;/p&gt;
    &lt;p&gt;I’m still working on turning a wishlist app that I built for my friends into a real product — it’s called https://thingstohave.app. I wrote a comment about it in summer, and these are the updates:&lt;/p&gt;
    &lt;p&gt;1. I shared the app with the small audience I have and received some feedback in very unexpected places. First, it was hard to understand how lists work because putting things into lists was an unobvious process. I fixed that by adding DnD that works well both with mouse and touch (turned out it’s two separate APIs). Second, users thought that the screenshot on the quite minimal landing page was the real app, and they clicked on it. The problem was so frequent and surprising that I decided to add something funny for people who do that, as I’m not willing to contribute a lot of time to the landing right now.&lt;/p&gt;
    &lt;p&gt;2. I underestimated how bad discoverability on the internet is. My expectation was that I would make my site fully server-side rendered, add a basic sitemap to Search Console, and have a few dozen organic users during the pre-holiday season when users are filling their wishlists. In reality, I got zero — not even users, but even visits. So I started actually working on SEO, no black magic but just adding slightly more complex sitemaps, micro-markup, and other stuff which I thought only products competing for the first page would need.&lt;/p&gt;
    &lt;p&gt;My next steps are to work on getting some minimal organic inflow of users and improving stuff related to auth and user management, which is the most time-consuming part of the work right now.&lt;/p&gt;
    &lt;p&gt;I’m creating an electronic avionics sensor and display for experimental aircraft. I’m having a fantastic time learning about circuits and MCUs (I have a pure CS degree, zero background with EE stuff). I’ve been working on this in my off hours for over a year now, maybe someday it will be a product that people buy!&lt;/p&gt;
    &lt;p&gt;The current challenge is the display. I’ve struggled to learn about this part more than any other. After studying DVI and LVDS, and after trying to figure out what MIPI/DSI is all about, I think parallel RGB is the path forward, so I’ve just designed a test PCB for that, and ordered it from JLCPCB’s PCBA service.&lt;/p&gt;
    &lt;p&gt;Our waitlist is open for https://flatm8.co.uk - the platform for anonymous reviews of Landlords and Estate Agents in Britain and Ireland.&lt;/p&gt;
    &lt;p&gt;We’re working directly with partner housing unions and charities in Britain and Ireland to build the first central database of rogue landlords and estate agents. Users can search an address and see if it’s marked as rogue/dangerous by the local union, as well as whether you can expect to see your deposit returned, maintenance, communication - etc.&lt;/p&gt;
    &lt;p&gt;After renting for close to a decade, it’s the same old problems with no accountability. We wanted to change this, and empower tenants to share their experiences freely and easily with one another.&lt;/p&gt;
    &lt;p&gt;We’re launching in November, and I’m very excited to announce our partner organisations! We know this relies on a network effect to work, and we’re hoping to run it as a social venture. I welcome any feedback.&lt;/p&gt;
    &lt;p&gt;I’d love to know how it went for you and if there’s anything we can learn from your experiences - you’re right that it’s sorely needed! The statistics are getting worse and worse and worse… please feel free to email any thoughts or ideas based on your launch to team @ domain !&lt;/p&gt;
    &lt;p&gt;I'm calling it a "Micro Functions as a Service" platform.&lt;/p&gt;
    &lt;p&gt;What it really is, is hosted Lua scripts that run in response to incoming HTTP requests to static URLs.&lt;/p&gt;
    &lt;p&gt;It's basically my version of the old https://webscript.io/ (that site is mostly the same as it was as long as you ignore the added SEO spam on the homepage). I used to subscribe to webscript and I'd been constantly missing it since it went away years ago, so I made my own.&lt;/p&gt;
    &lt;p&gt;I mostly just made this for myself, but since I'd put so much effort into it, I figure I'm going to try to put it out there and see if anyone wants to pay me to use it. Turns out there's a _lot_ of work that goes into abuse prevention when you're code from literally anyone on the internet, so it's not ready to actually take signups yet. But, there is a demo on the homepage.&lt;/p&gt;
    &lt;p&gt;This one's going to be out of left field, but last Thursday I launched Countdown Treasure (https://countdowntreasure.com)&lt;/p&gt;
    &lt;p&gt;It's a real life treasure hunt in the Blue Ridge Mountains with a current total prize of $31,200+ in gold coins and a growing side pot.&lt;/p&gt;
    &lt;p&gt;I modeled it off of last year's Project Skydrop (https://projectskydrop.com) which was in the Boston area.&lt;/p&gt;
    &lt;p&gt;* Shrinking search area (today, Day 5, it will be 160 miles, on Day 21 it'll be just 1 foot wide)&lt;/p&gt;
    &lt;p&gt;* 24/7 webcam trained on the jar of gold coins sitting on the forest floor just off a public hiking trail&lt;/p&gt;
    &lt;p&gt;* Premium upgrades ($10 from each upgrade goes towards the side pot) for aerial photos above the treasure and access to a private online community (and you get your daily clues earlier)&lt;/p&gt;
    &lt;p&gt;* $2 from each upgrade goes towards the goal of raising $20k for continued Hurricane Helene relief&lt;/p&gt;
    &lt;p&gt;So far the side pot is $6k and climbing.&lt;/p&gt;
    &lt;p&gt;It's been such a fun project to work on, but also a lot of work. Tons of moving parts and checking twice and three times to make sure you've scrubbed all the EXIF data, etc.&lt;/p&gt;
    &lt;p&gt;Thanks! Yeah, I unfortunately can't shell out a straight $20k donation, but I saved the $25k over the year from my e-commerce business and justified it as a marketing expense if worse case scenario happens and it's found earlier than the math predicts. But if we get past break even then can't wait to write that check to help out the communities around here that are still recovering.&lt;/p&gt;
    &lt;p&gt;Maybe some? But the circle today will be 160 miles wide or about the same width as Switzerland and Denmark. So I'm not sure how much shadows would help you pinpoint a specific location in an entire country worth of mountains.&lt;/p&gt;
    &lt;p&gt;I'm attempting to work on a "spiritual successor" to Dramatica Story Expert, a crazy story theory/brainstorming program of days gone by. Technically, Dramatica is still around, but they never made a 64-bit version for Macs, and both the Mac and Windows version have been tenaciously clinging to the trailing edge of technology for decades. (The Mac version somehow never got retina fonts. I'm not sure how you even do that.)&lt;/p&gt;
    &lt;p&gt;I started my program in Swift and SwiftUI, although for various reasons I'm starting to look at Dart and Flutter (in part because being multiplatform would be beneficial, and in part because I am getting the distinct feeling this program is more ambitious than where SwiftUI is at currently). It isn't a direct port of Dramatica by any stretch, instead drawing on what I've learned writing my own novels, getting taught by master fiction writers, and being part of writing workshops. But no other program that I've seen uses Dramatica's neatest concepts, other than Subtxt, a web-based, AI-focused app which has recently been anointed Dramatica's official successor. (It's a neat concept, but it's very expensive compared to the original Dramatica or any other extant "fiction plotting" program. Also, there's a space for non-AI software here, I suspect: there are a lot of creatives who are adamantly opposed to it in any form whatsoever.)&lt;/p&gt;
    &lt;p&gt;Came from my frustration with Google Maps in Germany constantly having take-down requests for bad reviews and ratings. To get around this, we only list places we recommend.&lt;/p&gt;
    &lt;p&gt;Working on improving the data pipeline for https://iplocate.io - an IP intelligence service I've worked on since 2017.&lt;/p&gt;
    &lt;p&gt;Recent focus has been on geolocation accuracy, and in particular being able to share more data about why we say a resource is in a certain place.&lt;/p&gt;
    &lt;p&gt;Lots of folks seem to be interested in this data, and there's very little out there. Most other industry players don't talk about their methodology, and those that do aren't overly honest about how X or Y strategy actually leads to a given prediction, or the realistic scale or inaccuracies of a given strategy, and so on. So this is an area I'm very interested in at the moment and I'm confident we can do better in. And it's overall a fascinating data challenge!&lt;/p&gt;
    &lt;p&gt;I'm working on a design system. I'm a software eng not a designer, but I started one a long while back because I wanted to get a sense of what designers go through. I've dropped it and came back a half dozen times but now I'm finishing it up.&lt;/p&gt;
    &lt;p&gt;It's been a great project to understand how design depends on a consistent narrative and purpose. At first I put together elements I thought looked good but nothing seemed to "work" and it's only when I took a step back and considered what the purpose and philosophy of the design was that it started to feel cohesive and intentional.&lt;/p&gt;
    &lt;p&gt;I'll never be a designer but I often do side projects outside my wheelhouse so I can build empathy for my teammates and better speak their language.&lt;/p&gt;
    &lt;p&gt;I'm working on Botnet of Ares, a hacking simulator game for PC [0]. It's an homage to classics such as Uplink and Hacknet, and also a commentary on the state of the IoT security industry.&lt;/p&gt;
    &lt;p&gt;Recently I've managed to port the game onto a real-world cyberdeck, the uConsole. [1]&lt;/p&gt;
    &lt;p&gt;I'm still rebuilding OnlineOrNot's frontend to be powered by the public REST API. Uptime checks are now fully powered by a public API (still have heartbeat checks, maintenance windows, and status pages to go).&lt;/p&gt;
    &lt;p&gt;Doing this both as a means of dogfooding, and adding features to the REST API that I easily dumped into the private GraphQL API without thinking too hard. That, and after I finish the first milestone (uptime checks + heartbeat/cron job monitors), I'll be able to start building a proper terraform provider, and audit logs.&lt;/p&gt;
    &lt;p&gt;Basically at the start of the year I realised GraphQL has taken me as far as it can, and I should've gone with REST to start with.&lt;/p&gt;
    &lt;p&gt;I think app icons are an underrated artistic format, but they’ve only been used for product logos. I made 001 to explore the idea of turning them into an open-ended creative canvas. There are 99 “exhibit spaces” in the gallery, and artists can claim an exhibit to install art within. Visitors purchase limited-edition copies of pieces to display as the app’s icon, the art’s native format.&lt;/p&gt;
    &lt;p&gt;It’s a real-money marketplace too - the app makes money by taking commission of sales (Not crypto). I like economic simulation games and I think the constraints here could be interesting.&lt;/p&gt;
    &lt;p&gt;I’m currently looking for artists to exhibit in the gallery, if anyone is interested, or knows someone who may be, please let me know!&lt;/p&gt;
    &lt;p&gt;Having migraines on and off the past few months, I wanted a way to try and narrow down triggers. All the existing apps out there were overly complicated. So I built something simpler.&lt;/p&gt;
    &lt;p&gt;It’s an iOS app to help tracking events and stats about my day as simple dots. How many cups of coffee? Did I take my supplements? How did I sleep? Did I have a migraine? Think of it like a digital bullet journal.&lt;/p&gt;
    &lt;p&gt;Then visualizing all those dots together helps me see patterns and correlations. It’s helped me cut down my occurrence of migraines significantly. I’m still just in the public beta phase but looking forward to a full release fairly soon.&lt;/p&gt;
    &lt;p&gt;Would love to hear more feedback on how to improve the app!&lt;/p&gt;
    &lt;p&gt;This is great! I can see this useful across a variety of self-assessment things: - I’m tired often, are there certain patterns that align with that? - I’m feeling anxious, what events in a day (or other inputs) align with that?&lt;/p&gt;
    &lt;p&gt;I've been working on a tool called Materia[0] for managing Podman Quadlets on hosts; I released a new version last month (and posted it on the September thread) and just merged automatic volume data migration the other day. Next goal is to design a system for downloading and loading remote components, similar to ansible roles. Hopefully I can tie it into the new podman quadlet install/etc commands.&lt;/p&gt;
    &lt;p&gt;A few months ago, I built a simple athlete profile page for my son (Track sprinting) to log his performance and progress over time.&lt;/p&gt;
    &lt;p&gt;He liked what I built for him and I got jealous, so I expanded it with my own profile (Trail running).&lt;/p&gt;
    &lt;p&gt;Then, I got curious… Could I build a full web platform for people to track their sporting life? I mean we have LinkedIn and CVs for our job career, why not celebrate all our sports/training efforts as well.&lt;/p&gt;
    &lt;p&gt;After a couple of months on the side, I'm pretty happy with Flexbase. If you're into sports, give it a try and let me know what's missing for you.&lt;/p&gt;
    &lt;p&gt;You can list the sports you're doing or did in your entire life, you can add your PRs, training routines, gear, competition results, photos. You can also list your clubs, and invite/follow your training buddies.&lt;/p&gt;
    &lt;p&gt;Honestly, I'm not sure where (or if) to expand it... Turn it into a Club-centric tool, make it more into a social network for sporty people.&lt;/p&gt;
    &lt;p&gt;Lots of ideas, but I'd love to find someone to work on it with me. I find that building alone is less fun.&lt;/p&gt;
    &lt;p&gt;Financial institutions and governments don’t spot crime because of incomplete information at individual firms. We help them understand federated learning and how to effectively collaborate and not just talk about it. All code is open source, so you can always help out ;-)&lt;/p&gt;
    &lt;p&gt;I'm fiddling with index optimizations for the Marginalia Search index software, with being able to add ad-hoc domain filters in mind.&lt;/p&gt;
    &lt;p&gt;Not sure if there's more to say about it right now except that fuzz tests are good for this sort of low level programming with disk layouts involved. They drive up test execution time, but it's still almost hard to build them too early or have too many of them, as there's almost always an unimaginable number of permutations of weird corner cases that are hard to get at with regards to block boundaries and so on that are hard to identify based on staring at the code and doing classic unit tests.&lt;/p&gt;
    &lt;p&gt;I'm working on Teletable (https://teletable.app), a macOS app that shows live football &amp;amp; F1 standings/results with a teletext interface (think BBC Ceefax). It's free and on the appstore:&lt;/p&gt;
    &lt;p&gt;I am currently working on a small side-project focused on React Native apps to manage their version update and maintenance mode. - https://appcockpit.dev&lt;/p&gt;
    &lt;p&gt;Right now I am getting my first users and already getting great feedback. Many things on the roadmap.&lt;/p&gt;
    &lt;p&gt;Always eager to learn more about others pain points when it comes to React Native/mobile development. Let me know what you think!&lt;/p&gt;
    &lt;p&gt;Currently building a suite of media inspection and encoding tools for video engineers: https://video-commander.com.&lt;/p&gt;
    &lt;p&gt;Still a work in progress, but expecting to release by end of year. Built on Rust + Tauri, in case anyone is curious.&lt;/p&gt;
    &lt;p&gt;I've created various open-source and commercial tools in the multimedia space over the last 10+ years and wanted to put it all together into something more premium.&lt;/p&gt;
    &lt;p&gt;Having worked for different startups for 10+ years and started 3 of my own (eventually failing), I always wanted a job board for local startups. Not necessarily IT-related jobs. Finally built it about a month ago: https://estonianstartupjobs.ee&lt;/p&gt;
    &lt;p&gt;There are few similar projects too, one is itself a startup which sadly on the verge of bankruptcy, and another aggregates only IT-related jobs.&lt;/p&gt;
    &lt;p&gt;I've been working to build some tools for detecting and monitoring lookalike domains - the kinds of things used in phishing / brand impersonation attacks.&lt;/p&gt;
    &lt;p&gt;My current prototype scans potential lookalikes for a target domain and then tracks DNS footprint over time. It's early, but functional - and makes it easier to understand if some lookalike domain is looking more "threat-y".&lt;/p&gt;
    &lt;p&gt;I've also been working on automating the processing of a parent-survey response for my kid's school using LLMs. The goal is to produce consistent summarization and statistics across multiple years and provide families with a clearer voice and helping staff and leadership at the school best understand what things have been working well (and where the school could improve).&lt;/p&gt;
    &lt;p&gt;I am working on Daestro[0], which is a cloud agnostic job orchestrator with built-in support for AWS, Vultr, DigitalOcean and Linode to run jobs on. Daestro can spawn and terminate compute instances based on requirement. It is suitable for running batch jobs or data engineering related jobs.&lt;/p&gt;
    &lt;p&gt;Self-hosted compute can also be linked to Daestro to run jobs on.&lt;/p&gt;
    &lt;p&gt;Collecting public datasets for training visual AI models to track and target drones.&lt;/p&gt;
    &lt;p&gt;Drones are real bastards - there's a lot of startups working on anti drone systems and interceptors, but most of them are using synthetic data. The data I'm collecting is designed to augment the synthetic data, so anti drone systems are closer to field testing&lt;/p&gt;
    &lt;p&gt;I help privacy and data sovereignty enthusiasts take back control of their data without needing to change their habits.&lt;/p&gt;
    &lt;p&gt;I’ve been working for the past 3 years on SelfHostBlocks https://github.com/ibizaman/selfhostblocks, making self-hosting a viable and convenient alternative to the cloud for non technical people.&lt;/p&gt;
    &lt;p&gt;It is based on NixOS and provides a hand-picked groupware stack: user-facing there is Vaultwarden and Nextcloud (and a bunch more but those 2 are the most important IMO for non technical people as it covers most of one’s important data) and on the backend Authelia, LLDAP, Nginx, PostgreSQL, Prometheus, Grafana and some more. My know-how is in how to configure all this so they play nice together and to have backups, SSO, LDAP, reverse proxy, etc. integration. I’m using it daily as the house server, I’m my first customer after all. And beginning of 2025 it passed my own internal checkpoint to be shared with others and there’s a handful of technical users using it.&lt;/p&gt;
    &lt;p&gt;My goal is to work on this full time. I started a company to provide a white glove installation, configuration and maintenance of a server with SelfHostBlocks. Everything I’ll be doing will always be open source, same as the whole stack and the server is DIY and repair friendly. The continuous maintenance is provided with a subscription which includes customer support and training on the software stack as needed.&lt;/p&gt;
    &lt;p&gt;I'm a filmmaker, so I built myself the filmmaking community tool that I wanted to use. I'm headed to filmquest in provo this month to premiere my short and that's gonna be a big test of my application.&lt;/p&gt;
    &lt;p&gt;* LLMs are accessible where telegram is accessible&lt;/p&gt;
    &lt;p&gt;* Multitude of models to choose from (chatgpt, claude, gemini) and more is coming.&lt;/p&gt;
    &lt;p&gt;* Full control over the bot behaviour is in user's hands: I don't add any system messages or temperature/top_p. I give UI for full control over system messages, temperature, top_p, thinking, web searching/scrapping and more to come.&lt;/p&gt;
    &lt;p&gt;* Q/A like context handling. Context is not carried through the whole bot, it's rather carried through chaing of replies. Naturally could be branched or use various models cross messages.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;This is my hobby project and one of main tools for working with LLMs, thus I'm going to stick to it for quite a while.&lt;/p&gt;
    &lt;p&gt;- one man project (me) - been doing it well over a year now - no sponsorship, no investors, no backers, no nothing just my passion - I haven't even advertised much, this may first ir second time I'm sharing a link - On a weekdays im building a serious stuff with it - On weekends preparing a new major version with lessons learned from doing a real project with it&lt;/p&gt;
    &lt;p&gt;Not going to stop. But I migh be seeking sponsors in future, not sure how that will turn out. If not that's ok, I'm cool to be only user.&lt;/p&gt;
    &lt;p&gt;Basically the title explains it, I challenged myself to making a chrome extension a day for a month. I've been posting my progress on reddit, and my first two extensions have just been accepted to the chrome store (I'm only done day 3 so far, those were quick reviews!). For those interested:&lt;/p&gt;
    &lt;p&gt;Day 1: Minimal Twitter&lt;/p&gt;
    &lt;p&gt;Day 2: No Google AI Overview in Google Search&lt;/p&gt;
    &lt;p&gt;Day 3: No Images Reddit (Not Published, yet!)&lt;/p&gt;
    &lt;p&gt;I'm posting daily, I would love to hear thoughts on the extensions!!&lt;/p&gt;
    &lt;p&gt;I built https://invoicepad.app which is a free, completely in-browser tool for creating invoices, estimates, and quotes. Yes, similar apps have been posted here before, but none were built the way I envisioned, so I made my own. The key difference: all invoice data is stored in the URL hash, not the querystring. This is important because querystrings are sent to the server with every request, while hashes stay local to your browser. This means I can never see your invoice data, unlike other similar apps. The workflow is simple: use your browser's bookmark manager as your invoice filing system. Or if you want to keep it offline, just copy and paste invoice URLs into a text document for storage. I’ve also included helpful features like saved profiles to save on repeated data input. The next step is to finish working on a browser extension (v1 is being tested) to make bookmarking, editing, and saving changes even easier, that is if I ever stop being distracted by other side projects.&lt;/p&gt;
    &lt;p&gt;I’ve been working on DB Pro — a modern desktop database workbench built with Electron, React, and Drizzle ORM. It’s designed to feel fast, cohesive, and genuinely enjoyable to use — something that sits somewhere between TablePlus, Notion, and VS Code.&lt;/p&gt;
    &lt;p&gt;Right now it connects to local and remote databases like SQLite and Postgres, lets you browse schemas and tables instantly, edit data inline, and create or modify tables visually. You can save and run queries, generate SQL using AI, and import or export data as CSV or JSON. There’s also a fully offline local mode that works great for prototyping and development.&lt;/p&gt;
    &lt;p&gt;One of the more unique aspects is that DB Pro lets you download and run a local LLM for AI-assisted querying, so nothing ever leaves your machine. You can also plug in your own cloud API key if you prefer. The idea is to make AI genuinely useful in a database context — helping you explore data and write queries safely, not replacing you.&lt;/p&gt;
    &lt;p&gt;The next big feature is a Visual Query Builder with JOIN support that keeps the Visual, SQL, and AI modes in sync. After that, I’m working on dashboards, workflow automation, and team collaboration — things like running scripts when data changes or sharing queries across a workspace.&lt;/p&gt;
    &lt;p&gt;The goal is to make DB Pro the most intuitive way to explore, query, and manage data — without the usual enterprise clutter. It’s still early, but it’s already feeling like the tool I always wanted to exist.&lt;/p&gt;
    &lt;p&gt;Would love to hear feedback, especially from people who spend a lot of time in database clients — what’s still missing or frustrating in the current landscape?&lt;/p&gt;
    &lt;p&gt;Lately, I've been hacking on improving its linear algebra support (as that's one of the key focuses I want - native matrix/vector types and easy math with them), which has also helped flush out a bunch of codegen bugs. When that gets tedious, I've also been working on general syntax ergonomics and fixing correctness bugs, with a view to self-hosting in the future.&lt;/p&gt;
    &lt;p&gt;Been reversing Sound Blaster Command so that I could control my external DAC/AMP without Windows. So far I can change the LED color and EQ presets, which was the main reason I wanted to do it in the first place. I am currently in the process of writing a GUI for it so that others can use it too (I only tested it with 1 soundcard, G6, though) for their older soundblaster cards that are not supported by Creative's multiplatform solutions. Will use Clay for it. Initially wanted to use Qt but I wrote the implementation in C and now I am too lazy to adapt it to cpp.&lt;/p&gt;
    &lt;p&gt;I am working on a platform to help user to enrich their data by AI. so that AI can understand their Data more, especially for ChatGPT. Also it's easy to host a data and publish a MCP for ChatGPT.&lt;/p&gt;
    &lt;p&gt;The challenge is how ChatGPT can understand your "query" or say "prompts"? Raw data is not good enough - so I try to use a term called "AI Understanding Score" to measure it: https://senify.ai/ai-understanding-score. I think this index will help user to build more context so that AI can know more and answer with correct result.&lt;/p&gt;
    &lt;p&gt;This is very early work without every detail considered, really would like to have your feedback and suggestions.&lt;/p&gt;
    &lt;p&gt;I’m building Skim: https://www.justskim.in/, A PWA that lets you read books as auto-swiping, short-form content on mobile. I use it to replace watching YouTube Shorts or Instagram with reading in the same form factor. It works offline and is entirely client-side.&lt;/p&gt;
    &lt;p&gt;This weekend I’m working on making the parsing more robust. The most common friction I’ve heard is that downloading books elsewhere and importing them into the app is distracting. I’m torn between expanding it to include a peer-to-peer book exchange or turning it into an RSS feed reader.&lt;/p&gt;
    &lt;p&gt;The glamourous world of data testing! A lightweight flexible data contracts library called Wimsey[0].&lt;/p&gt;
    &lt;p&gt;The main pitch is you have minimal dependencies and overheads and can run tests natively on pandas/polars/pyspark/dask/duckdb/etc (thanks to the awesome Narwhals project)&lt;/p&gt;
    &lt;p&gt;It's mostly there for v1 right now, but kean to add a tiny bit more functionality, and well a lot more docs. Working on something that's automated alongside the test suite, which should keep things reliable and fresh (I'll find out soon enough)&lt;/p&gt;
    &lt;p&gt;It's basically a reverse-proxy-as-a-service. I handle TLS termination and cert management, offer routing rules, rate limiting, WAF + DDOS protection, proxy + web analytics, redirects etc. All accessible via very simple API.&lt;/p&gt;
    &lt;p&gt;Underneath it's Caddy hosted on AWS for proxy fleets, and Heroku for Web + API fleets.&lt;/p&gt;
    &lt;p&gt;I'm currently working on two passions of mine. Both of them a one-man project.&lt;/p&gt;
    &lt;p&gt;The first is a DNS blocker called Quietnet - https://quietnet.app. Its born out of my interest in infrastructure and I wanted to build an opininated DNS blocker that helps mom and pops be safer on the Internet. At the end of the day its just the typical Pi-hole on the Cloud but with my personal interest in providing stronger privacy for our users while keeping their families safe.&lt;/p&gt;
    &lt;p&gt;The second, is a small newsletter aggregator tool called Newsletters.love - https://newsletters.love/.&lt;/p&gt;
    &lt;p&gt;I wanted to create a way for people to start curating their own list of newsletters and then sharing them with their friends and families. The service helps to generate a private email adddress that they can use to subscribe to newsletters and then start reading those newsletters whenever they want without it getting lost in their email inbox.&lt;/p&gt;
    &lt;p&gt;I'm working on Veila, a privacy‑first AI chat service. I wanted something that prevents model providers from profiling users and linking information from chats to their identity.&lt;/p&gt;
    &lt;p&gt;I'm a robotics engineer by training, this is my first public launch of a web app.&lt;/p&gt;
    &lt;p&gt;- What it is: - Anonymous AI chat via a privacy proxy (provider sees our server, not your IP or account info) - End‑to‑end encrypted history, keys derived from password and never leave your device - Pay‑as‑you‑go; switch models mid‑chat (OpenAI now; Claude, Gemini and others planned) - Practical UX: sort chats into folders, Markdown, copyable code blocks, mobile‑friendly - Notes/limits: - Not self‑hosted: prompts go to third‑party APIs - If you include identifying info, upstream sees it - Prompts take a bit long sometimes, because reasoning is set to "medium" for now. Plan to make this adjustable in the future. - Looking for feedback: - What do you need to trust this? Open source? Independent audit? - Gaps in the threat model I'm missing - Which UI features and AI models you'd want next - Any UX rough edges (esp. mobile) - Learn more: - Compare Veila to ChatGPT, Claude, Gemini, etc. (best viewed on desktop): https://veila.ai/docs/compare.html - Discord: https://discord.gg/RcrbZ25ytb - More background: https://veila.ai/about.html&lt;/p&gt;
    &lt;p&gt;Hmm. I can't say for others, but I can tell you what would work for me given that I might meet some the criteria of desired audience for this.&lt;/p&gt;
    &lt;p&gt;In this space, it is more about trust and what you have done in the past more than anything else. Audits and whatnot are nice, but I need to be able to trust that your decisions will be sound. Think how Steam's Gabe gained his reputation. Not exactly easy feat these days.&lt;/p&gt;
    &lt;p&gt;Thanks for sharing this! Fully agree that trust is key, normally being on the user side of privacy-focussed services myself. Open source can help build this trust, but it would be ideal to have a way to make what is actually running on and being served by the servers transparent.&lt;/p&gt;
    &lt;p&gt;I'd love to hear your feedback if you get around to test Veila, e.g. on hey@veila.ai.&lt;/p&gt;
    &lt;p&gt;We’re working on Fibre - secure file uploads for Intercom/Crisp, and uploads sent straight to your storage.&lt;/p&gt;
    &lt;p&gt;I noticed a gap - our customers are required to upload sensitive documents but often hesitate at the thought of uploading documents in the intercom/crisp interface, citing privacy concerns.&lt;/p&gt;
    &lt;p&gt;I thought, how difficult would it be to build an app that sends documents to your own Google drive - turns out it’s very easy. In a week, we built an app that renders an iframe in the intercom chat interface and sends documents straight to our google drive folder, bypassing intercom all together.&lt;/p&gt;
    &lt;p&gt;We’re now investigating uploading to s3 or azure blob storage and generating summaries of documents that are sent to the intercom conversation thread so ops teams can triage quicker.&lt;/p&gt;
    &lt;p&gt;• implemented adaptive quadrature with Newton–Cotes formulas&lt;/p&gt;
    &lt;p&gt;• wrote a tiny Markov-chain text generator&lt;/p&gt;
    &lt;p&gt;• prototyped an interactive pipeline system for non-normalized relational data in Lua by abusing operator overloading&lt;/p&gt;
    &lt;p&gt;• load-tested and taste-tested primary batteries at loads exceeding those in the datasheet; numerically simulated a programmable load circuit for automating the load testing&lt;/p&gt;
    &lt;p&gt;• measured the frequency of subroutine calls and leaf subroutine calls in several programs with Valgrind&lt;/p&gt;
    &lt;p&gt;• wrote a completely unhealthy quantity of commentary on HN&lt;/p&gt;
    &lt;p&gt;New ideas I'm thinking about include backward-compatible representations of soft newlines in plain ASCII text, multitouch calculators supporting programming by demonstration, virtual machines for perfectly reproducible computations, TCES energy storage for household applications beyond climate control such as cooking and laundry, canceling the harmonic poles of recursive comb filters with zeroes in the nonrecursive combs of a Hogenauer filter, differential planetary transmissions for compact extreme reductions similar to a cycloidal drive, rapid ECM punching in aluminum foil, air levigation of grog, ultra-cheap passive solar thermal collectors, etc. Happy to go into more detail if any of these sound interesting.&lt;/p&gt;
    &lt;p&gt;I'm putting a bunch of security tools / data feeds together as a service. The goal is to help teams and individuals run scans/analysis/security project management for "freemium" (certain number of scans/projects for free each month, haven't locked in on how it'll pan out fully $$ wise).&lt;/p&gt;
    &lt;p&gt;I want to help lower the technical hurdles to running and maintaining security tools for teams and individuals. There are a ton of great open source tools out there, most people either don't know or don't have the time to do a technical deep dive into each. So I'm adding utilities and tools by the day to the platform.&lt;/p&gt;
    &lt;p&gt;Likewise, there's a built in expert platform for you to get help on your security problems built into the system. (Currently an expert team consisting of [me]). Longer term, I'm working on some AI plugins to help alert on CVEs custom to you, generate automated scans, and some other fun stuff.&lt;/p&gt;
    &lt;p&gt;If you zoom out it's meant to look something like a thermal vent with cellular life. Rank and karma cause the cells to bio-illuminate. Each cell is a submission, each organelle is a comment thread, and every shape represents a live connection to the Firebase HN API. It also has features to search, filter, and go back in time as far as the backend has been running.&lt;/p&gt;
    &lt;p&gt;It's been a passion project of mine. My little Temple OS. And I'll keep adding little features that please me.&lt;/p&gt;
    &lt;p&gt;I’ve noticed that a lot of work is duplicated across projects that use the same libraries or SDKs (e.g., Stripe). Developers write a lot of glue code to shuffle data between the Stripe API and the app’s frontend or admin dashboard, as well as to handle incoming webhooks and persist data to the app’s database.&lt;/p&gt;
    &lt;p&gt;That’s why I’ve been building 'Fragno', a framework for creating full-stack libraries. It allows library authors to define backend routes and provides reactive primitives for building frontend logic around those routes. All of this integrates seamlessly into the user’s application.&lt;/p&gt;
    &lt;p&gt;With this approach, providers like Stripe can greatly improve the developer experience and integration speed for their users.&lt;/p&gt;
    &lt;p&gt;I’ve got a side project going that’s a browser extension (starting with Safari + Sign in with Apple) intended to add a comment layer to the internet as a whole. I’m calling it Chaffiti (https://chaffiti.com).&lt;/p&gt;
    &lt;p&gt;The idea is to enable a comment section on any webpage, right as you’re browsing. Viewing a Zillow listing? See what people are excited about with the property. Wonder what people think about a tourist attraction? It’ll be right there. Want to leave your referral or promo code on a checkout page for others? Post it.&lt;/p&gt;
    &lt;p&gt;Not sure what the business model will look like just yet. Just the kind of thing I wish existed compared to needing to venture out to a third party (traditional social media / forums etc) to see others’ thoughts on something I’m viewing online. I welcome any feedback!&lt;/p&gt;
    &lt;p&gt;Great idea but wouldn’t you run into storage issues pretty quickly without massive budget for large database clusters? The web is a big and constantly changing place. Covering it in useful comments seems prohibitively expensive.&lt;/p&gt;
    &lt;p&gt;I am working on Tailstream (https://tailstream.io/), turning logs into task time visual data streams. Built the web application, web site and a Go CLI agent (open source) and am now slightly pivoting into making it more log-focused.&lt;/p&gt;
    &lt;p&gt;Working on faceted search for logs and CLI client now and trying to share my progress on X.&lt;/p&gt;
    &lt;p&gt;An experimental mesh network protocol, that is still very much pre alpha and missing some features.&lt;/p&gt;
    &lt;p&gt;The big thing I wanted to try is automatic global routing via MQTT.&lt;/p&gt;
    &lt;p&gt;Everything is globally routable. You can roam around between gateway nodes, as long as all the gateways are on the same MQTT server.&lt;/p&gt;
    &lt;p&gt;And there's a JavaScript implementation that connects directly to MQTT. So you can make a sensor, go to the web app, type the sensor's channel key, and see the data, without needing to create any accounts or activate or provision anything.&lt;/p&gt;
    &lt;p&gt;Adding new transports and documentation to my Typescript logging library (MIT licensed), LogLayer (https://loglayer.dev). Just added documentation for Bun and Deno support added some new logging library transports (LogTape), and finishing up Logflare and Betterstack transports so you can send logs to their logging APIs.&lt;/p&gt;
    &lt;p&gt;Thanks, what do you mean with "any-script-managed HTML"? If you mean that you can use any script, like e.g. a bash script, to generate static HTML files, then yes, in a way Mastro is basically that script. Except that it comes with a server as well – both for local development and production, should a static site no longer suffice.&lt;/p&gt;
    &lt;p&gt;I'm currently chipping away at DSC, a tensor library I wrote from scratch to play with large language models. Last week I re-wrote flash attention from scratch in CUDA and was able to get good perf.&lt;/p&gt;
    &lt;p&gt;For fun, playing with Meshtastic https://meshtastic.org/ and contributing to the open source firmware and apps. They have something cool but need lots of help. I've patched 3 memory leaks and had a few other PRs merged already.&lt;/p&gt;
    &lt;p&gt;For work, https://heyoncall.com/ as the best tool for on-call alerting, website monitoring, cron job monitoring, especially for small teams and solo founders.&lt;/p&gt;
    &lt;p&gt;I guess they both fall under the category of "how do you build reliable systems out of unreliable distributed components" :)&lt;/p&gt;
    &lt;p&gt;Spent last week at a Java conference, and it made me realise that I haven't made many open source contributions of late. So I'm currently going through the issue trackers of the projects I rely on the most to see where I can pitch in.&lt;/p&gt;
    &lt;p&gt;I already mentioned last month I was working on a appendix in Wiktionary for synonym of Esperanto terms constructed with the mal- prefix[1]. Actually it's a bit more generic as it also encompasses a few other antonymic prefixes, but mal- is the main tool in that category.&lt;/p&gt;
    &lt;p&gt;With more than 300 references and around 1500 entries, covering more than all the lemma given in the reference dictionary Plena Ilustrita Vortaro de Esperanto, I now consider it achieved. Well, apart some formatting of references where I still need to fix issues related to import of template/modules from an other wiki. :D&lt;/p&gt;
    &lt;p&gt;To give a perspective, in one of the Esperanto sentence collection referenced in the appendix, I found a bit more than 7000 terms mal- words, which once stripped of the most common inflection and affixes went down to 3000 entries. I didn't check in details this remaining set, but my guess is that the remaining difference was still mostly due to less frequent affix combinations that my naive filter didn't catch. For recall Esperanto is a highly agglutinative language and encourage the use of a regular affix set to express many derivative terms from a common stem, so empowering expressivity though combinatorial reuse. So only twice the size of the proposed entries in the appendix is a very low figure.&lt;/p&gt;
    &lt;p&gt;I initially had this project ideas years ago, and it came back to my mind as I started to contribute to the port of Raku into esperanto[3]. This came back as we were going through the considerations for the lsb routine, where LSB stands for Least Significant Bit. The common way to express least is malplej (countryman-of-most), which is generally ok but can be instead replaced by mej, for example if terseness is a highly weighted desired trait. That allows for example to use mejpezbit’ instead of some alternative synonym like malplej signifa duumaĵo.&lt;/p&gt;
    &lt;p&gt;I'm working on a DnD character sheet app! I spent last week implementing the core DnD SRD ruleset, but what I'm really excited about is ML integration. I want to add a self-hosted fine-tuned ML model that acts as a character and DM assistant. Obviously an LLM via API can do the job, but I'm really curious if it's possible to build smaller, cheaper, task-specific models. Plus, I've never integrated an ML model into a product before, and I'm curious to play with it. I'm thinking of it like clippy for DnD: "it looks like you're trying to cast fireball?"&lt;/p&gt;
    &lt;p&gt;Besides the LLM experimentation, this project has allowed me to dive into interesting new tech stacks. I'm working in Hono on Bun, writing server-side components in JSX and then updating the UI via htmx. I'm really happy with how it's coming together so far!&lt;/p&gt;
    &lt;p&gt;- What: Sun Grid Engine–style scheduler + Docker on System-on-Module (SoM) boards for reproducible tests/benchmarks and interactive SSH sessions (remote dev). - Who: Robotics/embedded engineers comparing SoMs and tuning models/pipelines on target platforms. - Why: Reproducible runs, easy board access, comparable reports.&lt;/p&gt;
    &lt;p&gt;Pulled this side project off the shelf — something I started after covid, when I was working at one of the consumer robotics companies (used to be the largest back then). Got it mostly working, but never actually released. I tend to dust it off and push it along a bit whenever I’m between jobs. Like now... Feels good to be back at it.&lt;/p&gt;
    &lt;p&gt;I am working on Lunch Flow (https://lunchflow.app), a tool that allows people to automatically sync their bank accounts to their favorite budgeting apps (Google Sheets, Lunch Money, Actual Budget, or use our API!)&lt;/p&gt;
    &lt;p&gt;I was motivated to build this as I found that many great personal finance and budget apps didn't offer integrations with the banks I used, which is understandable given the complexity and costs involved, so I wanted to tackle this problem and help build the missing open banking layer for personal finance apps, with very low costs (a few dollars a month) and a very simple api, or built-in integrations.&lt;/p&gt;
    &lt;p&gt;Still working on making this sustainable, but been quite a learning experience so far, and quite excited to see it already making a difference for so many people :)&lt;/p&gt;
    &lt;p&gt;- Getting into RTL SDR, ordered a dongle, should be fun, want to build a grid people can plug into&lt;/p&gt;
    &lt;p&gt;- Bringing live transcripts, search and AI to wisprnote&lt;/p&gt;
    &lt;p&gt;- Moving BrowserBox to a binary release distribution channel for IP enforcement and ease of installation. Public repo will no longer be updated except for docs/version/base install script, and all dev happens in internal with binaries released to https://github.com/BrowserBox/BrowserBox. Too many "companies" (even "legit", large ones) abusing ancient forks and stealing our commercial updates without license, or violating previous permissive's conditions like AGPL source provision. Business lesson is even commercial licensed source-available eats into sales pipeline due to violators who could pay but assume false impunity and steal "freebies" "because they can." No perfect protection, but from now enforcement will ramp up, and source access is only for minimum ACV customers as add-on. So many enhancements coming down the pipe so it's gonna be many improved versions from here&lt;/p&gt;
    &lt;p&gt;- Creating an improved keyboard for iOS swipe typing, I don't like the settings or word choices in ambiguity and think it can be better&lt;/p&gt;
    &lt;p&gt;It’s an instagram style UI but for scrolling through record releases and snippets, worked on making it responsive as possible with low latency audio playback so you can browse a lot of stuff quickly.&lt;/p&gt;
    &lt;p&gt;I have been working on a one week side-project that ended up taking over a year… Working on it periodically with friends to add new features and patch bugs, at the moment I'm trying to expand the file sharing capabilities. It's been a journey and I have learnt quite a lot.&lt;/p&gt;
    &lt;p&gt;The aim of this is to be a simple platform to share content with others. Appreciate any feedback, this is my first time building a user facing platform. If the free tier is limiting, I've made a coupon "HELLOWORLD" if you want to stress test or try the bigger plans, it gives you 100% off for 3 months.&lt;/p&gt;
    &lt;p&gt;It’s designed to plug into frameworks like CrewAI, AutoGen, or LangChain and help agents learn from both successful and failed interactions - so instead of each execution being isolated, the system builds up knowledge about what actually works in specific scenarios and applies that as contextual guidance next time. The aim is to move beyond static prompts and manual tweaks by letting agents improve continuously from their own runs.&lt;/p&gt;
    &lt;p&gt;Currently also working on an MCP interface to it, so people can easily try it in e.g. Cursor.&lt;/p&gt;
    &lt;p&gt;I'm working on a tool[0] to address how hard it is for non-technical people to understand the text-based code from vibe coding tools.&lt;/p&gt;
    &lt;p&gt;Our approach is to make the complexity more readable by using three simple block types to represent logic, data, and UI, which are connected by cables – a bit like wiring up components on an electronics breadboard –.&lt;/p&gt;
    &lt;p&gt;Instead of spitting out a wall of code, the AI generates these visual blocks and makes the right connections between them. The ultimate goal is to make the output from LLM more accessible and actionable for everyone, not just developers.&lt;/p&gt;
    &lt;p&gt;Metacognitive AI system. The focus here is on the various internal systems versus the LLM itself. Basically giving an AI agent the ability to do all the things it cant do from a one step turn interaction that you usually see from just a chat bot. It is comprised of many specialized LLM's that have all their own roles and specialties. They will have the ability to cross talk with each other internally share post processed information make analysis and all that jazz, but not to do tasks but to reason in a similar manner to how a human reasons. Think of it as many thought traces that are giving advice to the main "orchestrator" human front facing agent and have him consolidate all the relevant information before interacting with the human. At first I am introducing basic subagent systems like logical fallacy and leading questions subagent (watches to notice if the human is making assumptions without evidence), paranoia subagent (watched for intentional or unintentional human lying and fact checker), and many other sub systems. But I also have plans for introducing a "pain" management subagent which will take notice of errors in tool calling or some sort of failures and bring that to the front of the attention of the orchestrator based on a threshold criteria. Also it will have a memory system that if working correctly should allow it to reduce the amount of mistakes it makes on something that it had already made a mistake of before. Anyways there is a lot more to it this is just scratching the surface but basically its my attempt to create the human brain communication system virtually with llm systems and many scripts and grounding metadata and a bunch of other goodies. The cherry on top will be once I am done making a text based translation layer for the system that will allow the agent to modify its own internal structure as it needs for any specific task.&lt;/p&gt;
    &lt;p&gt;I am working on a infinite canvas for AI image/video/audio/3D generation. in other tools, its easy to create one off AI images/videos, but to create a cohesive story with consistent characters and locations is very difficult. https://www.flickspeed.ai/&lt;/p&gt;
    &lt;p&gt;I’m currently building YTVidHub—a tool that focuses on solving a very specific, repetitive workflow pain for researchers and content analysts.&lt;/p&gt;
    &lt;p&gt;The Pain Point: If you are analyzing a large YouTube channel (e.g., for language study, competitive analysis, or data modeling), you often need the subtitle files for 50, 100, or more videos. The current process is agonizing: copy-paste URL, click, download, repeat dozens of times. It's a massive time sink.&lt;/p&gt;
    &lt;p&gt;My Solution: YTVidHub is designed around bulk processing. The core feature is a clean interface where you can paste dozens of YouTube URLs at once, and the system intelligently extracts all available subtitles (including auto-generated ones) and packages them into a single, organized ZIP file for one-click download.&lt;/p&gt;
    &lt;p&gt;Target Users: Academic researchers needing data sets, content creators doing competitive keyword analysis, and language learners building large vocabulary corpora.&lt;/p&gt;
    &lt;p&gt;The architecture challenge right now is optimizing the backend queuing system for high-volume, concurrent requests to ensure we can handle large batches quickly and reliably without hitting rate limits.&lt;/p&gt;
    &lt;p&gt;It's still pre-launch, but I'd love any feedback on this specific problem space. Is this a pain point you've encountered? What's your current workaround?&lt;/p&gt;
    &lt;p&gt;How coincidental - I needed exactly this just a couple days ago. I ended up vibecoding a script to feed an individual URL into yt-dlp then pipe the downloaded audio through Whisper - not quite the same thing as it's not downloading the _actual_ subtitles but rather generating its own transcription, but similar. I've only run it on a single video to test, but it seemed to work satisfactorily.&lt;/p&gt;
    &lt;p&gt;I haven't upgraded to bulk processing yet, but I imagine I'd look for some API to get "all URLs for a channel" and then process them in parallel.&lt;/p&gt;
    &lt;p&gt;That is some fantastic validation, thank you! It’s cool to hear you already vibecoded a solution for this.&lt;/p&gt;
    &lt;p&gt;You've basically hit on the two main challenges:&lt;/p&gt;
    &lt;p&gt;Transcription Quality vs. Official Subtitles: The Whisper approach is brilliant for videos without captions, but the downside is potential errors, especially with specialized terminology. YTVidHub's core differentiator is leveraging the official (manual or auto-generated) captions provided by YouTube. When accuracy is crucial (like for research), getting that clean, time-synced file is essential.&lt;/p&gt;
    &lt;p&gt;The Bulk Challenge (Channel/Playlist Harvesting): You're spot on. We were just discussing that getting a full list of URLs for a channel is the biggest hurdle against API limits.&lt;/p&gt;
    &lt;p&gt;You actually mentioned the perfect workaround! We tap into that exact yt-dlp capability—passing the channel or playlist link to internally get all the video IDs. That's the most reliable way to create a large batch request. We then take that list of IDs and feed them into our own optimized, parallel extraction system to pull the subtitles only.&lt;/p&gt;
    &lt;p&gt;It's tricky to keep that pipeline stable against YouTube’s front-end changes, but using that list/channel parsing capability is definitely the right architectural starting point for handling bulk requests gracefully.&lt;/p&gt;
    &lt;p&gt;Quick question for you: For your analysis, is the SRT timestamp structure important (e.g., for aligning data), or would a plain TXT file suffice? We're optimizing the output options now and your use case is highly relevant.&lt;/p&gt;
    &lt;p&gt;Good luck with your script development! Let me know if you run into any other interesting architectural issues.&lt;/p&gt;
    &lt;p&gt;I've built something similar before for my own use cases and one thing I'd push back on are official subtitles. Basically no video I care about has ever had "official" subtitles and the auto generated subtitles are significantly worse than what you get by piping content through an LLM. I used Gemini because it was the cheapest option and still did very well.&lt;/p&gt;
    &lt;p&gt;The biggest challenge with this approach is that you probably need to pass extra context to LLMs depending on the content. If you are researching a niche topic, there will be lots of mistakes if the audio isn't if high quality because that knowledge isn't in the LLM weights.&lt;/p&gt;
    &lt;p&gt;Another challenge is that I often wanted to extract content from live streams, but they are very long with lots of pauses, so I needed to do some cutting and processing on the audio clips.&lt;/p&gt;
    &lt;p&gt;In the app I built I would feed an RSS feed of video subscriptions in, and at the other end a fully built website with summaries, analysis, and transcriptions comes out that is automatically updated based on the youtube subscription rss feed.&lt;/p&gt;
    &lt;p&gt;This is amazing feedback, thanks for sharing your deep experience with this problem space. You've clearly pushed past the 'download' step into true content analysis.&lt;/p&gt;
    &lt;p&gt;You've raised two absolutely critical architectural points that we're wrestling with:&lt;/p&gt;
    &lt;p&gt;Official Subtitles vs. LLM Transcription: You are 100% correct about auto-generated subs being junk. We view official subtitles as the "trusted baseline" when available (especially for major educational channels), but your experience with Gemini confirms that an optimized LLM-based transcription module is non-negotiable for niche, high-value content. We're planning to introduce an optional, higher-accuracy LLM-powered transcription feature to handle those cases where the official subs don't exist, specifically addressing the need to inject custom context (e.g., topic keywords) to improve accuracy on technical jargon.&lt;/p&gt;
    &lt;p&gt;The Automation Pipeline (RSS/RAG): This is the future. Your RSS-to-Website pipeline is exactly what turns a utility into a Research Engine. We want YTVidHub to be the first mile of that process. The challenges you mentioned—pre-processing long live stream audio—is exactly why our parallel processing architecture needs to be robust enough to handle the audio extraction and cleaning before the LLM call.&lt;/p&gt;
    &lt;p&gt;I'd be genuinely interested in learning more about your approach to pre-processing the live stream audio to remove pauses and dead air—that’s a huge performance bottleneck we’re trying to optimize. Any high-level insights you can share would be highly appreciated!&lt;/p&gt;
    &lt;p&gt;For the long videos I just relied in ffmpeg to remove silence. It has lots of options for it, but you may need to fiddle with the parameters to make it work. I ended up with something like:&lt;/p&gt;
    &lt;p&gt;I did consider building a tool like this before I pivot to something else. I'm learning materials in Chinese Mandarin language from a YouTube playlist. NotebookLLM doesn't support Chinese language yet so you must make sure your app supports Chinese Mandarin so I can use it. :)&lt;/p&gt;
    &lt;p&gt;A way to find specific materials would be nice. Think of converting the whole playlist into something like RAG then you can search anything from this playlist.&lt;/p&gt;
    &lt;p&gt;Wow, thanks for this validation! Hearing from someone who almost built the solution themselves confirms we’re on the right track.&lt;/p&gt;
    &lt;p&gt;You hit the nail on the head regarding language support.&lt;/p&gt;
    &lt;p&gt;Mandarin/Multilingual Support: Absolutely, supporting a wide range of languages—especially Mandarin—is a top priority. Since we focus on extracting the official subtitles provided by YouTube, the language support is inherently tied to what the YouTube platform offers. We just need to ensure our system correctly parses and handles those specific Unicode character sets on the backend. We'll make sure CJK (Chinese, Japanese, Korean) languages are handled cleanly from Day 1.&lt;/p&gt;
    &lt;p&gt;The RAG/Semantic Search Idea: That is an excellent feature suggestion and exactly where I see the tool evolving! Instead of just giving the user a zip file of raw data, the true value is transforming that data into a searchable corpus. The idea of using RAG to search across an entire playlist/channel transcript is something we're actively exploring as a roadmap feature, turning the tool from a downloader into a Research Assistant.&lt;/p&gt;
    &lt;p&gt;Thanks for the use case and the specific requirements! It helps us prioritize the architecture.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Since we focus on extracting the official subtitles provided by YouTube, the language support is inherently tied to what the YouTube platform offers.&lt;/p&gt;
    &lt;p&gt;You can use video understanding from Gemini LLM models to extract subtitles even the video doesn't have official subtitles. That's expensive for sure. But you should provide this option to willing users. I think.&lt;/p&gt;
    &lt;p&gt;That is a fantastic point, and you've perfectly articulated the core trade-off we're facing: Accuracy vs. Cost.&lt;/p&gt;
    &lt;p&gt;You are 100% right. For the serious user (researcher, data analyst, etc.) the lack of an official subtitle is a non-starter. Relying solely on official captions severely limits the available corpus.&lt;/p&gt;
    &lt;p&gt;The suggestion to use powerful models like Gemini for high-accuracy, custom transcription is excellent, but as you noted, the costs can spiral quickly, especially with bulk processing of long videos.&lt;/p&gt;
    &lt;p&gt;Here is where we are leaning for the business model:&lt;/p&gt;
    &lt;p&gt;We are committed to keeping the Bulk Download of all YouTube-provided subtitles free, but we must implement a fair-use limit on the number of requests per user to manage the substantial bandwidth and processing costs.&lt;/p&gt;
    &lt;p&gt;We plan to introduce a "Pro Transcription" tier for those high-value, high-volume use cases. This premium tier would cover:&lt;/p&gt;
    &lt;p&gt;Unlimited/High-Volume Bulk Requests.&lt;/p&gt;
    &lt;p&gt;LLM-Powered Transcription: Access to the high-accuracy models (like the ones you mentioned) with custom context injection, bypassing the "no official subs" problem entirely—and covering the heavy processing costs.&lt;/p&gt;
    &lt;p&gt;We are currently doing market research on fair pricing for the Pro tier. Your input helps us frame the value proposition immesnely. Thank you for pushing us on this critical commercial decision!&lt;/p&gt;
    &lt;p&gt;Working on my SaaS that monitors third-party status pages - https://incidenthub.cloud/ It's a one-person project I started last year.&lt;/p&gt;
    &lt;p&gt;My biggest technical challenge remains dealing with the immense number of different APIs (and not-APIs) in the different status pages out there. Marketing remains my biggest overall challenge as my background is engineering, but I've learnt quite a bit since I launched this.&lt;/p&gt;
    &lt;p&gt;Thank you. That means a lot. I hope to fully finish it by the end of the month as it's still riddled with small bugs. But feel free to fork it: https://github.com/danielterwiel/terwiel.io&lt;/p&gt;
    &lt;p&gt;Should be as easy as updating all data in the data/ folder and you can get your own version. Mind you: getting the SVG logos right is the hard part&lt;/p&gt;
    &lt;p&gt;I’ve been working on AirSend — we help workers to get paid in fiat currency but spend in stablecoins. Clients can pay invoices in fiat (like USD or EUR), and it’s automatically converted into USDC inside your wallet (0.5% platform fees).&lt;/p&gt;
    &lt;p&gt;From there, users can either send funds to another wallet or spend directly using a pre-funded debit card. It’s still early, but we’re testing with a small group of users who want to receive payments faster and avoid PayPal or wire fees.&lt;/p&gt;
    &lt;p&gt;If you’re a freelancer or digital nomads interested in trying it out, you can check it out here: https://useairsend.com&lt;/p&gt;
    &lt;p&gt;It's quite an interesting process to vibe code game stuff where I have a vague concept of how to achieve things but no experience/muscle memory with three.js &amp;amp; friends.&lt;/p&gt;
    &lt;p&gt;A unified platform for product teams to announce updates, maintain a changelog, share roadmaps, provide help documentation and collect feedback with the help of AI.&lt;/p&gt;
    &lt;p&gt;My goal is to help product teams tell users about new features (so they actually use them), gather meaningful feedback (so they build the right things), share plans (so users know what's coming), and provide help (so users don't get stuck).&lt;/p&gt;
    &lt;p&gt;Doing it as an indie hacker + solo founder + lean. Started 13 days ago. Posting about my journey on Youtube every week day https://www.youtube.com/@dave_cheong&lt;/p&gt;
    &lt;p&gt;I have been building a mostly free website and API to interact with sec EDGAR filings, get realtime new filing alerts (and preview those alerts), and see the historical impact of financial filings.&lt;/p&gt;
    &lt;p&gt;Right now I am working on adding historical tables extracted from filings, as well as historical financials and their calculations.&lt;/p&gt;
    &lt;p&gt;I'm working on https://www.fontofweb.com because design inspiration platforms don’t give enough real material to work with.&lt;/p&gt;
    &lt;p&gt;Most sites fall into extremes: Dribbble leans toward polished mockups that never shipped, while Awwwards and Mobbin go heavy on curation. The problem isn’t just what they pick — it’s that you only ever see a narrow slice. High curation means low volume, slow updates, and a bias toward showcase projects instead of the everyday, functional interfaces most of us actually design.&lt;/p&gt;
    &lt;p&gt;Font of Web takes a different approach. It’s closer to Pinterest, but purely for web design. Every “pin” comes with metadata: fonts, colors, and the exact domain it came from, so you can search, filter, and sort in ways you can’t elsewhere. The text search is powered by multimodal embeddings, so you can use search queries like “minimalist pricing page with illustrations at the side” and get live matches from real websites.&lt;/p&gt;
    &lt;p&gt;What you can do:&lt;/p&gt;
    &lt;p&gt;natural language search (e.g. “elegant serif blog with sage green”)&lt;/p&gt;
    &lt;p&gt;I'm working on a compiler for WebAssembly. The idea is you use the raw wasm instructions like you’d use JSX in React, so you can make reusable components and compose them into higher abstractions. Inlining is just a function call.&lt;/p&gt;
    &lt;p&gt;It’s implemented in Elixir and uses its powerful macro system. This is paired with a philosophy of static &amp;amp; bump allocation, so I’m trying to find a happy medium of simplicity with a powerful-enough paradigm yet generate simple, compact code.&lt;/p&gt;
    &lt;p&gt;I've been working on an engine that will allow me to play the old DOS game "Eye of the Beholder" with the original assets. It's mostly an exercise for me to up my golang skills and to explore what coding was like in the early 90s.&lt;/p&gt;
    &lt;p&gt;It is hard to show that AI can reimplement for example special relativity - because we don't even have enough text from 19th century to train an LLM on it - so we need a new idea something that was invented after an LLM was trained. I took the Gwern's essay and checked with deep search and deep research which ideas from that essay are truly novel and apparently there are some so reinventing them seemed like a good target: https://github.com/zby/DayDreamingDayDreaming/blob/main/repo...https://github.com/zby/DayDreamingDayDreaming/blob/main/repo...&lt;/p&gt;
    &lt;p&gt;So here it is - a system that can reliably churn essays on daydreaming AIs. On one level it is kind of silly - we already knew that infinite monkeys could write Shakespeare works. The generator was always theoretically possible, the hard part is the verifier. But still - the search space in my system is much smaller than the search space of all possible letter sequences - so at least I can show that the system is a little more practical.&lt;/p&gt;
    &lt;p&gt;You can modify it to reinvent any other new idea - you just need to provide it the inspirations and evals for checking the generated essays.&lt;/p&gt;
    &lt;p&gt;I am thinking about next steps - maybe I could do it a little bit more universal - but it seems that to build something that would work as needed would require scale.&lt;/p&gt;
    &lt;p&gt;I kind of like the software framework I vibe coded for this. It lets you easily build uniform samples where you can legitimately do all kinds of comparisons. But I am not so sure about using Dagster as the base for the system.&lt;/p&gt;
    &lt;p&gt;I'm trying to figure out how modern internal API management should work like and started https://www.appear.sh/.&lt;/p&gt;
    &lt;p&gt;After spending so much of my career dealing with APIs and building tooling for that I feel there's huge gap between what is needed and possible vs how the space generally works. There's a plethora of great tools that do one job really well, but when you want to use them the integration will kill you. When you want to get your existing system in them it takes forever. When you want to connect those tools that takes even longer.&lt;/p&gt;
    &lt;p&gt;The reality I'm seeing around myself and hearing from people we talk to is that most companies have many services in various stages of decay. Some brand new and healthy, some very old, written by people who left, acquired from different companies or in languages that were abandoned. And all of that software is still generating a lot of value for the company and to be able to leverage that value APIs are essential. But they are incredibly hard and slow to use, and the existing tools don't make it easier.&lt;/p&gt;
    &lt;p&gt;I slowly work on the public release of Submerge VCS, and SQLite driver for my factory simulation game (in Odin) with online trading system (in Erlang).&lt;/p&gt;
    &lt;p&gt;- Message inspection from any topic — trace and analyze messages, view flow, lag, and delivery status&lt;/p&gt;
    &lt;p&gt;- Anomaly detection &amp;amp; forecasting — predict lag spikes, throughput drops, and other unusual behaviors&lt;/p&gt;
    &lt;p&gt;- Real-time dashboards for brokers, topics, partitions, and consumer groups&lt;/p&gt;
    &lt;p&gt;- Track config changes across clusters and understand their impact on performance&lt;/p&gt;
    &lt;p&gt;- Interactive log search with filtering by topic, partition, host, and message fields&lt;/p&gt;
    &lt;p&gt;- Build custom dashboards &amp;amp; widgets to visualize metrics that matter to your team&lt;/p&gt;
    &lt;p&gt;What pain points do you face in monitoring Kafka, which features would you like next, and any improvements to dashboards, log search, or message inspection?&lt;/p&gt;
    &lt;p&gt;i got a side project mirubato https://mirubato.com/, a web app tracking instrument practice logs. there were wild ideas such as enable AI training, grading, managing score, practice plans, and such, but in the end, i removed most of features. not only because it takes more time (i am only using a part of my free time to work on this) and effort, talent, plannings, but also because during vibe (yes, most of coding done by claude code) i realized that it still requires ultra deep thinking to design the minimal minimal UI i would like.&lt;/p&gt;
    &lt;p&gt;now the foundation is done, i've learnt a lot. i'm actually eating dog food by using it to track my own classical guitar practice everyday. i am pausing a while to process the requirements by ultra deep thinking to understand what would be helpful and how to shape the product.&lt;/p&gt;
    &lt;p&gt;LLMs such as codex and claude code definitely helped a lot, but I guess human beings' opinions would be more helpful - after all, the tool is made for humans instead of being used by claude code.&lt;/p&gt;
    &lt;p&gt;I would also like to hear when you start a project, if you know your audience are not super close to AI, would you still consider to enable the AI feature for them?&lt;/p&gt;
    &lt;p&gt;Porting a game to SteamDeck my friends did. The game is python based with a bespoke OpenGL game engine. Got a native linux build out a week ago. Currently working on controller support.&lt;/p&gt;
    &lt;p&gt;Started working on digital nomad event and workation aggregator two months ago. https://reorient.guide/&lt;/p&gt;
    &lt;p&gt;That main usecase is done. I’m now focusing on travel guides for remote workers. Goal is to help those new to a country to become as productive as they would be at home within 2-3 hours upon landing at the airport. I completed 80% of a guide to South Korea.&lt;/p&gt;
    &lt;p&gt;I started working on these guides after my friends in Tokyo commented during our last co-working session on how fast I got to our favourite spot (Tokyo Innovation Base) from Narita Airport; they thought I was already in-town.&lt;/p&gt;
    &lt;p&gt;A monster trainer game where you can _actually teach new, creative moves_ to your monsters: https://youtu.be/ThOCM9TK_yo&lt;/p&gt;
    &lt;p&gt;Basically, think of it as "Pokemon the anime, but for real". We allow you to use your voice to talk to, command, and train your monster. You and your monster are in this sandbox-y, dynamic environment where your actions have side effects.&lt;/p&gt;
    &lt;p&gt;You can train to fight or just to mess around.&lt;/p&gt;
    &lt;p&gt;Behind the scenes, we are converting player's voice into code in real time to give life to these monsters.&lt;/p&gt;
    &lt;p&gt;Hi HN, I am working on Circuitscript, a language based on python to describe electronic schematics: https://circuitscript.net/. A basic IDE (called the Bench) to try Circuitscript is available online: https://bench.circuitscript.net/&lt;/p&gt;
    &lt;p&gt;Since the last month, I have created a complete schematic with Circuitscript, exported the netlist to pcbnew and designed the PCB. The boards have been produced and currently waiting for them to be delivered to verify that it works. Quite excited since this will be the first design ever produced with Circuitscript as the schematic capture tool!&lt;/p&gt;
    &lt;p&gt;The motivation for creating Circuitscript is to describe schematics in terms of code rather than graphical UIs after using different CAD packages extensively (Allegro, Altium, KiCAD) in the past. I wanted to spend more time thinking about the schematic design itself rather than fiddling around with GUIs.&lt;/p&gt;
    &lt;p&gt;The main language goals are to be easy to write and reason, generated graphical schematics should be displayed according to how the designer wishes so (because this is also part of the design process) and to encourage code reuse.&lt;/p&gt;
    &lt;p&gt;Please check it out and I look forward to your feedback, especially from electronics designers/hobbyists. Thanks!&lt;/p&gt;
    &lt;p&gt;TPS takt scheduling and execution system. It is a system to support any kind of production or logistics process in Toyota Production System way of working.&lt;/p&gt;
    &lt;p&gt;You define resources needed for activity, time per activity, dependencies between activities to complete a process.&lt;/p&gt;
    &lt;p&gt;After you input the process you want to complete, you get a schedule similar to a gantt chart.&lt;/p&gt;
    &lt;p&gt;System displays which activities should be ongoing at any moment, you click gui or call API to complete the activities.&lt;/p&gt;
    &lt;p&gt;After process is complete you get a report of delays and deviations by Takts, activities and resources.&lt;/p&gt;
    &lt;p&gt;Based on that report you can decide what improvements to make to your process.&lt;/p&gt;
    &lt;p&gt;https://finbodhi.com — It helps you track, understand, and plan your personal finances — with a double-entry accounting. You own your data. It’s local-first, syncs across devices, and everything’s encrypted in transit. Supports multi-currency.&lt;/p&gt;
    &lt;p&gt;We are in it for long term. Not a startup, not looking for investment. Just plain paid product (free while in beta) by a few people. We have a few active users, and are looking for more before we remove the beta label :) It's a PWA app. Currently targeted for desktops. For personal software, I think local-first makes a lot of sense.&lt;/p&gt;
    &lt;p&gt;Besides a master's degree and an internship at a nutrition AI app startup, I'm taking another pass at procedural dungeon generation for my world-building website.&lt;/p&gt;
    &lt;p&gt;Building a new version of my distraction free writing app, poe-writer. https://getpoe.com&lt;/p&gt;
    &lt;p&gt;New version is a rebuild in react with cleaner interface, localisation, a bunch of new features and lays the groundwork to allow full html docs instead of only markdown&lt;/p&gt;
    &lt;p&gt;Between contributing to FrankenPHP (FrankenPHP.dev), I’ve been working on AtlasDb (https://github.com/bottledcode/atlas-db), its a distributed edge database — or it will be when I’m finished. There are a few unique things that make it more robust than etcd, and more scalable. Right now, it basically works except under certain types of contention, which I’ve been trying to solve for a couple of days now.&lt;/p&gt;
    &lt;p&gt;Working on a project: https://hpyhn.xyz. It's a website for analyzing posts and comments on HN. The idea started as a way to help me learn from discussions and filter out posts I don't interest in.&lt;/p&gt;
    &lt;p&gt;We are in the early stage of building the platformed version of our current WordPress plugin https://www.pathmetrics.io&lt;/p&gt;
    &lt;p&gt;It's a full funnel marketing attribution &amp;amp; insights tool with the intent of making marketing &amp;amp; marketing spends more transparent. We started from creating an utm tracking tool for our agency clients and currently it's a product on its own. We'll make it a platform to remove some of the limits that we have with WordPress and reach a larger audience.&lt;/p&gt;
    &lt;p&gt;My personal website/webring. It's mostly a collection of ideas I've been mulling over and holding off on due to not being able to iterate on them fast enough. Nowadays thanks to AI, a lot of these a short errands so it's been a fun few weeks. I've also started chucking a few previous side projects under more unified domains. [1][2]&lt;/p&gt;
    &lt;p&gt;Also working on a youtube channel [3] for my climbing/travel videos, but the dreary state of that website has me wondering whether it's worth it, tbh. I haven't been able to change my channel name after trying for weeks. It's apparently the best place to archive edited GoPro footage at least.&lt;/p&gt;
    &lt;p&gt;Drawing a lot of inspiration from interval.com. It was an amazing product but was a hosted SAAS. I'm exploring taking the idea to the .NET ecosystem and also making it a Nuget package that can be installed and served through any ASP.NET project.&lt;/p&gt;
    &lt;p&gt;I got tired of Spotify recommending me the same songs, from the same artists, over and over again.&lt;/p&gt;
    &lt;p&gt;So I built Riff Radar - it creates playlists from your followed artists' complete discography, and allows you to tailor them in multiple ways. Those playlists are my top listened to. I know, because you can also see your listening statistics (at the mercy of Spotify's API).&lt;/p&gt;
    &lt;p&gt;The playlists also get updated daily. Think of it as a better version of the daily mixes Spotify creates.&lt;/p&gt;
    &lt;p&gt;When I tried to save a newly created playlist I got a 500 XHR with message: "failed to fetch user playlists: Error 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'AND disabled = 0' at line 4", about 2mins ago, if that help finding this is logs.&lt;/p&gt;
    &lt;p&gt;A “code index” tool that finds symbols in a codebase and creates a single table sqlite database for querying. It’s my second month using Claude Code, and I see a common pattern where Claude tries to guess patterns with grep, and often comes back with empty results. I’m writing the tool to prevent these fruitless searches. Using tree-sitter to parse the AST and add the symbols and what they are (function, class, argument, etc) to the db. I have it working with TypeScript, and am working on adding C and PHP.&lt;/p&gt;
    &lt;p&gt;This is why codepathfinder.dev is born. It underhood use tree-sitter to search functions, class, member variables and pulls code accurately instead of regex.&lt;/p&gt;
    &lt;p&gt;I started using it like tool call in Security scanning (think of something like claude-code for security scanning)&lt;/p&gt;
    &lt;p&gt;Aider builds something it calls a "repo map" that I believe is for a similar purpose. Might be worth taking a look!&lt;/p&gt;
    &lt;p&gt;I haven't used Claude Code, but recently switched to OpenCode. My token usage and cost is a lot higher, I'm not sure why yet, but I suspect Aider's approach is much more lean.&lt;/p&gt;
    &lt;p&gt;https://lustroczynszowe.pl/ The aggregator of rental values in Poland. We want to increase the transparency of the real estate market, empowering consumers and enabling them to make fully informed financial decisions. We will also suggest savings on specific fields.&lt;/p&gt;
    &lt;p&gt;An app to track my work automatically by feeding screenshots to LLMs and analyzing those. https://donethat.ai&lt;/p&gt;
    &lt;p&gt;Obviously this is quite sensitive data so architected it to never store raw data, allow for bring-your-own-key, and even in team settings be fully private by default, everybody keeps control of all their results.&lt;/p&gt;
    &lt;p&gt;Started about six months ago, have some first users, and always looking for feedback!&lt;/p&gt;
    &lt;p&gt;I’m currently working on https://www.dreamly.in - automated, personalized, and localized bedtime stories for kids.&lt;/p&gt;
    &lt;p&gt;My daughter loves stories, and I often struggled to come up with new ones every night. I remember enjoying local folk tales and Indian mythological stories from my childhood, and I wanted her to experience that too — while also learning new things like basic science concepts and morals through stories.&lt;/p&gt;
    &lt;p&gt;So I built Dreamly and opened it up to friends and families. Parents can set up their child’s profile once - name, age, favorite shows or characters, and preferred themes (e.g. morals, history, mythology, or school concepts). After that, personalized stories are automatically delivered to their inbox every night. No more scrambling to think of stories on the spot!&lt;/p&gt;
    &lt;p&gt;I like reading to my kids and try to read to them in English and Mandarin. My Chinese is conversational, but I have a hard time finding books for them because I’m not good at writing. Something like this with language learning tools would be awesome.&lt;/p&gt;
    &lt;p&gt;I also like making up stories when we go on hikes. Long, rambling stories about unicorns befriending spiders and flying to faraway lands.&lt;/p&gt;
    &lt;p&gt;Building a donations powered marketplace, zero platform fee: https://shomp.co&lt;/p&gt;
    &lt;p&gt;Merchants who want to sell on Etsy or Shopify either have to pay a listing fee or pay per month just to keep an online store on the web. Our goal is to provide a perpetually free marketplace that is powered solely off donations. The only fees merchants pay are the Stripe fees, and it's possible that at some volume of usage we will be able to negotiate those down.&lt;/p&gt;
    &lt;p&gt;You can sell digital goods as well as physical goods. Right now in the "manual onboarding" phase for our first batch of sellers.&lt;/p&gt;
    &lt;p&gt;For digital goods, purchasers get a download link for files (hosted on R3).&lt;/p&gt;
    &lt;p&gt;For physical goods, once a purchase comes through, the seller gets an SMS notification and a shipping label gets created. The buyer gets notified of the tracking number and on status changes.&lt;/p&gt;
    &lt;p&gt;We use Stripe Connect to manage KYC (know your customer) identities so we don't store any of your sensitive details other than your name and email. Since we are in the process of incorporating as a 501(c)(3) nonprofit, we are only serving sellers based in the United States.&lt;/p&gt;
    &lt;p&gt;The mission of the company is to provide entrepreneurial training to people via our online platform, as well as educational materials to that aim.&lt;/p&gt;
    &lt;p&gt;Right now the API is nonexistent, relying entirely on people using the web interface to make listings, upload photos, and set prices. But if you would find this useful I can happily build it out. Our stack is Elixir and building APIs is very straightforward. Our code is open-source, too!&lt;/p&gt;
    &lt;p&gt;When you say "algorithmically driven print-on-demand" do you mean that prices would automatically adjust based on inventory? Or like, how do you mean.&lt;/p&gt;
    &lt;p&gt;Also, when you say "see them show up in a request on sale" — can you clarify? I interpret this to mean you want a webhook triggered when an order comes in.&lt;/p&gt;
    &lt;p&gt;The goal is to provide a fully typed nodeJS framework that allows you to write a typescript function once and then decide whether to wire it up to http, websocket, queues, scheduled tasks, mcp server, cli and other interactions.&lt;/p&gt;
    &lt;p&gt;You can switch between serverless and server deployments without any refactoring / completely agnostic to whatever platform your running it on&lt;/p&gt;
    &lt;p&gt;It also provides services, permissions, auth, eventhub, advanced tree shaking, middleware, schema generation and validation and more&lt;/p&gt;
    &lt;p&gt;The way it works is by scanning your project via the typescript compiler and generating a bootstrap file that imports everything you need (hence tree shaking), and allows you to filter down your backend to only the endpoints needed (great to pluck out individual entry points for serverless). It also generates types fetch, rpc, websocket and queue client files. Types is pretty much most of what pikku is about.&lt;/p&gt;
    &lt;p&gt;Think honoJS and nestJS sort of combined together and also decided to support most server standards / not just http.&lt;/p&gt;
    &lt;p&gt;Website needs love, currently working on a release to support CLI support and full tree shaking.&lt;/p&gt;
    &lt;p&gt;I agree framing pikku has been a pretty hard challenge for me.&lt;/p&gt;
    &lt;p&gt;It supports different runtimes in the sense of deno / bun or custom nodeJS runtimes in the cloud, but ultimately relies purely on typescript / a JavaScript compatible backend.&lt;/p&gt;
    &lt;p&gt;It’s less of a webserver and more of a lightweight framework though, since it also supports CLIs or Frontend SDKs / isn’t tied to running an actual server.&lt;/p&gt;
    &lt;p&gt;I’m working on a performance capture library for Python because I often need to know the performance of backend systems I maintain. I frequently build tooling to capture performance and save it for later analysis. I/O operations get costly when writing lots of data to disk and creating good real-time analytics tools takes a lot of my time. I wanted a library that captures real-time performance analytics from Python backends.&lt;/p&gt;
    &lt;p&gt;This is why I wrote kronicler to record performance metrics while being fast and simple to implement. I built my own columnar database in Rust to capture and analyze these logs.&lt;/p&gt;
    &lt;p&gt;To capture logs, `import kronicler` and add `@kronicler.capture` as a decorator to functions in Python. It will then start saving performance metrics to the custom database on disk.&lt;/p&gt;
    &lt;p&gt;You can then view these performance metrics by adding a route to your server called `/logs` where you return `DB.logs()`. You can paste your hosted URL into the settings of usekronicler.com (the online dashboard) and view your data with a couple charts. View the readme or the website for more details for how to do this.&lt;/p&gt;
    &lt;p&gt;I'm still working on features like concurrency and other overall improvements. I would love some feedback to help shape this product into something useful for you all.&lt;/p&gt;
    &lt;p&gt;Currently working on an advanced analytics tool for 0DTE trade data, which allows to scan through massive amounts of data in the least amount of time, find seasonal patterns created by institutional investors, blast credit chains for profitable trades and simulate / backtest arbitrary trades in conjunction to create portfolios. So far the software yielded several successful trading strategies, which outperform standard approaches ever since the new administration in the US came to power. Currently in closed beta but planning to release around Xmas eventually.&lt;/p&gt;
    &lt;p&gt;Thanks for checking it out. I'm using a 3rd party API for all the station info (radio-browser.info) and sometimes the station streams result in that error. You may have just had some bad luck clicking on a few in a row that don't work. I'll try to think of a way to filter those broken ones out.&lt;/p&gt;
    &lt;p&gt;- Working on Kanji Palace (https://kanjipalace.com): We're going to publish the iOS app on the App Store and adding vocabulary. Currently, the app converts single Kanji (e.g., 生) into vivid mnemonic images. We aim to support vocabulary like 先生.&lt;/p&gt;
    &lt;p&gt;- Writing a book about Claude Code, not just for assisted programming, but as a general AI agent framework.&lt;/p&gt;
    &lt;p&gt;It's a sync infra product that is meant to cut down 6 months of development time, and years of maintenance of deep CRM sync for B2B SaaS.&lt;/p&gt;
    &lt;p&gt;Every Salesforce instance is a unique snowflake. I am moving that customization into configuration and building a resilient infrastructure for bi-directional sync.&lt;/p&gt;
    &lt;p&gt;I'm working on two graph optimization libraries for quantum computers. The first one was released a few months ago and the next version will make it much more powerful. The second one is currently being tested. Both of them work on actual quantum computers, which makes them exciting :)&lt;/p&gt;
    &lt;p&gt;In parallel, I'm trying to figure out how to train a LLM for SAST.&lt;/p&gt;
    &lt;p&gt;Currently working on expanding the Pacific Northwest’s largest durable carbon removal project using Enhanced Rock Weathering and starting a $1m SAFE fundraising round.&lt;/p&gt;
    &lt;p&gt;We received data last week verifying we are effectively mineralizing CO2 at a high rate while saving our farmer $135/acre annually in liming costs.&lt;/p&gt;
    &lt;p&gt;We’ve come this far on grants. Now it’s time to fundraise so we can bankroll our PhDs whilst we secure pre-purchase offtake deals.&lt;/p&gt;
    &lt;p&gt;If you know of any impact investors or are an offtake buyer at a large company, please email me at zach@goal300.earth&lt;/p&gt;
    &lt;p&gt;Currently building a Declarative Web Assembler of Html/Json using AI in multiple languages for the past 1 month: https://github.com/Srid68/Arshu.Assembler deployed to fly.io&lt;/p&gt;
    &lt;p&gt;The purpose is to find if can i build declarative software in multiple langauges (Rust, Go, Node.Js, PHP and Javascript) knowing only one language (C#) without understanding the implementation deeply.&lt;/p&gt;
    &lt;p&gt;Another purpose is validate AI models and their efficiency since development using AI is hard but highly productive and having a declarative rules to recreate the implementation may be used to validate models&lt;/p&gt;
    &lt;p&gt;Currently i am convinced it is possible to build, but now working on creating a solid foundation with tests of the two assembler engines, structure dumps, logging, logging outputs so that those can be used by the AI which it needs to fix issues iteratively.&lt;/p&gt;
    &lt;p&gt;Need to add more declarative rules and implement a full stack web assembler to see if AI will hit the technical debt which slows/stop progress. Only time will tell.&lt;/p&gt;
    &lt;p&gt;I have been working on a new Python HTTP client which is 100% Rust-based (sync+async). Using reqwest under the hood and providing everything it has to offer to Python land + much more. Also including mocking capabilities. Here: https://github.com/MarkusSintonen/pyreqwest&lt;/p&gt;
    &lt;p&gt;Started from the poor state of many Python HTTP clients and poor testing utilities there is for them. (Eg the neglected state of httpx and its all perf issues)&lt;/p&gt;
    &lt;p&gt;Working on a multisig solution for authenticated file distribution, initially targeting GitHub releases. Based on minisig and git.&lt;/p&gt;
    &lt;p&gt;I think this project is an interesting addition as a software supply chain solution, but generating interest in the project in this early stage proves difficult.&lt;/p&gt;
    &lt;p&gt;I've been working on https://booplet.com. It's like Lovable but for desktop apps and heavily inspired by Robin Sloan's home-cooked app essay [1][2]. The idea is to let anyone, especially non-technical folks, build and use personal apps. Instead of cloud deployment, we focused on a local-first setup so that users can fully own their apps and data.&lt;/p&gt;
    &lt;p&gt;Same as last time, I guess. It's a voxel building environment that uses irregular voxels (voxels with sloped faces). I've been working on fixing the bugs for a while now (and there are still a lot of them left).&lt;/p&gt;
    &lt;p&gt;I'm working on Plaid / Perplexity for business data.&lt;/p&gt;
    &lt;p&gt;The basic idea is that integrating business data into a B2B app or AI agent process is a pain. On one side there's web data providers (Clearbit, Apollo, ZoomInfo) then on the other, 150 year old legacy providers based on government data (D&amp;amp;B, Factset, Moody's, etc). You'd be surprised to learn how much manual work is still happening - teams of people just manually researching business entities all day.&lt;/p&gt;
    &lt;p&gt;At a high level, we're building out a series of composable deep research APIs. It's built on a business graph powered by integrations to global government registrars and a realtime web search index. Our government data index is 265M records so far.&lt;/p&gt;
    &lt;p&gt;We're still pretty early and working with enterprise design partners for finance and compliance use cases. Open to any thoughts or feedback.&lt;/p&gt;
    &lt;p&gt;There is this popular coffee cup cooling problem: assume you want to keep your morning cup of coffee hot for as long as possible. When do you add your milk? Immediately or later?&lt;/p&gt;
    &lt;p&gt;I am overengineering a simulation-based solution to this because I think there are scenarios based on cup shapes and environmental temperatures that allow either answer to be true. This will end up as a blog post I guess.&lt;/p&gt;
    &lt;p&gt;A lot of people often ask questions like: - How do I lose body fat and build muscle? - How can I track progress over time? - How much exercise do I actually need? - What should my calorie and macro targets be?&lt;/p&gt;
    &lt;p&gt;I am working on www.accrux.co. It basically just a project that allows finance and investors bring their diversified portfolios and manage multiple portfolios together into one place. As a little investor myself that have some crypto, stock and fixed assets, I find it difficult bringing some of my investments together that was why i decided to build this. The goal and aim is just to bring clarity into your investments and have it as your investment companion which gives your detailed insight and time to time alerts on the health of your portfolios. I currently have it in testing and if anyone is willing to give it a try https://appstaging.accrux.co/signup here or reply so i can take you through a demo.. It is completely free for the few months and may charge a couple of dollars after a while after more features are added to cater for the service.&lt;/p&gt;
    &lt;p&gt;For context, I'm a UX Designer at a low-code company. LLMs are great at cranking out prototypes using well-known React component libraries. But lesser known low-code syntax takes more work. We made an MCP server that helps a lot, but what I'm working on now is a set of steering docs to generate components and prototypes that are "backwards compatible" with our bespoke front end language. This way our vibe prototyping has our default look out of the box and translates more directly to production code. https://github.com/pglevy/sail-zero&lt;/p&gt;
    &lt;p&gt;A LLM‑powered OSINT helper app that lets you build an interactive research graph. People, organizations, websites, locations, and other entities are captured as nodes, and evidence is represented as relationships between them.&lt;/p&gt;
    &lt;p&gt;Usually assistants like ChatGPT Deep Research or Perplexity are focused on fully automatic question answering, and this app lets you guide the search process interactively, while retaining knowledge in the graph.&lt;/p&gt;
    &lt;p&gt;The plan is to integrate it with multiple OSINT-related APIs, scrapers, etc&lt;/p&gt;
    &lt;p&gt;I'm trying to make manual focus work on my Lenovo tablet. Everything seems ok, onCaptureStarted shows focal distance set to 5 diopters yet the camera takes photo at infinity.&lt;/p&gt;
    &lt;p&gt;I’m refactoring https://harcstack.org so that it can handle Theme plugins. Next after Pico CSS is Bulma. The idea is to complement HTMX on the server side with functional HTML coding (inspired by elmlang), components and a base library.&lt;/p&gt;
    &lt;p&gt;Now that I can finally test on hardware, I completely rewrote input handling. I can now support original NES controllers, but also SNES and the Power Pad dance mat, for anyone crazy enough to try that. The hardest part was working around a particularly nasty hardware bug: if you try to read the input ports on even cycles while one of the sound channels is playing, the data becomes corrupted. Perform the exact same read on an odd cycle and it works every time.&lt;/p&gt;
    &lt;p&gt;The solution? Have the cartridge keep track of CPU parity (there's no simple way to do this with just the CPU), then check that, skip one cycle if needed... and very carefully cycle time the rest of the routine, making sure that your reads land on safe cycles, and your writes land in places that won't throw off the alignment.&lt;/p&gt;
    &lt;p&gt;But it works! It's quite reliable on every console revision I've thrown it at so far. Suuuper happy with that.&lt;/p&gt;
    &lt;p&gt;It is a DNS service for AWS EC2 to keep the ever changing IPs when you cannot use the Elastic IP like ASG or when you don't want to install any third party clients to your instances.&lt;/p&gt;
    &lt;p&gt;It fetches the IPs regularly via AWS API and assign them to fixed subdomains.&lt;/p&gt;
    &lt;p&gt;I'm trying to turn code into a design tool. Kind of like if you ask yourself - what if Cursor had been built for designers?&lt;/p&gt;
    &lt;p&gt;Currently it looks like this:&lt;/p&gt;
    &lt;p&gt;- code editor directly in the browser - writes to your local file system - UI-specific features built into the editor - ways to edit the CSS visually as well as using code - integrated AI chat&lt;/p&gt;
    &lt;p&gt;But I have tons of features I want to add. Asset management, image generation, collaborative editing, etc.&lt;/p&gt;
    &lt;p&gt;It's still a prototype, but I'm actively posting about it on twitter as I go. Soon, I'll probably start publishing versioned builds for people to play with: https://x.com/danielvaughn&lt;/p&gt;
    &lt;p&gt;It would be helpful for the box for each test to include some explanation, eg:&lt;/p&gt;
    &lt;p&gt;25-Hydroxyvitamin D, also known as calcidiol, regulates calcium absorption in the intestines, promotes bone formation and mineralization, and supports immune function.&lt;/p&gt;
    &lt;p&gt;Apolipoprotein B (ApoB) is a protein that binds to LDL receptors on cells, allowing lipoproteins to deliver cholesterol and triglycerides to tissues for energy or storage.&lt;/p&gt;
    &lt;p&gt;Lipoprotein(a) is a low-density lipoprotein variant identified as a risk factor for atherosclerosis and related diseases, such as coronary heart disease and stroke.&lt;/p&gt;
    &lt;p&gt;Would love to see the Red Cross partner with someone like you here in Australia. Not affiliated, just a donor. We're not financially incentivised like other countries but there's a big culture here about celebrating the free milkshake and/or sausage roll you get after donating.&lt;/p&gt;
    &lt;p&gt;The goal is to catch vulnerabilities early in the SDLC by running agentic loop that autonomously hunt for security issues in codebases.Currently available as a CLI tool, VSCode extension.I've been actively using to scan WordPress, odoo plugins and found several privilege escalation vuln. I have documented as blog post here: https://codepathfinder.dev/blog/introducing-secureflow-cli-t...&lt;/p&gt;
    &lt;p&gt;I'm in The Hague right now at a digital democracy conference, where I was invited to present on my prototype that I've been building the past few months!&lt;/p&gt;
    &lt;p&gt;It's for doing realtime "human cartography", to make maps of who we are together in complex large-scale discourse (even messy protest).&lt;/p&gt;
    &lt;p&gt;It's for exploring human perspective data -- agree, disagree, pass reactions to dozens or hundreds of belief statements -- so we can read it as if it were Google Maps.&lt;/p&gt;
    &lt;p&gt;My operating assumption is that if a critical mass of us can understand culture and value clashes as mere shapes of discourse, and we can all see it together, the we can navigate them more dispassionately and with clear heads. Kinda like reading a map or watching the weather report -- islands that rise from oceans, or plate tectonics that move like currents over months, and terraform the human landscape -- maybe if we can see these things together, we'll act less out of fear of fun-house caricatures. (E.g., "Hey, dad, it seems like the peninsula you're on is becoming a land bridge toward the alt right corner. I feel a little bummed about that. How do you feel about it?")&lt;/p&gt;
    &lt;p&gt;(It builds on data and the mathematical primitives of a great tool called Pol.is, which I've worked with for almost a decade.)&lt;/p&gt;
    &lt;p&gt;I’ve been building proficiency with quantum optics equipment. Repeating classic quantum entanglement experiments like the quantum eraser [0] and violating the CHSH inequality (which won the 2022 Nobel). I’m working towards a novel quantum eraser variant.&lt;/p&gt;
    &lt;p&gt;I’ve been casually getting into thrifting and realized pretty quickly that Lens is super limited in its functionality and is mostly a shopping app. I put a site together that is like a supercharged version of Lens for thrifters where you can get info on price, demand, and condition. Share function is borked atm tho&lt;/p&gt;
    &lt;p&gt;They’re always on. They log into real sites, click around, fill out forms, and adapt when pages change — no brittle scripts, no APIs needed. You can deploy one in minutes, host it yourself, and watch it do work like a human (but faster, cheaper, never tired).&lt;/p&gt;
    &lt;p&gt;Kind of like a “browser-use cloud,” except it’s yours — open, self-hostable, and way more capable.&lt;/p&gt;
    &lt;p&gt;https://ivyreader.com I am working on my RSS Reader/Podcast player. I am currently searching and patching all the little bugs, fixing the ui and creating the landingpage.&lt;/p&gt;
    &lt;p&gt;It’s fast, free, keyboard-only, cross-platform, and add-free. It’s been my only source of music for the past 6 months or so.&lt;/p&gt;
    &lt;p&gt;I’m not sharing the link because of music copyright issues. But I think more people should do that, to break free of the yoke of greedy music platforms.&lt;/p&gt;
    &lt;p&gt;- I create channels that play tracks in a defined order, on repeat, but with a duration of at least 80 hours (and ever-growing). Old-school album-based listening.&lt;/p&gt;
    &lt;p&gt;- I think learning of new stuff is twisted in the current environment. "New stuff" in the sense of radio/Spotify is mostly "same stuff as I know and like, but slightly different so it feels new". You don’t discover truly new stuff unless by actively searching for it. No radio or service is going to passively do that for you.&lt;/p&gt;
    &lt;p&gt;I'm working on a web app that creates easy-to-understand stories and explainers for the sake of language learning. You can listen in your favourite podcast app, or directly on the website with illustrations.&lt;/p&gt;
    &lt;p&gt;I'm eager to add more languages if anyone is fluent/able to help me evaluate the text-to-speech.&lt;/p&gt;
    &lt;p&gt;Still working on cataloging a curated list of craft beer venues across the world at https://wheretodrink.beer Unsure what the plan is going forward with it, apart from adding more venues and more countries. As long as it's fun for me I'll just keep adding things.&lt;/p&gt;
    &lt;p&gt;Just added health inspection data from countries that have that in open datasets (UK and Denmark). If anyone know of others I'd be appreciative of hints.&lt;/p&gt;
    &lt;p&gt;Thinking of focusing on another idea for the rest of the year, have a rough idea for a map based ui to structure history by geofences or lat / lng points for small local museums&lt;/p&gt;
    &lt;p&gt;I was tired of only having 1 or 2 things per newsletter that interested me, multiplied by however many newsletters I've subscribed to. Trying to solve that.&lt;/p&gt;
    &lt;p&gt;The idea: design newsletter sections on whatever topics you want (football scores, tech news, new restaurants in your area, etc.), choose your tone and length preferences, then get a fully cited digest delivered weekly to your inbox. Completely automated after initial setup (but you can refine it anytime).&lt;/p&gt;
    &lt;p&gt;Have the architecture sorted and a pretty good dev plan, but collecting interest before I invest a ton of time into it.&lt;/p&gt;
    &lt;p&gt;Conductor is a LLM agnostic framework for building sophisticated AI applications using a subagent architecture. It provides a robust platform for orchestrating multiple specialized AI agents to accomplish complex tasks, with features like LLM-based planning, memory persistence, and dynamic tool use.&lt;/p&gt;
    &lt;p&gt;It provides a robust and flexible platform for orchestrating multiple specialized AI agents to accomplish complex tasks. This project is inspired by the concepts outlined in "The Rise of Subagents" by Phil Schmid at https://www.philschmid.de/the-rise-of-subagents and it aims to provide a practical implementation of this powerful architectural pattern.&lt;/p&gt;
    &lt;p&gt;For the past 2 years we are trying to bring some order to the chaos called restaurant menu creation. Correctify is a platform combining all the features restaurants need for both online and print menus with most tasbs being automated with AI https://correctify.com.cy/&lt;/p&gt;
    &lt;p&gt;I’m working on Leggen (https://github.com/elisiariocouto/leggen), a self hosted personal banking account management system. It started out as a CLI that syncs your bank account transactions and balances, saves them in a sqlite database and can alert you via Telegram or Discord if a transaction matches a filter. Recently I started refactoring the project with the help of Claude Code and Copilot Agent to include an API and a Web app to explore the data and configure it. The product is using GoCardless Bank Accout Data APIs to connect to banks via PSD2 but I found out recently that registering a new account is no longer possible so I’m currently looking into alternatives.&lt;/p&gt;
    &lt;p&gt;Check out Lunch Flow (https://lunchflow.app) for a global open banking API that's accessible for personal finance apps :) We integrate with Gocardless, among other global open banking providers.&lt;/p&gt;
    &lt;p&gt;I'm doing some experiments in LLM (historical) fiction writing. I feel like we can get pretty good writing out of an LLM (especially Sonnet) with enough prompting, reasoning, and guided thinking. Still with a human as producer and guidance.&lt;/p&gt;
    &lt;p&gt;I'm trying to use this to create stories that would be somewhat unreasonable to write otherwise. Branching stories (i.e., CYOA), multiperspective stories, some multimedia. I'm still trying to figure out the narrative structures that might work well.&lt;/p&gt;
    &lt;p&gt;LLMs can overproduce and write in different directions than is reasonable for a regular author. Though even then I'm finding branching hard to handle.&lt;/p&gt;
    &lt;p&gt;The big challenges are rhythm, pacing, following an arc. Those have been hard for LLMs all along.&lt;/p&gt;
    &lt;p&gt;Currently working on the web reader of WithAudio. Just add with.audio/ to begining of a public URL and get the text and audio in your browser. It runs the TTS in your browser so its free and unlimited.&lt;/p&gt;
    &lt;p&gt;I buit this to get some traffic to my main project's website using a free tool people might like. The main project: https://desktop.with.audio -&amp;gt; a one time payment text to speech app with text highlighting and export mp3 and other features on MacOS (ARM only) and Windows.&lt;/p&gt;
    &lt;p&gt;The goal was to make the learning material very malleable, so all content can be viewed through different "lenses" (e.g. made simpler, more thorough, from first principles, etc.). A bit like Wikipedia it also allows for infinite depth/rabbit holing. Each document links to other documents, which link to other documents (...).&lt;/p&gt;
    &lt;p&gt;I'm also currently in the middle of adding interactive visualizations which actually work better than expected! Some demos:&lt;/p&gt;
    &lt;p&gt;I'm still working on the Mint programming language (https://mint-lang.com/) and DevBox (https://www.dev-box.app/) which is a desktop application/browser extension/web application with a bunch of small tools.&lt;/p&gt;
    &lt;p&gt;I have been following Mint for a while and it feels like a project that truly focuses on making developers happy. The mix of Elm style structure, strong typing, and builtin tools like testing and formatting makes it really enjoyable to use. Keep up your great works!&lt;/p&gt;
    &lt;p&gt;Building an open-source workflow engine for automating repetitive dev-ops and data-ops tasks. Focused on portability and running on bare metal or small VPS.&lt;/p&gt;
    &lt;p&gt;I'm making an app for self-tracking. Combining elements from habit trackers, health logging and journaling. Built for rich customization and local-first. Want to be free of rigid structures of many existing apps while providing a better UX / usability than using a spreadhsheet.&lt;/p&gt;
    &lt;p&gt;Porting my binary &amp;amp; decimal palindromes[0] finding code[1] to CUDA, with which I had no experience before starting this project.&lt;/p&gt;
    &lt;p&gt;It's already working, and slightly faster than the CPU version, but that's far from an acceptable result. The occupancy (which is a term I first learned this week) is currently at a disappointing 50%, so there's a clear target for optimisation.&lt;/p&gt;
    &lt;p&gt;Once I'm satisfied with how the code runs on my modest GPU at home, the plan is to use some online GPU renting service to make it go brrrrrrrrrr and see how many new elements I can find in the series.&lt;/p&gt;
    &lt;p&gt;Working on https://practicecallai.com/ - simple saas that lets users run practice calls / role play against a custom AI partner. Goal is to make it the easiest to use &amp;amp; fastest to get started with in the market.&lt;/p&gt;
    &lt;p&gt;It’s been a fun, practical way to continuously evaluate the latest models two ways - via coding assistance &amp;amp; swapping between models to power the conversational AI voice partner. I’ve been trying to add one big new feature each time the model generation updates.&lt;/p&gt;
    &lt;p&gt;The next thing I want to add is a self improving feedback loop where it uses user ratings of the calls &amp;amp; evaluations to refine the prompts that generate them.&lt;/p&gt;
    &lt;p&gt;I've been working on a browser plugin for Amazon that attempts to identify the brand and seller country: https://www.wheresthatfrom.org/&lt;/p&gt;
    &lt;p&gt;It's mostly where I want it to be now, but still need to automate the ingest of USPTO data. I'd really like it to show a country flag on the search results page next to each item, but inferring the brand name just from the item title would probably need some kind of natural language processing; if there's even a brand in the title.&lt;/p&gt;
    &lt;p&gt;No support for their mobile layout. Do many people buy from their phone?&lt;/p&gt;
    &lt;p&gt;Plugging away with reviews of Genrative AI tech with detailed comparisons. I announced the launch on HN a while ago, thought I’d use this month’s for a status update.&lt;/p&gt;
    &lt;p&gt;I just took Qwen-Image and Google’s image AIs for a spin and I keep a side by side comparison of many of them.&lt;/p&gt;
    &lt;p&gt;Thanks, the 3D asset creators are very interesting. I'm working on LLM -&amp;gt; CAD tool (for 3d printing) and your post confirms that I should keep my focus, because there is so much other things to do (uv unwrapping!) if you are targeting games for example.&lt;/p&gt;
    &lt;p&gt;It’s a simple NPM package that produces colorful avatars from input data to aid with quick visual verification. I’d like to see it adopted as a standard.&lt;/p&gt;
    &lt;p&gt;I just finished writing a small script that finds all optimally bad Wordle guesses. More precisely, on hard Wordle, where you must give a valid word (from the guesses list), and you must use yellows + greens, and must not use greys, what are all the combinations of answer + 6 guesses where there is only grey. This is equivalent to finding all answer + 6 guesses where no letters are in common between any pair.&lt;/p&gt;
    &lt;p&gt;This is basically a variation on bit-packing (which is NP-hard), but it's tractable if you prune the search space enough.&lt;/p&gt;
    &lt;p&gt;lpviz is like Desmos, but for linear programming - I've implemented a few LP solvers in Typescript and hooked them up to a canvas so you can draw a feasible region, set an objective direction, and see how the algorithms work. And it all runs locally, in the browser!&lt;/p&gt;
    &lt;p&gt;If you go to https://lpviz.net/?demo it should show you a short tour of the features/how to use it.&lt;/p&gt;
    &lt;p&gt;It's by no means complete but I figured there may be some fellow optimization enthusiasts here who might be interested to take a look :) Super open to feedback, feature requests, comments!&lt;/p&gt;
    &lt;p&gt;I'm building Monadic DNA explorer, a tool to explore thousands of genetic traits from GWAS Catalog in your browser and plug in your own DNA data from 23andMe, Ancestry, etc. All processing happens locally on your machine and AI insights are run in a private LLM inside a TEE.&lt;/p&gt;
    &lt;p&gt;Working on the finishing touches for my first bigger game, created and published entirely by myself! It's a pixelly courier-adventure, questioning the pace of our modern world a little bit: https://store.steampowered.com/app/3644970/Fading_Serenades/&lt;/p&gt;
    &lt;p&gt;Done with Godot in just 7-8 months, it's fun how fast you can create things when you really focus on something :)&lt;/p&gt;
    &lt;p&gt;This is going in fits and starts, but I'm working on a Win16 decompiler. The problems with existing decompiler tools for 16-bit code are a) support the NE file format is far less widespread; b) 16-bit code means geating to deal with segment registers, which are largely unmodelled for most binary tools; and c) turns out that you also have to get really good at recognizing "this is a 32-bit value being accessed entirely in 16-bit word chunks," which tends to be under-supported for most optimization toolchains.&lt;/p&gt;
    &lt;p&gt;Intercom is awful. There is a huge market here, and chatwoot haven’t done a great job.&lt;/p&gt;
    &lt;p&gt;Our company would love a well designed chat button linked to Slack, combined with a helpdesk that supports email queries and also allows people to raise issues via the web.&lt;/p&gt;
    &lt;p&gt;That’s it, that’s all we need. Happy to pay.&lt;/p&gt;
    &lt;p&gt;It’s hard to express how badly intercom is designed and engineered. It’s also very expensive and constantly upsold, despite being rubbish. If no one fixes this it will be my next startup.&lt;/p&gt;
    &lt;p&gt;Too many companies have gone down the road of “AI support”, without understanding that AI must rest on the foundation of great infrastructure. Intercom are pushing their AI so hard it’s absolutely infuriating.&lt;/p&gt;
    &lt;p&gt;Eidetica is a decentralized database project that I've been working on that is finally in a somewhat usable state. It basically wraps CRDTs into a close to normal Database interface, with decentralized authentication, background sync, etc.&lt;/p&gt;
    &lt;p&gt;Trying to get a new release of Video Hub App - my 7+ years passion project to browse videos from local storage in style. Maybe will finally finish the (optional!) facial recognition feature I started 5+ years ago.&lt;/p&gt;
    &lt;p&gt;An open source campaign management app for TTRPGs. There are a ton out there, that are basically just fancy wikis. I'm working on one in Django for running my old school D&amp;amp;D game i'm starting back up this fall.&lt;/p&gt;
    &lt;p&gt;It is a modified version of Shopify's CEO Tobi try implementation[0]. It extends his implementation with sandboxing capabilities and designed with functional core, imperative shell in mind.&lt;/p&gt;
    &lt;p&gt;I had success using it to manage multiple coding agents at once.&lt;/p&gt;
    &lt;p&gt;You provide your URL and an LLM browses your site and writes up feedback. Currently working on increasing the quality of the feedback. Trying to start with a narrower set of tests that give what I think is good feedback, then increase from there.&lt;/p&gt;
    &lt;p&gt;If a tool like this analyzed your website, what would you actually want it to tell you? What feedback would be most useful?&lt;/p&gt;
    &lt;p&gt;I'm working on adding favicons support to listings on my website directory I recently launched: https://intrasti.com&lt;/p&gt;
    &lt;p&gt;I just released the changelog 5 minutes ago https://intrasti.com/changelog which I went with a directory based approach using the international date format YYYY-MM-DD so in the source code it's ./changelog/docs/YYYY/MM/DD.md - seems to do the trick and ready for pagination which I haven't implemented yet.&lt;/p&gt;
    &lt;p&gt;bash scripts that make terraform configurations for scaling bioinfo work to n h100s/a100s spot vms with resistance to the vms getting terminated. i now have it for alphafold3 jobs but i need to make the same for boltz, gnina, gromacs (although spot vms do not make sense here), etc&lt;/p&gt;
    &lt;p&gt;next step is to make a simple login portal for non trusted persons to be able to submit work as this a uni project, mail the result / process.&lt;/p&gt;
    &lt;p&gt;Working away at https://TempMailDetector.com, a privacy focused disposable email detection API which only requires the domain part and not the user part of the email. The service is able to determine if a domain is likely a disposable email, a forwarding service, and actively crawls for new domains.&lt;/p&gt;
    &lt;p&gt;I'm working on that thing the world really needs - yet another javascript framework. The code is all in the repo for my app though. Hopefully I can break it out into it's own repo to share by the end of the year.&lt;/p&gt;
    &lt;p&gt;I've been working on https://edugram.live to make learning fun like a social media feed. Planning to build an insta like mobile app feed on top of this.&lt;/p&gt;
    &lt;p&gt;Currently working on Note Cargo, basically self-hosted Markdown note-taking app, but I tried to not using database. So similar to Obsidian/Logseq but its web-based.&lt;/p&gt;
    &lt;p&gt;And currently working to make things shareable, also don't want to use database.&lt;/p&gt;
    &lt;p&gt;it says OneNote but does it also support stylus handwriting, links, tables, images? id love a onenote replacement but these features are what makes me stick to onenote&lt;/p&gt;
    &lt;p&gt;ive got a couple ai scripts on the go, and i want to see if i can get the inference to run on my phone.&lt;/p&gt;
    &lt;p&gt;1. is something that can poll a bunch of websites workshop/events pages to see if theres any new events [my mother] wants to go to and send a digest to her email&lt;/p&gt;
    &lt;p&gt;2. is a poller to look up the different safeway/coop/save on flyers and so on to see whats on sale between the different places, then send a mail with some recipes it found based on those ingredients&lt;/p&gt;
    &lt;p&gt;Im most of the way through 1, but havent started on 2 yet.&lt;/p&gt;
    &lt;p&gt;I’m working on Reflect [0], it’s a private self discovery and self experimentation app. You can track metrics, set goals, get alerted to anomalies, view correlations, visualize your data, etc.&lt;/p&gt;
    &lt;p&gt;I needed to let off steam regarding Chatcontrol, so I created a little site, where people can post and comment while sitting on the toilet since we take our smartphones everywhere with us, right? It surely is not influencial but it gave me a good time and a better feeling. https://shitcontrol.eu/&lt;/p&gt;
    &lt;p&gt;Working on: to teach myself Rust, I’ve been working on a NYT Letter Boxed solver, with some ambitions to turn it into a game by itself. I think this game could be made a lot more fun.&lt;/p&gt;
    &lt;p&gt;Thinking about: A new take on LinkedIn/web-of-trust, bootstrapped by in-person interactions with devices. It seems that the problem of proving who is actually human and getting a sense of how your community values you might be getting more important, and now devices have some new tools to bring that within reach.&lt;/p&gt;
    &lt;p&gt;I'm working on Debtmap - An open source Rust-based code complexity analyzer that tells you exactly which code to refactor and which code to test for maximum impact. Combines complexity metrics with test coverage data to identify the riskiest code in your codebase. Uses entropy analysis to reduce false positives by distinguishing genuinely complex code from repetitive patterns.&lt;/p&gt;
    &lt;p&gt;My partner and I are working on Supabird.io (https://supabird.io), a tool to help people grow on X in a more consistent and structured way. It analyzes viral posts within specific communities so users can learn what works and apply those insights to their own content.&lt;/p&gt;
    &lt;p&gt;My partner shares our journey on X (@hustle_fred), while I’ve been focused on building the product (yep, the techie here :). We’re excited to have onboarded 43 users in our first month, and we're looking forward to getting feedback from the HN community!&lt;/p&gt;
    &lt;p&gt;Shipping pets and animals across borders is a big problem, and we are building the operating system to solve it at scale. If you are a vet (or work in the veterinary space), we would love to talk to you.&lt;/p&gt;
    &lt;p&gt;We just brought an IFR 2947a communications service monitor back from the dead. It's amazing how much functionality that you can pack into about 6U of rack space. I was testing it out, and detecting signals down to 0.1 uV on the spectrum analyzer.&lt;/p&gt;
    &lt;p&gt;I've been gathering up the supplies to set up a proper radio/computer repair workshop.&lt;/p&gt;
    &lt;p&gt;What do you guys think of this? https://www.textaurant.app. It's an AI "agentic" SMS ordering system that's hopefully better than Taco Bell's attempt... I got sick of navigating every restaurants nuanced online order placing and figured I'd try to standardize it myself with an SMS based assistant (yes I'm aware of the XKCD). The idea is every restaurant would have their own number, or down the road I could have a single number for all restaurants but I'm somewhat token/context limited right now. It uses GPT 4o and I've been working on it for the past 4 months. Closed source for now but who knows I might open it up but I'm deciding if it's worth trying to patent.&lt;/p&gt;
    &lt;p&gt;I am playing at creating a FTP interface for all file transfer protocols (including the Dropbox API) so we can settle the argument of the infamous top comment of the Dropbox launch: https://github.com/mickael-kerjean/filestash&lt;/p&gt;
    &lt;p&gt;Duckyscript is a language for the USB rubber ducky that costs approximately 100$. A usb rubber ducky is an usb key that gets recognized as a keyboard and that starts typing text and shortcuts automatically once you plug it to anything. To specify to the key what to type, you can use duckyscript.&lt;/p&gt;
    &lt;p&gt;I'm using circuitpython. The last thing I did was to de-recursify the interpreter with a stack.&lt;/p&gt;
    &lt;p&gt;The more I'm implementing of duckyscript, the more i think that i should create my own language. Duckyscript sucks as a language...&lt;/p&gt;
    &lt;p&gt;I'm expanding my computational biology toolkit in rust. Of recent interest is optimizing long-range molecular dynamics forces on GPU and SIMD, adding support to generate lipid membranes and LNPs, and a 3D small molecule editor with integrated dynamics.&lt;/p&gt;
    &lt;p&gt;I'm currently working on building a local delivery service using electric cargo bikes in NYC: https://hudsonshipping.co. We are planning to launch our first pilot in early 2026 with our first customers in Brooklyn. We've built all of the tech in-house to manage the fleet, deliveries and optimize our routes. If you know of anyone that would like to be a part of the pilot program, feel free to reach out to me!&lt;/p&gt;
    &lt;p&gt;Building a better way to make design comparisons for electronics engineers. Starting with ceramic capacitors for now but expanding to other components types soon. www.get-merlin.com&lt;/p&gt;
    &lt;p&gt;Haunted house trope, but it's a chatbot. Not done yet, but it's going well. The only real blocker is that I ran into the parental controls on the commercial models right away when trying to make gory images, so I had to spin up my own generators. (Compositing by hand definitely taking forever).&lt;/p&gt;
    &lt;p&gt;I built a website that lets you browse Pokemon ENS (Ethereum Name Service) names, view their registration statuses and recent sales. It's a small but engaged niche&lt;/p&gt;
    &lt;p&gt;A React Three Fiber-based avatar engine with speech synthesis, voice automation, and real-time interaction with tools such as Home Assistant, n8n, and LLMs.&lt;/p&gt;
    &lt;p&gt;I've spent the last few months working on a custom RL model for coding tasks. The biggest headache has been the lack of good tooling for tuning the autorater's prompt. (That's the judge that gives the training feedback.) The process is like any other quality-focused task—running batch rating jobs and doing SxS evaluations—but the tooling really falls short. I think I'll have to build my own tools once I wrap up the current project&lt;/p&gt;
    &lt;p&gt;Working on https://videotobe.com a audio/video transcription service. VideoToBe started as a user friendly Whisper wrapper — but is evolving into a full pipeline that extracts, summarizes, and structures insights from multimedia content.&lt;/p&gt;
    &lt;p&gt;Working on my lisp. I recently added delimited continuations, even wrote a blog post about it. Now I'm working on adding more control primitives. Just finished researching generators. I'm going to implement them as a separate interpreter of sorts.&lt;/p&gt;
    &lt;p&gt;An implementation of statecharts. I'm working through core functionality using recursive algorithms.&lt;/p&gt;
    &lt;p&gt;I discovered that "least common ancestor" boils down to the intersection of 'root-path' sets, where you select the last item in the set as the 'first/least common ancestor'.&lt;/p&gt;
    &lt;p&gt;The main idea is to bring as many of the agentic tools and features into a single cohesive platform as much as possible so that we can unlock more useful AI use-cases.&lt;/p&gt;
    &lt;p&gt;Building a new layer of hyper-personalization over the web. Instead of generating more content, it helps you reformat and interact with what already exists, turning any page, paper, or YouTube video into a summary, mind-map, podcast, infographic or chat.&lt;/p&gt;
    &lt;p&gt;The broader idea is to make the web adaptive to how each person thinks and learns.&lt;/p&gt;
    &lt;p&gt;-Many say they want to stop doomscrolling and clout-chasing but I don't know how many are actually willing to do so&lt;/p&gt;
    &lt;p&gt;-Individuals may move here but their friends won't. So the feed will be initially empty by design. Introducing any kind of reward is against our ethos so we are clueless about how to convince existing friend circles to move.&lt;/p&gt;
    &lt;p&gt;This may work in your favour - it's one of the reasons I enjoy Mastodon so much - friction is/was a little higher which kept my network small but focussed&lt;/p&gt;
    &lt;p&gt;On-site surveys for eCommerce and SaaS. It's been an amazing ride leveling up back and forth between product, design, and marketing. Marketing is way more involved than most people on this site realize...&lt;/p&gt;
    &lt;p&gt;I am working on a paper about solving the Royal Game of Ur, one of the world's oldest board games. We solved it a while ago, and are now trying to get more formal about it (https://royalur.net/solved).&lt;/p&gt;
    &lt;p&gt;Writing a course for a customer on how to use Claude Code well, especially around brownfield development (working on existing code bases, not so much around vibe-coding something new).&lt;/p&gt;
    &lt;p&gt;Headbang, a rhythm game that you play by bobbing your head while wearing Airpods while listening to music, is what I'm considering building next. The idea came from someone else using Airpods to create a racing game (RidePods).&lt;/p&gt;
    &lt;p&gt;Headbang concept sounds really fun, I'd love to play that as a fan of rhythm games, but wow my neck would hate me I think. I am no George Fisher in that regard&lt;/p&gt;
    &lt;p&gt;Hmm, a personal assistant of sorts that does evaluation of you to get to the bottom of who you really are. For very obvious reasons, it is a local only project and not exactly intended for consumption.&lt;/p&gt;
    &lt;p&gt;Beyond that, just regular random stuff that comes up here and there, but, for once, my hdd with sidelined projects is slowly being worked through.&lt;/p&gt;
    &lt;p&gt;I'm working on a workout tracker that you can actually use for things like TRX and gymnastic rings. Along with normal workouts too. Let me know if there's anything you'd like on there. https://gravitygainsapp.com/&lt;/p&gt;
    &lt;p&gt;An application that helps deaf and nonverbal individuals with daily interactions when they’re out and about.&lt;/p&gt;
    &lt;p&gt;My first career was in sales. And most of the time these interactions began with grabbing a sheet of paper and writing to one another. I think small LLMs can help here.&lt;/p&gt;
    &lt;p&gt;Currently making use of api’s but I think small models on phones will be good enough soon. Just completed my MVP.&lt;/p&gt;
    &lt;p&gt;This is a job board for AI jobs and companies. The job market in AI is pretty hot right now, and there are a lot of cool AI companies out there. I'm hoping to connect job seekers with fast-growing AI companies.&lt;/p&gt;
    &lt;p&gt;Lovely interface. This is quite impressive. I can't seem to get a terminal running though. Can I actually execute scripts here? I opened code and created a hello.py, terminal did not come up in Code either.&lt;/p&gt;
    &lt;p&gt;A mobile app that checks my email to find and extract family-related events/activities. The kind of things that are buried in a 12-point bullet list with font 8, inside of one of 10 school email messages received during the week&lt;/p&gt;
    &lt;p&gt;It runs fully on-device, including email classification and event extraction&lt;/p&gt;
    &lt;p&gt;AppGoblin is a free place to do app research for understanding which apps use which companies to monetize, track where data is sent and what kinds of ads are shown.&lt;/p&gt;
    &lt;p&gt;I'm building a mod for the game Subway Builder (http://subwaybuilder.com) that lets me undo/redo individual stations and tracks, instead of clearing all blueprints.&lt;/p&gt;
    &lt;p&gt;This is built with Rust, egui and SQLite3. The app has a downloader for NSE India reports. These are the daily end of day stock prices. Out of the box the app is really fast, which is expected but still surprises me. I am going to work on improving the stocks chart. I also want to add an AI assisted stocks analyst. Since all the stocks data is on the SQLite3 DB, I should be able to express my stocks screening ideas as plain text and let an LLM generate the SQL and show me in my data grid.&lt;/p&gt;
    &lt;p&gt;It was really interesting to generate it within 3 days. I had just a few places where I had to copy from app (std) log and paste into my prompt. Most of the time just describing the features was enough. Rust compiler did most of the heavy lifting. I have used a mix of Claude Code and OpenCode (with either GLM 4.5 or Grok Code Fast 1).&lt;/p&gt;
    &lt;p&gt;I have been generating full-stack web apps. I built and launched https://github.com/brainless/letsorder (https://letsorder.app/). Building full-stack web apps is basically building 2 apps (at a minimum) so desktop apps are way better it seems.&lt;/p&gt;
    &lt;p&gt;In the long-term, I plan to build and help others generated apps. I am building a vibe coding platform (https://github.com/brainless/nocodo). I have a couple early stage founders I consult for who take my guidance to generate their products (web and mobile apps + backend).&lt;/p&gt;
    &lt;p&gt;I should also point out - if you download the current version, you should immediately apply the update that will pop up. And even then, you're results may be flakey.&lt;/p&gt;
    &lt;p&gt;While working on Shelvica, a personal library management service and reading tracker, I realized I needed a source of data for book information, and none of the solutions available provided all the data I needed. One might provide the series, the other might provide genres, and yet another might provide a cover with good dimensions, but none provided everything.&lt;/p&gt;
    &lt;p&gt;So I started working on Librario, an ISBN database that fetches information from several other services, such as Hardcover.app, Google Books, and ISBNDB, merges that information, and return something more complete than using them alone. It also saves that information in the database for future lookups.&lt;/p&gt;
    &lt;p&gt;You can see an example response here[1]. Pricing information for books is missing right now because I need to finish the extractor for those, genres need some work[2], and having a 5 months old baby make development a tad slow, but the service is almost ready for a preview.&lt;/p&gt;
    &lt;p&gt;The algorithm to decide what to merge is the hardest part, in my opinion, and very basic right now. It's based on a priority and score system for now, where different extractors have different priorities, and different fields have different scores. Eventually, I wanna try doing something with machine learning instead.&lt;/p&gt;
    &lt;p&gt;I'd also like to add book summaries to the data somehow, but I haven't figured out a way to do this legally yet. For books in the public domain I could feed the entire book to an LLM and ask them to write a spoiler-free summary of the book, but for other books, that'd land me in legal trouble.&lt;/p&gt;
    &lt;p&gt;Oh, and related books, and things of the sort. But I'd like to do that based on the information stored in the database itself instead of external sources, so it's something for the future.&lt;/p&gt;
    &lt;p&gt;Last time I posted about Shelvica some people showed interest in Librario instead, so I decided to make it something I can sell instead of just a service I use in Shelvica[3], hence why I'm focusing more on it these past two weeks.&lt;/p&gt;
    &lt;p&gt;[2]: In the example you'll see genres such as "English" and "Fiction In English", which is mostly noise. Also things like "Humor", "Humorous", and "Humorous Fiction" for the same book.&lt;/p&gt;
    &lt;p&gt;[3]: Which is nice, cause that way there are two possible sources of income for the project.&lt;/p&gt;
    &lt;p&gt;Working on https://JobBoardSearch.com a meta directory of job boards helping job site owners with their DR, visibility, jobs cross posting and promoting in general&lt;/p&gt;
    &lt;p&gt;I'm working on ServBay, a local development environment I built to end the constant pain of juggling different versions of program languages like Python, PHP, Node.js, Golang, Rust and so on, databases, and local SSL certificates.&lt;/p&gt;
    &lt;p&gt;It's an all-in-one toolkit with one-click version switching, automatic HTTPS for local domains, and an integrated mail catcher.&lt;/p&gt;
    &lt;p&gt;I've just rolled out some major updates: 1. Local AI Deployment: Now can run models like Llama 3 &amp;amp; Code Llama directly within ServBay. 2. Built-in Tunneling: Share the local sites with anyone on the internet, ngrok-style or frp or Cloudflare. 3. Windows is Live! The new Windows version is out and quickly reaching feature parity with our macOS app.&lt;/p&gt;
    &lt;p&gt;Next up is ServBay 2.0. I'm currently gathering feedback on features like deeper Docker integration and more flexible site configurations. I'd love to hear what the HN community thinks is important.&lt;/p&gt;
    &lt;p&gt;Currently running some finetuning experiments on non-verbal sounds to teach TTS how to laugh. I have had some success to add the necessary tags and tokens to multiple systems, but assembling the necessary dataset with sufficient quality is hard.&lt;/p&gt;
    &lt;p&gt;Working on an AI governance and security platform that gives security and GRC visibility into what AI tools people are actually using but also what is going into them.&lt;/p&gt;
    &lt;p&gt;It's a browser extension right now and the platform integrates with SSO providers and AI APIs, to help discover shadow AI, enforce policies and creates audit trails. Think observability for AI adoption but also Grammerly since we help coach endusers to better behavior/outcomes.&lt;/p&gt;
    &lt;p&gt;Early days but the problem is real, have a few design partners in the F500 already&lt;/p&gt;
    &lt;p&gt;- hiragana / katakana / number / time reading quizzes&lt;/p&gt;
    &lt;p&gt;- vocabulary quizzes based on wordlists you define and build&lt;/p&gt;
    &lt;p&gt;- learn and practice kanji anki-style (using FSRS algo)&lt;/p&gt;
    &lt;p&gt;- the coolest feature (imo) is a "reader": upload Japanese texts (light novels, children's books, etc), then translate them to your native language to practice your reading comprehension. Select text anywhere on the page (with your cursor) to instantly do a dictionary lookup. A LLM evaluates your translation accuracy (0..100%) and suggests other possible interpretations.&lt;/p&gt;
    &lt;p&gt;I just revamped the UI look and feel the other day after implementing some other user feedback! I'm now exploring ads as a way to monetize it.&lt;/p&gt;
    &lt;p&gt;Camera Search (camerasearch.ai) is my iOS app for tradespeople and DIY users. It combines voice, video, image understanding, and chat—backed by tuned LLM API—to help diagnose issues and guide builds/repairs in realtime.&lt;/p&gt;
    &lt;p&gt;a tool to help California home owners to lower their property taxes. This works for people who bought in the past years low interest environment and are overpaying in taxes because of that.&lt;/p&gt;
    &lt;p&gt;Feel free to email me, if you have questions: phl.berner@gmail.com&lt;/p&gt;
    &lt;p&gt;I just tried your app and after providing my email the analysis I get is for a completely different address than what I entered. I tried twice just to make sure the address i entered was right.&lt;/p&gt;
    &lt;p&gt;Working on dev tools for MCP servers. As a building block I recently published a library to help write tests for MCPs - https://facetlayer.github.io/expect-mcp/&lt;/p&gt;
    &lt;p&gt;(you could fix your link so it's clickable) 1. thanks for building this. I will get back on my iron deficiency diet. I now understand it takes over 7 weeks to reliably fix 2. when doing data input, I'm lazy, especially for the blood age calc. So my process is: upload list + my blood results to the LLM and spit out the list of values I need (terrible privacy job right here for me) but anyway, I wonder if you could offer another route for data input, like a text field, with the full list and empty values, that I could copy to an LLM and ask to populate with my results and then spit back to paste into the form. Keep up the good work!&lt;/p&gt;
    &lt;p&gt;A fucking 16bit robot controller in the Australian outback. Because the contractor who provided the robot didn't provide a proper PLC, nor a controller. And hitting your target with 16bit on 80m sounds like a moonlander&lt;/p&gt;
    &lt;p&gt;Working on securing software against backdoors and hidden exploits using a set of debloating tools. First one available here: github.com/negativa-ai/BLAFS&lt;/p&gt;
    &lt;p&gt;Any chance you'll take a look at power tools next?&lt;/p&gt;
    &lt;p&gt;There are some Amish people who rebuild Dewalt, Milwaukee etc battery packs. I'd like a repairable/sustainable platform where I can actually check the health of the battery packs and replace worn out cells as needed.&lt;/p&gt;
    &lt;p&gt;To give you an idea of the market, original batteries are about $149, and their knockoffs are around $100.&lt;/p&gt;
    &lt;p&gt;Very nice, looking forward to a deal with Décath' ;) How hard is it to make it compatible with the various motors when there is communication involved?&lt;/p&gt;
    &lt;p&gt;I've been wondering for a while if the display on ebikes could also be a more open and durable part of it.&lt;/p&gt;
    &lt;p&gt;I'm working on mTOR (https://mtor.club), a free, science-based workout tracker I built to automate progressive overload. It's a local-first PWA that works completely offline, syncs encrypted between your devices using, passwordless passkeys, and allows for plan sharing via a simple link.&lt;/p&gt;
    &lt;p&gt;The core idea is to make progression easier to track and follow. After a workout, it analyzes your performance (weight, reps, and RIR), highlights new personal records (PRs), and generates specific targets for your next session. It also reviews your entire program to provide scientific analysis on weekly volume, frequency, and recovery for each muscle group. This gets displayed visually on an anatomy model to help you learn which muscles are involved, and you can track your gains over time with historical performance charts for each exercise.&lt;/p&gt;
    &lt;p&gt;During a workout, you get a total session timer, an automatic rest timer, and can see your performance from the last session for a clear target to beat. It automatically advances to the next incomplete exercise, and when you need to swap an exercise, it provides context-aware alternatives targeting the same muscles.&lt;/p&gt;
    &lt;p&gt;It's also deeply customizable:&lt;/p&gt;
    &lt;p&gt;- The UI has a dark theme, supports multiple languages (English, Spanish, German), lets you adjust the UI scale, and toggle the visibility of detailed muscle names, exercise types, historical performance badges, and a full history card. - You can set global defaults for weight units (kg/lbs), rest times, and plan targets, or enable/disable metrics like Reps in Reserve (RIR) and estimated 1-Rep Max. The exercise library can be filtered by your available equipment, you can create your own custom exercises with global notes, and there's a built-in weight plate calculator. - The progression system lets you define default rep ranges and RIR targets, or create specific overrides for different lifts (e.g., a 3-5 rep range for strength, 10-15 for accessories). - Editing is flexible: you can drag-and-drop to reorder days, exercises, and sets, duplicate workout days, track unilateral exercises (left/right side), and enter data with a quick wheel picker.&lt;/p&gt;
    &lt;p&gt;Working on a original algorithm to explain human behavior from 3rd person perspective (1st stage). The whole research is divided into 6 stages.&lt;/p&gt;
    &lt;p&gt;In 2nd stage, I will mathematically establish the best course of action as an individual given the base theory.&lt;/p&gt;
    &lt;p&gt;In 3rd stage, I will explain common psychological phenomenon through the theory, things like narcissism, anxiety, self-doubt, how to forgive others, etc.&lt;/p&gt;
    &lt;p&gt;In 4th stage, I will explain how the theory is the fastest way to learn across multiple domains and anyone can become a generalist and critical thinker.&lt;/p&gt;
    &lt;p&gt;In 5th stage, I will explain how society will unfold if everyone can become generalist and critical thinker through the theory. And how this is the next big societal breakthrough like Industrial revolution.&lt;/p&gt;
    &lt;p&gt;In 6th and last stage, I will think about how to use this theory to make India the next superpower, as this theory can give us the demographic advantage.&lt;/p&gt;
    &lt;p&gt;A little library to define functions in English (through LLM of course; for TypeScript initially) and use these functions like ordinary (async) functions (calling &amp;amp; be called). Agents as functions and multi-step concurrent orchestration of agents with event loops, if fanciness is in order.&lt;/p&gt;
    &lt;p&gt;And an agentic news digest service which scrapes a few sources (like HackerNews) for technical news and create a daily digest, which you can instruct and skew with words.&lt;/p&gt;
    &lt;p&gt;I've struggled with adding evals to my AI agents for last few months, and felt that vibe evals should have a path to building a robust system down the line.&lt;/p&gt;
    &lt;p&gt;Working on a plugin for langfuse to create evals functions and dataset from ingested traces automatically, based on ad-hoc user feedback.&lt;/p&gt;
    &lt;p&gt;trying to build some opportunity for the VR/XR community with https://vr.dev&lt;/p&gt;
    &lt;p&gt;right now, it’s a better way to showcase your really specific industry skills and portfolio of 3D assets (i.e., “LinkedIn for VR/XR) with hiring layered on&lt;/p&gt;
    &lt;p&gt;starting to add onto the current perf analysis tools and think more about how to get to a “lovable for VR/XR”&lt;/p&gt;
    &lt;p&gt;YouTube's algorithm is all about engagement - more video game videos, more brainrot, their algorithm doesn't care about the content as long as the kid is watching.&lt;/p&gt;
    &lt;p&gt;My system allows parents to define their children's interests (e.g., a 12-year-old who enjoys DIY engineering projects, Neil deGrasse Tyson, and drawing fantasy figures)&lt;/p&gt;
    &lt;p&gt;.. and specify how the AI should filter video candidates (e.g., excluding YouTube Shorts).&lt;/p&gt;
    &lt;p&gt;Periodically, the system prompts the child with something like&lt;/p&gt;
    &lt;p&gt;"Tell me about your favorite family vacation."&lt;/p&gt;
    &lt;p&gt;And their response to that prompt provides the system with more ideas and interests to suggest videos to them.&lt;/p&gt;
    &lt;p&gt;email me if you'd like to test jim.jones1@gmail.com&lt;/p&gt;
    &lt;p&gt;iOS/Mac app for learning Japanese by reading, all in one solution with optional Anki integration&lt;/p&gt;
    &lt;p&gt;I went full-time on this a couple years ago. I’m now doing a full iOS 26 redesign, just added kanji drawing, and am almost done adding a manga mode via Mokuro. I’m also preparing influencer UGC campaigns as I haven’t marketed it basically at all yet.&lt;/p&gt;
    &lt;p&gt;Currently building out an application to help me allocate project funding in an academic setting to contracts and scientific staff. It'll be for internal use first, depending on my motivation I might release it at one point. The main features will be the management of split contracts (30% project A, 70% project B), pay grade progressions (German system), handling of unique spending and budget requirements from funding agencies (also for now only Germany + EU Funding), and reporting features for internal planning. I am not a computer scientist, so we will see how this goes and if it can replace the currently used disgusting excel sheet.&lt;/p&gt;
    &lt;p&gt;https://revise.io - a new word processor with live collaboration, git-like revision history, and an AI agent like Cursor.&lt;/p&gt;
    &lt;p&gt;Basically, an agentic platform for working with rich text documents.&lt;/p&gt;
    &lt;p&gt;I’ve been building this solo since May and having so much fun with it. I created a canvas renderer and all of the word processor interactions from scratch so I can have maximum control over how things are display when it comes to features like AI suggestions and other more novel features I have planned for the future.&lt;/p&gt;
    &lt;p&gt;Working on revamping our calculator page on Levels.fyi to make it more useful to see refreshers and stock growth over time. Check it out at https://levels.fyi/calculator/&lt;/p&gt;
    &lt;p&gt;Kind of have been wasting time with Cloudflare workers engine. Trying to build a system that schedules these workers for a lightweight alternative to GitHub actions. If you are interested in WASM feel free to reach out. Looking to connect with other developers working on the WASM space.&lt;/p&gt;
    &lt;p&gt;I think getting a clear picture about what it is about yourself that needs work is actually a lot of the real work. Much of the rest of it is picking a direction and then living in that direction.&lt;/p&gt;
    &lt;p&gt;I am still [0] working on trying to recover who I was before whatever -- a couple of years ago -- rendered me progressively unable to concentrate on anything.&lt;/p&gt;
    &lt;p&gt;Last month was an improvement. This month I can't concentrate for long and I distract very easily, but I seem to be able to do more with what I have, A small sense of ambition that I might be able to do bigger things, and might not need to drop out of tech and get a simple job, is returning.&lt;/p&gt;
    &lt;p&gt;I am trying to use this inhibited, fractured state to clarify thoughts about useless technology and distractions, and about what really matters, because (without wishing to sound haughty) I used to be unusually good at a lot of tech stuff, and now I am not. It is sobering but it is also an insight into what it might be like to be on the outside of technology bullshit, looking in.&lt;/p&gt;
    &lt;p&gt;To prove my expertise in anything from infrastructure, over backend to frontend, I learned how to use terraform to provision a managed kubernetes cluster (on oracle clouds excellent free forever tier).&lt;/p&gt;
    &lt;p&gt;I am currently developing a web app consisting of a spring/kotlin backend for an angular frontend that is meant to provide a UI for kubectl. It has oAuth login and allows you to store several kubernetes configs, select which one to use and makes it unnecessary to remember all the kubectl commands I can never remember.&lt;/p&gt;
    &lt;p&gt;It's what I'd like to have if I had to interact with a kubernetes cluster at work. Yes, I know there are several kubernetes UIs already, but remember, this is for 1) learning and 2) following through and completing a project at least somewhat.&lt;/p&gt;
    &lt;p&gt;Working on https://run-phx.com ... a guide to trail running in the Valley of the Sun with notable routes, curated by actual human beings in the running community. (whoa)&lt;/p&gt;
    &lt;p&gt;Not earth shattering, but something that should exist.&lt;/p&gt;
    &lt;p&gt;I have been trying to study Chinese on my own for a while now and found it very frustrating to spend half the time just looking for simple content to read and listen to. Apps and websites exist, but they usually only have very little content or they ramp up the difficulty too quickly.&lt;/p&gt;
    &lt;p&gt;Now that LLMs and TTS are quite good I wanted to try it out for languages learning. The goal is to create a vast number of short AI-generated stories to bridge the gap between knowing a few characters and reading real content in Chinese.&lt;/p&gt;
    &lt;p&gt;Curious to see if it is possible to automatically create stories which are comfortable to read for beginners, or if they sound too much like AI-slop.&lt;/p&gt;
    &lt;p&gt;Custom rack mount enclosure for the low-cost metal 3D printer controller, and debugging slicer software in Blender geometry nodes. Had to abandon classic CAM format export as the complexity of the tooling ballooned for various reasons.&lt;/p&gt;
    &lt;p&gt;Still reducing design costs of a micro positing stage for hobbyists. I observed the driver motion was mostly synchronous and symmetric... Accordingly, given the scale only a single multiplexed piezoelectric actuator motor driver was actually needed, and cut that part of the design cost by 75%.&lt;/p&gt;
    &lt;p&gt;Still designing various test platforms to validate other key technologies. Sorry, no spoilers =3&lt;/p&gt;
    &lt;p&gt;Currently I've been working on a CLI tool [1] for my WebASM UI library [2] with the idea that all the gluecode generating stuff is abstracted away in nice CLI wizards.&lt;/p&gt;
    &lt;p&gt;Essentially like yeoman back then, to bootstrap your webapp and all the necessary files more easily.&lt;/p&gt;
    &lt;p&gt;Currently I am somewhat stuck because of Go's type system, as the UI components require a specific interface for the Dataset or Data/Record entries.&lt;/p&gt;
    &lt;p&gt;For example, a Pie chart would require a map[string]number which could be a float, percentage string or an integer.&lt;/p&gt;
    &lt;p&gt;A Line chart would require a slice of map[string]number, where each slice index would represent a step in the timeline.&lt;/p&gt;
    &lt;p&gt;A table would require a slice of map[string]any where each slice index would represent a step in the culling, but the data types would require a custom rendering method or Stringifier(?) of sorts attached to the data type. So that it's possible to serialize or deserialize the properties (e.g. yes/no in the UI meaning true/false, etc).&lt;/p&gt;
    &lt;p&gt;As I want to provide UI components that can use whatever struct the developer provides, the Go way would be to use an interface. But that would imply that all data type structs on the backend side would have this type of clutter on them attached.&lt;/p&gt;
    &lt;p&gt;No idea if something like a Parser and Stringifier method definition would make more sense for the UI components here...or whether or not it's better to have something like a Render method attached per component that does all the stringifying on a per-property basis like a "func(dataset any, index int, column string) string" where the developer needs to do all the typecasting manually.&lt;/p&gt;
    &lt;p&gt;Manual typecasting like this would be pretty painful as components then cannot exist in pure HTML serialized form, which is essentially the core value proposition of my whole UI components framework.&lt;/p&gt;
    &lt;p&gt;An alternative would be offering a marshal/unmarshal API similar to how JSON does it, but that would require the reflect package which bloats up the runtime binary by several MB and wouldn't be tinygo compatible, so I heavily would wanna avoid that.&lt;/p&gt;
    &lt;p&gt;Currently looking for other libraries and best practices, as this issue is really bugging me a lot in the app I'm currently building [3] and it's a pretty annoying type system problem.&lt;/p&gt;
    &lt;p&gt;Feedback as to how it's solved in other frameworks or languages would be appreciated. Maybe there's an architectural convention I'm not aware of that could solve this.&lt;/p&gt;
    &lt;p&gt;I have been building OpenRun, a declarative web app deployment platform https://github.com/openrundev/openrun. It is an open source alternative to Google Cloud Run and AWS App Runner, running on your own hardware.&lt;/p&gt;
    &lt;p&gt;OpenRun allows defining your web app configuration in a declarative config using Starlark (which is like a subset of Python). Setting up a full GitOps workflow is just one command:&lt;/p&gt;
    &lt;p&gt;This will set up a scheduled sync, which will look for new apps in the config and create them. It will also apply any config updates on existing apps and reload apps with the latest source code. After this, no further CLI operations are required, all updates are done declaratively. For containerized apps, OpenRun will directly talk to Docker/Podman to manage the container build and startup. There are lots of tools which simplify web app deployment. Most of them use a UI driven approach or an imperative CLI approach. That makes it difficult to recreate an environment. Managing these tools when multiple people need to coordinate changes is also difficult.&lt;/p&gt;
    &lt;p&gt;Any repo which has a Dockerfile can be deployed directly. For frameworks like Streamlit/Gradio/FastHTML/Shiny/Reflex/Flask/FastAPI, OpenRun supports zero-config deployments, there is no need to even have a Dockerfile. Domain based deployment is supported for all apps. Path based deployment is also supported for most frameworks, which makes DNS routing and certificate management easier.&lt;/p&gt;
    &lt;p&gt;OpenRun currently runs on a single machine with an embedded SQLite database or on multiple machines with an external Postgres database. I plan to support OpenRun as a service on top of Kubernetes, to support auto-scaling. OpenRun implements its own web server, instead of using Traefik/Nginx. That makes it possible to implement features like scaling down to zero and RBAC. The goal with OpenRun is to support declarative deployment for web apps while removing the complexity of maintaining multiple YAML config files. See https://github.com/openrundev/openrun/blob/main/examples/uti... for an example config, each app is just one or two lines of config.&lt;/p&gt;
    &lt;p&gt;OpenRun makes it easy to set up OAuth/OIDC/SAML based auth, with RBAC. See https://openrun.dev/docs/use-cases/ for a couple of use cases examples: sharing apps with family and sharing across a team. Outside of managed services, I have found it difficult to implement this type of RBAC with any other open source solution.&lt;/p&gt;
    &lt;p&gt;I'm working on a set of TypeScript libraries to make it really really easy to spin up an agent, or an chatbot, or pretty much anything else you want to prototype. It's based around sensible interfaces, and while batteries are included, they're also meant to be removed when you've got something you want.&lt;/p&gt;
    &lt;p&gt;The idea is that a beginner should be able to wire up a personally useful agent (like a file-finder for your computer) in ten minutes by writing a simple prompt, some simple tools, and running it. Easy to plugin any kind of tracing, etc you want. Have three or four projects in prod which I'll be switching to use it just to make sure it fits all those use-cases.&lt;/p&gt;
    &lt;p&gt;But I want to be able to go from someone saying "can we build an agent to" to having the PoC done in a few minutes. Everything else I've looked at so far seems limited, or complicated, or insufficiently hackable for niche use-cases. Or, worse of all, in Python.&lt;/p&gt;
    &lt;p&gt;I'm working on a card game for android, it's being built with Monogame and C#. It's just go fish at the moment, but I'm thinking of expanding it into a full suite of card games like solitaire and poker. The source is available on GitHub if anyone wants to poke around and perhaps collaborate. https://github.com/joshsiegl1/GoFishRefresh&lt;/p&gt;
    &lt;p&gt;A tool for threshold signing software releases that I eventually want to integrate with SigStore, etc. to help folks distribute their code-signing. https://github.com/soatok/freeon&lt;/p&gt;
    &lt;p&gt;-----&lt;/p&gt;
    &lt;p&gt;Want E2EE for Mastodon (and other ActivityPub-based software), so you can have encrypted Fediverse DMs? I've been working on the public key transparency aspect of this too.&lt;/p&gt;
    &lt;p&gt;It's an AI-webapp builder with a twist: I proxy all OpenAI API calls your webapp makes and charge 2x the token rate; so when you publish your webapp onto a subdomain, the users who use your webapp will be charged 2x on their token usage. Then you, the webapp creator, gets 80% of what's left over after I pay OpenAI (and I get 20%).&lt;/p&gt;
    &lt;p&gt;It's also a fun project because I'm making code changes a different way than most people are: I'm having the LLM write AST modification code; My site immediately runs the code spit out by the LLM in order to make the changes you requested in a ticket. I blogged about how this works here: https://codeplusequalsai.com/static/blog/prompting_llms_to_m...&lt;/p&gt;
    &lt;p&gt;I'm building an open source NAT traversal and networking framework called P2PD. Built from the ground up to allow things like multi-network interface applications, improved network programming in Python, and if people want it: an easy way to bypass NATs. The thing is: it depends on public servers for some of this which tends to change a lot, causing errors when they're all down.&lt;/p&gt;
    &lt;p&gt;What I'm building at the moment is a server monitoring solution for STUN, TURN, MQTT, and NTP servers. I wanted to allow the software for this to be portable. So I wrote a simple work queue myself. Python doesn't have linked-lists which is the data structure I'm using for the queues. They allow for O(1) deletes which you can't really get on many Python data structures. Important for work items when you're moving work between queues.&lt;/p&gt;
    &lt;p&gt;For the actual workers I keep things very simple. I make like 100 independent Python processes each with an event loop. This uses up a crap load of memory but the advantage is that you can parallel execution without any complexity. It would be extremely complex trying to do that with code alone and asyncio's event loop doesn't play well with parallelism. So you really only want one per process.&lt;/p&gt;
    &lt;p&gt;Result: simple, portable Python code that can easily manage monitoring hundreds of servers (sorry didnt mean for that to sound like chatgpt, lmao, incidental.) The DB for this is memory-based to avoid locking issues. I did use sqlite at first but even with optimizations there were locking issues. Now, I only use sqlite for import / export (checksums.)&lt;/p&gt;
    &lt;p&gt;yet another nvr (in python). also trying to make a switch for hpm style rocker light switches. these things are devilish. the switch requires a lot of force at a strange angle but i dont want to break it so knowing nothing about mechanical stuff ive had to learn about slip clutches, idling gears, worm gears, ratchet wheels. rack and pinions (ofc. from a hobbyist perspective). i know theres a switchbot and a fingerbot but neither of those will work with that type of switch unless you tape some sort of torque lever onto the light (which i dont want to do). its a rabbit hole :/&lt;/p&gt;
    &lt;p&gt;Here's a breakdown of the projects people are working on, with AI-related projects in their own category:&lt;/p&gt;
    &lt;p&gt;## AI-Related Projects&lt;/p&gt;
    &lt;p&gt;* *[justinc8687] Migraine Tracker:* This project aims to help users track their migraines using voice input, with the goal of analyzing unstructured data with AI to find root causes. It uses Deepgram for transcription and an LLM for analysis, with a "chat with your data" feature. * *[dcheong] User Mastery:* A platform for product teams to manage updates, changelogs, roadmaps, documentation, and feedback, utilizing AI to assist. * *[jared_stewart] Survey Response Automation:* Using LLMs to automate the processing of parent survey responses for a school, aiming for consistent summarization and statistics. * *[codybontecou] Voice-Script:* A tool that allows users to discuss and generate GitHub issues, pull requests, and code diffs using ChatGPT's voice agents. * *[conditionnumber] LLM for Data Matching:* Proposes using an LLM to score and match candidates identified by a tool like "jellyjoin," reducing a large number of potential matches to a manageable set for AI analysis. * *[taherchhabra] Infinite Canvas for AI Generation:* A platform for AI image, video, audio, and 3D generation, designed to help create cohesive stories with consistent characters and locations. * *[chipotle_coyote] Story Theory Program (Spiritual Successor to Dramatica):* Aims to create a story theory and brainstorming program, drawing inspiration from Dramatica but incorporating modern concepts, and potentially using AI for some aspects. * *[rhl314] Magnetron (Whiteboard Explainers):* An AI-powered tool that generates whiteboard explainer videos from prompts or documents, using AI for design, animations, and voiceovers. * *[adamsaparudin] AI SaaS Workflow:* A project focused on enabling users to launch their own AI SaaS applications quickly, abstracting away complexities like user management and billing. * *[garbage] Dreamly.in (AI Bedtime Stories):* An automated, personalized, and localized bedtime story generator for children, using AI to create stories based on child profiles and themes. * *[nowittyusername] Metacognitive AI System:* This project focuses on creating an AI agent with multiple specialized LLMs that can reason, analyze, and communicate internally to provide more sophisticated responses to humans, rather than just acting as a simple chatbot. * *[fjulian] Veila (Privacy-First AI Chat):* A privacy-focused AI chat service that uses a proxy to prevent user profiling and offers end-to-end encrypted history, allowing users to switch models mid-chat. * *[ai-christianson] Gobii Platform (Open-Source AI Employees):* Browser-based AI agents that can log into real websites, fill out forms, and adapt to changes, functioning as "browser-use cloud" employees. * *[apf6] Dev Tools for MCP Servers:* Building libraries to help write tests for MCP (Model-Centric Programming) servers, focusing on AI-related development. * *[mfrye0] Plaid/Perplexity for Business Data:* Creating composable deep research APIs powered by a business graph and web search index to integrate business data into applications and AI agent processes. * *[vishakh82] Monadic DNA Explorer:* A tool to explore genetic traits from GWAS Catalog and user DNA data, with AI insights run locally in a TEE (Trusted Execution Environment). * *[jerrygoyal] JetWriter.ai:* A Chrome extension that uses AI to assist with tasks on any website, such as chatting with pages, fixing grammar, replying to emails, translating, and summarizing. * *[chadwittman] Eldrick.golf (AI Golf Club Fitter):* An AI-powered golf club fitting tool that aims to rival human professionals in custom club fitting. * *[jiffylabs] AI Governance and Security Platform:* A platform and browser extension to provide visibility into AI tool usage within organizations, discover shadow AI, enforce policies, and create audit trails. It also acts as a coach for end-users. * *[aantix] Alternative YouTube App for Kids:* An app that uses AI to filter YouTube videos based on parental-defined interests and prompts children for input to discover new interests, moving away from engagement-driven algorithms. * *[qwikhost] Video AI Editor:* A tool for editing videos using AI. * *[accountisha] CPA Exam Prep Tool:* A system that generates word problems and step-by-step solutions to help individuals prepare for the American CPA exams. * *[felixding] Kintoun.ai:* A simple document translator that preserves file formatting and layout, likely using AI for translation. * *[skyfantom] LLM + Stocks Market Analysis:* Experimenting with LLMs for stock market analysis and comparing different models for their effectiveness. * *[braheus] English-to-Function Definition (LLM):* A library that allows defining functions in English using an LLM, which can then be used like regular TypeScript functions, enabling agentic orchestration. * *[gametorch] AI Sprite Animator:* An AI-powered tool for animating sprites in 2D video games. * *[sab_hn] Endless Chinese:* An AI-generated story platform for learning Chinese, aiming to create a vast number of short stories for beginners. * *[asdev] FleetCode (Coding Agent Control Panel):* An open-source control panel for running coding agents in parallel. * *[trogdor] AI Document Summarization/Analysis:* A tool that uses AI to analyze documents and provide summaries, potentially for research or other forms of content consumption. * *[osint.moe] LLM-Powered OSINT Helper:* An app that uses LLMs to build an interactive research graph for Open Source Intelligence (OSINT) gathering. * *[kintoun.ai] Document Translator:* A tool that translates documents while preserving formatting and layout, likely leveraging AI. * *[mclaren] AI-powered code generation and analysis tools.* * *[skanga] Conductor (LLM-Agnostic Framework):* A framework for building sophisticated AI applications using a subagent architecture, inspired by concepts of "The Rise of Subagents." * *[ashdnazg] Palindrome Finding (CUDA):* Porting code to CUDA to find palindromes, with a focus on GPU optimization and exploring new elements in number series. * *[veesahni] AI in Customer Communications:* Exploring effective, hype-free usage of AI in customer communications. * *[cryptoz] Code+=AI (AI Webapp Builder):* A platform for building AI web apps where API calls are proxied, and users are charged for token usage, with creators earning a percentage of the revenue. The LLM is also used to modify code. * *[exasperaited] Recovering from Cognitive Impairment:* Using AI tools to help clarify thoughts and potentially recover cognitive abilities lost due to a past event. * *[waxycaps] CEO Replacement:* A project related to AI that has the goal of replacing a CEO. * *[vladoh] Simple Photo Gallery (V2):* While not AI-specific, the mention of a future SaaS offer for users who don't want to self-host suggests potential for AI-driven features in the future. * *[dheera] Invoice Generators for "Inconvenience Fees":* While not directly AI, the idea of invoicing for "inconvenience fees" could be an interesting application for AI to determine and quantify such fees. * *[yomismoaqui] HN Post/Comment Analyzer:* A website for analyzing posts and comments on Hacker News, potentially using AI to filter or summarize content. * *[ce0.ai] CEO Replacement:* A project explicitly stating it's about replacing a CEO with AI. * *[robinsloan] Home-cooked App Essay Inspiration:* While not directly an AI project, the mention of this essay and the focus on personal apps could lead to AI-integrated personal tools. * *[zuhayeer] Levels.fyi Calculator Revamp:* Focusing on improving a calculator page for refreshers and stock growth, which could involve AI for analysis or predictions. * *[lukehan] AI Data Enrichment Platform:* A platform to help users enrich their data so AI, like ChatGPT, can understand it better, measured by an "AI Understanding Score." * *[asimovDev] Sound Blaster Command Control:* While primarily reverse engineering, the mention of "creative's multiplatform solutions" could imply future AI integration for smarter control. * *[daveevad] "Myself, myself needs work":* This self-reflection could involve AI tools for personal development or understanding oneself better. * *[thenipper] Campaign Management App for TTRPGs:* While primarily a wiki-like app, the potential for AI to assist in game mastering or content generation is present. * *&lt;/p&gt;
    &lt;p&gt;new PostgreSQL index type which outperforms B-Trees for many common cases. As a wild experiment, I'm entirely vibe coding this and not hand-writing it.&lt;/p&gt;
    &lt;p&gt;It works by specializing for the common case of read-only workloads and short, fixed-length keys/includes (int, uuid, text&amp;lt;=32b, numeric, money, etc - not json) and (optionally) repetitive key-values (a common case with short fixed-length keys). These kinds of indexes/tables are found in nearly every database for lookups, many-many JOIN relationships, materialized views of popular statistics, etc.&lt;/p&gt;
    &lt;p&gt;Currently, it's "starting to work" with 100% code coverage and performance that usually matches/beats btree in query speed. Due to compression, it can consume as little as 99.95% less memory (!) and associated "pressure" on cache/ram/IO. Of course, there are degenerate cases (e.g. all unique UUID, many INCLUDEs, etc) where it's about the same size. As with all indexes, performance is limited by the PostgreSQL executor's interface which is record-at-a-time with high overhead records. I'd love help coding a FDW which allows aggregates (e.g. count()) to be "pushed down" and executed in still requires returning every record instead of a single final answer. OTT help would be a FDW interface where substantial query plans could be "pushed down" e.g. COUNT().&lt;/p&gt;
    &lt;p&gt;The plan is to publish and open source this work.&lt;/p&gt;
    &lt;p&gt;I'd welcome collaborators and have lots of experience working on small teams at major companies. I'm based in NYC but remote is fine.&lt;/p&gt;
    &lt;p&gt;- must be willing to work with LLMs and not "cheat" by hand-writing code.&lt;/p&gt;
    &lt;p&gt;- Usage testing: must be comfortable with PostgreSQL and indexes. No other experience required!&lt;/p&gt;
    &lt;p&gt;- Benchmarking, must know SQL indexes and have benchmarking experience - no pgsql internals required.&lt;/p&gt;
    &lt;p&gt;- For internals work, must know C and SQL. PostgreSQL is tricky to learn but LLMs are excellent teachers!&lt;/p&gt;
    &lt;p&gt;- Scripting code is in bash, python and Makefile, but again this is all vibe coded and you can ask LLMs what it's doing.&lt;/p&gt;
    &lt;p&gt;- any environment is fine. I'm using linux/docker (multi-core x86 and arm) but would love help with Windows, native MacOS and SIMD optimization.&lt;/p&gt;
    &lt;p&gt;- I'm open to porting/moving to Rust, especially if that provides a faster path to restricted environments like AWS RDS/Aurora.&lt;/p&gt;
    &lt;p&gt;- your ideas welcome! but obviously, we'll need to divide and conquer since the LLMs are making rapid changes to the core and we'll have to deal with code conflicts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45561428"/><published>2025-10-12T20:09:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45561672</id><title>Emacs agent-shell (powered by ACP)</title><updated>2025-10-13T14:11:05.098960+00:00</updated><content>&lt;doc fingerprint="35d78c5f8911c81d"&gt;
  &lt;main&gt;
    &lt;p&gt;Not long ago, I introduced acp.el, an Emacs lisp implementation of ACP (Agent Client Protocol), the agent protocol developed between Zed and Google folks.&lt;/p&gt;
    &lt;p&gt;While I've been happily accessing LLMs from my beloved text editor via chatgpt-shell (a multi-model package I built), I've been fairly slow on the AI agents uptake. Probably a severe case of old-man-shouts-at-cloud sorta thing, but hey I want well-integrated tools in my text editor. When I heard of ACP, I knew this was the thing I was waiting for to play around with agents.&lt;/p&gt;
    &lt;p&gt;With an early acp.el client library in place, I set out to build an Emacs-native agent integrationâ¦ Today, I have an initial version of agent-shell I can share.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;agent-shell&lt;/code&gt; is a native Emacs shell, powered by comint-mode (check out Mickey's comint article btw). As such, we don't have to dance between char and line modes to interact with things. &lt;code&gt;agent-shell&lt;/code&gt; is just a regular Emacs buffer like any other you're used to.&lt;/p&gt;
    &lt;p&gt;Thanks to ACP, we can now build agent-agnostic experiences by simply configuring our clients to communicate with their respective agents using a common protocol. As users, we benefit from a single, consistent experience, powered by any agent of our choice.&lt;/p&gt;
    &lt;p&gt;Configuring different agents from &lt;code&gt;agent-shell&lt;/code&gt; boils down which agent we want running in the comms process. Here's an example of Gemini CLI vs Claude Code configuration:&lt;/p&gt;
    &lt;code&gt;(defun agent-shell-start-gemini-agent ()
  "Start an interactive Gemini CLI agent shell."
  (interactive)
  (agent-shell--start
   :new-session t
   :mode-line-name "Gemini"
   :buffer-name "Gemini"
   :shell-prompt "Gemini&amp;gt; "
   :shell-prompt-regexp "Gemini&amp;gt; "
   :needs-authentication t
   :authenticate-request-maker (lambda ()
                                 (acp-make-authenticate-request :method-id "gemini-api-key"))
   :client-maker (lambda ()
                   (acp-make-client :command "gemini"
                                    :command-params '("--experimental-acp")
                                    :environment-variables (list (format "GEMINI_API_KEY=%s" (agent-shell-google-key)))))))
&lt;/code&gt;
    &lt;code&gt;(defun agent-shell-start-claude-code-agent ()
  "Start an interactive Claude Code agent shell."
  (interactive)
  (agent-shell--start
   :new-session t
   :mode-line-name "Claude Code"
   :buffer-name "Claude Code"
   :shell-prompt "Claude Code&amp;gt; "
   :shell-prompt-regexp "Claude Code&amp;gt; "
   :client-maker (lambda ()
                   (acp-make-client :command "claude-code-acp"
                                    :environment-variables (list (format "ANTHROPIC_API_KEY=%s" (agent-shell-anthropic-key)))))))
&lt;/code&gt;
    &lt;p&gt;I've yet to try other agents. If you get another agent running, I'd love to hear about it. Maybe submit a pull request?&lt;/p&gt;
    &lt;p&gt;While I've been relying on my acp.el client library, I'm still fairly new to the protocol. I often inspect traffic to see what's going on. After staring at json for far too long, I figured I may as well build some tooling around acp.el to make my life easier. I added a traffic buffer for that. From &lt;code&gt;agent-shell&lt;/code&gt;, you can invoke it via &lt;code&gt;M-x agent-shell-view-traffic&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Developing &lt;code&gt;agent-shell&lt;/code&gt; against paid agents got expensive quickly. Not only expensive, but my edit-compile-run cycle also became boringly slow waiting for agents. While I knew I wanted some sort of fake agent to work against, I didn't want to craft the fake traffic myself. Remember that traffic buffer I showed ya? Well, I can now save that traffic to disk and replay it later. This enabled me to run problematic sessions once and quickly replay multiple times to fix things. While re-playing has its quirks and limitations, it's done the job for now.&lt;/p&gt;
    &lt;p&gt;You can see a Claude Code session below, followed by its replayed counterpart via fake infrastructure.&lt;/p&gt;
    &lt;p&gt;Getting here took quite a bit of work. Having said that, it's only a start. I myself need to get more familiar with agent usage and evolve the package UX however it feels most natural within its new habitat. Lately, I've been experimenting with a quick diff buffer, driven by n/p keys, shown along the permission dialog.&lt;/p&gt;
    &lt;code&gt;#+ATTR_HTML: :width 99%
&lt;/code&gt;
    &lt;p&gt;While I've implemented enough parts of the Agent Client Protocol Schema to make the package useful, it's hardly complete. I've yet to fully familiarize myself with most protocol features.&lt;/p&gt;
    &lt;p&gt;Both of my new Emacs packages, agent-shell and acp.el, are now available on GitHub. As an agent user, go straight to agent-shell. If you're a package author and would like to build an ACP experience, then give acp.el a try. Both packages are brand new and may have rough edges. Be sure to file bugs or feature requests as needed.&lt;/p&gt;
    &lt;p&gt;I've been heads down, working on these packages for some time. If you're using cloud LLM services, you're likely already paying for tokens. If you find my work useful, please consider routing some of those coins to help fund it. Maybe my tools make you more productive at work? Ask your employer to support the work. These packages not only take time and effort, but also cost me money. Help fund the work.&lt;/p&gt;
    &lt;p&gt;powered by LMNO.lol&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xenodium.com/introducing-agent-shell"/><published>2025-10-12T20:37:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45565646</id><title>HTTP3 Explained</title><updated>2025-10-13T14:11:04.597120+00:00</updated><content>&lt;doc fingerprint="bfdb5d5505963b2e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;README&lt;/head&gt;
    &lt;p&gt;HTTP/3 explained is a collaborative effort to document the HTTP/3 and the QUIC protocols. Join in and help!&lt;/p&gt;
    &lt;p&gt;Get the Web or PDF versions on http3-explained.haxx.se.&lt;/p&gt;
    &lt;p&gt;The contents get updated automatically on every commit to this git repository.&lt;/p&gt;
    &lt;p&gt;Last updated&lt;/p&gt;
    &lt;p&gt;Was this helpful?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://http3-explained.haxx.se"/><published>2025-10-13T07:15:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566123</id><title>LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives</title><updated>2025-10-13T14:11:04.238260+00:00</updated><content>&lt;doc fingerprint="ad9eb10045f3128f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Cryptography and Security&lt;/head&gt;&lt;p&gt; [Submitted on 4 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The widespread use of preprint repositories such as arXiv has accelerated the communication of scientific results but also introduced overlooked security risks. Beyond PDFs, these platforms provide unrestricted access to original source materials, including LaTeX sources, auxiliary code, figures, and embedded comments. In the absence of sanitization, submissions may disclose sensitive information that adversaries can harvest using open-source intelligence. In this work, we present the first large-scale security audit of preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates pattern matching, logical filtering, traditional harvesting techniques, and large language models (LLMs) to uncover hidden disclosures within non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25 state-of-the-art models. Our analysis uncovered thousands of PII leaks, GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders, editable private SharePoint links, exposed GitHub and Google credentials, and cloud API keys. We also uncovered confidential author communications, internal disagreements, and conference submission credentials, exposing information that poses serious reputational risks to both researchers and institutions. We urge the research community and repository operators to take immediate action to close these hidden security gaps. To support open science, we release all scripts and methods from this study but withhold sensitive findings that could be misused, in line with ethical principles. The source code and related material are available at the project website this https URL&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2510.03761"/><published>2025-10-13T08:33:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566139</id><title>Spotlight on pdfly, the Swiss Army knife for PDF files</title><updated>2025-10-13T14:11:02.891869+00:00</updated><content>&lt;doc fingerprint="d2c16a62e09737b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Project documentation: pdfly.readthedocs.io&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pdfly&lt;/code&gt; is the youngest project of the &lt;code&gt;py-pdf&lt;/code&gt; organization.
It has been created by Martin Thoma in 2022.&lt;/p&gt;
    &lt;p&gt;It's simply a CLI tool to manipulate PDF files, written in Python and based on the fpdf2 &amp;amp; pypdf libraries.&lt;/p&gt;
    &lt;p&gt;I'm a maintainer of the project 🙂&lt;/p&gt;
    &lt;head rend="h2"&gt;What can it do?&lt;/head&gt;
    &lt;p&gt;It has meany features, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;display PDF metadata using &lt;code&gt;pdfly meta&lt;/code&gt;and&lt;code&gt;pdfly pagemeta&lt;/code&gt;commands. Example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ pdfly meta minimal-document.pdf
                      Operating System Data
┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃         Attribute ┃ Value                     ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         File Name │ /tmp/minimal-document.pdf │
│  File Permissions │ -rw-r--r--                │
│         File Size │ 16,978 bytes              │
│     Creation Time │ 2025-10-13 09:44:32       │
│ Modification Time │ 2025-10-13 09:44:32       │
│       Access Time │ 2025-10-13 09:44:46       │
└───────────────────┴───────────────────────────┘
                       PDF Data
┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃          Attribute ┃ Value                                                    ┃
┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       CreationDate │ 2022-04-03 18:05:42+02:00                                │
│            Creator │ TeX                                                      │
│           Producer │ pdfTeX-1.40.23                                           │
│              Pages │ 1                                                        │
│          Encrypted │ None                                                     │
│   PDF File Version │ %PDF-1.5                                                 │
│        Page Layout │                                                          │
│          Page Mode │                                                          │
│             PDF ID │ ID1=b'q\x96\xc3\xe3U\xc1|\x9fS\xba\x9a\r\xcap\xcd\xd0'   │
│                    │ ID2=b'q\x96\xc3\xe3U\xc1|\x9fS\xba\x9a\r\xcap\xcd\xd0'   │
│ Fonts (embedded) │                                                          │
│   Fonts (embedded) │ /KNEUFH+CMR10                                            │
│        Attachments │ []                                                       │
│             Images │ 0 images (0 bytes)                                       │
└────────────────────┴──────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;pdfly&lt;/code&gt;can also combine files into new PDF documents: it can extract specific pages &amp;amp; merge documents (&lt;code&gt;pdfly cat&lt;/code&gt;); selectively remove pages (&lt;code&gt;pdfly rm&lt;/code&gt;); convert images to PDF documents (&lt;code&gt;pdfly x2pdf&lt;/code&gt;); and even compress documents (&lt;code&gt;pdfly compress&lt;/code&gt;) or build booklets (&lt;code&gt;pdfly 2-up&lt;/code&gt;&amp;amp;&lt;code&gt;pdfly booklet&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pdfly&lt;/code&gt;includes some commands to pull out specific content from PDF files:&lt;code&gt;pdfly extract-images&lt;/code&gt;&amp;amp;&lt;code&gt;pdfly extract-annotated-text&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;sometimes you want to edit a PDF file manually, in a text editor. But when you do so, you break its&lt;/p&gt;&lt;code&gt;xref&lt;/code&gt;table, that is an index of byte offsets in the document.&lt;code&gt;pdfly update-offsets&lt;/code&gt;is there to save the day, fixing manually-edited PDF documents, so that they can be opened in a PDF viewer again!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Release 0.5.0 &amp;amp; new features&lt;/head&gt;
    &lt;p&gt;Today we released a new version: &lt;code&gt;pdfly release 0.5.0&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Thanks to several contributors, including developers taking part in Hacktoberfest, new exciting features have been added:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;pdfly sign&lt;/code&gt;allows you to easily sign PDF documents, while&lt;code&gt;pdfly check-sign&lt;/code&gt;makes it easy to check a PDF document signature. Thanks to @moormaster for implementing this in PRs #165 &amp;amp; #166 👍🙏.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pdfly extract-annotated-pages&lt;/code&gt;extract only annotated pages from a PDF, hence helping to review or rework pages from a large document iteratively. Thanks to Hal Wine (@hwine) for implementing this in PR #128 👍🙏.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pdfly rotate&lt;/code&gt;rotate specific pages of a document. Thanks to Subhajit Sahu (@wolfram77) for implementing this in PR #98 👍🙏.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;We have a bunch of feature ideas: &lt;code&gt;up-for-grabs&lt;/code&gt; issues, including some &lt;code&gt;good first issues&lt;/code&gt; aimed specially at new contributors, that are willing to help but new to open-source.&lt;/p&gt;
    &lt;p&gt;Personally, I think the &lt;code&gt;pdfly sign&lt;/code&gt; &amp;amp; &lt;code&gt;check-sign&lt;/code&gt; could become handy to many end-users, and I think we should continue to extend those commands usage options, as described in issue #71.&lt;/p&gt;
    &lt;p&gt;We would also be happy to get your feedbacks, bug reports &amp;amp; feature suggestions! 🙂&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chezsoi.org/lucas/blog/spotlight-on-pdfly.html"/><published>2025-10-13T08:36:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566253</id><title>Some graphene firms have reaped its potential but others are struggling</title><updated>2025-10-13T14:11:02.760151+00:00</updated><content>&lt;doc fingerprint="6088806b0f7d57fb"&gt;
  &lt;main&gt;
    &lt;p&gt;After graphene was first produced at the University of Manchester in 2004, it was hailed as a wonder material, stronger than steel but lighter than paper. But two decades on, not every UK graphene company has made the most of that potential. Some show promise but others are struggling.&lt;/p&gt;
    &lt;p&gt;Extracted from graphite, commonly used in pencils, graphene is a latticed sheet of carbon one atom thick, and is highly effective at conducting heat and electricity. China is the world’s biggest producer, using it to try to get ahead in the global race to produce microchips and in sectors such as construction.&lt;/p&gt;
    &lt;p&gt;In the UK, a graphene-enhanced, low-carbon concrete was laid at a Northumbrian Water site in July, developed by the Graphene Engineering Innovation Centre (GEIC) at the University of Manchester and Cemex UK.&lt;/p&gt;
    &lt;p&gt;“The material when it came out of academia was hyped to death … but the challenge is going from lab to fab,” says Ben Jensen, the chief executive of 2D Photonics, a startup spun out from the University of Cambridge that makes graphene-based photonic technology for datacentres.&lt;/p&gt;
    &lt;p&gt;Jensen also invented Vantablack coatings, made of carbon nanotubes – rolled-up sheets of graphene – and known as the world’s “blackest black” because it absorbs 99.96% of light, at the UK company Surrey NanoSystems that he founded in 2007. The material’s artistic rights were sold exclusively to the sculptor Anish Kapoor, and BMW used it on its X6 coupe to create the “blackest black car” six years ago.&lt;/p&gt;
    &lt;p&gt;“This is the challenge when you have new materials trying to displace an incumbent technology,” Jensen says. “The value proposition must be extremely good, but there also must be a way to manufacture the material and manufacture it at scale for the application … then you have to meet price expectations because there’s no point in delivering something that’s costing 10 times more than the incumbent.”&lt;/p&gt;
    &lt;p&gt;Germany’s Bayer tried to produce carbon nanotube products in bulk but shut down its pilot factory more than a decade ago after the expected surge in demand failed to materialise. The material is now mainly used as a filler to strengthen plastic products. The company described the potential applications of nanotubes as “fragmented”.&lt;/p&gt;
    &lt;p&gt;More promising are the graphene-based optical microchips developed by 2D Photonics’ subsidiary CamGraPhIC, which are based on research done at the University of Cambridge and Italy’s CNIT research institute.&lt;/p&gt;
    &lt;p&gt;At the moment, silicon-photonics microchips convert electrical data into optical data to transmit it through fibre-optic cables, but the firm says its graphene chips deliver more data in the same period of time and at far lower cost.&lt;/p&gt;
    &lt;p&gt;They consume 80% less energy and can operate in a much wider range of temperatures, reducing the need for costly water- and energy-hungry cooling systems for AI datacentres.&lt;/p&gt;
    &lt;p&gt;Sending data via silicon also causes delays. Jensen compares it to a 16-lane motorway that suddenly narrows to a single lane because of roadworks, forcing everything to slow down. Graphene photonics, he says, are like a motorway with hundreds of lanes.&lt;/p&gt;
    &lt;p&gt;“What we’ve solved is the ability to grow consistent ultra high-performance graphene and to build it into a device,” he says. “And don’t forget, this is a material that’s one atom thick. It’s insanely difficult to do this.”&lt;/p&gt;
    &lt;p&gt;CamGraPhIC was founded in 2018 by the Cambridge nanotechnology professor Andrea Ferrari, who also runs the Cambridge graphene centre, and Marco Romagnoli, who leads advanced photonics at CNIT in Pisa and is the startup’s chief scientific officer.&lt;/p&gt;
    &lt;p&gt;Its parent, 2D Photonics, has just secured funding of £25m from backers including Italy’s sovereign wealth fund, Nato and Sony innovation funds, Bosch Ventures and the UK’s Frontier IP Group, and uses a former Pirelli photonics research site in Pisa. It plans to create a pilot manufacturing site in the Milan area to produce 200mm-wide wafers at scale, and is confident of obtaining the necessary funding of €317m (£276m) by the end of the year.&lt;/p&gt;
    &lt;p&gt;Aside from datacentres, the company’s chips can be used in high-performance computing, 5G and 6G mobile data systems, aircraft systems, autonomous cars, advanced digital radar and satellite-free space communications.&lt;/p&gt;
    &lt;p&gt;Paragraf, a University of Cambridge spinout based in the nearby village of Somersham, has also fared well over the past decade with backing from the UK Treasury. It produces graphene-based electronic devices, including sensors for electric cars, and biosensors for the early detection of disease and other uses in healthcare and agriculture. It recently secured $55m (£41m) from investors including the United Arab Emirates sovereign wealth fund, which took a 12.8% stake in Paragraf.&lt;/p&gt;
    &lt;p&gt;A more recent startup, Graphene Innovations Manchester, launched in 2021 by Vivek Koncherry, struck a deal with Saudi Arabia last December for the world’s first commercial production of graphene-enriched carbon fibre, used in construction for roofs and facades, as well as street light poles. It has started making it in Tabuk with a local partner, and says it is on track to produce 3,000 tonnes by 2026.&lt;/p&gt;
    &lt;p&gt;Other businesses, however, have found the going tougher. One of the sector’s first companies was Applied Graphene Materials, founded by Prof Karl Coleman in 2010 and spun out from Durham University. It launched a number of products including an anti-corrosion primer and a protective bike-detailing spray, which made it on to the shelves in Halfords. But the loss-making enterprise was wound down in 2023 and Canada’s Universal Matter acquired its main business.&lt;/p&gt;
    &lt;p&gt;Ron Mertens, the owner of the website Graphene-Info, says: “As is true in the wider materials industry, things take a very long time to reach the market. Many graphene producers and developers never managed to generate meaningful revenues or become profitable.”&lt;/p&gt;
    &lt;p&gt;The Gloucestershire-based Versarien grew from a garage startup in 2010. Supported by Innovate UK, a government agency, it has developed graphene powders and other products for use in sensors, low-carbon concrete, paints, inks for electronics and textiles such as Umbro running gear and prototype stealth materials for the US military.&lt;/p&gt;
    &lt;p&gt;The Aim-listed company expanded into Spain and South Korea but ran into financial difficulties and was forced to place several subsidiaries in administration or voluntary liquidation in July. Versarien has been seeking to sell its assets, including its patent portfolio, and only has enough cash to keep going until the end of October.&lt;/p&gt;
    &lt;p&gt;Depending on the deal, it may undertake a solvent liquidation of the company or become a cash shell. An investment deal with a Chinese partner collapsed after the UK government intervened to block any collaboration on technology. It would be a sad end for a once promising graphene business.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/business/2025/oct/13/lab-to-fab-are-promises-of-a-graphene-revolution-finally-coming-true"/><published>2025-10-13T08:53:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566421</id><title>Switch to Jujutsu Already: A Tutorial</title><updated>2025-10-13T14:11:02.286750+00:00</updated><content>&lt;doc fingerprint="36385cdbe8f8799f"&gt;
  &lt;main&gt;
    &lt;p&gt;As all developers, I’ve been using git since the dawn of time, since its commands were an inscrutable jumble of ill-fitting incantations, and it has remained this way until today. Needless to say, I just don’t get git. I never got it, even though I’ve read a bunch of stuff on how it represents things internally. I’ve been using it for years knowing what a few commands do, and whenever it gets into a weird state because I fat-fingered something, I have my trusty alias, &lt;code&gt;fuckgit&lt;/code&gt;, that deletes the &lt;code&gt;.git&lt;/code&gt; directory, clones the repo again into a temp folder, and moves the &lt;code&gt;.git&lt;/code&gt; directory from that into my directory, and I’ve managed to eke out a living for my family this way.&lt;/p&gt;
    &lt;p&gt;Over the past few years, I’ve been seeing people rave about Jujutsu, and I always wanted to try it, but it never seemed worth the trouble, even though I hate how hard git makes some things. I idly read a few tutorials, trying to understand how it works, but in the end I decided it wasn’t for me.&lt;/p&gt;
    &lt;p&gt;One day I randomly decided to try again, but this time I asked Claude how to do with Jujutsu whatever operation I wanted to do with git. That’s when the mental model of jj clicked for me, and I finally understood everything, including how git works. I never thought a VCS would spark joy in me, but here we are, and I figured maybe I can write something that will make jj click for you as well.&lt;/p&gt;
    &lt;p&gt;It also doesn’t hurt that Jujutsu is completely interoperable with git (and thus with providers like GitHub), and I can have all the power of Jujutsu locally on my git repos, without anyone knowing I’m not actually using git.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem&lt;/head&gt;
    &lt;p&gt;The problem I had with the other tutorials, without realizing it, was that there was a fundamental tension between two basic things: The best way to explain jj to someone who knows git is to use all the git terms they already know (because that makes it easy for them), but also to tell them to think about the git terms they know differently (because otherwise they’ll form the wrong mental model). You can’t really explain something by saying “a jj commit is like a git commit, except where it’s not”, so I’ll try to do things a bit differently.&lt;/p&gt;
    &lt;p&gt;This will be a short post (or, at least, not as long as other jj tutorials), I’ll explain the high-level mental model you should have, and then give a FAQ for how to do various git things with jj.&lt;/p&gt;
    &lt;head rend="h2"&gt;Warnings&lt;/head&gt;
    &lt;p&gt;Just a disclaimer before we start, this is going to be far from an exhaustive reference. I’m not an expert in either git or Jujutsu, but I know enough to hopefully make jj click for you enough to learn the rest on your own, so don’t be too annoyed if I omit something.&lt;/p&gt;
    &lt;p&gt;Also, you’re going to read here some things about the way Jujutsu likes doing things that will offend you to your very core, and your first reaction will be “madness, this cannot possibly work”. When you think this, I want you to relax, it’s fine, it does work, it just means I haven’t managed to make the whole thing click together for you yet. Just read on.&lt;/p&gt;
    &lt;p&gt;I’m not going to show you any Jujutsu commands here. I might refer to them by name, but I want you to understand the mental model enough to go look stuff up on your own, Jujutsu only has, like, three commands you’re going to use for everything anyway (yes, you can do everything you do with git with them).&lt;/p&gt;
    &lt;p&gt;(By the way, if you’re going to be trying things out while reading this post, definitely get jjui, it lets you visually work with the repository in a way that makes everything much easier to understand.)&lt;/p&gt;
    &lt;head rend="h2"&gt;The high-level mental model you should have&lt;/head&gt;
    &lt;p&gt;First of all, all the basic git things you’re already familiar with are there in jj: Commits, branches, operations on those, all those things carry over, with some small differences.&lt;/p&gt;
    &lt;p&gt;The main difference is in the general way the two work, jj simplifies git’s model a lot by getting rid of some inconsistencies, and makes it much easier to understand what’s going on “under the hood”, because the “under the hood” is now so much smaller and simpler, that it can just be over the hood.&lt;/p&gt;
    &lt;head rend="h3"&gt;git&lt;/head&gt;
    &lt;p&gt;The mental model that you probably have with git is something like an assembly line. You take a bunch of components, you form them into a widget, you put the widget into a box, you write “General bug fixes” onto the box, seal it, and send it off, never to be seen again by anyone.&lt;/p&gt;
    &lt;p&gt;That’s what git thinks of as a commit. You have some work that is The Thing You’re Working On Now, and then at some point that’s kind of done, you select which pieces of that work you want to immortalize, and you commit them, freezing them in time forever from then on. (I know you can edit commits, but this is largely git’s mental model, commits are immutable).&lt;/p&gt;
    &lt;head rend="h3"&gt;Jujutsu&lt;/head&gt;
    &lt;p&gt;Jujutsu, in contrast, is more like playing with Play-Doh. You take a lump, cut it into two, shape one piece into something, give it a name, change your mind, give it another name, take a bit of the second piece and stick it on the first piece, and generally go back and forth all around your play area, making changes.&lt;/p&gt;
    &lt;p&gt;Jujutsu wants you to be able to go back to an old commit, change it (gasp!), go to another branch (three commits back from that HEAD), change that commit too, move whole branches of your tree to other parts of it, whatever you want. Your worktree in Jujutsu is a free-for-all where you can rearrange things as you like.&lt;/p&gt;
    &lt;head rend="h3"&gt;Recap&lt;/head&gt;
    &lt;p&gt;Basically, in git, you manipulate the code, put it in a commit, and you’re largely done. In Jujutsu, the commits themselves are also the object of manipulation. This isn’t the most natural workflow in git, as git makes it much harder than jj does, but maybe this is the workflow you already have in git (with extensive squashing/rebasing/amending). In that case, grasping the Jujutsu workflow will probably be easier, and will make things easier for you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Madness, this cannot possibly work&lt;/head&gt;
    &lt;p&gt;Yes yes, nobody wants their commits changing from under them, that’s why Jujutsu doesn’t let you easily change commits that have been pushed to a remote, you can relax now.&lt;/p&gt;
    &lt;p&gt;However, if you spend a moment thinking about what I said above, you’ll probably realize that a few things need to be different from git for this to work (and they are):&lt;/p&gt;
    &lt;head rend="h3"&gt;Commits have to be mutable.&lt;/head&gt;
    &lt;p&gt;Indeed, Jujutsu commits are mutable (until you push them). Right now you’re thinking of commits as something that can’t change, but this is one of the things you need to accept. You can (and will) go back to a previous commit (that you haven’t yet pushed) to fix a bug in it that you just hit, and it’s as simple as checking out (jj calls it &lt;code&gt;edit&lt;/code&gt;ing) that commit and making the change.
You don’t have to commit again!
Jujutsu does whatever it needs to do under the hood when you run the &lt;code&gt;jj&lt;/code&gt; command, to you it just looks like your edits are automatically persisted in the commit, in real time.&lt;/p&gt;
    &lt;p&gt;To clarify, Jujutsu doesn’t create new commits while this goes on, you just see one “open” commit that you keep making changes to your code in.&lt;/p&gt;
    &lt;head rend="h3"&gt;If I can just go into a commit and edit it and jj auto-saves, there must not be a staging area.&lt;/head&gt;
    &lt;p&gt;Indeed, there is no staging area like git has. git splits code to either be in the repo (in a commit), or outside it (staged/unstaged).&lt;/p&gt;
    &lt;p&gt;Jujutsu doesn’t have that, you are always in a commit. This is important: In git, you’re outside a commit until you create one. In Jujutsu, you are always inside a commit. Nothing is ever outside a commit, “outside a commit” isn’t a thing in Jujutsu.&lt;/p&gt;
    &lt;p&gt;Even the very &lt;code&gt;commit&lt;/code&gt; command in Jujutsu is an alias that adds a message to the commit you’re on, and then creates a new (empty) one that you’ll now be working on.
Even when you create a new repo, you start in a commit.&lt;/p&gt;
    &lt;p&gt;This is the most important difference between jj and git, and the one thing you should think a bit about, as it enables many really interesting workflows.&lt;/p&gt;
    &lt;p&gt;Always being in a commit means that yes, you will have commits that are half-finished work. Maybe lots of them! I usually indicate this in the commit message, to remind myself.&lt;/p&gt;
    &lt;head rend="h3"&gt;So commits might not have a commit message?&lt;/head&gt;
    &lt;p&gt;You are impressively perceptive for a hypothetical straw man in whose mouth I’m putting words. Exactly, commits might not have a commit message. They start out blank, and you can add a commit message at any point, whenever you have an idea of what that commit will do. It might be when you start working on it, it might be half-way through, or it might be at the end. Personally, I usually add the message at the end, but that’s just preference.&lt;/p&gt;
    &lt;head rend="h3"&gt;So there’s no stashing either?&lt;/head&gt;
    &lt;p&gt;Yes, since everything is always in a commit, there’s nothing to stash.&lt;/p&gt;
    &lt;p&gt;In git, if you have some uncommitted changes and want to check out an old commit, you need to stash them first. In Jujutsu, since all your changes are automatically persisted in a commit at all times, you can have some new changes (which, if this were git, would be uncommitted), you can check out (or &lt;code&gt;edit&lt;/code&gt;) an older commit, then come back to your new changes in the latest commit, and they’ll all be there.&lt;/p&gt;
    &lt;head rend="h3"&gt;But then branches need to be lightweight.&lt;/head&gt;
    &lt;p&gt;If you’re going to be jumping around the tree all the time, making commits and branches, they can’t require names. Jujutsu lets you create branches by just creating a commit, you don’t need to name the branch. In Jujutsu (and in git!), branches are simply two or more commits with the same parent, it’s just that git artificially makes you think of branches as special, because it makes you name them.&lt;/p&gt;
    &lt;p&gt;In Jujutsu, creating a branch is as simple as checking out the commit you want to branch from, and creating a new commit on top of it. This is one thing Jujutsu simplifies over git. In git, branches are a fairly heavy thing, you have to name them, you have the mental model of “being” on the branch, and your workflow is centered around them. In Jujutsu, you just… add a new commit, and if that commit has siblings, well, that’s now a branch.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conflicts&lt;/head&gt;
    &lt;p&gt;I haven’t talked about conflicts much, because, unlike git, in practice they haven’t really been anything special. Jujutsu doesn’t stop the world at all, it doesn’t even particularly complain, it just marks a commit as conflicted, but you can continue working on other places in the worktree and then later come back at your leisure and fix that commit’s conflicts!&lt;/p&gt;
    &lt;p&gt;Whereas in git you have to quit what you’re doing and fix the conflicts right now, jj is more “by the way, when you have some time, let me know what this commit should look like*. The changes also cascade to all subsequent commits, which is fantastic. You only fix conflicts once, and jj takes care of the rest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Snapshots&lt;/head&gt;
    &lt;p&gt;Under the hood, jj automatically and transparently commits whatever you’re working on when you invoke the jj command (it can also be configured to do it on its own whenever a file in the repo changes). This is safe, as these intermediate changes won’t be pushed anywhere, but this means that you get snapshots for free!.&lt;/p&gt;
    &lt;p&gt;If you’ve ever had Claude get to a working solution, but then trip over itself and mess it up, jj can help, you can use the oplog to go back to the way your repo looked a few minutes ago, even if you didn’t explicitly commit anything! Even using the &lt;code&gt;status&lt;/code&gt; or &lt;code&gt;log&lt;/code&gt; command to look at stuff will take a snapshot of your repo, allowing you to return to it if something goes wrong.
No more losing unstaged changes, ever!&lt;/p&gt;
    &lt;p&gt;This has saved my ass a few times already.&lt;/p&gt;
    &lt;head rend="h2"&gt;Questions and answers&lt;/head&gt;
    &lt;p&gt;By now you probably have lots of questions, I’ll try to answer some of them here. If you have more questions, just send them to me and I’ll add them here, along with the answer.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I branch off main?&lt;/head&gt;
    &lt;p&gt;You don’t really branch off main, in that you usually won’t need to create two commits off main, you’ll only create one.&lt;/p&gt;
    &lt;head rend="h4"&gt;git&lt;/head&gt;
    &lt;p&gt;In git, we branch off of main, and now our mental model is that “we’re in that branch”. In reality, if you look at the graph on the right, it’s all still just a line, we’ve just made a mental “bend” in the graph to tell ourselves that we’re on a branch.&lt;/p&gt;
    &lt;p&gt;As far as the graph is concerned, though, nothing special really actually happened, we just added more commits. The only real difference is that “main” stops at the third commit, whereas “my branch” stops at the sixth commit. Other than that, the entire history is just one line.&lt;/p&gt;
    &lt;head rend="h4"&gt;Jujutsu&lt;/head&gt;
    &lt;p&gt;Jujutsu, on the other hand, doesn’t care what you think. It only cares what parents, children, and siblings commits have.&lt;/p&gt;
    &lt;p&gt;There are two reasons you might want to branch:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;History legitimately diverges into multiple directions, or&lt;/item&gt;
      &lt;item&gt;You want to communicate to other people (or to yourself) that this part of the history is different (e.g. it contains some feature). This is also the case when you want to create a new branch so you can open a PR for it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To Jujutsu, this repo’s history is a straight line, so there is no actual “branching”. The only reason to have branches here is communication, so Jujutsu asks you to label the commits that you want on the branches yourself. You can see these tags on the example on the right, and it’s the same as the git example above. There are still three commits in &lt;code&gt;main&lt;/code&gt;, and three more in &lt;code&gt;my branch&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Jujutsu calls these labels “bookmarks”, and they correspond to whatever git uses to tag branches. Bookmarks are what you’ll tag your commits with to tell git what your branches are.&lt;/p&gt;
    &lt;p&gt;Continuing the earlier example, if we create a second commit off main, even if that’s a merge commit (a commit with two parents) that’s when the tree actually diverges. In the graph on the right, the commit where we branched off now is a parent to two commits, and history is no longer linear. This isn’t special, it’s just how things are, but this is what’s actually a real “branch” to Jujutsu.&lt;/p&gt;
    &lt;p&gt;The way that git does things, ie creating a branch without history actually diverging, is just for us humans and our communication needs.&lt;/p&gt;
    &lt;p&gt;Jujutsu doesn’t require you to name its branches. You can happily work without any branch names at all, and you can easily see what branch is for what from the commit descriptions. You can name them, if you prefer, but you don’t have to.&lt;/p&gt;
    &lt;p&gt;This sounds a bit alien right now, but it’s actually a really nice way to work.&lt;/p&gt;
    &lt;p&gt;I’m worried I’ve lost you here, but it doesn’t matter. You’ll understand all of this easily when you play around with the tree a bit in jjui.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I add a commit message?&lt;/head&gt;
    &lt;p&gt;You can add a commit message at any time to the current, using the &lt;code&gt;describe&lt;/code&gt; command.
You can do this at any time, you can even go back to other commits and amend their messages (again with the &lt;code&gt;describe&lt;/code&gt; command).&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I choose which of my changes to commit?&lt;/head&gt;
    &lt;p&gt;You don’t! Everything is already in a commit! What you do is you interactively select some of the changes in the current commit (whether this commit is blank/new or an old commit, it doesn’t matter), and you &lt;code&gt;split&lt;/code&gt; that commit into two.&lt;/p&gt;
    &lt;p&gt;Jujutsu can also do this automatically! If you have a commit with a bunch of small changes to various files, jj can &lt;code&gt;absorb&lt;/code&gt; these changes into the closest ancestor commit where each thing changed.
This is pretty magical, as you can add a few one-liner bugfixes here and there, and jj will just automatically include them in the commits where those lines were touched.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I check out a commit?&lt;/head&gt;
    &lt;p&gt;Without getting too much into specifics, you just &lt;code&gt;edit&lt;/code&gt; the commit you want.
This checks it out and you can make changes to it, however keep in mind that, if the commit was previously pushed to a remote, jj will give you a warning that you shouldn’t change commits you’ve pushed.&lt;/p&gt;
    &lt;p&gt;jjui will make navigation around the repo really easy, so use it for checking out commits as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I cherry-pick a commit onto another branch?&lt;/head&gt;
    &lt;p&gt;You just… move it. In jjui, go to the commit you want to move, press r (for &lt;code&gt;rebase&lt;/code&gt;), go to the commit you want to move it after, press enter, and that’s it.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I reset soft/hard?&lt;/head&gt;
    &lt;p&gt;There isn’t really a soft reset, as there isn’t a staging area for your changes to be reset in. Simply check out (&lt;code&gt;edit&lt;/code&gt;) the commit you want to edit, that’s a soft reset in Jujutsu.&lt;/p&gt;
    &lt;p&gt;For a hard reset (ie to throw away a commit), you &lt;code&gt;abandon&lt;/code&gt; that commit.
jjui will, again, make it much easier to do this.&lt;/p&gt;
    &lt;head rend="h3"&gt;What if I make a mistake?&lt;/head&gt;
    &lt;p&gt;No matter what you do, you can &lt;code&gt;undo&lt;/code&gt; it.
Not just changes, but any jj operation, you can undo rebases, pulls, anything.&lt;/p&gt;
    &lt;p&gt;You can also use the oplog (again, jjui makes this really easy) to go back to how the whole repo looked at any point in time. Don’t be afraid to try things, with jj it’s really easy to undo any mistake.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I amend a commit?&lt;/head&gt;
    &lt;p&gt;Simply &lt;code&gt;edit&lt;/code&gt; it and make the changes you want.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I move unstaged changes from one branch to another?&lt;/head&gt;
    &lt;p&gt;There are no unstaged changes in jj. All changes are in a commit, if you want to move the changes in your current commit to another branch, simply move your current commit to the target branch by rebasing. I can never remember what “rebase X onto Y” does, so just move the commit with your changes to be a child of your branch’s tip (again, use jjui for this).&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I open a PR on GitHub?&lt;/head&gt;
    &lt;p&gt;To do that, you need to push a new branch. Go to the commit you want to push, then probably create a new one on top of that (I tend to create a new commit when I’m done with an old one, just so I’m remember I’m done, but this is personal preference). Then, bookmark that commit with the branch name you want to give your PR, and push the commit along with the bookmark.&lt;/p&gt;
    &lt;p&gt;That’s all, now you can open the PR.&lt;/p&gt;
    &lt;p&gt;Here, jj exposes the low-level operations much more than git: You need to move the bookmark on your own to the commit you want to push (git does that automatically for you), and you need to push the bookmark manually as well. This is very helpful for understanding how things work under the hood, but usually you’ll set a jj alias to do this in one step.&lt;/p&gt;
    &lt;p&gt;Personally, I have an alias (which I’ll include below) to find the bookmark name, move it to the latest commit, and push.&lt;/p&gt;
    &lt;head rend="h2"&gt;My aliases&lt;/head&gt;
    &lt;p&gt;Here’s my alias config:&lt;/p&gt;
    &lt;code&gt;[aliases]
init = ["git", "init", "--colocate"]
ps = ["util", "exec", "--", "bash", "-c", """
set -e

# Check if current commit has both description and changes
has_description=$(jj log -r @ --no-graph --color never -T 'description' | grep -q . &amp;amp;&amp;amp; echo "yes" || echo "no")
# Use 'empty' template keyword to check if commit has changes
has_changes=$(jj log -r @ --no-graph --color never -T 'empty' | grep -q "false" &amp;amp;&amp;amp; echo "yes" || echo "no")

if [ "$has_description" = "yes" ] &amp;amp;&amp;amp; [ "$has_changes" = "yes" ]; then
    echo "Current commit has description and changes, creating new commit..."
    jj new
fi

# Get the bookmark from the parent commit directly
bookmark=$(jj log -r 'ancestors(@) &amp;amp; bookmarks()' -n 1 --no-graph --color never -T 'bookmarks' | sed 's/\\*$//' | tr -d ' ')

if [ -z "$bookmark" ]; then
    echo "No bookmark found on parent commit"
    exit 1
fi

echo "Moving bookmark '$bookmark' to parent commit and pushing..."
jj bookmark set "$bookmark" -r @-
jj git fetch
jj git push --bookmark "$bookmark" --allow-new
"""]
cma = ["commit", "-m"]
&lt;/code&gt;
    &lt;p&gt;This means I can &lt;code&gt;jj init&lt;/code&gt; to add jj to a git repo, and &lt;code&gt;jj cma "message"&lt;/code&gt; to describe the current commit and create a new one on top of it (that’s what &lt;code&gt;commit&lt;/code&gt; does under the hood).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;jj ps&lt;/code&gt; is a convenience alias that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Looks backward in history&lt;/item&gt;
      &lt;item&gt;Finds the last bookmark there (if this were git, this would be my branch name)&lt;/item&gt;
      &lt;item&gt;Checks if the current commit has changes in it&lt;/item&gt;
      &lt;item&gt;If it does, it creates a new commit&lt;/item&gt;
      &lt;item&gt;Moves the bookmark to the parent commit (the one I was on before I ran the command)&lt;/item&gt;
      &lt;item&gt;Fetches changes from upstream (to update my tree)&lt;/item&gt;
      &lt;item&gt;Pushes the changes to the remote&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I use this a lot!&lt;/p&gt;
    &lt;head rend="h2"&gt;Epilogue&lt;/head&gt;
    &lt;p&gt;Jujutsu doesn’t do anything that git can’t do, but it removes so much friction that you’ll actually end up doing things all the time that git could do, but that were so fiddly with git that you never actually did them.&lt;/p&gt;
    &lt;p&gt;Creating a branch for a minute just to try an idea out even though you’re in the middle of some changes, going back to a previous commit to add a line you forgot, moving commits around the tree, all of these things are so easy that they’re now actually your everyday workflow.&lt;/p&gt;
    &lt;p&gt;With git, I never used to switch branches in the middle of work, because I was too worried that stashing multiple things onto the stack would eat my work. I’d never go back to a previous commit and amend it, because here be dragons. I was extremely afraid of rebasing because I always got one conflict per commit and had to unconflict the same thing fifty times.&lt;/p&gt;
    &lt;p&gt;Jujutsu gives you the confidence and understanding to do all of these things, and if you fuck something up (which I haven’t yet, miraculously!) the oplog is right there to fix everything to how it was 30 seconds ago.&lt;/p&gt;
    &lt;p&gt;I hope this tutorial made sense, but I’m worried it didn’t. Please contact me on Twitter or Bluesky, or email me directly, if you have feedback or corrections.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.stavros.io/posts/switch-to-jujutsu-already-a-tutorial/"/><published>2025-10-13T09:22:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566441</id><title>MPTCP for Linux</title><updated>2025-10-13T14:11:02.052270+00:00</updated><content>&lt;doc fingerprint="a7b5e91a5cbdcba9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Multipath TCP or MPTCP is an extension to the standard TCP and is described in RFC 8684. It allows a device to make use of multiple interfaces at once to send and receive TCP packets over a single MPTCP connection. MPTCP can aggregate the bandwidth of multiple interfaces or prefer the one with the lowest latency. It also allows a fail-over if one path is down, and the traffic is seamlessly reinjected on other paths.&lt;/p&gt;
    &lt;code&gt;graph TD;
    subgraph MPTCP
        direction LR
        C_1(&amp;lt;div style="display: inline-block; min-width: 32px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-mobile&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;)
        S_1((&amp;lt;div style="display: inline-block; min-width: 55px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-cloud&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;))
    end

    subgraph TCP
        direction LR
        C_2(&amp;lt;div style="display: inline-block; min-width: 32px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-mobile&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;)
        S_2((&amp;lt;div style="display: inline-block; min-width: 55px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-cloud&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;))
    end

    C_1 &amp;lt;== "5G" ==&amp;gt; S_1
    C_1 &amp;lt;== "Wi-Fi&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;Multiple paths (&amp;lt;i&amp;gt;subflows&amp;lt;/i&amp;gt;)&amp;lt;br /&amp;gt;at the same time" ==&amp;gt; S_1

    C_2 x-. "5G" .-x S_2
    C_2 &amp;lt;== "Wi-Fi&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;One path at a time" ==&amp;gt; S_2

    linkStyle 0 stroke:green;
    linkStyle 1 stroke:green;
    linkStyle 2 stroke:red;
    linkStyle 3 stroke:green;
&lt;/code&gt;
    &lt;head rend="h3"&gt;Use cases&lt;/head&gt;
    &lt;p&gt;Thanks to MPTCP, being able to use multiple paths in parallel or simultaneously brings new use-cases, compared to TCP:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seamless handovers: switching from one path to another while preserving established connections, e.g. Apple is using Multipath TCP on smartphones mainly for this reason since 2013.&lt;/item&gt;
      &lt;item&gt;Best network selection: using the “best” available path depending on some conditions, e.g. latency, losses, cost, bandwidth, etc.&lt;/item&gt;
      &lt;item&gt;Network aggregation: using multiple paths at the same time to have a higher throughput, e.g. to combine fixed and mobile networks to send files faster.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Concepts&lt;/head&gt;
    &lt;p&gt;Technically, when a new socket is created with the &lt;code&gt;IPPROTO_MPTCP&lt;/code&gt; protocol (Linux-specific), a subflow (or path) is created. This subflow consists of a regular TCP connection that is used to transmit data through one interface. Additional subflows can be negotiated later between the hosts. For the remote host to be able to detect the use of MPTCP, a new field is added to the TCP option field of the underlying TCP subflow. This field contains, amongst other things, a &lt;code&gt;MP_CAPABLE&lt;/code&gt; option that tells the other host to use MPTCP if it is supported. If the remote host or any middlebox in between does not support it, the returned &lt;code&gt;SYN+ACK&lt;/code&gt; packet will not contain MPTCP options in the TCP option field. In that case, the connection will be “downgraded” to plain TCP, and it will continue with a single path.&lt;/p&gt;
    &lt;p&gt;This behavior is made possible by two internal components: the path manager, and the packet scheduler.&lt;/p&gt;
    &lt;head rend="h3"&gt;Path Manager&lt;/head&gt;
    &lt;p&gt;The Path Manager is in charge of subflows, from creation to deletion, and also address announcements. Typically, it is the client side that initiates subflows, and the server side that announces additional addresses via the &lt;code&gt;ADD_ADDR&lt;/code&gt; and &lt;code&gt;REMOVE_ADDR&lt;/code&gt; options.&lt;/p&gt;
    &lt;code&gt;graph LR;
    C_1(&amp;lt;div style="display: inline-block; min-width: 35px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-mobile&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;)
    S_1((&amp;lt;div style="display: inline-block; min-width: 60px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-cloud&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;))

    C_1 -. "Potential subflow" -.- S_1
    C_1 &amp;lt;== "Initial subflow" ==&amp;gt; S_1
    C_1 ~~~|"Subflows creation"| C_1
    S_1 ~~~|"Addresses announcement"| S_1

    linkStyle 0 stroke:orange;
    linkStyle 1 stroke:green;
&lt;/code&gt;
    &lt;p&gt;As of Linux v5.19, there are two path managers, controlled by the &lt;code&gt;net.mptcp.pm_type&lt;/code&gt; sysctl knob: the in-kernel one (type &lt;code&gt;0&lt;/code&gt;) where the same rules are applied for all the connections (see: &lt;code&gt;ip mptcp&lt;/code&gt;) ; and the userspace one (type &lt;code&gt;1&lt;/code&gt;), controlled by a userspace daemon (i.e. &lt;code&gt;mptcpd&lt;/code&gt;) where different rules can be applied for each connection.&lt;/p&gt;
    &lt;head rend="h3"&gt;Packet Scheduler&lt;/head&gt;
    &lt;p&gt;The Packet Scheduler is in charge of selecting which available subflow(s) to use to send the next data packet. It can decide to maximize the use of the available bandwidth, only to pick the path with the lower latency, or any other policy depending on the configuration.&lt;/p&gt;
    &lt;code&gt;graph LR;
    A_2(&amp;lt;div style="display: inline-block; min-width: 40px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-user&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;)

    PS{Packet&amp;lt;br /&amp;gt;Scheduler}

    I_21(subflow 1)
    I_22(subflow 2)

    A_2 == "&amp;lt;div style='display: inline-block; min-width: 50px'&amp;gt;fa:fa-box fa:fa-box fa:fa-box&amp;lt;/div&amp;gt;" ==&amp;gt; PS
    PS -- "&amp;lt;div style='display: inline-block; min-width: 32px'&amp;gt;fa:fa-box fa:fa-box&amp;lt;/div&amp;gt;" --&amp;gt; I_21
    PS -- "&amp;lt;div style='display: inline-block; min-width: 14px'&amp;gt;fa:fa-box&amp;lt;/div&amp;gt;" --&amp;gt; I_22
    PS ~~~|"Packets distribution between subflows"| PS
&lt;/code&gt;
    &lt;p&gt;As of Linux v6.8, there is only one packet scheduler, controlled by sysctl knobs in &lt;code&gt;net.mptcp&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;p&gt;As of Linux v6.10, major features of MPTCP include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support of the &lt;code&gt;IPPROTO_MPTCP&lt;/code&gt;protocol in&lt;code&gt;socket()&lt;/code&gt;system calls.&lt;/item&gt;
      &lt;item&gt;Fallback from MPTCP to TCP if the peer or a middlebox do not support MPTCP.&lt;/item&gt;
      &lt;item&gt;Path management using either an in-kernel or userspace path manager.&lt;/item&gt;
      &lt;item&gt;Socket options that are commonly used with TCP sockets.&lt;/item&gt;
      &lt;item&gt;Debug features including MIB counters, diag support (used by the &lt;code&gt;ss&lt;/code&gt;command), and tracepoints.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the ChangeLog for more details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Communication&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mailing List: mptcp@lists.linux.dev (plain text only): &lt;list rend="ul"&gt;&lt;item&gt;Archives&lt;/item&gt;&lt;item&gt;Info&lt;/item&gt;&lt;item&gt;Subscribe by sending an empty email in plain text to mptcp+subscribe@lists.linux.dev, and by replying to the challenge email.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;IRC: #mptcp on libera.chat&lt;/item&gt;
      &lt;item&gt;Online Meetings&lt;/item&gt;
      &lt;item&gt;Blog&lt;/item&gt;
      &lt;item&gt;Fediverse&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Projects&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Maintained by MPTCP community members&lt;/item&gt;
      &lt;item&gt;Projects with MPTCP-related enhancements &lt;list rend="ul"&gt;&lt;item&gt;iproute2 (for the &lt;code&gt;ip mptcp&lt;/code&gt;command)&lt;/item&gt;&lt;item&gt;Network Manager: MPTCP features are included starting with v1.40.&lt;/item&gt;&lt;item&gt;Multipath TCP applications: A project to coordinate MPTCP updates for popular TCP applications.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;iproute2 (for the &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.mptcp.dev/"/><published>2025-10-13T09:25:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566532</id><title>gsay: Fetch pronunciation of English vocabulary from Google</title><updated>2025-10-13T14:11:01.981803+00:00</updated><content>&lt;doc fingerprint="3e4abc7abbb2734a"&gt;
  &lt;main&gt;
    &lt;p&gt;A simple shell script to fetch pronunciation of an English word from Google.&lt;/p&gt;
    &lt;p&gt;When you search for an Enlgish vocabulary, Google shows a pronunciation in an "answer box" at top of the page. This script fetches that hopefully human-made mp3 sound file.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Query for 2020 and 2024 sound files&lt;/item&gt;
      &lt;item&gt;Support British and American accents&lt;/item&gt;
      &lt;item&gt;Cache to disk (enabled by default)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;curl&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ffplay | mpv | pw-play&lt;/code&gt;: A headless mp3 player, one is enough&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In a Debian-like distro, these can be installed with:&lt;/p&gt;
    &lt;code&gt;sudo apt install curl ffmpeg # or mpv&lt;/code&gt;
    &lt;p&gt;Check &lt;code&gt;gsay -h&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Usage: gsay [OPTIONS] QUERY
DESCRIPTION
    A simple client to fetch/play pronounciation of words from Google

ARGUMENTS
     QUERY
        Query string
        Note: Use "" or '' if query string contains special shell characters
OPTIONS
    [--year | -y]
        Set desired database year: 2020 (default) | 2024
    [--accent | -a]
        Set desired accent: gb (default) | us
    [--link | -l]
        Only print pronounciation link. Don't play their audio
    [--no-cache | -n]
        Disable cache mode: ignore cache and don't save to cache directory
        Default: cache mode is enabled. dir: /home/username/.cache/gsay
    [--debug | -v]
        Increase log verbosity to debug
    [--version | -V]
        Print version
    [--help | -h]
        Display the help message

EXAMPLES
    gsay legend
    gsay -y 2024 -a us Leicester
    gsay -n -y 2020 -a gb Forte
    gsay Pneumonoultramicroscopicsilicovolcanoconiosis
    gsay -l perfect
    gsay carte blanche&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I may be wrong but the &lt;code&gt;2024/04/19&lt;/code&gt;pronounciations sound synthetic to me! Hence,&lt;code&gt;2020/04/29&lt;/code&gt;is default despite being slower and less exhaustive. Caller scripts can run it like&lt;code&gt;gsay -y 2020 || gsay -y 2024&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;The HTTP URLs are faster, HTTPS ones are provided as comments too&lt;/item&gt;
      &lt;item&gt;Fun examples: &lt;code&gt;echo Supercalifragilisticexpialidocious Antidisestablishmentarianism Grandiloquent | xargs -n1 gsay&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Oxford 3000 word list pronunciations, limited to first 1000 words, can be downloaded (cached) to disk like this:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;curl -sL https://github.com/sapbmw/The-Oxford-3000/raw/refs/heads/master/The_Oxford_3000.txt \
    | tr -d \r | head -1000 | xargs -I {} gsay -l "{}"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The early versions of this script supported a scraping mode besides the current heuristic mode. But, recently Google has been preventing scrapers from accessing its search results page. I tested their "Custom Search JSON API" from "Programmable Search Engine", but it doesn't provide the output from Answer Box that contains pronounciations&lt;/item&gt;
      &lt;item&gt;linter: &lt;code&gt;shellcheck&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;formatter: &lt;code&gt;shfmt -i 4 -bn -ci -sr&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/pvonmoradi/gsay"/><published>2025-10-13T09:39:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566548</id><title>Modern Linux tools</title><updated>2025-10-13T14:11:01.758029+00:00</updated><content>&lt;doc fingerprint="3248c1cd4e981f8f"&gt;
  &lt;main&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;bat&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;&lt;code&gt;cat&lt;/code&gt; clone with syntax highlighting and &lt;code&gt;git&lt;/code&gt; integration&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;exa&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;modern replacement for &lt;code&gt;ls&lt;/code&gt;/&lt;code&gt;tree&lt;/code&gt;, not maintained&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;eza&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;modern &lt;code&gt;ls&lt;/code&gt;/&lt;code&gt;tree&lt;/code&gt; based on &lt;code&gt;exa&lt;/code&gt; fork&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;lsd&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;next gen &lt;code&gt;ls&lt;/code&gt;, backwards compatible&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;delta&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;viewer for &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;diff&lt;/code&gt; output&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;ncdu&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;intuitive &lt;code&gt;du&lt;/code&gt; with ncurses interface&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;dust&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;more intuitive version of &lt;code&gt;du&lt;/code&gt; written in rust&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;duf&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;better &lt;code&gt;df&lt;/code&gt; alternative&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;broot&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;better &lt;code&gt;tree&lt;/code&gt; with navigation support&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;fd&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;simple, fast and user-friendly alternative to &lt;code&gt;find&lt;/code&gt;&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;ripgrep&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;extremely fast alternative to &lt;code&gt;grep&lt;/code&gt; that respects your gitignore&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;ag&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;code searching tool similar to &lt;code&gt;ack&lt;/code&gt;, but faster&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;fzf&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;general purpose command-line fuzzy &lt;code&gt;find&lt;/code&gt;&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;bfs&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;breadth-first &lt;code&gt;find&lt;/code&gt; alternative&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;mcfly&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;fly through your shell &lt;code&gt;history&lt;/code&gt;&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;choose&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;human-friendly and fast alternative to &lt;code&gt;cut&lt;/code&gt; and (sometimes) &lt;code&gt;awk&lt;/code&gt;&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;jq&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;&lt;code&gt;sed&lt;/code&gt; for JSON data&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;sd&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;intuitive find/replace CLI. &lt;code&gt;sed&lt;/code&gt; alternative&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;bottom&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;another cross-platform graphical process/system monitor&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;glances&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;&lt;code&gt;top&lt;/code&gt;/&lt;code&gt;htop&lt;/code&gt; alternative&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;gtop&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;System monitoring dashboard for terminal&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;hyperfine&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;command-line benchmarking tool&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;gping&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;&lt;code&gt;ping&lt;/code&gt; with a graph&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;procs&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;&lt;code&gt;ps&lt;/code&gt; rust replacement&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;httpie&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;modern, user-friendly command-line HTTP client for the API era&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;curlie&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;power of &lt;code&gt;curl&lt;/code&gt;, the ease of use of &lt;code&gt;httpie&lt;/code&gt;&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;xh&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;performance focused alternative of &lt;code&gt;httpie&lt;/code&gt;&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;zoxide&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;smarter &lt;code&gt;cd&lt;/code&gt; command inspired by &lt;code&gt;z&lt;/code&gt;&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;micro&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;modern terminal text editor&lt;/cell&gt;
    &lt;/row&gt;
    &lt;row&gt;
      &lt;cell&gt;
        &lt;code&gt;nnn&lt;/code&gt;
      &lt;/cell&gt;
      &lt;cell&gt;fast lean terminal file manager&lt;/cell&gt;
    &lt;/row&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ikrima.dev/dev-notes/linux/linux-modern-tools/"/><published>2025-10-13T09:44:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566568</id><title>Nobel Prize in Economic Sciences 2025</title><updated>2025-10-13T14:11:01.636600+00:00</updated><content>&lt;doc fingerprint="ef38027599fd046d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Popular information&lt;/head&gt;
    &lt;p&gt;Popular science background: From stagnation to sustained growth (pdf)&lt;lb/&gt;Populärvetenskaplig information: Från stagnation till stadig tillväxt (pdf)&lt;/p&gt;
    &lt;head rend="h2"&gt;From stagnation to sustained growth&lt;/head&gt;
    &lt;p&gt;Over the past 200 years, the world has witnessed more economic growth than ever before. Its foundation is the constant flow of technological innovation; sustained economic growth occurs when new technologies replace old ones as part of the process known as creative destruction. This year’s laureates in economic sciences explain, using different methods, why this development was possible and what is necessary for continued growth.&lt;/p&gt;
    &lt;p&gt;For most of humankind’s history, living standards did not change considerably from one generation to the next, despite sporadic important discoveries. These sometimes led to improved quality of life, but growth always stopped eventually.&lt;/p&gt;
    &lt;p&gt;This was fundamentally changed by the Industrial Revolution, which occurred a little more than two centuries ago. Starting in Britain, and then progressing to other countries, technological innovation and scientific progress resulted in a never-ending cycle of innovation and progress, rather than isolated events. This led to sustained and remarkably stable growth.&lt;/p&gt;
    &lt;p&gt;This year’s prize relates to the explanations for sustained growth based on technological innovation. Economic historian Joel Mokyr is rewarded with one half of the prize for his description of the mechanisms that enable scientific breakthroughs and practical applications to enhance each other and create a self-generating process, leading to sustained economic growth. Because this is a process that challenges prevailing interests, he also demonstrates the importance of a society that is open to new ideas and permits change.&lt;/p&gt;
    &lt;p&gt;The other half of the prize is awarded to the economists Philippe Aghion and Peter Howitt. In a joint publication from 1992, they constructed a mathematical model of how companies invest in improved production processes and new, better-quality products, while the companies that previously had the best products are outcompeted. Growth arises through creative destruction. This process is creative because it builds upon innovation, but it is also destructive because older products become obsolete and lose their commercial value. Over time, this process has fundamentally changed our societies – over the span of one or two centuries, almost everything has changed.&lt;/p&gt;
    &lt;head rend="h3"&gt;The new normal&lt;/head&gt;
    &lt;p&gt;Economists measure economic growth by calculating increases in gross domestic product (GDP) but, actually, it involves much more than just money. New medicines, safer cars, better food, more efficient ways of heating and lighting our homes, the internet and increased opportunities for communication with other people over greater distances – these are just a few of the things included in growth.&lt;/p&gt;
    &lt;p&gt;However, as we have said, economic growth based on technological development was not the historical norm – quite the opposite. One example of this is the trend in Sweden and Britain from the early 14th century to the start of the 18th century. Income sometimes rose and sometimes fell but, overall, there was almost indiscernible growth, despite important innovation occurring.&lt;/p&gt;
    &lt;p&gt;These discoveries thus had no noticeable effect on long-run economic growth. According to Mokyr, this is because the new ideas did not continue to evolve or lead to the flow of improvements and new applications that we now take for granted, as a natural consequence of major technological and scientific advancements.&lt;/p&gt;
    &lt;p&gt;Instead, when we look at economic growth in Britain and Sweden from the start of the 19th century to the present day, we see something entirely different. Apart from easily identifiable episodes such as the Great Depression in the 1930s and other crises, growth – rather than stagnation – has become the new normal. A similar pattern, with sustained annual growth of almost two per cent, arose in many industrialised nations after the early 19th century. It may not sound like much, but sustained growth at that level means a doubling of income over a person’s working life. Eventually, this has a revolutionary effect on the world and on people’s quality of life.&lt;/p&gt;
    &lt;head rend="h3"&gt;Useful knowledge&lt;/head&gt;
    &lt;p&gt;So – what creates this sustained economic growth? This year’s laureates used different methods to answer this question. Through his research in economic history, Joel Mokyr has demonstrated that a continual flow of useful knowledge is necessary. This useful knowledge has two parts: the first is what Mokyr refers to as propositional knowledge, a systematic description of regularities in the natural world that demonstrate why something works; the second is prescriptive knowledge, such as practical instructions, drawings or recipes that describe what is necessary for something to work.&lt;/p&gt;
    &lt;p&gt;Mokyr shows that prior to the Industrial Revolution, technological innovation was primarily based on prescriptive knowledge. People knew that something worked, but not why. Propositional knowledge, such as in mathematics and natural philosophy, was developed without reference to prescriptive knowledge, which made it difficult, even impossible, to build upon existing knowledge. Attempted innovations were often haphazard or had approaches that someone with adequate propositional knowledge would have understood were futile – such as building a perpetual motion machine or using alchemy to make gold.&lt;/p&gt;
    &lt;p&gt;The 16th and 17th centuries witnessed the Scientific Revolution as part of the Enlightenment. Scientists began to insist upon precise measurement methods, controlled experiments, and that results should be reproducible, leading to improved feedback between propositional and prescriptive knowledge. This increased the accumulation of useful knowledge that could be utilised in the production of goods and services. Typical examples include how the steam engine was improved thanks to contemporaneous insights into atmospheric pressure and vacuums, and advances in steel production due to the understanding of how oxygen reduces the carbon content of molten pig iron. Gains in useful knowledge facilitated the improvement of existing inventions and provided them with new areas of use.&lt;/p&gt;
    &lt;head rend="h3"&gt;From theory to practice&lt;/head&gt;
    &lt;p&gt;However, if new ideas are to be realised, practical, technical and, not least, commercial knowledge are all necessary. Without these, even the most brilliant ideas will remain on the drawing board, such as Leonardo da Vinci’s helicopter designs. Mokyr stressed that sustained growth first occurred in Britain because it was home to many skilled artisans and engineers. They were able to understand designs and transform ideas into commercial products, and this was vital in achieving sustained growth.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduced resistance to change&lt;/head&gt;
    &lt;p&gt;Another factor that Mokyr claims is necessary for sustained growth is that society is open to change. Growth based upon technological change not only creates winners, it also creates losers. New inventions replace old technologies and can destroy existing structures and ways of working. He also showed that this is why new technology is often met with resistance from established interest groups who feel their privileges are threatened.&lt;/p&gt;
    &lt;p&gt;The Enlightenment brought a generally increased acceptance of change. New institutions, such as the British Parliament, did not provide the same opportunities for those with privilege to block change. Instead, representatives from interest groups had the opportunity to gather and reach mutually beneficial compromises. These changes to societal institutions removed a major barrier to sustained growth.&lt;/p&gt;
    &lt;p&gt;Propositional knowledge can sometimes also contribute to reducing resistance to new ideas. In the 19th century, Hungarian physician Ignaz Semmelweis realised that maternal mortality rates dropped drastically if physicians and other staff washed their hands. If he had known why and been able to prove the existence of dangerous bacteria that are killed by handwashing, his ideas may have had an earlier impact.&lt;/p&gt;
    &lt;head rend="h3"&gt;Growth – a transformative process&lt;/head&gt;
    &lt;p&gt;Joel Mokyr used historical observations to identify the factors necessary for sustained growth. Instead, inspired by modern data, Philippe Aghion and Peter Howitt constructed a mathematical economic model that shows how technological advancement leads to sustained growth. These approaches are different, but fundamentally they deal with the same questions and phenomena.&lt;/p&gt;
    &lt;p&gt;As we have seen above, economic growth in industrialised nations such as Britain and Sweden has been remarkably stable. However, below the surface, the reality is anything but stable. In the US, for example, over ten per cent of all companies go out of business every year, and just as many are started. Among the remaining businesses, a large number of jobs are created or disappear every year; even if these figures are not as high in other countries, the pattern is the same.&lt;/p&gt;
    &lt;p&gt;Aghion and Howitt realised that this transformative process of creative destruction, in which companies and jobs continually disappear and are replaced, is at the heart of the process that leads to sustained growth. A company that has an idea for a better product or a more efficient means of production can outcompete others to become the market leader. However, as soon as this happens, it creates an incentive for other companies to further improve the product or production method and so climb to the top of the ladder.&lt;/p&gt;
    &lt;head rend="h3"&gt;A groundbreaking model&lt;/head&gt;
    &lt;p&gt;A simplified description of some of the model’s important mechanisms would be that an economy includes companies with the best and most advanced technology; when these take out patents on their products they can be paid more than their production costs and thus profit from a monopoly. These are the companies that have moved to the top of the ladder. A patent offers protection from competition, but not from another company making a new patentable innovation. If the new product or production process is good enough, it can outcompete the old one and further climb the ladder.&lt;/p&gt;
    &lt;p&gt;The potential to profit from a monopoly, even temporarily, creates incentives for companies to invest in research and development (R&amp;amp;D). The longer a company believes it can remain at the top of the ladder, the stronger the incentives, and the greater the investment in R&amp;amp;D. However, more R&amp;amp;D will lead to the average time to innovation decreasing, and the company at the top being pushed off the ladder. In the economy, a balance arises between these forces that decide how much is invested in R&amp;amp;D, thus also deciding the speed of creative destruction and economic growth.&lt;/p&gt;
    &lt;p&gt;Money for investment in R&amp;amp;D originates in households’ savings. How much they save depends on the interest rate which, in turn, is affected by the growth rate of the economy. Production, R&amp;amp;D, the financial markets and household savings are therefore linked and cannot be analysed in isolation. Economists call a model in which different markets are in balance a macroeconomic model that has general equilibrium. The model that Aghion and Howitt presented in their 1992 paper was the first macroeconomic model for creative destruction to have general equilibrium.&lt;/p&gt;
    &lt;head rend="h3"&gt;Welfare effects&lt;/head&gt;
    &lt;p&gt;Aghion and Howitt’s model can be used to analyse whether there is an optimal volume of R&amp;amp;D, and thus economic growth, if the market has free reign and there is no political interference. Previous models, which did not analyse the economy as a whole, could not answer that question. It turned out that the answer was far from simple, because two mechanisms pull in different directions.&lt;/p&gt;
    &lt;p&gt;The first mechanism is based upon companies that invest in R&amp;amp;D understanding that their current profits from an innovation will not continue forever. Sooner or later, another company will launch a better product. From the perspective of society, however, the value of the old innovation does not disappear, because the new one builds upon the old knowledge. Outcompeted innovations thus have a greater value for society than for the companies that develop them, which makes the private incentives for R&amp;amp;D smaller than the gains to society as a whole. Society can therefore benefit from subsidising R&amp;amp;D.&lt;/p&gt;
    &lt;p&gt;The second mechanism looks at how, when one company succeeds in pushing another from the top of the ladder, the new company makes a profit while the old company’s profit disappears. The latter is often called “business stealing”, although it is of course not stealing in the legal sense. Therefore, even if the new innovation is only slightly better than the old one, profits may be significant and larger than the socioeconomic gains. Therefore, from a socioeconomic perspective, investments in R&amp;amp;D can be too large; technological development can be too rapid and growth too high. This creates arguments against society subsidising R&amp;amp;D.&lt;/p&gt;
    &lt;p&gt;Which of these two forces dominates depends on a range of factors, which vary from market to market and time to time. Aghion and Howitt’s theory is useful for the understanding of which measures will be most effective and the extent to which society needs to support R&amp;amp;D.&lt;/p&gt;
    &lt;head rend="h3"&gt;Research led to more research&lt;/head&gt;
    &lt;p&gt;The model that Aghion and Howitt constructed in 1992 has led to new research, including the study of levels of market concentration, which involves the number of companies that compete with each other. The researchers’ theory shows that concentrations that are both too high and too low are bad for the innovation process. Despite promising advances in technology, growth has fallen in recent decades. One explanation for this, based on Aghion and Howitt’s model, is that some companies have become too dominant. More forceful policies that aim to counteract too much market dominance may be necessary.&lt;/p&gt;
    &lt;p&gt;Another important lesson is that innovation creates winners and losers. This not only applies to companies, but also to their employees. High growth requires a lot of creative destruction, which means that more jobs disappear and there is potentially high unemployment. Therefore, it is important to support people who are affected while making it easy for them to move to more productive workplaces. Protecting workers but not jobs, for example through a system that is sometimes called flexicurity, may be the right solution.&lt;/p&gt;
    &lt;p&gt;The laureates also demonstrate the importance of society creating conditions conducive to skilled innovators and entrepreneurs. Social mobility, where your profession is not decided by your parents’ identity, is thus important for growth.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tools for future societies&lt;/head&gt;
    &lt;p&gt;Mokyr’s, Aghion’s and Howitt’s research helps us to understand contemporary trends and how we can deal with important problems. For example, Mokyr’s work shows that AI could reinforce the feedback between propositional and prescriptive knowledge, and increase the rate at which useful knowledge is accumulated.&lt;/p&gt;
    &lt;p&gt;It is apparent that, in the long run, sustained growth does not only have positive consequences for human wellbeing. First, sustained growth is not synonymous with sustainable growth. Innovations can have significant negative side effects. Mokyr argues that such negative effects sometimes initiate processes that uncover solutions to problems, making technological development a self-correcting process. Clearly, however, this often requires well-designed policies, such as in the areas of climate change, pollution, antibiotic resistance, increasing inequality and the unsustainable use of natural resources.&lt;/p&gt;
    &lt;p&gt;In conclusion, and perhaps most importantly, the laureates have taught us that sustained growth cannot be taken for granted. Economic stagnation, not growth, has been the norm for most of human history. Their work shows that we must be aware of, and counteract, threats to continued growth. These threats may come from a few companies being allowed to dominate the market, restrictions on academic freedom, expanding knowledge at regional rather than global levels, and blockades from potentially disadvantaged groups. If we fail to respond to these threats, the machine that has given us sustained growth, creative destruction, may cease working – and we would once again need to become accustomed to stagnation. We can avoid this if we heed the laureates’ vital insights.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further reading&lt;/head&gt;
    &lt;p&gt;Additional information on this year’s prizes, including a scientific background in English, is available on the website of the Royal Swedish Academy of Sciences, www.kva.se, and at www.nobelprize.org, where you can watch video from the press conferences, the Nobel Prize lectures and more. Information on exhibitions and activities related to the Nobel Prizes and the prize in economic sciences is available at www.nobelprizemuseum.se.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Royal Swedish Academy of Sciences has decided to award the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel 2025 to Joel Mokyr, Philippe Aghion and Peter Howitt&lt;/head&gt;
    &lt;p&gt;“for having explained innovation-driven economic growth”&lt;/p&gt;
    &lt;p&gt;with one half to&lt;/p&gt;
    &lt;p&gt;JOEL MOKYR&lt;lb/&gt;Born 1946 in Leiden, the Netherlands. PhD 1974 from Yale University, New Haven, CT, USA. Professor at Northwestern University, Evanston, IL, USA and Eitan Berglas School of Economics, Tel Aviv University, Israel.&lt;/p&gt;
    &lt;p&gt;“for having identified the prerequisites for sustained growth through technological progress”&lt;/p&gt;
    &lt;p&gt;and the other half jointly to&lt;/p&gt;
    &lt;p&gt;PHILIPPE AGHION&lt;lb/&gt;Born 1956 in Paris, France. PhD 1987 from Harvard University, Cambridge, MA, USA. Professor at Collège de France and INSEAD, Paris, France and The London School of Economics and Political Science, UK.&lt;/p&gt;
    &lt;p&gt;PETER HOWITT&lt;lb/&gt;Born 1946 in Canada. PhD 1973 from Northwestern University, Evanston, IL, USA. Professor at Brown University, Providence, RI, USA.&lt;/p&gt;
    &lt;p&gt;“for the theory of sustained growth through creative destruction”&lt;/p&gt;
    &lt;p&gt;Science Editors: Kerstin Enflo and John Hassler, members of the Committee for the Prize in Economic Sciences in Memory of Alfred Nobel&lt;lb/&gt;Illustrations: Johan Jarnestad&lt;lb/&gt;Translation: Clare Barnes&lt;lb/&gt;Editor: Sara Rylander&lt;lb/&gt;© The Royal Swedish Academy of Sciences&lt;/p&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6–13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nobelprize.org/prizes/economic-sciences/2025/popular-information/"/><published>2025-10-13T09:48:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566638</id><title>American solar farms</title><updated>2025-10-13T14:11:01.049420+00:00</updated><content>&lt;doc fingerprint="ab95e1f33b2412f0"&gt;
  &lt;main&gt;
    &lt;p&gt;Last week, Jake Stid, a postdoctoral research associate at Michigan State University, announced Ground-Mounted Solar Energy in the United States (GM-SEUS). This is a 15K-array, 2.9M-panel dataset of utility and commercial-grade solar farms across the lower 48 states plus the District of Columbia. This dataset was constructed by a team of researchers including alumni from NOAA, NASA and the USGS.&lt;/p&gt;
    &lt;p&gt;Below is a heatmap of the assets catalogued in this dataset.&lt;/p&gt;
    &lt;p&gt;GM-SEUS is broken up into two datasets, one for arrays and another panels. Below you can see a solar farm with the array outlined in red and the panels covered purple.&lt;/p&gt;
    &lt;p&gt;In this post, I'll explore GM-SEUS's Solar Farm dataset.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Workstation&lt;/head&gt;
    &lt;p&gt;I'm using a 5.7 GHz AMD Ryzen 9 9950X CPU. It has 16 cores and 32 threads and 1.2 MB of L1, 16 MB of L2 and 64 MB of L3 cache. It has a liquid cooler attached and is housed in a spacious, full-sized Cooler Master HAF 700 computer case.&lt;/p&gt;
    &lt;p&gt;The system has 96 GB of DDR5 RAM clocked at 4,800 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.&lt;/p&gt;
    &lt;p&gt;The system is powered by a 1,200-watt, fully modular Corsair Power Supply and is sat on an ASRock X870E Nova 90 Motherboard.&lt;/p&gt;
    &lt;p&gt;I'm running Ubuntu 24 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and ArcGIS Pro only supports Windows natively.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing Prerequisites&lt;/head&gt;
    &lt;p&gt;I'll use GDAL 3.9.3 and a few other tools to help analyse the data in this post.&lt;/p&gt;
    &lt;code&gt;$ sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable
$ sudo apt update
$ sudo apt install \
    gdal-bin \
    jq
&lt;/code&gt;
    &lt;p&gt;I'll use DuckDB v1.4.1, along with its H3, JSON, Lindel, Parquet and Spatial extensions, in this post.&lt;/p&gt;
    &lt;code&gt;$ cd ~
$ wget -c https://github.com/duckdb/duckdb/releases/download/v1.4.1/duckdb_cli-linux-amd64.zip
$ unzip -j duckdb_cli-linux-amd64.zip
$ chmod +x duckdb
$ ~/duckdb
&lt;/code&gt;
    &lt;code&gt;INSTALL h3 FROM community;
INSTALL lindel FROM community;
INSTALL json;
INSTALL parquet;
INSTALL spatial;
&lt;/code&gt;
    &lt;p&gt;I'll set up DuckDB to load every installed extension each time it launches.&lt;/p&gt;
    &lt;code&gt;$ vi ~/.duckdbrc
&lt;/code&gt;
    &lt;code&gt;.timer on
.width 180
LOAD h3;
LOAD lindel;
LOAD json;
LOAD parquet;
LOAD spatial;
&lt;/code&gt;
    &lt;p&gt;The maps in this post were mostly rendered with QGIS version 3.44. QGIS is a desktop application that runs on Windows, macOS and Linux. The application has grown in popularity in recent years and has ~15M application launches from users all around the world each month.&lt;/p&gt;
    &lt;p&gt;I used QGIS' Tile+ plugin to add basemaps from Esri to the maps in this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Analysis-Ready Data&lt;/head&gt;
    &lt;p&gt;I'll download a dataset containing the US CENSUS State codes. This will let me map the state ID in the arrays dataset to their state name.&lt;/p&gt;
    &lt;code&gt;$ wget https://gist.github.com/a8dx/2340f9527af64f8ef8439366de981168/raw/81d876daea10eab5c2675811c39bcd18a79a9212/US_State_Bounding_Boxes.csv
&lt;/code&gt;
    &lt;p&gt;I'll download the ZIP file of deliverables for GM-SEUS.&lt;/p&gt;
    &lt;code&gt;$ wget -O GMSEUS_v1_0.zip \
    'https://zenodo.org/records/14827819/files/GMSEUS_v1_0.zip?download=1'
$ unzip GMSEUS_v1_0.zip
&lt;/code&gt;
    &lt;p&gt;I'll extract the projection used. This proj4 string will be used to below to re-project the data into EPSG:4326.&lt;/p&gt;
    &lt;code&gt;$ gdalsrsinfo \
    -o proj4 \
    GMSEUS_v1_0/GPKG/GMSEUS_Arrays_Final.gpkg
&lt;/code&gt;
    &lt;code&gt;+proj=aea +lat_0=37.5 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs
&lt;/code&gt;
    &lt;p&gt;I'll use DuckDB to clean up the values and produce both a geometry field and a bounding box for each feature in this dataset. This will make working with this dataset remotely, such as from AWS S3, much easier.&lt;/p&gt;
    &lt;code&gt;$ ~/duckdb
&lt;/code&gt;
    &lt;p&gt;This following produced a ZStandard-compressed, spatially-sorted Parquet file of the arrays dataset. I dropped the Z dimension as it was unused. The unknown values have been turned into NULLs. The original GPKG file was 108 MB and the resulting Parquet file is 37 MB.&lt;/p&gt;
    &lt;code&gt;COPY (
   WITH a AS (
       SELECT * EXCLUDE(geom),
              ST_FORCE2D(
                ST_FLIPCOORDINATES(
                  ST_TRANSFORM(
                      geom,
                      '+proj=aea +lat_0=37.5 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs',
                      'EPSG:4326'))) geometry
       FROM   ST_READ('GMSEUS_v1_0/GPKG/GMSEUS_Arrays_Final.gpkg')
   )
   SELECT   a.* EXCLUDE (geometry,
                         tilt,
                         tiltEst,
                         instYr,
                         instYrLT,
                         effInit,
                         avgAzimuth,
                         avgLength,
                         avgSpace,
                         avgWidth),
            {'xmin': ST_XMIN(ST_EXTENT(geometry)),
             'ymin': ST_YMIN(ST_EXTENT(geometry)),
             'xmax': ST_XMAX(ST_EXTENT(geometry)),
             'ymax': ST_YMAX(ST_EXTENT(geometry))} AS bbox,
             ST_ASWKB(geometry) geometry,
             CASE WHEN instYr::INT     = -9999 THEN NULL ELSE instYr::INT   END AS instYr,
             CASE WHEN instYrLT::INT   = -9999 THEN NULL ELSE instYrLT::INT END AS instYrLT,
             CASE WHEN numRow::INT     = -9999 THEN NULL ELSE numRow::INT   END AS numRow,
             CASE WHEN tilt::INT       = -9999 THEN NULL ELSE tilt::INT     END AS tilt,
             CASE WHEN tiltEst::INT    = -9999 THEN NULL ELSE tiltEst::INT  END AS tiltEst,
             CASE WHEN effInit::INT    = -9999 THEN NULL ELSE effInit       END AS effInit,
             CASE WHEN avgAzimuth::INT = -9999 THEN NULL ELSE avgAzimuth    END AS avgAzimuth,
             CASE WHEN avgLength::INT  = -9999 THEN NULL ELSE avgLength     END AS avgLength,
             CASE WHEN avgSpace::INT   = -9999 THEN NULL ELSE avgSpace      END AS avgSpace,
             CASE WHEN avgWidth::INT   = -9999 THEN NULL ELSE avgWidth      END AS avgWidth,
             b.NAME state_name
   FROM     a
   JOIN     'US_State_Bounding_Boxes.csv' b ON a.STATEFP = b.STATEFP
   ORDER BY HILBERT_ENCODE([ST_Y(ST_CENTROID(geometry)),
                            ST_X(ST_CENTROID(geometry))]::double[2])
) TO 'arrays.parquet' (
   FORMAT            'PARQUET',
   CODEC             'ZSTD',
   COMPRESSION_LEVEL 22,
   ROW_GROUP_SIZE    15000);
&lt;/code&gt;
    &lt;p&gt;The original GPKG file for the panels dataset was 1.1 GB and the resulting Parquet file is 334 MB.&lt;/p&gt;
    &lt;code&gt;COPY (
   WITH a AS (
       SELECT * EXCLUDE(geom),
              ST_FORCE2D(
                ST_FLIPCOORDINATES(
                  ST_TRANSFORM(
                      geom,
                      '+proj=aea +lat_0=37.5 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs',
                      'EPSG:4326'))) geometry
       FROM   ST_READ('GMSEUS_v1_0/GPKG/GMSEUS_Panels_Final.gpkg')
   )
   SELECT   * EXCLUDE (geometry,
                       rowSpace),
            {'xmin': ST_XMIN(ST_EXTENT(geometry)),
             'ymin': ST_YMIN(ST_EXTENT(geometry)),
             'xmax': ST_XMAX(ST_EXTENT(geometry)),
             'ymax': ST_YMAX(ST_EXTENT(geometry))} AS bbox,
             ST_ASWKB(geometry) geometry,
             CASE WHEN rowSpace::INT = -9999 THEN NULL ELSE rowSpace END AS rowSpace
   FROM     a
   ORDER BY HILBERT_ENCODE([ST_Y(ST_CENTROID(geometry)),
                            ST_X(ST_CENTROID(geometry))]::double[2])
) TO 'panels.parquet' (
   FORMAT            'PARQUET',
   CODEC             'ZSTD',
   COMPRESSION_LEVEL 22,
   ROW_GROUP_SIZE    15000);
&lt;/code&gt;
    &lt;head rend="h2"&gt;Solar Arrays&lt;/head&gt;
    &lt;p&gt;The arrays Parquet file has 15,017 rows. Below is an example record.&lt;/p&gt;
    &lt;code&gt;$ echo "SELECT * EXCLUDE(bbox,
                         geometry),
               bbox::JSON bbox
        FROM   'arrays.parquet'
        LIMIT  1" \
    | ~/duckdb -json \
    | jq -S .
&lt;/code&gt;
    &lt;code&gt;[
  {
    "COUNTYFP": "019",
    "GCR1": 0.6996,
    "GCR2": 0.614,
    "STATEFP": "45",
    "Source": "OSM",
    "arrayID": 2807,
    "avgAzimuth": 170.63,
    "avgLength": 47.76166666666666,
    "avgSpace": 3.003333333333333,
    "avgWidth": 4.776666666666666,
    "bbox": {
      "xmax": -79.97229830431786,
      "xmin": -79.97325770533094,
      "ymax": 32.87833627192598,
      "ymin": 32.87808294640646
    },
    "capMW": 0.246,
    "capMWest": 0.246,
    "effInit": 0.197963503102977,
    "instYr": 2021,
    "instYrLT": 2021,
    "latitude": 32.87818725544087,
    "longitude": -79.97276617375104,
    "modType": "c-si",
    "mount": "fixed_axis",
    "nativeID": "9324",
    "newBound": 1,
    "numRow": 6.0,
    "numRow_1": 6,
    "state_name": "South Carolina",
    "tilt": 30,
    "tiltEst": 30,
    "totArea": 1779.0,
    "totRowArea": 1244.93,
    "version": "v1.0"
  }
]
&lt;/code&gt;
    &lt;p&gt;Below are the field names, data types, percentages of NULLs per column, number of unique values and minimum and maximum values for each column.&lt;/p&gt;
    &lt;code&gt;$ ~/duckdb
&lt;/code&gt;
    &lt;code&gt;SELECT   column_name,
         column_type,
         null_percentage,
         approx_unique,
         min,
         max
FROM     (SUMMARIZE
          FROM READ_PARQUET('arrays.parquet'))
WHERE    column_name != 'geometry'
AND      column_name != 'bbox'
ORDER BY 1;
&lt;/code&gt;
    &lt;code&gt;┌─────────────┬─────────────┬─────────────────┬───────────────┬─────────────────────┬────────────────────┐
│ column_name │ column_type │ null_percentage │ approx_unique │         min         │        max         │
│   varchar   │   varchar   │  decimal(9,2)   │     int64     │       varchar       │      varchar       │
├─────────────┼─────────────┼─────────────────┼───────────────┼─────────────────────┼────────────────────┤
│ COUNTYFP    │ VARCHAR     │            0.00 │           235 │ 001                 │ 810                │
│ GCR1        │ DOUBLE      │            0.00 │          5057 │ 0.1047              │ 1.0                │
│ GCR2        │ DOUBLE      │            0.00 │          5013 │ 0.1245              │ 0.988              │
│ STATEFP     │ VARCHAR     │            0.00 │            49 │ 01                  │ 56                 │
│ Source      │ VARCHAR     │            0.00 │             6 │ CCVPV               │ USPVDB             │
│ arrayID     │ BIGINT      │            0.00 │         13155 │ 1                   │ 15017              │
│ avgAzimuth  │ DOUBLE      │           32.84 │          4295 │ 25.0                │ 269.27             │
│ avgLength   │ DOUBLE      │           39.79 │          8358 │ 4.02                │ 449.5004           │
│ avgSpace    │ DOUBLE      │           39.79 │          8623 │ 0.024               │ 20.0               │
│ avgWidth    │ DOUBLE      │           39.79 │          9185 │ 0.67625             │ 29.80222222222222  │
│ capMW       │ DOUBLE      │            0.00 │          5280 │ 0.001250225184651   │ 1051.703           │
│ capMWest    │ DOUBLE      │            0.00 │          7863 │ 0.004               │ 3170.1             │
│ effInit     │ DOUBLE      │            0.49 │            39 │ 0.132210289727273   │ 0.205484167047619  │
│ instYr      │ INTEGER     │            0.00 │            24 │ 1985                │ 2024               │
│ instYrLT    │ INTEGER     │            0.24 │            17 │ 2009                │ 2023               │
│ latitude    │ DOUBLE      │            0.00 │         16986 │ 25.53796582594631   │ 48.99547137225406  │
│ longitude   │ DOUBLE      │            0.00 │         15656 │ -124.10440474967092 │ -67.15066374183608 │
│ modType     │ VARCHAR     │            0.00 │             3 │ c-si                │ thin-film          │
│ mount       │ VARCHAR     │            0.00 │            10 │ dual_axis           │ unknown            │
│ nativeID    │ VARCHAR     │            0.00 │         15141 │ 1                   │ York Solar         │
│ newBound    │ BIGINT      │            0.00 │             2 │ 0                   │ 1                  │
│ numRow      │ DOUBLE      │            0.00 │          1461 │ 0.0                 │ 56782.0            │
│ numRow_1    │ INTEGER     │            0.00 │          1117 │ 0                   │ 56782              │
│ state_name  │ VARCHAR     │            0.00 │            57 │ Alabama             │ Wyoming            │
│ tilt        │ INTEGER     │           55.46 │            47 │ 0                   │ 83                 │
│ tiltEst     │ INTEGER     │           55.46 │            30 │ 10                  │ 43                 │
│ totArea     │ DOUBLE      │            0.00 │         13182 │ 54.0                │ 13735113.0         │
│ totRowArea  │ DOUBLE      │            0.00 │         15396 │ 44.97               │ 7223924.662        │
│ version     │ VARCHAR     │            0.00 │             1 │ v1.0                │ v1.0               │
├─────────────┴─────────────┴─────────────────┴───────────────┴─────────────────────┴────────────────────┤
│ 29 rows                                                                                      6 columns │
└────────────────────────────────────────────────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;I'll generate a heatmap of the asset locations in this dataset.&lt;/p&gt;
    &lt;code&gt;CREATE OR REPLACE TABLE h3_4_stats AS
    SELECT   H3_LATLNG_TO_CELL(
                bbox.ymin,
                bbox.xmin, 4) AS h3_4,
             COUNT(*) num_buildings
    FROM     READ_PARQUET('arrays.parquet')
    WHERE    bbox.xmin BETWEEN -178.5 AND 178.5
    GROUP BY 1;

COPY (
    SELECT ST_ASWKB(H3_CELL_TO_BOUNDARY_WKT(h3_4)::geometry) geometry,
           num_buildings
    FROM   h3_4_stats
) TO 'h3_4_stats.gpkg'
  WITH (FORMAT GDAL,
        DRIVER 'GPKG',
        LAYER_CREATION_OPTIONS 'WRITE_BBOX=YES');
&lt;/code&gt;
    &lt;p&gt;Normally I would produce a Parquet file as even with 10s of thousands of records it'll generate in seconds versus a minute or so with GPKG. But ArcGIS Pro 3.5 didn't want to open the Parquet file I generated. QGIS 3.44 was fine with it but I wanted to use Esri's Nova basemap for the rendering below.&lt;/p&gt;
    &lt;p&gt;ArcGIS Pro 3.6 should be released sometime in the next few weeks so I'll re-examine this issue when it's out.&lt;/p&gt;
    &lt;p&gt;Below is the relationship between the sources of data and the installation year.&lt;/p&gt;
    &lt;code&gt;PIVOT    'arrays.parquet'
ON       Source
USING    COUNT(*)
GROUP BY instYr
ORDER BY instYr;
&lt;/code&gt;
    &lt;code&gt;┌────────┬───────┬───────┬───────────────┬───────┬───────┬────────┐
│ instYr │ CCVPV │ CWSD  │ GMSEUSgeorect │  OSM  │  SAM  │ USPVDB │
│ int32  │ int64 │ int64 │     int64     │ int64 │ int64 │ int64  │
├────────┼───────┼───────┼───────────────┼───────┼───────┼────────┤
│   1985 │     0 │     0 │             0 │     0 │     0 │      1 │
│   1986 │     0 │     0 │             0 │     1 │     0 │      0 │
│   2002 │     0 │     0 │             0 │     0 │     0 │      1 │
│   2005 │     0 │     0 │             0 │    26 │     0 │      0 │
│   2006 │     0 │     0 │             0 │     2 │     0 │      1 │
│   2007 │     0 │     0 │             0 │    44 │     0 │      5 │
│   2008 │     0 │     0 │             0 │    58 │     1 │     11 │
│   2009 │     5 │     0 │             0 │    10 │     5 │     19 │
│   2010 │    20 │     0 │             0 │    71 │    20 │     37 │
│   2011 │    24 │     0 │             2 │   193 │    30 │    102 │
│   2012 │    59 │     0 │             2 │   267 │    88 │    157 │
│   2013 │    83 │     0 │             3 │   259 │    82 │    209 │
│   2014 │   102 │     0 │             1 │   335 │   119 │    291 │
│   2015 │   107 │     3 │             0 │   532 │   125 │    320 │
│   2016 │   145 │     1 │             2 │   564 │   170 │    412 │
│   2017 │   135 │     0 │             1 │   661 │   167 │    476 │
│   2018 │    66 │    34 │             4 │   644 │   210 │    414 │
│   2019 │    28 │    39 │             6 │   467 │   178 │    453 │
│   2020 │    10 │    75 │             1 │   437 │   186 │    496 │
│   2021 │     5 │    33 │             6 │   406 │   241 │    446 │
│   2022 │     1 │   173 │             3 │   231 │   354 │    166 │
│   2023 │     0 │     0 │             3 │   176 │   722 │    134 │
│   2024 │     0 │     0 │             0 │    31 │  1571 │      0 │
├────────┴───────┴───────┴───────────────┴───────┴───────┴────────┤
│ 23 rows                                               7 columns │
└─────────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;Below is the relationship between the mount and mod type.&lt;/p&gt;
    &lt;code&gt;PIVOT    'arrays.parquet'
ON       modType
USING    COUNT(*)
GROUP BY mount
ORDER BY mount;
&lt;/code&gt;
    &lt;code&gt;┌─────────────┬───────┬───────┬───────────┐
│    mount    │ c-si  │  csp  │ thin-film │
│   varchar   │ int64 │ int64 │   int64   │
├─────────────┼───────┼───────┼───────────┤
│ dual_axis   │   301 │    18 │         1 │
│ fixed_axis  │  6057 │    32 │       208 │
│ mixed       │     2 │     0 │         0 │
│ mixed_df    │   189 │     7 │         0 │
│ mixed_dfs   │    94 │     0 │         0 │
│ mixed_ds    │    38 │     1 │         0 │
│ mixed_fs    │    60 │     0 │         1 │
│ single_axis │  2876 │    11 │       231 │
│ unknown     │  4885 │     5 │         0 │
└─────────────┴───────┴───────┴───────────┘
&lt;/code&gt;
    &lt;p&gt;Below are the array capacity counts rounded to the neared 100 MW and broken down by source.&lt;/p&gt;
    &lt;code&gt;WITH a AS (
    SELECT   Source,
             ROUND(capMW / 100) * 100 AS capacity,
             COUNT(*) num_recs
    FROM     'arrays.parquet'
    GROUP BY 1, 2
)
PIVOT    a
ON       Source
USING    SUM(num_recs)
GROUP BY capacity
ORDER BY capacity;
&lt;/code&gt;
    &lt;code&gt;┌──────────┬────────┬────────┬───────────────┬────────┬────────┬────────┐
│ capacity │ CCVPV  │  CWSD  │ GMSEUSgeorect │  OSM   │  SAM   │ USPVDB │
│  double  │ int128 │ int128 │    int128     │ int128 │ int128 │ int128 │
├──────────┼────────┼────────┼───────────────┼────────┼────────┼────────┤
│      0.0 │    790 │    356 │            33 │   5295 │   4022 │   3669 │
│    100.0 │   NULL │      2 │          NULL │     67 │    143 │    350 │
│    200.0 │   NULL │   NULL │             1 │     22 │     49 │     73 │
│    300.0 │   NULL │   NULL │          NULL │     17 │     21 │     49 │
│    400.0 │   NULL │   NULL │          NULL │      6 │     13 │      7 │
│    500.0 │   NULL │   NULL │          NULL │      4 │     11 │      2 │
│    600.0 │   NULL │   NULL │          NULL │      2 │      3 │   NULL │
│    700.0 │   NULL │   NULL │          NULL │      1 │      3 │   NULL │
│    800.0 │   NULL │   NULL │          NULL │   NULL │      2 │      1 │
│    900.0 │   NULL │   NULL │          NULL │   NULL │      1 │   NULL │
│   1000.0 │   NULL │   NULL │          NULL │   NULL │      1 │   NULL │
│   1100.0 │   NULL │   NULL │          NULL │      1 │   NULL │   NULL │
├──────────┴────────┴────────┴───────────────┴────────┴────────┴────────┤
│ 12 rows                                                     7 columns │
└───────────────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;head rend="h2"&gt;Solar Panels&lt;/head&gt;
    &lt;p&gt;The panels Parquet file has 2,917,782 rows. Below is an example record.&lt;/p&gt;
    &lt;code&gt;$ echo "SELECT * EXCLUDE(bbox,
                         geometry),
               bbox::JSON bbox
        FROM   'panels.parquet'
        LIMIT  1" \
    | ~/duckdb -json \
    | jq -S .
&lt;/code&gt;
    &lt;code&gt;[
  {
    "Source": "gmseus",
    "arrayID": 2807.0,
    "bbox": {
      "xmax": -79.97312295800064,
      "xmin": -79.97325770533483,
      "ymax": 32.87833627193374,
      "ymin": 32.87830393275682
    },
    "panelID": 2620732,
    "rowArea": 29.1,
    "rowAzimuth": 174.62,
    "rowLength": 12.77,
    "rowMount": "fixed_axis",
    "rowSpace": 8.42,
    "rowWidth": 3.0,
    "version": "v1.0"
  }
]
&lt;/code&gt;
    &lt;p&gt;Below are the field names, data types, percentages of NULLs per column, number of unique values and minimum and maximum values for each column.&lt;/p&gt;
    &lt;code&gt;$ ~/duckdb
&lt;/code&gt;
    &lt;code&gt;SELECT   column_name,
         column_type,
         null_percentage,
         approx_unique,
         min,
         max
FROM     (SUMMARIZE
          FROM READ_PARQUET('panels.parquet'))
WHERE    column_name != 'geometry'
AND      column_name != 'bbox'
ORDER BY 1;
&lt;/code&gt;
    &lt;code&gt;┌─────────────┬─────────────┬─────────────────┬───────────────┬───────────────┬─────────────┐
│ column_name │ column_type │ null_percentage │ approx_unique │      min      │     max     │
│   varchar   │   varchar   │  decimal(9,2)   │     int64     │    varchar    │   varchar   │
├─────────────┼─────────────┼─────────────────┼───────────────┼───────────────┼─────────────┤
│ Source      │ VARCHAR     │            0.00 │             3 │ CCVPV         │ gmseus      │
│ arrayID     │ DOUBLE      │            0.08 │          9451 │ 1.0           │ 15017.0     │
│ panelID     │ BIGINT      │            0.00 │       2703164 │ 1             │ 2917782     │
│ rowArea     │ DOUBLE      │            0.00 │         88974 │ 15.01         │ 1999.76     │
│ rowAzimuth  │ DOUBLE      │            0.00 │         14901 │ 90.0          │ 270.0       │
│ rowLength   │ DOUBLE      │            0.00 │         26759 │ 4.02          │ 530.05      │
│ rowMount    │ VARCHAR     │            0.00 │             3 │ dual_axis     │ single_axis │
│ rowSpace    │ DOUBLE      │            0.17 │         13376 │ 7.4765186e-08 │ 20.0        │
│ rowWidth    │ DOUBLE      │            0.00 │          1863 │ 0.45          │ 102.14      │
│ version     │ VARCHAR     │            0.00 │             1 │ v1.0          │ v1.0        │
├─────────────┴─────────────┴─────────────────┴───────────────┴───────────────┴─────────────┤
│ 10 rows                                                                         6 columns │
└───────────────────────────────────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;Below is the relationship between the sources of data and the row mount.&lt;/p&gt;
    &lt;code&gt;PIVOT    'panels.parquet'
ON       Source
USING    COUNT(*)
GROUP BY rowMount
ORDER BY rowMount;
&lt;/code&gt;
    &lt;code&gt;┌─────────────┬────────┬────────┬─────────┐
│  rowMount   │ CCVPV  │  OSM   │ gmseus  │
│   varchar   │ int64  │ int64  │  int64  │
├─────────────┼────────┼────────┼─────────┤
│ dual_axis   │     44 │   5975 │   80225 │
│ fixed_axis  │  13344 │ 118639 │  163512 │
│ single_axis │ 189699 │ 743371 │ 1602973 │
└─────────────┴────────┴────────┴─────────┘
&lt;/code&gt;
    &lt;p&gt;Of the 15,017 arrays in this dataset, only 5,358 have any panels in them.&lt;/p&gt;
    &lt;code&gt;.maxrows 20

SELECT   a.arrayID,
         COUNT(DISTINCT b.panelID)
FROM     READ_PARQUET('arrays.parquet') a
JOIN     READ_PARQUET('panels.parquet') b
ON       ST_COVERS(a.geometry, b.geometry)
GROUP BY a.arrayID
ORDER BY 2 DESC;
&lt;/code&gt;
    &lt;code&gt;┌─────────┬───────────────────────────┐
│ arrayID │ count(DISTINCT b.panelID) │
│  int64  │           int64           │
├─────────┼───────────────────────────┤
│   11958 │                     56762 │
│   14225 │                     51140 │
│   12162 │                     43741 │
│   12433 │                     37304 │
│   14461 │                     31898 │
│   13229 │                     30093 │
│    6589 │                     27080 │
│   13329 │                     25120 │
│   12597 │                     24054 │
│   12224 │                     23449 │
│      ·  │                         · │
│      ·  │                         · │
│      ·  │                         · │
│    1792 │                         1 │
│    2286 │                         1 │
│     863 │                         1 │
│    1816 │                         1 │
│    8997 │                         1 │
│   12358 │                         1 │
│    6564 │                         1 │
│    3845 │                         1 │
│    6574 │                         1 │
│     991 │                         1 │
├─────────┴───────────────────────────┤
│ 5358 rows (20 shown)      2 columns │
└─────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;Below is a solar farm in Nevada where some arrays have panels and others do not.&lt;/p&gt;
    &lt;p&gt;I was interested in seeing the solar farm with 56K panels. Below are its coordinates.&lt;/p&gt;
    &lt;code&gt;SELECT ST_CENTROID(geometry)
FROM   'arrays.parquet'
WHERE  arrayID = 11958;
&lt;/code&gt;
    &lt;code&gt;┌────────────────────────────────────────────────┐
│             st_centroid(geometry)              │
│                    geometry                    │
├────────────────────────────────────────────────┤
│ POINT (-115.34248808114013 35.611919498003175) │
└────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;Even this has arrays without marked panels.&lt;/p&gt;
    &lt;p&gt;I'm looking forward to v2 of this dataset with better panel detection. It'll be great to get a good approximation of how many are deployed in the US.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tech.marksblogg.com/american-solar-farms.html"/><published>2025-10-13T10:02:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566660</id><title>Two Paths to Memory Safety: CHERI and OMA</title><updated>2025-10-13T14:11:00.859006+00:00</updated><content>&lt;doc fingerprint="a4eb1c9974b7e359"&gt;
  &lt;main&gt;
    &lt;p&gt;The last year has been brutal for businesses globally. Taking examples from my home country, the UK, the cost is over £1B and still rising, as well as the loss of at least one life due to cybercrime.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Marks &amp;amp; Spencer lost £300M when ransomware crippled its systems for weeks.&lt;/item&gt;
      &lt;item&gt;The Co-op suffered a related attack, losing over £200M in sales and the customer data of more than 20 million people.&lt;/item&gt;
      &lt;item&gt;Jaguar Land Rover’s assembly lines have been shut down for weeks, haemorrhaging £70M per week and requiring a £1.5B loan secured by the government.&lt;/item&gt;
      &lt;item&gt;Transport for London’s systems were compromised, with the ensuing disruption lasting months, costing £39mn and exposing 5,000 customers’ banking details. Two teenagers are being prosecuted for the attack.&lt;/item&gt;
      &lt;item&gt;Most tragically, a patient at King’s College Hospital died after ransomware delayed critical blood test results. Speaking to friends that were sat in meetings to decide who got blood tests each day, the human toll was evident. Cyberattacks aren’t just about money!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These aren’t isolated incidents - they’re symptoms of a systemic vulnerability in how we build computer systems.&lt;/p&gt;
    &lt;p&gt;According to the Verizon 2025 Data Breach Investigations Report, credential abuse and exploitation of vulnerabilities continue to dominate as attack vectors, accounting for 22% and 20% of breaches respectively. The exploitation of vulnerabilities saw a 34% surge year-over-year, creating what Verizon describes as a “concerning threat landscape”.&lt;/p&gt;
    &lt;p&gt;We’re yet to learn the root causes and attack chains involved in each of the examples above, but many involved ransomware, which frequently uses software exploits as a post-initial-access vector to gain control of target systems and spread across a network.&lt;/p&gt;
    &lt;p&gt;Here’s the kicker: approximately 70% of all software vulnerabilities stem from a single root cause - memory safety issues. This isn’t a new problem. Google, Microsoft, Apple, Mozilla and the Linux Foundation have all reported similar figures for their software over the last two decades. The uncomfortable truth is that current CPUs are fundamentally incapable of preventing these vulnerabilities, and traditional software patches have proven woefully inadequate.&lt;/p&gt;
    &lt;p&gt;Rewriting all the world’s software into memory safe languages, such as C#, Java and Rust, is unviable. While new projects may be adopting Rust over C/C++, and some critical components are being rewritten into safe languages, the scale and depth of the C and C++ ecosystems makes it practically impossible to rewrite all the world’s unsafe software. The risk of introducing other (non-memory-safety) issues during a software rewrite also poses a substantial barrier. Given sufficient software compatibility, it is actually easier to swap the hardware!&lt;/p&gt;
    &lt;p&gt;Two architectural approaches have emerged to tackle this trillion-dollar problem at the hardware level:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CHERI: Capability Hardware Enhanced RISC Instructions - pioneered at University of Cambridge (UK), and&lt;/item&gt;
      &lt;item&gt;OMA: Object Memory Architecture - pioneered at University of Bristol (UK) and now being commercialised by Doubtless Computing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both aim to make memory-unsafe systems safe-by-design but they take different paths to get there. Understanding these differences matters because the choice between them will shape the security and performance characteristics of computing for decades to come.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Memory Safety Crisis&lt;/head&gt;
    &lt;p&gt;Before diving into solutions, it’s worth understanding what we’re solving. When software runs, it constantly allocates and deallocates memory - think of it like booking rooms in a hotel. Memory safety vulnerabilities arise when this process goes wrong. If you stay in the same hotel twice, you shouldn’t be able to access your old room even if you remember the number (use-after-free/ use-after-reallocate). Similarly, you shouldn’t be able to enter a neighbouring room (buffer overflow), or use a room without booking one in the first place (invalid pointer dereference). Software has these same problems with memory allocations (room bookings).&lt;/p&gt;
    &lt;p&gt;These bugs become catastrophic vulnerabilities when attackers exploit them to read sensitive data they shouldn’t access, manipulate critical system variables, or inject malicious code. The underlying architecture of today’s processors - paging-based virtual memory - lacks the granularity needed to enforce security within a single application or process.&lt;/p&gt;
    &lt;p&gt;Memory safety breaks down into three categories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Referential safety ensures pointers genuinely reference allocated memory and can’t be forged. Think of it as ensuring software has a valid booking for a room, ensuring accesses to memory are authorized, and that bookings can’t be faked.&lt;/item&gt;
      &lt;item&gt;Spatial safety prevents accessing memory outside allocated bounds - no going into neighbouring rooms.&lt;/item&gt;
      &lt;item&gt;Temporal safety addresses what happens over time, ensuring memory can’t be accessed after it’s been freed and reallocated. In our hotel analogy, a second stay at the hotel shouldn’t allow you to access your previous room, even if you remember the room number.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Traditional architectures like x86, Arm, and RISC-V rely on coarse-grained page-level protection (typically 4KB or larger pages), which is far too blunt an instrument for modern security needs.&lt;/p&gt;
    &lt;head rend="h2"&gt;CHERI: Capabilities Meet Legacy Systems&lt;/head&gt;
    &lt;p&gt;CHERI, developed over more than a decade by the University of Cambridge and SRI International, extends conventional instruction set architectures with hardware-enforced capabilities. A CHERI capability is a form of fat pointer - it contains not just a memory address but also bounds information, permissions, and validity metadata. Every memory access gets checked against these constraints in hardware, catching violations before they can be exploited.&lt;/p&gt;
    &lt;p&gt;The architecture provides strong referential and spatial safety guarantees. When you have a CHERI capability, you provably have legitimate access to a specific bounded region of memory, and the hardware won’t let you stray outside those bounds. CHERI achieves this while maintaining compatibility with existing paged memory architectures, which is both its greatest strength and a source of limitations.&lt;/p&gt;
    &lt;p&gt;Here’s where it gets interesting: CHERI’s capabilities are large. On a 64-bit system, a CHERI pointer requires 129 bits (including the hidden tag bit) - essentially double the data width of the base architecture. This decision to encode all protection metadata within the pointer itself has profound implications. Every data structure that stores pointers effectively doubles in memory consumption for those fields. Capabilities in memory (stack/heap) must be aligned to natural 128-bit boundaries. Cache lines, which are precious and limited, now hold fewer actual pointers. Memory bandwidth requirements increase because for each pointer you’re moving twice as much data around.&lt;/p&gt;
    &lt;p&gt;CHERI provides hardware-enforced referential and spatial safety but leaves temporal safety to software. You can achieve temporal memory safety with CHERI, but it requires modifying your memory allocator and implementing pointer revocation mechanisms - essentially software to scan memory to find and invalidate stale pointers. This software-based approach to temporal safety remains part of the trusted computing base and requires careful verification. It’s also closely related to software garbage collection.&lt;/p&gt;
    &lt;p&gt;Research has explored various temporal safety mechanisms for CHERI, but they all involve non-trivial software complexity and performance overhead. In theory, hardware acceleration may be possible but is likely to always require software involvement. This is because a CHERI capability covers a range of memory, which may include more than one object. Software allocation and object type information is required to differentiate objects and thus revoke capabilities appropriately.&lt;/p&gt;
    &lt;p&gt;The software ecosystem for CHERI has made impressive progress. Most code recompiles with minimal changes, though the capability width difference can require significant rewrites for certain applications. Additionally, it causes a division in the ISA where load/stores of capabilities must be handled separately from ordinary data. This leads to some complexity in the compiler to detect edge cases where the compiler does not know for certain whether a register or memory slot contains a capability or not. C/C++ code which abuses pointers by treating them as integers, which is uncommon but frequent enough to cause a headache, requires some effort to address.&lt;/p&gt;
    &lt;p&gt;Arm’s Morello project, which implemented CHERI on a modified Neoverse N1 core, revealed performance challenges that have pushed commercial CHERI efforts toward smaller embedded processors for the time being. Notably, Arm declined to join the CHERI Alliance, instead indicating they will take a step back from new work on Morello and wait to see if CHERI gains the long-sought commercial traction.&lt;/p&gt;
    &lt;head rend="h2"&gt;OMA: Rethinking Memory From the Ground Up&lt;/head&gt;
    &lt;p&gt;Doubtless Computing’s Object Memory Architecture takes a fundamentally different approach. Rather than extending paged memory, OMA implements object-based memory management directly in hardware. Every allocation becomes a first-class hardware object with its own identity, bounds, and metadata maintained by the processor itself.&lt;/p&gt;
    &lt;p&gt;This architectural choice enables several key advantages. OMA pointers are leaner - 65 bits on a 64-bit architecture, including the hidden tag bit. Rather than carrying all metadata with every pointer, OMA stores object information centrally in hardware-managed directories. This reduces memory bandwidth requirements and means that multiple pointers to the same object don’t duplicate metadata. The hardware maintains a complete understanding of object relationships and lifecycles, enabling optimizations that software-only approaches can’t match.&lt;/p&gt;
    &lt;p&gt;A critical differentiator is temporal safety. OMA implements garbage collection in hardware, scanning for and reclaiming unreachable objects in real-time as part of the processor’s normal operation. This isn’t the same as software garbage collection - it’s parallel, highly optimized, and doesn’t block program execution. By managing object lifecycles in hardware, OMA provides hardware-guaranteed temporal safety alongside referential and spatial protections, completing the trinity of memory safety properties.&lt;/p&gt;
    &lt;p&gt;It would be tempting to say that memory safety is solved by using a managed language like Java, JavaScript, Swift or Python. Unfortunately, this doesn’t hold up in practice. Managed language runtimes, as well as many supporting libraries, are written in C/C++ and suffer memory safety issues just as much as any other C/C++ code. The operating systems and hypervisors are also exposed to these languages, offering yet another attack surface. This leaves managed language apps vulnerable. Memory safe languages, including both Rust and managed languages, are a distinct improvement over traditional C and C++, but only hardware can provide the safety guarantees we need in today’s systems.&lt;/p&gt;
    &lt;p&gt;For managed languages like Java, JavaScript, Python, C# and Go, the OMA architecture delivers dramatic performance improvements. Doubtless Computing’s analysis of CPython 3.12 reveals that 32-44% of instructions are spent on memory management operations - allocation, deallocation, reference counting, and garbage collection. Moving these operations into parallel hardware execution, along with microarchitectural optimisations derived from hardware’s new understanding of the structure of data in memory, yields 2-5x speedups for managed language applications. Even C/C++ applications see 1.2-2x improvements as the hardware optimizes memory management functions and eliminates per-object metadata from cache.&lt;/p&gt;
    &lt;p&gt;The architecture maintains full source code compatibility for managed languages - all changes are confined to the runtime. For C/C++, the story is much the same as with CHERI: recompilation with modified standard libraries and a modified compiler, such as LLVM or GCC. Maintaining the pointer width the same as the data width, and the same alignment requirements, avoids the ISA-level split for handling pointers, which simplifies the compiler and improves compatibility with legacy C/C++ code. This compatibility approach differs from CHERI’s and aligns with OMA’s target market: server-class and application processors, where managed languages dominate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fundamental Trade-offs: Where the Architectures Diverge&lt;/head&gt;
    &lt;p&gt;The philosophical differences between CHERI and OMA create distinct trade-off profiles. CHERI carries all metadata with pointers, enabling incremental adoption where different parts of a program can use capabilities independently. OMA’s centralized metadata requires the hardware to maintain a consistent view of all objects but enables more aggressive optimization. CHERI works within the existing paged memory model, simplifying system software migration. OMA introduces a new memory model that requires deeper changes but delivers performance gains that paged architectures can’t match.&lt;/p&gt;
    &lt;p&gt;These differences manifest in pointer width - CHERI’s 129-bit capabilities versus OMA’s 65-bit pointers. While both exceed the base address width, the doubling effect in CHERI has more severe implications for data structure layouts, cache efficiency, and memory bandwidth. Research on CHERI implementations has shown there is a long road ahead to achieve performance parity for managed languages. In the meantime, OMA offers a shorter path with substantial speedups rather than equal performance.&lt;/p&gt;
    &lt;p&gt;Temporal safety represents perhaps the most significant divergence in security. CHERI’s software-based pointer revocation requires explicit memory scanning and manipulation, adding complexity to the trusted computing base and verification burden. OMA’s hardware garbage collection happens transparently and continuously, providing stronger guarantees with less software complexity. This matters enormously for total cost of ownership - every line of security-critical software that doesn’t need to be written, verified, and maintained is a win.&lt;/p&gt;
    &lt;p&gt;The instruction set philosophies differ too. CHERI historically opts for ISA changes beyond pure memory safety to achieve its security goals, which can complicate adoption. OMA has historically prioritized backward compatibility, though this is adaptable based on market requirements. The consensus in the industry is that software compatibility presents the primary barrier to new processor designs, which favours architectures that minimize disruption.&lt;/p&gt;
    &lt;head rend="h2"&gt;Industrial Relevance and Market Fit&lt;/head&gt;
    &lt;p&gt;CHERI and OMA target fundamentally different computing environments, which is why calling them competitors misses the point. They’re complementary solutions to a shared problem, each optimized for distinct use cases.&lt;/p&gt;
    &lt;p&gt;CHERI finds its natural home in embedded systems and microcontrollers. These environments predominantly use C, C++, or Rust with restricted or no dynamic memory allocation. The code bases are smaller and more amenable to the verification required to ensure CHERI capabilities are used correctly. The memory overhead from wider pointers, while still present, matters less in resource-constrained designs that carefully manage every allocation. Four companies - SCI Semiconductor, Codasip, lowRISC, and Secqai - are actively commercializing CHERI for embedded applications. SCI’s ICENI family of CHERIoT microcontrollers, built on Microsoft’s open-source CHERIoT-Ibex core, targets the IoT and operational technology markets. Codasip offers CHERI-enabled RISC-V IP cores for custom processor designs. lowRISC’s Sonata platform provides an open-source FPGA-based development environment for CHERIoT research and prototyping.&lt;/p&gt;
    &lt;p&gt;Arm’s experience with CHERI tells an important story about scaling limitations. The Morello project, which implemented CHERI on a modified Neoverse N1 server-class core, yielded results that Arm appears to have found unsatisfactory. There has been no apparent follow-up on the substantial initial investment made into the Arm Morello designs. This assessment seems to reflect the performance challenges that CHERI faces in larger systems.&lt;/p&gt;
    &lt;p&gt;OMA’s sweet spot sits at the opposite end of the spectrum. Application-class and server-class processors running managed languages benefit enormously from hardware-accelerated memory management. Python, Java, JavaScript, C#, and Go all share similar memory models that align naturally with OMA’s object-based approach. These environments already use garbage collection extensively, so moving that functionality into hardware removes overhead, rather than adding it as it would in embedded systems. The performance gains - up to 5x for managed languages - become transformational for data centre workloads where every percentage point of efficiency translates to millions in operating costs.&lt;/p&gt;
    &lt;p&gt;The market dynamics favour different adoption paths. CHERI benefits from strong government backing, particularly from the now-ended UK’s Digital Security by Design programme and recognition from the US White House and NSA. This institutional support hopes to accelerate adoption in defence and critical infrastructure applications. CHERI’s open-source foundation through the CHERI Alliance creates a broad ecosystem but limits opportunities for proprietary differentiation.&lt;/p&gt;
    &lt;p&gt;OMA’s proprietary nature and performance advantages position it for commercial data centre deployment. The technology directly addresses the performance problems that hindered CHERI at scale. While OMA lacks CHERI’s first-mover advantage and government momentum, it offers compelling value for cloud providers and enterprises running managed language workloads. The economic argument is straightforward: if you can eliminate 70% of vulnerabilities while quintupling performance for your Python services, the return on investment is measured in weeks from deployment, rather than years.&lt;/p&gt;
    &lt;p&gt;OMA’s proprietary technology makes it attractive for investment as it can be patented. However, CHERI’s openness makes it possible for independent security teams to verify the safety of the architecture. Open implementations of CHERI processors also enables those designs to be independently verified. Doubtless Computing will need to make its ISA public, which is inevitable anyway for a new CPU as customers will require it. Doubtless will also need to offer a public platform for independent researchers to build confidence in the security claims.&lt;/p&gt;
    &lt;head rend="h2"&gt;The CHERI Ecosystem: Who’s Building What&lt;/head&gt;
    &lt;p&gt;The CHERI Alliance, formally launched in 2024, coordinates standardization and adoption efforts across industry and academia. Founding members include the FreeBSD Foundation, Capabilities Limited, SCI Semiconductor, Codasip, lowRISC, and the University of Cambridge. Google’s participation as a founding member signals serious industry interest, though notably Arm is not a member.&lt;/p&gt;
    &lt;p&gt;SCI Semiconductor, based in Cambridge, leads commercialization of Microsoft’s open-source CHERIoT Ibex implementation for embedded systems. Their ICENI family of processors targets microcontroller applications in automotive, industrial control, defence, and aerospace. The company has secured strategic distribution through EPS Global, which specializes in automotive tier-one suppliers and contract manufacturers. SCI’s early access program, in collaboration with lowRISC, allows select partners to begin development on lowRISC’s FPGA-based Sonata platform with guaranteed migration paths to production silicon.&lt;/p&gt;
    &lt;p&gt;Codasip, a RISC-V processor IP vendor, offers the X730 - a CHERI-enabled 64-bit application-class core based on their A730 design. Their “Custom Compute” methodology allows customers to license CHERI-enhanced cores or customize them further using Codasip Studio. The company has donated a CHERI SDK built on open-source tools to the CHERI Alliance, making it freely available for anyone implementing CHERI on RISC-V. Codasip is also developing Linux kernel support for RISC-V CHERI, which will be crucial for broader adoption.&lt;/p&gt;
    &lt;p&gt;lowRISC, a not-for-profit organization spun out of Cambridge University, maintains the Sonata evaluation platform and leads the UK-government-funded Sunburst Project. Sonata provides a complete FPGA-based development environment for CHERIoT, enabling software development and hardware experimentation before silicon is available. The Sunburst Project’s recent expansion to include SCI Semiconductor aims to validate CHERIoT designs through commercial tapeout on GlobalFoundries’ 22nm process, with all project deliverables remaining open-source.&lt;/p&gt;
    &lt;p&gt;zeroRISC is a startup and partner in the OpenTitan project administered by lowRISC. Their goal is to commercialise the OpenTitan silicon IP through their Integrity Management Platform.&lt;/p&gt;
    &lt;p&gt;Microsoft’s role deserves special mention. Microsoft Research developed CHERIoT-Ibex, an open-source RISC-V core optimized for embedded systems. They’ve made this core freely available and co-maintain the CHERIoT Platform repository with SCI Semiconductor. David Weston, Microsoft’s VP of Enterprise and OS Security, has publicly endorsed SCI’s commercialization efforts, stating that CHERI represents a “promising technology that can be used to enhance computer security.” This corporate backing from a major software vendor adds credibility to the embedded CHERI ecosystem.&lt;/p&gt;
    &lt;p&gt;The UK government’s support through the now-ended Digital Security by Design programme and UKRI funding has been instrumental in advancing CHERI. The programme provided ~£190 million in research funding over five years and continues to support development through initiatives like Sunburst. This institutional backing, combined with endorsements from the US White House and NSA, positions CHERI advantageously for government and defence procurements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;CHERI and OMA represent two responses to the memory safety crisis, each with distinct strengths that make them suited to different computing environments. The notion that one must “win” while the other “loses” misunderstands the landscape - the computing world is large enough, and varied enough, that multiple approaches can and should coexist. Cybersecurity principles also demand diversity of solutions.&lt;/p&gt;
    &lt;p&gt;CHERI’s compatibility with existing paged memory architectures and incremental deployment model make it an excellent fit for embedded systems where code bases are manageable, languages are predominantly C/C++/Rust, and the verification burden is acceptable. The active CHERI ecosystem, backed by government support and open-source collaboration, has created momentum that shouldn’t be underestimated. For IoT devices, industrial control systems, and safety-critical embedded applications, CHERI offers a practical path to hardware-enforced memory safety that companies can adopt today.&lt;/p&gt;
    &lt;p&gt;OMA’s object-based architecture and integrated hardware garbage collection (IHGC) deliver transformational performance for managed language workloads. By tackling temporal safety in hardware alongside referential and spatial protections, OMA provides more complete memory safety with less software complexity. The performance gains - up to 5x for Python, Java, JavaScript, C#, and Go - directly address the scalability problems that have limited CHERI in larger systems. For data centres, cloud infrastructure, and application servers where managed languages dominate, OMA presents compelling advantages.&lt;/p&gt;
    &lt;p&gt;Both architectures eliminate memory safety vulnerabilities. The formal guarantees that CHERI can provide are a subset of what OMA delivers, since OMA includes hardware-enforced temporal safety. However, CHERI’s earlier start and ecosystem momentum matter significantly in technology adoption. The question isn’t which architecture is “better” in absolute terms but rather which is more appropriate for specific use cases and deployment contexts.&lt;/p&gt;
    &lt;p&gt;Looking ahead, memory safety will increasingly become a non-negotiable requirement. The UK National Cyber Security Centre, US White House, and NSA have all called for fundamental changes in how we build secure systems. The attacks on Marks &amp;amp; Spencer, Co-op, Jaguar Land Rover, the NHS, Transport for London, and many others, demonstrate that our current approach isn’t working. Software-only solutions like Rust, while valuable, face adoption barriers that make them insufficient on their own. Hardware-based memory safety, whether through CHERI, OMA, or future approaches we haven’t yet invented, represents the most practical path to eliminating this class of vulnerability at scale.&lt;/p&gt;
    &lt;p&gt;The semiconductor industry moves slowly, with design cycles measured in years and deployment timelines measured in decades. Today’s architectural decisions will shape computing security through 2040 and beyond. The good news is that we now have proven approaches to memory safety that work in real hardware. CHERI has demonstrated its viability in embedded systems. OMA has shown it can deliver both security and performance for managed languages with a hardware prototype on AWS Cloud FPGAs supporting CPython 3.12 and Jupyter Notebooks. The challenge now isn’t technical feasibility - it’s economic deployment and ecosystem coordination.&lt;/p&gt;
    &lt;p&gt;For embedded designers, CHERI offers immediate benefits with manageable overhead. For cloud and data centre operators, OMA promises to eliminate vulnerabilities while dramatically improving performance. The fundamental insight is that both approaches work by making the right choices for their target markets. We don’t need to pick one winner. We need both, deployed where each makes the most sense, steadily displacing the insecure architectures that enabled the attacks we’ve seen this year. The trillion-dollar memory safety problem is solvable - and the will to deploy the solutions we’ve built is growing as organisations can no longer afford the risk of being vulnerable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ednutting.com/2025/10/05/cheri-vs-oma.html"/><published>2025-10-13T10:05:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45566766</id><title>Matrices can be your Friends</title><updated>2025-10-13T14:11:00.572215+00:00</updated><content>&lt;doc fingerprint="2eac01b79ba60999"&gt;
  &lt;main&gt;
    &lt;p&gt;Take an OpenGL matrix:&lt;/p&gt;
    &lt;quote&gt;float m [ 16 ] ;Consider this as a 4x4 array with it's elements laid out into four columns like this:&lt;/quote&gt;
    &lt;quote&gt;m[0] m[4] m[ 8] m[12] m[1] m[5] m[ 9] m[13] m[2] m[6] m[10] m[14] m[3] m[7] m[11] m[15]WARNING: Mathematicians like to see their matrices laid out on paper this way (with the array indices increasing down the columns instead of across the rows as a programmer would usually write them). Look CAREFULLY at the order of the matrix elements in the layout above!&lt;/quote&gt;
    &lt;p&gt;...but we are OpenGL programmers - not mathematicians - right?! The reason OpenGL arrays are laid out in what some people would consider to be the opposite direction to mathematical convention is somewhat lost in the mists of time. However, it turns out to be a happy accident as we will see later.&lt;/p&gt;
    &lt;p&gt;If you are dealing with a matrix which only deals with rigid bodies (ie no scale, shear, squash, etc) then the last row (array elements 3,7,11 and 15) are always 0,0,0 and 1 respectively and so long as they always maintain those values, we can safely forget about them for now.&lt;/p&gt;
    &lt;p&gt;The first three elements of the rightmost column of the matrix is just the overall translation. If you imagine some kind of neat little compact object (like a teapot), then array elements 12,13 and 14 tell you where it is in the world. It doesn't matter what combinations of rotations and translations it took to produce the matrix, the rightmost column tells you where the object basically is. It is often fortunate that the OpenGL matrix array is laid out the way it is because it results in those three elements being consecutive in memory.&lt;/p&gt;
    &lt;p&gt;OK, so now we are down to only nine random-looking numbers. These are the top three elements of each of the first three columns - and collectively they represent the rotation of the object.&lt;/p&gt;
    &lt;p&gt;The easy way to decode those numbers is to imagine what happens to four points near to the origin after they are transformed by the matrix:&lt;/p&gt;
    &lt;quote&gt;(0,1,0) | /(0,0,1) | / |/___(1,0,0) (0,0,0)These are four vertices on a 1x1x1 cube that has one corner at the origin.&lt;/quote&gt;
    &lt;p&gt;After the matrix has transformed this cube, where does it end up?&lt;/p&gt;
    &lt;p&gt;Well, if we neglect the translation part (the bottom row), then the pure rotation part simply describes the new location of the points on the cube:&lt;/p&gt;
    &lt;quote&gt;(1,0,0) ---&amp;gt; ( m[0], m[1], m[2] ) (0,1,0) ---&amp;gt; ( m[4], m[5], m[6] ) (0,0,1) ---&amp;gt; ( m[8], m[9], m[10]) (0,0,0) ---&amp;gt; ( 0, 0, 0 )After that, you just add the translation onto each point so that:&lt;/quote&gt;
    &lt;quote&gt;(1,0,0) ---&amp;gt; ( m[0], m[1], m[2] ) + ( m[12], m[13], m[14] ) (0,1,0) ---&amp;gt; ( m[4], m[5], m[6] ) + ( m[12], m[13], m[14] ) (0,0,1) ---&amp;gt; ( m[8], m[9], m[10]) + ( m[12], m[13], m[14] ) (0,0,0) ---&amp;gt; ( 0, 0, 0 ) + ( m[12], m[13], m[14] )Once you know this, it becomes quite easy to use matrices to position objects exactly where you need them without messing around with multiple calls to glRotate.&lt;/quote&gt;
    &lt;p&gt;Just imagine a little cube at the origin - pretend it's firmly attached to your model. Think about where the cube ends up as the model moves - write down where it's vertices would end up and there is your matrix.&lt;/p&gt;
    &lt;p&gt;So, if I gave you this matrix:&lt;/p&gt;
    &lt;quote&gt;0.707, -0.707, 0, 10 0.707, 0.707, 0, 10 0 , 0 , 1, 0 0 , 0 , 0, 1...you could easily see that the X axis of that little cube is now pointing somewhere between the X and Y axes, the Y axis is pointing somewhere between Y and negative X and the Z axis is unchanged. The entire cube has been moved 10 units off in X and Y. This is a 45 degree rotation about Z and a 10,10,0 translation! You didn't need any hard math - just a mental picture of what the little cube did - and no concerns about the order of operations or anything hard like that. What would have happened to something out at 100,100,0? Well, just imagine it was glued to the cube (on the end of a long stick)...as the cube rotated, the thing at 100,100 would have moved quite a bit too - in fact, you can see that the rotation would put it onto the Y axis and the translation would have moved it 10 units up and to the right.&lt;/quote&gt;
    &lt;p&gt;With practice, you can figure out what that last row of numbers does to the little cube too.&lt;/p&gt;
    &lt;p&gt;So, would you like to know how to use a matrix to squash, stretch, shear, etc? Just think about where the axes of that little cube end up - write them down and you are done. What does a cube of jello look like when there is a strong wind blowing from X=-infinity?&lt;/p&gt;
    &lt;quote&gt;1, 0.3, 0, 0 0, 0.9, 0, 0 0, 0 , 1, 0 0, 0 , 0, 1Look - the Y axis is leaning a third of a unit to the right and the cube got a bit shorter.&lt;/quote&gt;
    &lt;p&gt;Suppose your cartoon character is going to jump vertically, and you want to do a bit of pre-squash before the jump... and post-stretch during the jump. Just gradually vary the matrix from:&lt;/p&gt;
    &lt;quote&gt;1 , 0 , 0, 0 1 , 0 , 0, 0 0 , 0.8, 0, 0 0 , 1.2, 0, 0 0 , 0 , 1, 0 ===&amp;gt; 0 , 0 , 1, 0 0 , 0 , 0, 1 0 , 0 , 0, 1Not bad - he got shorter then longer - how about getting a bit fatter too (conservation of cartoon volume) ?&lt;/quote&gt;
    &lt;quote&gt;1.2, 0 , 0 , 0 0.9,0 , 0 , 0 0 , 0.8, 0 , 0 0 ,1.2, 0 , 0 0 , 0 , 1.2, 0 ===&amp;gt; 0 ,0 ,0.9, 0 0 , 0 , 0 , 1 0 ,0 , 0 , 1Now the cube got smaller in Y and bigger in X and Z then got bigger in Y and smaller in X/Z...easy!&lt;/quote&gt;
    &lt;p&gt;Not only is it easier to think transforms out this way, but it's invariably more efficient too. By seeing the entire transformation as one whole operation on a unit cube, you save a long sequence of glRotate/glTranslate/glScale commands - which each imply a complicated set of multiply/add steps to concatenate the new transform with whatever is on the top of the stack.&lt;/p&gt;
    &lt;p&gt;Finally, there is one matrix that we all need to know - the "Identity" matrix:&lt;/p&gt;
    &lt;quote&gt;1, 0, 0, 0 0, 1, 0, 0 0, 0, 1, 0 0, 0, 0, 1As you can see, this matrix leaves all the axes completely alone and performs no translation. This is a "do nothing" matrix.&lt;/quote&gt;
    &lt;p&gt;Matrices are really easy - it's just a matter of looking at them pictorially.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sjbaker.org/steve/omniv/matrices_can_be_your_friends.html"/><published>2025-10-13T10:23:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45567153</id><title>The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel 2025</title><updated>2025-10-13T14:11:00.199776+00:00</updated><content>&lt;doc fingerprint="f71c327fa9d021e6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel 2025 was awarded "for having explained innovation-driven economic growth" with one half to Joel Mokyr "for having identified the prerequisites for sustained growth through technological progress" and the other half jointly to Philippe Aghion and Peter Howitt "for the theory of sustained growth through creative destruction"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6–13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
    &lt;head rend="h3"&gt;Explore prizes and laureates&lt;/head&gt;
    &lt;p&gt; Look for popular awards and laureates in different fields, and discover the history of the Nobel Prize. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nobelprize.org/prizes/economic-sciences/2025/summary/"/><published>2025-10-13T11:23:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45567770</id><title>Show HN: SQLite Online – 11 years of solo development, 11K daily users</title><updated>2025-10-13T14:10:59.950514+00:00</updated><content>&lt;doc fingerprint="5120987566fd4fbd"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;Chart for Data Science&lt;/head&gt;
    &lt;code&gt;-- Change first word "SELECT" to "QLINE-SELECT"&lt;/code&gt;
    &lt;quote&gt;SELECT QLINE-SELECT&lt;/quote&gt;
    &lt;code&gt;â&lt;/code&gt;
    &lt;code&gt;-- Axis X:&lt;/code&gt;
    &lt;code&gt;-- X - column name, axis: x1, x2, ..xn Value: Number&lt;/code&gt;
    &lt;code&gt;-- L - column name, axis: l Value: Text&lt;/code&gt;
    &lt;code&gt;-- T - column name, axis: t Value: UnixTime Number&lt;/code&gt;
    &lt;code&gt;-- Axis Y:&lt;/code&gt;
    &lt;code&gt;-- Y - column name, axis: y1, y2, ..yn Value: Number&lt;/code&gt;
    &lt;code&gt;-- Y - color line: y_cFF00FF (HEX6)&lt;/code&gt;
    &lt;code&gt;-- Option:&lt;/code&gt;
    &lt;code&gt;-- C - color point: c  Value: FF00FF (HEX6)&lt;/code&gt;
    &lt;code&gt;-- V - radius point: v  Value: Number&lt;/code&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QLINE-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QAREA-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QBAR-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QPIE-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QBUBBLE-SELECT example&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sqliteonline.com/"/><published>2025-10-13T12:46:52+00:00</published></entry></feed>