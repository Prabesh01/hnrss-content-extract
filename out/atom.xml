<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-05T00:50:50.533333+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45443662</id><title>Study confirms that pianists can shape piano timbre through touch</title><updated>2025-10-05T00:51:00.592929+00:00</updated><content>&lt;doc fingerprint="bd701ab9ab33ddb2"&gt;
  &lt;main&gt;
    &lt;p&gt;Summary: A new study has scientifically confirmed that pianists can change piano timbre mid-performance through touch alone. Using high-speed sensors, researchers captured key movements and showed that subtle differences in motion aligned with the timbre that listeners perceived.&lt;/p&gt;
    &lt;p&gt;The findings prove that timbre manipulation isn’t just a metaphor, but a skill rooted in precise motor control. Beyond music, this discovery highlights how refined body movement can shape artistic expression, with potential applications in training, therapy, and technology.&lt;/p&gt;
    &lt;p&gt;Key Facts&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scientific Proof: Pianists’ touch can directly alter piano timbre.&lt;/item&gt;
      &lt;item&gt;Motor Skill Link: High-speed sensors revealed subtle key movements shape sound.&lt;/item&gt;
      &lt;item&gt;Broader Impact: Applications include music education, rehabilitation, and interface design.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Source: NeuroPiano Institute&lt;/p&gt;
    &lt;p&gt;A research group led by Dr. Shinichi Furuya of the NeuroPiano Institute and Sony Computer Science Laboratories, Inc. announced research findings that for the first time scientifically clarified how pianists’ manipulations of keys alters piano timbre.&lt;/p&gt;
    &lt;p&gt;Creativity in painting, music, and other arts is underpinned by the ability to create diverse perceptual experiences for audiences. However, whether timbre could actually be changed mid-instrumental performance, and what physical motor skills would be required to do so, remained unclear.&lt;/p&gt;
    &lt;p&gt;The research group developed a unique sensor system that can measure piano key movements at a temporal resolution of 1,000 fps, and measured key movements when professional pianists expressed various timbres on the piano.&lt;/p&gt;
    &lt;p&gt;The results revealed that listeners could distinguish the pianists’ intended timbres, regardless of whether or not they had any piano performance training experience. The group further successfully identified the key movement features that produce these timbre differences.&lt;/p&gt;
    &lt;p&gt;This discovery addresses the over century-old question, “Can pianists alter timbre through touch?” by demonstrating that timbre manipulation through touch cultivated by pianists is not a mere sensory metaphor but a scientifically backed skill.&lt;/p&gt;
    &lt;p&gt;“These findings open the possibility of visualizing and teaching specific movement features that produce timbre, which would lead to more efficient practice and prevention of mislearning.&lt;/p&gt;
    &lt;p&gt;“They also reveal that high-level body motor control shapes artistic perception, suggesting potential applications across various disciplines, including rehabilitation, skill transfer, and human interface design.&lt;/p&gt;
    &lt;p&gt;These research findings will be published in the international scientific journal Proceedings of the National Academy of Sciences(PNAS).&lt;/p&gt;
    &lt;p&gt;More Information&lt;/p&gt;
    &lt;p&gt;Musicians and other performing artists, surgeons, traditional craftsmen and others considered experts in various fields acquire their skills through years of extensive training.&lt;/p&gt;
    &lt;p&gt;In particular, in the performing arts, it has long been thought that the mastering of physical motor skills that produce diverse perceptions is essential for embodying creativity.&lt;/p&gt;
    &lt;p&gt;For instance, while pitch and volume in instrumental performance clearly depend on manipulation of the instrument, there had been no scientific evidence for cases where “an instrument that should produce a certain sound produces a different timbre”—a phenomenon that was widely believed possible among performers and educators.&lt;/p&gt;
    &lt;p&gt;This question was discussed regarding the piano in Nature magazine in the early 20th century, but systematic perceptual experiments and data analysis had not been carried out to date, leaving the question unanswered.&lt;/p&gt;
    &lt;p&gt;Thus, the means for acquiring skills that produce diverse expressions remained unknown, and problems during this process—such as misrecognition of personal limitations and risk of injury or disability arising during skill-acquisition training—persisted.&lt;/p&gt;
    &lt;p&gt;An evidence-based understanding of the mechanisms of technical skill is essential for humans and systems to be able to recommend appropriate training methods, and for the resulting recommendations to be trusted by learners and teachers.&lt;/p&gt;
    &lt;p&gt;Background and History of the Research&lt;/p&gt;
    &lt;p&gt;A research team from the NeuroPiano Institute and Sony Computer Science Laboratories (Sony CSL) revealed that the timbral qualities pianists intended to express were conveyed to listeners, and that the high-precision control of fingertip movement was involved.&lt;/p&gt;
    &lt;p&gt;The research group used Hackkey, their proprietary high-precision non-contact sensor system, to measure the movements of all 88 keys at 1,000 fps (1 ms temporal precision) and 0.01 mm spatial resolution. This apparatus analyzed keyboard movements when 20 internationally renowned pianists performed with the intent to produce diverse timbral qualities, including bright/dark and light/heavy.&lt;/p&gt;
    &lt;p&gt;Additionally, the team carried out a psychophysical experiment, with 40 participants—including pianists and individuals with no musical experience—who listened to the recorded performances.&lt;/p&gt;
    &lt;p&gt;The results revealed that the pianists’ intended timbres were consistently perceived by the listeners, regardless of their musical experience. The listeners who were pianists, in particular, were able to distinguish timbral differences with greater sensitivity.&lt;/p&gt;
    &lt;p&gt;This timbral discernment was found to be possible even when controlling for volume and tempo, factors previously thought to influence timbral perception.&lt;/p&gt;
    &lt;p&gt;Data analysis using a linear mixed-effects (LME) model revealed that contributions to timbral differences are concentrated in a limited set of movement features (e.g., acceleration during escapement, deviation in hand synchronization). It was further experimentally confirmed that notes played by varying only one of these features were perceived by listeners as having a different timbre, providing the first empirical evidence of a causal relationship between key movement and timbre.&lt;/p&gt;
    &lt;p&gt;These findings have the following significance for musicians and educators:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Building a technical foundation to support artistic creativity：This research quantifies the “tacit knowledge” of how pianists produce timbre, paving the way for understanding an artist’s expressive intent and developing new educational methods and technologies that will maximize it. Furthermore, proving that the manipulation of timbre through touch cultivated by artists is a scientifically grounded skill rather than a mere sensory metaphor makes it possible to efficiently learn and acquire the skills to create timbral expressions—which had been difficult to verbalize in instruction to date—by applying it to recommendation systems that present the appropriate movement features to learners.&lt;/item&gt;
      &lt;item&gt;Illuminating the biological mechanisms that produce higher-order perception: The phenomenon in which the same sound can be perceived differently indicates advanced integration of human sensory and motor systems. This research clarifies how dexterous motor control produces higher-order perception and aesthetic experiences, opening new avenues for interdisciplinary research in neuroscience, psychology, and arts studies, and holds additional promise for applications across multiple fields, including skill transfer, rehabilitation, and human interface design.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Future Developments&lt;/p&gt;
    &lt;p&gt;This research has clarified the relationship between key movement features and piano timbre, suggesting the possibility for explicitly acquiring a repertoire of movements that can produce a diverse palette of perceptions.&lt;/p&gt;
    &lt;p&gt;This is essential for recommending evidence-based body use and practice methods in physical education for the performing arts, and for empowering both teachers and learners to pursue learning with confidence.&lt;/p&gt;
    &lt;p&gt;While perception research has until now focused mainly on lower-level perceptual information such as pitch, loudness, and rhythm, the advance of future research into timbre and other higher-level perceptual information is expected to lead to clarification of the underlying brain information processing mechanisms, as well as the development of training methods that skillfully utilize advanced technologies.&lt;/p&gt;
    &lt;p&gt;Moreover, the thrill of using one’s body to improve and achieve something that was once impossible is something that is shared across disciplines beyond music performance, including sports, cooking, painting, and even surgery. This research holds promise for generating ripple effects across multiple disciplines.&lt;/p&gt;
    &lt;p&gt;The involvement of science and technology in music learning has lagged behind significantly, compared to that in fields such as sports and medicine. As a result, many artists all over the world have long been beset with the problem of embodying artistic expression and creativity while being constrained by physical and mental limitations.&lt;/p&gt;
    &lt;p&gt;The knowledge regarding the foundational skills for producing diverse expressions provided by this research will contribute to the creation of a future society where artists are liberated from physical and mental constraints and can fully embody their creativity.&lt;/p&gt;
    &lt;p&gt;This will be achieved through the establishment of a new evidence-based form of music education grounded in dynaformics, the science of music performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Questions Answered:&lt;/head&gt;
    &lt;p&gt;A: Yes. The study scientifically confirmed that professional pianists can manipulate timbre mid-performance through subtle key movement differences.&lt;/p&gt;
    &lt;p&gt;A: Researchers used a high-speed sensor system (1,000 fps) to track key motions while pianists played with different timbre intentions, then matched these movements to listener perceptions.&lt;/p&gt;
    &lt;p&gt;A: It validates a century-old debate, showing that timbre alteration is a teachable motor skill, with implications for music education, rehabilitation, and human interface design.&lt;/p&gt;
    &lt;head rend="h2"&gt;About this touch perception research news&lt;/head&gt;
    &lt;p&gt;Author: Public Relations&lt;lb/&gt;Source: NeuroPiano Institute&lt;lb/&gt;Contact: Public Relations – NeuroPiano Institute&lt;lb/&gt;Image: The image is credited to Neuroscience News&lt;/p&gt;
    &lt;p&gt;Original Research: Closed access.&lt;lb/&gt;“Motor origins of timbre in piano performance” by Shinichi Furuya et al. PNAS&lt;/p&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Motor origins of timbre in piano performance&lt;/p&gt;
    &lt;p&gt;Creativity in the arts, such as painting and musical performance, hinges on the ability to produce a wide spectrogram of perceptual experiences. In music, it has long been believed that the timbre of tones can be altered by nuanced movements of performers.&lt;/p&gt;
    &lt;p&gt;Previous studies have described relationships between fundamental elements of auditory perceptions (e.g., loudness, tempo) and physical movements (e.g., force, speed), but it remains unknown whether and how delicate features of perceptual experiences such as tone timbre are manipulated through dexterous motor skills. Here, we bridge this gap using a twofold experimental approach.&lt;/p&gt;
    &lt;p&gt;First, our listening test revealed that the timbral qualities pianists intended to express in piano playing were perceived as intended by both pianists and musically untrained individuals, with pianists showing a greater perceptual sensitivity to different timbres.&lt;/p&gt;
    &lt;p&gt;Second, through a motor behavioral experiment using a noncontact, high-resolution sensing system, we identified five specific movement features in piano touch that were intricately linked to three categories of perceived timbre; weight, clarity, and brightness.&lt;/p&gt;
    &lt;p&gt;Furthermore, the direct manipulation of a specific key movement feature resulted in systematic changes in perceived timbre, providing evidence for a causal relationship.&lt;/p&gt;
    &lt;p&gt;The result indicates that pianists share common motor skills to modify perceived tone timbre by manipulating specific movement features.&lt;/p&gt;
    &lt;p&gt;Our findings underscore the pivotal roles of subtle physical gestures in creating the rich timbral palette of piano tones, advancing our understanding of the intersection between motor control and artistic expression.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://neurosciencenews.com/piano-touch-timbre-neuroscience-29755/"/><published>2025-10-01T21:15:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45455164</id><title>Where it's at://</title><updated>2025-10-05T00:51:00.200737+00:00</updated><content>&lt;doc fingerprint="9e199d86dcdee5d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Where It's at://&lt;/head&gt;
    &lt;p&gt;October 2, 2025&lt;/p&gt;
    &lt;p&gt;You might have heard about the AT protocol (if not, read this!)&lt;/p&gt;
    &lt;p&gt;Together, all servers speaking the AT protocol comprise the atmosphere—a web of hyperlinked JSON. Each piece of JSON on the atmosphere has its own &lt;code&gt;at://&lt;/code&gt; URI:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;at://ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://danabra.mov/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://tessa.germnetwork.com/pub.leaflet.publication/3lzz6juivnc2d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But where do they point, exactly?&lt;/p&gt;
    &lt;p&gt;Given an &lt;code&gt;at://&lt;/code&gt; URI, how do you locate the corresponding JSON?&lt;/p&gt;
    &lt;p&gt;In this post, I’ll show you the exact process of resolving an &lt;code&gt;at://&lt;/code&gt; URI step by step. Turns out, this is also a great way to learn the details of how &lt;code&gt;at://&lt;/code&gt; works.&lt;/p&gt;
    &lt;p&gt;Let’s start with the structure of a URI itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;The User as the Authority&lt;/head&gt;
    &lt;p&gt;As you might know, a URI often contains a scheme (for example, &lt;code&gt;https://&lt;/code&gt;), an authority (like &lt;code&gt;wikipedia.com&lt;/code&gt;), a path (like &lt;code&gt;/Main_Page&lt;/code&gt;), and maybe a query.&lt;/p&gt;
    &lt;p&gt;In most protocols, including &lt;code&gt;https://&lt;/code&gt;, the authority part points at whoever’s hosting the data. Whoever created this data is either not present, or is in the path:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;at://&lt;/code&gt; protocol flips that around.&lt;/p&gt;
    &lt;p&gt;In &lt;code&gt;at://&lt;/code&gt; URIs, whoever created the data is the authority, in the most literal sense:&lt;/p&gt;
    &lt;p&gt;The user is the authority for their own data. Whoever’s hosting the data could change over time, and is not directly included in an &lt;code&gt;at://&lt;/code&gt; URI. To find out the actual physical server hosting that JSON, you’re gonna need to take a few steps.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Post in the Atmosphere&lt;/head&gt;
    &lt;p&gt;Let’s try to resolve this &lt;code&gt;at://&lt;/code&gt; URI to the piece of JSON it represents:&lt;/p&gt;
    &lt;p&gt;An easy way to resolve an &lt;code&gt;at://&lt;/code&gt; URI is to use an SDK or a client app. Let’s try an online client, for example, pdsls or Taproot or atproto-browser. They’ll figure out the physical server where its JSON is currently hosted, and show that JSON for you.&lt;/p&gt;
    &lt;p&gt;The above &lt;code&gt;at://&lt;/code&gt; URI points at this JSON, wherever it is currently being hosted:&lt;/p&gt;
    &lt;p&gt;You can guess by the &lt;code&gt;$type&lt;/code&gt; field being &lt;code&gt;"app.bsky.feed.post"&lt;/code&gt; that this is some kind of a post (which might explain why it has fields like &lt;code&gt;text&lt;/code&gt; and &lt;code&gt;langs&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;However, note that this piece of JSON represents a certain social media post itself, not a web page or a piece of some app. It’s pure data as a piece of JSON, not a piece of UI. You may think of the &lt;code&gt;$type&lt;/code&gt; stating the data format; the &lt;code&gt;app.bsky.*&lt;/code&gt; prefix tells us that the &lt;code&gt;bsky.app&lt;/code&gt; application might know something about what to do with it. Other applications may also consume and produce data in this format.&lt;/p&gt;
    &lt;p&gt;A careful reader might notice that the &lt;code&gt;uri&lt;/code&gt; in the JSON block is also an &lt;code&gt;at://&lt;/code&gt; URI but it’s slightly different from the original &lt;code&gt;at://&lt;/code&gt; URI we requested:&lt;/p&gt;
    &lt;p&gt;In particular, the short &lt;code&gt;ruuuuu.de&lt;/code&gt; authority has expanded into a longer &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt; authority. Maybe that’s the physical host?&lt;/p&gt;
    &lt;p&gt;Actually, no, that’s not the physical host either—it’s something called an identity. Turns out, resolving an &lt;code&gt;at://&lt;/code&gt; URI is done in three distinct steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Resolve the handle to an identity (“who are you?”)&lt;/item&gt;
      &lt;item&gt;Resolve that identity to a hosting (“who holds your data?”)&lt;/item&gt;
      &lt;item&gt;Request the JSON from that hosting (“what is the data?”)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s go through each of these steps and see how they work.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Handles to Identities&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;at://&lt;/code&gt; URIs you’ve seen earlier are fragile because they use handles.&lt;/p&gt;
    &lt;p&gt;Here, &lt;code&gt;ruuuuu.de&lt;/code&gt;, &lt;code&gt;danabra.mov&lt;/code&gt;, and &lt;code&gt;tessa.germnetwork.com&lt;/code&gt; are handles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;at://ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://danabra.mov/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://tessa.germnetwork.com/pub.leaflet.publication/3lzz6juivnc2d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(Read more about domains as “internet handles” here.)&lt;/p&gt;
    &lt;p&gt;The user may choose to change their &lt;code&gt;at://&lt;/code&gt; handle later, and it is important for that not to break any links between pieces of JSON already on the network.&lt;/p&gt;
    &lt;p&gt;This is why, before you store an &lt;code&gt;at://&lt;/code&gt; URI, you should turn it into a canonical form by resolving the handle to something that never changes—an identity. An identity is like an account ID, but global and meant for the entire web. There are two mechanisms to resolve a handle to an identity (also known as a “DID”):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query the DNS TXT record at &lt;code&gt;_atproto.&amp;lt;handle&amp;gt;&lt;/code&gt;looking for&lt;code&gt;did=???&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Make an HTTPS GET to &lt;code&gt;https://&amp;lt;handle&amp;gt;/.well-known/atproto-did&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The thing you’re looking for, the DID, is going to have a shape like &lt;code&gt;did:something:whatever&lt;/code&gt;. (We’ll revisit what that means later.)&lt;/p&gt;
    &lt;p&gt;For example, let’s try to resolve &lt;code&gt;ruuuuu.de&lt;/code&gt; via the DNS mechanism:&lt;/p&gt;
    &lt;p&gt;Found it!&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;ruuuuu.de&lt;/code&gt; handle claims to be owned by &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;, whoever that may be. That’s all that we wanted to know at this point:&lt;/p&gt;
    &lt;p&gt;Note this doesn’t prove their association yet. We’ll need to verify that whoever controls the &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt; identity “agrees” with &lt;code&gt;ruuuuu.de&lt;/code&gt; being their handle. The mapping is bidirectional. But we’ll confirm that in a later step.&lt;/p&gt;
    &lt;p&gt;Now let’s try to resolve &lt;code&gt;danabra.mov&lt;/code&gt; using the DNS route:&lt;/p&gt;
    &lt;p&gt;That also worked! The &lt;code&gt;danabra.mov&lt;/code&gt; handle claims to be owned by the &lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt; identity, whoever that may be:&lt;/p&gt;
    &lt;p&gt;This DID looks a bit different than what you saw earlier but it’s also a valid DID. Again, it’s important to emphasize we’ve not confirmed the association yet.&lt;/p&gt;
    &lt;p&gt;Subdomains like &lt;code&gt;barackobama.bsky.social&lt;/code&gt; can also be handles.&lt;/p&gt;
    &lt;p&gt;Let’s try to resolve it:&lt;/p&gt;
    &lt;p&gt;The DNS mechanism didn’t work, so let’s try with HTTPS:&lt;/p&gt;
    &lt;p&gt;That worked! This means that &lt;code&gt;barackobama.bsky.social&lt;/code&gt; handle claims to be owned by the &lt;code&gt;did:plc:5c6cw3veuqruljoy5ahzerfx&lt;/code&gt; identity, whoever that is:&lt;/p&gt;
    &lt;p&gt;So you get the idea. When you see a handle, you can probe it with DNS and HTTPS to see if it claims to be owned by some identity (a DID). If you found a DID, you’ll then be able to (1) verify it actually owns that handle, and (2) locate the server that hosts the data for that DID. And that will be the server you’ll ask for the JSON.&lt;/p&gt;
    &lt;p&gt;In practice, if you’re building with AT, you’ll likely want to either deploy your own handle/did resolution cache or hit an existing one. (Here’s one implementation.)&lt;/p&gt;
    &lt;head rend="h3"&gt;AT Permalinks&lt;/head&gt;
    &lt;p&gt;Now you know how handles resolve to identities, also known as DIDs. Unlike handles, which change over time, DIDs never change—they’re immutable.&lt;/p&gt;
    &lt;p&gt;These &lt;code&gt;at://&lt;/code&gt; links, which use handles, are human-readable but fragile:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;at://ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://danabra.mov/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://tessa.germnetwork.com/pub.leaflet.publication/3lzz6juivnc2d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They will break if one of us changes a handle again.&lt;/p&gt;
    &lt;p&gt;In contrast, the &lt;code&gt;at://&lt;/code&gt; links below, which use DIDs, will not break until we either delete our accounts, delete these records, or permanently take down our hosting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;at://did:web:iam.ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://did:plc:fpruhuo22xkm5o7ttr2ktxdo/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;at://did:plc:ad4m72ykh2evfdqen3qowxmg/pub.leaflet.publication/3lzz6juivnc2d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, really, this is the “true form” of an &lt;code&gt;at://&lt;/code&gt; URI:&lt;/p&gt;
    &lt;p&gt;Think of &lt;code&gt;at://&lt;/code&gt; links with DIDs as “permalinks”. Any application storing &lt;code&gt;at://&lt;/code&gt; URIs should store them in this canonical form so that logical links between our pieces of JSON don’t break when we change our handles or change our hosting.&lt;/p&gt;
    &lt;p&gt;Now that you know how to resolve a handle to a DID, you want to do two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that whoever owns this DID actually goes by that handle.&lt;/item&gt;
      &lt;item&gt;Find the server that hosts all the data for this DID.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can do both of these things by fetching a piece of JSON called the DID Document. You can think of it as sort of a “passport” for a given DID.&lt;/p&gt;
    &lt;p&gt;How you do that depends on what kind of DID it is.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Identities to Hosting&lt;/head&gt;
    &lt;p&gt;Currently, there are two kinds of DIDs, known as DID methods, supported by the AT protocol: &lt;code&gt;did:web&lt;/code&gt; (a W3C draft) and &lt;code&gt;did:plc&lt;/code&gt; (specified by Bluesky).&lt;/p&gt;
    &lt;p&gt;Let’s compare them.&lt;/p&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;did:web&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;ruuuuu.de&lt;/code&gt; handle claims to be owned by &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;To check this claim, let’s find the DID Document for &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;. The &lt;code&gt;did:web&lt;/code&gt; method is a specification that specifies an algorithm for that.&lt;/p&gt;
    &lt;p&gt;In short, you cut off the &lt;code&gt;did:web:&lt;/code&gt; from the DID, append &lt;code&gt;/.well-known/did.json&lt;/code&gt; to the end, and run an HTTPS GET request:&lt;/p&gt;
    &lt;p&gt;This DID Document looks sleep-inducing but it tells us three important things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to refer to them. The &lt;code&gt;alsoKnownAs&lt;/code&gt;field confirms that whoever controls&lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;indeed wants to use&lt;code&gt;@ruuuuu.de&lt;/code&gt;as a handle. ✅&lt;/item&gt;
      &lt;item&gt;How to verify the integrity of their data. The &lt;code&gt;publicKeyMultibase&lt;/code&gt;field tells us the public key with which all changes to their data are signed.&lt;/item&gt;
      &lt;item&gt;Where their data is stored. The &lt;code&gt;serviceEndpoint&lt;/code&gt;field tells us the actual server with their data. Rudy’s data is currently hosted at&lt;code&gt;https://blacksky.app&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A DID Document really is like an internet passport for an identity: here’s their handle, here’s their signature, and here’s their location. It connects a handle to a hosting while letting the identity owner change either the handle or the hosting.&lt;/p&gt;
    &lt;p&gt;Users who interact with &lt;code&gt;@ruuuuu.de&lt;/code&gt; on different apps in the atmosphere don’t need to know or care about his DID or about his current hosting (and whether it moves). From their perspective, his current handle is the only relevant identifier. As for developers, they’ll refer to him by DID, which conveniently never changes.&lt;/p&gt;
    &lt;p&gt;All of this sounds great, but there is one big downside to the &lt;code&gt;did:web&lt;/code&gt; identity. If &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt; ever loses control of the &lt;code&gt;iam.ruuuuu.de&lt;/code&gt; domain, he will lose control over his DID Document, and thus over his entire identity.&lt;/p&gt;
    &lt;p&gt;Let’s have a look at an alternative to &lt;code&gt;did:web&lt;/code&gt; that avoids this problem.&lt;/p&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;did:plc&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;We already know the &lt;code&gt;danabra.mov&lt;/code&gt; handle claims to be owned by the &lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt; identity (actually, that’s me!)&lt;/p&gt;
    &lt;p&gt;To check this claim, let’s find the DID Document for &lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;did:plc&lt;/code&gt; method is a specification that specifies an algorithm for that.&lt;/p&gt;
    &lt;p&gt;Essentially, you need to hit the &lt;code&gt;https://plc.directory&lt;/code&gt; service with a &lt;code&gt;GET&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;The DID Document itself works exactly the same way. It specifies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to refer to me. The &lt;code&gt;alsoKnownAs&lt;/code&gt;field confirms that whoever controls&lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt;uses&lt;code&gt;@danabra.mov&lt;/code&gt;as a handle. ✅&lt;/item&gt;
      &lt;item&gt;How to verify the integrity of my data. The &lt;code&gt;publicKeyMultibase&lt;/code&gt;field tells us the public key with which all changes to my data are signed.&lt;/item&gt;
      &lt;item&gt;Where my data is stored. The &lt;code&gt;serviceEndpoint&lt;/code&gt;field tells us the actual server with my data. It’s currently at&lt;code&gt;https://morel.us-east.host.bsky.network&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s visualize this:&lt;/p&gt;
    &lt;p&gt;Although my handle is &lt;code&gt;@danabra.mov&lt;/code&gt;, the actual server storing my data is currently &lt;code&gt;https://morel.us-east.host.bsky.network&lt;/code&gt;. I’m happy to keep hosting it there but I’m thinking of moving it to a host I control in the future. I can change both my handle and my hosting without disruption to my social apps.&lt;/p&gt;
    &lt;p&gt;Unlike Rudy, who has a &lt;code&gt;did:web&lt;/code&gt; identity, I stuck with &lt;code&gt;did:plc&lt;/code&gt; (which is the default one when you create an account on Bluesky) so that I’m not irrecovably tying myself to any web domain. “PLC” officially stands for a “Public Ledger of Credentials”—essentially, it is like an npm registry but for DID Documents. (Fun fact: originally PLC meant “placeholder” but they’ve decided it’s a good tradeoff.)&lt;/p&gt;
    &lt;p&gt;The upside of a &lt;code&gt;did:plc&lt;/code&gt; identity is that I can’t lose my identity if I forget to renew a domain, or if something bad happens at the top level to my TLD.&lt;/p&gt;
    &lt;p&gt;The downside of a &lt;code&gt;did:plc&lt;/code&gt; identity is that whoever operates the PLC registry has some degree of control over my identity. They can’t outright change it because every version is recursively signed with the hash of the previous version, every past version is queryable, and the hash of the initial version is the DID itself.&lt;/p&gt;
    &lt;p&gt;However, in theory, whoever operates the PLC registry could deny my requests to update the DID Document, or refuse to serve some information about it. Bluesky is currently moving PLC to an independent legal entity in Switzerland to address some of these concerns. The AT community is also thinking and experimenting.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Hosting to JSON&lt;/head&gt;
    &lt;p&gt;So far, you’ve learned how to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Resolve a handle to a DID.&lt;/item&gt;
      &lt;item&gt;Grab the DID Document for that DID.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That actually tells you enough to get the JSON by its &lt;code&gt;at://&lt;/code&gt; URI!&lt;/p&gt;
    &lt;p&gt;Each DID Document includes the &lt;code&gt;serviceEndpoint&lt;/code&gt; which is the actual hosting. That’s the service you can hit by HTTPS to grab any JSON record it stores.&lt;/p&gt;
    &lt;p&gt;For example, the &lt;code&gt;@ruuuuu.de&lt;/code&gt; handle resolves to &lt;code&gt;did:web:iam.ruuuuu.de&lt;/code&gt;, and its DID Document has a &lt;code&gt;serviceEndpoint&lt;/code&gt; pointing at &lt;code&gt;https://blacksky.app&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To get the &lt;code&gt;at://ruuuuu.de/app.bsky.feed.post/3lzy2ji4nms2z&lt;/code&gt; record, hit the &lt;code&gt;https://blacksky.app&lt;/code&gt; server with the &lt;code&gt;com.atproto.repo.getRecord&lt;/code&gt; endpoint, passing different parts of the &lt;code&gt;at://&lt;/code&gt; URI as parameters:&lt;/p&gt;
    &lt;p&gt;And there it is:&lt;/p&gt;
    &lt;p&gt;Now let’s get &lt;code&gt;at://danabra.mov/sh.tangled.feed.star/3m23ddgjpgn22&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;@danabra.mov&lt;/code&gt;handle resolves to&lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The DID Document for &lt;code&gt;did:plc:fpruhuo22xkm5o7ttr2ktxdo&lt;/code&gt;points at&lt;code&gt;https://morel.us-east.host.bsky.network&lt;/code&gt;as the current hosting.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s hit it:&lt;/p&gt;
    &lt;p&gt;And there you have it:&lt;/p&gt;
    &lt;p&gt;And that’s how you resolve an &lt;code&gt;at://&lt;/code&gt; URI.&lt;/p&gt;
    &lt;p&gt;Exercise: In the record above, the &lt;code&gt;subject&lt;/code&gt; is a link to another record. Figure out the handle of its owner and the contents of that record. Use pdsls to check your answer.&lt;/p&gt;
    &lt;head rend="h3"&gt;In Conclusion&lt;/head&gt;
    &lt;p&gt;To resolve an arbitrary &lt;code&gt;at://&lt;/code&gt; URI, you need to follow three steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Resolve the handle to an identity (using DNS and/or HTTPS).&lt;/item&gt;
      &lt;item&gt;Resolve that identity to a hosting (using the DID Document).&lt;/item&gt;
      &lt;item&gt;Request the JSON from that hosting (by hitting it with &lt;code&gt;getRecord&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re building a client app or a small project, an SDK will handle all of this for you. However, for good performance, you’ll want to hit a resolution cache instead of doing DNS/HTTPS lookups on every request. QuickDID is one such cache. You can also check out the pdsls source to see how exactly it handles resolution.&lt;/p&gt;
    &lt;p&gt;In practice, a lot of apps don’t end up needing to resolve &lt;code&gt;at://&lt;/code&gt; URIs or load JSON records because they receive data from the network via a websocket and aggregate it in a local database. If that’s your approach, you’ll still use the &lt;code&gt;at://&lt;/code&gt; URIs as unique identifiers for user-created data, but the data itself will get pushed to you rather than pulled by you. Still, it’s useful to know that you can fetch it on demand.&lt;/p&gt;
    &lt;p&gt;The AT protocol is fundamentally an abstraction over HTTP, DNS, and JSON. But by standardizing how these pieces fit together—putting the user in the authority position, separating identity from hosting, and making data portable—it turns the web into a place where your content belongs to you, not to the apps that display it.&lt;/p&gt;
    &lt;p&gt;There’s more to explore in the atmosphere, but now you know where it’s &lt;code&gt;at://&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://overreacted.io/where-its-at/"/><published>2025-10-02T20:31:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45467500</id><title>Offline card payments should be possible no later than 1 July 2026</title><updated>2025-10-05T00:50:59.784552+00:00</updated><content>&lt;doc fingerprint="cb8cfa25092dfffd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Offline card payments should be possible no later than 1 July 2026&lt;/head&gt;
    &lt;p&gt;Press release The Riksbank and representatives from the payment market have today reached an agreement to increase the possibility to make offline card payments for essential goods. The agreement is an important step in the work to strengthen Sweden's payment preparedness and increase resilience to disruptions in the digital payments system. The goal is for the measures to be in place no later than 1 July 2026.&lt;/p&gt;
    &lt;p&gt;“In Sweden, we pay digitally to a large degree and the use of cash is low. The general public being able to pay by card for example for food and medicines even in the event of a serious breakdown in data communication, that is offline, is a milestone in our intensified efforts to strengthen emergency preparedness”, says Governor Erik Thedéen.&lt;/p&gt;
    &lt;p&gt;The agreement describes the measures that participants in Swedish card payments – card issuers, card networks, card acquirers, the retail sector and the Riksbank – will implement to increase the possibility of offline payments by card. For instance, financial agents will adapt their regulatory frameworks, and the retail trade will introduce technological solutions. The Riksbank is leading this work and is responsible for monitoring its implementation.&lt;/p&gt;
    &lt;p&gt;“We are very pleased that all participants involved are taking responsibility for strengthening Sweden's payment readiness. Some are covered by the Riksbank's regulations, but far from all. We regard the fact that so many are nevertheless choosing to contribute as very positive for Sweden's overall civil preparedness”, concludes Erik Thedéen.&lt;/p&gt;
    &lt;p&gt;The online function shall apply to physical payment cards and accompanying PIN code when purchasing essential goods such as food, medicine and fuel. The Riksbank will continue its work on enabling offline payments for other payment methods after 1 July 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.riksbank.se/en-gb/press-and-published/notices-and-press-releases/press-releases/2025/offline-card-payments-should-be-possible-no-later-than-1-july-2026/"/><published>2025-10-03T20:36:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45468698</id><title>Zig builds are getting faster</title><updated>2025-10-05T00:50:59.615779+00:00</updated><content>&lt;doc fingerprint="adc8b06e29012720"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;Zig Builds Are Getting Faster&lt;/head&gt;
    &lt;p&gt;Andrew Kelley famously (or infamously, depending on your views) said "the compiler is too damn slow, that's why we have bugs."1&lt;/p&gt;
    &lt;p&gt;As a result, one of the primary stated goals of Zig for years has been faster compile times. The Zig team has been working on extremely hard problems to make this a reality (such as yeeting LLVM, writing their own code generation backends, building their own linkers, and marching towards incremental compilation in general).2&lt;/p&gt;
    &lt;p&gt;The fruits of this multi-year labor are finally starting to show with Zig 0.15.1. The Ghostty project just completed upgrading to Zig 0.15.1, and I'd like to share some real-world build times.3&lt;/p&gt;
    &lt;head rend="h2"&gt;Build Script Compilation&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 7sec 167ms&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 1sec 702ms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to build the &lt;code&gt;build.zig&lt;/code&gt; script itself. The
times above were measured by running &lt;code&gt;zig build --help&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A well-written build script should only rebuild itself rarely. However, this is a cost every new uncached source build will pay (e.g. a user downloading the project to build from source one time). As such, it directly impacts the time to build a usable binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Full Uncached Ghostty Binary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 41sec&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 32sec&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This includes the time to build the build script itself. Given the prior results, Zig 0.15 is building everything else ~2 seconds faster. But, you can still see in wall time the change in this initial build time.&lt;/p&gt;
    &lt;p&gt;Important: most of this is still using LLVM. Ghostty still can't fully build and link using the self-hosted x86_64 backend, since the backend still has bugs. So, this just shows the general improvements in the Zig compiler itself, even with LLVM in the picture.&lt;/p&gt;
    &lt;p&gt;Once Ghostty can use the self-hosted x86_64 backend completely, I expect this time to plummet to around 25 seconds or less, fully half the time it would take with Zig 0.14.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incremental Build (Ghostty Executable)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 19sec&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 16sec&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to rebuild Ghostty after a one-line change to the most core terminal emulation code (adding a log function call to the escape sequence parser).&lt;/p&gt;
    &lt;p&gt;This build has a fully cached build script and dependency graph, so it is only rebuilding what it needs to. Incremental compilation in Zig isn't functional yet, so this still recompiles a considerable amount of code. Additionally, as with the prior section, this is still using LLVM. By simply dropping LLVM out of the picture, I expect this time to drop to around 12 seconds or so (less the time LLVM is emitting).&lt;/p&gt;
    &lt;p&gt;Going further, once Zig supports incremental compilation, I expect we'll be able to measure incremental builds like this within milliseconds at worst. But, let's wait and see when that is reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incremental Build (libghostty-vt)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.14: 2sec 884ms&lt;/item&gt;
      &lt;item&gt;Zig 0.15: 975ms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the time it takes to rebuild only libghostty-vt after a one-line change. Unlike the Ghostty executable, &lt;code&gt;libghostty-vt&lt;/code&gt;
is fully functional with the self-hosted x86_64 backend, so this
shows the differences in build times without LLVM in the picture.&lt;/p&gt;
    &lt;p&gt;Similar to the Ghostty executable, this is still rebuilding the full Zig module for &lt;code&gt;libghostty-vt&lt;/code&gt;, since incremental compilation isn't
fully functional yet. I expect this to also drop to single-digit milliseconds
at worst once incremental compilation is a reality.&lt;/p&gt;
    &lt;p&gt;But still, a sub-second build time for a non-trivial library is amazing. This is the library I'm spending most of my time working on right now, and even in a few short days since upgrading to Zig 0.15.1, I've felt a huge difference in my workflow. Previously, I might tab out to read an email between builds or tests, but now its so fast I can stay in flow in my terminal.&lt;/p&gt;
    &lt;p&gt;This improvement is most indicative of what's to come in the short term. The self-hosted x86_64 backend is already stable enough to build all debug builds by default and the aarch64 backend is getting there, too. We aren't able to build the full Ghostty executable yet, but I bet this will get ironed out within months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Faster Builds Are Here&lt;/head&gt;
    &lt;p&gt;As you can see, building Ghostty with Zig 0.15.1 is faster in every single scenario, despite the fact that a lot of Ghostty still can't even take advantage of the self-hosted backend! And despite the fact that incremental compilation isn't functional yet!&lt;/p&gt;
    &lt;p&gt;I've loved betting on Zig for Ghostty, and I love that they're focusing on compile times. These improvements are real, and they're here now. And I suspect in the next couple years, the results posted today will look downright slow. 😜&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Timestamped link: https://youtu.be/5eL_LcxwwHg?t=565 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This ignores an astronomical amount of work that has gone into making every aspect of the Zig compiler faster, more parallelizable, etc. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;All measurements done on the same x86_64 Linux machine. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/zig-builds-getting-faster"/><published>2025-10-03T22:45:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45469579</id><title>New antibiotic targets IBD and AI predicted how it would work</title><updated>2025-10-05T00:50:59.484976+00:00</updated><content/><link href="https://healthsci.mcmaster.ca/new-antibiotic-targets-ibd-and-ai-predicted-how-it-would-work-before-scientists-could-prove-it/"/><published>2025-10-04T01:09:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45471136</id><title>Alibaba cloud FPGA: the $200 Kintex UltraScale+</title><updated>2025-10-05T00:50:59.169412+00:00</updated><content>&lt;doc fingerprint="bd18a05c4875d6f6"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction#&lt;/head&gt;
    &lt;p&gt;I was recently in the market for a new FPGA to start building my upcoming projects on.&lt;/p&gt;
    &lt;p&gt;Due to the scale of my upcoming projects a Xilinx series 7 UltraScale+ FPGA of the Virtex family would be perfect, but a Kintex series FPGA will be sufficient for early prototyping. Due to not wanting to part ways with the eye watering amounts of money that is required for an Vivado enterprise edition license my choice was effectively narrowed to the FPGA chips available under the WebPack version of Vivado.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly Xilinx are well aware of how top of the range the Virtex series are, and doesn’t offer any Virtex UltraScale+ chips with the webpack license. That said, they do offer support for two very respectable Kintex UltraScale+ FPGA models, the &lt;code&gt;XCKU3P&lt;/code&gt; and the &lt;code&gt;XCKU5P&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;These two chips are far from being small hobbyist toys, with the smaller &lt;code&gt;XCUK3P&lt;/code&gt; already boasting +162K LUTs and
16 GTY transceivers, capable, depending on the physical constraints imposed by the chip packaging of
operating at up to 32.75Gb/s.&lt;/p&gt;
    &lt;p&gt;Now that the chip selection has been narrowed down I set out to look for a dev board.&lt;/p&gt;
    &lt;p&gt;My requirements for the board where that it featured :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;at least 2 SFP+ or 1 QSFP connector&lt;/item&gt;
      &lt;item&gt;a JTAG interface&lt;/item&gt;
      &lt;item&gt;a PCIe interface at least x8 wide&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As to where to get the board from, my options where :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Design the board myself&lt;/item&gt;
      &lt;item&gt;Get the AXKU5 or AXKU3 from Alinx&lt;/item&gt;
      &lt;item&gt;See what I could unearth on the second hand market&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Although option &lt;code&gt;1&lt;/code&gt; could have been very interesting, designing a
dev board with both a high speed PCIe and ethernet interface was not the goal of
today’s project.&lt;/p&gt;
    &lt;p&gt;As for option &lt;code&gt;2&lt;/code&gt;,
Alinx is newer vendor that is still building up its credibility in the west,
their technical documentation is a bit sparse, but the feedback seems to be positive with no major issues being reported.
Most importantly, Alinx provided very fairly priced development boards
in the 900 to 1050 dollar range ( +150$ for the HPC FMC SFP+ extension board ).
Although these are not cheap by any metric, compared to the competitions
price point, they are the best value.&lt;/p&gt;
    &lt;p&gt;Option &lt;code&gt;2&lt;/code&gt; was coming up ahead until I stumbled upon this ebay listing :&lt;/p&gt;
    &lt;p&gt;For 200$ this board featured a &lt;code&gt;XCKU3P-FFVB676&lt;/code&gt;, 2 SPF+ connector and a x8 PCIe interface.
On the flip side it came with no documentation whatsoever, no guaranty it worked, and the
faint promise in the listing that there was a JTAG interface.
A sane person would likely have dismissed this as an interesting internet oddity, a remanence
of what happens when a generation of accelerator cards gets phased out in favor of the next,
or maybe just an expensive paperweight.&lt;/p&gt;
    &lt;p&gt;But I like a challenge, and the appeal of unlocking the 200$ Kintex UltraScale+ development board was too great to ignore.&lt;/p&gt;
    &lt;p&gt;As such, I aim for this article to become the documentation paving the way to though this mirage.&lt;/p&gt;
    &lt;head rend="h2"&gt;The debugger challenge#&lt;/head&gt;
    &lt;p&gt;Xilinx’s UG908 Programming and Debugging User Guide (Appendix D) specifies their blessed JTAG probe ecosystem for FPGA configuration and debug. Rather than dropping $100+ on yet another proprietary dongle that’ll collect dust after the project ends, I’m exploring alternatives. The obvious tradeoff: abandoning Xilinx’s toolchain means losing ILA integration. However, the ILA fundamentally just captures samples and streams them via JTAG USER registers, there’s nothing preventing us from building our own logic analyzer with equivalent functionality and a custom host interface.&lt;/p&gt;
    &lt;p&gt;Enter OpenOCD. While primarily targeting ARM/RISC-V SoCs, it maintains an impressive database of supported probe hardware and provides granular control over JTAG operations. More importantly, it natively supports SVF (Serial Vector Format), a vendor-neutral bitstream format that Vivado can export.&lt;/p&gt;
    &lt;p&gt;The documentation landscape is admittedly sparse for anything beyond 7-series FPGAs, and the most recent OpenOCD documentation I could unearth was focused on Zynq ARM core debugging rather than fabric configuration. But the fundamentals remain sound: JTAG is JTAG, SVF is standardized, and the boundary scan architecture hasn’t fundamentally changed.&lt;/p&gt;
    &lt;p&gt;The approach should be straightforward: generate SVF from Vivado, feed it through OpenOCD with a commodity JTAG adapter, and validate the configuration. Worst case, we’ll need to patch some adapter-specific quirks or boundary scan chain register addresses. Time to find out if this theory holds up in practice.&lt;/p&gt;
    &lt;head rend="h2"&gt;The plan#&lt;/head&gt;
    &lt;p&gt;So, to resume, the current plan is to buy a second hand hardware accelerator of eBay at a too good to be true price, and try to configure it with an unofficial probe using open source software without any clear official support.&lt;lb/&gt;The answer to the obvious question you are thinking if you, like me, have been around the block a few times is: many things.&lt;/p&gt;
    &lt;p&gt;As such, we need a plan for approaching this. The goal of this plan is to outline incremental steps that will build upon themselves with the end goal of being able to use this as a dev board.&lt;/p&gt;
    &lt;head rend="h3"&gt;1 - Confirming the board works#&lt;/head&gt;
    &lt;p&gt;First order of business will be to confirm the board is showing signs of working as intended.&lt;/p&gt;
    &lt;p&gt;There is a high probability that the flash wasn’t wiped before this board was sold off, as such the previous bitstream should still be in the flash. Given this board was used as an accelerator, we should be able to use that to confirm the board is working by either checking if the board is presenting itself as a PCIe endpoint or if the SFP’s are sending the ethernet PHY idle sequence.&lt;/p&gt;
    &lt;head rend="h3"&gt;2 - Connecting a debugger to it#&lt;/head&gt;
    &lt;p&gt;The next step is going to be to try and connect the debugger. The eBay listing advertised there is a JTAG interface, but the picture is grainy enough that where that JTAG is and what pins are available is unclear.&lt;/p&gt;
    &lt;p&gt;Additionally, we have no indication of what devices are daisy chained together onto the JTAG scan chain. This is an essential question for flashing over JTAG, so it will need to be figured out.&lt;/p&gt;
    &lt;p&gt;At this point, it would also be strategic to try and do some more probing into the FPGA via JTAG. Xilinx FPGAs exposes a handful of useful system registers accessible over JTAG. The most well known of these interfaces is the SYSMON, which allows us, among other things, to get real time temperature and voltage reading from inside the chip. Although openOCD doesn’t have SYSMON support out of the box it would be worth while to build it, to :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Familiarise myself with openOCD scripting, this might come in handy when building my ILA replacement down the line&lt;/item&gt;
      &lt;item&gt;Having an easy side channel to monitor FPGA operating parameters&lt;/item&gt;
      &lt;item&gt;Make a contribution to openOCD as it have support for the interfacing with XADC but not SYSMON&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;3 - Figuring out the Pinout#&lt;/head&gt;
    &lt;p&gt;The hardest part will be figuring out the FPGA’s pinout and my clock sources. The questions that need answering are :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;what external clocks sources do I have, what are there frequencies and which pins are they connected to&lt;/item&gt;
      &lt;item&gt;which transceivers are the SFPs connected to&lt;/item&gt;
      &lt;item&gt;which transceivers is the PCIe connected to&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;4 - Writing a bitstream#&lt;/head&gt;
    &lt;p&gt;For now I will be focusing on writing a temporary configurations over JTAG to the CCLs and not re-writing the flash.&lt;/p&gt;
    &lt;p&gt;That plan is to trying writing either the bitstream directly though openOCD’s &lt;code&gt;virtex2&lt;/code&gt; + &lt;code&gt;pld&lt;/code&gt; drivers, or by replaying the
SVF generated by Vivado.&lt;/p&gt;
    &lt;p&gt;Since I believe a low iteration time is paramount to project velocity and getting big things done, I also want automatize all of the Vivado flow from taking the rtl to the SVF generation.&lt;/p&gt;
    &lt;p&gt;Simple enough ?&lt;/p&gt;
    &lt;head rend="h2"&gt;Liveness test#&lt;/head&gt;
    &lt;p&gt;A few days later my prize arrived via express mail.&lt;/p&gt;
    &lt;p&gt;Unexpectedly it even came with a free 25G SFP28 Huawei transceiver rated for a 300m distance and a single 1m long OS2 fiber patch cable. This was likely not intentional as the transceiver was jammed in the SFP cage, but it was still very generous of them to include the fiber patch cable.&lt;/p&gt;
    &lt;p&gt;The board also came with a travel case and half of a PCIe to USB adapter and a 12V power supply that one could use to power the board as a standalone device. Although this standalone configuration will not be of any use to me, for those looking to develop just networking interfaces without any PCIe interface, this could come in handy.&lt;/p&gt;
    &lt;p&gt;Overall the board looked a little worn, but both the transceiver cages and PCIe connectors didn’t look to be damaged.&lt;/p&gt;
    &lt;head rend="h3"&gt;Standalone configuration#&lt;/head&gt;
    &lt;p&gt;Before real testing could start I first did a small power-up test using the PCIe to USB adapter that the seller provided. I was able to do a quick check using the LEDs and the FPGAs dissipated heat that the board seemed to be powering up at a surface level (pun intended).&lt;/p&gt;
    &lt;head rend="h3"&gt;PCIe interface#&lt;/head&gt;
    &lt;p&gt;Since I didn’t want to directly plug mystery hardware into my prized build server, I decided to use a Raspberry Pi 5 as my sacrificial test device and got myself an external PCIe adapter.&lt;/p&gt;
    &lt;p&gt;It just so happened that the latest Raspberry Pi version, the Pi 5, now features an external PCIe Gen 2.0 x1 interface. Though our FPGA can handle up to a PCIe Gen 3.0 and the board had a x8 wide interface, since PCIe standard is backwards compatible and the number of lanes on the interface can be downgraded, plugging our FPGA with this Raspberry Pi will work.&lt;/p&gt;
    &lt;p&gt;After both the Raspberry and the FPGA were booted, I SSHed into my rpi and started looking for the PCIe enumeration sequence logged from the Linux PCIe core subsystem.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;dmesg&lt;/code&gt; log :&lt;/p&gt;
    &lt;code&gt;[    0.388790] pci 0000:00:00.0: [14e4:2712] type 01 class 0x060400
[    0.388817] pci 0000:00:00.0: PME# supported from D0 D3hot
[    0.389752] pci 0000:00:00.0: bridge configuration invalid ([bus 00-00]), reconfiguring
[    0.495733] brcm-pcie 1000110000.pcie: link up, 5.0 GT/s PCIe x1 (!SSC)
[    0.495759] pci 0000:01:00.0: [dabc:1017] type 00 class 0x020000
&lt;/code&gt;
    &lt;head rend="h4"&gt;Background information#&lt;/head&gt;
    &lt;p&gt;Since most people might not be intimately as familiar with PCIe terminology, allow me to quickly document what is going on here.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;0000:00:00.0&lt;/code&gt;: is the identifier of a specific PCIe device connected through the PCIe network
to the kernel, it read as &lt;code&gt;domain&lt;/code&gt;:&lt;code&gt;bus&lt;/code&gt;:&lt;code&gt;device&lt;/code&gt;.&lt;code&gt;function&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;[14e4:2712]&lt;/code&gt;: is the device’s &lt;code&gt;[vendor id:device id]&lt;/code&gt;, these vendor id identifiers are
assigned by the PCI standard body to hardware vendors. Vendors are then free to define there
own vendor id’s.&lt;/p&gt;
    &lt;p&gt;The full list of official vendor id’s and released device id can be found : https://admin.pci-ids.ucw.cz/read/PC/14e4 or in the linux kernel code : https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;type 01&lt;/code&gt;: PCIe has two types of devices, bridges allowing the connection of multiple downstream devices to an
upstream device, and endpoints are the leafs.
Bridges are of type &lt;code&gt;01&lt;/code&gt; and endpoints of type &lt;code&gt;00&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;class 0x60400&lt;/code&gt;: is the PCIe device class, it categorizes the kind of function the device performs. It
uses the following format &lt;code&gt;0x[Base Class (8 bits)][Sub Class (8 bits)][Programming Interface (8 bits)]&lt;/code&gt;,
( note : the sub class field might be unused ).&lt;/p&gt;
    &lt;p&gt;A list of class and sub class identifiers can be found: https://admin.pci-ids.ucw.cz/read/PD or again in the linux codebase : https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158&lt;/p&gt;
    &lt;head rend="h4"&gt;Dmesg log#&lt;/head&gt;
    &lt;p&gt;The two most interesting lines of the &lt;code&gt;dmesg&lt;/code&gt; log are :&lt;/p&gt;
    &lt;code&gt;[    0.388790] pci 0000:00:00.0: [14e4:2712] type 01 class 0x060400
[    0.495759] pci 0000:01:00.0: [dabc:1017] type 00 class 0x020000
&lt;/code&gt;
    &lt;p&gt;Firstly the PCIe subsystem logs that at &lt;code&gt;0000:00:00.0&lt;/code&gt; it has discovered a Broadcom BCM2712 PCIe Bridge ( vendor id &lt;code&gt;14e4&lt;/code&gt;, device id &lt;code&gt;0x2712&lt;/code&gt; ).This bridge (type &lt;code&gt;01&lt;/code&gt;) class &lt;code&gt;0x0604xx&lt;/code&gt; tells us it is a PCI-to-PCI bridge, meaning it is essentially creating additional PCIe lanes downstream for endpoint devices or additional bridges.&lt;/p&gt;
    &lt;p&gt;The subsystem then discovers a second device at &lt;code&gt;0000:01:00.0&lt;/code&gt;, this is an endpoint (type &lt;code&gt;00&lt;/code&gt;), and class &lt;code&gt;0x02000&lt;/code&gt; tells us it is an ethernet networking equipment.&lt;lb/&gt;Of note &lt;code&gt;dabc&lt;/code&gt; doesn’t correspond to a known vendor id.
When designing a PCIe interface in hardware these
are parameters we can configured. Additionally, among the different ways Linux uses to identify which driver to load for a PCIe device
the vendor id and device id can be used for matching. Supposing we are implementing custom logic, in order to prevent any bug where the wrong driver
might be loaded, it is best to use a separate vendor id.
This also helps identify your custom accelerator at a glance and use it to load your custom driver.&lt;/p&gt;
    &lt;p&gt;As such, it is not surprising to see an unknown vendor id appear for an FPGA, this with the class as an ethernet networking device is a strong hint this is our board.&lt;/p&gt;
    &lt;head rend="h4"&gt;Full PCIe device status#&lt;/head&gt;
    &lt;p&gt;Dmesg logs have already given us a good indication that our FPGA board and its PCIe interface was working but to confirm with certainty that the device with vendor id &lt;code&gt;dabc&lt;/code&gt; is our FPGA we now turn to &lt;code&gt;lspci&lt;/code&gt;.
&lt;code&gt;lspci -vvv&lt;/code&gt; is the most verbose output and gives us a full overview of the detected PCIe devices capabilities and current configurations.&lt;/p&gt;
    &lt;p&gt;Broadcom bridge:&lt;/p&gt;
    &lt;code&gt;0000:00:00.0 PCI bridge: Broadcom Inc. and subsidiaries BCM2712 PCIe Bridge (rev 21) (prog-if 00 [Normal decode])
        Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;gt;SERR- &amp;lt;PERR- INTx-
        Latency: 0
        Interrupt: pin A routed to IRQ 38
        Bus: primary=00, secondary=01, subordinate=01, sec-latency=0
        Memory behind bridge: [disabled] [32-bit]
        Prefetchable memory behind bridge: 1800000000-182fffffff [size=768M] [32-bit]
        Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;lt;SERR- &amp;lt;PERR-
        BridgeCtl: Parity- SERR- NoISA- VGA- VGA16- MAbort- &amp;gt;Reset- FastB2B-
                PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
        Capabilities: [48] Power Management version 3
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold-)
                Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=1 PME-
        Capabilities: [ac] Express (v2) Root Port (Slot-), MSI 00
                DevCap: MaxPayload 512 bytes, PhantFunc 0
                        ExtTag- RBE+
                DevCtl: CorrErr- NonFatalErr- FatalErr- UnsupReq-
                        RlxdOrd+ ExtTag- PhantFunc- AuxPwr+ NoSnoop+
                        MaxPayload 512 bytes, MaxReadReq 512 bytes
                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 5GT/s, Width x1, ASPM L0s L1, Exit Latency L0s &amp;lt;2us, L1 &amp;lt;4us
                        ClockPM+ Surprise- LLActRep- BwNot+ ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s, Width x1
                        TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt+
                RootCap: CRSVisible+
                RootCtl: ErrCorrectable- ErrNon-Fatal- ErrFatal- PMEIntEna+ CRSVisible+
                RootSta: PME ReqID 0000, PMEStatus- PMEPending-
                DevCap2: Completion Timeout: Range ABCD, TimeoutDis+ NROPrPrP- LTR+
                         10BitTagComp- 10BitTagReq- OBFF Via WAKE#, ExtFmt- EETLPPrefix-
                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
                         FRS- LN System CLS Not Supported, TPHComp- ExtTPHComp- ARIFwd+
                         AtomicOpsCap: Routing- 32bit- 64bit- 128bitCAS-
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled, ARIFwd-
                         AtomicOpsCtl: ReqEn- EgressBlck-
                LnkCap2: Supported Link Speeds: 2.5-5GT/s, Crosslink- Retimer- 2Retimers- DRS+
                LnkCtl2: Target Link Speed: 5GT/s, EnterCompliance- SpeedDis-
                         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
                         Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete- EqualizationPhase1-
                         EqualizationPhase2- EqualizationPhase3- LinkEqualizationRequest-
                         Retimer- 2Retimers- CrosslinkRes: unsupported, DRS-
                         DownstreamComp: Link Up - Present
        Capabilities: [100 v1] Advanced Error Reporting
                UESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
                CESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
                CEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
                AERCap: First Error Pointer: 00, ECRCGenCap+ ECRCGenEn- ECRCChkCap+ ECRCChkEn-
                        MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
                HeaderLog: 00000000 00000000 00000000 00000000
                RootCmd: CERptEn+ NFERptEn+ FERptEn+
                RootSta: CERcvd- MultCERcvd- UERcvd- MultUERcvd-
                         FirstFatal- NonFatalMsg- FatalMsg- IntMsg 0
                ErrorSrc: ERR_COR: 0000 ERR_FATAL/NONFATAL: 0000
        Capabilities: [160 v1] Virtual Channel
                Caps:   LPEVC=0 RefClk=100ns PATEntryBits=1
                Arb:    Fixed- WRR32- WRR64- WRR128-
                Ctrl:   ArbSelect=Fixed
                Status: InProgress-
                VC0:    Caps:   PATOffset=00 MaxTimeSlots=1 RejSnoopTrans-
                        Arb:    Fixed- WRR32- WRR64- WRR128- TWRR128- WRR256-
                        Ctrl:   Enable+ ID=0 ArbSelect=Fixed TC/VC=ff
                        Status: NegoPending- InProgress-
        Capabilities: [180 v1] Vendor Specific Information: ID=0000 Rev=0 Len=028 &amp;lt;?&amp;gt;
        Capabilities: [240 v1] L1 PM Substates
                L1SubCap: PCI-PM_L1.2+ PCI-PM_L1.1+ ASPM_L1.2+ ASPM_L1.1+ L1_PM_Substates+
                          PortCommonModeRestoreTime=8us PortTPowerOnTime=10us
                L1SubCtl1: PCI-PM_L1.2- PCI-PM_L1.1- ASPM_L1.2- ASPM_L1.1-
                           T_CommonMode=1us LTR1.2_Threshold=0ns
                L1SubCtl2: T_PwrOn=10us
        Capabilities: [300 v1] Secondary PCI Express
                LnkCtl3: LnkEquIntrruptEn- PerformEqu-
                LaneErrStat: 0
        Kernel driver in use: pcieport
&lt;/code&gt;
    &lt;p&gt;FPGA board:&lt;/p&gt;
    &lt;code&gt;0000:01:00.0 Ethernet controller: Device dabc:1017
        Subsystem: Red Hat, Inc. Device a001
        Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &amp;gt;TAbort- &amp;lt;TAbort- &amp;lt;MAbort- &amp;gt;SERR- &amp;lt;PERR- INTx-
        Region 0: Memory at 1820000000 (64-bit, prefetchable) [disabled] [size=2K]
        Region 2: Memory at 1800000000 (64-bit, prefetchable) [disabled] [size=512M]
        Capabilities: [40] Power Management version 3
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)
                Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=0 PME-
        Capabilities: [70] Express (v2) Endpoint, MSI 00
                DevCap: MaxPayload 1024 bytes, PhantFunc 0, Latency L0s &amp;lt;64ns, L1 &amp;lt;1us
                        ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset- SlotPowerLimit 0W
                DevCtl: CorrErr+ NonFatalErr+ FatalErr+ UnsupReq+
                        RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+
                        MaxPayload 512 bytes, MaxReadReq 512 bytes
                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s (downgraded), Width x1 (downgraded)
                        TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-
                DevCap2: Completion Timeout: Range BC, TimeoutDis+ NROPrPrP- LTR-
                         10BitTagComp- 10BitTagReq- OBFF Not Supported, ExtFmt- EETLPPrefix-
                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
                         FRS- TPHComp- ExtTPHComp-
                         AtomicOpsCap: 32bit- 64bit- 128bitCAS-
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled,
                         AtomicOpsCtl: ReqEn-
                LnkCap2: Supported Link Speeds: 2.5-8GT/s, Crosslink- Retimer- 2Retimers- DRS-
                LnkCtl2: Target Link Speed: 8GT/s, EnterCompliance- SpeedDis-
                         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
                         Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete- EqualizationPhase1-
                         EqualizationPhase2- EqualizationPhase3- LinkEqualizationRequest-
                         Retimer- 2Retimers- CrosslinkRes: unsupported
        Capabilities: [100 v1] Advanced Error Reporting
                UESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
                CESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
                CEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
                AERCap: First Error Pointer: 00, ECRCGenCap- ECRCGenEn- ECRCChkCap- ECRCChkEn-
                        MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
                HeaderLog: 00000000 00000000 00000000 00000000
        Capabilities: [1c0 v1] Secondary PCI Express
                LnkCtl3: LnkEquIntrruptEn- PerformEqu-
                LaneErrStat: 0
&lt;/code&gt;
    &lt;p&gt;For our board, the following lines are particularly interesting:&lt;/p&gt;
    &lt;code&gt;                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s (downgraded), Width x1 (downgraded)0x060400
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;LnkCap&lt;/code&gt; tells us about the full capabilities of this PCIe device, here we can see that
the current design supports PCIe Gen 3.0 x8.
The &lt;code&gt;LnkSta&lt;/code&gt; tells us the current configuration, here we have been downgraded to PCIe Gen 2.0 at 5GT/s with a width of only x1.&lt;/p&gt;
    &lt;p&gt;During startup of when a new PCIe device is plugged, PCIe performs a link speed and width negotiation where it tries to reach the highest supported stable configuration for the current system. In our current system, though our FPGA is capable of 8GT/s, as it is located downstream of the Broadcom bridge with a maximum link capacity of Gen 2.0 ( 5GT/s ), the FPGA has been downgraded to 5GT/s.&lt;/p&gt;
    &lt;p&gt;As for the width of x1, that is expected since the Broadcom bridge is also only x1 wide, and our board’s other 7 PCIe lanes are literally hanging over the side.&lt;/p&gt;
    &lt;p&gt;Thus, we can finally confirm that this is our board and that the PCIe interface is working. We can now proceed to establishing the JTAG connection.&lt;/p&gt;
    &lt;head rend="h2"&gt;JTAG interface#&lt;/head&gt;
    &lt;p&gt;Xilinx FPGAs can be configured by writing a bitstream to their internal CMOS Configuration Latches (CCL). CCL is SRAM memory and volatile, thus the configuration is re-done on every power cycle. For devices in the field this bitstream would be read from an external SPI memory during initialization, or written from an external device, such as an embedded controller. But for development purposes overwriting the contents of the CCLs over JTAG is acceptable.&lt;/p&gt;
    &lt;p&gt;This configuration is done by shifting in the entire FPGA bitstream into the device’s configuration logic over the JTAG bus.&lt;/p&gt;
    &lt;head rend="h3"&gt;FPGA board JTAG interface#&lt;/head&gt;
    &lt;p&gt;As promised by the original eBay listing the board did come with an accessible JTAG interface, and gloriously enough, this time there wasn’t even the need for any additional soldering.&lt;/p&gt;
    &lt;p&gt;In addition to a power reference, and ground, conformely to the Xilinx JTAG interface it featured the four mandatory signals comprising the JTAG TAP :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TCK Test Clock&lt;/item&gt;
      &lt;item&gt;TMS Test Mode Select&lt;/item&gt;
      &lt;item&gt;TDI Test Data Input&lt;/item&gt;
      &lt;item&gt;TDO Test Data Output&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of note, the JTAG interface can also come with an independent reset signal. But since Xilinx JTAG interfaces do not have this independent reset signal, we be using the JTAG FSM reset state for our reset signal.&lt;/p&gt;
    &lt;p&gt;This interface layout doesn’t follow a standard layout so I cannot just plug in one of my debug probes, it requires some re-wiring.&lt;/p&gt;
    &lt;head rend="h3"&gt;Segger JLINK :heart:#&lt;/head&gt;
    &lt;p&gt;I do not own an AMD approved JTAG programmer.&lt;/p&gt;
    &lt;p&gt;Traditionally speaking, the Segger JLink is used for debugging embedded CPUs let them be standalone or in a Zynq, and not for configuring FPGAs.&lt;/p&gt;
    &lt;p&gt;That said, all we need to do is use JTAG to shift in a bitstream to the CCLs, so technically speaking any programmable device with 4 sufficiently fast GPIOs can be used as a JTAG programmer. Additionally, the JLink is well supported by OpenOCD, the JLink’s libraries are open source, and I happened to own one.&lt;/p&gt;
    &lt;head rend="h4"&gt;Wiring#&lt;/head&gt;
    &lt;p&gt;Rewiring :&lt;/p&gt;
    &lt;p&gt;JTAG is a parallel protocol where &lt;code&gt;TDI&lt;/code&gt; and &lt;code&gt;TMS&lt;/code&gt; will be captured according to &lt;code&gt;TCK&lt;/code&gt;.
Because of this, good JTAG PCB trace length matching is advised in order to minimize skew.&lt;/p&gt;
    &lt;p&gt;Ideally a custom connector with length matched traces to work as an interface between the JLink’s probe and a board specific connector would be used.&lt;/p&gt;
    &lt;p&gt;Yet, here we are shoving breadboard wires between our debugger and the board. Since OpenOCD allows us to easily control the debugger clock speed, we can increase the skew tolerance by slowing down the TCK clock signal. As such there is no immediate need for a custom connector but we will not be able to reach the maximum JTAG speeds.&lt;/p&gt;
    &lt;p&gt;No issues were encountered at these speeds.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenOCD#&lt;/head&gt;
    &lt;p&gt;OpenOCD is a free and open source on-chip debugger software that aims to be compatible with as many probes, boards and chips as possible.&lt;/p&gt;
    &lt;p&gt;Since OpenOCD has support for the standard SVF file format, my plan for my flashing flow will be to use Vivado to generate the SVF and have OpenOCD flash it. Now, some of you might be starting to notice that I am diverging quite far from the well lit path of officially supported tools. Not only am I using a not officially supported debug probe, but I am also using some obscure open source software with questionable support for interfacing with Xilinx UltraScale+ FPGAs. You might be wondering, given that the officially supported tools can already prove themselves to be a headache to get working properly, why am I seemingly making my life even harder?&lt;/p&gt;
    &lt;p&gt;The reason is quite simple: when things inevitably start going wrong, as they will, having an entirely open toolchain, allows me to have more visibility as to what is going on and the ability to fix it. I cannot delve into a black box.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building OpenOCD#&lt;/head&gt;
    &lt;p&gt;By default the version of OpenOCD that I got on my server via the official packet manager was outdated and missing features I will need.&lt;/p&gt;
    &lt;p&gt;Also, since saving the ability to modify OpenOCD’s source code could come in handy, I decided to re-build it from source.&lt;/p&gt;
    &lt;p&gt;Thus, in the following logs, I will be running OpenOCD version &lt;code&gt;0.12.0+dev-02170-gfcff4b712&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note : I have also re-build the JLink libs from source.&lt;/p&gt;
    &lt;head rend="h3"&gt;Determining the scan chain#&lt;/head&gt;
    &lt;p&gt;Since I do not have the schematics for the board I do not know how many devices are daisy-chainned on the board JTAG bus. Also, I want to confirm if the FPGA on the ebay listing is actually the one on the board. In JTAG, each chained device exposes an accessible &lt;code&gt;IDCODE&lt;/code&gt; register used to identify the manufacturer, device type, and revision number.&lt;/p&gt;
    &lt;p&gt;When setting up the JTAG server, we typically define the scan chain by specifying the expected &lt;code&gt;IDCODE&lt;/code&gt; for each TAP and the corresponding instruction register length, so that instructions can be correctly aligned and routed to the intended device.
Given this is an undocumented board off Ebay, I do not know what the chain looks like.
Fortunately, OpenOCD has an autoprobing functionality, to do a blind interrogation in an attempt to discover
the available devices.&lt;/p&gt;
    &lt;p&gt;Thus, my first order of business was doing this autoprobing.&lt;/p&gt;
    &lt;p&gt;In OpenOCD the autoprobing is done when the configuration does not specify any taps.&lt;/p&gt;
    &lt;code&gt;source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED

reset_config none
&lt;/code&gt;
    &lt;p&gt;The blind interrogation successfully discovered a single device on the chain with an &lt;code&gt;IDCODE&lt;/code&gt; of &lt;code&gt;0x04a63093&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test/autoprob$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
none separate
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Warn : There are no enabled taps.  AUTO PROBING MIGHT NOT WORK!!
Info : JTAG tap: auto0.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : AUTO auto0.tap - use "jtag newtap auto0 tap -irlen 2 -expected-id 0x04a63093"
Error: IR capture error at bit 2, saw 0x3ffffffffffffff5 not 0x...3
Warn : Bypassing JTAG setup events due to errors
Warn : gdb services need one or more targets defined
&lt;/code&gt;
    &lt;p&gt;Comparing against the &lt;code&gt;UltraScale Architecture Configuration User Guide (UG570)&lt;/code&gt; we see that this &lt;code&gt;IDCODE&lt;/code&gt; matches up
precisely with the expected value for the &lt;code&gt;KU3P&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;By default OpenOCD assumes a JTAG IR length of 2 bits, while our FPGA has an IR length of 6 bits. This is the cause behind the IR capture error encountered during autoprobing. By updating the script with an IR length of 6 bits we can re-detect the FPGA with no errors.&lt;/p&gt;
    &lt;code&gt;source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED

reset_config none

jtag newtap auto_detect tap -irlen 6
&lt;/code&gt;
    &lt;p&gt;Output :&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test/autoprob$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Info : JTAG tap: auto_detect.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
&lt;/code&gt;
    &lt;p&gt;Based on the probing, this is the JTAG scan chain for our board :&lt;/p&gt;
    &lt;head rend="h3"&gt;System Monitor Registers#&lt;/head&gt;
    &lt;p&gt;Previous generations of Xilinx FPGA had a system called the XADC that, among other features, allowed you to acquire chip temperature and voltage readings. The newer UltraScale and UltraScale+ family have deprecated this XADC module in favor of the SYSMON (and SYSMON4) which allows you to also get these temperature readings, just better.&lt;/p&gt;
    &lt;p&gt;Unfortunately, openOCD didn’t have support for reading the SYSMON over JTAG out of the box, so I will be adding it.&lt;/p&gt;
    &lt;p&gt;To be more precise, the Kintex UltraScale+ has a SYSMON4 and not a SYSMON. For full context, there are 3 flavors of SYSMON:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SYSMON1&lt;/code&gt;used in the Kintex and Virtex UltraScale series&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SYSMON4&lt;/code&gt;used in the Kintex, Virtex and in the Zynq programmable logic for the UltraScale+ series&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SYSMON&lt;/code&gt;used in the Zynq in the processing system of the UltraScale+ series.&lt;lb/&gt;Yes, you read that correctly the Zynq of the UltraScale+ series features not one, but at least two unique SYSMON instances.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the purpose of this article, all these instances are similar enough that I will be using the terms SYSMON4 and SYSMON interchangeably.&lt;/p&gt;
    &lt;p&gt;In order for the JTAG to interact with the SYSMON, we first need to write the &lt;code&gt;SYSMON_DRP&lt;/code&gt; command to the
JTAG Instruction Register (IR).
Based on the documentation, we see that this command has a value of &lt;code&gt;0x37&lt;/code&gt;, which funnily enough,
is the same command code as the XADC, solidifying the SYSMON as the XADC’s descendant.&lt;/p&gt;
    &lt;p&gt;The SYSMON offers a lot more additional functionalities than just being used to read voltage and temperature, but for today’s use case we will not be using any of that. Rather, we will focus only on reading a subset of the SYSMON status registers.&lt;/p&gt;
    &lt;p&gt;These status registers are located at addresses &lt;code&gt;(00h-3Fh, 80h-BFh)&lt;/code&gt;,
and contain the measurement results of the analog-to-digital conversions, the flag registers,
and the calibration coefficients. We can select which address we wish to read by writing the
address to the Data Register (DR) over JTAG and the data will be read out of &lt;code&gt;TDO&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# SPDX-License-Identifier: GPL-2.0-or-later

# Xilinx SYSMON4 support
#
# Based on UG580, used for UltraScale+ Xilinx FPGA
# This code implements access through the JTAG TAP.
#
# build a 32 bit DRP command for the SYSMON DRP
proc sysmon_cmd {cmd addr data} {
	array set cmds {
		NOP 0x00
		READ 0x01
		WRITE 0x02
	}
	return [expr {($cmds($cmd) &amp;lt;&amp;lt; 26) | ($addr &amp;lt;&amp;lt; 16) | ($data &amp;lt;&amp;lt; 0)}]
}

# Status register addresses
# Some addresses (status registers 0-3) have special function when written to.
proc SYSMON {key} {
	array set addrs {
		TEMP 0x00
		VCCINT 0x01
		VCCAUX 0x02
		VPVN 0x03
		VREFP 0x04
		VREFN 0x05
		VCCBRAM 0x06
		SUPAOFFS 0x08
		ADCAOFFS 0x09
		ADCAGAIN 0x0a
		VCCPINTLP 0x0d
		VCCPINTFP 0x0e
		VCCPAUX 0x0f
		VAUX0 0x10
		VAUX1 0x11
		VAUX2 0x12
		VAUX3 0x13
		VAUX4 0x14
		VAUX5 0x15
		VAUX6 0x16
		VAUX7 0x17
		VAUX8 0x18
		VAUX9 0x19
		VAUX10 0x1a
		VAUX11 0x1b
		VAUX12 0x1c
		VAUX13 0x1d
		VAUX14 0x1e
		VAUX15 0x1f
		MAXTEMP 0x20
		MAXVCC 0x21
		MAXVCCAUX 0x22
	}
	return $addrs($key)
}

# transfer
proc sysmon_xfer {tap cmd addr data} {
	set ret [drscan $tap 32 [sysmon_cmd $cmd $addr $data]]
	runtest 10
	return [expr "0x$ret"]
}

# sysmon register write
proc sysmon_write {tap addr data} {
	sysmon_xfer $tap WRITE $addr $data
}

# sysmon register read, non-pipelined
proc sysmon_read {tap addr} {
	sysmon_xfer $tap READ $addr 0
	return [sysmon_xfer $tap NOP 0 0]
}


# Select the sysmon DR, SYSMON_DRP has the same binary code value as the XADC
proc sysmon_select {tap} {
	set SYSMON_IR 0x37
	irscan $tap $SYSMON_IR
	runtest 10
}

# convert 16 bit temperature measurement to Celsius
proc sysmon_temp_internal {code} {
	return [expr {$code * 509.314/(1 &amp;lt;&amp;lt; 16) - 280.23}]
}

# convert 16 bit supply voltage measurments to Volt
proc sysmon_sup {code} {
	return [expr {$code * 3./(1 &amp;lt;&amp;lt; 16)}]
}

# measure all internal voltages
proc sysmon_report {tap} {
	puts "Sysmon status report :"
	sysmon_select $tap
	foreach ch [list TEMP MAXTEMP] {
		echo "$ch [format %.2f [sysmon_temp_internal [sysmon_read $tap [SYSMON $ch]]]] C"
	}
	foreach ch [list VCCINT MAXVCC VCCAUX MAXVCCAUX] {
		echo "$ch [format %.3f [sysmon_sup [sysmon_read $tap [SYSMON $ch]]]] V"	
	}
}
&lt;/code&gt;
    &lt;p&gt;I added a report that reads the current chip temperature, internal and external voltages as well as the maximum values for these recorded since FPGA power cycle, to my flashing script output:&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-20:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
set chipname XCKU3P
Read temperature sysmon 4
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.819 V
Info : clock speed 1 kHz
Info : JTAG tap: XCKU3P.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
--------------------
Sysmon status report :
TEMP 31.12 C
MAXTEMP 34.62 C
VCCINT 0.852 V
MAXVCC 0.855 V
VCCAUX 1.805 V
MAXVCCAUX 1.807 V
&lt;/code&gt;
    &lt;head rend="h2"&gt;Pinout#&lt;/head&gt;
    &lt;p&gt;To my indescribable joy I happened to stumble onto this gold mine, in which we get the board pinout. This most likely fell off a truck: https://blog.csdn.net/qq_37650251/article/details/145716953&lt;/p&gt;
    &lt;p&gt;So far this pinout looks correct.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Pin Index&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;IO Standard&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
        &lt;cell role="head"&gt;Bank&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;diff_100mhz_clk_p&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;E18&lt;/cell&gt;
        &lt;cell&gt;BANK67&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;diff_100mhz_clk_n&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;D18&lt;/cell&gt;
        &lt;cell&gt;BANK67&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;sfp_mgt_clk_p&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;K7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;sfp_mgt_clk_n&lt;/cell&gt;
        &lt;cell&gt;LVDS&lt;/cell&gt;
        &lt;cell&gt;K6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;sfp_1_txn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;sfp_1_txp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;sfp_1_rxn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;A3&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;sfp_1_rxp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;A4&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;sfp_2_txn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;D6&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;sfp_2_txp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;D7&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;sfp_2_rxn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B1&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;sfp_2_rxp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;B2&lt;/cell&gt;
        &lt;cell&gt;BANK227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;SFP_1_MOD_DEF_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;SFP_1_TX_FAULT&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;SFP_1_LOS&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;SFP_1_LED&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;SFP_2_MOD_DEF_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;E11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;SFP_2_TX_FAULT&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;F9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;SFP_2_LOS&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;E10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;SFP_2_LED&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_SFP_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_SFP_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_SFP_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_SFP_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;D10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_EEPROM_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;G10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_EEPROM_0&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;G9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;26&lt;/cell&gt;
        &lt;cell&gt;IIC_SDA_EEPROM_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;J15&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;IIC_SCL_EEPROM_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;J14&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_R&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A13&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;29&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_G&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A12&lt;/cell&gt;
        &lt;cell&gt;BANK87&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_H&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;31&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_1&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_2&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;C11&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_3&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;34&lt;/cell&gt;
        &lt;cell&gt;GPIO_LED_4&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;B10&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;pcie_mgt_clkn&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T6&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;36&lt;/cell&gt;
        &lt;cell&gt;pcie_mgt_clkp&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T7&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;pcie_tx0_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;R4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;pcie_tx1_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;U4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;pcie_tx2_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;W4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;pcie_tx3_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AA4&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;41&lt;/cell&gt;
        &lt;cell&gt;pcie_tx4_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AC4&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;pcie_tx5_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD6&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;pcie_tx6_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE8&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;44&lt;/cell&gt;
        &lt;cell&gt;pcie_tx7_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF6&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;pcie_rx0_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;P1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;46&lt;/cell&gt;
        &lt;cell&gt;pcie_rx1_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell&gt;pcie_rx2_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;V1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;48&lt;/cell&gt;
        &lt;cell&gt;pcie_rx3_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;Y1&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;49&lt;/cell&gt;
        &lt;cell&gt;pcie_rx4_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AB1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;pcie_rx5_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;51&lt;/cell&gt;
        &lt;cell&gt;pcie_rx6_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE3&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;52&lt;/cell&gt;
        &lt;cell&gt;pcie_rx7_n&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF1&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;53&lt;/cell&gt;
        &lt;cell&gt;pcie_tx0_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;R5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;54&lt;/cell&gt;
        &lt;cell&gt;pcie_tx1_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;U5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;55&lt;/cell&gt;
        &lt;cell&gt;pcie_tx2_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;W5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;56&lt;/cell&gt;
        &lt;cell&gt;pcie_tx3_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AA5&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;57&lt;/cell&gt;
        &lt;cell&gt;pcie_tx4_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AC5&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;58&lt;/cell&gt;
        &lt;cell&gt;pcie_tx5_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD7&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;59&lt;/cell&gt;
        &lt;cell&gt;pcie_tx6_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE9&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;60&lt;/cell&gt;
        &lt;cell&gt;pcie_tx7_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF7&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;61&lt;/cell&gt;
        &lt;cell&gt;pcie_rx0_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;P2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;62&lt;/cell&gt;
        &lt;cell&gt;pcie_rx1_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;T2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;63&lt;/cell&gt;
        &lt;cell&gt;pcie_rx2_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;V2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;64&lt;/cell&gt;
        &lt;cell&gt;pcie_rx3_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;Y2&lt;/cell&gt;
        &lt;cell&gt;BANK225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;65&lt;/cell&gt;
        &lt;cell&gt;pcie_rx4_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AB2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;66&lt;/cell&gt;
        &lt;cell&gt;pcie_rx5_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AD2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;67&lt;/cell&gt;
        &lt;cell&gt;pcie_rx6_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AE4&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;68&lt;/cell&gt;
        &lt;cell&gt;pcie_rx7_p&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;AF2&lt;/cell&gt;
        &lt;cell&gt;BANK224&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;69&lt;/cell&gt;
        &lt;cell&gt;pcie_perstn_rst&lt;/cell&gt;
        &lt;cell&gt;LVCMOS18&lt;/cell&gt;
        &lt;cell&gt;A9&lt;/cell&gt;
        &lt;cell&gt;BANK86&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Global clock#&lt;/head&gt;
    &lt;p&gt;On high end FPGAs like the UltraScale+ family, high-speed global clocks are typically driven from external sources using differential pairs for better signal integrity.&lt;/p&gt;
    &lt;p&gt;According to the pinout we have two such differential pairs.&lt;/p&gt;
    &lt;p&gt;First I must determine the nature of these external reference clocks to see how I can use them to drive my clocks.&lt;/p&gt;
    &lt;p&gt;These differential pairs are provided over the following pins:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;100MHz : {E18, D18}&lt;/item&gt;
      &lt;item&gt;156.25MHz : {K7, K6}&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Judging by the naming and the frequencies, the 156.25MHz clock is likely my SFP reference clock, and the 100MHz can be used as my global clock.&lt;/p&gt;
    &lt;p&gt;We can confirm by querying the pin properties.&lt;/p&gt;
    &lt;p&gt;K6 properties :&lt;/p&gt;
    &lt;code&gt;Vivado% report_property [get_package_pins K6]
Property                Type    Read-only  Value
BANK                    string  true       227
BUFIO_2_REGION          string  true       TR
CLASS                   string  true       package_pin
DIFF_PAIR_PIN           string  true       K7
IS_BONDED               bool    true       1
IS_DIFFERENTIAL         bool    true       1
IS_GENERAL_PURPOSE      bool    true       0
IS_GLOBAL_CLK           bool    true       0
IS_LOW_CAP              bool    true       0
IS_MASTER               bool    true       0
IS_VREF                 bool    true       0
IS_VRN                  bool    true       0
IS_VRP                  bool    true       0
MAX_DELAY               int     true       38764
MIN_DELAY               int     true       38378
NAME                    string  true       K6
PIN_FUNC                enum    true       MGTREFCLK0N_227
PIN_FUNC_COUNT          int     true       1
PKGPIN_BYTEGROUP_INDEX  int     true       0
PKGPIN_NIBBLE_INDEX     int     true       0
&lt;/code&gt;
    &lt;p&gt;E18 properties :&lt;/p&gt;
    &lt;code&gt;Vivado% report_property [get_package_pins E18]
Property                Type    Read-only  Value
BANK                    string  true       67
BUFIO_2_REGION          string  true       TL
CLASS                   string  true       package_pin
DIFF_PAIR_PIN           string  true       D18
IS_BONDED               bool    true       1
IS_DIFFERENTIAL         bool    true       1
IS_GENERAL_PURPOSE      bool    true       1
IS_GLOBAL_CLK           bool    true       1
IS_LOW_CAP              bool    true       0
IS_MASTER               bool    true       1
IS_VREF                 bool    true       0
IS_VRN                  bool    true       0
IS_VRP                  bool    true       0
MAX_DELAY               int     true       87126
MIN_DELAY               int     true       86259
NAME                    string  true       E18
PIN_FUNC                enum    true       IO_L11P_T1U_N8_GC_67
PIN_FUNC_COUNT          int     true       2
PKGPIN_BYTEGROUP_INDEX  int     true       8
PKGPIN_NIBBLE_INDEX     int     true       2
&lt;/code&gt;
    &lt;p&gt;This tells us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The differential pairings are correct: {K6, K7}, {E18, D18}&lt;/item&gt;
      &lt;item&gt;We can easily use the 100MHz as a source to drive our global clocking network&lt;/item&gt;
      &lt;item&gt;The 156.25MHz clock is to be used as the reference clock for our GTY transceivers and lands on bank 227 as indicated by the &lt;code&gt;PIN_FUNC&lt;/code&gt;property&lt;code&gt;MGTREFCLK0N_227&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;We cannot directly use the 156.25MHz clock to drive our global clock network&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all this we have sufficient information to write a constraint file (&lt;code&gt;xdc&lt;/code&gt;) for this board.&lt;/p&gt;
    &lt;head rend="h2"&gt;Test design#&lt;/head&gt;
    &lt;p&gt;Further sections will be using the following design files.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;top.v&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;module top (
    input wire Clk_100mhz_p_i, 
    input wire Clk_100mhz_n_i,

    output wire [3:0] Led_o 
);
    wire        clk_ibuf;
    reg  [28:0] ctr_q; 
    reg         unused_ctr_q;


    IBUFDS #(
        .DIFF_TERM("TRUE"),
        .IOSTANDARD("LVDS")
    ) m_ibufds (
        .I(Clk_100mhz_p_i),
        .IB(Clk_100mhz_n_i),
        .O(clk_ibuf)
    );

    BUFG m_bufg (
        .I(clk_ibuf),
        .O(clk)
    );

    always @(posedge clk)
        { unused_ctr_q, ctr_q } &amp;lt;= ctr_q + 29'b1;    
    
    assign Led_o = ctr_q[28:25];
endmodule
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;alibaba_cloud.xdc&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;# Global clock signal 
set_property -dict {LOC E18 IOSTANDARD LVDS} [get_ports Clk_100mhz_p_i]
set_property -dict {LOC D18 IOSTANDARD LVDS} [get_ports Clk_100mhz_n_i]
create_clock -period 10 -name clk_100mhz [get_ports Clk_100mhz_p_i]

# LEDS
set_property -dict {LOC B11 IOSTANDARD LVCMOS18} [get_ports { Led_o[0]}]
set_property -dict {LOC C11 IOSTANDARD LVCMOS18} [get_ports { Led_o[1]}]
set_property -dict {LOC A10 IOSTANDARD LVCMOS18} [get_ports { Led_o[2]}]
set_property -dict {LOC B10 IOSTANDARD LVCMOS18} [get_ports { Led_o[3]}]
&lt;/code&gt;
    &lt;head rend="h2"&gt;Writing the bitstream#&lt;/head&gt;
    &lt;p&gt;My personal belief is that one of the most important contributors to design quality is iteration cost. The lower your iteration cost, the higher your design quality is going to be.&lt;/p&gt;
    &lt;p&gt;As such I will invest the small upfront cost to have the workflow be as streamlined as efficiently feasible.&lt;/p&gt;
    &lt;p&gt;Thus, my workflow evolved into doing practically everything over the command line interfaces and only interacting with the tools, Vivado in this case, through tcl scripts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vivado flow#&lt;/head&gt;
    &lt;p&gt;The goal of this flow is to, given a few verilog design and constraint files produce a SVF file. Our steps are :&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;creat the Vivado project &lt;code&gt;setup.tcl&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;run the implementation &lt;code&gt;build.tcl&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;generate the bitstream and the SVF &lt;code&gt;gen.tcl&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I will be using &lt;code&gt;make&lt;/code&gt; to kick off and manage the dependencies between the different steps, though I recognise this isn’t a widespread practice for hardware projects. &lt;code&gt;make&lt;/code&gt; is a highly flexible, reliable and powerful tool and I believe its ability to tie together any type of workflow makes it a prime tool for this use case.&lt;/p&gt;
    &lt;p&gt;We will be invoking Vivado in batch mode, this allows us to provide a tcl script alongside script arguments, the format is as following :&lt;/p&gt;
    &lt;code&gt;vivado -mode batch &amp;lt;path to tcl script&amp;gt; -tclargs &amp;lt;script args&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Though this allows us to easily break down our flow into incremental stages, invoking a single script in batch mode has the drawback of restarting Vivado and needing to re-load the project or the project checkpoint on each invocation.&lt;/p&gt;
    &lt;p&gt;As the project size grows so will the project load time, so segmenting the flow into a large number of independent scripts comes at an increasing cost.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Makefile&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;SHELL := /bin/bash

VIVADO_PRJ_DIR=prj
VIVADO_PRJ_NAME=$(VIVADO_PRJ_DIR)
VIVADO_PRJ_PATH=$(VIVADO_PRJ_DIR)/$(VIVADO_PRJ_NAME).xpr
VIVADO_CHECKPOINT_PATH=$(VIVADO_PRJ_DIR)/$(VIVADO_PRJ_NAME)_checkpoint.dcp

VIVADO_CMD=vivado -mode batch -source

SRC_PATH=src
OUT_DIR=out


all: setup build gen

$(VIVADO_PRJ_PATH):  
    mkdir -p $(VIVADO_PRJ_DIR)
    $(VIVADO_CMD) setup.tcl -tclargs $(VIVADO_PRJ_DIR) $(VIVADO_PRJ_NAME)

setup: $(VIVADO_PRJ_PATH) 

$(VIVADO_CHECKPOINT_PATH): $(VIVADO_PRJ_PATH) $(wildcard $(SRC_PATH)/*.xdc) $(wildcard $(SRC_PATH)/*.v)
    $(VIVADO_CMD) build.tcl -tclargs $(VIVADO_PRJ_PATH) $(SRC_PATH) $(VIVADO_CHECKPOINT_PATH)

build: $(VIVADO_CHECKPOINT_PATH)

$(OUT_DIR)/$(VIVADO_PRJ_NAME).svf: $(VIVADO_CHECKPOINT_PATH) 
    mkdir -p $(OUT_DIR)
    $(VIVADO_CMD) gen.tcl -tclargs $(VIVADO_CHECKPOINT_PATH) $(OUT_DIR)

gen: $(OUT_DIR)/$(VIVADO_PRJ_NAME).svf

flash: $(OUT_DIR)/$(VIVADO_PRJ_NAME).svf
    openocd	

clean: 
    rm -rf $(VIVADO_PRJ_DIR)
    rm -rf $(OUT_DIR)
    rm -f vivado*{log,jou}
    rm -f webtalk*{log,jou}
    rm -f usage_statistics_webtalk*{html,xml}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;setup.tcl&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set project_dir [lindex $argv 0]
set project_name [lindex $argv 1]

puts "Creating project $project_name at path [pwd]/$project_dir"
create_project -part xcku3p-ffvb676-2-e -force $project_name $project_dir

close_project
exit 0
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;build.tcl&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set project_path [lindex $argv 0]
set src_path [lindex $argv 1]
set checkpoint_path [lindex $argv 2]
puts "Implementation script called with project path $project_path and src path $src_path, generating checkpoint at $checkpoint_path"

open_project $project_path 

# load src
read_verilog [glob -directory $src_path *.v]
read_xdc [glob -directory $src_path *.xdc]


# synth
synth_design -top top

# implement
opt_design
place_design
route_design
phys_opt_design

write_checkpoint $checkpoint_path -force 
close_project
exit 0
&lt;/code&gt;
    &lt;head rend="h4"&gt;Generating the SVF file#&lt;/head&gt;
    &lt;p&gt;The SVF for Serial Vector Format is a human readable, vendor agnostic specification used to specify JTAG bus operations.&lt;/p&gt;
    &lt;p&gt;Example SVF file, test program:&lt;/p&gt;
    &lt;code&gt;! Initialize UUT
STATE RESET;
! End IR scans in DRPAUSE
ENDIR DRPAUSE;
! End DR scans in DRPAUSE
ENDDR DRPAUSE;
! 24 bit IR header
HIR 24 TDI (FFFFFF);
! 3 bit DR header
HDR 3 TDI (7);
! 16 bit IR trailer
TIR 16 TDI (FFFF);
! 2 bit DR trailer
TDR 2 TDI (3);
! 8 bit IR scan, load BIST opcode
SIR 8 TDI (41) TDO (81) MASK (FF);
! 16 bit DR scan, load BIST seed
SDR 16 TDI (ABCD);
! RUNBIST for 95 TCK Clocks
RUNTEST 95 TCK ENDSTATE IRPAUSE;
! 16 bit DR scan, check BIST status
SDR 16 TDI (0000) TDO(1234) MASK(FFFF);
! Enter Test-Logic-Reset
STATE RESET;
! End Test Program
&lt;/code&gt;
    &lt;p&gt;Vivado can generate a hardware aware SVF file containing the configuration sequence for an FPGA board, allowing us to write a bitstream.&lt;/p&gt;
    &lt;p&gt;Given the SVF file literally contains the bitstream written in clear hexademical, in the file, our first step is to generate our design’s bitstream.&lt;/p&gt;
    &lt;p&gt;Vivado proper isn’t the software that generates the SVF file, this task is done by the hardware manager which handles all of the configuration.&lt;/p&gt;
    &lt;p&gt;We can launch a new instance &lt;code&gt;open_hw_manager&lt;/code&gt; and connect to it &lt;code&gt;connect_hw_server&lt;/code&gt;.
Since JTAG is a daisy chained bus, and given the SVF file is just a standardised way of specifying
JTAG bus operations, in order to generate a correct JTAG configuration sequence, we must inform the hardware manger
of our scan chain.&lt;/p&gt;
    &lt;p&gt;During our earlier probing of the scan chain, we have established that our FPGA is the only device on the chain. We inform the hardware manager of this by creating a new device configuration ( the term “device” refers to the “board” here ) and add our fpga to the chain using the &lt;code&gt;create_hw_device -part &amp;lt;device name&amp;gt;&lt;/code&gt;.When we have multiple
devices we should register them following the order in which they appear on the chain.&lt;/p&gt;
    &lt;p&gt;Finally to generate the SVF file, we must select the device we wish to program with &lt;code&gt;program_hw_device &amp;lt;hw_device&amp;gt;&lt;/code&gt;,
then write out the SVF to the file using &lt;code&gt;write_hw_svf &amp;lt;path to svf file&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;gen.tcl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;set checkpoint_path [lindex $argv 0]
set out_dir [lindex $argv 1]
puts "SVF generation script called with checkpoint path $checkpoint_path, generating to $out_dir"

open_checkpoint $checkpoint_path

# defines
set hw_target "alibaba_board_svf_target"
set fpga_device "xcku3p"
set bin_path "$out_dir/[current_project]"

write_bitstream "$bin_path.bit" -force

open_hw_manager

# connect to hw server with default config
connect_hw_server
puts "connected to hw server at [current_hw_server]"

create_hw_target $hw_target
puts "current hw target [current_hw_target]"

open_hw_target

# single device on scan chain
create_hw_device -part $fpga_device
puts "scan chain : [get_hw_devices]"

set_property PROGRAM.FILE "$bin_path.bit" [get_hw_device]

#select device to program
program_hw_device [get_hw_device]

# generate svf file
write_hw_svf -force "$bin_path.svf"

close_hw_manager
exit 0
&lt;/code&gt;
    &lt;head rend="h3"&gt;Configuring the FPGA using OpenOCD#&lt;/head&gt;
    &lt;p&gt;Although not widespread openOCD has a very nice &lt;code&gt;svf&lt;/code&gt; execution command :&lt;/p&gt;
    &lt;quote&gt;&lt;head&gt;18.1 SVF: Serial Vector Format#&lt;/head&gt;&lt;p&gt;The Serial Vector Format, better known as SVF, is a way to represent JTAG test patterns in text files. In a debug session using JTAG for its transport protocol, OpenOCD supports running such test files.&lt;/p&gt;&lt;code&gt;[Command]svf filename [-tap tapname] [[-]quiet] [[-]nil] [[-]progress] [[-]ignore_error]&lt;/code&gt;&lt;p&gt;This issues a JTAG reset (Test-Logic-Reset) and then runs the SVF script from filename. Arguments can be specified in any order; the optional dash doesn’t affect their se- mantics.&lt;/p&gt;&lt;p&gt;Command options:&lt;/p&gt;&lt;code&gt;-tap&lt;/code&gt;tapname ignore IR and DR headers and footers specified by the SVF file with HIR, TIR, HDR and TDR commands; instead, calculate them automatically according to the current JTAG chain configuration, targeting tapname;&lt;code&gt;[-]quiet&lt;/code&gt;do not log every command before execution;&lt;code&gt;[-]nil&lt;/code&gt;“dry run”, i.e., do not perform any operations on the real interface;&lt;code&gt;[-]progress&lt;/code&gt;enable progress indication;&lt;code&gt;[-]ignore&lt;/code&gt;_error continue execution despite TDO check errors.&lt;/quote&gt;
    &lt;p&gt;We invoke it in our openOCD script using the &lt;code&gt;-progress&lt;/code&gt; option for additional logging.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;openocd&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;set svf_path "out/project_prj_checkpoint.svf"

source [find interface/jlink.cfg]
transport select jtag

set SPEED 1
jtag_rclk $SPEED
adapter speed $SPEED 
reset_config none

# jlink config

set CHIPNAME XCKU3P
set CHIP $CHIPNAME
puts "set chipname "$CHIP

source [find ../openocd/tcl/cpld/xilinx-xcu.cfg]

source [find ../openocd/tcl/fpga/xilinx-sysmon.cfg]

init 

puts "--------------------"

sysmon_report $CHIP.tap

puts "--------------------"

# program
if {![file exists $svf_path]} {
    puts "Svf path not found : $svf_path"
    exit
}

svf $svf_path -progress 
 
exit 
&lt;/code&gt;
    &lt;p&gt;Flashing sequence log :&lt;/p&gt;
    &lt;code&gt;gp@workhorse:~/tools/openocd_jlink_test$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
set chipname XCKU3P
Read temperature sysmon 4
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Info : JTAG tap: XCKU3P.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
--------------------
Sysmon status report :
TEMP 50.46 C
MAXTEMP 52.79 C
VCCINT 0.846 V
MAXVCC 0.860 V
VCCAUX 1.799 V
MAXVCCAUX 1.809 V
--------------------
svf processing file: "out/project_prj_checkpoint.svf"
  0%  TRST OFF;
  0%  ENDIR IDLE;
  0%  ENDDR IDLE;
  0%  STATE RESET;
  0%  STATE IDLE;
  0%  FREQUENCY 1.00E+07 HZ;
adapter speed: 10000 kHz
  0%  HIR 0 ;
  0%  TIR 0 ;
  0%  HDR 0 ;
  0%  TDR 0 ;
  0%  SIR 6 TDI (09) ;
  0%  SDR 32 TDI (00000000) TDO (04a63093) MASK (0fffffff) ;
  0%  STATE RESET;
  0%  STATE IDLE;
  0%  SIR 6 TDI (0b) ;
  0%  SIR 6 TDI (14) ;
  0%  RUNTEST 0.100000 SEC;
  0%  RUNTEST 10000 TCK;
  0%  SIR 6 TDI (14) TDO (11) MASK (31) ;
  0%  SIR 6 TDI (05) ;
 95%  ffffffffffff) ;
 95%  SIR 6 TDI (09) TDO (31) MASK (11) ;
 95%  STATE RESET;
 95%  RUNTEST 5 TCK;
 95%  SIR 6 TDI (05) ;
 95%  SDR 160 TDI (0000000400000004800700140000000466aa9955) ;
 95%  SIR 6 TDI (04) ;
 95%  SDR 32 TDI (00000000) TDO (3f5e0d40) MASK (08000000) ;
 95%  STATE RESET;
 95%  RUNTEST 5 TCK;
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
&lt;/code&gt;
    &lt;p&gt;Resulting in a successfully configured our FPGA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;For $200 we got a fully working decommissioned Alibaba Cloud accelerator featuring a Kintex UltraScale+ FPGA with an easily accessible debugging/programming interface and enough pinout information to define our own constraint files.&lt;/p&gt;
    &lt;p&gt;We also have a fully automated Vivado workflow to implement our designs and the ability to write the bitstream, and interface with the FPGA’s internal JTAG accessible registers using an open source programming tool without the need for an official Xilinx programmer.&lt;/p&gt;
    &lt;p&gt;In the end, this project delivered an at least 5x cost savings over commercial boards (compared to the lowest cost $900-1050 Alinx alternatives), making this perhaps the most cost effective entry point for a Kintex UltraScale+ board.&lt;/p&gt;
    &lt;head rend="h2"&gt;External ressources#&lt;/head&gt;
    &lt;p&gt;Xilinx Vivado Supported Devices : https://docs.amd.com/r/en-US/ug973-vivado-release-notes-install-license/Supported-Devices&lt;/p&gt;
    &lt;p&gt;Official Xilinx dev board : https://www.amd.com/en/products/adaptive-socs-and-fpgas/evaluation-boards/ek-u1-kcu116-g.html&lt;/p&gt;
    &lt;p&gt;Alinx Kintex UltraScale+ dev boards : https://www.en.alinx.com/Product/FPGA-Development-Boards/Kintex-UltraScale-plus.html&lt;/p&gt;
    &lt;p&gt;UltraScale Architecture Configuration User Guide (UG570) : https://docs.amd.com/r/en-US/ug570-ultrascale-configuration/Device-Resources-and-Configuration-Bitstream-Lengths?section=gyn1703168518425__table_vyh_4hs_szb&lt;/p&gt;
    &lt;p&gt;UltraScale Architecture System Monitor User Guide (UG580): https://docs.amd.com/v/u/en-US/ug580-ultrascale-sysmon&lt;/p&gt;
    &lt;p&gt;Vivado Design Suite Tcl Command Reference Guide (UG835): https://docs.amd.com/r/en-US/ug835-vivado-tcl-commands/Tcl-Initialization-Scripts&lt;/p&gt;
    &lt;p&gt;PCI vendor/device ID database: https://admin.pci-ids.ucw.cz/read/PC/14e4&lt;/p&gt;
    &lt;p&gt;PCI device classes: https://admin.pci-ids.ucw.cz/read/PD&lt;/p&gt;
    &lt;p&gt;Linux kernel PCI IDs: https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256&lt;/p&gt;
    &lt;p&gt;Linux kernel PCI classes: https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158&lt;/p&gt;
    &lt;p&gt;Truck-kun pinout: https://blog.csdn.net/qq_37650251/article/details/145716953&lt;/p&gt;
    &lt;p&gt;Ebay listing: https://www.ebay.com/itm/167626831054?_trksid=p4375194.c101800.m5481&lt;/p&gt;
    &lt;p&gt;OpenOCD documentation: https://openocd.org/doc-release/pdf/openocd.pdf&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://essenceia.github.io/projects/alibaba_cloud_fpga/"/><published>2025-10-04T06:49:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45472319</id><title>Paged Out Issue #7 [pdf]</title><updated>2025-10-05T00:50:56.957781+00:00</updated><content/><link href="https://pagedout.institute/download/PagedOut_007.pdf"/><published>2025-10-04T10:38:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45472678</id><title>The Buchstabenmuseum Berlin is closing</title><updated>2025-10-05T00:50:55.043992+00:00</updated><content>&lt;doc fingerprint="7e81930193d6e0f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;After 20 Years:&lt;/head&gt;
    &lt;head rend="h3"&gt;The Buchstabenmuseum Berlin is closing!&lt;/head&gt;
    &lt;p&gt;Until 5 October 2025, you can visit our museum every Thursday to Sunday from 1 to 5 pm.&lt;lb/&gt; A visit outside opening hours is possible with a guided tour.&lt;lb/&gt; visit@buchstabenmuseum.de&lt;/p&gt;
    &lt;p&gt;We are still looking for long-term storage for our collection.&lt;lb/&gt; For this we need support:&lt;lb/&gt; &amp;gt;&amp;gt; betterplace&lt;/p&gt;
    &lt;head rend="h4"&gt;We look forward to your visit!&lt;/head&gt;
    &lt;p&gt;______________________________&lt;/p&gt;
    &lt;head rend="h1"&gt;20 Years of the Buchstabenmuseum!&lt;/head&gt;
    &lt;head rend="h4"&gt;On Sat 14 June 2025 from 3 to 9 pm we want to celebrate our birthday and toast with you!&lt;lb/&gt; You and all your friends and family are cordially invited.&lt;/head&gt;
    &lt;p&gt;20 years is a very long time to look back on.&lt;lb/&gt; That’s why we’ve put together a colourful brochure that we’ll be presenting on Saturday.&lt;/p&gt;
    &lt;p&gt;In addition, Sabrina Hauck (student at TU Berlin / architect at gkks) will give a talk on the use,&lt;lb/&gt; vacancy and potential of Berlin’s S-Bahn arches at 18:00.&lt;/p&gt;
    &lt;head rend="h1"&gt;FINAL SALE – FROM DEPARTMENT STORES’ TO MUSEUM &lt;/head&gt;
    &lt;p&gt;Extended Term until Autumn 2025!&lt;/p&gt;
    &lt;p&gt;AN EXHIBITION OF FORMER DEPARTMENT STORES FROM 1980 TO TODAY&lt;/p&gt;
    &lt;p&gt;Horten, Quelle, Hertie, Kaufhof and Karstadt – corporate names that are disappearing from German city centres. Galeria Karstadt Kaufhof is currently struggling with closure in instalments. With the creeping loss of the corporations, the distinctive lettering of the department stores’ chains is also being lost.&lt;lb/&gt; “FINAL SALE” tells the typographic and urban-historical stories of the letters and shows the former significance of the department stores and department stores with their architecture.&lt;lb/&gt; We invite you to discover the typographic department stores’ icons and to buy selected items in the exhibition: “Final Sale – from department stores’ to museum” in the Buchstabenmuseum in the Stadtbahnbögen in the Hansaviertel.&lt;/p&gt;
    &lt;head rend="h4"&gt;COOPERATION WITH THE STAATSBIBLIOTHEK BERLIN&lt;/head&gt;
    &lt;p&gt;In the »Staatsbibliothek Berlin«, Unter den Linden, selected Ks from the collection of the Buchstabenmuseum point the way to the in-house museum "Kulturwerk".&lt;/p&gt;
    &lt;head rend="h4"&gt;INDIVIDUAL GUIDED TOURS&lt;/head&gt;
    &lt;p&gt;Discover our unique collection and learn the exciting background stories to our letters.&lt;/p&gt;
    &lt;head rend="h4"&gt;NEON CLASSES: BENDING BASICS&lt;/head&gt;
    &lt;p&gt;The art of neon and the bending of neon tubes is a fascinating craft! Learn the basics of neon and glass bending.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE BUCHSTABENMUSEUM&lt;/head&gt;
    &lt;p&gt;Preservation and documentation of letters&lt;lb/&gt; The Buchstabenmuseum is the first museum in the world to collect letterforms from public spaces and display them as part of urban history. We preserve and document three-dimensional letters and signage, and their history, as well as providing information about their origins and construction. Our collection has captured the imagination of visitors from all around the world for over 10 years. Hundreds of letters have been saved from being battered by the elements or ending up on the scrap heap. A selection of what we offer can be found under » COLLECTION&lt;/p&gt;
    &lt;head rend="h2"&gt;COLLECTION&lt;/head&gt;
    &lt;head rend="h2"&gt;SPECIALS&lt;/head&gt;
    &lt;p&gt;— BECOME A MEMBER —&lt;/p&gt;
    &lt;p&gt;Become a member of the club and actively support our museum.&lt;lb/&gt; We are always in need of help with the rescue of historic lettering, looking after our international guests or even with the classic work of the association.&lt;lb/&gt; Just write to us!&lt;lb/&gt; bindabei@buchstabenmuseum.de&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.buchstabenmuseum.de/en/"/><published>2025-10-04T11:58:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45472765</id><title>Thunderscan: A clever device transforms a printer into a scanner (2004)</title><updated>2025-10-05T00:50:54.449267+00:00</updated><content>&lt;doc fingerprint="8549597119742b9e"&gt;
  &lt;main&gt;
    &lt;p&gt;The first project that I worked on for Apple after starting in August 1979 was writing low level software for the Silentype printer (see What Hath Woz Wrought), a cute, inexpensive thermal printer for the Apple II, that was based on technology licensed from a local company named Trendcom. In typical Apple fashion, we improved on Trendcom's design by replacing their relatively expensive controller board with a much simpler one that relied on the microprocessor in the Apple II to do most of the dirty work.&lt;/p&gt;
    &lt;p&gt;The only other engineer working on the project was Victor Bull, who was the hardware designer and also the project leader. Vic was smart, taciturn and easy to work with, and I learned a lot from him about how thermal printers worked, as well as how things worked at Apple. We finished the project quickly, and the Silentype shipped in November 1979, less than four months after I began working on it.&lt;/p&gt;
    &lt;p&gt;In May 1984, during my leave of absence from Apple (see Leave Of Absence), I received a phone call from Victor Bull, who I hadn't heard from in a couple of years. He had left Apple more than a year ago to work with his friend Tom Petrie at a tiny company based in Orinda named Thunderware, that sold a single product called Thunderclock, an inexpensive calendar/clock card for the Apple II. Victor said that he thought that I might be interested in writing software for an exciting, clever new product that Thunderware was developing for the Macintosh, which he refused to describe over the phone. He invited me to come visit them to check it out.&lt;/p&gt;
    &lt;p&gt;In early June, I drove up to Thunderware's office in Orinda, which was about an hour's drive from my house in Palo Alto. After I arrived at their modest headquarters, Vic introduced me to his partner, Tom Petrie, and I signed a non-disclosure agreement before they ushered me into a back room to see their demo.&lt;/p&gt;
    &lt;p&gt;The most popular printer for both the Apple II and the Macintosh was the ImageWriter, a $500 dot-matrix printer capable of rendering bitmapped graphics, that was designed and manufactured by Japanese company named C.Itoh Electronics and marketed by Apple. Virtually every Macintosh owner purchased an ImageWriter, since it was the only printer that was supported by Apple. Tom's demo consisted of an ImageWriter printer hooked up to an Apple II, that at first glance appeared to be busily printing away. But when I looked closer, I noticed that instead of blank paper, there was a glossy photograph of a cat threaded through the printer's platen, and the printer's black plastic ribbon cartridge was missing, replaced by a makeshift contraption containing an optical sensing device that trailed an umbilical cord back to the Apple II.&lt;/p&gt;
    &lt;p&gt;Their potential new product, Thunderscan, was a low cost way to temporarily turn an ImageWriter printer into a high resolution scanner, by replacing the ribbon cartridge with an optical sensor and writing some clever software. Since the resolution was determined by the precision of the printer's stepper motors, which had to be very accurate in order to print detailed graphics, Thunderscan, priced at under $200, had better resolution than flat bed scanners costing more than ten times as much. I loved the cleverness of the ingenious concept, and the Woz-like elegance of saving money and adding flexibility by doing everything in software, but there were also a few problems.&lt;/p&gt;
    &lt;p&gt;The biggest problem was that Thunderscan could only capture one scan line's worth of data on each pass of the print head, which made it nine times slower than regular printing, since the print head could deposit nine dots at a time. This made for frustratingly slow scanning, often taking over an hour to scan a full page at the highest resolution. Thunderscan was never going to win any races.&lt;/p&gt;
    &lt;p&gt;Another apparent problem was the disappointingly low quality of the image being captured and displayed by Tom Petrie's Apple II application. Tom and Vic said their scanner was capable of capturing up to 32 different levels of light intensity, but both the Apple II (in hi-res mode) and the Macintosh only had one bit per pixel to display, so the software had to simulate gray scales using patterns of black and white dots. It looked like Tom was using a simple threshold algorithm to do the rendering, which threw away most of the gray scale information and made the resulting image look unacceptably blotchy. It was hard to tell if the quality promised by Tom and Vic was there or not.&lt;/p&gt;
    &lt;p&gt;Tom and Vic proposed to hire me to write Macintosh software for Thunderscan. I knew that a low cost scanner would be a great product for the image hungry Macintosh, but only if it had sufficient quality, and I wasn't sure about that. I told them that I'd think it over during the next few days, and, as I did, I became more excited about the potential of Thunderscan for the Macintosh, realizing that the slow speed wouldn't be that much of an impediment if the quality and resolution was good enough. The low image quality in Tom's prototype was probably caused more by the Apple II software than by anything inherent in the scanner. The Macintosh was almost ten times faster than the Apple II, so it should be able to sample the incoming data better to obtain more horizontal resolution. Plus, I knew a much better algorithm for gray scale rendering that would be fun to try out in practice.&lt;/p&gt;
    &lt;p&gt;My friend and colleague Bill Atkinson was a talented photographer, and one of his hobbies was playing around with digitized pictures, periodically experimenting to find the best algorithms for rendering them. Bill loved to explain his current work to whoever would listen to him, so I learned a lot about rendering gray scale images over the years simply by being around him. Bill had progressed over the years from using an "ordered dither" algorithm, where varying threshold values are specified in a sliding matrix, to his current favorite, which was a modified version of what was known as the "Floyd-Steinberg" algorithm, where an error term is maintained and distributed proportionally to neighboring pixels.&lt;/p&gt;
    &lt;p&gt;I called Thunderware and told them I was interested in working on Macintosh software for Thunderscan, in exchange for a per-unit royalty. I drove back up to Orinda, where Tom and Vic gave me lots of documentation about the scanner, and the sample code that Tom had written for the Apple II. For the next couple of months, I drove up to Orinda once a week, usually on Thursday, to meet with Tom and Vic show them my progress, prioritize development issues and discussion various complications as they arose. We would also discuss business terms, but we didn't sign a formal contract until the software was almost finished, when we settled on a royalty of $7.50 per unit.&lt;/p&gt;
    &lt;p&gt;Tom and Vic had already encountered and surmounted a number of tough problems just to get scanning going at all. For example, the ImageWriter printer was not really designed to be stepped one scanline at a time, and if you tried that the paper would bunch up against the platen, causing distortion. Tom and Vic solved the problem by commanding the printer to move three steps up and then two steps back, instead of a single step up, which held the paper snugly against the platen as required. There were also various techniques for sensing the beginning and end of the scan line, and some timings that were determined by tedious experimentation for how long it took the printer to respond to a command.&lt;/p&gt;
    &lt;p&gt;It took a week or so to get basic scanning working on the Macintosh, and then a few more days to render the gray scale data with Bill's modified Floyd-Steinberg dithering. After shaking out a variety of problems, mostly involving synchronization between the printer and the software, I was surprised and impressed by the consistent high quality of the results. I went through a brief, elated phase of scanning every image in sight that would fit through the printer, just to see how it would turn out.&lt;/p&gt;
    &lt;p&gt;One important design decision that I made early on was to keep the gray scale data around, to allow more flexible image processing. Thunderscan documents were five bits per pixel, before the Macintosh generally supported gray scale, and the user could manipulate the contrast and brightness of selected areas of the image, dodging and burning to reveal detail in the captured image. This also paid off in later versions when we implemented gray scale printing for Postscript printers.&lt;/p&gt;
    &lt;p&gt;My favorite feature that I came up with for Thunderscan had to do with two dimensional scrolling. Thunderscan documents could be quite large, so you could only show a portion of them in the image area of the window. You could scroll the image by dragging with a MacPaint-style "hand" scrolling tool, but you had to drag an awful lot to get to the extremes of a large image. I decided to add what I called "inertial" scrolling, where you gave the image a push and it kept scrolling at a variable speed in the direction of the push, after the mouse button was released. I had to add some hysteresis to keep the image from moving accidentally, and make various other tweaks, but soon I had it working and it felt great to be able to zip around large images by pushing them.&lt;/p&gt;
    &lt;p&gt;The hardest feature to perfect was bidirectional scanning. At first, Thunderscan only scanned from left to right, but it wasted time to return the scannner to the left after every scan line. We could almost double the speed if we scanned in both directions, but it was hard to get the adjacent scan lines that were scanned in opposite directions to line up properly. Ultimately, we made bidirectional scanning an optional feature, if you wanted to trade a little quality for greater speed.&lt;/p&gt;
    &lt;p&gt;I finished the software in November 1984, after taking a short break to work on something else (see Switcher). Thunderscan shipped in December 1984, and did well from the very beginning, with sales gradually rising from around 1,000 units/month to over 7,500 units/month at its peak in 1987. For a while, it was both the least expensive and highest quality scanning alternative for the Macintosh, although I'm sure it frustrated a lot of users by being too slow. I did three major revisions of the software over the next few years, improving the scan quality and adding features like gray scale printing and eventually gray scale display for the Macintosh II.&lt;/p&gt;
    &lt;p&gt;Eventually, the flat bed scanners caught up to Thunderscan, and then surpassed it, in both cost, quality and convenience. Over its lifetime, Thunderscan sold approximately 100,000 units and improved countless documents by providing users with an inexpensive way to capture high resolution graphics with their Macintoshes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.folklore.org/Thunderscan.html"/><published>2025-10-04T12:16:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45473730</id><title>Self-hosting email like it's 1984</title><updated>2025-10-05T00:50:54.041444+00:00</updated><content>&lt;doc fingerprint="e89d2d5b3a50be95"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Self-hosting an email server is useful for automating tasks like mailing lists, newsletters, or email verification APIs.&lt;/p&gt;
    &lt;p&gt;The elephant in the room is real-world deliverability. With self-hosting you risk not receiving mail or someone missing your mail. I accept this for my personal projects, but you may not. Keep this in mind.&lt;/p&gt;
    &lt;p&gt;For me the selling point of self-hosting is that itâs practically free. If youâre already self-hosting a website, installing some extra packages on your server and just a bit of your time is all thatâs required. Mail takes very little storage and the software is light, so youâre unlikely to significantly change energy consumption or disk usage.&lt;/p&gt;
    &lt;p&gt;For the longest time, I perceived self-hosting email as too difficult, but after doing it for one of my projects, I can say itâs not much harder or more time-consuming than configuring some email SaaS.&lt;/p&gt;
    &lt;p&gt;I changed my goals a bit to make the setup easier though. Self-hosting a multi-user webmail looks heavy and is more involved than I was willing to get into, so I just skipped it. That way, I didnât have to bother with user accounts, databases, or the web at all, and the task became easy.&lt;/p&gt;
    &lt;p&gt;With my config, manually sending and receiving email is possible if you SSH to your mail server and use the minimal sendmail or mailx commands, or Mutt if you like TUI. I've been semi-comfortably using mailx for a month already (with its ancient user interface!), so the setup is enough for me now, but I could expand it in the future, and multi-user webmail isnât completely off the table. Maybe Iâll even write a simple webmail package myself!&lt;/p&gt;
    &lt;head rend="h2"&gt;Postfix&lt;/head&gt;
    &lt;p&gt;You just need to open port 25, and install and configure Postfix and OpenDKIM on your machine. Postfix is a complete SMTP server, and is enough for basic mail alone, but in practice you also need OpenDKIM to get your mail delivered to popular services like Gmail.&lt;/p&gt;
    &lt;p&gt;Here's my Postfix config to show how easy it is. I left the master.cf file as it was, because Iâm always submitting email locally.&lt;/p&gt;
    &lt;p&gt;The default alias and header check config files are practically self-explanatory (just open them and read the comments!).&lt;/p&gt;
    &lt;head&gt;/etc/postfix/main.cf&lt;/head&gt;
    &lt;quote&gt;compatibility_level = 3.8 mail_owner = postfix myhostname = mx.idx.cy myorigin = idx.cy mydestination = localhost, idx.cy, maxadamski.com, localchat.cc inet_interfaces = all inet_protocols = ipv4 # Addresses alias_maps = hash:/etc/postfix/aliases alias_database = $alias_maps recipient_delimiter = + # I'm the only user on my machine, so I send from whichever address I want. #smtpd_sender_login_maps = hash:/etc/postfix/sender_login_maps #smtpd_sender_restrictions = reject_authenticated_sender_login_mismatch # spam #in_flow_delay = 1s header_checks = regexp:/etc/postfix/header_checks setgid_group = postdrop # TLS (strict) smtpd_tls_cert_file = /etc/ssl/tls/mx.idx.cy.crt smtpd_tls_key_file = /etc/ssl/tls/mx.idx.cy.key smtpd_tls_security_level = encrypt smtpd_tls_mandatory_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtpd_tls_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtp_tls_security_level = encrypt smtp_tls_mandatory_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtp_tls_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 # DKIM smtpd_milters = inet:localhost:8891 non_smtpd_milters = inet:localhost:8891 milter_default_action = accept&lt;/quote&gt;
    &lt;head&gt;/etc/postfix/master.cf&lt;/head&gt;
    &lt;quote&gt;# ========================================================================== # service type private unpriv chroot wakeup maxproc command + args # (yes) (yes) (no) (never) (100) # ========================================================================== smtp inet n - n - - smtpd pickup unix n - n 60 1 pickup cleanup unix n - n - 0 cleanup qmgr unix n - n 300 1 qmgr tlsmgr unix - - n 1000? 1 tlsmgr rewrite unix - - n - - trivial-rewrite bounce unix - - n - 0 bounce defer unix - - n - 0 bounce trace unix - - n - 0 bounce verify unix - - n - 1 verify flush unix n - n 1000? 0 flush proxymap unix - - n - - proxymap proxywrite unix - - n - 1 proxymap smtp unix - - n - - smtp relay unix - - n - - smtp -o syslog_name=postfix/$service_name showq unix n - n - - showq error unix - - n - - error retry unix - - n - - error discard unix - - n - - discard local unix - n n - - local virtual unix - n n - - virtual lmtp unix - - n - - lmtp anvil unix - - n - 1 anvil scache unix - - n - 1 scache postlog unix-dgram n - n - 1 postlogd&lt;/quote&gt;
    &lt;p&gt;Notice that there's no mention of POP3 or IMAP! I did waste some time trying to set them up with Dovecot (because they changed their config format too much, so guides became outdated, and their web docs were just hard to read for me). Ultimately I can just SSH to my server and I feel comfortable with mailx, so I skipped Dovecot. One package less in my system :)&lt;/p&gt;
    &lt;head rend="h2"&gt;TLS&lt;/head&gt;
    &lt;p&gt;You will also need an SSL certificate for encryption in transit. I hate getting and renewing SSL certificates, because the tools are bulky and automation is yet another moving part in your system (I used the lego package, with the manual DNS challenge for simplicity, but Iâm not too happy about it). I wonât give you a tutorial on getting SSL certificates, but note that you donât have to get and renew a certificate for each of your custom domains!&lt;/p&gt;
    &lt;p&gt;You just need one SSL certificate for your machine to encrypt data in transit to other SMTP servers. If you create an A record mx.example.com pointing to your email machineâs IP address, then grab a free certificate for mx.example.com from Letâs Encrypt. Then point to it in the Postfix configuration, and youâve got transport encryption! In short, only the MX hostname needs a cert for STARTTLS to be used for encryption.&lt;/p&gt;
    &lt;p&gt;Why no certificates for your actual email domains like example.com? Because the email domain has little to do with transport encryption. TLS only secures the connection between servers. You can still set whatever you want in the From header.&lt;/p&gt;
    &lt;head rend="h2"&gt;DKIM, SPF, and DMARC&lt;/head&gt;
    &lt;p&gt;You should prove that your emails actually come from your domain to make your mail trustworthy and deliver to Gmail and co. Thatâs what DKIM is for, and fortunately itâs a one-time deal per email domain. First you generate a key pair for each domain with OpenDKIM, and then you publish the public key in a TXT record in DNS. The keys donât expire automatically, but itâs best practice to rotate them periodically. My config uses a naming scheme that allows smooth rotation, but it doesnât complicate things if you skip it.&lt;/p&gt;
    &lt;p&gt;There are two more TXT records that you need to publish in DNS: the SPF and DMARC records. You say which hosts are allowed to send mail from your email domain, and give instructions to other email servers about what to do with mail that fails DKIM checks. In my case I told others to reject mail that canât be verified as coming from my domains, and send reports to my postmaster address.&lt;/p&gt;
    &lt;p&gt;Take a look at my OpenDKIM config to understand how things come together.&lt;/p&gt;
    &lt;head&gt;/etc/opendkim.conf&lt;/head&gt;
    &lt;quote&gt;UserID opendkim:opendkim Socket inet:8891@localhost KeyTable refile:/etc/opendkim/KeyTable SigningTable refile:/etc/opendkim/SigningTable ExternalIgnoreList refile:/etc/opendkim/TrustedHosts InternalHosts refile:/etc/opendkim/TrustedHosts Canonicalization relaxed/relaxed ReportAddress postmaster@idx.cy SendReports no LogWhy yes Syslog yes SyslogSuccess no&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/KeyTable&lt;/head&gt;
    &lt;quote&gt;key1._domainkey.idx.cy idx.cy:key1:/etc/opendkim/keys/idx.cy/key1.private key1._domainkey.maxadamski.com maxadamski.com:key1:/etc/opendkim/keys/maxadamski.com/key1.private key1._domainkey.localchat.cc localchat.cc:key1:/etc/opendkim/keys/localchat.cc/key1.private&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/SigningTable&lt;/head&gt;
    &lt;quote&gt;*@idx.cy key1._domainkey.idx.cy *@maxadamski.com key1._domainkey.maxadamski.com *@localchat.cc key1._domainkey.localchat.cc&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/TrustedHosts&lt;/head&gt;
    &lt;quote&gt;127.0.0.1 localhost&lt;/quote&gt;
    &lt;p&gt;I generate DKIM keys with the following command:&lt;/p&gt;
    &lt;quote&gt;opendkim-genkey -D /etc/opendkim/keys/example.com -d example.com -s key1&lt;/quote&gt;
    &lt;p&gt;And for each email domain I have the following records in DNS:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MX&lt;/cell&gt;
        &lt;cell&gt;example.com&lt;/cell&gt;
        &lt;cell&gt;mx.idx.cy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;example.com&lt;/cell&gt;
        &lt;cell&gt;v=spf1 mx a -all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;key1._domainkey&lt;/cell&gt;
        &lt;cell&gt;v=DKIM1; k=rsa; s=email; p=&amp;lt;public-key&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;_dmarc&lt;/cell&gt;
        &lt;cell&gt;v=DMARC1; p=reject; rua=mailto:postmaster@idx.cy&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Reverse DNS&lt;/head&gt;
    &lt;p&gt;One more thing about self-hosted email deliverability. I've read that reverse DNS (PTR record) will boost the reputation of your email server. The thing is that your ISP has to set it up, and I suspect my ISP to reply with a polite "no", so I didn't do it yet. As you'll see in the next section, my email gets delivered to Gmail just fine. GMX and Outlook also didn't mark my mail as spam. Maybe my IP is lucky :)&lt;/p&gt;
    &lt;p&gt;But in general, if you want wide deliverability, PTR isn't optional.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gmail&lt;/head&gt;
    &lt;p&gt;To try it out, let's send a test mail to Gmail with the sendmail command:&lt;/p&gt;
    &lt;quote&gt;sendmail -vt &amp;lt; test.mail&lt;/quote&gt;
    &lt;head&gt;test.mail&lt;/head&gt;
    &lt;quote&gt;Content-Type: text/html From: max@idx.cy To: myaddress@gmail.com Subject: DKIM test Test message from idx.cy!&lt;/quote&gt;
    &lt;p&gt;I got the mail instantly and Gmail confirmed TLS encryption.&lt;/p&gt;
    &lt;p&gt;Click "Show original" in Gmail to see the raw mail. There's lots of text in the headers, so let's just focus on passing SPF, DKIM, and DMARC :)&lt;/p&gt;
    &lt;p&gt;You'll also get a mail with a report because of the -v option. I receive mail with Heirloom Mail like this:&lt;/p&gt;
    &lt;quote&gt;You have new mail in /var/mail/max fool ~ | mailx Heirloom Mail version 12.5 7/5/10. Type ? for help. "/var/mail/max": 1 message 1 new &amp;gt;N 1 Mail Delivery System Sat Oct 4 15:40 74/2437 "Mail Delivery Status Report"&lt;/quote&gt;
    &lt;p&gt;I use the p command to print the mail.&lt;/p&gt;
    &lt;head&gt;&amp;amp; p&lt;/head&gt;
    &lt;quote&gt;Message 1: From MAILER-DAEMON Sat Oct 4 15:40:50 2025 X-Original-To: max@idx.cy Delivered-To: max@idx.cy Date: Sat, 4 Oct 2025 15:40:50 +0200 (CEST) From: Mail Delivery System &amp;lt;MAILER-DAEMON@idx.cy&amp;gt; Subject: Mail Delivery Status Report To: max@idx.cy Auto-Submitted: auto-replied Content-Type: multipart/report; report-type=delivery-status; boundary="3C311BFF8D.1759585250/mx.idx.cy" Status: R Part 1: Content-Description: Notification Content-Type: text/plain; charset=utf-8 This is the mail system at host mx.idx.cy. Enclosed is the mail delivery report that you requested. The mail system &amp;lt;myaddress@gmail.com&amp;gt;: delivery via gmail-smtp-in.l.google.com[X.X.X.X]:25: 250 2.0.0 OK 1759585250 4fb4d7f45d1cf-6393b6ba951si3187039a12.40 - gsmtp&lt;/quote&gt;
    &lt;p&gt;Great, everything is working!&lt;/p&gt;
    &lt;p&gt;If something isn't working for you, please double-check your DNS records, and triple-check that TLS certificates are readable by the Postfix user, and that DKIM keys are readable by the OpenDKIM user. Postfix and OpenDKIM logs will also be useful. The OpenDKIM config file is especially unforgiving of typos, so watch out for small mistakes!&lt;/p&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;In my next post on email, I'll show you how to use Python to build useful email applications. Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Btw, if you notice anything about my config (or want to share some thoughts) just email me at max@idx.cy :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maxadamski.com/blog/2025/10/email.html"/><published>2025-10-04T14:53:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45473861</id><title>A comparison of Ada and Rust, using solutions to the Advent of Code</title><updated>2025-10-05T00:50:53.656499+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/johnperry-math/AoC2023/blob/master/More_Detailed_Comparison.md"/><published>2025-10-04T15:10:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45474900</id><title>How to inject knowledge efficiently? Knowledge infusion scaling law for LLMs</title><updated>2025-10-05T00:50:53.559335+00:00</updated><content>&lt;doc fingerprint="ad1db54858d58a0a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 19 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.19371"/><published>2025-10-04T17:18:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45475528</id><title>Show HN: Run – a CLI universal code runner I built while learning Rust</title><updated>2025-10-05T00:50:53.067455+00:00</updated><content>&lt;doc fingerprint="24c05ddd9ba0d2c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Polyglot command runner &amp;amp; smart REPL that lets you script, compile, and iterate in 25+ languages without touching another CLI.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Built in Rust for developers who live in multiple runtimes.&lt;/p&gt;&lt;code&gt;run&lt;/code&gt;gives you a consistent CLI, persistent REPLs, and batteries-included examples for your favorite languages.&lt;/quote&gt;
    &lt;head&gt;Table of contents&lt;/head&gt;
    &lt;code&gt;# Show build metadata for the current binary
run --version

# Execute a snippet explicitly
run --lang python --code "print('hello, polyglot world!')"

# Let run detect language from the file extension
run examples/go/hello/main.go

# Drop into the interactive REPL (type :help inside)
run

# Pipe stdin (here: JSON) into Node.js
echo '{"name":"Ada"}' | run js --code "const data = JSON.parse(require('fs').readFileSync(0, 'utf8')); console.log(`hi ${data.name}`)"&lt;/code&gt;
    &lt;p&gt;All release assets are published on the GitHub Releases page, including macOS builds for both Apple Silicon (arm64) and Intel (x86_64). Pick the method that fits your platform:&lt;/p&gt;
    &lt;head&gt;Cargo (Rust)&lt;/head&gt;
    &lt;code&gt;cargo install run-kit&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Installs the&lt;/p&gt;&lt;code&gt;run&lt;/code&gt;binary from the&lt;code&gt;run-kit&lt;/code&gt;crate. Updating? Run&lt;code&gt;cargo install run-kit --force&lt;/code&gt;.&lt;/quote&gt;
    &lt;head&gt;Homebrew (macOS)&lt;/head&gt;
    &lt;code&gt;brew install --formula https://github.com/Esubaalew/run/releases/latest/download/homebrew-run.rb&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;This formula is published as a standalone file on each release; it isn’t part of the default Homebrew taps. Installing by name (&lt;/p&gt;&lt;code&gt;brew install homebrew-run&lt;/code&gt;) will fail—always point Homebrew to the release URL above (or download the file and run&lt;code&gt;brew install ./homebrew-run.rb&lt;/code&gt;).&lt;/quote&gt;
    &lt;p&gt;Once the latest release artifacts are published, Homebrew automatically selects the correct macOS binary for your CPU (Intel or Apple Silicon) based on this formula.&lt;/p&gt;
    &lt;head&gt;Debian / Ubuntu&lt;/head&gt;
    &lt;code&gt;curl -LO https://github.com/Esubaalew/run/releases/latest/download/run-deb.sha256
DEB_FILE=$(awk '{print $2}' run-deb.sha256)
curl -LO "https://github.com/Esubaalew/run/releases/latest/download/${DEB_FILE}"
sha256sum --check run-deb.sha256
sudo apt install "./${DEB_FILE}"&lt;/code&gt;
    &lt;head&gt;Windows (Scoop)&lt;/head&gt;
    &lt;code&gt;scoop install https://github.com/Esubaalew/run/releases/latest/download/run-scoop.json&lt;/code&gt;
    &lt;head&gt;Install script (macOS / Linux)&lt;/head&gt;
    &lt;code&gt;curl -fsSLO https://raw.githubusercontent.com/Esubaalew/run/master/scripts/install.sh
chmod +x install.sh
./install.sh --add-path           # optional: append ~/.local/bin to PATH&lt;/code&gt;
    &lt;p&gt;Pass &lt;code&gt;--version v0.2.0&lt;/code&gt;, &lt;code&gt;--prefix /usr/local/bin&lt;/code&gt;, or &lt;code&gt;--repo yourname/run&lt;/code&gt; to customize the install.&lt;/p&gt;
    &lt;head&gt;Download the archive directly&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Grab the &lt;code&gt;tar.gz&lt;/code&gt;(macOS/Linux) or&lt;code&gt;zip&lt;/code&gt;(Windows) from the latest release.&lt;/item&gt;
      &lt;item&gt;Extract it and copy &lt;code&gt;run&lt;/code&gt;/&lt;code&gt;run.exe&lt;/code&gt;onto your&lt;code&gt;PATH&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Optionally execute the bundled &lt;code&gt;install.sh&lt;/code&gt;to handle the copy for you.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Build from source&lt;/head&gt;
    &lt;code&gt;cargo install run-kit&lt;/code&gt;
    &lt;p&gt;The project targets Rust 1.70+. Installing from crates.io gives you the same &lt;code&gt;run&lt;/code&gt; binary that CI publishes; use &lt;code&gt;--force&lt;/code&gt; when upgrading to a newer release.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;run&lt;/code&gt; shells out to real toolchains under the hood. Each &lt;code&gt;LanguageEngine&lt;/code&gt; implements a small trait that knows how to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detect whether the toolchain is available (e.g. &lt;code&gt;python3&lt;/code&gt;,&lt;code&gt;go&lt;/code&gt;,&lt;code&gt;rustc&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Prepare a temporary workspace (compilation for compiled languages, transient scripts for interpreters).&lt;/item&gt;
      &lt;item&gt;Execute snippets, files, or stdin streams and surface stdout/stderr consistently.&lt;/item&gt;
      &lt;item&gt;Manage session state for the interactive REPL (persistent modules, stateful scripts, or regenerated translation units).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This architecture keeps the core lightweight while making it easy to add new runtimes or swap implementations.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;run&lt;/code&gt; supports 25+ languages:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Languages &amp;amp; aliases&lt;/cell&gt;
        &lt;cell role="head"&gt;Toolchain expectations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scripting &amp;amp; shells&lt;/cell&gt;
        &lt;cell&gt;Bash (&lt;code&gt;bash&lt;/code&gt;), Python (&lt;code&gt;py&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt;), Ruby (&lt;code&gt;rb&lt;/code&gt;, &lt;code&gt;ruby&lt;/code&gt;), PHP (&lt;code&gt;php&lt;/code&gt;), Perl (&lt;code&gt;perl&lt;/code&gt;), Lua (&lt;code&gt;lua&lt;/code&gt;), R (&lt;code&gt;r&lt;/code&gt;), Elixir (&lt;code&gt;ex&lt;/code&gt;, &lt;code&gt;elixir&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Matching interpreter on &lt;code&gt;PATH&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Web &amp;amp; typed scripting&lt;/cell&gt;
        &lt;cell&gt;JavaScript (&lt;code&gt;js&lt;/code&gt;, &lt;code&gt;node&lt;/code&gt;), TypeScript (&lt;code&gt;ts&lt;/code&gt;, &lt;code&gt;deno&lt;/code&gt;), Dart (&lt;code&gt;dart&lt;/code&gt;), Swift (&lt;code&gt;swift&lt;/code&gt;), Kotlin (&lt;code&gt;kt&lt;/code&gt;, &lt;code&gt;kotlin&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;node&lt;/code&gt;, &lt;code&gt;deno&lt;/code&gt;, &lt;code&gt;dart&lt;/code&gt;, &lt;code&gt;swift&lt;/code&gt;, &lt;code&gt;kotlinc&lt;/code&gt; + JRE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Systems &amp;amp; compiled&lt;/cell&gt;
        &lt;cell&gt;C (&lt;code&gt;c&lt;/code&gt;), C++ (&lt;code&gt;cpp&lt;/code&gt;, &lt;code&gt;cxx&lt;/code&gt;), Rust (&lt;code&gt;rs&lt;/code&gt;, &lt;code&gt;rust&lt;/code&gt;), Go (&lt;code&gt;go&lt;/code&gt;), Zig (&lt;code&gt;zig&lt;/code&gt;), Nim (&lt;code&gt;nim&lt;/code&gt;), Haskell (&lt;code&gt;hs&lt;/code&gt;, &lt;code&gt;haskell&lt;/code&gt;), Crystal (&lt;code&gt;cr&lt;/code&gt;, &lt;code&gt;crystal&lt;/code&gt;), C# (&lt;code&gt;cs&lt;/code&gt;, &lt;code&gt;csharp&lt;/code&gt;), Java (&lt;code&gt;java&lt;/code&gt;), Julia (&lt;code&gt;jl&lt;/code&gt;, &lt;code&gt;julia&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Respective compiler / toolchain&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Real programs live under the &lt;code&gt;examples/&lt;/code&gt; tree—each language has a &lt;code&gt;hello&lt;/code&gt; and a &lt;code&gt;progress&lt;/code&gt; scenario. The headers document expected output so you can diff your toolchain.&lt;/p&gt;
    &lt;code&gt;run examples/rust/hello.rs
run examples/typescript/progress.ts
run examples/python/counter.py&lt;/code&gt;
    &lt;p&gt;Being inside REPL we can use the ff commands&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:help&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List available meta commands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:languages&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show detected engines and status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;:lang &amp;lt;id&amp;gt;&lt;/code&gt; or &lt;code&gt;:&amp;lt;alias&amp;gt;&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Switch the active language (&lt;code&gt;:py&lt;/code&gt;, &lt;code&gt;:go&lt;/code&gt;, …)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:detect on/off/toggle&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Control snippet language auto-detection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:load path/to/file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execute a file inside the current session&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:reset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Clear the accumulated session state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;:exit&lt;/code&gt; / &lt;code&gt;:quit&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Leave the REPL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Apache 2.0. See LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Built with ❤️ in Rust. If &lt;code&gt;run&lt;/code&gt; unblocks your workflow, star the repo and share it with other polyglot hackers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Esubaalew/run"/><published>2025-10-04T18:34:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45475529</id><title>ProofOfThought: LLM-based reasoning using Z3 theorem proving</title><updated>2025-10-05T00:50:52.731334+00:00</updated><content>&lt;doc fingerprint="880be56c94434dea"&gt;
  &lt;main&gt;
    &lt;p&gt;LLM-based reasoning using Z3 theorem proving.&lt;/p&gt;
    &lt;code&gt;from openai import OpenAI
from z3dsl.reasoning import ProofOfThought

client = OpenAI(api_key="...")
pot = ProofOfThought(llm_client=client)

result = pot.query("Would Nancy Pelosi publicly denounce abortion?")
print(result.answer)  # False&lt;/code&gt;
    &lt;code&gt;from z3dsl.reasoning import EvaluationPipeline

evaluator = EvaluationPipeline(pot, output_dir="results/")
result = evaluator.evaluate(
    dataset="strategyqa_train.json",
    max_samples=10
)
print(f"Accuracy: {result.metrics.accuracy:.2%}")&lt;/code&gt;
    &lt;code&gt;pip install z3-solver openai scikit-learn numpy&lt;/code&gt;
    &lt;p&gt;The system has two layers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;High-level API (&lt;code&gt;z3dsl.reasoning&lt;/code&gt;) - Simple Python interface for reasoning tasks&lt;/item&gt;
      &lt;item&gt;Low-level DSL (&lt;code&gt;z3dsl&lt;/code&gt;) - JSON-based Z3 theorem prover interface&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most users should use the high-level API.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;examples/&lt;/code&gt; directory for complete examples including Azure OpenAI support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DebarghaG/proofofthought"/><published>2025-10-04T18:34:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45475808</id><title>Blog Feeds</title><updated>2025-10-05T00:50:52.223404+00:00</updated><content>&lt;doc fingerprint="769f1fb99e6596b5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tired of social media?&lt;/p&gt;
    &lt;p&gt;Keep doom scrolling through addicting feeds?&lt;/p&gt;
    &lt;p&gt;Miss the days when the web was just about connecting with people and their thoughts or ideas?&lt;/p&gt;
    &lt;p&gt;We believe there's an answer to that problem, and it's called&lt;/p&gt;
    &lt;p&gt;Starting a blog is actually a lot simpler than what you're probably thinking. This doesn't have to be some well polished highly viewed monetization machine, or even something professional or formal. It's just a simple website where you can casually talk about whatever you want to talk about! It can be long, short, a list of small things, or just a quote. It should be how you talk with other people in your own life, or how you communicate with the outside world. It should be you on a page. Here's a few places you can make a blog that are RSS enabled:&lt;/p&gt;
    &lt;p&gt;These are great if you are not quite a technical person and need everything to be simple and easy to use.&lt;/p&gt;
    &lt;p&gt;RSS is actually already familiar to you if you have ever subscribed to a newsletter. You put your email into someoneâs website, and when they have updates, they send you emails to your inbox so you can stay in the loop. In the case of RSS, you have a dedicated app, called an RSS reader usually, and you can put in someoneâs website into the app to subscribe. When they make a new post, just open your news reader app, and their posts will be retrieved and ready to read. Some reader apps even let you make folders and tags to organize blogs you are subscribed to, similar to how an email app lets you make folders to sort mail. Would highly recommend trying a few of the apps or services and seeing which works best!&lt;/p&gt;
    &lt;p&gt;This takes us to our final point: Feeds. You can probably get away with just the first two items and then sharing it with people you already know, but what about meeting or talking to people you don't know? That's where Feeds come in. The idea is to create another page on your blog that has all the RSS feeds you're subscribed to. By keeping this public and always up to date, someone can visit your page, find someone new and follow them. Perhaps that person also has a feeds page, and the cycle continues until there is a natural and organic network of people all sharing with each other. So if you have a blog, consider making a feeds page and sharing it! If your RSS reader supports OPML file exports and imports, perhaps you can share that file as well to make it easier to share your feeds.&lt;/p&gt;
    &lt;p&gt;Here's an example Feeds Page which should help get the idea across!&lt;/p&gt;
    &lt;p&gt;The best part about blog feeds? It's just an idea. There's no central authority. There's no platform. No massive tech giant trying to take your data. It's just you, basic web standards, and the people you care about.&lt;/p&gt;
    &lt;p&gt;Made by Steve&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blogfeeds.net"/><published>2025-10-04T19:08:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45476273</id><title>The UK is still trying to backdoor encryption for Apple users</title><updated>2025-10-05T00:50:51.899035+00:00</updated><content>&lt;doc fingerprint="8f988c715aa8d158"&gt;
  &lt;main&gt;
    &lt;p&gt;The Financial Times reports that the U.K. is once again demanding that Apple create a backdoor into its encrypted backup services. The only change since the last time they demanded this is that the order is allegedly limited to only apply to British users. That doesn’t make it any better.&lt;/p&gt;
    &lt;p&gt;The demand uses a power called a “Technical Capability Notice” (TCN) in the U.K.’s Investigatory Powers Act. At the time of its signing we noted this law would likely be used to demand Apple spy on its users. &lt;/p&gt;
    &lt;p&gt;After the U.K. government first issued the TCN in January, Apple was forced to either create a backdoor or block its Advanced Data Protection feature—which turns on end-to-end encryption for iCloud—for all U.K. users. The company decided to remove the feature in the U.K. instead of creating the backdoor.&lt;/p&gt;
    &lt;p&gt;The initial order from January targeted the data of all Apple users. In August, the US claimed the U.K. withdrew the demand, but Apple did not re-enable Advanced Data Protection. The new order provides insight into why: the U.K. was just rewriting it to only apply to British users. &lt;/p&gt;
    &lt;p&gt;This is still an unsettling overreach that makes U.K. users less safe and less free. As we’ve said time and time again, any backdoor built for the government puts everyone at greater risk of hacking, identity theft, and fraud. It sets a dangerous precedent to demand similar data from other companies, and provides a runway for other authoritarian governments to issue comparable orders. The news of continued server-side access to users' data comes just days after the UK government announced an intrusive mandatory digital ID scheme, framed as a measure against illegal migration.&lt;/p&gt;
    &lt;p&gt;A tribunal hearing was initially set to take place in January 2026, though it’s currently unclear if that will proceed or if the new order changes the legal process. Apple must continue to refuse these types of backdoors. Breaking end-to-end encryption for one country breaks it for everyone. These repeated attempts to weaken encryption violates fundamental human rights and destroys our right to private spaces.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/deeplinks/2025/10/uk-still-trying-backdoor-encryption-apple-users"/><published>2025-10-04T20:07:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45476820</id><title>$912 energy independence without red tape</title><updated>2025-10-05T00:50:51.797768+00:00</updated><content>&lt;doc fingerprint="b358d976fe9abd02"&gt;
  &lt;main&gt;
    &lt;p&gt;Cables / Tender:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://amzn.to/43NkcuJ - $14.79&lt;/item&gt;
      &lt;item&gt;https://amzn.to/4cwxY8Y - $30.42&lt;/item&gt;
      &lt;item&gt;https://amzn.to/4cuS3fx - $13.00&lt;/item&gt;
      &lt;item&gt;https://amzn.to/43u3ikz x2 - $17.00&lt;/item&gt;
      &lt;item&gt;https://amzn.to/43u3r7B - $12.99&lt;/item&gt;
      &lt;item&gt;https://amzn.to/43y5Qhx - $8.89&lt;/item&gt;
      &lt;item&gt;Total: $114.09&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;=&amp;gt; $912 total&lt;/p&gt;
    &lt;p&gt;Remote tracking (optional):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://amzn.to/3Vs3COI - $7.99&lt;/item&gt;
      &lt;item&gt;https://amzn.to/3TPaXGI - $23.92&lt;/item&gt;
      &lt;item&gt;https://amzn.to/4a6UMdI - $6.99&lt;/item&gt;
      &lt;item&gt;https://solar-assistant.io - $55.83&lt;/item&gt;
      &lt;item&gt;Total (additional): $94.73&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How to guide&lt;/head&gt;
    &lt;p&gt;Coming soon, for now refer to Will Prowse’s wiring guide on his very-similar 48V 3000W off-grid solar system which I followed and works great for me!&lt;/p&gt;
    &lt;head rend="h3"&gt;Financial Payback &amp;amp; Embodied Energy&lt;/head&gt;
    &lt;code&gt;Financial payback period for 3000W
System cost : $1,124 on Amazon in 2024 (now $912)
Yearly energy creation: 365d * 4.26hsun/d * 1.280kW = 2,000kWh/y (but more like 1,000kWh/year after losses)
Yearly value creation: 1,000kWh/y * $0.55/kWh in SF = $550/y energy created
100W system payback period: $1,124 / $550 = 2 years until payback
&lt;/code&gt;
    &lt;head rend="h3"&gt;How green is it:&lt;/head&gt;
    &lt;p&gt;Production footprint PV (source): &lt;code&gt;2,900kWhee/kW * 1.28kW = 3,712kWh embodied energy&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Production footprint LiFePo4 battery (source): &lt;code&gt;106kWhee/kWh * 2.4kWh = 254kWh embodied energy&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Annual energy production system: 1100kWh/y &lt;code&gt;Payback period: 3966kWh / 1100kWh/y = 3.5 year footprint payback&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;What’s the catch? Seems to good to be true? Well, this thing sits between your devices and the wall. So you need to neatly run extension cables from every room in the house to the “sun box”, and then run one cable from the box to the panels, and another to the wall (optional, just so it can fall back to pulling power from the wall). Photos of this are coming soon.&lt;/p&gt;
    &lt;p&gt;Will it ever push power back into the wall? Nope! It’ll only ever draw from the wall in the event that both the sun is down and the battery is dead (so your fridge won’t go off overnight for example).&lt;/p&gt;
    &lt;p&gt;Is this legal? Yes, see above. No difference to plugging your fridge into your wall, as far as the utility is concerned.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sunboxlabs.com/"/><published>2025-10-04T21:22:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45476821</id><title>Matrix Core Programming on AMD GPUs</title><updated>2025-10-05T00:50:51.585862+00:00</updated><content>&lt;doc fingerprint="179c1252016d1d30"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Matrix Core Programming on AMD CDNA3 and CDNA4 architecture&lt;/head&gt;
    &lt;p&gt;TL;DR In this blog post, we walk through how to use Matrix Cores in HIP kernels, with a focus on low-precision data types such as FP16, FP8, and FP4, as well as the new family of Matrix Core instructions with exponent block scaling introduced in the AMD CDNA™4 architecture. Through code examples and illustrations, we provide the necessary knowledge to start programming Matrix Cores, covering modern low-precision floating-point types, the Matrix Core compiler intrinsics, and the data layouts required by the Matrix Core instructions. The blog post is also available on ROCm Blogs.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Matrix Cores&lt;/head&gt;
    &lt;p&gt;Matrix multiplication is an essential part of AI and HPC workloads. The AMD CDNA™ architecture features special-purpose hardware, the Matrix Cores, to accelerate matrix fused-multiply-add (MFMA) operations defined as &lt;code&gt;D:=A*B+C&lt;/code&gt;. Please note that MFMA instructions are often used to update a matrix in-place (=accumulation) so that &lt;code&gt;D=C&lt;/code&gt; and &lt;code&gt;C:=A*B+C&lt;/code&gt;. The matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are called input matrices, while the matrix &lt;code&gt;D&lt;/code&gt; is referred to as the output matrix or accumulator.&lt;/p&gt;
    &lt;p&gt;The performance gains from using Matrix Cores are especially significant in mixed-precision mode, where the input matrices use lower-precision data types instead of FP32. The output matrix, however, is stored in FP32 to minimize accuracy loss during accumulation. The tables below show the theoretical peak performance of Matrix Cores with different input data types on both AMD CDNA™3 and AMD CDNA™4 architectures. On the AMD Instinct™ MI325X, using FP16 input matrices delivers nearly an 8x performance increase compared to single-precision, with only minimal accuracy loss. Switching to FP8 further doubles the performance providing a 16x increase when compared to FP32. The AMD CDNA™4 architecture further improves Matrix Core performance, delivering up to 2x higher throughput for FP16 and FP8 compared to the AMD CDNA™3 architecture. In addition, AMD CDNA™4 introduces new low-precision data types such as FP6 and FP4, enabling up to 64x performance gain relative to FP32. Please refer to the AMD CDNA™3 and AMD CDNA™4 white papers for detailed architecture specifications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI325X (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;1307.4 TF&lt;/cell&gt;
        &lt;cell&gt;~8x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;2614.9 TF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI355X (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;78.6 TF&lt;/cell&gt;
        &lt;cell&gt;~0.5x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;157.3 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;2.5 PF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;5 PF&lt;/cell&gt;
        &lt;cell&gt;~32x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP6&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP4&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;2. Low-Precision Floating-Point Types&lt;/head&gt;
    &lt;p&gt;A binary representation of a floating-point number consists of &lt;code&gt;n&lt;/code&gt; bits, where &lt;code&gt;m&lt;/code&gt; of &lt;code&gt;n&lt;/code&gt; bits represent the mantissa, 1 bit determines the sign and &lt;code&gt;n-m-1&lt;/code&gt; bits represent the exponent. The following image illustrates the binary format of a floating-point number and how the exponent and mantissa are calculated based on its binary representation.&lt;/p&gt;
    &lt;p&gt;Figure 1: Binary representation of a floating-point number.&lt;/p&gt;
    &lt;p&gt;Floating-point types are characterized by the number of bits used for the exponent and for the mantissa. Increasing the exponent width extends the range of representable values, while increasing the mantissa width improves precision. Since all floating-point types include the sign bit, a shorthand notation typically specifies only the exponent and mantissa widths. For example, the E4M3 type is an 8-bit floating-point type with 4-bit exponent and 3-bit mantissa. Additionally, a floating-point type is specified by exponent bias - a number that is subtracted from the exponent during conversion from binary format to real value. Given the exponent width, mantissa width, and exponent bias, one can convert the binary representation of a floating-point type (except E8M0) into its real value using the following equation:&lt;/p&gt;
    &lt;p&gt;Figure 2: Conversion to real value from binary representation for floating-point numbers.&lt;/p&gt;
    &lt;p&gt;Please note that the equation takes different forms depending on whether the exponent is zero or not. Often, certain exponent and mantissa values are reserved for special values (e.g. &lt;code&gt;NaN&lt;/code&gt;, &lt;code&gt;Infinity&lt;/code&gt;), which limits the range of representable real numbers. For example, the FP16 type has 5-bit exponent with a nominal range of &lt;code&gt;[0, 1, ... 2^5-1] = [0, 1, ... 31]&lt;/code&gt;. However, the exponent value &lt;code&gt;E = 31&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; (if the mantissa &lt;code&gt;M != 0&lt;/code&gt;) and &lt;code&gt;infinity&lt;/code&gt; (if the mantissa &lt;code&gt;M = 0&lt;/code&gt;). Therefore, the largest exponent value that can represent a real number is &lt;code&gt;E = 30&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The following table summarizes low-precision types commonly used in modern AI/ML workloads:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Shorthand&lt;/cell&gt;
        &lt;cell role="head"&gt;Exp. bias&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Zero&lt;/cell&gt;
        &lt;cell role="head"&gt;NaN&lt;/cell&gt;
        &lt;cell role="head"&gt;Infinity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;16-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M10 (FP16)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±65504&lt;/cell&gt;
        &lt;cell&gt;S 00000 0000000000&lt;/cell&gt;
        &lt;cell&gt;S 11111 xxxxxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111 0000000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M7 (BF16)&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;±3.3895 * 10^38&lt;/cell&gt;
        &lt;cell&gt;S 00000000 0000000&lt;/cell&gt;
        &lt;cell&gt;S 11111111 xxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111111 0000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;8-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FN (FP8, OCP)&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;±448&lt;/cell&gt;
        &lt;cell&gt;S 0000 000&lt;/cell&gt;
        &lt;cell&gt;S 1111 111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FNUZ (FP8)&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;±240&lt;/cell&gt;
        &lt;cell&gt;0 0000 000&lt;/cell&gt;
        &lt;cell&gt;1 0000 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2 (BF8, OCP)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 11111 {01, 10 11}&lt;/cell&gt;
        &lt;cell&gt;S 11111 00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2FNUZ (BF8)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;0 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M0&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;2^(±127)&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;11111111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;6-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E2M3&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±7.5&lt;/cell&gt;
        &lt;cell&gt;S 00 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E3M2 (BF6)&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;±28&lt;/cell&gt;
        &lt;cell&gt;S 000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;4-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;E2M1 (FP4)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±6&lt;/cell&gt;
        &lt;cell&gt;S 00 0&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the E4M3 type has two variants: E4M3FN and E4M3FNUZ. Both E4M3FN and E4M3FNUZ use 4 bits for the exponent and 3 bits for the mantissa. They use different exponent biases and differ in the special values they can represent. Neither variant supports infinities, which is why their notations include FN (FiNite). However, E4M3FN supports &lt;code&gt;+0&lt;/code&gt;, &lt;code&gt;-0&lt;/code&gt;, &lt;code&gt;+NaN&lt;/code&gt; and &lt;code&gt;-Nan&lt;/code&gt;, while E4M3FNUZ supports only &lt;code&gt;+0&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt;, hence &lt;code&gt;UZ&lt;/code&gt; (Unsigned Zero). The image below demonstrates how to convert a binary sequence into a real value, using E4M3FNUZ type as an example:&lt;/p&gt;
    &lt;p&gt;Figure 3: E4M3FNUZ encoding details.&lt;/p&gt;
    &lt;p&gt;FP8 types are divided into E4M3 and E5M2 formats. The E5M2 format is sometimes referred to as BF8, similar to BF16, where exponent width is larger compared to FP16. Similar to E4M3, E5M2 is further subdivided into two variants: E5M2 (OCP) and E5M2FNUZ. The AMD CDNA™3 architecture uses FNUZ variants for both E4M3 and E5M2, whereas the CDNA™4 architecture uses E4M3FN and E5M2 (OCP) variants. E4M3FN and E5M2 are standardized formats defined by the Open Compute Project (OCP). For detailed specifications, see the OCP Microscaling Formats (MX) Specification and the ONNX documentation. For visualization of FP8 values and their binary representations please refer to the FP8 Data table. Additionally, see the chapter “Low-precision floating-point types” in the AMD ROCm™ documentation for details on using low-precision types in HIP.&lt;/p&gt;
    &lt;p&gt;There is a special 8-bit format, E8M0, which is not used as a standard element data type but instead serves as a scale factor for microscaling types and block-scaled MFMA operations (discussed later in this article). Its value is calculated according to the equation below:&lt;/p&gt;
    &lt;p&gt;Figure 4: E8M0 encoding details.&lt;/p&gt;
    &lt;p&gt;The exponent value &lt;code&gt;E = 255&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; values, limiting the range of representable real numbers to &lt;code&gt;[2^-127 ... 2^127]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similar to FP8, FP6 has two formats: E2M3 and E3M2. The latter, E3M2, is often referred to as BF6 due to its larger exponent width compared to E2M3.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Matrix fused-multiply-add (MFMA) Instructions&lt;/head&gt;
    &lt;p&gt;The AMD CDNA™3 and CDNA™4 architectures support a variety of MFMA operations, which are characterized by the matrix dimensions &lt;code&gt;M&lt;/code&gt;, &lt;code&gt;N&lt;/code&gt;, &lt;code&gt;K&lt;/code&gt; and the data type of input/output matrices. The following table lists all available floating-point MFMA instructions for the AMD CDNA™3 and CDNA™4 architectures. As can be seen from the table, the AMD CDNA™4 architecture extends the set of available MFMA instructions by adding new FP16/BF16 instructions with larger matrix dimensions. Furthermore, it introduces FP6/FP4 data types and provides a completely new set of FP8/FP6/FP4 instructions where the types can be independently used for the matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. Finally, the AMD CDNA™4 architecture enables MFMA with block exponent scaling.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Type (C,D) ← (A,B)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;Note&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP64 ← FP64&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP32&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP16 (BF16)&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;Both A and B are either FP16 or BF16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;FP8 (E4M3) or BF8 (E5M2) can be used independently for A and B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8/FP6/FP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← MXFP8/MXFP6/MXFP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the table lists only floating-point type MFMA instructions with batch size = 1. In addition to them, the AMD CDNA™3 and CDNA™4 architectures support batched MFMA operations, where multiple output matrices are computed in parallel. These instructions are not covered in this article. See the Chapter 7 “Matrix Arithmetic Instructions” in the AMD CDNA™3 and AMD CDNA™4 ISA reference guides for the full list of available MFMA instructions.&lt;/p&gt;
    &lt;p&gt;The table above specifies cycle count for each MFMA operation. Given a known cycle count, one can estimate theoretical peak performance in TFLOP/s of corresponding MFMA operation using the formula below:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;
2*M*N*K * num_matrix_cores * (max_engine_clock / cycle_count) / 10^6,
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;num_matrix_cores&lt;/code&gt;is total number of matrix cores in a GPU (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_engine_clock&lt;/code&gt;is max engine clock (peak) in MHz (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cycle_count&lt;/code&gt;is cycle count of corresponding MFMA instruction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M, N, K&lt;/code&gt;are matrix dimensions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using this formula and the MFMA instruction &lt;code&gt;32x32x8 FP16&lt;/code&gt; as an example, we can estimate theoretical peak FP16 Matrix Core performance on the AMD Instinct™ MI325X:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;2*32*32*8 * 1216 * (2100 / 32) / 10^6 = 1307.4 TFLOP/s&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;To use Matrix Core instructions in HIP kernels, LLVM provides built-in compiler intrinsic functions. The list of all available compiler intrinsics can be found in the LLVM Github repository. The syntax of the MFMA intrinsics has the following format:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;d_reg = __builtin_amdgcn_mfma_ODType_MxNxKInDType(a_reg, b_reg, c_reg, cbsz, abid, blgp)&lt;/code&gt;,&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies the shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ODType&lt;/code&gt;is data type of the matrices&lt;code&gt;C&lt;/code&gt;and&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;InDType&lt;/code&gt;is data type of the input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cbsz&lt;/code&gt;,&lt;code&gt;abid&lt;/code&gt;,&lt;code&gt;blgp&lt;/code&gt;are broadcast flags. For the following discussion, these flags are irrelevant and are, therefore, set to 0 by default, unless specified otherwise. Please refer to the ISA reference guide for detailed information on the broadcast flags.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example,&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_16x16x16f16&lt;/code&gt;performs&lt;code&gt;16x16x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP16&lt;/code&gt;and the output matrix has type&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the output matrix is stored in&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_bf8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where the matrix&lt;code&gt;A&lt;/code&gt;has type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the matrix&lt;code&gt;B&lt;/code&gt;has type&lt;code&gt;BF8(E5M2)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MFMA instructions are wavefront-level (warp-level) instructions, where all work-items (threads) within a wavefront collectively perform a single MFMA operation and the operands &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt; are distributed across work-items so that each work-item in the wavefront holds a portion of the operands. In order to use the MFMA instructions, it’s required to understand how the operands are distributed across threads within a wavefront. The ISA reference guide fully specifies the data layout for all available MFMA instructions. For illustrative purposes, the next chapter explains a subset of the MFMA instructions and the corresponding data layouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Examples&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Important note: In the following discussion we assume the matrices are stored in row-major order. The wavefront size on the AMD CDNA™ architecture is 64. The shapes of the matrices&lt;/p&gt;&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;are&lt;code&gt;MxK&lt;/code&gt;,&lt;code&gt;KxN&lt;/code&gt;,&lt;code&gt;MxN&lt;/code&gt;, and&lt;code&gt;MxN&lt;/code&gt;, respectively. The first dimension denotes the number of rows and the second dimension the number of columns in a matrix. For example, the matrix&lt;code&gt;A&lt;/code&gt;has&lt;code&gt;M&lt;/code&gt;rows and&lt;code&gt;K&lt;/code&gt;columns.&lt;/quote&gt;
    &lt;head rend="h3"&gt;5.1. __builtin_amdgcn_mfma_f32_32x32x2f32&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x2&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;2x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input and output matrices are FP32. Since threads within a wavefront collectively perform single MFMA instruction, the operands are distributed across the threads. Each thread stores&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;M * K / wavefront_size = 32 * 2 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;A&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;K * N / wavefront_size = 2 * 32 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;B&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M * N / wavefront_size = 32 * 32 / 64 = 16&lt;/code&gt;entries of the matrix&lt;code&gt;C&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The operands are distributed according to the scheme below. The matrix elements highlighted in light red are those stored by the thread with index &lt;code&gt;0&lt;/code&gt; within the wavefront.&lt;/p&gt;
    &lt;p&gt;Figure 5: Data layout for `__builtin_amdgcn_mfma_f32_32x32x2f32`. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below demonstrates how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;

using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x2_fp32(const float* A, const float* B, float* C) {
    float a_reg;
    float b_reg;
    fp32x16_t c_reg {};

    const float* ldg_a_ptr = A + threadIdx.x / 32 + 2 * (threadIdx.x % 32);
    const float* ldg_b_ptr = B + threadIdx.x % 32 + (threadIdx.x / 32) * 32;

    a_reg = *ldg_a_ptr;
    b_reg = *ldg_b_ptr;

    c_reg = __builtin_amdgcn_mfma_f32_32x32x2f32(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;The GPU kernel can then be invoked on the host using a single wavefront:&lt;/p&gt;
    &lt;code&gt;mfma_fp32_32x32x2_fp32&amp;lt;&amp;lt;&amp;lt;1, 64&amp;gt;&amp;gt;&amp;gt;(A_device, B_device, C_device);
&lt;/code&gt;
    &lt;p&gt;Please note that we use the vector data type &lt;code&gt;fp32x16_t&lt;/code&gt; to store the entries of the matrix &lt;code&gt;C&lt;/code&gt; in registers. Additionally, we zero-initialize &lt;code&gt;c&lt;/code&gt;, since we compute &lt;code&gt;C = A * B&lt;/code&gt; without accumulation.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.2. __builtin_amdgcn_mfma_f32_16x16x16f16&lt;/head&gt;
    &lt;p&gt;This example demonstrates how to multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;16x16&lt;/code&gt;. The input matrices are stored in FP16 and the output matrix stored in FP32. In this case, each thread stores &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for this instruction is shown below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 6: Data layout for __builtin_amdgcn_mfma_f32_16x16x16f16. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;Corresponding HIP kernel is implemented below:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp16.h&amp;gt;

using fp16_t = _Float16;
using fp16x4_t = __attribute__((vector_size(4 * sizeof(fp16_t)))) fp16_t;
using fp32x4_t = __attribute__((vector_size(4 * sizeof(float)))) float;

__global__ void
mfma_fp32_16x16x16_fp16(const fp16_t* A, const fp16_t* B, float* C) {

    fp16x4_t a_reg;
    fp16x4_t b_reg;
    fp32x4_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp16x4_t*&amp;gt;(A + 4 * (threadIdx.x / 16) + 16 * (threadIdx.x % 16));

    for (int i = 0; i &amp;lt; 4; i++) {
        b_reg[i] = *(B + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64);
    }

    c_reg = __builtin_amdgcn_mfma_f32_16x16x16f16(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        *(C + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64) = c_reg[i];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that both &lt;code&gt;__half&lt;/code&gt; and &lt;code&gt;_Float16&lt;/code&gt; types can be used in device code. However, the host supports only &lt;code&gt;_Float16&lt;/code&gt; type for arithmetic operations. As in the previous example, we use vector data types to store the matrix elements in registers.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.3. __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input matrices are stored in FP8 and the output matrix is stored in FP32. In this scenario, each thread stores &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; elements of the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 7: Data layout for __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below implements this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp8.h&amp;gt;

using fp8_t = __hip_fp8_storage_t;
using fp8x8_t = __attribute__((vector_size(8 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x16_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x8_t a_reg;
    fp8x8_t b_reg;
    fp32x16_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp8x8_t*&amp;gt;(A + (threadIdx.x / 32) * 8 + (threadIdx.x % 32) * 16);

    for (int i = 0; i &amp;lt; 8; i++) {
        b_reg[i] = *(B + i * 32 + threadIdx.x % 32 + (threadIdx.x / 32) * 8 * 32);
    }

    c_reg = __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8((long)a_reg, (long)b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;To define FP8, we use &lt;code&gt;__hip_fp8_storage_t&lt;/code&gt; type from &lt;code&gt;hip_fp8.h&lt;/code&gt;. Note that the intrinsic function expects its first two operands to be of type &lt;code&gt;long&lt;/code&gt;. To compile the code, the operands &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are, therefore, converted to &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.4. __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f8&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Important note: the MFMA instruction discussed in this example is supported only on AMD CDNA™4 GPUs (gfx950). Please make sure to install AMD ROCm™ version 7.0 or later.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The AMD CDNA™4 architecture introduces a new family of MFMA instructions with block exponent scaling. The syntax of these instructions differs from the classic MFMA compiler intrinsics:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;d_reg = __builtin_amdgcn_mfma_scale_f32_MxNxK_f8f6f4(a_reg, b_reg, c_reg, Atype, Btype, OPSEL_A, scale_a, OPSEL_B, scale_b)&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Atype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;A&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Btype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;B&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OPSEL_A&lt;/code&gt;,&lt;code&gt;OPSEL_B&lt;/code&gt;are OPSEL codes. These arguments are not relevant for the discussion and therefore will be set to&lt;code&gt;0&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scale_a&lt;/code&gt;,&lt;code&gt;scale_b&lt;/code&gt;are scalars / vectors containing scale factors of type&lt;code&gt;E8M0&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As an example, let’s take a closer look at the instruction &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt;. The inputs to this instruction are&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix &lt;code&gt;A&lt;/code&gt;of size&lt;code&gt;32x64&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Ax&lt;/code&gt;of size&lt;code&gt;32x2&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;B&lt;/code&gt;of size&lt;code&gt;64x32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Bx&lt;/code&gt;of size&lt;code&gt;2x32&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. Specifically, this instruction performs the following operation using single wavefront (64 threads):&lt;/p&gt;
    &lt;p&gt;Figure 8: Block-scaled matrix multiplication via __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4.&lt;/p&gt;
    &lt;p&gt;During dot product operations, the scales &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; are applied after the normal dot product and prior to output/accumulation.&lt;/p&gt;
    &lt;p&gt;In this example, we will multiply two FP8 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. The input matrices &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; are stored in FP8 format, while the output matrix is stored in FP32. The scale matrices &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; contain elements of type &lt;code&gt;E8M0&lt;/code&gt;. Each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. Please note that this scheme is valid only if both input matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; have FP8 type. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 9: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP8 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The following code example shows how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp8_t = __amd_fp8_storage_t;
using fp8x32_t = __attribute__((vector_size(32 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x32_t a_reg;
    fp8x32_t b_reg;
    fp32x16_t c_reg {};

    const fp8_t* ldg_a = A + (threadIdx.x % 32) * 64 + (threadIdx.x / 32) * 16;
    for (int i=0; i &amp;lt; 2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            a_reg[i*16 + j] = *(ldg_a + i * 32 + j);
        }
    }

    const fp8_t* ldg_b = B + threadIdx.x % 32 + 32 * 16 * (threadIdx.x / 32);

    for (int i=0; i&amp;lt;2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            b_reg[i*16 + j] = *(ldg_b + 32 * j + i * 32 * 32);
        }
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 0, 0, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that in this example we use &lt;code&gt;__amd_fp8_storage_t&lt;/code&gt; type defined in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt; to represent FP8. This library provides extensions APIs for low-precision and micro-scaling formats, and compared to &lt;code&gt;hip_fp8.h&lt;/code&gt;, exposes a wider capability set. &lt;code&gt;gfx950&lt;/code&gt; provides hardware acceleration for these APIs. Most of the APIs are 1 to 1 mapping of hardware instruction. Additionally, we use &lt;code&gt;uint8_t&lt;/code&gt; type to represent &lt;code&gt;E8M0&lt;/code&gt; scale factors. Since &lt;code&gt;scale_a&lt;/code&gt; and &lt;code&gt;scale_b&lt;/code&gt; encode exponent values, the corresponding actual scale factors are &lt;code&gt;2^(scale_a - 127)&lt;/code&gt; and &lt;code&gt;2^(scale_b - 127)&lt;/code&gt;. If &lt;code&gt;scale_a = scale_b = 127&lt;/code&gt;, the actual scale factors are equal to &lt;code&gt;1&lt;/code&gt; and no scaling is applied.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.5. __builtin_amdgcn_mfma_scale_f32_32x32x64_f4f4&lt;/head&gt;
    &lt;p&gt;In our last example, we demonstrate how to multiply two FP4 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. As in the previous example, each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for the output matrix remains the same as in the FP8 case. However, the data layout for the input matrices is different and depicted below. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 10: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP4 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code snippet below demonstrates how to implement this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp4x2_t = __amd_fp4x2_storage_t;
using fp4x64_t  = fp4x2_t __attribute__((ext_vector_type(32)));
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp4_fp4(const fp4x2_t* A, const fp4x2_t* B, float* C) {

    fp4x64_t a_reg {};
    fp4x64_t b_reg {};
    fp32x16_t c_reg {};

    const fp4x2_t* ldg_a = A + (threadIdx.x % 32) * 32 + (threadIdx.x / 32) * 16;

    for (int i = 0; i &amp;lt; 16; i++) {
        a_reg[i] = *(ldg_a + i);
    }

    const fp4x2_t* ldg_b = B + (threadIdx.x % 32) / 2 + 16 * 32 * (threadIdx.x / 32);
    int b_extract_idx = threadIdx.x % 2;

    for (int i = 0; i &amp;lt; 16; i++) {
        uint8_t tmp0 = __amd_extract_fp4(*(ldg_b + 16 * 2 * i), b_extract_idx);
        uint8_t tmp1 = __amd_extract_fp4(*(ldg_b + 16 * (2 * i + 1)), b_extract_idx);
        b_reg[i] = __amd_create_fp4x2(tmp0, tmp1);
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 4, 4, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Since memory addressing is not allowed at a granularity smaller than 8 bits, we use &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; (an alias for &lt;code&gt;uint8_t&lt;/code&gt;) to store the input matrices and enable pointer operations. Note that the FP4 elements that need to be loaded from the matrix &lt;code&gt;B&lt;/code&gt; are not contiguous in memory. To extract a single FP4 element, we use the &lt;code&gt;__amd_extract_fp4&lt;/code&gt; function provided in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt;. This function returns one FP4 element (of type &lt;code&gt;uint8_t&lt;/code&gt;) from a fp4x2 vector, based on the index passed as the second argument:&lt;/p&gt;
    &lt;code&gt;uint8_t __amd_extract_fp4(const __amd_fp4x2_storage_t x, const size_t index) {
    if (index == 0) return (x &amp;amp; 0xFu);
    return (x &amp;gt;&amp;gt; 4);
}
&lt;/code&gt;
    &lt;p&gt;Two FP4 values are then combined into &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; using &lt;code&gt;__amd_create_fp4x2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;__amd_fp4x2_storage_t __amd_create_fp4x2(const uint8_t x, const uint8_t y) {
    __amd_fp4x2_storage_t ret = 0;
    ret = x | (y &amp;lt;&amp;lt; 4);
    return ret;
}
&lt;/code&gt;
    &lt;p&gt;The compiler intrinsic function &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; requires its first two arguments to be 256 bits wide. Since 32 FP4 elements occupy only 128 bits, we define &lt;code&gt;fp4x64_t&lt;/code&gt;, which is 256 bits wide. In this type, 128 bits contain data, while the remaining 128 bits are zero. This allows us to pass &lt;code&gt;a_reg&lt;/code&gt; and &lt;code&gt;b_reg&lt;/code&gt; to the intrinsic function and compile the code successfully.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;In this article, we introduced Matrix Core instructions available on the AMD CDNA™3 and CDNA™4 architectures. We covered floating-point formats in detail, including modern low-precision element data types such as FP8, FP6, FP4, and the scale data type E8M0. We further explained how the floating-point types are represented as binary sequences and demonstrated, with concrete examples, how to convert their binary representations into real values. Next, we listed Matrix Core instructions supported by the modern CDNA™ architectures and discussed how to calculate the theoretical peak performance of Matrix Cores for specific MFMA instructions. To make the discussion more practical, we reviewed the compiler intrinsic functions that allow users to program Matrix Cores inside HIP kernels. Finally, we examined a subset of MFMA instructions in detail, providing code examples and illustrations to explain data layout and demonstrate how to implement simple mixed-precision MFMA operations in HIP. For additional information on Matrix Cores and low-precision data types, please refer to the following resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://salykova.github.io/matrix-cores-cdna"/><published>2025-10-04T21:22:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45477118</id><title>XiangShan Vector Floating-Point Unit Design</title><updated>2025-10-05T00:50:51.112220+00:00</updated><content>&lt;doc fingerprint="4631822b103a94a4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VFPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Version: V2R2&lt;/item&gt;
      &lt;item&gt;Status: OK&lt;/item&gt;
      &lt;item&gt;Date: 2025/01/20&lt;/item&gt;
      &lt;item&gt;commit：xxx&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Glossary of Terms&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Abbreviation&lt;/cell&gt;
        &lt;cell role="head"&gt;Full name&lt;/cell&gt;
        &lt;cell role="head"&gt;Descrption&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VFPU&lt;/cell&gt;
        &lt;cell&gt;Vector Floating-Point Unit&lt;/cell&gt;
        &lt;cell&gt;Vector Floating-Point Functional Unit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;IQ&lt;/cell&gt;
        &lt;cell&gt;Issue Queue&lt;/cell&gt;
        &lt;cell&gt;Issue Queue&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Design specifications&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Support Vector Floating-Point Mul Calculation&lt;/item&gt;
      &lt;item&gt;Support vector floating-point FMA computation&lt;/item&gt;
      &lt;item&gt;Support Vector Floating-Point Div Calculation&lt;/item&gt;
      &lt;item&gt;Support for vector floating-point Sqrt computation&lt;/item&gt;
      &lt;item&gt;Supports fp32, fp64, fp16 computation&lt;/item&gt;
      &lt;item&gt;Supports computation of RV-V1.0 version vector floating-point instructions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Function&lt;/head&gt;
    &lt;p&gt;The VFPU module receives uop information issued from the Issue Queue and performs vector floating-point instruction calculations based on fuType and fuOpType information. It mainly consists of four modules: VFAlu, VFMA, VFDivSqrt, and VFCvt.&lt;/p&gt;
    &lt;p&gt;VFAlu is primarily responsible for fadd-related instructions and some other simple instructions, such as comparison instructions and sign injection instructions. Notably, the reduction sum instruction is also computed in this module by splitting into micro-operations (uops).&lt;/p&gt;
    &lt;p&gt;VFMA is primarily responsible for multiplication and multiply-add related instructions.&lt;/p&gt;
    &lt;p&gt;VFDivSqrt is primarily responsible for instructions related to division and square root.&lt;/p&gt;
    &lt;p&gt;VFCvt is primarily responsible for format conversion and reciprocal estimation-related instructions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Algorithm design&lt;/head&gt;
    &lt;p&gt;The challenge of the vector floating-point unit lies in supporting multiple single-precision format calculations (where the floating-point formats of operands and results are the same) and mixed-precision calculations (where the floating-point formats of operands and results differ). Taking common formats such as half-precision (\(f16\)), single-precision (\(f32\)), and double-precision (\(f64\)) as examples, the differences between scalar and vector floating-point units are compared.&lt;/p&gt;
    &lt;p&gt;Taking a typical floating-point addition as an example, for a scalar floating-point unit, it only needs to support calculations in three single-precision formats. The input operands and output results of this unit should all be \(64\)-bit, meaning it must support calculations in three formats:&lt;/p&gt;
    &lt;p&gt;(1) One \(f64 = f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) \(1\) \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(1\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;At first glance, three modules seem necessary to handle these three formats. However, since floating-point numbers consist of a sign bit, exponent, and mantissa, and higher-precision floating-point numbers have wider exponent and mantissa bit widths than lower-precision ones, the hardware design for higher-precision floating-point numbers can fully meet the requirements of lower-precision floating-point calculations. With slight modifications, adding \(Mux\) (multiplexers) to the hardware can enable compatibility with multiple single-precision formats, with only a marginal increase in area.&lt;/p&gt;
    &lt;p&gt;The vector floating-point unit needs to support vector operations, which are characterized by high data bandwidth utilization. For example, although the interface of a scalar arithmetic unit is 64-bit, when computing f32/f16, the effective data is only 32/16 bits, reducing bandwidth utilization to 50%/25%. The vector arithmetic unit also has a 64-bit interface, but when computing single-precision formats f32/f16, it can perform 2/4 sets of operations simultaneously, maintaining 100% bandwidth utilization. The supported single-precision format computations are as follows:&lt;/p&gt;
    &lt;p&gt;(1) One \(f64 = f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) 2 \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(4\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Performing multiple sets of floating-point additions with the same format simultaneously makes hardware design more challenging than scalar operations, but it also allows the reuse of high-precision format hardware for low-precision formats. Additionally, a key feature that vector floating-point units must support is mixed-precision computation. The \(RISC-V\) vector instruction set extension defines a series of \(widening\) instructions requiring mixed-precision computation, mandating that floating-point addition units also support the following four computation formats:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64 = f64 + f32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(f64 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) Two \(f32 = f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(4) Two \(f32 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;The design difficulty of mixed-precision computation is much greater than that of multiple single-precision formats. On one hand, operands of different data formats need to be converted to the same format as the result before computation, increasing logical complexity. On the other hand, format conversion imposes significant pressure on circuit timing, especially when converting low-precision denormal numbers to high-precision floating-point numbers. Therefore, this paper specifically designs a fast data format conversion algorithm to address the timing issue.&lt;/p&gt;
    &lt;p&gt;In summary, the design challenges of the vector floating-point unit lie in the implementation of multiple single-precision formats and mixed-precision formats. This section will introduce the vector floating-point addition algorithm, floating-point sequential accumulation algorithm, vector fused multiply-add algorithm, and vector floating-point division algorithm to address these challenges, achieving a high-performance vector floating-point unit with a frequency of up to \(3GHz\).&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Addition&lt;/head&gt;
    &lt;p&gt;Floating-point addition is one of the most commonly used arithmetic operations in scientific computing. Although conceptually simple, the traditional single-path floating-point addition algorithm requires two to three signed addition steps, which is a relatively time-consuming operation. The dual-path floating-point addition algorithm has only one signed addition operation on the critical path in the worst case, thus offering significant speed advantages over the single-path algorithm. Based on the dual-path floating-point addition algorithm, this paper designs an even faster improved dual-path floating-point addition algorithm. This section first introduces the single-path floating-point addition algorithm, the dual-path floating-point addition algorithm, and the improved dual-path floating-point addition algorithm for single-precision format, and finally presents the vector floating-point addition algorithm.&lt;/p&gt;
    &lt;p&gt;The floating-point addition formula is expressed as: \(fp\_result = fp\_a + fp\_b\). When \(fp\_a\) and \(fp\_b\) have the same sign, the significands are aligned and added, which is referred to as equivalent addition. When \(fp\_a\) and \(fp\_b\) have opposite signs, the significands are aligned and subtracted, which is referred to as equivalent subtraction. For denormal numbers, the exponent is \(0\), and for normalized numbers, the exponent is \(1\), but the corresponding normalized exponent is the same. Therefore, when calculating the exponent difference, an exponent of \(0\) should be treated as \(1\) (referred to as the normalized exponent). The absolute difference between the normalized exponents is the normalized exponent difference.&lt;/p&gt;
    &lt;head rend="h4"&gt;Single-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The traditional single-path floating-point addition operation is illustrated as follows, consisting of the following steps:&lt;/p&gt;
    &lt;p&gt;(1) Normalized exponent subtraction (ES): Calculate the difference between normalized exponents, d = |Ea - Eb|, where Ea and Eb are both normalized exponents.&lt;/p&gt;
    &lt;p&gt;(2) Alignment (\(Align\)): Shift the significand of the smaller operand right by \(d\) bits. The larger exponent is denoted as \(Ef\).&lt;/p&gt;
    &lt;p&gt;(3) Significand addition (\(SA\)): Performs addition or subtraction based on the effective operation \(Eo\), which is the arithmetic operation executed by the adder in the floating-point addition unit, determined by the sign bits of the two floating-point operands.&lt;/p&gt;
    &lt;p&gt;(4) Conversion (\(Conv\)): If the significand addition result is negative, convert the result to sign-magnitude representation. The conversion is completed through an addition step, with the result denoted as \(Sf\).&lt;/p&gt;
    &lt;p&gt;(5) Leading zero detection (LZD): Calculates the required left or right shift amount, expressed as \(En\), where right shift is positive and left shift is negative.&lt;/p&gt;
    &lt;p&gt;(6) Normalization (\(Norm\)): Normalize the significand by shifting \(En\) bits and add \(En\) to \(Ef\).&lt;/p&gt;
    &lt;p&gt;(7) Rounding (\(Round\)): Round according to the \(IEEE\)-\(754\) standard, adding \(1\) to the \(LSB\) of \(Sf\) if necessary. This step may cause overflow, requiring the mantissa result to be right-shifted by one bit while incrementing the exponent \(Ef\) by \(1\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Dual-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The above single-path floating-point algorithm is slow because the steps in the addition operation are essentially executed serially. This algorithm can be improved in the following ways:&lt;/p&gt;
    &lt;p&gt;(1) In the single-path floating-point addition algorithm, the \(Conv\) step is only needed when the result is negative, and it can be avoided by swapping the significands of the two operands. By checking the sign of the \(ES\) step result, the significands can be swapped (\(Swap\)) accordingly, always computing the larger significand minus the smaller one. When exponents are equal, the result may still be negative, requiring conversion, but no rounding is needed in this case. Thus, the swap step makes rounding and conversion mutually exclusive, allowing them to be parallelized. Note that another advantage of swapping is that only one shifter is required.&lt;/p&gt;
    &lt;p&gt;(2) The leading zero detection step can be executed in parallel with the significand addition step, removing it from the critical path. This optimization is particularly important in cases where subtraction requires significant left shifts.&lt;/p&gt;
    &lt;p&gt;(3) So far, the critical path steps have been reduced to: normalized exponent subtraction, swapping, alignment, significand addition \(||\) leading zero detection, conversion \(||\) rounding, normalization (where \(||\) denotes steps that can be executed in parallel). The alignment and normalization steps are mutually exclusive and can be further optimized. Normalization requires a large left shift only when \(d≤1\) or during equivalent subtraction. Conversely, alignment requires a large right shift only when \(d &amp;gt; 1\). By distinguishing these two cases, only one large shift—either alignment or normalization—remains on the critical path.&lt;/p&gt;
    &lt;p&gt;The steps for single-path and dual-path floating-point addition algorithms are shown in the table. In the dual-path algorithm, the preprocessing step (\(Pred\)) in the \(d ≤ 1\) path determines whether a right shift is needed to align significands based on the value of \(d\). The dual-path algorithm improves speed by executing more steps in parallel, requiring additional hardware for implementation.&lt;/p&gt;
    &lt;p&gt;Table: Steps for Two Floating-Point Addition Algorithms&lt;/p&gt;
    &lt;p&gt;+------------------+-----------------------------------------------------+ | Single-Path Floating-Point Addition | Dual-Path Floating-Point Addition Algorithm | | +-----------------------------+-----------------------+ | | \(d\leq1\) and Equivalent Subtraction | \(d&amp;gt;1\) or Equivalent Addition | +:================:+:===========================:+:=====================:+ | Normalized Exponent Addition | Preprocessing + Swap | Normalized Exponent Subtraction + Swap | +------------------+-----------------------------+-----------------------+ | Alignment | -- | Alignment | +------------------+-----------------------------+-----------------------+ | Significant Digit Addition | Significant Digit Addition or Leading Zero Detection | Significant Digit Addition | +------------------+-----------------------------+-----------------------+ | Conversion | Conversion or Rounding | Rounding | +------------------+-----------------------------+-----------------------+ | Leading Zero Detection | -- | -- | +------------------+-----------------------------+-----------------------+ | Normalization | Normalization | -- | +------------------+-----------------------------+-----------------------+ | Rounding | Path Selection | Path Selection | +------------------+-----------------------------+-----------------------+&lt;/p&gt;
    &lt;p&gt;In the dual-path floating-point addition algorithm, during the \(SA\) step in the case of equivalent subtraction, one of the significant digits is in 2's complement form. The complementation step and the rounding step are mutually exclusive, thus they can be performed in parallel. The optimized dual-path floating-point addition algorithm is shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(d≤1\) and equivalent subtraction&lt;/cell&gt;
        &lt;cell role="head"&gt;\(d&amp;gt;1\) or equivalent addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Preprocessing + Exchange&lt;/cell&gt;
        &lt;cell&gt;Normalized Instruction Subtraction + Swap&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Significant Digit Addition Conversion&lt;/cell&gt;
        &lt;cell&gt;Rounding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Normalization&lt;/cell&gt;
        &lt;cell&gt;Significand addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Selection Path&lt;/cell&gt;
        &lt;cell&gt;Selection Path&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In the IEEE round-to-nearest (\(RTN\)) mode, computing \(A+B\) and \(A+B+1\) suffices to address all normalization possibilities (additional computation of \(A+B+2\) is required for rounding toward positive or negative infinity). By utilizing \(Cin\) to select the final rounded mantissa result from multiple sets of significand adder outputs, both two's complement conversion and rounding can be completed simultaneously, saving an addition step. Since floating-point addition may require normalization through a right shift by one bit, no shift, or a left shift (potentially as extensive as the significand's length), \(Cin\) must account for all these normalization possibilities to ensure the selected result is the rounded one.&lt;/p&gt;
    &lt;head rend="h4"&gt;Improved dual-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;This section details the improved dual-path floating-point addition algorithm proposed in this paper. The path for equivalent addition or equivalent subtraction with d &amp;gt; 1 is called the far path, while the path for equivalent subtraction with d ≤ 1 is called the close path. Cases involving infinity or NaN operands are handled separately and do not belong to the far or close paths.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(far\) path&lt;/head&gt;
    &lt;p&gt;The \(far\) path algorithm is illustrated in the figure, with the main steps as follows:&lt;/p&gt;
    &lt;p&gt;Step 1: In the \(far\) path, when the exponent difference \(d\) is greater than \(1\), the smaller significand is shifted right by \(d\) bits to align with the larger significand. First, calculate the normalized exponent difference. To accelerate computation, two adders are used to compute the normalized exponent difference while comparing the magnitudes of \(Efp\_a\) and \(Efp\_b\). The correct normalized exponent difference is selected based on the comparison result of the exponent magnitudes.&lt;/p&gt;
    &lt;p&gt;In the second step, based on the exponent comparison from the first step, the significand of the operand with the larger exponent and the significand of the operand with the smaller exponent can be selected in parallel while also selecting the larger exponent \(EA\). For equivalent subtraction, \(EA\) is decremented by \(1\) (in this case, \(EA\) cannot be \(0\), as that would fall under the \(close\) path). This adjustment aims to align the value range of the significand after subtraction with that of equivalent addition, facilitating the selection of the final result. The adjusted significand addition or subtraction result falls within the range \([1\)-\(4)\), divided into two cases: \([1\)-\(2)\) and \([2\)-\(4)\).&lt;/p&gt;
    &lt;p&gt;Step three involves right-shifting the smaller significand, which is divided into two scenarios: during equivalent subtraction, the smaller significand is first inverted and then arithmetically right-shifted, saving some time compared to right-shifting first and then inverting; during equivalent addition, a logical right shift is directly applied. To reduce the number of shifter stages, when the high-order bits of the normalized exponent difference are all \(0\), the lower bits (the specific number depends on the significand width) are used for the right shift. If the high-order bits are not all \(0\), the right-shift result is \(0\). Here, the adder result from the first step, which calculates the normalized exponent difference between the two, is used, with the least significant bit applied first (since the adder result's least significant bit is obtained earliest). Specifically: if \(fp\_a\)'s exponent is larger, only \(fp\_b\)'s significand is right-shifted by the value of \(fp\_a\)'s normalized exponent minus \(fp\_b\)'s normalized exponent; if \(fp\_b\)'s exponent is larger, only \(fp\_a\)'s significand is right-shifted by the value of \(fp\_b\)'s normalized exponent minus \(fp\_a\)'s normalized exponent. The final right-shifted significand is then selected based on the exponent magnitude relationship and the normalized exponent difference, and the \(grs\) (\(guard\), \(round\), \(sticky\)) bits after the shift are calculated. To ensure correct rounding for the two scenarios in step two, two sets of \(grs\) need to be computed for the significand addition/subtraction results within \([1\)-\(2)\) and \([2\)-\(4)\).&lt;/p&gt;
    &lt;p&gt;Step 4: Perform significand addition. For equivalent subtraction, the smaller significand is inverted before right-shifting. Denote the larger significand as \(A\) and the right-shifted smaller significand as \(B\). Two adders compute \(A+B\) and \(A+B+2\), and the final rounded result is selected from these two adder outputs.&lt;/p&gt;
    &lt;p&gt;Step five: generate the final result. Depending on whether the significant digits \(A+B\) result falls within \([1\)-\(2)\) (case one) or \([2\)-\(4)\) (case two), and based on the two sets of \(grs\) and rounding modes calculated during the previous right shift, determine the conditions for selecting the two significant digit adders in case one and case two, respectively. Finally, use a one-hot four-way selection to choose the mantissa result. The exponent result is either \(EA\) (case one and mantissa rounded to \(&amp;lt;1\)) or \(EA+1\) (case two or case one rounded to \(=2\)). Note whether the exponent overflows after rounding, and the final result is selected between the overflow result and the normal computation result based on \(overflow\). The exception flags in the \(far\) path only produce overflow and inexact results.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(close\) path&lt;/head&gt;
    &lt;p&gt;In the \(close\) path, it must be an effective subtraction with \(d \leq 1\), specifically categorized as \(d=0\) or \(d=1\). The algorithm is illustrated in the figure, with the following detailed steps:&lt;/p&gt;
    &lt;p&gt;Step 1: Perform four sets of significand subtractions in parallel. Based on \(d=0\) (\(fp\_a\) significand is larger, \(fp\_b\) significand is larger) and \(d=1\) (\(fp\_a\) normalized exponent is larger, \(fp\_b\) normalized exponent is larger), combine the four scenarios for effective subtraction. The first subtractor: \(fp\_a\) significand \(-\) \(fp\_b\) significand; the second subtractor: \(fp\_b\) significand \(-\) \(fp\_a\) significand; the third subtractor: \(fp\_a\) significand \(×2\) \(-\) \(fp\_b\) significand; the fourth subtractor: \(fp\_b\) significand \(×2\) \(-\) \(fp\_a\) significand. Simultaneously, calculate the \(grs\) bits based on the exponent magnitude relationship. When \(d=0\), all \(grs\) bits are \(0\); when \(d=1\), only \(g\) may be non-zero. These four sets of adders cannot produce all rounding results, so a fifth slower adder is added: the significand with the larger exponent \(–\) the significand with the smaller exponent shifted right by one bit.&lt;/p&gt;
    &lt;p&gt;Step two: Determine the four conditions for selecting the four sets of significand subtractions, based on the value of \(d\), the most significant bit of the adder result, \(grs\), and the rounding mode. After selecting the subtraction result from the four sets of adders, perform \(LZD\) \(+\) left shift on the subtraction result. Here, attention must be paid to the value of the larger exponent \(EA\). The left shift is controlled jointly by \(LZD\) and \(EA\), generating a \(mask\) value (with the same bit width as the subtraction result but with at most one bit set to \(1\)) based on the value of \(EA\). This \(mask\) is ORed with the subtraction result before performing \(LZD+\) left shift.&lt;/p&gt;
    &lt;p&gt;Step 3: Determine the condition for selecting the fifth subtractor. When selecting the result of the fifth subtractor, no left shift is required, so a slower adder is used, and the final mantissa result can then be selected.&lt;/p&gt;
    &lt;p&gt;Step four: exponent and sign bit results. The exponent result requires subtracting the \(LZD\) value from step two from \(EA\). If the fifth subtractor is selected as the mantissa result, the exponent remains unchanged. When \(d=1\), the sign bit is the sign of the operand with the larger exponent. When \(d=0\), the sign bit is selected based on the mantissa size. Note that when the result is \(0\) and rounded down, the sign bit is \(1\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The vector floating-point adder's output signal width is \(64\) bits, supporting mixed precision and widening instructions. It must support calculations for the following data formats:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64\) \(= f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) \(1\) \(f64\) \(= f64 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) 1 \(f64\) = \(f32\) + \(f32\);&lt;/p&gt;
    &lt;p&gt;(4) \(2\) \(f32\) values \(= f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(5) \(2\) \(f32\) \(= f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(6) Two \(f32\) \(= f16 + f16\);&lt;/p&gt;
    &lt;p&gt;(7) Four \(f16\) = \(f16 + f16\).&lt;/p&gt;
    &lt;head rend="h5"&gt;Module partitioning&lt;/head&gt;
    &lt;p&gt;The computation approach uses one module for the first three formats, all outputting 64-bit results. The single-precision floating-point adder for \(f64 = f64 + f64\) is reused to compute \(f64 = f64 + f32\) and \(f64 = f32 + f32\). This paper proposes a fast data format conversion algorithm to convert \(f32\) operands to \(f64\), enabling \(f64 = f64 + f64\) computation and yielding results in \(f64\) format.&lt;/p&gt;
    &lt;p&gt;The same approach is applied to computation formats where the output is \(f32\). Since \(f32\) has less timing pressure, integrating a \(f16 = f16 + f16\) operation into the module that computes \(f32\) results saves area while supporting:&lt;/p&gt;
    &lt;p&gt;(1) One \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(f32 = f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(3) One \(f32 = f16 + f16\);&lt;/p&gt;
    &lt;p&gt;(4) One \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Clearly, this module needs to be instantiated twice, and there are still two \(f16 = f16 + f16\) operations missing. Two single-precision floating-point adders dedicated to computing \(f16 = f16 + f16\) are instantiated separately, totaling four modules, to implement all vector addition calculation formats.&lt;/p&gt;
    &lt;head rend="h5"&gt;Fast format conversion algorithm&lt;/head&gt;
    &lt;p&gt;Taking the conversion from \(f16\) to \(f32\) as an example, a fast format conversion algorithm is introduced.&lt;/p&gt;
    &lt;p&gt;When \(f16\) is a normalized number, converting it to \(f32\) will also result in a normalized number. For \(f16\) exponents, they are biased to match \(f32\) exponents. Since \(f32\) has a larger exponent range, there is no concern about exponent overflow after conversion. Additionally, the \(f16\) significand is \(10\) bits, while the \(f32\) significand is \(23\) bits. Simply appending \(13\) zeros to the \(f16\) significand yields the \(f32\) significand. This is a conversion from lower to higher precision, ensuring the result is exact.&lt;/p&gt;
    &lt;p&gt;For a normalized \(f16\) exponent (5-bit width), the actual exponent \(Ereal = Ef16 – 15\). For a normalized \(f32\) exponent (8-bit width), \(Ereal = Ef32 – 127\). Thus, converting \(Ef16\) to \(Ef32\) via \(Ereal\): \(Ef16 – 15 = Ef32 – 127\), \(Ef32 = Ef16 – 15 + 127\), \(Ef32 = Ef16 + 112\). The 8-bit binary representation of \(112\) is \(01110000\). Computing \(Ef16 + 112\) requires an adder for a variable plus a constant, but this adder can be avoided by identifying the following pattern:&lt;/p&gt;
    &lt;p&gt;When the highest bit of \(Ef16\) is \(0\), \(Ef16 + 112 = (0111, Ef16(3, 0))\)&lt;/p&gt;
    &lt;p&gt;When the most significant bit of \(Ef16\) is \(1\), \(Ef16 + 112 = (1000, Ef16(3, 0))\).&lt;/p&gt;
    &lt;p&gt;Using this pattern, an \(Mux\) can quickly convert \(Ef16\) to \(Ef32\). Thus, for normalized \(f16\) to \(f32\) conversion, the exponent bits use an \(Mux\), the significand bits are padded with 0, and the sign bit remains unchanged. The challenge arises when \(f16\) is denormal. In this case, all exponent bits of \(f16\) are 0, and the number of leading zeros in the significand determines the exponent after conversion to \(f32\). When all exponent bits of \(f16\) are zero and only the \(lsb\) of the significand is 1, the converted \(f32\) exponent is minimized at \(-15-9=-24\), which still falls within the range of \(f32\) normalized numbers. Therefore, for denormal \(f16\), leading zero detection (\(lzd\)) and left shifting of the significand are required.&lt;/p&gt;
    &lt;p&gt;Chisel's built-in priority encoder can implement the \(lzd\) function. Tests show it synthesizes better than traditional \(lzd\) implementations using binary search. The syntax is: \(PriorityEncoder(Reverse(Cat(in,1.U)))\). For a \(5\)-bit \(in\), the generated Verilog code is as follows:&lt;/p&gt;
    &lt;code&gt;module LZDPriorityEncoder(
  input        clock,
  input        reset,
  input  [4:0] in,
  output [2:0] out
);
  wire [5:0] _out_T = {in,1'h1};
  wire [5:0] _out_T_15 = {_out_T[0],_out_T[1],_out_T[2],_out_T[3],_out_T[4],_out_T[5]};
  wire [2:0] _out_T_22 = _out_T_15[4] ? 3'h4 : 3'h5;
  wire [2:0] _out_T_23 = _out_T_15[3] ? 3'h3 : _out_T_22;
  wire [2:0] _out_T_24 = _out_T_15[2] ? 3'h2 : _out_T_23;
  wire [2:0] _out_T_25 = _out_T_15[1] ? 3'h1 : _out_T_24;
  assign out = _out_T_15[0] ? 3'h0 : _out_T_25;
endmodule
&lt;/code&gt;
    &lt;p&gt;Although this code appears to use many cascaded \(Mux\)es, the synthesizer produces good timing results for such code. Inspired by this, this paper designs a novel priority-based left-shift algorithm to accelerate \(lzd+\) left-shift, with the \(Chisel\) code as follows:&lt;/p&gt;
    &lt;code&gt;def shiftLeftPriorityWithF32EXPResult(srcValue: UInt, priorityShiftValue: UInt): UInt = {
  val width = srcValue.getWidth
  val lzdWidth = srcValue.getWidth.U.getWidth
  def do_shiftLeftPriority(srcValue: UInt, priorityShiftValue: UInt, i:Int): UInt = {
    if (i==0) Cat(
      Mux(
        priorityShiftValue(i),
        Cat(srcValue(0),0.U((width-1).W)),
        0.U(width.W)
      ),
      Mux(
        priorityShiftValue(i),
        "b01110000".U-(width-i-1).U(8.W),
        "b01110000".U-(width-i).U(8.W)
      )
    )
    else Mux(
      priorityShiftValue(i),
      if (i==width-1) Cat(srcValue(i,0),"b01110000".U-(width-i-1).U(8.W)) 
      else Cat(Cat(srcValue(i,0),0.U((width-1-i).W)), "b01110000".U-(width-i-1).U(8.W)),
      do_shiftLeftPriority(srcValue = srcValue, priorityShiftValue = priorityShiftValue, i = i - 1)
      )
    }
    do_shiftLeftPriority(srcValue = srcValue, priorityShiftValue = priorityShiftValue, i = width-1)
  }
&lt;/code&gt;
    &lt;p&gt;Both \(srcValue\) and \(priorityShiftValue\) pass the mantissa of \(f16\), starting from the most significant bit (MSB) of the mantissa. If the MSB is \(1\), the original value of \(srcValue\) is returned along with the corresponding exponent (the exponent is selected from multiple constants and depends on the position of the first \(1\) in the mantissa). If the MSB is \(0\), the next bit is checked for \(1\). If it is \(1\), \(srcValue\) is left-shifted by one bit and returned (no actual left shift is needed here since the high bits after shifting are not retained; truncation and zero-padding suffice), along with the corresponding exponent. This process continues iteratively. Thus, a priority left shifter simultaneously performs the \(lzd\) and left shift operations while also generating the corresponding \(Ef32\), eliminating the need to calculate the \(Ef32\) exponent based on \(lzd\). This enables a fast algorithm for converting \(f16\) denormal numbers to \(f32\). A similar algorithm is used for converting \(f32\) to \(f64\), which is not elaborated here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Fused Multiply-Add Algorithm&lt;/head&gt;
    &lt;p&gt;Floating-point fused multiply-add computation \(fpa × fp\_b + fp\_c\), where the intermediate multiplication \(fpa × fp\_b\) is performed as if without range and precision limitations, without rounding, and only rounded once to the target format at the end. FMA is typically implemented using a pipeline, with steps including multiplication, addition, normalization shift, and rounding. This chapter introduces the vector floating-point fused multiply-add algorithm, whose functionalities include:&lt;/p&gt;
    &lt;p&gt;(1) 1 \(fp64 = fp64 × fp64 + fp64\);&lt;/p&gt;
    &lt;p&gt;(2) \(2\) \(fp32 = fp32 × fp32 + fp32\);&lt;/p&gt;
    &lt;p&gt;(3) Four \(fp16 = fp16 × fp16 + fp16\);&lt;/p&gt;
    &lt;p&gt;(4) \(2\) \(fp32 = fp16 × fp16 + fp32\);&lt;/p&gt;
    &lt;p&gt;(5) \(1\) \(fp64 = fp32 × fp32 + fp64\).&lt;/p&gt;
    &lt;p&gt;(\(1\)) (\(2\)) (\(3\)) The source and destination operands are in the same floating-point format, while in (\(4\)) (\(5\)), the two multipliers have the same width, and the other addend and the result share the same width, which is twice that of the multipliers.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar single-precision format algorithm&lt;/head&gt;
    &lt;p&gt;The computation flow first calculates the unrounded result of multiplying two floating-point numbers, then adds this unrounded product to a third number. The algorithm flowchart is illustrated, expressed by the formula \(fp\_result = fp\_a × fp\_b + fp\_c\), where \(Sa\), \(Sb\), and \(Sc\) are the significands of \(fp\_a\), \(fp\_b\), and \(fp\_c\) respectively, and \(Ea\), \(Eb\), and \(Ec\) are their exponents:&lt;/p&gt;
    &lt;p&gt;For ease of description below, some parameters are defined, with their meanings and values listed in the table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Parameters&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f16\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f32\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f64\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(significandWidth\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(24\)&lt;/cell&gt;
        &lt;cell&gt;\(53\)&lt;/cell&gt;
        &lt;cell&gt;Significant Digit Width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(exponentWidth\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;Exponent width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(rshiftBasic\)&lt;/cell&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
        &lt;cell&gt;\(27\)&lt;/cell&gt;
        &lt;cell&gt;\(56\)&lt;/cell&gt;
        &lt;cell&gt;Number of right shifts required to align \(fp\_c\)'s significand with the product significand&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(rshiftMax\)&lt;/cell&gt;
        &lt;cell&gt;\(37\)&lt;/cell&gt;
        &lt;cell&gt;\(76\)&lt;/cell&gt;
        &lt;cell&gt;\(163\)&lt;/cell&gt;
        &lt;cell&gt;\(fp\_c\) maximum right shift count for significant digits (beyond this value, \(g\) and \(r\) are both \(0\))&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Unsigned Integer Multiplication&lt;/head&gt;
    &lt;p&gt;The rule for multiplying two floating-point numbers is to multiply the sign bits, add the exponent bits (not simply added, as bias must be considered), and multiply the significands (including the implicit bit and mantissa bits). The significand multiplication is essentially fixed-point multiplication, which follows the same principle as unsigned integer multiplication.&lt;/p&gt;
    &lt;p&gt;Binary vertical multiplication is the original multiplication algorithm, where an \(n\)-bit \(C=A×B\) vertical method is illustrated. This process generates \(n\) partial products, which are then added with staggered alignment.&lt;/p&gt;
    &lt;p&gt;The multiplication algorithm using the vertical method has significant latency. Optimization efforts for multiplication operations primarily focus on two aspects: reducing the number of partial products (e.g., \(Booth\) encoding) and minimizing the latency introduced by adders (e.g., \(CSA\) compression).&lt;/p&gt;
    &lt;p&gt;When computing the multiplication of two floating-point numbers, their significands are multiplied. Since significands are unsigned, unsigned integer multiplication suffices for this computation. There are many algorithms for implementing unsigned integer multiplication, and three of them are compared below.&lt;/p&gt;
    &lt;p&gt;Method 1: Directly use the multiplication symbol \(×\), letting the synthesis tool decide.&lt;/p&gt;
    &lt;p&gt;Method two: Use a vertical multiplication method similar to manual decimal multiplication. Multiplying two \(n\)-bit numbers generates \(n\) partial products, which are then compressed using \(CSA\) (to be introduced later) into two numbers for addition.&lt;/p&gt;
    &lt;p&gt;Method 3: Use \(Booth\) encoding to generate \((n+1)/2\) rounded-up partial products, then compress them into two numbers for addition using \(CSA\).&lt;/p&gt;
    &lt;p&gt;The data in the table are the results of multiplying two 53-bit unsigned integers (for f64) using the TSMC 7nm process library. The target frequency is 3GHz, with a theoretical cycle time of 333.33ps. However, considering clock uncertainty and process corner variations, a design margin is reserved for the backend, leaving approximately 280ps per cycle. Therefore, it is evident that multiplication cannot be completed within one cycle. In practice, additional time is required to determine the implicit bit, making it even more impossible to achieve 53-bit multiplication in a single cycle. Although Method 1 has a smaller area and shorter latency, it cannot be pipelined, leaving only Methods 2 or 3 as viable options. Method 3 offers shorter latency and a smaller area compared to Method 2, making it the chosen implementation for unsigned integer multiplication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Algorithm&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Pipelining feasibility&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Method one&lt;/cell&gt;
        &lt;cell&gt;\(285.15\)&lt;/cell&gt;
        &lt;cell&gt;\(1458.95\)&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Method two&lt;/cell&gt;
        &lt;cell&gt;\(320.41\)&lt;/cell&gt;
        &lt;cell&gt;\(2426.34\)&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Method three&lt;/cell&gt;
        &lt;cell&gt;\(302.19\)&lt;/cell&gt;
        &lt;cell&gt;\(2042.46\)&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;\(Booth\) encoding&lt;/head&gt;
    &lt;p&gt;The purpose of Booth encoding is to reduce the number of partial products in a multiplier. Taking the binary unsigned integer multiplication C=A*B as an example, the Booth encoding algorithm is derived.&lt;/p&gt;
    &lt;p&gt;The following expression is a general form of unsigned binary integers. To facilitate subsequent transformations, a \(0\) is added at both the beginning and the end, leaving its value unchanged.&lt;/p&gt;
    &lt;p&gt;After equivalent transformation, adjacent two bits of \(1\) cancel out to \(0\). For consecutive \(1\)s, the least significant \(1\) becomes \(-1\), and the bit above the most significant \(1\) changes from \(0\) to \(1\), with all \(1\)s turning to \(0\). This transformation is known as Booth transformation. It simplifies sequences of three or more consecutive \(1\)s, with greater simplification for longer sequences. However, this transformation does not optimize hardware circuits because it does not guarantee any partial product will always be \(0\). Therefore, modified Booth encoding is typically used in circuit design to effectively reduce the number of partial products.&lt;/p&gt;
    &lt;p&gt;Perform an equivalent transformation again, but this time with additional constraints on \(n\). Assuming \(n\) is odd, a zero is still appended at the end, increasing the length to an even number. Then, a zero is prepended at the highest bit, making the total length \(n+2\). This is done to facilitate subsequent derivations.&lt;/p&gt;
    &lt;p&gt;After equivalent transformation, it can be observed that the number of terms in the polynomial expression becomes \((n+1)/2\) (when \(n\) is odd). If \(n\) is even, a zero needs to be appended at the end, and two zeros are prepended before the most significant bit, making the number of terms \(n/2+1\) (when \(n\) is even). Combining both odd and even cases, the number of terms in the polynomial expression is the ceiling of \((n+1)/2\). Starting from the LSB of the original binary number, groups of three bits are formed (the first group's least significant bit requires an additional appended bit \(0\), and the most significant bit is padded with one \(0\) if \(n\) is odd or two \(0\)s if \(n\) is even, ensuring the padded length is odd). Adjacent groups overlap by one bit (the highest bit of the lower group overlaps with the lowest bit of the higher group), forming new polynomial factors. This is the improved Booth encoding method.&lt;/p&gt;
    &lt;p&gt;When multiplying two binary numbers, modified Booth encoding of the multiplier can halve the number of partial products. Let the multiplicand be \(A\) and the multiplier be \(B\), with \(B_{2i+1}\), \(B_{2i}\), and \(B_{2i-1}\) representing three consecutive bits of \(X\), where \(i\) is a natural number \(N\). \(PP_i\) denotes the partial product for each \(i\). After applying modified Booth transformation to \(B\) and multiplying by \(A\), the Booth encoding and \(PP\) truth table are as shown.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(B_{2i+1}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B_{2i}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B_{2i-1}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(PP_i\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-2A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;By evaluating each consecutive three-bit segment of the multiplier, the corresponding partial product is derived, halving the number of partial products. This approach treats the multiplier as a quaternary number, hence termed radix-4 Booth encoding. Multiplication using radix-4 Booth encoding offers significant optimization over traditional methods, is straightforward to implement, and meets most application requirements.&lt;/p&gt;
    &lt;p&gt;In \(Booth\) encoding, five types of partial products need to be calculated: \(0\), \(A\), \(2A\), \(-A\), \(-2A\). \(0\) and \(A\) require no computation, \(2A\) is obtained by a one-bit left shift, while \(-A\) and \(-2A\) require the operation of inversion plus one. This paper introduces a fast algorithm for handling inversion plus one.&lt;/p&gt;
    &lt;p&gt;To simplify the explanation of the principle, we assume computing \(f16\) with 11 significant bits, generating 6 partial products. Each partial product is 22 bits wide, as shown in the figure. The colored positions in the figure are 12 bits wide, representing \(A\) possibly multiplied by \(0\), \(1\), or \(2\). Since the last partial product's three-bit encoding is \(0\)xx, its value cannot be negative. Assuming all other partial products are negative, we invert and add one to each of them. The colored parts represent the results after inversion only. We place the added one for the current partial product into the corresponding position of the next partial product, ensuring the sum of partial products remains unchanged and avoiding the issue of a carry chain from adding one to the current partial product. The last partial product is non-negative and does not require this adjustment.&lt;/p&gt;
    &lt;p&gt;The \(1\) in the above figure can first be simplified through summation to obtain the result shown in the following figure.&lt;/p&gt;
    &lt;p&gt;If the actual partial product value is positive, the above result needs to be corrected by adding one to the bit position immediately to the left of the colored bit and setting the next partial product's tail addition to zero. As shown in the figure, \(Si\) (where \(i\) starts from \(0\)) represents the sign bit of the \(i\)-th partial product, transforming it into a general form where the colored position only computes \(0\), \(A\), \(2A\), \(\sim A\), or \(\sim 2A\), speeding up partial product generation.&lt;/p&gt;
    &lt;p&gt;One additional point to note is that the sum of partial products yields the multiplication result, but the summation of partial products may also generate carries. These carries are meaningless for multiplication, but they can cause erroneous carries when the product is added to a wider number. The correction method involves adding an extra bit to the most significant bit of the partial product, as illustrated.&lt;/p&gt;
    &lt;p&gt;This ensures that the carry is correct after summing all partial products. This concludes the introduction to Booth encoding. Note that the example uses an 11-bit multiplication. While \(f16\) and \(f64\) have an odd number of significant digits, \(f32\) has an even number, requiring slight differences in zero-padding the most significant bit. Other steps are similar and thus omitted.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(CSA\) Compression&lt;/head&gt;
    &lt;p&gt;\(Carry\)-\(Save\)-\(Adder\) is a carry-save adder that compresses \(n\) addends into \(m\) addends ($m&lt;/p&gt;
    &lt;p&gt;Assuming the calculation of adding two binary numbers \(A+B\), the truth table for their sum and carry, where \(A[i]+B[i]\) is the decimal result and also the count of \(1\)s in \(A[i]\) and \(B[i]\):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;\(A[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(A[i] + B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Simplified into the following logical expression:&lt;/p&gt;
    &lt;p&gt;\(Sum = A\) ^ \(B\)&lt;/p&gt;
    &lt;p&gt;\(Car = A\) &amp;amp; \(B\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;For three-number addition, the sum is the XOR of two numbers, and the carry occurs when both numbers are \(1\). \((Car &amp;lt;&amp;lt; 1)\) reflects that the current bit's carry propagates to the next bit. This derivation is for clarity; in practice, generating sum and carry from two addends does not accelerate addition.&lt;/p&gt;
    &lt;p&gt;Suppose we want to calculate the sum of three numbers \(A+B+C\), where the \(CSA\) key is to generate the sum and carry, as shown in the truth table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;\(A[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(A[i] + B[i] + C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;From the above table, some patterns can be observed. The generation of \(Sum[i]\) and \(Car[i]\) actually depends only on the sum of \(A[i]+B[i]+C[i]\), i.e., the number of \(1\)s in \(A[i]\), \(B[i]\), and \(C[i]\). The simplified expression is as follows:&lt;/p&gt;
    &lt;p&gt;\(Sum = A\) ^ \(B\) ^ \(C\)&lt;/p&gt;
    &lt;p&gt;\(Car = (A\) &amp;amp; \(B) \quad | \quad (A\) &amp;amp; \(C) \quad | \quad (B\) &amp;amp; \(C)\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B+C = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;For three-number addition, the sum is the XOR of the three numbers, and the carry occurs when at least two numbers are \(1\). \((Car &amp;lt;&amp;lt; 1)\) accounts for the current bit's carry propagating to the next bit. This method converts three-number addition into two-number addition with just two XOR gate delays, significantly saving time, especially for longer bit widths.&lt;/p&gt;
    &lt;p&gt;Adding four numbers is slightly more complex because when all four are \(1\), the sum is \(4\), requiring a carry of \(2\). We designate one carry as \(Cout\) and the other as \(Car\). The \(Cout\) generated from the current four-bit addition is passed to the next stage as \(Cin\). With \(Cin\) and the four numbers, the operation now involves five inputs: \(A[i]\), \(B[i]\), \(C[i]\), \(D[i]\), and \(Cin[i]\), producing three outputs: \(Sum[i]\), \(Cout[i]\), and \(Car[i]\). The least significant bit's \(Cin[0]\) is \(0\), while other bits' \(Cin[i]\) is the \(Cout[i-1]\) from the previous bit, as shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]+D[i]+Cin[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Cout[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1/0\)&lt;/cell&gt;
        &lt;cell&gt;\(0/1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1/0\)&lt;/cell&gt;
        &lt;cell&gt;\(0/1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are many ways to simplify this truth table. One feasible method is described below. The value of \(Sum[i]\) can be easily derived as the XOR of the five inputs: \(Sum[i] = A[i]\)^\(B[i]\)^\(C[i]\)^\(D[i]\)^\(Cin[i]\). \(Car[i]\) and \(Cout[i]\) are more complex. We define \(Cout[i]\) to be generated only by the first three numbers, i.e., when the sum of the first three numbers is greater than \(1\), \(Cout[i] = 1\). The table shows the truth table for \(Cout[i]\):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Cout[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;\(Cout[i]\) can be expressed as: \(Cout[i] = (A[i]\)^\(B[i])?C[i]:A[i]\), while \(Car[i]\) is generated by \(D[i]\) and \(Cin[i]\), with the table showing the truth table for \(Car[i]\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]+D[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(D[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(Cin[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(D[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(Cin[i]\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;\(Car[i]\) can be expressed as: \(Car[i] = (A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) ? Cin[i] : D[i]\). Specifically, when \((A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) = 1\), \(A[i]+B[i]+C[i]+D[i] = 1/3\), and \(Cin[i] = 1\) will generate a carry. When \((A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) = 0\), \(A[i]+B[i]+C[i]+D[i] = 0/4\). Here, \(D[i] = 0\) indicates \(A[i]+B[i]+C[i]+D[i] = 0\), and adding \(Cin\) will not produce a carry, while \(D[i] = 1\) indicates \(A[i]+B[i]+C[i]+D[i] = 4\), and adding \(Cin\) will generate a carry. Based on the above derivation, the expression for \(CSA4\_2\) is as follows:&lt;/p&gt;
    &lt;p&gt;Sum[i] = A[i] ^ B[i] ^ C[i] ^ D[i] ^ Cin[i], Cin[i] = Cout[i-1], Cin[0] = 0&lt;/p&gt;
    &lt;p&gt;\(Cout[i] = (A[i]\) ^ \(B[i])?C[i]:A[i]\)&lt;/p&gt;
    &lt;p&gt;\(Car[i] = (A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i])?Cin[i]:D[i]\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B+C+D = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;Using the \(TSMC7nm\) process library, a comprehensive comparison of delay and area was conducted for different input XOR gates, \(CSA3\_2\), and \(CSA4\_2\). The synthesis results for different input XOR gates are shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(106\) bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)&lt;/cell&gt;
        &lt;cell&gt;\(13.74\)&lt;/cell&gt;
        &lt;cell&gt;\(38.66880\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)&lt;/cell&gt;
        &lt;cell&gt;\(23.01\)&lt;/cell&gt;
        &lt;cell&gt;\(63.09120\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)^\(D\)&lt;/cell&gt;
        &lt;cell&gt;\(24.69\)&lt;/cell&gt;
        &lt;cell&gt;\(87.51360\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)^\(D\)^\(E\)&lt;/cell&gt;
        &lt;cell&gt;\(37.21\)&lt;/cell&gt;
        &lt;cell&gt;\(99.72480\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The synthesis results of \(CSA3\_2\) and \(CSA4\_2\) are shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(106\) bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(CSA3\_2\)&lt;/cell&gt;
        &lt;cell&gt;\(23.23\)&lt;/cell&gt;
        &lt;cell&gt;\(104.42880\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(CSA4\_2\)&lt;/cell&gt;
        &lt;cell&gt;\(40.63\)&lt;/cell&gt;
        &lt;cell&gt;\(237.86881\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It can be seen that although \(CSA4\_2\) theoretically has a delay of three XOR gates and \(CSA3\_2\) theoretically has a delay of two XOR gates, in actual physical implementation, \(CSA4\_2\) is only slightly faster than two levels of \(CSA3\_2\). Therefore, \(CSA3\_2\) should be used whenever possible, unless one level of \(CSA4\_2\) can replace two levels of \(CSA3\_2\), such as in \(4-&amp;gt;2\) compression or \(8-&amp;gt;2\) compression.&lt;/p&gt;
    &lt;head rend="h5"&gt;CSAn_2&lt;/head&gt;
    &lt;p&gt;For two unsigned integer multiplications using Booth encoding, the number of partial products is ceil((n+1)/2). To ensure correct carry propagation, the partial product bit width is extended by one bit. The number and bit width of partial products for each data format are listed in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of significant digits&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of partial products&lt;/cell&gt;
        &lt;cell role="head"&gt;Partial product bit width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp16\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp32\)&lt;/cell&gt;
        &lt;cell&gt;\(24\)&lt;/cell&gt;
        &lt;cell&gt;\(13\)&lt;/cell&gt;
        &lt;cell&gt;\(25\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fp64\)&lt;/cell&gt;
        &lt;cell&gt;\(53\)&lt;/cell&gt;
        &lt;cell&gt;\(27\)&lt;/cell&gt;
        &lt;cell&gt;54&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Following the principle of prioritizing \(CSA3\_2\) unless one level of \(CSA4\_2\) can replace two levels of \(CSA3\_2\), the number of \(CSA3\_2\) and \(CSA4\_2\) stages used for each data format is listed in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of \(CSA3\_2\) Stages&lt;/cell&gt;
        &lt;cell role="head"&gt;\(CSA4\_2\) Stages&lt;/cell&gt;
        &lt;cell role="head"&gt;Process (\(-&amp;gt;\) denotes \(CSA3\_2\), \(--&amp;gt;\) denotes \(CSA4\_2\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp16\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(6-&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp32\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(13-&amp;gt;9-&amp;gt;6-&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fp64\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(27-&amp;gt;18-&amp;gt;12-&amp;gt;8--&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Exponent processing and right shift&lt;/head&gt;
    &lt;p&gt;Following conventional methods, if the exponent relationship between the product of \(fp\_a\) and \(fp\_b\) and the exponent of \(fp\_c\) is unknown, the smaller exponent must be right-shifted, similar to floating-point addition. This would require both the significand of the \(fp\_a\) and \(fp\_b\) product and the significand of \(fp\_c\) to potentially shift right, necessitating two shifters and increasing area. Additionally, waiting for the \(fp\_a\) and \(fp\_b\) product to be computed before right-shifting its significand increases circuit latency. An alternative algorithm avoids using two shifters and reduces latency by parallelizing the computation with the \(fp\_a\) and \(fp\_b\) product.&lt;/p&gt;
    &lt;p&gt;The exponent bits are treated as unsigned numbers, but there is an exponent bias between them and the actual exponent. Additionally, the \(denormal\) case must be considered. Let \(E\_fix\) denote the exponent bits after handling the \(denormal\) case, and \(E\_bit\) denote the original exponent bits. When all bits of \(E\_bit\) are 0, \(E\_fix = 1\); otherwise, \(E\_fix = E\_bit\).&lt;/p&gt;
    &lt;p&gt;In the above equation, the true exponent \(E\_real\) equals \(E\_fix\) minus a bias value \(bias\), where \(exponentWidth\) is the width of \(E\_bit\), and \(bias\) equals the value where the highest bit of \(E\_bit\) is \(0\) and all other bits are \(1\). Without considering the carry or borrow of the significand product, the true exponent result \(Eab\_real\) of multiplying \(fp\_a\) and \(fp\_b\) is given by:&lt;/p&gt;
    &lt;p&gt;The calculation formula for the binary exponent result \(Eab\_bit\) of the multiplication of \(fp\_a\) and \(fp\_b\) is shown below:&lt;/p&gt;
    &lt;p&gt;The operation of \(+\)&amp;amp; extends the result of \(Ea\_fix + Eb\_fix\) by one bit to retain the carry. The carry is preserved because a bias value will be subtracted later, and without retaining the carry, the result would be incorrect. Additionally, subtracting the bias might result in a negative value, so another bit is extended by appending a 0 at the highest bit. Finally, the bias \(bias\) is subtracted, yielding the binary exponent result \(Eab\_bit\) for the multiplication of \(fp\_a\) and \(fp\_b\) without considering the carry or borrow from the significand product. Then, we construct an exponent \(Eab\) with the following value:&lt;/p&gt;
    &lt;p&gt;Assuming the binary exponent result of multiplying \(fp\_a\) and \(fp\_b\) is \(Eab\), to ensure lossless precision when adding the significant digits of \(fp\_a \times fp\_b\) and \(fp\_c\), both addends are extended in width. The significant digits of \(fp\_c\) are extended to \(3 \times significandWidth + 4\), with the bit distribution shown in the figure. Here, \(g0\), \(r0\), \(g1\), and \(r1\) are used to preserve the \(guard\) and \(round\) bits during right-shifting:&lt;/p&gt;
    &lt;p&gt;As shown above, the significand of \(fp\_c\) is \(significandWidth+2\) bits wider than the product of the significands of \(fp\_a\) and \(fp\_b\). Since the product result has two digits before the decimal point, aligning it as \(1\).xxx requires \(significandWidth+3\) bits, which explains why \(rshiftBasic = significandWidth+3\).&lt;/p&gt;
    &lt;p&gt;Let \(fp\_c\_significand\_cat0 = Cat(fp\_c\_significand, 0.U(2 \times significandWidth + 4))\), where \(fp\_c\_significand\) is the significand of \(fp\_c\). If \(Ec\_fix = Eab = Eab\_bit + rshiftBasic.S\), \(fp\_c\_significand\_cat0\) is exactly \(significandWidth + 3\) larger than \(Eab\_bit\), so no right shift is needed for alignment. If \(Ec\_fix &amp;gt; Eab\), theoretically \(fp\_c\_significand\_cat0\) would require a left shift, but due to the presence of \(g0\) and \(g1\) as buffers and the fact that lower bits cannot generate carry (only affecting rounding), no actual left shift is needed. If \(Ec\_fix &amp;lt; Eab\), \(fp\_c\_significand\_cat0\) must be right-shifted by \(rshift\_value = Eab - Cat(0.U, Ec\_fix).asSInt\). Since \(rshift\_value\) is derived from the addition of multiple numbers, its LSB is computed first. Thus, during right-shifting, the LSB of \(rshift\_value\) is first used as the Mux select signal, followed by higher bits. The shifting process must compute \(guard\), \(round\), and \(sticky\) (collectively \(grs\)). For \(guard\) and \(round\), these positions are already preserved during bit-width extension, requiring no additional computation. For \(sticky\), two methods exist: (1) Extend the bit-width further to store shifted-out bits and compute \(sticky\) after all shifts, or (2) Compute \(sticky\) during shifting based on Mux select signals. Method 2 offers lower latency than Method 1. Below is the design code for Method 2:&lt;/p&gt;
    &lt;code&gt;/**
 * 使用Mux进行移位，先用最低位，输出位宽为srcValue + 1(Sticky)
 */
def shiftRightWithMuxSticky(srcValue: UInt, shiftValue: UInt): UInt = {
  val vecLength  = shiftValue.getWidth + 1
  val res_vec    = Wire(Vec(vecLength,UInt(srcValue.getWidth.W)))
  val sticky_vec = Wire(Vec(vecLength,UInt(1.W)))
  res_vec(0)    := srcValue
  sticky_vec(0) := 0.U
  for (i &amp;lt;- 0 until shiftValue.getWidth) {
    res_vec(i+1) := Mux(shiftValue(i), res_vec(i) &amp;gt;&amp;gt; (1&amp;lt;&amp;lt;i), res_vec(i))
    sticky_vec(i+1) := Mux(shiftValue(i), sticky_vec(i) | res_vec(i)((1&amp;lt;&amp;lt;i)-1,0).orR,
    sticky_vec(i))
  }
  Cat(res_vec(vecLength-1),sticky_vec(vecLength-1))
}
&lt;/code&gt;
    &lt;p&gt;There is another method to speed up the right shift. The bit width of \(rshift\_value\) is \(exponentWidth+1\), while the width of \(fp\_c\_significand\_cat0\) is \(3*significandWidth+4\). There may be overflow bits in \(rshift\_value\). For example, using a 5-bit number to right-shift a 7-bit number, \(a(6,0) &amp;gt;&amp;gt; b(4,0)\), the maximum value of the third bit in \(b\) is \(7\), which is sufficient for the bit width of \(a\). Therefore, if the upper two bits of \(b\) contain any non-zero value, the right-shift result of \(a\) will be zero. The right-shift result can be simplified to \(Mux(b(4,3).orR,0.U, a(6,0) &amp;gt;&amp;gt; b(2,0))\). The table below lists the bit widths of \(rshift\_value\) used for three floating-point data formats.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;\(fp\_c\_significand\_cat0\) bit width&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width of \(rshift\_value\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit width used&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(37\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(76\)&lt;/cell&gt;
        &lt;cell&gt;\(9\)&lt;/cell&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(163\)&lt;/cell&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are three cases based on the value of \(rshift\_value\): \(rshift\_value &amp;lt;= 0\) means no right shift is needed, and the \(sticky\) result is \(0\); \(rshift\_value &amp;gt; rshiftMax\) means the right shift result is \(0\), and the \(sticky\) result is \(fp\_c\_significand\_cat0\) or reduced; \(0 &amp;lt; rshift\_value &amp;lt;= rshiftMax\) means the right shift result and \(sticky\) are calculated by \(shiftRightWithMuxSticky\).&lt;/p&gt;
    &lt;p&gt;Thus, this section has covered the methods for exponent processing, the design of the right shifter, and the handling of \(grs\) during the right-shift operation.&lt;/p&gt;
    &lt;head rend="h5"&gt;Significand addition&lt;/head&gt;
    &lt;p&gt;The \(rshift\_result\) of the significand of \(fp\_c\) after right-shifting must be added to the two results compressed by \(CSAn\_2\). Since the signs of \(fp\_c\) and \(fp\_a \times fp\_b\) may differ, subtraction is performed when they are opposite, and the result may be negative. To determine the sign, an additional sign bit is appended. \(fp\_c\_rshiftValue\_inv\) selects either \(rshift\_result\) (with a \(0\) sign bit) or its negation (with a \(1\) sign bit) based on whether the signs differ. Thus, \(fp\_c\_rshiftValue\_inv\) is added to the two results compressed by \(CSAn\_2\). However, during subtraction, \(fp\_c\_rshiftValue\_inv\) only negates \(rshift\_result\), and a \(+1\) is required at the least significant bit when all right-shifted \(grs\) bits are \(0\). This \(+1\) is placed in the \(carry\) bit of the two results compressed by \(CSAn\_2\), as the \(carry\) bit is always \(0\), saving adder usage and area. The three numbers have different bit widths: the right-shifted significand of \(fp\_c\) has a width of \(3 \times significandWidth + 4\), while the two results compressed by \(CSAn\_2\) have a width of \(2 \times significandWidth + 1\) (the \(+1\) accounts for the partial product extension to ensure correct carry). The strategy for summing these three numbers involves first compressing the lower \(2 \times significandWidth + 1\) bits of the \(CSAn\_2\) results and the lower \(2 \times significandWidth\) bits of \(rshift\_result\) (with a \(0\) appended to form \(2 \times significandWidth + 1\) bits) using \(CSA3\_2\) compression. The two compressed results are then summed, denoted as \(adder\_low\_bits\). Simultaneously, the higher \(significandWidth + 4\) bits of \(rshift\_result\) are incremented by \(1\). The final result selects either the higher \(significandWidth + 4\) bits of \(fp\_c\_rshiftValue\_inv\) or its incremented version based on whether the highest bit of the lower \(2 \times significandWidth + 1\) sum is \(1\), denoted as \(adder\_high\_bits\).&lt;/p&gt;
    &lt;p&gt;Additionally, consider the inversion and increment by one of the right-shifted \(grs\) during subtraction. The final significand addition result \(adder\) (including the right-shifted \(grs\)) consists of: \(adder\_high\_bits\), \(adder\_low\_bits\), and the right-shifted \(grs\) (inverted and incremented by one for subtraction). Since \(adder\) may be negative, an extra \(1\)-bit is extended solely for sign determination of \(adder\), which is later discarded. \(adder\_inv\) inverts \(adder\) when it is negative and removes this sign bit.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(LZD\), left shift, rounded and unrounded mantissa results&lt;/head&gt;
    &lt;p&gt;After computing \(adder\_inv\), a leading-zero detection must be performed on \(adder\_inv\) to determine the number of left shifts required, thereby normalizing and rounding the mantissa result.&lt;/p&gt;
    &lt;p&gt;When performing LZD on \(adder\_inv\), there is an issue of exponent limitation. Let \(E\_greater\) be \(Eab\) (the exponent result from multiplying \(fp\_a\) and \(fp\_b\)). The left shift amount cannot exceed \(E\_greater\) because the exponent result would already be all zeros at that point. To address this, similar to the floating-point adder, a \(mask\) is used during left shift to limit the shift amount.&lt;/p&gt;
    &lt;p&gt;For cases where \(adder\) is negative, \(-adder\) should be the inversion of \(adder\) plus \(1\). Since adding \(1\) would create a long carry chain, only the inversion is performed, and then the \(LZD\) of \(adder\_inv\) is calculated. This may result in a one-bit deviation. When the inversion of \(adder\) ends with consecutive \(1\)s, adding \(1\) would cause a carry at the highest bit. To resolve this one-bit deviation, a trailing zero detection (\(TZD\)) is performed on \(adder\). If \(LZD + TZD\) equals the width of \(adder\), the inversion of \(adder\) ends with consecutive \(1\)s, requiring a correction to the left-shift result. After the left-shift correction, the unrounded result is obtained, and adding \(1\) to it yields the rounded result.&lt;/p&gt;
    &lt;head rend="h5"&gt;Final result&lt;/head&gt;
    &lt;p&gt;The sign bit result is determined based on the sign of \(adder\), while the calculation of \(grs\) requires combining both the right-shift process in step five and the left-shift process in step seven. The rounding strategy employs \(after \quad rounding\). To detect \(underflow\), an additional set of \(grs\) specifically for \(underflow\) checking is used. Based on the rounding mode and \(grs\), the necessity of rounding is determined, selecting the final mantissa result. The exponent result is derived according to the rounding outcome.&lt;/p&gt;
    &lt;p&gt;When input operands contain special values such as \(NaN\), infinity, or zero, the result is calculated separately. Depending on the actual input values, either the special result or the normal result is selected. Except for the divide-by-zero flag, all four other flag results can be generated.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector single-precision format algorithm&lt;/head&gt;
    &lt;p&gt;The main design principle for vector operations is to share hardware where timing requirements are met.&lt;/p&gt;
    &lt;p&gt;During Booth encoding, \(f16\) generates 6 partial products (pp), \(f32\) generates 13 pp, and \(f64\) generates 27 pp. Thus, the 27 pp positions generated by \(f64\) during Booth encoding can accommodate two sets of 13 pp from \(f32\), and similarly, the 13 pp positions from \(f32\) can hold two sets of 6 pp from \(f16\). This allows continued sharing of a single \(CSA\_27to2\) compression unit. The vector shared Booth encoding is illustrated in the figure.&lt;/p&gt;
    &lt;p&gt;During the right shift of the \(fp\_c\) mantissa, one of the right shifts for the mantissas in \(f64\) and \(f32\) can share a single shifter, while the other shifters remain independent.&lt;/p&gt;
    &lt;p&gt;The \(CSA\_3to2\) is also shared, with the third operand derived from the right-shifted result of the \(fp\_c\) mantissa. The right-shifted results of two \(f32\) or four \(f16\) mantissas are concatenated and then compressed with the two operands from the shared \(Booth\) encoding for \(3\_2\) compression.&lt;/p&gt;
    &lt;p&gt;The adder after compression is also shared. Different formats are assigned different bits, and the bits are separated to prevent low-bit carries from affecting high-bit results.&lt;/p&gt;
    &lt;p&gt;The shared logic for \(LZD\), \(TZD\), and the left shifter is similar to the right shifter, with \(f64\) and \(f32\) sharing one unit while others remain independent.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector Mixed-Precision Format Algorithm&lt;/head&gt;
    &lt;p&gt;There are two types of vector mixed-precision format calculations:&lt;/p&gt;
    &lt;p&gt;(1) \(2\) instances of \(fp32 = fp16 × fp16 + fp32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(fp64 = fp32 × fp32 + fp64\).&lt;/p&gt;
    &lt;p&gt;For two multipliers of the same width, the essence is still adding exponents and multiplying significant bits. Unlike floating-point addition, there's no need to first convert their formats to match the result's format. Simply extending the bit width suffices—padding the exponent's high bits with zeros and the mantissa's low bits with zeros to align with high-precision floating-point operands. After alignment, computation proceeds according to the single-precision format.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;Division is one of the most representative floating-point functions in modern processors. There are two main algorithms for computing division in hardware: digit iteration algorithms based on subtraction with linear convergence, and multiplicative algorithms based on multiplication with quadratic convergence. The subtraction-based digit iteration algorithms are more energy-efficient and require less area. Subsequent references to digit iteration in this paper refer to subtraction-based digit iteration. For common floating-point precisions—double, single, and half—digit iteration methods are significantly faster. In digit iteration division, the most critical aspect is the selection of quotient bits, where each iteration yields one bit of the quotient. To implement a simple \(Radix-4\) selection function independent of the divisor, the divisor must be adjusted to a value sufficiently close to 1. This scaling is performed before digit iteration.&lt;/p&gt;
    &lt;p&gt;Digital iterative algorithms are widely used in high-performance processors due to their excellent trade-offs in performance, area, and power consumption. This paper is based on the \(SRT\) division (\(Sweeney-Robertson-Tocher Division\)), employing a \(Radix-64\) floating-point division algorithm that computes \(6\) quotient bits per cycle. To reduce overhead, each \(Radix-64\) iteration consists of three \(Radix-4\) iterations. Speculative algorithms are used between consecutive \(Radix-4\) iterations to reduce latency.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;The \(Radix-64\) scalar floating-point division algorithm implemented in this paper has low latency for double-precision, single-precision, and half-precision floating-point division when both input operands and results are normalized numbers, with latencies of \(11\), \(6\), and \(4\) cycles, respectively, including scaling and rounding cycles. In cases where input operands or results include denormalized numbers, one or two additional normalization cycles are required.&lt;/p&gt;
    &lt;p&gt;The exponent result can be easily derived, with the focus being on the division of significands. The significand divider performs floating-point division of the dividend significand \(x\) by the divisor significand \(d\) to obtain the significand quotient \(q = x/d\). Both operands need to be normalized numbers, \(x, d ∈ [1, 2)\). Denormalized operands are also permitted, with normalization applied before the digital iteration. If both operands are normalized within \([1, 2)\), the result lies within \([0.5, 2)\). Thus, two bits to the right of the least significant bit (\(LSB\)) of the quotient are required for rounding, namely the guard bit and the rounding bit.&lt;/p&gt;
    &lt;p&gt;When the result is normalized, the guard bit is used for rounding, with \(q ∈ [1, 2)\). When the result is unnormalized, the rounding bit is used for rounding, with \(q ∈ [0.5, 1)\). In the latter case, the result is left-shifted by \(1\) bit, and the guard and rounding bits become the \(LSB\) and guard bit of the normalized result, respectively. To simplify rounding, the result is forced to \(q ∈ [1, 2)\). Note that \(q &amp;lt; 1\) only occurs when \(x &amp;lt; d\). This condition is detected early, and the dividend is left-shifted by \(1\) bit, making \(q = 2 × x/d\) and \(q ∈ [1, 2)\). Note that the exponent result must be adjusted accordingly.&lt;/p&gt;
    &lt;p&gt;The algorithm used for division is the \(Radix-4\) digit iteration algorithm, with three iterations per cycle. The quotient's signed-digit representation uses the digit set {\(−2, −1, 0, +1, +2\)}, meaning the radix \(r = 4\) and the digit set \(a = 2\). In each iteration, a digit of the quotient is obtained through a selection function. To have a quotient digit selection function independent of the divisor, the divisor must be scaled to be close to \(1\). Naturally, to maintain result correctness, the dividend must be scaled by the same factor as the divisor.&lt;/p&gt;
    &lt;p&gt;Using the radix\(-4\) algorithm, each iteration yields 2 bits of the quotient. Since three radix\(-4\) iterations are performed per clock cycle, 6 quotient bits are obtained per cycle, equivalent to a \(Radix-64\) divider. Additionally, note that the first quotient bit of the integer result can only take values {\(+1, +2\)}, and its computation is much simpler than that of the remaining bits. By computing it in parallel with operand prescaling, one single-precision floating-point iteration is saved. On the other hand, there is an early termination mode for exceptional operands. Early termination is triggered when any operand is \(NaN\), infinity, or zero, or when dividing by a power of 2 with both operands normalized. In the latter case, the result is obtained simply by reducing the exponent of the dividend. The main features of the \(Radix-64\) divider are as follows:&lt;/p&gt;
    &lt;p&gt;(1) Pre-scaling of divisor and dividend.&lt;/p&gt;
    &lt;p&gt;(2) The first quotient digit is executed in parallel with pre-scaling.&lt;/p&gt;
    &lt;p&gt;(3) Compare the scaled dividend and divisor, and left-shift the dividend to obtain a result in the range \([1, 2)\).&lt;/p&gt;
    &lt;p&gt;(4) Three \(Radix-4\) iterations per cycle, processing \(6\) bits each cycle.&lt;/p&gt;
    &lt;p&gt;(5) Supports half-precision, single-precision, and double-precision.&lt;/p&gt;
    &lt;p&gt;(6) Denormal number support requires an additional cycle for normalization before iteration.&lt;/p&gt;
    &lt;p&gt;(7) Early termination for exceptional operands.&lt;/p&gt;
    &lt;head rend="h5"&gt;Digit-Recurrence Division Algorithm&lt;/head&gt;
    &lt;p&gt;Digit-recurrence division is an iterative algorithm where each iteration computes a \(radix-r\) quotient digit \(q_{i+1}\) and a remainder. The remainder \(rem[i]\) is used to obtain the next \(radix-r\) digit. For fast iteration, the remainder is stored in a carry-save adder using a signed-digit redundant representation. This paper selects a \(radix-2\) signed-digit representation for the remainder, consisting of a positive and a negative number. For radix \(r =4\), the following expression represents the partial quotient before the \(i\)-th iteration:&lt;/p&gt;
    &lt;p&gt;After scaling the divisor to around 1, the \(radix-4\) algorithm describes the quotient and remainder as follows:&lt;/p&gt;
    &lt;p&gt;Here, \(\widehat{rem}[i]\) is an estimate of the remainder \(rem[i]\), which consists of only a few bits. For this algorithm, it has been determined that only the 6 most significant bits (MSB) of the remainder are needed, i.e., 3 integer bits and 3 fractional bits. Then, each iteration extracts a quotient bit from the current remainder and computes a new remainder for the next iteration. The formula below calculates the number of iterations \(it\):&lt;/p&gt;
    &lt;p&gt;Here, \(n\) is the number of bits in the result, including those needed for rounding. The division latency, i.e., the number of cycles, is directly related to the number of iterations. It also depends on the number of iterations performed per cycle. Three iterations per cycle have been implemented to achieve \(6\) bits per cycle, equivalent to \(Radix-64\) division. The cycles (\(cycles\)) required for normalized floating-point numbers are determined by the following formula. In addition to the (\(it/3\)) cycles needed for iterations, there are two extra cycles for operand pre-scaling and rounding.&lt;/p&gt;
    &lt;p&gt;Some examples of digital iterative division, including the \(Radix-4\) algorithm, can be found in [\(38\)]. A simple implementation is shown in the figure. Note that only the most significant bit of the remainder is used to select the quotient bit. The remainder is updated using a carry-save adder (\(CSA\)) and stored in a redundant representation. The quotient bit selection then requires the \(t\) most significant bits of the remainder to be summed in a carry-propagate adder (\(CPA\)) to obtain its non-redundant representation. However, this implementation is too slow. To accelerate the iteration loop, speculative algorithms must be employed for both the remainder computation between iterations and the quotient bit selection.&lt;/p&gt;
    &lt;head rend="h5"&gt;Operand pre-scaling&lt;/head&gt;
    &lt;p&gt;During prescaling, the divisor is scaled to a value close to 1, making the selection of quotient digits independent of the divisor. For the \(radix-4\) algorithm, scaling the divisor to the range \([1 − 1/64, 1+1/8]\) is sufficient. As shown in the prescaling factor truth table, only three bits determine the scaling factor. Note that during prescaling, the divisor should be scaled by a factor of \(1-2\). The dividend should also be scaled by the same factor.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(0.1\)xxx&lt;/cell&gt;
        &lt;cell role="head"&gt;Pre-scaling factor&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(000\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+1/2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(001\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+1/2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;010&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(011\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(100\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(101\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(110\)&lt;/cell&gt;
        &lt;cell&gt;\(1+0+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(111\)&lt;/cell&gt;
        &lt;cell&gt;\(1+0+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Integer Quotient Calculation&lt;/head&gt;
    &lt;p&gt;While computing the integer quotient, the following data is provided for the digit iteration steps (each digit iteration performs three \(radix-4\) operations, corresponding to the \(s0\), \(s1\), and \(s2\) stages):&lt;/p&gt;
    &lt;p&gt;(1) Redundant remainder in carry-save representation: \(f\_r\_s\), \(f\_r\_c\).&lt;/p&gt;
    &lt;p&gt;(2) Pre-scaled divisor: \(divisor\).&lt;/p&gt;
    &lt;p&gt;(3) Provides a 6-bit remainder result for the quotient selection in the \(s0\) stage of the first digital iteration.&lt;/p&gt;
    &lt;p&gt;(4) Provides a 7-bit remainder result for the quotient selection in the \(s1\) stage of the first digit iteration.&lt;/p&gt;
    &lt;head rend="h6"&gt;Digital iteration&lt;/head&gt;
    &lt;p&gt;The actual implementation of the floating-point divider requires executing three \(radix-4\) iterations per cycle. Conventional sequential iteration three times is too slow to meet timing requirements, so the logic has been optimized. The figure illustrates the block diagram of the digit-recurrence loop.&lt;/p&gt;
    &lt;p&gt;(1) Process the divisor to obtain five possible quotient selection results, requiring the use of divisor multiples (only negate when the quotient is negative).&lt;/p&gt;
    &lt;p&gt;(2) In the \(s0\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)) to predictively compute the five remainder redundant representations needed for the \(s1\) stage in parallel during \(s0\).&lt;/p&gt;
    &lt;p&gt;(3) In the \(s0\) stage, using the five remainder redundant representations calculated in the second step, predictively compute five 7-bit remainder results for the \(s2\) stage.&lt;/p&gt;
    &lt;p&gt;(4) In the \(s0\) stage, the quotient for the \(s0\) stage is selected based on the 6-bit remainder result in the input signal. The quotient is represented using a 5-bit one-hot code.&lt;/p&gt;
    &lt;p&gt;(5) Based on the quotient from stage \(s0\), select the redundant remainder representation needed for stage \(s1\), and predictively choose one of the five 7-bit remainder results calculated in step three for stage \(s2\).&lt;/p&gt;
    &lt;p&gt;(6) In the \(s1\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)), and the five remainder redundant representations needed for the \(s2\) stage are predictively calculated in parallel.&lt;/p&gt;
    &lt;p&gt;(7) In the \(s1\) stage, predictively perform the quotient selection for the \(s1\) stage based on the \(7\)-bit remainder result from the input signal, the divisor multiples used for the five quotient selection results, and the quotient from the \(s0\) stage.&lt;/p&gt;
    &lt;p&gt;(8) Based on the quotient from stage \(s1\), select the redundant remainder representation required for stage \(s2\).&lt;/p&gt;
    &lt;p&gt;(9) In the \(s2\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)) to predictively compute the five redundant remainder representations needed for the next digit iteration in the \(s0\) stage in parallel.&lt;/p&gt;
    &lt;p&gt;(10) In the s2 stage, predictively compute five possible results for the 6-bit remainder required in the s0 stage and the 7-bit remainder required in the s1 stage of the next digit iteration.&lt;/p&gt;
    &lt;p&gt;(11) In the \(s2\) stage, based on the \(7\)-bit remainder result selected for the \(s2\) stage in the fifth step, the divisor multiples used for the five quotient selection results, and the quotient from the \(s1\) stage, the quotient for the \(s2\) stage is predictively selected.&lt;/p&gt;
    &lt;p&gt;(12) Based on the quotient selection result from the \(s2\) stage, the following are selected for the next digit iteration: the carry-save representation of the redundant remainder, the 6-bit remainder result required for the \(s0\) stage, and the 7-bit remainder result required for the \(s1\) stage.&lt;/p&gt;
    &lt;p&gt;Since the divisor's multiple is only inverted in the first step without \(+1\), there will be a deviation in the remainder calculation. Correction logic is added during the quotient selection process to rectify this. The table below shows the standard quotient selection function, and the subsequent table presents the quotient selection function after logical correction.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(4 × rem[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(q_{i+1}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([13/8,31/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([4/8,12/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([-3/8,3/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([-12/8,-4/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\([-32/8,-13/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(4 × rem[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(carry\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(q_{i+1}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(31/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([13/8,30/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(12/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(12/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([4/8,11/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(3/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(3/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([-3/8,2/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-4/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-4/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([-12/8, -5/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-13/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-13/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\([-32/8,14/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Convert the redundant remainder representation of the iteratively output digits back to a standard remainder. Use \(On\) \(the\) \(Fly\) \(Conversion\) to compute both the quotient and quotient minus one, calculate two sets of \(grs\) and the signal for whether rounding up is needed, determine the selection signal for choosing between the quotient or quotient minus one, and finally select the correct quotient result. Perform rounding using the correct quotient result and its corresponding rounding-up signal.&lt;/p&gt;
    &lt;head rend="h6"&gt;Denormal numbers and early termination&lt;/head&gt;
    &lt;p&gt;(1) The input contains denormal numbers. The significand of a denormal number is less than \(1\) and cannot be pre-scaled together with normal numbers. Therefore, an additional cycle is added to normalize the significand of denormal numbers while simultaneously adjusting their exponents.&lt;/p&gt;
    &lt;p&gt;(2) The result is a denormal number. The quotient result after digit iteration is greater than 1, which does not meet the denormal significand range. An additional cycle is required to right-shift the quotient result for normalization.&lt;/p&gt;
    &lt;p&gt;(3) Early termination occurs in two scenarios: when the result is \(NaN\), infinity, or exact \(0\), computation can terminate early and output the result since this information is available in the first cycle, allowing the division result to be output in the second cycle; when the divisor is a power of \(2\), its significand \(=1\), and division only requires processing the exponent of the dividend, skipping the digit iteration phase, enabling the division result to be output as early as the second cycle. However, additional cycles are still needed if the dividend or result is a denormal number.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;For vector floating-point division, the RISC-V vector instruction set extension does not support mixed-precision floating-point division, thus only the following needs to be supported:&lt;/p&gt;
    &lt;p&gt;(1) 1 f64 = f64 + f64;&lt;/p&gt;
    &lt;p&gt;(2) \(2\) \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(4\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Considering that vector division involves multiple division computations simultaneously, and early termination can cause asynchronous output of results unless all cases terminate early under the same conditions, the early termination mechanism is disabled for vector division. If early termination occurs, the result is temporarily stored internally and output simultaneously with other division results.&lt;/p&gt;
    &lt;p&gt;To unify timing, the divider's cycle count is standardized to the worst-case scenario, i.e., when the input contains denormal numbers and the output also contains denormal numbers. Other cases that could produce results faster are internally buffered until the standardized cycle count is reached before outputting the result.&lt;/p&gt;
    &lt;p&gt;The main design employs resource reuse, with the following data reuse in the non-numeric iteration module:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64/f32/f16 = f64/f32/f16 + f64/f32/f16\);&lt;/p&gt;
    &lt;p&gt;(2) 1 \(f32/f16 = f32/f16 + f32/f16\);&lt;/p&gt;
    &lt;p&gt;(3) \(2\) \(f16\) values \(= f16 + f16\).&lt;/p&gt;
    &lt;p&gt;A total of 4 signal groups are used to achieve the functionality of 7 division groups.&lt;/p&gt;
    &lt;p&gt;Since the digital iteration module is a critical path with significant timing pressure, achieving high reuse with non-digital iteration modules is not feasible without compromising timing requirements. Therefore, a partial reuse design is implemented for the digital iteration module:&lt;/p&gt;
    &lt;p&gt;(1) The interface consists of four sets of quotients and redundant remainders.&lt;/p&gt;
    &lt;p&gt;(2) The \(s0\) stage uses \(7\) sets of \(CSA\) and \(7\) sets of prediction, with \(4\) sets of quotient selection.&lt;/p&gt;
    &lt;p&gt;(3) Stages \(s1\) and \(s2\) utilize \(4\) sets of \(CSA\), \(4\) sets of prediction, and \(4\) sets of quotient selection.&lt;/p&gt;
    &lt;p&gt;Registers also adopt resource reuse. For divisor, redundant remainder, quotient, and other registers, the bit width is allocated based on the maximum required by \(4\) \(f16\), \(2\) \(f32\), or \(1\) \(f64\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Hardware Design&lt;/head&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Adder&lt;/head&gt;
    &lt;head rend="h4"&gt;Scalar single-precision floating-point adder&lt;/head&gt;
    &lt;p&gt;A scalar single-precision floating-point adder is designed based on the improved dual-path floating-point addition algorithm, with its hardware implementation architecture shown in the figure.&lt;/p&gt;
    &lt;p&gt;The two input operands on the left are \(fp\_a\) and \(fp\_b\), while \(fp\_c\) on the right represents the addition result. \(fflags\) is a 5-bit exception flag, and \(rm\) is the rounding mode, with five modes represented by 3 bits. When \(is\_sub\) is 0, \(fp\_c = fp\_a + fp\_b\) is computed; when \(is\_sub\) is 1, \(fp\_c = fp\_a - fp\_b\) is computed. The difference between floating-point addition and subtraction lies only in the sign bit of \(fp\_b\), so minor adjustments to \(fp\_b\)'s sign bit enable the floating-point adder to support both operations. The overall design consists of three parts: the \(far\) path, the \(close\) path, and the exception path.&lt;/p&gt;
    &lt;p&gt;The far path first performs two parallel normalized exponent subtractions with significand right shifts, handling the cases where Efp_a ≥ Efp_b and Efp_b ≥ Efp_a separately. The correct right-shift result is selected based on the magnitude relationship between Efp_a and Efp_b and sent to the FS0 and FS1 significand adders. For subtraction, the far path sets EA as the larger exponent minus one, while for addition, EA is the larger exponent. This ensures the significand addition result falls within the range [1,4). During the right shift, two sets of grs are computed: grs_normal for rounding when the value is in [1,2), and grs_overflow for rounding when the value is in [2,4). Finally, based on the FS0 result and rounding mode, either FS0 or FS1 is selected as the significand result, and either EA or EA+1 is chosen as the exponent result. The sign bit result is determined by the exponent magnitude. The flag results indicate overflow if EA+1 is all ones and inexactness based on grs. The far path does not generate divide-by-zero, invalid operation, or underflow flags.&lt;/p&gt;
    &lt;p&gt;The \(close\) path uses four significant-digit adders, \(CS0\), \(CS1\), \(CS2\), and \(CS3\), to handle significant-digit subtraction for the cases where \(Efp\_a = Efp\_b\), \(Efp\_a = Efp\_b + 1\), and \(Efp\_a = Efp\_b – 1\). Based on the \(CS0\) result and \(grs\), four one-hot selection signals, \(sel\_CS0\), \(sel\_CS1\), \(sel\_CS2\), and \(sel\_CS3\), are generated. A four-input one-hot multiplexer (\(Mux1H\)) selects one result, which is ORed with the left-shifted \(mask\). A priority left shifter then normalizes the mantissa, outputting the \(lzd\) value during the shift. The exponent result is \(EA – lzd\), and the mantissa result is chosen between the normalized mantissa and \(CS4\), where \(CS4\) is a supplementary rounding result that does not require left-shift normalization. The sign result is derived from the exponent difference and the \(CS0\) result. The flag result only indicates imprecision; no other exception flags are generated.&lt;/p&gt;
    &lt;p&gt;The exception path is used to determine whether the operation is invalid, whether the result is \(NaN\), or whether the result is infinite. When none of these conditions are met, normal computation proceeds, generating a selection signal to choose the result and flags from either the \(far\) path or the \(close\) path as output.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar mixed-precision floating-point adder&lt;/head&gt;
    &lt;p&gt;Building upon the scalar single-precision floating-point adder, a mixed-precision hardware design is implemented. The main difference lies in supporting mixed-precision computation. Taking the result as \(f32\) as an example, the table below shows the truth table for the operations corresponding to \(res\_widen\) and \(opb\_widen\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(res\_widen\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(opb\_widen\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f32 + f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f16 + f16\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f16 + f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Not allowed&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The figure below shows the architecture of a scalar mixed-precision floating-point adder. The main difference is the addition of a fast format conversion module at the data input. Based on the operation type, this module converts the operands into the result's data format before processing, after which the computation flow is identical to that of a single-precision floating-point adder.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector Floating-Point Adder&lt;/head&gt;
    &lt;p&gt;The diagram below shows the architecture of the vector floating-point adder. To meet timing requirements, it is composed of four modules: \(FloatAdderF64Widen\) handles all operations with 64-bit output results, \(FloatAdderF32WidenF16\) handles all operations with 16-bit or 32-bit output results, and \(FloatAdderF16\) handles only operations with 16-bit output results.&lt;/p&gt;
    &lt;p&gt;Here, \(fp\_format\) is a 2-bit result format control signal: \(00\) indicates the result format is \(f16\), \(01\) indicates \(f32\), and \(10\) indicates \(f64\). The output flags are 20 bits, arranged with lower bits being significant. When the result format is \(f16\), all 20 bits are valid; for \(f32\), the lower 10 bits are valid; and for \(f64\), the lower 5 bits are valid.&lt;/p&gt;
    &lt;p&gt;The vector floating-point adder employs a two-stage pipeline design. To achieve rapid wake-up, the addition result is computed in approximately 1.5 cycles. Pipeline partitioning is performed within each submodule, requiring only the insertion of a single register level. Below is an explanation of the pipeline partitioning for the three modules shown in the diagram.&lt;/p&gt;
    &lt;p&gt;The diagram below illustrates the pipeline partitioning of the \(FloatAdderF64Widen\) module. The \(far\) path inserts registers after the significand right shift, while the \(close\) path inserts registers after the \(Mux1H\).&lt;/p&gt;
    &lt;p&gt;The figure below shows the pipeline division of the \(FloatAdderF32WidenF16\) module, which includes calculations for two different output formats. The selection logic in the second cycle is complex, so registers are inserted within the adder in the \(far\) path. The first cycle performs the addition of the lower \(18\) bits and the higher bits, while the second cycle combines the carry from the lower \(18\)-bit addition of the first cycle with the higher bits to obtain the final result. The \(close\) path also inserts registers after \(Mux1H\).&lt;/p&gt;
    &lt;p&gt;The following diagram shows the pipeline partitioning of the \(FloatAdderF16\) module. This module has minimal timing pressure and adopts a partitioning method where the \(far\) path inserts registers after the right shift of significant bits, and the \(close\) path inserts registers after \(Mux1H\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;The previously introduced vector floating-point adder has a width of \(64\) bits, requiring both operands to be in vector form. However, \(RVV\) not only specifies that both operands are in vector form (\(vector-vector\), abbreviated as \(vv\)) but also allows one operand to be a vector and the other a scalar (\(vector-scalar\), abbreviated as \(vf\)). Additionally, under \(widening\) instructions, the arrangement of source operands is not limited to the lower significant part. When the source register width is half of the destination register width, the data source may come from either the lower or upper half.&lt;/p&gt;
    &lt;p&gt;To implement all floating-point instruction calculations in \(RVV\) and support \(VLEN\) extension, simple instruction computations are added to the vector floating-point adder, transforming it into a vector floating-point "\(ALU\)", referred to as \(VFALU\).&lt;/p&gt;
    &lt;p&gt;Therefore, the vector floating-point adder needs to be modified to adapt to the features of \(RVV\). The modifications consist of two parts: functional modifications and interface modifications.&lt;/p&gt;
    &lt;p&gt;The table below lists the opcodes supported by \(VFALU\), totaling \(16\) operations, where (\(w\)) indicates operations involving \(widen\). The operand formats for \(vfmerge\), \(vfmove\), and \(vfclass\) are special: \(vfmerge.vfm\) has three source operands—a vector register, a floating-point register, and a \(mask\) register; \(vfmove.v.f\) has only one floating-point register as the source operand; \(vfclass\) has only one vector register as the source operand.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Corresponding instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;Operand format&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)add\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)sub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Subtraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmin\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Find the minimum value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmax\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Find Maximum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmerge\)&lt;/cell&gt;
        &lt;cell&gt;\(vfm\)&lt;/cell&gt;
        &lt;cell&gt;Data merging&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmove\)&lt;/cell&gt;
        &lt;cell&gt;\(v.f\)&lt;/cell&gt;
        &lt;cell&gt;Data movement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnj\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Sign Injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnjn\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Sign inversion injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnjx\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;XOR sign injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(9\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfeq\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether equal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(10\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfnq\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Not Equal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(vmflt\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether it is less than&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfle\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Less than or equal to&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(13\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfgt\)&lt;/cell&gt;
        &lt;cell&gt;\(vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether greater than&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfge\)&lt;/cell&gt;
        &lt;cell&gt;\(vf\)&lt;/cell&gt;
        &lt;cell&gt;Is greater than or equal to&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;\(vfclass\)&lt;/cell&gt;
        &lt;cell&gt;\(v\)&lt;/cell&gt;
        &lt;cell&gt;Classification&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The table below defines the \(VFALU\) interface. Compared to the vector floating-point adder, it adds two mixed-precision data sources, \(widen\_a\) and \(widen\_b\). When the source and destination operand formats are the same, the data comes from \(fp\_a\) and \(fp\_b\); otherwise, it comes from \(widen\_a\) and \(widen\_b\). When \(uop\_idx=0\), the lower half is taken, and when \(uop\_idx=1\), the upper half is taken. When \(is\_frs1=1\), the source operand \(vs1\) comes from the floating-point register \(frs1\), which needs to be replicated into a vector register for computation. \(mask\) participates in the calculation of the \(merge\) instruction, and \(op\_code\) is the operation code indicating the operation to be performed.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_a&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_a\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Floating-Point Register Data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Addend sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(mask\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;Participate in \(merge\) instruction computation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(uop\_idx\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Select upper/lower half when \(widen\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(round\_mode\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(res\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\) instruction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opb\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Is the source operand \(vs1\) in the same format as the result?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;Opcode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_result&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Fused Multiply-Add Unit&lt;/head&gt;
    &lt;head rend="h4"&gt;Pipeline Partitioning&lt;/head&gt;
    &lt;p&gt;The vector floating-point fused multiply-adder adopts a four-stage pipeline design to achieve rapid wake-up, ensuring the multiply-add result is computed in approximately \(3.5\) cycles. The vector unit's latency is \(3.5\) cycles. The diagram below illustrates the architecture of the vector floating-point fused multiply-adder, where \(reg\_0\) denotes the first-stage register, \(reg\_1\) the second-stage, and \(reg\_2\) the third-stage. The vector floating-point fused multiply-adder also supports \(widen\) functionality, limited to \(f32 = f16 × f16 + f32\) and \(f64 = f32 × f32 + f64\) cases. Thus, only a single-bit \(widen\) signal is needed for control when the output format is fixed. The output \(fflags\) is also \(20\) bits, consistent with the representation in the vector floating-point adder.&lt;/p&gt;
    &lt;p&gt;To save area while meeting timing constraints, a resource-sharing implementation is adopted. Calculations for all data formats use the same vector Booth encoder and CSA compression. By interleaving the layout, the 107-bit adder also achieves resource sharing.&lt;/p&gt;
    &lt;p&gt;In the first cycle, seven sets of exponent processing are performed to obtain seven right-shift values. The corresponding right-shift value is selected based on the computation format. For the right shifters, the \(f64\) right shifter is shared with one \(f32\), while a separate \(f32\) and four \(f16\) right shifters are dedicated. If subtraction is performed, the right-shifted result of \(fp\_c\)'s mantissa is inverted before being fed into the first-stage register. Simultaneously, vector \(Booth\) encoding is performed in the first cycle, generating 27 partial products, which are compressed into 4 partial products using \(CSA\) and then registered.&lt;/p&gt;
    &lt;p&gt;In the second cycle, compress the remaining 4 partial products using \(CSA4\_2\), then compress the result with the first cycle's right-shifted significand using \(CSA3\_2\). Perform a 107-bit addition and register the result in the second-stage register.&lt;/p&gt;
    &lt;p&gt;In the third cycle, the sum result from the second cycle undergoes \(lzd\) and \(tzd\), followed by a left shift with \(mask\) limitation. The shifted result is stored in the third-stage register.&lt;/p&gt;
    &lt;p&gt;In the fourth cycle, rounding is performed to obtain the mantissa result. The exponent result is calculated based on the left shift condition in the third cycle. The sign bit can be obtained from the \(107\)-bit adder in the second cycle. The flag results can generate four types of flags: overflow, underflow, invalid operation, and inexact. Note the method for detecting underflow. \(IEEE-754\) specifies two methods for detecting underflow: \(before \quad rounding\) and \(after \quad rounding\). This design uses the \(after \quad rounding\) method selected by \(RISC-V\) to detect underflow.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;According to the \(RVV\) instruction definitions, vector floating-point fused multiply-add units can be reused for multiplication calculations, controlled by \(op\_code\). When performing multiplication, the internal adder is set to zero. Additionally, \(RVV\) defines a series of floating-point fused multiply-add instructions, primarily differing in sign bits and operand order. The vector floating-point fused multiply-add unit is modified to support all related instructions as \(VFMA\), with added \(op\_code\) and interfaces. The following table lists the \(VFMA\) opcodes, totaling \(9\) operations, all supporting \(vv\) and \(vf\) operand forms. For \(vf\), \(vs1[i]\) is replaced by the floating-point register \(frs1\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Corresponding instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;Operand format&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)mul\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = vs[2] × vs1[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)macc\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vs2[i]) + vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)nmacc\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vs2[i]) - vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)msac\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vs2[i]) - vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)nmsac\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vs2[i]) + vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmadd\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vd[i]) + vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(vfnamdd\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vd[i]) - vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmsub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vd[i]) - vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(vfnmsub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vd[i]) + vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The table below shows the \(VFMA\) interface. To simplify control logic complexity, the three operands sent to \(VFMA\) are fixed in the order \(vs2\), \(vs1\), \(vd\). The functional unit internally adjusts the order based on \(op\_code\). Since the \(fma\) instruction uses a fixed target format for the addend during \(widen\), only \(widen\_a\) and \(widen\_b\) need to be added. \(uop\_idx\) is similarly used to select the upper or lower half of \(widen\_a\) and \(widen\_b\). \(frs1\) and \(is\_frs1\) are used to support \(vf\) instructions.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_a&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_c\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vd\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_a\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Floating-Point Register Data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Addend sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(uop\_idx\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Select upper/lower half when \(widen\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(round\_mode\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(res\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\) instruction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;Opcode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_result&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector floating-point divider&lt;/head&gt;
    &lt;head rend="h4"&gt;Scalar Floating-Point Divider&lt;/head&gt;
    &lt;p&gt;The scalar floating-point divider supports computations in three formats: \(1\) \(f16 = f16 / f16\), \(1\) \(f32 = f32 / f32\), and \(1\) \(f64 = f64 / f64\). The divider employs a \(Radix-64\) algorithm, where the iterative module performs three \(Radix-4\) iterations per cycle to achieve \(Radix-64\). The figure below shows the architecture of the scalar floating-point divider. The divider operates in a blocking manner and cannot accept the next division operation during computation, requiring handshake signals for control. This design uses \(start-valid\) handshake signals. Since the \(CPU\) may encounter branch prediction failures that flush pipeline states, a dedicated \(flush\) signal is included to clear the divider's internal state, allowing it to immediately start a new division operation in the next cycle.&lt;/p&gt;
    &lt;p&gt;Input data falls into three categories: both are normalized numbers (excluding divisors that are powers of \(2\)), at least one is a denormal number, and early termination (input contains \(NaN\), infinity, zero, or the divisor is a power of \(2\)). Results fall into two categories: the result is a normalized number, or the result is a denormal number.&lt;/p&gt;
    &lt;p&gt;When the inputs are all normalized numbers (excluding divisors that are powers of 2), the mantissas are normalized, and the process directly proceeds to the pre-scaling stage. When at least one input is a denormalized number, compared to the case where all inputs are normalized, an additional cycle is required for mantissa normalization before pre-scaling.&lt;/p&gt;
    &lt;p&gt;The prescaling stage takes one cycle, followed by integer quotient selection, where the two-bit integer quotient result is selected, and the prescaled divisor, dividend, and remainder's carry-save redundant representation are provided for the \(Radix-4\) iteration. The \(Radix-4\) iteration module calculates 6 bits of the quotient per cycle. \(f16\) division requires 2 cycles of \(Radix-4\) iteration, \(f32\) division requires 6 cycles, and \(f64\) division requires 9 cycles. After \(Radix-4\) iteration, the resulting mantissa quotient ranges between \((1, 2)\). When the result is a normalized number, only one cycle is needed for rounding and exponent result calculation to obtain the final division result. When the result is a denormal number, an additional cycle is required to denormalize the quotient before rounding.&lt;/p&gt;
    &lt;p&gt;Early termination is divided into two scenarios: (1) When the input operands contain NaN, infinity, or zero, division computation is unnecessary, and the result can be output in the second cycle. (2) When the divisor is a power of 2, the exponent result can be obtained in the first cycle. If the result does not require denormalization steps, it can be output in the second cycle; if denormalization is needed, an additional cycle is required, and the result is output in the third cycle.&lt;/p&gt;
    &lt;p&gt;The table below shows the required computation cycles for scalar dividers under different data formats, where \(+1\) indicates an additional cycle for post-processing when the division result is denormalized. In early termination cases, division operations for all data formats can be completed in just \(1\) to \(2\) cycles. Without early termination, \(f16\) division requires \(5\) to \(7\) cycles, \(f32\) division requires \(7\) to \(9\) cycles, and \(f64\) division requires \(12\) to \(14\) cycles.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Normalized Number&lt;/cell&gt;
        &lt;cell role="head"&gt;Denormal number&lt;/cell&gt;
        &lt;cell role="head"&gt;Early termination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(5+1\)&lt;/cell&gt;
        &lt;cell&gt;\(6+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(7+1\)&lt;/cell&gt;
        &lt;cell&gt;\(8+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(12+1\)&lt;/cell&gt;
        &lt;cell&gt;\(13+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Vector floating-point divider&lt;/head&gt;
    &lt;p&gt;The figure below shows the architecture of the vector floating-point divider. Compared to the scalar floating-point divider, since vector division computes multiple divisions simultaneously and all results must be written back to the register file together, early termination of a single division offers little benefit for vector division acceleration. Thus, the feature of variable output latency is removed. In all cases, the latency of the vector floating-point divider is fixed based on the input data format, as shown in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Calculation Cycle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In hardware design, aside from the \(Radix-64\) iteration module, the vector floating-point divider employs logic reuse, utilizing four signal groups for computation and control: the first group computes \(f64\_0\), \(f32\_0\), or \(f16\_0\); the second computes \(f32\_1\) or \(f16\_1\); the third computes \(f16\_2\); and the fourth computes \(f16\_3\). Registers are also reused to store intermediate results, with widths sized to \(max\) (1 \(f64\), 2 \(f32\), or 4 \(f16\)) to meet maximum requirements. The \(Radix-64\) iteration module is the critical path, optimized for timing while minimizing area. The first \(Radix-4\) iteration uses 7 independent \(CSA\) and quotient selection units, while the second and third iterations reuse 4 \(CSA\) and quotient selection units.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;The \(RVV\) specification defines three vector floating-point division instructions:&lt;/p&gt;
    &lt;p&gt;① \(vfdiv.vv \quad vd[i] = vs2[i]/vs1[i]\)&lt;/p&gt;
    &lt;p&gt;② \(vfdiv.vf \quad vd[i] = vs2[i]/f[rs1]\)&lt;/p&gt;
    &lt;p&gt;③ \(vfrdiv.vf \quad vd[i] = f[rs1]/vs2[i]\)&lt;/p&gt;
    &lt;p&gt;Case ③ is special as the operand order differs from cases ① and ②. For the vector division unit, the first operand is passed by the control logic as \(vs2[i]/f[rs1]\), and the second operand is passed as \(vs1[i]/f[rs1]/vs2[i]\). Thus, the functional unit sees the dividend in either vector or scalar form, and the divisor is also in vector or scalar form. Therefore, two additional scalar data interfaces are required. After adding these interfaces, the module is named \(VFDIV\), with the interfaces as shown in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(start\_valid\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(finish\_ready\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(flush\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Flush signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opa\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Dividend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opb\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Divisor&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs2\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Dividend comes from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Divisor sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs2\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Dividend sourced from floating-point register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;The divisor comes from the floating-point register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(rm\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(start\_ready\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(finish\_valid\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fpdiv\_res\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector format conversion module \(VCVT\)&lt;/head&gt;
    &lt;p&gt;The \(VCVT\) module is a three-stage pipelined vector floating-point format conversion module. It instantiates two \(VectorCvt\) submodules capable of processing \(64\)-bit data. Each \(VectorCvt\) contains one \(cvt64\), one \(cvt32\), and two \(cvt16\) modules. The \(cvt64\) supports processing floating-point/integer formats of \(64\), \(32\), \(16\), and \(8\) bits. The \(cvt32\) supports \(32\), \(16\), and \(8\)-bit floating-point/integer formats, while the \(cvt16\) supports \(16\) and \(8\)-bit floating-point/integer formats. Thus, \(VectorCvt\) can simultaneously process one \(64\)-bit (or two \(32\)-bit, or four \(16\)-bit, or four \(8\)-bit) floating-point/integer format input data for conversion.&lt;/p&gt;
    &lt;head rend="h4"&gt;Overall design&lt;/head&gt;
    &lt;head rend="h4"&gt;Module Design&lt;/head&gt;
    &lt;p&gt;The \(CVT\) module includes single-width floating-point/integer type conversion instructions, widening floating-point/integer type conversion instructions, narrowing floating-point/integer type conversion instructions, vector floating-point reciprocal square root estimation instructions, and vector floating-point reciprocal estimation instructions.&lt;/p&gt;
    &lt;p&gt;Select different \(cvt\) module calls based on \(width\). The design approach for the \(cvt\) module is divided into four types based on instruction type: \(fp2int\), \(int2fp\), \(fp2fp\), and \(vfr\). The overall design approach for \(fcvt64\) is to unify the format of the input \(64bit\) data:&lt;/p&gt;
    &lt;p&gt;different width unsigned/signed int -&amp;gt; 65 signed int&lt;/p&gt;
    &lt;p&gt;\(f16/f32/f64 -&amp;gt; 65bit (f64 \#\# false.B)\)&lt;/p&gt;
    &lt;p&gt;After standardizing the format, there is no longer a need to distinguish between different types of data, their bit widths, or field positions during the conversion process to a certain extent.&lt;/p&gt;
    &lt;p&gt;Building on this, \(VFCVT64\) is divided into 5 categories: \(int -&amp;gt; fp\), \(fp -&amp;gt; fp\) widen, \(fp -&amp;gt; fp\) narrow, estimate7 (\(rsqrt7\) &amp;amp; \(rec7\)), and \(fp -&amp;gt; int\).&lt;/p&gt;
    &lt;head rend="h4"&gt;\(FuopType\) decoding logic&lt;/head&gt;
    &lt;p&gt;For the \(cvt\) instruction: its \(fuopType\) consists of \(9\) bits, with each bit representing the following information:&lt;/p&gt;
    &lt;p&gt;Here, \([5:0]\) is obtained from the manual, and \([8:6]\) is additionally added during the design of control signal generation for convenience.&lt;/p&gt;
    &lt;p&gt;\([8]:1\) indicates it is a \(move\) instruction, \(0\) represents \(cvt\) instruction or the two estimation instructions \(vfrsqrt7\) and \(vfrec7\).&lt;/p&gt;
    &lt;p&gt;\([7]: 1\) indicates the input is \(fp\), \(0\) indicates the input is \(int\).&lt;/p&gt;
    &lt;p&gt;\([6]\): \(1\) indicates the output is \(fp\), \(0\) indicates the output is \(int\).&lt;/p&gt;
    &lt;p&gt;\([5]:1\) indicates it is one of the two estimation instructions, \(vfrsqrt7\) or \(vfrec7\); otherwise, it is a \(cvt\) instruction. When it is \(1\), \([0]\) distinguishes between \(vfrsqrt7\) and \(vfrec7\).&lt;/p&gt;
    &lt;p&gt;\([4:3]: 00\) denotes \(single\) type, \(01\) denotes \(widen\), \(10\) denotes \(narrow\).&lt;/p&gt;
    &lt;p&gt;\([2:0]\): For different instructions, it serves different purposes: For conversions between floating-point and integer, \([0]\) distinguishes whether the integer is signed or unsigned; in other cases, \([2:1]=11\) indicates it is an \(rtz\) type instruction, and \([2:0]=101\) indicates it is \(rod\) (vfncvt_rod_ffw).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.xiangshan.cc/projects/design/en/latest/backend/VFPU/"/><published>2025-10-04T22:02:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45477192</id><title>OpenAI's hunger for computing power has Sam Altman dashing around the globe</title><updated>2025-10-05T00:50:50.911921+00:00</updated><content/><link href="https://www.wsj.com/tech/ai/openai-sam-altman-asia-middle-east-7b660809"/><published>2025-10-04T22:14:29+00:00</published></entry></feed>