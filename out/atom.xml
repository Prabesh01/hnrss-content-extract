<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-14T16:12:59.880681+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45920677</id><title>OpenMANET Wi-Fi HaLow open-source project for Raspberry Pi–based MANET radios</title><updated>2025-11-14T16:13:13.859928+00:00</updated><content>&lt;doc fingerprint="5e7b8569114a82a3"&gt;
  &lt;main&gt;&lt;p&gt;OpenMANET is an open-source project for building Raspberry Piâbased MANET radios on Wi-Fi HaLow (915 MHz) using Morse Micro chipsets. A MANET (Mobile Ad-Hoc Network) is a self-forming wireless mesh where each node connects directly without centralized infrastructure. This technology is especially useful in the civilian space for search and rescue, disaster response, airsoft events, and any disconnected communications scenario. Designed to be budget-friendly with excellent long-range performance. The build is designed to integrate with ATAK over multicast, but works equally well over standard IP and internet links.&lt;/p&gt;View Docs GitHub Organization Buy Me a Coffee Instagram Visit the Store&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openmanet.net/"/><published>2025-11-13T21:18:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45920881</id><title>650GB of Data (Delta Lake on S3). Polars vs. DuckDB vs. Daft vs. Spark</title><updated>2025-11-14T16:13:13.781222+00:00</updated><content/><link href="https://dataengineeringcentral.substack.com/p/650gb-of-data-delta-lake-on-s3-polars"/><published>2025-11-13T21:33:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45922420</id><title>What Happened with the CIA and The Paris Review?</title><updated>2025-11-14T16:13:13.567810+00:00</updated><content>&lt;doc fingerprint="6e458016c3a699a1"&gt;
  &lt;main&gt;
    &lt;p&gt;Peter Matthiessen in New York City, 1961. Photograph by Ben Martin/Getty Images.&lt;/p&gt;
    &lt;p&gt;When Peter Matthiessen’s name comes up in conjunction with The Paris Review, two facts are sure to emerge. The first is that Matthiessen was one of the magazine’s founders, and that his enchantingly shabby Paris apartment provided a bumptious gathering place in its earliest days. The second is that he was, at the time, an undercover CIA operative, and that the creation of the magazine was somehow wrapped up in his spycraft. The New York Times revealed Matthiessen’s CIA affiliation in a bombshell 1977 story with the headline “Worldwide Propaganda Network Built by the C.I.A,” which examined dozens of publications and cultural organizations that had been secretly “owned, subsidized or influenced in some way by the C.I.A. over the past three decades.” Matthiessen’s connection rated only three brief sentences buried at the center of what he called a “long gray article”; the reporter, John Crewdson, noted that there was no evidence the CIA had used the writer “to influence the Paris Review.” Even so, Matthiessen spent the rest of his life facing questions about his role. He had left the agency in 1953, after about two years, but he never divulged the details of his work for the organization, which remain unclear even now, eleven years after his death.&lt;/p&gt;
    &lt;p&gt;Some have speculated that the Review itself received CIA support as part of the agency’s broader effort to prop up pro-Western art and literature. At the peak of its influence, in the fifties and sixties, the CIA fronted money to support a broad array of cultural production, from the seemingly innocuous to the expressly anti-communist. Among many other ventures, it had its hand in abstract-expressionist painting, jazz, Radio Free Asia, literary magazines, academic books on Finland and East Germany, a Roman newspaper, and an animated film adaptation of Animal Farm. While some artists were aware of the source of their funding, many were not. Given that The Paris Review portrayed itself as studiously apolitical—recall William Styron’s famous anti-manifesto in the first issue, fashioning it as a home for “the non-drumbeaters and non-axe-grinders”—Matthiessen’s CIA involvement has raised questions and eyebrows since its revelation in the seventies.&lt;/p&gt;
    &lt;p&gt;Lance Richardson’s True Nature: The Pilgrimage of Peter Matthiessen is the first biography of the writer. Matthiessen, born in New York in 1927, was the author of ten novels, two collections of stories, and nearly two dozen works of nonfiction; he is the only writer to have won the National Book Award for both fiction (for Shadow Country, in 2008) and nonfiction (for The Snow Leopard, in 1980). A keen observer of the natural world, he traveled widely in Africa, Latin America, and the Caribbean in search of remote places where one could find a “glimpse of the earth’s morning,” as he described it. True Nature offers a deft assessment of his work and a capacious telling of the forces that shaped his interest in everything from Zen Buddhism to environmentalism to cryptozoology to labor rights. Richardson conducted hundreds of interviews over seven and a half years, and his archival research yielded, among many other insights, a clearer picture of The Paris Review’s first years, when Matthiessen was doing double duty as a fiction editor and a secret agent. I spoke to Richardson by phone to ask what he’d discovered about Matthiessen’s years in Paris.&lt;/p&gt;
    &lt;p&gt;INTERVIEWER&lt;/p&gt;
    &lt;p&gt;What do we know about why Peter Matthiessen decided to join the CIA—the decision that led, eventually, to the founding of The Paris Review?&lt;/p&gt;
    &lt;p&gt;LANCE RICHARDSON&lt;/p&gt;
    &lt;p&gt;Before he died, in anticipation of a possible memoir, Matthiessen wrote out a series of narratives about what he’d been doing in Paris. The title of one of them is “THE PARIS REVIEW V. THE CIA: My Half-life as a Capitalist Running Dog.” They were incomplete, and I had to be careful about assuming everything was one hundred percent accurate—not because Peter was necessarily trying to leave a trail of lies or anything, but because he was writing this decades after it happened, and he had his own agenda. In terms of other materials, the CIA wouldn’t give me anything. I filed FOIA requests. I talked to their entertainment liaison, who works with Hollywood. But they don’t declassify personnel records.&lt;/p&gt;
    &lt;p&gt;As Matthiessen tells it, he had finished Yale in 1950 and wanted to be a writer, but how do you just become a writer? His English professor Norman Holmes Pearson tapped him on the shoulder and asked if he wanted to do something for his country. This was happening quite a lot at Yale at the time. One of Matthiessen’s contemporaries estimated that two dozen of their classmates were recruited for the CIA through various professors. The agency called them the “P source,” for “professor.” Matthiessen wrote that Pearson opened him “like an oyster.” Not because he was ideologically driven—his politics at that point were unformed and chaotic—but because he wanted a stipend and an excuse to go to Paris, which was a city that he and his first wife, Patsy Southgate, really loved. The CIA then was reputationally much more benign, at least domestically. It hadn’t yet become known by most Americans for its involvement in coups and things like that.&lt;/p&gt;
    &lt;p&gt;They were active in Korea, Guatemala, and Iran in those years, arranging paramilitary operations and working, in the last case, to bring the shah back to power, though as you say none of that had come to light. At this point, then, they were into election interference and some psyops, but no exploding cigars and mind-control experiments yet?&lt;/p&gt;
    &lt;p&gt;RICHARDSON&lt;/p&gt;
    &lt;p&gt;Right. They sent Matthiessen first to D.C. to meet with James Angleton, a now-famous spymaster who at that time headed up the agency’s Office of Special Operations, which handled foreign intelligence, counterintelligence, and espionage. Then Matthiessen went to spycraft training in New York, which he called “great fun,” and he got on a boat to Paris in 1951. He stumbled into this world of espionage as an excuse to write a novel and be in a city that he associated with freedom.&lt;/p&gt;
    &lt;p&gt;Do you think Matthiessen’s time in the navy, during the final stages of World War II, may have motivated him to join the CIA?&lt;/p&gt;
    &lt;p&gt;Absolutely. He was at the Hotchkiss School during the war, in high school, and he would watch a lot of the young men slightly older than him go off to fight. He saw it as a rite of passage, a badge of honor. By the time it was his turn, when he was doing basic training in Sampson, New York, V-J Day happened. So he missed out. His letters to his girlfriends at the time are really conflicted. He was happy the war was over, but he also felt that he’d been denied something. Eventually, toward the end of 1945, he got sent off anyway, to Hawaii. His job was to do the laundry of the real soldiers who were being demobilized and sent home. He felt incredibly emasculated by this. There’s one point in his notes for his memoir when he says, and I’m paraphrasing here, that he saw joining the CIA as another opportunity to make up for something he had been denied during the war.&lt;/p&gt;
    &lt;p&gt;His apartment in Paris, at 14, rue Perceval, was integral to the romance of the early days of The Paris Review—a kind of midcentury bohemian, glass-walled paradise where they threw all these parties. It was heated with lumps of coal dust, and there was a huge painting of a cat’s head on the wall. What was his life like in Paris?&lt;/p&gt;
    &lt;p&gt;In an article Gay Talese wrote for Esquire in 1963 about those early days of the magazine, he called the apartment “a monstrous fishbowl.” But those parties, and that bohemian lifestyle, were just one aspect of Matthiessen’s life. He would take the metro to meet his CIA handler in the Jeu de Paume, and they would stroll from the museum to the gardens near the Louvre and discuss his assignments. What he was actually working on for the CIA is still opaque. Matthiessen described it later as “deceiving people” and “serial lying.” Until the CIA releases its files, it’s always going to be a bit shadowy. I assume he was spying on other expat Americans, his friends. That’s probably why he was always cagey about it—the shame he felt about doing that.&lt;/p&gt;
    &lt;p&gt;Matthiessen wrote about cultivating a source he dubbed “Monsieur X,” whom he called “a near fanatic” and “a veteran Communist agitator.” But there was speculation that he could’ve been spying on the novelist Richard Wright as well, right?&lt;/p&gt;
    &lt;p&gt;I would not have been surprised if he was reporting on Wright. Wright was being watched at the time by the CIA. And then Matthiessen turns up in Paris around the same time, and they have an overlapping social circle. It seems unlikely to me that he wouldn’t have reported back about Wright.&lt;/p&gt;
    &lt;p&gt;What led him from spying to starting a magazine?&lt;/p&gt;
    &lt;p&gt;The problem with Matthiessen’s cover soon became clear—the labor of a writer is pretty invisible to the outside world. It looks like we’re just sitting inside and not doing anything at all. Matthiessen’s handler told him he needed a visible profession. And one day in one of the cafés he runs into Harold “Doc” Humes, another American who was running a magazine called the Paris News Post, which he had acquired for six hundred bucks, because that was the trend among expats in postwar Paris. Everyone had a little magazine going in that time—there was Merlin, Points, Zero.&lt;/p&gt;
    &lt;p&gt;Humes was a real character, a bit of a loose cannon. He was wearing a cape when Matthiessen saw him at the café that day. He brought on Matthiessen as his fiction editor. But Matthiessen saw the Paris News Post as a lightweight endeavor. He suggested one day to Doc that they flick it off and make something better. Doc jumped at the idea—or, if you take his word for it, it was really his idea, and he planted it in Matthiessen’s head. Peter didn’t want to be the top editor, so he phoned up his friend George Plimpton, whom he’d known since they were children on the Upper East Side. Plimpton was in Cambridge, England, at the time, about to graduate, not sure what he was going to do with his life. And he seized the opportunity to come over to Paris and start editing this new magazine, with Matthiessen still on as the fiction editor.&lt;/p&gt;
    &lt;p&gt;So, in a funny way, it was really the fact that writing is far too solitudinous an activity that gave us The Paris Review. Along with the CIA, of course. Matthiessen was intimately involved with choosing work for the first issues—he really did two jobs at once. I mean, it wasn’t like he was phoning it in at the magazine. But did the CIA ever give The Paris Review money?&lt;/p&gt;
    &lt;p&gt;The question of whether the CIA ever directly funded The Paris Review is an incredibly complicated one. The editors were all raising money to run the magazine, canvassing all their parents’ friends. Julius Fleischmann, of the instant-yeast family, was one of Matthiessen’s father’s friends. He and Matty Matthiessen would drink highballs on boats down in the Caribbean together. Fleischmann was a well-known philanthropist and arts patron, but it came out later that he was also a frontman for the CIA. So it’s hard to say, when he gave money to the Review, if it was his own money or if he was funneling it to the magazine through the Farfield Foundation, which the agency used to fund pro-Western propaganda.&lt;/p&gt;
    &lt;p&gt;You write about “arguably the most contentious document in the Paris Review archive,” a letter from Matthiessen soliciting funding from Fleischmann. His donation was comparatively small—a thousand dollars.&lt;/p&gt;
    &lt;p&gt;That was still quite a lot of money, but not compared to the check that the Farfield Foundation sent to a more political London literary magazine called Encounter in the same year, 1953, for forty thousand dollars. A few years later, there’s a letter in the archive in which Plimpton gets his secretary to go back to Fleischmann for more money, and Fleischmann’s secretary says, Sorry, we can’t help you. So if the magazine really was of interest to the CIA as an ideological tool, why would they give a small donation and then decline any further donations later? I think they were interested in the magazine purely as a cover for Matthiessen, and once Matthiessen quit his spying job, in 1953, they no longer needed it. He was working as the fiction editor by then, and brought in stories like Sue Kaufman’s “Tea at Le Gord,” which Plimpton especially liked, and which appeared in the third issue. It’s about an American student negotiating the price of a homestay with a French woman.&lt;/p&gt;
    &lt;p&gt;You could argue that, ideologically, the magazine’s founders toed the CIA line unintentionally. In the biography, you have this amazing quote from an interview Patsy Southgate gave to Talese in 1963—“They’re a bunch of reactionaries; their idea of a radical step is to eliminate the comma.” Did your research change your thinking about the politics of the magazine in those early days?&lt;/p&gt;
    &lt;p&gt;Talese is meticulous with his archives. In his basement on the Upper East Side there’s a box of files of interviews with all the original Review people, and he very graciously allowed me to see them. Southgate, by the time he spoke with her, was Matthiessen’s ex-wife, and she was quite bitter about their relationship and her time in Paris. She gave an interview where she’s strafing all the founders of the magazine, saying that they were trying on these bohemian masks because they were “very insecure about their maleness,” as a way of making up for not doing anything in the war—that it was very macho, and she was relegated to the kitchen. Her version of the story hadn’t really been told—and provides more of a feminist take on the early years of The Paris Review. A lot of the existing accounts of The Paris Review’s founding had involved a lot of mythmaking. They were more like Plimpton burnishing the legend of the magazine, the expat community, the parties, and the scrappiness of the staff—a legend that started somewhat unintentionally with Talese’s article in Esquire,which is actually fairly caustic about the privilege and entitlement of these young men. Talese did not come from that world. His father was a tailor—he always likes to tell that story—so he was skeptical of the whole thing. But because that article is so evocative of an era, he helped create the legend of The Paris Review, and then Plimpton ran with it, because that’s who he was.&lt;/p&gt;
    &lt;p&gt;How do you think the revelations about Matthiessen’s intelligence work affected his relationship with Plimpton and the magazine? You note that his editorial correspondence with the Review mostly stopped sometime around the summer of 1955, and that once he moved back to New York he felt increasingly detached from the day-to-day operations. He would mail his story selections to Plimpton in Paris, whom he felt was “needlessly abrasive” in his responses to writers. They had a fight about this sometime in the late fifties or in 1960, at the latest, and Matthiessen resigned as fiction editor, though he remained on the magazine’s board as a founder. But it’s not until later that he decides to come clean about his CIA involvement. He told Plimpton in 1964 or ’65, and I don’t think there’s a record of how that went. But Humes, whom he told in ’66, had recently taken a heroic dose of LSD and had a breakdown—&lt;/p&gt;
    &lt;p&gt;Doc was in London having a mental health crisis, and Peter was like, Now is the right time to tell you that all of your paranoid fantasies are actually based in reality. His timing was a little questionable. Humes threatened to resign from the magazine afterward, and Plimpton had to talk him down—which meant Plimpton was now upset at Peter, too, for rocking the boat. Plimpton was shocked and outraged, too. Their friendship was already tinged with ambivalence. Plimpton looked at what Matthiessen was doing, and wanted to be a writer himself, but became better known as an editor. Matthiessen looked at Plimpton and, I think, saw him as a bit of dilettante. There was some animosity. I’m speculating here, but I imagine Plimpton resented that Matthiessen’s CIA affiliation gave a taint to his life’s project. There was a bit of grit in the shell even decades later. In 1988, Matthiessen submitted a short story to the magazine about a CIA agent, and Plimpton supposedly threw it across the room because he thought it was rubbish. Matthiessen got very mad about that. The story, “Lumumba Lives,” which was published in Wigwag, went on to become a runner-up for the O. Henry Award. But that reaction is telling—these were grown men throwing each other’s papers across the room.&lt;/p&gt;
    &lt;p&gt;Maybe there’s also a difference in the ways they wore their WASP backgrounds. Both men came from rich, patrician families, eager to keep up appearances, worried about how things looked on the outside. I think of Plimpton with his table at Elaine’s, ever the bon vivant. Meanwhile, Matthiessen wrote of his urge to “simplify” himself. He craved acceptance from men with blue-collar backgrounds, and he took pains to expunge himself from the Social Register, literally. His interests in Zen and LSD, his ceaseless wandering—was he always, in a sense, running away from his past?&lt;/p&gt;
    &lt;p&gt;Matthiessen felt he had to atone for all the advantages he’d enjoyed coming from this powerful family. Around 1968, he got involved with social justice movements, with Cesar Chavez and then later with the American Indian Movement. He wrote a two-part New Yorker profile of Chavez, which he then expanded into a book. And then In the Spirit of Crazy Horse, his chronicle of the shoot-out at Pine Ridge in 1975, where two FBI agents and a Native man died, was the most controversial thing he ever wrote. He was subjected to a lawsuit from the governor of South Dakota and another from an active FBI agent. His third wife, Maria, said to me at one point that he felt like he had to make up for not only his own privilege, but to atone for all the dreadful things America had done. He took this enormous burden on his shoulders, and he put it on the shoulders of his children as well. It was part of the problem of his home life—his own neurosis about where he’d come from, which he then foisted onto his children.&lt;/p&gt;
    &lt;p&gt;You write that his family “were often made to feel like rocks in his rucksack that he was desperate to offload.” Given that sense of conflict, and his later political leanings more generally, why do you think he could never fully admit what he’d done for the CIA?&lt;/p&gt;
    &lt;p&gt;He never legally had clearance to talk about it publicly. It wasn’t declassified. So on one level, he would say he wasn’t allowed to. On another level—and probably a more significant one—I think part of his evasiveness was just because there was shame involved for him. When he did get involved with Chavez’s United Farm Workers, and also the American Indian Movement, these were groups that had already been infiltrated by government agents. If Matthiessen were exposed as actually having been one of those government agents, he would lose all credibility with these people, and everything he’d done to further their cause would be thrown out the window. There’s an amazing letter that he wrote to Leonard Peltier in 2008, after the Times had run a piece about Doc Humes that mentioned the CIA link. And Matthiessen says that he’s ashamed of his former association, but that it had been over for more than half a century, and it never had anything to do with his commitment to Peltier’s cause. Leonard writes back and says, I’ve known about this for decades—it doesn’t matter at all.&lt;/p&gt;
    &lt;p&gt;I notice that Matthiessen had a fondness for the word primordial—that he was attracted to an idea of prehistory, and that he sought out landscapes that still evoked the world as it was before humankind put it under the plow. What do you think was driving his wandering from place to place, and his interest in remoteness especially?&lt;/p&gt;
    &lt;p&gt;Matthiessen had this idea of what he called “the island.” He was always looking for a lost paradise, and in the late sixties he considered writing a book called “The Search for an Island.” His editor wrote in a memo that Matthiessen’s “most deep-felt interest is in finding a place isolated from the world.” Sometimes it was a physical place—he typed up a note once about having a “bay for crabs and oysters” and a house “close-chinked against the wind, with its pine fire and fat pile of drying wood”—but sometimes it was more of an abstraction. In either case it was a place where you could exist without all the encrustations of ego and the expectations that inevitably emerge as you become older. You’re in a childlike state. You can think of it as an Eden before the fall, a prelapsarian place where you can be your pure self without having to worry about, I don’t know, paying taxes and all the responsibilities we have as members of society.&lt;/p&gt;
    &lt;p&gt;He yearned for the island, and he found it in his life, in fleeting glimpses. The most important one he found was Shey Gompa, the “Crystal Monastery,” in Nepal. The weeks that he spent there were some of the happiest in his life, following the wolves or the blue sheep, meditating, just existing. I wanted to go there and physically be in that space. Even though I was only there briefly because I was ill, I got it. It’s an extraordinary place, so high that you feel like you’re at the edge of the atmosphere. I felt for a moment what it was he’d been searching for.&lt;/p&gt;
    &lt;p&gt;What drew you to Matthiessen as a subject?&lt;/p&gt;
    &lt;p&gt;I read his book The Snow Leopard about fifteen years ago. It came out in 1978, and covers the two months he spent in the Himalayas, when he was mourning the death of his second wife and hoping to glimpse this legendarily elusive animal of the mountains. I couldn’t initially explain why this book struck me so forcefully. It was something about his sensibility. Matthiessen took science and spirituality—these two modes of thinking that we often treat as incompatible—and wrote in both registers simultaneously. I was really interested in how this allowed him to see the world. He had this unique capacity to glimpse these two separate traditions at once.&lt;/p&gt;
    &lt;p&gt;And that led you to a kind of method biography in which you followed in his footsteps, taking a trip to the Himalayas like the one in The Snow Leopard. How did that trip—which you describe as somewhat disastrous—inform your biography?&lt;/p&gt;
    &lt;p&gt;Initially the idea of the trip to the Himalayas was just what I put in the book proposal, because I wanted to have an adventure. I wasn’t even planning on doing a biography. I was going to write a book about the landscape and animals Peter had written about. I was going to revisit them and see how they had changed. In the process of doing the research, it became clear that his life was so far-flung, that he had never settled, and that he had seen so much of the twentieth century from these unexpected angles. It was impossible to categorize his work, which was much more idiosyncratic than was sometimes believed—he wrote one novel, Far Tortuga, entirely in Caribbean dialect—and he never succeeded in figuring himself out. So I backed into the biography. But I had been like, Yeah, sure, I’ll go walk across the Himalayas. How hard can it be? I had no idea what I was getting myself into. I got very ill at that altitude. A doctor, when I got back, told me I had the symptoms of pulmonary edema. But it was worth it. I’d do it all again.&lt;/p&gt;
    &lt;p&gt;Matthiessen wrote so well about the natural world and the environment, and yet he resented being pigeonholed as a nature writer. Why do you think he didn’t like that term?&lt;/p&gt;
    &lt;p&gt;He thought it was passive and soft. He was more interested in something aggressive or active that was connected to his desire to create change. He preferred the term “environmental writer”—he didn’t see a difference between being an environmental writer and being an environmental activist. But he resisted that, too, because in his mind, he was a novelist. He had a hierarchy of forms of writing, and at the very top was the novelist. When it came to nonfiction, he saw that as a lower tier, as a type of cabinetmaking, whereas fiction was art. He really bristled at having become more famous for his nonfiction than his fiction. And ultimately, in 2008, when he won the National Book Award for his novel Shadow Country, he saw that as a vindication. That meant more to him than all the success of The Snow Leopard, a book that he always felt very conflicted about—which I find extraordinary. If I wrote something on the level of The Snow Leopard, I would hang up my hat. I’d be done.&lt;/p&gt;
    &lt;p&gt;Dan Piepenbring writes the New Books column for Harper’s Magazine. He is working on a book about ketamine.&lt;/p&gt;
    &lt;p&gt;Last / Next Article&lt;/p&gt;
    &lt;p&gt;Share&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theparisreview.org/blog/2025/11/11/what-really-happened-with-the-cia-and-the-paris-review-a-conversation-with-lance-richardson/"/><published>2025-11-14T00:18:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45922850</id><title>How to Get a North Korea / Antarctica VPS</title><updated>2025-11-14T16:13:13.304644+00:00</updated><content>&lt;doc fingerprint="4523309bb8e07a88"&gt;
  &lt;main&gt;
    &lt;p&gt;This article is currently an experimental machine translation and may contain errors. If anything is unclear, please refer to the original Chinese version. I am continuously working to improve the translation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;This blog post should be the final part of the “Running Your Own ISP at Home” series, and we’re going to talk about how to modify the geolocation of the IP addresses we announce.&lt;/p&gt;
    &lt;p&gt;By tweaking IP geolocation, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Display absurd IP locations on various platforms — for example, Antarctica (which barely has internet infrastructure), North Korea (which isn’t connected to the global internet), or some obscure tiny country with only tens of thousands of people&lt;/item&gt;
      &lt;item&gt;Use a single VPS to obtain IP addresses from all over the world, &lt;del rend="overstrike"&gt;show off on probe networks&lt;/del&gt;and achieve a weird kind of “All In One” status (yep, even this is All In One now)&lt;/item&gt;
      &lt;item&gt;Unlock region-locked streaming services — see this hostloc thread&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Run a one-man IDC selling VPSes from all corners of the globe&lt;/del&gt;— I found one called GlobalVM, but haven’t tried it, so no recommendation. Feel free to search on your own.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This article will mainly focus on modifying IP geolocation and using WARP to get a corresponding-region IPv4 address. Unlocking streaming content and running an IDC won’t be covered in depth — refer to the link above if interested.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prerequisites&lt;/head&gt;
    &lt;p&gt;This is probably common knowledge for many, but for completeness, let’s go over it briefly.&lt;/p&gt;
    &lt;head rend="h3"&gt;IP Databases&lt;/head&gt;
    &lt;p&gt;IP database providers compile mappings from &lt;code&gt;IP → geographical location&lt;/code&gt; using methods like network scanning and WHOIS lookups. They also include data such as IP threat scores and type (residential, server, or VPN). These databases are sold to users — typically websites — which then query them in the backend to display location info and perform risk assessment. A handy tool for querying multiple geolocation databases at once: https://iplark.com/&lt;/p&gt;
    &lt;p&gt;Popular IP databases include Maxmind, IPInfo, and DB-IP. Smaller databases often sync data from larger ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;WARP&lt;/head&gt;
    &lt;p&gt;WARP is a WireGuard-based VPN service provided by Cloudflare. While they offer an official Linux client, most people use native WireGuard to connect. WARP can provide your server with both IPv4 and IPv6 addresses, commonly used to add IPv4 connectivity to IPv6-only VPSes (or vice versa). One key feature of WARP is that the public IP it assigns will have the same geolocation as the IP you’re connecting from — we’ll use this property later. For a detailed WARP setup guide, check out: https://p3terx.com/archives/use-cloudflare-warp-to-add-extra-ipv4-or-ipv6-network-support-to-vps-servers-for-free.html&lt;/p&gt;
    &lt;head rend="h2"&gt;Submitting Geolocation Correction Requests&lt;/head&gt;
    &lt;p&gt;In reality, the “location” of an IP is inherently fuzzy. For instance, my &lt;code&gt;2a14:7c0:4d00::/40&lt;/code&gt; block was originally allocated to Israel. But later, I bought parts of this range and announced them via BGP in Germany, the US, and Singapore (see previous article on Anycast networks). Meanwhile, I’m physically located in mainland China. As the owner of this IP block, I can also freely edit the &lt;code&gt;country&lt;/code&gt; field in the WHOIS database — and I set it to KP (North Korea).&lt;/p&gt;
    &lt;p&gt;Because of this ambiguity, it’s nearly impossible to precisely determine an IP’s location using any single technical method. As a result, almost all geolocation databases accept public/user-submitted correction requests.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparation&lt;/head&gt;
    &lt;p&gt;Before submitting any requests, let’s do a little prep work.&lt;/p&gt;
    &lt;p&gt;IP databases collect IP ranges from global routing tables. Previously, we were announcing the entire &lt;code&gt;2a14:7c0:4d00::/40&lt;/code&gt; block without subdividing it in RIPE NCC, which makes it harder for databases to process smaller segments. So let’s fix that.&lt;/p&gt;
    &lt;p&gt;Log in to the RIPE Database, go to &lt;code&gt;My Resources → IPv6 → Create assignment&lt;/code&gt;, and fill out the form to create a new &lt;code&gt;inet6num&lt;/code&gt; (which represents an IPv6 address block):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;inet6num&lt;/code&gt;: Enter a subnet. The smallest allowed is&lt;code&gt;/48&lt;/code&gt;, so I entered&lt;code&gt;2a14:7c0:4d00::/48&lt;/code&gt;. If you only own a&lt;code&gt;/48&lt;/code&gt;, you can’t subdivide further — you can only edit the LIR-assigned block.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;netname&lt;/code&gt;: Pick a name you like&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;country&lt;/code&gt;: Choose the country/region you want this IP block to appear in&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;admin-c&lt;/code&gt;&amp;amp;&lt;code&gt;tech-c&lt;/code&gt;: Fill in two contact objects — use the ones you created earlier&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;status&lt;/code&gt;: Select&lt;code&gt;ASSIGNED&lt;/code&gt;to indicate it’s assigned&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After creation, you can see all your subnets under “My Resources”:&lt;/p&gt;
    &lt;p&gt;Next, update the BIRD configuration from our previous article, changing &lt;code&gt;2a14:7c0:4d00::/40&lt;/code&gt; to &lt;code&gt;2a14:7c0:4d00::/48&lt;/code&gt;, then restart BIRD.&lt;/p&gt;
    &lt;p&gt;After some time, use BGP Tools to verify that &lt;code&gt;2a14:7c0:4d00::/48&lt;/code&gt; is now visible. The old &lt;code&gt;/40&lt;/code&gt; page should return 404.&lt;/p&gt;
    &lt;head rend="h3"&gt;Submitting Correction Requests&lt;/head&gt;
    &lt;p&gt;You can submit geolocation correction requests to common IP databases: Maxmind, IPInfo, Google&lt;/p&gt;
    &lt;p&gt;If asked for justification, write something like “Due to incorrect IP geolocation, I/my clients cannot access region-restricted websites” (in English). Avoid mentioning use for anonymous proxies — that might violate their correction policies.&lt;/p&gt;
    &lt;p&gt;Each database has its own review process. Some involve manual checks, and changes usually take 3 days to 2 weeks to go live. Most offer online lookup tools (like Maxmind’s Demo) — you can use them to check progress, or use IPLark for batch queries.&lt;/p&gt;
    &lt;p&gt;In my test, IPInfo accepted my request within a week. Maxmind didn’t respond after two weeks, so I followed up via their contact form, and they finally approved it. (Wait a bit first — only reach out after multiple failed submissions.)&lt;/p&gt;
    &lt;p&gt;(p.s. Recently, Maxmind has been rejecting requests to set location to Antarctica (AQ) — &lt;del&gt;probably too many people trying to go there&lt;/del&gt;. That’s why this article uses North Korea as an example. If you really want an Antarctica IP, try the geofeed method at the end to bypass manual review.)&lt;/p&gt;
    &lt;p&gt;Below is for reference only — feel free to &lt;del&gt;make up&lt;/del&gt; craft your own justification:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Q: Hello, I am the network operator and owner of AS214775. I found out that my IP address segment 2a14:7c0:4d00::/40 is incorrectly localized to Israel, causing me to be denied access to other websites. I have tried several times to submit data corrections using the data correction form, but no response. I have corrected the country of my IP segment in the RIPE NCC database, and some other databases such as ipinfo.io have been synchronized, but Maxmind keeps locating my IP segment to Israel. I would like to politely ask why MaxMind has not responded to my correction request?&lt;/p&gt;
      &lt;p&gt;A: Thank you for your email. This will be updated in Tuesday’s release of the database.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Using WARP to Get a Region-Matched IPv4&lt;/head&gt;
    &lt;p&gt;Cloudflare uses Maxmind’s database, so as long as Maxmind reflects your desired location, WARP will follow suit. Note that Cloudflare may lag behind Maxmind by 1–2 weeks. If Maxmind shows the correct location but Cloudflare hasn’t updated, just wait a little longer.&lt;/p&gt;
    &lt;p&gt;WARP assigns IPv4 (and IPv6) addresses based on your connection IP’s geolocation. The IPv4 address not only allows access to IPv4-only sites, but its geolocation is maintained by Cloudflare — highly accurate and consistent across databases, much more reliable than manually submitting corrections everywhere.&lt;/p&gt;
    &lt;p&gt;We’ve already introduced WARP, so let’s jump straight into setup using this guide:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;# Generate a WARP WireGuard config&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Now test your VPS’s IPv4 geolocation using Cloudflare’s &lt;code&gt;/cdn-cgi/trace&lt;/code&gt; endpoint (available on any site behind CF). &lt;code&gt;ip=104.28.212.208&lt;/code&gt; means we got that IP, &lt;code&gt;colo=DUS&lt;/code&gt; means we’re connecting via the DUS (Düsseldorf Airport) data center (IATA code), &lt;code&gt;loc=IL&lt;/code&gt; means geolocation is IL (Israel) (country code), and &lt;code&gt;warp=on&lt;/code&gt; confirms WARP is active:&lt;/p&gt;
    &lt;p&gt;&lt;del&gt;We did successfully change our location, but &lt;/del&gt;&lt;code&gt;loc=IL&lt;/code&gt; means Cloudflare hasn’t picked up Maxmind’s update yet — let’s wait a bit longer&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;root@s39230 ~ # curl -4 https://www.cloudflare.com/cdn-cgi/trace&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;After nearly ten real-world days, Cloudflare WARP finally updated its database! Even slower than Cloudflare’s other services… At this point, it had been about two weeks since Maxmind updated, and a full month since my first correction request — almost missed the deadline before my server expired (thankfully, it didn’t).&lt;/p&gt;
    &lt;p&gt;Retest, and now we see the new IP &lt;code&gt;104.28.197.243&lt;/code&gt; returns &lt;code&gt;loc=KP&lt;/code&gt;, and Bilibili’s API shows North Korea:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;root@s39230 ~ # curl -4 https://www.cloudflare.com/cdn-cgi/trace&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Let’s check our own IPv6 and WARP-assigned IPv4 using IPLark:&lt;/p&gt;
    &lt;p&gt;&lt;del&gt;Now just set up a proxy on this VPS, and you can proudly flaunt your North Korean IP across the web.&lt;/del&gt; (If you’ve read this far, I assume you know how to set up a proxy.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Optional: Geofeed and Preventing Reversion&lt;/head&gt;
    &lt;p&gt;Lastly, the promised “Light Up the Globe” trick. For large providers with IPs all over the world, manually submitting corrections isn’t practical.&lt;/p&gt;
    &lt;p&gt;That’s where Geofeed comes in — a standard allowing bulk geolocation submissions: https://docs.ipdata.co/docs/publishing-a-geofeed. Besides submitting your Geofeed via support ticket to Maxmind, you can also embed the Geofeed URL in the &lt;code&gt;inet6num&lt;/code&gt; object in WHOIS, allowing databases to automatically crawl and update your IP locations. With this, you can get IPs from all sorts of bizarre countries, &lt;del&gt;show off on probe dashboards&lt;/del&gt; and achieve “Light Up the Globe” status.&lt;/p&gt;
    &lt;p&gt;IP geolocation isn’t set-and-forget — databases may re-scan and revert your location. To reduce this risk, block ICMP (ping) and common ports via firewall to avoid scanning. Also, avoid using your server’s native IPv6 to browse the web — stick to WARP-assigned IPv4. Some providers (cough Google cough) may even use client-side (mobile) location to correct server IP geolocation. See this article for details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Finally… This series began planning in June 2024, went through countless hurdles and waiting periods, and now wraps up just before December. If I waited any longer, my ASN and server would’ve expired (quietly).&lt;/p&gt;
    &lt;p&gt;We’ve explored setting up and maintaining an autonomous system on the Internet, configured BGP, peers, Anycast, and now IP geolocation spoofing — satisfying some bizarre curiosities, &lt;del&gt;and gaining a new appreciation for ISPs and one-man IDCs (or not)&lt;/del&gt;.&lt;/p&gt;
    &lt;p&gt;I might try DN42 next, or maybe not. For now, this series ends here. See you in the next blog post~ o/&lt;/p&gt;
    &lt;p&gt;This article is licensed under the CC BY-NC-SA 4.0 license.&lt;/p&gt;
    &lt;p&gt;Author: lyc8503, Article link: https://blog.lyc8503.net/en/post/asn-5-worldwide-servers/&lt;lb/&gt;If this article was helpful or interesting to you, consider sponsoring me¬_¬&lt;lb/&gt;Feel free to comment in English below~&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.lyc8503.net/en/post/asn-5-worldwide-servers/"/><published>2025-11-14T01:30:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45924345</id><title>Hooked on Sonics: Experimenting with Sound in 19th-Century Popular Science</title><updated>2025-11-14T16:13:13.045027+00:00</updated><content>&lt;doc fingerprint="b7e21b59ec73261a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Hooked on Sonics Experimenting with Sound in 19th-Century Popular Science&lt;/head&gt;
    &lt;p&gt;Of all the senses cultivated throughout the 19th century, it was the sense of hearing that experienced the most dramatic transformation, as the science of sound underwent rapid advancement. Lucas Thompson delves into a particular genre of popular acoustics primers aimed at children and amateurs alike, which reveal the pedagogical, ludic, and transcendental strivings of Victorian society.&lt;/p&gt;
    &lt;p&gt;October 23, 2025&lt;/p&gt;
    &lt;p&gt;In 1777, the German physicist Ernst Chladni, who would later be crowned the Father of Acoustics, designed an experiment that revolutionized our understanding of sound. After placing grains of sand on a thin metal plate and drawing a violin bow along one edge, Chladni watched in wonder as the sand danced and jiggled into surprising shapes — all perfectly even and symmetrical, but changing their formations depending on how the bow was used. In their beauty and complexity, these shapes (which the physicist himself cannily called “Chladni figures”) seemed to be arranged by invisible hands. In one simple and elegant experiment, sound had become visible.1&lt;/p&gt;
    &lt;p&gt;Here at last was clear proof that sound was not produced by generating tiny particles of matter within air, as the dominant theorists of the seventeenth and eighteenth centuries had insisted, but was instead the result of vibrations from waves. While earlier claims about the wave-like properties of sound (which in fact date back to Aristotle’s Physics) had fallen mostly on deaf ears, Chladni’s experiment provided undeniable evidence that sound was caused by waves that could move through both air and matter.&lt;/p&gt;
    &lt;p&gt;Chladni’s ingenious demonstration also showed that sound could be observed in a variety of new ways, and would no longer be consigned to the invisible aether. Moreover, it was an easy experiment to replicate for anyone who could get their hands on a copper plate, a violin bow, and some sand. In fact, it was so widely reproduced that, in 1901, Annie Besant and Charles Leadbetter, in their wonderful (and completely bizarre) theosophical study Thought-Forms, could write that Chladni figures were “already familiar to every student of acoustics”, being “continually reproduced in every physical laboratory”.2&lt;/p&gt;
    &lt;p&gt;The students of acoustics Besant and Leadbetter had in mind were educated through a vast collection of primers, textbooks, and popular introductions to science that were widely read across the nineteenth century. Despite being little known today, such texts were part of an important wave of science popularization, whose authors, according to the historian Bernard Lightman, “saw themselves as providing both entertainment and instruction to their readers”.3 Written for a growing middle-class audience, such books and periodicals gave detailed descriptions of groundbreaking experiments, encouraging readers to imagine their own homes as sites of scientific discovery and playful experimentation. Genuine learning and rich enjoyment, these books proclaimed, could be had within the home, and any reader with patience, curiosity, and some basic equipment could follow along with the latest scientific revelations.&lt;/p&gt;
    &lt;p&gt;In fact, even children could do so, and the experiments in these books were democratically pitched to the whole family. Although they ranged over many scientific disciplines — including chemistry, optics, physics, magnetism, and astronomy — it is their presentation of the emerging subfield of acoustics that is particularly intriguing, since it reveals many facets of nineteenth-century culture. These books speak to a widespread amateur fascination with science and reveal a desire to initiate even the very young into a world of intellectual discovery and delight. In doing so, they set forth a new model of learning — based on play, beauty, and pleasure — that anticipates many later approaches to education. These popularizing books also offer a vision of science that has now largely been forgotten. While, in our own time, scientific understanding is usually thought of in terms of detachment and objectivity, here beauty and knowledge were often intertwined. Finally, and perhaps most unexpectedly, these books prompted readers to reflect on questions of spirituality and transcendence, since they positioned the science of acoustics as a fresh avenue for moving beyond the material plane.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Century of Sound&lt;/head&gt;
    &lt;p&gt;These sonic experiments reflected new listening practices and new theories of sound that unfolded across the nineteenth century. It is a century that has been described by the literary critic John Picker as the “auscultative age”, extending the term that René Laennec coined for the invention of his stethoscope to describe the Victorians’ “careful listening to a world at large — and in flux”.4 The century also saw the birth of technologies designed to amplify, transmit, and record sound — the self-performing player piano, the phonograph, telephone, and radio, for instance. Of all the senses that the Victorians cultivated, it was the sense of hearing that experienced the most dramatic transformation. The Victorians, according to Jonathan Sterne, underwent what he terms “ensoniment”: an acoustic Enlightenment.5&lt;/p&gt;
    &lt;p&gt;Part of this transformation included a new understanding of children’s sensitivity to sound. In his 1878 essay “Child’s Play”, Robert Louis Stevenson argued that children’s hearing is far more acute and developed than their other senses. He suggests that while children “have no great faculty for looking” (since “they do not use their eyes for the pleasure of using them, but for by-ends of their own”) and have a “sense of touch” that is not “so clean and poignant . . . as it is in a man”, both their hearing and their sense of smell are superior and “more developed” to that of their elders. But Stevenson was also convinced that the child’s naturally superior hearing could be further cultivated. For all the freshness of sound to a child, he wrote, “hearing is capable of vast improvement as a means of pleasure; and there is all the world between gaping wonderment at the jargon of birds, and the emotion with which a man listens to articulate music.”6 The writers of these popular books of science for children may well have shared Stevenson’s conviction: they, too, wanted to educate the ears and minds of young readers, allowing them to experience and understand sound with greater precision and sensitivity.&lt;/p&gt;
    &lt;p&gt;Middle-class families of this era often read aloud to one another from novels, poetry collections, newspapers, and periodicals — that is, even printed texts were very often experienced sonically, suggesting yet another everyday aspect of Victorian listening. But the long evenings of the pre-electric nineteenth century also allowed ample time for other pursuits, including amateur science. The authors of these books stressed that their experiments could be carried out by the entire family, and even the smallest children need not miss out on the fun.&lt;/p&gt;
    &lt;head rend="h3"&gt;Playful Discovery&lt;/head&gt;
    &lt;p&gt;And there was indeed fun to be had. Clearly, part of the appeal of these experiments was in their sheer entertainment value. These books often use the language of “scientific amusements”, “scientific recreations”, and even scientific “parlour magic” to stress how diverting and delightful science could be. A young child might easily create intricate “acoustic curves” with the help of a basic pendulum, or construct a small siren that allowed for instructive observations on differences in pitch. The books contained advice for finding and listening to various kinds of harmonics, vibrating cords, and for observing sounds being reflected by small flames. Even a very simple experiment, such as swinging a whistle around on a string at various speeds, could yield valuable knowledge about vibration and frequency.&lt;/p&gt;
    &lt;p&gt;Arabella Buckley’s whimsical Fairy-Land of Science (1879), for instance, encourages children to experiment with all manner of scientific principles, and her chapter “The Voices of Nature and How We Hear Them” included details of many intriguing sonic demonstrations. At one point, she instructs her readers to “take a poker and tie a piece of string to it, and holding the ends of the string to your ears, strike the poker against the fender.”7 After noting the way the sound travelled through the string, she then invites children to hold the string in their teeth and block their ears, demonstrating the power of bone to conduct sound waves in a simple — but surely unforgettable — experiment. Elsewhere, she explains how birds produce such complexly beautiful trills and calls, and explores other miracles of the natural world. Like many popular science writers of the era, she encourages her young readers to poke around in their own ear to investigate its features: “Put your finger round your ear and feel how the gristly part is curved towards the front of your head”, Buckley writes. “This concha makes a curve much like the curve a deaf man makes with his hand behind his ear to catch the sound.”8 By following her lead, anyone could acquire anatomical as well as acoustic knowledge.&lt;/p&gt;
    &lt;p&gt;Written in the same spirit of playful discovery, John Henry Pepper’s The Boys’ Playbook of Science (1860) and Scientific Amusements for Young People (1861) offer countless experiments and demonstrations of acoustic principles, as does Light Science for Leisure Hours (1871) by Richard Proctor. Other books, such as William Henry Stone’s Elementary Lessons on Sound (1879), Worthington Hooker’s Science for the School and Family (1863), and Rodolphe Radau’s Wonders of Acoustics (1870) emphasize sonic curiosities from history and the natural world, such as the ancient Horn of Alexander (which could reportedly be heard at a distance of many miles) and the complex interaction of echoes with rock formations. Many of these popular science books include detailed illustrations (The Boys’ Playbook, for instance, boasted of 470 engravings) showing either disembodied hands or well-dressed Victorian youths carrying out different experiments.9&lt;/p&gt;
    &lt;head rend="h3"&gt;Luminous Flowers and Talking Machines&lt;/head&gt;
    &lt;p&gt;One of the most compelling of all the nineteenth-century books that popularized acoustics is Alfred Marshall Mayer’s Sound: A Series of Simple, Entertaining, and Inexpensive Experiments in the Phenomena of Sound, for the Use of Students of Every Age, from 1879. A professor of physics at the Stevens Institute of Technology in New Jersey, Mayer made important contributions to astronomy, optics, and acoustics, and wrote several books for the public that translated important scientific discoveries into language that any interested observer could follow. Like many other popular science writers before him, he was careful to centre his book (as its subtitle suggests) around “simple, entertaining, and inexpensive experiments”. Mayer tells his young readers that for the relatively low outlay of “just $27.50” (around £670 or $894 in today’s currency), they, too, can have a working laboratory for the investigation of acoustics, with the capacity not merely to replicate the demonstrations described in Sound, but to invent instructive experiments of their own.&lt;/p&gt;
    &lt;p&gt;Mayer patiently introduces children to many of the cutting-edge principles and theories of sound that were circulating in the late nineteenth century, covering topics such as reflection, transmission, vibration, and velocity, along with many newly discovered techniques for rendering sound visible, among them the ubiquitous Chladni figures. But in addition to imparting scientific knowledge, it is striking to note how many of these demonstrations are described in aesthetic terms, as being “beautiful”, “lovely”, or “harmonious”. Mayer clearly perceived both aesthetic and intellectual value in his experiments, and he encouraged his young readers to do the same. After one involving a pendulum that registered the vibrations of different musical intervals, for instance, Mayer advised them to frame the curves produced by the pendulum by fixing them onto glass, which will both “make beautiful ornaments for the window or mantel, and will remind you that you are becoming an experimenter”. Another “very beautiful and striking experiment” involved sprinkling silica powder into a wooden whistle, while elsewhere he describes the pleasure of discovering “beautiful little luminous flowers, like forget-me-nots” that are produced by a singing cone piped directly into a König’s flame.10 While science in the twenty-first century is often regarded as a dispassionate and purely rational endeavour, in these books beauty and scientific knowledge go hand in hand.&lt;/p&gt;
    &lt;p&gt;It is hard to know what age group Mayer imagined himself to be addressing. Some of the simpler experiments could be carried out by young children (perhaps with adult supervision), such as the construction of a so-called “talking machine” from an orange with a peanut nose, black bean eyes, and completed (in a slightly unsettling touch) with a “baby’s cap”. By puffing air through a small tube, and carefully controlling the “mouth” aperture, a highly realistic imitation of a baby’s “Mama!” could be achieved. (The accompanying line drawing bears an uncanny resemblance to Sesame Street’s Grover.) Others are considerably more complex, and would surely require the dexterity and understanding of a teenager. (Several of the illustrations feature a youth of somewhere between ten and fifteen years, neatly dressed in a blazer, tie, and striped trousers.) It must be said, too, that many of Mayer’s experiments and demonstrations are highly dangerous. Bunsen burners, heliostats, gas flames of various kinds, fragile glass tubes, and even volatile substances like lycopodium and silica powder are commonly used.&lt;/p&gt;
    &lt;p&gt;Mayer’s introduction to acoustics is representative of many of the books in this genre, especially in its palpable enthusiasm for scientific discovery. The experiments in all of these popular science books on sound are often pitched to the reader as delightful diversions — entertaining escapes from daily life. Yet as delightful as such experiments were, many of the authors also went to great lengths to stress their educational value. Amateur experimenters were not just acquiring sophisticated party tricks for the sake of amusement, but were also gaining genuine knowledge of acoustic principles. Playing around with different kinds of pendulums, for instance, may well be enjoyable in and of itself, but was also imparting knowledge about sound waves. In the same way, clapping near small flames revealed important principles of sonic reflection, while using whistles and “lamp chimneys” instructed young scientists about the effects of vibrating columns of air. Here in these books was a new vision of what education might be — real knowledge, the authors insisted, might arise naturally from play. Simply by encouraging their natural curiosity, children could be gently nudged in the direction of scientific discovery. To read these books even today is to recapture a childlike thrill in the process of learning.&lt;/p&gt;
    &lt;p&gt;Such pedagogical principles were far from the norm during the nineteenth century, which largely took a joyless, authoritarian approach to educating the young. The Victorian vision of institutional education was characterized by “harsh and coercive lessons”, writes Elizabeth Gargano, centred on “rote recitations and enforced silence.”11 Many popular science books of this era stand in stark contrast to such principles, offering a very different vision of education that is based on a harmony between play and learning. Instead of the austere silences of institutional education, such books are alive with sound and show readers precisely how to produce unusual acoustic phenomena. During the early years of the twentieth century, such a vision would be central to many new and radical approaches to educating children, including those of Maria Montessori, Rudolf Steiner, and the Reggio Emilia community. Joy, play, tactile discovery, and self-directed learning were at the heart of such novel ways of learning. The popular scientific books for children that were so successful in the nineteenth century may well have anticipated these later advances in educational theory and practice.&lt;/p&gt;
    &lt;head rend="h3"&gt;On the Sonic Plane&lt;/head&gt;
    &lt;p&gt;It is clear that these scientific instructionals reveal much about Victorian attitudes to science, children, entertainment, and learning. But there is another intriguing dimension to these forgotten texts: their insistence that sound itself, when properly understood, can allow for mysterious experiences of transcendence and spiritual communion. Many of these authors understood hearing as an inherently spiritual sense, an intuition that animated many other reverential and quasi-mystical conceptions of sound that were advanced across the nineteenth century. They stressed the “mysterious” and “angelic” properties of sound waves, telling young readers of the unearthly ways in which they interact with the human ear. It is no accident that several books (such as Buckley’s) invoke a realm of fairies and magic, and encourage new ways of perceiving and attending to the sensory world.&lt;/p&gt;
    &lt;p&gt;For many scientific writers, sound itself was part of a divine, ethereal realm that had only recently, through experimental science, drawn slightly closer. Something about sound itself readily moved the Victorian mind in a spiritual direction. Whether the grains of sand in Chladni’s experiment that seemed to be moved by unseen hands, or the mysterious forces that seemed to be channeled in other demonstrations, sound itself stood in for powerful forces of other kinds. Now that sound could be seen, perhaps other once-invisible energies might also reveal themselves. It is not too much of a leap from thinking about the effect of sound waves on matter to that of spirit on matter. In this way, the newly discovered visibility of sound in the Victorian age has obvious parallels with the Christian doctrine of the Incarnation: here, too, albeit on a far smaller and more manageable scale, a once-distant and invisible force was given physical form. The fact that spiritualism and theosophy were first becoming popular and widely practiced during this period also testifies to a broader interest in the ethereal realm. And since many artistic practices and new technologies were quickly pressed into the service of exploring such a realm, it is no surprise that science was too.&lt;/p&gt;
    &lt;p&gt;The newly discovered materiality of sound prompted many strange claims about its spiritual power: in 1837, Charles Babbage famously declared that “The air itself is one vast library on whose pages are forever written all that man has ever said or woman whispered” — a cosmic vision of all speech and sound as being potentially retrievable. The Victorians speculated that modern acoustic science might well be bringing lost or once-hidden realms nearer, such that we might someday be able to hear “the grass grow and the squirrel’s heart beat”, as the narrator of George Eliot’s Middlemarch (1872) imagines.12 It is telling, too, that the authors of popular books on acoustics often wrote with an air of initiating the young into a world of profound mystery, as though imparting great and secret knowledge. In Sound and Music (1879), the Reverend J. A. Zahm even included a poem that stresses the spiritual significance of sound, writing of God’s voice at the moment of creation moving through “soundless realms of space” and setting in motion a world that is now “vibrant”, containing shadowy whispers of “choral raptures grand” that resound in the heavens.13 For Zahm at least, exploring sound was a project of spiritual significance, promising illumination far beyond mere scientific knowledge.&lt;/p&gt;
    &lt;head rend="h3"&gt;Amateur Enthusiasms&lt;/head&gt;
    &lt;p&gt;Nowadays, the term “pop-science” is often used disapprovingly, as though something important is always lost when genuine scientific research is translated into less nuanced terms that the public can comprehend. But the hard distinction between professional and amateur science in our own era — between expertise and general interest — was not yet fully present in the nineteenth century.&lt;/p&gt;
    &lt;p&gt;To read these surprising, delightful, and often beautiful popular science books is to be made aware of the enormous gulf that has opened up between professional scientists and the public. As science became increasingly specialized in the twentieth century, the public were no longer able to follow along with new findings, let alone have any hope of reproducing important experiments. It is difficult to imagine an amateur enthusiast recreating the latest research, regarding the quantum phenomena of sound, for example, or the way that spiders “listen” to their surroundings via vibrations in their webs, at home. Of course, contemporary publishers still put out science primers, textbooks, and explainers, but something vital has vanished. The frontier of scientific discovery has receded from view, moving far beyond what non-specialists can comprehend. These nineteenth-century popularizing books arose during a brief period in which even children could somewhat keep pace with scientific advancement. They offer a crucial window into what has been lost, and reveal how new understandings of sound filtered through Victorian culture and beyond.&lt;/p&gt;
    &lt;p&gt;Lucas Thompson is a Senior Lecturer in English &amp;amp; Writing at the University of Sydney. He teaches and writes on contemporary US and Anglophone literature, ordinary language philosophy, literary aesthetics, and film and television. He is the author of Global Wallace: David Foster Wallace and World Literature (Bloomsbury, 2016) and Metaphors We Read By: Rethinking Literary Experience and Interpretation (Edinburgh University Press, 2025).&lt;/p&gt;
    &lt;p&gt;Enjoyed this piece? We need your help to keep publishing.&lt;/p&gt;
    &lt;p&gt;The PDR is a non-profit project kept alive by reader donations – no ads, no paywalls, just the generosity of our community. It’s a really exciting model, but we need your help to keep it thriving. Visit our support page to become a Friend and receive our themed postcard packs. Or give a one-off donation. Already a supporter? A huge thank you for making all this possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://publicdomainreview.org/essay/science-of-sound/"/><published>2025-11-14T06:13:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45924619</id><title>RegreSQL: Regression Testing for PostgreSQL Queries</title><updated>2025-11-14T16:13:12.453780+00:00</updated><content>&lt;doc fingerprint="6318b6147df597f4"&gt;
  &lt;main&gt;
    &lt;p&gt;TL;DR - RegreSQL brings PostgreSQL's regression testing methodology to your application queries, catching both correctness bugs and performance regressions before production.&lt;/p&gt;
    &lt;p&gt;As puzzling as it might seem, the common problem with production changes is the ever-present "AHA" moment when things start slowing down or crashing straight away. Testing isn't easy as it is, but there's a widespread practice gap when it comes to testing SQL queries. Some might pretend to "fix it" by using ORMs to abstract away the problem. Others treat SQL as "just glue code" that doesn't deserve systematic testing. Most settle for integration tests that verify the application layer works, never actually testing whether their queries will survive the next schema change or index modification.&lt;/p&gt;
    &lt;p&gt;For PostgreSQL development itself, the project has a robust regression test suite that has been preventing disasters in core development for decades. The database itself knows how to test SQL systematically - we just don't use those same techniques for our own queries. Enter RegreSQL, a tool originally created by Dimitri Fontaine for The Art of PostgreSQL book (which is excellent for understanding and mastering PostgreSQL as a database system), designed to bring the same regression testing framework to our application queries.&lt;/p&gt;
    &lt;p&gt;I've been trying to use it for some time, but due to missing features and limitations gave up several times. Until now. I decided to fork the project and spend the time needed to take it to the next level.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;The RegreSQL promise starts with the biggest strength and perceived weakness of SQL queries. They are just strings. And unless you use something like sqlc (for Go), PG'OCaml or Rust's SQLx toolkit giving you compile-time checking, your queries are validated only when they are executed. Which in better case mean either usually slow-ish test suite or integration tests, in worst scenario only when deployed. ORMs are another possibility - completely abstracting away SQL (but more on that later).&lt;/p&gt;
    &lt;p&gt;But even with compile-time checking, you are only checking for one class of problems: schema mismatches. What about behavior changes after schema migration or performance regressions? What about understanding whether your optimization actually made things faster or just moved the problem elsewhere?&lt;/p&gt;
    &lt;p&gt;This is where RegreSQL comes in. Rather than trying to turn SQL into something else, RegreSQL embraces "SQL as strings" reality and applies the same testing methodology PostgreSQL itself uses: regression testing. You write (or generate - continue reading) your SQL queries, provide input data, and RegreSQL verifies that future changes don't break those expectations.&lt;/p&gt;
    &lt;p&gt;The features don't stop there though - it tracks performance baselines, detects common query plan regressions (like sequential scans), and gives you framework for systematic experimentation with the schema changes and query change management.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic regression testing&lt;/head&gt;
    &lt;p&gt;Enough with theory. Let's jump in straight into the action and see what a sample run of RegreSQL looks like&lt;/p&gt;
    &lt;code&gt;$ regresql text
Connecting to 'postgres://radim:password123@192.168.139.28/cdstore_test'… ✓

Running regression tests...

✓ album-by-artist_list-albums-by-artist.1.json (0.00s)
✓ album-by-artist_list-albums-by-artist.2.json (0.00s)

✓ album-tracks_list-tracks-by-albumid.2.json (0.00s)
✓ album-tracks_list-tracks-by-albumid.1.json (0.00s)

✓ artist_top-artists-by-album.1.json (0.00s)

✓ genre-topn_genre-top-n.top-1.json (0.00s)
✓ genre-topn_genre-top-n.top-3.json (0.00s)

✓ genre-tracks_tracks-by-genre.json (0.00s)

Results: 8 passed, 0 failed, 8 skipped (0.00s)
&lt;/code&gt;
    &lt;p&gt;In this example based on Chinook database (as used originally in The Art of PostgreSQL book), RegreSQL scans the current directory (or one provided by &lt;code&gt;-C /path/to/project&lt;/code&gt;) for &lt;code&gt;*.sql&lt;/code&gt; files and attempts to run all queries against the configured PostgreSQL connection.&lt;/p&gt;
    &lt;p&gt;The individual files can contain either single or multiple sql queries. Like following example&lt;/p&gt;
    &lt;code&gt;-- name: top-artists-by-album
-- Get the list of the N artists with the most albums
SELECT
    artist.name,
    count(*) AS albums
FROM
    artist
    LEFT JOIN album USING (artist_id)
GROUP BY
    artist.name
ORDER BY
    albums DESC
LIMIT :n;
&lt;/code&gt;
    &lt;p&gt;The syntax for the queries supports both positional arguments (like &lt;code&gt;$1&lt;/code&gt; known from libpq library) or (preferred) &lt;code&gt;psql&lt;/code&gt; style variable (&lt;code&gt;:varname&lt;/code&gt;). The each identified query (not file) is then executed for 0..N times, based on number of predefined plans and verified to the expected results - validating the expected data matches the one returned. The support for SQL files handling is available separately with https://github.com/boringSQL/queries (Go version only for now).&lt;/p&gt;
    &lt;p&gt;This gives you what original RegreSQL tool has introduced - change your schema, refactor a query, run &lt;code&gt;regresql test&lt;/code&gt; and see immediately what broke. The test suite now has ability to catch regressions before they are committed / shipped. The current version built on top of it, giving you better console formatter instead of TAP style output, as well as jUnit, JSON and GitHub actions formatters for better integration into your CI/CD pipelines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance regression testing&lt;/head&gt;
    &lt;p&gt;Basic regression testing catches correctness issues - wrong results, broken queries, schema mismatches. But there's another class of production issues it misses. Performance regressions. No matter how unbelievable it might sound but queries get deployed without appropriate indexes, or they change over time. Simple fix - both for handwritten SQL or ORM code - can switch from milliseconds to seconds. You add index that helps one query, but tanks another. You modify conditionals and accidently force sequential scan of millions of rows. This is where it hurts.&lt;/p&gt;
    &lt;p&gt;RegreSQL addresses this by tracking performance baselines alongside correctness. Once baselines are generated&lt;/p&gt;
    &lt;code&gt;$ regresql baseline
Connecting to 'postgres://appuser:password123@192.168.139.28/cdstore_test'… ✓
Creating baselines directory: regresql/baselines
Creating directory 'regresql/baselines'

Creating baselines for queries:

  ./
  Created baseline: album-by-artist_list-albums-by-artist.1.json
  Created baseline: album-by-artist_list-albums-by-artist.2.json
  Created baseline: album-tracks_list-tracks-by-albumid.1.json
  Created baseline: album-tracks_list-tracks-by-albumid.2.json
  Created baseline: artist_top-artists-by-album.1.json
  Created baseline: genre-topn_genre-top-n.top-1.json
  Created baseline: genre-topn_genre-top-n.top-3.json
  Created baseline: genre-tracks_tracks-by-genre.json

Baselines have been created successfully!
Baseline files are stored in: regresql/baselines
&lt;/code&gt;
    &lt;p&gt;the test command not only tests the regressions to the captured times, but also detects the common bad patterns in query execution plans. For now it provides warnings for detection of sequential scans - both on their and/or with nested loops and multiple sort operations. I believe this alone might provide a valuable insights and reduce the mishaps in production. It's also a place where further development of RegreSQL will take place.&lt;/p&gt;
    &lt;p&gt;To demonstrate this, let's review the test output with the baselines.&lt;/p&gt;
    &lt;code&gt;Connecting to 'postgres://appuser:password123@192.168.139.28/cdstore_test'… ✓

Running regression tests...

✓ album-by-artist_list-albums-by-artist.1.json (0.00s)
✓ album-by-artist_list-albums-by-artist.2.json (0.00s)
✓ album-by-artist_list-albums-by-artist.1.cost (22.09 &amp;lt;= 22.09 * 110%) (0.00s)
  ⚠️  Sequential scan detected on table 'artist'
    Suggestion: Consider adding an index if this table is large or this query is frequently executed
  ⚠️  Nested loop join with sequential scan detected
    Suggestion: Add index on join column to avoid repeated sequential scans
✓ album-by-artist_list-albums-by-artist.2.cost (22.09 &amp;lt;= 22.09 * 110%) (0.00s)
  ⚠️  Sequential scan detected on table 'artist'
    Suggestion: Consider adding an index if this table is large or this query is frequently executed
  ⚠️  Nested loop join with sequential scan detected
    Suggestion: Add index on join column to avoid repeated sequential scans

✓ album-tracks_list-tracks-by-albumid.1.json (0.00s)
✓ album-tracks_list-tracks-by-albumid.2.json (0.00s)
✓ album-tracks_list-tracks-by-albumid.1.cost (8.23 &amp;lt;= 8.23 * 110%) (0.00s)
✓ album-tracks_list-tracks-by-albumid.2.cost (8.23 &amp;lt;= 8.23 * 110%) (0.00s)

✓ artist_top-artists-by-album.1.json (0.00s)
✓ artist_top-artists-by-album.1.cost (35.70 &amp;lt;= 35.70 * 110%) (0.00s)
  ⚠️  Multiple sequential scans detected on tables: album, artist
    Suggestion: Review query and consider adding indexes on filtered/joined columns

✓ genre-topn_genre-top-n.top-1.json (0.00s)
✓ genre-topn_genre-top-n.top-3.json (0.00s)
✓ genre-topn_genre-top-n.top-1.cost (6610.59 &amp;lt;= 6610.59 * 110%) (0.00s)
  ⚠️  Multiple sequential scans detected on tables: genre, artist
    Suggestion: Review query and consider adding indexes on filtered/joined columns
  ⚠️  Multiple sort operations detected (2 sorts)
    Suggestion: Consider composite indexes for ORDER BY clauses to avoid sorting
  ⚠️  Nested loop join with sequential scan detected
    Suggestion: Add index on join column to avoid repeated sequential scans
✓ genre-topn_genre-top-n.top-3.cost (6610.59 &amp;lt;= 6610.59 * 110%) (0.00s)
  ⚠️  Multiple sequential scans detected on tables: artist, genre
    Suggestion: Review query and consider adding indexes on filtered/joined columns
  ⚠️  Multiple sort operations detected (2 sorts)
    Suggestion: Consider composite indexes for ORDER BY clauses to avoid sorting
  ⚠️  Nested loop join with sequential scan detected
    Suggestion: Add index on join column to avoid repeated sequential scans

✓ genre-tracks_tracks-by-genre.json (0.00s)
✓ genre-tracks_tracks-by-genre.cost (37.99 &amp;lt;= 37.99 * 110%) (0.00s)
  ⚠️  Multiple sequential scans detected on tables: genre, track
    Suggestion: Review query and consider adding indexes on filtered/joined columns

Results: 16 passed (0.00s)
&lt;/code&gt;
    &lt;p&gt;As you can see, despite from not having baseline, RegreSQL is able to detect the basic bad patterns that should be addressed before queries can be considered "production ready".&lt;/p&gt;
    &lt;p&gt;In some cases, having the detection of sequential scans, or just tracking query costs baselines might be considered undesirable, which would lead to false positives. RegreSQL enables this to be addressed by query metadata as demonstrated below.&lt;/p&gt;
    &lt;code&gt;-- name: query_name
-- metadata: key1=value1, key2=value2
SELECT ...;
&lt;/code&gt;
    &lt;p&gt;At this point RegreSQL recognizes&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;notest&lt;/code&gt;to skip the query testing altogether (not just cost tracking)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;nobaseline&lt;/code&gt;to skip cost tracking&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;noseqscanwarn&lt;/code&gt;to keep cost tracking but disable sequential scan warnings&lt;/item&gt;
      &lt;item&gt;and &lt;code&gt;difffloattolerance&lt;/code&gt;to cost failure threshold (default 10% at the moment).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;-- name: query_name
-- regresql: notest, nobaseline
-- regresql: noseqscanwarn
-- regresql: difffloattolerance:0.25
-- query that can vary in cost by 20% without being considered a failure
SELECT ...;
&lt;/code&gt;
    &lt;head rend="h2"&gt;ORM enters the room&lt;/head&gt;
    &lt;p&gt;ORMs abstract away SQL, but they still generate it - and that generated SQL can have performance problems you won't catch until production. Consider this common scenario: you start with a simple SQLAlchemy query that works fine, then months later add eager loading for related data:&lt;/p&gt;
    &lt;code&gt;orders = (
    session.query(Order)
    .filter(Order.user_id == user_id)
    .options(
        joinedload(Order.user),
        joinedload(Order.shipping_address),
        selectinload(Order.items)  # NEW: Load order items
    )
    .all()
)
&lt;/code&gt;
    &lt;p&gt;That innocent &lt;code&gt;selectinload(Order.items)&lt;/code&gt; generates a separate query - and without an index on &lt;code&gt;order_items.order_id&lt;/code&gt;, it performs a sequential scan.&lt;/p&gt;
    &lt;p&gt;RegreSQL can catch this by intercepting ORM-generated SQL using SQLAlchemy's event system:&lt;/p&gt;
    &lt;code&gt;@event.listens_for(engine, "before_cursor_execute")
def capture_sql(conn, cursor, statement, *args):
    captured_queries.append(statement)
&lt;/code&gt;
    &lt;p&gt;Run your ORM code, capture the SQL, save it as a .sql file, and test it with RegreSQL. The performance baseline testing will flag the missing index before it hits production. This is currently experimental, but ORM integration is a key area for RegreSQL's future development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Test Data Management&lt;/head&gt;
    &lt;p&gt;Up until now we have covered how RegreSQL verifies query correctness and tracks performance regressions. But there's a critical prerequisite we've only skimmed through. Every regression test needs consistent, reproducible data. Change the data, change their cardinality, and your expected results become meaningless. Your performance baselines drift. Your tests become flaky.&lt;/p&gt;
    &lt;p&gt;Traditional approach to create test data might involve&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Database dumps become unmanageable - 500MB files you can't review, can't understand, that break with every schema migration, and whose data becomes stale as production evolves. Which version of the dump are your tests even using?&lt;/item&gt;
      &lt;item&gt;SQL scripts might be better than dumps, but still imperative and hard to maintain. You end up with INSERT statements scattered across multiple files, managing foreign keys manually, and debugging constraint violations.&lt;/item&gt;
      &lt;item&gt;Factories in application code might work great for integration tests, but we're testing SQL directly. Do you really want to maintain parallel data generation in your application language just for SQL tests?&lt;/item&gt;
      &lt;item&gt;Shared test database is the synonym for classic "works on my machine" problem. State leaks between tests. Parallel execution becomes impossible. Debugging is a nightmare.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What we need is something that's declarative (what data, not how to insert it), reproducible (similar data every time), composable (build complex scenarios from simple pieces), and scalable (from 10 rows to 100,000).&lt;/p&gt;
    &lt;p&gt;This is where next improvement in RegreSQL's fixture system comes in. Think of it as infrastructure-as-code for your test data. You describe the data you need in YAML files, and RegreSQL handles the rest - dependencies, cleanup, foreign keys, and even realistic data generation at scale.&lt;/p&gt;
    &lt;p&gt;RegreSQL's fixture system lets you define test data in YAML files stored in &lt;code&gt;regresql/fixtures/&lt;/code&gt;. Here's a simple example&lt;/p&gt;
    &lt;code&gt;  fixture: basic_users
  description: a handful of test users
  cleanup: rollback

  data:
    - table: users
      rows:
        - id: 1
          email: alice@example.com
          name: Alice Anderson
          created_at: 2024-01-15
        - id: 2
          email: bob@example.com
          name: Bob Builder
          created_at: 2024-02-20
&lt;/code&gt;
    &lt;p&gt;To use this fixture in your tests, reference it in the query's plan file (&lt;code&gt;regresql/plans/get-user.yaml&lt;/code&gt;) you can just reference the fixture&lt;/p&gt;
    &lt;code&gt;  fixtures:
    - basic_users

  "1":
    email: alice@example.com

  "2":
    email: bob@example.com
&lt;/code&gt;
    &lt;p&gt;And when you run &lt;code&gt;regresql test&lt;/code&gt;, the fixture is automatically loaded before the query executes, and cleaned up afterward. No manual setup scripts, no state leakage between tests. But it does not stop with static fixtures. When you want to test queries against realistic volumes you can use range of data generators including&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;sequences, random integer, decimal, string, uuid, email and name generators&lt;/item&gt;
      &lt;item&gt;date_between for generating random timestamps within a range&lt;/item&gt;
      &lt;item&gt;foreign key references to be able to reuse data from other table's fixtures&lt;/item&gt;
      &lt;item&gt;range to select value from predefined sources&lt;/item&gt;
      &lt;item&gt;Go template support&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; fixture: realistic_orders
  generate:
    - table: customers
      count: 1000
      columns:
        id:
          generator: sequence
          start: 1
        email:
          generator: email
          domain: shop.example.com
        name:
          generator: name
          type: full
        created_at:
          generator: date_between
          start: "2023-01-01"
          end: "2024-12-31"

    - table: orders
      count: 5000
      columns:
        id:
          generator: sequence
          start: 1
        customer_id:
          generator: int
          min: 1
          max: 1000
        amount:
          generator: decimal
          min: 10.00
          max: 999.99
          precision: 2
        order_date:
          generator: date_between
          start: "2023-01-01"
          end: "2024-12-31"
&lt;/code&gt;
    &lt;p&gt;This generates 1,000 customers and 5,000 orders with realistic-looking data - names, emails, dates, and amounts that feel production-like.&lt;/p&gt;
    &lt;p&gt;The fixtures are also stackable and can be build on top of each other. For example if you need to make sure users fixtures are created before orders fixtures, just declare the dependency (the already planned improvement is to include the support automatic foreign-key detection to avoid ID hard-coding). RegreSQL loads fixtures in dependency order and handles cleanup in reverse.&lt;/p&gt;
    &lt;code&gt;  fixture: orders_with_shipping
  depends_on:
    - basic_users

  data:
    - table: orders
      rows:
        - id: 101
          user_id: 1  # References Alice from basic_users
          total: 99.99
          status: shipped
&lt;/code&gt;
    &lt;p&gt;Should the available options for fixtures (manual data or data generators) not be enough, you always have options to use good old SQL based data generation.&lt;/p&gt;
    &lt;code&gt;  fixture: mixed_setup
  description: Combine SQL with YAML and generated data
  cleanup: rollback

  # SQL executes first (either as file or inline)
  sql:
    - file: sql/setup_schema.sql
    - inline: "INSERT INTO config (key, value) VALUES ('version', '1.0');"

  # followed YAML data
  data:
    - table: users
      rows:
        - id: 1
          email: admin@example.com

  # and finally generated data
  generate:
    - table: orders
      count: 100
      columns:
        id:
          generator: sequence
          start: 1
        user_id:
          generator: int
          min: 1
          max: 1
&lt;/code&gt;
    &lt;p&gt;RegreSQL provides commands to inspect and validate your fixtures&lt;/p&gt;
    &lt;code&gt;  # List all available fixtures
  regresql fixtures list

  # Show fixture details and dependencies
  regresql fixtures show realistic_orders

  # Validate fixture definitions
  regresql fixtures validate

  # Show dependency graph
  regresql fixtures deps

  # Apply fixture manually (for debugging)
  regresql fixtures apply basic_users
&lt;/code&gt;
    &lt;p&gt;The fixture system has been design to transforms test data from a maintenance burden into a documented, version-controlled process. Your YAML files become the single source of truth for what data your tests need, making it easy to understand test scenarios and maintain test data as the application evolves.&lt;/p&gt;
    &lt;head rend="h2"&gt;RegreSQL future&lt;/head&gt;
    &lt;p&gt;Introducing a new open source project is an ambitious goal, and RegreSQL is just starting up. Despite the fork being in works for almost 2 years. In coming weeks and months I plan further improvements, as well as better documentation and more tutorials. The project is maintained as part of my boringSQL brand, where it's vital component (together with pgTap) for building SQL Labs which (as I sincerely hope) will provide a foundation for its further development.&lt;/p&gt;
    &lt;p&gt;At the same time RegreSQL is an attempt to give back to welcoming PostgreSQL community, make developer user experience slightly better if possible and (just maybe) provide one more argument against the case that SQL queries are not testable.&lt;/p&gt;
    &lt;p&gt;RegreSQL is available at GitHub - feel free to open issue, or drop me email about the project at radim@boringsql.com or connect on LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://boringsql.com/posts/regresql-testing-queries/"/><published>2025-11-14T07:10:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45925431</id><title>V8 Garbage Collector</title><updated>2025-11-14T16:13:12.052258+00:00</updated><content>&lt;doc fingerprint="9a69d915f492a974"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;the last couple years in v8's garbage collector&lt;/head&gt;
    &lt;p&gt;Let’s talk about memory management! Following up on my article about 5 years of developments in V8’s garbage collector, today I’d like to bring that up to date with what went down in V8’s GC over the last couple years.&lt;/p&gt;
    &lt;head rend="h3"&gt;methodololology&lt;/head&gt;
    &lt;p&gt;I selected all of the commits to src/heap since my previous roundup. There were 1600 of them, including reverts and relands. I read all of the commit logs, some of the changes, some of the linked bugs, and any design document I could get my hands on. From what I can tell, there have been about 4 FTE from Google over this period, and the commit rate is fairly constant. There are very occasional patches from Igalia, Cloudflare, Intel, and Red Hat, but it’s mostly a Google affair.&lt;/p&gt;
    &lt;p&gt;Then, by the very rigorous process of, um, just writing things down and thinking about it, I see three big stories for V8’s GC over this time, and I’m going to give them to you with some made-up numbers for how much of the effort was spent on them. Firstly, the effort to improve memory safety via the sandbox: this is around 20% of the time. Secondly, the Oilpan odyssey: maybe 40%. Third, preparation for multiple JavaScript and WebAssembly mutator threads: 20%. Then there are a number of lesser side quests: heuristics wrangling (10%!!!!), and a long list of miscellanea. Let’s take a deeper look at each of these in turn.&lt;/p&gt;
    &lt;head rend="h3"&gt;the sandbox&lt;/head&gt;
    &lt;p&gt;There was a nice blog post in June last year summarizing the sandbox effort: basically, the goal is to prevent user-controlled writes from corrupting memory outside the JavaScript heap. We start from the assumption that the user is somehow able to obtain a write-anywhere primitive, and we work to mitigate the effect of such writes. The most fundamental way is to reduce the range of addressable memory, notably by encoding pointers as 32-bit offsets and then ensuring that no host memory is within the addressable virtual memory that an attacker can write. The sandbox also uses some 40-bit offsets for references to larger objects, with similar guarantees. (Yes, a sandbox really does reserve a terabyte of virtual memory).&lt;/p&gt;
    &lt;p&gt;But there are many, many details. Access to external objects is intermediated via type-checked external pointer tables. Some objects that should never be directly referenced by user code go in a separate “trusted space”, which is outside the sandbox. Then you have read-only spaces, used to allocate data that might be shared between different isolates, you might want multiple cages, there are “shared” variants of the other spaces, for use in shared-memory multi-threading, executable code spaces with embedded object references, and so on and so on. Tweaking, elaborating, and maintaining all of these details has taken a lot of V8 GC developer time.&lt;/p&gt;
    &lt;p&gt;I think it has paid off, though, because the new development is that V8 has managed to turn on hardware memory protection for the sandbox: sandboxed code is prevented by the hardware from writing memory outside the sandbox.&lt;/p&gt;
    &lt;p&gt;Leaning into the “attacker can write anything in their address space” threat model has led to some funny patches. For example, sometimes code needs to check flags about the page that an object is on, as part of a write barrier. So some GC-managed metadata needs to be in the sandbox. However, the garbage collector itself, which is outside the sandbox, can’t trust that the metadata is valid. We end up having two copies of state in some cases: in the sandbox, for use by sandboxed code, and outside, for use by the collector.&lt;/p&gt;
    &lt;p&gt;The best and most amusing instance of this phenomenon is related to integers. Google’s style guide recommends signed integers by default, so you end up with on-heap data structures with int32_t len and such. But if an attacker overwrites a length with a negative number, there are a couple funny things that can happen. The first is a sign-extending conversion to size_t by run-time code, which can lead to sandbox escapes. The other is mistakenly concluding that an object is small, because its length is less than a limit, because it is unexpectedly negative. Good times!&lt;/p&gt;
    &lt;head rend="h3"&gt;oilpan&lt;/head&gt;
    &lt;p&gt;It took 10 years for Odysseus to get back from Troy, which is about as long as it has taken for conservative stack scanning to make it from Oilpan into V8 proper. Basically, Oilpan is garbage collection for C++ as used in Blink and Chromium. Sometimes it runs when the stack is empty; then it can be precise. But sometimes it runs when there might be references to GC-managed objects on the stack; in that case it runs conservatively.&lt;/p&gt;
    &lt;p&gt;Last time I described how V8 would like to add support for generational garbage collection to Oilpan, but that for that, you’d need a way to promote objects to the old generation that is compatible with the ambiguous references visited by conservative stack scanning. I thought V8 had a chance at success with their new mark-sweep nursery, but that seems to have turned out to be a lose relative to the copying nursery. They even tried sticky mark-bit generational collection, but it didn’t work out. Oh well; one good thing about Google is that they seem willing to try projects that have uncertain payoff, though I hope that the hackers involved came through their OKR reviews with their mental health intact.&lt;/p&gt;
    &lt;p&gt;Instead, V8 added support for pinning to the Scavenger copying nursery implementation. If a page has incoming ambiguous edges, it will be placed in a kind of quarantine area for a while. I am not sure what the difference is between a quarantined page, which logically belongs to the nursery, and a pinned page from the mark-compact old-space; they seem to require similar treatment. In any case, we seem to have settled into a design that was mostly the same as before, but in which any given page can opt out of evacuation-based collection.&lt;/p&gt;
    &lt;p&gt;What do we get out of all of this? Well, not only can we get generational collection for Oilpan, but also we unlock cheaper, less bug-prone “direct handles” in V8 itself.&lt;/p&gt;
    &lt;p&gt;The funny thing is that I don’t think any of this is shipping yet; or, if it is, it’s only in a Finch trial to a minority of users or something. I am looking forward in interest to seeing a post from upstream V8 folks; whole doctoral theses have been written on this topic, and it would be a delight to see some actual numbers.&lt;/p&gt;
    &lt;head rend="h3"&gt;shared-memory multi-threading&lt;/head&gt;
    &lt;p&gt;JavaScript implementations have had the luxury of a single-threadedness: with just one mutator, garbage collection is a lot simpler. But this is ending. I don’t know what the state of shared-memory multi-threading is in JS, but in WebAssembly it seems to be moving apace, and Wasm uses the JS GC. Maybe I am overstating the effort here—probably it doesn’t come to 20%—but wiring this up has been a whole thing.&lt;/p&gt;
    &lt;p&gt;I will mention just one patch here that I found to be funny. So with pointer compression, an object’s fields are mostly 32-bit words, with the exception of 64-bit doubles, so we can reduce the alignment on most objects to 4 bytes. V8 has had a bug open forever about alignment of double-holding objects that it mostly ignores via unaligned loads.&lt;/p&gt;
    &lt;p&gt;Thing is, if you have an object visible to multiple threads, and that object might have a 64-bit field, then the field should be 64-bit aligned to prevent tearing during atomic access, which usually means the object should be 64-bit aligned. That is now the case for Wasm structs and arrays in the shared space.&lt;/p&gt;
    &lt;head rend="h3"&gt;side quests&lt;/head&gt;
    &lt;p&gt;Right, we’ve covered what to me are the main stories of V8’s GC over the past couple years. But let me mention a few funny side quests that I saw.&lt;/p&gt;
    &lt;head rend="h4"&gt;the heuristics two-step&lt;/head&gt;
    &lt;p&gt;This one I find to be hilariousad. Tragicomical. Anyway I am amused. So any real GC has a bunch of heuristics: when to promote an object or a page, when to kick off incremental marking, how to use background threads, when to grow the heap, how to choose whether to make a minor or major collection, when to aggressively reduce memory, how much virtual address space can you reasonably reserve, what to do on hard out-of-memory situations, how to account for off-heap mallocated memory, how to compute whether concurrent marking is going to finish in time or if you need to pause... and V8 needs to do this all in all its many configurations, with pointer compression off or on, on desktop, high-end Android, low-end Android, iOS where everything is weird, something called Starboard which is apparently part of Cobalt which is apparently a whole new platform that Youtube uses to show videos on set-top boxes, on machines with different memory models and operating systems with different interfaces, and on and on and on. Simply tuning the system appears to involve a dose of science, a dose of flailing around and trying things, and a whole cauldron of witchcraft. There appears to be one person whose full-time job it is to implement and monitor metrics on V8 memory performance and implement appropriate tweaks. Good grief!&lt;/p&gt;
    &lt;head rend="h4"&gt;mutex mayhem&lt;/head&gt;
    &lt;p&gt;Toon Verwaest noticed that V8 was exhibiting many more context switches on MacOS than Safari, and identified V8’s use of platform mutexes as the problem. So he rewrote them to use os_unfair_lock on MacOS. Then implemented adaptive locking on all platforms. Then... removed it all and switched to abseil.&lt;/p&gt;
    &lt;p&gt;Personally, I am delighted to see this patch series, I wouldn’t have thought that there was juice to squeeze in V8’s use of locking. It gives me hope that I will find a place to do the same in one of my projects :)&lt;/p&gt;
    &lt;head rend="h4"&gt;ta-ta, third-party heap&lt;/head&gt;
    &lt;p&gt;It used to be that MMTk was trying to get a number of production language virtual machines to support abstract APIs so that MMTk could slot in a garbage collector implementation. Though this seems to work with OpenJDK, with V8 I think the churn rate and laser-like focus on the browser use-case makes an interstitial API abstraction a lose. V8 removed it a little more than a year ago.&lt;/p&gt;
    &lt;head rend="h3"&gt;fin&lt;/head&gt;
    &lt;p&gt;So what’s next? I don’t know; it’s been a while since I have been to Munich to drink from the source. That said, shared-memory multithreading and wasm effect handlers will extend the memory management hacker’s full employment act indefinitely, not to mention actually landing and shipping conservative stack scanning. There is a lot to be done in non-browser V8 environments, whether in Node or on the edge, but it is admittedly harder to read the future than the past.&lt;/p&gt;
    &lt;p&gt;In any case, it was fun taking this look back, and perhaps I will have the opportunity to do this again in a few years. Until then, happy hacking!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wingolog.org/archives/2025/11/13/the-last-couple-years-in-v8s-garbage-collector"/><published>2025-11-14T09:53:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45925890</id><title>Show HN: Encore – Type-safe back end framework that generates infra from code</title><updated>2025-11-14T16:13:11.551158+00:00</updated><content>&lt;doc fingerprint="f167ae19572d887a"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt; Open Source Framework for creating type-safe distributed systems with declarative infrastructure&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Backend Frameworks: Encore.ts and Encore.go simplify creating microservices and type-safe APIs, and provide an AI-ready declarative approach to define infrastructure in code.&lt;/item&gt;
      &lt;item&gt;Local Development: Encore's CLI automatically manages local infrastructure and provides a development dashboard with tracing, service catalog, and architecture diagrams.&lt;/item&gt;
      &lt;item&gt;Infrastructure Integration: Simplified integration with cloud infrastructure using the open source CLI (learn more), or using the optional Encore Cloud platform to automate DevOps and infrastructure provisioning in your own cloud on AWS and GCP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;⭐ Star this repository to help spread the word.&lt;/p&gt;
    &lt;p&gt;Install Encore:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: &lt;code&gt;brew install encoredev/tap/encore&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux: &lt;code&gt;curl -L https://encore.dev/install.sh | bash&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows: &lt;code&gt;iwr https://encore.dev/install.ps1 | iex&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create your first app:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TypeScript: &lt;code&gt;encore app create --example=ts/hello-world&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Go: &lt;code&gt;encore app create --example=hello-world&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add Encore LLM instructions to your app:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encore.ts: ts_llm_instructions.txt&lt;/item&gt;
      &lt;item&gt;Encore.go: go_llm_instructions.txt&lt;/item&gt;
      &lt;item&gt;How to use: &lt;list rend="ul"&gt;&lt;item&gt;Cursor: Rename the file to &lt;code&gt;.cursorrules&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;GitHub Copilot: Paste content in &lt;code&gt;.github/copilot-instructions.md&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;For other tools, place the file in your app root.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Cursor: Rename the file to &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore's open source backend frameworks Encore.ts and Encore.go enable you to define resources like services, databases, cron jobs, and Pub/Sub, as type-safe objects in your application code.&lt;/p&gt;
    &lt;p&gt;With the frameworks you only define infrastructure semantics — the things that matter to your application's behavior — not configuration for specific cloud services. Encore parses your application and builds a graph of both its logical architecture and its infrastructure requirements, it then automatically generates boilerplate and helps orchestrate the relevant infrastructure for each environment. This means the same application code can be used to run locally, test in preview environments, and deploy to cloud environments on e.g. AWS and GCP.&lt;/p&gt;
    &lt;p&gt;This often removes the need for separate infrastructure configuration like Terraform, increases standardization in both your codebase and infrastructure, and makes your application highly portable across cloud providers.&lt;/p&gt;
    &lt;p&gt;Encore is fully open source, there is no proprietary code running in your application.&lt;/p&gt;
    &lt;p&gt;Defining microservices and API endpoints is incredibly simple—with less than 10 lines of code, you can create a production-ready, deployable service.&lt;/p&gt;
    &lt;p&gt;Hello World in Encore.ts&lt;/p&gt;
    &lt;code&gt;import { api } from "encore.dev/api";

export const get = api(
  { expose: true, method: "GET", path: "/hello/:name" },
  async ({ name }: { name: string }): Promise&amp;lt;Response&amp;gt; =&amp;gt; {
    const msg = `Hello ${name}!`;
    return { message: msg };
  }
);

interface Response {
  message: string;
}&lt;/code&gt;
    &lt;p&gt;Hello World in Encore.go&lt;/p&gt;
    &lt;code&gt;package hello

//encore:api public path=/hello/:name
func World(ctx context.Context, name string) (*Response, error) {
	msg := fmt.Sprintf("Hello, %s!", name)
	return &amp;amp;Response{Message: msg}, nil
}

type Response struct {
	Message string
}&lt;/code&gt;
    &lt;p&gt;If you want a Pub/Sub Topic, you declare it directly in your application code and Encore will integrate the infrastructure and generate the boilerplate code necessary. Encore supports the following Pub/Sub infrastructure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NSQ for local environments (automatically provisioned by Encore's CLI)&lt;/item&gt;
      &lt;item&gt;GCP Pub/Sub for environments on GCP&lt;/item&gt;
      &lt;item&gt;SNS/SQS for environments on AWS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using Pub/Sub in Encore.ts&lt;/p&gt;
    &lt;code&gt;import { Topic } "encore.dev/pubsub"

export interface SignupEvent {
    userID: string;
}

export const signups = new Topic&amp;lt;SignupEvent&amp;gt;("signups", {
    deliveryGuarantee: "at-least-once",
});&lt;/code&gt;
    &lt;p&gt;Using Pub/Sub in Encore.go&lt;/p&gt;
    &lt;code&gt;import "encore.dev/pubsub"

type User struct { /* fields... */ }

var Signup = pubsub.NewTopic[*User]("signup", pubsub.TopicConfig{
  DeliveryGuarantee: pubsub.AtLeastOnce,
})

// Publish messages by calling a method
Signup.Publish(ctx, &amp;amp;User{...})&lt;/code&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For more info: See example apps: Example Apps Repo&lt;/p&gt;
    &lt;p&gt;See products being build with Encore: Showcase&lt;/p&gt;
    &lt;p&gt;Have questions? Join the friendly developer community on Discord.&lt;/p&gt;
    &lt;p&gt;Talk to a human: Book a 1:1 demo with one of our founders.&lt;/p&gt;
    &lt;p&gt;Watch the intro video for a quick introduction to Encore concepts &amp;amp; code examples.&lt;/p&gt;
    &lt;p&gt;Building scalable applications with cloud services is powerful but often frustrating. Developers face complex setups and repetitive tasks that slow them down.&lt;/p&gt;
    &lt;p&gt;Encore solves this with an all-in-one backend development toolkit, streamlining everything from local testing to cloud integration and DevOps.&lt;/p&gt;
    &lt;p&gt;See how to use the backend frameworks in the docs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Services: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;APIs: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Databases: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Cron Jobs: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Pub/Sub: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Object Storage: Go / TypeScript&lt;/item&gt;
      &lt;item&gt;Caching: Go / TypeScript (Coming soon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore provides purpose-built tooling for each step in the development process, from local development and testing, to cloud DevOps. Here we'll cover the key features for each part of the process.&lt;/p&gt;
    &lt;p&gt;When you run your app locally using the Encore CLI, Encore parses your code and automatically sets up the necessary local infrastructure on the fly. No more messing around with Docker Compose!&lt;/p&gt;
    &lt;p&gt;You also get built-in tools for an efficient workflow when creating distributed systems and event-driven applications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local environment matches cloud: Encore automatically handles the semantics of service communication and interfacing with different types of infrastructure services, so that the local environment is a 1:1 representation of your cloud environment.&lt;/item&gt;
      &lt;item&gt;Cross-service type-safety: When building microservices applications with Encore, you get type-safety and auto-complete in your IDE when making cross-service API calls.&lt;/item&gt;
      &lt;item&gt;Type-aware infrastructure: With Encore, infrastructure like Pub/Sub queues are type-aware objects in your program. This enables full end-to-end type-safety when building event-driven applications.&lt;/item&gt;
      &lt;item&gt;Secrets management: Built-in secrets management for all environments.&lt;/item&gt;
      &lt;item&gt;Tracing: The local development dashboard provides local tracing to help understand application behavior and find bugs.&lt;/item&gt;
      &lt;item&gt;Automatic API docs &amp;amp; clients: Encore generates API docs and API clients in Go, TypeScript, JavaScript, and OpenAPI specification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here's a video showing the local development dashboard:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;localdashvideo.2.mp4&lt;/head&gt;
    &lt;p&gt;Encore comes with several built-in tools to help with testing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built-in service/API mocking: Encore provides built-in support for mocking API calls, and interfaces for automatically generating mock objects for your services.&lt;/item&gt;
      &lt;item&gt;Local test infra: When running tests locally, Encore automatically provides dedicated test infrastructure to isolate individual tests.&lt;/item&gt;
      &lt;item&gt;Local test tracing: The Local Development Dashboard provides distributed tracing for tests, providing great visibility into what's happening and making it easier to understand why a test failed.&lt;/item&gt;
      &lt;item&gt;Preview Environments: When using Encore Cloud (optional), it automatically provisions a temporary cloud-based Preview Environment for each Pull Request, an effective tool when doing end-to-end testing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore Cloud is Encore's managed service offering for teams wanting to focus their engineering effort on their product development, avoiding investing time in platformization and DevOps.&lt;/p&gt;
    &lt;p&gt;Encore Cloud provides automatic infrastructure provisioning in your cloud on AWS &amp;amp; GCP. So instead of writing Terraform, YAML, or clicking in cloud consoles, you connect your cloud account and simply deploy your application. Since using Encore's open source backend frameworks means your application code is cloud agnostic and not tied to any specific infrastructure services, Encore Cloud enables you to change your infrastructure depending on your evolving needs, without needing to make code changes or manually update infrastructure config files.&lt;/p&gt;
    &lt;p&gt;When you deploy, Encore Cloud automatically provisions infrastructure using battle-tested cloud services on AWS and GCP, such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compute: GCP Cloud Run, AWS Fargate, Kubernetes (GKE and EKS)&lt;/item&gt;
      &lt;item&gt;Databases: GCP Cloud SQL, AWS RDS&lt;/item&gt;
      &lt;item&gt;Pub/Sub: GCP Pub/Sub, AWS SQS/SNS&lt;/item&gt;
      &lt;item&gt;Caches: GCP Memorystore, Amazon ElastiCache&lt;/item&gt;
      &lt;item&gt;Object Storage: GCS, Amazon S3&lt;/item&gt;
      &lt;item&gt;Secrets: GCP Secret Manager, AWS Secrets Manager&lt;/item&gt;
      &lt;item&gt;Etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore Cloud also includes cloud versions of Encore's built-in development tools:&lt;/p&gt;
    &lt;p&gt;Here's a video showing the Encore Cloud dashboard:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;envs.mp4&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Faster Development: Encore streamlines the development process with its backend frameworks, clear abstractions, and built-in local development tools.&lt;/item&gt;
      &lt;item&gt;Scalability &amp;amp; Performance: Encore simplifies building large-scale microservices applications that can handle growing user bases and demands, without the normal boilerplate and complexity.&lt;/item&gt;
      &lt;item&gt;Control &amp;amp; Standardization: Built-in tools like automated architecture diagrams, infrastructure tracking and approval workflows, make it easy for teams and leaders to get an overview of the entire application.&lt;/item&gt;
      &lt;item&gt;Security &amp;amp; Compliance: Encore Cloud helps ensure your application is secure and compliant by enforcing security standards like least privilege IAM, and provisioning infrastructure according to best practices for each cloud provider.&lt;/item&gt;
      &lt;item&gt;Reduced Costs: Encore Cloud's automatic infrastructure management minimizes wasteful cloud expenses and reduces DevOps workload, allowing you to work more efficiently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encore is designed to give teams a productive and less complex experience when solving most backend use cases. Many teams use Encore to build things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High-performance B2B Platforms&lt;/item&gt;
      &lt;item&gt;Fintech &amp;amp; Consumer apps&lt;/item&gt;
      &lt;item&gt;Global E-commerce marketplaces&lt;/item&gt;
      &lt;item&gt;Microservices backends for SaaS applications and mobile apps&lt;/item&gt;
      &lt;item&gt;And much more...&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1. Install Encore: &lt;list rend="ul"&gt;&lt;item&gt;macOS: &lt;code&gt;brew install encoredev/tap/encore&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Linux: &lt;code&gt;curl -L https://encore.dev/install.sh | bash&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Windows: &lt;code&gt;iwr https://encore.dev/install.ps1 | iex&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;macOS: &lt;/item&gt;
      &lt;item&gt;2. Create your first app: &lt;list rend="ul"&gt;&lt;item&gt;TypeScript: &lt;code&gt;encore app create --example=ts/introduction&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Go: &lt;code&gt;encore app create --example=hello-world&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;TypeScript: &lt;/item&gt;
      &lt;item&gt;3. Star the project on GitHub to stay up-to-date&lt;/item&gt;
      &lt;item&gt;4. Explore the Documentation to learn more about Encore's features&lt;/item&gt;
      &lt;item&gt;5. Join Discord to ask questions and meet other Encore developers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything needed to develop and deploy Encore applications is Open Source, including the backend frameworks, parser, compiler, runtime, and CLI. This includes all code needed for local development and everything that runs in your application when it is deployed.&lt;/p&gt;
    &lt;p&gt;The Open Source CLI also provides a mechanism to generate a Docker images for your application, so you easily self-host your application. Learn more in the docs.&lt;/p&gt;
    &lt;p&gt;Developers building with Encore are forward-thinkers who want to focus on creative programming and building great software to solve meaningful problems. It's a friendly place, great for exchanging ideas and learning new things! Join the conversation on Discord.&lt;/p&gt;
    &lt;p&gt;We rely on your contributions and feedback to improve Encore for everyone who is using it. Here's how you can contribute:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;⭐ Star and watch this repository to help spread the word and stay up to date.&lt;/item&gt;
      &lt;item&gt;Meet fellow Encore developers and chat on Discord.&lt;/item&gt;
      &lt;item&gt;Follow Encore on Twitter.&lt;/item&gt;
      &lt;item&gt;Share feedback or ask questions via email.&lt;/item&gt;
      &lt;item&gt;Leave feedback on the Public Roadmap.&lt;/item&gt;
      &lt;item&gt;Send a pull request here on GitHub with your contribution.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intro: Encore concepts &amp;amp; features&lt;/item&gt;
      &lt;item&gt;Demo video: Getting started with Encore.ts&lt;/item&gt;
      &lt;item&gt;Demo: Building and deploying a simple Go service&lt;/item&gt;
      &lt;item&gt;Demo: Building an event-driven system in Go&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;Encore.example.1.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;indexvideo_5.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;index_1.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;envs.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;indexxxxx.mp4&lt;/head&gt;
    &lt;p&gt;Encore was founded by long-time backend engineers from Spotify, Google, and Monzo with over 50 years of collective experience. We’ve lived through the challenges of building complex distributed systems with thousands of services, and scaling to hundreds of millions of users.&lt;/p&gt;
    &lt;p&gt;Encore grew out of these experiences and is a solution to the frustrations that came with them: unnecessary crippling complexity and constant repetition of undifferentiated work that suffocates the developer’s creativity. With Encore, we want to set developers free to achieve their creative potential.&lt;/p&gt;
    &lt;p&gt;For individual developers building for the cloud, Encore provides a radically improved experience. With Encore you’re able to stay in the flow state and experience the joy and creativity of building.&lt;/p&gt;
    &lt;p&gt;For startup teams who need to build a scalable backend to support the growth of their product, Encore lets them get up and running in the cloud within minutes. It lets them focus on solving the needs of their users, instead of spending most of their time re-solving the everyday challenges of building distributed systems in the cloud.&lt;/p&gt;
    &lt;p&gt;For individual teams in large organizations that want to focus on innovating and building new features, Encore lets them stop spending time on operations and onboarding new team members. Using Encore for new feature development is easy, just spin up a new backend service in a few minutes.&lt;/p&gt;
    &lt;p&gt;Encore is the only tool that understands what you’re building. Encore uses static analysis to deeply understand the application you’re building. This enables a unique developer experience that helps you stay in the flow as you’re building. For instance, you don't need to bother with configuring and managing infrastructure, setting up environments and keeping them in sync, or writing documentation and drafting architecture diagrams. Encore does all of this automatically out of the box.&lt;/p&gt;
    &lt;p&gt;We've found that to meaningfully improve the developer experience, you have to operate across the full stack. Unless you understand how an application is deployed, there are a large number of things in the development process that you can't simplify. That's why so many other developer tools have such a limited impact. With Encore's more integrated approach, we're able to unlock a radically better experience for developers.&lt;/p&gt;
    &lt;p&gt;Encore is designed to let you go outside of the framework when you want to, and easily drop down in abstraction level when you need to, so you never run into any dead-ends.&lt;/p&gt;
    &lt;p&gt;Should you want to migrate away, it's straightforward and does not require a big rewrite. 99% of your code is regular Go or TypeScript.&lt;/p&gt;
    &lt;p&gt;Encore provides tools for self-hosting your application, by using the Open Source CLI to produce a standalone Docker image that can be deployed anywhere you'd like.&lt;/p&gt;
    &lt;p&gt;See CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/encoredev/encore"/><published>2025-11-14T11:41:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45925996</id><title>Show HN: European Tech News in 6 Languages</title><updated>2025-11-14T16:13:10.539145+00:00</updated><content>&lt;doc fingerprint="ed925a513922ac49"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;News&lt;/head&gt;
    &lt;p&gt;Daily digest of all European digital development news&lt;/p&gt;
    &lt;head rend="h3"&gt;Verifactu is driving half of Spain crazy, but there's good news: freelancers with simple invoices are exempt&lt;/head&gt;
    &lt;p&gt;Spanish businesses must implement Verifactu, a new digital invoicing system by January 1, 2026, to combat fraud. Freelancers using simple invoices created with programs like Word or Excel are exempt from the requirement.&lt;/p&gt;
    &lt;head rend="h3"&gt;Holo raises €1 million to bring personalised lab testing and daily-life health tracking to more users&lt;/head&gt;
    &lt;p&gt;Barcelona-based Holo secured €1 million in pre-Seed funding to launch its health app that combines AI, lab testing, and health tracking. The app integrates with devices like Apple Watch and Garmin to give users more control over their well-being.&lt;/p&gt;
    &lt;head rend="h3"&gt;Being able to talk on WhatsApp without using WhatsApp: it's the app's most important change and it's already underway&lt;/head&gt;
    &lt;p&gt;WhatsApp launches interoperability in Europe, allowing users to send and receive messages from other messaging apps. This move follows the EU's Digital Markets Act, forcing the platform to open up to third-party integrations.&lt;/p&gt;
    &lt;head rend="h3"&gt;WhatsApp in Europe can now chat with other applications&lt;/head&gt;
    &lt;p&gt;WhatsApp is rolling out third-party chats in Europe, allowing users to connect with individuals on other messaging services. This move follows new rules requiring interoperability.&lt;/p&gt;
    &lt;head rend="h3"&gt;With 300k add-to-cart events, Paris-based Dialog secures €3.7 million round to scale its AI shopping agent&lt;/head&gt;
    &lt;p&gt;Paris-based Dialog secured a €3. 7 million Seed round to scale its AI-powered shopping agent trained on brand data for e-commerce....&lt;/p&gt;
    &lt;head rend="h3"&gt;The EU already knows how to curb the dominance of Temu and Shein in e-commerce: small packages&lt;/head&gt;
    &lt;p&gt;Brussels aims to curtail the dominance of e-commerce giants like Temu and Shein by ending tax exemptions on small packages. The EU plans to eliminate the exemption for packages under €150, potentially by 2026, which previously cost the EU €1....&lt;/p&gt;
    &lt;head rend="h3"&gt;Rookoo, out of Belgium, raises €900k to help the global event industry escape administrative chaos&lt;/head&gt;
    &lt;p&gt;Belgian startup Rookoo secured €900k in funding to expand its digital platform for events and hospitality teams, aiming to automate administrative tasks. This investment follows a trend of nearly €79 million in funding for European hospitality tech, including AI-driven solutions, in 2025.&lt;/p&gt;
    &lt;head rend="h3"&gt;WhatsApp is launching third-party chat integration in Europe&lt;/head&gt;
    &lt;p&gt;Meta is set to launch third-party chat integration within WhatsApp across Europe, complying with the Digital Markets Act. Users will soon be able to message with apps like BirdyChat and Haiket, with end-to-end encryption maintained.&lt;/p&gt;
    &lt;head rend="h3"&gt;Internet consultation new cyber rules started&lt;/head&gt;
    &lt;p&gt;Brussels is launching a public consultation on new cybersecurity rules for the government, open to citizens, businesses, and experts. The consultation period will run from November 11, 2025, to December 23, 2025, as part of the Cyber Security Act.&lt;/p&gt;
    &lt;head rend="h3"&gt;New data-driven report highlights the AI opportunity for Public Sector Services&lt;/head&gt;
    &lt;p&gt;The EU-funded report highlights how AI can transform public services and create opportunities for startups across the EU. Between 2021 and 2024, venture capital investment in AI-powered public sector technologies soared, with nearly 50% of deals in 2024 involving AI-first GovTech startups.&lt;/p&gt;
    &lt;head rend="h3"&gt;Merz: Huawei and Co. are banned from the outset in 6G networks&lt;/head&gt;
    &lt;p&gt;Germany will ban Chinese suppliers like Huawei from its future 6G networks, ensuring they have no place in the country’s telecom infrastructure. This decision aims to establish planning security for the next generation of mobile networks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Digital Sovereignty: Think Tank Recommends More Investment in Big Tech Alternatives&lt;/head&gt;
    &lt;p&gt;A think tank urges the German government to boost investment in open-source platforms as an alternative to Big Tech. The call for action aims to shift open networks, like the Fediverse, from niche status to mainstream adoption, promoting digital sovereignty.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trade Republic launches Crypto Wallet, its crypto wallet: 4 things to know&lt;/head&gt;
    &lt;p&gt;Trade Republic launches its "Crypto Wallet," offering access to nearly 50 cryptocurrencies with regulated banking protection. The move expands digital asset access for over 10 million users across 18 countries.&lt;/p&gt;
    &lt;head rend="h3"&gt;Irish sales planning and intelligence platform Lative raises $7.5m&lt;/head&gt;
    &lt;p&gt;Irish sales planning platform Lative secured $7. 5M to boost its product development and expand its market reach....&lt;/p&gt;
    &lt;head rend="h3"&gt;With users reporting 24% productivity gains, Lative secures €6.4 million to scale its AI-driven sales planning tool&lt;/head&gt;
    &lt;p&gt;Dublin-based Lative secured €6. 4 million to scale its AI-driven sales planning platform, boosting product development and go-to-market efforts....&lt;/p&gt;
    &lt;head rend="h3"&gt;Swiss startup Forgis raises €3.8 million to automate industrial machines as Europe confronts Asia’s manufacturing lead&lt;/head&gt;
    &lt;p&gt;Forgis, a Swiss startup, secured €3. 8 million to automate industrial machines using software in the automotive and manufacturing sectors....&lt;/p&gt;
    &lt;head rend="h3"&gt;They stole almost 23,000 euros with the SIM swapping scam. Now Vodafone and Ibercaja will have to return it&lt;/head&gt;
    &lt;p&gt;Vodafone and Ibercaja were ordered to refund nearly €23,000 to a customer scammed via SIM swapping. The court ruled the companies were liable after fraudsters duplicated the victim's SIM and stole the money.&lt;/p&gt;
    &lt;head rend="h3"&gt;The anti-abuse bracelets were going to be a technical solution to a social problem. They are generating a chaos of incidents&lt;/head&gt;
    &lt;p&gt;The Spanish Cometa system, managing anti-abuse bracelets, suffered a technical incident causing service overload and triggering an emergency protocol. The issue affected the safety of approximately 4,500 women using the devices, with roughly 10% of alerts causing system failures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Digital Markets Act: EU Commission Accuses Google of Discrimination Against News Sites&lt;/head&gt;
    &lt;p&gt;Brussels launched a new investigation into Google's parent company, Alphabet, for potential Digital Markets Act violations. The EU suspects Google's search results may discriminate against news websites, impacting their visibility.&lt;/p&gt;
    &lt;head rend="h3"&gt;FMC raises €100M as it unveils new class of memory chips for the AI era&lt;/head&gt;
    &lt;p&gt;FMC secured €100 million to revolutionize memory chips, aiming for faster, more energy-efficient AI data centers. The funding, led by HV Capital and DTCF, will help the company compete with global chip manufacturers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://europedigital.cloud/en/news"/><published>2025-11-14T12:00:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926037</id><title>Magit manuals are available online again</title><updated>2025-11-14T16:13:09.547436+00:00</updated><content>&lt;doc fingerprint="b7accebf398fa351"&gt;
  &lt;main&gt;
    &lt;p&gt;You signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert&lt;/p&gt;
    &lt;p&gt;I'm just getting started with Magit, and it works in my Emacs, but I wanted to go over some tutorials from the webpage. magit.vc does not work currently.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/magit/magit/issues/5472"/><published>2025-11-14T12:09:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926105</id><title>Arrival Radar</title><updated>2025-11-14T16:13:09.155709+00:00</updated><content>&lt;doc fingerprint="b6db4d55bded2b1d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Arrival Radar&lt;/head&gt;
    &lt;p&gt;I think the Pico-8 is great. Other fantasy consoles are also great, but the Pico-8 has a special place in my heart. It’s the complete package: console, development environment, resource editing tools, community, etc. The Pico-8 really captures the spirit of the tools I learned programming on1 Game Maker 4.3–5.3, which was far different from the overcommercialised thing Game Maker is today., in a way few other things do.&lt;/p&gt;
    &lt;p&gt;I wish I had more time to play around with it. I did however take a couple of nights a little while back to create a game I’ve always wanted to have: a minimalistic atc simulator.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;You can play Arrival Radar here!&lt;/p&gt;
    &lt;head rend="h1"&gt;Arrival Radar&lt;/head&gt;
    &lt;p&gt;Your objective is to land planes as quickly as possible while maintaining separation between them, and you do this by assigning them routes that take them to the final approach. That’s all you can do, in contrast to other atc simulators which let you vector planes directly, assign altitudes and speeds, etc. This constraint makes for some challenge, because planes arrive randomly and tend to end up violating separation minima unless you assign routes cleverly and actively.&lt;/p&gt;
    &lt;p&gt;In order to prevent conflicts, you’ll have to be deft at re-assigning routes and asking planes to join their routes at specific waypoints. To help you in your mission, the planes do slow down a little if there is other traffic ahead of them, and there are speed restrictions on waypoints closer to the airport to make higher density possible where the routes come together.&lt;/p&gt;
    &lt;p&gt;It’s fun and addicting! Click the link above to read more about the game and how to play.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/arrival-radar"/><published>2025-11-14T12:23:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926224</id><title>Winamp for OS/X</title><updated>2025-11-14T16:13:08.760798+00:00</updated><content>&lt;doc fingerprint="6cc6844dbf363dae"&gt;
  &lt;main&gt;
    &lt;p&gt;A native macOS application that recreates the classic Winamp experience for playing MP3 and FLAC audio files.&lt;/p&gt;
    &lt;p&gt;If you enjoy using Winamp macOS and would like to support its development, consider buying me a coffee:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;🎵 MP3 and FLAC playback support&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🎨 Winamp-inspired UI&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;📝 Playlist management / M3U&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;⏯️ Full playback controls (play, pause, stop, next, previous)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;📊 Spectrum analyzer visualization&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🎚️ 10-band equalizer&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🔍 File browser with drag-and-drop support&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple oscilliscope visualizations&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Milkdrop (click on the icon in the main app) - supports fullscreen mode&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lyrics overlay in Milkdrop&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 or later&lt;/item&gt;
      &lt;item&gt;Xcode 15.0 or later&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open &lt;code&gt;Winamp.xcodeproj&lt;/code&gt;in Xcode&lt;/item&gt;
      &lt;item&gt;Select the Winamp scheme&lt;/item&gt;
      &lt;item&gt;Build and run (⌘R)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;alternatively&lt;/p&gt;
    &lt;code&gt;./build.sh --run&lt;/code&gt;
    &lt;code&gt;swift build
swift run&lt;/code&gt;
    &lt;p&gt;alternatively&lt;/p&gt;
    &lt;code&gt;./build.sh --release&lt;/code&gt;
    &lt;p&gt;MIT License - Feel free to use and modify as needed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mgreenwood1001/winamp"/><published>2025-11-14T12:44:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926263</id><title>EDE: Small and Fast Desktop Environment</title><updated>2025-11-14T16:13:03.117293+00:00</updated><content>&lt;doc fingerprint="36df547a55af023e"&gt;
  &lt;main&gt;
    &lt;p&gt;small and fast desktop environment&lt;/p&gt;
    &lt;p&gt;EDE is small desktop environment built to be responsive, light in resource usage and to have familiar look and feel. It runs on Linux, *BSD, Solaris, Minix, Zaurus and even on XBox.&lt;/p&gt;
    &lt;p&gt;or you can browse for older releases.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://edeproject.org/"/><published>2025-11-14T12:51:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926383</id><title>Backblaze Drive Stats for Q3 2025</title><updated>2025-11-14T16:13:02.611955+00:00</updated><content>&lt;doc fingerprint="5654bed345af2d9d"&gt;
  &lt;main&gt;
    &lt;p&gt;Every quarter, Drive Stats gives us the numbers. This quarter, it gave us a crisis of meaning. What does it really mean for a hard drive to fail? Is it the moment the lights go out, or the moment we decide they have? Philosophers might call that an ontological gray area. We just call it Q3.&lt;/p&gt;
    &lt;p&gt;As of June 30, 2025, we had 332,915 drives under management. Of that total, there were 3,970 boot drives and 328,348 data drives. Let’s dig into our stats, then talk about the meaning of failure.&lt;/p&gt;
    &lt;head rend="h4"&gt;This quarter, we have more to talk about (Stats-wise)&lt;/head&gt;
    &lt;p&gt;Drive Stats was the beginning. Want to see more of the full picture? Check out the Stats Lab webinar, bringing together content from all of our Stats articles. We’re going to chat about all things Backblaze (and beyond)—by the numbers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Drive Stats: The digest version&lt;/head&gt;
    &lt;head rend="h2"&gt;Q3 2025 hard drive failure rates&lt;/head&gt;
    &lt;p&gt;During Q3 2025, we were tracking 328,348 storage drives. Here are the numbers:&lt;/p&gt;
    &lt;p&gt;Backblaze Hard Drive Failure Rates for Q3 2025&lt;/p&gt;
    &lt;p&gt;Reporting period July 1, 2025–September 30, 2025 inclusive&lt;lb/&gt;Drive models with drive count &amp;gt; 100 as of July 1, 2025 and drive days &amp;gt; 10,000 in Q3 2025&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes and observations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The failure rate has increased: The failure rate has changed, and by quite a bit. As a reminder, last quarter’s AFR was 1.36% compared with this quarter’s 1.55%. (Interestingly, the 2024 yearly AFR was 1.57%.)&lt;/item&gt;
      &lt;item&gt;That new drive energy: Say hello to the 24TB Toshiba MG11ACA24TE, joining the drive pool with 2,400 drives and 24,148 drive days. That means that we’ve hit the thresholds for the quarterly stats, but not the lifetime.&lt;/item&gt;
      &lt;item&gt;The zero failure club: It was a big month for the zero failure club, with four drives making the cut: &lt;list rend="ul"&gt;&lt;item&gt;Seagate HMS5C4040BLE640 (4TB)&lt;/item&gt;&lt;item&gt;Seagate ST8000NM000A (8TB)&lt;/item&gt;&lt;item&gt;Toshiba MG09ACA16TE (16TB)&lt;/item&gt;&lt;item&gt;Toshiba MG11ACA24TE (24TB)—and yes, that’s the new drive.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For those of you tracking the stats closely, you’ll notice that the Seagate ST8000NM000A (8TB) is a frequent flier on this list. The last time it had a failure was in Q3 2024—and it was just a single failure for the whole quarter!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The highest AFRs were really high: The high end was so high that this month, it inspired us to run an outlier analysis using the standard quartile analysis (Tukey method). Based on that information, any drive with a quarterly AFR higher than 5.88% is an outlier, and there are three: &lt;list rend="ul"&gt;&lt;item&gt;Seagate ST10000NM0086 (10TB): 7.97%&lt;/item&gt;&lt;item&gt;Seagate ST14000NM0138 (14TB): 6.86%&lt;/item&gt;&lt;item&gt;Toshiba MG08ACA16TEY (16TB): 16.95%&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s going on there? Great question, and we’ll get into that after the lifetime failure rates.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lifetime hard drive failure rates&lt;/head&gt;
    &lt;p&gt;To be considered for the lifetime review, a drive model was required to have 500 or more drives as of the end of Q2 2025 and have over 100,000 accumulated drive days during their lifetime. When we removed those drive models which did not meet the lifetime criteria, we had drives grouped into 27 models remaining for analysis as shown in the table below.&lt;/p&gt;
    &lt;p&gt;Backblaze Hard Drive Failure Rates for Q2 2025&lt;/p&gt;
    &lt;p&gt;Reporting period ending September 30, 2025&lt;lb/&gt;Drive models &amp;gt; 500 drives and &amp;gt; 100,000 lifetime drive days&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes and observations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;That lifetime AFR is pretty consistent, isn’t it? The lifetime AFR is 1.31%. Last quarter we reported that it was 1.30%, and the quarter before that, it was 1.31%.&lt;/item&gt;
      &lt;item&gt;The 4TB average age hasn’t shifted: As we’ve reported on previously, the 4TB drives are being decommissioned over time. Now, we’re down to just a handful left—just 11 of the ALE models and 187 of the BLE models. But, because their lifetime populations are so comparatively large, the additional drive days aren’t enough to move the needle on the average age in months. So, no ghosts in the machine here, and decommissioning is proceeding as planned.&lt;/item&gt;
      &lt;item&gt;Steady uptick in higher capacity drives: Of the 20TB+ drives that meet our lifetime data parameters, we’ve added 7,936 since last quarter. And, don’t forget that our newest entrée to the cohort, the Toshiba MG11ACA24TE (24TB), hasn’t made its way to this table yet—that adds an additional 2,400 drive models. All together, the 20TB+ club represents 67,939 drives, or about 21% of the drive pool.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Defining a failure—from a technical perspective&lt;/head&gt;
    &lt;p&gt;A question that’s come up a few times when we’re hosting a webinar or chatting in the comments section is how we define a failure. While it may seem intuitive, it’s actually something of a meaty conundrum, and something we haven’t addressed since the early days of this series. Tracking down the answer to this question touches internal drive fleet monitoring tools (via SMART stats), the actual Drive Stats collection program, and our data engineering layer. I’ll dig into each of these in detail, then we’ll take a look at the outliers for this quarter.&lt;/p&gt;
    &lt;head rend="h3"&gt;SMART stats reporting&lt;/head&gt;
    &lt;p&gt;We use Smartmontools to collect the SMART attributes of drives, and another monitoring tool called drive sentinel to flag read/write errors that exceed a certain threshold as well as some other anomalies.&lt;/p&gt;
    &lt;p&gt;The main indicator we use for determining if a drive should be replaced is when it responds to reads with uncorrectable medium errors. When a drive reads the data from the disk, but the data fails its integrity check, the drive will try to reconstruct the data using internal error correction codes. If it is unable to reconstruct the data, it notifies the host by reporting it as an uncorrectable error and marks that part of the disk as pending reallocation, which shows up in SMART under an attribute like &lt;code&gt;Current_Pending_Sector&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;On Storage Pods that control drives through SATA links, the drive sentinel will count the number of these uncorrectable errors a drive reports and if it exceeds a threshold, access to the drive will be removed. This is important in the classic Backblaze Storage Pods where five drives share a single SATA link and errors by one drive will affect all drives on the link.&lt;/p&gt;
    &lt;p&gt;On Dell and SMCI pods that use a SAS topology to connect drives, drive sentinel doesn’t remove access to drives because the errors are reported differently; but, that’s also not as critical since SAS minimizes the impact that a problem disk can have on others.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Drive Stats program&lt;/head&gt;
    &lt;p&gt;We’ve talked about the custom program we use to collect Drive Stats in the past, and here’s a quick recap:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The podstats generator runs on every Storage Pod, what we call any host that holds customer data, every few minutes. It’s a C++ program that collects SMART stats and a few other attributes, then converts them into an .xml file (“podstats”). Those are then pushed to a central host in each datacenter and bundled. Once the data leaves these central hosts, it has entered the domain of what we will call Drive Stats.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For this program, the logic is relatively simple: A failure in Drive Stats occurs when a drive vanishes out of the reporting population. It is considered “failed” until it shows up again. Drives are tracked by serial number and we report daily logs on a per-drive basis, so truly, we can get pretty granular here.&lt;/p&gt;
    &lt;head rend="h3"&gt;The data engineering layer&lt;/head&gt;
    &lt;p&gt;To recap, we’ve collected our SMART stats and compiled them with the podstats program. Now we’ve got all the information, and data intelligence needs to add the context. A drive may go offline for a day or so (not return a response to those tools that collect daily logs of SMART stats), but it could be something as simple as a loose cable. So, time-wise, if a drive reappears after one day or 30, at what point in that period of time do we classify it as an official failure?&lt;/p&gt;
    &lt;p&gt;Previously, we manually cross-referenced data center work tickets, but these days, we’ve automated that process. On the backend, it’s a SQL query, but in human speak, this is what it comes down to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If a drive logs data on the last day of the selection period (which in this case is a quarter) then it has not failed.&lt;/item&gt;
      &lt;item&gt;There are three human-curated tables that the query cross references. If a drive serial number appears on one of them, it tells us whether there’s a failure or not (depending on the table’s function).&lt;/item&gt;
      &lt;item&gt;If the drive serial number is the primary serial number in a drive replacement Jira ticket then it has failed. (Jira is where we track our data center work tickets.)&lt;/item&gt;
      &lt;item&gt;If the drive serial number is the target serial number in a clone Jira ticket or a (temp) replacement ticket, then it has not failed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Basically, when we go to write the Drive Stats reports at the end of the quarter, if a drive has either appeared in one of our various work trackers or hasn’t re-entered the population, then it’s considered failed.&lt;/p&gt;
    &lt;p&gt;In rare instances, that can mean that we have so-called “cosmetic” failures when we have some work we’re doing on a drive model that lasts more than that quarterly collection period. And, spoiler, we have one of those instances that showed up in the data this month—our outlier Toshiba drive with the 16.9% failure rate. We’ll dig in in just a minute; but first, some context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Connecting drive failure to overall picture of the drive pool&lt;/head&gt;
    &lt;p&gt;As we mentioned above, certain drives in the pool had such high swings in AFR that we ended up running an outlier analysis using the quartile method. (It’s also worth mentioning that a cluster analysis could potentially be a better fit, but we can save that for another day.) Based on that analysis, anything that has above a 5.88% failure rate is an outlier.&lt;/p&gt;
    &lt;p&gt;The primary motivation was inspired by an attempt to visualize the relationship between the age in months of a drive versus this quarter’s AFRs.&lt;/p&gt;
    &lt;p&gt;And yes, we’re fully aware that that’s a… super unreadable scatter plot. Removing the labels, this is a bit better:&lt;/p&gt;
    &lt;p&gt;We’re interested, really, in the shape of the relationship. If we posit that the older drives get, the higher their failure rates, you’d expect a larger concentration in the top right quadrant. But, our data follows a much more interesting pattern than that, with most of our data points concentrated in the lowest regions of the graph regardless of age—something you’d expect from a set of data that reflects a bunch of smart folks actively working towards the goal of a healthy drive population. And yet, we have some data points that break the mold.&lt;/p&gt;
    &lt;p&gt;As is pretty intuitive to my business intelligence folks in the audience, the process of identifying outliers is actionable data as well. Just like all press is good press; in our world, more data is more better. So, let’s take a closer look at those outliers. As a reminder, that’s these three drive models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seagate ST10000NM0086 (10TB): 7.97%&lt;/item&gt;
      &lt;item&gt;Seagate ST14000NM0138 (14TB): 6.86%&lt;/item&gt;
      &lt;item&gt;Toshiba MG08ACA16TEY (16TB): 16.95%&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Seagate ST10000NM0086 (10TB)&lt;/head&gt;
    &lt;p&gt;This drive has some pretty explainable factors for the high failure rate. It’s well over seven years old (92.35 months). And, since it only has 1,018 drive models in operation, single failures hold a lot of weight compared with the average drive count per model—which comes in at 10,952 if you use the mean of this quarterly data and 6,177 if you use the median.&lt;/p&gt;
    &lt;p&gt;And, you can see that borne out in the trend in the last year of data:&lt;/p&gt;
    &lt;head rend="h3"&gt;Seagate ST14000NM0138 (14TB)&lt;/head&gt;
    &lt;p&gt;This drive is nearing five years in age (56.57 months) and, again, has a lower drive count at 1,286. More importantly, this particular drive model has had historically high failure rates. In parallel with above, here’s the last year of quarterly failure rates:&lt;/p&gt;
    &lt;head rend="h3"&gt;Toshiba MG08ACA16TEY (16TB)&lt;/head&gt;
    &lt;p&gt;Finally, our Toshiba model is the most interesting of all. It’s less than four years old (44.61 months), and has 5,145 drives in the pool. And, this quarter is clearly a change from its normal, decent, AFRs.&lt;/p&gt;
    &lt;p&gt;When we see deviations like this one, it’s usually an indication that there’s something afoot.&lt;/p&gt;
    &lt;p&gt;Never fear, Drive Stats fans; this was a known quantity before we went on this journey. This past quarter, working with Toshiba, we deployed some firmware updates they provided to optimize performance on these drives. Because we needed to pull drives to achieve this in some cases, we had an abnormal number of “failed” drives in this population.&lt;/p&gt;
    &lt;p&gt;What that means for this drive is that it’s actually not a bad drive model; and, given the ways we and Toshiba have worked together on a fix, we should see failure rates normalizing in the near future. And, this also goes back to our conversation of defining a failure—in this case, while the drives “failed,” the failure wasn’t mechanical and was based on something that we’ll be able to fix without replacing the drives. In short, don’t sweat the spike and pay attention to the long arc of performance on this population. We expect to see those drives happy and spinning for years to come (and with better performance, too).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hard Drive dataset (and beyond)&lt;/head&gt;
    &lt;p&gt;Thank you, as always, for making it through ~2,500 or so words to examine the fun side of data. Here’s our standard fine print:&lt;/p&gt;
    &lt;p&gt;The complete dataset used to create the tables and charts in this report is available on our Hard Drive Test Data page. You can download and use this data for free for your own purpose. All we ask are three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You cite Backblaze as the source if you use the data;&lt;/item&gt;
      &lt;item&gt;You accept that you are solely responsible for how you use the data, and;&lt;/item&gt;
      &lt;item&gt;You do not sell this data itself to anyone; it is free.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re a new Drive Stats fan, consider signing up for the newsletter. If you’re not ready for that kind of commitment, sound off in the comments section below or reach out directly to us to let us know what you’re working on. Happy investigating!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2025/"/><published>2025-11-14T13:10:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926395</id><title>Don't turn your brain off</title><updated>2025-11-14T16:13:02.541937+00:00</updated><content/><link href="https://computingeducationthings.substack.com/p/22-dont-turn-your-brain-off"/><published>2025-11-14T13:12:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926439</id><title>Nvidia is gearing up to sell servers instead of just GPUs and components</title><updated>2025-11-14T16:13:02.270678+00:00</updated><content>&lt;doc fingerprint="b7f8d4517b86a56a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;JP Morgan says Nvidia is gearing up to sell entire AI servers instead of just AI GPUs and components — Jensen's master plan of vertical integration will boost Nvidia profits, purportedly starting with Vera Rubin&lt;/head&gt;
    &lt;p&gt;The launch of Nvidia's Vera Rubin platform for AI and HPC next year could mark significant changes in the AI hardware supply chain as Nvidia plans to ship its partners fully assembled Level-10 (L10) VR200 compute trays with all compute hardware, cooling systems, and interfaces pre-installed, according to J.P. Morgan (via @Jukanlosreve). The move would leave major ODMs with very little design or integration work, making their lives easier, but would also trim their margins in favor of Nvidia's. The information remains unofficial at this stage.&lt;/p&gt;
    &lt;p&gt;Starting with the VR200 platform, Nvidia is reportedly preparing to take over production of fully built L10 compute trays with a pre-installed Vera CPU, Rubin GPUs, and a cooling system instead of allowing hyperscalers and ODM partners to build their own motherboards and cooling solutions. This would not be the first time the company has supplied its partners with a partially integrated server sub-assembly: it did so with its GB200 platform when it supplied the whole Bianca board with key components pre-installed. However, at the time, this could be considered as L7 – L8 integration, whereas now the company is reportedly considering going all the way to L10, selling the whole tray assembly — including accelerators, CPU, memory, NICs, power-delivery hardware, midplane interfaces, and liquid-cooling cold plates — as a pre-built, tested module.&lt;/p&gt;
    &lt;p&gt;If the information is correct, and Nvidia will indeed ship its partners L10 compute trays (which probably account for 90% of the cost of a server), then Nvidia will only leave its partners with rack-level integration rather than server design. They would still build the outer chassis, integrate power supplies depending on requirements, install sidecars or CDUs for rack-level cooling, add their own BMC and management stack, and perform final assembly and testing. These tasks matter operationally, but they do not differentiate hardware in a meaningful way.&lt;/p&gt;
    &lt;p&gt;This move promises to shorten the ramp for VR200 as Nvidia's partners will not have to design everything in-house and could lower production costs due to the volume of scale ensured by a direct contract between Nvidia and an EMS (most likely Foxconn as the primary supplier and then Quanta and Wistron, but that is speculation). For example, a Vera Rubin Superchip board recently demonstrated by Jensen Huang uses a very complex design, a very thick PCB, and only solid-state components. Designing such a board takes time and costs a lot of money, so using select EMS provider(s) to build it makes a lot of sense.&lt;/p&gt;
    &lt;p&gt;J.P. Morgan reportedly mentions the increase in power consumption of one Rubin GPU from 1.4 kW (Blackwell Ultra) to 1.8 kW (R200) and even 2.3 kW (a previously unannounced TDP for an allegedly unannounced SKU (Nvidia declined a Tom's Hardware request for comment on the matter) and increased cooling requirements as one of the motivations for moving to supply the whole tray instead of individual components. However, we know from reported supply chain sources that various OEMs and ODMs, as well as hyperscalers like Microsoft, are experimenting with very advanced cooling systems, including immersion and embedded cooling, which underscores their experience.&lt;/p&gt;
    &lt;p&gt;However, Nvidia's partners will shift from being system designers to becoming system integrators, installers, and support providers. They are going to keep enterprise features, service contracts, firmware ecosystem work, and deployment logistics, but the 'heart' of the server — the compute engine — is now fixed, standardized, and produced by Nvidia rather than by OEMs or ODMs themselves.&lt;/p&gt;
    &lt;p&gt;Also, we can only wonder what will happen with Nvidia's Kyber NVL576 rack-scale solution based on the Rubin Ultra platform, which is set to launch alongside the emergence of 800V data center architecture meant to enable megawatt-class racks and beyond. Now the only question is whether Nvidia further increases its share in the supply chain to, say, rack-level integration?&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Anton Shilov is a contributing writer at Tom’s Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;DS426&lt;/header&gt;There are definitely advantages of such levels of integration, but ultimately, it only bolsters Nvidia's monopolistic powers. When the AI world desperately needs a more open ecosystem, Nvidia is sure to double-down on forcing the standard for everyone.Reply&lt;lb/&gt;Hard as heck to change the status quo sometimes, ain't it!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;JTWrenn&lt;/header&gt;This makes me wonder if the stock slide is actually just heavy money moving around in the background at lower rates to try to free up capital to do this. Wonder how much this will tank Super MicroReply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;edzieba&lt;/header&gt;"Gearing up"? They've been selling not just servers but entire racks for years now (starting with DGX-1 back in 2016, so nearly a decade). If anything, this is unbundling those DGX boxes from the DGX pods to be sold as individual RUs for OEMS to integrate.Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/artificial-intelligence/jp-morgan-says-nvidia-is-gearing-up-to-sell-entire-ai-servers-instead-of-just-ai-gpus-and-componentry-jensens-master-plan-of-vertical-integration-will-boost-profits-purportedly-starting-with-vera-rubin"/><published>2025-11-14T13:18:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926469</id><title>AGI fantasy is a blocker to actual engineering</title><updated>2025-11-14T16:13:02.129459+00:00</updated><content>&lt;doc fingerprint="6cff1130952e1f98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AGI fantasy is a blocker to actual engineering&lt;/head&gt;
    &lt;p&gt;Reading Empire of AI by Karen Hao, I was struck by how people associated with OpenAI believe in AGI. They really do think someone, perhaps them, will build AGI, and that it will lead to either the flourishing or destruction of humanity.&lt;/p&gt;
    &lt;p&gt;Elon Musk founded OpenAI because he thought Demis Hassabis was an evil genius who would build AGI first:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…Musk would regularly characterise Hassabis as a supervillain who needed to be stopped. Musk would make unequivocally clear that OpenAI was the good to DeepMind’s evil. … “He literally made a video game where an evil genius tries to create AI to take over the world,” Musk shouted [at an OpenAI off-site], referring to Hassabis’s 2004 title Evil Genius, “and fucking people don’t see it. Fucking people don’t see it! And Larry [Page]? Larry thinks he controls Demis but he’s too busy fucking windsurfing to realize that Demis is gathering the power.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;OpenAI’s co-founder and chief scientist Ilya Sutskever regularly told audiences and employees to “feel the AGI”. At a company off-site in Yosemite in September 2022, employees gathered around a firepit:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In the pit, [Sutskever] had placed a wooden effigy that he’d commissioned from a local artist, and began a dramatic performance. This effigy, he explained represented a good, aligned AGI that OpenAI had built, only to discover it was actually lying and deceitful. OpenAI’s duty, he said, was to destroy it. … Sutskever doused the effigy in lighter fluid and lit on fire.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think it’s remarkable that what was until recently sci-fi fantasy has become a mainstream view in Silicon Valley.&lt;/p&gt;
    &lt;p&gt;Hao writes that GPT-2 was a bet on the “pure language” hypothesis, that asserts that since we communicate through language, then AGI should emerge from training a model solely on language. This is contrast to the “grounding” hypothesis, that asserts an AGI needs to perceive the world. Successfully scaling GPT to GPT-2 convinced enough people at OpenAI that the pure language hypothesis was valid. They just needed more data, more model parameters, and more compute.&lt;/p&gt;
    &lt;p&gt;So the belief in AGI, plus the recent results from LLMs, necessitates scaling, and justifies building data centres that consume hundreds of litres of water a second, run on polluting gas generators because the grid can’t supply the power (and might use as much power as entire cities), driving up CO2 emissions from manufacture and operation of new hardware, and exploits and traumatises data workers to make sure ChatGPT doesn’t generate outputs like child sexual abuse material and hate speech or encourage users to self-harm. (The thirst for data is so great that they stopped curating training data and instead consume the internet, warts and all, and manage the model output using RLHF.)&lt;/p&gt;
    &lt;p&gt;And this is all fine, because they’re going to make AGI and the expected value (EV) of it will be huge! (Briefly, the argument goes that if there is a 0.001% chance of AGI delivering an extremely large amount of value, and 99.999% chance of much less or zero value, then the EV is still extremely large because &lt;code&gt;(0.001% * very_large_value) + (99.999% * small_value) = very_large_value&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;But AGI arguments based on EV are nonsensical because the values and probabilities are made up and unfalsifiable. They also ignore externalities like environmental damage, which in contrast to AGI, have known negative value and certain probability: costs borne by everyone else right now.&lt;/p&gt;
    &lt;p&gt;As a technologist I want to solve problems effectively (by bringing about the desired, correct result), efficiently (with minimal waste) and without harm (to people or the environment).&lt;/p&gt;
    &lt;p&gt;LLMs-as-AGI fail on all three fronts. The computational profligacy of LLMs-as-AGI is dissatisfying, and the exploitation of data workers and the environment unacceptable. Instead, if we drop the AGI fantasy, we can evaluate LLMs and other generative models as solutions for specific problems, rather than all problems, with proper cost benefit analysis. For example, by using smaller purpose-built generative models, or even discriminative (non-generative) models. In other words, make trade-offs and actually do engineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomwphillips.co.uk/2025/11/agi-fantasy-is-a-blocker-to-actual-engineering/"/><published>2025-11-14T13:21:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45926779</id><title>I think nobody wants AI in Firefox, Mozilla</title><updated>2025-11-14T16:13:02.005902+00:00</updated><content>&lt;doc fingerprint="f7ee53523cb1437d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I think nobody wants AI in Firefox, Mozilla&lt;/head&gt;
    &lt;p&gt;Mozilla is developing a built‑in AI assistant for Firefox that will be offered as a third browsing mode alongside Normal and Private tabs. They’re calling it “Window AI.”&lt;/p&gt;
    &lt;p&gt;Details are still scarce. Based on Mozilla’s official announcement on Thursday (13th), it looks like a deeper implementation than the existing sidebar that gives access to third‑party chatbots (ChatGPT, Gemini, Copilot, etc.). The post stresses the feature will be opt-in and that the user “is in control.”&lt;/p&gt;
    &lt;p&gt;There’s a waitlist to try the feature and a Mozilla forum thread inviting people to “help shape” the initiative.&lt;/p&gt;
    &lt;p&gt;It’s safe to say that the people who volunteered to “shape” the initiative want it dead and buried. Of the 52 responses at the time of writing, *all* rejected the idea and asked Mozilla to stop shoving AI features into Firefox.&lt;/p&gt;
    &lt;p&gt;I don’t know whether the negative reactions reflect the majority of Firefox users or are just a noisy minority. Mozilla, after all, likely has a clearer view of the whole user base.&lt;/p&gt;
    &lt;p&gt;What strikes me as odd is the decision to position itself as just another AI‑enabled web browser, picking a fight with big techs and better‑funded startups whose users are less hostile (and sometimes enthusiastic) about adding AI to web browsing.&lt;/p&gt;
    &lt;p&gt;Mozilla seems to be trying to wedge itself between those who reject AI and those who want generative‑AI features in the browser — trying to please everyone — as this excerpt from the post shows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We see a lot of promise in AI browser features making your online experience smoother, more helpful, and free from the everyday disruptions that break your flow. But browsers made by AI companies ask you to make a hard choice — either use AI all the time or don’t use it at all.&lt;/p&gt;
      &lt;p&gt;We’re focused on making the best browser, which means recognizing that everyone has different needs. For some, AI is part of everyday life. For others, it’s useful only occasionally. And many are simply curious about what it can offer, but unsure where to start.&lt;/p&gt;
      &lt;p&gt;Regardless of your choice, with Firefox, you’re in control.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Those unhappy have another option: use an AI‑free Firefox fork such as LibreWolf, Waterfox, or Zen Browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://manualdousuario.net/en/mozilla-firefox-window-ai/"/><published>2025-11-14T14:05:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45927035</id><title>Wealthy foreigners 'paid for chance to shoot civilians in Sarajevo'</title><updated>2025-11-14T16:13:01.214887+00:00</updated><content>&lt;doc fingerprint="3d47018f39e1e5cf"&gt;
  &lt;main&gt;
    &lt;p&gt;Wealthy foreign gun enthusiasts paid Bosnian Serb forces for the chance to shoot residents of Sarajevo during the siege of the city during the 1990s, according to claims being investigated by Italian magistrates.&lt;/p&gt;
    &lt;p&gt;The investigation was prompted by new evidence that “weekend snipers” paid handsomely to line the hills around Sarajevo and join in the Bosnian Serb siege, which killed more than 11,500 people between 1992 and 1996 during the Balkan Wars.&lt;/p&gt;
    &lt;p&gt;The investigation into the alleged “human safari” in Sarajevo has been opened after years of research by the Italian writer Ezio Gavazzeni, who said a key source was a former Bosnian intelligence officer.&lt;/p&gt;
    &lt;p&gt;“What I learnt is that Bosnian intelligence warned the local office of the Italian secret service … about the presence of at least five Italians who were taken to the hills above Sarajevo to shoot civilians,” he told the Italian daily La Repubblica.&lt;/p&gt;
    &lt;p&gt;Gavazzeni started investigating after watching the 2022 documentary Sarajevo Safari, by the Slovenian film-maker Miran Zupanic. The film quoted an unnamed American former spy saying that he had seen visitors paying to shoot civilians. Serbian veterans have denied the claims.&lt;/p&gt;
    &lt;p&gt;Although there are allegations that snipers arrived from around Europe, Gavazzeni focused on reports of Italians gathering in the Trieste before they were escorted to Sarajevo.&lt;/p&gt;
    &lt;p&gt;“One of the Italian snipers identified to SISMI [the Italian secret service] in 1993 was from Milan, and the owner of a private clinic specialising in cosmetic surgery,” he said. “We are talking about wealthy people, entrepreneurs with a reputation, who during the siege of Sarajevo paid to be able to kill helpless civilians.”&lt;/p&gt;
    &lt;p&gt;• His job is to keep Bosnia peaceful — now he must take on Putin too&lt;/p&gt;
    &lt;p&gt;After their trips to Sarajevo, he said, “they returned to their respectable lives”. He added that the snipers proved “the indifference of evil — becoming God and remaining unpunished”.&lt;/p&gt;
    &lt;p&gt;As Milan magistrates seek to identify the Italian snipers, Gavazzeni said he hoped to get the secret service documents about them. “I would really like to read them. I hope they haven’t vanished — it would be very serious.”&lt;/p&gt;
    &lt;p&gt;The Bosnian consul in Milan, Dag Dumrukcic, told La Repubblica that the Bosnian government would offer “total collaboration” to the magistrates. “We are impatient to discover the truth about such a cruel matter in order to close a chapter of history. I am in possession of certain information I will be sharing with the investigators,” he said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.thetimes.com/world/europe/article/wealthy-foreigners-paid-for-chance-to-shoot-civilians-in-sarajevo-zrljbb27z"/><published>2025-11-14T14:32:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45927435</id><title>Oracle hit hard in Wall Street's tech sell-off over its AI bet</title><updated>2025-11-14T16:13:00.184044+00:00</updated><content>&lt;doc fingerprint="9c7f1d515e9a93e9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;&lt;quote&gt;Oracle hit hard in Wall Street’s tech sell-off over its huge AI bet&lt;/quote&gt;&lt;/head&gt;&lt;head rend="h2"&gt;Join FT Edit&lt;/head&gt;Only $49 a year&lt;p&gt;Get 2 months free with an annual subscription at was $59.88 now $49. Access to eight surprising articles a day, hand-picked by FT editors. For seamless reading, access content via the FT Edit page on FT.com and receive the FT Edit newsletter.&lt;/p&gt;&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;Trial&lt;/head&gt;&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel or change your plan anytime during your trial.&lt;/p&gt;&lt;head rend="h3"&gt;Standard Digital&lt;/head&gt;&lt;p&gt;Essential digital access to quality FT journalism on any device. Pay a year upfront and save 20%.&lt;/p&gt;&lt;head rend="h3"&gt;Premium Digital&lt;/head&gt;&lt;p&gt;Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ft.com/content/583e9391-bdd0-433e-91e0-b1b93038d51e"/><published>2025-11-14T15:04:22+00:00</published></entry></feed>