<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-05T11:34:46.459067+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45761594</id><title>SPy: An interpreter and compiler for a fast statically typed variant of Python</title><updated>2025-11-05T11:34:54.561516+00:00</updated><content>&lt;doc fingerprint="202909f576e7b2d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Inside SPyü•∏, part 1: Motivations and Goals¬∂&lt;/head&gt;
    &lt;p&gt;This is the first of a series of posts in which I will try to give a deep explanation of SPy, including motivations, goals, rules of the language, differences with Python and implementation details.&lt;/p&gt;
    &lt;p&gt;This post focuses primarily on the problem space: why Python is fundamentally hard to optimize, what trade-offs existing solutions require, and where current approaches fall short. Subsequent posts in this series will explore the solutions in depth. For now, let's start with the essential question: what is SPy?&lt;/p&gt;
    &lt;p&gt;Before diving in, I want to express my gratitude to my employer, Anaconda, for giving me the opportunity to dedicate 100% of my time to this open-source project.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is SPy?¬∂&lt;/head&gt;
    &lt;p&gt;There are multiple answers to that, depending on the point of view. The most technically accurate is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;SPy is an interpreter and a compiler for a statically typed variant of Python, with focus on performance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think it's very important to be clear and honest since the beginning: SPy is not a "compiler for Python". There are features of the Python language which will never be supported by SPy by design. Don't expect to compile Django or FastAPI with SPy.&lt;/p&gt;
    &lt;p&gt;A small corollary is that for now I decided that SPy programs live in &lt;code&gt;*.spy&lt;/code&gt; files, to
make the distinction between the two worlds very clear.&lt;/p&gt;
    &lt;p&gt;Whilst compiling 100% of Python is not a goal, SPy still aims to have a very tight integration with the existing Python ecosystem. It is possible to &lt;code&gt;import&lt;/code&gt; Python
libraries from SPy, and SPy modules from Python.&lt;/p&gt;
    &lt;p&gt;The myth of 100% compatibility&lt;/p&gt;
    &lt;p&gt;The vast majority of "compilers for Python" which exists or existed in the past are not "100% compatible". They don't support the full language even when they claim so. SPy prefers to be more explicit and honest about it.&lt;/p&gt;
    &lt;p&gt;Current Status&lt;/p&gt;
    &lt;p&gt;SPy is still very work-in-progress and there are many things which are part of the plan and the design, but they are currently not there.&lt;/p&gt;
    &lt;p&gt;To make reading easier in this series I always use the present tense when talking about SPy capabilities -- even when they are not implemented -- and defer a more detailed explanation to these "Current Status" boxes.&lt;/p&gt;
    &lt;p&gt;At this point, SPy is still not usable for anything bigger than demos. The most complex piece of code written in SPy is probably the raytracing example, which is 200x faster than CPython.&lt;/p&gt;
    &lt;p&gt;Current Status: Python integration&lt;/p&gt;
    &lt;p&gt;At the moment of writing, SPy cannot import Python libraries yet. The plan is to support this use case by embedding &lt;code&gt;libpython.so&lt;/code&gt; (only if and when needed!) but to
do that we need a way to call C from SPy, which we haven't yet implemented.&lt;/p&gt;
    &lt;p&gt;The other direction is partially supported: the SPy compiler can generate CFFI-based extensions which can be imported by CPython. This is useful for experimentation and for early testing, but it exposes only a partial and low-level API. Eventually, the SPy compiler will be able to generate fully fledged CPython extensions, similar to what Cython does.&lt;/p&gt;
    &lt;p&gt;Another possible answer is the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;SPy is a thought experiment to determine how much dynamicity we can remove from Python while still feeling Pythonic.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;During the years there have been many attempts to improve Python speed; generally they fall into two categories:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Implement "full Python". To be able to support all dynamic features and be fast, they usually employ a Just In Time (JIT) compiler. Examples are PyPy, GraalPy, Pyston, and CPython's own JIT.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Implement a "subset of Python" or "variant of Python", either as an Ahead of Time (AOT) or JIT compiler which is able to produce fast code. The usual approach here is to remove many (if not all) of the dynamic features which make Python hard to compile. Examples are RPython, Mypyc, Cython and Numba.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem of "full Python" JIT compilers is that sometimes they work very well and produce huge speedups, other times they don't produce any speedup at all, or might even introduce slowdowns, or they might use too much memory, or they are slow to "warm up".&lt;/p&gt;
    &lt;p&gt;The problem of the subset/variant approach is that by removing the dynamic features of Python, you end up with something which does not feel pythonic, and in which many typical and idiomatic Python patterns just don't work. You often end up with "Java with Python syntax" (nothing in particular against Java, but I hope it gives an idea of what I mean).&lt;/p&gt;
    &lt;p&gt;SPy does something different: on one hand, it removes the dynamic features which make Python "slow", but on the other hand it introduces new features which make it possible to implement and use the same pythonic patterns which we like. How to achieve this result is not possible to explain in a few sentences, that's why we need a full series of posts :).&lt;/p&gt;
    &lt;p&gt;Subset vs Variant&lt;/p&gt;
    &lt;p&gt;If a compiler implements a subset of Python, then all programs which can be compiled can also run on top of CPython. If the compiler also add new features which are not available on CPython, then it's a variant.&lt;/p&gt;
    &lt;p&gt;For example according to this definition, RPython is a subset and Cython is a variant. SPy is also a variant because it offers unique features, as we will see later.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Zen of SPy¬∂&lt;/head&gt;
    &lt;p&gt;As mentioned above, 100% compatibility with Python is explicitly not a goal.&lt;/p&gt;
    &lt;p&gt;If you are a "language nerd" (like me :)) who knows the inner details of the attribute lookup logic or when Python invokes &lt;code&gt;__add__&lt;/code&gt; or &lt;code&gt;__radd__&lt;/code&gt; in case of inheritance from builtin types, then SPy is
definitely not Python, and doesn't even try to be.&lt;/p&gt;
    &lt;p&gt;However, I expect that for a large fraction of Python users, it will not matter. There are many users who mainly consume complex libraries written by others and write "straightforward" Python code: for them, writing SPy should be as easy as writing Python.&lt;/p&gt;
    &lt;p&gt;The following is a list of goals and design guidelines of SPy:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Easy to use and implement. The language is easy to understand. Moreover, it must be possible to implement SPy without huge engineering teams.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We have an interpreter for ease of development and debugging.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We have a compiler for deployment and performance. The interpreter and the compiler are guaranteed to produce the exact same results at runtime.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Static typing. Type annotations are enforced by the language and checked by both the interpreter and the compiler.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Performance matters. SPy aims to have performance comparable to low level languages such as C and Rust.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Predictable performance. We should be able to reason about the performance characteristics of a piece of code, without relying on a "magical optimizer" which we don't fully understand, and without having "performance cliffs" in which modifying a line of code makes everything 10x slower.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rich metaprogramming capabilities. SPy has 1st class support for metaprogramming, although the precise spelling and characteristics might deviate from Python. For example, it would totally be possible to recreate something like FastAPI or SQLAlchemy in SPy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Zero cost abstractions. SPy supports things like decorators,&lt;/p&gt;&lt;code&gt;**kwargs&lt;/code&gt;, the descriptor protocol,&lt;code&gt;__getattr__&lt;/code&gt;, etc. without extra runtime costs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Opt-in dynamism. Some of the dynamic features of Python are off by default, but it's still possible to opt-in explicitly, when needed. As an example, SPy provides a&lt;/p&gt;&lt;code&gt;dynamic&lt;/code&gt;type which provides full dynamic dispatch.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;One language, two levels. SPy supports the low-level capabilities of C, C++, Rust, etc., as well as the high-level abstractions and expressivity of Python. For example, SPy's own list and dict types are written in SPy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Current Status: &lt;code&gt;**kwargs&lt;/code&gt; and &lt;code&gt;dynamic&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;At the time of writing, &lt;code&gt;**kwargs&lt;/code&gt; and keyword arguments are not implemented yet.
The &lt;code&gt;dynamic&lt;/code&gt; type works in the interpreter, but not yet in the compiler.&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivations and background¬∂&lt;/head&gt;
    &lt;p&gt;There are several factors which motivated me to start such an ambitious project. Some of them come directly from my first hand experience as a PyPy core dev in the last ~20 years. Others come from insights which I gained while doing my professional activity, which for years consisted in trying to optimize real world Python code used in production. Finally, some come from observing how Python is actually used in practice.&lt;/p&gt;
    &lt;p&gt;Throughout this post, we'll see variations on a theme: Python developers already write in constrained subsets of the language (for readability, for the JIT, for the type checker), but these subsets are informal and poorly specified. SPy formalizes these constraints and gives you powerful tools in return.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Python is slow¬∂&lt;/head&gt;
    &lt;p&gt;Thanks to my work on PyPy, I came to the conclusion that Python is fundamentally impossible to optimize to the level of performance which I aim for. There are some features of the language which make Python "intrinsically slow": I have talked about it extensively in my EuroPython talk Myths and fairy tales around Python performance (video, slides and LWN write-up).&lt;/p&gt;
    &lt;p&gt;Levels of performance&lt;/p&gt;
    &lt;p&gt;When comparing two different languages and implementations, it doesn't make sense to say "X is Nx faster than Y": the precise number can vary a lot depending on the benchmark and on the hardware.&lt;/p&gt;
    &lt;p&gt;Neverthelss, we can at least write down the order of magnitude of the expected speeups in the average and best case scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;CPython's JIT aims to be 10% - 50% faster than CPython's interpreter&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;PyPy's JIT aims to be 2x - 10x faster than CPython&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SPy aims to be 10x - 100x faster than CPython.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first problem is that Python is extremely dynamic. I'm not talking only about dynamic typing, but also about the fact that in a given Python process, the "world" is a moving target and "everything can change at any time". For example, &lt;code&gt;import&lt;/code&gt; statements
are resolved dynamically at runtime and it's impossible to reliably determine them
statically. Modules and classes are mutable: their content can change at any time, and a
compiler must be defensive about it.  The &lt;code&gt;__class__&lt;/code&gt; of an object can change, etc.&lt;/p&gt;
    &lt;p&gt;On top of that, operation dispatch is also very dynamic. Most syntactical constructs like &lt;code&gt;.&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;[]&lt;/code&gt;, etc., trigger
very complex
lookup logic which must be resolved at runtime.  And even knowing the type of an object
is not enough to predict its behavior, because you can have per-instance attributes
which overrides class behavior.&lt;/p&gt;
    &lt;p&gt;A JIT compiler can solve these two problems, and PyPy proves it. However, an approach based on JIT compilation introduces its own unique problems, as we will see in the dedicated section.&lt;/p&gt;
    &lt;p&gt;Finally, we have a problem which cannot be solved even by a JIT (as far as I know). Python semantics makes it intrinsically cache unfriendly. In Python &lt;del&gt;everything is an object&lt;/del&gt; everything is a pointer, and objects are mutable by default. In CPython object references are implemented as &lt;code&gt;PyObject *&lt;/code&gt; in C, which means that any
time we do an attribute and/or item lookup we need to dereference a pointer.  It is not
uncommon to have to dereference 4 or 5 pointers to execute just a single line code: this
is called Pointer Chasing and in short,
it's Very Bad‚Ñ¢ for performance because it destroys
memory locality. Take
the classical example of a &lt;code&gt;Rect&lt;/code&gt; which has two &lt;code&gt;Points&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;@dataclass
class Point:
    x: float
    y: float

@dataclass
class Rect:
    a: Point
    b: Point

r = Rect(Point(1, 2), Point(2, 3))
width = abs(r.b.x - r.a.x)
height = abs(r.b.y - r.a.y)
&lt;/code&gt;
    &lt;p&gt;We can use the excellent PyTutor to visualize a simplified version of the memory layout of a &lt;code&gt;Rect&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Each arrow is a pointer, and following an arrow means that we have a chance to do a "cache miss". A better memory layout for this precise case would be to store the 4 &lt;code&gt;float&lt;/code&gt;s in a contiguous area of memory, but doing so would violate some property which
the language guarantees (e.g. &lt;code&gt;id(r.a) == id(r.a)&lt;/code&gt;, or that &lt;code&gt;r.a&lt;/code&gt; doesn't make a copy),
so a compliant Python implementation cannot do that.&lt;/p&gt;
    &lt;p&gt;About CPUs, cache and RAM&lt;/p&gt;
    &lt;p&gt;Modern CPUs are very complex beasts, thus the following is a simplified explanation.&lt;/p&gt;
    &lt;p&gt;The cost of loading values from RAM is very slow compared to the cost of computation itself. If you want to add two numbers which are already in CPU registers, you can do that in 1 cycle, but if you need to fetch those values from memory, the CPU must sit idle for hundreds of cycles while it waits for the data to be loaded.&lt;/p&gt;
    &lt;p&gt;Since loading from RAM is so slow, CPUs store frequently used data into a "cache". Loading from the cache is much faster, and thus CPUs can execute many more instructions per second when they operate on cached data. Normally on modern systems we have three levels of cache: L1, L2 and L3. L1 is the smallest and fastest, then each level is bigger and slower than the previous; the RAM is the slowest. Loading an address of memory which is in the cache is a cache hit, else it's a cache miss.&lt;/p&gt;
    &lt;p&gt;For multiple reasons which I cannot explain in this box, if the address &lt;code&gt;A&lt;/code&gt; is in
cache, then also all the values which are "close" to &lt;code&gt;A&lt;/code&gt; are in cache. That's why
having a good memory locality increases the chance of cache hits. On the other hand,
when we follow a pointer there is a high risk of landing in a "far" region of the
memory, and thus each pointer dereference is a potential cache miss.&lt;/p&gt;
    &lt;p&gt;This video gives a visual intuition of the relative performance of each level. Compared to L1, RAM is super slow, and that's why each cache miss is a disaster for performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;RealPython is a subset of Python¬∂&lt;/head&gt;
    &lt;p&gt;In the previous section we saw how Python allows a great deal of dynamism. However, in practice this doesn't happen so often.&lt;/p&gt;
    &lt;p&gt;We run applications in very well defined environments, with locked set of dependencies. But the interpreter doesn't know and must rediscover at every single run that &lt;code&gt;import numpy&lt;/code&gt; actually imports the same version of numpy from the same directory
again and again.&lt;/p&gt;
    &lt;p&gt;Moreover, during the years, as a community we understood that certain patterns are "good", while others are detrimental to readability and maintainability: we tend to have functions which operate on well defined types, creating attributes outside of &lt;code&gt;__init__&lt;/code&gt;
is a bad practice, monkey-patching is permitted only in certain contexts (e.g. testing),
we never change the &lt;code&gt;__class__&lt;/code&gt; of an object, and so on.&lt;/p&gt;
    &lt;p&gt;De facto, we are already using a subset of Python, which I'd like to call &lt;code&gt;RealPython&lt;/code&gt;, but it's a subset which is not formally specified and is slightly
different from case to case.  The interpreter cannot take advantage of it because it
needs to be ready to handle the 0.1% of the cases in which these things actually happen.&lt;/p&gt;
    &lt;p&gt;But at the same time, we cannot just remove dynamism and metaprogramming from the language: it's what makes it possible to have some of the most powerful and rich libraries in the ecosystem!&lt;/p&gt;
    &lt;head rend="h3"&gt;Static Typing in Python¬∂&lt;/head&gt;
    &lt;p&gt;In the recent years, static typing and type checkers have become more and more popular in the Python community. Let's be clear: I think that given the constraints, the Python typing story is good enough and well designed. I wouldn't be able to do it better. But still, Python is not a language designed for static typing and, in absolute terms, the current situation leaves a lot to be desired.&lt;/p&gt;
    &lt;p&gt;The static-vs-dynamic typing debate has been going on for decades. Let's try to examine the typical pros&amp;amp;cons of each.&lt;/p&gt;
    &lt;p&gt;The first typical advantage of static typing is that the typecheker can prove (in the mathematical sense) that a certain class of bugs cannot happen in your program. Unfortunately, this doesn't happen in Python.&lt;/p&gt;
    &lt;p&gt;The interpreter just ignores type annotations. The typechecker tries its best, but there are cases in which it just doesn't understand the full semantics of Python and/or deliberately ignores it and thus has a wrong view on what's actually happening. The following silly example happily passes &lt;code&gt;mypy&lt;/code&gt;, although it's clearly wrong and raises
&lt;code&gt;AttributeError&lt;/code&gt; at runtime:&lt;/p&gt;
    &lt;p&gt;As such, we need to treat Python type checkers more like linters than actual theorem provers -- which is still better than nothing, but very far from having the advantages of an actual sound type system.&lt;/p&gt;
    &lt;p&gt;The second typical advantage of static typing is that the compiler can emit more efficient code. But since we cannot actually be sure that types are correct, we cannot use them to guide compilation. There are projects like &lt;code&gt;mypyc&lt;/code&gt; which go in this
direction, but by doing so they break compatibility and limit dynamism, so we are back
to the category of "compiler for a subset/variant of Python".&lt;/p&gt;
    &lt;p&gt;Finally, another advantage of static typing is that IDEs and tooling can use type knowledge to assist development. I think this is actually a great success in Python and I suspect this plays a big role in the enthusiasm around the Python typing system.&lt;/p&gt;
    &lt;p&gt;On the other hand, the typical advantage of dynamic typing is that it allows more flexibility. This is another pain point of Python typing: I often find cases in which there are patterns which would make my code "better", but they cannot be understood by the typechecker even if they are totally correct. The part of the language which is type-checkable is another subset of Python: this time it's better specified, although very far from being a formal specification.&lt;/p&gt;
    &lt;p&gt;I am aware that what I'm going to say it's a bit of exaggeration, it wants to be provocative. But, from some point of view, by using static typing in Python we get the worst of both worlds: zero guarantees, still slow, and it prevents patterns where dynamic typing is actually useful.&lt;/p&gt;
    &lt;p&gt;Wouldn't it be better to have static typing in such a way which is safe, fast and still allows pythonic patterns?&lt;/p&gt;
    &lt;head rend="h3"&gt;About PyPy and JIT compilers¬∂&lt;/head&gt;
    &lt;p&gt;In the section "Why Python is slow", we listed three problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;mutable world&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;dynamic &amp;amp; complex dispatch&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;cache unfriendliness&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The PyPy JIT actually solves (1) and (2) very well, by taking advantage of what we realized in RealPython is a subset of Python: a lot of the "crazy things" don't actually happen in practice, or happen very rarely. The essence is that it speculatively assumes conditions such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;the types of variables in a given piece of code are stable&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;the&lt;/p&gt;&lt;code&gt;__dict__&lt;/code&gt;of classes and modules doesn't change&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;the&lt;/p&gt;&lt;code&gt;__class__&lt;/code&gt;of an object doesn't change&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;etc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then it generates code which is very fast as long as those assumptions hold. Moreover, it generates extra code and guards to check that the assumptions are still valid, and deoptimize when they are not.&lt;/p&gt;
    &lt;p&gt;This high-level overview is correct not only for PyPy, but for all Python JIT compilers that I know of, although then the actual low-level details can vary a lot depending on the implementation.&lt;/p&gt;
    &lt;p&gt;The main drawback of this strategy is that the code is fast only as long as the heuristics are correct. We can easily find cases in which by modifying a single line of code the generated code becomes 2x, 5x or even 10x slower, because the JIT is no longer able to optimize it correctly. What is infuriating is that in those cases the JIT perfectly knows why it cannot optimize, but it just doesn't tell us. We could probably try to emit warnings in those cases, but from the point of view of the JIT is hard to distinguish the cases in which a warning makes sense from those in which it doesn't.&lt;/p&gt;
    &lt;p&gt;If you want maximum performance, you need to write code which complies with the heuristics: it's yet another subset of Python, but this time it is very loosely specified and often requires deep knowledge of the JIT internals to know exactly what you can and cannot do; let's call this &lt;code&gt;JITPython&lt;/code&gt;. It becomes very hard to reason about
performance and to predict whether a given piece of code will be fast or slow.&lt;/p&gt;
    &lt;p&gt;Moreover, there are other problems with JITs. In random order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;the implementation is much more complex. It's harder to contribute, and it requires lot of engineering power.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No matter how advanced a JIT is, there will always be cases which it fails to optimize.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Guards and checks have a runtime cost.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Even when everything goes well, the quality of the generated code is worse than an AOT compiler, because the JIT cannot spend too much time on optimizations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There is a warmup phase before reaching peak performance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They tend to use more memory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For the specific case of tracing JITs like PyPy and CPython's own JIT, there are other kinds of problems which I discuss in full details here.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;C API¬∂&lt;/head&gt;
    &lt;p&gt;There is another problem with JIT compilers: the C API is completely opaque to any JIT.&lt;/p&gt;
    &lt;p&gt;Huge parts of the Python ecosystem are written in compiled languages (C, C++, Rust, Cython, ...) and they communicate with the Python interpreter through the C API. As soon as the JIT needs to call into a C extension, it loses track of what's going on and has to assume that "everything changed", which often needs to deoptimize and/or do expensive sanity checks.&lt;/p&gt;
    &lt;p&gt;In an ideal world, we would like to have an optimizer which is able to see the whole program across the language boundary and e.g. to inline a C function called from Python or viceversa. This is something which happens almost for free when using AOT compiled language with Link Time Optimization (LTO), but which is just impossible to get with Python.&lt;/p&gt;
    &lt;head rend="h3"&gt;RPython¬∂&lt;/head&gt;
    &lt;p&gt;RPython is an implementation detail of PyPy, but a very interesting one. It stands for "Restricted Python", it's a subset of Python which is compilable to C, and it's the language in which the PyPy interpreter is written in:&lt;/p&gt;
    &lt;code&gt;PyPy : RPython = CPython : C
&lt;/code&gt;
    &lt;p&gt;RPython programs can usually run unmodified on CPython, and you get the same results as with the compiled version. This means that you can use the CPython interpreter for development and debugging, and the RPython compiler for deployment -- the best of both worlds. This pattern is used heavily to develop PyPy.&lt;/p&gt;
    &lt;p&gt;Another interesting feature of RPython is its metaprogramming capabilities, which come directly from the way it is implemented. The RPython compiler is written in Python, and it works by first importing the entry point of the target program inside CPython, then analyzing the bytecode of the live function objects recursively referenced by the entry point.&lt;/p&gt;
    &lt;p&gt;The interesting part is that the initial &lt;code&gt;import&lt;/code&gt; happens entirely inside CPython: at
"import time" the RPython program can use the full power of Python to do
metaprogramming, including using decorators, metaclasses, code generation, etc.  This
works because the RPython compiler kicks in only after this phase ends.&lt;/p&gt;
    &lt;p&gt;Inside PyPy, RPython is just a tool to be able to write the "full Python" which we give the end users. RPython was never meant to be used by final users, and thus its ergonomics is very bad: it happens quite often that if you try to compile an RPython program which contains a type error, you end up with an &lt;code&gt;AssertionError&lt;/code&gt; inside the
compiler itself, or with an obscure error message.&lt;/p&gt;
    &lt;p&gt;Despite those shortcomings, the combination of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;possibility to use an interpreter for development and debugging&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;full metaprogramming capabilities at "compile time"&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;static typing at runtime&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;proved to be very nice and enjoyable to use in practice.&lt;/p&gt;
    &lt;p&gt;SPy takes a lot of inspiration from that. From many points of view, it can be considered "RPython 2.0". After all, the &lt;code&gt;S&lt;/code&gt; stands for "Static" but it's also the letter after &lt;code&gt;R&lt;/code&gt;
:).&lt;/p&gt;
    &lt;head rend="h2"&gt;SPy: Putting It All Together¬∂&lt;/head&gt;
    &lt;p&gt;SPy aims to solve all the issues explained in the previous sections. It does so in part by formalizing the limitations of the various "subsets of Python" which we saw above, and in part by adding novel features which hopefully are as powerful as the ones which we removed, but more suitable to performance oriented compilation.&lt;/p&gt;
    &lt;p&gt;The next posts of this series will cover all in great detail, but for now let's give a hint of the main characteristics of SPy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Static typing in SPy¬∂&lt;/head&gt;
    &lt;p&gt;SPy's type system is designed with different goals in mind and it is more limited compared to e.g. &lt;code&gt;mypy&lt;/code&gt;, although it tries to be compatible when it makes sense. For
many of the common cases, the final user won't see any noticeable difference.&lt;/p&gt;
    &lt;p&gt;Types are actively enforced by the interpreter. If you try to assign a value to a variable of an incompatible type, you get a &lt;code&gt;TypeError&lt;/code&gt;.  You can opt-in and use the
&lt;code&gt;dynamic&lt;/code&gt; type, but any assignment or cast to a more precise type inserts a runtime
check.&lt;/p&gt;
    &lt;p&gt;The typesystem is sound and if a program passes the type checker it is guaranteed not to have &lt;code&gt;TypeError&lt;/code&gt; at runtime.  The compiler can use this knowledge to generate more
efficient code.  It is guaranteed that the interpreter and the compiler produce the
exact same output.&lt;/p&gt;
    &lt;p&gt;This alone is an improvement over Static Typing in Python, as now it's guaranteed that our programs are free of type errors, and they are much faster. But if SPy had only a static type system, it wouldn't be much different than all the other "Python compilers". What makes the difference are the features which enable metaprogramming and other "pythonic" patterns in a type safe way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Import time vs runtime¬∂&lt;/head&gt;
    &lt;p&gt;Before launching a program SPy analyzes the source code and statically determines the set of modules which will be imported.&lt;/p&gt;
    &lt;p&gt;After that, we enter what we call the import time phase and we import all the needed modules. Things such as decorators, metaclasses and module-level initialization run in this phase and the world is "mutable as usual".&lt;/p&gt;
    &lt;p&gt;Then we freeze the world: all global constants are frozen and become immutable, including modules and classes.&lt;/p&gt;
    &lt;p&gt;Finally, at runtime the program runs "as usual", inside an immutable world.&lt;/p&gt;
    &lt;p&gt;This is clearly similar to what RPython does. The big difference is that RPython uses CPython as its "Import time" interpreter, while SPy uses its own interpreter. A lot of Python metaprogramming patterns fit very well in this scheme, and the experience of the PyPy team with RPython validates this claim.&lt;/p&gt;
    &lt;p&gt;This is basically a formalization of some of the rules of both &lt;code&gt;RealPython&lt;/code&gt; and
&lt;code&gt;JITPython&lt;/code&gt;: the big advantage is that if you violate the rule you get a very clear
error message instead of a subtle slowdown.&lt;/p&gt;
    &lt;head rend="h3"&gt;Redshifting¬∂&lt;/head&gt;
    &lt;p&gt;The other fundamental concept in SPy is redshifting.&lt;/p&gt;
    &lt;p&gt;Each expression is given a color:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;blue&lt;/code&gt;expressions are those which can safely be evaluated ahead of time, because they don't have side effects and all operands are statically known.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;red&lt;/code&gt;expressions are those which needs to be evaluated at runtime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;During redshifting we eagerly evaluate all the &lt;code&gt;blue&lt;/code&gt; parts of the code: it's a form
of partial evaluation.  This process plays very well with the freezing that we discussed
above, because a lot of operations on frozen data become automatically blue: for
example, if we statically know the type of an object, the logic to look up a method
inside the frozen class hierarchy is a blue operation and it's optimized away, leaving
just a direct call as a result.&lt;/p&gt;
    &lt;p&gt;So far, this is not different than usual constant folding, with the difference that it's guaranteed to happen. What makes it more powerful is the ability to mark some functions as &lt;code&gt;@blue&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Calling a &lt;code&gt;@blue&lt;/code&gt; function is always a blue operation, and the function body is executed
during redshifting. This is in a way similar to what you do with e.g. C++ templates,
with the important difference that the language that you use for metaprogramming is
exactly the same language that you use at runtime.  Moreover, the &lt;code&gt;@blue&lt;/code&gt; functions are
executed by the familiar SPy interpreter, so they are much easier to debug, e.g. by
using &lt;code&gt;breakpoint()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Generics are just a special case of &lt;code&gt;@blue&lt;/code&gt; functions operating on types. The syntax
&lt;code&gt;MyList[T]&lt;/code&gt; is just syntax sugar over this:&lt;/p&gt;
    &lt;p&gt;Current status: generics&lt;/p&gt;
    &lt;p&gt;The syntax sugar &lt;code&gt;Type[T]&lt;/code&gt; is not implemented yet, but writing generic types in
&lt;code&gt;@blue&lt;/code&gt; function totally works and it's how SPy implement its own &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;dict&lt;/code&gt; and
&lt;code&gt;array&lt;/code&gt; types.&lt;/p&gt;
    &lt;p&gt;Zig &lt;code&gt;comptime&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;SPy &lt;code&gt;@blue&lt;/code&gt; functions have lot in common to Zig's
comptime feature.&lt;/p&gt;
    &lt;p&gt;When I originally designed SPy I didn't know about Zig (I swear!), but after showing the initial ideas about &lt;code&gt;@blue&lt;/code&gt; code, someone pointed me to Zig: I was pleasantly
surprised to see it already implemented in a very different context, because it was
an important validation that my ideas made sense.&lt;/p&gt;
    &lt;p&gt;Very recently I had the pleasure to meet Andrew Kelly, Zig's inventor and BDFL. We discussed details of both systems and we agreed that they are basically equivalent, apart from some minor differences.&lt;/p&gt;
    &lt;head rend="h3"&gt;Static dispatch¬∂&lt;/head&gt;
    &lt;p&gt;When you do &lt;code&gt;a + b&lt;/code&gt; in Python, the interpreter must execute complex logic at runtime to
determine which method to call.  In SPy we do almost the same logic, but at compile
time.  Operators are implemented as a two-phase mechanism:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;first, we lookup the implementation by examining the information which is available at compile time: in particular, the static type of the operands.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then, we call the implementation which we got at point (1).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The trick is that phase (1) is entirely &lt;code&gt;blue&lt;/code&gt;, and it's completely optimized away by
the redshifter. For example, take this code:&lt;/p&gt;
    &lt;p&gt;After redshifting it becomes:&lt;/p&gt;
    &lt;p&gt;This is a big departure from Python semantics, because we operate on the static type of the operands, as opposed to the actual types at runtime. I believe this covers the majority of use cases. For those cases in which you really need full dynamic dispatch, you can opt-in by using the type &lt;code&gt;dynamic&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Custom types can still override &lt;code&gt;__add__&lt;/code&gt; &amp;amp; co. and participate to this "blue time"
lookup logic. The details will be explained in a later post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion¬∂&lt;/head&gt;
    &lt;p&gt;Python's dynamic nature and expressivity plays a big part of why it became so popular: it allowed power users to write all the incredible libraries with very intuitive and high level APIs which we love. However, such expressivity comes with many problems in terms of performance, type safety and so on.&lt;/p&gt;
    &lt;p&gt;SPy attempts to fix those problems by constraining the dynamicity into well defined places, without hurting performance.&lt;/p&gt;
    &lt;p&gt;In the next posts of this series, we'll dive deep into how this actually works: the type system, the blue/red evaluation model, static dispatch, and the implementation of both the interpreter and the compiler. We'll see concrete examples of SPy code and explore how features like zero-cost abstractions are achieved in practice.&lt;/p&gt;
    &lt;p&gt;Want to explore SPy yourself? Visit the SPy repository on GitHub to see the code, try out examples, and follow the project's development. If you have questions, want to discuss design decisions, or are interested in contributing, join us on the SPy Discord server. SPy is still early in development, and this is a great time to get involved and help shape its future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://antocuni.eu/2025/10/29/inside-spy-part-1-motivations-and-goals/"/><published>2025-10-30T16:08:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45771151</id><title>By the Power of Grayscale</title><updated>2025-11-05T11:34:54.207520+00:00</updated><content>&lt;doc fingerprint="362f87793ff11b93"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;By the power of grayscale!&lt;/head&gt;
    &lt;p&gt;When people talk about computer vision, they usually think of OpenCV or deep neural networks like YOLO. But in most cases, doing computer vision implies understanding of the core algorithms, so you can use or adapt them for your own needs.&lt;/p&gt;
    &lt;p&gt;I wanted to see how far I could go by stripping computer vision down to the bare minimum: only grayscale 8-bit images, no fancy data structures, plain old C, some byte arrays and a single header file. After all, an image is just a rectangle of numbers, right?&lt;/p&gt;
    &lt;p&gt;This post is a guided tour through the algorithms behind Grayskull ‚Äì a minimal computer vision library designed for resource-constrained devices.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pixels&lt;/head&gt;
    &lt;p&gt;A grayscale pixel is normally represented as a single byte, where &lt;code&gt;0&lt;/code&gt; means black, &lt;code&gt;255&lt;/code&gt; means white, and values in between represent various shades of gray.&lt;/p&gt;
    &lt;p&gt;A grayscale image is essentially a 2D array of these pixels, defined by its width and height, but for a simpler memory layout languages such as C often represent it as a 1D array of size &lt;code&gt;width * height&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// An image of WxH pixels, stored as a flat array of bytes
struct gs_image { unsigned w, h; uint8_t *data; };

// Helpers to get/set pixel values respecting the bounds
uint8_t gs_get(struct gs_image img, unsigned x, unsigned y) {
  return (x &amp;lt; img.w &amp;amp;&amp;amp; y &amp;lt; img.h) ? img.data[y * img.w + x] : 0;
}
void gs_set(struct gs_image img, unsigned x, unsigned y, uint8_t value) {
  if (x &amp;lt; img.w &amp;amp;&amp;amp; y &amp;lt; img.h) img.data[y * img.w + x] = value;
}

// A somewhat convenient macro to iterate over all pixels
#define gs_for(img, x, y)                \
  for (unsigned y = 0; y &amp;lt; (img).h; y++) \
    for (unsigned x = 0; x &amp;lt; (img).w; x++)
&lt;/code&gt;
    &lt;p&gt;This humble start already allows us to do some tricks like inverting or mirroring images:&lt;/p&gt;
    &lt;code&gt;// invert image (negative): px[x,y] = 255 - px[x,y]
gs_for(img, x, y) gs_set(img, x, y, 255 - gs_get(img, x, y));

// mirror the image: swap px[x,y] with px[w-x-1,y]
gs_for(img, x, y) {
for (unsigned y = 0; y &amp;lt; img.h; y++) {
  for (unsigned x = 0; x &amp;lt; img.w/2; x++) { // iterate only through the first half
    uint8_t tmp = gs_get(img, x, y);
    gs_set(img, x, y, gs_get(img, img.w - x - 1, y));
    gs_set(img, img.w - x - 1, y, tmp);
  }
}
&lt;/code&gt;
    &lt;p&gt;We can just as well copy images, crop images, resize or rotate them. It‚Äôs not computer vision yet, but still some basic image processing:&lt;/p&gt;
    &lt;code&gt;struct gs_rect { unsigned x, y, w, h; }; // for regions of interest (ROI)

// crop image src into dst using region of interest (roi)
gs_for(roi, x, y) gs_set(dst, x, y, gs_get(src, roi.x + x, roi.y + y));

// downscale 2x: set pixel to average from 4 neighbouring pixels (2x2)
gs_for(dst, x, y) {
    unsigned sum = 0;
    for (unsigned j = 0; j &amp;lt; 2; j++)
      for (unsigned i = 0; i &amp;lt; 2; i++)
        sum += gs_get(src, x * 2 + i, y * 2 + j);
    gs_set(dst, x, y, sum / 4);
}
&lt;/code&gt;
    &lt;p&gt;One can do na√Øve nearest-neighbour resizing, which is fast but looks blocky, or do bilinear interpolation, which is slower and requires floating point operations, but often looks better:&lt;/p&gt;
    &lt;code&gt;// Nearest-neighbour resize
void gs_resize_nn(struct gs_image dst, struct gs_image src) {
    gs_for(dst, x, y) {
        unsigned sx = x * src.w / dst.w, sy = y * src.h / dst.h;
        gs_set(dst, x, y, gs_get(src, sx, sy));
    }
}

// Bilinear resize
GS_API void gs_resize(struct gs_image dst, struct gs_image src) {
  gs_for(dst, x, y) {
    float sx = ((float)x + 0.5f) * src.w / dst.w, sy = ((float)y + 0.5f) * src.h / dst.h;
    sx = GS_MAX(0.0f, GS_MIN(sx, src.w - 1.0f)), sy = GS_MAX(0.0f, GS_MIN(sy, src.h - 1.0f));
    unsigned sx_int = (unsigned)sx, sy_int = (unsigned)sy;
    unsigned sx1 = GS_MIN(sx_int + 1, src.w - 1), sy1 = GS_MIN(sy_int + 1, src.h - 1);
    float dx = sx - sx_int, dy = sy - sy_int;
    uint8_t c00 = gs_get(src, sx_int, sy_int), c01 = gs_get(src, sx1, sy_int),
            c10 = gs_get(src, sx_int, sy1), c11 = gs_get(src, sx1, sy1);
    uint8_t p = (c00 * (1 - dx) * (1 - dy)) + (c01 * dx * (1 - dy)) + (c10 * (1 - dx) * dy) + (c11 * dx * dy);
    gs_set(dst, x, y, p);
  }
}
&lt;/code&gt;
    &lt;p&gt;Here‚Äôs how the original image (left) looks after bilinear resizing (middle) compared to nearest-neighbour resizing (right).&lt;/p&gt;
    &lt;head rend="h2"&gt;Image processing&lt;/head&gt;
    &lt;p&gt;Now that we can manipulate individual pixels, we can start doing more serious image processing.&lt;/p&gt;
    &lt;p&gt;One useful tool is convolutional filters. A filter is a small 2D array (kernel) that is applied to each pixel in the image. The new pixel value is computed as a weighted sum of the neighbouring pixels, where weights are defined by the kernel.&lt;/p&gt;
    &lt;code&gt;void gs_filter(struct gs_image dst, struct gs_image src, struct gs_image kernel, unsigned norm) {
    gs_for(dst, x, y) {
        int sum = 0;
        gs_for(kernel, i, j) {
            sum += gs_get(src, x + i - kernel.w / 2, y + j - kernel.h / 2) * (int8_t)gs_get(kernel, i, j);
        }
        gs_set(dst, x, y, sum / norm);
    }
}
&lt;/code&gt;
    &lt;p&gt;This technique can be used for blurring, sharpening, edge detection and many other effects. Here are some common kernels. Note that they are defined as signed 8-bit integers:&lt;/p&gt;
    &lt;code&gt;// box blur 3x3, all pixels have equal weight
struct gs_image gs_blur_box = {3, 3, (uint8_t *)(int8_t[]){1, 1, 1, 1, 1, 1, 1, 1, 1}};
// gaussian blur 3x3, central pixels have more weight
struct gs_image gs_blur_gaussian = {3, 3, (uint8_t *)(int8_t[]){1, 2, 1, 2, 4, 2, 1, 2, 1}};
// sharpen, enhance edges
struct gs_image gs_sharpen = {3, 3, (uint8_t *)(int8_t[]){0, -1, 0, -1, 5, -1, 0, -1, 0}};
// emboss, make image look "3D"
struct gs_image gs_emboss = {3, 3, (uint8_t *)(int8_t[]){-2, -1, 0, -1, 1, 1, 0, 1, 2}};
&lt;/code&gt;
    &lt;p&gt;Similarly, we can apply Sobel filters, that are useful if we want to detect edges in the image:&lt;/p&gt;
    &lt;code&gt;struct gs_image gs_sobel_x = {3, 3, (uint8_t *)(int8_t[]){-1, 0, 1, -2, 0, 2, -1, 0, 1}};
struct gs_image gs_sobel_y = {3, 3, (uint8_t *)(int8_t[]){1, 2, 1, 0, 0, 0, -1, -2, -1}};

void gs_sobel(struct gs_image dst, struct gs_image src) {
    struct gs_image gx = {src.w, src.h, malloc(src.w * src.h)};
    struct gs_image gy = {src.w, src.h, malloc(src.w * src.h)};
    gs_filter(gx, src, gs_sobel_x, 1);
    gs_filter(gy, src, gs_sobel_y, 1);
    gs_for(dst, x, y) {
        int mag = sqrt(gs_get(gx, x, y) * gs_get(gx, x, y) + gs_get(gy, x, y) * gs_get(gy, x, y));
        gs_set(dst, x, y, GS_MIN(mag, 255));
    }
    free(gx.data);
    free(gy.data);
}
&lt;/code&gt;
    &lt;p&gt;Here are some examples of these filters, notice how some of them remove the noise or enhance the edges:&lt;/p&gt;
    &lt;p&gt;The first image is the original, followed by box filter and Gaussian blur filter. Next is the sharpen filter, then emboss filter and finally Sobel filter that highlights edges.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thresholding&lt;/head&gt;
    &lt;p&gt;To actually ‚Äúsee‚Äù objects in an image, we need to segment it into foreground and background, and then operate on the foreground segments, trying to locate objects of interest.&lt;/p&gt;
    &lt;p&gt;This is much easier done when an image is binary, so that each pixel is either fully black or fully white. This conversion from grayscale to black-and-white is called thresholding.&lt;/p&gt;
    &lt;p&gt;In the simplest case we may consider any pixel above 127 white, and below ‚Äì black. This is fixed-level thresholding. Of course, in darker environments this thresholding value could be too high and many meaningful details would be lost. So how to pick a threshold value more accurately?&lt;/p&gt;
    &lt;code&gt;// apply fixed threshold value and binarise the image
GS_API void gs_threshold(struct gs_image img, uint8_t thresh) {
  for (unsigned i = 0; i &amp;lt; img.w * img.h; i++) img.data[i] = (img.data[i] &amp;gt; thresh) ? 255 : 0;
}
&lt;/code&gt;
    &lt;p&gt;One approach would be to calculate the brightness distribution as a histogram. We know that there are 255 unique values of pixels in a grayscale image, so we iterate through all pixels and count how many of them have this or that value. Analysing the resulting histogram could give us some clue about which pixel value would work best as a threshold for the particular image.&lt;/p&gt;
    &lt;p&gt;A clever way to do this is with Otsu‚Äôs method. It automatically determines the optimal threshold by testing every possible value (from 0 to 255). For each value, it splits the image‚Äôs pixels into two classes‚Äîbackground and foreground‚Äîand calculates their ‚Äúinter-class variance.‚Äù The threshold that maximizes this variance is the one that creates the best separation between the two classes, making it the ideal choice. It works fiarly well on images with good contrast:&lt;/p&gt;
    &lt;code&gt;// try to find the best threshold to separate "foreground" from "background"
uint8_t gs_otsu_threshold(struct gs_image img) {
  unsigned hist[256] = {0}, wb = 0, wf = 0, threshold = 0;
  // calculate how many pixels of each brightness value we have
  for (unsigned i = 0; i &amp;lt; img.w * img.h; i++) hist[img.data[i]]++;
  float sum = 0, sumB = 0, varMax = -1.0;
  for (unsigned i = 0; i &amp;lt; 256; i++) sum += (float)i * hist[i];
  // try to find the threshold that maximises inter-class variance
  for (unsigned t = 0; t &amp;lt; 256; t++) {
    wb += hist[t];
    if (wb == 0) continue;
    wf = (img.w * img.h) - wb;
    if (wf == 0) break;
    sumB += (float)t * hist[t];
    float mB = (float)sumB / wb;
    float mF = (float)(sum - sumB) / wf;
    float varBetween = (float)wb * (float)wf * (mB - mF) * (mB - mF);
    if (varBetween &amp;gt; varMax) varMax = varBetween, threshold = t;
  }
  return threshold;
}
&lt;/code&gt;
    &lt;p&gt;In real life, however, the lighting conditions are often uneven. In such cases a single global threshold may not work well, and neither of the possible 255 threshold values would give a good result.&lt;/p&gt;
    &lt;p&gt;To address this, we can use adaptive thresholding. Instead of using a single threshold for the entire image, we compute a local threshold for each pixel based on the average brightness of its neighbouring pixels. This way, we can better handle varying lighting conditions across the image:&lt;/p&gt;
    &lt;code&gt;gs_for(src, x, y) {
  unsigned sum = 0, count = 0;
  for (int dy = -radius; dy &amp;lt;= (int)radius; dy++) {
    for (int dx = -radius; dx &amp;lt;= (int)radius; dx++) {
      int sy = (int)y + dy, sx = (int)x + dx;
      if (sy &amp;gt;= 0 &amp;amp;&amp;amp; sy &amp;lt; (int)src.h &amp;amp;&amp;amp; sx &amp;gt;= 0 &amp;amp;&amp;amp; sx &amp;lt; (int)src.w) {
        sum += gs_get(src, sx, sy);
        count++;
      }
    }
  }
  int threshold = sum / count - c;
  gs_set(dst, x, y, (gs_get(src, x, y) &amp;gt; threshold) ? 255 : 0);
}
&lt;/code&gt;
    &lt;p&gt;Compare how various thresholding approaches work on the same image:&lt;/p&gt;
    &lt;p&gt;The first image is the original, followed by fixed-level thresholding (80), then Otsu‚Äôs method and adaptive thresholding. Notice how adaptive thresholding preserves more details in both bright and dark areas of the image, while Otsu‚Äôs method struggles with uneven lighting.&lt;/p&gt;
    &lt;head rend="h2"&gt;Morphological operations&lt;/head&gt;
    &lt;p&gt;Due to the way how image sensors work in cameras, images often contain noise.&lt;/p&gt;
    &lt;p&gt;This means that after thresholding there would be random individual pixels that do not belong to any object, or small holes in objects, or small gaps between object parts. All of this will confuse the object detection algorithms, but morphological operations can help to clean up the binary image.&lt;/p&gt;
    &lt;p&gt;Two most common operations are erosion and dilation. Erosion removes pixels on object boundaries (shrinking the objects), while dilation adds pixels to their boundaries (expanding the objects).&lt;/p&gt;
    &lt;p&gt;There is also opening (erosion followed by dilation) and closing (dilation followed by erosion). Opening is useful for removing small objects or noise, while closing is useful for filling small holes in objects.&lt;/p&gt;
    &lt;code&gt;void gs_erode(struct gs_image dst, struct gs_image src, unsigned radius) {
  gs_for(dst, x, y) {
    uint8_t min_val = 255;
    for (int dy = -radius; dy &amp;lt;= (int)radius; dy++) {
      for (int dx = -radius; dx &amp;lt;= (int)radius; dx++) {
        int sy = (int)y + dy, sx = (int)x + dx;
        if (sy &amp;gt;= 0 &amp;amp;&amp;amp; sy &amp;lt; (int)src.h &amp;amp;&amp;amp; sx &amp;gt;= 0 &amp;amp;&amp;amp; sx &amp;lt; (int)src.w) {
          uint8_t val = gs_get(src, sx, sy);
          if (val &amp;lt; min_val) min_val = val;
        }
      }
    }
    gs_set(dst, x, y, min_val);
  }
}
void gs_dilate(struct gs_image dst, struct gs_image src, unsigned radius) {
  gs_for(dst, x, y) {
    uint8_t max_val = 0;
    for (int dy = -radius; dy &amp;lt;= (int)radius; dy++) {
      for (int dx = -radius; dx &amp;lt;= (int)radius; dx++) {
        int sy = (int)y + dy, sx = (int)x + dx;
        if (sy &amp;gt;= 0 &amp;amp;&amp;amp; sy &amp;lt; (int)src.h &amp;amp;&amp;amp; sx &amp;gt;= 0 &amp;amp;&amp;amp; sx &amp;lt; (int)src.w) {
          uint8_t val = gs_get(src, sx, sy);
          if (val &amp;gt; max_val) max_val = val;
        }
      }
    }
    gs_set(dst, x, y, max_val);
  }
}
&lt;/code&gt;
    &lt;p&gt;Here‚Äôs how morphological operations can clean up a rather confusing image with ArUco markers:&lt;/p&gt;
    &lt;p&gt;The first image is the original, followed by thresholded image (Otsu). Next we do erosion followed by dilation. Since the markers are black it might sound odd, but since morpological operations work on white pixels we essentially perform closing, but for black pixels. We could also invert the image first, then do opening and invert back.&lt;/p&gt;
    &lt;p&gt;At the end all markers are shrinked to their original dimensions and all of them are easily detectable by shape and size.&lt;/p&gt;
    &lt;head rend="h2"&gt;Blobs and contours&lt;/head&gt;
    &lt;p&gt;Once we have a clean binary image, we can start detecting objects in it. A classic way to do this is by finding connected components (blobs).&lt;/p&gt;
    &lt;p&gt;A ‚Äúblob‚Äù is a group of connected white pixels (255) that form an object. The simplest way to find blobs is by using a flood-fill algorithm or depth-first search (DFS) to label connected pixels:&lt;/p&gt;
    &lt;code&gt;void gs_flood_fill(struct gs_image img, unsigned x, unsigned y, uint8_t target, uint8_t replacement) {
  if (x &amp;gt;= img.w || y &amp;gt;= img.h) return;
  if (gs_get(img, x, y) != target || gs_get(img, x, y) == replacement) return;
  gs_set(img, x, y, replacement);
  gs_flood_fill(img, x + 1, y, target, replacement);
  gs_flood_fill(img, x - 1, y, target, replacement);
  gs_flood_fill(img, x, y + 1, target, replacement);
  gs_flood_fill(img, x, y - 1, target, replacement);
}
&lt;/code&gt;
    &lt;p&gt;Of course this would immediately blow up your stack on most real-life images. So an iterative approach using a queue or stack is preferred.&lt;/p&gt;
    &lt;p&gt;However, it‚Äôs still a fairly suboptimal way to find blobs. A more efficient approach is a two-pass algorithm, which scans the image twice: first to assign ‚Äúprovisional‚Äù labels and record equivalences between them, followed by the second scan to resolve these equivalences and assign final unique labels to every blob.&lt;/p&gt;
    &lt;p&gt;It would be nice if we could use the same &lt;code&gt;gs_image&lt;/code&gt; type for labels, but in most cases it would require more than 256 labels (especially for temporary provisional labels). So we need a separate array of larger integers to store labels:&lt;/p&gt;
    &lt;code&gt;typedef uint16_t gs_label; // 64K should be enough, right?

struct gs_blob {
  gs_label label; // what label the blob has?
  unsigned area; // how many white pixels are in a blob?
  struct gs_rect box; // bounding box
  struct gs_point centroid; // center of "mass"
};
unsigned gs_blobs(struct gs_image img, gs_label *labels, struct gs_blob *blobs, unsigned nblobs) { ... }
&lt;/code&gt;
    &lt;p&gt;Before we go futher, let‚Äôs talk about connectivity. There are two common types of pixel connectivity: 4-connectivity and 8-connectivity. In 4-connectivity, a pixel is connected to its four direct neighbours (up, down, left, right). In 8-connectivity, a pixel is connected to all eight surrounding pixels (including diagonals). So, in case of 8-connectivity this would be one blob and in case of 4-connectivity - two separate blobs:&lt;/p&gt;
    &lt;code&gt;......
.#..#.
.##.#.
.###..
......
&lt;/code&gt;
    &lt;p&gt;We will be using 4-connectivity in our implementation, for simplicity. Let‚Äôs consider the following image:&lt;/p&gt;
    &lt;code&gt;.........
.###..#..
.###.##..
.#####..#
.......##
&lt;/code&gt;
    &lt;p&gt;We start scanning it row after row, if a white pixel is found, we check its left and top neighbours:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If both are black (0), we assign a new label to the pixel, assuming that it could be a new blob.&lt;/item&gt;
      &lt;item&gt;If one of them is white, we assign its label to the current pixel, as it is a continuation of the existing known blob.&lt;/item&gt;
      &lt;item&gt;If both are white but have different labels, we assign the smallest of the labels to the current pixel and record the equivalence between the two labels in a special data structure (like a union-find).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After this first pass the labels array would look like this:&lt;/p&gt;
    &lt;code&gt;.........
.111..2..
.111.32..
.11111..4
.......55
&lt;/code&gt;
    &lt;p&gt;Our equivalence table would look like this: &lt;code&gt;1 &amp;lt;-&amp;gt; 3, 3 &amp;lt;-&amp;gt; 2, 4 &amp;lt;-&amp;gt; 5&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;During the second pass we resolve the equivalences and assign final labels to each pixel. The final labels array would look like this:&lt;/p&gt;
    &lt;code&gt;.........
.111..1..
.111.11..
.11111..4
.......44
&lt;/code&gt;
    &lt;p&gt;During the second pass we can also calculate blob properties, such as area, bounding box and centroid. Largest blob‚Äôs area is 14 pixels, bounding box is from (1,1) to (6,3). Centroid is calculated as the average of all pixel coordinates in the blob, in our case for X it would be &lt;code&gt;(3*1+3*2+3*3+4+2*5+2*6)/14&lt;/code&gt;, or rounded to 4. Y coordinate would be &lt;code&gt;(4*1+5*2+5*3)/14&lt;/code&gt;, or rounded to 2. So the centroid is (4,2), which is not part of the blob ‚Äúbody‚Äù, but still is the center of mass.&lt;/p&gt;
    &lt;p&gt;Such geometric properties already give us plenty of information about blobs. For example, we can filter blobs by area to remove small noise blobs. We can also calculate aspect ratio (width/height) of the bounding box to filter out very tall or very wide blobs. Ratio between blob actual area and bounding box area helps to filter out blobs that are not compact enough. Rectangles tend to have a ratio of &lt;code&gt;1.0&lt;/code&gt;, circles are &lt;code&gt;pi/4 = 0.785&lt;/code&gt;, while lines approach to zero.&lt;/p&gt;
    &lt;p&gt;Another clues could be centroid position, orientation using moments or contour shape. There is a fairly simple method to trace a contour of a blob using the Moore-Neighbor tracing algorithm. It starts from a known border pixel and follows the contour clockwise by checking neighbouring pixels, until it returns to the starting pixel:&lt;/p&gt;
    &lt;code&gt;struct gs_contour { struct gs_rect box; struct gs_point start; unsigned length; };
void gs_trace_contour(struct gs_image img, struct gs_image visited, struct gs_contour *c) {
  static const int dx[] = {1, 1, 0, -1, -1, -1, 0, 1};
  static const int dy[] = {0, 1, 1, 1, 0, -1, -1, -1};
  c-&amp;gt;length = 0;
  c-&amp;gt;box = (struct gs_rect){c-&amp;gt;start.x, c-&amp;gt;start.y, 1, 1};
  struct gs_point p = c-&amp;gt;start;
  unsigned dir = 7, seenstart = 0;
  for (;;) {
    if (!visited.data[p.y * visited.w + p.x]) c-&amp;gt;length++;
    visited.data[p.y * visited.w + p.x] = 255;
    int ndir = (dir + 1) % 8, found = 0;
    for (int i = 0; i &amp;lt; 8; i++) {
      int d = (ndir + i) % 8, nx = p.x + dx[d], ny = p.y + dy[d];
      if (nx &amp;gt;= 0 &amp;amp;&amp;amp; nx &amp;lt; (int)img.w &amp;amp;&amp;amp; ny &amp;gt;= 0 &amp;amp;&amp;amp; ny &amp;lt; (int)img.h &amp;amp;&amp;amp;
          img.data[ny * img.w + nx] &amp;gt; 128) {
        p = (struct gs_point){nx, ny};
        dir = (d + 6) % 8;
        found = 1;
        break;
      }
    }
    if (!found) break;  // open contour
    c-&amp;gt;box.x = GS_MIN(c-&amp;gt;box.x, p.x);
    c-&amp;gt;box.y = GS_MIN(c-&amp;gt;box.y, p.y);
    c-&amp;gt;box.w = GS_MAX(c-&amp;gt;box.w, p.x - c-&amp;gt;box.x + 1);
    c-&amp;gt;box.h = GS_MAX(c-&amp;gt;box.h, p.y - c-&amp;gt;box.y + 1);
    if (p.x == c-&amp;gt;start.x &amp;amp;&amp;amp; p.y == c-&amp;gt;start.y) {
      if (seenstart) break;
      seenstart = 1;
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;This way we can extract contours for each blob and analyse their shapes or compare contour length (perimeter) to the blob area. It is also possible to approximate contours with straight lines using Douglas-Peucker algirithm, which replaces a curve with a series of straight line segments, while preserving the overall shape.&lt;/p&gt;
    &lt;p&gt;All of this is good if we want to detect simple objects in a static image. But what if we want to detect or track more complex objects, like faces, cars or pedestrians?&lt;/p&gt;
    &lt;head rend="h2"&gt;Keypoints and descriptors&lt;/head&gt;
    &lt;p&gt;A keypoint is a specific location in an image that is distinctive and can be reliably detected no matter what scale, rotation or lighting of the object are.&lt;/p&gt;
    &lt;p&gt;In practice, keypoints are often corners (also known as ‚Äúfeatures‚Äù). One of the most intuitive algorithms to detect features is FAST (Features from Accelerated Segment Test). It works by examining a circle of 16 pixels around a candidate pixel (4 pixels away from it). If at least 9 of contiguous pixels in this &lt;code&gt;r=4px&lt;/code&gt; circle around pixel &lt;code&gt;P&lt;/code&gt; are all brighter or all darker - then the candidate pixel is considered a corner:&lt;/p&gt;
    &lt;code&gt;..............
......012.....
.....F...3....
....E.....4...
....D..P..5...
....C.....6...
.....B...7....
......A98.....
..............
&lt;/code&gt;
    &lt;p&gt;As simple as it gets, this approach finds too many features in real-life images. A solution is to keep ‚Äúscore‚Äù of each feature and only keep the points with the highest score. The score can be calculated as the sum of absolute differences between the candidate pixel and the contiguous pixels in the circle, or by the minimum difference between central pixel and the pixels on the circle.&lt;/p&gt;
    &lt;code&gt;  gs_assert(gs_valid(img) &amp;amp;&amp;amp; kps &amp;amp;&amp;amp; nkps &amp;gt; 0);
  static const int dx[16] = {0, 1, 2, 3, 3, 3, 2, 1, 0, -1, -2, -3, -3, -3, -2, -1};
  static const int dy[16] = {-3, -3, -2, -1, 0, 1, 2, 3, 3, 3, 2, 1, 0, -1, -2, -3};
  unsigned n = 0;
  // first pass: compute score map
  for (unsigned y = 3; y &amp;lt; img.h - 3; y++) {
    for (unsigned x = 3; x &amp;lt; img.w - 3; x++) {
      uint8_t p = img.data[y * img.w + x];
      int run = 0, score = 0;
      for (int i = 0; i &amp;lt; 16 + 9; i++) {
        int idx = (i % 16);
        uint8_t v = img.data[(y + dy[idx]) * img.w + (x + dx[idx])];
        if (v &amp;gt; p + threshold) {
          run = (run &amp;gt; 0) ? run + 1 : 1;
        } else if (v &amp;lt; p - threshold) {
          run = (run &amp;lt; 0) ? run - 1 : -1;
        } else {
          run = 0;
        }
        if (run &amp;gt;= 9 || run &amp;lt;= -9) {
          score = 255;
          for (int j = 0; j &amp;lt; 16; j++) {
            int d = gs_get(img, x + dx[j], y + dy[j]) - p;
            if (d &amp;lt; 0) d = -d;
            if (d &amp;lt; score) score = d;
          }
          break;
        }
      }
      scoremap.data[y * img.w + x] = score;
    }
  }
  // second pass: non-maximum suppression
  for (unsigned y = 3; y &amp;lt; img.h - 3; y++) {
    for (unsigned x = 3; x &amp;lt; img.w - 3; x++) {
      int s = scoremap.data[y * img.w + x], is_max = 1;
      if (s == 0) continue;
      for (int yy = -1; yy &amp;lt;= 1 &amp;amp;&amp;amp; is_max; yy++) {
        for (int xx = -1; xx &amp;lt;= 1; xx++) {
          if (xx == 0 &amp;amp;&amp;amp; yy == 0) continue;
          if (scoremap.data[(y + yy) * img.w + (x + xx)] &amp;gt; s) {
            is_max = 0;
            break;
          }
        }
      }
      if (is_max &amp;amp;&amp;amp; n &amp;lt; nkps) kps[n++] = (struct gs_keypoint){{x, y}, (unsigned)s, 0, {0}};
    }
  }
  return n;
}
&lt;/code&gt;
    &lt;p&gt;Notice how keypoints are detected on corners and distinctive features of the cat image, such as eyes, nose, whiskers and other ‚Äúmeaningful‚Äù corners. But how to use keypoints to detect objects?&lt;/p&gt;
    &lt;p&gt;This is where ORB enters the stage. ORB builds on top of the same FAST corner detector, it tries to find sharp intensity changes, but also adds two more components: orientation and descriptor.&lt;/p&gt;
    &lt;p&gt;Once the corners are found, ORB estimates their orientation by calculating image moments in a small patch around each keypoint. This way, each keypoint gets an angle, basically saying which way is ‚Äúup‚Äù for this keypoint.&lt;/p&gt;
    &lt;p&gt;Then comes the descriptor. A descriptor is a compact representation of the local image patch around the keypoint, designed to be invariant to scale and rotation. ORB uses a modified version of BRIEF (Binary Robust Independent Elementary Features) descriptor, which is a clever way to encode image patch as a small bit string.&lt;/p&gt;
    &lt;p&gt;It simply compares bit intensities, if one pixel is lighter than another, it sets the corresponding bit to &lt;code&gt;1&lt;/code&gt;, otherwise to &lt;code&gt;0&lt;/code&gt;. By performing multiple such comparisons, we can create a binary string that represents the local image patch.&lt;/p&gt;
    &lt;p&gt;The descriptor is 256 bit long, and for each of the 256 ‚Äúsamples‚Äù we pick two sampling points within the patch area using some pseudo-random lookup table and encode the following bit as 0 or 1 depending on the pixel relative values.&lt;/p&gt;
    &lt;p&gt;Comparing keypoints becomes trivial, we simply XOR two bitstrings and count the bits.&lt;/p&gt;
    &lt;p&gt;Since keypoints are agnostic to rotation and lighting conditions, we can use them to detect objects in various scenarios.&lt;/p&gt;
    &lt;p&gt;One last addition to this algorithm is to resize/scale the image multiple times and detect keypoints at different scales. This way we can detect objects that are closer or further away from the camera, rotated at any angle, or partially occluded.&lt;/p&gt;
    &lt;head rend="h2"&gt;LBP Cascades&lt;/head&gt;
    &lt;p&gt;While keypoints and descriptors are great for detecting arbitrary objects, sometimes we need a more specialised approach for specific object types, like faces, vehicles, or hand gestures. This is where cascade classifiers, famously used in the Viola-Jones object detection framework, come into play.&lt;/p&gt;
    &lt;p&gt;Instead of complex Haar-like features used by Viola and Jones, we can use something simpler: Local Binary Patterns (LBP). LBP is a powerful texture descriptor. For each pixel, it looks at its 8 neighbours. If a neighbour is brighter than the central pixel, we write a ‚Äò1‚Äô, otherwise a ‚Äò0‚Äô. This gives us an 8-bit number that describes the local texture.&lt;/p&gt;
    &lt;p&gt;The ‚Äúcascade‚Äù is a series of simple classifiers, or ‚Äústages‚Äù. Each stage looks at a sub-window of the image and uses a few LBP features to decide if that window could possibly contain the object of interest (e.g., a face).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If the sub-window fails the test at any stage, it‚Äôs immediately rejected. This is very fast.&lt;/item&gt;
      &lt;item&gt;Only if a sub-window passes all stages is it classified as a positive detection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This structure allows the classifier to quickly discard the vast majority of the image, focusing computational power only on promising regions. By sliding this detection window across the entire image (and across multiple scales), we can find objects of a specific, pre-trained class. Grayskull provides a pre-trained frontal face detector that uses this exact technique.&lt;/p&gt;
    &lt;p&gt;Here you can see the LBP cascade classifier in action, successfully detecting Sir Gary Oldman in all variety of his faces. The picture on the left uses minimum number of neighbours set to 4, while the right one uses 14. This means the left image detects more faces, but also has more false positives, while the right one is more conservative and only detect camera-facing images with no visual obstructions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;We‚Äôve taken a journey from the humble pixel to sophisticated object detection, all using simple C structures and fundamental algorithms. We tried to manipulate pixels, apply filters to enhance images, segment objects using thresholding, and clean them up. We then learned to find and analyze blobs, detect robust keypoints with FAST and ORB, and finally, use LBP cascades for specialized object detection.&lt;/p&gt;
    &lt;p&gt;This is the core philosophy of Grayskull: to demystify computer vision by providing a minimal, dependency-free, and understandable toolkit. It proves that you don‚Äôt always need massive libraries or deep learning frameworks to achieve decent results, especially on resource-constrained systems.&lt;/p&gt;
    &lt;p&gt;An image is indeed just a rectangle of numbers, and with a bit of algorithmic knowledge, you have the power to make it see. As always, I encourage you to check out the repository, experiment with the code, and maybe even try building your own simple CV project!&lt;/p&gt;
    &lt;p&gt;I hope you‚Äôve enjoyed this article. You can follow ‚Äì and contribute to ‚Äì on Github, Mastodon, Twitter or subscribe via rss.&lt;/p&gt;
    &lt;p&gt;Oct 26, 2025&lt;/p&gt;
    &lt;p&gt;See also: √âtude in C minor and more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://zserge.com/posts/grayskull/"/><published>2025-10-31T12:11:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45788385</id><title>Intervaltree with Rust Back End</title><updated>2025-11-05T11:34:53.583010+00:00</updated><content>&lt;doc fingerprint="1a5b32fdd0e4e110"&gt;
  &lt;main&gt;
    &lt;p&gt;This crate exposes an interval tree implementation written in Rust to Python via PyO3. The Python wrapper provides the ability to build a tree from tuples, insert additional intervals, search for overlaps, and delete intervals by their &lt;code&gt;(left, right)&lt;/code&gt; key.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust toolchain (for compiling the extension module)&lt;/item&gt;
      &lt;item&gt;Python 3.8+&lt;/item&gt;
      &lt;item&gt;maturin for building/installing the package&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python -m venv .venv
source .venv/bin/activate
pip install maturin
maturin develop&lt;/code&gt;
    &lt;p&gt;You can install the package with (also with uv)&lt;/p&gt;
    &lt;code&gt;pip install intervaltree_rs
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;maturin develop&lt;/code&gt; builds the extension module in-place and installs it into the active virtual environment, making it importable as &lt;code&gt;intervaltree_rs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Once installed, you can use the interval tree directly from Python:&lt;/p&gt;
    &lt;code&gt;from intervaltree_rs import IntervalTree

# Build a tree from tuples: (left, right, payload)
intervals = [
    (5, 10, "a"),
    (12, 18, "b"),
    (1, 4, "c"),
]
tree = IntervalTree.from_tuples(intervals)

# Insert another interval
tree.insert((8, 11, "d"))

# Search for overlaps. Inclusive bounds are enabled by default.
hits = tree.search(9, 10)
for left, right, value in hits:
    print(left, right, value)

# Delete by the interval key
removed = tree.delete((12, 18))
print("Removed:", removed)&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;IntervalTree.search(ql, qr, inclusive=True)&lt;/code&gt; accepts an &lt;code&gt;inclusive&lt;/code&gt; flag. Set it to &lt;code&gt;False&lt;/code&gt; to perform exclusive range queries.&lt;/p&gt;
    &lt;p&gt;To build a wheel that you can distribute or upload to PyPI, run:&lt;/p&gt;
    &lt;code&gt;maturin build --release&lt;/code&gt;
    &lt;p&gt;The built wheels will be placed under &lt;code&gt;target/wheels/&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The Python bindings are covered by Rust unit tests. Run them with:&lt;/p&gt;
    &lt;code&gt;cargo test&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Athe-kunal/intervaltree_rs"/><published>2025-11-02T07:04:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45812024</id><title>This week in 1988, Robert Morris unleashed his eponymous worm</title><updated>2025-11-05T11:34:53.299893+00:00</updated><content>&lt;doc fingerprint="ff40223f0be08a6f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;37 years ago this week, the Morris worm infected 10% of the Internet within 24 hours ‚Äî worm slithered out and sparked a new era in cybersecurity&lt;/head&gt;
    &lt;p&gt;The Internet contracted worms a year before the World Wide Web was even a thing.&lt;/p&gt;
    &lt;p&gt;This week in 1988, Cornell graduate student Robert Tappan Morris unleashed his eponymous worm upon the Internet. The wave of infections grew to 10% of the entire Internet within 24 hours, causing astronomically expensive damage for the time. However, the pioneering Morris worm malware wasn‚Äôt made with malice, says an FBI retrospective on the ‚Äúprogramming error.‚Äù It was designed to gauge the size of the Internet, resulting in a classic case of unintended consequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Morris worm dissection&lt;/head&gt;
    &lt;p&gt;Known to be something of a prankster, Morris must have felt some foreboding about releasing his ‚Äòinnocent‚Äô program into the wild. Evidence of this comes from his release method. ‚ÄúHe released it by hacking into an MIT computer from his Cornell terminal in Ithaca, New York,‚Äù according to the FBI.&lt;/p&gt;
    &lt;p&gt;The Morris worm was written in C and targeted BSD UNIX systems, like VAX and Sun-3 machines. Specifically, the FBI writes, it ‚Äúexploited a backdoor in the Internet‚Äôs electronic mail system and a bug in the ‚Äòfinger‚Äô program that identified network users.‚Äù In contrast to computer viruses, the worm Morris had devised had no need of a host program, but could self-replicate and spread autonomously.&lt;/p&gt;
    &lt;p&gt;Thankfully, the Morris worm wasn‚Äôt written to cause damage to files. Due to those unintended consequences, though, it precipitated massive slowdowns, and messaging delays and system crashes were common symptoms. It became a computer news sensation in the worst possible way. Just to get rid of the worm in a timely fashion, some institutions ended up wiping complete systems and unplugging networks for as long as a week.&lt;/p&gt;
    &lt;p&gt;Among the Morris worm's casualties were prestigious institutions such as Berkeley, Harvard, Princeton, Stanford, Johns Hopkins, NASA, and the Lawrence Livermore National Laboratory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whodunit?&lt;/head&gt;
    &lt;p&gt;Experts worked hard to find a fix, and while they did so, the question of who was behind the worm came to the fore. Understandably, whoever created and unleashed this worm needed to feel some consequences, and thus, the FBI was brought in.&lt;/p&gt;
    &lt;p&gt;Apparently, Morris sought to anonymously explain and apologize for the worm, but an inadvertent slip of his initials by a friend landed Morris in it.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;FBI interviews and computer file analysis would subsequently confirm Morris was the culprit. He was indicted under the rather freshly inked Computer Fraud and Abuse Act of 1986. After a court appearance for his misdemeanors in 1989, Morris ended up not with jail time, but with a fine, probation, and 400 hours of community service to complete.&lt;/p&gt;
    &lt;head rend="h2"&gt;Computer worms have been around longer than the World Wide Web&lt;/head&gt;
    &lt;p&gt;Back in November 1988, the Internet bore little resemblance to what it is today. For example, the World Wide Web (WWW) wasn‚Äôt even a thing. Though the WWW would soon form the core experience for the first tide of surfers in the 90s.&lt;/p&gt;
    &lt;p&gt;At the time, the Internet‚Äôs backbone was the NSFNET, the recent successor to ARPANET. Its purpose was mostly to expand the prior backbone‚Äôs reach beyond military and defense institutions, and it more broadly embraced academia. While we are here, it is worth mentioning that NSFNET was decommissioned in 1995, and succeeded by the commercial Internet, which emerged in the 1990s off the back of private ISPs and commercial backbones.&lt;/p&gt;
    &lt;p&gt;So, when we talk about 10% of the Internet being paralyzed by the Morris Worm, contemporary estimates are that about 6,000 of the approximately 60,000 connected systems were infected and impacted. Moreover, when we highlighted the potentially massive costs of this first worm propagating, estimates range from $100,000 to millions of dollars.&lt;/p&gt;
    &lt;p&gt;Computer worms have remained a scary phenomenon in recent times. For example, we reported on the first-generation AI worm, the Morris II generative AI worm, last year.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;sb5k&lt;/header&gt;I was working at DEC when the worm slithered its way across the Internet, as part of an engineering team. I also helped manage our Ultrix systems; our IT department knew VMS only.Reply&lt;lb/&gt;I don't remember which CPU was in our systems, but the worm was not able to run on our systems, but I did find it dropped in them.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Gaston404&lt;/header&gt;I completely disagree with the tone of the article. Depicting this as an accident without consequences and limited effect is simply incorrect.Reply&lt;lb/&gt;Back then as a part time job I managed some of the traffic routing through Washington DC. Mail relays were shutdown and backed up queues were spooked off to tape. By today‚Äôs standards the volume of traffic may seem trivial but when many of these links ran at 56kbps or less. It was a mess. The main way administrators communicated with each other was email. This also affected collaboration between University researchers and access to the NSF super computer centers.&lt;lb/&gt;At the time rumors maintained that Morris used exploits that he learned from his father who had a consulting agreement with the NSA. So if this is true there is a certain level of non-originality.&lt;lb/&gt;On one hand stronger persecution may have reduced follow on internet crime. On the other hand the fragility demonstrated by this crime, resulted in the creation of procedures to deal with outages. If anything the naive sense of trusted collaboration that pervaded the Internet started to fade.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;derekullo&lt;/header&gt;In 9 years, Tiktok has infected over 90% of the internet!Reply&lt;lb/&gt;Much slower but also much more insidious!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;DS426&lt;/header&gt;Reply&lt;quote/&gt;The next big social media craze is probably just around the corner. I shutter to think how ludicrous it will be.derekullo said:In 9 years, Tiktok has infected over 90% of the internet!&lt;lb/&gt;Much slower but also much more insidious!&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/cyber-security/on-this-day-in-1988-the-morris-worm-slithered-out-and-sparked-a-new-era-in-cybersecurity-10-percent-of-the-internet-was-infected-within-24-hours"/><published>2025-11-04T15:23:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45812606</id><title>Pg_lake: Postgres with Iceberg and data lake access</title><updated>2025-11-05T11:34:52.589730+00:00</updated><content>&lt;doc fingerprint="fb9ba072642955e8"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; integrates Iceberg and data lake files into Postgres. With the &lt;code&gt;pg_lake&lt;/code&gt; extensions, you can use Postgres as a stand-alone lakehouse system that supports transactions and fast queries on Iceberg tables, and can directly work with raw data files in object stores like S3.&lt;/p&gt;
    &lt;p&gt;At a high level, &lt;code&gt;pg_lake&lt;/code&gt; lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create and modify Iceberg tables directly from PostgreSQL, with full transactional guarantees and query them from other engines&lt;/item&gt;
      &lt;item&gt;Query and import data files in object storage in Parquet, CSV, JSON, and Iceberg format&lt;/item&gt;
      &lt;item&gt;Export query results back to object storage in Parquet, CSV, or JSON formats using COPY commands&lt;/item&gt;
      &lt;item&gt;Read geospatial formats supported by GDAL, such as GeoJSON and Shapefiles&lt;/item&gt;
      &lt;item&gt;Use the built-in map type for semi-structured or key‚Äìvalue data&lt;/item&gt;
      &lt;item&gt;Combine heap, Iceberg, and external Parquet/CSV/JSON files in the same SQL queries and modifications ‚Äî all with full transactional guarantees and no SQL limitations&lt;/item&gt;
      &lt;item&gt;Infer table columns and types from external data sources such as Iceberg, Parquet, JSON, and CSV files&lt;/item&gt;
      &lt;item&gt;Leverage DuckDB‚Äôs query engine underneath for fast execution without leaving Postgres&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are two ways to set up &lt;code&gt;pg_lake&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Using Docker, for an easy, ready-to-run test environment.&lt;/item&gt;
      &lt;item&gt;Building from source, for a manual setup or development use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both approaches include the PostgreSQL extensions, the &lt;code&gt;pgduck_server&lt;/code&gt; application and setting up S3-compatible storage.&lt;/p&gt;
    &lt;p&gt;Follow the Docker README to set up and run &lt;code&gt;pg_lake&lt;/code&gt; with Docker.&lt;/p&gt;
    &lt;p&gt;Once you‚Äôve built and installed the required components, you can initialize &lt;code&gt;pg_lake&lt;/code&gt; inside Postgres.&lt;/p&gt;
    &lt;p&gt;Create all required extensions at once using &lt;code&gt;CASCADE&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;CREATE EXTENSION pg_lake CASCADE;
NOTICE:  installing required extension "pg_lake_table"
NOTICE:  installing required extension "pg_lake_engine"
NOTICE:  installing required extension "pg_extension_base"
NOTICE:  installing required extension "pg_lake_iceberg"
NOTICE:  installing required extension "pg_lake_copy"
CREATE EXTENSION&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;pgduck_server&lt;/code&gt; is a standalone process that implements the Postgres wire-protocol (locally), and underneath uses &lt;code&gt;DuckDB&lt;/code&gt; to execute queries.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;pgduck_server&lt;/code&gt; it starts listening to port &lt;code&gt;5332&lt;/code&gt; on unix domain socket:&lt;/p&gt;
    &lt;code&gt;pgduck_server
LOG pgduck_server is listening on unix_socket_directory: /tmp with port 5332, max_clients allowed 10000
&lt;/code&gt;
    &lt;p&gt;As &lt;code&gt;pgduck_server&lt;/code&gt; implements Postgres wire protocol, you can access it via &lt;code&gt;psql&lt;/code&gt; on port &lt;code&gt;5332&lt;/code&gt; and host &lt;code&gt;/tmp&lt;/code&gt; and run commands via DuckDB.&lt;/p&gt;
    &lt;p&gt;For example, you can get the DuckDB version:&lt;/p&gt;
    &lt;code&gt;psql -p 5332 -h /tmp

select version() as duckdb_version; 
duckdb_version 
---------------- 
v1.3.2 (1 row)&lt;/code&gt;
    &lt;p&gt;You can also provide some additional settings while starting the server, to see all:&lt;/p&gt;
    &lt;code&gt;pgduck_server --help
&lt;/code&gt;
    &lt;p&gt;There are some important settings that should be adjusted, especially on production systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--memory_limit&lt;/code&gt;: Optionally specify the maximum memory of pgduck_server similar to DuckDB's memory_limit, the default is 80 percent of the system memory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--init_file_path &amp;lt;path&amp;gt;&lt;/code&gt;: Execute all statements in this file on start-up&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--cache_dir&lt;/code&gt;: Specify the directory to use to cache remote files (from S3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;pgduck_server&lt;/code&gt; relies on the DuckDB secrets manager for credentials and it follows the credentials chain by default for AWS and GCP. Make sure your cloud credentials are configured properly ‚Äî for example, by setting them in ~/.aws/credentials.&lt;/p&gt;
    &lt;p&gt;Once you set up the credential chain, you should set the &lt;code&gt;pg_lake_iceberg.default_location_prefix&lt;/code&gt;. This is the location where Iceberg tables are stored:&lt;/p&gt;
    &lt;code&gt;SET pg_lake_iceberg.default_location_prefix TO 's3://testbucketpglake';&lt;/code&gt;
    &lt;p&gt;You can also set the credentials on &lt;code&gt;pgduck_server&lt;/code&gt; for local development with &lt;code&gt;minio&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can create Iceberg tables by adding &lt;code&gt;USING iceberg&lt;/code&gt; to your &lt;code&gt;CREATE TABLE&lt;/code&gt; statements.&lt;/p&gt;
    &lt;code&gt;CREATE TABLE iceberg_test USING iceberg 
      AS SELECT 
            i as key, 'val_'|| i  as val
         FROM 
            generate_series(0,99)i;&lt;/code&gt;
    &lt;p&gt;Then, query it:&lt;/p&gt;
    &lt;code&gt;SELECT count(*) FROM iceberg_test;
 count 
-------
   100
(1 row)&lt;/code&gt;
    &lt;p&gt;You can then see the Iceberg metadata location:&lt;/p&gt;
    &lt;code&gt;SELECT table_name, metadata_location FROM iceberg_tables;


    table_name     |                                                metadata_location
-------------------+--------------------------------------------------------------------------------------------------------------------
 iceberg_test      | s3://testbucketpglake/postgres/public/test/435029/metadata/00001-f0c6e20a-fd1c-4645-87c9-c0c64b92992b.metadata.json&lt;/code&gt;
    &lt;p&gt;You can import or export data directly using &lt;code&gt;COPY&lt;/code&gt; in Parquet, CSV, or newline-delimited JSON formats.  The format is automatically inferred from the file extension, or you can specify it explicitly with &lt;code&gt;COPY&lt;/code&gt; options like &lt;code&gt;WITH (format 'csv', compression 'gzip')&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;-- Copy data from Postgres to S3 with format parquet
-- Read from any data source, including iceberg tables, heap tables or any query results
COPY (SELECT * FROM iceberg_test) TO 's3://testbucketpglake/parquet_data/iceberg_test.parquet';

-- Copy back from S3 to any table in Postgres
-- This example copies into an iceberg table, but could be heap table as well
COPY iceberg_test FROM 's3://testbucketpglake/parquet_data/iceberg_test.parquet';&lt;/code&gt;
    &lt;p&gt;You can create a foreign table directly from a file or set of files without having to specify column names or types.&lt;/p&gt;
    &lt;code&gt;-- use the files under the path, can use * for all files
CREATE FOREIGN TABLE parquet_table() 
SERVER pg_lake 
OPTIONS (path 's3://testbucketpglake/parquet_data/*.parquet');

-- note that we infer the columns from the file
\d parquet_table
              Foreign table "public.parquet_table"
 Column |  Type   | Collation | Nullable | Default | FDW options 
--------+---------+-----------+----------+---------+-------------
 key    | integer |           |          |         | 
 val    | text    |           |          |         | 
Server: pg_lake
FDW options: (path 's3://testbucketpglake/parquet_data/*.parquet')

-- and, query it
select count(*) from parquet_table;
 count 
-------
   100
(1 row)
&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;pg_lake&lt;/code&gt; instance consists of two main components: PostgreSQL with the pg_lake extensions and pgduck_server.&lt;/p&gt;
    &lt;p&gt;Users connect to PostgreSQL to run SQL queries, and the &lt;code&gt;pg_lake&lt;/code&gt; extensions integrate with Postgres‚Äôs hooks to handle query planning, transaction boundaries, and overall orchestration of execution.&lt;/p&gt;
    &lt;p&gt;Behind the scenes, parts of query execution are delegated to DuckDB through pgduck_server, a separate multi-threaded process that implements the PostgreSQL wire protocol (locally). This process runs DuckDB together with our duckdb_pglake extension, which adds PostgreSQL-compatible functions and behavior.&lt;/p&gt;
    &lt;p&gt;Users typically don‚Äôt need to be aware of &lt;code&gt;pgduck_server&lt;/code&gt;; it operates transparently to improve performance. When appropriate, &lt;code&gt;pg_lake&lt;/code&gt; delegates scanning of the data and the computation to DuckDB‚Äôs highly parallel, column-oriented execution engine.&lt;/p&gt;
    &lt;p&gt;This separation also avoids the threading and memory-safety limitations that would arise from embedding DuckDB directly inside the Postgres process, which is designed around process isolation rather than multi-threaded execution. Moreover, it lets us interact with the query engine directly by connecting to it using standard Postgres clients.&lt;/p&gt;
    &lt;p&gt;The team behind pg_lake has a lot of experience building Postgres extensions (e.g. Citus, pg_cron, pg_documentdb). Over time, we‚Äôve learned that large, monolithic PostgreSQL extensions are harder to evolve and maintain.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; follows a modular design built around a set of interoperating components ‚Äî mostly implemented as PostgreSQL extensions, others as supporting services or libraries.&lt;lb/&gt; Each part focuses on a well-defined layer, such as table and metadata management, catalog and object store integration, query execution, or data format handling. This approach makes it easier to extend, test, and evolve the system, while keeping it familiar to anyone with a PostgreSQL background.&lt;/p&gt;
    &lt;p&gt;The current set of components are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pg_lake_iceberg: a PostgreSQL extension that implements the Iceberg specification&lt;/item&gt;
      &lt;item&gt;pg_lake_table: a PostgreSQL extension that implements a foreign data wrapper to query files in object storage&lt;/item&gt;
      &lt;item&gt;pg_lake_copy: a PostgreSQL extension that implements COPY to/from your data lake&lt;/item&gt;
      &lt;item&gt;pg_lake_engine: a common module for different pg_lake extensions&lt;/item&gt;
      &lt;item&gt;pg_extension_base: A foundational building block for other extensions&lt;/item&gt;
      &lt;item&gt;pg_extension_updater: An extension for updating all extensions on start-up. See README.md.&lt;/item&gt;
      &lt;item&gt;pg_lake_benchmark: a PostgreSQL extension that performs various benchmarks on lake tables. See README.md.&lt;/item&gt;
      &lt;item&gt;pg_map: A generic map type generator&lt;/item&gt;
      &lt;item&gt;pgduck_server: a stand-alone server that loads DuckDB into the same server machine and exposes DuckDB via the PostgreSQL protocol&lt;/item&gt;
      &lt;item&gt;duckdb_pglake: a DuckDB extension that adds missing PostgreSQL functions to DuckDB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; development started in early 2024 at Crunchy Data with the goal of bringing Iceberg to PostgreSQL. The first few months were focused on building a robust integration of an external query engine (DuckDB). To get to market early, we made the query/import/export features available to Crunchy Bridge customers as Crunchy Bridge for Analytics.&lt;/p&gt;
    &lt;p&gt;Next, we started building a comprehensive implementation of the Iceberg (v2) protocol with support for transactions and almost all PostgreSQL features. In November 2024, we relaunched Crunchy Bridge for Analytics as Crunchy Data Warehouse available on Crunchy Bridge and on-premises.&lt;/p&gt;
    &lt;p&gt;In June 2025, Crunchy Data was acquired by Snowflake. Following the acquisition, Snowflake decided to open source the project as &lt;code&gt;pg_lake&lt;/code&gt; in November 2025. The initial version is 3.0 because of the two prior generations. If you‚Äôre currently a Crunchy Data Warehouse user there will be an automatic upgrade path, though some names will change.&lt;/p&gt;
    &lt;p&gt;Full project documentation can be found in the docs directory.&lt;/p&gt;
    &lt;p&gt;Copyright (c) Snowflake Inc. All rights reserved. Licensed under the Apache 2.0 license.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; is dependent on third-party projects Apache Avro and DuckDB. During build, &lt;code&gt;pg_lake&lt;/code&gt; applies patches to Avro and certain DuckDB extensions in order to provide the &lt;code&gt;pg_lake&lt;/code&gt; functionality. The source code associated with the Avro and DuckDB extensions is downloaded from the applicable upstream repos and the source code associated with those projects remains under the original licenses. If you are packaging or redistributing packages that include &lt;code&gt;pg_lake&lt;/code&gt;, please note that you should review those upstream license terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Snowflake-Labs/pg_lake"/><published>2025-11-04T16:12:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45813310</id><title>Launch HN: Plexe (YC X25) ‚Äì Build production-grade ML models from prompts</title><updated>2025-11-05T11:34:51.939895+00:00</updated><content>&lt;doc fingerprint="d1d50ecbdfaae78"&gt;
  &lt;main&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;Your Agentic ML Engineering&lt;/p&gt;
    &lt;p&gt;Team&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your raw data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;√∞ Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: √¢ Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: √∞ Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;√∞ Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and it will build a production-ready model that√¢s engineered for your exact business challenge.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your model√¢s purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
    &lt;p&gt;Spotlight&lt;/p&gt;
    &lt;p&gt;Spotlight&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Featured in BI√¢s 10 Most Exciting AI Startups from YC Spring 2025&lt;/p&gt;
    &lt;p&gt;Featured in BI√¢s 10 Most Exciting AI Startups from YC Spring 2025&lt;/p&gt;
    &lt;p&gt;Plexe AI Redefines Credit Underwriting With Real-Time ML Models&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Solutions&lt;/p&gt;
    &lt;p&gt;Solutions&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding who√¢s truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding who√¢s truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding who√¢s truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding who√¢s truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;Questions? We√¢ve Got Answers.&lt;/p&gt;
    &lt;p&gt;Questions? We√¢ve Got Answers.&lt;/p&gt;
    &lt;p&gt;Questions? We√¢ve Got Answers.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Let√¢s Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether you√¢re starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Whether you√¢re starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your data into your competitive advantage.&lt;/p&gt;
    &lt;p&gt;Let√¢s Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether you√¢re starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Let√¢s Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether you√¢re starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;√Ç¬© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;√Ç¬© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;√Ç¬© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;√Ç¬© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most. You√¢ll see what√¢s working, what√¢s not, and where the real opportunities are hiding. No code, no setup, no fuss.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;√∞ Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: √¢ Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: √∞ Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;√∞ Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and we√¢ll build a production-ready model that√¢s engineered for your exact business challenge. Whether it√¢s predicting churn or fraud detection, you√¢ll go from idea to working AI in hours, not months.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your model√¢s purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most. You√¢ll see what√¢s working, what√¢s not, and where the real opportunities are hiding. No code, no setup, no fuss.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;√∞ Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: √¢ Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: √∞ Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;√∞ Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and we√¢ll build a production-ready model that√¢s engineered for your exact business challenge. Whether it√¢s predicting churn or fraud detection, you√¢ll go from idea to working AI in hours, not months.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your model√¢s purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.plexe.ai/"/><published>2025-11-04T17:07:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45813767</id><title>Codemaps: Understand Code, Before You Vibe It</title><updated>2025-11-05T11:34:51.633533+00:00</updated><content>&lt;doc fingerprint="3fc296052cb3ae5f"&gt;
  &lt;main&gt;
    &lt;quote&gt;‚ÄúYour code is your understanding of the problem you‚Äôre exploring. So it‚Äôs only when you have your code in your head that you really understand the problem.‚Äù ‚Äî Paul Graham&lt;/quote&gt;
    &lt;p&gt;Software development only becomes engineering with understanding. Your ability to reason through your most challenging coding tasks is constrained by your mental model of how things work ‚Äî in other words, how quickly and how well you onboard to any codebase for solving any problem. However most AI vibe coding tools are aimed at relieving you of that burden by reading ‚Üí thinking ‚Üí writing the code for you, increasing the separation from you and your code. This is fine for low value, commodity tasks, but absolutely unacceptable for the hard, sensitive, and high value work that defines real engineering.&lt;/p&gt;
    &lt;p&gt;We all need more AI that turns your brain ON, not OFF.&lt;/p&gt;
    &lt;p&gt;Today we are announcing Windsurf Codemaps, which are first-of-its-kind AI-annotated structured maps of your code, powered by SWE-1.5 and Claude Sonnet 4.5. Building on our popular work from DeepWiki and Ask Devin, Codemaps is the next step in hyper-contextualized codebase understanding, grounded in precise code navigation.&lt;/p&gt;
    &lt;p&gt;Every engineering task ‚Äî debugging, refactors, new features ‚Äî starts with understanding. Great engineers aren‚Äôt just good at writing code; they‚Äôre good at reading it, building mental models that span files, layers, and systems.&lt;/p&gt;
    &lt;p&gt;But modern codebases are sprawling: hundreds of files, multiple services, dense abstractions. Based on own experience and deep conversations with our customers across the Fortune 500, even top engineers spend much of their deep-work time finding and remembering what matters.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a huge tax on productivity:&lt;/p&gt;
    &lt;p&gt;This is the frontier that AI coding tools haven‚Äôt yet solved. Onboarding isn‚Äôt even a onetime cost, you pay it every time you switch context and codebases. The faster and better you understand your codebase, the faster and better you‚Äôll be able to fix it yourself, or prompt agents to do it.&lt;/p&gt;
    &lt;p&gt;Until today, the standard approach by Copilot, Claude Code, Codex, and even Windsurf Cascade, was to have you ask questions of a generalist agent with access to your code in a typical chat experience. But those solutions don‚Äôt solve focused onboarding and strongly grounded navigation to onboard, debug, and better context engineer for your codebase.&lt;/p&gt;
    &lt;p&gt;At Cognition, we‚Äôve been investing far more deeply in understanding:&lt;/p&gt;
    &lt;p&gt;Codemaps is our next investment in tooling that makes engineers the best versions of themselves.&lt;/p&gt;
    &lt;p&gt;When you first open Codemaps (click the new maps icon or Cmd+Shift+C in Windsurf) with a codebase opened in Windsurf, you can enter in a prompt for the task you are trying to do, or take one of the automatic suggestions. You can choose a Fast (SWE-1.5) or Smart (Sonnet 4.5) model to generate your Codemap. Every Codemap is a snapshot of your code and respects ZDR.&lt;/p&gt;
    &lt;p&gt;Based on our demos to customers, you will experience Codemaps best on your own codebase and asking a question about how or where some functionality works. In our dogfooding, we find particular effectiveness tracing through client-server problems or a data pipeline or debugging auth/security issues:&lt;/p&gt;
    &lt;p&gt;If all you wanted was to quickly jump through grouped and nested parts of your code that related to your question, this is already an improvement compared to asking the same question in Cascade, where answers are not as densely linked to the exact lines of code.&lt;/p&gt;
    &lt;p&gt;You can also toggle over to a visually drawn Codemap, which performs the same functions when you click on individual nodes: they send you to the exact part of the codebase you clicked on.&lt;/p&gt;
    &lt;p&gt;However, if you want a little more context, then you can hit ‚ÄúSee more‚Äù in any section to expand our ‚Äútrace guide‚Äù that gives a more descriptive explanation of what groups the discovered lines together.&lt;/p&gt;
    &lt;p&gt;Finally, inside Cascade you can also reference a codemap for the agent with &lt;code&gt;@{codemap}&lt;/code&gt; (all of it, or a particular subsection) in your prompt to provide more specific context and dramatically improve the performance of your agent for your task.&lt;/p&gt;
    &lt;p&gt;We feel that the popular usage of ‚Äúvibe coding‚Äù has strayed far from the original intent, into a blanket endorsement of plowing through any and all AI generated code slop. If you look at the difference between the most productive vs the problematic AI-assisted coders, the productive ones can surf the vibes of code that they understand well, whereas people get into trouble when the code they generate and maintain starts to outstrip their ability to understand it.&lt;/p&gt;
    &lt;p&gt;To understand is to be accountable. As AI takes on more of the easy work, the hard problems left to humans are the ones that demand real comprehension: debugging complex systems, refactoring legacy code, making architecture decisions. In this new era, the engineer‚Äôs role shifts from authoring to accountability ‚Äî you might not write every line, but you‚Äôre still responsible for what ships. That accountability depends on understanding what the AI produced, why it changed, and whether it‚Äôs safe. Codemaps closes that gap by giving both the human and the AI a shared picture of the system: how it‚Äôs structured, how data flows, where dependencies live. Codemaps is our latest Fast Agent, but as we discussed in the Semi-Async Valley of Death, our goal isn't just about speed, it is to help your human engineers stay in flow, stay on top of their code, and to move faster and more confidently on the hardest problems, never shipping slop that they don't understand.&lt;/p&gt;
    &lt;p&gt;Augment engineers for high value work, relieve them of low value work. The other local minima that the coding agent industry has gotten stuck in is in the general messaging of replacing engineers for low value work and not having any solutions for the hardest tasks apart from ‚Äúpls ultrathink high, no mistakes‚Äù, which only gives autonomy to the agent, at the expense of the engineer. The long history of human-machine collaboration teaches us that we can always do more with the synergy rather than humans-alone or AI-alone. Our view is that the AI product that engineers will love most is the one that makes them better at their job, not the one that tries to replace them with a sloppy facsimile of themselves.&lt;/p&gt;
    &lt;p&gt;With Codemaps, we are now exposing to humans some of the indexing and analysis we do inside of our coding agents. These artifacts are sharable today across teams for learning and discussion, but we have yet to benchmark how much better they can make our coding agents like Devin and Cascade in solving challenging tasks on their own. We also see opportunities for connecting and annotating codemaps, as well as defining an open &lt;code&gt;.codemap&lt;/code&gt; protocol that can be used by other code agents and custom tooling built by you. Complementing our Fast Context feature, this is an advancement in human-readable automatic context engineering.&lt;/p&gt;
    &lt;p&gt;You can try Codemaps on the latest versions of Windsurf, or DeepWiki!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cognition.ai/blog/codemaps"/><published>2025-11-04T17:47:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816041</id><title>I took all my projects off the cloud, saving thousands of dollars</title><updated>2025-11-05T11:34:51.456643+00:00</updated><content/><link href="https://rameerez.com/send-this-article-to-your-friend-who-still-thinks-the-cloud-is-a-good-idea/"/><published>2025-11-04T21:22:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816673</id><title>Grayskull: A tiny computer vision library in C for embedded systems, etc.</title><updated>2025-11-05T11:34:50.844021+00:00</updated><content>&lt;doc fingerprint="6e340ed9bad71609"&gt;
  &lt;main&gt;
    &lt;p&gt;Grayskull is a minimalist, dependency-free computer vision library designed for microcontrollers and other resource-constrained devices. It focuses on grayscale images and provides modern, practical algorithms that fit in a few kilobytes of code. Single-header design, integer-based operations, pure C99.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Image operations: copy, crop, resize (bilinear), downsample&lt;/item&gt;
      &lt;item&gt;Filtering: blur, Sobel edges, thresholding (global, Otsu, adaptive)&lt;/item&gt;
      &lt;item&gt;Morphology: erosion, dilation&lt;/item&gt;
      &lt;item&gt;Geometry: connected components, perspective warp&lt;/item&gt;
      &lt;item&gt;Features: FAST/ORB keypoints and descriptors (object tracking)&lt;/item&gt;
      &lt;item&gt;Local binary patterns: LBP cascades to detect faces, vehicles etc&lt;/item&gt;
      &lt;item&gt;Utilities: PGM read/write&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As usual, no dependencies, no dynamic memory allocation, no C++, no surprises. Just a single header file under 1KLOC.&lt;/p&gt;
    &lt;p&gt;Check out the examples folder for more!&lt;/p&gt;
    &lt;p&gt;Online demo: try Grayskull in your browser.&lt;/p&gt;
    &lt;code&gt;#include "grayskull.h"

struct gs_image img = gs_read_pgm("input.pgm");
struct gs_image blurred = gs_alloc(img.w, img.h);
struct gs_image binary = gs_alloc(img.w, img.h);

gs_blur(blurred, img, 2);
gs_threshold(binary, blurred, gs_otsu_theshold(blurred));

gs_write_pgm(binary, "output.pgm");
gs_free(img);
gs_free(blurred);
gs_free(binary);&lt;/code&gt;
    &lt;p&gt;Note that &lt;code&gt;gs_alloc&lt;/code&gt;/&lt;code&gt;gs_free&lt;/code&gt; are optional helpers; you can allocate image pixel buffers any way you like.&lt;/p&gt;
    &lt;code&gt;struct gs_image { unsigned w, h; uint8_t *data; };
struct gs_rect { unsigned x, y, w, h; }; // ROI
struct gs_point { unsigned x, y; }; // corners

uint8_t gs_get(struct gs_image img, unsigned x, unsigned y);
void gs_set(struct gs_image img, unsigned x, unsigned y, uint8_t value);
void gs_crop(struct gs_image dst, struct gs_image src, struct gs_rect roi);
void gs_copy(struct gs_image dst, struct gs_image src);
void gs_resize(struct gs_image dst, struct gs_image src);
void gs_downsample(struct gs_image dst, struct gs_image src);

// Thresholding
void gs_histogram(struct gs_image img, unsigned hist[256]);
void gs_threshold(struct gs_image img, uint8_t threshold);
uint8_t gs_otsu_threshold(struct gs_image img);
void gs_adaptive_threshold(struct gs_image dst, struct gs_image src, unsigned radius, int c);

// Filters
void gs_blur(struct gs_image dst, struct gs_image src, unsigned radius);
void gs_erode(struct gs_image dst, struct gs_image src);
void gs_dilate(struct gs_image dst, struct gs_image src);
void gs_sobel(struct gs_image dst, struct gs_image src);

// Blobs (connected components) and contours
typedef uint16_t gs_label;
struct gs_blob { gs_label label; unsigned area; struct gs_rect box; struct gs_point centroid; };
struct gs_contour { struct gs_rect box; struct gs_point start; unsigned length; };
unsigned gs_blobs(struct gs_image img, gs_label *labels, struct gs_blob *blobs, unsigned nblobs);
void gs_blob_corners(struct gs_image img, gs_label *labels, struct gs_blob *b, struct gs_point c[4]);
void gs_perspective_correct(struct gs_image dst, struct gs_image src, struct gs_point c[4]);
void gs_trace_contour(struct gs_image img, struct gs_image visited, struct gs_contour *c);

// FAST/ORB
struct gs_keypoint { struct gs_point pt; unsigned response; float angle; uint32_t descriptor[8]; };
struct gs_match { unsigned idx1, idx2; unsigned distance; };
unsigned gs_fast(struct gs_image img, struct gs_image scoremap, struct gs_keypoint *kps, unsigned nkps, unsigned threshold);
float gs_compute_orientation(struct gs_image img, unsigned x, unsigned y, unsigned r);
void gs_brief_descriptor(struct gs_image img, struct gs_keypoint *kp);
unsigned gs_orb_extract(struct gs_image img, struct gs_keypoint *kps, unsigned nkps, unsigned threshold, uint8_t *scoremap_buffer);
unsigned gs_match_orb(const struct gs_keypoint *kps1, unsigned n1, const struct gs_keypoint *kps2, unsigned n2, struct gs_match *matches, unsigned max_matches, float max_distance);

// LBP cascades
struct gs_lbp_cascade { uint16_t window_w, window_h; uint16_t nfeatures, nweaks, nstages; const int8_t *features; /* [nfeatures * 4] */ const uint16_t *weak_feature_idx; const float *weak_left_val, *weak_right_val; const uint16_t *weak_subset_offset, *weak_num_subsets; const int32_t *subsets; const uint16_t *stage_weak_start, *stage_nweaks; const float *stage_threshold; };
void gs_integral(struct gs_image src, unsigned *ii);
unsigned gs_lbp_window(const struct gs_lbp_cascade *c, const unsigned *ii, unsigned iw, unsigned ih, int x, int y, float scale);
unsigned gs_lbp_detect(const struct gs_lbp_cascade *c, const unsigned *ii, unsigned iw, unsigned ih, struct gs_rect *rects, unsigned max_rects, float scale_factor, float min_scale, float max_scale, int step);

// Optional:
struct gs_image gs_alloc(unsigned w, unsigned h);
void gs_free(struct gs_image img);
struct gs_image gs_read_pgm(const char *path);
int gs_write_pgm(struct gs_image img, const char *path);&lt;/code&gt;
    &lt;p&gt;This project is licensed under the MIT License. Feel free to use in research, products, and your next embedded vision project!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/zserge/grayskull"/><published>2025-11-04T22:35:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816853</id><title>Mr TIFF</title><updated>2025-11-05T11:34:50.580232+00:00</updated><content>&lt;doc fingerprint="6498b18bb326a2bd"&gt;
  &lt;main&gt;
    &lt;p&gt;For as long as I have published my books, one of my overarching goals was to give credit to those who actually invented the hardware and software that we use.&lt;/p&gt;
    &lt;p&gt;I have spent 10,000+ hours to create an accurate record of their work but I'm not complaining. The 'as-close-to-possible' truth of invention by individuals or teams meant identifying the work, educating myself, writing questions, and sending emails. And after that process, I set up a chat because it all gets down to talking to someone on the other side of the world, about something that happened 30 or 40 years ago.&lt;/p&gt;
    &lt;p&gt;If the invention involves a team, I try to interview more than one person, so I can cross-check the facts. Not to call anyone out, it‚Äôs just that, given time, we all forget the facts. And everyone adds their personal take. It‚Äôs because of that, for example, that I know the English musician Peter Gabriel really did visit Apple's research labs as they tested the Apple Sound Chip, and gave the team his personal approval to use the song 'Red Rain' for the Macintosh II launch. Wil Oxford, Steve Perlman, Mike Potel, Mark Lentczner and Steve Milne told me so.&lt;/p&gt;
    &lt;p&gt;As I was wrapping up Version 2.3 of Inventing the Future, I spoke with Steve M and Mark about the AIFF (Audio Interchange File Format) audio standard that they built around the same time as their VIP visit. They did so as professional programmers, amateur musicians and electronic music experts. Milne and Lentczner knew users needed a standard file format to make their work lives easier and to fend off confusion in the nascent MIDI marketplace. But it didn't exist. So Steve and Mark consulted with users and manufacturers in the Apple cafeteria after hours. This work is interesting on its own but it also underpinned other research. The AIFF, Apple Sound Chip, and MIDI Manager work scaffolded QuickTime and its extensible video formats and programs in 1991. Senior engineer Toby Farrand told me:&lt;/p&gt;
    &lt;p&gt;Audio drove the development of QuickTime more than anything.&lt;/p&gt;
    &lt;p&gt;So who or what drove the development of AIFF?&lt;/p&gt;
    &lt;p&gt;Steve and Mark referred me to the IFF (Interchange File Format (IFF) and the TIFF (Tag Image File Format) that were built before AIFF, in 1985 and 1986 respectively. These file formats were the benchmark for open media standards. My search pivoted, as it always does, to understand those inventions. I expected to be able to find the engineer or engineers names, track them down and interview them. It has worked around 100 times before.&lt;/p&gt;
    &lt;p&gt;Jerry Morrison created IFF while working at Electronic Arts and then went to Apple, where he liaised with the AIFF team. I could easily background his work.&lt;/p&gt;
    &lt;p&gt;So I turned my attention to TIFF, built initially as an image standard for desktop publishing. TIFF was able to store monochrome, grayscale, and color images, alongside metadata such as size, compression algorithms, and color space information. In many ways, it was a lot like AIFF so I was keen to know more. But I couldn't find a TIFF creator. No matter how I enquired, Aldus created TIFF.&lt;/p&gt;
    &lt;p&gt;To be clear, while a search for AIFF will offer up a company (Apple) not a person, I was able to find Milne and Lentczner in part because of their unique names and because Apple publicised the AIFF work and those publications are archived.&lt;/p&gt;
    &lt;p&gt;All I had was Aldus, an American company that created desktop publishing with the help of Apple and Adobe. In fact, Paul Brainerd, the cofounder of Aldus coined the term 'desktop publishing' to quickly explain the technicality of what they were doing to potential investors. But Aldus and their seminal product, PageMaker, are long gone, and there were no breadcrumbs for TIFF's creation.&lt;/p&gt;
    &lt;p&gt;Finally, after a day-long trawl through MacWeek back issues, I found Steve Carlson. (below)&lt;/p&gt;
    &lt;p&gt;Then I ran a similar length search through the Computer History Museum‚Äôs amazing Oral Histories transcriptions. Brainerd mentioned Carlson's name in an interview. (below)&lt;/p&gt;
    &lt;p&gt;But it was too brief an explanation so I kept looking. Then the trail went cold.&lt;/p&gt;
    &lt;p&gt;And that was because, folks had misspelt his name when quoting him and then that was copied into magazines, and reviews and so forth. Brainerd's CHM interview transcript was wrong. But I didn‚Äôt know that.&lt;/p&gt;
    &lt;p&gt;I just kept looking for Steve Carlson.&lt;/p&gt;
    &lt;p&gt;I found other inventors because they had unique middle or last names or by random methods such as searching glider pilot licences in the Napa Valley after a tip from a former colleague that 'so and so' was a pilot in retirement. I had no tips, no links, nothing.&lt;/p&gt;
    &lt;p&gt;Why couldn‚Äôt I find Steve Carlson?&lt;/p&gt;
    &lt;p&gt;All the while, the answer was right under my nose. I had downloaded the final Aldus TIFF specifications document, hoping to find the author‚Äôs name. However, the name is seemingly written in white text on white paper - making it invisible. What?&lt;/p&gt;
    &lt;p&gt;See below where I have highlighted the region with a blue block over the text.&lt;/p&gt;
    &lt;p&gt;For a reason I can‚Äôt recall, I downloaded a plain text version and typed in Carlson to see if he was mentioned, but I must have paused at ‚ÄòCarls...' and the search functionality automatically filled in the rest. Suddenly I was staring at:&lt;/p&gt;
    &lt;p&gt;Author/Editor/Arbitrator: Steve Carlsen.&lt;/p&gt;
    &lt;p&gt;‚ÄòCarls-EN‚Äô&lt;/p&gt;
    &lt;p&gt;A quick trip to Google patents, and a search for Steve Carlsen, Stephen Carlsen. Bingo! Stephen E. Carlsen‚Äôs patents at Aldus (and Adobe) in Issaquah, WA.&lt;/p&gt;
    &lt;p&gt;I checked the geography, as most folks of a certain age do not stray far from the addresses filed in their patents, and typed Stephen‚Äôs correctly spelled surname into the online US White Pages for Washington State. There was ‚Äòa‚Äô Stephen Carlsen listed in a retirement village in WA. His age matched, but there were no public facing email addresses.&lt;/p&gt;
    &lt;p&gt;I searched bulletin boards on the topic of TIFF, as I had found a former Apple engineer that way. Don had picked an abbreviation of his initials and numbers to post on BBS in his college days and then carried that same combination into adulthood. Many of us did. I took a punt pasting his unique prefix into hotmail, gmail etc. and found Don and interviewed him, but - Stephen Carlsen did not show up in a BBS. So, no email to try.&lt;/p&gt;
    &lt;p&gt;My ‚Äòlast straw' method for finding someone is a stamped envelope. I wrote, printed and mailed a one-page letter to Stephen's listed address, and crossed my fingers. Four months later he popped up in my email.&lt;/p&gt;
    &lt;p&gt;It was a surprise and a relief. We swapped a few emails, and he confirmed the TIFF catalyst story. For Stephen it was 'no big deal'. Once he had built the initial TIFF, Aldus needed to convince 3rd party developers and scanner manufacturers to agree to TIFF as a standard.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe had to define and promote an industry standard for storing and processing scanned images, so that we wouldn't have to write import filters for every model of every scanner that would soon be entering the budding desktop scanner market."&lt;/p&gt;
    &lt;p&gt;Stephen himself did much of the evangelizing as Paul Brainerd later pointed out:&lt;/p&gt;
    &lt;p&gt;‚Äú(Steve) developed the standard, and then we went out and promoted it in a series of meetings with specific companies - as well as some workshops we ran in Seattle and the Bay Area during the Seybold shows and the MacWorld shows.‚Äù&lt;/p&gt;
    &lt;p&gt;I sent Stephen a draft of what I had written and he sent a prompt reply saying - ‚ÄòLooks good‚Äô.&lt;/p&gt;
    &lt;p&gt;I followed up asking him how he ended up at a tiny startup in Seattle called Aldus.&lt;/p&gt;
    &lt;p&gt;At that time, I was interviewing for a graphics position at Boeing Computer Services in Seattle, and noticed a small wanted ad that sounded really interesting, and seemed to be an excellent match for my background and interests. I interviewed with Paul and the 5-person mostly-ex-Atex engineering team, and I was hired.&lt;/p&gt;
    &lt;p&gt;Out of curiosity I put Stephen's email address, now that I knew it, into a Duck Duck search and found him helping people online with TIFF queries long after Aldus had been acquired by Adobe. He also contributed to a Google Group called tiffcentral.&lt;/p&gt;
    &lt;p&gt;Having interviewed so many people across more than a decade, I‚Äôve got pretty good at judging those who would like to talk or type, those who are verbose and those that are not. I knew Stephen had said what he was going to say. I added his pioneering work on TIFF to the AIFF story and moved on.&lt;/p&gt;
    &lt;p&gt;Two years had flown by when I received an email yesterday. His ex-wife Peggy found my paper letter and wrote to me. Stephen passed away earlier this year.&lt;/p&gt;
    &lt;p&gt;Thank you for your interest in and support of Stephen‚Äôs brilliant work creating TIFF. I‚Äôm not surprised Stephen didn‚Äôt finish corresponding with you, as he had begun to struggle with using his computer and phone. Some days were better than others for him, but he began to lose touch with people during those months you were reaching out to him. He was a humble man, and I guess never pushed to be recognized, although I believe those who worked with him knew the truth. His last week was in my home, where he was never left alone.&lt;/p&gt;
    &lt;p&gt;Peggy finished the email with, ‚ÄòI called him Mr TIFF up to his last moment.'&lt;/p&gt;
    &lt;p&gt;The 10,000+ hours of book research disappeared in an instant. As sad as it was, I could see clearly that all of my work was worth it. Every single second. Because of this email.&lt;/p&gt;
    &lt;p&gt;Mr TIFF.&lt;/p&gt;
    &lt;p&gt;Last night, as everyone in my house went to sleep, I took a deep breath and edited the Wikipedia page for TIFF, the Tag Image File Format.&lt;/p&gt;
    &lt;p&gt;It no longer reads ‚Äòcreated by Aldus‚Äô, it reads ‚Äò‚Ä¶created by Stephen Carlsen, an engineer at Aldus'&lt;/p&gt;
    &lt;p&gt;"Inventing the Future" here -&amp;gt; https://books.by/john-buck/inventing-the-future&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://inventingthefuture.ghost.io/mr-tiff/"/><published>2025-11-04T22:57:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816879</id><title>Patching 68K Software ‚Äì SimpleText</title><updated>2025-11-05T11:34:50.320276+00:00</updated><content>&lt;doc fingerprint="b2497ef403174708"&gt;
  &lt;main&gt;
    &lt;p&gt;Someone asked to have SimpleText open a smaller text window at startup. Initially, I assumed this would be a fairly easy fix by just overwriting a few constant values in SimpleText code. It turned out to be a pain -- but I learned a lot along the way.&lt;lb/&gt;You need to have the code editor (from one of the Apple developer CDs) in your ResEdit preference file in order to disassemble code resources within ResEdit. Then, open each 'CODE' resource of SimpleText and search for _SizeWindow (A91D). Or, just skim the code until you see system calls that look like they have something to do with windows. Here is a nice chunk in routine Anon48. A new window is created, then it looks at the main device, looks at the size of the menu bar (MBarHeight), sets the port, and moves and sizes the window.&lt;lb/&gt;But, uh oh, earlier in the code it checks what type of window it is opening. A patch is going to need to avoid resizing code for pictures, video, and the about box.&lt;lb/&gt;I could not locate any obvious constants to adjust. Instead, I would need to inject a more complicated routine that detects whether it was a text window and substitute a new rectangle for the size. But, you cannot simply insert code, as it would move all subsequent code down, and the jumps (subroutine calls) that cross over that location would now jump to the wrong spots. So, I need to jump out of their code to my own (appended at the end of the code resource) and return.&lt;lb/&gt;For example, SimpleText's code to get the size of MBarHeight can easily be performed elsewhere. My routine need only return the MBarHeight in register D0 before returning. That gives me 8 bytes to overwrite with my jump.&lt;lb/&gt;Here is my replacement code. It still is only 8 bytes. But, I now jump to my subroutine, check the result, and jump over their resizing code if my routine says it is changing the window size.&lt;lb/&gt;In my subroutine, I make sure all the needed information is in registers (which I checked that SimpleText was not using), I call my various functions and then perform any work that was lost from overwriting or jumping over the original code. Specifically, I get the MBarHeight into D0 and set the current port to the window.&lt;lb/&gt;Easy! But, later in the code, SimpleText reads the content of whatever document is being opened and once again resizes the window. So, I needed to patch later code as well. How could I determine at that point that a replacement window size was being used? I simply store the result of the first subroutine (see SetRecentResult above) and then check it on later calls.&lt;lb/&gt;Where could I store this information? it is not possible to add global variables and the registers are all reused by SimpleText between the first and second routines. Well, you can store a variable (or an entire structure with many variables) within the code itself. Here is my little workaround for CodeWarrior.&lt;lb/&gt;A couple of other tricks.&lt;lb/&gt;1. The system routine GetHandleSize has some glue code (they intercept the call in a local library before calling the system). I needed this call, but didn't want to add the weight of CodeWarrior's libary. So, I defined the direct call to GetHandleSize (I didn't need the glue fix).&lt;lb/&gt;2. You can pass any of the scratch registers (D0-D2, A0-A1, FP0-FP3) to a C function. The way of defining that in CodeWarrior is noted below. You cannot use any other registers. To make debugging easier, I wrote the original subroutine as a true C function, and the register-&amp;gt;C function as a wrapper.&lt;lb/&gt;3. CodeWarrior does not support BSR for some reason. Use JSR instead. Also, a called routine must be placed before the caller routine in order to generate a short relative JSR rather than absolute address. See my 'RecentResult' example above, where the routines that call RecentResult are placed after it in code.&lt;lb/&gt;4. SimpleText stores literals (strings, constants) at the end of the 'CODE' resource. After that is where I placed my code. Unfortunately, this breaks disassembly in ResEdit. Below, do you see 'A9FF'? That's the '_Debugger' trap call. It is follow by the rest of the code, and the MacsBug symbols for "SimpleTextWindowChoicePrep'&lt;lb/&gt;I then needed to hand compute the JSR patched in the original SimpleText code to this location at the end of the code resource.&lt;lb/&gt;5. To make it easy to redefine window sizes in the future, I added a resource.&lt;lb/&gt;The ResEdit definition of this resource is the TMPL. By the way, I have experienced corruption twice with ResEdit 2.1.3. Perhaps it has a bug with templates?&lt;lb/&gt;I doubt this information will be useful to most people. However, it may help avoid some frustrating issues for those few people that attempt patching old software.&lt;lb/&gt;Attached is the hacked version of SimpleText.&lt;lb/&gt;- David&lt;/p&gt;
    &lt;p&gt;You need to have the code editor (from one of the Apple developer CDs) in your ResEdit preference file in order to disassemble code resources within ResEdit. Then, open each 'CODE' resource of SimpleText and search for _SizeWindow (A91D). Or, just skim the code until you see system calls that look like they have something to do with windows. Here is a nice chunk in routine Anon48. A new window is created, then it looks at the main device, looks at the size of the menu bar (MBarHeight), sets the port, and moves and sizes the window.&lt;/p&gt;
    &lt;p&gt;But, uh oh, earlier in the code it checks what type of window it is opening. A patch is going to need to avoid resizing code for pictures, video, and the about box.&lt;/p&gt;
    &lt;p&gt;I could not locate any obvious constants to adjust. Instead, I would need to inject a more complicated routine that detects whether it was a text window and substitute a new rectangle for the size. But, you cannot simply insert code, as it would move all subsequent code down, and the jumps (subroutine calls) that cross over that location would now jump to the wrong spots. So, I need to jump out of their code to my own (appended at the end of the code resource) and return.&lt;/p&gt;
    &lt;p&gt;For example, SimpleText's code to get the size of MBarHeight can easily be performed elsewhere. My routine need only return the MBarHeight in register D0 before returning. That gives me 8 bytes to overwrite with my jump.&lt;/p&gt;
    &lt;p&gt;Here is my replacement code. It still is only 8 bytes. But, I now jump to my subroutine, check the result, and jump over their resizing code if my routine says it is changing the window size.&lt;/p&gt;
    &lt;p&gt;In my subroutine, I make sure all the needed information is in registers (which I checked that SimpleText was not using), I call my various functions and then perform any work that was lost from overwriting or jumping over the original code. Specifically, I get the MBarHeight into D0 and set the current port to the window.&lt;/p&gt;
    &lt;p&gt;Easy! But, later in the code, SimpleText reads the content of whatever document is being opened and once again resizes the window. So, I needed to patch later code as well. How could I determine at that point that a replacement window size was being used? I simply store the result of the first subroutine (see SetRecentResult above) and then check it on later calls.&lt;/p&gt;
    &lt;p&gt;Where could I store this information? it is not possible to add global variables and the registers are all reused by SimpleText between the first and second routines. Well, you can store a variable (or an entire structure with many variables) within the code itself. Here is my little workaround for CodeWarrior.&lt;/p&gt;
    &lt;p&gt;A couple of other tricks.&lt;/p&gt;
    &lt;p&gt;1. The system routine GetHandleSize has some glue code (they intercept the call in a local library before calling the system). I needed this call, but didn't want to add the weight of CodeWarrior's libary. So, I defined the direct call to GetHandleSize (I didn't need the glue fix).&lt;/p&gt;
    &lt;p&gt;2. You can pass any of the scratch registers (D0-D2, A0-A1, FP0-FP3) to a C function. The way of defining that in CodeWarrior is noted below. You cannot use any other registers. To make debugging easier, I wrote the original subroutine as a true C function, and the register-&amp;gt;C function as a wrapper.&lt;/p&gt;
    &lt;p&gt;3. CodeWarrior does not support BSR for some reason. Use JSR instead. Also, a called routine must be placed before the caller routine in order to generate a short relative JSR rather than absolute address. See my 'RecentResult' example above, where the routines that call RecentResult are placed after it in code.&lt;/p&gt;
    &lt;p&gt;4. SimpleText stores literals (strings, constants) at the end of the 'CODE' resource. After that is where I placed my code. Unfortunately, this breaks disassembly in ResEdit. Below, do you see 'A9FF'? That's the '_Debugger' trap call. It is follow by the rest of the code, and the MacsBug symbols for "SimpleTextWindowChoicePrep'&lt;/p&gt;
    &lt;p&gt;I then needed to hand compute the JSR patched in the original SimpleText code to this location at the end of the code resource.&lt;/p&gt;
    &lt;p&gt;5. To make it easy to redefine window sizes in the future, I added a resource.&lt;/p&gt;
    &lt;p&gt;The ResEdit definition of this resource is the TMPL. By the way, I have experienced corruption twice with ResEdit 2.1.3. Perhaps it has a bug with templates?&lt;/p&gt;
    &lt;p&gt;I doubt this information will be useful to most people. However, it may help avoid some frustrating issues for those few people that attempt patching old software.&lt;/p&gt;
    &lt;p&gt;Attached is the hacked version of SimpleText.&lt;/p&gt;
    &lt;p&gt;- David&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tinkerdifferent.com/threads/patching-68k-software-simpletext.4793/"/><published>2025-11-04T22:59:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816963</id><title>UPS plane crashes near Louisville airport</title><updated>2025-11-05T11:34:50.041603+00:00</updated><content>&lt;doc fingerprint="771c0157a07387ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;UPS plane crashes near Louisville airport, at least 7 killed, officials say&lt;/head&gt;
    &lt;p&gt;At least seven people were killed and several others injured after a UPS plane crashed shortly after takeoff near the Louisville International Airport on Tuesday, officials said.&lt;/p&gt;
    &lt;p&gt;The number of fatalities is expected to increase, Kentucky Gov. Andy Beshear wrote on social media Tuesday night. At least 11 people were injured, officials said earlier.&lt;/p&gt;
    &lt;p&gt;Louisville Mayor Craig Greenberg confirmed at a news conference Tuesday night that at least four people had been killed on the ground.&lt;/p&gt;
    &lt;p&gt;UPS Flight 2976 crashed around 5:15 p.m. local time after it departed from the Louisville airport, according to the Federal Aviation Administration. The aircraft was headed to Daniel K. Inouye International Airport in Honolulu, Hawaii, when it went down three miles south of the airfield, Louisville airport public safety officer Jonathan Biven said at a news conference.&lt;/p&gt;
    &lt;p&gt;Beshear said there was no hazardous cargo onboard the plane that would create environmental issues around the crash site, but urged residents to follow any shelter-in-place orders.&lt;/p&gt;
    &lt;p&gt;The Louisville Metro Police Department described the scene as active with "fire and debris," warning residents to stay away from Fern Valley and Grade Lane, an intersection located on the south side of the airport, which serves as the hub of UPS. More than 100 firefighters responded to the crash and were still battling hot spots as of Tuesday night, Greenberg said. Louisville Fire Chief Brian O'Neill noted the fire was "almost entirely contained."&lt;/p&gt;
    &lt;p&gt;O'Neill said the fire had been extinguished enough to allow a formal grid search for any possible victims in the area.&lt;/p&gt;
    &lt;p&gt;Videos of the crash showed the aircraft partially on fire as it sped down the runway before it burst into flames.&lt;/p&gt;
    &lt;p&gt;A shelter-in-place order has been reduced to a one-mile radius of the crash site, authorities said. The police department also urged those in the area to turn off any air intake systems as soon as possible due to the smoke in the area.&lt;/p&gt;
    &lt;p&gt;"Anybody who has seen the images and the video knows how violent this crash is, and there are a lot of families that are gonna be waiting and wondering for a period of time. We're going to try to get them that information as fast as we can," Beshear said.&lt;/p&gt;
    &lt;p&gt;UPS said in a statement that it was notified of an incident involving one of its aircraft. Three UPS crewmembers were on board, the company said. It didn't immediately provide more details.&lt;/p&gt;
    &lt;p&gt;"We do not at the moment have the status of the crew," Beshear said. "Watching that video, I think we're all very, very worried about them."&lt;/p&gt;
    &lt;p&gt;Businesses in the area were heavily impacted by the crash, including Kentucky Petroleum Recycling and Grade A Auto Parts, the governor said.&lt;/p&gt;
    &lt;p&gt;All arriving and departing flights at the Louisville airport were temporarily suspended. The airport will remain closed Tuesday night, but is expected to reopen Wednesday morning, Greenberg said.&lt;/p&gt;
    &lt;p&gt;According to preliminary flight data from FlightRadar24, the plane appeared to hit 175 feet in altitude briefly after takeoff. It would have been full of fuel for the flight to Hawaii, which likely led to the significant fire as seen from CBS affiliate WLKY-TV's chopper.&lt;/p&gt;
    &lt;p&gt;The three-engine McDonnell Douglas MD-11 was manufactured in 1991, according to FAA data.&lt;/p&gt;
    &lt;p&gt;It was carrying approximately 38,000 gallons of fuel, which weighs about 233,000 pounds, Louisville Fire Chief Brian O'Neill said. The area affected by the crash is about a city block wide, he said, but it has been difficult to contain the fire due to surrounding hazardous materials.&lt;/p&gt;
    &lt;p&gt;Greenberg urged any residents who find debris on their property not to touch it, and instead report it through a website that should be live by Wednesday.&lt;/p&gt;
    &lt;p&gt;The crash is where UPS Worldport, an international air hub for the parcel service, is located. UPS said it was halting package sorting operations at Worldport on Tuesday night.&lt;/p&gt;
    &lt;p&gt;"This is a UPS town," Louisville Metro Councilwoman Betsy Ruhe, whose district is part of the crash site, said during the news conference Tuesday night. "We all know somebody who works at UPS, and they're all texting their friends, their family, trying to make sure everyone is safe. Sadly, some of those texts are probably going to go unanswered."&lt;/p&gt;
    &lt;p&gt;The 5.2 million-square-foot facility processes more than 400,000 packages an hour and is home to 20,000 UPS workers and 300 daily flights, according to the company.&lt;/p&gt;
    &lt;p&gt;"My team and I are closely monitoring the plane crash near Louisville Muhammad Ali International Airport," Kentucky Sen. Rand Paul said. "We continue to pray for the safety of the aircrew, everyone in the area, and for the first-responders on the scene."&lt;/p&gt;
    &lt;p&gt;The National Transportation Safety Board will lead the investigation into the crash. A team of 28 investigators is expected to arrive Wednesday, Greenberg said.&lt;/p&gt;
    &lt;p&gt;All public schools in the Jefferson County School District, the largest school district in Kentucky with a little under 100,000 students, will be closed Wednesday, Greenberg said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cbsnews.com/news/ups-plane-crash-louisville-kentucky/"/><published>2025-11-04T23:10:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816968</id><title>Google Removed 749M Anna's Archive URLs from Its Search Results</title><updated>2025-11-05T11:34:49.800643+00:00</updated><content>&lt;doc fingerprint="3b2013d053b71323"&gt;
  &lt;main&gt;
    &lt;p&gt;Anna‚Äôs Archive is a meta-search engine for shadow libraries that allows users to find pirated books and other related sources.&lt;/p&gt;
    &lt;p&gt;The site launched in the fall of 2022, just days after Z-Library was targeted in a U.S. criminal crackdown, to ensure continued availability of ‚Äòfree‚Äô books and articles to the broader public.&lt;/p&gt;
    &lt;p&gt;In the three years since then, Anna‚Äôs Archive has built up quite the track record. The site has been blocked in various countries, was sued in the U.S. after it scraped WorldCat, and actively provides assistance to AI researchers who want to use its library for model training.&lt;/p&gt;
    &lt;p&gt;Despite legal pressure, Annas-archive.org and the related .li and .se domains remain operational. This is a thorn in the side of publishers who are actively trying to take the site down. In the absence of options to target the site directly, they ask third-party intermediaries such as Google to lend a hand.&lt;/p&gt;
    &lt;head rend="h2"&gt;749 Million URLs&lt;/head&gt;
    &lt;p&gt;Google and other major search engines allow rightsholders to request removal of allegedly infringing URLs. The aim is to ensure that pirate sites no longer show up in search results when people search for books, movies, music, or other copyrighted content.&lt;/p&gt;
    &lt;p&gt;The Pirate Bay, for example, has been a popular target; Google has removed more than 4.2 million thepiratebay.org URLs over the years in response to copyright holder complaints. While this sounds like a sizable number, it pales in comparison to the volume of takedowns targeting Anna‚Äôs Archive.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs transparency report reveals that rightsholders asked Google to remove 784 million URLs, divided over the three main Anna‚Äôs Archive domains. A small number were rejected, mainly because Google didn‚Äôt index the reported links, resulting in 749 million confirmed removals.&lt;/p&gt;
    &lt;p&gt;The comparison to sites such as The Pirate Bay isn‚Äôt fair, as Anna‚Äôs Archive has many more pages in its archive and uses multiple country-specific subdomains. This means that there‚Äôs simply more content to take down. That said, in terms of takedown activity, the site‚Äôs three domain names clearly dwarf all pirate competition.&lt;/p&gt;
    &lt;head rend="h2"&gt;5% of All Google Takedowns, Ever&lt;/head&gt;
    &lt;p&gt;Since Google published its first transparency report in May 2012, rightsholders have flagged 15.1 billion allegedly infringing URLs. That‚Äôs a staggering number, but the fact that 5% of the total targeted Anna‚Äôs Archive URLs is remarkable.&lt;/p&gt;
    &lt;p&gt;Penguin Random House and John Wiley &amp;amp; Sons are the most active publishers targeting the site, but they are certainly not alone. According to Google data, more than 1,000 authors or publishers have sent DMCA notices targeting Anna‚Äôs Archive domains.&lt;/p&gt;
    &lt;p&gt;Yet, there appears to be no end in sight. Rightsholders are reporting roughly 10 million new URLs per week for the popular piracy library, so there is no shortage of content to report.&lt;/p&gt;
    &lt;p&gt;With these DMCA takedown notices, publishers are aiming to make it as difficult as possible for people to find books on the site using Google. This works, as many URLs are now delisted while others are actively being demoted by the search engine for book-related queries.&lt;/p&gt;
    &lt;p&gt;That said, the Anna‚Äôs Archive website is certainly not unfindable. Searching for the site‚Äôs name in Google still shows the main domain as the top search result.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://torrentfreak.com/google-removed-749-million-annas-archive-urls-from-its-search-results/"/><published>2025-11-04T23:11:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45817114</id><title>Bluetui ‚Äì A TUI for managing Bluetooth on Linux</title><updated>2025-11-05T11:34:49.194809+00:00</updated><content>&lt;doc fingerprint="e4d181ddbeaa2ea"&gt;
  &lt;main&gt;
    &lt;p&gt;A Linux based OS with bluez installed.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;You might need to install nerdfonts for the icons to be displayed correctly.&lt;/p&gt;
    &lt;p&gt;You can download the pre-built binaries from the release page release page&lt;/p&gt;
    &lt;p&gt;You can install &lt;code&gt;bluetui&lt;/code&gt; from crates.io&lt;/p&gt;
    &lt;code&gt;cargo install bluetui&lt;/code&gt;
    &lt;p&gt;You can install &lt;code&gt;bluetui&lt;/code&gt; from the extra repository:&lt;/p&gt;
    &lt;code&gt;pacman -S bluetui&lt;/code&gt;
    &lt;p&gt;You can install &lt;code&gt;bluetui&lt;/code&gt; from the lamdness Gentoo Overlay:&lt;/p&gt;
    &lt;code&gt;sudo eselect repository enable lamdness
sudo emaint -r lamdness sync
sudo emerge -av net-wireless/bluetui&lt;/code&gt;
    &lt;p&gt;If you are a user of x-cmd, you can run:&lt;/p&gt;
    &lt;code&gt;x install bluetui&lt;/code&gt;
    &lt;p&gt;Run the following command:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/pythops/bluetui
cd bluetui
cargo build --release&lt;/code&gt;
    &lt;p&gt;This will produce an executable file at &lt;code&gt;target/release/bluetui&lt;/code&gt; that you can copy to a directory in your &lt;code&gt;$PATH&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Tab&lt;/code&gt;: Switch between different sections.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;j&lt;/code&gt; or &lt;code&gt;Down&lt;/code&gt; : Scroll down.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;k&lt;/code&gt; or &lt;code&gt;Up&lt;/code&gt;: Scroll up.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;s&lt;/code&gt;: Start/Stop scanning.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;?&lt;/code&gt;: Show help.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;esc&lt;/code&gt;: Dismiss the help pop-up.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;ctrl+c&lt;/code&gt; or &lt;code&gt;q&lt;/code&gt;: Quit the app.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;p&lt;/code&gt;: Enable/Disable the pairing.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;o&lt;/code&gt;: Power on/off the adapter.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;d&lt;/code&gt;: Enable/Disable the discovery.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;u&lt;/code&gt;: Unpair the device.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Space or Enter&lt;/code&gt;: Connect/Disconnect the device.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;t&lt;/code&gt;: Trust/Untrust the device.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;e&lt;/code&gt;: Rename the device.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Space or Enter&lt;/code&gt;: Pair the device.&lt;/p&gt;
    &lt;p&gt;Keybindings can be customized in the default config file location &lt;code&gt;$HOME/.config/bluetui/config.toml&lt;/code&gt; or from a custom path with &lt;code&gt;-c&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;# Possible values: "Legacy", "Start", "End", "Center", "SpaceAround", "SpaceBetween"
layout = "SpaceAround"

# Window width
# Possible values: "auto" or a positive integer
width = "auto"

toggle_scanning = "s"

[adapter]
toggle_pairing = "p"
toggle_power = "o"
toggle_discovery = "d"

[paired_device]
unpair = "u"
toggle_trust = "t"
rename = "e"&lt;/code&gt;
    &lt;p&gt;GPLv3&lt;/p&gt;
    &lt;p&gt;Bluetui logo: Marco Bulgarelli&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/pythops/bluetui"/><published>2025-11-04T23:29:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45817167</id><title>Uncle Sam wants to scan your iris and collect your DNA, citizen or not</title><updated>2025-11-05T11:34:48.952354+00:00</updated><content>&lt;doc fingerprint="fbca09582881b604"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Uncle Sam wants to scan your iris and collect your DNA, citizen or not&lt;/head&gt;
    &lt;head rend="h2"&gt;DHS rule would expand biometric collection to immigrants and some citizens linked to them&lt;/head&gt;
    &lt;p&gt;If you're filing an immigration form - or helping someone who is - the Feds may soon want to look in your eyes, swab your cheek, and scan your face. The US Department of Homeland Security wants to greatly expand biometric data collection for immigration applications, covering immigrants and even some US citizens tied to those cases.&lt;/p&gt;
    &lt;p&gt;DHS, through its component agency US Citizenship and Immigration Services, on Monday proposed a sweeping expansion of the agency's collection of biometric data. While ostensibly about verifying identities and preventing fraud in immigration benefit applications, the proposed rule goes much further than simply ensuring applicants are who they claim to be.&lt;/p&gt;
    &lt;p&gt;First off, the rule proposes expanding when DHS can collect biometric data from immigration benefit applicants, as "submission of biometrics is currently only mandatory for certain benefit requests and enforcement actions." DHS wants to change that, including by requiring practically everyone an immigrant is associated with to submit their biometric data.&lt;/p&gt;
    &lt;p&gt;"DHS proposes in this rule that any applicant, petitioner, sponsor, supporter, derivative, dependent, beneficiary, or individual filing or associated with a benefit request or other request or collection of information, including U.S. citizens, U.S. nationals and lawful permanent residents, and without regard to age, must submit biometrics unless DHS otherwise exempts the requirement," the rule proposal said.&lt;/p&gt;
    &lt;p&gt;DHS also wants to require the collection of biometric data from "any alien apprehended, arrested or encountered by DHS."&lt;/p&gt;
    &lt;p&gt;It's not explicitly stated in the rule proposal why US citizens associated with immigrants who are applying for benefits would have to have their biometric data collected. DHS didn't answer questions to that end, though the rule stated that US citizens would also be required to submit biometric data "when they submit a family-based visa petition."&lt;/p&gt;
    &lt;head rend="h3"&gt;Give me your voice, your eye print, your DNA samples&lt;/head&gt;
    &lt;p&gt;In addition to expanded collection, the proposed rule also changes the definition of what DHS considers to be valid biometric data.&lt;/p&gt;
    &lt;p&gt;"Government agencies have grouped together identifying features and actions, such as fingerprints, photographs, and signatures under the broad term, biometrics," the proposal states. "DHS proposes to define the term 'biometrics' to mean 'measurable biological (anatomical, physiological or molecular structure) or behavioral characteristics of an individual,'" thus giving DHS broad leeway to begin collecting new types of biometric data as new technologies are developed.&lt;/p&gt;
    &lt;p&gt;The proposal mentions several new biometric technologies DHS wants the option to use, including ocular imagery, voice prints and DNA, all on the table per the new rule.&lt;/p&gt;
    &lt;p&gt;"The rule proposes to grant DHS express authority to require, request, or accept raw DNA or DNA test results," DHS said, including "to prove or disprove ‚Ä¶ biological sex" in situations where that can affect benefit eligibility.&lt;/p&gt;
    &lt;p&gt;DHS wants to use all that data for identity enrollment, verification and management of the immigration lifecycle, national security and criminal history checks, "the production of secure identity documents," to prove familial relationships, and to perform other administrative functions, the rule states.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Facial recognition works better in the lab than on the street, researchers show&lt;/item&gt;
      &lt;item&gt;EU biometric border system launch hits inevitable teething problems&lt;/item&gt;
      &lt;item&gt;Vietnam to collect biometrics - even DNA - for new ID cards&lt;/item&gt;
      &lt;item&gt;Altman's eyeball-scanning biometric blockchain orbs officially come to America&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we noted in our story last week about DHS' new rule expanding biometric data collection on entry into and exit from the US, biometric technology - especially the often-used facial recognition scan - is ripe for misuse and prone to errors.&lt;/p&gt;
    &lt;p&gt;This new proposed rule goes far beyond subjecting immigrants to algorithmic identification tech prone to misidentifying non-white individuals, however, and reaches a new level of surveillance, with DHS seeking to collect and keep DNA test results - including partial profiles - from immigrants and some US citizens to verify family ties or biological sex when relevant. It's not much more assuring that DHS also wants to collect new forms of biometric data like voice records, which are increasingly easy to spoof with AI.&lt;/p&gt;
    &lt;p&gt;When we asked DHS questions about its biometric expansion proposal, it only sent us a statement identical to the one it sent last week when we inquired about the new entry/exit biometric requirements. The agency didn't respond when we asked for a statement pertaining to this latest proposed rule.&lt;/p&gt;
    &lt;p&gt;DHS is taking comments on the proposal until January 2; so far the submissions are nearly entirely negative, with posters decrying the plan as government overreach, comparing the proposal to communist China, and calling it a violation of Constitutional guarantees against unreasonable search and seizure. ¬Æ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/11/04/dhs_wants_to_collect_biometric_data/"/><published>2025-11-04T23:35:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45818319</id><title>Direct File won't happen in 2026, IRS tells states</title><updated>2025-11-05T11:34:48.495355+00:00</updated><content>&lt;doc fingerprint="b8a1c09cd9635069"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Direct File won‚Äôt happen in 2026, IRS tells states&lt;/head&gt;
    &lt;head rend="h2"&gt;The free service that allowed taxpayers to file online directly with the IRS was used by hundreds of thousands of taxpayers in 2024 and 2025, who gave it high marks ‚Äî although tax prep companies and Republicans have sought its end.&lt;/head&gt;
    &lt;p&gt;The IRS has notified states that offered the free, government tax filing service known as Direct File in 2025 that the program won‚Äôt be available next filing season.&lt;/p&gt;
    &lt;p&gt;In an email sent from the IRS to 25 states, the tax agency thanked them for collaborating and noted that ‚Äúno launch date has been set for the future.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIRS Direct File will not be available in Filing Season 2026,‚Äù says the Monday email, obtained by Nextgov/FCW and confirmed by multiple sources. It follows reports that the program was ending and Trump‚Äôs former tax chief, Billy Long, remarking over the summer that the service was ‚Äúgone.‚Äù&lt;/p&gt;
    &lt;p&gt;The program, which debuted in 2024, was a big shift from the decades-long IRS policy of not competing with the tax prep industry in offering its own free, online tax filing service for Americans. Many Republicans had opposed Direct File, and tax prep companies also lobbied against it.&lt;/p&gt;
    &lt;p&gt;Still, most of the taxpayers that used Direct File earlier this year ‚Äî over 296,500 ‚Äî gave it high marks.&lt;/p&gt;
    &lt;p&gt;Those users won‚Äôt be able to log on to the Direct File website to get their returns anymore, according to the new email, which directs anyone needing a transcript to their IRS online accounts.&lt;/p&gt;
    &lt;p&gt;The Trump administration‚Äôs massive tax and spending policy bill signed into law over the summer directed the IRS to set up a task force to examine how the tax agency can use public-private partnerships to replace Direct File.&lt;/p&gt;
    &lt;p&gt;The IRS has relied on a public-private partnership called Free File for decades to give most Americans a free way to file their taxes, although it's been extremely underutilized. Only 3% of eligible taxpayers used it in recent years. Some of the member companies were found to have pushed people toward products they‚Äôd have to pay for, even when they could‚Äôve used free options.&lt;/p&gt;
    &lt;p&gt;"It's not surprising since the Trump administration sabotaged Direct File all through this year's filing season, at the urging of tax prep monopolies like TurboTax," Adam Ruben, the vice president of the Economic Security Project, told Nextgov/FCW. "Trump's billionaire friends get favors while honest hardworking Americans will pay more to file their taxes."&lt;/p&gt;
    &lt;p&gt;Sen. Elizabeth Warren, D-Mass., told Nextgov/FCW that "the fight isn't over," saying that "giant tax prep companies are popping champagne, while Americans are forced to spend more time and more money to file their taxes."&lt;/p&gt;
    &lt;p&gt;The IRS did not respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Editor's note: This article has been updated to include comment from Sen. Elizabeth Warren.&lt;/p&gt;
    &lt;p&gt;If you have a tip you'd like to share, Natalie Alms can be securely contacted at nalms.41 on Signal.&lt;/p&gt;
    &lt;p&gt;NEXT STORY: CBP expands facial recognition for non-citizens at borders&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nextgov.com/digital-government/2025/11/direct-file-wont-happen-2026-irs-tells-states/409309/"/><published>2025-11-05T02:30:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45818471</id><title>The Microsoft SoftCard for the Apple II: Getting two processors to share memory</title><updated>2025-11-05T11:34:48.210450+00:00</updated><content>&lt;doc fingerprint="a2dfd9092d34a089"&gt;
  &lt;main&gt;
    &lt;p&gt;The Microsoft Z-80 SoftCard was a plug-in expansion card for the Apple II that added the ability to run CP/M software. According to Wikipedia, it was Microsoft‚Äôs first hardware product and in 1980 was the single largest revenue source for the company.&lt;/p&gt;
    &lt;p&gt;CP/M runs on an 8080 processor, but the Apple II has a 6502 processor. So how can you run CP/M on an Apple II? Answer: The card comes with its own 8080-compatible processor, the Zilog Z80, which was arguable better than the 8080 for a bunch of reasons given on its Wikipedia page.¬π&lt;/p&gt;
    &lt;p&gt;Great, you now have a processor. But what happens to the old 6502 processor? Ideally, you would just shut it off, but you can‚Äôt go cold turkey because some things still had to be handled by the 6502.¬≤ Nicole Branagan digs deeper into the story of how the two processors coexist. The idea is that the SoftCard tells the 6502 that it‚Äôs doing DMA, so the 6502 pauses and waits for the DMA to complete. However, you can‚Äôt leave the 6502 paused for too long or its internal registers degrade and lose their values.&lt;/p&gt;
    &lt;p&gt;The solution is to take advantage of the Z80‚Äôs REFRESH line, which the processor uses to signal that it‚Äôs not accessing memory right now (because it‚Äôs decoding an instruction). This tells external memory refresh circuitry that it can run and keep the RAM values refreshed so that they don‚Äôt degrade and lose their values.&lt;/p&gt;
    &lt;p&gt;On the Apple II, memory refreshing is done by the video circuitry, so there is need for a dedicated REFRESH signal. The SoftCard uses this signal to allow the 6502 to execute a tiny little bit. (Presumably it is sitting in a spin loop waiting to be woken.) This keeps the 6502‚Äôs registers refreshed.&lt;/p&gt;
    &lt;p&gt;When the SoftCard needs the 6502 to do actual work, it can update some memory to tell the 6502, ‚ÄúBreak out of your spin loop and do something for me, then let me know the answer and go back to the spin loop.‚Äù The Z80 then goes to sleep until it gets an answer from the 6502.&lt;/p&gt;
    &lt;p&gt;Another wrinkle in the way that the 6502 and Z80 shared memory is in the memory map. Both the Z80 and 6502 consider the first 256 bytes of memory to be special and want to use it for different things. Furthermore, CP/M programs expect to be loaded at $0100, but the 6502 hard-codes its CPU stack to live in the range $0100‚Äì$01FF. There are other obstacles in the low part of the Apple II memory map: The Apple II system monitor uses $0200‚Äì$02FF as its keyboard input buffer, the bytes in the range $03F0‚Äì$03FF are used to hold interrupt vectors, and the text video frame buffer goes from $0400‚Äì$07FF. (There is a second text video frame buffer from $0800‚Äì$0BFF, but almost nobody uses it.) Other big obstacles are the memory range from $C000‚Äì$CFFF, which is used by peripheral devices, and the memory range from $D000‚Äì$FFFF, which holds the Apple II monitor ROM, but can be replaced by RAM if you have the Language Card (a 16KB memory expansion card), except that the last few bytes $FFFA‚Äì$FFFF are used by the CPU as interrupt vectors.&lt;/p&gt;
    &lt;p&gt;The solution is to remap the memory by putting address translation circuitry on the SoftCard, so that when the Z80 asks for memory address $0000, say, it actually gets physical memory $1000. The remapping is carefully arranged so that all of the Apple II‚Äôs special reserved addresses get shuffled to the end of the Z80 memory map, and all of the Apple II‚Äôs normal RAM occupies contiguous address space in the Z80 memory map starting at $0000.¬≥&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;6502&lt;/cell&gt;
        &lt;cell&gt;Physical&lt;/cell&gt;
        &lt;cell&gt;Z80&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Special use&lt;/cell&gt;
        &lt;cell&gt;$0000‚Äì$0FFF&lt;/cell&gt;
        &lt;cell&gt;‚Üò&lt;/cell&gt;
        &lt;cell&gt;$1000‚Äì$1FFF&lt;/cell&gt;
        &lt;cell&gt;$0000‚Äì$0FFF&lt;/cell&gt;
        &lt;cell&gt;normal RAM&lt;p&gt;(contiguous,&lt;/p&gt;&lt;p&gt;up to&lt;/p&gt;&lt;p&gt;installed RAM)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;normal RAM&lt;p&gt;(contiguous,&lt;/p&gt;&lt;p&gt;up to&lt;/p&gt;&lt;p&gt;installed RAM)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;$1000‚Äì$1FFF&lt;/cell&gt;
        &lt;cell&gt;‚Üó&lt;/cell&gt;
        &lt;cell&gt;$2000‚Äì$2FFF&lt;/cell&gt;
        &lt;cell&gt;$1000‚Äì$1FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$2000‚Äì$2FFF&lt;/cell&gt;
        &lt;cell&gt;$3000‚Äì$3FFF&lt;/cell&gt;
        &lt;cell&gt;$2000‚Äì$2FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$3000‚Äì$3FFF&lt;/cell&gt;
        &lt;cell&gt;$4000‚Äì$4FFF&lt;/cell&gt;
        &lt;cell&gt;$3000‚Äì$3FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$4000‚Äì$4FFF&lt;/cell&gt;
        &lt;cell&gt;$5000‚Äì$5FFF&lt;/cell&gt;
        &lt;cell&gt;$4000‚Äì$4FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$5000‚Äì$5FFF&lt;/cell&gt;
        &lt;cell&gt;$6000‚Äì$6FFF&lt;/cell&gt;
        &lt;cell&gt;$5000‚Äì$5FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$6000‚Äì$6FFF&lt;/cell&gt;
        &lt;cell&gt;$7000‚Äì$7FFF&lt;/cell&gt;
        &lt;cell&gt;$6000‚Äì$6FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$7000‚Äì$7FFF&lt;/cell&gt;
        &lt;cell&gt;$8000‚Äì$8FFF&lt;/cell&gt;
        &lt;cell&gt;$7000‚Äì$7FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$8000‚Äì$8FFF&lt;/cell&gt;
        &lt;cell&gt;$9000‚Äì$9FFF&lt;/cell&gt;
        &lt;cell&gt;$8000‚Äì$8FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$9000‚Äì$9FFF&lt;/cell&gt;
        &lt;cell&gt;$A000‚Äì$AFFF&lt;/cell&gt;
        &lt;cell&gt;$9000‚Äì$9FFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$A000‚Äì$AFFF&lt;/cell&gt;
        &lt;cell&gt;$B000‚Äì$BFFF&lt;/cell&gt;
        &lt;cell&gt;$A000‚Äì$AFFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$B000‚Äì$BFFF&lt;/cell&gt;
        &lt;cell&gt;$D000‚Äì$DFFF&lt;/cell&gt;
        &lt;cell&gt;$B000‚Äì$BFFF&lt;/cell&gt;
        &lt;cell&gt;expansion RAM&lt;p&gt;(except for&lt;/p&gt;&lt;p&gt;last 6 bytes)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;I/O space&lt;/cell&gt;
        &lt;cell&gt;$C000‚Äì$CFFF&lt;/cell&gt;
        &lt;cell&gt;‚Üò&lt;/cell&gt;
        &lt;cell&gt;$E000‚Äì$EFFF&lt;/cell&gt;
        &lt;cell&gt;$C000‚Äì$CFFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;expansion RAM&lt;p&gt;(except for&lt;/p&gt;&lt;p&gt;last 6 bytes)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;$D000‚Äì$DFFF&lt;/cell&gt;
        &lt;cell&gt;‚Üó&lt;/cell&gt;
        &lt;cell&gt;$F000‚Äì$FFFF&lt;/cell&gt;
        &lt;cell&gt;$D000‚Äì$DFFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;$E000‚Äì$EFFF&lt;/cell&gt;
        &lt;cell&gt;$C000‚Äì$CFFF&lt;/cell&gt;
        &lt;cell&gt;$E000‚Äì$EFFF&lt;/cell&gt;
        &lt;cell&gt;I/O space&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;$F000‚Äì$FFFF&lt;/cell&gt;
        &lt;cell&gt;$0000‚Äì$0FFF&lt;/cell&gt;
        &lt;cell&gt;$F000‚Äì$FFFF&lt;/cell&gt;
        &lt;cell&gt;Special use&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The SoftCard manual contained lots of details on how to write code for it. For example, it included instructions on how to call into a 6502 subroutine from Z80 and had a chart showing how the memory was remapped for the Z80. It even included the Z80 processor reference manual, listing all the instructions. This will come in handy in a future story.&lt;/p&gt;
    &lt;p&gt;¬π I don‚Äôt know where the hyphen in Z-80 came from.&lt;/p&gt;
    &lt;p&gt;¬≤ In many places, I/O was handled by timing loops, so if you wanted to access, say, the game paddles, you had to let the 6502 do the I/O with its precise software timing loops.&lt;/p&gt;
    &lt;p&gt;¬≥ There were also two high resolution graphics frame buffers, one at $2000‚Äì$3FFF, and another at $4000‚Äì$5FFF. These were right in the middle of the Z80 memory map, but in practice it wasn‚Äôt a problem because CP/M was a text-mode operating system, so the programs you were running didn‚Äôt try to do graphics anyway.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://devblogs.microsoft.com/oldnewthing/20251104-00/?p=111758"/><published>2025-11-05T02:58:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45818499</id><title>Preventing Kubernetes from Pulling the Pause Image from the Internet</title><updated>2025-11-05T11:34:47.373369+00:00</updated><content>&lt;doc fingerprint="3510c03015708d17"&gt;
  &lt;main&gt;
    &lt;p&gt;I don‚Äôt normally write blog posts that regurgitate information from normal documentation, but this particular subject irks me.&lt;/p&gt;
    &lt;p&gt;If you are running an internal Kubernetes (k8s) platform, you owe it to yourself to make sure there is nothing external to your platform determining your reliability.&lt;/p&gt;
    &lt;p&gt;You could ask yourself: How many internet dependencies do you have to start a pod? Should be zero, right???&lt;/p&gt;
    &lt;p&gt;If you use stock k8s, you might be surprised to know that each of your k8s nodes is actually reaching out to &lt;code&gt;registry.k8s.io&lt;/code&gt; on first pod creation to get the &lt;code&gt;pause&lt;/code&gt; image:&lt;/p&gt;
    &lt;code&gt;$ sudo crictl images
IMAGE                                     TAG                 IMAGE ID            SIZE
registry.k8s.io/pause                     3.9                 e6f1816883972
&lt;/code&gt;
    &lt;p&gt;If you want to change that, you can update your containerd (1.x) toml:&lt;/p&gt;
    &lt;code&gt;[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = "YOUR_REGISTRY/pause:3.10"
&lt;/code&gt;
    &lt;p&gt;And depend on one less thing. The rest of the blog post will go deeper into why this is the case.&lt;/p&gt;
    &lt;head rend="h1"&gt;What Is The Pause Image Anyway?&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;pause&lt;/code&gt; image is the container image that backs the k8s ‚Äúsandbox‚Äù of a pod.
This &lt;code&gt;pause&lt;/code&gt; container is designed to hold the linux namespaces.
The &lt;code&gt;pause&lt;/code&gt; container used to also reap zombie processes from the other containers in a pod, its duty as PID1, but that isn‚Äôt the case by default anymore in k8s 1.8+.&lt;/p&gt;
    &lt;p&gt;The sandbox of a pod is part of the CRI spec. The CRI spec is a generic way for k8s to talk pods (and sandboxes) that is not specific to any particular container runtime (like containerd). Any container runtime that implements the CRI spec can, in theory, run k8s pods.&lt;/p&gt;
    &lt;p&gt;This means that the &lt;code&gt;pause&lt;/code&gt; image has more to do with CRI than it does with k8s.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where The Pause Image Comes From (CRI)&lt;/head&gt;
    &lt;p&gt;When a CRI-enabled container runtime needs to create a sandbox, at least with the case of containerd, it does this by creating a real container.&lt;/p&gt;
    &lt;p&gt;The image containerd is configured to use (by default) to create that sandbox, is the &lt;code&gt;pause&lt;/code&gt; image.
You can see this in code here.&lt;/p&gt;
    &lt;head rend="h1"&gt;How To Point Containerd To Your Local Pause Image&lt;/head&gt;
    &lt;p&gt;Per the current docs, you can overwrite the containerd sandbox image with a containerd configuration like this (assuming you have mirrored to a local registry):&lt;/p&gt;
    &lt;p&gt;(containerd 1.x)&lt;/p&gt;
    &lt;code&gt;[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = "YOUR_REGISTRY/pause:3.10"
&lt;/code&gt;
    &lt;p&gt;(containerd 2.x)&lt;/p&gt;
    &lt;code&gt;version = 3

[plugins]
  [plugins.'io.containerd.cri.v1.images']
    ...
    [plugins.'io.containerd.cri.v1.images'.pinned_images]
      sandbox = 'YOUR_REGISTRY/pause:3.10'
&lt;/code&gt;
    &lt;p&gt;Don‚Äôt take my word for it here, this particular setting has changed over time, check the official docs.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;If you go to registry.k8s.io you will see:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Please note that there is NO uptime SLA as this is a free, volunteer managed service. We will however do our best to respond to issues and the system is designed to be reliable and low-maintenance. If you need higher uptime guarantees please consider mirroring images to a location you control.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So yea, this is your PSA. Please mirror like they recommend and reconfigure as needed to not depend on the internet.&lt;/p&gt;
    &lt;p&gt;Comment via email&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kyle.cascade.family/posts/preventing-kubernetes-from-pulling-the-pause-image-from-the-internet/"/><published>2025-11-05T03:04:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45818562</id><title>Hypothesis: Property-Based Testing for Python</title><updated>2025-11-05T11:34:47.113543+00:00</updated><content>&lt;doc fingerprint="a1031a81e5b71397"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome to Hypothesis!¬∂&lt;/head&gt;
    &lt;p&gt;Hypothesis is the property-based testing library for Python. With Hypothesis, you write tests which should pass for all inputs in whatever range you describe, and let Hypothesis randomly choose which of those inputs to check - including edge cases you might not have thought about. For example:&lt;/p&gt;
    &lt;p&gt;You should start with the tutorial, or alternatively the more condensed quickstart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tutorial¬∂&lt;/head&gt;
    &lt;p&gt;An introduction to Hypothesis.&lt;/p&gt;
    &lt;p&gt;New users should start here, or with the more condensed quickstart.&lt;/p&gt;
    &lt;head rend="h2"&gt;How-to guides¬∂&lt;/head&gt;
    &lt;p&gt;Practical guides for applying Hypothesis in specific scenarios.&lt;/p&gt;
    &lt;head rend="h2"&gt;Explanations¬∂&lt;/head&gt;
    &lt;p&gt;Commentary oriented towards deepening your understanding of Hypothesis.&lt;/p&gt;
    &lt;head rend="h2"&gt;API Reference¬∂&lt;/head&gt;
    &lt;p&gt;Technical API reference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hypothesis.readthedocs.io/en/latest/"/><published>2025-11-05T03:15:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45820715</id><title>The Hackers Manifesto (The Conscience of a Hacker) (1986)</title><updated>2025-11-05T11:34:46.833114+00:00</updated><content>&lt;doc fingerprint="646e8917c32493a6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Title : Hacker's Manifesto&lt;/p&gt;
      &lt;p&gt; Author : The Mentor&lt;/p&gt;
      &lt;quote&gt; ==Phrack Inc.== Volume One, Issue 7, Phile 3 of 10 =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= The following was written shortly after my arrest... \/\The Conscience of a Hacker/\/ by +++The Mentor+++ Written on January 8, 1986 =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Another one got caught today, it's all over the papers. "Teenager Arrested in Computer Crime Scandal", "Hacker Arrested after Bank Tampering"... Damn kids. They're all alike. But did you, in your three-piece psychology and 1950's technobrain, ever take a look behind the eyes of the hacker? Did you ever wonder what made him tick, what forces shaped him, what may have molded him? I am a hacker, enter my world... Mine is a world that begins with school... I'm smarter than most of the other kids, this crap they teach us bores me... Damn underachiever. They're all alike. I'm in junior high or high school. I've listened to teachers explain for the fifteenth time how to reduce a fraction. I understand it. "No, Ms. Smith, I didn't show my work. I did it in my head..." Damn kid. Probably copied it. They're all alike. I made a discovery today. I found a computer. Wait a second, this is cool. It does what I want it to. If it makes a mistake, it's because I screwed it up. Not because it doesn't like me... Or feels threatened by me... Or thinks I'm a smart ass... Or doesn't like teaching and shouldn't be here... Damn kid. All he does is play games. They're all alike. And then it happened... a door opened to a world... rushing through the phone line like heroin through an addict's veins, an electronic pulse is sent out, a refuge from the day-to-day incompetencies is sought... a board is found. "This is it... this is where I belong..." I know everyone here... even if I've never met them, never talked to them, may never hear from them again... I know you all... Damn kid. Tying up the phone line again. They're all alike... You bet your ass we're all alike... we've been spoon-fed baby food at school when we hungered for steak... the bits of meat that you did let slip through were pre-chewed and tasteless. We've been dominated by sadists, or ignored by the apathetic. The few that had something to teach found us will- ing pupils, but those few are like drops of water in the desert. This is our world now... the world of the electron and the switch, the beauty of the baud. We make use of a service already existing without paying for what could be dirt-cheap if it wasn't run by profiteering gluttons, and you call us criminals. We explore... and you call us criminals. We seek after knowledge... and you call us criminals. We exist without skin color, without nationality, without religious bias... and you call us criminals. You build atomic bombs, you wage wars, you murder, cheat, and lie to us and try to make us believe it's for our own good, yet we're the criminals. Yes, I am a criminal. My crime is that of curiosity. My crime is that of judging people by what they say and think, not what they look like. My crime is that of outsmarting you, something that you will never forgive me for. I am a hacker, and this is my manifesto. You may stop this individual, but you can't stop us all... after all, we're all alike. +++The Mentor+++ _______________________________________________________________________________ &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://phrack.org/issues/7/3"/><published>2025-11-05T08:28:04+00:00</published></entry></feed>