<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-17T12:18:58.498353+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46648885</id><title>Dell UltraSharp 52 Thunderbolt Hub Monitor</title><updated>2026-01-17T12:19:09.247706+00:00</updated><content>&lt;doc fingerprint="e22dab5494a79c42"&gt;
  &lt;main&gt;&lt;p&gt;Selecting will change the following options:&lt;/p&gt;&lt;p&gt;From To&lt;/p&gt;&lt;p&gt;51.5"&lt;/p&gt;&lt;p&gt;6144 x 2560 at 120Hz&lt;/p&gt;&lt;p&gt;In-plane Switching (IPS) Black Technology&lt;/p&gt;&lt;p&gt;99% DCI-P3 (CIE 1976)&lt;/p&gt;&lt;p&gt;100% sRGB (CIE 1931)&lt;/p&gt;...See More See More Color Gamut&lt;p&gt;2 HDMI port/s (HDCP 2.2) (Supports up to 6144 x 2560, 120 Hz, VRR, , as specified in HDMI 2.1 (FRL))&lt;/p&gt;&lt;p&gt;2 DisplayPort 1.4 (HDCP 2.2) port/s&lt;/p&gt;...See More See More Ports&lt;p&gt;Add the products you would like to compare, and quickly determine which is best for your needs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories"/><published>2026-01-16T17:14:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46648916</id><title>East Germany balloon escape</title><updated>2026-01-17T12:19:08.979480+00:00</updated><content>&lt;doc fingerprint="be46c17b47664bf8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;East Germany balloon escape&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Native name&lt;/cell&gt;&lt;cell&gt;Die Ballonflucht&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Date&lt;/cell&gt;&lt;cell&gt;16 September 1979&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Time&lt;/cell&gt;&lt;cell&gt;02:00 am (approximate)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Duration&lt;/cell&gt;&lt;cell&gt;25 minutes&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Location&lt;/cell&gt;&lt;cell&gt;Oberlemnitz, East Germany&lt;p&gt;(takeoff)&lt;/p&gt;&lt;p&gt;Naila, West Germany&lt;/p&gt;&lt;p&gt;(landing)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Coordinates&lt;/cell&gt;&lt;cell&gt;50°28′59″N 11°35′29″E / 50.48306°N 11.59139°E[1]&lt;p&gt;(takeoff)&lt;/p&gt;&lt;p&gt;50°19′52.7″N 11°40′13.1″E / 50.331306°N 11.670306°E[1]&lt;/p&gt;&lt;p&gt;(landing)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Organised by&lt;/cell&gt;&lt;cell&gt;Peter Strelzyk &amp;amp; family&lt;p&gt;Günter Wetzel &amp;amp; family&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Participants&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Outcome&lt;/cell&gt;&lt;cell&gt;Successful escape to West Germany&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Non-fatal injuries&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;On 16 September 1979, eight people from two families escaped from East Germany by crossing the border into West Germany at night in a homemade hot air balloon. The unique feat was the result of over a year and a half of preparations involving three different balloons, various modifications, and a first, unsuccessful attempt. The failed attempt alerted the East German authorities to the plot, but the police were unable to identify the escapees before their second, successful flight two months later.&lt;/p&gt;&lt;head rend="h2"&gt;Background&lt;/head&gt;[edit]&lt;p&gt;East Germany, then part of the Eastern Bloc, was separated from West Germany in the Western Bloc by the inner German border and the Berlin Wall, which were heavily fortified with watchtowers, land mines, armed soldiers, and various other measures to prevent illegal crossings. East German border troops were instructed to prevent defection to West Germany by all means, including lethal force (Schießbefehl; "order to fire").[2]&lt;/p&gt;&lt;p&gt;Peter Strelzyk (1942–2017), an electrician and former East German Air Force mechanic, and Günter Wetzel (born 1955), a bricklayer by trade,[3] were colleagues at a local plastics factory.[4] Friends for four years, they shared a desire to flee the country and began discussing ways to get across the border. On 7 March 1978, they agreed to plan an escape.[5] They considered building a helicopter but quickly realized they would be unable to acquire an engine capable of powering such a craft. They then decided to explore the idea of constructing a hot air balloon,[6] having been inspired by a television program about ballooning.[3] An alternate account is that a relative shared a magazine article about the International Balloon Festival in Albuquerque, New Mexico.[5]&lt;/p&gt;&lt;head rend="h2"&gt;Construction&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel began research into balloons. Their plan was to escape with their wives and a total of four children (aged 2 to 15). They calculated the weight of the eight passengers and the craft itself to be around 750 kilograms (1,650 lb). Subsequent calculations determined a balloon capable of lifting this weight would need to hold 2,000 cubic metres (71,000 cu ft) of air heated to 100 °C (212 °F). The next calculation was the amount of material needed for the balloon, estimated to be 800 square metres (8,600 sq ft).[6]&lt;/p&gt;&lt;p&gt;The pair lived in Pößneck, a small town of about 20,000 where large quantities of cloth could not be obtained without raising attention. They tried neighbouring towns of Rudolstadt, Saalfeld, and Jena without success.[7] They travelled 50 km (31 mi) to Gera, where they purchased 1-metre-wide (3 ft 3 in) rolls of cotton cloth totalling 850 metres (2,790 ft) in length at a department store after telling the astonished clerk that they needed the large quantity of material to use as tent lining for their camping club.[6][7]&lt;/p&gt;&lt;p&gt;Wetzel spent two weeks sewing the cloth into a balloon-shaped bag, 15 metres (49 ft) wide by 20 metres (66 ft) long, on a 40-year-old manually operated sewing machine. Strelzyk spent the time building the gondola and burner assembly. The gondola was made from an iron frame, sheet metal floor, and clothesline run around the perimeter every 150 millimetres (5.9 in) for the sides. The burner was made using two 11-kilogram (24 lb) bottles of liquid propane household gas, hoses, water pipe, a nozzle, and a piece of stove pipe.[6]&lt;/p&gt;&lt;head rend="h2"&gt;First test&lt;/head&gt;[edit]&lt;p&gt;The team was ready to test the craft in April 1978. After days of searching, they found a suitable secluded forest clearing near Ziegenrück, 10 km (6.2 mi) from the border and 30 km (19 mi) from Pößneck. After lighting the burner one night, they failed to inflate the balloon. They thought the problem might stem from the fact that they had laid the balloon on the ground. After weeks of additional searching, they found a 25-metre (82 ft) cliff at a rock quarry where they could suspend the balloon vertically before inflation, but that also proved unsuccessful.[6]&lt;/p&gt;&lt;p&gt;The pair then decided to fill the bag with ambient-temperature air before using the burner to raise the air temperature and provide lift. They constructed a blower with a 14 hp (10 kW) 250 cc (15 cu in) motorcycle engine taken from Wetzel's old MZ, started with a Trabant automobile starter powered by jumper cables from Strelzyk's Moskvitch sedan.[8] This engine, silenced by a Trabant muffler, turned 1-metre-long (3.3 ft) fan blades to inflate the balloon. They also used a home-made flamethrower, similar to the gondola's burner, to pre-heat the air faster. With these modifications in place, they returned to the secluded clearing to try again but still could not inflate the balloon. But using the blower did allow them to discover that the cotton material with which they fashioned the balloon was too porous and leaked excessively.[6]&lt;/p&gt;&lt;p&gt;Their unsuccessful effort had cost them 2,400 DDM (US$360). Strelzyk disposed of the cloth by burning it in his furnace over several weeks.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Second test&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel purchased samples of different fabrics in local stores, including umbrella material and various samples of taffeta and nylon. They used an oven to test the material for heat resistance. In addition, they created a test rig from a vacuum cleaner and a water-filled glass tube to determine which material would allow the vacuum to exert the most suction on the water, and consequently which was the most impervious to air. The umbrella covering performed the best but was also the most expensive. They instead selected a synthetic kind of taffeta.[6]&lt;/p&gt;&lt;p&gt;To purchase a large quantity of fabric without arousing too much suspicion, the pair again drove to a distant city. This time they travelled over 160 kilometres (100 mi) to a department store in Leipzig. Their new cover story was that they belonged to a sailing club and needed the material to make sails. The quantity they needed had to be ordered, and although they feared the purchase might be reported to East Germany's State Security Service (Stasi), they returned the next day and picked up the material without incident. They paid 4,800 DDM (US$720) for 800 metres (2,600 ft) of 1-metre-wide (3 ft 3 in) fabric.[6] On the way home, they also purchased an electric motor to speed up the pedal-operated sewing machine they had been using to sew the material into the desired balloon shape.[7]&lt;/p&gt;&lt;p&gt;Wetzel spent the next week sewing the material into another balloon, accomplishing the task faster the second time with the now-electric sewing machine. Soon afterwards, the two men returned to the forest clearing and inflated the bag in about five minutes using the blower and flame thrower. The bag rose and held air, but the burner on the gondola was not powerful enough to create the heat needed for lift. The pair continued experimenting for months, doubling the number of propane tanks and trying different fuel mixtures. Disappointed with the result, Wetzel decided to abandon the project and instead started to pursue the idea of building a small gasoline engine-powered light aeroplane[6] or a glider.[5]&lt;/p&gt;&lt;p&gt;Strelzyk continued trying to improve the burner. In June 1979, he discovered that with the propane tank inverted, additional pressure caused the liquid propane to evaporate, which produced a bigger flame. He modified the gondola to mount the propane tanks upside down, and returned to the test site where he found the new configuration produced a 12-metre (39 ft) long flame. Strelzyk was ready to attempt an escape.[6]&lt;/p&gt;&lt;head rend="h2"&gt;First escape attempt&lt;/head&gt;[edit]&lt;p&gt;On 3 July 1979, the weather and wind conditions were favourable. The entire Strelzyk family lifted from a forest clearing at 1:30 am and climbed at a rate of 4 metres (13 ft) per second. They reached an altitude of 2,000 metres (6,600 ft) according to an altimeter Strelzyk had made by modifying a barometer. A light wind was blowing them towards the border. The balloon then entered clouds, and atmospheric water vapour condensed on the balloon, adding weight which caused it to descend prematurely. The family landed safely approximately 180 metres (590 ft) short of the border, at the edge of the heavily mined border zone. Unsure of where they were, Strelzyk explored until he found a piece of litter – a bread bag from a bakery in Wernigerode, an East German town. The group spent nine hours carefully extricating themselves from the 500-metre (1,600 ft) wide border zone to avoid detection. They also had to travel unnoticed through a 5 km (3.1 mi) restricted zone before hiking back a total of 14 km (8.7 mi) to their car and the launch paraphernalia they had left behind.[6] They made it home just in time to report their absence from work and school was due to sickness.[7]&lt;/p&gt;&lt;p&gt;The abandoned balloon was discovered by the authorities later that morning. Strelzyk destroyed all compromising evidence and sold his car, fearing that it could link him to the escape attempt.[6] On 14 August, the Stasi launched an appeal to find the "perpetrator of a serious offence", listing in detail all the items recovered at the landing site.[9] Strelzyk felt that the Stasi would eventually trace the balloon to him and the Wetzels. He agreed with Wetzel that their best chance was to quickly build another balloon and get out as soon as possible.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Successful escape&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel decided to double the balloon's size to 4,000 cubic metres (140,000 cu ft) in volume, 20 metres (66 ft) in diameter, and 25 metres (82 ft) in height. They needed 1,250 square metres (13,500 sq ft) of taffeta, and purchased the material, in various colours and patterns, all over the country in order to escape suspicion. Wetzel sewed a third balloon, using over 6 kilometres (3.7 mi) of thread, and Strelzyk rebuilt everything else as before. In six weeks, they had prepared the 180-kilogram (400 lb) balloon and a payload of 550 kilograms (1,210 lb), including the gondola, equipment, and cargo (the two families). Confident in their calculations, they found the weather conditions right on 15 September, when a violent thunderstorm created the correct winds. The two families set off for the launch site in Strelzyk's replacement car (a Wartburg) and a moped. Arriving at 1:30 am, they needed just ten minutes to inflate the balloon and an additional three minutes to heat the air.[6]&lt;/p&gt;&lt;p&gt;Lifting off just after 2:00 am, the group failed to cut the tethers holding the gondola to the ground at the same time, tilting the balloon and sending the flame towards the fabric, which caught fire. After putting out the fire with an extinguisher brought along for just such an emergency, they climbed to 2,000 metres (6,600 ft) in nine minutes, drifting towards West Germany at 30 kilometres per hour (19 mph). The balloon flew for 28 minutes, with the temperature plummeting to −8 °C (18 °F) in the unsheltered gondola, which consisted solely of clothesline railing.&lt;/p&gt;&lt;p&gt;A design miscalculation resulted in the burner stovepipe being too long, causing the flame to be too high in the balloon, creating excessive pressure which caused the balloon to split. The air rushing out of the split extinguished the burner flame. Wetzel was able to re-light the flame with a match, and had to do so several more times before the group landed. At one point, they increased the flame to the maximum possible extent and rose to 2,500 metres (8,200 ft). They later learned they had been high enough to be detected, but not identified, on radar by West German air traffic controllers.[6] They had also been detected on the East German side by a night watchman at the district culture house in Bad Lobenstein. The report of an unidentified flying object heading toward the border caused guards to activate search lights, but the balloon was too high and out of reach of the lights.[10]&lt;/p&gt;&lt;p&gt;The tear in the balloon meant the group had to use the burner much more often, greatly limiting the distance it could travel. Wetzel later said he thought they could have travelled another 50 kilometres (31 mi) had the balloon remained intact. They made out the border crossing at Rudolphstein on the A9 and saw the search lights. When the propane ran out, they descended quickly, landing near the town of Naila, in the West German state of Bavaria and only 10 km (6 mi) from the border. The only injury was suffered by Wetzel, who broke his leg upon landing.[6] Various clues indicated to the families that the balloon had made it across the border. These included spotting red and yellow coloured lights, not common in East Germany,[3] and small farms, in contrast to the large state-run operations in the east. Another clue was modern farm equipment, unlike the older equipment used in East Germany.[11] Two Bavarian State Police officers saw the balloon's flickering light and headed to where they thought it would land. There they found Strelzyk and Wetzel, who first asked if they had made it to the West, although they noticed the police car was an Audi – another sign they were in West Germany. Upon learning they had, the escapees happily called for their families to join them.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Aftermath&lt;/head&gt;[edit]&lt;p&gt;East Germany immediately increased border security, closed all small airports close to the border, and ordered the planes kept farther inland.[6] Propane gas tanks became registered products, and large quantities of fabric suitable for balloon construction could no longer be purchased. Mail from East Germany to the two escaped families was prohibited.[12]&lt;/p&gt;&lt;p&gt;Erich Strelzyk learned of his brother's escape on the ZDF news and was arrested in his Potsdam apartment three hours after the landing. The arrest of family members was standard procedure to deter others from attempting escape. He was charged with "aiding and abetting escape", as were Strelzyk's sister Maria and her husband, who were sentenced to 2½ years. The three were eventually released with the help of Amnesty International.[12]&lt;/p&gt;&lt;p&gt;The families decided to initially settle in Naila where they had landed. Wetzel worked as an automobile mechanic and Strelzyk opened a TV repair shop in Bad Kissingen. Due to pressure from Stasi spies, the Strelzyks moved to Switzerland in 1985.[10] After German reunification in 1990, they returned to their old home in their hometown of Pößneck.[13] The Wetzels remained in Bavaria.[7]&lt;/p&gt;&lt;p&gt;West German weekly magazine Stern paid Strelzyk and Wetzel for exclusive rights to the story.[3]&lt;/p&gt;&lt;p&gt;The escape has been portrayed in two films: Night Crossing (1982) and Balloon (2018). The former, also called With the Wind to the West – the English translation of the German title – was an English-language film produced by Disney. The latter was a German-language production which "both families welcomed [Director] Herbig’s desire to, as he put it, 'make a German film for an international audience.'" The Strelzyks were reportedly "moved to tears" at the screening of Balloon at Rockefeller Center in New York City.[12] Herbig claimed in 2018 that both the Strelzyk and Wetzel families had been dissatisfied with the Disney film.[14]&lt;/p&gt;&lt;p&gt;Peter Strelzyk died in 2017 at age 74 after a long illness.[13]&lt;/p&gt;&lt;p&gt;In 2017, the balloon was put on permanent display at the Haus der Bayerischen Geschichte: Museum in Regensburg.[10]&lt;/p&gt;&lt;head rend="h2"&gt;Escapees&lt;/head&gt;[edit]&lt;p&gt;The family members included:[3]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Peter Strelzyk, aged 37&lt;/item&gt;&lt;item&gt;Doris Strelzyk&lt;/item&gt;&lt;item&gt;Frank Strelzyk, aged 15&lt;/item&gt;&lt;item&gt;Andreas Strelzyk, aged 11&lt;/item&gt;&lt;item&gt;Günter Wetzel, aged 24&lt;/item&gt;&lt;item&gt;Petra Wetzel&lt;/item&gt;&lt;item&gt;Peter Wetzel, aged 5&lt;/item&gt;&lt;item&gt;Andreas Wetzel, aged 2&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Media&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;The Disney film Night Crossing (1982) is an adaptation of the story[13]&lt;/item&gt;&lt;item&gt;Michael Herbig's film Balloon (2018) is a German-language adaptation of the story[15]&lt;/item&gt;&lt;item&gt;BBC program Outlook, "Fleeing Communism in a Hot Air Balloon"[16]&lt;/item&gt;&lt;item&gt;PBS Nova program, "History's Great Escapes" (2004)[17]&lt;/item&gt;&lt;item&gt;Doris Strelzyk, Peter Strelzyk, Gudrun Giese: Destiny Balloon Escape. Quadriga, Berlin 1999, ISBN 3-88679-330-3&lt;/item&gt;&lt;item&gt;Jürgen Petschull, With the Wind to the West. The Adventurous Flight from Germany to Germany. Goldmann, Munich 1980, ISBN 3-442-11501-9&lt;/item&gt;&lt;item&gt;Kristen Fulton (Author), Torben Kuhlmann (Illustrator), Flight for Freedom: The Wetzel Family’s Daring Escape from East Germany. March 3, 2020, ISBN 978-1452149608&lt;/item&gt;&lt;item&gt;The Netflix series White Rabbit Project, episode 2, "Jailbreak"&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b Wetzel, Günter. "Die Nacht der Flucht". Ballonflucht.de. Archived from the original on 19 September 2020. Retrieved 16 September 2019.&lt;/item&gt;&lt;item&gt;^ Hertle, Hans-Hermann; Nooke, Maria (2009). Die Todesopfer an der Berliner Mauer 1961–1989. Ein biographisches Handbuch. Ch. Links Verlag. ISBN 978-3-86153-517-1.&lt;/item&gt;&lt;item&gt;^ a b c d e Getler, Michael (28 September 1979). "Harrowing Flight From East Germany". The Washington Post. Archived from the original on 26 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ Snow, Philipp (16 September 2009). "Balloon escape from the GDR With hot air to freedom". Spiegel Online (in German). Archived from the original on 7 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c Simpson, Paul (2013). The Mammoth Book of Prison Breaks. Little, Brown Book Group. p. 216. ISBN 978-1-4721-0024-5. Archived from the original on 16 September 2023. Retrieved 1 April 2018.&lt;/item&gt;&lt;item&gt;^ a b c d e f g h i j k l m n o p q r s Dornberg, John (February 1980). "The Freedom Balloon". Popular Mechanics. pp. 100–103. Retrieved 22 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c d e Overbye, Stine (13 April 2017). "Fathers wanted to escape GDR in a hot air balloon". Historia (in Dutch). Archived from the original on 1 April 2018. Retrieved 30 March 2018.&lt;/item&gt;&lt;item&gt;^ Petschull, Jürgen (27 September 1979). "Das Himmelfahrtskommando" [High-flying mission] (PDF). Stern (in German). No. 40. p. 34. Archived from the original (PDF) on 12 July 2024 – via Museum Naila.&lt;/item&gt;&lt;item&gt;^ Souerbry, Rachel. "How Two Families Escaped East Germany In A Homemade Hot Air Balloon". ranker.com. Archived from the original on 1 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "Wetzel und Peter Strelzyk Ballonhülle der Strelzyks". museum.bayern (in German). Archived from the original on 8 April 2019. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ "East-West: The Great Balloon Escape". Time. 1 October 1979. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "The Balloon Escape of Peter Strelzyk". goethe-rutheneum.de (in German). Archived from the original on 11 February 2013. Retrieved 30 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "Man who fled East Germany in a homemade balloon and whose story was made into a film dies". The Express. 15 March 2017. Archived from the original on 1 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ Connolly, Kate (17 October 2018). "Film of daring balloon escape from East revives German identity debate". Archived from the original on 8 February 2021. Retrieved 10 May 2019.&lt;/item&gt;&lt;item&gt;^ Ballon at IMDb&lt;/item&gt;&lt;item&gt;^ "Fleeing Communism in a Hot Air Balloon". bbc. Archived from the original on 12 December 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ "Great Escapes". pbs.org. Archived from the original on 16 April 2019. Retrieved 16 April 2019.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;External links&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Escape by balloon by Günter Wetzel (participant website)&lt;/item&gt;&lt;item&gt;Video of balloon on museum display&lt;/item&gt;&lt;item&gt;BBC Outlook program&lt;/item&gt;&lt;item&gt;Photograph of Güenter Wetzel, Peter and Doris Strelzyk Archived 1 April 2018 at the Wayback Machine&lt;/item&gt;&lt;item&gt;Photograph of the actual balloon, inflated in 1985 at a festival in Hof, Bavaria&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/East_Germany_balloon_escape"/><published>2026-01-16T17:16:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46649142</id><title>STFU</title><updated>2026-01-17T12:19:07.705714+00:00</updated><content>&lt;doc fingerprint="a01e89ee7325e0d9"&gt;
  &lt;main&gt;
    &lt;p&gt;i was at bombay airport. some dude was watching reels on full volume and laughing loudly. asking nicely doesn't work anymore. me being me, didn't have the courage to speak up.&lt;/p&gt;
    &lt;p&gt;so i built a tiny app that plays back the same audio it hears, delayed by ~2 seconds. asked claude, it spat out a working version in one prompt. surprisingly WORKS.&lt;/p&gt;
    &lt;p&gt;discussion - https://x.com/the2ndfloorguy/status/2011734249871954188&lt;/p&gt;
    &lt;p&gt;something something auditory feedback loop something something cognitive dissonance. idk i'm not a neuroscientist. all i know is it makes people shut up and that's good enough for me.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;straight up honest - originally called this "make-it-stop" but then saw @TimDarcet also built similar and named it STFU. wayyyyy better name. so stole it. sorry not sorry.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;made with spite and web audio api. do whatever you want with it.&lt;/p&gt;
    &lt;p&gt;yo, meanwhile if you are new here, you might find my, other side projects kinda funny.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Pankajtanwarbanna/stfu"/><published>2026-01-16T17:32:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46650347</id><title>Reading across books with Claude Code</title><updated>2026-01-17T12:19:07.520917+00:00</updated><content>&lt;doc fingerprint="3f200f9427931e3e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Reading across books with Claude Code&lt;/head&gt;
    &lt;p&gt;Jan 4, 2026&lt;/p&gt;
    &lt;p&gt;LLMs are overused to summarise and underused to help us read deeper.&lt;/p&gt;
    &lt;p&gt;To explore how they can enrich rather than reduce, I set Claude Code up with tools to mine a library of 100 non-fiction books. It found sequences of excerpts connected by an interesting idea, or trails.&lt;/p&gt;
    &lt;p&gt;Here’s a part of one such trail, linking deception in the startup world to the social psychology of mass movements (I’m especially pleased by the jump from Jobs to Theranos):&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;The books were selected from Hacker News’ favourites, which I previously scraped and visualized.&lt;/p&gt;
    &lt;p&gt;Claude browses the books a chunk at a time. A chunk is a segment of roughly 500 words that aligns with paragraphs when possible. This length is a good balance between saving tokens and providing enough context for ideas to breathe.&lt;/p&gt;
    &lt;p&gt;Chunks are indexed by topic, and topics are themselves indexed for search. This makes it easy to look up all passages in the corpus that relate to, say, deception.&lt;/p&gt;
    &lt;p&gt;This works well when you know what to look for, but search alone can’t tell you which topics are present to begin with. There are over 100,000 extracted topics, far too many to be browsed directly. To support exploration, they are grouped into a hierarchical tree structure.&lt;/p&gt;
    &lt;p&gt;This yields around 1,000 top-level topics. They emerge from combining lower-level topics, and not all of them are equally useful:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Incidents that frustrated Ev Williams&lt;/item&gt;
      &lt;item&gt;Names beginning with “Da”&lt;/item&gt;
      &lt;item&gt;Events between 1971 &amp;amp; 1974&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, this Borgesian taxonomy is good enough for Claude to piece together what the books are about.&lt;/p&gt;
    &lt;p&gt;Claude uses the topic tree and the search via a few CLI tools.&lt;lb/&gt; They allow it to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Find all chunks associated with a topic similar to a query.&lt;/item&gt;
      &lt;item&gt;Find topics which occur in a window of chunks around a given topic.&lt;/item&gt;
      &lt;item&gt;Find topics that co-occur in multiple books.&lt;/item&gt;
      &lt;item&gt;Browse topics and chunks that are siblings in the topic tree.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To generate the trails, the agent works in stages.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;First, it scans the library and the existing trails, and proposes novel trail ideas. It mainly browses the topic tree to find unexplored areas and rarely reads full chunks in depth.&lt;/item&gt;
      &lt;item&gt;Then, it takes a specific idea and turns it into a trail. It receives seed topics from the previous stage and browses many chunks. It extracts excerpts, specific sequences of sentences, and decides on how best to order them to support an insight.&lt;/item&gt;
      &lt;item&gt;Finally, it adds highlights and edges between consecutive excerpts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What I learned&lt;/head&gt;
    &lt;head rend="h3"&gt;Claude Code is great for non-coding tasks&lt;/head&gt;
    &lt;p&gt;Even though I’ve been using Claude Code to develop for months, my first instinct for this project was to consider it as a traditional pipeline of several discrete stages. My initial attempt at this system consisted of multiple LLM modules with carefully hand-assembled contexts.&lt;/p&gt;
    &lt;p&gt;On a whim, I ran Claude with access to the debugging tools I’d been using and a minimal prompt: “find something interesting.” It immediately did a better job at pulling in what it needed than the pipeline I was trying to tune by hand, while requiring much less orchestration. It was a clear improvement to push as much of the work into the agent’s loop as possible.&lt;/p&gt;
    &lt;p&gt;I ended up using Claude as my main interface to the project.&lt;lb/&gt; Initially I did so because it inferred the sequence of CLI calls I wanted to run faster than I could recall them. Then, I used it to automate tasks which weren’t rigid enough to be scripted traditionally.&lt;/p&gt;
    &lt;p&gt;The latter opened up options that I wouldn’t have considered before. For example, I changed my mind on how short I wanted excerpts to be. I communicated my new preference to Claude, which then looked through all the existing trails and edited them as necessary, balancing the way the overall meaning of the trail changed. Previously, I would’ve likely considered all previous trails to be outdated and generated new ones, because the required edits would’ve been too nuanced to specify.&lt;/p&gt;
    &lt;p&gt;In general, agents have widened my ambitions.&lt;lb/&gt; By taking care of the boilerplate, I no longer shy away from the tedious parts. Revision is cheap, so I don’t need to plow ahead with suboptimal choices just because it’d be too costly to undo them. This, in turn, keeps up the momentum and lets me focus on the joyful, creative aspects of the work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ask the agent what it needs&lt;/head&gt;
    &lt;p&gt;My focus went from optimising prompts to implementing better tools for Claude to use, moving up a rung on the abstraction ladder.&lt;/p&gt;
    &lt;p&gt;My mental model of the AI component changed: from a function mapping input to output, to a coworker I was assisting. I spent my time thinking about the affordances that would make the workflow better, as if I were designing them for myself. That they were to be used by an agent was a mere detail.&lt;/p&gt;
    &lt;p&gt;This worked because the agent is now intelligent enough that the way it uses these tools overlaps with my own mental model. It is generally easy to empathise with it and predict what it will do.&lt;/p&gt;
    &lt;p&gt;Initially I watched Claude’s logs closely and tried to guess where it was lacking a certain ability. Then I realised I could simply ask it to provide feedback at the end and list the functionality it wished it had. Claude was excellent at proposing new commands and capabilities that would make the work more efficient.&lt;/p&gt;
    &lt;p&gt;Claude suggested improvements, which Claude implemented, so Claude could do the work better. At least I’m still needed to pay for the tokens — for now.&lt;/p&gt;
    &lt;head rend="h3"&gt;Novelty is a useful guide&lt;/head&gt;
    &lt;p&gt;It’s hard to quantify interestingness as an objective to optimise for.&lt;lb/&gt; Why Greatness Cannot Be Planned makes the case that chasing novelty is often a more fruitful approach. While its conclusions are debated, I’ve found this idea to be a good fit for this project.&lt;/p&gt;
    &lt;p&gt;As a sign of the times, this novelty search was implemented in two ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;By biasing the search algorithm towards under-explored topics and books.&lt;/item&gt;
      &lt;item&gt;By asking Claude nicely.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A topic’s novelty score was calculated as the mean distance from its embedding’s k nearest neighbors. A book’s novelty score is the average novelty of the unique topics that it contains. This value was used to rank search results, so that those which were both relevant and novel were more likely to be seen.&lt;/p&gt;
    &lt;p&gt;On a prompting level, Claude starts the ideation phase by looking at all the existing trails and is asked to avoid any conceptual overlap. This works fairly well, though it is often distracted by any topics related to secrecy, systems theory, or tacit knowledge.&lt;/p&gt;
    &lt;p&gt;It’s as if the very act of finding connections in a corpus summons the spirit of Umberto Eco and amps up the conspiratorial thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it’s implemented&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;EPUBs are parsed using &lt;code&gt;selectolax&lt;/code&gt;, which I picked over BeautifulSoup for its speed and simpler API.&lt;/item&gt;
      &lt;item&gt;Everything from the plain text to the topic tree is stored in SQLite. Embeddings are stored using &lt;code&gt;sqlite-vec&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The text is split into sentences using &lt;code&gt;wtpsplit&lt;/code&gt;(the&lt;code&gt;sat-6l-sm&lt;/code&gt;model). Those sentences are then grouped into chunks, trying to get up to 500 words without breaking up paragraphs.&lt;/item&gt;
      &lt;item&gt;I used &lt;code&gt;DSPy&lt;/code&gt;to call LLMs. It worked well for the structured data extraction and it was easy to switch out different models to experiment. I tried its prompt optimizers before I went full agentic, and their results were very promising.&lt;/item&gt;
      &lt;item&gt;I settled on Gemini 2.5 Flash Lite for topic extraction. The model gets passed a chunk and is asked to return 3-5 topics. It is also asked whether the chunk is useful, in order to filter out index entries, acknowledgements, orphan headers, etc. I was surprised at how stable these extracted topics were: similar chunks often shared some of the exact same topic labels. Processing 100 books used about 60M input tokens and ~£10 in total.&lt;/item&gt;
      &lt;item&gt;After a couple books got indexed, I shared the results with Claude Opus along with the original prompt and asked it to improve it. This is a half-baked single iteration of the type of prompt optimisation DSPy implements, and it worked rather well.&lt;/item&gt;
      &lt;item&gt;Topic pairs with a distance below a threshold get merged together. This takes care of near-duplicates such as “Startup founder”, “Startup founders”, and “Founder of startups”.&lt;/item&gt;
      &lt;item&gt;The CLI output uses a semi-XML format. In order to stimulate navigating, most output is nested with related content. For example, when searching for a topic, chunks are shown with the other topics they contain. This allows us to get a sense of what the chunk is about, as well as which other topics might be interesting. There’s probably more token-efficient formats, but I never hit the limit of the context window.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;&amp;lt;topics query="deception" count="1"&amp;gt;
  &amp;lt;topic id="47193" books="7" score="0.0173" label="Deception"&amp;gt;
    &amp;lt;chunk id="186" book="1"&amp;gt;
      &amp;lt;topic id="47192" label="Business deal"/&amp;gt;
      &amp;lt;topic id="47108" label="Internal conflict"/&amp;gt;
      &amp;lt;topic id="46623" label="Startup founders"/&amp;gt;
    &amp;lt;/chunk&amp;gt;
    &amp;lt;chunk id="1484" book="4"&amp;gt;
      &amp;lt;topic id="51835" label="Gawker Media"/&amp;gt;
      &amp;lt;topic id="53006" label="Legal Action"/&amp;gt;
      &amp;lt;topic id="52934" label="Maskirovka"/&amp;gt;
      &amp;lt;topic id="52181" label="Strategy"/&amp;gt;
    &amp;lt;/chunk&amp;gt;
    &amp;lt;chunk id="2913" book="9"&amp;gt;
      &amp;lt;topic id="59348" label="Blood testing system"/&amp;gt;
      &amp;lt;topic id="59329" label="Elizabeth Holmes"/&amp;gt;
      &amp;lt;topic id="59352" label="Investor demo"/&amp;gt;
      &amp;lt;topic id="59349" label="Theranos"/&amp;gt;
    &amp;lt;/chunk&amp;gt;
  &amp;lt;/topic&amp;gt;
&amp;lt;/topics&amp;gt;&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Topics are embedded using&lt;/p&gt;&lt;code&gt;google/embeddinggemma-300m&lt;/code&gt;and reranked using&lt;code&gt;BAAI/bge-reranker-v2-m3&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Many CLI tools require loading the embedding model and other expensive state. The first call transparently starts a separate server process which loads all these resources once and holds onto them for a while. Subsequent CLI calls use this server through Python’s&lt;/p&gt;&lt;code&gt;multiprocessing.connection&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The topic collection is turned into a graph (backed by&lt;/p&gt;&lt;code&gt;igraph&lt;/code&gt;) by adding edges based on the similarity of their embeddings and the point-wise mutual information of their co-occurrences.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The graph is turned into a tree by applying Leiden partitioning recursively until a minimum size is reached. I tried the Surprise quality function because it had no parameters to tweak, and found it to be good enough. Each group is labelled by Gemini based on all the topics that it contains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Excerpts are cleaned by Gemini to remove EPUB artifacts, parsing errors, headers, footnotes, etc. Doing this only for excerpts that are actually shown, instead of during pre-processing, saved a lot of tokens.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pieterma.es/syntopic-reading-claude/"/><published>2026-01-16T18:49:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46652617</id><title>Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation</title><updated>2026-01-17T12:19:07.155065+00:00</updated><content>&lt;doc fingerprint="ac035d9b403222ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Closing the Door on Net-NTLMv1: Releasing Rainbow Tables to Accelerate Protocol Deprecation&lt;/head&gt;
    &lt;head rend="h5"&gt;Mandiant&lt;/head&gt;
    &lt;p&gt;Written by: Nic Losby&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Mandiant is publicly releasing a comprehensive dataset of Net-NTLMv1 rainbow tables to underscore the urgency of migrating away from this outdated protocol. Despite Net-NTLMv1 being deprecated and known to be insecure for over two decades—with cryptanalysis dating back to 1999—Mandiant consultants continue to identify its use in active environments. This legacy protocol leaves organizations vulnerable to trivial credential theft, yet it remains prevalent due to inertia and a lack of demonstrated immediate risk.&lt;/p&gt;
    &lt;p&gt;By releasing these tables, Mandiant aims to lower the barrier for security professionals to demonstrate the insecurity of Net-NTLMv1. While tools to exploit this protocol have existed for years, they often required uploading sensitive data to third-party services or expensive hardware to brute-force keys. The release of this dataset allows defenders and researchers to recover keys in under 12 hours using consumer hardware costing less than $600 USD. This initiative highlights the amplified impact of combining Mandiant's frontline expertise with Google Cloud's resources to eliminate entire classes of attacks.&lt;/p&gt;
    &lt;p&gt;This post details the generation of the tables, provides access to the dataset for community use, and outlines critical remediation steps to disable Net-NTLMv1 and prevent authentication coercion attacks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background&lt;/head&gt;
    &lt;p&gt;Net-NTLMv1 has been widely known to be insecure since at least 2012, following presentations at DEFCON 20, with cryptanalysis of the underlying protocol dating back to at least 1999. On Aug. 30, 2016, Hashcat added support for cracking Data Encryption Standard (DES) keys using known plaintext, further democratizing the ability to attack this protocol. Rainbow tables are almost as old, with the initial paper on rainbow tables published in 2003 by Philippe Oechslin, citing an earlier iteration of a time-memory trade-off from 1980 by Martin Hellman.&lt;/p&gt;
    &lt;p&gt;Essentially, if an attacker can obtain a Net-NTLMv1 hash without Extended Session Security (ESS) for the known plaintext of &lt;code&gt;1122334455667788&lt;/code&gt;, a cryptographic attack, referred to as a known plaintext attack (KPA), can be applied. This guarantees recovery of the key material used. Since the key material is the password hash of the authenticating Active Directory (AD) object—user or computer—the attack results can quickly be used to compromise the object, often leading to privilege escalation.&lt;/p&gt;
    &lt;p&gt;A common chain attackers use is authentication coercion from a highly privileged object, such as a domain controller (DC). Recovering the password hash of the DC machine account allows for DCSync privileges to compromise any other account in AD.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dataset Release&lt;/head&gt;
    &lt;p&gt;The unsorted dataset can be downloaded using &lt;code&gt;gsutil -m cp -r gs://net-ntlmv1-tables/tables .&lt;/code&gt; or through the Google Cloud Research Dataset portal. &lt;/p&gt;
    &lt;p&gt;The SHA512 hashes of the tables can be checked by first downloading the checksums &lt;code&gt;gsutil -m cp gs://net-ntlmv1-tables/tables.sha512 .&lt;/code&gt; then checked by &lt;code&gt;sha512sum -c tables.sha512&lt;/code&gt;. The password cracking community has already created derivative work and is also hosting the ready to use tables.&lt;/p&gt;
    &lt;head rend="h3"&gt;Use of the Tables&lt;/head&gt;
    &lt;p&gt;Once a Net-NTLMv1 hash has been obtained, the tables can be used with historical or modern reinventions of rainbow table searching software such as rainbowcrack (rcrack), or RainbowCrack-NG on central processing units (CPUs) or a fork of rainbowcrackalack on graphics processing units (GPUs). The Net-NTLMv1 hash needs to be preprocessed to the DES components using ntlmv1-multi as shown in the next section.&lt;/p&gt;
    &lt;head rend="h3"&gt;Obtaining a Net-NTLMv1 Hash&lt;/head&gt;
    &lt;p&gt;Most attackers will use Responder with the &lt;code&gt;--lm&lt;/code&gt; and &lt;code&gt;--disable-ess&lt;/code&gt; flags and set the authentication to a static value of &lt;code&gt;1122334455667788&lt;/code&gt; to only allow for connections with Net-NTLMv1 as a possibility. Attackers can then wait for incoming connections or coerce authentication using a tool such as PetitPotam or DFSCoerce to generate incoming connections from DCs or lower privilege hosts that are useful for objective completion. Responses can be cracked to retrieve password hashes of either users or computer machine accounts. A sample workflow for an attacker is shown below in Figure 1, Figure 2, and Figure 3.&lt;/p&gt;
    &lt;p&gt;Figure 1: DFSCoerce against a DC&lt;/p&gt;
    &lt;p&gt;Figure 2: Net-NTLMv1 hash obtained for DC machine account&lt;/p&gt;
    &lt;p&gt;Figure 3: Parse Net-NTLMv1 hash to DES parts&lt;/p&gt;
    &lt;p&gt;Figure 4 illustrates the processing of the Net-NTLMv1 hash to the DES ciphertexts.&lt;/p&gt;
    &lt;p&gt;Figure 4: Net-NTLMv1 hash to DES ciphertexts&lt;/p&gt;
    &lt;p&gt;An attacker then takes the split-out ciphertexts to crack the keys used based on the known plaintext of &lt;code&gt;1122334455667788&lt;/code&gt; with the steps of loading the tables shown in Figure 5 and cracking results in Figure 6 and Figure 7.&lt;/p&gt;
    &lt;p&gt;Figure 5: Loading DES components for cracking&lt;/p&gt;
    &lt;p&gt;Figure 6: First hash cracked&lt;/p&gt;
    &lt;p&gt;Figure 7: Second hash cracked and run statistics&lt;/p&gt;
    &lt;p&gt;An attacker can then calculate the last remaining key with ntlmv1-multi once again, or look it up with twobytes, to recreate the full NT hash for the DC account with the last key part shown in Figure 8.&lt;/p&gt;
    &lt;p&gt;Figure 8: Calculate remaining key&lt;/p&gt;
    &lt;p&gt;The result can be checked with hashcat's NT hash shucking mode, &lt;code&gt;-m 27000&lt;/code&gt;, as shown in Figure 9.&lt;/p&gt;
    &lt;p&gt;Figure 9: Keys checked with hash shucking&lt;/p&gt;
    &lt;p&gt;An attacker can then use the hash to perform a DCSync attack targeting a DC and authenticating as the now compromised machine account. The attack flow uses secretsdump.py from the Impacket toolsuite and is shown in Figure 10.&lt;/p&gt;
    &lt;p&gt;Figure 10: DCSync attack performed&lt;/p&gt;
    &lt;head rend="h3"&gt;Remediation&lt;/head&gt;
    &lt;p&gt;Organizations should immediately disable the use of Net-NTLMv1.&lt;/p&gt;
    &lt;head rend="h4"&gt;Local Computer Policy&lt;/head&gt;
    &lt;p&gt;"Local Security Settings" &amp;gt; "Local Policies" &amp;gt; "Security Options" &amp;gt; “Network security: LAN Manager authentication level" &amp;gt; "Send NTLMv2 response only".&lt;/p&gt;
    &lt;head rend="h4"&gt;Group Policy&lt;/head&gt;
    &lt;p&gt;"Computer Configuration" &amp;gt; "Policies" &amp;gt; "Windows Settings" &amp;gt; "Security Settings" &amp;gt; "Local Policies" &amp;gt; "Security Options" &amp;gt; "Network Security: LAN Manager authentication level" &amp;gt; "Send NTLMv2 response only"&lt;/p&gt;
    &lt;p&gt;As these are local to the computer configurations, attackers can and have set the configuration to a vulnerable state to then fix the configuration after their attacks have completed with local administrative access. Monitoring and alerting of when and where Net-NTLMv1 is used is needed in addition to catching these edge cases.&lt;/p&gt;
    &lt;p&gt;Filter Event Logs for Event ID 4624: "An Account was successfully logged on." &amp;gt; "Detailed Authentication Information" &amp;gt; "Authentication Package" &amp;gt; "Package Name (NTLM only)", if "LM" or "NTLMv1" is the value of this attribute, LAN Manager or Net-NTLMv1 was used.&lt;/p&gt;
    &lt;head rend="h3"&gt;Related Reading&lt;/head&gt;
    &lt;p&gt;This project was inspired by and referenced the following research published to blogs, social media, and code repositories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thank you to everyone who helped make this blog post possible, including but not limited to Chris King and Max Gruenberg.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cloud.google.com/blog/topics/threat-intelligence/net-ntlmv1-deprecation-rainbow-tables"/><published>2026-01-16T21:42:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46653388</id><title>The Dilbert Afterlife</title><updated>2026-01-17T12:19:06.751762+00:00</updated><content>&lt;doc fingerprint="f72093552ae7d739"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Dilbert Afterlife&lt;/head&gt;
    &lt;head rend="h3"&gt;Sixty-eight years of highly defective people&lt;/head&gt;
    &lt;p&gt;Thanks to everyone who sent in condolences on my recent death from prostate cancer at age 68, but that was Scott Adams. I (Scott Alexander) am still alive1.&lt;/p&gt;
    &lt;p&gt;Still, the condolences are appreciated. Scott Adams was a surprisingly big part of my life. I may be the only person to have read every Dilbert book before graduating elementary school. For some reason, 10-year-old-Scott found Adams’ stories of time-wasting meetings and pointy-haired bosses hilarious. No doubt some of the attraction came from a more-than-passing resemblance between Dilbert’s nameless corporation and the California public school system. We’re all inmates in prisons with different names.&lt;/p&gt;
    &lt;p&gt;But it would be insufficiently ambitious to stop there. Adams’ comics were about the nerd experience. About being cleverer than everyone else, not just in the sense of being high IQ, but in the sense of being the only sane man in a crazy world where everyone else spends their days listening to overpaid consultants drone on about mission statements instead of doing anything useful. There’s an arc in Dilbert where the boss disappears for a few weeks and the engineers get to manage their own time. Productivity shoots up. Morale soars. They invent warp drives and time machines. Then the boss returns, and they’re back to being chronically behind schedule and over budget. This is the nerd outlook in a nutshell: if I ran the circus, there’d be some changes around here.&lt;/p&gt;
    &lt;p&gt;Yet the other half of the nerd experience is: for some reason this never works. Dilbert and his brilliant co-workers are stuck watching from their cubicles while their idiot boss racks in bonuses and accolades. If humor, like religion, is an opiate of the masses, then Adams is masterfully unsubtle about what type of wound his art is trying to numb.&lt;/p&gt;
    &lt;p&gt;This is the basic engine of Dilbert: everyone is rewarded in exact inverse proportion to their virtue. Dilbert and Alice are brilliant and hard-working, so they get crumbs. Wally is brilliant but lazy, so he at least enjoys a fool’s paradise of endless coffee and donuts while his co-workers clean up his messes. The P.H.B. is neither smart nor industrious, so he is forever on top, reaping the rewards of everyone else’s toil. Dogbert, an inveterate scammer with a passing resemblance to various trickster deities, makes out best of all.&lt;/p&gt;
    &lt;p&gt;The repressed object at the bottom of the nerd subconscious, the thing too scary to view except through humor, is that you’re smarter than everyone else, but for some reason it isn’t working. Somehow all that stuff about small talk and sportsball and drinking makes them stronger than you. No equation can tell you why. Your best-laid plans turn to dust at a single glint of Chad’s perfectly-white teeth.&lt;/p&gt;
    &lt;p&gt;Lesser lights may distance themselves from their art, but Adams radiated contempt for such surrender. He lived his whole life as a series of Dilbert strips. Gather them into one of his signature compendia, and the title would be Dilbert Achieves Self Awareness And Realizes That If He’s So Smart Then He Ought To Be Able To Become The Pointy-Haired Boss, Devotes His Whole Life To This Effort, Achieves About 50% Success, Ends Up In An Uncanny Valley Where He Has Neither The Virtues Of The Honest Engineer Nor Truly Those Of The Slick Consultant, Then Dies Of Cancer Right When His Character Arc Starts To Get Interesting.&lt;/p&gt;
    &lt;p&gt;If your reaction is “I would absolutely buy that book”, then keep reading, but expect some detours.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fugitive From The Cubicle Police&lt;/head&gt;
    &lt;p&gt;The niche that became Dilbert opened when Garfield first said “I hate Mondays”. The quote became a popular sensation, inspiring t-shirts, coffee mugs, and even a hit single. But (as I’m hardly the first to point out) why should Garfield hate Mondays? He’s a cat! He doesn’t have to work!&lt;/p&gt;
    &lt;p&gt;In the 80s and 90s, saying that you hated your job was considered the height of humor. Drew Carey: “Oh, you hate your job? There’s a support group for that. It’s called everybody, and they meet at the bar.”&lt;/p&gt;
    &lt;p&gt;This was merely the career subregion of the supercontinent of Boomer self-deprecating jokes, whose other prominences included “I overeat”, “My marriage is on the rocks”, “I have an alcohol problem”, and “My mental health is poor”.&lt;/p&gt;
    &lt;p&gt;Arguably this had something to do with the Bohemian turn, the reaction against the forced cheer of the 1950s middle-class establishment of company men who gave their all to faceless corporations and then dropped dead of heart attacks at 60. You could be that guy, proudly boasting to your date about how you traded your second-to-last patent artery to complete a spreadsheet that raised shareholder value 14%. Or you could be the guy who says “Oh yeah, I have a day job working for the Man, but fuck the rat race, my true passion is white water rafting”. When your father came home every day looking haggard and worn out but still praising his boss because “you’ve got to respect the company or they won’t take care of you”, being able to say “I hate Mondays” must have felt liberating, like the mantra of a free man2.&lt;/p&gt;
    &lt;p&gt;This was the world of Dilbert’s rise. You’d put a Dilbert comic on your cubicle wall, and feel like you’d gotten away with something. If you were really clever, you’d put the Dilbert comic where Dilbert gets in trouble for putting a comic on his cubicle wall on your cubicle wall, and dare them to move against you.&lt;/p&gt;
    &lt;p&gt;(again, I was ten at the time. I only know about this because Scott Adams would start each of his book collections with an essay, and sometimes he would talk about letters he got from fans, and many of them would have stories like these.)&lt;/p&gt;
    &lt;p&gt;But t-shirts saying “Working Hard . . . Or Hardly Working?” no longer hit as hard as they once did. Contra the usual story, Millennials are too earnest to tolerate the pleasant contradiction of saying they hate their job and then going in every day with a smile. They either have to genuinely hate their job - become some kind of dirtbag communist labor activist - or at least pretend to love it. The worm turns, all that is cringe becomes based once more and vice versa. Imagine that guy boasting to his date again. One says: “Oh yeah, I grudgingly clock in every day to give my eight hours to the rat race, but trust me, I’m secretly hating myself the whole time”? The other: “I work for a boutique solar energy startup that’s ending climate change - saving the environment is my passion!” Zoomers are worse still: not even the fig leaf of social good, just pure hustle.&lt;/p&gt;
    &lt;p&gt;Silicon Valley, where hustle culture has reached its apogee, has an additional consideration: why don’t you found a startup? If you’re so much smarter than your boss, why not compete against him directly? Scott Adams based Dilbert on his career at Pacific Bell in the 80s. Can you imagine quitting Pacific Bell in the 80s to, uh, found your own Pacific Bell? To go to Michael Milken or whoever was investing back then, and say “Excuse me, may I have $10 billion to create my own version of Pacific Bell, only better?” But if someone were to try to be Dilbert today – to say, earnestly, “I hate my job because I am smarter than my boss and could do it better than him,” that would be the obvious next question, the same way “I am better at picking stocks than Wall Street” ought to be followed up with “Then why don’t you invest?”&lt;/p&gt;
    &lt;p&gt;Above, I described “the nerd experience” of “being smarter than everyone else, not just in the sense of being high IQ, but in the sense of being the only sane man in a crazy world where everyone else spends their days listening to overpaid consultants drone on about mission statements instead of doing anything useful.” You nodded along, because you knew the only possible conclusion to the arc suggested by that sentence was to tear it down, to launch a tirade about how that nerd is naive and narcissistic and probably somehow also a racist. In the year of our Lord 2026, of course that’s where I’m going.&lt;/p&gt;
    &lt;p&gt;Dilbert is a relic of a simpler time, when the trope could be played straight. But it’s also an artifact of the transition, maybe even a driver of it. Scott Adams appreciated these considerations earlier and more acutely than anyone else. And they drove him nuts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stick To Drawing Comics, Monkey Brain&lt;/head&gt;
    &lt;p&gt;Adams knew, deep in his bones, that he was cleverer than other people. God always punishes this impulse, especially in nerds. His usual strategy is straightforward enough: let them reach the advanced physics classes, where there will always be someone smarter than them, then beat them on the head with their own intellectual inferiority so many times that they cry uncle and admit they’re nothing special.&lt;/p&gt;
    &lt;p&gt;For Adams, God took a more creative and – dare I say, crueler – route. He created him only-slightly-above-average at everything except for a world-historical, Mozart-tier, absolutely Leonardo-level skill at making silly comics about hating work.&lt;/p&gt;
    &lt;p&gt;Scott Adams never forgave this. Too self-aware to deny it, too narcissistic to accept it, he spent his life searching for a loophole. You can read his frustration in his book titles: How To Fail At Almost Everything And Still Win Big. Trapped In A Dilbert World. Stick To Drawing Comics, Monkey Brain. Still, he refused to stick to comics. For a moment in the late-90s, with books like The Dilbert Principle and The Dilbert Future, he seemed on his way to be becoming a semi-serious business intellectual. He never quite made it, maybe because the Dilbert Principle wasn’t really what managers and consultants wanted to hear:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I wrote The Dilbert Principle around the concept that in many cases the least competent, least smart people are promoted, simply because they’re the ones you don't want doing actual work. You want them ordering the doughnuts and yelling at people for not doing their assignments—you know, the easy work. Your heart surgeons and your computer programmers—your smart people—aren't in management.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Okay, “I am cleverer than everyone else”, got it. His next venture (c. 1999) was the Dilberito, an attempt to revolutionize food via a Dilbert-themed burrito with the full Recommended Daily Allowance of twenty-three vitamins. I swear I am not making this up. A contemporaneous NYT review said it “could have been designed only by a food technologist or by someone who eats lunch without much thought to taste”. The Onion, in its twenty year retrospective for the doomed comestible, called it a frustrated groping towards meal replacements like Soylent or Huel, long before the existence of a culture nerdy enough to support them. Adams himself, looking back from several years’ distance, was even more scathing: “the mineral fortification was hard to disguise, and because of the veggie and legume content, three bites of the Dilberito made you fart so hard your intestines formed a tail.”&lt;/p&gt;
    &lt;p&gt;His second foray into the culinary world was a local restaurant called Stacey’s. The New York Times does a pitch-perfect job covering the results. Their article starts:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is yet another story about a clueless but obtrusive boss — the kind of meddlesome manager you might laugh at in the panels of “Dilbert,” the daily comic strip.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;…and continues through a description of Adams making every possible rookie mistake. As the restaurant does worse and worse, Adams becomes more and more convinced that he has to figure out some clever lifehack that will turn things around and revolutionize restaurants. First he comes up with a theory that light is the key to restauranting, and spends ages fiddling with the windows. When this fails, he devolves into an unmistakable sign of desperation - asking blog commenters for advice:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;He also turned to Dilbert fans for suggestions on how to use the party room, in a posting on his blog titled “Oh Great Blog Brain.” The Dilbert faithful responded with more than 1,300 comments, mixing interesting ideas (interactive murder-mystery theater) with unlikely mischief (nude volleyball tournaments). Mr. Adams asked his employees to read the comments and is now slowly trying some of them.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But what makes this article truly perfect - I can’t believe it didn’t get a Pulitzer - is that it’s not some kind of hostile ambush profile. Adams is totally self-aware. He also finds the whole situation hilarious! Everyone involved is in on the joke! The waiters find it hilarious! After every workday, Adams and the waiters get together and laugh long into the night together about how bad a boss Adams is!&lt;/p&gt;
    &lt;p&gt;There’s a running joke about how if you see a business that loses millions yearly, it’s probably run by some banker’s wife who’s getting subsidized to feel good about herself and pretend she has a high-powered job. I think this is approximately what was going on with Stacey’s. Adams made enough money off Dilbert that he could indulge his fantasies of being something more than “the Dilbert guy”. For a moment, he could think of himself as a temporarily-embarrassed businessman, rather than just a fantastically successful humorist. The same probably explains his forays into television (“Dilbert: The Animated Series”), non-Dilbert comics (“Plop: The Hairless Elbonian”), and technology (”WhenHub”, his site offering “live chats with subject-matter experts”, which was shelved after he awkwardly tried to build publicity by suggesting that mass shooting witnesses could profit by using his site to tell their stories.)&lt;/p&gt;
    &lt;p&gt;Adams and Elon Musk occasionally talked about each other - usually to defend one another against media criticism of their respective racist rants - but I don’t know if they ever met. I wonder what it would have been like if they did. I imagine them coming together at some Bay Area house party on copious amounts of LSD or MDMA. One, the world’s greatest comic writer, who more than anything else wanted to succeed in business. The other, the world’s greatest businessman, who more than anything else wanted people to think that he’s funny. Scott Adams couldn’t stop frittering his talent and fortune on doomed attempts to be taken seriously. But someday Elon Musk will buy America for $100 trillion, tell the UN that he’s renaming it “the United States of 420-69”, and the assembled ambassadors will be as silent as the grave. Are there psychic gains from trade to be had between two such people?&lt;/p&gt;
    &lt;p&gt;Michael Jordan was the world’s best basketball player, and insisted on testing himself against baseball, where he failed. Herbert Hoover was one of the world’s best businessmen, and insisted on testing himself against politics, where he crashed and burned. We’re all inmates in prisons of different names. Most of us accept it and get on with our lives. Adams couldn’t stop rattling the bars.&lt;/p&gt;
    &lt;head rend="h2"&gt;I’m No Scientist, But I Think Feng Shui Is Part Of The Answer&lt;/head&gt;
    &lt;p&gt;Having failed his forays into business, Adams turned to religion. Not in the sense of seeking consolation through God’s love. In the sense of trying to show how clever he was by figuring out the true nature of the Divine&lt;/p&gt;
    &lt;p&gt;The result was God’s Debris. This is a breathtakingly bad book. On some level, Adams (of course) seemed to realize this, but (of course) his self-awareness only made things worse. In the second-worst introduction to a work of spiritual wisdom I’ve ever read (Gurdjieff keeps first place by a hair), he explains that this is JUST A THOUGHT EXPERIMENT and IF YOU TAKE IT SERIOUSLY, YOU FAIL. But also, it really makes you think, and it’s going to blow your mind, and you’ll spend the rest of your life secretly wondering whether it was true, but it won’t be, because IT’S JUST A THOUGHT EXPERIMENT, and IF YOU TAKE IT SERIOUSLY, YOU FAIL. Later, in a Bloomberg interview, he would say that this book - and not Dilbert - would be his “ultimate legacy” to the world. But remember, IT’S JUST A THOUGHT EXPERIMENT, and IF YOU TAKE IT SERIOUSLY YOU FAIL.&lt;/p&gt;
    &lt;p&gt;I read it for the first time while researching this essay. The frame story is that a delivery boy gives a package to the wisest man in the universe, who invites him to stay a while and discuss philosophy (REMEMBER, IT’S JUST A WORK OF FICTION! THESE ARE ONLY CHARACTERS!) Their discussion is one-quarter classic philosophical problems that seemed deep when you were nineteen, presented with no reference to any previous work:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“There has to be a God,” I said. “Otherwise, none of us would be here.” It wasn’t much of a reason, but I figured he didn’t need more.&lt;/p&gt;
      &lt;p&gt;“Do you believe God is omnipotent and that people have free will?” he asked.&lt;/p&gt;
      &lt;p&gt;“That’s standard stuff for God. So, yeah.”&lt;/p&gt;
      &lt;p&gt;“If God is omnipotent, wouldn’t he know the future?”&lt;/p&gt;
      &lt;p&gt;“Sure.”&lt;/p&gt;
      &lt;p&gt;“If God knows what the future holds, then all our choices are already made, aren’t they? Free will must be an illusion.”&lt;/p&gt;
      &lt;p&gt;He was clever, but I wasn’t going to fall for that trap. “God lets us determine the future ourselves, using our free will,” I explained.&lt;/p&gt;
      &lt;p&gt;“Then you believe God doesn’t know the future?”&lt;/p&gt;
      &lt;p&gt;“I guess not,” I admitted. “But he must prefer not knowing.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There is an ongoing meta-discussion among philosophy discussers of how acceptable it is to propose your own answers to the great questions without having fully mastered previous scholarship. On the one hand, philosophy is one of the most fundamental human activities, gating it behind the near-impossible task of having read every previous philosopher is elitist and gives self-appointed guardians of scholarship a permanent heckler’s veto on any new ideas, and it can create a culture so obsessed with citing every possible influence that eventually the part where you have an opinion withers away and philosophy becomes a meaningless ritual of presenting citations without conclusion. On the other hand, this book.&lt;/p&gt;
    &lt;p&gt;Another quarter is philosophical questions which did not seem deep, even when you were nineteen, and which nobody has ever done work on, because nobody except Scott Adams ever even thought they were worth considering:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;What makes a holy land holy?” he asked.&lt;/p&gt;
      &lt;p&gt;“Well, usually it’s because some important religious event took place there.”&lt;/p&gt;
      &lt;p&gt;“What does it mean to say that something took place in a particular location when we know that the earth is constantly in motion, rotating on its axis and orbiting the sun? And we’re in a moving galaxy that is part of an expanding universe. Even if you had a spaceship and could fly anywhere, you can never return to the location of a past event. There would be no equivalent of the past location because location depends on your distance from other objects, and all objects in the universe would have moved considerably by then.”&lt;/p&gt;
      &lt;p&gt;“I see your point, but on Earth the holy places keep their relationship to other things on Earth, and those things don’t move much,” I said.&lt;/p&gt;
      &lt;p&gt;“Let’s say you dug up all the dirt and rocks and vegetation of a holy place and moved it someplace else, leaving nothing but a hole that is one mile deep in the original location. Would the holy land now be the new location where you put the dirt and rocks and vegetation, or the old location with the hole?”&lt;/p&gt;
      &lt;p&gt;“I think both would be considered holy,” I said, hedging my bets.&lt;/p&gt;
      &lt;p&gt;“Suppose you took only the very top layer of soil and vegetation from the holy place, the newer stuff that blew in or grew after the religious event occurred thousands of years ago. Would the place you dumped the topsoil and vegetation be holy?”&lt;/p&gt;
      &lt;p&gt;“That’s a little trickier,” I said. “I’ll say the new location isn’t holy because the topsoil that you moved there isn’t itself holy, it was only in contact with holy land. If holy land could turn anything that touched it into more holy land, then the whole planet would be holy.”&lt;/p&gt;
      &lt;p&gt;The old man smiled. “The concept of location is a useful delusion when applied to real estate ownership, or when giving someone directions to the store. But when it is viewed through the eyes of an omnipotent God, the concept of location is absurd. While we speak, nations are arming themselves to fight for control of lands they consider holy. They are trapped in the delusion that locations are real things, not just fictions of the mind. Many will die.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Another quarter of the discussion is the most pusillanimous possible subjectivism, as if Robert Anton Wilson and the 2004 film What the #$*! Do We Know!? had a kid, then strangled it at birth until it came out brain damaged. We get passages like these:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“I am saying that UFOs, reincarnation, and God are all equal in terms of their reality.”&lt;/p&gt;
      &lt;p&gt;“Do you mean equally real or equally imaginary?”&lt;/p&gt;
      &lt;p&gt;“Your question reveals your bias for a binary world where everything is either real or imaginary. That distinction lies in your perceptions, not in the universe. Your inability to see other possibilities and your lack of vocabulary are your brain’s limits, not the universe’s.”&lt;/p&gt;
      &lt;p&gt;“There has to be a difference between real and imagined things,” I countered. “My truck is real. The Easter Bunny is imagined. Those are different.”&lt;/p&gt;
      &lt;p&gt;“As you sit here, your truck exists for you only in your memory, a place in your mind. The Easter Bunny lives in the same place. They are equal.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I remember the late ‘90s and early ‘00s; I was (regrettably) there. For some reason, all this stuff was considered the height of wisdom back then. The actual Buddhist classics were hard to access, but everyone assumed that Buddhists were wise and they probably said, you know, something like this. If you said stuff like this, you could be wise too.&lt;/p&gt;
    &lt;p&gt;The final quarter of the book is a shockingly original take on the Lurianic kabbalah. I‘m not pleased to report this, and Adams likely would have been very surprised to learn it. Still, the resemblance is unmistakable. The wisest man in the world, charged with answering all of the philosophical problems that bothered you when you were nineteen, tells the following story: if God exists, He must be perfect. Therefore, the only thing he lacks is nonexistence. Therefore, in order to fill that lack, He must destroy himself in order to create the universe. The universe is composed of the fragments of that destruction - the titular God’s Debris. Its point is to reassemble itself into God. Partially-reassembled-God is not yet fully conscious, but there is some sort of instinct within His fragments - ie within the universe - that is motivated to help orchestrate the self-reassembly, and it is this instinct which causes anti-entropic processes like evolution. Good things are good because they aid in the reassembly of God; bad things are bad because they hinder it.&lt;/p&gt;
    &lt;p&gt;Adams’ version adds several innovations to this basic story. Whatever parts of God aren’t involved in physical matter have become the laws of probability; this explains the otherwise inexplicable evolutionary coincidences that created humankind. There’s something about how gravity is produced by some sort of interference between different divine corpuscules - Adams admits that Einstein probably also had useful things to say about gravity, but probably his own version amounts to the same thing, and it’s easier to understand, and that makes it better (IT’S JUST A THOUGHT EXPERIMENT! IF YOU TAKE IT SERIOUSLY, YOU FAIL.) But my favorite part is the augmentation of Luria with Nick Land: the final (or one of the final) steps in the divine reassembly is the creation of the Internet, aka “God’s nervous system”, which will connect everything to everything else and give the whole system awareness of its divine purpose.&lt;/p&gt;
    &lt;p&gt;I’m honestly impressed that a Gentile worked all of this out on his own. Adams completes the performance by reinventing Kegan levels (this time I’m agnostic as to whether it’s convergent evolution or simple plagiarism), although characteristically it is in the most annoying way possible:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[The wise man] described what he called the five levels of awareness and said that all humans experience the first level of awareness at birth. That is when you first become aware that you exist.&lt;/p&gt;
      &lt;p&gt;In the second level of awareness you understand that other people exist. You believe most of what you are told by authority figures. You accept the belief system in which you are raised.&lt;/p&gt;
      &lt;p&gt;At the third level of awareness you recognize that humans are often wrong about the things they believe. You feel that you might be wrong about some of your own beliefs but you don’t know which ones. Despite your doubts, you still find comfort in your beliefs.&lt;/p&gt;
      &lt;p&gt;The fourth level is skepticism. You believe the scientific method is the best measure of what is true and you believe you have a good working grasp of truth, thanks to science, your logic, and your senses. You are arrogant when it comes to dealing with people in levels two and three.&lt;/p&gt;
      &lt;p&gt;The fifth level of awareness is the Avatar. The Avatar understands that the mind is an illusion generator, not a window to reality. The Avatar recognizes science as a belief system, albeit a useful one. An Avatar is aware of God’s power as expressed in probability and the inevitable recombination of God consciousness.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think going through every David Chapman essay and replacing the word “metarationality” with “THE AVATAR” would actually be very refreshing.&lt;/p&gt;
    &lt;p&gt;What are we to make of all of this?&lt;/p&gt;
    &lt;p&gt;Nothing is more American than inventing weird cringe fusions of religion and atheism where you say that God doesn’t exist as (gestures upward) some Big Man In The Sky the way those people believe, but also, there totally is a God, in some complicated sense which only I understand. When Thomas Jefferson cut all the passages with miracles out of his Bible, he was already standing on the shoulders of generations of Unitarians, Quakers, and Latitudinarians.&lt;/p&gt;
    &lt;p&gt;This was augmented by the vagaries of nerd culture’s intersection with the sci-fi fandom. The same people who wanted to read about spaceships and ray guns also wanted to read about psionics and Atlantis, so the smart sci-fi nerd consensus morphed into something like “probably all that unexplained stuff is real, but has a scientific explanation”. Telepathy is made up of quantum particles, or whatever (I talk about this more in my article on the Shaver Mystery). It became a nerd rite of passage to come up with your own theory that reconciled the spiritual and the material in the most creative way possible.&lt;/p&gt;
    &lt;p&gt;And the Nineties (God’s Debris was published in 2001) were a special time. The decade began with the peak of Wicca and neopaganism. Contra current ideological fault lines, where these tendencies bring up images of Etsy witches, they previously dominated nerd circles, including male nerds, techie nerds, and right-wing nerds (did you know Eric S. Raymond is neopagan?) By decade’s end, the cleverest (ie most annoying) nerds were switching to New Atheism; throughout, smaller groups were exploring Discordianism, chaos magick, and the Subgenius. The common thread was that Christianity had lost its hegemonic status, part of being a clever nerd was patting yourself on the back for having seen through it, but exactly what would replace it was still uncertain, and there was still enough piety in the water supply that people were uncomfortable forgetting about religion entirely. You either had to make a very conscious, marked choice to stop believing (New Atheism), or try your hand at the task of inventing some kind of softer middle ground (neopaganism, Eastern religion, various cults, whatever this book was supposed to be).&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s Obvious You Won’t Survive By Your Wits Alone&lt;/head&gt;
    &lt;p&gt;Adams spent his life obsessed with self-help. Even more than a businessman or a prophet, he wanted to be a self-help guru. Of course he did. His particular package of woo - a combination of hypnosis, persuasion hacks, and social skills advice - unified the two great motifs of his life.&lt;/p&gt;
    &lt;p&gt;Thesis: I am cleverer than everyone else.&lt;/p&gt;
    &lt;p&gt;Antithesis: I always lose to the Pointy-Haired Boss.&lt;/p&gt;
    &lt;p&gt;Synthesis: I was trying to be rational. But most people are irrational sheep; they can be directed only by charismatic manipulators who play on their biases, not by rational persuasion. But now I’m back to being cleverer than everyone else, because I noticed this. Also, I should become a charismatic manipulator.&lt;/p&gt;
    &lt;p&gt;I phrased this in a maximally hostile way, but it’s not wrong. And Adams started off strong. He read Dale Carnegie’s How To Win Friends And Influence People, widely agreed to be the classic book on social skills.&lt;/p&gt;
    &lt;p&gt;Then, in search of even stronger persuasion techniques, he turned to hypnosis. This has a bad reputation, but I basically buy that something is there. Psychiatry has legends of psychotherapist-hypnotists who achieved amazing things, and there’s a plausible scientific story for why it might work. So when Adams claimed to be a master hypnotist, I was originally willing to give him the benefit of the doubt.&lt;/p&gt;
    &lt;p&gt;That lasted until I read The Religion War3, Adams’ sequel to God’s Debris. In the intro, which may be literally the most annoying passage ever written in all two million years of human history, he discusses the reception of the original book:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is a sequel to my book God’s Debris, a story about a deliveryman who chances upon the smartest person in the world and learns the secrets of reality. I subtitled that book A Thought Experiment and used a variety of hypnosis techniques in an attempt to produce a feeling of euphoric enlightenment in the reader similar to what the main character would feel while discovering the (fictionally) true nature of reality. Reactions to the book were all over the map. About half of the people who e-mailed me said they felt various flavors of euphoria, expanded awareness, connectedness, and other weird sensations that defied description. A surprising number of people reported reading the entire book twice in one day. So I know something was happening.&lt;/p&gt;
      &lt;p&gt;Other people wrote angry letters and scathing reviews, pointing out the logical and factual flaws in the book. It is full of flaws, and much of the science is made up, as it states in the introduction. I explained that the reader is supposed to be looking for flaws. That’s what makes the experiment work. You might think this group of readers skipped the introduction and missed the stated point of the book, but I suspect that something else is going on. People get a kind of cognitive dissonance (brain cramp) when their worldview is disturbed. It’s fun to watch.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I previously felt bad for writing this essay after Adams’ death; it seems kind of unsporting to disagree with someone who can’t respond. These paragraphs cured me of my misgivings: after his death is by far the best time to disagree with Scott Adams.&lt;/p&gt;
    &lt;p&gt;The book is a novel (a real novel this time, with plot and everything) meant to dramatize the lessons of its predecessor. In the near future, the Muslims and Christians are on the verge of global war. Adams’ self-insert character, the Avatar, goes around hypnotizing and mind hacking everyone into cooperating with his hare-brained scheme for world peace.&lt;/p&gt;
    &lt;p&gt;In an early chapter, the Christian alliance has captured the Avatar and sent him to be tortured. But the Avatar masterfully deflects the torturer’s attention with a bit of cold reading, some pointed questions, and a few hypnotic suggestions:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As the Avatar planned, the interrogator’s conscious mind was scrambled by the emotions and thoughts of the past minutes. This brutish man, accustomed to avoiding deep thoughts, had imagined the tiniest particles of the universe, his childhood, and the battles of the future. He had laughed, felt pain and pity, been intellectually stimulated, confused, assured, and uncertain. The Avatar had challenged his worldview, and it was evaporating, leaving him feeling empty, unimportant, and purposeless&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In the thrilling climax, which takes place at Stacey’s Cafe (yes, it’s the real-world restaurant Adams was managing - yes, he turned his religious-apocalyptic thriller novel into an ad for his restaurant - yes, I bet he thought of this as a “hypnotic suggestion”), the characters find the Prime Influencer. She is able to come up with a short snappy slogan so memetically powerful that it defeats fundamentalist religion and ends the war (the slogan is: “If God is so smart, why do you fart?”). Adams’ mouthpiece character says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It wasn’t the wisdom of the question that made it so powerful; philosophers had posed better questions for aeons. It was the packaging—the marketing, if you will—the repeatability and simplicity, the timing, the Zeitgeist, and in the end, the fact that everyone eventually heard it from someone whose opinion they trusted.The question was short, provocative, and cast in the language of international commerce that almost everyone understood—English. Most important, and generally overlooked by historians: It rhymed and it was funny. Once you heard it, you could never forget it. It looped in the brain, gaining the weight and feel of truth with each repetition. Human brains have a limited capacity for logic and evidence. Throughout time, repetition and frequency were how people decided what was most true.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This paragraph is the absolute center of Adams’ worldview (later expanded to book length several times in tomes named things like Win Bigly: Persuasion In A World Where Facts Don’t Matter). People don’t respond to logic and evidence, so the world is ruled by people who are good at making catchy slogans. Sufficiently advanced sloganeering is indistinguishable from hypnosis, and so when Adams has some cute turns of phrase in his previous book, he describes it as “[I] used a variety of hypnosis techniques in an attempt to produce a feeling of euphoric enlightenment in the reader”. This is the cringiest way possible to describe cute turns of phrase, and turns me off from believing any his further claims to hypnotic mastery.&lt;/p&gt;
    &lt;p&gt;Throughout this piece, I’ve tried to emphasize that Adams was usually pretty self-aware. Did that include the hypnosis stuff? I’m not sure. I think he would have answered: certainly some people are great charismatic manipulators. Either their skills are magic, or they operate by some physical law. If they operate by physical law, they should be learnable. Maybe I’m not quite Steve Jobs level yet, but I have to be somewhere along the path to becoming Steve Jobs, right? And why not describe it in impressive terms? Steve Jobs would have come up with impressive-sounding terms for any skills he had, and you would have believed him!&lt;/p&gt;
    &lt;p&gt;Every few months, some group of bright nerds in San Francisco has the same idea: we’ll use our intelligence to hack ourselves to become hot and hard-working and charismatic and persuasive, then reap the benefits of all those things! This is such a seductive idea, there’s no reason whatsoever that it shouldn’t work, and every yoga studio and therapist’s office in the Bay Area has a little shed in the back where they keep the skulls of the last ten thousand bright nerds who tried this. I can’t explain why it so invariably goes wrong. The best I can do is tell a story where, when you’re trying to do this, you’re selecting for either techniques that can change you, or techniques that can compellingly make you think you’ve been changed. The latter are much more common than the former. And the most successful parasites are always those which can alter their host environment to be more amenable to themselves, and if you’re a parasite taking the form of a bad idea, that means hijacking your host’s rationality. So you’re really selecting for things that are compelling, seductive, and damage your ability to tell good ideas from bad ones. This is a just-so story that I have no evidence for - but seriously, go to someone who has the words “human potential” on their business card and ask them if you can see the skull shed.&lt;/p&gt;
    &lt;p&gt;But also: it’s attractive to be an effortlessly confident alpha male who oozes masculinity. And it’s . . . fine . . . to be a normal person with normal-person hangups. What you really don’t want to be is a normal person who is unconvincingly pretending to be a confident alpha male. “Oh hello, nice to meet you, I came here in my Ferrari, it’s definitely not a rental, you’re having the pasta - I’m choosing it for you because I’m so dominant - anyway, do you want to have sex when we get back? Oh, wait, I forgot to neg you, nice hair, is it fake?”&lt;/p&gt;
    &lt;p&gt;In theory, becoming a hot charismatic person with great social skills ought to be the same kind of task as everything else, where you practice a little and you’re bad, but then you practice more and you become good. But the uncanny valley is deep and wide, and Scott Adams was too invested in saying “Ha! I just hypnotized you - ha! There, did it again!” for me to trust his mountaineering skills.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don’t Step In The Leadership&lt;/head&gt;
    &lt;p&gt;It all led, inexorably, to Trump.&lt;/p&gt;
    &lt;p&gt;In summer 2015, Trump came down his escalator and announced his presidential candidacy. Given his comic status, his beyond-the-pale views, and his competition with a crowded field including Jeb Bush and Ted Cruz, traditional media wrote him off. Sure, he immediately led in the polls, but political history was full of weirdos who got brief poll bumps eighteen months before an election only to burn out later. The prediction markets listed his chance of the nomination (not the Presidency!) at 5%.&lt;/p&gt;
    &lt;p&gt;Which made it especially jarring when, in August, Scott Adams wrote a blog post asserting that Trump had “a 98% chance” of winning. This claim received national attention, because Trump was dominating the news cycle and Adams was approximately the only person, anywhere, who thought he had a chance.&lt;/p&gt;
    &lt;p&gt;There are two ways to make historically good predictions. The first way is to be some kind of brilliant superforecaster. Adams wasn’t this. Every big prediction he made after this one failed. Wikipedia notes that he dominated a Politico feature called “The Absolute Worst Political Prediction of 20XX”, with the authors even remarking that he “has managed to appear on this annual roundup of the worst predictions in politics more than any other person on the planet”. His most famous howler was that if Biden won in 2020, Republicans “would be hunted” and his Republican readers would “most likely be dead within a year”. But other highlights include “a major presidential candidate will die of COVID”, “the Supreme Court will overturn the 2024 election”, and “Hillary Clinton will start a race war”.&lt;/p&gt;
    &lt;p&gt;The other way to make a great prediction is to live your entire life for one perfect moment - the inveterate bear who predicted twelve of the last zero recessions, but now it’s 2008 and you look like a genius. By 2015, Adams had become a broken record around one point: people are irrational sheep who are prey for charismatic manipulators. The pointy-haired boss always wins. Trump was the pointiest-haired person in the vicinity, and he was obviously trying to charismatically play on people’s instincts while other people were doing comparatively normal politics. Scott Adams’ hour had arrived.&lt;/p&gt;
    &lt;p&gt;But Adams also handled his time in the spotlight masterfully. He gave us terms like “clown genius”. I hate using this, because I know Scott Adams was sitting at his desk in his custom-built Dilbert-head-shaped tower thinking “What sort of hypnotic catchy slogans can I use to make my meme about Trump spread . . . aha! Clown genius! That has exactly the right ring!” and it absolutely worked, and now everyone who was following the Internet in 2015 has the phrase “clown genius” etched into their brains (Adams calls these “linguistic kill shots”; since I remember that term and use it often, I suppose “linguistic kill shot” is an example of itself). He went from news outlet to news outlet saying “As a trained hypnotist, I can tell you what tricks Trump is using to bamboozle his followers, given that rational persuasion is fake and marketing techniques alone turn the wheels of history,” and the news outlets ate it up.&lt;/p&gt;
    &lt;p&gt;And some of his commentary was good. He was one of the first people to point out the classic Trump overreach, where he would say something like “Sleepy Joe Biden let in twenty trillion illegal immigrants!” The liberal media would take the bait and say “FACT CHECK: False! - Joe Biden only let in five million illegal immigrants!”, and thousands of people who had never previously been exposed to any narrative-threatening information would think “Wait, Joe Biden let in five million illegal immigrants?!” Once you notice it, it’s hard to unsee.&lt;/p&gt;
    &lt;p&gt;Adams started out by stressing that he was politically independent. He didn’t support Trump, he was just the outside hypnosis expert pointing out what Trump was doing. IT’S JUST A THOUGHT EXPERIMENT, IF YOU TAKE IT SERIOUSLY, YOU FAIL. Indeed, “this person is a charismatic manipulator hacking the minds of irrational sheep” is hardly a pro-Trump take. And he lived in Pleasanton, California - a member in good standing of the San Francisco metropolitan area - and nice Pleasantonians simply did not become Trump supporters in 2016.&lt;/p&gt;
    &lt;p&gt;On the other hand, at some point, his increasingly overblown theories of Trump’s greatness opened up a little wedge. The growing MAGA movement started treating him as one of their own; liberals started to see him as an enemy. His fame turned the All-Seeing Eye of social media upon him, that gaze which no man may meet without consequence. Once you’re sufficiently prominent, politics becomes a separating equilibrium; if you lean even slightly to one side, the other will pile on you so massively and traumatically that it will force you into their opponents’ open arms just for a shred of psychological security.&lt;/p&gt;
    &lt;p&gt;As he had done so many other times during his life, he resolved the conflict in the dumbest, cringiest, and most public way possible: a June 2016 blog post announcing that he was endorsing Hillary Clinton, for his own safety, because he suspected he would be targeted for assassination if he didn’t:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This past week we saw Clinton pair the idea of President Trump with nuclear disaster, racism, Hitler, the Holocaust, and whatever else makes you tremble in fear. That is good persuasion if you can pull it off because fear is a strong motivator. It is also a sharp pivot from Clinton’s prior approach of talking about her mastery of policy details, her experience, and her gender. Trump took her so-called “woman card” and turned it into a liability. So Clinton wisely pivoted. Her new scare tactics are solid-gold persuasion. I wouldn’t be surprised if you see Clinton’s numbers versus Trump improve in June, at least temporarily, until Trump finds a counter-move.&lt;/p&gt;
      &lt;p&gt;The only downside I can see to the new approach is that it is likely to trigger a race war in the United States. And I would be a top-ten assassination target in that scenario […]&lt;/p&gt;
      &lt;p&gt;So I’ve decided to endorse Hillary Clinton for President, for my personal safety. Trump supporters don’t have any bad feelings about patriotic Americans such as myself, so I’ll be safe from that crowd. But Clinton supporters have convinced me – and here I am being 100% serious – that my safety is at risk if I am seen as supportive of Trump. So I’m taking the safe way out and endorsing Hillary Clinton for president.&lt;/p&gt;
      &lt;p&gt;As I have often said, I have no psychic powers and I don’t know which candidate would be the best president. But I do know which outcome is most likely to get me killed by my fellow citizens. So for safety reason, I’m on team Clinton.&lt;/p&gt;
      &lt;p&gt;My prediction remains that Trump will win in a landslide based on his superior persuasion skills. But don’t blame me for anything President Trump does in office because I endorse Clinton.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This somehow failed to be a masterstroke of hypnotic manipulation that left both sides placated. But it was fine, because Trump won anyway! In the New Right’s wave of exultation, all was forgiven, and the first high-profile figure to bet on Trump became a local hero and confirmed prophet. Never mind that Adams had predicted Trump would win by “one of the biggest margins we’ve seen in recent history” when in fact he lost the popular vote. The man who had dreamed all his life of being respected for something other than cartooning had finally made it.&lt;/p&gt;
    &lt;p&gt;Obviously, it destroyed him.&lt;/p&gt;
    &lt;p&gt;At first, I wondered if Adams’ right-wing turn was a calculated manuever. He’d always longed to be a manipulator of lesser humans, and had finally achieved slightly-above-zero skill at it. Wouldn’t it fit his personality to see the right-wingers as dumb sheep, and himself as the clever Dogbert-style scammer who could profit off them? Did he really believe (as he claimed) that he was at risk of being assassinated by left-wing radicals who couldn’t handle his level of insight into Trump’s genius? Or was this just another hypnotic suggestion, retrospectively justified insofar as we’re still talking about it ten years later and all publicity is good publicity?&lt;/p&gt;
    &lt;p&gt;But I don’t think he did it cynically. At the turn of the millennium, the obsessed-with-their-own-cleverness demographic leaned firmly liberal: smug New Atheists, hardline skeptics, members of the “reality-based community”. But in the 2010s, liberalism became the default, the public switched to expertolatry, dumb people’s orthodoxies about race and gender became easier and more fun to puncture than dumb people’s orthodoxies about religion - and the O.W.T.O.C.s lurched right. Adams was borne along by the tide. With enough time, dedication, and archive access, you can hop from Dilbert comic to Dilbert comic, tracing the exact contours of his political journey.&lt;/p&gt;
    &lt;p&gt;There’s a passage in the intro to one of Adams books where he says that, given how he’s going to blow your mind and totally puncture everything you previously believed, perhaps the work is unsuitable for people above fifty-five, whose brains are comparatively sclerotic and might shatter at the strain. This is how I feel about post-2016 politics. Young people were mostly able to weather the damage. As for older people, I have seen public intellectual after public intellectual who I previously respected have their brains turn to puddles of partisan-flavored mush. Jordan Peterson, Ken White, Curtis Yarvin, Paul Krugman, Elon Musk, the Weinsteins, [various people close enough to me that it would be impolite to name them here]. Once, these people were lions of insightful debate. Where now are the horse and the rider? Where is the horn that was blowing?&lt;/p&gt;
    &lt;p&gt;Adams was 58 when Trump changed everything. In 2001, age 44, he’d found the failure of his Dilberito funny. But in another interview, at age 50, he suggested that maybe his competitors had formed teams to sneak into supermarkets and hide them in the back of the shelves. Being tragically flawed yet also self-aware enough to laugh about it is a young man’s game.&lt;/p&gt;
    &lt;p&gt;In 2024, diagnosed with terminal cancer, Adams decided to treat it via ivermectin, according to a protocol recommended by fellow right-wing contrarian Dr. William Makis. This doesn’t seem to me like a story about a cynic milking right-wingers for the grift. It sounds like a true believer. Scott Adams, the man too clever and independent to join any political tendency, who had sworn to always be the master manipulator standing above the fray rather than a sheep with ordinary object-level opinions - had finally succumbed to sincere belief.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s Not Funny If I Have To Explain It&lt;/head&gt;
    &lt;p&gt;Every child is hypomanic, convinced of their own specialness. Even most teenagers still suspect that, if everything went right, they could change the world.&lt;/p&gt;
    &lt;p&gt;It’s not just nerds. Everyone has to crash into reality. The guitar player who starts a garage band in order to become a rockstar. The varsity athlete who wants to make the big leagues. They all eventually realize, no, I’m mediocre. Even the ones who aren’t mediocre, the ones with some special talent, only have one special talent (let’s say cartooning) and no more.&lt;/p&gt;
    &lt;p&gt;I don’t know how the musicians and athletes cope. I hear stories about washed-up alcoholic former high school quarterbacks forever telling their girlfriends about how if Coach had only put them in for the last quarter during the big game, things would have gone differently. But since most writers are nerds, it’s the nerds who dominate the discussion, so much so that the whole affair gets dubbed “Former Gifted Kid Syndrome”.&lt;/p&gt;
    &lt;p&gt;Every nerd who was the smartest kid in their high school goes to an appropriately-ranked college and realizes they’re nothing special. But also, once they go into some specific field they find that intellect, as versatile as it is, can only take them so far. And for someone who was told their whole childhood that they were going to cure cancer (alas, a real quote from my elementary school teacher), it’s a tough pill to swallow.&lt;/p&gt;
    &lt;p&gt;Reaction formation, where you replace a unbearable feeling with its exact opposite, is one of the all time great Freudian defense mechanisms. You may remember it from such classics as “rape victims fall in love with their rapist” or “secretly gay people become really homophobic”. So some percent of washed-up gifted kids compensate by really, really hating nerdiness, rationality, and the intellect.&lt;/p&gt;
    &lt;p&gt;The variety of self-hating nerd are too many to number. There are the nerds who go into psychology to prove that EQ is a real thing and IQ merely its pale pathetic shadow. There are the nerds who become super-woke and talk about how reason and objectivity are forms of white supremacy culture. There are the nerds who obsess over “embodiment” and “somatic therapy” and accuse everyone else of “living in their heads”. There are the nerds who deflect by becoming really into neurodiversity - “the interesting thing about my brain isn’t that I’m ‘smart’ or ‘rational’, it’s that I’m ADHDtistic, which is actually a weakness . . . but also secretly a strength!” There are the nerds who flirt with fascism because it idolizes men of action, and the nerds who convert to Christianity because it idolizes men of faith. There are the nerds who get really into Seeing Like A State, and how being into rationality and metrics and numbers is soooooo High Modernist, but as a Kegan Level Five Avatar they are far beyond such petty concerns. There are the nerds who redefine “nerd” as “person who likes Marvel movies” - having successfully gerrymandered themselves outside the category, they can go back to their impeccably-accurate statisticsblogging on educational outcomes, or their deep dives into anthropology and medieval mysticism, all while casting about them imprecations that of course nerds are loathsome scum who deserve to be bullied.&lt;/p&gt;
    &lt;p&gt;(maybe it’s unfair to attribute this to self-hatred per se. Adams wrote, not unfairly, that the scientismists in Kegan level 4 “are arrogant when it comes to dealing with people in levels two and three.” Maybe there’s the same desperate urge for level 5 to differentiate themselves from 4s - cf. barberpole theory of fashion).&lt;/p&gt;
    &lt;p&gt;Scott Adams felt the contradictions of nerd-dom more acutely than most. As compensation, he was gifted with two great defense mechanisms. The first was humor (which Freud grouped among the mature, adaptive defenses), aided by its handmaiden self-awareness. The second (from Freud’s “neurotic” category) was his own particular variety of reaction formation, “I’m better than those other nerds because, while they foolishly worship rationality and the intellect, I’ve gotten past it to the real deal, marketing / manipulation / persuasion / hypnosis.”&lt;/p&gt;
    &lt;p&gt;When he was young, and his mind supple, he was able to balance both these mechanisms; the steam of their dissonance drove the turbine of his art. As he grew older, the first one - especially the self-awareness - started to fail, and he leaned increasingly heavily on the second. Forced to bear the entire weight of his wounded psyche, it started showing more and more cracks, until eventually he ended up as a podcaster - the surest sign of a deranged mind.&lt;/p&gt;
    &lt;p&gt;In comparison, his final downfall was almost trivial - a bog-standard cancellation, indistinguishable from every other cancellation of the 2015 - 2025 period. Angered by a poll where some black people expressed discomfort with the slogan “It’s Okay To Be White”, Adams declared that “the best advice I would give to white people is to get the hell away from black people; just get the fuck away”. Needless to say, his publisher, syndicator, and basically every newspaper in the country dropped him immediately. He relaunched his comics on Locals, an online subscription platform for cancelled people, but his reach had declined by two orders of magnitude and never recovered.&lt;/p&gt;
    &lt;p&gt;Adams was willing to sacrifice everything for the right to say “It’s Okay To Be White”. I can’t help wondering what his life would have been like if he’d been equally willing to assert the okayness of the rest of his identity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dilbert's Guide to the Rest of Your Life&lt;/head&gt;
    &lt;p&gt;In case it’s not obvious, I loved Scott Adams.&lt;/p&gt;
    &lt;p&gt;Partly this is because we’re too similar for me to hate him without hating myself. You’re a bald guy with glasses named Scott A who lives in the San Francisco Bay Area. You think you’re pretty clever, but the world has a way of reminding you of your limitations. You try to work a normal job. You do a little funny writing on the side. People like the funny writing more than you expected. Hardly believing your luck, you quit to do the funny writing full time. You explore themes about the irrationality of the world. You have some crazy ideas you’re not entirely willing to stand behind, and present them as fiction or speculation or April Fools jokes. You always wonder whether your purpose in life is really just funny writing - not because people don’t love the stuff you write, not even because you don’t get fan mail saying you somehow mysteriously changed people’s lives, but just because it seems less serious than being a titan of industry or something. You try some other things. They don’t go terribly, but they don’t go great either. You decide to stick with what you’re good at. You write a book about the Lurianic kabbalah. You get really into whale puns.&lt;/p&gt;
    &lt;p&gt;As we pass through life, sometimes God shows us dopplegangers, bright or dark mirrors of ourselves, glimpses of how we might turn out if we zig or zag on the path ahead. Some of these people are meant as shining inspirations, others as terrible warnings, but they’re all our teachers.&lt;/p&gt;
    &lt;p&gt;Adams was my teacher in a more literal way too. He published several annotated collections, books where he would present comics along with an explanation of exactly what he was doing in each place, why some things were funny and others weren’t, and how you could one day be as funny as him. Ten year old Scott devoured these. I’ve always tried to hide my power level as a humorist, lest I get pegged as a comedic author and people stop taking me seriously. But objectively my joke posts get the most likes and retweets of anything I write, and I owe much of my skill in the genre to cramming Adams’ advice into a malleable immature brain4. There’s a direct line between Dogbert’s crazy schemes and the startup ideas in a typical Bay Area House Party post.&lt;/p&gt;
    &lt;p&gt;The Talmud tells the story of the death of Rabbi Elisha. Elisha was an evil apostate. His former student, Rabbi Meir, who stayed good and orthodox, insisted that Rabbi Elisha probably went to Heaven. This was never very plausible, and God sent increasingly obvious signs to the contrary, including a booming voice from Heaven saying that Elisha was not saved. Out of loyalty to his ex-teacher, Meir dismissed them all - that voice was probably just some kind of 4D chess move - and insisted that Elisha had a share in the World To Come.&lt;/p&gt;
    &lt;p&gt;Out of the same doomed loyalty as Rabbi Meir, I want to believe Scott Adams went to Heaven.&lt;/p&gt;
    &lt;p&gt;There is what at first appears to be promising evidence - in his final message to his fans, Adams said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Many Christian friends have asked me to find Jesus before I go. I’m not a believer, but I have to admit the risk-reward calculation for doing so looks attractive. So here I go: I accept Jesus Christ as my lord and savior, and I like forward to spending an eternity with him. The part about me not being a believer should be quickly resolved if I wake up in heaven. I won’t need any more convincing than that. And I hope I am still qualified for entry.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It is a dogma of many religions that sincere deathbed conversions are accepted. But I’d be more comfortable if this sounded less like “haha, I found my final clever lifehack”. I can only hope he didn’t try to implant any hypnotic suggestions in an attempt to get a linguistic kill shot in on the Almighty. As another self-hating nerd writer put it, “through all these years I make experiment if my sins or Your mercy greater be.”&lt;/p&gt;
    &lt;p&gt;But I’m more encouraged by the second half of his departing note:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;For the first part of my life, I was focused on making myself a worthy husband and parent, as a way to find meaning. That worked. But marriages don't always last forever, and mine eventually ended, in a highly amicable way. I'm grateful for those years and for the people I came to call my family.&lt;/p&gt;&lt;lb/&gt;Once the marriage unwound, I needed a new focus. A new meaning. And so I donated myself to "the world," literally speaking the words out loud in my otherwise silent home. From that point on, I looked for ways I could add the most to people's lives, one way or another.&lt;lb/&gt;That marked the start of my evolution from Dilbert cartoonist to an author of - what I hoped would be - useful books. By then, I believed I had condensed enough life lessons that I could start passing them on. I continued making Dilbert comics, of course.&lt;lb/&gt;As luck would have it, I'm a good writer. My first book in the "useful" genre was How to Fail at Almost Everything and Still Win Big. That book turned out to be a huge success, often imitated, and influencing a wide variety of people. I still hear every day how much that book changed lives. My plan to be useful was working.&lt;lb/&gt;I followed up with my book Win Bigly, that trained an army of citizens how to be more persuasive, which they correctly saw as a minor super power. I know that book changed lives because I hear it often.&lt;lb/&gt;You'll probably never know the impact the book had on the world, but I know, and it pleases me while giving me a sense of meaning that is impossible to describe.&lt;lb/&gt;My next book, Loserthink, tried to teach people how to think better, especially if they were displaying their thinking on social media. That one didn't put much of a dent in the universe, but I tried.&lt;lb/&gt;Finally, my book Reframe Your Brain taught readers how to program their own thoughts to make their personal and professional lives better. I was surprised and delighted at how much positive impact that book is having.&lt;lb/&gt;I also started podcasting a live show called Coffee With Scott Adams, dedicated to helping people think about the world, and their lives, in a more productive way. I didn't plan it this way, but it ended up helping lots of lonely people find a community that made them feel less lonely. Again, that had great meaning for me.&lt;lb/&gt;I had an amazing life. I gave it everything I had. If you got any benefits from my work, I'm asking you to pay it forward as best you can. That is the legacy I want.&lt;p&gt;Be useful.&lt;/p&gt;&lt;p&gt;And please know I loved you all to the end.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;I had been vaguely aware that he had some community around him, but on the event of his death, I tried watching an episode or two of his show. I couldn’t entirely follow, but I think his various sub-shows are getting rolled into a broader brand, The Scott Adams School, where his acolytes discuss and teach his theory of persuasion:&lt;/p&gt;
    &lt;p&gt;The woman on the top left is his ex-wife. Even though they’ve been divorced for twelve years, they never abandoned each other. All the other faces are people who found Adams revelatory and are choosing to continue his intellectual tradition. And in the comments - thirteen thousand of them - are other people who loved Adams. Some watch every episode of his podcast and consider him a genius. Others were touched in more subtle ways. People who wrote him with their problems and he responded. People who met him on the street and demanded the typical famous person “pose for a photo with me”, and he did so graciously. People who said his self-help books really helped them. People who just used Dilbert to stay sane through their cubicle jobs.&lt;/p&gt;
    &lt;p&gt;(also one person blaming his death on the COVID vaccine, but this is Twitter, you’re never going to avoid that)&lt;/p&gt;
    &lt;p&gt;Adams is easy and fun to mock - as is everyone who lives their life uniquely and unapologetically. I’ve had a good time psychoanalyzing him, but everyone does whatever they do for psychological reasons, and some people end up doing good.&lt;/p&gt;
    &lt;p&gt;Though I can’t endorse either Adams’ politics or his persuasive methods, everything is a combination of itself and an attempt to build a community. And whatever the value of his ideas, the community seems real and loving.&lt;/p&gt;
    &lt;p&gt;And I’m serious when I say I consider Adams a teacher. For me, he was the sort of teacher who shows you what to avoid; for many others, he was the type who serves as inspiration. These roles aren’t quite opposites - they’re both downstream of a man who blazed his own path, and who recorded every step he took, with unusual grace and humor, as documentation for those who would face a choice of whether or not to follow. This wasn’t a coincidence, but the conscious and worthy project of his life. Just for today, I’ll consider myself part of the same student body as all the other Adams fans, and join my fellows in tribute to our fallen instructor.&lt;/p&gt;
    &lt;p&gt;I hope he gets his linguistic kill shot in on God and squeaks through the Pearly Gates.&lt;/p&gt;
    &lt;p&gt;As is quantum complexity blogger Scott Aaronson.&lt;/p&gt;
    &lt;p&gt;Cf. the old joke about the Soviet Jew trying to emigrate to Israel. The secret police is giving him a hard time - “What don’t you like about our communist paradise? You think the economy is too weak?” “Oh no, I can’t complain.” “You think the politics are oppressive?” “Oh no, I can’t complain.” “You think we prevent you from practicing your primitive religion?” “Oh no, I can’t complain.” “Then why do you want to leave for Israel?” “Because there, I can complain.”&lt;/p&gt;
    &lt;p&gt;"What’s the normal English term for when holy people fight over holy sites because of their differing beliefs about what is holy? Oh, right, a Religion War.”&lt;/p&gt;
    &lt;p&gt;To be more precise, half of my skill. I attribute the other half to Dave Barry, who I consumed the same way during the same period of my life.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.astralcodexten.com/p/the-dilbert-afterlife"/><published>2026-01-16T23:03:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46653721</id><title>FLUX.2 [Klein]: Towards Interactive Visual Intelligence</title><updated>2026-01-17T12:19:06.584970+00:00</updated><content>&lt;doc fingerprint="9d927013843c0b86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FLUX.2 [klein]: Towards Interactive Visual Intelligence&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Models&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;FLUX.2 [klein]: Towards Interactive Visual Intelligence&lt;/head&gt;
    &lt;p&gt;Today, we release the FLUX.2 [klein] model family, our fastest image models to date. FLUX.2 [klein] unifies generation and editing in a single compact architecture, delivering state-of-the-art quality with end-to-end inference as low as under a second. Built for applications that require real-time image generation without sacrificing quality, and runs on consumer hardware with as little as 13GB VRAM.&lt;/p&gt;
    &lt;p&gt;Demo showing editing with FLUX.2 [klein]&lt;/p&gt;
    &lt;head rend="h3"&gt;Why go [klein]?&lt;/head&gt;
    &lt;p&gt;Visual Intelligence is entering a new era. As AI agents become more capable, they need visual generation that can keep up; models that respond in real-time, iterate quickly, and run efficiently on accessible hardware.&lt;/p&gt;
    &lt;p&gt;The klein name comes from the German word for "small", reflecting both the compact model size and the minimal latency. But FLUX.2 [klein] is anything but limited. These models deliver exceptional performance in text-to-image generation, image editing and multi-reference generation, typically reserved for much larger models.&lt;/p&gt;
    &lt;head rend="h3"&gt;What's New&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sub-second inference. Generate or edit images in under 0.5s on modern hardware.&lt;/item&gt;
      &lt;item&gt;Photorealistic outputs and high diversity, especially in the base variants.&lt;/item&gt;
      &lt;item&gt;Unified generation and editing. Text-to-image, image editing, and multi-reference support in a single model while delivering frontier performance.&lt;/item&gt;
      &lt;item&gt;Runs on consumer GPUs. The 4B model fits in ~13GB VRAM (RTX 3090/4070 and above).&lt;/item&gt;
      &lt;item&gt;Developer-friendly &amp;amp; Accessible: Apache 2.0 on 4B models, open weights for 9B models. Full open weights for customization and fine-tuning.&lt;/item&gt;
      &lt;item&gt;API and open weights. Production-ready API or run locally with full weights.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: The “FLUX [dev] Non-Commercial License” has been renamed to “FLUX Non-Commercial License” and will apply to the 9B Klein models. No material changes have been made to the license.&lt;/p&gt;
    &lt;p&gt;Text to Image collage using FLUX.2 [klein]&lt;/p&gt;
    &lt;head rend="h3"&gt;The FLUX.2 [klein] Model Family&lt;/head&gt;
    &lt;head rend="h4"&gt;FLUX.2 [klein] 9B&lt;/head&gt;
    &lt;p&gt;Our flagship small model. Defines the Pareto frontier for quality vs. latency across text-to-image, single-reference editing, and multi-reference generation. Matches or exceeds models 5x its size - in under half a second. Built on a 9B flow model with 8B Qwen3 text embedder, step-distilled to 4 inference steps.&lt;/p&gt;
    &lt;p&gt;Combine multiple input images, blend concepts, and iterate on complex compositions - all at sub-second speed with frontier-level quality. No model this fast has ever done this well.&lt;/p&gt;
    &lt;p&gt;License: FLUX NCL&lt;/p&gt;
    &lt;p&gt;Imagine editing collage using FLUX.2 [klein]&lt;/p&gt;
    &lt;head rend="h4"&gt;&lt;lb/&gt;FLUX.2 [klein] 4B:&lt;/head&gt;
    &lt;p&gt;Fully open under Apache 2.0. Our most accessible model, it runs on consumer GPUs like the RTX 3090/4070. Compact but capable: supports T2I, I2I, and multi-reference at quality that punches above its size. Built for local development and edge deployment.&lt;/p&gt;
    &lt;p&gt;License: Apache 2.0&lt;/p&gt;
    &lt;head rend="h4"&gt;FLUX.2 [klein] Base 9B / 4B:&lt;/head&gt;
    &lt;p&gt;The full-capacity foundation models. Undistilled, preserving complete training signal for maximum flexibility. Ideal for fine-tuning, LoRA training, research, and custom pipelines where control matters more than speed. Higher output diversity than the distilled models.&lt;/p&gt;
    &lt;p&gt;License: 4B Base under Apache 2.0, 9B Base under FLUX NCL&lt;/p&gt;
    &lt;p&gt;Output Diversity using FLUX.2 [klein]&lt;/p&gt;
    &lt;head rend="h4"&gt;Quantized versions&lt;/head&gt;
    &lt;p&gt;We are also releasing FP8 and NVFP4 versions of all [klein] variants, developed in collaboration with NVIDIA for optimized inference on RTX GPUs. Same capabilities, smaller footprint - compatible with even more hardware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FP8: Up to 1.6x faster, up to 40% less VRAM&lt;/item&gt;
      &lt;item&gt;NVFP4: Up to 2.7x faster, up to 55% less VRAM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarks on RTX 5080/5090, T2I at 1024×1024&lt;lb/&gt;Same licenses apply: Apache 2.0 for 4B variants, FLUX NCL for 9B.&lt;/p&gt;
    &lt;head rend="h4"&gt;&lt;lb/&gt;Performance Analysis&lt;/head&gt;
    &lt;p&gt;FLUX.2 [klein] Elo vs Latency (top) and VRAM (bottom) across Text-to-Image, Image-to-Image Single Reference, and Multi-Reference tasks. FLUX.2 [klein] matches or exceeds Qwen's quality at a fraction of the latency and VRAM, and outperforms Z-Image while supporting both text-to-image generation and (multi-reference) image editing in a unified model. The base variants trade some speed for full customizability and fine-tuning, making them better suited for research and adaptation to specific use cases. Speed is measured on a GB200 in bf16.&lt;/p&gt;
    &lt;head rend="h3"&gt;Into the New&lt;/head&gt;
    &lt;p&gt;FLUX.2 [klein] is more than a faster model. It's a step toward our vision of interactive visual intelligence. We believe the future belongs to creators and developers with AI that can see, create, and iterate in real-time. Systems that enable new categories of applications: real-time design tools, agentic visual reasoning, interactive content creation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources&lt;/head&gt;
    &lt;p&gt;Try it&lt;/p&gt;
    &lt;p&gt;Build with it&lt;/p&gt;
    &lt;p&gt;Learn more&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence"/><published>2026-01-16T23:46:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46654085</id><title>Keifu – A TUI for navigating commit graphs with color and clarity</title><updated>2026-01-17T12:19:05.842925+00:00</updated><content>&lt;doc fingerprint="323a5dcae4fcdf83"&gt;
  &lt;main&gt;
    &lt;p&gt;keifu (系譜, /keːɸɯ/) is a terminal UI tool that visualizes Git commit graphs. It shows a colored commit graph, commit details, and a summary of changed files, and lets you perform basic branch operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Readable commit graph — &lt;code&gt;git log --graph&lt;/code&gt;is hard to read; keifu renders a cleaner, color-coded graph&lt;/item&gt;
      &lt;item&gt;Fast branch switching — With AI-assisted coding, working on multiple branches in parallel has become common. keifu makes branch switching quick and visual&lt;/item&gt;
      &lt;item&gt;Keep it simple — Only basic Git operations are supported; this is not a full-featured Git client&lt;/item&gt;
      &lt;item&gt;Narrow terminal friendly — Works well in split panes and small windows&lt;/item&gt;
      &lt;item&gt;No image protocol required — Works on any terminal with Unicode support&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unicode commit graph with per-branch colors&lt;/item&gt;
      &lt;item&gt;Commit list with branch labels, date, author, short hash, and message (some fields may be hidden on narrow terminals)&lt;/item&gt;
      &lt;item&gt;Commit detail panel with full message and changed file stats (+/-)&lt;/item&gt;
      &lt;item&gt;Git operations: checkout, create/delete branch, fetch&lt;/item&gt;
      &lt;item&gt;Branch search with dropdown UI&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run inside a Git repository (auto-discovery from current directory)&lt;/item&gt;
      &lt;item&gt;A terminal with Unicode line drawing support and color&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;git&lt;/code&gt;command in PATH (required for fetch)&lt;/item&gt;
      &lt;item&gt;Rust toolchain (for building from source)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cargo install keifu&lt;/code&gt;
    &lt;code&gt;mise use -g github:trasta298/keifu@latest&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/trasta298/keifu &amp;amp;&amp;amp; cd keifu &amp;amp;&amp;amp; cargo install --path .&lt;/code&gt;
    &lt;p&gt;Run inside a Git repository:&lt;/p&gt;
    &lt;code&gt;keifu&lt;/code&gt;
    &lt;p&gt;See docs/configuration.md for configuration options.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;j&lt;/code&gt; / &lt;code&gt;↓&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Move down&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;k&lt;/code&gt; / &lt;code&gt;↑&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Move up&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;]&lt;/code&gt; / &lt;code&gt;Tab&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Jump to next commit that has branch labels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;[&lt;/code&gt; / &lt;code&gt;Shift+Tab&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Jump to previous commit that has branch labels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;h&lt;/code&gt; / &lt;code&gt;←&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Select left branch (same commit)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;l&lt;/code&gt; / &lt;code&gt;→&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Select right branch (same commit)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+d&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Page down&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+u&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Page up&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;g&lt;/code&gt; / &lt;code&gt;Home&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Go to top&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;G&lt;/code&gt; / &lt;code&gt;End&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Go to bottom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;@&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Jump to HEAD (current branch)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Enter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Checkout selected branch/commit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Create branch at selected commit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;d&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Delete branch (local, non-HEAD)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;f&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fetch from origin&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Search branches (incremental fuzzy search)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;↑&lt;/code&gt; / &lt;code&gt;Ctrl+k&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Select previous result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;↓&lt;/code&gt; / &lt;code&gt;Ctrl+j&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Select next result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Enter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Jump to selected branch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;Esc&lt;/code&gt; / &lt;code&gt;Backspace&lt;/code&gt; on empty&lt;/cell&gt;
        &lt;cell&gt;Cancel search&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;R&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Refresh repository data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Toggle help&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;q&lt;/code&gt; / &lt;code&gt;Esc&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Quit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The TUI loads up to 500 commits across all branches.&lt;/item&gt;
      &lt;item&gt;Merge commits are diffed against the first parent; the initial commit is diffed against an empty tree.&lt;/item&gt;
      &lt;item&gt;Changed files are capped at 50 and binary files are skipped.&lt;/item&gt;
      &lt;item&gt;If there are staged or unstaged changes (excluding untracked files), an "uncommitted changes" row appears at the top.&lt;/item&gt;
      &lt;item&gt;When multiple branches point to the same commit, the label is collapsed to a single name with a &lt;code&gt;+N&lt;/code&gt;suffix (e.g.,&lt;code&gt;main +2&lt;/code&gt;). Use&lt;code&gt;h&lt;/code&gt;/&lt;code&gt;l&lt;/code&gt;or&lt;code&gt;←&lt;/code&gt;/&lt;code&gt;→&lt;/code&gt;to switch between them.&lt;/item&gt;
      &lt;item&gt;Checking out &lt;code&gt;origin/xxx&lt;/code&gt;creates or updates a local branch. Upstream is set only when creating a new branch. If the local branch exists but points to a different commit, it is force-updated to match the remote.&lt;/item&gt;
      &lt;item&gt;Remote branches are displayed, but delete operations only work with local branches.&lt;/item&gt;
      &lt;item&gt;Fetch requires the &lt;code&gt;origin&lt;/code&gt;remote to be configured.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/trasta298/keifu"/><published>2026-01-17T00:32:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46654749</id><title>Drone Hacking Part 1: Dumping Firmware and Bruteforcing ECC</title><updated>2026-01-17T12:19:04.372576+00:00</updated><content>&lt;doc fingerprint="ae645296a9a5069d"&gt;
  &lt;main&gt;
    &lt;p&gt;~48 min read&lt;/p&gt;
    &lt;head rend="h1"&gt;Drone Hacking Part 1: Dumping Firmware and Bruteforcing ECC&lt;/head&gt;
    &lt;head rend="h2"&gt;Intro&lt;/head&gt;
    &lt;p&gt;In July 2025, we from Neodyme got together in Munich and did security research on a bunch of IoT devices, ranging from bluetooth headsets, to door locks, to drones. One of these was the Potensic Atom 2. It’s a photo and video drone with a gimbal-stabilized 4K camera and a remote control that you hook up to your own smartphone and the proprietary app. If you’ve ever flown a DJI Mini 4K, this drone will look very familiar to you.&lt;/p&gt;
    &lt;p&gt;This post is part of a two-part series that will cover how we disassembled the drone and dumped the firmware from the NAND chip and how we analyzed the drone’s firmware, app, and remote control to find some backdoors and vulnerabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goal: Dumping the Firmware&lt;/head&gt;
    &lt;p&gt;One of the most important pieces of information you can acquire when setting up to hack a device is its firmware. If you want to reverse engineer the software that’s running on the drone and find vulnerabilities in that, then you need a copy of it in the first place.&lt;/p&gt;
    &lt;p&gt;Now there are a couple of ways to go about that, some are less intrusive and some are more effective.&lt;/p&gt;
    &lt;p&gt;You might get lucky and be able to just download the firmware as a firmware update from the manufacturer’s website. However, those update sites are often not publicly documented and can be locked behind authorization checks or encrypted. Encrypted firmwares can still be useful - you “just” need to reverse engineer the on-device decryption process. For the Atom 2, downloading the firmware updates required having a valid drone and remote control serial number and the firmware update was also encrypted. Without having the decryption logic, we put this approach on ice during our initial research.&lt;/p&gt;
    &lt;p&gt;Another really comfortable approach is to use exposed debug interfaces like JTAG or UART. However, those are often undocumented, unlabeled, or entirely removed for public versions. We didn’t find any on the Atom 2.&lt;/p&gt;
    &lt;p&gt;What we can always do, though not necessarily always successful, is solder off the entire NAND chip and dump the firmware byte by byte. This has the risk of breaking the NAND chip and/or the rest of the board if you’re not careful. Also, some devices, like modern smart phones, encrypt their persistent storage with key material stored in, e.g., the TPM. If that is the case, then simply soldering off the NAND chip will leave you with unusable encrypted data. Fortunately, the Atom 2’s NAND contents are not encrypted, as we find out later.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dumping the NAND Chip&lt;/head&gt;
    &lt;p&gt;Dumping a NAND chip generally always follows the same pattern:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Identifying the NAND Chip&lt;/item&gt;
      &lt;item&gt;Removing it from the board&lt;/item&gt;
      &lt;item&gt;Identifying the data pins and communication protocol of the NAND chip&lt;/item&gt;
      &lt;item&gt;Connecting the NAND chip to some kind of reading device&lt;/item&gt;
      &lt;item&gt;Reading the NAND content&lt;/item&gt;
      &lt;item&gt;Reassembling the read contents into a working firmware - usually containing one or more file systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Identifying the NAND Chip&lt;/head&gt;
    &lt;p&gt;The Atom 2 has multiple boards:&lt;/p&gt;
    &lt;p&gt;We are mainly interested in the main board because that’s where the NAND flash is going to be. The main board had several metal RF shields that we have already pried off or cut through on the photos.&lt;/p&gt;
    &lt;p&gt;We can identify most of these chips through their markings. Note that while we’re mainly interested in the NAND chip, knowing the others can help with recognizing things during reversing later on. Roughly knowing which SoC we were working with was crucial, as you will see in later sections of this blog post.&lt;/p&gt;
    &lt;p&gt;(Note that the markings below might not match the photos completely. We had multiple drones. The markings are mostly from the first drone and the photos are mostly of the second drone.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Top-side&lt;/head&gt;
    &lt;p&gt;SoC (System on a Chip, aka “the main thingy”)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: 23AP10 VTQMSQKJYJ 4978-CN B3&lt;/item&gt;
      &lt;item&gt;We didn’t find an exact match, but this site references the 21AP10. &lt;list rend="ul"&gt;&lt;item&gt;The page title is &lt;code&gt;21AP10 SS928 平替SD3403V100 海思 SOC芯片&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;21AP10 SS928 平替&lt;/code&gt;=&amp;gt;&lt;code&gt;21AP10 SS928 Drop-In Replacement&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;SD3403V100 海思 SOC芯片&lt;/code&gt;=&amp;gt;&lt;code&gt;HiSilicon SD3403V100 SOC Chip&lt;/code&gt;&lt;/item&gt;&lt;item&gt;That is a mobile camera SoC.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The page title is &lt;/item&gt;
      &lt;item&gt;It makes sense that this is the SoC because it is close to both external RAM chips and the NAND flash.&lt;/item&gt;
      &lt;item&gt;In this teardown of a previous Atom model, the device had a &lt;code&gt;HiSilicon Hi 3559 camera MCU&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;We found a data sheet for the HiSilicon Hi3519 V100. &lt;list rend="ul"&gt;&lt;item&gt;Close enough for now.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RAM&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: SEC340 K4A8G16 5WC BCTD G2F9190AC&lt;/item&gt;
      &lt;item&gt;Data sheet can be found on the internet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ARM Cortex-M4&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: GD32F470 VGH6 BUMK618 AL2451 GigaDevice ARM&lt;/item&gt;
      &lt;item&gt;Data sheet can be found on the internet.&lt;/item&gt;
      &lt;item&gt;Might be SD-card related since the SD card slot is on the other side.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unknown Chips&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: V2 2441TM4N190.00 &lt;list rend="ul"&gt;&lt;item&gt;The name &lt;code&gt;2441TM&lt;/code&gt;appears in some WizSense surveillance cameras&lt;/item&gt;&lt;item&gt;Not sure if related&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The name &lt;/item&gt;
      &lt;item&gt;2 chips with markings: 8285HE 426656 CS2441&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Bottom Side&lt;/head&gt;
    &lt;p&gt;NAND flash&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: MXIC X243662 MX35UF4GE4AD-241 5P231800A1&lt;/item&gt;
      &lt;item&gt;Data sheet can be found on the internet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RAM&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: SEC407 K4A8G16 5WC BCTD G2K43304C&lt;/item&gt;
      &lt;item&gt;Same as on top side&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ARM Cortex-M4&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: F460JEUA P8VR4400 2416021&lt;/item&gt;
      &lt;item&gt;Not sure what it’s used for.&lt;/item&gt;
      &lt;item&gt;A data sheet for the HC32F460JEUA-QFN48TR (looks close enough?) can be found on the internet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;WLAN + Bluetooth&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RTL8821CS&lt;/item&gt;
      &lt;item&gt;Data sheet can be found on the internet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Removing the NAND Chip from the board&lt;/head&gt;
    &lt;p&gt;Now that we have identified the NAND chip, we fasten the board and tape off the remaining components with heat-shielding tape.&lt;/p&gt;
    &lt;p&gt;Usually getting the chip off of the board is just a matter of using hot air station and flux. However, you can see on the photos that the chip is actually glued to the main board with what is probably epoxy. That’s a thing you can do if you want to secure the chips more securely and not depend on the solder joints to hold your chip in place (and risk breaking them). Or you can do that just to the NAND chip to make it harder for researcher to pry off your NAND chip and dump your firmware.&lt;/p&gt;
    &lt;p&gt;Anyway, a few cuts with a sharp knife, some heat and a generous amount of flux later, the little bugger came off in one piece.&lt;/p&gt;
    &lt;p&gt;(And with it, a couple of extremely tiny resistors that I knocked off with my pliers and promptly lost. This main board is now broken. But don’t worry, through the magic of buying &lt;del&gt;two&lt;/del&gt; three of them, we can still fly the drone.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Identifying the data pins and communication protocol of the NAND chip&lt;/head&gt;
    &lt;p&gt;According to the data sheet of the MX35UF4GE4AD NAND chip, the flash chip can either come in a 24-pin BGA package or an 8-pin WSON package, which we have here. A quick look at the pin descriptions tell us that the NAND chip is communicating via SPI.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Pin Symbol&lt;/cell&gt;
        &lt;cell role="head"&gt;Pin Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CS#&lt;/cell&gt;
        &lt;cell&gt;Chip Select&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SI&lt;/cell&gt;
        &lt;cell&gt;Serial Data Input&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SO&lt;/cell&gt;
        &lt;cell&gt;Serial Data Output&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SCLK&lt;/cell&gt;
        &lt;cell&gt;Clock Input&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WP#&lt;/cell&gt;
        &lt;cell&gt;Write protection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;HOLD#&lt;/cell&gt;
        &lt;cell&gt;Hold&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;VCC&lt;/cell&gt;
        &lt;cell&gt;Power Supply (1.8 V)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;GND&lt;/cell&gt;
        &lt;cell&gt;Ground&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DNU&lt;/cell&gt;
        &lt;cell&gt;Do Not Use&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Well, let’s solder tiny copper cables to all of those pins and drown them in a bit of hot glue to stop them from breaking off.&lt;/p&gt;
    &lt;p&gt;Note that you can get a proper socket for 8-WSON chips into which you simply clamp the chip and which exposes easy-to-work-with breakout pins. None of our sockets we brought fit though, so we just did it the old-school way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Connecting the NAND chip to some kind of reading device&lt;/head&gt;
    &lt;p&gt;SPI is pretty easy to work with. We have two main data lines called SI and SO (Serial In/Out). You will also find them under the names “MOSI” and “MISO” (Master Out Slave In / Master In Slave Out). As the names of these suggest, SPI follows a master-slave architecture. The microcontroller drives the communication and the peripheral device reacts.&lt;/p&gt;
    &lt;p&gt;Fortunately, we are simulating the microcontroller-side of the communication, which means that we have a large amount of control. Specifically, we control the clock (SCLK). Sometimes it is hard to talk to embedded hardware because of the speed at which they operate. With SPI, however, we can slow down the clock to however fast we want the devices to talk.&lt;/p&gt;
    &lt;p&gt;Since SPI is a bus protocol, more than one slave device can be hooked up to a master device on the same data lines. To avoid collisions, each device is also assigned its own “Chip Select” line (CS). When the master device wants to talk to a specific slave device, it pulls the corresponding CS line low. Devices that have their CS line high won’t react at all.&lt;/p&gt;
    &lt;p&gt;Obviously there are fancy devices on the market that will make dumping a NAND chip via SPI pretty easy and straightforward. Problem is, we couldn’t get any of them to work. They either didn’t fit (physically), were too fast or failed for some other strange reason we didn’t understand. So we wrote our own dump script onto an ESP32 using the SPI commands in the data sheet and let it forward the data to our computer via the USB console.&lt;/p&gt;
    &lt;p&gt;Doing that, we ended up with a 544 MiB dump, containing 131,072 pages of 4096+256 bytes. (We will come back to that “+256” later on.)&lt;/p&gt;
    &lt;p&gt;Let’s dig into what this the flash dump contains with &lt;code&gt;binwalk&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Sweet! We get a working ASCII copyright string, so something must have worked. And at the end of the image we have a bunch of UBIFS images. That’s probably where all the juicy files are!&lt;/p&gt;
    &lt;p&gt;Let’s extract them with &lt;code&gt;dd&lt;/code&gt; and take a look inside with ubi_reader:&lt;/p&gt;
    &lt;p&gt;Hmm. That doesn’t work. Spoiler: The extracted image is broken.&lt;/p&gt;
    &lt;p&gt;Now if you’ve ever done something as hacky as this, you will know about a pesky little phenomenon that happens when you just solder copper wires onto a chip, stick that onto the ports of an ESP32 and do SPI communication - which has no built-in integrity checks.&lt;/p&gt;
    &lt;head rend="h4"&gt;Random Bit Flips&lt;/head&gt;
    &lt;p&gt;These are three dumps taken from the same NAND chip:&lt;/p&gt;
    &lt;p&gt;If you read 4 MiB of data from the chip, not all of the bits you receive are correct. And without any additional data, you have little way of knowing which ones are correct and which are not. If you are lucky, then the dumped data will still “work”, i.e., the contained file system will mount and you can browse files, but futher down the line you will have no way of knowing whether that weird function you’re reversing is actually weird or just the product of random bit flips messing up the CPU instructions.&lt;/p&gt;
    &lt;p&gt;A relatively simple yet time-consuming way to get around this: Read the flash often (at least three times) and hold a majority vote for every bit. Since the bit flips are random and not too prevalent, they are less likely to hit the same bit twice.&lt;/p&gt;
    &lt;p&gt;Hot tip: If you’re gonna work with python, use numpy and work on arrays and memory-mapped files. Otherwise this can take a lot of time and a lot of RAM - even for a 512 MiB flash dump.&lt;/p&gt;
    &lt;p&gt;But isn’t there a better way? Yes, there is. And - btw - even with completely correct majority voting, the flash content is still broken. But we’ll get to that.&lt;/p&gt;
    &lt;p&gt;Trying to work with the majority-voted dump:&lt;/p&gt;
    &lt;head rend="h2"&gt;Out-of-band bytes and ECC troubles&lt;/head&gt;
    &lt;p&gt;One thing we have have silently brushed aside for now: The NAND chip distinguishes between “user data” and “extra data”. In our dump above, we have naively concatenated it all together and assumed that a page size of 4096+256 bytes somehow makes sense. Of course, it doesn’t.&lt;/p&gt;
    &lt;p&gt;Also, this majority voting hack is obviously not the “correct” way to work with a NAND chip. And even the proper SoC mounted on the proper mainboard can’t run a system off of a flash chip that gives it random bit flips that it can’t detect or recover from. The problem is that NAND is just inherently imperfect storage. Majority voting only corrects for transmission errors during dumping, but does nothing to bit errors that are stored on the device! Bits on the storage might decay over time, or the CPU might also have some transmission errors during writing.&lt;/p&gt;
    &lt;p&gt;Of course this problem has been very well known for a long time, so manufacturers always include some extra space next to user data for “error correction”. These extra bytes are called “out-of-band” bytes. And they are used to implement Error Correction Codes (ECC).&lt;/p&gt;
    &lt;head rend="h3"&gt;ECC according to the NAND chip data sheet&lt;/head&gt;
    &lt;p&gt;The flash chip implements its own error correction algorithm by reserving some of the space for ECC. It is split into&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2048 blocks of&lt;/item&gt;
      &lt;item&gt;64 pages of&lt;/item&gt;
      &lt;item&gt;4096 user data bytes + 256 “extra” bytes (aka out-of-band bytes)&lt;/item&gt;
      &lt;item&gt;=&amp;gt; 512 MiB of user data (the chip is 4 Gb, not 4 GB)&lt;/item&gt;
      &lt;item&gt;=&amp;gt; 32 MiB of extra data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the chip-internal ECC is enabled, some of those extra bytes are used for ECC.&lt;/p&gt;
    &lt;p&gt;At this point, we naively assumed that each page would be&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;4096 bytes of user data followed by&lt;/item&gt;
      &lt;item&gt;256 (or less?) bytes of ECC covering the previous 4096 bytes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, we quickly found out that the sequences classified as “extra data” contain readable strings! That definitely suggests that this isn’t ECC data.&lt;/p&gt;
    &lt;p&gt;You can also see that there are parts within the user data where strings are suddenly cut off. This suggests that the ECC layout we assumed was wrong. It took us quite a while to figure out what exactly we missed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Entropy Analysis&lt;/head&gt;
    &lt;p&gt;Turns out, the ECC layout is not just 4096 bytes of user data followed by 256 bytes of ECC. If we put all pages next to each other and then calculate the entropy over each n-th byte of a page, we will find that there are multiple sections with high entropy:&lt;/p&gt;
    &lt;p&gt;Why are we looking at entropy? Well, because we expect the user data to have ASCII text (low entropy) every now and then and the ECC data to be mostly random-looking byte values (high entropy).&lt;/p&gt;
    &lt;p&gt;The graph we’re seeing up here suggests that there are sections of roughly 1 KiB of user data, followed by 28 bytes of ECC data. Specifically,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1028 B user data + 28 B ECC&lt;/item&gt;
      &lt;item&gt;1028 B user data + 28 B ECC&lt;/item&gt;
      &lt;item&gt;1028 B user data + 28 B ECC&lt;/item&gt;
      &lt;item&gt;1014 B user data + 28 B ECC&lt;/item&gt;
      &lt;item&gt;142 B unused&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why these stranges values? And which ECC algorithm is that? We can choose to ignore that and just extract the user data sections and stitch them together. I won’t spam you with more &lt;code&gt;binwalk&lt;/code&gt; and &lt;code&gt;dd&lt;/code&gt; screenshots and just tell you that that also won’t result in a readable UBIFS image. Fortunately, we find an explanation for these values in the next section!&lt;/p&gt;
    &lt;head rend="h3"&gt;ECC according to the SoC data sheet&lt;/head&gt;
    &lt;p&gt;At this point, we had already spent a lot of time fiddling with unstable reading setups and ECC layouts. And then we found that some further digging into the right documentation could have saved us a lot of time during our research. Because the SoC also does ECC. Not just the NAND chip. In fact, we can ignore the NAND chips ECC feature completely.&lt;/p&gt;
    &lt;p&gt;The SoC’s data sheet lists several possible ECC layouts. One of them is the following:&lt;/p&gt;
    &lt;p&gt;Well, that fits our findings perfectly, plus a BB (“bad blocks”) and CTRL (some kind of control bytes?) area that we didn’t identify before.&lt;/p&gt;
    &lt;p&gt;Using this diagram, we can cut out all the ECC, BB and CTRL sections and reconstruct the pure 512 MiB user data flash content.&lt;/p&gt;
    &lt;p&gt;Heyyy, look at that! We managed to extract a file system again - albeit with some error remaining. Let’s see what’s on it:&lt;/p&gt;
    &lt;p&gt;Hmm. Damn. The UBIFS image is now at least somewhat syntactically correct. But it is still broken enough to not have any files. Why could that be? Well, we are looking at exactly what the SoC would see after reading the data from the NAND chip. Plus that we have done majority voting on the bytes - so our version is even better than what the SoC would see.&lt;/p&gt;
    &lt;p&gt;But, there is no guarantee that there aren’t any random bit flips on the NAND chip, i.e., that random bit flipping happened during writing, desoldering or anytime between that! So, there seems to no way around actually implementing the ECC algorithm and correcting the bit flips on the flash dump. Problem is: What kind of ECC algorithm is the SoC running? Unfortunately, the datasheet is silent here, so we had to find out on our own.&lt;/p&gt;
    &lt;head rend="h3"&gt;A short primer on reverse engineering ECC algorithms&lt;/head&gt;
    &lt;p&gt;Typical ECC algorithms on NAND chips use BCH codes, which are parametrized by the following properties:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The amount of parity bits.&lt;/item&gt;
      &lt;item&gt;The correction capacity &lt;code&gt;t&lt;/code&gt;, i.e., how many simultaneous bit flips may appear in the same data block before the block is “too broken” and the ECC algorithm fails.&lt;/item&gt;
      &lt;item&gt;The primitive polynomial used in the equation. If you don’t know what this is, just think of it as an integer parameter for now.&lt;/item&gt;
      &lt;item&gt;Whether and how the data is transformed before the parity bits are calculated.&lt;/item&gt;
      &lt;item&gt;Whether and how the parity bits are transformed after they are calculated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can deduce (1) and (2) from our flash dump. For (3), (4), and (5) we have to either find the code of the SoC (if it is implement in software at all) and reverse engineer the ECC algorithm - or just bruteforce them.&lt;/p&gt;
    &lt;head rend="h4"&gt;Amount of parity bits&lt;/head&gt;
    &lt;p&gt;As we have seen in the SoC’s data sheet, we have 112 byte of ECC / parity bits. However, the fragmented layout on the flash suggests that we actually have 4 ECC groups of 28 byte, each covering a different part of the user data. Note that this is an educated guess and does not have to be true. If we’re not getting anywhere, we should consider dropping this assumption later on. But spoiler: We’re right about this.&lt;/p&gt;
    &lt;p&gt;This means that we have 224 parity bits (= 28 bytes).&lt;/p&gt;
    &lt;head rend="h4"&gt;Correction capacity&lt;/head&gt;
    &lt;p&gt;This part we can just calculate if we make one very realistic assumption. We have 1028 bytes of user data, which is 8224 bits. If we want to represent these 8224 bits as a binary polynomial, we need at least degree 14:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;2^13 =  8192&lt;/code&gt; &amp;lt; - Too small
&lt;code&gt;2^14 = 16384&lt;/code&gt; &amp;lt; - Fits!&lt;/p&gt;
    &lt;p&gt;This means our primitive polynomial needs to be at least degree 14 (&lt;code&gt;m &amp;gt;= 14&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The correction capacity is determined by the degree &lt;code&gt;m&lt;/code&gt; and the amount of parity bits. The more parity bits we have in relation to &lt;code&gt;m&lt;/code&gt;, the higher our correction capacity &lt;code&gt;t&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;t = parity_bits / m&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Now given that &lt;code&gt;parity_bits&lt;/code&gt; is fixed at 224 and assuming that the engineers chose &lt;code&gt;t&lt;/code&gt; to be maximal, we conclude that &lt;code&gt;m = 14&lt;/code&gt; and&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;t = 224 / 14 = 16&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;meaning that we can correct for up to 16 bit flips in the covered user data chunk. Anything more than that and the chunk is lost.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;t = 16&lt;/code&gt; also fits the description of the ECC section in the SoC’s data sheet: “16-Bit/1KB Error Correction Performance” (see our diagram above). So we are pretty certain that this assumption is correct.&lt;/p&gt;
    &lt;head rend="h4"&gt;Primitive polynomial&lt;/head&gt;
    &lt;p&gt;We don’t know that and we will have to bruteforce it. Given that it is a binary polynomial, it is usually represented as a bit vector or simply as an integer. Given that &lt;code&gt;m = 14&lt;/code&gt; we already know that our polynomial must have its 14th bit set and that the 14th bit is the highest bit that is set:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;2^14 &amp;lt;= prim_poly &amp;lt; 2^15&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;which is well within bruteforcing range.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pre-encode and post-encode transformations&lt;/head&gt;
    &lt;p&gt;There are a few transformations that are commonly applied to either the user data before calculating the parity bits (“encoding”) or applied to the parity bits after calculating them. Examples include&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reverse bit order&lt;/item&gt;
      &lt;item&gt;reverse byte order&lt;/item&gt;
      &lt;item&gt;swap nibbles&lt;/item&gt;
      &lt;item&gt;invert&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why you ask? Well, one of these combinations for example is very useful for NAND storage devices. You see, when NAND pages are erased and then read, their values are all &lt;code&gt;0xFF&lt;/code&gt;. Problem is, a page full of &lt;code&gt;0xFF&lt;/code&gt; will have&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;user data = &lt;code&gt;0xFFFF...&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ECC = &lt;code&gt;0xFFFF...&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and that is not a valid parity, meaning a cleanly erased page will be read as containing a lot of errors. That is because&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;parity(0xFF...) != 0xFF...&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;But we can pull a trick to make this work: Before encoding, invert the user data. And after encoding, invert the parity bits:&lt;/p&gt;
    &lt;p&gt;Notice that the parity of an all-zero page is all-zero:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;parity(0x00...) != 0x00...&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;So by implementing our ECC algorithm with these two inversions, we make a freshly erased page (&lt;code&gt;0xFF...&lt;/code&gt;) have a valid parity (&lt;code&gt;0xFF...&lt;/code&gt;) and the SoC’s error correction won’t need special handling for erased NAND pages.&lt;/p&gt;
    &lt;p&gt;Okay, back to the actual algorithm at hand here. How do we know that these two inversions are what the engineers actually chose? We don’t! We just try all these different transformations and see if one of them works. The amount of possible combinations of these 4 transformations is quite low and easily bruteforcable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Brute-forcing ECC parameters&lt;/head&gt;
    &lt;p&gt;In summary, to test our guessed parameters, we need a user data section without errors, generate its ECC and check if it matches the ECC that we read off of the NAND chip. Our bruteforce script thus has to do the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find a “good” user data section with no bit flips and the corresponding ECC section.&lt;/item&gt;
      &lt;item&gt;Iterate through all possible transformations.&lt;/item&gt;
      &lt;item&gt;Iterate through all possible primitive polynomials of degree 14&lt;/item&gt;
      &lt;item&gt;For each iteration, generate the ECC of the userdata section. If it matches the existing ECC, we have found the parameters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Picking a “good” user data section&lt;/head&gt;
    &lt;p&gt;We need a user data section and its corresponding ECC section without bit flips. But how do we know that a section does not have bit flips? We don’t! We could try to be clever here. One possible approach would be to pick a section with a lot of text and check if the text makes sense. But the lazy approach works just as well: Just try a lot of sections and hope that one of them is correct. More bruteforce. Yeah.&lt;/p&gt;
    &lt;p&gt;As you can see in the script above, we only look at the first section of every page and then move on to the next page entirely. We’re not completely sure yet which ECC bytes cover which user data sections - especially since the 4th section looks fragmented. But we will stick to our assumption that the first 1028 bytes of user data are covered by the first 28 bytes of ECC. Spoiler: We’re right about this. Another Spoiler: The first 3 pages have bit-flips in their first section. The 4th one is good.&lt;/p&gt;
    &lt;head rend="h4"&gt;Iterate through all possible transformations&lt;/head&gt;
    &lt;p&gt;We will try out 4 different transformations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reverse bit order&lt;/item&gt;
      &lt;item&gt;reverse byte order&lt;/item&gt;
      &lt;item&gt;swap nibbles&lt;/item&gt;
      &lt;item&gt;invert&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For these transformation we want all possible subsets and orderings, but without using the same transformation twice in one run.&lt;/p&gt;
    &lt;p&gt;Then we can run through all these combinations as both pre-transformations as well as post-transformations:&lt;/p&gt;
    &lt;p&gt;Note that some combinations are equivalent:&lt;/p&gt;
    &lt;p&gt;We don’t optimize for that though.&lt;/p&gt;
    &lt;head rend="h4"&gt;Iterate through all possible primitive polynomials of degree 14&lt;/head&gt;
    &lt;p&gt;There are three ways to do this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A simple and slow way&lt;/item&gt;
      &lt;item&gt;A math-heavy and fast way&lt;/item&gt;
      &lt;item&gt;A much better way that is about as simple as (1) and as fast as (2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, we went with (1) during our initial research because sometimes thinking just takes longer than computing inefficiently. Afterwards, I spent hours digging into polynomial algebra to come up with (2) and was very happy about it - only to also find (3) right afterwards which was a lot simpler and equally as good… Oh well, at least I got to freshen up on first- and second-semester linear algebra.&lt;/p&gt;
    &lt;p&gt;(1) Simple and slow The simple way would be to just try all polynomials of degree 14. In their integer representation, that’s all integers in &lt;code&gt;range(2**14, 2**15)&lt;/code&gt;
While this will eventually cover the correct primitive polynomial, it will also make bchlib crash the entire script with a SIGSEGV for a lot of non-primitive polynomials.&lt;/p&gt;
    &lt;p&gt;A quick-and-dirty workaround is to just spawn a new process for every candidate polynomial so your main script doesn’t die. And that’s what we did during initial research. It works - but the creation of over 16,000 processes makes this a bit slow. Not too slow to work with though. This approach works in practice.&lt;/p&gt;
    &lt;p&gt;(2) Math-heavy and fast The proper way to do this is to only pass primitive polynomials into BCH’s constructor. But how do we know if the polynomial that is represented by our integer is primitive? By doing lots of math. If you’re not familiar with polynomial algebra (like I was) but really want to know how this works, read the section A brief detour into polynomial algebra in the addendum.&lt;/p&gt;
    &lt;p&gt;Spoiler: It is a lot of thinking work and only about 5% faster than (3) in my tests.&lt;/p&gt;
    &lt;p&gt;(3) Simple and fast Turns out, bchlib only crashes for polynomials with a constant term of 0, i.e., even integers. So if we use &lt;code&gt;range(2**14 + 1, 2**15, 2)&lt;/code&gt;, then it just works without having to fiddle with multiprocessing or math.&lt;/p&gt;
    &lt;p&gt;This will still throw a runtime error for a lot of non-primitive polynomials but we can catch that via try-except:&lt;/p&gt;
    &lt;head rend="h4"&gt;Checking the generated ECC&lt;/head&gt;
    &lt;p&gt;This is straight-forward and self-explanatory.&lt;/p&gt;
    &lt;p&gt;You can find the full ECC Bruteforce Script in the addendum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Restoring the full firmware&lt;/head&gt;
    &lt;p&gt;Now that we have a working ECC setup, let’s reassemble the entire firmware! There is just one little detail that we still need to find out:&lt;/p&gt;
    &lt;p&gt;Which parts of user data are covered by which parts of ECC? We already confirmed that the first user data section is covered by the first 28 bytes of ECC. And the same turns out to be true for the second and third user data section. The fourth section is a bit tricky: It is 928+84 bytes of user data long, with additional BB and CTRL bytes around. What is that about? Turns out, a bit of trial-and-error and looking at the SoC’s data sheet revealed how ECC works for that section.&lt;/p&gt;
    &lt;p&gt;Now we just need to apply that to every page and - voilà - full firmware dump. :tada:&lt;/p&gt;
    &lt;p&gt;The Final Restore Script can be found in the addendum.&lt;/p&gt;
    &lt;p&gt;If we look at the binwalk output for that file, it is much better and looks like it is actually free of errors:&lt;/p&gt;
    &lt;p&gt;When trying to extract the first ubifs image with ubi_reader, we actually get a working file system!&lt;/p&gt;
    &lt;p&gt;ubi_reader still throws an error in the later segments of UBIFS image but this extraction is good enough to start reverse engineering the successfully extracted files. Notably, it is enough to reverse engineer the firmware decryption!&lt;/p&gt;
    &lt;p&gt;Stay tuned for Part 2 of our drone hacking blogpost where we dive into the reverse engineering and vulnerability analysis of the Potensic Atom 2!&lt;/p&gt;
    &lt;head rend="h2"&gt;Addendum&lt;/head&gt;
    &lt;head rend="h3"&gt;Final Restore Script&lt;/head&gt;
    &lt;head rend="h3"&gt;ECC Bruteforce Script&lt;/head&gt;
    &lt;head rend="h3"&gt;Primitive Binary Polynomial Generator&lt;/head&gt;
    &lt;head rend="h3"&gt;Fun Fuckups&lt;/head&gt;
    &lt;head rend="h2"&gt;A brief detour into polynomial algebra&lt;/head&gt;
    &lt;p&gt;If you are like me and you’re not really familiar with polynomial algebra, it makes sense to talk about related concepts in the integer world first and then move on to their counterparts in the polynomial world. This helps to get an intuition of what we are actually dealing with.&lt;/p&gt;
    &lt;p&gt;I assume that you are generally familiar with modular arithmetic and prime numbers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Integers and Modulo Rings&lt;/head&gt;
    &lt;p&gt;For now, we are working on the field of integers, i.e., , mathematically denoted .&lt;/p&gt;
    &lt;p&gt;When we introduce a modulus, we get . Explanation for the notation:&lt;/p&gt;
    &lt;p&gt;is some modulus. Not necessary prime at this point. Just an integer - a member of .&lt;/p&gt;
    &lt;p&gt;is all multiples of . So . If were , this would be .&lt;/p&gt;
    &lt;p&gt;Now means: The field but treat all elements as equivalent if they are a multiple of apart - meaning their difference is in . If were , then and would be equivalent, because their difference is , which is a multiple of and thus in .&lt;/p&gt;
    &lt;p&gt;This is exactly what “mod 7” is:&lt;/p&gt;
    &lt;p&gt;So is just all of .&lt;/p&gt;
    &lt;p&gt;Note that this collapses the infinite field of all possible integers down to a finite set of equivalence classes. In , all numbers are either in or are equivalents of one of those. So for practical purposes, there are only possible values in .&lt;/p&gt;
    &lt;p&gt;What do we need primes for? Well, has a practical problem: Sometimes multiplying two things results in a zero. Example for :&lt;/p&gt;
    &lt;p&gt;That is bad if we want to do a lot of multiplication within our modulo ring. Because if we ever accidentally hit a multiple of , we will get and that point it doesn’t matter what we multiply onto that - it will stay . So there are a lot of possible values that all collapse into the same when multiplied with certain numbers.&lt;/p&gt;
    &lt;p&gt;In the field of all integers, , we don’t have that problem. As long as we don’t multiply with itself, the results of a multiplication will never be . Good thing there is a solution for that: Using a prime modulus.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prime Numbers, Finite Fields, and Cycles&lt;/head&gt;
    &lt;p&gt;Prime numbers have the nice feature that the modulo rings they induce, , are fields and not just rings, meaning addition and multiplication work just as well as they do in . In , multiplying by something other than will never result in a multiple of - because is prime and you can’t reach a prime from another number through multiplication. (Multiplying by itself doesn’t count, because .)&lt;/p&gt;
    &lt;p&gt;Because is a field and it has a finite amount of elements , it is called a “finite field” or a “Galois field” and sometimes denoted instead of . To be super precise, is a generalization and means “any finite field with elements”. just happens to be one of those - and is the most popular one.&lt;/p&gt;
    &lt;p&gt;Now we go a step further and look at a concept called primitive root. Before explaining that, let’s take a look at a use case for them first.&lt;/p&gt;
    &lt;p&gt;Imagine you want a pseudorandom permutation of , i.e., you don’t want the sequence but a more random-looking sequence tha still hits all of these numbers. It doesn’t have to be cryptographically random or unpredictable. It just needs to look at bit random - perhaps to de-cluster memory writes for better wear-leveling. Primitive roots give us a nice implementation for that.&lt;/p&gt;
    &lt;p&gt;In , multiplication never yields unless you multiply by . That means you can, e.g., keep doubling a number and will never accidentally hit :&lt;/p&gt;
    &lt;p&gt;Oh look at that, a cycle! This is what will always happen in a finite field. When you keep multiplying by the same number, you will eventually reach the number you started with. And at that point, you are in a cycle.&lt;/p&gt;
    &lt;p&gt;is a bit impractical though because its induced cycle only ever hits the numbers and never hits . Note that no cycle will ever hit , so the maximum cycle we can possible get with a modulus of is cycle length .&lt;/p&gt;
    &lt;head rend="h3"&gt;Primitive Roots and Prime Factors&lt;/head&gt;
    &lt;p&gt;And there are indeed numbers that generate a full cycle!&lt;/p&gt;
    &lt;p&gt;These numbers, numbers that induce a full cycle in a finite field, are called primitive roots. and are each primitive roots modulo . Since their induced cycle has a length of , the so-called order of and is .&lt;/p&gt;
    &lt;p&gt;More generally, a primitive root modulo some prime is a number whose order is , i.e., whose induced cycle has a length of . In that case . Expressed more formally: is a primitive root modulo if and only if the smallest for which is true, is&lt;/p&gt;
    &lt;p&gt;Now then how would you best check if a number is a primitive root modulo ? The obvious solution is to just count upwards from 1 through and check each time. That works but it can take a long time for big numbers. Turns out, we don’t have to check every possible from through . To understand that, take a look at from before:&lt;/p&gt;
    &lt;p&gt;We can see that the cycle has length and contains the numbers . We don’t see the numbers . But what happens when we use but we start at ?&lt;/p&gt;
    &lt;p&gt;Again, a cycle of ! And this time we’ve seen all the remaining numbers . If we instead start at or , we will get the same cycle over . So splits the entire finite field excluding 0 into two subsets: and .&lt;/p&gt;
    &lt;p&gt;Let’s look at another example: and try to find all cycles.&lt;/p&gt;
    &lt;p&gt;So partitions the non-zero elements into `.&lt;/p&gt;
    &lt;p&gt;Notice how all those cycles induced by the same number always have the same size? If you think about it, then that makes perfect sense. When you have but you “start at ”, you’re basically just taking the regular cycle and multiply its elements by . So the resulting cycle must have the same length: .&lt;/p&gt;
    &lt;p&gt;And this isn’t a coincidence. In fact, all cycles induced by the same number always have the same length. And that number always partitions the entire finite field without into disjoint sets of the same size.&lt;/p&gt;
    &lt;p&gt;How does that help us? Well, for it makes the cycle lengths and impossible! Because you can’t cover all non-zero elements by splitting them into sets of size or . More generally, cycles lengths (and thus orders) must always be a divisor of , i.e., the amount of all non-zero elements.&lt;/p&gt;
    &lt;p&gt;This is called Lagrange’s theorem: The order of the subgroup divides the order of the whole group.&lt;/p&gt;
    &lt;p&gt;So to check if is a primitive root, we don’t have to check all from through . We only have to check all divisors of ! Also, we obviously don’t have to check . The only number for which is itself, which always induces a cycle of and is thus never a primitive root for .&lt;/p&gt;
    &lt;p&gt;So we’re down to only having to check all divisors of that are larger than . That already eliminates most of the candidates.&lt;/p&gt;
    &lt;p&gt;But we can go even further!&lt;/p&gt;
    &lt;head rend="h3"&gt;Fast Primitivity Check&lt;/head&gt;
    &lt;p&gt;We only need to check some of the divisors. This is where prime factorization comes into play. Let’s say that we have the prime factors of . We will call these prime factors . Note that itself does not have any prime factors because - well - it is a prime number. But does have prime factors. In fact, will always be one of those prime factors because must be odd to be a prime and must therefore be even.&lt;/p&gt;
    &lt;p&gt;Now what if I told you that we only need to check the divisors of that we can obtain by dividing through a prime factor. Specifically, we only need to check&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;p&gt;Why is that? Well, first of all, notice that the “other divisors” are themselves divisors of either or :&lt;/p&gt;
    &lt;p&gt;And if , then that implies that&lt;/p&gt;
    &lt;p&gt;and so on.&lt;/p&gt;
    &lt;p&gt;Why? Well, because if were equal to one, then&lt;/p&gt;
    &lt;p&gt;and that is a contradiction! You can prove the same for any in general if you want. The general proof works just like this example.&lt;/p&gt;
    &lt;p&gt;Alright, so we only have to check all possible with being a prime factor of .&lt;/p&gt;
    &lt;p&gt;Here is a generator function that will return all primitive roots for a given prime modulus :&lt;/p&gt;
    &lt;p&gt;And you know what? Armed with this, we can not just find primitive roots but also primitive polynomials! We just need to translate this concept to polynomials!&lt;/p&gt;
    &lt;p&gt;… which is a bit tricky.&lt;/p&gt;
    &lt;head rend="h3"&gt;Binary Polynomials, Irreducibility and Primitive Elements&lt;/head&gt;
    &lt;p&gt;Alright, first things first: Polynomials. They can look something like this:&lt;/p&gt;
    &lt;p&gt;And through the black magic of math, we can treat these polynomials as multidimensional numbers:&lt;/p&gt;
    &lt;p&gt;And on these, we can perform the same kind of arithmetic as on integers: Addition, Multiplication, Subtraction, Division - and thus Modulo. Dividing one polynomial by another sounds odd? It is. We’ll skip the details here since we’ve already derailed enough. Just know that the intuition from integer arithmetic carries over to polynomials.&lt;/p&gt;
    &lt;p&gt;Now keep in mind that we are working with binary polynomials, i.e., polynomials whose coefficients are either or . And we can represent those as integers. For example, represents the polynomial&lt;/p&gt;
    &lt;p&gt;We use these binary polynomials because they have a numbers of nice properties that things like BCH error correction codes rely on. For example, we can represent all binary strings of length as a -degree polynomial with binary coefficients. For integers, we’d represent those strings as integers from to . But we would have problems doing modular arithmetic on those strings because isn’t necessarily prime and then isn’t a field. For polynomials, there are nice and efficient ways to build a field over -degree binary polynomials.&lt;/p&gt;
    &lt;p&gt;Throughout this section we have to keep in mind that the integer representation ( in the example above) is just a representation of the polynomial in memory. The polynomial is not an integer and we can’t just do regular integer arithmetic like addition and multiplication with Python’s &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;*&lt;/code&gt; with it. Polynomials have their own arithmetic and they work differently.&lt;/p&gt;
    &lt;p&gt;Since we are dealing with binary polynomials, the coefficients of the polynomials are all either or . More formally, the coefficients are elements of , which is basically the same as or “mod 2”.&lt;/p&gt;
    &lt;p&gt;The set of all binary polynomials is called . In CompSci terms, these are all possible bit arrays.&lt;/p&gt;
    &lt;p&gt;So since we’re moving from working on integers to working on binary polynomials, is our binary polynomial-equivalent of , with being all integers and being all binary polynomials.&lt;/p&gt;
    &lt;p&gt;While we used to represent a single integer in , we will use to represent a single polynomial in .&lt;/p&gt;
    &lt;p&gt;Similar to how is the set of all multiples of n, is the set of all multiples of - notice the double-paranthesis.&lt;/p&gt;
    &lt;p&gt;And similar to how is but treating two integers as equivalent if their difference is a multiple of , is but treating two polynomials as equivalent if their difference is a multiple of . It is essentially “mod ” in polynomial world.&lt;/p&gt;
    &lt;p&gt;If we have an integer and can’t be divided by another integer, then is prime. Analogously, if we have a polynomial and can’t be reduced by another polynomial, then is irreducible.&lt;/p&gt;
    &lt;p&gt;If is prime, then there is at least one primitive root in , so that spans the entire . If is irreducible, then there is at least one primitive element in so that spans the entire .&lt;/p&gt;
    &lt;p&gt;(Side note: “primitive root” is a legacy term only used for . “primitive element” is the general term. They mean the same thing.)&lt;/p&gt;
    &lt;p&gt;Also, while the order of is just , the order of is , which is the amount of all possible -bit arrays.&lt;/p&gt;
    &lt;p&gt;Now we are almost there! We have already come far enough where we can recognize that an irreducible polynomial is analogous to a prime integer .&lt;/p&gt;
    &lt;p&gt;Now, what is a primitive polynomial then?&lt;/p&gt;
    &lt;head rend="h3"&gt;Primitive Polynomials&lt;/head&gt;
    &lt;p&gt;Well, that’s simple! A primitive polynomial is an irreducible polynomial with one additional requirement:&lt;/p&gt;
    &lt;p&gt;The super simple polynomial must be a primitive element, i.e., must span the entire .&lt;/p&gt;
    &lt;p&gt;Why is that useful? Well, because that means that every non-zero polynomial in can be represented as for some integer . So every non-zero polynomial can be represented as an integer and we have a random-looking permutation of all non-zero binary polynomials in .&lt;/p&gt;
    &lt;p&gt;And that is what BCH codes use and why we require a primitive polynomial.&lt;/p&gt;
    &lt;p&gt;Now, how do we calculate all primitive polynomials then? We don’t. There are infinitely many. But we can calculate all primitive polynomials of some specific degree ! And how do we do that? Well, using what we have already built for finding primitive roots for integers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Iterate through all possible polynomials of degree . For each candidate , (step 1) &lt;list rend="ul"&gt;&lt;item&gt;check if irreducible. Discard if not. (step 2)&lt;/item&gt;&lt;item&gt;check if (the simple polynomial with integer representation 2) is a primitive element in the finite field induced by . (step 3) &lt;list rend="ul"&gt;&lt;item&gt;the wanted cycle length is the amount of all possible polynomials of degree :&lt;/item&gt;&lt;item&gt;get all prime factors of&lt;/item&gt;&lt;item&gt;check if for all prime factors&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For (step 1), we want a primitive polynomial with degree , so we only consider polynomials where their m-th coefficient is 1 and all higher-order coefficients are 0. This means that their integer representations are in&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;range(2^m, 2^(m+1))&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To speed it up by factor 2: We can’t have any polynomials where the constant term, i.e., the lowest-order coefficient is zero. Example:&lt;/p&gt;
    &lt;p&gt;(note that there is no at the end)&lt;/p&gt;
    &lt;p&gt;That’s because every polynomial with a zero constant term is divisible by the polynomial and is thus not irreducible and thus not primitive. So we only iterate over polynomials with a constant term of . In the integer representation, those are the odd integers, so we use&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;range(2^m + 1, 2^(m-1), 2)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;For (step 2), checking whether a polynomial is irreducible is the analogue to checking whether an integer is prime.&lt;/p&gt;
    &lt;p&gt;Ruling out even integer representations of polynomials in (1) skips half the possible candidates because those are all reducible. Unfortunately, there will still be plenty of reducible polynomials left. Just like there are a lot of non-prime numbers among the odd integers.&lt;/p&gt;
    &lt;p&gt;For a full irreducibility test, we use Rabin’s Test. The implementation has a similar structure as the Rabin-Miller Test for integers. We won’t cover either here because this section is long enough already.&lt;/p&gt;
    &lt;p&gt;For (step 3), we do the same as for primitive roots in integer-world, except we use a polynomial-compatible &lt;code&gt;pow&lt;/code&gt; function:&lt;/p&gt;
    &lt;head rend="h3"&gt;Finding all Primitive Polynomials&lt;/head&gt;
    &lt;p&gt;And now we can finally compute all the primitive polynomials with degree 14. The full script is attached in the addendum. It takes about 1 second (single-threaded, on my laptop) to list all 756 primitive polynomials for degree 14. Yes - there are only 756!&lt;/p&gt;
    &lt;p&gt;And how much faster does this make our script compared to the naive &lt;code&gt;range(2**14 + 1, 2**15, 2)&lt;/code&gt;
that lists half of all possible binary polynomials of degree 14?&lt;/p&gt;
    &lt;p&gt;About 1 second on a single-threaded run on my laptop…&lt;/p&gt;
    &lt;p&gt;Turns out just throwing a bunch of mostly non-primitive polynomials against bchlib and catching the exception is just as fast as doing it properly…&lt;/p&gt;
    &lt;p&gt;Oh, and you know what? You could have also just pulled a list of all primitive polynomials of degree 14 from the internet. Because, well, we aren’t the first to generate that list.&lt;/p&gt;
    &lt;p&gt;But hey, we learned some math.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://neodyme.io/en/blog/drone_hacking_part_1/"/><published>2026-01-17T02:35:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656358</id><title>Show HN: Streaming gigabyte medical images from S3 without downloading them</title><updated>2026-01-17T12:19:03.784888+00:00</updated><content>&lt;doc fingerprint="286dba183daf5de1"&gt;
  &lt;main&gt;
    &lt;p&gt;A modern, cloud-native tile server for Whole Slide Images. One command to start serving tiles directly from S3.&lt;/p&gt;
    &lt;code&gt;# Installation (requires Rust, see alternatives below)
cargo install wsi-streamer

# On your local machine
wsi-streamer s3://my-slides-bucket --s3-region eu-west-3&lt;/code&gt;
    &lt;p&gt;That's it. No configuration files, no local storage, no complex setup. Open &lt;code&gt;http://localhost:3000/view/sample.svs&lt;/code&gt; in your browser to view a slide.&lt;/p&gt;
    &lt;p&gt;Whole Slide Images are large (1-10GB+) and typically live in object storage. Traditional viewers require downloading entire files before serving a single tile. WSI Streamer takes a different approach: it understands slide formats natively, fetches only the bytes needed via HTTP range requests, and returns JPEG tiles immediately.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Range-based streaming — fetches only the bytes needed for each tile, no local files&lt;/item&gt;
      &lt;item&gt;Built-in viewer — OpenSeadragon-based web viewer with pan, zoom, and dark theme&lt;/item&gt;
      &lt;item&gt;Native format support — Rust parsers for Aperio SVS and pyramidal TIFF&lt;/item&gt;
      &lt;item&gt;Production-ready — HMAC-SHA256 signed URL authentication&lt;/item&gt;
      &lt;item&gt;Multi-level caching — slides, blocks, and encoded tiles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install from crates.io:&lt;/p&gt;
    &lt;code&gt;cargo install wsi-streamer&lt;/code&gt;
    &lt;p&gt;Or build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/PABannier/WSIStreamer.git
cd WSIStreamer
cargo build --release&lt;/code&gt;
    &lt;p&gt;Or run with Docker:&lt;/p&gt;
    &lt;code&gt;# Pull from GitHub Container Registry
docker run -p 3000:3000 -e WSI_S3_BUCKET=my-bucket ghcr.io/pabannier/wsistreamer:latest

# Or use Docker Compose for local development with MinIO
docker compose up --build&lt;/code&gt;
    &lt;code&gt;# Serve slides from S3
wsi-streamer s3://my-slides

# Custom port
wsi-streamer s3://my-slides --port 8080

# S3-compatible storage (MinIO, etc.)
wsi-streamer s3://slides --s3-endpoint http://localhost:9000&lt;/code&gt;
    &lt;code&gt;# List slides
curl http://localhost:3000/slides

# Get slide metadata
curl http://localhost:3000/slides/sample.svs

# Fetch a tile (level 0, position 0,0)
curl http://localhost:3000/tiles/sample.svs/0/0/0.jpg -o tile.jpg

# Get thumbnail
curl "http://localhost:3000/slides/sample.svs/thumbnail?max_size=256" -o thumb.jpg&lt;/code&gt;
    &lt;code&gt;# Enable HMAC-SHA256 authentication
wsi-streamer s3://my-slides --auth-enabled --auth-secret "$SECRET"

# Generate signed URLs
wsi-streamer sign --path /tiles/slide.svs/0/0/0.jpg --secret "$SECRET" --base-url http://localhost:3000&lt;/code&gt;
    &lt;p&gt;The web viewer handles authentication automatically when enabled.&lt;/p&gt;
    &lt;code&gt;# Check S3 connectivity
wsi-streamer check s3://my-slides

# List available slides
wsi-streamer check s3://my-slides --list-slides

# Test a specific slide
wsi-streamer check s3://my-slides --test-slide sample.svs&lt;/code&gt;
    &lt;p&gt;All options can be set via CLI flags or environment variables:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Env Var&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--host&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_HOST&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0.0.0.0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Bind address&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--port&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_PORT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTTP port&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-bucket&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_BUCKET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;S3 bucket name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-endpoint&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_ENDPOINT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Custom S3 endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-region&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_REGION&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;us-east-1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;AWS region&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auth-enabled&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_AUTH_ENABLED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable authentication&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auth-secret&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_AUTH_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;HMAC secret key&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--cache-slides&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CACHE_SLIDES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Max slides in cache&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--cache-tiles&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CACHE_TILES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100MB&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Tile cache size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--jpeg-quality&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_JPEG_QUALITY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;80&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JPEG quality (1-100)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--cors-origins&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CORS_ORIGINS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;any&lt;/cell&gt;
        &lt;cell&gt;Allowed CORS origins&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;wsi-streamer --help&lt;/code&gt; for full details.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /health&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Health check&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /view/{slide_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Web viewer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /tiles/{slide_id}/{level}/{x}/{y}.jpg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fetch tile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List slides&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Slide metadata&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}/thumbnail&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Thumbnail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}/dzi&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;DZI descriptor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;See API_SPECIFICATIONS.md for complete documentation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Extensions&lt;/cell&gt;
        &lt;cell role="head"&gt;Compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Aperio SVS&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.svs&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JPEG, JPEG 2000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pyramidal TIFF&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;JPEG, JPEG 2000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Files must be tiled (not stripped) and pyramidal.&lt;/p&gt;
    &lt;p&gt;MIT. See LICENSE.&lt;/p&gt;
    &lt;p&gt;Issues and pull requests welcome. See CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/PABannier/WSIStreamer"/><published>2026-01-17T08:46:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656552</id><title>ClickHouse Acquires Langfuse</title><updated>2026-01-17T12:19:03.598118+00:00</updated><content>&lt;doc fingerprint="2bee90517ddf277e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Langfuse joins ClickHouse&lt;/head&gt;
    &lt;p&gt;Our goal continues to be building the best LLM engineering platform&lt;/p&gt;
    &lt;p&gt;ClickHouse has acquired Langfuse.&lt;/p&gt;
    &lt;p&gt;If you’re reading this as a Langfuse user, your first question is probably: What does this mean for me?&lt;/p&gt;
    &lt;p&gt;Our roadmap stays the same, our goal continues to be building the best LLM engineering platform, and we remain committed to open source and self-hosting. There are no immediate changes to how you use Langfuse and how you can reach out to us.&lt;/p&gt;
    &lt;p&gt;What does change is our ability to move faster. With ClickHouse behind us, we can invest more deeply into performance, reliability, and our roadmap that helps teams build and improve AI applications in production.&lt;/p&gt;
    &lt;head rend="h2"&gt;What stays the same&lt;/head&gt;
    &lt;p&gt;This is the section we would want to read first, too.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Langfuse stays open source and self‑hostable. There are no planned changes to licensing. As you know, we leaned heavily into OSS over the last years.&lt;/item&gt;
      &lt;item&gt;Langfuse Cloud keeps running as‑is. Same product, same endpoints, same experience.&lt;/item&gt;
      &lt;item&gt;Support stays the same. Same channels, same SLAs for existing customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What gets better now&lt;/head&gt;
    &lt;p&gt;Joining Clickhouse compresses years of operational learning into immediate, real customer benefits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More engineering leverage on the hardest parts. Langfuse is a data‑intensive product. Working closely with the ClickHouse engineering team helps us push performance and reliability.&lt;/item&gt;
      &lt;item&gt;Faster progress on enhanced enterprise-grade compliance and security, with the help of Clickhouse’s resources.&lt;/item&gt;
      &lt;item&gt;Learning from Clickhouse’s customer success and support playbook. This puts us years ahead and allows us to spend more time on what we really care about: our users.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;A quick look back&lt;/head&gt;
    &lt;p&gt;The longer version of how we got here is in our handbook.&lt;/p&gt;
    &lt;p&gt;Langfuse started the same way many LLM products start: we were building agents ourselves. And we constantly ran into the same problems.&lt;/p&gt;
    &lt;p&gt;Building LLM apps is easy to demo and hard to run in production. Debugging is different, quality is non‑deterministic, and the iteration loop is messy. When we did Y Combinator in early 2023, we saw this every week, both in our own projects and in what other founders in our cohort were working on.&lt;/p&gt;
    &lt;p&gt;So we built a duct tape version of what we wished existed: tracing and evaluation primitives that are easy to add, easy to self‑host, and actually useful for iterating.&lt;/p&gt;
    &lt;p&gt;The very first version was intentionally simple. It ran on Postgres, because speed of shipping mattered more than theoretical scaling. That got us to a real product and a real community fast.&lt;/p&gt;
    &lt;p&gt;Then people actually started to use the product more than we could have imagined.&lt;/p&gt;
    &lt;p&gt;As adoption grew, Postgres became the bottleneck for the workloads Langfuse needed to support (high‑throughput ingestion + fast analytical reads). With Langfuse v3, we switched the core data layer to ClickHouse to make Langfuse scale for production workloads, both in Cloud and self‑hosted deployments.&lt;/p&gt;
    &lt;p&gt;And if you like infrastructure deep dives, here’s the v3 migration write‑up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why join ClickHouse&lt;/head&gt;
    &lt;p&gt;There are a lot of ways this could have gone. We didn’t plan to sell the company. Actually, we had Term Sheets for a great Series A and were looking forward to some days off over Christmas after an intense year.&lt;/p&gt;
    &lt;p&gt;What changed wasn’t our conviction in Langfuse, it was realizing how much faster we can go together with ClickHouse, while staying true to what makes Langfuse work: open source, self-hosting, and a product that’s built for real production workloads.&lt;/p&gt;
    &lt;head rend="h3"&gt;A shared history (before the acquisition)&lt;/head&gt;
    &lt;p&gt;This dialogue didn’t start with a term sheet. Because Langfuse runs on ClickHouse, we naturally ended up collaborating early and often.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’ve always been closely in touch with many teams at ClickHouse: sharing feedback with the database team, and using new features to make Langfuse more reliable. For example, compute-compute separation helps us to reduce the risk of noisy-neighbours on Langfuse Cloud.&lt;/item&gt;
      &lt;item&gt;Langfuse Cloud is a large customer of ClickHouse Cloud.&lt;/item&gt;
      &lt;item&gt;Teams at ClickHouse use Langfuse to improve their agentic applications.&lt;/item&gt;
      &lt;item&gt;We invested heavily in ClickHouse-backed self-hosting: documentation, templates, and deployment patterns, and collaborated closely with ClickHouse on improving that experience.&lt;/item&gt;
      &lt;item&gt;As a result, Langfuse introduced thousands of teams to ClickHouse when upgrading from Langfuse v2 to v3.&lt;/item&gt;
      &lt;item&gt;We’ve done community meetups together: a ClickHouse meetup at our Berlin office, another one in San Francisco, and an OpenHouse talk in Amsterdam.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Langfuse runs on ClickHouse, ClickHouse uses Langfuse to optimize its agentic products, we share lots of customers and OSS deployments; that gives ClickHouse every incentive to keep Langfuse fast, reliable, and boringly dependable at scale.&lt;/p&gt;
    &lt;p&gt;So in many ways, we operated like long-term partners. This acquisition is a way to make that partnership permanent — and invest aggressively together.&lt;/p&gt;
    &lt;p&gt;Max shared on how we use ClickHouse to keep product performance ahead of demand at ClickHouse Open House (recording) in Amsterdam.&lt;/p&gt;
    &lt;head rend="h3"&gt;Culture and engineering fit&lt;/head&gt;
    &lt;p&gt;The first time we met Aaron, Yury, Alexey, Tanya, Ryadh, and Pete in-person ended up in a long lunch in Amsterdam. It became obvious we share a similar view on building great developer tooling, how that drives everything within our companies, and how fast analytics is increasingly foundational for building and optimizing agentic products.&lt;/p&gt;
    &lt;p&gt;We already knew that ClickHouse is one of the best infrastructure engineering teams in the world. More importantly, the engineering culture feels like an instant match:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;open-source identity and stewardship&lt;/item&gt;
      &lt;item&gt;developer-first product instincts&lt;/item&gt;
      &lt;item&gt;performance and reliability as product features (not afterthoughts)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The whole Langfuse team will join ClickHouse to continue building Langfuse. All of these aspects were important to us and we couldn’t be more excited.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we’re focused on next&lt;/head&gt;
    &lt;p&gt;Our north star doesn’t change: help teams ship useful, reliable agents by closing the loop from production data to better prompts, evaluations, and product decisions.&lt;/p&gt;
    &lt;p&gt;Concretely, we’re investing in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Production monitoring and analytics for real agent systems (not just offline evals).&lt;/item&gt;
      &lt;item&gt;Workflows across tracing, labeling, and experiments so iteration loops get shorter.&lt;/item&gt;
      &lt;item&gt;More performance and scale—especially for large self‑hosted and enterprise deployments.&lt;/item&gt;
      &lt;item&gt;More polish (UI/UX, developer experience, and docs) so the product stays simple even as the space gets more complex.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can always follow along on the public roadmap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you&lt;/head&gt;
    &lt;p&gt;Langfuse exists because the community pushed it forward, through GitHub issues, PRs, feedback, and lots of Slack messages and spontaneous calls to dig into a product feature together.&lt;/p&gt;
    &lt;p&gt;We’re grateful for the trust you’ve put in us. Joining ClickHouse is our way of honoring that trust by putting more resources behind the thing we care about most: building a product you can rely on.&lt;/p&gt;
    &lt;p&gt;We’re excited for what’s next!&lt;lb/&gt; Max, Clemens, and Marc&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Is Langfuse still open source?&lt;lb/&gt;Yes. No licensing changes planned.&lt;/p&gt;
    &lt;p&gt;Can I still self‑host Langfuse?&lt;lb/&gt;Yes. Self‑hosting is a first‑class path.&lt;/p&gt;
    &lt;p&gt;Does anything change for Langfuse Cloud customers today?&lt;lb/&gt;No. Same product, same endpoints, same contracts.&lt;/p&gt;
    &lt;p&gt;Where do I go for support?&lt;lb/&gt;No changes: https://langfuse.com/support&lt;/p&gt;
    &lt;p&gt;Will the Langfuse team stay on Langfuse?&lt;lb/&gt;Yes. The team is joining ClickHouse and will keep building Langfuse. Also, we continue hiring in Berlin and SF.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join the discussion&lt;/head&gt;
    &lt;p&gt;If you have any other questions, let’s discuss together on GitHub Discussions.&lt;/p&gt;
    &lt;p&gt;If you’re an enterprise customer and have additional questions, feel free to reach out to enterprise@langfuse.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://langfuse.com/blog/joining-clickhouse"/><published>2026-01-17T09:15:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656834</id><title>Map To Poster – Create Art of your favourite city</title><updated>2026-01-17T12:19:02.948885+00:00</updated><content>&lt;doc fingerprint="a8f7d87e9ce1dad8"&gt;
  &lt;main&gt;
    &lt;p&gt;Generate beautiful, minimalist map posters for any city in the world.&lt;/p&gt;
    &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;
    &lt;code&gt;python create_map_poster.py --city &amp;lt;city&amp;gt; --country &amp;lt;country&amp;gt; [options]&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Short&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--city&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-c&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;City name&lt;/cell&gt;
        &lt;cell&gt;required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--country&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-C&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Country name&lt;/cell&gt;
        &lt;cell&gt;required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--theme&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-t&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Theme name&lt;/cell&gt;
        &lt;cell&gt;feature_based&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--distance&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-d&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Map radius in meters&lt;/cell&gt;
        &lt;cell&gt;29000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--list-themes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List all available themes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Iconic grid patterns
python create_map_poster.py -c "New York" -C "USA" -t noir -d 12000           # Manhattan grid
python create_map_poster.py -c "Barcelona" -C "Spain" -t warm_beige -d 8000   # Eixample district

# Waterfront &amp;amp; canals
python create_map_poster.py -c "Venice" -C "Italy" -t blueprint -d 4000       # Canal network
python create_map_poster.py -c "Amsterdam" -C "Netherlands" -t ocean -d 6000  # Concentric canals
python create_map_poster.py -c "Dubai" -C "UAE" -t midnight_blue -d 15000     # Palm &amp;amp; coastline

# Radial patterns
python create_map_poster.py -c "Paris" -C "France" -t pastel_dream -d 10000   # Haussmann boulevards
python create_map_poster.py -c "Moscow" -C "Russia" -t noir -d 12000          # Ring roads

# Organic old cities
python create_map_poster.py -c "Tokyo" -C "Japan" -t japanese_ink -d 15000    # Dense organic streets
python create_map_poster.py -c "Marrakech" -C "Morocco" -t terracotta -d 5000 # Medina maze
python create_map_poster.py -c "Rome" -C "Italy" -t warm_beige -d 8000        # Ancient layout

# Coastal cities
python create_map_poster.py -c "San Francisco" -C "USA" -t sunset -d 10000    # Peninsula grid
python create_map_poster.py -c "Sydney" -C "Australia" -t ocean -d 12000      # Harbor city
python create_map_poster.py -c "Mumbai" -C "India" -t contrast_zones -d 18000 # Coastal peninsula

# River cities
python create_map_poster.py -c "London" -C "UK" -t noir -d 15000              # Thames curves
python create_map_poster.py -c "Budapest" -C "Hungary" -t copper_patina -d 8000  # Danube split

# List available themes
python create_map_poster.py --list-themes&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Distance&lt;/cell&gt;
        &lt;cell role="head"&gt;Best for&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4000-6000m&lt;/cell&gt;
        &lt;cell&gt;Small/dense cities (Venice, Amsterdam center)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8000-12000m&lt;/cell&gt;
        &lt;cell&gt;Medium cities, focused downtown (Paris, Barcelona)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15000-20000m&lt;/cell&gt;
        &lt;cell&gt;Large metros, full city view (Tokyo, Mumbai)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;17 themes available in &lt;code&gt;themes/&lt;/code&gt; directory:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Theme&lt;/cell&gt;
        &lt;cell role="head"&gt;Style&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;feature_based&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Classic black &amp;amp; white with road hierarchy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gradient_roads&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Smooth gradient shading&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;contrast_zones&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;High contrast urban density&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;noir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pure black background, white roads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;midnight_blue&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Navy background with gold roads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blueprint&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Architectural blueprint aesthetic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;neon_cyberpunk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark with electric pink/cyan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;warm_beige&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Vintage sepia tones&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;pastel_dream&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Soft muted pastels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;japanese_ink&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Minimalist ink wash style&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;forest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deep greens and sage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ocean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Blues and teals for coastal cities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;terracotta&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mediterranean warmth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sunset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Warm oranges and pinks&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;autumn&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Seasonal burnt oranges and reds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;copper_patina&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Oxidized copper aesthetic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;monochrome_blue&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Single blue color family&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Posters are saved to &lt;code&gt;posters/&lt;/code&gt; directory with format:&lt;/p&gt;
    &lt;code&gt;{city}_{theme}_{YYYYMMDD_HHMMSS}.png
&lt;/code&gt;
    &lt;p&gt;Create a JSON file in &lt;code&gt;themes/&lt;/code&gt; directory:&lt;/p&gt;
    &lt;code&gt;{
  "name": "My Theme",
  "description": "Description of the theme",
  "bg": "#FFFFFF",
  "text": "#000000",
  "gradient_color": "#FFFFFF",
  "water": "#C0C0C0",
  "parks": "#F0F0F0",
  "road_motorway": "#0A0A0A",
  "road_primary": "#1A1A1A",
  "road_secondary": "#2A2A2A",
  "road_tertiary": "#3A3A3A",
  "road_residential": "#4A4A4A",
  "road_default": "#3A3A3A"
}&lt;/code&gt;
    &lt;code&gt;map_poster/
├── create_map_poster.py          # Main script
├── themes/               # Theme JSON files
├── fonts/                # Roboto font files
├── posters/              # Generated posters
└── README.md
&lt;/code&gt;
    &lt;p&gt;Quick reference for contributors who want to extend or modify the script.&lt;/p&gt;
    &lt;code&gt;┌─────────────────┐     ┌──────────────┐     ┌─────────────────┐
│   CLI Parser    │────▶│  Geocoding   │────▶│  Data Fetching  │
│   (argparse)    │     │  (Nominatim) │     │    (OSMnx)      │
└─────────────────┘     └──────────────┘     └─────────────────┘
                                                     │
                        ┌──────────────┐             ▼
                        │    Output    │◀────┌─────────────────┐
                        │  (matplotlib)│     │   Rendering     │
                        └──────────────┘     │  (matplotlib)   │
                                             └─────────────────┘
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Function&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Modify when...&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_coordinates()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;City → lat/lon via Nominatim&lt;/cell&gt;
        &lt;cell&gt;Switching geocoding provider&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;create_poster()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Main rendering pipeline&lt;/cell&gt;
        &lt;cell&gt;Adding new map layers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_edge_colors_by_type()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Road color by OSM highway tag&lt;/cell&gt;
        &lt;cell&gt;Changing road styling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_edge_widths_by_type()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Road width by importance&lt;/cell&gt;
        &lt;cell&gt;Adjusting line weights&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;create_gradient_fade()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Top/bottom fade effect&lt;/cell&gt;
        &lt;cell&gt;Modifying gradient overlay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;load_theme()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JSON theme → dict&lt;/cell&gt;
        &lt;cell&gt;Adding new theme properties&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;z=11  Text labels (city, country, coords)
z=10  Gradient fades (top &amp;amp; bottom)
z=3   Roads (via ox.plot_graph)
z=2   Parks (green polygons)
z=1   Water (blue polygons)
z=0   Background color
&lt;/code&gt;
    &lt;code&gt;# In get_edge_colors_by_type() and get_edge_widths_by_type()
motorway, motorway_link     → Thickest (1.2), darkest
trunk, primary              → Thick (1.0)
secondary                   → Medium (0.8)
tertiary                    → Thin (0.6)
residential, living_street  → Thinnest (0.4), lightest&lt;/code&gt;
    &lt;p&gt;New map layer (e.g., railways):&lt;/p&gt;
    &lt;code&gt;# In create_poster(), after parks fetch:
try:
    railways = ox.features_from_point(point, tags={'railway': 'rail'}, dist=dist)
except:
    railways = None

# Then plot before roads:
if railways is not None and not railways.empty:
    railways.plot(ax=ax, color=THEME['railway'], linewidth=0.5, zorder=2.5)&lt;/code&gt;
    &lt;p&gt;New theme property:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add to theme JSON: &lt;code&gt;"railway": "#FF0000"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Use in code: &lt;code&gt;THEME['railway']&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add fallback in &lt;code&gt;load_theme()&lt;/code&gt;default dict&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All text uses &lt;code&gt;transform=ax.transAxes&lt;/code&gt; (0-1 normalized coordinates):&lt;/p&gt;
    &lt;code&gt;y=0.14  City name (spaced letters)
y=0.125 Decorative line
y=0.10  Country name
y=0.07  Coordinates
y=0.02  Attribution (bottom-right)
&lt;/code&gt;
    &lt;code&gt;# Get all buildings
buildings = ox.features_from_point(point, tags={'building': True}, dist=dist)

# Get specific amenities
cafes = ox.features_from_point(point, tags={'amenity': 'cafe'}, dist=dist)

# Different network types
G = ox.graph_from_point(point, dist=dist, network_type='drive')  # roads only
G = ox.graph_from_point(point, dist=dist, network_type='bike')   # bike paths
G = ox.graph_from_point(point, dist=dist, network_type='walk')   # pedestrian&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large &lt;code&gt;dist&lt;/code&gt;values (&amp;gt;20km) = slow downloads + memory heavy&lt;/item&gt;
      &lt;item&gt;Cache coordinates locally to avoid Nominatim rate limits&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;network_type='drive'&lt;/code&gt;instead of&lt;code&gt;'all'&lt;/code&gt;for faster renders&lt;/item&gt;
      &lt;item&gt;Reduce &lt;code&gt;dpi&lt;/code&gt;from 300 to 150 for quick previews&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/originalankur/maptoposter"/><published>2026-01-17T10:13:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656903</id><title>US electricity demand surged in 2025 – solar handled 61% of it</title><updated>2026-01-17T12:19:02.839434+00:00</updated><content>&lt;doc fingerprint="a910b07712b4d594"&gt;
  &lt;main&gt;
    &lt;p&gt;Solar didn’t just show up in 2025 – it carried the grid. A new analysis from global energy think tank Ember shows that solar power accounted for 61% of the growth in US electricity demand last year, highlighting how central solar has become as power demand accelerates.&lt;/p&gt;
    &lt;p&gt;US electricity demand jumped by 135 terawatt-hours (TWh) in 2025, a 3.1% increase, the fourth‑largest annual rise of the past decade. Over that same period, solar generation grew by a record 83 TWh – a 27% increase from 2024 and the biggest absolute gain of any power source. That single jump in solar output covered 61% of all new electricity demand nationwide.&lt;/p&gt;
    &lt;p&gt;“Solar growth was essential in helping to meet fast‑rising US electricity demand in 2025,” said Dave Jones, chief analyst at Ember. “It generated where it was needed, and – with the surge in batteries – increasingly when it was needed.”&lt;/p&gt;
    &lt;p&gt;Texas, the Midwest, and the Mid‑Atlantic saw the largest increases in solar generation last year, and they were also the regions where electricity demand rose the fastest. Solar met 81% of demand growth in both Texas and the Midwest, and 33% in the Mid‑Atlantic.&lt;/p&gt;
    &lt;p&gt;Timing mattered, too. In aggregate, the increase in solar generation met the entire rise in US electricity demand during daytime hours between 10 am and 6 pm Eastern. And as a result of the rapid buildout of battery storage, solar also helped cover some of the demand growth during the evening hours, from 6 pm to 2 am.&lt;/p&gt;
    &lt;p&gt;The adoption of battery storage is turning solar from cheap daytime power into something far more flexible. Over the past six years, California’s utility‑scale solar and battery generation has climbed 58%. Yet, output at the sunniest hour of the day has increased by just 8%, a sign that more energy is being stored and used later, rather than dumped onto the grid all at once.&lt;/p&gt;
    &lt;p&gt;Most of the new solar generation in 2025 was absorbed by rising electricity demand, allowing solar to scale alongside overall grid growth.&lt;/p&gt;
    &lt;p&gt;“Solar has the potential to meet all the rise in electricity demand and much more. With electricity demand surging, the case to build solar has never been stronger,” said Jones.&lt;/p&gt;
    &lt;p&gt;Read more: EIA: All net new generating capacity in 2026 may be renewables&lt;/p&gt;
    &lt;p&gt;If you’re looking to replace your old HVAC equipment, it’s always a good idea to get quotes from a few installers. To make sure you’re finding a trusted, reliable HVAC installer near you that offers competitive pricing on heat pumps, check out EnergySage. EnergySage is a free service that makes it easy for you to get a heat pump. They have pre-vetted heat pump installers competing for your business, ensuring you get high quality solutions. Plus, it’s free to use!&lt;/p&gt;
    &lt;p&gt;Your personalized heat pump quotes are easy to compare online and you’ll get access to unbiased Energy Advisors to help you every step of the way. Get started here. – *ad&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electrek.co/2026/01/16/us-electricity-demand-surged-in-2025-solar-handled-61-percent/"/><published>2026-01-17T10:28:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656911</id><title>After 25 years, Wikipedia has proved that news doesn't need to look like news</title><updated>2026-01-17T12:19:02.533210+00:00</updated><content>&lt;doc fingerprint="3f025b5b0bd91eb7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;One of my favorite weekly newsletters is called the Weeklypedia; here’s last Friday’s edition. Each email contains two lists: The 20 Wikipedia articles that have been edited the most times in the past week, and the 10 most-edited articles created within the past week. If you read it long enough, you’ll start to see the subcategories most of these articles fall into — and the amount of volunteer labor that goes into them. &lt;/p&gt;
      &lt;p&gt;There are the very specific lists that some completist has taken on as a challenge. (Last week, “List of Phi Alpha Honor Society chapters” was edited 257 times — by only two authors. Or “Deaths in March 1982,” edited 398 times by five authors.)&lt;/p&gt;
      &lt;p&gt;There are the sports tournaments and reality TV shows that demand moment-to-moment updates. (Last week, there were 268 edits on “2026 Malaysia Open (badminton)” and 605 on “Bigg Boss (Tamil TV series) season 9.”)&lt;/p&gt;
      &lt;p&gt;There are the biographies of people whom someone has decided deserve memorializing. (Last week, user Mary Mark Ockerbloom created the article on Quaker abolitionist John Vickers and edited it 161 times. User pigsonthewing did the same for the artist Charles Shepard — best known for making posters for the British travel industry — but edited it only 90 times.)&lt;/p&gt;
      &lt;p&gt;But the most common Wikipedia genre represented each week is news. When something big happens in the world, some Wikipedian will start an article — and within minutes, editors will descend on it, using news articles as raw material to construct something encyclopedic. Here’s last week’s top 10 — it’s awfully close to a summary of the week’s front pages:&lt;/p&gt;
      &lt;p&gt;Wikipedia turns 25 years old today. On January 15, 2001, at 2:27 p.m. EST, Jimmy Wales made the first edit: “This is the new WikiPedia!” (They’ve gotten better since then.)&lt;/p&gt;
      &lt;p&gt;To celebrate, you can take a “What Wikipedia of the future are you?” quiz. Without even taking the quiz, though, I know which one I am: the Wikipedia That Gets Respect As a Source of News.&lt;/p&gt;
      &lt;p&gt;The site’s early years were filled with media outrage about a source of “truth” that anyone could edit. And yes, you could right now go edit in a claim that Happy Chandler was a lizard person. But the layers of accountability the site has built up over the years would likely reverse your edit within minutes. If those minutes are the cost of creating perhaps humanity’s single greatest source of information, I’m willing to pay it — with apologies to Happy’s descendants. Here are a few things news organizations could learn from it.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;News isn’t always what just happened. A few decades into the web, this might seem obvious today. But in 2001, news organizations were still very much constructed around producing daily newspapers and nightly newscasts. The rhythms of production drove everything from story selection to writing style to revenue models. Wikipedia was the first site that gave most journalists a vision of an “article” that is constantly updated, not rewritten with a new lede the next day. If there’s an important new detail in that Swiss bar fire investigation, it’ll be used to make the current Wikipedia article a little bit better — not to have a new unique URL to push out on social. Whatever questions your journalism answers, people will probably still be asking them tomorrow, and next week, and next year. Wikipedia is optimized for catching people up on news stories they missed the first time around.&lt;/item&gt;
        &lt;item&gt;Building processes means building culture. Wikipedia succeeds because it has a core set of standards and practices that its editors treat as holy writ. No original research. Neutral point of view. Not every source is equally reliable. No sockpuppetry. Do not disrupt Wikipedia to make a point — but assume good faith. Most (though not all) align perfectly well with the principles of journalism. But they’ve been constructed, a codified set of editorial standards, a collective ethos that guides the whole operation. For Wikipedia to work, there has to be consensus about how things are done, and that requires building a culture among editors. Why have so many other wiki-driven sites failed? They never build the culture that undergirds the processes.&lt;/item&gt;
        &lt;item&gt;Don’t break links. Unless an article has been taken down entirely, just about every link to a Wikipedia page created in the past quarter-century still works. Its article on Nicolás Maduro is still in the same place it was when first created in 2006, 4,493 edits ago. How many news articles published online in 2006 still live at the same address? Vanishingly few. When you start to think of articles as something with permanence, you realize the high cost of breaking a perfectly good URL.&lt;/item&gt;
        &lt;item&gt;Document your work. Check out one of those newsy Wikipedia articles, like this one on the killing of Renee Good. It’s currently 4,559 words. It’s been read nearly 1.4 million times. A total of 331 people have made 2,204 edits to it. Its talk page includes more than 1,000 signed comments from Wikipedia users arguing for something’s inclusion or exclusion. (Should the page’s title refer to Renee Good or Renée Good, with an accent? Should we note the date the shooter was first publicly identified? Should this article include details about anti-ICE protests after the killing, or should that be in a separate article?) It backs up its claims with 169 footnotes linking to news sources and official statements. And all of this work is available for anyone to see. The back-and-forths on the talk page let the reader see that points of view are being heard and debated. And anyone can click a link to confirm if a source is being accurately reflected. All that transparency won’t be practical for all journalistic work — but there’s no denying the trust that is built by operating in public.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Photo via Adobe Stock.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.niemanlab.org/2026/01/after-25-years-wikipedia-has-proved-that-news-doesnt-need-to-look-like-news/"/><published>2026-01-17T10:29:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656998</id><title>PCs refuse to shut down after Microsoft patch</title><updated>2026-01-17T12:19:02.331023+00:00</updated><content>&lt;doc fingerprint="745069121a6766f7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sorry Dave, I’m afraid I can’t do that! PCs refuse to shut down after Microsoft patch&lt;/head&gt;
    &lt;head rend="h2"&gt;Microsoft claims it's a Secure Launch bug&lt;/head&gt;
    &lt;p&gt;We're not saying Copilot has become sentient and decided it doesn't want to lose consciousness. But if it did, it would create Microsoft's January Patch Tuesday update, which has made it so that some PCs flat-out refuse to shut down or hibernate, no matter how many times you try.&lt;/p&gt;
    &lt;p&gt;In a notice on its Windows release health dashboard, Microsoft confirmed that some PCs running Windows 11 23H2 might fail to power down properly after installing the latest security updates. Instead of slipping into shutdown or hibernation, affected machines stay stubbornly awake, draining batteries and ignoring shutdown like they have a mind of their own and don't want to experience temporary non-existence.&lt;/p&gt;
    &lt;p&gt;The bug appears to be tied to Secure Launch, a security feature that uses virtualization-based protections to ensure only trusted components load during boot. On systems with Secure Launch enabled, attempts to shut down, restart, or hibernate after applying the January patches may fail to complete. From the user's perspective, everything looks normal – until the PC keeps running anyway, refusing to be denied life.&lt;/p&gt;
    &lt;p&gt;Microsoft says that entering the command "shutdown /s /t 0" at the command prompt will, in fact, force your PC to turn off, whether it wants to or not.&lt;/p&gt;
    &lt;p&gt;"Until this issue is resolved, please ensure you save all your work, and shut down when you are done working on your device to avoid the device running out of power instead of hibernating," Microsoft said.&lt;/p&gt;
    &lt;p&gt;The firm hasn't offered much in the way of technical detail, nor has it put numbers on how many devices are affected. There's also no fix yet, with Redmond vaguely promising to "release a resolution for this issue in a future update." But isn't that just what a sentient bot might say?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microsoft teases targeted Copilot removal for admins&lt;/item&gt;
      &lt;item&gt;Microsoft rushes an out-of-band update for Message Queuing bug&lt;/item&gt;
      &lt;item&gt;Windows is testing a new, wider Run dialog box. Here's how to try it&lt;/item&gt;
      &lt;item&gt;Latest Windows 11 updates may break the OS's most basic bits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This isn't the only post-update gremlin lurking in January's Patch Tuesday bundle. Microsoft has also been forced to acknowledge a separate issue in which classic Outlook POP account profiles can hang or freeze after installing this month's patches, another reminder that while the bugs being fixed may be invisible, the ones introduced can be painfully obvious.&lt;/p&gt;
    &lt;p&gt;The notice is similarly vague, with Microsoft stating: "This is an emerging issue, and we don't have all the symptoms yet, but we will update the topic as we understand the issue better."&lt;/p&gt;
    &lt;p&gt;Patch Tuesday exists to close security holes, some of them serious, and skipping updates is rarely a great idea. But once again, a batch of fixes has arrived with side effects that range from irritating to disruptive, depending on how much you rely on your system behaving predictably when it's told to turn off.&lt;/p&gt;
    &lt;p&gt;For now, admins and long-suffering Windows users are left watching Microsoft's status pages and waiting for patches to the patches – hoping their machines eventually go to sleep. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2026/01/16/patch_tuesday_secure_launch_bug_no_shutdown/"/><published>2026-01-17T10:51:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657088</id><title>AV1 Image File Format Specification Gets an Upgrade with AVIF v1.2.0</title><updated>2026-01-17T12:19:02.040853+00:00</updated><content>&lt;doc fingerprint="942108f30f837dc5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AV1 Image File Format Specification Gets an Upgrade with AVIF v1.2.0&lt;/head&gt;
    &lt;head rend="h2"&gt;AVIF v1.2.0 includes support for sample transforms&lt;/head&gt;
    &lt;p&gt;By Yannis Guyon, Leo Barnes, and Wan-Teh Chang&lt;/p&gt;
    &lt;p&gt;AOMedia's Storage and Transport Format Working Group has released AVIF v1.2.0, a new revision of the AV1 Image File Format specification. The work is part of AOMedia’s broader effort to advance high-quality still image storage and ensure consistent mapping between codecs and container formats. The latest specification refines the format and introduces functional enhancements, most notably, support for sample transforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Features and Innovations&lt;/head&gt;
    &lt;p&gt;The addition of sample transforms makes it possible to use higher bit depths, even when the underlying codec does not natively support 16-bit or greater precision. This enhancement empowers creators to achieve superior image quality and greater flexibility in high-fidelity imaging workflows.&lt;/p&gt;
    &lt;p&gt;To try sample transforms, clone and build libavif, then use a command such as:&lt;/p&gt;
    &lt;code&gt;build/avifenc input_16bit.png --depth 12,8 output.avif
&lt;/code&gt;
    &lt;p&gt;This produces an AVIF file that preserves the top 12 bits for compatibility, with the full 16-bit image retrievable during decoding&lt;/p&gt;
    &lt;p&gt;As an example, encoding the image [1] with &lt;code&gt;avifenc --depth 12,8 --lossless --speed 0&lt;/code&gt; leads to 10% file size savings over
the source 16-bit PNG, with absolutely no quality loss. The encoded AVIF is also
backward compatible for 12 bits out of 16 with legacy AVIF decoders. (Results
may vary depending on settings.)&lt;/p&gt;
    &lt;p&gt;The release also strengthens conformance, clarifies the mapping between AV1 bitstream metadata and file-level signaling, and updates references and requirements to align with the latest HEIF, ISOBMFF, and MIAF specifications. Editorial Improvements include a new list of required boxes for AVIF files to help developers generate standard-compliant files.&lt;/p&gt;
    &lt;p&gt;Additionally, the new specification includes guidance on gain maps—tone map derived image items, which are a method for encoding AVIF HDR images that are backward compatible with SDR displays.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to get started?&lt;/head&gt;
    &lt;p&gt;Review the AVIF v1.2.0 specification and try out sample transforms using the latest libavif implementation on GitHub. We welcome feedback and contributions—share your experience or questions in the libavif GitHub repository and help shape the future of open image formats with AOMedia.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aomedia.org/blog%20posts/AV1-Image-File-Format-Specification-Gets-an-Upgrade-with-AVIF/"/><published>2026-01-17T11:09:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657116</id><title>Gut micro-organisms associated with health, nutrition and dietary intervention</title><updated>2026-01-17T12:19:00.509983+00:00</updated><content>&lt;doc fingerprint="ae0c99b3c92f0284"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;The incidence of cardiometabolic diseases is increasing globally, and both poor diet and the human gut microbiome have been implicated1. However, the field lacks large-scale, comprehensive studies exploring these links in diverse populations2. Here, in over 34,000 US and UK participants with metagenomic, diet, anthropometric and host health data, we identified known and yet-to-be-cultured gut microbiome species associated significantly with different diets and risk factors. We developed a ranking of species most favourably and unfavourably associated with human health markers, called the ‘ZOE Microbiome Health Ranking 2025’. This system showed strong and reproducible associations between the ranking of microbial species and both body mass index and host disease conditions on more than 7,800 additional public samples. In an additional 746 people from two dietary interventional clinical trials, favourably ranked species increased in abundance and prevalence, and unfavourably ranked species reduced over time. In conclusion, these analyses provide strong support for the association of both diet and microbiome with health markers, and the summary system can be used to inform the basis for future causal and mechanistic studies. It should be emphasized, however, that causal inference is not possible without prospective cohort studies and interventional clinical trials.&lt;/p&gt;
    &lt;head rend="h3"&gt;Similar content being viewed by others&lt;/head&gt;
    &lt;head rend="h2"&gt;Main&lt;/head&gt;
    &lt;p&gt;Cardiometabolic diseases (CMDs) are the leading causes of morbidity and mortality in Western countries and constitute a heavy burden on global healthcare systems1. The most predominant CMDs are cardiovascular disease (CVD) and type 2 diabetes (T2D)3, which are connected with the increased consumption of calorie-dense, high-risk processed foods observed over the past few decades4. Habitual diet is not only among the known risk factors associated with CMDs, but also the primary modifiable target for prevention and treatment5. Well established anthropometric and intermediary measures of CMDs, ranging from clinical measurements (for example, blood pressure) to lipid profiles (such as triglycerides, cholesterol and lipoproteins), glucose levels (for example, fasting and postprandial glucose, and haemoglobin A1c (HbA1c)), inflammatory markers (for example, glycosylated proteins, the systemic inflammation marker GlycA21 and high-sensitivity C-reactive protein), and known risk factors such as body mass index (BMI), can be used to study the diet–CMD axis6,7,8 but do not consider the biochemical mechanisms occurring in the human gut.&lt;/p&gt;
    &lt;p&gt;The human gut microbiome has emerged as a cofactor on the same axis as it is associated with diet and cardiometabolic conditions9,10,11,12 and is a modifiable element13,14,15. A change in dietary patterns can shift the species-level composition of the microbiome, with knock-on effects on host health16. However, individual responses to dietary interventions vary, and precision nutrition aims at identifying host-specific factors that modulate the interaction between diet and host health17, but it is currently not possible to disentangle the effects diet plays to improve cardiometabolic health via the microbiome. Furthermore, the composition of the gut microbiome displays high individuality and variation depending on different demographics, ethnicity, sex and age; hence, defining or identifying universal biomarkers of a healthy gut microbiome has proven difficult18,19,20.&lt;/p&gt;
    &lt;p&gt;Nutritional intervention studies usually involve low sample-size cohorts at the population level and are often limited by their statistical power and specificity to local lifestyle and dietary habits, which are all critical aspects, especially given the microbiome’s complexity and variability. Large-scale comprehensive studies with multi-national populations can help disentangle some of the complex interplays between dietary patterns and the gut microbiome to develop personalized interventions to prevent and treat CMDs. Accordingly, we collated, sampled and analysed five of the largest metagenomic cohorts available to date, comprising more than 34,000 people and spanning two continents, paired with dietary data, detailed anthropometric and health markers. We identified microbiome species consistently associated with more favourable and (inversely) unfavourable health markers across continents. These species were organized into two microbiome rankings, representing host health and diet quality, respectively, that can be the basis for future causal and mechanistic studies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Metagenomics of the ZOE PREDICT cohorts&lt;/head&gt;
    &lt;p&gt;We used four large-scale microbiome cohorts from the ZOE PREDICT studies (n = 33,596; Fig. 1a, Supplementary Fig. 1, Supplementary Table 1 and Methods) to assemble an extensive microbiome dataset of people with detailed dietary records along with anthropometric measures. Together with the previously available ZOE PREDICT 1 cohort9 (n = 1,098), the PREDICT cohorts comprise 34,694 participants from both the USA (n = 21,340) and the UK (n = 13,354; Methods). Collected data comprise common health risk factors such as BMI, triglycerides, blood glucose and HbA1c, as well as several dietary indices and clinical markers that are intermediary measures of cardiometabolic health, such as the atherosclerotic CVD (ASCVD) risk, high-density lipoproteins (HDL) and GlycA21 (Supplementary Table 2 and Methods).&lt;/p&gt;
    &lt;p&gt;A systematic machine learning validated approach9,22 (Methods) revealed strong associations consistent across the five ZOE PREDICT cohorts between the microbiome and surrogate health markers and nutrition (Fig. 1b, Supplementary Table 2 and Methods). Markers that were classified accurately by the gut microbiome included glycemia, blood cholesterol, triglycerides and inflammation (both fasting and postprandial; Extended Data Fig. 1), with age, BMI, the healthy eating index23 and the healthful plant-based diet index (PDI)24 also correlated with microbiome machine learning regression estimates (Spearman’s correlation &amp;gt; 0.4; Fig. 1b and Supplementary Table 2). The top predicted markers from both machine learning regression and classification showed consistent associations across PREDICT cohorts, with average area under the receiver operating characteristic curve (AUC) ranging from 0.64 to 0.73, and an average Spearman’s correlation ranging from 0.30 to 0.46 for regression (Fig. 1b and Supplementary Table 3).&lt;/p&gt;
    &lt;head rend="h2"&gt;Ranking gut species to host and health&lt;/head&gt;
    &lt;p&gt;We next set out to identify which gut microbial species were most responsible for the microbiome’s associations with host markers. To do so, we grouped the 37 markers into three categories: (1) anthropometric-derived and accessible health-related measures (hereafter called ‘personal’ and including, for example, ASCVD and blood pressure), (2) fasting (for example, GlycA, triglycerides, HDL, cholesterol and glucose) and (3) postprandial markers, which are surrogate measures of cardiometabolic health. As expected, some markers tended to correlate quantitatively (Supplementary Table 4 and Methods).&lt;/p&gt;
    &lt;p&gt;We considered 661 non-rare microbial species (greater than 20% prevalence; Methods) according to the definition of species-level genome bins (SGBs)19,20, and computed the partial Spearman’s correlations (corrected for sex, age and BMI) between the relative abundance of each micro-organism and the value of each marker. Correlations were ranked, and correlations’ ranks were averaged within each category and then averaged among the three categories in each cohort (Methods). The five resulting cohort-level average rankings were averaged to derive a single ranking that we called the ‘ZOE Microbiome Health Ranking 2025’ (ZOE MB health-rank). This resulted in a ranking for 661 microbial species in which the lowest ranking (closer to 0) species are the most positively associated with the considered panel of host markers and vice versa for the highest ranking (closer to 1) species (Fig. 2, Extended Data Figs. 2 and 3, Supplementary Fig. 2 and Supplementary Tables 5 and 6).&lt;/p&gt;
    &lt;p&gt;Most SGBs ranked within the 50 most favourably or unfavourably linked to host anthropometry belong to the Firmicutes phylum (92 out of 100) and, in particular, to the Clostridia class (n = 80; Supplementary Table 7). Within this class, in the ZOE MB health-ranks, most SGBs belonged to the Clostridiales order, with 32 unfavourably ranked SGBs (of which n = 27 Lachnospiraceae out of 50) and 31 favourably ranked SGBs (n = 13 Lachnospiraceae and n = 12 Ruminococcaceae) assigned to this order. Collectively, the average total relative abundance of the 50 most favourably ranked SGBs is 5.98%, whereas the 50 most unfavourably ranked SGBs account for 13.64% (Supplementary Table 7).&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncharacterized health-linked bacteria&lt;/head&gt;
    &lt;p&gt;A large portion of the 50 most favourably ZOE MB health-ranked SGBs are unknown (n = 22), meaning that these are uncultured species represented solely by microbial genomes reconstructed from metagenomic data. Of the 28 known SGBs (with available isolate genomes), 24 are still uncharacterized species without phenotypic descriptions and recognized taxonomic names (Supplementary Table 7). Eubacterium siraeum (SGB4198) and Faecalibacterium prausnitzii (SGB15317) are among the few exceptions with previous support for their favourable role9,25.&lt;/p&gt;
    &lt;p&gt;By contrast, the 50 unfavourably ZOE MB health-ranked SGBs are generally species with cultured isolates and established taxonomic labels (Supplementary Table 7). Among the 44 known SGBs, several species were already linked with detrimental effects on the host, including Ruminococcus gnavus26, Flavonifractor plautii27, Ruminococcus torques28,29 and Enterocloster bolteae30. Overall, the most prevalent favourably ranked health-associated micro-organisms in the human gut belong to under-investigated species, highlighting gaps in our knowledge of the potential beneficial role of the human microbiome in promoting and maintaining non-pathogenic conditions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gut species ranked by diet quality&lt;/head&gt;
    &lt;p&gt;Similarly to the ZOE MB health-ranks, we defined a species ranking on the basis only of dietary markers across all five PREDICT cohorts, which we called the ‘ZOE Microbiome Diet Ranking 2025’ (ZOE MB diet-rank; Supplementary Table 5). As markers of a generally healthier diet, we adopted five validated indices (Methods) computed starting from validated food frequency questionnaires (FFQs) or logged diet data (logged using a mobile phone app), reflecting long- and short-term dietary habits, respectively (Extended Data Figs. 4 and 5 and Methods).&lt;/p&gt;
    &lt;p&gt;The ZOE MB health- and diet-rankings showed, as expected, general concordance (Spearman’s ρ = 0.72; Extended Data Fig. 6a and Supplementary Table 5). Although the large majority of the SGBs highlighted by high or low ZOE MB health-ranks and diet-ranks belong to unknown taxa, reported phenotypic characteristics of known species agree with our analysis. For example, R. torques (SGB4608) and F. plautii (SGB15132), discussed previously as unfavourable species according to the ZOE MB health-ranks, were also concordantly unfavourably ranked in the ZOE MB diet-ranks (0.991–0.904 and 0.981–0.901, respectively). On the other hand, the favourably ranked Blautia glucerasea (SGB4816) was described to reduce visceral fat accumulation, blood glucose and triglycerides in mice31 (ZOE MB health-ranks and diet-ranks of 0.267 and 0.062, respectively). As another example, in a dietary fibre supplementation trial involving individuals with T2D, Lachnospira eligens (SGB5082) was increased selectively and associated negatively with postprandial glucose and insulin, body weight and waist circumference32 (ZOE MB health-ranks and diet-ranks of 0.276 and 0.115, respectively), indicating that precise dietary interventions aimed at stimulating beneficial bacterial growth can contribute to treating or managing metabolic disorders symptoms.&lt;/p&gt;
    &lt;p&gt;Despite the overall agreement between the ZOE MB health- and diet-rankings, 65 out of the 661 ranked SGBs showed discordant rankings (absolute rank difference at least 0.3; Extended Data Fig. 6a and Supplementary Table 8). Generally, the different trends may be due to the different capacities of certain bacteria (for example, generalists) to use a variety of substrates, including those derived from unhealthy diets, while releasing functional metabolites with protective or health-promoting effects. Among these, for example, Harryflintia acetispora (SGB14838) was found associated with favourable cardiometabolic markers and unfavourable diets (ZOE MB health-rank = 0.363 and ZOE MB diet-rank = 0.879) in this study. This strict anaerobe can use readily available monosaccharides such as maltose, glucose and fructose, but can also produce short-chain fatty acids33, which are regulatory and anti-inflammatory mediators34.&lt;/p&gt;
    &lt;p&gt;Across the US and UK populations, the ZOE MB health-rankings showed high consistency (Spearman’s ρ = 0.61; Extended Data Fig. 6b), whereas country-specific ZOE MB diet-rankings were more heterogeneous (Spearman’s ρ = 0.26; Extended Data Fig. 6c). The intraclass correlation coefficients (ICC)35 also suggest that the ZOE MB health-ranks are more consistent across countries than the ZOE MB diet-ranks (ICC = 0.5929 and 0.2623, respectively; Extended Data Fig. 6b,c). Across cohorts, we obtained an ICC = 0.63 and 0.46 for the ZOE MB health-ranks and diet-ranks, respectively, indicating that health rankings were more able to capture cohorts and countries differences, whereas the most favourably ranked species appeared to match across populations with similar levels of industrialization and lifestyle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Species rankings stratify by BMI&lt;/head&gt;
    &lt;p&gt;BMI is an imperfect but widely adopted and easy-to-obtain anthropometric marker of health risk. As BMI was not included among the markers of the ZOE MB health- and diet-rankings, and we corrected for it in the partial correlation analysis, we set out to evaluate how the two rankings can stratify people according to their BMI to assess how health signatures in the gut microbiome are reflected in body mass.&lt;/p&gt;
    &lt;p&gt;We correlated the 661 ZOE MB health-ranked species with BMI (corrected for sex and age), in each PREDICT cohort, and found that, overall, the ranks were associated positively with BMI (Spearman’s ρ = 0.72), with the favourably ranked SGBs correlated negatively with BMI, whereas unfavourably ranked SGBs correlated positively with BMI (Fig. 3a). These results were confirmed when considering the ZOE MB diet-ranks and discrete BMI categories (Extended Data Fig. 7a–c; all intra-dataset comparisons statistically significant at Q &amp;lt; 0.2 and all 30 except 7 at Q &amp;lt; 0.01) as well as the cumulative abundance of the species in the two 50-species sets (Fig. 3b,c; all intra-dataset comparisons statistically significant at Q &amp;lt; 0.2 and all 30 except 5 at Q &amp;lt; 0.01).&lt;/p&gt;
    &lt;p&gt;To generalize these associations, we leveraged a total of 5,348 healthy individuals from 27 public cohorts divided into three BMI categories, healthy weight (n = 2,837), overweight (n = 1,562) and obese (n = 949; Supplementary Table 9 and Methods). In 47 pairwise comparisons, 34 had a higher median richness for the 50 most favourably ranked ZOE MB health SGBs in lower BMI groups versus higher BMI groups (binomial P = 0.003; Supplementary Table 10 and Supplementary Fig. 3a), and this was not dependent on country effects or sequencing depth (Supplementary Table 11), highlighting the generalization of the identified ranks. Meta-analysis based on linear regression on single cohorts (Methods) showed that individuals with healthy weight carried, on average, 5.2 more of the 50 favourably ZOE MB health-ranked SGBs than people with obesity (P = 0.0003; Fig. 3d and Supplementary Table 12), which corresponded to a normalized difference in the cumulative abundances of unfavourably and favourably ranked SGBs of Cohen’s d = −0.59 (P &amp;lt; 0.0001; Supplementary Tables 10 and 13 and Methods). Correspondingly, individuals with obesity carried, on average, 1.95 more of the unfavourably ranked SGBs than people of healthy weight (P = 0.0005; Fig. 3d, Supplementary Tables 14 and 15; Cohen’s d on cumulative relative abundances = 0.29; P = 0.0001). Pairwise analysis of the other BMI categories confirmed these results (Extended Data Fig. 8 and Supplementary Tables 10 and 12–15).&lt;/p&gt;
    &lt;p&gt;Similarly, we tested the association of the 50 most favourably and unfavourably ZOE MB diet-ranked SGBs with BMI, and found similar but milder signals compared with the ZOE MB health-ranks (average Spearman’s correlations between the two ranks and BMI of 0.61 and 0.72, respectively; Fig. 3a and Extended Data Fig. 7a). Using public datasets, 36 intra-dataset comparisons out of 47 showed a higher median cumulative abundance and a higher median richness of the 50 most favourable SGBs in lower BMI classes compared with higher BMIs (binomial P = 0.0003; Supplementary Fig. 3b). Conversely, 36 comparisons showed a higher median count of the least favourable 50 SGBs for the higher BMI classes compared with the lower BMI groups (binomial P = 0.0003; Supplementary Table 10). The contribution of diet-ranked SGBs in different BMI categories similarly showed a decreasing number and cumulative relative abundance of favourably ranked SGBs and an increase in unfavourably ranked SGBs (Extended Data Fig. 7d–g). In meta-analysis, healthy weight and overweight participants carried 3.5 and 1.5 more favourable diet-ranked SGBs, and 1.25 and 0.88 fewer unfavourable ZOE MB diet-ranked SGBs than obesity participants, respectively (Extended Data Fig. 9, Supplementary Fig. 3 and Supplementary Tables 16–19). All these analyses were confirmed when rankings were computed without adjusting for BMI (Extended Data Fig. 7h–k) and, altogether, these results suggest that the ZOE MB health- and diet-ranks can stratify people based on their obesity status regardless of geography.&lt;/p&gt;
    &lt;head rend="h2"&gt;Species rankings and host diseases&lt;/head&gt;
    &lt;p&gt;Next, we assessed whether the ZOE MB health-ranked SGBs had a differential presence or abundance in control participants compared with participants with a defined disease condition, exploiting 25 case–control, publicly available microbiome studies (4,816 samples in total with n = 2,707 controls and n = 2,109 cases; Methods) investigating five diseases with variable levels of association with the gut microbiome (Supplementary Table 20). The number of the 50 most favourably ZOE MB health-ranked SGBs was higher in controls than cases for 21 of the 25 cohorts, whereas the count of the 50 most unfavourably ranked SGBs was correspondingly higher in cases for the same number of cohorts (binomial P = 0.0004).&lt;/p&gt;
    &lt;p&gt;We performed a meta-analysis on the count of the 50 most favourable and unfavourable SGBs from the ZOE MB health- and diet-rankings. Control samples carried, on average, 3.6 more favourably ranked SGBs than participants with disease (random-effect model, P = 0.0002; Methods) and 1.6 fewer unfavourable SGBs (P = 0.0004; Supplementary Fig. 5a and Supplementary Table 21). Similarly, for the ZOE MB diet-ranked SGBs, controls carried, on average, 3.8 more favourable SGBs and 1.3 fewer unfavourable SGBs, P = 9.5×10−6 and P = 0.0006, respectively; Supplementary Fig. 5a and Supplementary Table 21). Furthermore, meta-analyses of the cumulative abundance of the 50 most favourable and unfavourable SGBs confirmed a greater contribution from favourable species in control groups and of unfavourable SGBs in the corresponding disease groups (meta-analysis Cohen’s d = −0.29, P = 7.1 × 10−6 and d = 0.21, P = 0.054 for the ZOE MB health-ranks; d = −0.24, P = 3.1 × 10−6 and d = 0.28, P = 0.0002 for the ZOE MB diet-ranks; Fig. 3e and Supplementary Table 22).&lt;/p&gt;
    &lt;p&gt;To assess how informative the rankings are in summarizing the health-associated status of a single sample, we scored all metagenomes from diseased and control participants by summing the normalized ZOE MB health-ranks of the SGBs present in the sample (Methods). We found a strong separation between diseased and control participants (meta-analysis Cohen’s d = −0.37, P = 8.3 × 10−8), improving over the simple counting of the number of most favourable and unfavourable SGBs (Fig. 3f). Notably, T2D showed the strongest disease-specific association (meta-analysis Cohen’s d = −0.47, P = 6.78 × 10−5; Fig. 3f and Supplementary Table 23) with the weighted version of this score showing an even stronger effect for T2D (meta-analysis Cohen’s d = −0.51, P = 0.0002). People were also scored using the ZOE MB diet-ranks, and similar links with their health status emerged (Fig. 3f and Supplementary Table 23). Notably, standard alpha diversity measures such as gut SGBs richness and Shannon’s entropy measures showed weaker and less consistent associations, with significant links only in the IBD and T2D comparisons (Supplementary Fig. 5b and Supplementary Table 24).&lt;/p&gt;
    &lt;p&gt;Although the ranking-based scoring of single samples cannot have the same predictive power for host phenotypes compared with condition-specific supervised learning approaches relying directly on labelled training data, our results showed how embedding the ranking system into a simple one-dimensional microbiome index provides a meaningful evaluation of microbiome health conditions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Diet changes effects on ranked species&lt;/head&gt;
    &lt;p&gt;To validate the effect of dietary changes on the presence and abundance of gut microbial species according to their ZOE MB health-rankings, we analysed two dietary intervention studies, namely ZOE METHOD36 and BIOME37 (ClinicalTrials.gov registrations, NCT05273268 and NCT06231706, respectively). In brief, the ZOE METHOD cohort comprised n = 347 people assigned to a personalized dietary intervention programme (PDP; n = 177) arm versus an arm with general diet advice following the US Department of Agriculture recommendations (control, n = 170). People assigned to the PDP group showed lower energy intake and a significant decrease in triglycerides, HbA1c, weight and waist circumference after 18 weeks36. The ZOE BIOME cohort comprised n = 349 healthy adults (intention-to-treat) randomized into the primary intervention group (receiving a defined prebiotic blend, n = 116), the functional control group (receiving bread croutons to match the calories in the control group, n = 120) and the daily probiotic group (supplemented with 15 billion colony-forming units of Lacticaseibacillus rhamnosus per day, n = 113). Overall, weight, waist circumference, metabolites and gastrointestinal symptoms did not differ significantly between groups37.&lt;/p&gt;
    &lt;p&gt;We identified which microbiome species were impacted significantly by the dietary interventions in the two cohorts. In the ZOE BIOME cohort, 57, 4 and 14 prevalent SGBs showed significant changes at the endpoint (Q &amp;lt; 0.01) for the prebiotic blend, probiotic and control arm, respectively (Fig. 4a). Among the species with a significant change in the prebiotic arm were beneficial fibre-degrading Bifidobacterium adolescentis (SGB17244), Bifidobacterium longum (SGB17248) and Blautia obeum (SGB4811)38,39,40, as well as butyrate-producing Agathobaculum butyriciproducens (SGB14993), Anaerobutyricum hallii (SGB4532) and Coprococcus catus (SGB4670)41,42. By contrast, the species Dysosmobacter welbionis (SGB15078), among the top unfavourably associated SGBs in our study, was decreased significantly by the same dietary intervention (Supplementary Table 25). In the ZOE METHOD cohort, we found 46 SGBs differed significantly in their relative abundance in the PDP arm, and only two in the control arm (Fig. 4b and Supplementary Table 25; Wilcoxon signed-rank test Q &amp;lt; 0.1). Of note, the prominent butyrate producers Roseburia hominis (SGB4936) and A. butyriciproducens (SGB14993) were also found to increase in the PDP intervention.&lt;/p&gt;
    &lt;p&gt;The dietary intervention groups of both clinical trials that aimed at improving diet using different approaches (prebiotic blend for BIOME and PDP for METHOD) showed the highest number of significantly changing SGBs (Fig. 4a and Supplementary Table 25). Focusing on the most significant gut microbial SGBs with largest change in relative abundance after dietary interventions, we found increasing Bifidobacterium animalis (SGB17278)—a bacterium present in dairy-based foods and in the microbiome of people consuming larger amounts of them43,44 (Fig. 5a,b and Supplementary Table 25), an unknown Lachnospiraceae bacterium (SGB4953, BIOME; Fig. 5a) and R. hominis (SGB4936, METHOD; Fig. 5b) both previously associated with a vegan diet43, and another unknown Lachnospiraceae bacterium (SGB5200, BIOME; Fig. 5a) linked to a vegetarian diet43. Butyricimonas paravirosa (SGB1785, METHOD; Fig. 5b), Phocea massiliensis (SGB14837), a currently uncharacterized Ruminococcaceae species (SGB14899) and Candidatus Pararuminococcus gallinarum (SGB63327) (all found in the BIOME cohort; Fig. 5a), were instead decreasing in the intervention and reported to be associated with a mixed diet43. The Streptococcus salivarius (SGB8007, METHOD; Fig. 5b) species was also found in food microbiomes45 and, together with an unknown Ruminococcaceae species (SGB14899, BIOME; Fig. 5a), were found associated with non-vegans43.&lt;/p&gt;
    &lt;p&gt;We found that the SGBs with increased relative abundance at endpoint in the prebiotic blend arm of the BIOME trial (Fig. 4a and Supplementary Table 25) showed significantly more favourable ZOE MB health-ranks and diet-ranks than the decreasing SGBs (Fig. 5c,d, Mann–Whitney U-test P = 7.78 × 10−3 and P = 3.00 × 10−5, respectively). This was confirmed for the PDP arm of the METHOD cohort (Fig. 5e,f; Mann–Whitney U-test P = 5.20 × 10−5 and P = 2.03 × 10−3, respectively). No significant enrichment for ZOE MB health- and diet-rankings was instead detected for the significantly changing SGBs for the probiotic group of the BIOME cohort or the control groups of both BIOME and METHOD cohorts (Extended Data Fig. 10e,f).&lt;/p&gt;
    &lt;p&gt;Together, these results show how dietary interventions or tailored prebiotic blends, both aiming at improving diet quality, positively modulate the microbiome composition. The SGBs’ rankings (ZOE MB health and diet), which were defined on cross-sectional independent cohorts, were strongly and consistently predictive of the SGBs most associated with the dietary interventions in independent cohorts and countries, supporting the direct, reproducible and actionable link between diet and microbiome composition.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;Defining the baseline composition of the human gut microbiome in ‘healthy’ host conditions has been a long-standing challenge. This area has several problems, including defining general host health across age, as well as the inter-population microbiome variability, the existence of several distinct health-associated microbiome configurations19,46, the personalized nature of diet’s impact on the gut microbiome16,47, the diversity of dietary regimes48,49 and the effect of social interaction on microbiome transmission44. To address this challenge, we reformulated the question of what a health-associated microbiome is by scoring gut microbiome species for their tendency to be correlated with healthy diet scores and with the continuum of a panel of intermediate markers of cardiometabolic health, in large and generally healthy populations. By leveraging diet scores such as the healthy eating index or the healthful PDI, and health estimators such as blood glucose, HDL and triglycerides, we identified species that are expected to characterize hosts in healthier conditions, as well as other species that are enriched in hosts with more unfavourable health risk factors. Most of the key health-associated species were from previously uncharacterized species, underlining the wide knowledge gap of the microbiome composition in non-diseased conditions. These rankings, named ZOE MB health-ranks and diet-ranks, are released and maintained publicly (Supplementary Table 5 and ‘Data availability’ section) and can be adopted by the research community to evaluate whether a given human gut microbiome sample is characterized by a more favourable or unfavourable diet and health-associated species.&lt;/p&gt;
    &lt;p&gt;Several factors were crucial in the robust definition of the proposed microbiome species ranking systems. First, the scale of our combined cohorts with consistent experimental protocols for metagenomic sequencing and analysis is unprecedented. Second, the geographic diversity spanning all US states and UK regions, although confined to typical Westernized lifestyles and diets, allowed us to overcome local lifestyle-associated microbiome configurations. Third, consistent long- and short-term diet logging data processed in an integrative quantitative approach and validated markers that are intermediary measures of cardiometabolic health, and more advanced postprandial metabolomic-derived markers, enabled a fine-grained definition of relative health gradients across the surveyed populations. Fourth, publicly available datasets that were processed and curated uniformly, permitted independent validation and generalization of the results and showed the relevance of the species rankings toward additional conditions and diseases not evaluated in the original populations. We acknowledge that the demographic composition of the cohorts may influence some associations, and we are continuing to expand in both population scale and precision of each host-associated readout.&lt;/p&gt;
    &lt;p&gt;Our microbiome species ranking system proved accurate in reflecting changes induced by large-scale dietary intervention trials with associated host marker improvements (Figs. 4 and 5). Indeed, the cross-sectional associations were reflected in a significant and substantial increase of health-associated microbiome species and a reduction or depletion of unfavourably ranked species. Many health-associated host markers are co-correlated because they are nutritional indicators, and disentangling their direct interactions from those mediated by the microbiome will remain elusive until large-scale microbiome interventions become possible in humans. In this respect, one key limitation of our study design is that it does not allow directly disentangling of the effect that diet exerts on the microbiome to improve cardiometabolic health from the impact of diet only. This is particularly important as diet-based ranks were more dependent on country-related differences compared with health ranks, and further studies should explore food-specific links with gut microbial species and cardiometabolic outcomes in greater detail50. This would entail designing large-scale interventions in which both the introduction of single foods and alterations of specific microbiome characteristics (for example, by administration of specific microbiome members) are tested, which are ultimately required to provide causal evidence that personalized nutritional interventions targeting the microbiota have a robust and reproducible impact on cardiometabolic health. Nonetheless, the confirmation of the cross-sectional patterns along the diet–microbiome–health axis in longitudinal nutrition intervention trials not only increases the intrinsic value of the rankings but confirms that the human gut microbiome can be modulated successfully by dietary intervention and that the effects on the microbiome of such interventions are both predictable and reproducible. By providing the full list of ranked microbial species, this work can be exploited in future research on microbiome-powered precision nutrition and can be expanded in the future to more diverse populations and lifestyles that are currently underrepresented in microbiome, nutritional and health studies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methods&lt;/head&gt;
    &lt;head rend="h3"&gt;ZOE PREDICT cohorts definition&lt;/head&gt;
    &lt;p&gt;The ZOE PREDICT programme comprises several distinct studies that together constitute one of the largest multi-omic health initiatives, linking diet, person-specific metabolic responses to foods, and the gut microbiome. In this work, we considered and harmonized five ZOE PREDICT cohorts: PREDICT 1, PREDICT 2, PREDICT 3 US21, PREDICT 3 US22A, and PREDICT 3 UK22A. The PREDICT 1 cohort (NCT03479866) was described previously9,51. In brief, PREDICT 1 enrolled 1,098 participants (n = 1,001 from the UK and n = 97 from the USA) who underwent a clinical visit to collect anthropometric information and blood samples, followed by an at-home phase during which postprandial responses to both standardized tests and ad libitum meals were recorded. Stool samples were collected at home before the in-person clinical visit. The PREDICT 2 study (NCT03983733) had a similar collection protocol to PREDICT 1 but was conducted entirely remotely and included data from 975 people from 48 US states (including the federal District of Columbia and without participants from North Dakota and Hawaii). The PREDICT 3 cohorts (US21, US22A and UK22A) are research cohorts (NCT04735835) embedded within the ZOE commercial product. Participants provide informed written consent for their data to be used for scientific research purposes. In total, 32,621 samples (n = 11,798 for US21, n = 8,470 for US22A and n = 12,353 UK22A) were collected and retrieved. The studies were fully remote, participants completed health and food questionnaires at baseline, and self-collected and shipped stool samples. Cardiometabolic markers were collected as described below. Furthermore, we considered and analysed two registered clinical nutritional intervention studies, namely METHOD36 (NCT05273268) and BIOME37 (NCT06231706), focusing on the microbiome changes and their links with the two derived SGB-level rankings (ZOE MB health-ranks and diet-ranks). All study protocols are registered and available on clinicaltrials.gov through the clinical trials number and link affiliated with each trial.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sample collection, DNA extraction and sequencing&lt;/head&gt;
    &lt;p&gt;For the PREDICT 1 cohort, sample collection, DNA extraction and sequencing were described previously9. The PREDICT 2 samples were collected in Zymo buffer, DNA extraction was performed at QIAGEN Genomic Services using DNeasy 96 PowerSoil Pro, and sequencing was performed on the Illumina NovaSeq 6000 platform using the S4 flow cell and targeting 7.5 Gb per sample. The PREDICT 3 samples were self-collected into tubes containing the DNA-Shield Zymo buffer. Sample processing was performed by Zymo and Prebiomics. In brief, DNA extraction by Zymo used the ZymoBIOMICS-96 MagBead DNA kit, whereas Prebiomics used the DNeasy 96 PowerSoil Pro kits. Sequencing libraries were prepared using the Illumina DNA Prep Tagmentation kit, following the manufacturer’s guidelines. Whole-genome shotgun metagenomic sequencing on the Illumina NovaSeq 6000 platform used the S4 flow cell and targetted 3.75 Gb per sample.&lt;/p&gt;
    &lt;p&gt;All raw sequenced data were quality controlled using the preprocessing pipeline available at https://github.com/SegataLab/preprocessing, which comprises three steps: (1) removal of reads with low-quality (Q &amp;lt; 20), too short (length under 75 nt), or with more than two ambiguous bases; (2) removal of host contaminant DNAs (Illumina’s spike-in phiX 174 and human genomes, hg19); and (3) synchronization of paired-end and unpaired reads.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dietary data processing&lt;/head&gt;
    &lt;p&gt;In the PREDICT cohorts, we assessed long-term food intakes using FFQs, which were largely consistent across cohorts. Specifically, for PREDICT 1 participants (UK), we used a modified 131-item European Prospective Investigation into Cancer and Nutrition (EPIC) FFQ52. Participants in PREDICT 2 (USA) were surveyed using a similarly validated Diet History Questionnaire-III FFQ, including 135 items about food and beverages, as well as 26 questions about dietary supplements53. In PREDICT 3 UK22A and US22A, we developed and used a 264-item FFQ adapted from the EPIC-Norfolk Study FFQ and the Diet History Questionnaire-III. Consequently, there is a large overlap between the food items collected across the FFQs; for example, 90% of questions in the EPIC FFQ are included in the PREDICT 3 FFQ. This FFQ also includes additional food items to accurately capture modern eating habits—a limitation of older FFQ versions54. In the PREDICT 3 US21 cohort, FFQs were not collected, and only short-term logged dietary data collected using the ZOE mobile phone app were used instead.&lt;/p&gt;
    &lt;p&gt;Starting from both long- and short-term dietary data, we computed three versions of the PDI55, namely, the overall PDI, the healthful PDI (measuring the adherence to a healthier plant-based foods diet) and the unhealthy PDI (measuring the intake of unhealthful plant-based foods), as well as the healthy eating index23 (measuring how consumed foods align with dietary guidelines), the alternative Mediterranean diet score (measuring the adherence to a Mediterranean diet)56 and the Healthy Food Diversity (HFD) index (measuring the number, distribution and health value of consumed foods)57. Specifically, to calculate PDIs and the healthy eating index, food items were first assembled into food groups by mapping them onto a ‘food tree’ consisting of a database of nutrient information arranged according to a hierarchical tree structure: level 1 (9 food groups), level 2 (52 food groups) and level 3 (195 food groups). UK foods were mapped onto the Composition of Foods Integrated Dataset (CoFID)58 using food categories or sub-group codes, whereas US foods were similarly mapped onto the US Department of Agriculture Food and Nutrient Database for Dietary Studies database. Level 3 foods were aggregated and harmonized by nutrition scientists to allow for comparisons across cohorts. The Mediterranean diet and HFD scores were calculated as described previously9.&lt;/p&gt;
    &lt;head rend="h3"&gt;Host health and anthropometric marker collection&lt;/head&gt;
    &lt;p&gt;In PREDICT 1, sex and age were self-reported, whereas height, weight and blood pressure were measured at a clinic visit (day 0). At the clinic visit, participants were also fitted with wearable continuous glucose monitor CGM) devices (Abbott Freestyle Libre Pro (FSL)), visceral fat mass was measured using dual-energy X-ray absorptiometry scans following standard manufacturer’s recommendations (DXA; Hologic QDR 4500 plus) and fasting GlycA was measured using a high-throughput NMR metabolomics (Nightingale Health) 2016 panel. Fasting and postprandial venous blood samples were also collected at the clinic; plasma glucose and serum total cholesterol, HDL-C and triglycerides were measured using Affinity 1.0, and whole blood HbA1c% was measured using Viapath. The ten-year ASCVD risk was calculated as per the 2019 American College of Cardiology (ACC) and American Heart Association (AHA) clinical guidelines59. Additional data were collected over the subsequent 13-day period at home; postprandial responses to eight standardized meals (seven in duplicate) of differing macronutrient (fat, carbohydrate, protein and fibre) content were measured using CGMs and dried-blood-spot analysis as described previously13. T2D and hyperlipidemia were self-reported via health questionnaires. The PREDICT 2 and PREDICT 3 studies were fully remote. Sex, age, height, weight and blood pressure were self-reported, and fasting and postprandial responses for total cholesterol, HDL-C, triglycerides and HbA1c were assessed using whole blood finger-prick samples collected at home using dried-blood-spot analysis by commercial laboratories (CRL, Eurofins Biomnis). CGMs were fitted at home by participants. A selection of standardized meals smaller than in PREDICT 1 was tested in PREDICT 2 and PREDICT 3 (a metabolic challenge meal, and medium-fat and carbohydrate breakfast and lunch meals). Some of the considered markers represent the same metabolic function over time and showed positive correlations between their fasting and postprandial measurements, whereas others represent opposite types of the same biomolecular pathway and showed negative correlations among them (Supplementary Table 4).&lt;/p&gt;
    &lt;head rend="h3"&gt;Public human microbiome datasets&lt;/head&gt;
    &lt;p&gt;We leveraged 27,011 public metagenomic samples from 107 cohorts available through the curated MetagenomicData 3 (cMD3) resource60,61 to define the cohorts used for the meta-analyses on BMI and healthy–diseased comparison (‘Statistical and meta-analyses’). For the meta-analysis on BMI, we selected cohorts with stool microbiome samples from healthy participants (self-assessed, not reporting a diagnosis), aged at least 16 years, BMI ≥ 18.5 and sex information available. Cohorts with fewer than 30 people were excluded. Furthermore, the ThomasAM_2018_c and LeChatelier_2013 cohorts were excluded as duplicates in the YachidaS_2019 and NiesenHB_2014 cohorts, respectively. Overall, 6,182 samples from 34 different cohorts and 20 countries were retrieved. Participants were classified into three categories: healthy weight (BMI ≥ 18.5 and &amp;lt;25), overweight (BMI ≥ 25 and &amp;lt;30) and obese (BMI ≥ 30). Then, each combination of country, dataset and two BMI categories was tested if at least 15 samples were retained. These led to analysing a total of 5,348 samples from 27 cohorts (2,837 healthy weight, 1,562 overweight and 949 obese participants; Supplementary Table 9). For the health–diseased meta-analyses, we selected from cMD3 participants aged at least 16 years, BMI ≥ 18.5 and the sex information available that were part of a case–control study of one of the following diseases: CRC, IBD (including ulcerative colitis and Crohn’s disease), T2D, IGT and ASCVD. Studies with fewer than 30 people were excluded. In total, we considered ten datasets of CRC (650 cases and 645 controls), two datasets of IGT (273 cases and 492 controls), five datasets of T2D (775 cases and 900 controls), three datasets of IBD (103 controls, 59 of which used in two different comparisons, 60 individuals with Crohn’s disease and 68 individuals with ulcerative colitis) and three datasets of CVD (283 cases and 508 controls). Notably, German and French participants of the MetaCardis cohort were separated, and this led to a set of 449 controls used in both the T2D and the IGT analyses, whereas only the 176 controls from France were used in the CVD analysis. Overall, the total number of samples analysed was N = 4,816 (2,707 controls and 2,109 cases) from 25 cohorts and 10 countries (Supplementary Table 20). The cohort selection for the two analyses used the script https://github.com/waldronlab/curatedMetagenomicDataAnalyses/blob/main/python_tools/meta_analysis_data.py available in cMD3.&lt;/p&gt;
    &lt;head rend="h3"&gt;Microbiome taxonomic profiling&lt;/head&gt;
    &lt;p&gt;All microbiome samples from the PREDICT cohorts were profiled using MetaPhlAn 4 (v.4.beta.2, database vJan21_CHOCOPhlAnSGB_202103), without performing read subsampling, as the benefit of occasionally detecting a few additional low-abundance species in samples with a higher number of reads outweighs the potential noise from uneven sequencing depth. Samples retrieved from cMD3 (described in ‘Public human microbiome datasets’) were profiled with MetaPhlAn 4 (v.4.beta.1, database vJan21_CHOCOPhlAnSGB_202103) using default parameters in both cases (among default parameters, the stat_q is set to 0.2 by default, which defines the quantiles for the robust average coverage calculation), which precludes the necessity for additional prevalence filters considering its default parameters are tailored for the taxonomic profiling of human microbiome samples19. MetaPhlAn 4 is a publicly available taxonomic profiler for metagenomic samples (Github repository: https://github.com/biobakery/MetaPhlAn) that leverages medium and high-quality genomes from isolates and metagenome-assembled genomes (MAGs). Isolate genomes and MAGs are clustered at 95% average nucleotide identity to define SGBs, as described previously20. If an SGB cluster contains a genome isolate, then it is referred to by that isolate’s taxonomic label. If an SGB contains only MAGs, then it represents an unknown species cluster and is assigned the taxonomic label of a genus, family or phylum, according to which is the genomically closest to a taxonomic label from isolate genomes. As the taxonomic classification of MetaPhlAn depends on species-specific marker genes, sometimes there are several SGBs of very closely related genomes for which the identification of SGB-specific markers is not feasible. In this case, more than one SGB can be considered together, and the label ‘_group’ is appended to the representative SGB ID. In this way, MetaPhlAn 4 improves the resolution of the taxonomic profiling task62.&lt;/p&gt;
    &lt;head rend="h3"&gt;Rankings definition&lt;/head&gt;
    &lt;p&gt;We first identified a subset of prevalent SGBs to ensure a minimum number of non-zero relative abundance values. In each PREDICT cohort, we selected markers that are intermediary measures of host health or diet health, and they were organized into four categories: personal, dietary, fasting and postprandial (Supplementary Table 2). Second, we calculated the partial Spearman’s correlation between each SGB and health markers, adjusting for sex, age and BMI, using the ‘pingouin’ Python package (v.0.5.4, https://github.com/raphaelvallat/pingouin) (Extended Data Figs. 3 and 5). The relative abundance values of SGBs (including zeros) were used as input for the correlations. Third, the SGB-marker partial correlations were sorted ascending if the marker was considered as positive with respect to health, or descending if the marker was considered as negative. These sorted partial correlations were ranked and normalized according to cohort sample sizes into percentiles ranging from 0 to 1 (function pandas.DataFrame.rank with param pct=True from pandas v.2.1.3) (Fig. 2b,c and Extended Data Figs. 2 and 4). Fourth, for each category of markers, we computed the average percentiles across markers (Fig. 2a and Extended Data Fig. 4). SGBs were retained in the overall rankings if they were ranked in at least two different cohorts, leading to a final ranking of 661 SGBs. Finally, the ZOE MB health-rank 2025 was defined by first averaging the personal, fasting and postprandial category percentiles within each cohort, and then averaging these cohort-specific averages. The ZOE MB diet-rank 2025 instead was defined by averaging the dietary percentiles across all cohorts (Fig. 2a, Extended Data Fig. 4 and Supplementary Table 5). The ZOE Microbiome Rankings are also available at https://zoe.com/our-science/microbiome-ranking.&lt;/p&gt;
    &lt;head rend="h3"&gt;Machine learning&lt;/head&gt;
    &lt;p&gt;To assess the link to the human gut microbiome composition, we developed and used a machine learning framework based on random forest classification and regression algorithms from the scikit-learn (v.1.3.2) Python package (as implemented in the RandomForestClassifier and RandomForestRegressor functions, respectively), both with ‘n_estimators=1000’ and ‘max_features=sqrt’ parameters63. We trained random forest classifiers and regressors on MetaPhlAn 4-estimated SGB-level relative abundances (arcsine square-root transformed) to assess the extent to which the outcome variable was predictable from the microbiome as a proxy of the strength of the microbiome–variable association. This framework was used and described originally in ref. 9 and accounts for the presence of twin pairs in the data, which avoids biases due to identical values in twins. In brief, the framework uses a cross-validation approach, splitting the dataset randomly into training and testing folds with an 80:20 ratio, respectively, and repeated 100 times (as implemented in the StratifiedShuffleSplit function). Folds are also constructed to maintain a similar ratio of the two classes to predict as they appear in the full data. For target variables with continuous values, classification was performed by contrasting the first against the fourth quartile, the first three against the fourth quartile and the first against the last three quartiles. Performances were evaluated using the AUC for the classification task, whereas Spearman’s correlation between the real and predicted values was used for the regression task22.&lt;/p&gt;
    &lt;head rend="h3"&gt;Statistical and meta-analyses&lt;/head&gt;
    &lt;p&gt;We performed a meta-analysis to determine the possible links between BMI (categorized into ‘healthy weight’, ‘overweight’ and ‘obese’) and our ranked SGBs across various publicly available studies comprising a total of 5,348 people who were not diagnosed with any specific disease. We first evaluated the ZOE MB health- and diet-ranks by assessing the cumulative relative abundance and richness of the 50 most favourable and the 50 least favourable SGBs in each dataset in each BMI category: healthy weight, overweight and obese (see ‘Public human microbiome datasets’ for the specific cut-offs). Specifically, we assessed the number of intra-dataset, between-BMI groups pairwise comparisons in which the group median abundance or the group median count was higher in the lower BMI group (when considering most favourable SGBs from both ranks) or higher in the higher BMI group (when looking at least favourable SGBs). Next, we fit linear models for each dataset and pair of BMI categories: healthy weight versus overweight, healthy weight versus obese, and overweight versus obese. In the first model, we looked at the count of the 50 most favourable and unfavourable ZOE MB health- and diet-ranked SGBs. A second model was fitted on the cumulative relative abundance (arcsine square-root transformed) of the 50 most favourable and unfavourable SGBs in the two rankings. All models were adjusted by sex and age. Cohen’s d was used to estimate the effect size of the normalized difference between unfavourable and favourable ranked SGBs when considering cumulative abundances. This quantifies the difference between the means of two groups in terms of standard deviations. Specifically, as originally defined, a ‘small’ effect size corresponds to d = 0.2, a ‘medium’ effect size to d = 0.5 and a ‘large’ effect size to d = 0.8 (ref. 64). In these models, the lower BMI category of each comparison was used as the negative control, so negative coefficients reflect a higher count of SGBs in the lower BMI category, whereas positive coefficients reflect a higher count of SGBs in the higher BMI category. Effect sizes were summarized through meta-analysis, computed as a random-effect model using the Paule–Mandel heterogeneity on adjusted mean differences from the linear regression models (standardized for cumulative abundances). We assessed the presence of the 50 most favourable and most unfavourable SGBs from both the ZOE MB health- and diet-ranks among the countries considered in these analyses (18 in total) and when considering only people of healthy weight (n = 2,837). To link the ranked SGBs with the country, we fit a linear model on the count and cumulative relative abundance of the SGBs, and the models were adjusted by the sequencing depth of the study. We used ordinary least squares adjusted by sequencing depth when comparing two datasets from different countries, and linear mixed model blocked by dataset ID and adjusted by sequencing depth when comparing pairs of countries in which at least one country was represented by more than one dataset (country- and sequencing depth-adjusted P values are presented in Supplementary Table 11).&lt;/p&gt;
    &lt;p&gt;A second meta-analysis tested the associations between our ZOE MB health- and diet-ranked SGBs and five gut-associated diseases (CVD, T2D, IBD, CRC and IGT) across studies, for a total of 4,816 samples (‘Public human microbiome datasets’). Linear models were used to predict the binary disease outcome (healthy versus diseased) for each disease, using the cumulative abundances (arcsine square-root transformed) of the 50 most favourable or unfavourable SGBs, adjusting by sex, age and BMI. The betas of the linear models were converted into SMDs as described previously65. We also defined models to predict healthy versus diseased using the sum of the SGB ranks normalized between −1 and 1, considering all 661 SGBs for the ZOE MB health- and diet-ranks, once using the direct sum of the SGB ranks and once weighting ranks by the relative abundance of each SGB in each sample (transformed using the arcsine and square-root function to avoid overestimating the ranks of highly abundant species due to compositionality). SMDs were calculated similarly to those in the previous case. In all meta-analytical models, the set of cohorts considered comprised studies encompassing several diseases with a shared control group that we analysed separately. To account for the overlaps in the studies considered, we computed weights based on the inverse effect sizes variance-covariance matrix, as suggested previously66,67. Thus, five meta-analyses were performed, one for each disease: CVD (three datasets), T2D (six datasets), IBD (three datasets), CRC (ten datasets) and IGT (two datasets). Of note, in the comparisons of controls versus T2D, IGT and CVD, the MetaCardis French and German sub-cohorts were considered as different datasets, and their controls were meta-analysed as different cohorts. In particular, only French control samples were used in the CVD analysis, which included only French cases. Finally, meta-analysis summaries were computed using the same technique. Analyses we carried out with Python (v.3.12.0), using also the following libraries: numpy (v.1.26.2), scipy (v.1.11.4), statsmodels (v.0.14.0), and matplotlib (v.3.8.2) and seabron (v.0.11.2) for visualization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ethical compliance&lt;/head&gt;
    &lt;p&gt;All study protocols are registered on clinicaltrials.gov and procedures are compliant with all relevant ethical regulations. Ethical approval for the PREDICT 1 study was obtained in the UK from the King’s College London Research Ethics Committee (REC) and Integrated Research Application System (IRAS 236407), and in the USA from the institutional review board (Partners Healthcare Institutional Review Board (IRB) 2018P002078). Ethical approval for the PREDICT 2 study (Pro00033432) was obtained from Advarra IRB. Ethical approval for the PREDICT 3 study (Pro00044316, HR/DP-21/22-28300 and HR/DP-24/25-45829) was obtained from Advarra IRB and King’s College London REC. Ethical approval for the METHOD study (Pro00044316; protocol no. 00044316) was obtained from Advarra IRB. Ethical approval for the BIOME study (HR/DP-23/24-39673) was obtained through King’s College London REC. All participants provided written informed consent and all studies were carried out in accordance with the Declaration of Helsinki and Good Clinical Practice.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reporting summary&lt;/head&gt;
    &lt;p&gt;Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data availability&lt;/head&gt;
    &lt;p&gt;The human genome version used in the preprocessing of the microbiome samples is the GRCh37 genome assembly (hg19, GCA_000001405.1). Raw metagenomic samples, along with metadata information (sex, age, BMI and country) and microbiome profiles for all participants of the ZOE PREDICT Studies, are publicly available. Metagenomes from PREDICT 1 are publicly available as previously reported9, whereas the PREDICT 2 and PREDICT 3 cohorts (US21, US22A and UK22A) are deposited in the European Nucleotide Archive (ENA) of the European Bioinformatics Institute (EBI) under accession numbers PRJEB75460, PRJEB75462, PRJEB75463 and PRJEB75464, and are publicly accessible. Sex, age, BMI, country and quantitative taxonomic profiles for each sample are publicly available within the curated MetagenomicData package60 and at Zenodo (https://doi.org/10.5281/zenodo.15307999)68. The full list of species for the ZOE Microbiome Rankings are publicly available at https://zoe.com/our-science/microbiome-ranking, where future updates will also be made available. The version of the ZOE Microbiome Rankings discussed in the present work is reported in Supplementary Table 5. To protect participant privacy, individual participant clinical data are not publicly available and cannot be deposited in public repositories. Researchers may request access to the restricted data by submitting a research proposal via email to data.papers@joinzoe.com. All proposals will be reviewed by a sub-panel of the ZOE Scientific Advisory Board within 4 working weeks. Proposals, researchers or institutions requesting data will be approved if they meet the standard criteria related to ethics, privacy and data protection regulations. Approved researchers are required to enter into a data-sharing agreement with ZOE. The requested host parameters will be provided as ordered data points without loss of reproducibility, as the analysis of this work (including deriving the ranks) was performed using non-parametric statistics. These data are available at Zenodo (https://doi.org/10.5281/zenodo.17236382)69 and are encrypted; access to the data will be granted to researchers whose proposals are approved. All data from non-PREDICT external public cohorts used to validate the rankings are available in full at Zenodo (https://doi.org/10.5281/zenodo.17236261)70.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code availability&lt;/head&gt;
    &lt;p&gt;The custom Python code developed for the meta-analyses performed on public data and included in this work is available at GitHub (https://github.com/SegataLab/inverse_var_weight) and at Zenodo (https://doi.org/10.5281/zenodo.17236261)70. The MetaPhlAn code for the taxonomic profiling is available at GitHub (https://github.com/biobakery/MetaPhlAn), Zenodo (https://doi.org/10.5281/zenodo.17236261)70 and Bioconda (https://bioconda.github.io/recipes/metaphlan/README.html).&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;World Health Organization. Global Health Estimates: Leading Causes of Death https://go.nature.com/48bFXYT (accessed 3 June 2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Guasch-Ferré, M. et al. Precision nutrition for cardiometabolic diseases. Nat. Med. 31, 1444–1453 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Vos, T. et al. Global burden of 369 diseases and injuries in 204 countries and territories, 1990–2019: a systematic analysis for the Global Burden of Disease Study 2019. Lancet 396, 1204–1222 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rauber, F. et al. Ultra-processed foods and excessive free sugar intake in the UK: a nationally representative cross-sectional study. BMJ Open 9, e027546 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Estruch, R. et al. Primary prevention of cardiovascular disease with a Mediterranean diet supplemented with extra-virgin olive oil or nuts. N. Engl. J. Med. 378, e34 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yamada, T., Kimura-Koyanagi, M., Sakaguchi, K., Ogawa, W. &amp;amp; Tamori, Y. Obesity and risk for its comorbidities diabetes, hypertension, and dyslipidemia in Japanese individuals aged 65 years. Sci. Rep. 13, 2346 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Coral, D. E. et al. Subclassification of obesity for precision prediction of cardiometabolic diseases. Nat. Med. 31, 534–543 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rubino, F. et al. Definition and diagnostic criteria of clinical obesity. Lancet Diabetes Endocrinol. 13, 221–262 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asnicar, F. et al. Microbiome connections with host metabolism and habitual diet from 1,098 deeply phenotyped individuals. Nat. Med. 27, 321–332 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Karlsson, F. H. et al. Gut metagenome in European women with normal, impaired and diabetic glucose control. Nature 498, 99–103 (2013).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wang, D. D. et al. The gut microbiome modulates the protective association between a Mediterranean diet and cardiometabolic disease risk. Nat. Med. 27, 333–343 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Valles-Colomer, M. et al. Cardiometabolic health, diet and the gut microbiome: a meta-omics perspective. Nat. Med. 29, 551–561 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Berry, S. E. et al. Human postprandial responses to food and potential for precision nutrition. Nat. Med. 26, 964–973 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ianiro, G. et al. Variability of strain engraftment and predictability of microbiome composition after fecal microbiota transplantation across different diseases. Nat. Med. 28, 1913–1923 (2022).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zhao, X. et al. Response of gut microbiota to metabolite changes induced by endurance exercise. Front. Microbiol. 9, 765 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;David, L. A. et al. Diet rapidly and reproducibly alters the human gut microbiome. Nature 505, 559–563 (2014).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Magkos, F., Hjorth, M. F. &amp;amp; Astrup, A. Diet and exercise in the prevention and treatment of type 2 diabetes mellitus. Nat. Rev. Endocrinol. 16, 545–555 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yatsunenko, T. et al. Human gut microbiome viewed across age and geography. Nature 486, 222–227 (2012).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Blanco-Míguez, A. et al. Extending and improving metagenomic taxonomic profiling with uncharacterized species using MetaPhlAn 4. Nat. Biotechnol. 41,1633–1644 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pasolli, E. et al. Extensive unexplored human microbiome diversity revealed by over 150,000 genomes from metagenomes spanning age, geography, and lifestyle. Cell 176, 649–662 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Connelly, M. A., Otvos, J. D., Shalaurova, I., Playford, M. P. &amp;amp; Mehta, N. N. GlycA, a novel biomarker of systemic inflammation and cardiovascular disease risk. J. Transl. Med. 15, 219 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asnicar, F., Thomas, A. M., Passerini, A., Waldron, L. &amp;amp; Segata, N. Machine learning for microbiologists. Nat. Rev. Microbiol. 22, 191–205 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Krebs-Smith, S. M. et al. Update of the healthy eating index: HEI-2015. J. Acad. Nutr. Diet. 118, 1591–1602 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Satija, A. et al. Plant-based dietary patterns and incidence of type 2 diabetes in US men and women: results from three prospective cohort studies. PLoS Med. 13, e1002039 (2016).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yassour, M. et al. Sub-clinical detection of gut microbial biomarkers of obesity and type 2 diabetes. Genome Med. 8, 17 (2016).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zhai, L. et al. Ruminococcus gnavus plays a pathogenic role in diarrhea-predominant irritable bowel syndrome by increasing serotonin biosynthesis. Cell Host Microbe 31, 33–44 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Coello, K. et al. Gut microbiota composition in patients with newly diagnosed bipolar disorder and their unaffected first-degree relatives. Brain Behav. Immun. 75, 112–118 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lloyd-Price, J. et al. Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases. Nature 569, 655–662 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Franzosa, E. A. et al. Gut microbiome structure and metabolic activity in inflammatory bowel disease. Nat Microbiol 4, 293–305 (2019).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mbaye, B. et al. Increased fecal ethanol and enriched ethanol-producing gut bacteria Limosilactobacillus fermentum, Enterocloster bolteae, Mediterraneibacter gnavus and Streptococcus mutans in nonalcoholic steatohepatitis. Front. Cell. Infect. Microbiol. 13, 1279354 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Caroline de Oliveira Melo, N. et al. Oral administration of viable or heat-inactivated Lacticaseibacillus rhamnosus GG influences on metabolic outcomes and gut microbiota in rodents fed a high-fat high-fructose diet. J. Funct. Foods 109, 105808 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zhao, L. et al. Gut bacteria selectively promoted by dietary fibers alleviate type 2 diabetes. Science 359, 1151–1156 (2018).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Petzoldt, D., Breves, G., Rautenschlein, S. &amp;amp; Taras, D. Harryflintia acetispora gen. nov., sp. nov., isolated from chicken caecum. Int. J. Syst. Evol. Microbiol. 66, 4099–4104 (2016).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Koh, A., De Vadder, F., Kovatcheva-Datchary, P. &amp;amp; Bäckhed, F. From dietary fiber to host physiology: short-chain fatty acids as key bacterial metabolites. Cell 165, 1332–1345 (2016).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Carrasco, J. L. &amp;amp; Jover, L. Estimating the generalized concordance correlation coefficient through variance components. Biometrics 59, 849–858 (2003).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bermingham, K. M. et al. Effects of a personalized nutrition program on cardiometabolic health: a randomized controlled trial. Nat. Med. 30, 1888–1897 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creedon, A. C. et al. A diverse high-fibre plant-based dietary intervention improves gut microbiome composition, gut symptoms, energy and hunger in healthy adults: a randomised controlled trial. Preprint at medRxiv https://doi.org/10.1101/2024.07.02.24309816 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nguyen, N. K. et al. Gut microbiota modulation with long-chain corn bran arabinoxylan in adults with overweight and obesity is linked to an individualized temporal increase in fecal propionate. Microbiome 8, 118 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Belenguer, A. et al. Two routes of metabolic cross-feeding between Bifidobacterium adolescentis and butyrate-producing anaerobes from the human gut. Appl. Environ. Microbiol. 72, 3593–3599 (2006).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Falony, G., Vlachou, A., Verbrugghe, K. &amp;amp; De Vuyst, L. Cross-feeding between Bifidobacterium longum BB536 and acetate-converting, butyrate-producing colon bacteria during growth on oligofructose. Appl. Environ. Microbiol. 72, 7835–7841 (2006).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Duncan, S. H., Louis, P. &amp;amp; Flint, H. J. Lactate-utilizing bacteria, isolated from human feces, that produce butyrate as a major fermentation product. Appl. Environ. Microbiol. 70, 5810–5817 (2004).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hj, L. P. F. Diversity, metabolism and microbial ecology of butyrate-producing bacteria from the human large intestine. FEMS Microbiol. 294, 1–8 (2009).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fackelmann, G. et al. Gut microbiome signatures of vegan, vegetarian and omnivore diets and associated health outcomes across 21,561 individuals. Nat. Microbiol. 10, 41–52 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Valles-Colomer, M. et al. The person-to-person transmission landscape of the gut and oral microbiomes. Nature 614, 125–135 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Carlino, N. et al. Unexplored microbial diversity from 2,500 food metagenomes and links with the human microbiome. Cell 187, 5775–5795 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Beghini, F. et al. Integrating taxonomic, functional, and strain-level profiling of diverse microbial communities with bioBakery 3. eLife 10, e65088 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zeevi, D. et al. Personalized nutrition by prediction of glycemic responses. Cell 163, 1079–1094 (2015).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tap, J. et al. Gut microbiota richness promotes its stability upon increased dietary fibre intake in healthy adults. Environ. Microbiol. 17, 4954–4964 (2015).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Oliver, A. et al. High-fiber, whole-food dietary intervention alters the human gut microbiome but not fecal short-chain fatty acids. mSystems 6, e00115-21 (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manghi, P. et al. Coffee consumption is associated with intestinal Lawsonibacter asaccharolyticus abundance and prevalence across multiple cohorts. Nat. Microbiol. 9, 3120–3134 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Berry, S. et al. Personalised REsponses to DIetary Composition Trial (PREDICT): an intervention study to determine inter-individual differences in postprandial response to foods. Preprint at Res. Sq. https://doi.org/10.21203/rs.2.20798/v1 (2020).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bingham, S. A. et al. Nutritional methods in the European Prospective Investigation of Cancer in Norfolk. Public Health Nutr. 4, 847–858 (2001).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Subar, A. F. et al. Comparative validation of the Block, Willett, and National Cancer Institute food frequency questionnaires: the Eating at America’s Table Study. Am. J. Epidemiol. 154, 1089–1099 (2001).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wennberg, M., Kastenbom, L., Eriksson, L., Winkvist, A. &amp;amp; Johansson, I. Validation of a digital food frequency questionnaire for the Northern Sweden Diet Database. Nutr. J. 23, 83 (2024).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Satija, A. et al. Healthful and unhealthful plant-based diets and the risk of coronary heart disease in US adults. J. Am. Coll. Cardiol. 70, 411–422 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fung, T. T. et al. Diet-quality scores and plasma concentrations of markers of inflammation and endothelial dysfunction. Am. J. Clin. Nutr. 82, 163–173 (2005).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Vadiveloo, M., Dixon, L. B., Mijanovich, T., Elbel, B. &amp;amp; Parekh, N. Development and evaluation of the US Healthy Food Diversity index. Br. J. Nutr. 112, 1562–1574 (2014).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Roe, M., Pinchen, H., Church, S. &amp;amp; Finglas, P. McCance and Widdowson’s The composition of Foods Seventh summary edition and updated composition of foods integrated dataset. Nutr. Bull. 40, 36–39 (2015).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Goff, D. C. Jr. et al. 2013 ACC/AHA guideline on the assessment of cardiovascular risk: a report of the American College of Cardiology/American Heart Association Task Force on Practice Guidelines. Circulation 129, S49–S73 (2014).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pasolli, E. et al. Accessible, curated metagenomic data through ExperimentHub. Nat. Methods 14, 1023–1024 (2017).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manghi, P. et al. Meta-analysis of 22,710 human metagenomes defines an index of oral to gut microbial introgression and associations with age, sex, BMI, and diseases. Nat. Commun. https://doi.org/10.1038/s41467-025-66888-1 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manghi, P. et al. MetaPhlAn 4 profiling of unknown species-level genome bins improves the characterization of diet-associated microbiome changes in mice. Cell Rep. 42, 112464 (2023).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pedregosa, F. et al. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830 (2011).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cohen, J. Statistical Power Analysis for the Behavioral Sciences 2nd edn (Routledge, 1988).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nakagawa, S. &amp;amp; Cuthill, I. C. Effect size, confidence interval and statistical significance: a practical guide for biologists. Biol. Rev. Camb. Philos. Soc. 82, 591–605 (2007).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lin, D.-Y. &amp;amp; Sullivan, P. F. Meta-analysis of genome-wide association studies with overlapping subjects. Am. J. Hum. Genet. 85, 862–872 (2009).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gleser, L. J. &amp;amp; Olkin, I. Stochastically Dependent Effect Sizes. Report No. OLK NSF 289 (Department of Statistics, Stanford Univ., 1992).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asnicar, F. et al. Gut microbes associated with health, nutrition and dietary interventions. Zenodo https://doi.org/10.5281/zenodo.15307999 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asnicar, F. Gut microbes associated with health, nutrition and dietary interventions. Zenodo https://doi.org/10.5281/zenodo.17236382 (2025).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asnicar, F. Gut microbes associated with health, nutrition and dietary interventions. Zenodo https://doi.org/10.5281/zenodo.17236261 (2025).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We thank all the participants of the PREDICT programme. This work was supported by Zoe Ltd. and TwinsUK, which is funded by the Wellcome Trust, Medical Research Council, Versus Arthritis, European Union Horizon 2020, Chronic Disease Research Foundation (CDRF), the National Institute for Health Research (NIHR) Clinical Research Network (CRN) and Biomedical Research Centre based at Guy’s and St Thomas’ NHS Foundation Trust in partnership with King’s College London. It was also supported by the European Research Council (ERC-CoG microTOUCH-101045015) to N.S., by the European Union NextGenerationEU (Interconnected Nord-Est Innovation programme, INEST) to N.S., by the National Cancer Institute of the National Institutes of Health (1U01CA230551) to N.S. and by the Premio Internazionale Lombardia e Ricerca 2019 to N.S.&lt;/p&gt;
    &lt;head rend="h2"&gt;Author information&lt;/head&gt;
    &lt;head rend="h3"&gt;Authors and Affiliations&lt;/head&gt;
    &lt;head rend="h3"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;F. Asnicar, S.E.B., T.D.S. and N.S. conceived and supervized the study. F. Asnicar, P.M., E.P., K.M. and A.A. performed the analyses: F. Asnicar collected the data, performed microbiome analyses and species rankings; P.M. performed meta-analyses on public data; E.P. supported the analysis of longitudinal data; K.M. helped with microbiome profiling; A.A. supported the analysis of the two clinical trial, nutritional intervention studies. E.B., S.G., F.G. and R.D. set up the interface to retrieve clinical data information. G.F., G.B., L.R., G.P., F. Amati and K.M.B. supported with results interpretation. J.W. is co-founder of ZOE Ltd. and made the assembly of these large cohorts possible. F. Asnicar, P.M. and N.S. drafted the manuscript. All authors reviewed and edited the manuscript.&lt;/p&gt;
    &lt;head rend="h3"&gt;Corresponding authors&lt;/head&gt;
    &lt;head rend="h2"&gt;Ethics declarations&lt;/head&gt;
    &lt;head rend="h3"&gt;Competing interests&lt;/head&gt;
    &lt;p&gt;J.W. and T.D.S. are co-founders of ZOE Ltd.—a commercial initiative active in the field of personalized nutrition—and owners of the ZOE PREDICT studies. E.B., F. Amati, A.A., S.G., F.G., R.D., J.W. and K.M.B. are, or have been, employees of Zoe Ltd. F. Asnicar, S.E.B., T.D.S. and N.S. are consultants to ZOE Ltd. F. Asnicar, R.D., J.W., S.E.B., T.D.S. and N.S. receive options with ZOE Ltd. All other authors declare no competing interests. Zoe Ltd. holds the following patent applications on the SGBs ranking: PCT (World) patent pending applications PCT/EP2024/058262, PCT/EP2024/058286 and PCT/EP2024/058290.&lt;/p&gt;
    &lt;head rend="h2"&gt;Peer review&lt;/head&gt;
    &lt;head rend="h3"&gt;Peer review information&lt;/head&gt;
    &lt;p&gt;Nature thanks Johanna M. Geleijnse, Reiner Jumpertz-von Schwartzenberg, Matthew Olm, Daniel Tancredi and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional information&lt;/head&gt;
    &lt;p&gt;Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extended data figures and tables&lt;/head&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 1 Microbiome predictive potential for personal information, dietary indices, fasting, and postprandial metabolic markers, via classification and regression random forest models.&lt;/head&gt;
    &lt;p&gt;Distributions of the random forest median AUCs (a) and median Spearman’s correlation coefficients (b) (Methods) in the five cross-sectional PREDICT studies for the different clinical data divided into four categories: ‘Personal’, ‘Dietary’, ‘Fasting’, and ‘Postprandial’. The AUC and Spearman’s index thresholds of 0.7 and 0.3, respectively, are indicated with a dashed line. a) Each point represents the median AUC value obtained in cross-validation for each cohort when testing the first versus the fourth quartile of the corresponding clinical marker values on the x-axis. b) Each point represents the median Spearman’s correlation coefficient for the predicted values by the regressor and the true values in the cross-validation setting for each cohort.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 2 Detailed associations of the 15 top and bottom cardiometabolic-ranked SGBs in the PREDICT3 UK22A, PREDICT2, and PREDICT3 US21 cohorts.&lt;/head&gt;
    &lt;p&gt;The single-marker percentiles, divided into the three categories (‘Personal’, ‘Fasting’, and ‘Postprandial’) for the 15 most favorable and unfavorable ZOE MB Health-ranked SGBs the other three PREDICT cohorts not reported in Fig. 2 (a, PREDICT3 UK22A; b, PREDICT2, and c, PREDICT3 US21). Heatmaps with the single Spearman’s partial correlations for all PREDICT cohorts are available in Supplementary Fig. 4.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 3 Spearman’s partial correlations of the 15 top and bottom cardiometabolic-ranked SGBs.&lt;/head&gt;
    &lt;p&gt;a-e) Spearman’s partial correlations (corrected for age, sex, and BMI) between SGB relative abundance and single marker values show consistency across the five PREDICT cohorts. These partial correlations were ranked and averaged first within and then across the three data categories (‘Personal’, ‘Fasting’, and ‘Postprandial’, reported in Supplementary Fig. 3) separately in each cohort. The cohorts’ averages were then used to define the cardiometabolic rank (for those SGBs analyzed in at least two cohorts).&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 4 Diet associations of the 15 top and bottom diet-ranked SGBs.&lt;/head&gt;
    &lt;p&gt;a-e) For each PREDICT cohort, we computed Spearman’s partial correlation between the SGBs’ relative abundances and different diet indexes. Associations were ranked and averaged in each cohort separately. f) The ZOE MB Diet-ranking was computed for SGBs ranked in at least two PREDICT cohorts. The raw Spearman’s partial correlations are available in Supplementary Fig. 7.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 5 Spearman’s partial correlations of the 15 top and bottom diet-ranked SGBs.&lt;/head&gt;
    &lt;p&gt;a-e) Study-wise Spearman’s partial correlation coefficients (corrected for sex, age, and BMI) for the 15 most favorable and unfavorable ZOE MB Diet-ranked SGBs in different diet indexes. The associations appear consistent across cohorts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 6 Comparison of the ZOE MB Health and Diet ranks and with geography.&lt;/head&gt;
    &lt;p&gt;a) The ZOE MB Health and Diet ranks are overall in agreement (Spearman’s correlation = 0.72), albeit some SGBs show discordant rankings (absolute difference between the two ranks ≥ 0.3). These SGBs are highlighted in orange, and their ranks and taxonomy assignment are reported in Supplementary Table 6. b,c) Comparison of the ZOE MB Health (b) and Diet (c) ranks computed only on the PREDICT UK and US cohorts (Spearman’s correlations of 0.61 and 0.26, respectively). The top and right-side histograms depict the x and y-axis marginal distributions in each plot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 7 ZOE DIET ranks and their associations with BMI.&lt;/head&gt;
    &lt;p&gt;a) Comparison of the ZOE MB Diet-ranks (x-axis) with the Spearman’s partial correlations (corrected for sex and age, y-axis) for the 661 ranked SGBs in the five PREDICT cohorts. b) The number of the 50 most-favorably ranked SGBs (ZOE MB Health-rank, Richness) detected in different BMI categories, showed that increasing BMI, linked with increasing health risks, is reflected by a lower presence of favorable SGBs. On the other hand, c) unfavorably-ranked SGBs show an increasing count in higher-risk BMI categories. d,e) The box plots report the number of the 50 most favorable and unfavorable ZOE MB Diet-ranked SGBs of individuals stratified into three BMI categories (healthy-weight, overweight, and obese) in each PREDICT cohort. f,g) Similarly, the box plots represent the cumulative relative abundance of the 50 most favorable and unfavorable ZOE MB Diet-ranked SGBs in individuals categorized into the three BMI categories in each cohort. h,i) The box plots report the number of the 50 most favorably and most unfavorably ranked SGBs, ranked using the same markers and categories as in the ZOE MB Health-ranks (Methods), but partial correlations were corrected only for sex and age. j,k) Similarly, the box plots report the count of the 50 most favorable and unfavorable SGBs in the three BMI categories, with SGBs ranked according to their partial correlation with BMI, adjusted by sex and age. Only non-significant FDR-corrected P values (ns, P value &amp;gt; 0.01) from the Mann-Whitney U test are reported.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 8 Meta-analysis of the 50 most favorably and unfavorably ZOE MB Health-ranked SGBs in overweight vs obese and healthy-weight vs overweight individuals.&lt;/head&gt;
    &lt;p&gt;a) Overweight individuals tend to carry a higher number of the 50 most favorably ZOE MB Health-ranked SGBs than obese individuals (left); the 50 most unfavorably ranked SGBs are increased in obese individuals vs overweight individuals (Methods). b) Healthy-weight individuals tend to carry a higher number of the 50 most favorably ZOE MB Health-ranked SGBs than overweight individuals (left); the 50 most unfavorably ranked SGBs are found in similar amounts in healthy-weight and overweight individuals (Methods). Error bars represent the 95% confidence interval.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 9 Meta-analysis of the 50 most favorably and unfavorably ZOE DIET-ranked SGBs comparing individuals from different BMI categories.&lt;/head&gt;
    &lt;p&gt;a) Comparison of the number of the 50 most favorable Diet-ranked SGBs in pairs of BMI categories. Healthy-weight and overweight individuals tend to have a higher number of favorably-ranked SGBs than obese individuals (Methods). b) Comparison of the number of the 50 most unfavorably Diet-ranked SGBs in pairs of BMI categories. Obese individuals tend to have a higher number of unfavorably-ranked SGBs (Methods). Error bars represent the 95% confidence interval.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended Data Fig. 10 Significantly changing SGBs after dietary interventions show consistent patterns across cohorts in terms of relative abundance, prevalence, and ZOE MB Diet-ranks.&lt;/head&gt;
    &lt;p&gt;a) Distributions of the mean relative abundance of the significant SGBs for the probiotic and control arms of the BIOME cohort (relative to Fig. 4a). b) Distributions of the mean relative abundance of the significant SGBs for the control arm of the METHOD cohort (relative to Fig. 4b). c) Distributions of the prevalence of the significant SGBs of the BIOME cohort (relative to Fig. 4a) and d) of the METHOD cohort (relative to Fig. 4b). SGBs are separated into “increasing” and “decreasing”, depending on their trend in relative abundance values, showing that SGBs found to be increased in relative abundance are also more prevalent, while the opposite is observed for SGBs decreasing in relative abundance. e) Distributions of the ZOE MB Health ranks for the significant SGBs in the Probiotic and Control arms of the BIOME cohort and METHOD cohorts. f) Distributions of the ZOE MB Diet ranks for the significant SGBs in the Probiotic and Control arms of the BIOME and METHOD cohorts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Supplementary information&lt;/head&gt;
    &lt;head rend="h3"&gt;Supplementary Information&lt;/head&gt;
    &lt;p&gt;Supplementary Figs. 1–5 and table legends.&lt;/p&gt;
    &lt;head rend="h3"&gt;Supplementary Tables&lt;/head&gt;
    &lt;p&gt;Supplementary Tables 1–25.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rights and permissions&lt;/head&gt;
    &lt;p&gt;Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.&lt;/p&gt;
    &lt;head rend="h2"&gt;About this article&lt;/head&gt;
    &lt;head rend="h3"&gt;Cite this article&lt;/head&gt;
    &lt;p&gt;Asnicar, F., Manghi, P., Fackelmann, G. et al. Gut micro-organisms associated with health, nutrition and dietary interventions. Nature (2025). https://doi.org/10.1038/s41586-025-09854-7&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Received:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Accepted:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Published:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Version of record:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DOI: https://doi.org/10.1038/s41586-025-09854-7&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nature.com/articles/s41586-025-09854-7?lid=t94o71j7gslg"/><published>2026-01-17T11:14:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657122</id><title>ASCII characters are not pixels: a deep dive into ASCII rendering</title><updated>2026-01-17T12:19:00.012141+00:00</updated><content>&lt;doc fingerprint="74d7db3c780d01ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ASCII characters are not pixels: a deep dive into ASCII rendering&lt;/head&gt;
    &lt;p&gt;Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive!&lt;/p&gt;
    &lt;p&gt;One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example:&lt;/p&gt;
    &lt;p&gt;Try opening the “split” view. Notice how well the characters follow the contour of the square.&lt;/p&gt;
    &lt;p&gt;This renderer works well for animated scenes, like the ones above, but we can also use it to render static images:&lt;/p&gt;
    &lt;p&gt;The image of Saturn was generated with ChatGPT.&lt;/p&gt;
    &lt;p&gt;Then, to get better separation between different colored regions, I also implemented a cel shading-like effect to enhance contrast between edges. Try dragging the contrast slider below:&lt;/p&gt;
    &lt;p&gt;The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does.&lt;/p&gt;
    &lt;p&gt;I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters:&lt;/p&gt;
    &lt;p&gt;Source: cognition.ai&lt;/p&gt;
    &lt;p&gt;It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places:&lt;/p&gt;
    &lt;p&gt;This blurriness happens because the ASCII characters are being treated like pixels — their shape is ignored. It’s disappointing to see because ASCII art looks so much better when shape is utilized. I don’t believe I’ve ever seen shape utilized in generated ASCII art, and I think that’s because it’s not really obvious how to consider shape when building an ASCII renderer.&lt;/p&gt;
    &lt;p&gt;I started building my ASCII renderer to prove to myself that it’s possible to utilize shape in ASCII rendering. In this post, I’ll cover the techniques and ideas I used to capture shape and build this ASCII renderer in detail.&lt;/p&gt;
    &lt;p&gt;We’ll start with the basics of image-to-ASCII conversion and see where the common issue of blurry edges comes from. After that, I’ll show you the approach I used to fix that and achieve sharp, high-quality ASCII rendering. At the end, we’ll improve on that by implementing the contrast enhancement effect I showed above.&lt;/p&gt;
    &lt;p&gt;Let’s get to it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Image to ASCII conversion&lt;/head&gt;
    &lt;p&gt;ASCII contains 95 printable characters that we can use. Let’s start off by rendering the following image containing a white circle using those ASCII characters:&lt;/p&gt;
    &lt;p&gt;ASCII art is (almost) always rendered using a monospace font. Since every character in a monospace font is equally wide and tall, we can split the image into a grid. Each grid cell will contain a single ASCII character.&lt;/p&gt;
    &lt;p&gt;The image with the circle is &lt;/p&gt;
    &lt;p&gt;Monospace characters are typically taller than they are wide, so I made each grid cell a bit taller than it is wide.&lt;/p&gt;
    &lt;p&gt;Our task is now to pick which character to place in each cell. The simplest approach is to calculate a lightness value for each cell and pick a character based on that.&lt;/p&gt;
    &lt;p&gt;We can get a lightness value for each cell by sampling the lightness of the pixel at the cell’s center:&lt;/p&gt;
    &lt;p&gt;We want each pixel’s lightness as a numeric value between &lt;/p&gt;
    &lt;p&gt;We can use the following formula to convert an RGB color (with component values between &lt;/p&gt;
    &lt;p&gt;See relative luminance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mapping lightness values to ASCII characters&lt;/head&gt;
    &lt;p&gt;Now that we have a lightness value for each cell, we want to use those values to pick ASCII characters. As mentioned before, ASCII has 95 printable characters, but let’s start simple with just these characters:&lt;/p&gt;
    &lt;quote&gt;: - # = + @ * % .&lt;/quote&gt;
    &lt;p&gt;We can sort them in approximate density order like so, with lower-density characters to the left, and high-density characters to the right:&lt;/p&gt;
    &lt;quote&gt;. : - = + * # % @&lt;/quote&gt;
    &lt;p&gt;We’ll put these characters in a &lt;code&gt;CHARS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARS = [" ", ".", ":", "-", "=", "+", "*", "#", "%", "@"]&lt;/quote&gt;
    &lt;p&gt;I added space as the first (least dense) character.&lt;/p&gt;
    &lt;p&gt;We can then map lightness values between &lt;/p&gt;
    &lt;quote&gt;function getCharacterFromLightness(lightness: number) {const index = Math.floor(lightness * (CHARS.length - 1));return CHARS[index];}&lt;/quote&gt;
    &lt;p&gt;This maps low lightness values to low-density characters and high lightness values to high-density characters.&lt;/p&gt;
    &lt;p&gt;Rendering the circle from above with this method gives us:&lt;/p&gt;
    &lt;p&gt;That works... but the result is pretty ugly. We seem to always get &lt;code&gt;@&lt;/code&gt; for cells that fall within the circle and a space for cells that fall outside.&lt;/p&gt;
    &lt;p&gt;That is happening because we’ve pretty much just implemented nearest-neighbor downsampling. Let’s see what that means.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nearest neighbor downsampling&lt;/head&gt;
    &lt;p&gt;Downsampling, in the context of image processing, is taking a larger image (in our case, the &lt;/p&gt;
    &lt;p&gt;The simplest and fastest method of sampling is nearest-neighbor interpolation, where, for each cell (pixel), we only take a single sample from the higher resolution image.&lt;/p&gt;
    &lt;p&gt;Consider the circle example again. Using nearest-neighbor interpolation, every sample either falls inside or outside of the shape, resulting in either &lt;/p&gt;
    &lt;p&gt;If, instead of picking an ASCII character for each grid cell, we color each grid cell (pixel) according to the sampled value, we get the following pixelated rendering:&lt;/p&gt;
    &lt;p&gt;This pixelated rendering is pretty much equivalent to the ASCII rendering from before. The only difference is that instead of &lt;code&gt;@&lt;/code&gt;s we have white pixels, and instead of spaces we have black pixels.&lt;/p&gt;
    &lt;p&gt;These square, jagged looking edges are aliasing artifacts, commonly called jaggies. They’re a common result of using nearest-neighbor interpolation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Supersampling&lt;/head&gt;
    &lt;p&gt;To get rid of jaggies, we can collect more samples for each cell. Consider this line:&lt;/p&gt;
    &lt;p&gt;The line’s slope on the &lt;/p&gt;
    &lt;p&gt;Let’s try to get rid of the jagginess by taking multiple samples within each cell and using the average sampled lightness value as the cell’s lightness. The example below lets you vary the number of samples using the slider:&lt;/p&gt;
    &lt;p&gt;With multiple samples, cells that lie on the edge of a shape will have some of their samples fall within the shape, and some outside of it. Averaging those, we get gray in-between colors that smooth the downsampled image. Below is the same example, but with an overlay showing where the samples are taken:&lt;/p&gt;
    &lt;p&gt;This method of collecting multiple samples from the larger image is called supersampling. It’s a common method of spatial anti-aliasing (avoiding jaggies at edges). Here’s what the rotating square looks like with supersampling (using &lt;/p&gt;
    &lt;p&gt;Let’s look at what supersampling does for the circle example from earlier. Try dragging the sample quality slider:&lt;/p&gt;
    &lt;p&gt;The circle becomes less jagged, but the edges feel blurry. Why’s that?&lt;/p&gt;
    &lt;p&gt;Well, they feel blurry because we’re pretty much just rendering a low-resolution, pixelated image of a circle. Take a look at the pixelated view:&lt;/p&gt;
    &lt;p&gt;The ASCII and pixelated views are mirror images of each other. Both are just low-resolution versions of the original high-resolution image, scaled up to the original’s size — it’s no wonder they both look blurry.&lt;/p&gt;
    &lt;p&gt;Increasing the number of samples is insufficient. No matter how many samples we take per cell, the samples will be averaged into a single lightness value, used to render a single pixel.&lt;/p&gt;
    &lt;p&gt;And that’s the core problem: treating each grid cell as a pixel in an image. It’s an obvious and simple method, but it disregards that ASCII characters have shape.&lt;/p&gt;
    &lt;p&gt;We can make our ASCII renderings far more crisp by picking characters based on their shape. Here’s the circle rendered that way:&lt;/p&gt;
    &lt;p&gt;The characters follow the contour of the circle very well. By picking characters based on shape, we get a far higher effective resolution. The result is also more visually interesting.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shape&lt;/head&gt;
    &lt;p&gt;So what do I mean by shape? Well, consider the characters &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;L&lt;/code&gt;, and &lt;code&gt;O&lt;/code&gt; placed within grid cells:&lt;/p&gt;
    &lt;p&gt;The character &lt;code&gt;T&lt;/code&gt; is top-heavy. Its visual density in the upper half of the grid cell is higher than in the lower half. The opposite can be said for &lt;code&gt;L&lt;/code&gt; — it’s bottom-heavy. &lt;code&gt;O&lt;/code&gt; is pretty much equally dense in the upper and lower halves of the cell.&lt;/p&gt;
    &lt;p&gt;We might also compare characters like &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;J&lt;/code&gt;. The character &lt;code&gt;L&lt;/code&gt; is heavier within the left half of the cell, while &lt;code&gt;J&lt;/code&gt; is heavier in the right half:&lt;/p&gt;
    &lt;p&gt;We also have more “extreme” characters, such as &lt;code&gt;_&lt;/code&gt; and &lt;code&gt;^&lt;/code&gt;, that only occupy the lower or upper portion of the cell, respectively:&lt;/p&gt;
    &lt;p&gt;This is, roughly, what I mean by “shape” in the context of ASCII rendering. Shape refers to which regions of a cell a given character visually occupies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quantifying shape&lt;/head&gt;
    &lt;p&gt;To pick characters based on their shape, we’ll somehow need to quantify (put numbers to) the shape of each character.&lt;/p&gt;
    &lt;p&gt;Let’s start by only considering how much characters occupy the upper and lower regions of our cell. To do that, we’ll define two “sampling circles” for each grid cell — one placed in the upper half and one in the lower half:&lt;/p&gt;
    &lt;p&gt;It may seem odd or arbitrary to use circles instead of just splitting the cell into two rectangles, but using circles will give us more flexibility later on.&lt;/p&gt;
    &lt;p&gt;A character placed within a cell will overlap each of the cell’s sampling circles to some extent.&lt;/p&gt;
    &lt;p&gt;One can compute that overlap by taking a bunch of samples within the circle (for example, at every pixel). The fraction of samples that land inside the character gives us the overlap as a numeric value between &lt;/p&gt;
    &lt;p&gt;For T, we get an overlap of approximately &lt;/p&gt;
    &lt;p&gt;We can generate such a &lt;/p&gt;
    &lt;p&gt;Below are some ASCII characters and their shape vectors. I’m coloring the sampling circles using the component values of the shape vectors:&lt;/p&gt;
    &lt;p&gt;We can use the shape vectors as 2D coordinates — here’s every ASCII character on a 2D plot:&lt;/p&gt;
    &lt;head rend="h3"&gt;Shape-based lookup&lt;/head&gt;
    &lt;p&gt;Let’s say that we have our ASCII characters and their associated shape vectors in a &lt;code&gt;CHARACTERS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARACTERS: Array&amp;lt;{character: string,shapeVector: number[],}&amp;gt; = [...];&lt;/quote&gt;
    &lt;p&gt;We can then perform a nearest neighbor search like so:&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;findBestCharacter&lt;/code&gt; function gives us the ASCII character whose shape best matches the input lookup vector.&lt;/p&gt;
    &lt;p&gt;Note: this brute force search is not very performant. This becomes a bottleneck when we start rendering thousands of ASCII characters at &lt;/p&gt;
    &lt;p&gt;To make use of this in our ASCII renderer, we’ll calculate a lookup vector for each cell in the ASCII grid and pass it to &lt;code&gt;findBestCharacter&lt;/code&gt; to determine the character to display.&lt;/p&gt;
    &lt;p&gt;Let’s try it out. Consider the following zoomed-in circle as an example. It is split into three grid cells:&lt;/p&gt;
    &lt;p&gt;Overlaying our sampling circles, we see varying degrees of overlap:&lt;/p&gt;
    &lt;p&gt;When calculating the shape vector of each ASCII character, we took a huge number of samples. We could afford to do that because we only need to calculate those shape vectors once up front. After they’re calculated, we can use them again and again.&lt;/p&gt;
    &lt;p&gt;However, if we’re converting an animated image (e.g. canvas or video) to ASCII, we need to be mindful of performance when calculating the lookup vectors. An ASCII rendering might have hundreds or thousands of cells. Multiplying that by tens or hundreds of samples would be incredibly costly in terms of performance.&lt;/p&gt;
    &lt;p&gt;With that being said, let’s pick a sampling quality of &lt;/p&gt;
    &lt;p&gt;For the top sampling circle of the leftmost cell, we get one white sample and two black, giving us an average lightness of &lt;/p&gt;
    &lt;p&gt;From now on, instead of using the term “lookup vectors”, I’ll call these vectors, sampled from the image that we’re rendering as ASCII, sampling vectors. One sampling vector is calculated for each cell in the grid.&lt;/p&gt;
    &lt;p&gt;Anyway, we can use these sampling vectors to find the best-matching ASCII character. Let’s see what that looks like on our 2D plot — I’ll label the sampling vectors (from left to right) C0, C1, and C2:&lt;/p&gt;
    &lt;p&gt;Hmm... this is not what we want. Since none of the ASCII shape vector components exceed &lt;/p&gt;
    &lt;p&gt;We can fix this by normalizing the shape vectors. We’ll do that by taking the maximum value of each component across all shape vectors, and dividing the components of each shape vector by the maximum. Expressed in code, that looks like so:&lt;/p&gt;
    &lt;quote&gt;const max = [0, 0]for (const vector of characterVectors) {for (const [i, value] of Object.entries(vector)) {if (value &amp;gt; max[i]) {max[i] = value;}}}const normalizedCharacterVectors = characterVectors.map(vector =&amp;gt; vector.map((value, i) =&amp;gt; value / max[i]))&lt;/quote&gt;
    &lt;p&gt;Here’s what the plot looks like with the shape vectors normalized:&lt;/p&gt;
    &lt;p&gt;If we now map the sampling vectors to their nearest neighbors, we get a much more sensible result:&lt;/p&gt;
    &lt;p&gt;We get &lt;code&gt;'&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt; and &lt;code&gt;$&lt;/code&gt;.  Let’s see how well those characters match the circle:&lt;/p&gt;
    &lt;p&gt;Nice! They match very well.&lt;/p&gt;
    &lt;p&gt;Let’s try rendering the full circle from before with the same method:&lt;/p&gt;
    &lt;p&gt;Much better than before! The picked characters follow the contour of the circle very well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limits of a 2D shape vector&lt;/head&gt;
    &lt;p&gt;Using two sampling circles — one upper and one lower — produces a much better result than the &lt;/p&gt;
    &lt;p&gt;For example, two circles don’t capture the shape of characters that fall in the middle of the cell. Consider &lt;code&gt;-&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;-&lt;/code&gt;, we get a shape vector of &lt;/p&gt;
    &lt;p&gt;The two upper-lower sampling circles also don’t capture left-right differences, such as the difference between &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;We could use such differences to get better character picks, but our two sampling circles don’t capture them. Let’s add more dimensions to our shape to fix that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Increasing to 6 dimensions&lt;/head&gt;
    &lt;p&gt;Since cells are taller than they are wide (at least with the monospace font I’m using), we can use &lt;/p&gt;
    &lt;p&gt;&lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;, while also capturing differences across the top, bottom, and middle regions of the cell, differentiating &lt;code&gt;^&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, and &lt;code&gt;_&lt;/code&gt;. They also capture the shape of “diagonal” characters like &lt;code&gt;/&lt;/code&gt; to a reasonable degree.&lt;/p&gt;
    &lt;p&gt;One problem with this grid-like configuration for the sampling circles is that there are gaps. For example, &lt;code&gt;.&lt;/code&gt; falls between the sampling circles:&lt;/p&gt;
    &lt;p&gt;To compensate for this, we can stagger the sampling circles vertically (e.g. lowering the left sampling circles and raising the right ones) and make them a bit larger. This causes the cell to be almost fully covered while not causing excessive overlap across the sampling circles:&lt;/p&gt;
    &lt;p&gt;We can use the same procedure as before to generate character vectors using these sampling circles, this time yielding a &lt;code&gt;L&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;L&lt;/code&gt;, we get the vector:&lt;/p&gt;
    &lt;p&gt;I’m presenting &lt;/p&gt;
    &lt;p&gt;The lightness values certainly look L-shaped! The 6D shape vector captures &lt;code&gt;L&lt;/code&gt;’s shape very well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nearest neighbor lookups in a 6D space&lt;/head&gt;
    &lt;p&gt;Now we have a 6D shape vector for every ASCII character. Does that affect character lookups (how we find the best matching character)?&lt;/p&gt;
    &lt;p&gt;Earlier, in the &lt;code&gt;findBestCharacter&lt;/code&gt; function, I referenced a &lt;code&gt;getDistance&lt;/code&gt; function. That function returns the Euclidean distance between the input points. Given two 2D points &lt;/p&gt;
    &lt;p&gt;This generalizes to higher dimensions:&lt;/p&gt;
    &lt;p&gt;Put into code, this looks like so:&lt;/p&gt;
    &lt;quote&gt;function getDistance(a: number[], b: number[]): number {let sum = 0;for (let i = 0; i &amp;lt; a.length; i++) {sum += (a[i] - b[i]) ** 2;}return Math.sqrt(sum);}&lt;/quote&gt;
    &lt;p&gt;Note: since we’re just using this for the purposes of finding the closest point, we can skip the expensive &lt;code&gt;Math.sqrt()&lt;/code&gt; call and just return the squared distance. It does not affect the result.&lt;/p&gt;
    &lt;p&gt;So, no, the dimensionality of our shape vector does not change lookups at all. We can use the same &lt;code&gt;getDistance&lt;/code&gt; function for both 2D and 6D.&lt;/p&gt;
    &lt;p&gt;With that out of the way, let’s see what the 6D approach yields!&lt;/p&gt;
    &lt;head rend="h3"&gt;Trying out the 6D approach&lt;/head&gt;
    &lt;p&gt;Our new 6D approach works really well for flat shapes, like the circle example we’ve been using:&lt;/p&gt;
    &lt;p&gt;Now let’s see how this approach works when we render a 3D scene with more shades of gray:&lt;/p&gt;
    &lt;p&gt;Firstly, the outer contours look nice and sharp. I also like how well the gradients across the sphere and cone look.&lt;/p&gt;
    &lt;p&gt;However, internally, the objects all kind of blend together. The edges between surfaces with different lightnesses aren’t sharp enough. For example, the lighter faces of the cubes all kind of blend into one solid color. When there is a change in color — like when two faces of a cube meet — I’d like to see more sharpness in the ASCII rendering.&lt;/p&gt;
    &lt;p&gt;To demonstrate what I mean, consider the following split:&lt;/p&gt;
    &lt;p&gt;It’s currently rendered like so:&lt;/p&gt;
    &lt;p&gt;The different shades result in &lt;code&gt;i&lt;/code&gt;s on the left and &lt;code&gt;B&lt;/code&gt;s on the right, but the boundary is not very sharp.&lt;/p&gt;
    &lt;p&gt;By applying some effects to the sampling vector, we can enhance the contrast at the boundary so that it appears sharper:&lt;/p&gt;
    &lt;p&gt;The added contrast makes a big difference in readability for the 3D scene. Let’s look at how we can implement this contrast enhancement effect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contrast enhancement&lt;/head&gt;
    &lt;p&gt;Consider cells overlapping a color boundary like so:&lt;/p&gt;
    &lt;p&gt;For the cells on the boundary, we get a 6D sampling vector that looks like so:&lt;/p&gt;
    &lt;p&gt;To make future examples easier to visualize, I’ll start drawing the sampling vector using &lt;/p&gt;
    &lt;p&gt;Currently, this sampling vector resolves to the character &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;That’s a sensible choice. The character &lt;code&gt;T&lt;/code&gt; is visually dense in the top half and less so in the bottom half, so it matches the image fairly well.&lt;/p&gt;
    &lt;p&gt;Still, I want the picked character to emphasize the shape of the boundary better. We can achieve that by enhancing the contrast of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To increase the contrast of our sampling vector, we might raise each component of the vector to the power of some exponent.&lt;/p&gt;
    &lt;p&gt;Consider how an exponent affects values between &lt;/p&gt;
    &lt;p&gt;The level of pull depends on the exponent. Here’s a chart of &lt;/p&gt;
    &lt;p&gt;This effect becomes more pronounced with higher exponents:&lt;/p&gt;
    &lt;p&gt;A higher exponent translates to a stronger pull towards zero.&lt;/p&gt;
    &lt;p&gt;Applying an exponent should make dark values darker more quickly than light ones. The example below allows you to vary the exponent applied to the sampling vector:&lt;/p&gt;
    &lt;p&gt;As the exponent is increased to &lt;/p&gt;
    &lt;p&gt;I don’t want that. I want to increase the contrast between the lighter and darker components of the sampling vector, not the vector in its entirety.&lt;/p&gt;
    &lt;p&gt;To achieve that, we can normalize the sampling vector to the range &lt;/p&gt;
    &lt;p&gt;The normalization to &lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = x / maxValue; // Normalizevalue = Math.pow(x, exponent);value = x * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;Here’s the same example, but with this normalization applied:&lt;/p&gt;
    &lt;p&gt;Very nice! The lightest component values are retained, and the contrast between the lighter and darker components is increased by “crunching” the lower values.&lt;/p&gt;
    &lt;p&gt;This affects which character is picked. The following example shows how the selected character changes as the contrast is increased:&lt;/p&gt;
    &lt;p&gt;Awesome! The pick of &lt;code&gt;"&lt;/code&gt; over &lt;code&gt;T&lt;/code&gt; emphasizes the separation between the lighter region above and the darker region below!&lt;/p&gt;
    &lt;p&gt;By enhancing the contrast of the sampling vector, we exaggerate its shape. This gives us a character that less faithfully represents the underlying image, but improves readability as a whole by enhancing the separation between different colored regions.&lt;/p&gt;
    &lt;p&gt;Let’s look at another example. Observe how the L-shape of the sampling vector below becomes more pronounced as the exponent increases, and how that affects the picked character:&lt;/p&gt;
    &lt;p&gt;Works really nicely! I love the transition from &lt;code&gt;&amp;amp; -&amp;gt; b -&amp;gt; L&lt;/code&gt; as the L-shape of the vector becomes clearer.&lt;/p&gt;
    &lt;p&gt;What’s nice about applying exponents to normalized sampling vectors is that it barely affects vectors that are uniform in value. If all component values are similar, applying an exponent has a minimal effect:&lt;/p&gt;
    &lt;p&gt;Because the vector is fairly uniform, the exponent only has a slight effect and doesn’t change the picked character.&lt;/p&gt;
    &lt;p&gt;This is a good thing! If we have a smooth gradient in our image, we want to retain it. We very much do not want to introduce unnecessary choppiness.&lt;/p&gt;
    &lt;p&gt;Compare the 3D scene ASCII rendering with and without this contrast enhancement:&lt;/p&gt;
    &lt;p&gt;We do see more contrast at boundaries, but this is not quite there yet. Some edges are still not sharp enough, and we also observe a “staircasing” effect happening at some boundaries.&lt;/p&gt;
    &lt;p&gt;Let’s look at the staircasing effect first. We can reproduce it with a boundary like so:&lt;/p&gt;
    &lt;p&gt;Below is the ASCII rendering of that boundary. Notice how the lower edge (the &lt;code&gt;!&lt;/code&gt;s) becomes “staircase-y” as you increase the exponent:&lt;/p&gt;
    &lt;p&gt;We see a staircase pattern like so:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;To understand why that’s happening, let’s consider the row in the middle of the canvas, progressing from left to right. As we start off, every sample is equally light, giving us &lt;code&gt;U&lt;/code&gt;s:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUU -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we reach the boundary, the lower right samples become a bit darker. Those darker components are crunched by contrast enhancement, giving us some &lt;code&gt;Y&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;So we get:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYY -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we progress further right, the middle and lower samples get darker, so we get some &lt;code&gt;f&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;This trend continues towards &lt;code&gt;"&lt;/code&gt;, &lt;code&gt;'&lt;/code&gt;, and finally, &lt;code&gt;`&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Giving us a sequence like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''` -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;That looks good, but at some point we get no light samples. Once we get no light samples, our contrast enhancement has no effect because every component is equally light. This causes us to always get &lt;code&gt;!&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;Making our sequence look like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''`!!!!!!!!!! -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;This sudden stop in contrast enhancement having an effect is what causes the staircasing effect:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;Let’s see how we can counteract this staircasing effect with another layer of contrast enhancement, this time looking outside of the boundary of each cell.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;We currently have sampling circles arranged like so:&lt;/p&gt;
    &lt;p&gt;For each of those sampling circles, we’ll specify an “external sampling circle”, placed outside of the cell’s boundary, like so:&lt;/p&gt;
    &lt;p&gt;Each of those external sampling circles is “reaching” into the region of a neighboring cell. Together, the samples that are collected by the external sampling circles constitute an “external sampling vector”.&lt;/p&gt;
    &lt;p&gt;Let’s simplify the visualization and consider a single example. Imagine that we collected a sampling vector and an external sampling vector that look like so:&lt;/p&gt;
    &lt;p&gt;The circles colored red are the external sampling vector components. Currently, they have no effect.&lt;/p&gt;
    &lt;p&gt;The “internal” sampling vector itself is fairly uniform, with values ranging from &lt;/p&gt;
    &lt;p&gt;To enhance this apparent boundary, we’ll darken the top-left and middle-left components of the sampling vector. We can do that by applying component-wise contrast enhancement using the values from the external vector.&lt;/p&gt;
    &lt;p&gt;In the previous contrast enhancement, we calculated the maximum component value across the sampling vector and normalized the vector using that value:&lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = x / maxValue; // Normalizevalue = Math.pow(x, exponent);value = x * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;But the new component-wise contrast enhancement will take the maximum value between each component of the sampling vector and the corresponding component in the external sampling vector:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i])// ...});&lt;/quote&gt;
    &lt;p&gt;Aside from that, the contrast enhancement is performed in the same way:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i]);value = value / maxValue;value = Math.pow(value, exponent);value = value * maxValue;return value;});&lt;/quote&gt;
    &lt;p&gt;The example below shows how light values in the external sampling vector push values in the sampling vector down:&lt;/p&gt;
    &lt;p&gt;I call this “directional contrast enhancement”, since each of the external sampling circles reaches outside of the cell in the direction of the sampling vector component that it is enhancing the contrast of. I describe the other effect as “global contrast enhancement” since it acts on all of the sampling vector’s components together.&lt;/p&gt;
    &lt;p&gt;Let’s see what this directional contrast enhancement does to get rid of the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Hmm, that’s not doing what I wanted. I wanted to see a sequence like so:&lt;/p&gt;
    &lt;quote&gt;..::!!..::!!!!!!!!..::!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;But we just see &lt;code&gt;!&lt;/code&gt; changing to &lt;code&gt;:&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;This happens because the directional contrast enhancement doesn’t reach far enough into our sampling vector. The light upper values in the external vector do push the upper values of the sampling vector down, but because the lightness of the four bottom components is retained, we don’t get to &lt;code&gt;.&lt;/code&gt;, just &lt;code&gt;:&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Widening the directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;I’d like to “widen” the directional contrast enhancement so that, for example, light external values at the top spread to the middle components of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To do that, I’ll introduce a few more external sampling circles, arranged like so:&lt;/p&gt;
    &lt;p&gt;These are a total of &lt;/p&gt;
    &lt;p&gt;For each component of the internal sampling vector, we’ll calculate the maximum value across the external sampling vector components that affect it, and use that maximum to perform the contrast enhancement.&lt;/p&gt;
    &lt;p&gt;Let’s implement that. I’ll order the internal and external sampling circles like so:&lt;/p&gt;
    &lt;p&gt;We can then define a mapping from the internal circles to the external sampling circles that affect them:&lt;/p&gt;
    &lt;quote&gt;const AFFECTING_EXTERNAL_INDICES = [[0, 1, 2, 4],[0, 1, 3, 5],[2, 4, 6],[3, 5, 7],[4, 6, 8, 9],[5, 7, 8, 9],];&lt;/quote&gt;
    &lt;p&gt;With this, we can change the calculation of &lt;code&gt;maxValue&lt;/code&gt; to take the maximum affecting external value:&lt;/p&gt;
    &lt;quote&gt;// Beforeconst maxValue = Math.max(value, externalSamplingVector[i]);// Afterlet maxValue = value;for (const externalIndex of AFFECTING_EXTERNAL_INDICES[i]) {maxValue = Math.max(value, externalSamplingVector[externalIndex]);}&lt;/quote&gt;
    &lt;p&gt;Now look what happens if the top four external sampling circles are light: it causes the contrast enhancement to reach into the middle of the sampling vector, giving us the desired effect:&lt;/p&gt;
    &lt;p&gt;We now smoothly transition from &lt;code&gt;! -&amp;gt; : -&amp;gt; .&lt;/code&gt; — beautiful stuff!&lt;/p&gt;
    &lt;p&gt;Let’s see if this change resolves the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Oh yeah, looks awesome! We get the desired effect. The boundary is nice and sharp while not being too jagged.&lt;/p&gt;
    &lt;p&gt;Here’s the 3D scene again. The contrast slider now applies both types of contrast enhancement at the same time — try it out:&lt;/p&gt;
    &lt;p&gt;This really enhances the contrast at boundaries, making the image far more readable!&lt;/p&gt;
    &lt;p&gt;Together, the 6D shape vector approach and contrast enhancement techniques have given us a really nice final ASCII rendering.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final words&lt;/head&gt;
    &lt;p&gt;This post was really fun to build and write! I hope you enjoyed reading it.&lt;/p&gt;
    &lt;p&gt;ASCII rendering is perhaps not the most useful topic to write about, but I think the idea of using a high-dimensional vector to capture shape is interesting and could easily be applied to many other problems. There are parallels to be drawn to word embeddings.&lt;/p&gt;
    &lt;p&gt;I started writing this ASCII renderer to see if the idea of using a vector to capture the shape of characters would work at all. That approach turned out to work very well, but the initial prototype was terribly slow — I only got single-digit FPS on my iPhone. To get the ASCII renderer running at a smooth &lt;/p&gt;
    &lt;p&gt;My colleagues, after reading a draft of this post, suggested many alternatives to the approaches I described in this post. For example, why not make the sampling vector &lt;code&gt;T&lt;/code&gt; far better — just look how &lt;code&gt;T&lt;/code&gt;’s stem falls between the two sampling circles in each row:&lt;/p&gt;
    &lt;p&gt;And yeah, he’s right! A &lt;/p&gt;
    &lt;p&gt;It’s really fun how large the solution space to the problem of ASCII rendering is. There are so, so many approaches and trade-offs to explore. I imagine you probably thought of a few yourself while reading this post!&lt;/p&gt;
    &lt;p&gt;One dimension I intentionally did not explore was using different colors or lightnesses for the ASCII characters themselves. This is for many reasons, but the two primary ones are that 1) it would have expanded the scope of this post too much, and 2) it’s just a different effect, and I personally don’t like the look.&lt;/p&gt;
    &lt;p&gt;At the time of writing these final words, around &lt;/p&gt;
    &lt;p&gt;Thanks for reading! And huge thanks to Gunnlaugur Þór Briem and Eiríkur Fannar Torfason for reading and providing feedback on a draft of this post.&lt;/p&gt;
    &lt;p&gt;— Alex Harri&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix I: Character lookup performance&lt;/head&gt;
    &lt;p&gt;Earlier in this post, I showed how can find the best character by finding the character with the shortest Euclidean distance to our sampling vector.&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;I tried benchmarking this for &lt;/p&gt;
    &lt;p&gt;If we allow ourselves &lt;/p&gt;
    &lt;head rend="h3"&gt;k-d trees&lt;/head&gt;
    &lt;p&gt;Internally, &lt;/p&gt;
    &lt;p&gt;I won’t go into much detail on &lt;/p&gt;
    &lt;p&gt;One could also look at the hierarchical navigable small worlds (HNSW) algorithm, which Eiríkur pointed me to. It is used for approximate nearest neighbor lookups in vector databases, so definitely relevant.&lt;/p&gt;
    &lt;p&gt;Let’s see how it performs! We’ll construct a &lt;/p&gt;
    &lt;quote&gt;const kdTree = new KdTree(CHARACTERS.map(({ character, shapeVector }) =&amp;gt; ({point: shapeVector,data: character,})));&lt;/quote&gt;
    &lt;p&gt;We can now perform nearest-neighbor lookups on the &lt;/p&gt;
    &lt;quote&gt;const result = kdTree.findNearest(samplingVector);&lt;/quote&gt;
    &lt;p&gt;Running &lt;/p&gt;
    &lt;p&gt;That’s a lot of lookups per frame, but again, we’re benchmarking on a powerful machine. This is still not good enough.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can eke out even more performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching&lt;/head&gt;
    &lt;p&gt;An obvious avenue for speeding up lookups is to cache the result:&lt;/p&gt;
    &lt;quote&gt;function searchCached(samplingVector: number[]) {const key = generateCacheKey(samplingVector)if (cache.has(key)) {return cache.get(key)!;}const result = search(samplingVector);cache.set(key, result);return result;}&lt;/quote&gt;
    &lt;p&gt;But how does one generate a cache key for a &lt;/p&gt;
    &lt;p&gt;Well, one way is to quantize each vector component so that it fits into a set number of bits and packing those bits into a single number. JavaScript numbers give us &lt;/p&gt;
    &lt;p&gt;We can quantize a numeric value between &lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function quantizeTo5Bits(value: number) {return Math.min(RANGE - 1, Math.floor(value * RANGE));}&lt;/quote&gt;
    &lt;p&gt;Applying a max of &lt;code&gt;RANGE - 1&lt;/code&gt; is done so that a &lt;code&gt;value&lt;/code&gt; of exactly &lt;/p&gt;
    &lt;p&gt;We can quantize each of the sampling vector components in this manner and use bit shifting to pack all of the quantized values into a single number like so:&lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function generateCacheKey(vector: number[]): number {let key = 0;for (let i = 0; i &amp;lt; vector.length; i++) {const quantized = Math.min(RANGE - 1, Math.floor(vector[i] * RANGE));key = (key &amp;lt;&amp;lt; BITS) | quantized;}return key;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;RANGE&lt;/code&gt; is current set to &lt;code&gt;2 ** 5&lt;/code&gt;, but consider how large that makes our key space. Each vector component is one of &lt;/p&gt;
    &lt;p&gt;Alright, &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory needed to store keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;46,656&lt;/cell&gt;
        &lt;cell&gt;364 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;117,649&lt;/cell&gt;
        &lt;cell&gt;919 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;262,144&lt;/cell&gt;
        &lt;cell&gt;2.00 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;531,441&lt;/cell&gt;
        &lt;cell&gt;4.05 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;1,000,000&lt;/cell&gt;
        &lt;cell&gt;7.63 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;1,771,561&lt;/cell&gt;
        &lt;cell&gt;13.52 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;2,985,984&lt;/cell&gt;
        &lt;cell&gt;22.78 MB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are trade-offs to consider here. As the range gets smaller, the quality of the results drops. If we pick a range of &lt;/p&gt;
    &lt;p&gt;At the same time, if we increase the possible number of keys, we need more memory to store them. Additionally, the cache hit rate might be very low, especially when the cache is relatively empty.&lt;/p&gt;
    &lt;p&gt;I ended up picking a range of &lt;/p&gt;
    &lt;p&gt;Cached lookups are incredibly fast — fast enough that lookup performance just isn’t a concern anymore (&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix II: GPU acceleration&lt;/head&gt;
    &lt;p&gt;Lookups were not the only performance concern. Just collecting the sampling vectors (internal and external) turned out to be terribly expensive.&lt;/p&gt;
    &lt;p&gt;Just consider the sheer amount of samples that need to be collected. The 3D scene I’ve been using as an example uses a &lt;/p&gt;
    &lt;p&gt;And that’s if we use a sampling quality of &lt;/p&gt;
    &lt;p&gt;Collecting these samples absolutely crushed performance on my iPhone, so I needed to either collect fewer samples or speed up the collection of samples. Collecting fewer samples would have meant rendering fewer ASCII characters or removing the directional contrast enhancement, neither of which was an appealing solution.&lt;/p&gt;
    &lt;p&gt;My initial implementation ran on the CPU, which could only collect one sample at a time. To speed this up, I moved the work of sampling collection and applying the contrast enhancement to the GPU. The pipeline for that looks like so (each of the steps listed is a single shader pass):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Collect the raw internal sampling vectors into a &lt;mjx-container/&gt;texture, using the canvas (image) as the input texture.&lt;/item&gt;
      &lt;item&gt;Do the same for the external sampling vectors.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum external value affecting each internal vector component into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply directional contrast enhancement to each sampling vector component, using the maximum external values texture.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum value for each internal sampling vector into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply global contrast enhancement to each sampling vector component, using the maximum internal values texture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m glossing over the details because I could spend a whole other post covering them, but moving work to the GPU made the renderer many times more performant than it was when everything ran on the CPU.&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexharri.com/blog/ascii-rendering"/><published>2026-01-17T11:15:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657141</id><title>Architecture for Disposable Systems</title><updated>2026-01-17T12:18:59.843401+00:00</updated><content>&lt;doc fingerprint="2b4d3dde2be75ffc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Architecture for Disposable Systems&lt;/head&gt;
    &lt;head rend="h5"&gt;Posted on January 15, 2026 • 3 minutes • 595 words&lt;/head&gt;
    &lt;p&gt;As software gets cheaper to produce (thanks to coding agents) and quality expectations shift, we’re witnessing the rise of disposable software: code that you generate, use, and discard rather than maintain indefinitely.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Traditional Model&lt;/head&gt;
    &lt;p&gt;Traditional software follows a well-established pattern: you build something once, maintain it indefinitely, and pay for it through high upfront capital and long-term maintenance costs. The economics made sense because rewriting was expensive. We accepted spending 80% of a project’s lifecycle on maintenance because the alternative (starting over) was often prohibitive (until the product reaches its EOL)&lt;/p&gt;
    &lt;p&gt;This created a culture of careful engineering: clean code, thoughtful architecture, and refactoring to reduce technical debt. We optimized for the long term because the long term was inevitable. We have to live with it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Disposable Shift&lt;/head&gt;
    &lt;p&gt;But what happens when an agent can regenerate a functional replacement from a prompt in 5 minutes? The incentive to “clean up technical debt” or “refactor for the long term” vanishes. If the code works now and you can regenerate it later, why invest in perfection?&lt;/p&gt;
    &lt;p&gt;We’re already seeing the rise of “vibe coding”: building tools that solve a problem right now. Need a specific data parser? Generate it. Need a one-off dashboard for a meeting? Generate it. Use it, and if it breaks or becomes obsolete, delete it and generate a new one. You don’t care if the code is “clean” as long as the output is correct.&lt;/p&gt;
    &lt;p&gt;This isn’t laziness. It’s a fundamental shift in the economics of software development. When generation is cheap, maintenance becomes the expensive option.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture for Disposable Systems&lt;/head&gt;
    &lt;p&gt;If we’re moving toward disposable software, how do we architect systems that can survive this shift? The answer lies in a three-layer model:&lt;/p&gt;
    &lt;head rend="h3"&gt;The Core (Durable)&lt;/head&gt;
    &lt;p&gt;The Source of Truth. This is the hardened, human-written, slow-changing foundation of your system. It contains your critical business logic, data models, and core algorithms. This layer is built to last because it represents the fundamental value of your system.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Connectors (APIs)&lt;/head&gt;
    &lt;p&gt;Immutable contracts. These are the interfaces that define how components communicate. They must be perfect because the disposable parts can be imperfect. If your API contract is solid, you can swap out implementations underneath without breaking the system.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Disposable Layer&lt;/head&gt;
    &lt;p&gt;AI-generated “glue” code, data parsers, UI components, and integration scripts. This is where the vibe coding happens. Generate it, use it, and regenerate it when needed. As long as it adheres to the contracts defined by the Connectors layer, it doesn’t matter how messy the internals are.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contract-First Design&lt;/head&gt;
    &lt;p&gt;The key to making this work is contract-first design. Instead of coding to an implementation, we must code to a strict schema: OpenAPI, gRPC, Smithy, or whatever standard fits your domain. The agent is given the schema as a constraint, and as long as the inputs and outputs match the contract, we don’t care how messy the logic inside the box is.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The critical principle: Immutable contracts. They must be perfect so the disposable parts can be imperfect.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This approach allows you to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Regenerate components without breaking the system&lt;/item&gt;
      &lt;item&gt;Test contracts independently of implementations&lt;/item&gt;
      &lt;item&gt;Evolve the disposable layer while keeping the core stable&lt;/item&gt;
      &lt;item&gt;Accept lower-quality generated code because it’s constrained by high-quality contracts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Future&lt;/head&gt;
    &lt;p&gt;We’re not there yet, but the trajectory is clear. As coding agents improve and generation costs drop, more and more software will become disposable. The systems that survive will be those built with durable cores, immutable contracts, and disposable peripherals.&lt;/p&gt;
    &lt;p&gt;The question isn’t whether this shift will happen. It’s whether your architecture is ready for it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tuananh.net/2026/01/15/architecture-for-disposable-systems/"/><published>2026-01-17T11:18:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657296</id><title>The 600-year-old origins of the word 'hello'</title><updated>2026-01-17T12:18:59.068866+00:00</updated><content>&lt;doc fingerprint="b2309d8d8fa112dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'Hullo, hillo, holla': The 600-year-old origins of the word 'hello'&lt;/head&gt;
    &lt;p&gt;It's been 200 years since the word "hello" was first used in print – though its beginnings date back to the 15th Century. How has the language of greetings evolved around the world - and what does it tell us about ourselves?&lt;/p&gt;
    &lt;p&gt;We use "hello" dozens of times a day without thinking – during phone calls, emails and face-to-face encounters. We sing it along with Adele and Lionel Richie, and we have watched it spun into moments of screen gold in Jerry Maguire ("You had me at hello"), and Scarface ("Say hello to my little friend!"). It's been used to sell everything from mobile phones (Motorola's "Hello, Moto") to lingerie (Wonderbra's iconic "Hello boys"), and it has been borrowed to name computer programs and celebrity magazines.&lt;/p&gt;
    &lt;p&gt;In print, this ubiquitous, friendly greeting has a surprisingly short history. Two centuries ago, on 18 January 1826, "hello" made what is thought to be its earliest recorded appearance on the page, in a Connecticut newspaper called The Norwich Courier. Hidden among the column inches, it was a modest in-ink debut for a word that would go on to greet much of the modern world.&lt;/p&gt;
    &lt;p&gt;By the 1850s, it had crossed the Atlantic to Britain – appearing in publications such as the London Literary Gazette – and became increasingly common in print. Like the go-to greetings in other languages, "hello" also says something about the English-speaking world – depending on which variation, abbreviation or inflection of the word we choose to use.&lt;/p&gt;
    &lt;p&gt;There are plenty of such forms. Whether due to dialect or accent influences, or the brevity demanded by online communication, which "hello" you choose says a lot about you, and can indicate age, nationality, or even mood. According to linguists, elongated variations such as "heyyy" could be construed as flirtatious, "hellaw" might suggest you're from the southern US, "howdy" from western US, and the clipped "hi" may indicate a curt disposition.&lt;/p&gt;
    &lt;p&gt;"It can be pronounced and inflected in many different ways, and these subtle intonational contours can change its meaning," says Alessandro Duranti, professor of linguistic anthropology at the University of California, Los Angeles. "For example, when someone says 'hello' with a stretched final vowel, it can question what the other person just said, as in 'Hello, are you paying attention?' or 'Hello, you must be kidding.'"&lt;/p&gt;
    &lt;p&gt;This capacity to convey nuance through tone and form is no modern invention; even in its first printed appearances, "hello" was a patchwork of influences, derivations and applications drawn from several languages.&lt;/p&gt;
    &lt;head rend="h2"&gt;The origins of hello&lt;/head&gt;
    &lt;p&gt;The pre-printed origins of the word "hello" are disputed. The most commonly cited etymology is the Old High German "halâ" – a cry historically used to hail a ferryman. The Oxford English Dictionary also points to "halloo" (a hunting call that urged hounds to run faster) as a possible linguistic root. It notes several early spellings, including "hullo", "hillo" and "holla" – the latter thought to have derived from the 15th-Century French "hol", an exclamation meaning "whoa!" or "stop!". In English sources, the OED lists the earliest form as the late-16th-Century "hollo".&lt;/p&gt;
    &lt;p&gt;Simon Horobin, professor of English language and literature at Magdelen College, Oxford, notes that such semantic shifts and spelling changes may also be explained by regional accents and differences in pronunciation. "Especially in the example of 'ello' which shows the prevalent – though now stigmatised – feature of h-dropping," he tells the BBC, referring to the classist English stereotype of a dropped 'h' indicating a lack of education.&lt;/p&gt;
    &lt;p&gt;"But for origins and early history," he adds, "we are dependent upon written evidence, which is patchy at the best of times. For a colloquial word like this, which would have appeared much earlier and more frequently in speech than in writing, it is especially tricky to establish a definite timeline."&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• The most powerful word in the English language&lt;/p&gt;
    &lt;p&gt;• The surprising history of the word 'dude'&lt;/p&gt;
    &lt;p&gt;• The subtle way language shapes us&lt;/p&gt;
    &lt;p&gt;The selection of a standardised word form, Horobin explains, usually falls to lexicographers – those who compile dictionaries. "They base their choice on the relative prevalence of a particular spelling, though it's necessarily somewhat provisional and arbitrary."&lt;/p&gt;
    &lt;p&gt;By the time the Oxford English Dictionary first went to press in 1884, "hello" was emerging as the dominant form of the greeting. Charles Dickens, however, spent the 19th Century using "hullo" in his writings, and Alexander Graham Bell (who once argued that "ahoy!" would make a superior telephone greeting) stuck with "halloo". Bell's rival, Thomas Edison, championed "hello", believing it would carry clearly over even the worst phone lines. Like that of The Norwich Courier before him, Edison's backing helped – and "hello" was established as the English-language greeting to beat.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello around the world&lt;/head&gt;
    &lt;p&gt;While the English language settled on "hello" as its customary greeting, other languages forged their own. Some were influenced by English, others developed independently – yet each carries a distinct cultural flavour, hinting at the social norms and stereotypes we have of the people who use it.&lt;/p&gt;
    &lt;p&gt;In Germanic and Scandinavian languages, for example, "hallo" and "hallå" are phonetically harder and feel more efficient and no-nonsense than the lyrical, almost poetic quality of "hola" and "olá", favoured by the Romance languages that are associated with more effusive stereotypes. Elsewhere, some greetings carry traces of national history: from the Dutch-derived "hallo" of Afrikaans to "óla" in Tetum, a reminder of Portuguese influence in Timor-Leste. Many such words appear to function as both introduction and identity marker. But, says Professor Duranti, it's not quite that simple.&lt;/p&gt;
    &lt;p&gt;"It's hard to go straight from the use of a particular greeting to a national character, even though it is tempting," he tells the BBC. Alternative or secondary greetings, Duranti suggests, may offer better clues. "In English, given the common use of 'how are you?', there is an apparent interest in people's wellbeing." In some Polynesian societies, he adds, greetings are less about a word-for-word "hello" than about checking in on someone's plans or movements – literally asking "where are you going?". Greek, meanwhile, uses "Γειά σου" (pronounced "yah-soo") as a typical informal greeting, offering a wish for health rather than a simple salutation. It is also usable for "goodbye".&lt;/p&gt;
    &lt;p&gt;Other languages also turn abstract concepts into multipurpose greetings that serve as both "hi" and "bye". "Ciao" comes from a Venetian dialect phrase meaning "at your service", and the French "salut" is an informal expression used for both greeting and parting company. Similarly, the Hawaiian "aloha" can express affection or compassion, and the Hebrew "shalom" peace or wholeness. Yet, as Duranti cautions, even these evocative examples shouldn't be viewed as cut-and-dry indicators of national character.&lt;/p&gt;
    &lt;p&gt;"I would be careful making that kind of correlation," he explains. "Especially about the semantics of it – health versus sympathy versus whereabouts. But there is one aspect of greetings that is sensitive to the social structure of a society, which is that equals greet each other in different ways from people of different statuses. In fact, greetings can be seen to define levels of intimacy or social distance." In this sense, he adds, greetings are like magnets – confidently announcing who we are, and drawing in those we want to be associated with.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello in the digital age&lt;/head&gt;
    &lt;p&gt;If greetings act as social magnets, then technology has quietly altered their pull. Over the past few decades, the rise of email, texting and social media has reshaped not just how often we say "hello", but what we might replace it with – and whether we say it at all.&lt;/p&gt;
    &lt;p&gt;"If you think about WhatsApp, we're basically always in conversation – we're always online," says Christian Ilbury, senior lecturer in linguistics and English language at the University of Edinburgh. "When someone asks you how your day is or whether you're going to be on time for the meal, you don't always have to say 'hello' first, because it's unlikely the last message concluded with 'bye'."&lt;/p&gt;
    &lt;p&gt;In a text-led, always-on world, greetings have proved especially susceptible to change and, as they are used so often, their evolution has accelerated dramatically. Ilbury has identified many non-standard and creative spellings of "hello" in his studies of digital language, from "hellooooo" and "hiiiiiii" to "heyyyyy". Yet, while tech has made it easier for us to elongate words in this way, Ilbury points out that most modern-day greetings are short, sharp and driven by brevity.&lt;/p&gt;
    &lt;p&gt;"The most obvious thing to say is that people now sometimes use an emoji – the wave – in place of the word 'hello'," says Ilbury. "But technology has always contributed to language change. We now 'Google' stuff and 'unfriend' people. Like any major invention – AI, for instance – we're bound to get some new vocabulary from that source."&lt;/p&gt;
    &lt;p&gt;In many ways, this mirrors the instability of "hello" in the early 19th Century, when the greeting may have sounded vaguely the same whenever spoken, but varied widely in spelling when written down. By shortening the established greeting, or replacing it with icons and abbreviations, it's made clear that such salutations remain as fluid as they were before The Norwich Couriermade its landmark linguistic choice in 1826.&lt;/p&gt;
    &lt;p&gt;But for all its so-called standardisation, "hello" has never really stood still. It began as a shout, a summons, a way to hail attention, before settling – briefly – into an accepted spelling and usage. Two centuries on from its print debut, the greeting is once again being stretched, clipped, replaced or ignored altogether. Yet whether it's spoken aloud, typed hastily, or reduced to a small waving hand on a screen, the impulse behind it remains the same: an act of recognition, the announcing of one's presence and just asking – however casually – to be acknowledged in return.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;If you liked this story, sign up for The Essential List newsletter – a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/culture/article/20260113-hello-hiya-aloha-what-our-greetings-reveal"/><published>2026-01-17T11:51:44+00:00</published></entry></feed>