<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-01T15:36:02.536074+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45083952</id><title>Jujutsu for everyone</title><updated>2025-09-01T15:36:11.112950+00:00</updated><content>&lt;doc fingerprint="97d01e5b68f7a9ce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;This is a tutorial for the Jujutsu version control system. It requires no previous experience with Git or any other version control system.&lt;/p&gt;
    &lt;p&gt;At the time of writing, most Jujutsu tutorials are targeted at experienced Git users, teaching them how to transfer their existing Git skills over to Jujutsu. This tutorial is my attempt to fill the void of beginner learning material for Jujutsu. If you are already experienced with Git, I recommend Steve Klabnik's tutorial instead of this one.&lt;/p&gt;
    &lt;p&gt;This tutorial requires you to work in the terminal. Don't worry, there's a chapter covering some terminal basics in case you're not 100% comfortable with that yet. The commands I tell you to run will often only work on Unix-like operating systems like Linux and Mac. If you're on Windows (and can't switch to Linux), consider using WSL.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to read this tutorial&lt;/head&gt;
    &lt;p&gt;The tutorial is split into levels, which are the top-level chapters in the sidebar. The idea is that once you complete a level, you should probably put this tutorial away for a while and practice what you've learned. Once you're comfortable with those skills, come back for the next level.&lt;/p&gt;
    &lt;p&gt;There is one exception to this: If you're here because you need to collaborate with other people, you should complete the levels 1 and 2 right away.&lt;/p&gt;
    &lt;p&gt;Here's an overview of the planned levels:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Level&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;The bare minimum to get started. This is only enough for the simplest use cases where you're working alone. For example, students who track and submit their homework with a Git repository can get by with only this.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;The bare minimum for any sort of collaboration. Students who are working on a group project and professional software developers need to know this. Going further is highly recommended, but you can take a break after this.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;Basic problem solving skills like conflict resolution and restoring files from history. Without this knowledge, it's only a matter of time until you run into trouble. Completing this level is comparable to the skill level of the average software developer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;History rewriting skills. These will allow you to iterate toward a polished version history, which pays dividends long-term. Some projects require you to have these skills in order to meet their quality standards.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;Productivity boosters, advanced workflows, lesser-known CLI functions and a little VCS theory. Completing this level means you have mastered Jujutsu.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;Additional topics that only come up in specific situations: tags, submodules, workspaces etc. Consider skimming the list of topics and come back once you have an actual need for it.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Only a few levels are complete right now, the rest are on the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reset your progress&lt;/head&gt;
    &lt;p&gt;Throughout the tutorial, you will build an example repository. Later chapters depend on the state of previous ones. Losing the state of the example repo can therefore block you from making smooth progress. This might happen for several reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You use the example repo for practice and experimentation.&lt;/item&gt;
      &lt;item&gt;You switch to a different computer or reinstall the OS.&lt;/item&gt;
      &lt;item&gt;You intentionally delete it to clean up your home directory.&lt;/item&gt;
      &lt;item&gt;The tutorial is updated significantly while you're taking a break.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To solve this problem, there is a script which automates the task of resetting your progress to the start of any chapter. To identify the chapter you want to continue with, the script expects a keyword as an argument. Each chapter includes its precise reset command at the beginning, so you can easily copy-paste it.&lt;/p&gt;
    &lt;p&gt;The script is not complicated, you can verify that it's not doing anything malicious. Basically, it's just the list of commands I tell you to run manually. For convenience, it's included in the expandable text box below. You can also download the script here and then execute it locally once you have inspected it.&lt;/p&gt;
    &lt;head class="admonition-title"&gt;
      &lt;p&gt;Source of reset script&lt;/p&gt;
    &lt;/head&gt;
    &lt;p&gt;Source of reset script&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env bash
set -euxo pipefail

if [ "${1:-x}" = "x" ] ; then
    echo "Please provide the chapter keyword as the first argument."
    exit 1
fi
chapter="$1"

function success() {
    set +x
    echo "✅✅✅ Reset script completed successfully! ✅✅✅"
    exit 0
}

# Ensure existing user configuration does not affect script behavior.
export JJ_CONFIG=/dev/null

rm -rf ~/jj-tutorial

if ! command -v jj &amp;gt; /dev/null ; then
    echo "ERROR: Jujutsu doesn't seem to be installed."
    echo "       Please install it and rerun the script."
    exit 1
fi

if [ "$chapter" = initialize ] ; then success ; fi

mkdir -p ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj git init --colocate

jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = log ] ; then success ; fi

if [ "$chapter" = make_changes ] ; then success ; fi

echo "# jj-tutorial" &amp;gt; README.md
jj log -r 'none()' # trigger snapshot

if [ "$chapter" = commit ] ; then success ; fi

jj commit --message "Add readme with project title

It's common practice for software projects to include a file called
README.md in the root directory of their source code repository. As the
file extension indicates, the content is usually written in markdown,
where the title of the document is written on the first line with a
prefixed \`#\` symbol.
"

if [ "$chapter" = remote ] ; then success ; fi

git init --bare ~/jj-tutorial/remote
jj git remote add origin ~/jj-tutorial/remote
jj bookmark create main --revision @-
jj git push --bookmark main --allow-new

if [ "$chapter" = clone ] ; then success ; fi

cd ~
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = github ] ; then success ; fi

if [ "$chapter" = update_bookmark ] ; then success ; fi

printf "\nThis is a toy repository for learning Jujutsu.\n" &amp;gt;&amp;gt; README.md
jj commit -m "Add project description to readme"

jj bookmark move main --to @-

jj git push

if [ "$chapter" = branch ] ; then success ; fi

echo "print('Hello, world!')" &amp;gt; hello.py

jj commit -m "Add Python script for greeting the world

Printing the text \"Hello, world!\" is a classic exercise in introductory
programming courses. It's easy to complete in basically any language and
makes students feel accomplished and curious for more at the same time."

jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo-bob
cd ~/jj-tutorial/repo-bob
jj config set --repo user.name Bob
jj config set --repo user.email bob@local
jj describe --reset-author --no-edit

echo "# jj-tutorial

The file hello.py contains a script that greets the world.
It can be executed with the command 'python hello.py'.
Programming is fun!" &amp;gt; README.md
jj commit -m "Document hello.py in README.md

The file hello.py doesn't exist yet, because Alice is working on that.
Once our changes are combined, this documentation will be accurate."

jj bookmark move main --to @-
jj git push

cd ~/jj-tutorial/repo
jj bookmark move main --to @-
jj git fetch

if [ "$chapter" = show ] ; then success ; fi

if [ "$chapter" = merge ] ; then success ; fi

jj new main@origin @-

jj commit -m "Merge code and documentation for hello-world"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = ignore ] ; then success ; fi

cd ~/jj-tutorial/repo-bob

tar czf submission_alice_bob.tar.gz README.md

echo "
## Submission

Run the following command to create the submission tarball:

~~~sh
tar czf submission_alice_bob.tar.gz [FILE...]
~~~" &amp;gt;&amp;gt; README.md

echo "*.tar.gz" &amp;gt; .gitignore

jj file untrack submission_alice_bob.tar.gz

jj commit -m "Add submission instructions"

if [ "$chapter" = rebase ] ; then success ; fi

jj bookmark move main --to @-
jj git fetch
jj rebase --destination main@origin
jj git push

if [ "$chapter" = more_bookmark ] ; then success ; fi

cd ~/jj-tutorial/repo

echo "for (i = 0; i &amp;lt; 10; i = i + 1):
    print('Hello, world!')" &amp;gt; hello.py

jj commit -m "WIP: Add for loop (need to fix syntax)"

jj git push --change @-

if [ "$chapter" = navigate ] ; then success ; fi

jj git fetch
jj new main

if [ "$chapter" = undo ] ; then success ; fi

echo "print('Hallo, Welt!')" &amp;gt;&amp;gt; hello.py
echo "print('Bonjour, le monde!')" &amp;gt;&amp;gt; hello.py

jj commit -m "code improvements"

jj undo

jj commit -m "Print German and French greetings as well"

jj undo
jj undo
jj undo

jj redo
jj redo
jj redo

if [ "$chapter" = track ] ; then success ; fi

cd ~ # move out of the directory we're about to delete
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo

# roleplay as Alice
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

echo "print('Hallo, Welt!')" &amp;gt;&amp;gt; hello.py
echo "print('Bonjour, le monde!')" &amp;gt;&amp;gt; hello.py
jj commit -m "Print German and French greetings as well"

jj bookmark move main -t @-
jj git push

jj bookmark track 'glob:push-*@origin'

if [ "$chapter" = conflict ] ; then success ; fi

jj new 'description("WIP: Add for loop")'

echo "for _ in range(10):
    print('Hello, world!')" &amp;gt; hello.py

jj commit -m "Fix loop syntax"

jj new main @-

echo "for _ in range(10):
    print('Hello, world!')
    print('Hallo, Welt!')
    print('Bonjour, le monde!')" &amp;gt; hello.py

jj commit -m "Merge repetition and translation of greeting"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = abandon ] ; then success ; fi

jj commit -m "Experiment: Migrate to shiny new framework"
jj git push --change @-
jj new main
jj commit -m "Experiment: Improve scalability using microservices"
jj git push --change @-
jj new main
jj commit -m "Experiment: Apply SOLID design patterns"
jj git push --change @-
jj new main

jj abandon 'description("Experiment")'

jj git push --deleted

if [ "$chapter" = restore ] ; then success ; fi

rm README.md
jj show &amp;amp;&amp;gt; /dev/null

jj restore README.md

jj restore --from 'description("Fix loop syntax")' hello.py

jj commit -m "Remove translations"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = complete ] ; then success ; fi

set +x
echo "Error: Didn't recognize the chapter keyword: '$chapter'."
exit 1
&lt;/code&gt;
    &lt;head rend="h2"&gt;Stay up to date&lt;/head&gt;
    &lt;p&gt;Both this tutorial and Jujutsu are still evolving. In order to keep your Jujutsu knowledge updated, subscribe to releases of the tutorial's GitHub repo. You will be notified of important changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A new level becomes available.&lt;/item&gt;
      &lt;item&gt;An existing level is changed significantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I especially intend to keep this tutorial updated as new version of Jujutsu come out with features and changes that are relevant to the tutorial's content. I consider this tutorial up-to-date with the latest version of Jujutsu (&lt;code&gt;0.32&lt;/code&gt;) as of August 2025.
If that's more than a couple months in the past, I probably stopped updating this tutorial.&lt;/p&gt;
    &lt;p&gt;You can subscribe to these updates by visiting the GitHub repo and clicking on "Watch", "Custom" and then selecting "Releases".&lt;/p&gt;
    &lt;head rend="h2"&gt;Help make this tutorial better&lt;/head&gt;
    &lt;p&gt;If you find a typo, you can suggest a fix directly by clicking on the "edit" icon in the top-right corner. If you have general suggestions for improvement, please open an issue. I am also very interested in experience reports, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Do you have any frustrations with Jujutsu which the tutorial did not help you overcome?&lt;/item&gt;
      &lt;item&gt;Was there a section that wasn't explained clearly? (If you didn't understand something, it's probably the tutorial's fault, not yours!)&lt;/item&gt;
      &lt;item&gt;Did you complete a level but didn't feel like you had the skills that were promised in the level overview?&lt;/item&gt;
      &lt;item&gt;Is there something missing that's not being taught but should?&lt;/item&gt;
      &lt;item&gt;Do you feel like the content could be structured better?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you for helping me improve this tutorial!&lt;/p&gt;
    &lt;head rend="h2"&gt;What is version control and why should you use it?&lt;/head&gt;
    &lt;p&gt;I will assume you're using version control for software development, but it can be used for other things as well. For example, authoring professionally formatted documents with tools like Typst. The source of this tutorial is stored in version control too!&lt;/p&gt;
    &lt;p&gt;What these scenarios have in common is that a large body of work (mostly in the form of text) is slowly being expanded and improved over time. You don't want to lose any of it and you want to be able to go back to previous states of your work. Often, several people need to work on the project at the same time.&lt;/p&gt;
    &lt;p&gt;A general-purpose backup solution can keep a few copies of your files around. A graphical document editor can allow multiple people to edit the text simultaneously. But sometimes, you need a sharper knife. Jujutsu is the sharpest knife available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Jujutsu instead of Git?&lt;/head&gt;
    &lt;p&gt;Git is by far the most commonly used VCS in the software development industry. So why not use that? Using the most popular thing has undeniable benefits. There is lots of learning material, lots of people can help you with problems, lots of other tools integrate with it etc. Why make life harder on yourself by using a lesser-known alternative?&lt;/p&gt;
    &lt;p&gt;Here's my elevator pitch:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is compatible with Git. You're not actually losing anything by using Jujutsu. You can work with it on any existing project that uses Git for version control without issues. Tools that integrate with Git mostly work just as well with Jujutsu.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is easier to learn than Git. (That is, assuming I did a decent job writing this tutorial.) Git is known for its complicated, unintuitive user interface. Jujutsu gives you all the functionality of Git with a lot less complexity. Experienced users of Git usually don't care about this, because they've paid the price of learning Git already. (I was one of these people once.) But you care!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is more powerful than Git. Despite the fact that it's easier to learn and more intuitive, it actually has loads of awesome capabilities for power users that completely leave Git in the dust. Don't worry, you don't have to use that power right away. But you can be confident that if your VCS-workflow becomes more demanding in the future, Jujutsu will have your back. This is not a watered-down "we have Git at home" for slow learners!&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Learning Jujutsu instead of Git as your first VCS does have some downsides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;When talking about version control with peers, they will likely use Git-centric vocabulary. Jujutsu shares a lot of Git's concepts, but there are also differences. Translating between the two in conversation can add some mental overhead. (solution: convince your peers to use Jujutsu 😉)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is relatively new and doesn't cover 100% of the features of Git yet. When you do run into the rare problem where Jujutsu doesn't have an answer, you can always fall back to use Git directly, which works quite seamlessly. Still, having to use two tools instead of one is slightly annoying. I plan to teach such Git features in this tutorial in later levels. The tutorial should be a one-stop-shop for all Jujutsu users.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The command line interface of Jujutsu is not yet stable. That means in future versions of Jujutsu, some commands might work a little differently or be renamed. I personally don't think this should scare you away. Many people including me have used Jujutsu as a daily driver for a long time. Whenever something did change, my reaction was usually: "Great, that was one of the less-than-perfect parts of Jujutsu! Now it's even more intuitive than before!" Consider subscribing to GitHub releases of this tutorial. You will be notified if new versions of Jujutsu change something in a way that's relevant to what you learned in this tutorial.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Despite some downsides, I think the benefits are well worth it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jj-for-everyone.github.io/"/></entry><entry><id>https://news.ycombinator.com/item?id=45085029</id><title>Use One Big Server (2022)</title><updated>2025-09-01T15:36:10.688791+00:00</updated><content>&lt;doc fingerprint="dfdab7597d822fd7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Use One Big Server&lt;/head&gt;
    &lt;p&gt;A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind this debate is about whether distributed system architecture is worth the developer time and cost overheads. By thinking about the real operational considerations of our systems, we can get some insight into whether we actually need distributed systems for most things.&lt;/p&gt;
    &lt;p&gt;We have all gotten so familiar with virtualization and abstractions between our software and the servers that run it. These days, "serverless" computing is all the rage, and even "bare metal" is a class of virtual machine. However, every piece of software runs on a server. Since we now live in a world of virtualization, most of these servers are a lot bigger and a lot cheaper than we actually think.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meet Your Server&lt;/head&gt;
    &lt;p&gt;This is a picture of a server used by Microsoft Azure with AMD CPUs. Starting from the left, the big metal fixture on the left (with the copper tubes) is a heatsink, and the metal boxes that the copper tubes are attached to are heat exchangers on each CPU. The CPUs are AMD's third generation server CPU, each of which has the following specifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;64 cores&lt;/item&gt;
      &lt;item&gt;128 threads&lt;/item&gt;
      &lt;item&gt;~2-2.5 GHz clock&lt;/item&gt;
      &lt;item&gt;Cores capable of 4-6 instructions per clock cycle&lt;/item&gt;
      &lt;item&gt;256 MB of L3 cache&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In total, this server has 128 cores with 256 simultaneous threads. With all of the cores working together, this server is capable of 4 TFLOPs of peak double precision computing performance. This server would sit at the top of the top500 supercomputer list in early 2000. It would take until 2007 for this server to leave the top500 list. Each CPU core is substantially more powerful than a single core from 10 years ago, and boasts a much wider computation pipeline.&lt;/p&gt;
    &lt;p&gt;Above and below each CPU is the memory: 16 slots of DDR4-3200 RAM per socket. The largest capacity "cost effective" DIMMs today are 64 GB. Populated cost-efficiently, this server can hold 1 TB of memory. Populated with specialized high-capacity DIMMs (which are generally slower than the smaller DIMMs), this server supports up to 8 TB of memory total. At DDR4-3200, with a total of 16 memory channels, this server will likely see ~200 Gbps of memory throughput across all of its cores.&lt;/p&gt;
    &lt;p&gt;In terms of I/O, each CPU offers 64 PCIe gen 4 lanes. With 128 PCIe lanes total, this server is capable of supporting 30 NVMe SSDs plus a network card. Typical configurations you can buy will offer slots for around 16 SSDs or disks. The final thing I wanted to point out in this picture is in the top right, the network card. This server is likely equipped with a 50-100 Gbps network connection.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Capabilities of One Server&lt;/head&gt;
    &lt;p&gt;One server today is capable of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Serving video files at 400 Gbps (now 800 Gbps)&lt;/item&gt;
      &lt;item&gt;1 million IOPS on a NoSQL database&lt;/item&gt;
      &lt;item&gt;70k IOPS in PostgreSQL&lt;/item&gt;
      &lt;item&gt;500k requests per second to nginx&lt;/item&gt;
      &lt;item&gt;Compiling the linux kernel in 20 seconds&lt;/item&gt;
      &lt;item&gt;Rendering 4k video with x264 at 75 FPS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Among other things. There are a lot of public benchmarks these days, and if you know how your service behaves, you can probably find a similar benchmark.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Cost of One Server&lt;/head&gt;
    &lt;p&gt;In a large hosting provider, OVHCloud, you can rent an HGR-HCI-6 server with similar specifications to the above, with 128 physical cores (256 threads), 512 GB of memory, and 50 Gbps of bandwidth for $1,318/month.&lt;/p&gt;
    &lt;p&gt;Moving to the popular budget option, Hetzner, you can rent a smaller server with 32 physical cores and 128 GB of RAM for about â¬140.00/month. This is a smaller server than the one from OVHCloud (1/4 the size), but it gives you some idea of the price spread between hosting providers.&lt;/p&gt;
    &lt;p&gt;In AWS, one of the largest servers you can rent is the m6a.metal server. It offers 50 Gbps of network bandwidth, 192 vCPUs (96 physical cores), and 768 GB of memory, and costs $8.2944/hour in the US East region. This comes out to $6,055/month. The cloud premium is real!&lt;/p&gt;
    &lt;p&gt;A similar server, with 128 physical cores and 512 GB of memory (as well as appropriate NICs, SSDs, and support contracts), can be purchased from the Dell website for about $40,000. However, if you are going to spend this much on a server, you should probably chat with a salesperson to make sure you are getting the best deal you can. You will also need to pay to host this server and connect it to a network, though.&lt;/p&gt;
    &lt;p&gt;In comparison, buying servers takes about 8 months to break even compared to using cloud servers, and 30 months to break even compared to renting. Of course, buying servers has a lot of drawbacks, and so does renting, so going forward, we will think a little bit about the "cloud premium" and whether you should be willing to pay it (spoiler alert: the answer is "yes, but not as much as the cloud companies want you to pay").&lt;/p&gt;
    &lt;head rend="h2"&gt;Thinking about the Cloud&lt;/head&gt;
    &lt;p&gt;The "cloud era" began in earnest around 2010. At the time, the state of the art CPU was an 8-core Intel Nehalem CPU. Hyperthreading had just begun, so that 8-core CPU offered a whopping 16 threads. Hardware acceleration was about to arrive for AES encryption, and vectors were 128 bits wide. The largest CPUs had 24 MB of cache, and your server could fit a whopping 256 GB of DDR3-1066 memory. If you wanted to store data, Seagate had just begun to offer a 3 TB hard drive. Each core offered 4 FLOPs per cycle, meaning that your 8-core server running at 2.5 GHz offered a blazing fast 80 GFLOPs.&lt;/p&gt;
    &lt;p&gt;The boom in distributed computing rode on this wave: if you wanted to do anything that involved retrieval of data, you needed a lot of disks to get the storage throughput you want. If you wanted to do large computations, you generally needed a lot of CPUs. This meant that you needed to coordinate between a lot of CPUs to get most things done.&lt;/p&gt;
    &lt;p&gt;Since that time began, the size of servers has increased a lot, and SSDs have increased available IOPS by a factor of at least 100, but the size of mainstream VMs and containers hasn't increased much, and we still use virtualized drives that perform more like hard drives than SSDs (although this gap is closing).&lt;/p&gt;
    &lt;head rend="h4"&gt;One Server (Plus a Backup) is Usually Plenty&lt;/head&gt;
    &lt;p&gt;If you are doing anything short of video streaming, and you have under 10k QPS, one server will generally be fine for most web services. For really simple services, one server could even make it to a million QPS or so. Very few web services get this much traffic - if you have one, you know about it. Even if you're serving video, running only one server for your control plane is very reasonable. A benchmark can help you determine where you are. Alternatively, you can use common benchmarks of similar applications, or tables of common performance numbers to estimate how big of a machine you might need.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tall is Better than Wide&lt;/head&gt;
    &lt;p&gt;When you need a cluster of computers, if one server is not enough, using fewer larger servers will often be better than using a large fleet of small machines. There is non-zero overhead to coordinate a cluster, and that overhead is frequently O(n) on each server. To reduce this overhead, you should generally prefer to use a few large servers than to use many small servers. In the case of things like serverless computing, where you allocate tiny short-lived containers, this overhead accounts for a large fraction of the cost of use. On the other extreme end, coordinating a cluster of one computer is trivial.&lt;/p&gt;
    &lt;head rend="h4"&gt;Big Servers and Availability&lt;/head&gt;
    &lt;p&gt;The big drawback of using a single big server is availability. Your server is going to need downtime, and it is going to break. Running a primary and a backup server is usually enough, keeping them in different datacenters. A 2x2 configuration should appease the truly paranoid: two servers in a primary datacenter (or cloud provider) and two servers in a backup datacenter will give you a lot of redundancy. If you want a third backup deployment, you can often make that smaller than your primary and secondary.&lt;/p&gt;
    &lt;p&gt;However, you may still have to be concerned about correlated hardware failures. Hard drives (and now SSDs) have been known to occasionally have correlated failures: if you see one disk fail, you are a lot more likely to see a second failure before getting back up if your disks are from the same manufacturing batch. Services like Backblaze overcome this by using many different models of disks from multiple manufacturers. Hacker news learned this the hard way recently when the primary and backup server went down at the same time.&lt;/p&gt;
    &lt;p&gt;If you are using a hosting provider which rents pre-built servers, it is prudent to rent two different types of servers in each of your primary and backup datacenters. This should avoid almost every failure mode present in modern systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Use the Cloud, but don't be too Cloudy&lt;/head&gt;
    &lt;p&gt;A combination of availability and ease of use is one of the big reasons why I (and most other engineers) like cloud computers. Yes, you pay a significant premium to rent the machines, but your cloud provider has so much experience building servers that you don't even see most failures, and for the other failures, you can get back up and running really quickly by renting a new machine in their nearly-limitless pool of compute. It is their job to make sure that you don't experience downtime, and while they don't always do it perfectly, they are pretty good at it.&lt;/p&gt;
    &lt;p&gt;Hosting providers who are willing to rent you a server are a cheaper alternative to cloud providers, but these providers can sometimes have poor quality and some of them don't understand things like network provisioning and correlated hardware failures. Also, moving from one rented server to a larger one is a lot more annoying than resizing a cloud VM. Cloud servers have a price premium for a good reason.&lt;/p&gt;
    &lt;p&gt;However, when you deal with clouds, your salespeople will generally push you towards "cloud-native" architecture. These are things like microservices in auto-scaling VM groups with legions of load balancers between them, and vendor-lock-in-enhancing products like serverless computing and managed high-availability databases. There is a good reason that cloud salespeople are the ones pushing "cloud architecture" - it's better for them!&lt;/p&gt;
    &lt;p&gt;The conventional wisdom is that using cloud architecture is good because it lets you scale up effortlessly. There are good reasons to use cloud-native architecture, but serving lots of people is not one of them: most services can serve millions of people at a time with one server, and will never give you a surprise five-figure bill.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Should I Pay for Peak Load?&lt;/head&gt;
    &lt;p&gt;One common criticism of the "one big server" approach is that you now have to pay for your peak usage instead of paying as you go for what you use. Thus, serverless computing or fleets of microservice VMs more closely align your costs with your profit.&lt;/p&gt;
    &lt;p&gt;Unfortunately, since all of your services run on servers (whether you like it or not), someone in that supply chain is charging you based on their peak load. Part of the "cloud premium" for load balancers, serverless computing, and small VMs is based on how much extra capacity your cloud provider needs to build in order to handle their peak load. You're paying for someone's peak load anyway!&lt;/p&gt;
    &lt;p&gt;This means that if your workload is exceptionally bursty - like a simulation that needs to run once and then turn off forever - you should prefer to reach for "cloudy" solutions, but if your workload is not so bursty, you will often have a cheaper system (and an easier time building it) if you go for few large servers. If your cloud provider's usage is more bursty than yours, you are going to pay that premium for no benefit.&lt;/p&gt;
    &lt;p&gt;This premium applies to VMs, too, not just cloud services. However, if you are running a cloud VM 24/7, you can avoid paying the "peak load premium" by using 1-year contracts or negotiating with a salesperson if you are big enough.&lt;/p&gt;
    &lt;p&gt;Generally, the burstier your workload is, the more cloudy your architecture should be.&lt;/p&gt;
    &lt;head rend="h4"&gt;How Much Does it Cost to be Cloudy?&lt;/head&gt;
    &lt;p&gt;Being cloudy is expensive. Generally, I would anticipate a 5-30x price premium depending on what you buy from a cloud company, and depending on the baseline. Not 5-30%, a factor of between 5 and 30.&lt;/p&gt;
    &lt;p&gt;Here is the pricing of AWS lambda: $0.20 per 1M requests + $0.0000166667 per GB-second of RAM. I am using pricing for an x86 CPU here to keep parity with the m6a.metal instance we saw above. Large ARM servers and serverless ARM compute are both cheaper.&lt;/p&gt;
    &lt;p&gt;Assuming your server costs $8.2944/hour, and is capable of 1k QPS with 768 GB of RAM:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;1k QPS is 60k queries per minute, or 3.6M queries per hour&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each query here gets 0.768 GB-seconds of RAM (amortized)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Replacing this server would cost about $46/hour using serverless computing&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The price premium for serverless computing over the instance is a factor of 5.5. If you can keep that server over 20% utilization, using the server will be cheaper than using serverless computing. This is before any form of savings plan you can apply to that server - if you can rent those big servers from the spot market or if you compare to the price you can get with a 1-year contract, the price premium is even higher.&lt;/p&gt;
    &lt;p&gt;If you compare to the OVHCloud rental price for the same server, the price premium of buying your compute through AWS lambda is a factor of 25&lt;/p&gt;
    &lt;p&gt;If you are considering renting a server from a low-cost hosting provider or using AWS lambda, you should prefer the hosting provider if you can keep the server operating at 5% capacity!&lt;/p&gt;
    &lt;p&gt;Also, note that the actual QPS number doesn't matter: if the $8.2944/hour server is capable of 100k QPS, the query would use 100x less memory-time, meaning that you would arrive at the same 5.5x (or 25x) premium. Of course, you should scale the size of the server to fit your application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common Objections to One Big Server&lt;/head&gt;
    &lt;p&gt;If you propose using the one big server approach, you will often get pushback from people who are more comfortable with the cloud, prefer to be fashionable, or have legitimate concerns. Use your judgment when you think about it, but most people vastly underestimate how much "cloud architecture" actually costs compared to the underlying compute. Here are some common objections.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Hire Sysadmins&lt;/head&gt;
    &lt;p&gt;Yes you do. They are just now called "Cloud Ops" and are under a different manager. Also, their ability to read the arcane documentation that comes from cloud companies and keep up with the corresponding torrents of updates and deprecations makes them 5x more expensive than system administrators.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Do Security Updates&lt;/head&gt;
    &lt;p&gt;Yes you do. You may have to do fewer of them, but the ones you don't have to do are the easy ones to automate. You are still going to share in the pain of auditing libraries you use, and making sure that all of your configurations are secure.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Worry About it Going Down&lt;/head&gt;
    &lt;p&gt;The "high availability" architectures you get from using cloudy constructs and microservices just about make up for the fragility they add due to complexity. At this point, if you use two different cloud regions or two cloud providers, you can generally assume that is good enough to avoid your service going down. However, cloud providers have often had global outages in the past, and there is no reason to assume that cloud datacenters will be down any less often than your individual servers.&lt;/p&gt;
    &lt;p&gt;Remember that we are trying to prevent correlated failures. Cloud datacenters have a lot of parts that can fail in correlated ways. Hosting providers have many fewer of these parts. Similarly, complex cloud services, like managed databases, have more failure modes than simple ones (VMs).&lt;/p&gt;
    &lt;head rend="h4"&gt;But I can Develop More Quickly if I use Cloud Architecture&lt;/head&gt;
    &lt;p&gt;Then do it, and just keep an eye on the bill and think about when it's worth it to switch. This is probably the strongest argument in favor of using cloudy constructs. However, if you don't think about it as you grow, you will likely end up burning a lot of money on your cloudy architecture long past the time to switch to something more boring.&lt;/p&gt;
    &lt;head rend="h4"&gt;My Workload is Really Bursty&lt;/head&gt;
    &lt;p&gt;Cloud away. That is a great reason to use things like serverless computing. One of the big benefits of cloud architecture constructs is that the scale down really well. If your workload goes through long periods of idleness punctuated with large unpredictable bursts of activity, cloud architecture probably works really well for you.&lt;/p&gt;
    &lt;head rend="h4"&gt;What about CDNs?&lt;/head&gt;
    &lt;p&gt;It's impossible to get the benefits of a CDN, both in latency improvements and bandwidth savings, with one big server. This is also true of other systems that need to be distributed, like backups. Thankfully CDNs and backups are competitive markets, and relatively cheap. These are the kind of thing to buy rather than build.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Note On Microservices and Monoliths&lt;/head&gt;
    &lt;p&gt;Thinking about "one big server" naturally lines up with thinking about monolithic architectures. However, you don't need to use a monolith to use one server. You can run many containers on one big server, with one microservice per container. However, microservice architectures in general add a lot of overhead to a system for dubious gain when you are running on one big server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;When you experience growing pains, and get close to the limits of your current servers, today's conventional wisdom is to go for sharding and horizontal scaling, or to use a cloud architecture that gives you horizontal scaling "for free." It is often easier and more efficient to scale vertically instead. Using one big server is comparatively cheap, keeps your overheads at a minimum, and actually has a pretty good availability story if you are careful to prevent correlated hardware failures. It's not glamorous and it won't help your resume, but one big server will serve you well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://specbranch.com/posts/one-big-server/"/></entry><entry><id>https://news.ycombinator.com/item?id=45086020</id><title>Eternal Struggle</title><updated>2025-09-01T15:36:10.640058+00:00</updated><content>&lt;doc fingerprint="57c9d1e55408cc08"&gt;
  &lt;main&gt;
    &lt;p&gt;change background&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yoavg.github.io/eternal/"/></entry><entry><id>https://news.ycombinator.com/item?id=45086210</id><title>What to do with C++ modules?</title><updated>2025-09-01T15:36:10.144150+00:00</updated><content/><link href="https://nibblestew.blogspot.com/2025/08/we-need-to-seriously-think-about-what.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45087396</id><title>We should have the ability to run any code we want on hardware we own</title><updated>2025-09-01T15:36:09.984429+00:00</updated><content>&lt;doc fingerprint="f58638d75127bfe5"&gt;
  &lt;main&gt;
    &lt;p&gt;Sideloading has been a hot topic for the last decade. Most recently, Google has announced further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: âI should be able to run whatever code I want on hardware I ownâ. I agree entirely with this point, but within the context of this discussion itâs moot.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âI should be able to run whatever code I want on hardware I ownâ&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When Google restricts your ability to install certain applications they arenât constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. Itâs through this control of the operating system that Google is exerting control, not at the hardware layer. You often donât have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. Appleâs success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.&lt;/p&gt;
    &lt;p&gt;You shouldnât take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldnât be of the restrictions in place in the operating systems they provide â rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer Sonyâs restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45087748</id><title>A Linux version of the Procmon Sysinternals tool</title><updated>2025-09-01T15:36:09.441576+00:00</updated><content>&lt;doc fingerprint="a010c81b3ca44182"&gt;
  &lt;main&gt;
    &lt;p&gt;Process Monitor (Procmon) is a Linux reimagining of the classic Procmon tool from the Sysinternals suite of tools for Windows. Procmon provides a convenient and efficient way for Linux developers to trace the syscall activity on the system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OS: Ubuntu 18.04 lts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cmake&lt;/code&gt;&amp;gt;= 3.14 (build-time only)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;libsqlite3-dev&lt;/code&gt;&amp;gt;= 3.22 (build-time only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please see installation instructions here.&lt;/p&gt;
    &lt;p&gt;Please see build instructions here.&lt;/p&gt;
    &lt;code&gt;Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file&lt;/code&gt;
    &lt;p&gt;The following traces all processes and syscalls on the system:&lt;/p&gt;
    &lt;code&gt;sudo procmon&lt;/code&gt;
    &lt;p&gt;The following traces processes with process id 10 and 20:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 10,20&lt;/code&gt;
    &lt;p&gt;The following traces process 20 only syscalls read, write and open at:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 20 -e read,write,openat&lt;/code&gt;
    &lt;p&gt;The following traces process 35 and opens Procmon in headless mode to output all captured events to file &lt;code&gt;procmon.db&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 35 -c procmon.db&lt;/code&gt;
    &lt;p&gt;The following opens a Procmon &lt;code&gt;tracefile&lt;/code&gt;, &lt;code&gt;procmon.db&lt;/code&gt;, within the Procmon TUI:&lt;/p&gt;
    &lt;code&gt;sudo procmon -f procmon.db&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ask a question on Stack Overflow (tag with ProcmonForLinux)&lt;/item&gt;
      &lt;item&gt;Request a new feature on GitHub&lt;/item&gt;
      &lt;item&gt;Vote for popular feature requests&lt;/item&gt;
      &lt;item&gt;File a bug in GitHub Issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are interested in fixing issues and contributing directly to the code base, please see the document How to Contribute, which covers the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to build and run from the source&lt;/item&gt;
      &lt;item&gt;The development workflow, including debugging and running tests&lt;/item&gt;
      &lt;item&gt;Coding Guidelines&lt;/item&gt;
      &lt;item&gt;Submitting pull requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please see also our Code of Conduct.&lt;/p&gt;
    &lt;p&gt;Copyright (c) Microsoft Corporation. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Licensed under the MIT License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/microsoft/ProcMon-for-Linux"/></entry><entry><id>https://news.ycombinator.com/item?id=45087815</id><title>Lewis and Clark marked their trail with laxatives</title><updated>2025-09-01T15:36:08.956878+00:00</updated><content>&lt;doc fingerprint="b4199d0d24278264"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;ASTORIA, CLATSOP COUNTY; 1800s:&lt;/head&gt;
    &lt;head rend="h1"&gt;Lewis and Clark marked their trail with laxatives&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Audio version is not yet available&lt;/head&gt;
          &lt;head&gt;By Finn J.D. John&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;“Some people have stated that the Lewis and Clark Expedition would have been better off if they had taken a trained physician along to care for the numerous problems that they encountered. I totally disagree,” says physician and historian David Peck. “I think a trained physician would have been overly confident and possibly would have been much more aggressive in their treatment of illnesses, often times to the detriment of the patient.”&lt;/p&gt;
          &lt;p&gt;In lieu of a trained physician, the Corps of Discovery’s leaders got some basic medical training, along with a bag full of the tools of allopathic intervention: lancets for bleeding patients, blister powder for inducing “heat,” opium products for relieving pain and inducing sleep â and purgatives.&lt;/p&gt;
          &lt;p&gt;Those purgatives are the heroes of our story today. They came in the form of beefy pills, about four times the size of a standard aspirin tablet, which Rush called “Dr. Rush’s Bilious Pills.” They contained about 10 grains of calomel and 10 to 15 grains of jalap.&lt;/p&gt;
          &lt;p&gt;Jalap, the powdered root of a Mexican variety of morning glory, is a natural laxative of considerable power.&lt;/p&gt;
          &lt;p&gt;And calomel ... ah, calomel. Calomel was the wonder drug of the age. Its chemical name is mercury chloride. In large doses (and they don’t get much larger than 10 grains, or 20 if a fellow takes two of them, as Dr. Rush recommended!) it functions as a savage purgative, causing lengthy and productive sessions in the outhouse and leaving a patient thoroughly depleted and hopefully in full restoration of his bile balance.&lt;/p&gt;
          &lt;p&gt;Calomel also was the only thing known to be effective against syphilis, which was always an issue with military outfits. Whether picked up from a friendly lady in a waterfront St. Louis “sporting house” before the journey, or from an equally friendly Native lady met along the way, syphilis went with soldiers like ice cold milk with an Oreo cookie.&lt;/p&gt;
          &lt;p&gt;When symptoms broke out, the patient would be dosed with “thunder clappers” and slathered with topical mercury ointments until he started salivating ferociously, which was a symptom of mild mercury poisoning but at the time was considered a sure sign that the body was purging the sickness out of itself.&lt;/p&gt;
          &lt;p&gt;And yes, a few of the men did end up needing treatment for syphilis. But everyone in the party needed a good laxative âon the regularâ (sorry about that). Week after week, hunting parties went out and brought back animals to eat. The explorers lived on almost nothing but meat.&lt;/p&gt;
          &lt;p&gt;And this low-fiber diet had predictable results.&lt;/p&gt;
          &lt;p&gt;It had another result, too, which was less predictable â although highly convenient for later historians. The fact is, mercury chloride is only slightly soluble in human digestion. Plus, the reason it works is, it irritates the tissues of the digestive tract severely, causing the body to expel it just as fast as it possibly can before more damage can be done. So, most of the calomel in any given “bilious pill” gets blown out post-haste in the ensuing “purge.”&lt;/p&gt;
          &lt;p&gt;Then, once out of the body and in the earth, it lasts literally for centuries without breaking down or dissolving away.&lt;/p&gt;
          &lt;p&gt;So as Lewis and Clark and their crew made their way across the continent, and across Oregon, they were unknowingly depositing a trail of heavy-metal laxatives along the way â a trail that historians and scientists have been able to detect and use to document almost their every, uh, movement.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45089938</id><title>Preserving Order in Concurrent Go Apps: Three Approaches Compared</title><updated>2025-09-01T15:36:08.512286+00:00</updated><content>&lt;doc fingerprint="763a97941ef85e95"&gt;
  &lt;main&gt;
    &lt;p&gt;Concurrency is one of Go’s greatest strengths, but it comes with a fundamental trade-off: when multiple goroutines process data simultaneously, the natural ordering gets scrambled. Most of the time, this is fine – unordered processing is enough, it’s faster and simpler.&lt;/p&gt;
    &lt;p&gt;But sometimes, order matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;When Order Matters&lt;/head&gt;
    &lt;p&gt;Here are three real-world scenarios where preserving order becomes critical:&lt;/p&gt;
    &lt;p&gt;Real-time Log Enrichment: You’re processing a high-volume log stream, enriching each entry with user metadata from a database or external API. Sequential processing can’t keep up with the incoming rate, but concurrent processing breaks the sequence, making the enriched logs unusable for downstream consumers that depend on chronological order.&lt;/p&gt;
    &lt;p&gt;Finding the First Match in a File List: You need to download a list of files from cloud storage and find the first one containing a specific string. Concurrent downloads are much faster, but they complete out of order – the 50th file might finish before the 5th file, so you can’t simply return the first match you find without knowing if an earlier file also contains the string.&lt;/p&gt;
    &lt;p&gt;Time Series Data Processing: This scenario inspired my original implementation. I needed to download 90 days of transaction logs (~600MB each), extract some data, then compare consecutive days for trend analysis. Sequential downloads took hours; concurrent downloads could give an order of magnitude speedup, but would destroy the temporal relationships I needed for comparison.&lt;/p&gt;
    &lt;p&gt;The challenge is clear: we need the speed benefits of concurrent processing without sacrificing the predictability of ordered results. This isn’t just a theoretical problem – it’s a practical constraint that affects real systems at scale.&lt;/p&gt;
    &lt;p&gt;In this article, we’ll explore three approaches I’ve developed and used in production Go applications. We’ll build a concurrent &lt;code&gt;OrderedMap&lt;/code&gt; function that transforms a channel of inputs into a channel of outputs while preserving order. Through benchmarks of each approach, we’ll understand their trade-offs and discover surprising performance insights along the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem: Why Concurrency Breaks Order&lt;/head&gt;
    &lt;p&gt;Let’s quickly recall why concurrency messes up ordering. One of the reasons is that goroutines process tasks at different speeds. Another common reason – we can’t predict how exactly goroutines will be scheduled by the Go runtime.&lt;/p&gt;
    &lt;p&gt;For example, goroutine #2 might finish processing item #50 before goroutine #1 finishes item #10, causing results to arrive out of order. This is the natural behavior of concurrent processing.&lt;/p&gt;
    &lt;p&gt;If you want to see this in action, here’s a quick demo the Go playground.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Philosophy: Backpressure vs Buffering&lt;/head&gt;
    &lt;p&gt;The classic approach to ordered concurrency uses some sort of reorder buffer or queue. When a worker calculates a result but it’s too early to write it to the output, the result gets stored in that buffer until it can be written in the correct order.&lt;/p&gt;
    &lt;p&gt;In such designs buffers can typically grow without bound. This happens when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The input is skewed – early items take longer to process than later items&lt;/item&gt;
      &lt;item&gt;Downstream consumers are slow&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The algorithms presented below are backpressure-first. If a worker can’t yet write its result to the output channel, it blocks. This design is memory-bound and preserves the behavior developers expect from Go channels.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Technically speaking, such algorithms also do buffering, but here out-of-order items are held on the stacks of running goroutines. So, to get a larger “buffer” in these algorithms, you can simply increase the concurrency level. This works well in practice since typically when applications need larger buffers they also need higher concurrency levels.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Establishing a Performance Baseline&lt;/head&gt;
    &lt;p&gt;To understand the true cost of ordering, we first need a baseline to measure against. Let’s implement and benchmark a basic concurrent &lt;code&gt;Map&lt;/code&gt; function that doesn’t preserve order – this will show us exactly what overhead the ordering approaches add.&lt;/p&gt;
    &lt;p&gt;Our &lt;code&gt;Map&lt;/code&gt; function transforms an input channel into an output channel using a user-supplied function &lt;code&gt;f&lt;/code&gt;. It’s built on top of a simple worker pool, which spawns multiple goroutines to process input items concurrently.&lt;/p&gt;
    &lt;code&gt;// Map transforms items from the input channel using n goroutines, and the
// provided function f. Returns a new channel with transformed items.
func Map[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	out := make(chan B)
	Loop(in, n, out, func(a A) {
		out &amp;lt;- f(a)
	})
	return out
}

// Loop is a worker pool implementation. It calls function f for each 
// item from the input channel using n goroutines. This is a non-blocking function 
// that signals completion by closing the done channel when all work is finished.
func Loop[A, B any](in &amp;lt;-chan A, n int, done chan&amp;lt;- B, f func(A)) {
	var wg sync.WaitGroup

	for i := 0; i &amp;lt; n; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for a := range in {
				f(a)
			}
		}()
	}

	go func() {
		wg.Wait()
		if done != nil {
			close(done)
		}
	}()
}

// Discard is a non-blocking function that consumes and discards
// all items from the input channel
func Discard[A any](in &amp;lt;-chan A) {
	go func() {
		for range in {
			// Discard the value
		}
	}()
}

func BenchmarkMap(b *testing.B) {
	for _, n := range []int{1, 2, 4, 8, 12, 50} {
		b.Run(fmt.Sprint("n=", n), func(b *testing.B) {
			in := make(chan int)
			defer close(in)
			out := Map(in, n, func(a int) int {
				//time.Sleep(50 * time.Microsecond)
				return a // no-op: just return the original value
			})
			Discard(out)

			b.ReportAllocs()
			b.ResetTimer()

			for i := 0; i &amp;lt; b.N; i++ {
				in &amp;lt;- 10 // write something to the in chan
			}
		})
	}
}&lt;/code&gt;
    &lt;p&gt;As you can see, &lt;code&gt;Map&lt;/code&gt; uses &lt;code&gt;Loop&lt;/code&gt; to create a worker pool that processes items concurrently, while &lt;code&gt;Loop&lt;/code&gt; itself handles the low-level goroutine management and synchronization. This separation of concerns will become important later when we build our ordered variants.&lt;/p&gt;
    &lt;p&gt;What exactly are we measuring here? We’re measuring throughput – how fast we can push items through the entire pipeline. Since the &lt;code&gt;Map&lt;/code&gt; function creates backpressure (blocking when the pipeline is full), the rate at which we can feed items into the input channel acts as an accurate proxy for overall processing speed.
Let’s run the benchmark (I used Apple M2 Max laptop to run it):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;408.6ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;445.1ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;546.4ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;600.2ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1053ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You might wonder: “Shouldn’t higher concurrency increase throughput?” In real applications, absolutely – but only when there’s actual work to parallelize. Here I used a trivial no-op transformation to isolate and benchmark the pure overhead of goroutines, channels, and coordination. As expected, this overhead grows with the number of goroutines.&lt;/p&gt;
    &lt;p&gt;We’ll use this overhead-focused benchmark for comparisons later in the article, but to demonstrate that concurrency improves performance, let’s run one more benchmark with some work simulated (50μs sleep):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;61656ns&lt;/cell&gt;
        &lt;cell&gt;1.0x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;30429ns&lt;/cell&gt;
        &lt;cell&gt;2.0x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;15207ns&lt;/cell&gt;
        &lt;cell&gt;4.1x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;7524ns&lt;/cell&gt;
        &lt;cell&gt;8.2x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;5034ns&lt;/cell&gt;
        &lt;cell&gt;12.2x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1277ns&lt;/cell&gt;
        &lt;cell&gt;48.3x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Perfect! Here we see the dramatic benefits of concurrency when there’s real work to be done. With 50μs of work per item, increasing concurrency from 1 to 50 goroutines improves performance by nearly 50x. This demonstrates why concurrent processing is so valuable in real applications.&lt;/p&gt;
    &lt;p&gt;We’re now ready to compare the 3 approaches and measure exactly what price we pay for adding order preservation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 1: ReplyTo Channels&lt;/head&gt;
    &lt;p&gt;This is probably the most Go-native way to implement ordered concurrency. The ReplyTo pattern is well-known in Go (I also used it in my batching article), but somehow this was the hardest approach for me to explain clearly.&lt;/p&gt;
    &lt;p&gt;Here’s how it works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A packer goroutine creates jobs by attaching a unique &lt;code&gt;replyTo&lt;/code&gt;channel to every input item.&lt;/item&gt;
      &lt;item&gt;Workers process jobs concurrently, and send results through those &lt;code&gt;replyTo&lt;/code&gt;channels.&lt;/item&gt;
      &lt;item&gt;An unpacker goroutine unpacks the values sent via &lt;code&gt;replyTo&lt;/code&gt;channels and writes them to the output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The following diagram illustrates how this pattern in more detail:&lt;/p&gt;
    &lt;p&gt;The left part of this diagram is sequential (packer and unpacker) while the worker pool on the right operates concurrently. Notice that workers can only send results when the unpacker is ready to receive them, because the &lt;code&gt;replyTo&lt;/code&gt; channels are unbuffered. This creates natural backpressure and prevents unnecessary buffering.&lt;/p&gt;
    &lt;code&gt;func OrderedMap1[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job struct {
		Item    A
		ReplyTo chan B
	}

	// Packer goroutine.
	// `jobs` chan will be processed by the pool
	// `replies` chan will be consumed by unpacker goroutine
	jobs := make(chan Job)
	replies := make(chan chan B, n)
	go func() {
		for item := range in {
			replyTo := make(chan B)
			jobs &amp;lt;- Job{Item: item, ReplyTo: replyTo}
			replies &amp;lt;- replyTo
		}
		close(jobs)
		close(replies)
	}()

	// Worker pool of n goroutines.
	// Sends results back via replyTo channels
	Loop[Job, any](jobs, n, nil, func(job Job) {
		job.ReplyTo &amp;lt;- f(job.Item) // Calculate the result and send it back
		close(job.ReplyTo)
	})

	// Unpacker goroutine.
	// Unpacks replyTo channels in order and sends results to the `out` channel
	out := make(chan B)
	go func() {
		defer close(out)
		for replyTo := range replies {
			result := &amp;lt;-replyTo
			out &amp;lt;- result
		}
	}()
	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;818.7ns&lt;/cell&gt;
        &lt;cell&gt;+410ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;808.9ns&lt;/cell&gt;
        &lt;cell&gt;+364ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;826.8ns&lt;/cell&gt;
        &lt;cell&gt;+280ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;825.6ns&lt;/cell&gt;
        &lt;cell&gt;+225ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;772.3ns&lt;/cell&gt;
        &lt;cell&gt;-281ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This approach introduces up to 410ns of overhead per input item compared to our baseline. Part of this cost comes from allocating a new &lt;code&gt;replyTo&lt;/code&gt; channel for every item. Unfortunately, we can’t use a package level &lt;code&gt;sync.Pool&lt;/code&gt; to mitigate this because our function is generic – channels for different types can’t share the same pool.&lt;/p&gt;
    &lt;p&gt;What’s also interesting about this result is that the overhead brought by ordering becomes smaller as the number of goroutines grows. At some point even an inversion happens – &lt;code&gt;OrderedMap1&lt;/code&gt; becomes faster than &lt;code&gt;Map&lt;/code&gt; (-281ns at 50 goroutines).&lt;/p&gt;
    &lt;p&gt;I haven’t investigated this phenomenon deeply. I believe it can’t be caused by inefficiencies inside &lt;code&gt;Map&lt;/code&gt; since it’s already based on the simplest possible channel-based worker pool. One guess that I have is that in &lt;code&gt;Map&lt;/code&gt; we have 50 goroutines competing to write into a single output channel. On the contrary, in &lt;code&gt;OrderedMap&lt;/code&gt;, despite additional moving parts, only one goroutine is writing to the output.&lt;/p&gt;
    &lt;p&gt;Let’s now move on to the next approach.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 2: sync.Cond for Turn-Taking&lt;/head&gt;
    &lt;p&gt;This was the first algorithm I implemented when I needed ordered concurrency, and it’s much easier to explain than the ReplyTo approach.&lt;/p&gt;
    &lt;p&gt;Here we attach an incremental index to each item and send it to the worker pool. Each worker performs the calculation, then waits its turn to write the result to the output channel.&lt;/p&gt;
    &lt;p&gt;This conditional waiting is implemented using a shared &lt;code&gt;currentIndex&lt;/code&gt; variable protected by &lt;code&gt;sync.Cond&lt;/code&gt;, a powerful but underused concurrency primitive from the standard library that allows goroutines to wait for specific conditions and be woken up when those conditions change.&lt;/p&gt;
    &lt;p&gt;Here’s how the turn-taking mechanism works:&lt;/p&gt;
    &lt;p&gt;Here, after each write, all workers wake up (using broadcast) and recheck “is it my turn?” condition&lt;/p&gt;
    &lt;code&gt;func OrderedMap2[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job struct {
		Item  A
		Index int
	}

	// Indexer goroutine.
	// Assign an index to each item from the input channel
	jobs := make(chan Job)
	go func() {
		i := 0
		for item := range in {
			jobs &amp;lt;- Job{Item: item, Index: i}
			i++
		}
		close(jobs)
	}()

	// Shared state.
	// Index of the next result that must be written to the output channel.
	nextIndex := 0
	cond := sync.NewCond(new(sync.Mutex))

	// Worker pool of n goroutines.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job) {
		result := f(job.Item) // Calculate the result

		// Cond must be used with a locked mutex (see stdlib docs)
		cond.L.Lock()

		// wait until it's our turn to write the result
		for job.Index != nextIndex {
			cond.Wait()
		}

		// Write the result
		out &amp;lt;- result

		// Increment the index and notify all other workers
		nextIndex++
		cond.Broadcast()

		cond.L.Unlock()
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;867.7ns&lt;/cell&gt;
        &lt;cell&gt;+459ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;1094ns&lt;/cell&gt;
        &lt;cell&gt;+649ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;1801ns&lt;/cell&gt;
        &lt;cell&gt;+1255ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;2987ns&lt;/cell&gt;
        &lt;cell&gt;+2387ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;16074ns&lt;/cell&gt;
        &lt;cell&gt;+15021ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The results are telling – no more per-item allocations, which is excellent for memory efficiency. But there’s a critical flaw: significant performance degradation as goroutine count increases. This happens because of the shared state and the “thundering herd” problem: after each write, all goroutines wake up via &lt;code&gt;cond.Broadcast()&lt;/code&gt;, but only one will do useful work.&lt;/p&gt;
    &lt;p&gt;This inefficiency led me to think: “How can I wake only the goroutine that should write next?” And this is how the 3rd approach was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 3: Permission Passing Chain&lt;/head&gt;
    &lt;p&gt;Here’s the key insight: when is it safe to write output #5? After output #4 was written. Who knows when output #4 was written? The goroutine that wrote it.&lt;/p&gt;
    &lt;p&gt;In this algorithm, any job must hold the write permission before its worker can send results to the output channel. We chain jobs together so each one knows exactly which job comes next and can pass the permission to it. This is done by attaching two channels to each job: &lt;code&gt;canWrite&lt;/code&gt; channel to receive the permission, and &lt;code&gt;nextCanWrite&lt;/code&gt; channel to pass the permission to the next job.&lt;/p&gt;
    &lt;p&gt;This chain structure makes the worker logic remarkably simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Calculate: Process the job using the provided function&lt;/item&gt;
      &lt;item&gt;Wait: Receive the permission from &lt;code&gt;canWrite&lt;/code&gt;channel&lt;/item&gt;
      &lt;item&gt;Write: Send the result to the output channel&lt;/item&gt;
      &lt;item&gt;Pass: Send the permission to the next job via &lt;code&gt;nextCanWrite&lt;/code&gt;channel&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the diagram that illustrates the whole flow:&lt;/p&gt;
    &lt;p&gt;The green arrows show how the permission to write is passed from one job to another along the chain. Essentially this is a token-passing algorithm that eliminates the “thundering herd” problem entirely – each goroutine wakes exactly one other goroutine, creating efficient point-to-point signaling rather than expensive broadcasts.&lt;/p&gt;
    &lt;p&gt;Let’s see how this translates to code. The implementation has two parts: a “linker” goroutine that builds the chain, and workers that follow the calculate-wait-write-pass pattern:&lt;/p&gt;
    &lt;code&gt;func OrderedMap3[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = make(chan struct{}, 1)
		close(nextCanWrite) // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, make(chan struct{}, 1)
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		&amp;lt;-job.CanWrite          // Wait for the write permission
		out &amp;lt;- result           // Write to the output channel
		close(job.NextCanWrite) // Pass the permission to the next job
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;927.2ns&lt;/cell&gt;
        &lt;cell&gt;+519ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;939.8ns&lt;/cell&gt;
        &lt;cell&gt;+495ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;860.7ns&lt;/cell&gt;
        &lt;cell&gt;+314ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;823.8ns&lt;/cell&gt;
        &lt;cell&gt;+224ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;609.8ns&lt;/cell&gt;
        &lt;cell&gt;-443ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here the result is very similar to what we’ve seen in the ReplyTo approach. Almost the same overhead, the same inversion at higher levels of concurrency, and the same extra allocation per item. But there’s one difference…&lt;/p&gt;
    &lt;p&gt;Unlike approach 1, here we’re allocating a non-generic &lt;code&gt;chan struct{}&lt;/code&gt;. This means we can use a package level &lt;code&gt;sync.Pool&lt;/code&gt; to eliminate those allocations – let’s explore that next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 3a: Zero-Allocation Permission Passing Chain&lt;/head&gt;
    &lt;p&gt;Let’s create a pool for &lt;code&gt;canWrite&lt;/code&gt; channels. Implementation is straightforward – the pool itself and make/release functions.&lt;/p&gt;
    &lt;code&gt;// Package-level pool for canWrite channels
type chainedItem[A any] struct {
	Value        A
	CanWrite     chan struct{}
	NextCanWrite chan struct{} // canWrite channel for the next item
}

var canWritePool sync.Pool

func makeCanWriteChan() chan struct{} {
	ch := canWritePool.Get()
	if ch == nil {
		return make(chan struct{}, 1)
	}
	return ch.(chan struct{})
}

func releaseCanWriteChan(ch chan struct{}) {
	canWritePool.Put(ch)
}&lt;/code&gt;
    &lt;p&gt;Now let’s use the pool in the permission passing algorithm. Since channels are reused, we can no longer signal by closing them. Instead workers must read and write empty structs form/to these channels.&lt;/p&gt;
    &lt;code&gt;func OrderedMap3a[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite &amp;lt;- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		&amp;lt;-job.CanWrite                    // Wait for the write permission
		out &amp;lt;- result                     // Write to the output channel
		releaseCanWriteChan(job.CanWrite) // Release our canWrite channel to the pool
		job.NextCanWrite &amp;lt;- struct{}{}    // Pass the permission to the next job
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results with Pooling:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;891.0ns&lt;/cell&gt;
        &lt;cell&gt;+482ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;916.5ns&lt;/cell&gt;
        &lt;cell&gt;+471ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;879.5ns&lt;/cell&gt;
        &lt;cell&gt;+333ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;872.6ns&lt;/cell&gt;
        &lt;cell&gt;+272ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;657.6ns&lt;/cell&gt;
        &lt;cell&gt;-395ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Perfect! Zero allocations and good performance, meaning less GC pressure for long running jobs. But this approach has one more trick up its sleeve…&lt;/p&gt;
    &lt;head rend="h2"&gt;One more thing: Building Reusable Abstractions&lt;/head&gt;
    &lt;p&gt;The permission passing approach has another significant advantage over the ReplyTo method: it controls when to write rather than where to write.&lt;/p&gt;
    &lt;p&gt;I’ll admit it – sometimes I get a bit obsessed with building clean abstractions. When working on rill, I really wanted to extract this ordering logic into something reusable and testable. This “when vs where” distinction was an AHA moment for me.&lt;/p&gt;
    &lt;p&gt;Since the algorithm doesn’t care where the outputs are written, it’s easy to abstract it into a separate function – &lt;code&gt;OrderedLoop&lt;/code&gt;. The API is very similar to the &lt;code&gt;Loop&lt;/code&gt; function we used before, but here the user function receives two arguments – an &lt;code&gt;item&lt;/code&gt; and a &lt;code&gt;canWrite&lt;/code&gt; channel. It’s important that the user function must read from the &lt;code&gt;canWrite&lt;/code&gt; channel exactly once to avoid deadlocks or undefined behavior.&lt;/p&gt;
    &lt;code&gt;func OrderedLoop[A, B any](in &amp;lt;-chan A, done chan&amp;lt;- B, n int, f func(a A, canWrite &amp;lt;-chan struct{})) {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite &amp;lt;- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	Loop(jobs, n, done, func(job Job[A]) {
		f(job.Item, job.CanWrite) // Do the work

		releaseCanWriteChan(job.CanWrite) // Release item's canWrite channel to the pool
		job.NextCanWrite &amp;lt;- struct{}{}    // Pass the permission to the next job
	})
}&lt;/code&gt;
    &lt;p&gt;The typical usage looks like:&lt;/p&gt;
    &lt;code&gt;OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
	// [Do processing here]
	
	// Everything above this line is executed concurrently,
	// everything below it is executed sequentially and in order
	&amp;lt;-canWrite
	
	// [Write results somewhere]
})
&lt;/code&gt;
    &lt;p&gt;With this abstraction in hand it’s remarkably simple to build any ordered operations. For example &lt;code&gt;OrderedMap&lt;/code&gt; becomes just 7 lines of code:&lt;/p&gt;
    &lt;code&gt;func OrderedMap3b[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	out := make(chan B)
	OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		result := f(a)
		&amp;lt;-canWrite
		out &amp;lt;- result
	})
	return out
}&lt;/code&gt;
    &lt;p&gt;We can also easily build an &lt;code&gt;OrderedFilter&lt;/code&gt; that conditionally writes outputs:&lt;/p&gt;
    &lt;code&gt;func OrderedFilter[A any](in &amp;lt;-chan A, n int, predicate func(A) bool) &amp;lt;-chan A {
	out := make(chan A)
	OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		keep := predicate(a)
		&amp;lt;-canWrite
		if keep {
			out &amp;lt;- a
		}
	})
	return out
}&lt;/code&gt;
    &lt;p&gt;Or even an &lt;code&gt;OrderedSplit&lt;/code&gt; that distributes items to two channels based on a predicate:&lt;/p&gt;
    &lt;code&gt;func OrderedSplit[A any](in &amp;lt;-chan A, n int, predicate func(A) bool) (&amp;lt;-chan A, &amp;lt;-chan A) {
	outTrue := make(chan A)
	outFalse := make(chan A)
	done := make(chan struct{})
	
	OrderedLoop(in, done, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		shouldGoToTrue := predicate(a)
		&amp;lt;-canWrite
		if shouldGoToTrue {
			outTrue &amp;lt;- a
		} else {
			outFalse &amp;lt;- a
		}
	})
	
	go func() {
		&amp;lt;-done
		close(outTrue)
		close(outFalse)
	}()
	
	return outTrue, outFalse
}&lt;/code&gt;
    &lt;p&gt;Simply put, this abstraction makes building ordered operations trivial.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Comparison&lt;/head&gt;
    &lt;p&gt;Here’s how all approaches perform across different concurrency levels:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Concurrency&lt;/cell&gt;
        &lt;cell role="head"&gt;Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 1&lt;p&gt;(ReplyTo)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 2&lt;p&gt;(sync.Cond)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 3&lt;p&gt;(Permission)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 3a&lt;p&gt;(+ Pool)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;408.6ns&lt;/cell&gt;
        &lt;cell&gt;818.7ns&lt;/cell&gt;
        &lt;cell&gt;867.7ns&lt;/cell&gt;
        &lt;cell&gt;927.2ns&lt;/cell&gt;
        &lt;cell&gt;891.0ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;445.1ns&lt;/cell&gt;
        &lt;cell&gt;808.9ns&lt;/cell&gt;
        &lt;cell&gt;1094ns&lt;/cell&gt;
        &lt;cell&gt;939.8ns&lt;/cell&gt;
        &lt;cell&gt;916.5ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;546.4ns&lt;/cell&gt;
        &lt;cell&gt;826.8ns&lt;/cell&gt;
        &lt;cell&gt;1801ns&lt;/cell&gt;
        &lt;cell&gt;860.7ns&lt;/cell&gt;
        &lt;cell&gt;879.5ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;600.2ns&lt;/cell&gt;
        &lt;cell&gt;825.6ns&lt;/cell&gt;
        &lt;cell&gt;2987ns&lt;/cell&gt;
        &lt;cell&gt;823.8ns&lt;/cell&gt;
        &lt;cell&gt;872.6ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1053ns&lt;/cell&gt;
        &lt;cell&gt;772.3ns&lt;/cell&gt;
        &lt;cell&gt;16074ns&lt;/cell&gt;
        &lt;cell&gt;609.8ns&lt;/cell&gt;
        &lt;cell&gt;657.6ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Zero allocs&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;sync.Cond is a no-go for ordered concurrency – While it starts with decent performance at low concurrency, it completely falls apart as goroutine count increases, due to the thundering herd problem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ReplyTo is a strong contender – it adds at most ~500ns of overhead compared to the baseline, but requires one additional allocation per input item, increasing GC pressure.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Permission Passing emerges as the clear winner – It has it all:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Good performance: at most ~500ns of overhead compared to the baseline&lt;/item&gt;
          &lt;item&gt;Zero allocations: Less GC pressure for long running tasks&lt;/item&gt;
          &lt;item&gt;Clean abstraction: Core synchronization logic can be abstracted away and used to build various concurrent operations.&lt;/item&gt;
          &lt;item&gt;Maintainability: Separation of concerns and the intuitive “calculate → wait → write → pass” pattern make code easy to support and reason about&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This exploration shows that ordered concurrency doesn’t have to be expensive. With the right approach, you can have concurrency, ordering and backpressure at the same time. The permission passing pattern, in particular, demonstrates how Go’s channels can be used creatively to solve complex coordination problems.&lt;/p&gt;
    &lt;p&gt;Finally, these patterns have been battle-tested in production through rill concurrency toolkit (1.7k 🌟 on GitHub). It implements &lt;code&gt;Map&lt;/code&gt;, &lt;code&gt;OrderedMap&lt;/code&gt;, and many other concurrent operations. Rill focuses on composability – operations chain together into larger pipelines – while adding comprehensive error handling, context-friendly design, and maintaining over 95% test coverage.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://destel.dev/blog/preserving-order-in-concurrent-go"/></entry><entry><id>https://news.ycombinator.com/item?id=45090216</id><title>Telli (YC F24) is hiring engineers, designers, and interns (on-site in Berlin)</title><updated>2025-09-01T15:36:07.870662+00:00</updated><link href="https://hi.telli.com/join-us"/></entry><entry><id>https://news.ycombinator.com/item?id=45091119</id><title>UK's largest battery storage facility at Tilbury substation</title><updated>2025-09-01T15:36:07.023285+00:00</updated><content>&lt;doc fingerprint="95c5807b1a9ef1c2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;National Grid connects UK’s largest battery storage facility at Tilbury substation&lt;/head&gt;
    &lt;p&gt;National Grid has connected the UK’s largest battery energy storage system (BESS) to its transmission network at Tilbury substation in Essex.&lt;/p&gt;
    &lt;p&gt;The 300MW Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.&lt;/p&gt;
    &lt;p&gt;With a total capacity of 600MWh, Thurrock Storage is capable of powering up to 680,000 homes, and can help to balance supply and demand by soaking up surplus clean electricity and discharging it instantaneously when the grid needs it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our Tilbury substation once served a coal plant, and with battery connections like this, it’s today helping to power a more sustainable future for the region and the country.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;National Grid reinforced its Tilbury substation to ensure the network in the region could safely carry the battery’s significant additional load, with new protection and control systems installed to ensure a robust connection.&lt;/p&gt;
    &lt;p&gt;The substation previously served the coal-fired Tilbury A and B power stations on adjacent land prior to their demolition, so the connection of the Thurrock Storage facility marks a symbolic transition from coal to clean electricity at the site.&lt;/p&gt;
    &lt;p&gt;John Twomey, director of customer and network development at National Grid Electricity Transmission, said:&lt;/p&gt;
    &lt;p&gt;“Battery storage plays a vital role in Britain’s clean energy transition. Connecting Thurrock Storage, the UK’s biggest battery, to our transmission network marks a significant step on that journey.&lt;/p&gt;
    &lt;p&gt;“Our Tilbury substation once served a coal plant, and with battery connections like this, it’s today helping to power a more sustainable future for the region and the country.”&lt;/p&gt;
    &lt;p&gt;Tom Vernon, Statera Energy CEO and founder, said:&lt;/p&gt;
    &lt;p&gt;“We are delighted that Thurrock Storage is now energised, following its successful connection to the grid by National Grid Electricity Transmission. Increasing BESS capacity is essential for supporting the grid when renewable generation, such as solar and wind, is low or changes quickly. It ensures that energy can be stored efficiently and returned to the grid whenever it’s needed.”&lt;/p&gt;
    &lt;p&gt;National Grid is continuing work at Tilbury substation to connect the 450MW Thurrock Flexible Generation facility, another Statera project that is set to support the energy needs of the region.&lt;/p&gt;
    &lt;p&gt;The connection of the UK’s biggest battery follows energisation in July of the 373MW Cleve Hill Solar Park in Kent – the largest solar plant in the country – which National Grid connected to its adjacent Cleve Hill substation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nationalgrid.com/national-grid-connects-uks-largest-battery-storage-facility-tilbury-substation"/></entry><entry><id>https://news.ycombinator.com/item?id=45091202</id><title>De-Googling TOTP Authenticator Codes</title><updated>2025-09-01T15:36:06.637153+00:00</updated><content>&lt;doc fingerprint="8ce975dd25b7e8d3"&gt;
  &lt;main&gt;
    &lt;p&gt;In the ongoing effort to extricate myself from Google's services, I've been paring down my usage of their apps on my (admittedly Android) phone. I'm now down to two Google apps I use regularly: Maps (for traffic data) and Authenticator (for TOTP[A]Time-based One Time Password codes).&lt;/p&gt;
    &lt;p&gt;Now, I spend most of my time in a terminal window on MacOS or connected to a Linux machine; it'd be nice if I could get TOTPs on the command-line, and it turns out there's a utility called &lt;code&gt;oathtool&lt;/code&gt; that allows for TOTP generation on the CLI. However, that would mean switching my OTP provider, which usually involves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Logging into each service that has an OTP registered in the app;&lt;/item&gt;
      &lt;item&gt;Disabling two-factor authentication (2FA);&lt;/item&gt;
      &lt;item&gt;Re-enabling 2FA and using the "manual entry" code as input to &lt;code&gt;oathtool&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;Doing it all again for the next website or service.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fortunately, Google's Authenticator provides a way to migrate codes between instances of the app based on scanning QR codes, and we can use this to migrate them away from Google into a TOTP handler of our choosing. It's another four-step process:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generating a QR code in Google Authenticator for the codes you want to export;&lt;/item&gt;
      &lt;item&gt;Decoding the QR somewhere off-device, into a URL;&lt;/item&gt;
      &lt;item&gt;Decoding the URL into its constituent services and secret values;&lt;/item&gt;
      &lt;item&gt;Setting up &lt;code&gt;oathtool&lt;/code&gt;to use the secrets.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that the below steps are presented just as I went through them, you may be able to find efficiencies or you may run into troubles that I didn't (especially if you're trying this exclusively on Windows); "your mileage may vary" is apt here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going from Authenticator to a migration URL&lt;/head&gt;
    &lt;p&gt;The first step is getting the code out of Authenticator, through the Transfer Codes menu option in the app. Picking the services you'd like to extract leads you to a code like this:&lt;/p&gt;
    &lt;p&gt;You may have an app on your phone that decodes QRs, but I don't; instead, I transferred the file to my MacOS machine over Tailscale, and used a command-line tool called &lt;code&gt;qrtool&lt;/code&gt; to get the QR content:&lt;/p&gt;
    &lt;head rend="h3"&gt;Decoding the migration QR&lt;/head&gt;
    &lt;quote&gt;$ brew install qrtool $ qrtool decode Screenshot_20250901_062719_Authenticator.jpg otpauth-migration://offline?data=CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D&lt;/quote&gt;
    &lt;head rend="h2"&gt;Decoding the URL into secrets&lt;/head&gt;
    &lt;p&gt;So we have our migration URL, with a Base64-encoded data block. Unfortunately, if we were to simply decode the data, we'd end up with some binary gibberish:&lt;/p&gt;
    &lt;head rend="h3"&gt;Trying to decode the URL directly&lt;/head&gt;
    &lt;quote&gt;$ php -r 'var_dump(base64_decode("CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D"));' string(69) " &amp;lt; i*H??h??ý&lt;/quote&gt;
    &lt;p&gt;It turns out that this is a Protobuf-encoded data string, and we need to use Google's Protobuf library to get the data out. It turns out Tim Brooks has already done this with a short piece of Python at: https://github.com/brookst/otpauth_migrate&lt;/p&gt;
    &lt;p&gt;I decided to install this on a Linux machine I tend to be connected to (entirely unrelated to my Python installation being broken on Mac...):&lt;/p&gt;
    &lt;head rend="h3"&gt;Extracting the data via &lt;code&gt;otpauth_migrate&lt;/code&gt;&lt;/head&gt;
    &lt;quote&gt;$ git clone https://github.com/brookst/otpauth_migrate $ cd otpauth_migrate $ ./otpauth_migrate.py otpauth-migration://offline?data=CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D secret: "i*H\231\315h\014\212\223\016\243" name: "The Rickroll Store" algorithm: ALGORITHM_SHA1 digits: DIGIT_COUNT_SIX type: OTP_TYPE_TOTP Secret code = NEVERGONNAGIVEYOUM======&lt;/quote&gt;
    &lt;p&gt;This tool is intelligent enough to extract any number of names and secrets from a migration URL, so you can export all your codes from Authenticator into one giant QR without needing to do each separately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Using oathtool to generate OTPs&lt;/head&gt;
    &lt;p&gt;The final step is to use this secret code with &lt;code&gt;oathtool&lt;/code&gt;, which takes the secret directly as a parameter. If you instead want to refer to the service by name, Michael Bushey[1]"CLI 2-Factor Authentication", Michael Bushey, 2023 has a quick wrapper script which extracts the secrets from a locally-stored file:&lt;/p&gt;
    &lt;head rend="h3"&gt;Wrapper script to generate OTPs: &lt;code&gt;/usr/local/bin/otp&lt;/code&gt;&lt;/head&gt;
    &lt;quote&gt;&lt;del&gt;#!/bin/bash&lt;/del&gt;OTPKEY="$(sed -n "s/${1}=//p" ~/.otpkeys)" if [ -z "$OTPKEY" ]; then echo "$(basename $0): Bad Service Name '$1'" exit fi date oathtool --totp -b "$OTPKEY"&lt;/quote&gt;
    &lt;head rend="h3"&gt;OTP key store: &lt;code&gt;~/.otpkeys&lt;/code&gt;&lt;/head&gt;
    &lt;quote&gt;rickroll=NEVERGONNAGIVEYOUM======&lt;/quote&gt;
    &lt;p&gt;With this in place, you won't need to use your Authenticator app again. The tool outputs the current date and time, so you can double-check that your code won't expire (at :00 seconds) before you get a chance to type it in:&lt;/p&gt;
    &lt;quote&gt;$ otp rickroll Mon Sep 1 07:10:42 AM UTC 2025 200213&lt;/quote&gt;
    &lt;head rend="h2"&gt;Future expansion&lt;/head&gt;
    &lt;p&gt;There's a security issue here, of course, which is the exposed secret key sitting in a file on-disk. I'm happy to sit with that and not require a password to generate OTPs every time, but if you're interested in adapting the wrapper script to use symmetric encryption to secure the keys, Vivek Gite[2]"Use oathtool Linux command line for 2 step verification (2FA)", Vivek Gite, updated Feb 2025 has a set of scripts which employ &lt;code&gt;gpg&lt;/code&gt; for the job.&lt;/p&gt;
    &lt;p&gt;Now I just need to find a way to get traffic data into a maps App that doesn't involve Google's servers... Thoughts welcome.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://imrannazar.com/articles/degoogle-otp"/></entry><entry><id>https://news.ycombinator.com/item?id=45091493</id><title>CocoaPods Is Deprecated</title><updated>2025-09-01T15:36:06.542306+00:00</updated><content>&lt;doc fingerprint="42f18d113d47e29d"&gt;
  &lt;main&gt;&lt;p&gt;30 November 2024&lt;/p&gt;Follow @orta&lt;p&gt;TLDR: In two years we plan to turn CocoaPods trunk to be read-only. At that point, no new versions or pods will be added to trunk. - Note, this post has been updated in May 2025.&lt;/p&gt;&lt;p&gt;Last month I wrote about how CocoaPods is currently being maintained, I also noted that we were discussing converting the main CocoaPods spec repo "trunk" to be read-only:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We are discussing that on a very long, multi-year, basis we can drastically simplify the security of CocoaPods trunk by converting the Specs Repo to be read-only. Infrastructure like the Specs repo and the CDN would still operate as long as GitHub and jsDelivr continue to exist, which is pretty likely to be a very long time. This will keep all existing builds working.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I plan to implement the read-only mode so that when someone submits a new Podspec to CocoaPods, it will always be denied at the server level. I would then convert the "CocoaPods/Specs" repo to be marked as "Archived" on GitHub which should cover all of our bases.&lt;/p&gt;&lt;p&gt;Making the switch will not break builds for people using CocoaPods in 2026 onwards, but at that point, you're not getting any more updates to dependencies which come though CocoaPods trunk. This shouldn't affect people who use CocoaPods with their own specs repos, or have all of their dependencies vendored (e.g. they all come from npm.)&lt;/p&gt;&lt;p&gt;May 2025 Update: Since this post was originally written, we've had enough security researchers abusing scripting capabilities in CocoaPods that we are now introducing a block on allowing new CocoaPods to use the &lt;code&gt;prepare_command&lt;/code&gt; field in a Podspec. Any existing Pods using &lt;code&gt;prepare_command&lt;/code&gt; are hard-coded to bypass this check.&lt;/p&gt;&lt;head rend="h2"&gt;Timeline&lt;/head&gt;&lt;p&gt;My goal is to send 2 very hard-to-miss notifications en-masse, and then do a test run a month before the final shutdown.&lt;/p&gt;&lt;head rend="h3"&gt;May 2025&lt;/head&gt;&lt;p&gt;We are stopping new CocoaPods from being added which use the &lt;code&gt;prepare_command&lt;/code&gt; field&lt;/p&gt;&lt;head rend="h3"&gt;Mid-late 2025&lt;/head&gt;&lt;p&gt;I will email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post.&lt;/p&gt;&lt;head rend="h3"&gt;September-October 2026&lt;/head&gt;&lt;p&gt;I will, again, email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post, noting that they have roughly a month before we do a test run of going read-only.&lt;/p&gt;&lt;head rend="h3"&gt;November 1-7th 2026&lt;/head&gt;&lt;p&gt;I will trigger a test run, giving automation a chance to break early&lt;/p&gt;&lt;head rend="h3"&gt;December 2nd 2026&lt;/head&gt;&lt;p&gt;I will switch trunk to not accept new Podspecs permanently. This is a Wednesday after American Thanksgiving, so I think folks won't be in rush mode.&lt;/p&gt;&lt;head rend="h2"&gt;Contact&lt;/head&gt;&lt;p&gt;These dates are not set in stone, and maybe someone out there has a good reason for us to amend the timeline. I don't think I'm amenable to moving it forwards, but within reason there's space for backwards.&lt;/p&gt;&lt;p&gt;If you have questions, you can contact the team via [email protected], me personally at [email protected] or reach out to me via Bluesky: @orta.io.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cocoapods.org/CocoaPods-Specs-Repo/"/></entry><entry><id>https://news.ycombinator.com/item?id=45092204</id><title>Ask HN: Do custom ROMs exist for electric cars, for example Teslas?</title><updated>2025-09-01T15:36:06.300230+00:00</updated><content>&lt;doc fingerprint="a5f891364747a790"&gt;
  &lt;main&gt;
    &lt;p&gt;I don't know about electric cars, but for gas powered cars there are open source ECUs [0][1]. There are also tuners that directly modify the car's firmware to improve performance. Finally, you can connect a computer to the CAN bus [2], which allows you to capture and replay commands, as well as craft your own commands. This is how Comma's openpilot [3] works: it connects to the CAN bus and sends commands for all supported functionality.&lt;/p&gt;
    &lt;p&gt;No. Aftermarket ECUs absolutely exist for almost all internal combustion engines. Other aftermarket modules are rare. Integration of them into a complete system even more so.&lt;/p&gt;
    &lt;p&gt;I think I know what you are asking but it is complicated.&lt;/p&gt;
    &lt;p&gt;For safety, regulator, historical and frankly common sense reasons, a car is not one system. It is a system of system that communicate via a CAN BUS, https://en.wikipedia.org/wiki/CAN_bus. This is still true for electric cars. Can this be hacked? Like everything else, yes.&lt;/p&gt;
    &lt;p&gt;Can you side load a new ROM like an android device? Not that know of and hope that never becomes a reality because your phone crashing is different than you car crashing (figuratively and literally). Can you enable/disable features? Yes, usually through ECU hacking. On my P3 Volvo, I bought a cheap stripped down Chinese clone of Volvo's diagnostic tool called DiCE. Once the ECU is decrypted, which is done through brute force, you can use something like https://d5t5.com/article/vdash-volvo-diagnostic or P3Tool to change level settings like the theme of LED dash or engine tuning.&lt;/p&gt;
    &lt;p&gt;There are of course after market ECU tweaks and parts that, for example, will change your throttle response with a physical piece of hardware—Pedal Commander is a simple example.&lt;/p&gt;
    &lt;p&gt;I believe these systems are quite coupled with the hardware itself, making it quite difficult to port any custom ROM or such on them. I am not aware of any projects with the goals of creating an open-source Android ROM for a car. Even Phone ROMs are slowly dying off, with the exceptions of Lineage and GrapheneOS.&lt;/p&gt;
    &lt;p&gt;Security (for vendor) from obscurity. AFAIK most of car owners cannot just buy the replace electronics for his car on used market so most of owners afraid of messing with proprietary computers in the car.&lt;/p&gt;
    &lt;p&gt;I believe law environment need to change to make possible digital custom car's ROM. Now everything can be closed in same of safety, security, user convience...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45092204"/></entry><entry><id>https://news.ycombinator.com/item?id=45092324</id><title>Tetris is NP-hard even with O(1) rows or columns [pdf]</title><updated>2025-09-01T15:36:05.055851+00:00</updated><content/><link href="https://martindemaine.org/papers/ThinTetris_JIP/paper.pdf"/></entry><entry><id>https://news.ycombinator.com/item?id=45092490</id><title>Bear is now source-available</title><updated>2025-09-01T15:36:04.651904+00:00</updated><content>&lt;doc fingerprint="1f43f87875e18099"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bear is now source-available&lt;/head&gt;
    &lt;p&gt;When I started building Bear I made the code available under an MIT license. I didn't give it much thought at the time, but knew that I wanted the code to be available for people to learn from, and to make it easily auditable so users could validate claims I have made about the privacy and security of the platform.&lt;/p&gt;
    &lt;p&gt;Unfortunately over the years there have been cases of people forking the project in the attempt to set up a competing service. And it hurts. It hurts to see something you've worked so hard on for so long get copied and distributed with only a few hours of modification. It hurts to have poured so much love into a piece of software to see it turned against you and threaten your livelihood. It hurts to believe in open-source and then be bitten by it.&lt;/p&gt;
    &lt;p&gt;After the last instance of this I have come to the difficult decision to change Bear's license from MIT to a version of copyleft called the Elastic License—created by the Elastic Search people.&lt;/p&gt;
    &lt;p&gt;This license is almost identical to the MIT license but with the stipulation that the software cannot be provided as a hosted or managed service. You can view the specific wording here.&lt;/p&gt;
    &lt;p&gt;After spending time researching how other projects are handling this, I realise I'm not alone. Many other open-source projects have updated their licenses to prevent "free-ride competition" in the past few years.123456&lt;/p&gt;
    &lt;p&gt;We're entering a new age of AI powered coding, where creating a competing product only involves typing "Create a fork of this repo and change its name to something cool and deploy it on an EC2 instance".&lt;/p&gt;
    &lt;p&gt;While Bear's code is good, what makes the platform special is the people who use it, and the commitment to longevity.&lt;/p&gt;
    &lt;p&gt;I will ensure the platform is taken care of, even if it means backtracking on what people can do with the code itself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://herman.bearblog.dev/license/"/></entry><entry><id>https://news.ycombinator.com/item?id=45092605</id><title>Zfsbackrest: Pgbackrest style encrypted backups for ZFS filesystems</title><updated>2025-09-01T15:36:04.486679+00:00</updated><content>&lt;doc fingerprint="bf55c3c41a079343"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Experimental:&lt;lb/&gt;Do not use it as your only way for backups. This is something I wrote over a weekend. There's a lot of things that need work here.&lt;/quote&gt;
    &lt;p&gt;pgbackrest style encrypted backups for ZFS filesystems.&lt;/p&gt;
    &lt;p&gt;You need age installed to generate encryption keys. Encryption is NOT optional.&lt;/p&gt;
    &lt;code&gt;$ go install github.com/gargakshit/zfsbackrest/cmd/zfsbackrest@latest&lt;/code&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/zfsbackrest.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;debug = true # warning, may log sensitive data

[repository]
# zfsbackrest does not support changing the list of datasets after a repository
# is initialized YET. That's one feature I need.
included_datasets = ["storage/*"] # Glob is supported

[repository.s3]
# zfsbackrest does NOT support non-secure S3 endpoints.
endpoint = "todo"
bucket = "todo"
key = "todo"
secret = "todo"
region = "todo"

[repository.expiry]
# Child backups expire if the parent expires. See the model below for a better
# explanation.
full = "336h" # 14 days
diff = "120h" # 5 days
incr = "24h" # 1 day

[upload_concurrency]
full = 2
diff = 4
incr = 4&lt;/code&gt;
    &lt;code&gt;$ zfsbackrest init --age-recipient-public-key="&amp;lt;your age public key&amp;gt;"&lt;/code&gt;
    &lt;code&gt;$ zfsbackrest backup --type &amp;lt;full | diff | incr&amp;gt;&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;full&lt;/code&gt; backups are standalone. They do not depend on any other backups. They are
also huge in size because of that.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;diff&lt;/code&gt; backups are sent incrementally from the latest &lt;code&gt;full&lt;/code&gt; backup. They depend
on the parent &lt;code&gt;full&lt;/code&gt; backup to be present in the repository to restore.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;incr&lt;/code&gt; backups are send incrementally from the latest &lt;code&gt;diff&lt;/code&gt; backup. They depend
on the parent &lt;code&gt;diff&lt;/code&gt; backup to restore.&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest detail&lt;/code&gt;
    &lt;p&gt;It shows a list of backups, orphans and all.&lt;/p&gt;
    &lt;p&gt;Sometimes, orphaned backups are left as an artefact of incomplete or cancelled backups. You can clean those by running&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest cleanup --orphans --dry-run=false&lt;/code&gt;
    &lt;p&gt;You can clean up expired backups by running&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest cleanup --expired --dru-run=false&lt;/code&gt;
    &lt;p&gt;To restore the backups, you'll need your age identity file (private key).&lt;/p&gt;
    &lt;code&gt;zfsbackrest restore -i &amp;lt;path-to-age-identity-file&amp;gt; \
  -s &amp;lt;name of the dataset to restore from&amp;gt; \
  -b &amp;lt;optionally, the backup ID to restore from, leave empty to restore the latest&amp;gt; \
  -d &amp;lt;name of the dataset to restore to&amp;gt; # Restoring to a dataset that already exists on your local FS will fail.&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;zfsbackrest&lt;/code&gt; doesn't write or modify actual &lt;code&gt;zfs&lt;/code&gt; datasets. It makes extensive
use of snapshots. List of &lt;code&gt;zfs&lt;/code&gt; operations used by &lt;code&gt;zfsbackrest&lt;/code&gt; are&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;backup&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;zfs snapshot&lt;/code&gt;- Creating a&lt;code&gt;zfs&lt;/code&gt;snapshot for&lt;code&gt;zfsbackrest&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;zfs hold&lt;/code&gt;- Creating a reference to that snapshot to prevent removal&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;zfs send&lt;/code&gt;- Sending the snapshot incrementally&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cleanup&lt;/code&gt;/&lt;code&gt;force-destroy&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;zfs release&lt;/code&gt;- Release the held snapshot&lt;/item&gt;&lt;item&gt;&lt;code&gt;zfs destroy&lt;/code&gt;- Destroy the snapshot&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;restore&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;zfs recv&lt;/code&gt;- Receiving the remote snapshot&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/gargakshit/zfsbackrest"/></entry><entry><id>https://news.ycombinator.com/item?id=45092734</id><title>Show HN: Simple modenized .NET NuGet server reached RC</title><updated>2025-09-01T15:36:04.245015+00:00</updated><content>&lt;doc fingerprint="8d11fb69a8142539"&gt;
  &lt;main&gt;
    &lt;p&gt;Simple modenized NuGet server implementation.&lt;/p&gt;
    &lt;p&gt;A simple NuGet server implementation built on Node.js that provides essential NuGet v3 API endpoints.&lt;/p&gt;
    &lt;p&gt;Compatible with &lt;code&gt;dotnet restore&lt;/code&gt; and standard NuGet clients for package publishing, querying, and manually downloading.&lt;/p&gt;
    &lt;p&gt;A modern browser-based UI is also provided:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can refer to registered packages. You can check various package attributes.&lt;/item&gt;
      &lt;item&gt;You can download packages by version.&lt;/item&gt;
      &lt;item&gt;You can also publish (upload) packages.&lt;/item&gt;
      &lt;item&gt;You can manage user accounts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Browse package list:&lt;/p&gt;
    &lt;p&gt;Publishing packages:&lt;/p&gt;
    &lt;p&gt;User account managements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Easy setup, run NuGet server in 10 seconds!&lt;/item&gt;
      &lt;item&gt;NuGet V3 API compatibility: Support for modern NuGet client operations&lt;/item&gt;
      &lt;item&gt;No need database management: Store package file and nuspecs into filesystem directly, feel free any database managements&lt;/item&gt;
      &lt;item&gt;Package publish: Flexible client to upload &lt;code&gt;.nupkg&lt;/code&gt;files via&lt;code&gt;HTTP POST&lt;/code&gt;using cURL and others&lt;/item&gt;
      &lt;item&gt;Basic authentication: Setup authentication for publish and general access when you want it&lt;/item&gt;
      &lt;item&gt;Reverse proxy support: Configurable trusted reverse proxy handling for proper URL resolution&lt;/item&gt;
      &lt;item&gt;Modern Web UI with enhanced features: &lt;list rend="ul"&gt;&lt;item&gt;Multiple package upload: Drag &amp;amp; drop multiple .nupkg files at once&lt;/item&gt;&lt;item&gt;User account management: Add/delete users, reset passwords (admin only)&lt;/item&gt;&lt;item&gt;API password regeneration: Self-service API password updates&lt;/item&gt;&lt;item&gt;Password change: Users can change their own passwords&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Package importer: Included package importer from existing NuGet server&lt;/item&gt;
      &lt;item&gt;Docker image available&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm install -g nuget-server&lt;/code&gt;
    &lt;p&gt;For using Docker images, refer to a separate chapter.&lt;/p&gt;
    &lt;code&gt;# Start server on default port 5963
nuget-server

# Custom port
nuget-server --port 3000

# Multiple options
nuget-server --port 3000 --config-file config/config.json --users-file config/users.json&lt;/code&gt;
    &lt;p&gt;The NuGet V3 API is served on the &lt;code&gt;/v3&lt;/code&gt; path.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default nuget-server served URL (Show UI): &lt;code&gt;http://localhost:5963&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Actual NuGet V3 API endpoint: &lt;code&gt;http://localhost:5963/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The default URL provided by nuget-server can be changed using the &lt;code&gt;--base-url&lt;/code&gt; option.
This is particularly necessary when public endpoint service using a reverse proxy. For details, refer to below chapter.&lt;/p&gt;
    &lt;p&gt;nuget-server only supports the NuGet V3 API. Therefore, NuGet clients must always access it using the V3 API.&lt;/p&gt;
    &lt;p&gt;If you do not explicitly specify to use the V3 API, some implementations may fall back to the V3 API while others may not, potentially causing unstable behavior. Therefore, you must always specify it. Example below.&lt;/p&gt;
    &lt;p&gt;Add as package source:&lt;/p&gt;
    &lt;p&gt;For HTTP endpoints:&lt;/p&gt;
    &lt;code&gt;dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" --protocol-version 3 --allow-insecure-connections&lt;/code&gt;
    &lt;p&gt;For HTTPS endpoints:&lt;/p&gt;
    &lt;code&gt;dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3&lt;/code&gt;
    &lt;p&gt;Or specify in &lt;code&gt;nuget.config&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;configuration&amp;gt;
  &amp;lt;packageSources&amp;gt;
    &amp;lt;add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" /&amp;gt;
  &amp;lt;/packageSources&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;
    &lt;p&gt;Upload packages by &lt;code&gt;HTTP POST&lt;/code&gt; method, using cURL or any HTTP client with &lt;code&gt;/api/publish&lt;/code&gt; endpoint:&lt;/p&gt;
    &lt;code&gt;# Upload "MyPackage.1.0.0.nupkg" file
curl -X POST http://localhost:5963/api/publish \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"&lt;/code&gt;
    &lt;p&gt;You may be dissatisfied with publishing using this method. The dotnet command includes &lt;code&gt;dotnet nuget push&lt;/code&gt;, which is the standard approach.
However, in my experience, this protocol uses &lt;code&gt;multipart/form-data&lt;/code&gt; for transmission, which has caused issues with gateway services, reverse proxies, load balancers, and similar components.
Therefore, the current nuget-server does not implement this method and instead uses the simplest binary transmission procedure.&lt;/p&gt;
    &lt;p&gt;Another advantage is that when authentication is enabled, you don't need to manage Basic authentication and V3 API keys separately. You might still feel issue with managing read operations and publish operation with the same key, but in that case, you can simply separate the users.&lt;/p&gt;
    &lt;p&gt;For authentication feature, please refer to below chapter.&lt;/p&gt;
    &lt;p&gt;By default, packages are stored in the &lt;code&gt;./packages&lt;/code&gt; directory relative to where you run nuget-server.
You can customize this location using the &lt;code&gt;--package-dir&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use default ./packages directory
nuget-server

# Use custom directory (relative or absolute path)
nuget-server --package-dir /another/package/location&lt;/code&gt;
    &lt;p&gt;Packages are stored in the filesystem using the following structure:&lt;/p&gt;
    &lt;code&gt;packages/
├── PackageName/
│   ├── 1.0.0/
│   │   ├── PackageName.1.0.0.nupkg
│   │   ├── PackageName.nuspec
│   │   └── icon.png            # Package icon (if present)
│   └── 2.0.0/
│       ├── PackageName.2.0.0.nupkg
│       ├── PackageName.nuspec
│       └── icon.jpg            # Package icon (if present)
└── AnotherPackage/
    └── 1.5.0/
        ├── AnotherPackage.1.5.0.nupkg
        ├── AnotherPackage.nuspec
        └── icon.png            # Package icon (if present)
&lt;/code&gt;
    &lt;p&gt;You can backup the package directory using simply &lt;code&gt;tar&lt;/code&gt; or other achiver:&lt;/p&gt;
    &lt;code&gt;cd /your/server/base/dir
tar -cf - ./packages | lz4 &amp;gt; backup-packages.tar.lz4&lt;/code&gt;
    &lt;p&gt;Restore is simply extract it and re-run nuget-server with the same package directory configuration, because nuget-server does not use any specialized storage such as databases.&lt;/p&gt;
    &lt;p&gt;nuget-server supports configuration through command-line options, environment variables, and JSON file.&lt;/p&gt;
    &lt;p&gt;Settings are applied in the following order (highest to lowest priority):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Command-line options&lt;/item&gt;
      &lt;item&gt;Environment variables&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;config.json&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;Default values&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can specify a custom configuration file:&lt;/p&gt;
    &lt;code&gt;# Using command line option
nuget-server --config-file /path/to/config.json
# or short alias
nuget-server -c /path/to/config.json

# Using environment variable
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
nuget-server&lt;/code&gt;
    &lt;p&gt;If not specified, nuget-server looks for &lt;code&gt;./config.json&lt;/code&gt; in the current directory.&lt;/p&gt;
    &lt;p&gt;Create a &lt;code&gt;config.json&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "packageDir": "./packages",
  "usersFile": "./users.json",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "trustedProxies": ["127.0.0.1", "::1"],
  "authMode": "none",
  "sessionSecret": "&amp;lt;your-secret-here&amp;gt;",
  "passwordMinScore": 2,
  "passwordStrengthCheck": true
}&lt;/code&gt;
    &lt;p&gt;All fields are optional. Only include the settings you want to override. Both &lt;code&gt;packageDir&lt;/code&gt; and &lt;code&gt;usersFile&lt;/code&gt; paths can be absolute or relative. If relative, they are resolved from the directory containing the &lt;code&gt;config.json&lt;/code&gt; file.&lt;/p&gt;
    &lt;p&gt;nuget-server also supports authentication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Authentication Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
        &lt;cell role="head"&gt;Auth Initialization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;none&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Default. No authentication required&lt;/cell&gt;
        &lt;cell&gt;Not required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;publish&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Authentication required only for package publishing&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;full&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Authentication required for all operations (must login first)&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To enable authentication on the NuGet server, first register an initial user using the &lt;code&gt;--auth-init&lt;/code&gt; option.&lt;/p&gt;
    &lt;p&gt;Create an initial admin user interactively:&lt;/p&gt;
    &lt;code&gt;nuget-server --auth-init&lt;/code&gt;
    &lt;p&gt;This command will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prompt for admin username (default: &lt;code&gt;admin&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Prompt for password (with strength checking, masked input)&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;users.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Exit after initialization (server does not start)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When enabling authentication using a Docker image, use this option to generate the initial user.&lt;/p&gt;
    &lt;code&gt;Initializing authentication...
Enter admin username [admin]:
Enter password: ********
Confirm password: ********

============================================================
Admin user created successfully!
============================================================
Username: admin
Password: *********************
============================================================
&lt;/code&gt;
    &lt;p&gt;Users added with &lt;code&gt;--auth-init&lt;/code&gt; automatically become administrator users.
Administrator users can add or remove other users via the UI. They can also reset user passwords.&lt;/p&gt;
    &lt;p&gt;While administrator users can also be assigned API passwords (described later), we recommend separating users for management whenever possible.&lt;/p&gt;
    &lt;p&gt;The NuGet server distinguishes between the password used to log in to the UI and the password used by NuGet clients when accessing the server. The password used by NuGet clients when accessing the server is called the "API password," and access is granted using the combination of the user and the API password.&lt;/p&gt;
    &lt;p&gt;Please log in by displaying the UI in the browser. Select the “API password” menu from the UI menu to generate an API password. Using this API password will enable access from the NuGet client.&lt;/p&gt;
    &lt;p&gt;Here is an example of using the API password:&lt;/p&gt;
    &lt;code&gt;# Add source with API password
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" \
  -u admin \
  -p xxxxxxxxxxxxxxxxxxxxxx \
  --protocol-version 3 --store-password-in-clear-text --allow-insecure-connections&lt;/code&gt;
    &lt;p&gt;Or specify &lt;code&gt;nuget.config&lt;/code&gt; with credentials:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;configuration&amp;gt;
  &amp;lt;packageSources&amp;gt;
    &amp;lt;add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" /&amp;gt;
  &amp;lt;/packageSources&amp;gt;
  &amp;lt;packageSourceCredentials&amp;gt;
    &amp;lt;local&amp;gt;
      &amp;lt;add key="Username" value="reader" /&amp;gt;
      &amp;lt;add key="ClearTextPassword" value="xxxxxxxxxxxxxxxxxxxxxx" /&amp;gt;
    &amp;lt;/local&amp;gt;
  &amp;lt;/packageSourceCredentials&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;
    &lt;p&gt;For package publishing:&lt;/p&gt;
    &lt;code&gt;# Publish packages with API password
curl -X POST http://localhost:5963/api/publish \
  -u admin:xxxxxxxxxxxxxxxxxxxxxx \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"&lt;/code&gt;
    &lt;p&gt;When publishing a package, you can send the package by setting Basic authentication in the &lt;code&gt;Authorization&lt;/code&gt; header.&lt;/p&gt;
    &lt;p&gt;nuget-server uses the &lt;code&gt;zxcvbn&lt;/code&gt; library to enforce strong password requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Evaluates password strength on a scale of 0-4 (Weak to Very Strong)&lt;/item&gt;
      &lt;item&gt;Default minimum score: 2 (Good)&lt;/item&gt;
      &lt;item&gt;Checks against common passwords, dictionary words, and patterns&lt;/item&gt;
      &lt;item&gt;Provides real-time feedback during password creation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Configure password requirements in &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "passwordMinScore": 2, // 0-4, default: 2 (Good)
  "passwordStrengthCheck": true // default: true
}&lt;/code&gt;
    &lt;p&gt;The NuGet server stores both "password" and "API password" as SALT hashed information, so no plaintext passwords are ever saved. However, if you do not use HTTPS (TLS), be aware that the &lt;code&gt;Authorization&lt;/code&gt; header will contain the plaintext password, making it vulnerable to sniffing.
When makes public endpoint, protect communications using HTTPS.&lt;/p&gt;
    &lt;p&gt;Import all packages from another NuGet server to your local nuget-server instance. This feature can be used when migrating the foreign NuGet server to nuget-server.&lt;/p&gt;
    &lt;p&gt;Import packages interactively in CLI:&lt;/p&gt;
    &lt;code&gt;nuget-server --import-packages --package-dir ./packages&lt;/code&gt;
    &lt;p&gt;This command will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prompt for source NuGet server URL&lt;/item&gt;
      &lt;item&gt;Ask if authentication is required&lt;/item&gt;
      &lt;item&gt;If needed, prompt for username and password (masked input)&lt;/item&gt;
      &lt;item&gt;Discover all packages from the source server&lt;/item&gt;
      &lt;item&gt;Download and import all packages to local storage&lt;/item&gt;
      &lt;item&gt;Display progress for each package (1% intervals)&lt;/item&gt;
      &lt;item&gt;Exit after import (server does not start)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Existing packages with the same version will be overwritten&lt;/item&gt;
      &lt;item&gt;Failed imports are logged with error details&lt;/item&gt;
      &lt;item&gt;Progress is reported at 1% intervals to reduce log noise&lt;/item&gt;
      &lt;item&gt;Package icons are preserved during import&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Parallel downloads are not done. This is to avoid making a large number of requests to the repository.&lt;/p&gt;
    &lt;p&gt;This feature is a type of downloader. Therefore, it does not need to be run on the actual host where it will operate. You can perform the import process in advance on a separate host and then move the &lt;code&gt;packages&lt;/code&gt; directory as-is.&lt;/p&gt;
    &lt;code&gt;Starting package import...
Enter source NuGet server URL [http://host.example.com/repository/nuget/]: https://nexus.example.com/repository/nuget/
Does the server require authentication? [y/N]: y
Enter username: reader
Enter password: **********

============================================================
Import Configuration:
Source: https://nexus.example.com/repository/nuget/
Target: ./packages
Authentication: reader (password hidden)
============================================================

Start importing packages? (existing packages will be overwritten) [y/N]: y

Discovering packages from source server...
Found 125 packages with 563 versions total.
Starting package import...
Progress: 100/563 packages (17%) - MyPackage.Core@1.2.3
Progress: 563/563 packages (100%) - AnotherPackage@2.0.0

============================================================
Import Complete!
============================================================
Total packages: 125
Total versions: 563
Successfully imported: 563
Failed: 0
Time elapsed: 125.3 seconds
============================================================
&lt;/code&gt;
    &lt;p&gt;The server supports running behind a reverse proxy. For example, when you have a public URL like &lt;code&gt;https://nuget.example.com&lt;/code&gt; and run nuget-server on a host within your internal network via a gateway.&lt;/p&gt;
    &lt;p&gt;In such cases, you MUST specify the base URL of the public URL to ensure the NuGet V3 API can provide the correct sub-endpoint address.&lt;/p&gt;
    &lt;p&gt;The server resolves URLs using the following priority order:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fixed base URL (highest priority): When &lt;code&gt;--base-url&lt;/code&gt;option is specified, it always takes precedence&lt;/item&gt;
      &lt;item&gt;Trusted proxy headers: When trusted proxies are configured with &lt;code&gt;--trusted-proxies&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;HTTP &lt;code&gt;Forwarded&lt;/code&gt;header (proto, host, port)&lt;/item&gt;&lt;item&gt;Traditional &lt;code&gt;X-Forwarded-*&lt;/code&gt;headers (&lt;code&gt;X-Forwarded-Proto&lt;/code&gt;,&lt;code&gt;X-Forwarded-Host&lt;/code&gt;,&lt;code&gt;X-Forwarded-Port&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;HTTP &lt;/item&gt;
      &lt;item&gt;Standard request information (fallback): Uses &lt;code&gt;Host&lt;/code&gt;header when proxy headers are not available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example &lt;code&gt;--base-url&lt;/code&gt; option:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;nuget-server served public base URL: &lt;code&gt;https://packages.example.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Actual NuGet V3 API endpoint: &lt;code&gt;https://packages.example.com/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Configure served base URL (do not include /v3 path)
nuget-server --base-url https://packages.example.com

# Add as NuGet source (HTTPS - no --allow-insecure-connections needed)
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3&lt;/code&gt;
    &lt;p&gt;Another option, you can configure with trusted proxy addresses:&lt;/p&gt;
    &lt;code&gt;# Configure trusted proxies for proper host header handling
nuget-server --trusted-proxies "10.0.0.1,192.168.1.100"&lt;/code&gt;
    &lt;p&gt;Environment variables are also supported:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_BASE_URL=https://packages.example.com
export NUGET_SERVER_TRUSTED_PROXIES=10.0.0.1,192.168.1.100
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
export NUGET_SERVER_USERS_FILE=/path/to/users.json
export NUGET_SERVER_SESSION_SECRET=your-secret-key-here&lt;/code&gt;
    &lt;p&gt;Docker images are available for multiple architectures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;linux/amd64&lt;/code&gt;(x86_64)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;linux/arm64&lt;/code&gt;(aarch64)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When pulling the image, Docker automatically selects the appropriate architecture for your platform.&lt;/p&gt;
    &lt;p&gt;Suppose you have configured the following directory structure for persistence (recommended):&lt;/p&gt;
    &lt;code&gt;docker-instance/
├── data/
│   ├── config.json
│   └── user.json
└── packages/
    └── (package files)
&lt;/code&gt;
    &lt;p&gt;Execute as follows:&lt;/p&gt;
    &lt;code&gt;# Pull and run the latest version
docker run -d -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# Or with Docker Compose
cat &amp;gt; docker-compose.yml &amp;lt;&amp;lt; EOF
version: '3'
services:
  nuget-server:
    image: kekyo/nuget-server:latest
    ports:
      - "5963:5963"
    volumes:
      - ./data:/data
      - ./packages:/packages
    environment:
      - NUGET_SERVER_AUTH_MODE=publish
EOF

docker-compose up -d&lt;/code&gt;
    &lt;p&gt;Your NuGet server is now available at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Web UI: &lt;code&gt;http://localhost:5963&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NuGet V3 API: &lt;code&gt;http://localhost:5963/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Docker container runs as the &lt;code&gt;nugetserver&lt;/code&gt; user (UID 1001) for security reasons. You need to ensure that the mounted directories have the appropriate permissions for this user to write files.&lt;/p&gt;
    &lt;p&gt;Set proper permissions for mounted directories:&lt;/p&gt;
    &lt;code&gt;# Create directories if they don't exist
mkdir -p ./data ./packages

# Set ownership to UID 1001 (matches the container's nugetserver user)
sudo chown -R 1001:1001 ./data ./packages&lt;/code&gt;
    &lt;p&gt;Important: Without proper permissions, you may encounter &lt;code&gt;500 Permission Denied&lt;/code&gt; errors when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating or updating user accounts&lt;/item&gt;
      &lt;item&gt;Publishing packages&lt;/item&gt;
      &lt;item&gt;Writing configuration files&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run with default settings (port 5963, packages and data stored in mounted volumes)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# With authentication (users.json will be created in /data)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  -e NUGET_SERVER_AUTH_MODE=publish \
  kekyo/nuget-server:latest&lt;/code&gt;
    &lt;p&gt;You can also change settings using environment variables or command-line options, but the easiest way to configure settings is to use &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since the Docker image has mount points configured, you can mount &lt;code&gt;/data&lt;/code&gt; and &lt;code&gt;/packages&lt;/code&gt; as shown in the example above and place &lt;code&gt;/data/config.json&lt;/code&gt; there to flexibly configure settings. Below is an example of &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "authMode": "publish"
}&lt;/code&gt;
    &lt;p&gt;When initializing credentials or importing packages, configure &lt;code&gt;config.json&lt;/code&gt; and perform the operation via the CLI before launching the Docker image:&lt;/p&gt;
    &lt;code&gt;# Initialize authentication
nuget-server -c ./data/config.json --auth-init&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/data&lt;/code&gt;: Default data directory for&lt;code&gt;config.json&lt;/code&gt;,&lt;code&gt;users.json&lt;/code&gt;and other persistent data&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/packages&lt;/code&gt;: Default package storage directory (mounted to persist packages)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Default behavior: The Docker image runs with &lt;code&gt;--users-file /data/users.json --package-dir /packages&lt;/code&gt; by default.&lt;/p&gt;
    &lt;p&gt;Configuration priority (highest to lowest):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Custom command line arguments (when overriding CMD)&lt;/item&gt;
      &lt;item&gt;Environment variables (e.g., &lt;code&gt;NUGET_SERVER_PACKAGE_DIR&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;config.json&lt;/code&gt;file (if explicitly specified)&lt;/item&gt;
      &lt;item&gt;Default command line arguments in Dockerfile&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Various methods exist for automatically starting containers with systemd. Below is a simple example of configuring a systemd service using Podman. This is a simple service unit file used before quadlets were introduced to Podman. By placing this file and having systemd recognize it, you can automatically start the nuget-server:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;/etc/systemd/system/container-nuget-server.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# container-nuget-server.service

[Unit]
Description=Podman container-nuget-server.service
Documentation=man:podman-generate-systemd(1)
Wants=network-online.target
After=network-online.target
RequiresMountsFor=%t/containers

[Service]
Environment=PODMAN_SYSTEMD_UNIT=%n
Restart=always
RestartSec=30
TimeoutStopSec=70
ExecStart=/usr/bin/podman run \
        --cidfile=%t/%n.ctr-id \
        --cgroups=no-conmon \
        --rm \
        --sdnotify=conmon \
        --replace \
        -d \
        -p 5963:5963 \
        --name nuget_server \
        -v /export/data:/data -v /export/packages:/packages docker.io/kekyo/nuget-server:latest
ExecStop=/usr/bin/podman stop \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
ExecStopPost=/usr/bin/podman rm \
        -f \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
Type=notify
NotifyAccess=all

[Install]
WantedBy=default.target&lt;/code&gt;
    &lt;p&gt;The build of the nuget-server Docker image uses Podman.&lt;/p&gt;
    &lt;p&gt;Use the provided multi-platform build script that uses Podman to build for all supported architectures:&lt;/p&gt;
    &lt;code&gt;# Build for all platforms (local only, no push)
./build-docker-multiplatform.sh

# Build and push to Docker Hub
./build-docker-multiplatform.sh --push

# Build for specific platforms only
./build-docker-multiplatform.sh --platforms linux/amd64,linux/arm64

# Push with custom Docker Hub username
OCI_SERVER_USER=yourusername ./build-docker-multiplatform.sh --push

# Inspect existing manifest
./build-docker-multiplatform.sh --inspect&lt;/code&gt;
    &lt;p&gt;Important: For cross-platform builds, QEMU emulation must be configured first:&lt;/p&gt;
    &lt;code&gt;# Option 1: Use QEMU container (recommended)
sudo podman run --rm --privileged docker.io/multiarch/qemu-user-static --reset -p yes

# Option 2: Install system packages
# Ubuntu/Debian:
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y qemu-user-static
# Fedora/RHEL:
sudo dnf install -y qemu-user-static

# Verify QEMU is working:
podman run --rm --platform linux/arm64 alpine:latest uname -m
# Should output: aarch64&lt;/code&gt;
    &lt;p&gt;Without QEMU, you can only build for your native architecture.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;--auth-init&lt;/code&gt; and &lt;code&gt;--import-packages&lt;/code&gt; options require interactive responses from the operator.
Therefore, attempting to automate these may not work properly.
In such cases, you can provide credentials via environment variables:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_ADMIN_USERNAME=admin
export NUGET_SERVER_ADMIN_PASSWORD=MySecurePassword123!
nuget-server --auth-init --config-file ./config.json&lt;/code&gt;
    &lt;p&gt;This allows initialization in CI/CD pipelines without user interaction.&lt;/p&gt;
    &lt;p&gt;For special configurations (or to support persistent sessions), you can set a fixed session secret. Specify a sufficiently long value for the secret:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_SESSION_SECRET=$(openssl rand -base64 32)
nuget-server&lt;/code&gt;
    &lt;p&gt;(Or use &lt;code&gt;config.json&lt;/code&gt;.)&lt;/p&gt;
    &lt;p&gt;If not set, a random secret is generated (warning will be logged).&lt;/p&gt;
    &lt;p&gt;The server implements a subset of the NuGet V3 API protocol:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Service index: &lt;code&gt;/v3/index.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Package content: &lt;code&gt;/v3/package/{id}/index.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Package downloads: &lt;code&gt;/v3/package/{id}/{version}/{filename}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Registration index: &lt;code&gt;/v3/registrations/{id}/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Under MIT.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/kekyo/nuget-server"/></entry><entry><id>https://news.ycombinator.com/item?id=45092895</id><title>Git for Music – Using Version Control for Music Production (2023)</title><updated>2025-09-01T15:36:03.509800+00:00</updated><content>&lt;doc fingerprint="1774f05c4fe586da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;git for music. Using version control for music production.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;last updated on 6 Apr 2024&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Being both a musician and a software engineer, I always felt that these two areas are almost completely separated. My developer skill-set seemed to have little to no use for my work as a musician. Which is a pity considering how cool it would be if there was some kind of a sinergy across these two sides of my life.&lt;/p&gt;
    &lt;p&gt;Recently, though, I have found a useful possibility to utilize something I previously used solely for my development work, namely, git, the version control tool, for my music production.&lt;/p&gt;
    &lt;p&gt;Okay, and now let’s get to the point and…&lt;/p&gt;
    &lt;head rend="h2"&gt;meet Git for music production&lt;/head&gt;
    &lt;p&gt;Did you notice yourself creating a dozen of versions of your project? Are the names like this familiar to you?&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;my-cool-song-new-vocals-brighter-mix-4.rpp&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Did you ever feel frustrated about unmanageability of all this and how sloppy you project directory ends up looking?&lt;/p&gt;
    &lt;p&gt;This version nightmare problem for software people has a solid and well-recognized solution: version control systems. Such as “git”, which is not only the most widely used one in the industry, but also completely free, open source and cross platform (that is working flawlessly on Win/Mac/Linux).&lt;/p&gt;
    &lt;p&gt;For music production, I use Reaper, and instead of creating dozens of copies of my project file (&lt;code&gt;my-cool-song.rpp&lt;/code&gt;), such as &lt;code&gt;my-cool-song-new-vocals-brighter-mix-4.rpp&lt;/code&gt;, I simply initialize a git repository in the project folder and put the file under version control. This git repository will be the “home” for managing the version of our music project.&lt;/p&gt;
    &lt;p&gt;By the way, a good supplementary for this reading could be this video of me going through an example. If you are not fan of watching videos, feel free to read on.&lt;/p&gt;
    &lt;head rend="h2"&gt;My git-based music production workflow&lt;/head&gt;
    &lt;p&gt;Although, when wearing a developer hat, I am normally in linux, for the music production stuff, due to the better availability of plugins and such, Windows is a better option. For Windows, you can install &lt;code&gt;git-bash&lt;/code&gt;, and have all the git functionality at your fingertips through a command-line interface.&lt;/p&gt;
    &lt;p&gt;First, I initialize a repository in the project directory. For me, it is most convenient to use a git bash command line terminal:&lt;/p&gt;
    &lt;code&gt;Acer@DESKTOP-NRN84IB MINGW64 /c/home/music
$ cd test_git_project/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project
$ git init .
Initialized empty Git repository in C:/home/music/test_git_project/.git/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project (master)
$
&lt;/code&gt;
    &lt;p&gt;in the example above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I first navigated to the directory with my project with &lt;code&gt;cd&lt;/code&gt;command&lt;/item&gt;
      &lt;item&gt;initialized a repository with &lt;code&gt;git init .&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;on the last, third line, my command prompt starts having a little &lt;code&gt;(master)&lt;/code&gt;thing, which is the default “branch” in my repository that Git has created for me&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I also create a &lt;code&gt;.gitignore&lt;/code&gt; file and that this is this particular project file that I want to track, and not any other, such as media or peak files:&lt;/p&gt;
    &lt;code&gt;*

!in_your_eyes_remix_git_managed.rpp
&lt;/code&gt;
    &lt;p&gt;Then I am free to work with the project in my DAW as usual. When I am done working on a specific version, I make a commit and give it a descriptive name, e.g. “bass vst settings adjusted”.&lt;/p&gt;
    &lt;p&gt;Then I can see all the versions of my project in &lt;code&gt;git gui&lt;/code&gt; tool.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;side note: you can use any git frontend, not only &lt;code&gt;git gui&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not only that, but I can also open any historical version of the project, create branches and so on. In other words, I can fully benefit from the version control system! If you are already using git, you know what I mean.&lt;/p&gt;
    &lt;p&gt;The days of versioned files mess in my project folder are finally gone! I wonder, though, if Reaper developers will be willing to incorporate that into their product one day.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managing other files (WAVs etc.)&lt;/head&gt;
    &lt;p&gt;Git is not super suited for managing big binary files (such as WAV samples and stems), but this is not a problem for me since I only manage the main project file.&lt;/p&gt;
    &lt;p&gt;About other files I do not care. Why? Becase I never remove them. The media files are either WAVs related to this project (and which are therefore kept in the project folder) or samples from my library. In both cases, these files are normally (at least withing the lifespan of the project under construction) never deleted.&lt;/p&gt;
    &lt;p&gt;This approach, which, I guess, I share with most producers, makes it easy to return to any historical version of the project and rely on the media files to be found.&lt;/p&gt;
    &lt;head rend="h3"&gt;collaborating with GIT? Not sure…&lt;/head&gt;
    &lt;p&gt;GIT is not only about versioning, but also about collaboration, with remote repositories and so on. Frankly, I don’t see it feasible for collaboration over music projects since the project files are normally opaque and we should not expect git or any other version control system to be able to merge/diff them.&lt;/p&gt;
    &lt;p&gt;And let’s not forget that to be able to work on your project, the collaborator needs to have very close set up: the DAW, the plugins and all the media files.&lt;/p&gt;
    &lt;p&gt;Another note of the remote repositories: I do find it useful that I can push my music project to github and this kind of a backup that will outlive my current PC. This is nice, but we can’t really consider it a real backup - because of missing media.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tracking TODO items for your music project in GitHub&lt;/head&gt;
    &lt;p&gt;Interesting use-case I’m currently testing is to have a “todo list”, think of an small per-project issue tracker with a list of things you plan to do later. Just a version-tracked text file of the format similar to this:&lt;/p&gt;
    &lt;code&gt;fix panning issues in chorus TODO
add one more synth layer TODO
&lt;/code&gt;
    &lt;p&gt;Once it’s in Github, you can update it from anywhere (GitHub allows you to edit files right in the browser), so, basically, you project gets its own, private, read/write website. On the go and got a cool idea? Now you know where to record it (don’t forget to &lt;code&gt;pull&lt;/code&gt; your update, though, once you are back to your DAW PC).&lt;/p&gt;
    &lt;p&gt;In conclusion, when we inspect this idea of “git for music” a bit closer, we can see that it does have a few viable applications. Yes, this tool is not magical, but still pretty useful!&lt;/p&gt;
    &lt;p&gt;Thanks for reading.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://grechin.org/2023/05/06/git-and-reaper.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45092925</id><title>"Turns out Google made up an elaborate story about me"</title><updated>2025-09-01T15:36:03.136286+00:00</updated><content>&lt;doc fingerprint="d0eece56e5def5e6"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. &lt;/p&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Post&lt;/head&gt;
      &lt;p&gt;Benn Jordan&lt;/p&gt;
      &lt;p&gt;bennjordan.bsky.social&lt;/p&gt;
      &lt;p&gt;did:plc:yhpu2wsyvdbsehluhclpqyyk&lt;/p&gt;
      &lt;p&gt;This is SO messed up. 🫥 I had a few messages and tags today asking me to clarify my stance on Israel, which was odd as I've been pretty outspoken against genocide and in full support of Palestinian statehood...&lt;/p&gt;
      &lt;p&gt;2025-08-31T07:09:14.503Z&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z"/></entry><entry><id>https://news.ycombinator.com/item?id=45093090</id><title>Cloudflare Radar: AI Insights</title><updated>2025-09-01T15:36:03.080420+00:00</updated><content/><link href="https://radar.cloudflare.com/ai-insights"/></entry></feed>