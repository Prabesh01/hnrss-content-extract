<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-14T00:57:13.206768+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46251620</id><title>A Lisp Interpreter Implemented in Conway's Game of Life (2021)</title><updated>2025-12-14T00:57:21.579204+00:00</updated><content>&lt;doc fingerprint="2b0165598cf3aac0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Lisp Interpreter Implemented in Conway's Game of Life&lt;/head&gt;
    &lt;p&gt;Lisp in Life is a Lisp interpreter implemented in Conway’s Game of Life.&lt;/p&gt;
    &lt;p&gt;The entire pattern is viewable on the browser here.&lt;/p&gt;
    &lt;p&gt;To the best of my knowledge, this is the first time a high-level programming language was interpreted in Conway’s Game of Life.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running Lisp on the Game of Life&lt;/head&gt;
    &lt;p&gt;Lisp is a language with a simple and elegant design, having an extensive ability to express sophisticated ideas as simple programs. Notably, the powerful feature of macros could be used to modify the language’s syntax to write programs in a highly flexible way. For example, macros can be used to introduce new programming paradigms to the language, as demonstrated in object-oriented-like.lisp (which can actually be evaluated by the interpreter, although complex programs take quite a long time to finish running), where a structure and syntax similar to classes in Object Oriented Programming is constructed. Despite the expressibility of Lisp, it is the world’s second oldest high-level programming language introduced in 1958, only to be preceded by Fortran.&lt;/p&gt;
    &lt;p&gt;Conway’s Game of Life is a cellular automaton proposed in 1970. Despite it having a very simple set of rules, it is known to be Turing Complete. Lisp in Life demonstrates this fact in a rather straightforward way.&lt;/p&gt;
    &lt;p&gt;How can simple systems allow human thoughts to be articulated and be expanded? With the expressibility of Lisp and the basis of Conway’s Game of Life, Lisp in Life provides an answer to this question.&lt;/p&gt;
    &lt;head rend="h3"&gt;Input and Output&lt;/head&gt;
    &lt;p&gt;The Lisp program is provided by editing certain cells within the pattern to represent the ASCII-encoding of the Lisp program. The pattern directly reads this text and evaluates the results. You can also load your own Lisp program into the pattern and run it. The standard output is written at the bottom end of the RAM module, which can be easily located and directly examined in a Game of Life viewer. The Lisp implementation supports lexical closures and macros, allowing one to write Lisp programs in a Lisp-like taste, as far as the memory limit allows you to.&lt;/p&gt;
    &lt;p&gt;The Lisp interpreter is written in C. Using the build system for this project, you can also compile your own C11-compatible C code and run in on Conway’s Game of Life.&lt;/p&gt;
    &lt;head rend="h3"&gt;Previous Work&lt;/head&gt;
    &lt;p&gt;As previously mentioned, to the best of my knowledge, this is the first time a high-level programming language was interpreted in Conway’s Game of Life.&lt;/p&gt;
    &lt;p&gt;The entry featuring Universal Computers in LifeWiki has a list of computers created in the Game of Life. Two important instances not mentioned in this entry are the Quest For Tetris (QFT) Project created by the authors of the QFT project, and APGsembly created by Adam P. Goucher. All of these work are designed to run an assembly language and are not designed to interpret a high-level language per se.&lt;/p&gt;
    &lt;p&gt;An example of a compiled high-level language targeted for the Game of Life is Cogol by the QFT project. Cogol is compiled to the assembly language QFTASM, targeted for the QFT architecture. Although Cogol is targeted for the QFT architecture, it requires compilation to QFTASM for the code to be run in the QFT architecture.&lt;/p&gt;
    &lt;p&gt;In Lisp in Life, a modified version of the QFT architecture is first created for improving the pattern’s runtime. Modifications include introducing a new cascaded storage architecture for the ROM, new opcodes, extending the ROM and RAM address space, etc. The Lisp source code is then written into the computer’s RAM module as its raw binary ASCII format. The Conway’s Game of Life pattern directly reads, parses, and evaluates this Lisp source code to produce its output. This feature of allowing a Conway’s Game of Life pattern to evaluate a high-level programming language expressed as a string of text is a novel feature that was newly achieved in this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;Here is a YouTube video showing Lisp in Life in action:&lt;/p&gt;
    &lt;head rend="h2"&gt;Screenshots&lt;/head&gt;
    &lt;p&gt;An overview of the entire architecture.&lt;/p&gt;
    &lt;p&gt;An overview of the CPU and its surrounding modules. On the top are the ROM modules, with the lookup module on the right, and the value modules on the left. On the bottom left is the CPU. On the bottom right is the RAM module.&lt;/p&gt;
    &lt;p&gt;This pattern is the VarLife version of the architecture. VarLife is an 8-state cellular automaton defined in the Quest For Tetris (QFT) Project, which is used as an intermediate layer to create the final Conway’s Game of Life pattern. The colors of the cells indicate the 8 distinct states of the VarLife rule.&lt;/p&gt;
    &lt;p&gt;The architecture is based on Tetris8.mc in the original QFT repository. Various modifications were made to make the pattern compact, such as introducing a new lookup table architecture for the ROM, removing and adding new opcodes, expanding the ROM and RAM address space, etc.&lt;/p&gt;
    &lt;p&gt;The Conway’s Game of Life version of the architecture, converted from the VarLife pattern. What appears to be a single cell in this image is actually an OTCA metapixel zoomed away to be shown 2048 times smaller.&lt;/p&gt;
    &lt;p&gt;A close-up view of a part of the ROM module in the Conway’s Game of Life version. Each pixel in the previous image is actually this square-shaped structure shown in this image. These structures are OTCA metapixels, which can be seen to be in the On and Off meta-states in this image. The OTCA Metapixel is a special Conway’s Game of Life pattern that can emulate cellular automatons with customized rules. The original VarLife pattern is simulated this way so that it can run in Conway’s Game of Life.&lt;/p&gt;
    &lt;p&gt;The OTCA Metapixel simulating Life in Life can be seen in this wonderful video by Phillip Bradbury: https://www.youtube.com/watch?v=xP5-iIeKXE8&lt;/p&gt;
    &lt;p&gt;A video of the RAM module in the VarLife rule in action.&lt;/p&gt;
    &lt;p&gt;The computer showing the results of the following Lisp program:&lt;/p&gt;
    &lt;code&gt;(define mult (lambda (m n)
  (* m n)))

(print (mult 3 14))
&lt;/code&gt;
    &lt;p&gt;The result is &lt;code&gt;42&lt;/code&gt;, shown in binary ascii format (&lt;code&gt;0b110100&lt;/code&gt;, &lt;code&gt;0b110010&lt;/code&gt;), read in bottom-to-up order.&lt;/p&gt;
    &lt;p&gt;As shown in this image, the standard output of the Lisp program gets written at the bottom end of the RAM module, and can be directly viewed in a Game of Life viewer. This repository also contains scripts that run on Golly to decode and view the contents of the output as strings.&lt;/p&gt;
    &lt;head rend="h2"&gt;How is it Done?&lt;/head&gt;
    &lt;p&gt;The Lisp interpreter, written in C, is compiled to an assembly language for a CPU architecture implemented in the Game of Life, which is a modification of the computer used in the Quest For Tetris (QFT) project. The compilation is done using an extended version of ELVM (the Esoteric Language Virtual Machine). The Game of Life backend for ELVM was implemented by myself.&lt;/p&gt;
    &lt;p&gt;Generating a small enough pattern that runs in a reasonable amount of time required a lot of effort. This required optimizations and improvements in every layer of the project; a brief summary would be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The C Compiler layer - adding the computed goto feature to the C compiler, preserving variable symbols to be used after compilation, etc.&lt;/item&gt;
      &lt;item&gt;The C layer (the Lisp interpreter) - using a string hashtable and binary search for Lisp symbol lookup, minimization of stack region usage with union memory structures, careful memory region map design, etc.&lt;/item&gt;
      &lt;item&gt;The QFTASM layer - writing a compiler optimizer to optimize the length of the assembly code&lt;/item&gt;
      &lt;item&gt;The VarLife layer (the CPU architecture) - creating a lookup table architecture for faster ROM access, expanding the size and length of the RAM module, adding new opcodes, etc.&lt;/item&gt;
      &lt;item&gt;The Game of Life layer - Hashlife-specific optimization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A more detailed description of the optimizations done in this project is available in the Implementation Details section.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conversion from VarLife to Conway’s Game of Life&lt;/head&gt;
    &lt;p&gt;VarLife is an 8-state cellular automaton defined in the Quest For Tetris (QFT) Project. It is used as an intermediate layer to generate the final Conway’s Game of Life pattern; the computer is first created in VarLife, and then converted to a Game of Life pattern.&lt;/p&gt;
    &lt;p&gt;When converting VarLife to Conway’s Game of Life, each VarLife cell is mapped to an OTCA Metapixel (OTCAMP). The conversion from VarLife to the Game of Life is done in a way so that the behavior of the states of the VarLife pattern matches exactly with the meta-states of the OTCA Metapixels in the converted Game of Life pattern. Therefore, it is enough to verify the behavior of the VarLife pattern to verify the behavior of the Game of Life pattern.&lt;/p&gt;
    &lt;p&gt;Due to the use of OTCA Metapixels, each VarLife cell becomes extended to a 2048x2048 Game of Life cell, and 1 VarLife generation requires 35328 Game of Life generations. Therefore, the VarLife patterns run significantly faster than the Game of Life (GoL) version.&lt;/p&gt;
    &lt;p&gt;Additional details on VarLife are available in the Miscellaneous section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern Files&lt;/head&gt;
    &lt;p&gt;Pattern files preloaded with various Lisp programs are available here. Detailed statistics such as the running time and the memory consumption are available in the Running Times and Statistics section.&lt;/p&gt;
    &lt;p&gt;The patterns can be simulated on the Game of Life simulator Golly.&lt;/p&gt;
    &lt;p&gt;The VarLife patterns can be simulated on Golly as well. To run the VarLife patterns, open Golly and see File -&amp;gt; Preferences -&amp;gt; Control, and Check the “Your Rules” directory. Open the directory, and copy https://github.com/woodrush/QFT-devkit/blob/main/QFT-devkit/Varlife.rule to the directory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Descriptions of the Lisp Programs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;object-oriented-like.lisp: This example creates a structure similar to classes in Object-Oriented Programming, using closures.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The class has methods and field variables, where each instance carries distinct and persistent memory locations of their own. The example instantiates two counters and concurrently modifies the value held by each instance.&lt;/item&gt;
          &lt;item&gt;New syntaxes for instantiation and method access, &lt;code&gt;(new classname)&lt;/code&gt;and&lt;code&gt;(. instance methodname)&lt;/code&gt;, are introduced using macros and functions.&lt;/item&gt;
        &lt;/list&gt;
        &lt;p&gt;The Lisp interpreter’s variable scope and the macro feature is powerful enough to manage complex memory management, and even providing a new syntax to support the target paradigm.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;printquote.lisp: A simple demonstration of macros.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;factorial.lisp: A simple demonstration of recursion with the factorial function.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;z-combinator.lisp: Demonstration of the Z Combinator to implement a factorial function using anonymous recursion.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;backquote-splice.lisp: Implements the backquote macro used commonly in Lisp to construct macros. It also supports the unquote and unquote-splice operations, each written as&lt;/p&gt;&lt;code&gt;~&lt;/code&gt;and&lt;code&gt;~@&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;primes.lisp: Prints a list of prime numbers up to 20. This example highlights the use of the&lt;/p&gt;&lt;code&gt;while&lt;/code&gt;syntax.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The contents of print.lisp is quite straightforward - it calculates and prints the result of &lt;code&gt;3 * 14&lt;/code&gt;.
backquote.lisp and primes-print.lisp are similar to backquote-splice.lisp and primes.lisp, mainly included for performance comparisons.
backquote.lisp doesn’t implement the unquote-splice operation, and demonstrates some more examples.
primes-print.lisp reduces the number of list operations to save memory usage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Details of the Lisp Interpreter&lt;/head&gt;
    &lt;head rend="h3"&gt;Special Forms and Builtin Functions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;define&lt;/item&gt;
      &lt;item&gt;if&lt;/item&gt;
      &lt;item&gt;quote&lt;/item&gt;
      &lt;item&gt;car, cdr&lt;/item&gt;
      &lt;item&gt;cons&lt;/item&gt;
      &lt;item&gt;list&lt;/item&gt;
      &lt;item&gt;atom&lt;/item&gt;
      &lt;item&gt;progn&lt;/item&gt;
      &lt;item&gt;while&lt;/item&gt;
      &lt;item&gt;lambda, macro&lt;/item&gt;
      &lt;item&gt;eval&lt;/item&gt;
      &lt;item&gt;eq&lt;/item&gt;
      &lt;item&gt;+, -, *, /, mod, &amp;lt;, &amp;gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Lexical Closures&lt;/head&gt;
    &lt;p&gt;This Lisp interpreter supports lexical closures. The implementation of lexical closures is powerful enough to write an object-oriented-like code as shown in object-oriented-like.lisp, where classes are represented as lexical closures over the field variables and the class methods.&lt;/p&gt;
    &lt;head rend="h3"&gt;Macros&lt;/head&gt;
    &lt;p&gt;This Lisp interpreter supports macros. Lisp macros can be thought as a function that receives code and returns code. Following this design, macros are treated exacly the same as lambdas, except that it takes the arguments as raw S-expressions, and evaluates the result twice (the first time to build the expression, and the second time to actually evaluate the builded expression).&lt;/p&gt;
    &lt;head rend="h2"&gt;Running Times and Statistics&lt;/head&gt;
    &lt;p&gt;VarLife Patterns&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Lisp Program and Pattern (VarLife)&lt;/cell&gt;
        &lt;cell role="head"&gt;#Halting Generations (VarLife)&lt;/cell&gt;
        &lt;cell role="head"&gt;Running Time (VarLife)&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory Usage (VarLife)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;print.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;105,413,068 (exact)&lt;/cell&gt;
        &lt;cell&gt;1.159 mins&lt;/cell&gt;
        &lt;cell&gt;5.0 GiB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;lambda.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;700,000,000&lt;/cell&gt;
        &lt;cell&gt;2.966 mins&lt;/cell&gt;
        &lt;cell&gt;12.5 GiB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;printquote.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;800,000,000&lt;/cell&gt;
        &lt;cell&gt;3.424 mins&lt;/cell&gt;
        &lt;cell&gt;12.5 GiB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;factorial.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;1,000,000,000&lt;/cell&gt;
        &lt;cell&gt;5.200 mins&lt;/cell&gt;
        &lt;cell&gt;17.9 GiB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;z-combinator.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;1,700,000,000&lt;/cell&gt;
        &lt;cell&gt;9.823 mins&lt;/cell&gt;
        &lt;cell&gt;23.4 GiB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;backquote-splice.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;4,100,000,000&lt;/cell&gt;
        &lt;cell&gt;20.467 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;backquote.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;4,100,000,000&lt;/cell&gt;
        &lt;cell&gt;21.663 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;object-oriented-like.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;4,673,000,000&lt;/cell&gt;
        &lt;cell&gt;22.363 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;primes-print.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;8,880,000,000&lt;/cell&gt;
        &lt;cell&gt;27.543 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;primes.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;9,607,100,000&lt;/cell&gt;
        &lt;cell&gt;38.334 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Conway’s Game of Life (GoL) Patterns&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Lisp Program and Pattern (GoL)&lt;/cell&gt;
        &lt;cell role="head"&gt;#Halting Generations (GoL)&lt;/cell&gt;
        &lt;cell role="head"&gt;Running Time (GoL)&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory Usage (GoL)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;print.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;3,724,032,866,304&lt;/cell&gt;
        &lt;cell&gt;382.415 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;lambda.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;24,729,600,000,000&lt;/cell&gt;
        &lt;cell&gt;1372.985 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;printquote.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;28,262,400,000,000&lt;/cell&gt;
        &lt;cell&gt;1938.455 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;factorial.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;35,328,000,000,000&lt;/cell&gt;
        &lt;cell&gt;3395.371 mins&lt;/cell&gt;
        &lt;cell&gt;27.5 GiB (max.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;z-combinator.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;60,057,600,000,000&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;backquote-splice.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;144,844,800,000,000&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;backquote.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;144,844,800,000,000&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;object-oriented-like.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;165,087,744,000,000&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;primes-print.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;313,712,640,000,000&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;primes.lisp [pattern]&lt;/cell&gt;
        &lt;cell&gt;339,399,628,800,000&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Common Statistics&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Lisp Program&lt;/cell&gt;
        &lt;cell role="head"&gt;#QFT CPU Cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;QFT RAM Usage (Words)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;print.lisp&lt;/cell&gt;
        &lt;cell&gt;4,425&lt;/cell&gt;
        &lt;cell&gt;92&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;lambda.lisp&lt;/cell&gt;
        &lt;cell&gt;13,814&lt;/cell&gt;
        &lt;cell&gt;227&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;printquote.lisp&lt;/cell&gt;
        &lt;cell&gt;18,730&lt;/cell&gt;
        &lt;cell&gt;271&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;factorial.lisp&lt;/cell&gt;
        &lt;cell&gt;28,623&lt;/cell&gt;
        &lt;cell&gt;371&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;z-combinator.lisp&lt;/cell&gt;
        &lt;cell&gt;58,883&lt;/cell&gt;
        &lt;cell&gt;544&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;backquote-splice.lisp&lt;/cell&gt;
        &lt;cell&gt;142,353&lt;/cell&gt;
        &lt;cell&gt;869&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;backquote.lisp&lt;/cell&gt;
        &lt;cell&gt;142,742&lt;/cell&gt;
        &lt;cell&gt;876&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;object-oriented-like.lisp&lt;/cell&gt;
        &lt;cell&gt;161,843&lt;/cell&gt;
        &lt;cell&gt;838&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;primes-print.lisp&lt;/cell&gt;
        &lt;cell&gt;281,883&lt;/cell&gt;
        &lt;cell&gt;527&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;primes.lisp&lt;/cell&gt;
        &lt;cell&gt;304,964&lt;/cell&gt;
        &lt;cell&gt;943&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The running times for each program are shown above. The Hashlife algorithm used for the simulation requires a lot of memory in exchange of speedups. The simulations were run on a 32GB-RAM computer, with Golly’s memory usage limit set to 28000 MB, and the default base step to 2 (configurable from the preferences). The memory usage was measured by Ubuntu’s activity monitor. “(max.)” shows where the maximum permitted memory was used. The number of CPU cycles and the QFT memory usage was obtained by running the QFTASM interpreter on the host PC. The QFT memory usage shows the number of RAM addresses that were written at least once. The memory usage is measured in words, which is 16 bits in this architecture.&lt;/p&gt;
    &lt;p&gt;All of the VarLife patterns can actually be run on a computer. The shortest running time is about 1 minute for print.lisp. A sophisticated program such as object-oriented-like.lisp can even run in about 22 minutes.&lt;/p&gt;
    &lt;p&gt;On the other hand, the Game of Life patterns take significantly more time than the VarLife patterns, but for short programs it can be run in a moderately reasonable amount of time. For example, print.lisp finishes running in about 6 hours in the Game of Life pattern. As mentioned in the “Conversion from VarLife to Conway’s Game of Life” section, since the Game of Life pattern emulates the behavior of the VarLife pattern using OTCA Metapixels, the behavior of the Game of Life patterns can be verified by running the VarLife patterns.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tests&lt;/head&gt;
    &lt;p&gt;There are tests to check the behavior of the Lisp interpreter. There is a test for checking the QFTASM-compiled Lisp interpreter using the QFTASM interpreter, and a test for checking the GCC-compiled Lisp interpreter on the host pc. To run these tests, use the following commands:&lt;/p&gt;
    &lt;code&gt;git submodule update --init --recursive # Required for building the source

make test             # Run the tests for the QFTASM-compiled Lisp interpreter, using the QFTASM interpreter
make test_executable  # Run the tests for the executable compiled by GCC
&lt;/code&gt;
    &lt;p&gt;Running &lt;code&gt;make test&lt;/code&gt; requires Hy, a Clojure-like Lisp implemented in Python available via &lt;code&gt;pip install hy&lt;/code&gt;.
Some of the tests compare the output results of Hy and the output of the QFTASM Lisp interpreter.&lt;/p&gt;
    &lt;p&gt;The tests were run on Ubuntu and Mac.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building from Source&lt;/head&gt;
    &lt;p&gt;This section explains how to load the Lisp interpreter (written in C) to the Game of Life pattern, and also how to load a custom Lisp program into the pattern to run it on Game of Life.&lt;/p&gt;
    &lt;p&gt;Please see build.md from the GitHub repository.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation Details&lt;/head&gt;
    &lt;p&gt;This section describes the implementation details for the various optimizations for the QFT assembly and the resulting Game of Life pattern.&lt;/p&gt;
    &lt;head rend="h3"&gt;The C Compiler layer&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Added the computed goto feature to ELVM &lt;list rend="ul"&gt;&lt;item&gt;This was merged into the original ELVM project.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Modified the compiler to preserve and output memory address symbols and program address symbols, for their usage in the compiler optimization tool in the QFTASM layer &lt;list rend="ul"&gt;&lt;item&gt;This allows to use memheader.eir, so that symbols used in the C source can be referenced in the ELVM assembly layer using the same variable symbols.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The ELVM Assembly layer&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wrote the QFTASM backend for ELVM &lt;list rend="ul"&gt;&lt;item&gt;This was merged into the original ELVM project.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Added further improvements to the QFTASM backend: &lt;list rend="ul"&gt;&lt;item&gt;Let the ELVM assembly’s memory address space match QFT’s native memory address space &lt;list rend="ul"&gt;&lt;item&gt;Originally, the ELVM assembly had to convert its memory address every time when a memory access occurs.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Support new opcodes added in the improved QFT architecture&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Let the ELVM assembly’s memory address space match QFT’s native memory address space &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The C layer (the implementation of the Lisp interpreter)&lt;/head&gt;
    &lt;head rend="h4"&gt;Usage of binary search and hashtables for string representations and comparisons&lt;/head&gt;
    &lt;p&gt;By profiling the GCC-compiled version of the Lisp interpreter, it was found that the string table lookup process was a large performance bottleneck. This was a large room for optimization.&lt;/p&gt;
    &lt;p&gt;The optimized string lookup process is as follows. First, when the Lisp parser accepts a symbol token, it creates a 4-bit hash of the string with the checksum of the ASCII representation of the string. The hash points to a hashtable that holds the root of a binary search tree for string comparison. Each node in the tree holds the string of the symbol token, and two nodes that are before and after the token in alphabetical order. When a query symbol token arrives in the parsing phase, a node with a matching token is returned, or a new node for the token is added into this binary tree if the token does not exist yet. This allows for each distinct symbol in the S-expression to have a distinct memory address.&lt;/p&gt;
    &lt;p&gt;In the interpretation phase, since each distinct symbol has a distinct memory address, and every string required for the Lisp program has already been parsed, string comparison can be done by simply comparing the memory address of the tokens. Since the interpreter only uses string equality operations for string comparison, simply checking for integer equality suffices for string comparison, speeding up the interpretation phase. Since the hash key is 4 bits long, this allows for reducing 4 searches in the binary tree compared to using a single binary tree.&lt;/p&gt;
    &lt;head rend="h4"&gt;Usage of jump hash tables for the special form evaluation procedure searches&lt;/head&gt;
    &lt;p&gt;There are 17 distinct procedures for evaluating the special forms in the Lisp interpreter, &lt;code&gt;define&lt;/code&gt;, &lt;code&gt;if&lt;/code&gt;, &lt;code&gt;quote&lt;/code&gt;, &lt;code&gt;car&lt;/code&gt;, &lt;code&gt;cdr&lt;/code&gt;, &lt;code&gt;cons&lt;/code&gt;, &lt;code&gt;atom&lt;/code&gt;, &lt;code&gt;print&lt;/code&gt;, &lt;code&gt;progn&lt;/code&gt;, &lt;code&gt;while&lt;/code&gt;, {&lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;macro&lt;/code&gt;}, &lt;code&gt;eval&lt;/code&gt;, &lt;code&gt;eq&lt;/code&gt;, {&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;mod&lt;/code&gt;}, {&lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;}, &lt;code&gt;list&lt;/code&gt;, and lambda/macro invocations (when if the token is not a special form). Using an &lt;code&gt;if&lt;/code&gt; statement to find the corresponding procedure for a given token becomes a linear search for the token comparisons. To speed up this search process, a hash table is created for jumping to the corresponding procedures. Since the memory addresses for the special forms can be determined before parsing the Lisp program, all of the symbols for the special forms have a fixed memory address. Therefore, the hash key can be created by subtracting an offset to the symbol’s memory address, to point to a hashtable that is created near the register locations. This hashtable is provided in memheader.eir. When the hash key is larger than the regions of this hashtable, it means that the symbol is not a special form, so the evaluation jumps to the lambda/macro invocation procedure.&lt;/p&gt;
    &lt;head rend="h4"&gt;Usage of 2-bit headers to represent value types&lt;/head&gt;
    &lt;p&gt;The Lisp implementation has 3 distinct value types, &lt;code&gt;ATOM&lt;/code&gt;, &lt;code&gt;INT&lt;/code&gt;, and &lt;code&gt;LAMBDA&lt;/code&gt;. Each value only consumes one QFT byte of memory; the &lt;code&gt;ATOM&lt;/code&gt; value holds the pointer to the symbol’s string hashtable, the &lt;code&gt;INT&lt;/code&gt; value holds the signed integer value, and &lt;code&gt;LAMBDA&lt;/code&gt; holds a pointer to the &lt;code&gt;Lambda&lt;/code&gt; struct, as well as its subtype information, of either &lt;code&gt;LAMBDA&lt;/code&gt;, &lt;code&gt;MACRO&lt;/code&gt;, &lt;code&gt;TEMPLAMBDA&lt;/code&gt; and &lt;code&gt;TEMPMACRO&lt;/code&gt;. (The &lt;code&gt;TEMPLAMBDA&lt;/code&gt; and &lt;code&gt;TEMPMACRO&lt;/code&gt; subtypes are lambda and macro types that recycles its argument value memory space every time it is called, but is unused in the final lisp programs.) Since the RAM’s address space is only 10 bits, there are 6 free bits that can be used for addresses holding pointers. Therefore, the value type and subtype information is held in these free bits. This makes the integer in the Lisp implementation to be a 14-bit signed integer, ranging from -8192 to 8191.&lt;/p&gt;
    &lt;head rend="h4"&gt;Minimization of Stack Region Usage&lt;/head&gt;
    &lt;p&gt;Since the C compiler used in this project does not have memory optimization features, this has to be done manually within the C source code. This led to the largest reason why the interpreter’s source code seems to be obfuscated.&lt;/p&gt;
    &lt;p&gt;One of the largest bottlenecks for memory access was stack region usage. Every time a stack region memory access occurs, the assembly code performs memory address offset operations to access the stack region. This does not happen when accessing the heap memory, since there is only one heap region used in the entire program, so the pointers for global variables can be hard-coded by the assembler. Therefore, it is favorable optimization-wise to use the heap memory as much as possible.&lt;/p&gt;
    &lt;p&gt;One way to make use of this fact is to use as much global variables as possible. Since registers and common RAM memory share the same memory space, global variables can be accessed with a speed comparable to registers (However, since the physical location of the RAM memory slot within the pattern affects the I/O signal arrival time, and the registers have the most smallest RAM addresses, i.e. they are the closest to the CPU unit, the registers have the fastest memory access time).&lt;/p&gt;
    &lt;p&gt;Another method of saving memory was to use union memory structures to minimize the stack region usage. In the C compiler used in this project, every time a new variable is introduced in a function, the function’s stack region usage (used per call) is increased to fit all of the variables. This happens even when two variables never appear at the same time. Therefore, using the fact that some variables never appear simultaneously, unions are used for every occurence of such variables, so that they can use a shared region within the stack space. This led to minimization of the stack region usage. Since the stack region is only 233 hextets (1 byte in the QFT RAM is 16 bits) large, this allowed to increase the number of nested function calls, especially the nested calls of &lt;code&gt;eval&lt;/code&gt; which evaluates the S-expressions. Since the S-expressions have a list structure, and &lt;code&gt;eval&lt;/code&gt; becomes nested when lambdas are called in the Lisp program, this optimization was significant for allowing more sophisticated Lisp programs to be run in the architecture.&lt;/p&gt;
    &lt;head rend="h3"&gt;The QFTASM layer&lt;/head&gt;
    &lt;p&gt;The QFT assembly generated by the C compiler has a lot of room for optimization. I therefore created a compiler optimization tool to reduce the QFTASM assembly size.&lt;/p&gt;
    &lt;head rend="h4"&gt;Constant folding&lt;/head&gt;
    &lt;p&gt;Immediate constant expressions such as &lt;code&gt;ADD 1 2 destination&lt;/code&gt; is folded to a &lt;code&gt;MOV&lt;/code&gt; operation.&lt;/p&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;MOV&lt;/code&gt; folding&lt;/head&gt;
    &lt;p&gt;The QFT assembly code can be splitted into subregions by jump operations, such that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each subregion doesn’t contain any jump operations&lt;/item&gt;
      &lt;item&gt;Each subregion ends with a jump operation&lt;/item&gt;
      &lt;item&gt;Every jump operation in the assembly is guaranteed to jump to the beginning of a subregion, and never to the middle of any subregion&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The last guarantee where jumps never occur in the middle of a subregion is provided by the C compiler. The ELVM assembly’s program counter is designed so that it increases only when a jump instruction appears. This makes an ELVM program counter to point to a sequence of multiple instructions, instead of a single instruction. Since the ELVM assembly uses the ELVM program counter for its jump instructions, it is guaranteed that the jump instructions in the QFT assembly never jump to the middle of any subregion, and always jumps to a beginning of a subregion.&lt;/p&gt;
    &lt;p&gt;In each subregion, the dependency graph for the memory address is created. If a memory address becomes written but is later overwritten without becoming used in that subregion at all, the instruction to write to that memory address is removed. Since it is guaranteed that jump operations never jump to the middle of any subregion, it is guaranteed that the overwritten values can be safely removed without affecting the outcome of the program. The &lt;code&gt;MOV&lt;/code&gt; folding optimization makes use of this fact to remove unnecessary instructions.&lt;/p&gt;
    &lt;p&gt;This folding process is also done with dereferences; if a dereferenced memory address is written, and the address is overwritten without being used at all, and the dereference source is unoverwritten at all during this process, the instruction for writingto the dereferenced memory address is removed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Jump folding&lt;/head&gt;
    &lt;p&gt;If the destination of a conditional or fixed-destination jump instruction points to another jump instruction with a fixed destination, the jump destination is folded to the latter jump instruction’s destination.&lt;/p&gt;
    &lt;p&gt;A similar folding is done when a fixed jump instruction points to a conditional jump instruction, where the fixed jump instruction is replaced by the latter conditional jump instruction.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Varlife layer (the computer architecture)&lt;/head&gt;
    &lt;head rend="h4"&gt;Created the with a lookup table structure for the ROM module&lt;/head&gt;
    &lt;p&gt;In this image of the CPU and its surrounding modules, the two modules on the top are the ROM modules. The original ROM module had one table, with the memory address as the key and the instruction as the value. I recreated the ROM module to add a lookup table layer, where each distinct instruction (not the opcodes, but the entire instruction including the values used within) holds a distinct serial integer key. The ROM module on the right accepts a program counter address and returns the instruction key for the program counter. The module on the left accepts the instruction key and returns the actual bits of the instruction as the output. This allows for dictionary compression to be performed to the ROM data, saving a lot of space. Since the instructions are 45 bits and the instruction keys are only 10 bits, the instruction key table is 1/4 the size of the original ROM module. Although the ROM size is 3223 for the entire Lisp interpreter, there were only 616 distinct instructions in the Lisp interpreter, making the size of the instruction table be 616 ROM units high, effectively reducing the ROM module size altogether.&lt;/p&gt;
    &lt;p&gt;The ROM module features another form of compression, where absence of cells are used to represent 0-valued bits within the instruction. Below is a close-up look of the ROM value module:&lt;/p&gt;
    &lt;p&gt;Notice that some cells on the left are absent, despite the table being expected to be a rectangular shape. This is because absent cells do not emit any signals, hence effectively emitting 0-valued bits as the output. To use this fact, all of the instructions are first alphabetically ordered at table creation time, so that instructions that start with trailing zeroes become located higher in the table (further from the signal source). This allows for a maximum number of cells to be replaced with absent units to represent 0-valued bits. In fact, the instruction for no-ops is represented as all zeroes, so all of the units in the value module are replaced by absent cells. The no-op instruction appears a lot of times immediately after the jump operation, due to the fact that the QFT architecture has a branch delay when invoking a jump instruction, requiring a no-op instruction to be present to compensate for the delay.&lt;/p&gt;
    &lt;head rend="h4"&gt;Added new optimized instructions to the ALU, and removed unused ones&lt;/head&gt;
    &lt;p&gt;I removed the &lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;, &lt;code&gt;SL&lt;/code&gt; (shift left), &lt;code&gt;SRL&lt;/code&gt; (shift right logical), and the &lt;code&gt;SRA&lt;/code&gt; (shift right arithmetical) opcodes, and added the &lt;code&gt;SRU&lt;/code&gt; (shift right unit) and &lt;code&gt;SRE&lt;/code&gt; (shift right eight) opcodes to the architecture. Since there already were opcodes for &lt;code&gt;XOR&lt;/code&gt; (bitwise-xor) and &lt;code&gt;ANT&lt;/code&gt; (bitwise-and-not), &lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt;, which were not used much in the interpreter, could be replaced by these opcodes. The bitshift operations had significantly larger patterns than the other opcodes, being more than 10 times larger than the other opcodes. These were reduced to a fixed-size shift operations which could be implemented in the same sizes as the other opcodes. Since the shift left opcode can be replaced by consecutively adding its own value, effectively multiplying by powers of 2, the opcode was safely removed. The main reason for the original bitshift units being large was due to the shift amounts being dependent on the values of the RAM. Converting a binary value to a physical (in-pattern) shift amount required a large pattern. On the other hand, shifting a fixed value could be implemented by a significantly more simpler pattern. The shift right eight instruction is mainly used for reading out the standard input, where each ASCII character in the input string is packed into one 16-bit RAM memory address.&lt;/p&gt;
    &lt;p&gt;This resulted in a total of exactly 8 opcodes, &lt;code&gt;ANT&lt;/code&gt;, &lt;code&gt;XOR&lt;/code&gt;, &lt;code&gt;SRE&lt;/code&gt;, &lt;code&gt;SRU&lt;/code&gt;, &lt;code&gt;SUB&lt;/code&gt;, &lt;code&gt;ADD&lt;/code&gt;, &lt;code&gt;MLZ&lt;/code&gt;, and &lt;code&gt;MNZ&lt;/code&gt;. Since this can fit in 3 bits, the opcode region for the instruction value was reduced by 1 bit. Since the RAM module is 10 bits, and the third value of the instruction is always the writing destination of the RAM, and the first instruction can also be made so that it becomes the reading source address of the RAM, this allows for an additional 6*2=12 bits to be reduced from the instruction length. These altogether has reduced the ROM word size from 58 to 45 bits, reducing nearly 1/4 of the original instruction size.&lt;/p&gt;
    &lt;head rend="h4"&gt;Extended the ROM and RAM address space from 9,7-bit to 12,10-bit&lt;/head&gt;
    &lt;p&gt;The original QFT architecture had a ROM and RAM address space of 9 and 7 bits. I extended the ROM and RAM address space to 12 and 10 bits, respectively. This was not a straightforward task as it first seemed, since the signal arrival timings between the modules had to be carefully adjusted in order for the signals to line up correctly. This involved reverse-engineering and experimenting undocumented VarLife pattern units used in the original QFT architecture. The same held for when redesigning other parts of the architecture.&lt;/p&gt;
    &lt;head rend="h4"&gt;Reducing the Standard Input Size&lt;/head&gt;
    &lt;p&gt;Since each byte of the RAM module can be ordered arbitrarily in the CPU’s architecture, the RAM is arranged so that the standard output is written at the very bottom of the RAM module, and proceeds upwards. Therefore, the contents of the RAM can easily be observed in a Game of Life viewer by directly examining the bottom of the RAM module.&lt;/p&gt;
    &lt;p&gt;Since RAM has 16 bits of memory per memory address, it allows to fit two ASCII-encoded characters per one address. Therefore, the standard input is read out by reading two characters per address. For the standard output, one character is written to one address for aesthetic reasons, so that the characters can be directly observed in a Game of Life viewer the pattern more easily. Also, for the standard output to proceed upwards within the RAM module pattern, the memory pointer for the standard output proceeds backwards in the memory space, while the pointer for the standard input proceeds forwards in the memory space.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Game of Life layer&lt;/head&gt;
    &lt;p&gt;Optimizing the Game of Life layer mainly revolved around understanding the Macrocell format for representing and saving Game of Life patterns, and the Hashlife algorithm. The Macrocell format uses quadtrees and memoization for compressing repeated patterns. Since the final Game of Life pattern is an array of OTCA metapixels which are 2048x2048 large, and even has repeated patterns in the VarLife layer (meaning that there are repeated configurations of OTCA metapixels), this compression reduces the file size for the QFT pattern significantly. The best example that let me understand the Macrocell format was an example provided by Adam P. Goucher in this thread in Golly’s mailing list.&lt;/p&gt;
    &lt;p&gt;The Hashlife algorithm also uses quadtrees and memoization to speed up the Game of Life simulations. This algorithm makes use of the fact that the same pattern in a same time frame influences only a fixed extent of its surrounding regions, hence allowing for memoization.&lt;/p&gt;
    &lt;p&gt;As for optimization, I first noticed that the QFT pattern had a 1-pixel high pattern concatenated to the entire pattern. The original QFT pattern in the original QFT repository was carefully designed so that it is composed of 8x8-sized pattern units. Therefore, most of the patterns can be represented by 8x8 tiles. However, since the 1-pixel high pattern at the top creates an offset that shifts away the pattern from this 8x8 grid, it causes the pattern to have fewer repeated patterns if interpreted from the corner of its bounding box, causing the memoization to work inefficiently. I therefore tried putting a redundant cell (which does not interfere with the rest of the pattern) to realign the entire pattern to its 8x8 grid, which actually slightly reduced the resulting Macrocell file size from the original one. Although I didn’t compare the running times, since the Hashlife algorithm uses memoization over repeated patterns as well, I expect this optimization to at least slightly contribute to the performance of the simulation.&lt;/p&gt;
    &lt;p&gt;Another optimization was improving the metafier script used to convert VarLife patterns to Game of Life (MetafierV3.py). The original script used a square region to fit the entire pattern to create the quadtree representation. However, since the Lisp in Life VarLife pattern is 968 pixels wide but 42354 pixels high, it tried to allocate a 65536x65536-sized integer array, which was prohibitively large to run. I modified the script so that it uses a rectangular region, where absent regions of the quadtree are represented as absent cells. Although this is very straightforward with the knowledge of the Macrocell format, it was difficult at first until I became fond of the algorithms surrounding the Game of Life.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory Region Map and the Phases of Operation&lt;/head&gt;
    &lt;p&gt;The memory region map is carefully designed to save space. This is best described with the operation phases of the interpreter.&lt;/p&gt;
    &lt;head rend="h4"&gt;Phase 0: Precalculations&lt;/head&gt;
    &lt;p&gt;Various precalculations are done after the interpreter starts running. The construction of the string interning hashtable for reserved atoms such as &lt;code&gt;define&lt;/code&gt;, &lt;code&gt;quote&lt;/code&gt;, etc. are done in this phase. For the GCC-compiled interpreter, some variables that are defined in the QFT memory header are defined in the C source.&lt;/p&gt;
    &lt;p&gt;Since the outcome of these precalculations are always the same for any incoming Lisp program, this phase is done on the host PC, and the results are saved as ramdump.csv during the QFTASM compile time. The results are then pre-loaded into the RAM when the VarLife and Game of Life patterns are created. This allows to saves some CPU cycles when running the interpreter.&lt;/p&gt;
    &lt;p&gt;As explained earlier, the QFT architecture holds register values in the RAM. There are 11 registers, which are placed in the addresses from 0 to 10.&lt;/p&gt;
    &lt;p&gt;The reserved values in the image include strings such as reserved atoms and the destinations of the jump hashtable used for evaluation. The rest of the region is used for storing global variables in the interpreter’s C source code.&lt;/p&gt;
    &lt;head rend="h4"&gt;Phase 1: Parsing&lt;/head&gt;
    &lt;p&gt;The Lisp program provided from the standard input is parsed into S-expressions, which is written into the heap region.&lt;/p&gt;
    &lt;p&gt;Notice that the string interning hashtables are created in the later end of the stack region. This is because these hashtables are only used during the parsing phase, and can be overwritten during the evaluation phase. For most Lisp programs including the ones in this repository, the stack region does not grow far enough to overwrite these values. This allows to place 3 growing memory regions during the parsing phase, the stack region used for nested S-expressions, the heap region which stores the parsed S-expressions, and the string interning hashtables when new strings are detected within the Lisp program. Newly detected strings such as variable names in the Lisp program are also written into the heap region.&lt;/p&gt;
    &lt;p&gt;The heap region is also designed so that it overwrites the standard input as it parses the program. Since older parts of the program can be discarded once it is parsed, this allows to naturally free the standard input region which save a lot of space after parsing. The standard input also gets overwritten by the Standard output if the output is long enough. However, due to this design, long programs may have trouble at parsing, since the input may be overwritten too far and get deleted before it is parsed. A workaround for this is to use indentation which places the program further ahead into the memory, which will prevent the program from being overwritten from the growing heap region. For all of the programs included in this repository, this is not an issue and the programs become successfully parsed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Phase 2: Evaluation&lt;/head&gt;
    &lt;p&gt;By this time, all of the contents of the stack region and what is ahead of the head of the heap region can be overwritten in the further steps. Note that a similar issue with the standard input happens with the standard output - when too many Lisp objects are created during runtime, it may overwrite the existing standard output, or may simply exceed the heap region and proceed into the stack region. Since the heap region is connected to the later end of the stack region, this may be safe if the standard output is carefully handled, but the interpreter will eventually start overwriting values of the stack region if the heap continues to grow.&lt;/p&gt;
    &lt;head rend="h3"&gt;Miscellaneous&lt;/head&gt;
    &lt;head rend="h4"&gt;How can a 2-state OTCA Metapixel emulate the behavior of an 8-state VarLife pattern?&lt;/head&gt;
    &lt;p&gt;This is one of the most interesting ideas in the original QFT project to make the QFT architecture possible. As explained in the original QFT post, the 8 states of VarLife are actually a mixture of 4 different birth/survival rules with binary states. This means that each VarLife cell can only transition between two fixed states, and the birth/survival rule for that cell does not change at any point in time. Moreover, the OTCA Metapixel is designed so that each metapixel can carry its own birth/survival rules. Therefore, each VarLife cell can be enoded into an OTCA Metapixel by specifying its birth/survival rule and the binary state. This means that the array of OTCA Metapixels in the metafied pattern is actually a mixture of metapixels with different birth/survival rules, arranged in a way so that it makes the computation possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Halting Time&lt;/head&gt;
    &lt;p&gt;After the program counter is set to 65535 and the program exits, no more ROM and RAM I/O signals become apparent in the entire module. This makes the VarLife pattern becomes completely stationary, where every pattern henceforth becomes completely identical. Defining this as the halting time for the calculation, the pattern for print.lisp halts at exactly 105,413,068 VarLife generations.&lt;/p&gt;
    &lt;p&gt;The halting time for the Game of Life patterns are defined similarly for the meta-states of the OTCA Metapixels. Since OTCA Metapixels never become stationary, the Game of Life states do not become stationary after the halting time, but the meta-states of the OTCA Metapixels will become stationary after the halting time.&lt;/p&gt;
    &lt;p&gt;For the VarLife pattern of print.lisp, by generation 105,387,540, the value 65535 gets written to the program counter. At generation 105,413,067, the last signal becomes just one step from disappearing, and at generation 105,413,068 and onwards, the pattern becomes completely stationary and every pattern becomes identical to each other. In the Game of Life version, since the OTCA Metapixel continues running indefinitely, the pattern does not become completly stationary, but the meta-states of the OTCA Metapixels will become completely stationary, since it is an emulation of the VarLife pattern. Note that the halting times for programs other than print.lisp is just a sufficient number of generations, and not the exact values.&lt;/p&gt;
    &lt;p&gt;The required number of generations per CPU cycle depends on many factors such as the ROM and RAM addresses and the types of opcodes, since the arriving times of the I/O signals depend on factors such as these as well. This makes the number of generations required for the program to halt become different between each program. For example, print.lisp has a rate of 23822.16 generations per CPU cycle (GpC), but z-combinator.lisp has a rate of 28870.81 GpC, and primes-print.lisp has 31502.43 GpC. 23822.16 GpC is in fact insufficient for z-combinator.lisp to finish running, and 28870.81 is also insufficient for primes-print.lisp to finish running.&lt;/p&gt;
    &lt;head rend="h4"&gt;Miscellaneous Screenshots&lt;/head&gt;
    &lt;p&gt;The ALU unit in the CPU. From the left are the modules for the &lt;code&gt;ANT&lt;/code&gt;, &lt;code&gt;XOR&lt;/code&gt;, &lt;code&gt;SRE&lt;/code&gt;, &lt;code&gt;SRU&lt;/code&gt;, &lt;code&gt;SUB&lt;/code&gt;, &lt;code&gt;ADD&lt;/code&gt;, &lt;code&gt;MLZ&lt;/code&gt;, and the &lt;code&gt;MNZ&lt;/code&gt; opcodes.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;SRE&lt;/code&gt; and the &lt;code&gt;SRU&lt;/code&gt; opcodes were newly added for this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits&lt;/head&gt;
    &lt;p&gt;The CPU architecture used in this project was originally created by the members of the Quest For Tetris (QFT) project, and was later optimized and modified by Hikaru Ikuta for the Lisp in Life project. The VarLife cellular automaton rule was also defined by the members of the QFT project. The metafier for converting VarLife patterns to Conway’s Game of Life patterns was written by the members of the QFT project, and was later modified by Hikaru Ikuta to support the pattern size of the Lisp in Life architecture. The assembly language for the QFT architecture, QFTASM, was also originally designed by the members of the QFT project, and was later modified by Hikaru Ikuta for this project for achieving a feasible running time. The Lisp interpreter was written by Hikaru Ikuta. The compilation of the interpreter’s C source code to the ELVM assembly is done using an extended version of 8cc written by Rui Ueyama from Google. The compilation from the ELVM assembly to QFTASM is done by an extended version of ELVM (the Esoteric Language Virtual Machine), a project by Shinichiro Hamaji from Preferred Networks, Inc. The Game of Life backend for ELVM was written by Hikaru Ikuta, and was later further extended by Hikaru for the Lisp in Life project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://woodrush.github.io/blog/posts/2022-01-12-lisp-in-life.html"/><published>2025-12-13T03:06:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46255175</id><title>A Giant Ball Will Help This Man Survive a Year on an Iceberg</title><updated>2025-12-14T00:57:20.954797+00:00</updated><content>&lt;doc fingerprint="65a0453917338b38"&gt;
  &lt;main&gt;
    &lt;p&gt;While rowing across the Pacific in 2008, wind pushing him and waves battering him, Italian explorer Alex Bellini felt an unsettling lack of control.&lt;/p&gt;
    &lt;p&gt;Playing in his mind was the story of another Italian explorer, Umberto Nobile, who crashed his zeppelin north of Svalbard after a 1928 polar expedition. Seven men died. The survivors, including Nobile, spent a month wandering the free-floating pack ice, at one point shooting and eating a polar bear, until their rescue. How people react to unpredictable situations fascinates Bellini, a dedicated student of psychology. In the Arctic Ocean, unpredictable situations are a way of life.&lt;/p&gt;
    &lt;p&gt;“All adventure is based on hypothesis, which can be very different to reality,” says Bellini. “An adventurer must adapt himself to the environment he faces.”&lt;/p&gt;
    &lt;p&gt;Bellini’s newest adventure highlights and relishes that lack of control. Sometime next winter, he plans to travel to Greenland’s west coast, pick an iceberg, and live on it for a year as it melts out in the Atlantic.&lt;/p&gt;
    &lt;p&gt;This is a precarious idea. Bellini will be completely isolated, and his adopted dwelling is liable to roll or fall apart at any moment, thrusting him into the icy sea or crushing him under hundreds of tons of ice.&lt;/p&gt;
    &lt;p&gt;His task: experience the uncontrollable nature of an iceberg at sea without getting himself killed. The solution: an indestructible survival capsule built by an aeronautics company that specializes in tsunami-proof escape pods.&lt;/p&gt;
    &lt;p&gt;“This adventure is about waiting for something to happen,” says Bellini. “But I knew since the beginning I needed to minimize the risk. An iceberg can flip over, and those events can be catastrophic.” Icebergs tend to get top-heavy as they melt from their submerged bottoms, so flips can be immediate and unpredictable. And, of course, so is the weather.&lt;/p&gt;
    &lt;p&gt;Bellini spent two years searching for the appropriate survival capsule, but most were too heavy to plant on a berg. But then, in October, he contacted aeronautical engineer Julian Sharpe, founder of Survival Capsule, a company that makes lightweight, indestructible floating capsules, or “personal safety systems.”&lt;/p&gt;
    &lt;p&gt;They can hold from two to ten people, depending on the model, and are made from aircraft-grade aluminum in what’s called a continuous monocoque structure, an interlocking frame of aluminum spars that evenly distribute force, underneath a brightly painted and highly visible aluminum shell. The inner frame can be stationary or mounted on roller balls so it rotates, allowing the passengers to remain upright at all times.&lt;/p&gt;
    &lt;p&gt;Inside are a number of race car–style seats with four-point seatbelts, arranged facing either outward from the center or inward around the circumference, depending on the number of chairs. Storage compartments, including food and water tanks, sit beneath the seats. Two watertight hatches open inward to avoid outside obstructions. Being watertight, it’s a highly buoyant vessel, displacing water like a boat does.&lt;/p&gt;
    &lt;p&gt;“I fell in love with the capsule,” says Bellini. “I’m in good hands.” He selected a three-meter, ten-person version, for which he’ll design his own interior.&lt;/p&gt;
    &lt;p&gt;Sharpe got the idea for his capsules after the 2004 Indonesian tsunami. He believes fewer people would have died had some sort of escape pod existed. With his three-man team, which includes a former NOAA director and a Boeing engineer, he brought the idea to fruition in 2011. Companies in Japan that operate in the line of fire for tsunamis expressed the most interest. But Sharpe hopes the products will be universal—in schools, retirement homes, and private residences, anywhere there is severe weather. The first testing prototypes of the capsules, which range from $12,000 to $20,000, depending on size, were shipped to Tokyo in 2013. Four are in Japan; two are in the United States. His two-person capsule is now for sale; the others will follow later this year.&lt;/p&gt;
    &lt;p&gt;“Right now there’s only horizontal and vertical evacuations,” Sharpe said. “We want to offer a third option: riding it out.”&lt;/p&gt;
    &lt;p&gt;The company intends to rely on an increasing market for survival equipment as sea level and the threat of major storms rise. Sharpe designed the capsules to be tethered to the ground using 20 to 50 meters of steel cable and to withstand a tsunami or storm surge. Each will have a water tank and a sophisticated GPS beacon system in case the tether snaps. Survival Capsule advises storing seven to ten days of food in each capsule.&lt;/p&gt;
    &lt;p&gt;The product appeals to Bellini because it’s strong enough to survive a storm at sea or getting crushed between two icebergs. It will rest on top of the ice using either its own weight or a specially designed stand that will detach if the berg rolls. The circular shape is crucial for avoiding a crushing blow. The capsule will just roll off any incoming mass, and the water will provide an equal and opposite reaction to any force exerted on the capsule. “A multicurved surface is almost uncrushable,” Sharpe said. “If you imagine shooting an arrow at a wooden ball, unless you hit dead center, it’ll ricochet.”&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The capsule is strong enough to survive a storm at sea or getting crushed between two icebergs. It will rest on top of the ice using either its own weight or a specially designed stand that will detach if the berg rolls.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The basic model ensures survival, but there’s more to life on an iceberg than just surviving. You can add windows, extra space, and other modular additions, even surround sound and color options. “You can trick your crib out all you want,” Sharpe said. And that’s exactly what Bellini plans to do. He doesn’t have a layout yet, but he has hired Italian designer Pietro Santoro to customize his ten-person pod. He will remove the other nine seats for extra room.&lt;/p&gt;
    &lt;p&gt;Other than modifications to keep him safe and healthy, the capsule is basic, Bellini said. It will carry 300 to 400 kilograms of food, a wind generator, solar panels, and an EPIRB beacon so rescuers can find him. He’ll have Wi-Fi to update his team and the public. The layout will consist of a work table, electronic panels, and a bed. “A foldable bed,” Bellini added. “I want to have room to work out.”&lt;/p&gt;
    &lt;p&gt;Bellini will spend almost all of his time in the capsule with the hatch closed, which will pose major challenges. He’ll have to stay active without venturing out onto a slippery, unstable iceberg. If it flips, he’ll have no time to react. He’s working with a company to develop nanosensors able to detect movement in the iceberg so he has advance warning of a flip. “Any step away from [the iceberg] will be in unknown territory,” he said. “You want to stretch your body. But then you risk your life.” He fears a lack of activity will dull his ability to stay safe. “I cannot permit myself to get crazy,” he said. “I need to keep my body fit, not for my body, but for my safety.” He is working on a routine of calisthenics that can be done in the capsule, and he might install a stationary bike, most likely a Ciclotte.&lt;/p&gt;
    &lt;p&gt;Lack of sunlight is another challenge of spending a year in an aluminum sphere. It will be winter in the Arctic, with maybe five hours of light each day. Bellini and Sharpe are working on a lighting system that will simulate natural light, allowing Bellini to get vitamins and maintain his circadian rhythm.&lt;/p&gt;
    &lt;p&gt;Bellini’s model is in development, and he expects it to be ready in about a year. He plans to write during his mission and will bring plenty of nonfiction books, especially psychology.&lt;/p&gt;
    &lt;p&gt;The capsule won’t ease his isolation, maybe his greatest challenge, but Bellini remains undaunted: “It’s the key to the inner part of myself.” The first step is relinquishing control.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.outsideonline.com/outdoor-adventure/exploration-survival/how-giant-ball-will-help-man-survive-year-iceberg/"/><published>2025-12-13T15:25:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46255285</id><title>Ask HN: How can I get better at using AI for programming?</title><updated>2025-12-14T00:57:20.158020+00:00</updated><content>&lt;doc fingerprint="1e94a839a7a45139"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I've been working on a personal project recently, rewriting an old jQuery + Django project into SvelteKit. The main work is translating the UI templates into idiomatic SvelteKit while maintaining the original styling. This includes things like using semantic HTML instead of div-spamming, not wrapping divs in divs in divs, and replacing bootstrap with minimal tailwind. It also includes some logic refactors, to maintain the original functionality but rewritten to avoid years of code debt. Things like replacing templates using boolean flags for multiple views with composable Svelte components.&lt;/p&gt;
      &lt;p&gt;I've had a fairly steady process for doing this: look at each route defined in Django, build out my `+page.server.ts`, and then split each major section of the page into a Svelte component with a matching Storybook story. It takes a lot of time to do this, since I have to ensure I'm not just copying the template but rather recreating it in a more idiomatic style.&lt;/p&gt;
      &lt;p&gt;This kind of work seems like a great use case for AI assisted programming, but I've failed to use it effectively. At most, I can only get Claude Code to recreate some slightly less spaghetti code in Svelte. Simple prompting just isn't able to get AI's code quality within 90% of what I'd write by hand. Ideally, AI could get it's code to something I could review manually in 15-20 minutes, which would massively speed up the time spent on this project (right now it takes me 1-2 hours to properly translate a route).&lt;/p&gt;
      &lt;p&gt;Do you guys have tips or suggestions on how to improve my efficiency and code quality with AI?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46255285"/><published>2025-12-13T15:37:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46255991</id><title>I tried Gleam for Advent of Code</title><updated>2025-12-14T00:57:20.012657+00:00</updated><content>&lt;doc fingerprint="4758503509883eb8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Tried Gleam for Advent of Code, and I Get the Hype&lt;/head&gt;
    &lt;p&gt;I do Advent of Code every year.&lt;/p&gt;
    &lt;p&gt;For the last seven years, including this one, I have managed to get all the stars. I do not say that to brag. I say it because it explains why I keep coming back.&lt;/p&gt;
    &lt;p&gt;It is one of the few tech traditions I never get bored of, even after doing it for a long time. I like the time pressure. I like the community vibe. I like that every December I can pick one language and go all in.&lt;/p&gt;
    &lt;p&gt;This year, I picked Gleam.&lt;/p&gt;
    &lt;head rend="h2"&gt;A much shorter year#&lt;/head&gt;
    &lt;p&gt;Advent of Code is usually 25 days. This year Eric decided to do 12 days instead.&lt;/p&gt;
    &lt;p&gt;So instead of 50 parts, it was 24.&lt;/p&gt;
    &lt;p&gt;That sounds like a relaxed year. It was not, but not in a bad way.&lt;/p&gt;
    &lt;p&gt;The easier days were harder than the easy days in past years, but they were also really engaging and fun to work through. The hard days were hard, especially the last three, but they were still the good kind of hard. They were problems I actually wanted to wrestle with.&lt;/p&gt;
    &lt;p&gt;It also changes the pacing in a funny way. In a normal year, by day 10 you have a pretty comfy toolbox. This year it felt like the puzzles were already demanding that toolbox while I was still building it.&lt;/p&gt;
    &lt;p&gt;That turned out to be a perfect setup for learning a new language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Gleam felt like a good AoC language#&lt;/head&gt;
    &lt;p&gt;Gleam is easy to like quickly.&lt;/p&gt;
    &lt;p&gt;The syntax is clean. The compiler is helpful, and the error messages are super duper good. Rust good.&lt;/p&gt;
    &lt;p&gt;Most importantly, the language strongly nudges you into a style that fits Advent of Code really well. Parse some text. Transform it a few times. Fold. Repeat.&lt;/p&gt;
    &lt;p&gt;Also, pipes. Pipes everywhere. I love pipes.&lt;/p&gt;
    &lt;p&gt;One thing I did not expect was how good the editor experience would be. The LSP worked much better than I expected. It basically worked perfectly the whole time. I used the Gleam extension for IntelliJ and it was great.&lt;/p&gt;
    &lt;p&gt;https://plugins.jetbrains.com/plugin/25254-gleam-language&lt;/p&gt;
    &lt;p&gt;I also just like FP.&lt;/p&gt;
    &lt;p&gt;FP is not always easier, but it is often easier. When it clicks, you stop writing instructions and you start describing the solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;The first Gleam superpower: &lt;code&gt;echo&lt;/code&gt;#&lt;/head&gt;
    &lt;p&gt;The first thing I fell in love with was &lt;code&gt;echo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It is basically a print statement that does not make you earn it. You can &lt;code&gt;echo&lt;/code&gt; any value. You do not have to format anything. You do not have to build a string. You can just drop it into a pipeline and keep going.&lt;/p&gt;
    &lt;p&gt;This is the kind of thing I mean:&lt;/p&gt;
    &lt;code&gt;list.range(0, 5)
|&amp;gt; echo
|&amp;gt; list.map(int.to_string)
|&amp;gt; echo
&lt;/code&gt;
    &lt;p&gt;You can quickly inspect values at multiple points without breaking the flow.&lt;/p&gt;
    &lt;p&gt;I did miss string interpolation, especially early on. &lt;code&gt;echo&lt;/code&gt; made up for a lot of that.&lt;/p&gt;
    &lt;p&gt;It mostly hit when I needed to generate text, not when I needed to inspect values. The day where I generated an LP file for &lt;code&gt;glpsol&lt;/code&gt; is the best example. It is not hard code, but it is a lot of string building. Without interpolation it turns into a bit of a mess of &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;s.&lt;/p&gt;
    &lt;p&gt;This is a small excerpt from my LP generator:&lt;/p&gt;
    &lt;code&gt;"Minimize\n"
&amp;lt;&amp;gt; "  total: "
&amp;lt;&amp;gt; buttons
|&amp;gt; string.join(" + ")
&amp;lt;&amp;gt; "\n\nSubject To\n"
&lt;/code&gt;
    &lt;p&gt;It works. It is just the kind of code where you really feel missing interpolation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Options everywhere, and why that matters for grid puzzles#&lt;/head&gt;
    &lt;p&gt;A lot of AoC is grids.&lt;/p&gt;
    &lt;p&gt;Grids are where you normally either crash into out of bounds bugs, or you litter your code with bounds checks you do not care about.&lt;/p&gt;
    &lt;p&gt;In my day 4 solution I used a dict as a grid. The key ergonomic part is that &lt;code&gt;dict.get&lt;/code&gt; gives you an option-like result, which makes neighbour checking safe by default.&lt;/p&gt;
    &lt;p&gt;This is the neighbour function from my solution:&lt;/p&gt;
    &lt;code&gt;fn get_neighbours(grid: Grid(Object), pos: Position) -&amp;gt; List(Object) {
  [
    #(pos.0 - 1, pos.1 - 1),
    #(pos.0 - 1, pos.1),
    #(pos.0 - 1, pos.1 + 1),
    #(pos.0, pos.1 - 1),
    #(pos.0, pos.1 + 1),
    #(pos.0 + 1, pos.1 - 1),
    #(pos.0 + 1, pos.1),
    #(pos.0 + 1, pos.1 + 1),
  ]
  |&amp;gt; list.filter_map(fn(neighbour_pos) { grid |&amp;gt; dict.get(neighbour_pos) })
}
&lt;/code&gt;
    &lt;p&gt;That last line is the whole point.&lt;/p&gt;
    &lt;p&gt;No bounds checks. No sentinel values. Out of bounds just disappears.&lt;/p&gt;
    &lt;head rend="h2"&gt;The list toolbox is genuinely good#&lt;/head&gt;
    &lt;p&gt;I expected to write parsers and helpers, and I did. What I did not expect was how often Gleam already had the exact list function I needed.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;list.transpose&lt;/code&gt; saved a whole day#&lt;/head&gt;
    &lt;p&gt;Day 6 part 1 was basically a transpose problem in disguise.&lt;/p&gt;
    &lt;p&gt;I read the input, chunked it into rows, transposed it, and suddenly the rest of the puzzle became obvious.&lt;/p&gt;
    &lt;code&gt;input
|&amp;gt; list.transpose
|&amp;gt; list.map(fn(line) { line |&amp;gt; calculate_instruction })
|&amp;gt; bigi.sum
&lt;/code&gt;
    &lt;p&gt;In a lot of languages you end up writing your own transpose yet again. In Gleam it is already there.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;list.combination_pairs&lt;/code&gt; is a cheat code#&lt;/head&gt;
    &lt;p&gt;Another example is &lt;code&gt;list.combination_pairs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In day 8 I needed all pairs of 3D points. In an imperative language you would probably write nested loops and then question your off by one logic.&lt;/p&gt;
    &lt;p&gt;In Gleam it is a one liner:&lt;/p&gt;
    &lt;code&gt;boxes
|&amp;gt; list.combination_pairs
&lt;/code&gt;
    &lt;p&gt;Sometimes FP is not about being clever. It is about having the right function name.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;fold_until&lt;/code&gt; is my favorite thing I found#&lt;/head&gt;
    &lt;p&gt;If I had to pick one feature that made me want to keep writing Gleam after AoC, it is &lt;code&gt;fold_until&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Early exit without hacks is fantastic in puzzles.&lt;/p&gt;
    &lt;p&gt;In day 8 part 2 I kept merging sets until the first set in the list contained all boxes. When that happens, I stop.&lt;/p&gt;
    &lt;p&gt;The core shape looks like this:&lt;/p&gt;
    &lt;code&gt;|&amp;gt; list.fold_until(initial, fn(acc, pair) {
  case done_yet {
    True -&amp;gt; Stop(new_acc)
    False -&amp;gt; Continue(new_acc)
  }
})
&lt;/code&gt;
    &lt;p&gt;It is small, explicit, and it reads like intent.&lt;/p&gt;
    &lt;p&gt;I also used &lt;code&gt;fold_until&lt;/code&gt; in day 10 part 1 to find the smallest combination size that works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Gleam fought me a bit#&lt;/head&gt;
    &lt;p&gt;Even though I enjoyed Gleam a lot, I did hit a few recurring friction points.&lt;/p&gt;
    &lt;p&gt;None of these are deal breakers. They are just the kind of things you notice when you do 24 parts in a row.&lt;/p&gt;
    &lt;head rend="h3"&gt;File IO is not in the standard library#&lt;/head&gt;
    &lt;p&gt;This one surprised me on day 1.&lt;/p&gt;
    &lt;p&gt;For AoC you read a file every day. In this repo I used &lt;code&gt;simplifile&lt;/code&gt; everywhere because you need something. It is fine, I just did not expect basic file IO to be outside the standard library.&lt;/p&gt;
    &lt;head rend="h3"&gt;Regex is a dependency too#&lt;/head&gt;
    &lt;p&gt;Day 2 part 2 pushed me into regex and I had to add &lt;code&gt;gleam_regexp&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is the style I used, building a regex from a substring:&lt;/p&gt;
    &lt;code&gt;let assert Ok(re) = regexp.from_string("^(" &amp;lt;&amp;gt; substring &amp;lt;&amp;gt; ")+$")
regexp.check(re, val)
&lt;/code&gt;
    &lt;p&gt;Again, totally fine. It just surprised me.&lt;/p&gt;
    &lt;head rend="h3"&gt;List pattern matching limitations#&lt;/head&gt;
    &lt;p&gt;You can do &lt;code&gt;[first, ..rest]&lt;/code&gt; and you can do &lt;code&gt;[first, second]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;But you cannot do &lt;code&gt;[first, ..middle, last]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It is not the end of the world, but it would have made some parsing cleaner.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comparisons are explicit#&lt;/head&gt;
    &lt;p&gt;In Gleam a lot of comparisons are not booleans. You get an &lt;code&gt;order&lt;/code&gt; value.&lt;/p&gt;
    &lt;p&gt;This is great for sorting. It is also very explicit. It can be a bit verbose when you just want an &lt;code&gt;&amp;lt;=&lt;/code&gt; check.&lt;/p&gt;
    &lt;p&gt;In day 5 I ended up writing patterns like this:&lt;/p&gt;
    &lt;code&gt;case cmp_start, cmp_end {
  order.Lt, _ -&amp;gt; False
  _, order.Gt -&amp;gt; False
  _, _ -&amp;gt; True
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Big integers, and targeting JavaScript#&lt;/head&gt;
    &lt;p&gt;I used &lt;code&gt;bigi&lt;/code&gt; a few times this year.&lt;/p&gt;
    &lt;p&gt;On the Erlang VM, integers are arbitrary precision, so you usually do not care about overflow. That is one of the nicest things about the BEAM.&lt;/p&gt;
    &lt;p&gt;If you want your Gleam code to also target JavaScript, you do care. JavaScript has limits, and suddenly using &lt;code&gt;bigi&lt;/code&gt; becomes necessary for some puzzles.&lt;/p&gt;
    &lt;p&gt;I wish that was just part of &lt;code&gt;Int&lt;/code&gt;, with a single consistent story across targets.&lt;/p&gt;
    &lt;head rend="h2"&gt;The most satisfying part: XOR as bitmasks#&lt;/head&gt;
    &lt;p&gt;Day 10 part 1 was my favorite part of the whole event.&lt;/p&gt;
    &lt;p&gt;The moment I saw the toggling behavior, it clicked as XOR. Represent the lights as a number. Represent each button as a bitmask. Find the smallest combination of bitmasks that XOR to the target.&lt;/p&gt;
    &lt;p&gt;This is the fold from my solution:&lt;/p&gt;
    &lt;code&gt;combination
|&amp;gt; list.fold(0, fn(acc, comb) {
  int.bitwise_exclusive_or(acc, comb)
})
&lt;/code&gt;
    &lt;p&gt;It felt clean, it felt fast, and it felt like the representation did most of the work.&lt;/p&gt;
    &lt;head rend="h2"&gt;The least satisfying part: shelling out to &lt;code&gt;glpsol&lt;/code&gt;#&lt;/head&gt;
    &lt;p&gt;Day 10 part 2 was the opposite feeling.&lt;/p&gt;
    &lt;p&gt;I knew brute force was out. It was clearly a system of linear equations.&lt;/p&gt;
    &lt;p&gt;In previous years I would reach for Z3, but there are no Z3 bindings for Gleam. I tried to stay in Gleam, and I ended up generating an LP file and shelling out to &lt;code&gt;glpsol&lt;/code&gt; using &lt;code&gt;shellout&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It worked, and honestly the LP format is beautiful.&lt;/p&gt;
    &lt;p&gt;Here is the call:&lt;/p&gt;
    &lt;code&gt;let _ =
  shellout.command(
    "glpsol",
    ["--lp", "temp.lp", "-w", "temp_sol.txt"],
    ".",
    [],
  )
&lt;/code&gt;
    &lt;p&gt;It is a hack, but it is a pragmatic hack, and that is also part of Advent of Code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memoization keys that actually model the problem#&lt;/head&gt;
    &lt;p&gt;Day 11 part 2 is where I was happy I was writing Gleam.&lt;/p&gt;
    &lt;p&gt;The important detail was that the memo key is not just the node. It is the node plus your state.&lt;/p&gt;
    &lt;p&gt;In my case the key was:&lt;/p&gt;
    &lt;code&gt;#(neighbour, new_seen_dac, new_seen_fft)
&lt;/code&gt;
    &lt;p&gt;Once I got the memo threading right, it ran instantly.&lt;/p&gt;
    &lt;head rend="h2"&gt;The finale, and the troll heuristic#&lt;/head&gt;
    &lt;p&gt;The last day was the only puzzle I did not fully enjoy.&lt;/p&gt;
    &lt;p&gt;Not because it was bad. It just felt like it relied on assumptions about the input, and I am one of those people that does not love doing that.&lt;/p&gt;
    &lt;p&gt;I overthought it for a bit, then I learned it was more of a troll problem. The “do the areas of the pieces, when fully interlocked, fit on the board” heuristic was enough.&lt;/p&gt;
    &lt;p&gt;In my solution it is literally this:&lt;/p&gt;
    &lt;code&gt;heuristic_area &amp;lt;= max_area
&lt;/code&gt;
    &lt;p&gt;Sometimes you build a beautiful mental model and then the right answer is a single inequality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing thoughts#&lt;/head&gt;
    &lt;p&gt;I am very happy I picked Gleam this year.&lt;/p&gt;
    &lt;p&gt;It has sharp edges, mostly around where the standard library draws the line and a few language constraints that show up in puzzle code. But it also has real strengths.&lt;/p&gt;
    &lt;p&gt;Pipelines feel good. Options and Results make unsafe problems feel safe. The list toolbox is better than I expected. &lt;code&gt;fold_until&lt;/code&gt; is incredible. Once you stop trying to write loops and you let it be functional, the solutions start to feel clearer.&lt;/p&gt;
    &lt;p&gt;I cannot wait to try Gleam in a real project. I have been thinking about using it to write a webserver, and I am genuinely excited to give it a go.&lt;/p&gt;
    &lt;p&gt;And of course, I cannot wait for next year’s Advent of Code.&lt;/p&gt;
    &lt;p&gt;If you want to look at the source for all 12 days, it is here:&lt;/p&gt;
    &lt;p&gt;https://github.com/tymscar/Advent-Of-Code/tree/master/2025/gleam/aoc/src&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.tymscar.com/posts/gleamaoc2025/"/><published>2025-12-13T17:00:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46256002</id><title>EasyPost (YC S13) Is Hiring</title><updated>2025-12-14T00:57:19.281468+00:00</updated><content>&lt;doc fingerprint="e60e91b34b3de70c"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance shipping made easy&lt;/p&gt;
    &lt;head rend="h3"&gt;Dear Job Seekers,&lt;/head&gt;
    &lt;p&gt;We want to ensure your safety and protect you from potential scams. Recently, there have been fraudulent recruitment initiatives online that impersonate our company. These scams aim to deceive unsuspecting applicants by offering nonexistent positions and requesting personal information or upfront fees.&lt;/p&gt;
    &lt;p&gt;Remember that our company does not endorse any job postings outside our official channels. If you encounter a suspicious offer, report it through the job platform on which you found it or report email as spam.&lt;/p&gt;
    &lt;p&gt;If you need to check on the validity of an email from EasyPost, feel free to reach out directly to recruiting@easypost.com&lt;/p&gt;
    &lt;p&gt;For more information on this scam, please see this FTC Consumer Alert.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future of you&lt;/head&gt;
    &lt;p&gt;As industry experts, we’re working not only to help our customers make sense of the industry, but to define where it’s headed. We are looking for candidates who are approachable, dynamic, inventive, intelligent, and reliable to join our team in unpacking the future of shipping.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future of shipping&lt;/head&gt;
    &lt;p&gt;How can modern, flexible technology improve the customer experience of shipping? What if every business was able to offer same-day shipping? How much waste would be removed from the environment if all our shipments were consolidated into one delivery per week? At EasyPost, we’re figuring out the answer to these questions and more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Life at EasyPost&lt;/head&gt;
    &lt;head rend="h4"&gt;Adaptive&lt;/head&gt;
    &lt;p&gt;Embrace new challenges to grow your skill set.&lt;/p&gt;
    &lt;head rend="h4"&gt;Simple&lt;/head&gt;
    &lt;p&gt;Create efficient solutions that are easy to execute.&lt;/p&gt;
    &lt;head rend="h4"&gt;Inclusive&lt;/head&gt;
    &lt;p&gt;Share new ideas and work collaboratively across teams.&lt;/p&gt;
    &lt;head rend="h2"&gt;Team and technology&lt;/head&gt;
    &lt;p&gt;We’re a fun group of passionate entrepreneurs who built our own revolutionary software designed to make shipping simple. EasyPost started as an Engineering first company and we are proud to have a pragmatic approach to software development. Our team has a wealth of diverse experience and different backgrounds ranging from startups to large technology companies.Be part of a leading technology company:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CI/CD inspired workflows – we deploy dozens of times a day&lt;/item&gt;
      &lt;item&gt;Small services over monoliths – we’ve deployed hundreds of services&lt;/item&gt;
      &lt;item&gt;Strong engineering tooling and developer support&lt;/item&gt;
      &lt;item&gt;Transparency and participation around architecture and technology decisions&lt;/item&gt;
      &lt;item&gt;Culture of blamelessness and improving today from yesterday’s shortcomings&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.easypost.com/careers"/><published>2025-12-13T17:01:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46256486</id><title>How does the CPU cache work?</title><updated>2025-12-14T00:57:19.154615+00:00</updated><content>&lt;doc fingerprint="ce765c20673039d1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;How CPU Caches Work&lt;/head&gt;- 18 mins&lt;head rend="h1"&gt;A Friendly Tour of CPU Caches&lt;/head&gt;&lt;p&gt;This post is a tour of how CPU caches actually work, using a simple real‑world picture: a busy office with desks, cabinets, and a big archive room.&lt;/p&gt;&lt;head rend="h2"&gt;What we’ll cover&lt;/head&gt;&lt;p&gt;Here’s the path we’ll walk:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Cache overview&lt;/item&gt;&lt;item&gt;Data layout and cache friendliness&lt;/item&gt;&lt;item&gt;Locality of reference&lt;/item&gt;&lt;item&gt;Cache lines&lt;/item&gt;&lt;item&gt;Sets, ways, and tags&lt;/item&gt;&lt;item&gt;Replacement policies&lt;/item&gt;&lt;item&gt;Cache writes and consistency&lt;/item&gt;&lt;item&gt;Write-through vs write-back&lt;/item&gt;&lt;item&gt;Write-allocate vs no-write-allocate&lt;/item&gt;&lt;item&gt;Cache hierarchy (L1, L2, L3)&lt;/item&gt;&lt;item&gt;SRAM vs DRAM&lt;/item&gt;&lt;item&gt;Summary&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Along the way we’ll sprinkle Tiny explainers to keep the tone light.&lt;/p&gt;&lt;head rend="h2"&gt;1. Cache overview&lt;/head&gt;&lt;p&gt;Imagine you work in a busy office.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;There is a huge archive room with shelves of folders. That’s your main memory (RAM).&lt;/item&gt;&lt;item&gt;You have a small cabinet next to your desk with the files you use a lot. That’s like your L2 / L3 cache.&lt;/item&gt;&lt;item&gt;On your desk, you keep a tiny pile of folders open right in front of you. That’s your L1 cache.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The closer something is to you:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The faster you can grab it.&lt;/item&gt;&lt;item&gt;The fewer things you can keep there (space is limited).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In the CPU, the picture looks like this:&lt;/p&gt;&lt;p&gt;The CPU always tries to use data from the closest place first. If it can’t find it there, it walks further away:&lt;/p&gt;&lt;p&gt;L1 → L2 → L3 → RAM&lt;/p&gt;&lt;p&gt;Every time it has to “walk to the archive room” your program slows down a bit. Caches exist so that doesn’t happen too often.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer - CPU core&lt;/p&gt;&lt;lb/&gt;A core is like one worker inside your computer’s “office” A CPU with 8 cores has 8 workers. Each core usually has its own small caches, but they share main memory.&lt;/quote&gt;&lt;head rend="h2"&gt;2. Data layout and cache friendliness&lt;/head&gt;&lt;p&gt;Let’s say you manage files for 1,000 customers. For each customer you have:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A contact sheet&lt;/item&gt;&lt;item&gt;A billing sheet&lt;/item&gt;&lt;item&gt;An orders sheet&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can organize your desk in two ways.&lt;/p&gt;&lt;head rend="h3"&gt;Option A - One big folder per customer (object-oriented)&lt;/head&gt;&lt;p&gt;You keep a big folder per customer.&lt;/p&gt;&lt;p&gt;Now your manager walks over and says: “Update the billing info for all customers.”&lt;/p&gt;&lt;p&gt;You now:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Grab Customer A’s folder → flip to billing → update&lt;/item&gt;&lt;item&gt;Grab Customer B’s folder → flip to billing → update&lt;/item&gt;&lt;item&gt;Grab Customer C’s folder → and so on…&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If your desk is crowded, some of these folders are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;On the desk,&lt;/item&gt;&lt;item&gt;Some in the side cabinet,&lt;/item&gt;&lt;item&gt;Some already back in the archive.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You end up constantly pulling a folder from somewhere, opening it, flipping pages, updating, putting it back. Stop‑start, lots of jumping around.&lt;/p&gt;&lt;p&gt;This is similar to a object‑oriented memory layout: each “object” (customer) contains all fields (contact, billing, orders) inside it, and those objects end up scattered across RAM. When the CPU wants “billing for all customers”, it keeps jumping to different places in memory.&lt;/p&gt;&lt;head rend="h3"&gt;Option B - Group by type (data-oriented)&lt;/head&gt;&lt;p&gt;Instead, you could make three stacks:&lt;/p&gt;&lt;p&gt;Now when your manager says:&lt;/p&gt;&lt;p&gt;“Update the billing info for all customers.”&lt;/p&gt;&lt;p&gt;You:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Take the billing stack&lt;/item&gt;&lt;item&gt;Start at the top&lt;/item&gt;&lt;item&gt;Go straight down: B0 → B1 → B2 → …&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is data‑oriented design. It lines up similar data in memory so the CPU can walk through it in order.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer - “data-oriented” vs “object-oriented”&lt;/p&gt;&lt;lb/&gt;Object-oriented: “all things for this one customer live together” but different customers may be scattered.&lt;lb/&gt;Data-oriented: “all billing sheets live together, all contacts live together” so operations that touch one kind of data can run in smooth loops.&lt;/quote&gt;&lt;head rend="h2"&gt;3. Locality of reference&lt;/head&gt;&lt;p&gt;Caches rely on a simple observation about most programs: when the CPU uses a piece of data once, it will likely touch it again soon and will likely touch nearby data as well.&lt;/p&gt;&lt;p&gt;There are two names for this:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Temporal locality - time based&lt;/item&gt;&lt;item&gt;Spatial locality - location based&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Temporal locality is about time: recently used → likely to be used again. Spatial locality is about location: used something at address X → likely to use X+1, X+2, X+3 soon.&lt;/p&gt;&lt;p&gt;Your desk stack reflects temporal locality: you keep recent folders near you. Your billing stack reflects spatial locality: items that will be processed together are next to each other.&lt;/p&gt;&lt;p&gt;Caches lean on both ideas.&lt;/p&gt;&lt;head rend="h2"&gt;4. Cache lines&lt;/head&gt;&lt;p&gt;The CPU doesn’t fetch one byte at a time from memory. That would be far too slow. Instead it fetches fixed‑size chunks called cache lines.&lt;/p&gt;&lt;p&gt;On many systems, a cache line is 64 bytes long. You can picture it like this:&lt;/p&gt;&lt;p&gt;When the CPU reads a single value from memory, the cache pulls in the entire line that contains that value. If your billing sheets are laid out one after another in memory, a single line can hold several consecutive billing entries.&lt;/p&gt;&lt;p&gt;Accessing B1 might pull B0 and B2 into the cache as well. If the program soon touches B2 or B3, they are already “on the desk”.&lt;/p&gt;&lt;p&gt;If the data is scattered (like folders all over the office), each access can drag in a line mostly filled with things you never touch, wasting cache space.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer - 1 byte, 1 KB, 1 MB&lt;/p&gt;&lt;lb/&gt;1 byte ≈ 1 letter of text.&lt;lb/&gt;1 KB (kilobyte) ≈ 1,000 bytes.&lt;lb/&gt;1 MB (megabyte) ≈ 1,000,000 bytes.&lt;lb/&gt;A 64‑byte cache line is tiny compared to modern memory sizes.&lt;/quote&gt;&lt;head rend="h2"&gt;5. Sets, ways, and tags&lt;/head&gt;&lt;p&gt;Inside the CPU, the cache is not just a random pile of lines.&lt;/p&gt;&lt;p&gt;Most CPU caches are set‑associative: they’re divided into rows called sets, and each set contains a small number of slots called ways, each way holding one cache line.&lt;/p&gt;&lt;p&gt;Each row is a set. Each box in that row is a way.&lt;/p&gt;&lt;p&gt;If there are 4 boxes per row, we call it a 4‑way set‑associative cache.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer - associativity&lt;/p&gt;&lt;lb/&gt;Associativity is just “how many slots per set”. 2‑way = 2 lines per set, 4‑way = 4 lines per set, etc. More ways gives more flexibility but more hardware complexity.&lt;/quote&gt;&lt;p&gt;When the CPU asks for data at some memory address, the cache doesn’t search everything. It uses part of the address to decide which row (which set) to look in.&lt;/p&gt;&lt;p&gt;You can think of the address being split into three fields:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A set index: which set (row) to use&lt;/item&gt;&lt;item&gt;An offset: where inside the 64‑byte line the exact byte is&lt;/item&gt;&lt;item&gt;A remaining tag: a label that says which region of memory this line belongs to In schematic form:&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Lookup works like this:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The cache uses the index bits to select a set.&lt;/item&gt;&lt;item&gt;It checks the tags of the lines in that set to see if any match.&lt;/item&gt;&lt;item&gt;If a tag matches, that’s a cache hit. The offset tells it which byte to return from that line.&lt;/item&gt;&lt;item&gt;If none match, that’s a cache miss, the cache must fetch the corresponding line from a lower cache level or from RAM, put it in one of the slots in that set, and then serve the request.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer - hit vs miss&lt;/p&gt;&lt;lb/&gt;Hit: the thing you need is already in the cache → fast. Miss: it isn’t, so the CPU has to go further out (L2 / L3 / RAM) → slower.&lt;/quote&gt;&lt;p&gt;This “rows and a few slots per row” structure is a compromise between two extremes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You could force each address to go to exactly one slot (very simple, but leads to more conflicts).&lt;/item&gt;&lt;item&gt;You could allow each address to go anywhere (very flexible, but hard and slow to search). Set‑associative caches sit in the middle.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;6. Replacement policies&lt;/head&gt;&lt;p&gt;Each set has a fixed number of slots. Eventually, all the slots in a set will be occupied. When a miss happens and a new line needs to be inserted into that full set, the cache has to evict one of the existing lines.&lt;/p&gt;&lt;p&gt;This is exactly like your desk: if you allow only four open folders and you need a fifth, one of the four must go back into the cabinet.&lt;/p&gt;&lt;p&gt;The rule for choosing which line to evict is the replacement policy.&lt;/p&gt;&lt;p&gt;The most natural policy is called Least Recently Used (LRU): throw out the line you haven’t touched for the longest time. If you haven’t looked at a folder in hours and you’ve used the other three in the last minute, the “cold” folder is the best candidate to return to the cabinet.&lt;/p&gt;&lt;p&gt;Implementing exact LRU, however, is expensive in hardware, because it requires tracking the exact order of use within each set and updating that on every access. For small sets this is doable, for larger ones it becomes complicated.&lt;/p&gt;&lt;p&gt;To get most of the benefit at lower cost, many CPU caches use approximations of LRU.&lt;/p&gt;&lt;p&gt;One approach is tree‑based pseudo‑LRU. Think of the four slots in a set as leaves of a tiny binary tree. Each internal node stores one bit indicating which side (left or right) was used more recently. When you access a line, you walk down the tree and update these bits along the path to mark that side as “most recent”. When you need to evict, you walk the tree in the opposite direction, following the “less recent” directions to arrive at a line that is likely to be older.&lt;/p&gt;&lt;p&gt;Another simple approximation is Not Recently Used (NRU). Each line gets a single bit. When you access that line, you clear its bit to 0 (“recently used”). When you need to evict something, you look for a line whose bit is still 1 (“not recently used”). If every line’s bit is 0, you reset all bits to 1 and then choose one. It’s not perfect, but it tends to evict lines that haven’t been touched in a while.&lt;/p&gt;&lt;p&gt;The exact details differ between cache levels and CPU designs, but the purpose stays the same: find a line that is unlikely to be used again soon, and reuse that slot.&lt;/p&gt;&lt;head rend="h2"&gt;7. Cache writes and consistency&lt;/head&gt;&lt;p&gt;So far, we’ve focused on reading: either the data is in the cache (hit) or it isn’t (miss).&lt;/p&gt;&lt;p&gt;Reads are conceptually simple. Writes, on the other hand, create a new problem: once you write to data in the cache, the value in the cache and the value in RAM may no longer match.&lt;/p&gt;&lt;p&gt;If the CPU has multiple cores, things get even trickier. Imagine two workers:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Worker A pulls a customer folder, changes the phone number, and keeps the folder on their desk.&lt;/item&gt;&lt;item&gt;Worker B pulled the same folder earlier and still has the old phone number in their copy.&lt;/item&gt;&lt;item&gt;The archive room also still has the old version. Now there are three different versions of the “truth”.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In computers, making sure different cores see consistent values is the job of cache coherence protocols. Those are an entire topic on their own. At the level of a single cache, though, we already need two decisions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;On a write hit (the line is already in the cache), do we update RAM immediately or delay it?&lt;/item&gt;&lt;item&gt;On a write miss (the line is not in the cache), do we bring it into the cache first or write straight to RAM? These lead to write‑through vs write‑back and write‑allocate vs no‑write‑allocate.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;8. Write-through vs write-back&lt;/head&gt;&lt;p&gt;First, assume the cache already holds the line we want to modify. That’s a write hit.&lt;/p&gt;&lt;p&gt;There are two main strategies.&lt;/p&gt;&lt;head rend="h4"&gt;Write-through&lt;/head&gt;&lt;p&gt;With write‑through, every time you update the cached copy, you also immediately update RAM. The cache and RAM are kept in sync at all times.&lt;/p&gt;&lt;p&gt;In the office analogy, this means:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You modify the folder on your desk.&lt;/item&gt;&lt;item&gt;Then you immediately walk to the archive room and update the master copy too. This policy is simpler to reason about, but it generates a lot of traffic to RAM: every write goes all the way out.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Write-back&lt;/head&gt;&lt;p&gt;With write‑back, the cache updates only its own copy at first. RAM is updated later, usually when the cache line is evicted or explicitly flushed.&lt;/p&gt;&lt;p&gt;To keep track of which lines differ from RAM, the cache adds a dirty bit to each line. When a line is modified, its dirty bit is set to 1. When that line must be evicted, the cache checks the bit: if it is 1, the line is written back to RAM, if it is 0, the line can be dropped without writing.&lt;/p&gt;&lt;p&gt;This policy can significantly reduce the number of RAM writes, because multiple updates to the same line get combined into a single write‑back. The trade‑off is extra complexity, and the risk that if power is lost at the wrong moment, dirty lines may not yet be reflected in RAM.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer - dirty vs clean&lt;/p&gt;&lt;lb/&gt;Clean line: cache and RAM have the same data. Dirty line: cache contains newer data than RAM, RAM must be updated before the line is discarded.&lt;/quote&gt;&lt;head rend="h2"&gt;9. Write-allocate vs no-write-allocate&lt;/head&gt;&lt;p&gt;Now consider a write miss: the CPU wants to write to an address that is not currently represented in the cache.&lt;/p&gt;&lt;p&gt;There are two main options.&lt;/p&gt;&lt;head rend="h4"&gt;Write-allocate&lt;/head&gt;&lt;p&gt;With write‑allocate, the cache treats the write miss similar to a read miss:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It fetches the corresponding line from RAM into the cache.&lt;/item&gt;&lt;item&gt;It applies the write to the cached line.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If the cache uses write‑back, the line is now marked dirty.&lt;/p&gt;&lt;p&gt;This approach assumes that if you are writing to a location, you might well read or write it again soon. Once the line is in the cache, future accesses will hit.&lt;/p&gt;&lt;head rend="h4"&gt;No-write-allocate&lt;/head&gt;&lt;p&gt;With no‑write‑allocate, the cache does not bring the line in on a write miss. Instead, it sends the write directly to RAM and leaves the cache unchanged.&lt;/p&gt;&lt;p&gt;In office terms: if you only need to jot a quick note in a file you never expect to touch again, you might walk to the archive, write the change directly in the master copy, and leave it there instead of bringing it to your desk.&lt;/p&gt;&lt;p&gt;This is useful for patterns like logging or streaming where data is written once and rarely read again. Pulling that data into the cache would just evict more useful lines.&lt;/p&gt;&lt;p&gt;In practice, many CPU data caches use write‑back + write‑allocate as a default combination: it keeps recently written data closer to the core and avoids unnecessary RAM traffic. Other parts of the system might use write‑through + no‑write‑allocate where simplicity or consistency are more important than raw speed.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tiny explainer - instruction vs data cache&lt;/p&gt;&lt;lb/&gt;Instruction cache: holds the CPU instructions (the program code) it expects to run soon.&lt;lb/&gt;Data cache: holds the variables and data that code works on.&lt;lb/&gt;Separating them (at L1) avoids some types of interference.&lt;/quote&gt;&lt;head rend="h2"&gt;10. Cache hierarchy: L1, L2, L3&lt;/head&gt;&lt;p&gt;Modern CPUs use more than one cache level:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;L1: the smallest and fastest, very close to the core&lt;/item&gt;&lt;item&gt;L2: larger, a little slower, usually per core&lt;/item&gt;&lt;item&gt;L3: larger again, slower than L2, typically shared across cores&lt;/item&gt;&lt;/list&gt;&lt;p&gt;L1 is often split into a data cache and an instruction cache:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The instruction cache holds the machine code that the CPU is about to execute.&lt;/item&gt;&lt;item&gt;The data cache holds the variables and data the code is working with.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;When the CPU needs data, it checks each level in order, starting from the closest:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;L1: check the tiny, very fast cache&lt;/item&gt;&lt;item&gt;If miss: check L2&lt;/item&gt;&lt;item&gt;If miss: check L3&lt;/item&gt;&lt;item&gt;If miss: go to RAM&lt;/item&gt;&lt;/list&gt;&lt;p&gt;There is one more structural choice: how multiple levels relate to each other.&lt;/p&gt;&lt;p&gt;An inclusive cache hierarchy ensures that if a line is in L1, it is also in L2 (and possibly in L3). That is, L2 “includes” everything in L1. This can simplify some coherence operations.&lt;/p&gt;&lt;p&gt;An exclusive hierarchy enforces that a line can live in only one level at a time. If a line is in L1, it is not in L2 and vice versa. This avoids duplicating the same line in multiple caches and can increase total effective capacity.&lt;/p&gt;&lt;p&gt;A non‑inclusive, non‑exclusive hierarchy does not enforce either property strictly. Lines may or may not be duplicated between levels, depending on what the cache controller decides is best.&lt;/p&gt;&lt;p&gt;For day‑to‑day work as a programmer, the important point is that the closer levels are much faster, much smaller, and much more sensitive to how your data is laid out and accessed. If the working set of your code fits in L1 or L2 and is accessed with good locality, it will run noticeably faster than if it constantly pulls data from RAM.&lt;/p&gt;&lt;head rend="h2"&gt;11. SRAM vs DRAM&lt;/head&gt;&lt;p&gt;So far, the differences between caches and RAM have mostly been about location and structure. There is also a hardware reason: caches and RAM are built out of different memory technologies.&lt;/p&gt;&lt;p&gt;Caches use SRAM (Static Random Access Memory). Main memory uses DRAM (Dynamic Random Access Memory).&lt;/p&gt;&lt;p&gt;An SRAM cell typically uses six transistors arranged as a tiny flip‑flop that can hold a 0 or 1 as long as power is applied. Reads and writes are direct and fast. There is no need to refresh the stored value periodically.&lt;/p&gt;&lt;p&gt;A DRAM cell, by contrast, uses one transistor and one capacitor. The presence or absence of charge in the capacitor represents a bit. This design is very compact and cheap per bit, which is why you can have many gigabytes of DRAM on a system.&lt;/p&gt;&lt;p&gt;However, capacitors leak charge over time. That means DRAM cells must be refreshed regularly: each cell is periodically read and then rewritten to restore the charge. Reading a DRAM cell is also effectively destructive: the sense amplifier drains the capacitor to determine its state and then restores it. Also, DRAM is organized as a matrix of rows and columns, and accessing it involves a two‑step addressing process (selecting a row, then a column), which adds extra delay.&lt;/p&gt;&lt;p&gt;SRAM doesn’t need refresh, and its access path is simpler, so it can be much faster. The cost is area: six transistors per bit occupy much more silicon than one transistor plus one capacitor.&lt;/p&gt;&lt;p&gt;The result is a natural split:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Use small amounts of SRAM very close to the CPU (caches) for speed.&lt;/item&gt;&lt;item&gt;Use large amounts of DRAM a bit further away (RAM) for capacity.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;12. Summary&lt;/head&gt;&lt;p&gt;Viewed from a distance, CPU caches are a fairly simple idea built out with a lot of detail.&lt;/p&gt;&lt;p&gt;Your CPU has:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A tiny, very fast store of data close to the core (L1).&lt;/item&gt;&lt;item&gt;Somewhat larger, still fast stores a bit farther away (L2, L3).&lt;/item&gt;&lt;item&gt;A much larger but slower main memory (RAM).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;It tries hard to keep the data it needs now and next in the close, fast levels, and uses locality of access “recently used” and “nearby in memory” to guess what that data will be.&lt;/p&gt;&lt;p&gt;Internally, caches divide their storage into lines, sets, and ways. They track which lines are in use and evict older ones when new data has to be brought in. They handle writes carefully to keep RAM consistent, sometimes delaying updates with write‑back and sometimes going straight through with write‑through. Multiple cache levels coordinate with inclusion policies. Underneath it all, caches use SRAM and RAM uses DRAM, which is why there is a speed difference in the first place.&lt;/p&gt;&lt;p&gt;For you as a programmer or a curious reader, the core practical ideas are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Data layout matters. Keeping related data close together in memory makes good use of cache lines.&lt;/item&gt;&lt;item&gt;Access patterns matter. Walking sequentially through memory is much friendlier to caches than jumping around.&lt;/item&gt;&lt;item&gt;Doing more work while data is “hot” (already in cache) is cheaper than revisiting it later after it has been evicted.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That’s the basic story of how CPU caches work: a hierarchy of small, fast memories trying to keep your most relevant data as close to the core as possible, so your programs spend less time waiting and more time doing actual work.&lt;/p&gt;&lt;p&gt;Feedback is extremely welcomed! You can reach out to me on X @0xkato&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.0xkato.xyz/How-CPU-Caches-Work/"/><published>2025-12-13T18:00:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46256834</id><title>Are we stuck with the same Desktop UX forever? [video]</title><updated>2025-12-14T00:57:18.528117+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=1fZTOjd_bOQ"/><published>2025-12-13T18:39:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46257125</id><title>llamafile: Distribute and Run LLMs with a Single File</title><updated>2025-12-14T00:57:17.992282+00:00</updated><content>&lt;doc fingerprint="498870607f7fc6bc"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;We want to hear from you! Mozilla.ai recently adopted the llamafile project, and we're planning an approach for codebase modernization. Please share what you find most valuable about llamafile and what would make it more useful for your work. Read more via the blog and add your voice to the discussion here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;llamafile lets you distribute and run LLMs with a single file. (announcement blog post)&lt;/p&gt;
    &lt;p&gt;Our goal is to make open LLMs much more accessible to both developers and end users. We're doing that by combining llama.cpp with Cosmopolitan Libc into one framework that collapses all the complexity of LLMs down to a single-file executable (called a "llamafile") that runs locally on most computers, with no installation.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; llamafile is a Mozilla Builders project.&lt;/p&gt;
    &lt;p&gt;Download and run your first llamafile in minutes:&lt;/p&gt;
    &lt;code&gt;# Download an example model (LLaVA 1.5 7B)
curl -LO https://huggingface.co/Mozilla/llava-v1.5-7b-llamafile/resolve/main/llava-v1.5-7b-q4.llamafile

# Make it executable (macOS/Linux/BSD)
chmod +x llava-v1.5-7b-q4.llamafile

# Run it (opens browser automatically)
./llava-v1.5-7b-q4.llamafile&lt;/code&gt;
    &lt;p&gt;Windows users: Rename the file to add &lt;code&gt;.exe&lt;/code&gt; extension before running.&lt;/p&gt;
    &lt;p&gt;Check the full documentation in the docs/ folder or online at mozilla-ai.github.io/llamafile, or directly jump into one of the following subsections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Quickstart&lt;/item&gt;
      &lt;item&gt;Supported Systems&lt;/item&gt;
      &lt;item&gt;Example llamafiles&lt;/item&gt;
      &lt;item&gt;Creating llamafiles&lt;/item&gt;
      &lt;item&gt;Source installation&lt;/item&gt;
      &lt;item&gt;Technical details&lt;/item&gt;
      &lt;item&gt;Security&lt;/item&gt;
      &lt;item&gt;Troubleshooting&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While the llamafile project is Apache 2.0-licensed, our changes to llama.cpp are licensed under MIT (just like the llama.cpp project itself) so as to remain compatible and upstreamable in the future, should that be desired.&lt;/p&gt;
    &lt;p&gt;The llamafile logo on this page was generated with the assistance of DALL·E 3.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mozilla-ai/llamafile"/><published>2025-12-13T19:22:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46257339</id><title>VPN location claims don't match real traffic exits</title><updated>2025-12-14T00:57:17.567233+00:00</updated><content>&lt;doc fingerprint="be631198b0447e48"&gt;
  &lt;main&gt;
    &lt;p&gt;In a large-scale analysis of 20 popular VPNs, IPinfo found that 17 of those VPNs exit traffic from different countries than they claim. Some claim 100+ countries, but many of them point to the same handful of physical data centers in the US or Europe.&lt;/p&gt;
    &lt;p&gt;That means the majority of VPN providers we analyzed don’t route your traffic via the countries they claim to, and they claim many more countries than they actually support.&lt;/p&gt;
    &lt;p&gt;Analyzing over 150,000 exit IPs across 137 possible exit countries, and comparing what providers claim to what IPinfo measures, shows that:&lt;/p&gt;
    &lt;p&gt;This report walks through what we saw across VPN and IP data providers, provides a closer look at two particularly interesting countries, explores why measurement-based IP data matters if you care where your traffic really goes, and shares how we ran the investigation.&lt;/p&gt;
    &lt;p&gt;Here is the overlap between the number of listed countries each VPN provider claims to offer versus the countries with real VPN traffic that we measured — lower percentages indicate providers whose claimed lists best match our data:&lt;/p&gt;
    &lt;p&gt;It's important to note that we used the most commonly and widely supported technologies in this research, to make comparison between providers as fair as possible while giving us significant data to analyze, so this will not be the full coverage for each provider.&lt;/p&gt;
    &lt;p&gt;These are some of the most visible names in the market. They also tend to have very long country lists on their websites. Notably, three well-known providers had zero mismatches across all the countries we tested: Mullvad, IVPN, and Windscribe.&lt;/p&gt;
    &lt;p&gt;Country mismatches doesn’t automatically mean some providers offer “bad VPNs,” but it does mean that if you’re choosing a VPN because it claims “100+ countries,” you should know that a significant share of those flags may be labels, or virtual locations.&lt;/p&gt;
    &lt;p&gt;When a VPN lets you connect to, for example, “Bahamas” or “Somalia,” that doesn’t always mean traffic routes through there. In many cases, it’s somewhere entirely different, like Miami or London, but presented as if traffic is in the country you picked.&lt;/p&gt;
    &lt;p&gt;This setup is known as a virtual location:&lt;/p&gt;
    &lt;p&gt;The problem? Without active network measurement, most IP datasets will rely on what the IP’s owner told the internet registry or published in WHOIS/geofeeds: a self-reported country tag. If that record is wrong or outdated, the mistake spreads everywhere. That’s where IPinfo’s ProbeNet comes in: by running live RTT tests from 1,200+ points of presence worldwide, we anchor each IP to its real-world location, not just its declared one.&lt;/p&gt;
    &lt;p&gt;Across the dataset, we found 97 countries where at least one VPN brand only ever appeared as virtual or unmeasurable in our data. In other words, for a noticeable slice of the world map, some “locations” in VPNs never show up as true exits in our measurements.&lt;/p&gt;
    &lt;p&gt;We also found 38 countries where every mention behaved this way: at least one VPN claimed them, but none ever produced a stable, measurable exit in that country in our sample.&lt;/p&gt;
    &lt;p&gt;You can think of these 38 as the “unmeasurable” countries in this study – places that exist in server lists, config files, and IP geofeeds, but never once appeared as the actual exit country in our measurements. They’re not randomly scattered – they cluster in specific parts of the map. By region, that includes:&lt;/p&gt;
    &lt;p&gt;This doesn’t prove there is zero VPN infrastructure in those countries globally. It does show that, across the providers and locations we measured, the dominant pattern is to serve those locations from elsewhere. Here are three of the most interesting examples of how this looks at the IP level.&lt;/p&gt;
    &lt;p&gt;To make this concrete, let’s look at three countries where every provider in our dataset turned out to be virtual: Bahamas, and Somalia.&lt;/p&gt;
    &lt;p&gt;In our measurements, five providers offered locations labeled as “Bahamas”: NordVPN, ExpressVPN, Private Internet Access, FastVPN, and IPVanish.&lt;/p&gt;
    &lt;p&gt;For all of them, measured traffic was in the United States, usually with sub-millisecond RTT to US probes.&lt;/p&gt;
    &lt;p&gt;Somalia appears in our sample for only two providers: NordVPN and ProtonVPN.&lt;/p&gt;
    &lt;p&gt;Both label Mogadishu explicitly in their naming, but these RTTs are exactly what you’d expect for traffic in Western Europe, and completely inconsistent with traffic in East Africa. Both providers go out of their way in the labels (e.g. “SO, Mogadishu”), but the actual traffic is in Nice and London, not Somalia.&lt;/p&gt;
    &lt;p&gt;So far, we’ve talked about VPN claims versus our measurements. But other IP data providers don’t run active RTT tests. They rely on self-declared IP data sources, and often assume that if an IP is tagged as “Country X,” it must actually be there.&lt;/p&gt;
    &lt;p&gt;In these cases, the IP legacy datasets typically “follow” the VPN provider’s story: if the VPN markets the endpoint as Country X, the legacy IP dataset also places it in Country X.&lt;/p&gt;
    &lt;p&gt;To quantify that, we looked at 736 VPN exits where ProbeNet’s measured country disagreed with one or more widely used legacy IP datasets.&lt;/p&gt;
    &lt;p&gt;We then compared the country IPinfo's ProbeNet measured (backed by RTT and routing) with the country reported by these other IP datasets and computed the distance between them. The gaps are large:&lt;/p&gt;
    &lt;p&gt;The median error between ProbeNet and the legacy datasets was roughly 3,100 km. On the ProbeNet side, we have strong latency evidence that our measured country is the right one:&lt;/p&gt;
    &lt;p&gt;That’s what you expect when traffic is genuinely in that country, not thousands of kilometers away.&lt;/p&gt;
    &lt;p&gt;This behavior is much more tangible if you can see it on a single IP.&lt;/p&gt;
    &lt;p&gt;Here's one VPN exit IP where ProbeNet places the server in the United Kingdom, backed by sub-millisecond RTT from local probes, while other widely used legacy IP datasets place the same IP in Mauritius, 9,691 kilometers away.&lt;/p&gt;
    &lt;p&gt;🇬🇧 United Kingdom vs 🇲🇺 Mauritius (ProtonVPN)&lt;/p&gt;
    &lt;p&gt;If you want to check this yourself, you can plug it into a public measurement tool like https://ping.sx/ and run pings or traceroutes from different regions. Tools like this one provide a clear visual for where latency is lowest.&lt;/p&gt;
    &lt;p&gt;ProbeNet uses the same basic idea, but at a different scale: we maintain a network of 1,200+ points of presence (PoPs) around the world, so we can usually get even closer to the real physical location than public tools with smaller networks.&lt;/p&gt;
    &lt;p&gt;If you’d like to play with more real IPs (not necessarily VPNs) where ProbeNet and IPinfo get the country right and other datasets don’t, you can find a fuller set of examples on our IP geolocation accuracy page.&lt;/p&gt;
    &lt;p&gt;It’s worth separating technical reasons from trust issues. There are technical reasons to use virtual or hubbed infrastructure:&lt;/p&gt;
    &lt;p&gt;From this perspective, a virtual location can be a reasonable compromise: you get a regional IP and content unblocking without the downsides of hosting in a fragile environment.&lt;/p&gt;
    &lt;p&gt;Three things change the picture:&lt;/p&gt;
    &lt;p&gt;That last point leads directly into the IP data problem that we are focused on solving.&lt;/p&gt;
    &lt;p&gt;If you’re a VPN user, here are some practical takeaways from this work:&lt;/p&gt;
    &lt;p&gt;Ultimately, this isn’t an argument against VPNs, or even against virtual locations. It’s an argument for honesty and evidence. If a VPN provider wants you to trust that map of flags, they should be willing, and able, to show that it matches the real network underneath.&lt;/p&gt;
    &lt;p&gt;Most legacy IP data providers rely on regional internet registry (RIR) allocation data and heuristics around routing and address blocks. These providers will often accept self-declared data like customer feedback, corrections, and geofeeds, without a clear way to verify them.&lt;/p&gt;
    &lt;p&gt;IPinfo takes a measurement-first approach:&lt;/p&gt;
    &lt;p&gt;This measurement-first approach is unique in the IP data space. Once we realized how much inaccuracy came from self-declared data, we started investing heavily in research and building ProbeNet to use active measurements at scale. Our goal is to make IP data as evidence-based as possible, verifying with observation on how the internet actually behaves.&lt;/p&gt;
    &lt;p&gt;We approached this VPN investigation the way a skeptical but well-equipped user would: start from the VPNs’ own claims, then test them.&lt;/p&gt;
    &lt;p&gt;For each of the 20 VPN providers, we pulled together three kinds of data:&lt;/p&gt;
    &lt;p&gt;Next, we used IPinfo infrastructure and ProbeNet to dial into those locations and watch what actually happens:&lt;/p&gt;
    &lt;p&gt;Now we had two views for each location:&lt;/p&gt;
    &lt;p&gt;For each location where a country was clearly specified, we asked a very simple question: Does the expected country match the measured country?&lt;/p&gt;
    &lt;p&gt;If yes, we counted it as a match. If not, it became a mismatch: a location where the app says one country, but the traffic exits somewhere else.&lt;/p&gt;
    &lt;p&gt;We deliberately used a very narrow definition of “mismatch.” For a location to be counted, two things had to be true: the provider had to clearly claim a specific country (on their website, in their app, or in configs), and we had direct active measurements from ProbeNet for the exit IPs behind that location.&lt;/p&gt;
    &lt;p&gt;We ignored any locations where the marketing was ambiguous, where we hadn’t measured the exit directly, or where we only had weaker hints like hostname strings, registry data, or third-party IP databases. Those signals can be useful and true, but we wanted our numbers to be as hard-to-argue-with as possible.&lt;/p&gt;
    &lt;p&gt;The result is that the mismatch rates we show here are conservative. With a looser methodology that also leaned on those additional hints, the numbers would almost certainly be higher, not lower.&lt;/p&gt;
    &lt;p&gt;Ben founded IPinfo in 2013 with the goal of providing reliable, easily accessible IP address data. As IPinfo co-CEO, he is committed to constantly improving that data and how customers can use it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ipinfo.io/blog/vpn-location-mismatch-report"/><published>2025-12-13T19:46:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46257599</id><title>The Rise of Computer Games, Part I: Adventure</title><updated>2025-12-14T00:57:17.275850+00:00</updated><content>&lt;doc fingerprint="a702114962a38afd"&gt;
  &lt;main&gt;
    &lt;p&gt;Author’s note: I originally intended for this post to cover adventure games, computer role-playing games, wargames and other simulations, a brief look at the home video game market, and finally the rise of hybrids that fused home video game systems with personal computers. In the grand scheme of the story about personal computers that I am trying to tell, it probably does not make sense to lavish nearly 7,000 words on early adventure games alone, but it’s a topic of personal interest to me and the tale grew in the telling.&lt;/p&gt;
    &lt;p&gt;Play was central to the formation of personal computer culture. For the early hobbyists who were fascinated by the guts of the machine, the computer was a plaything in and of itself. Many of those who joined the hobby in 1975 or 1976 did so because of games: they had experience with the extensive BASIC game culture that circulated in the time-sharing systems of universities, high schools, and even corporations, and wanted to keep playing at home.&lt;/p&gt;
    &lt;p&gt;Even after the rise of commercial personal computer software, when the first truly useful applications began appearing, games remained by far the most popular software category (counting by number of titles produced and number of units sold, although not by dollar value). One 1980 catalog of Apple II software, for example, lists 265 titles, of which roughly two-thirds are games, from Ack-Ack (an anti-aircraft target shooter)to Wipe Off (a Breakout clone). The rest of the catalog comprises demos, educational programs, and a smattering of business software. Whatever they might say about the practical value of the personal computer, buyers had an evident hunger for games.[1]&lt;/p&gt;
    &lt;head rend="h3"&gt;The Early Games and Their Market&lt;/head&gt;
    &lt;p&gt;Computer owners got their hands on games in one of three ways. In the early years, the most common means would be simply copying a paper or cassette tape from a friend or colleague, whether with the permission of the original author or not. In the early years, most hobbyists treated game software as a commons to be freely shared, just as it had been in the time-sharing culture through cooperatives like DECUS. This peer-to-peer copying would never entirely go away, despite the commercialization of game software and various schemes by publishers to try to prevent it.&lt;/p&gt;
    &lt;p&gt;Many magazines and books also published “type-ins,” complete computer programs (almost always written in BASIC) intended to be manually entered at the keyboard (and then saved to tape or disk), and these, too, were most often games. Dave Ahl’s BASIC Computer Games (first published in 1973 by Digital Equipment Corporation), a collection of over 100 type-ins, reputedly sold one million copies by 1979. Though type-in publication continued through the 1980s, the inherent limits on the length of such programs (only the most dedicated would tackle a type-in that was more than a few hundred lines long) and their reliance on the universality of BASIC (rather than more performant compiled languages) meant that their significance waned as the sophistication of the game market increased. They could serve as fun demos or educational tools for learning to code, but could not compare to similar games available commercially.[2]&lt;/p&gt;
    &lt;p&gt;Finally, of course, there were the commercial titles offered by software publishers. The game business began in the same way as the personal computer hardware business: with hobby-entrepreneurs selling their creations to fellow hobbyists. In July 1976, for example, D.E. Hipps of Miami, Florida offered a Star Trek game written for MicroSoft’s Altair BASIC for $10 (no one at this stage of the industry paid any attention to niceties such as licensing agreements for the use of the Star Trek name). No common standard data storage standard existed; hobbyists employed a mix of paper teletype tapes, cassette storage, and (for the most extravagant) floppy disks. So Hipps opted to distribute his game as printed source code: a type-in! SCELBI (creators of one of the early, pre-Altair hobby computers), offered another Star Trek variant called Galaxy in the same form. By the late 1970s, the convergence of the industry on a small number of popular storage standards (with CP/M dominant) resolved this problem, and most games were distributed in plastic baggies containing instructions and a cassette or floppy disk.[3]&lt;/p&gt;
    &lt;p&gt;It didn’t take long for other entrepreneurs to see a business opportunity in making it easier for software authors to publish their games. It took some time for clear business models and market verticals to emerge. No categorial distinction existed between publishers of games and publishers of utility and business software prior to 1980: Personal Software’s first big hit was MicroChess, followed by VisiCalc, followed by (as we’ll soon see) Zork. Programma International’s founder began as a hoarder of Apple II software, much of it acquired from copies unauthorized by the original author, then turned legitimate to sell those authors’ software instead. Softape tried selling bundles of software by subscription, and then started its own newsletter for subscribers, Softalk.&lt;/p&gt;
    &lt;p&gt;Some magazines went the other way around: Softside magazine (located the next town over from BYTE’s Peterborough, New Hampshire headquarters) created The Software Exchange (TSE), while Dave Ahl’s Creative Computing set up a label called Sensational Software. Type-ins printed in the magazines became a gateway drug to more convenient (and often more complex and interesting) software available for sale on cassette or diskette.[4]&lt;/p&gt;
    &lt;p&gt;Figure 21: Creative Computing heavily advertised the Sensational Software brand in the pages of the magazine, as in this July 1980 example describing some of their most popular hits and offering a free catalog of their full offering of 400 titles.&lt;/p&gt;
    &lt;p&gt;The early personal computer game culture imitated what came before it. The boundary between mini- and microcomputer culture was permeated by thousands who used time-sharing systems at work or school and then went home to a hobby computer. Prior to 1977, a game written for a personal computer was almost invariably based on a game drawn from the other side of that boundary.&lt;/p&gt;
    &lt;p&gt;Barring a few exceptions (such as the PLATO system available at some universities), users interacted with such computer systems through teletypes or video teletypes that alternated sending and receiving text. So, the resulting games were turn-based, purely textual, and relied on strategy and calculation (or pure luck) to win, not timing and reaction speed. These textual games suited the early hobbyists perfectly, since almost all of their computers also had text-only interfaces, whether mechanical teletypes or video displays like the TV Typewriter.&lt;/p&gt;
    &lt;p&gt;Other than simple quizzes, demos, and guessing games, popular titles included simulations such as Hammurabi, Civil War and Lunar Lander; statistical recreations of sports contests (baseball, basketball, golf, etc.); or classic games or puzzles from the physical world, like checkers, Yahtzee, and the towers of Hanoi. By far the most popular by far, however, judging by the number of variations published and references in hobby magazines, were descendants of Mike Mayfield’s 1971 Star Trek, a strategic game of galactic war against the Klingons.[5]&lt;/p&gt;
    &lt;p&gt;Figure 22:&lt;/p&gt;
    &lt;p&gt;Some early personal computers, however, had video controllers with built-in memory, which allowed for more sophisticated interfaces than the simple back-and-forth exchanges of a teletype. Processor Technology, whose VDM-1 display interface could paint characters at arbitrary points on the screen, sold real-time textual games by Steve Dompier like Trek-80 (released in 1976, despite the name). Its interface (including a galactic sector map made of text characters and readouts of the Enterprise’s weapon and shield status) updated in real-time in response to player and (simulated) enemy actions, rather than scrolling by one turn at a time. Cromemco, maker of the Dazzler, an Altair-compatible graphics board, offered the only personal computer games to use pixel graphics prior to the Apple II, starting with a version of the seminal Spacewar in early 1977. They followed with a suite of similar games such as Tankwar and Dogfight.[6]&lt;/p&gt;
    &lt;p&gt;After 1977, when computers with graphical displays became more widely available (especially the full-color Apple II), computer games tapped a new vein of inspiration (and imitation): arcade games. Originally commercialized by Atari and its imitators as standalone arcade cabinets in the early 1970s, then moving into homes by the mid-1970s, these games were typically real-time and focused on action. Relatively cheap and easy-to-make, and relatively disposable to the user (few took more than a few minutes to play a complete game), computer action games proliferated by the hundreds and thousands, many of them direct or near clones of pre-existing arcade or home video games.&lt;/p&gt;
    &lt;p&gt;By 1980, however, there were major innovations that set personal computer games apart from other game media. In-depth simulations, expansive adventures that took hours to solve, and dungeon crawls teeming with a variety of monsters, treasures, and traps provided immersive experiences that the action-oriented video game consoles, did not, and (given their limited memory and storage capacity) could not provide. Once combined with full-color, bitmapped graphics, these games also surpassed anything previously available on their time-sharing predecessors. The era of imitation was definitively over.&lt;/p&gt;
    &lt;head rend="h3"&gt;Adventure&lt;/head&gt;
    &lt;p&gt;For several years of my childhood, for reasons that I no longer recall, our family’s Apple IIe computer, equipped with a green-and-black monochrome monitor, resided in my bedroom. Though much of my autobiographical memory is quite hazy, I can clearly remember each of the Apple II games we owned, each with its own 5 ¼-inch-square floppy disk: Syzygy (a space shooter in the vein of Asteroids), One on One: Dr. J vs. Larry Bird, Winter Games, and Arcticfox (a sci-fi tank simulator with wireframe graphics).&lt;/p&gt;
    &lt;p&gt;But the game that truly captured my imagination, the game whose opening sequence and imagery remain etched (monochromatically) in my mind, was King’s Quest II: Romancing the Throne, a 1985 title by Sierra On-Line. The forty-nine-screen, hand-drawn fairy tale kingdom that you explore in the game (via your avatar, King Graham) felt like a vast world of endless possibility compared to the cramped half-court of One on One, the endlessly repeating monotony of a biathlon course in Winter Games, or the sterile polygonal landscape of Arcticfox’s Antarctica. That open-ended feeling was enhanced by the lure of hidden secrets just out of reach, and a freeform text interface that accepted English commands like “THROW APPLE” (though only a tiny subset of the commands you could imagine would actually work). Despite its limitations and many, many frustrations (at age seven or eight, with no hint book and no Internet walkthroughs, I certainly never came close to completing it), it made me feel that I was truly experiencing an adventure.&lt;/p&gt;
    &lt;p&gt;The adventure game genre originated in a freely shared, text-driven game created in the time-sharing world. The game, which I will call Adventure (it is variously called Colossal Cave Adventure, Colossal Cave, Adventure, or simply ADVENT, after the game’s PDP-10 file name)challenged players to find five treasures within a cave complex by navigating a maze, solving puzzles, and defeating a band of axe-wielding dwarves Its author was Will Crowther, a programmer at Bolt, Beranek and Newman (BBN), where he had written core infrastructural software for ARPANET, the first nationwide computer network.&lt;/p&gt;
    &lt;p&gt;In 1975, Crowther went through a painful divorce. He had always enjoyed playing games with his school age daughters, so he began crafting a game on the company’s DEC PDP-10 to help him stay connected with them. Crowther copied the physical structure of Adventure’s cave directly from a portion of the Mammoth complex in Kentucky. (He had met his wife through caving, and they had explored Mammoth together, so the game was also, in a sense, a means of staying connected to his former, married life.) It is probable (though not certain) that Crowther also drew some inspiration from a popular 1973 time-sharing game called Hunt the Wumpus, which required users to use textual clues to find and kill a Wumpus hidden in a system of caves without falling into a pit. But the conceptual structure of Adventure (delving into the earth to find treasure and magical artifacts in the face of devious obstacles and armed foes) came from a new game of pencil, paper, and imagination that Crowther was playing with some of his BBN friends, called Dungeons and Dragons.[7]&lt;/p&gt;
    &lt;p&gt;In Crowther’s words:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…the caving had stopped, because that had become awkward, so I decided I would fool around and write a program that was a re-creation in fantasy of my caving, and also would be a game for the kids, and perhaps had some aspects of the Dungeons and Dragons that I had been playing.[8]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Just as in the later King’s Quest II, the player used simple verb-noun commands (such as “TAKE LAMP”) to interact with the world, but lacking a graphical screen with a visible avatar, he or she also used text commands to move about the world, from one room of the cave to the next (e.g., “SOUTH” or “EAST”). Crowther showed the game off to his D&amp;amp;D buddies and his daughters, then took a new job in California, and forgot about it.[9]&lt;/p&gt;
    &lt;p&gt;Time-sharing games had once propagated gradually from computer to computer via collectives like the Digital Equipment Computer Users’ Society or colleagues and friends mailing paper tapes to one another. But BBN was on the ARPANET, and Crowther had put his game on a public directory in the BBN computer. From there, someone copied it across the network to a computer in a Stanford lab, where a graduate student, Don Woods, found it in early 1977.&lt;/p&gt;
    &lt;p&gt;Fascinated by Crowther’s game, Woods contacted him for the FORTRAN source code, and set about expanding it. He increased the scope by adding more rooms, more puzzles, more foes, and more ways to interact with the world; but he also added the ability to save your progress and a point-tracking system with a final objective: to find fifteen treasures and return them to the starting location. Woods’ larger, more polished version of Adventure spread rapidly across the time-sharing world, and became an obsession for some, keeping them at the office past midnight in search of that last treasure. (One of Woods’ additions was a setting to allow admins to disable the game during working hours.)[10]&lt;/p&gt;
    &lt;head rend="h3"&gt;Adventureland&lt;/head&gt;
    &lt;p&gt;A frizzy-haired Florida man, Scott Adams, was the first to commercialize a version of Adventure for the personal computer. He had first fallen in love with computers on a time-sharing terminal at his Miami high school in the late 1960s. He went on to earn a computer science degree and by the late 1970s, was working as a programmer at telecom manufacturer Stromberg-Carlson. On the side he had become an avid home computer hobbyist, purchasing a Sphere computer in 1975 and then a TRS-80 in 1977. Shortly thereafter he discovered Adventure on the company time-sharing system and, like many before and after, could not quit playing until he had beaten it.&lt;/p&gt;
    &lt;p&gt;Adams decided that it would be an interesting challenge to build something similar for the TRS-80. It would have to be much smaller to fit in the sixteen kilobytes of memory he had available. The Crowther-Woods Adventure contained 140 distinct locations and ran to eighty kilobytes of (uncompiled) FORTRAN and fifty-four kilobytes of data for the game text. Adams’ Adventureland was considerably smaller, with fewer than thirty-five locations—not necessarily to the detriment of gameplay; for example, the cutting lopped off most of Adventure’s huge and torturous mazes.[11]&lt;/p&gt;
    &lt;p&gt;Adams’ local TRS-80 buddies were impressed enough with his game that he decided to sell it through both The TRS-80 Software Exchange and Creative Computing, who offered it on cassette for $24.95 and $14.95, respectively, in their January 1979 magazine issues. He followed up with a whole series of games, starting with Pirate Adventure, and ported the games from the TRS-80 to other popular computer platforms. His wife Alexis joined the venture as a business manager and game designer, co-authoring Mystery Fun House and Voodoo Castle.[12]&lt;/p&gt;
    &lt;p&gt;The adventure game genre is often criticized for absurd and unfair puzzles, which can be guessed at only through trial-and-error, and tedious mazes or other navigational obfuscations. These early games from circa 1980 are among the worst offenders. In Adventureland, for example, a “very thin” black bear blocks your way, and the only way to get past it is to “yell” at it. Feeding this apparently hungry bear honey will prevent you from completing the game, because the honey is one of the treasures you must collect. You could easily get to state in these games where you have lost the game without knowing it.[13]&lt;/p&gt;
    &lt;p&gt;But these criticisms are retrospective: the contemporary press and the buying public lapped up the Adams’ adventures and all of their imitators. We have to remember that the appeal of this genre lay in getting immersed (one might say “lost”) in the game for hours every evening, clawing your way forward towards ultimate triumph for weeks, or even months, on end. In a market full of arcade-like games that offered the convenient but shallow fun of a bag of potato chips, adventure games provided a rich and fulfilling meal for the imagination. As one lover of the genre put it:&lt;/p&gt;
    &lt;p&gt;Adventure is the product of imagination appealing to imagination. It is not just the puzzle, or the theme, or the nonplayer characters and their personalities. It is a verbal tapestry of interwoven phrases that whisk you away to magical kingdoms of the mind. The computer becomes a tool of reaching that conveys you where it will. You go along eagerly, breathlessly, awaiting what comes next.[14]&lt;/p&gt;
    &lt;p&gt;The catch was that this delicacy was consumable only once: a solved adventure game was no more interesting to revisit than a solved crossword puzzle. So, they had to provide a challenge: no one wanted to pay $24.95 for a game on the way home from work and then breeze through it before bedtime. A game that was very fair would also risk being seen as a waste of money. Despite improvements in design in future years that would banish some of the worst practices of the genre, adventure games remained trapped on the horns of this dilemma.[15]&lt;/p&gt;
    &lt;head rend="h3"&gt;Zork&lt;/head&gt;
    &lt;p&gt;The Adams’ “Adventure” line made them wealthy enough to build a faux-castle outside Orlando, and kicked off one of the most popular computer game genres of the 1980s. By late 1980, half-a-dozen other companies were putting out personal computer adventure games, from The Programmer’s Guild to Mad Hatter Software, as well as a version of Crowther-Woods Adventure put out by Microsoft. But they are overshadowed in the historical record by a competitor that subsequently dominated both sales of and critical attention to text adventure games. It began at MIT. In the spring of 1977, the Crowther-Woods Adventure arrived over the ARPANET at the PDP-10 at the Laboratory for Computer Science (LCS), and sank its claws into its employees. Impressed by what Crowther and Woods had done, but convinced that it could be made even better, a group of LCS staff set out in May 1977 to one-up Adventure.[16]&lt;/p&gt;
    &lt;p&gt;Dave Lebling, who had already worked on several games (including Maze, the first first-person shooter game), kicked off the project. Lebling played Dungeons and Dragons in the same Cambridge D&amp;amp;D group as Crowther had (though not at the same time), and based the game’s combat system on the tabletop game. Then Marc Blank, Tim Anderson, and Bruce Daniels filled in most of the core structure of the program. They gave it the place-holder name of Zork (a term used as an inside-joke expletive at LCS, as in “why won’t this zorking thing work”), which ended up sticking permanently. The game reached its completed state in early 1979, by which point it greatly exceeded the original Adventure in scale, with 191 rooms and 211 items, occupying a full megabyte of memory.[17]&lt;/p&gt;
    &lt;p&gt;Coded in a LISP-descendant called MUDDLE or MDL, Zork had an elegant design that encapsulated all the information about the possible interactions of each room and item in a single block of code and data, making it much easier to extend than Adventure. It also had a much richer text interface: both Adventure and AdventureLand accepted only “verb noun” commands, but Zork also allowed for conjunctions and prepositions (for example, “TAKE SWORD AND LANTERN FROM SACK”).Though aping the basic tropes of Adventure (a small overland area leading to an underground treasure-hunt), its more complex architecture allowed for a richer and more clever set of puzzles.[18]&lt;/p&gt;
    &lt;p&gt;In the spring of 1979, several key staff members of LCS were poised to leave MIT. Their supervisor, Al Vezza, proposed to keep the band together by forming a company. Incorporated in June as Infocom, its new employees and shareholders included Lebling, Blank, and Anderson.&lt;/p&gt;
    &lt;p&gt;While the various partners mulled what exactly to do with their new business, Blank and a fellow LCS alum, Joel Berez, figured out how to cram Zork onto a microcomputer: they cut the number of rooms and items in the game in half and removed all the features of MDL not needed for the game, creating an interpreter for a simpler language they called Zork Implementation Language (ZIL). The resulting program occupied just seventy-seven kilobytes. To get this to fit into a microcomputer memory half that size, they had one last trick: a virtual memory system built into the interpreter, to swap chunks of the program on and off the disk as needed (typical floppy disk capacities at the time were over 100 kilobytes, and continued to grow). This meant that Zork could only run off of a floppy drive (whose rapidly spinning disk could sync to a new data location in a fraction of a second and supply data at fifteen kilobytes per second), never a cassette (which took a minute or more to fully unwind or rewind and supplied data at 300 bits per second). Or, to put it another way, the growing market prevalence of affordable floppy drives made larger personal computer adventure games feasible: it took about twenty minutes to load a Scott Adams adventure game from tape.[19]&lt;/p&gt;
    &lt;p&gt;In late 1979, Blank and Berez convinced a reluctant Vezza (who wanted to get into business software) to make a microcomputer Zork Infocom’s first product. They initially published through Personal Software, co-owned by MIT’s own Dan Fylstra, which had just recently released VisiCalc. But after VisiCalc’s smash success, Fylstra no longer wanted to deal in mere games, so Infocom became its own publisher for subsequent games—including Zork II and III, built from the remaining unused material from the original PDP-10 Zork.&lt;/p&gt;
    &lt;p&gt;Zork became available in December 1980 and sold 10,000 units in 1981, mostly on the Apple II, despite an eye-watering price of $39.95, at a time when most games cost fifteen to twenty-five dollars. Then, astonishingly, in an industry typically characterized by ephemerality and obsolescence, sales continued to grow, year after year. They peaked in 1984 with over 150,000 copies sold. No doubt Zork’s self-referential humor, its restrained but clever marketing, and the high quality of the game itself (certainly the most well-crafted adventure game to date) all helped to sell the game.[20]&lt;/p&gt;
    &lt;p&gt;But many sales also must have arisen from the startling impression given by sitting down in a store (or at a friend’s house) to interact with this remarkable piece of software. Bob Liddil, reviewing Zork for BYTE magazine, pointed to the fluency of the parser as the element that first pulled him in:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I was eager to test Zork’s biggest selling point, intelligent input (ie: its ability to accept free-form instructions). I typed “OPEN THE BAG AND GET THE LUNCH,” in reference to a brown paper sack inside the house. The computer complied. There was water and food, so I typed “EAT THE LUNCH AND DRINK THE WATER,” to which the computer responded with gratitude for satisfying its hunger and thirst. I was hooked.[21]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The game seemed to understand the user and to have an appropriate answer (or a witty retort) ready for everything they might try, from expletives (“FUCK &amp;gt; SUCH LANGUAGE IN A HIGH-CLASS ESTABLISHMENT LIKE THIS!”) to attempts to outwit the command system (“FIND HANDS &amp;gt; WITHIN SIX FEET OF YOUR HEAD, ASSUMING YOU HAVEN’T LEFT THAT SOMEWHERE.”), to questions about the imaginary world in which the game is played (“WHAT IS A ZORKMID? &amp;gt; THE ZORKMID IS THE UNIT OF CURRENCY OF THE GREAT UNDERGROUND EMPIRE.”) Along with VisiCalc and WordStar, Zork functioned not just as a piece of software that did something, but also as an existence proof (for the owner and for skeptical friends and family) that the microcomputer could be more than merely a toy version of a real computer.[22]&lt;/p&gt;
    &lt;p&gt;Zork sales finally fell off in the mid-1980s, not because new text adventure games had surpassed it (Infocom continued to rule that particular roost, and Zork remained their flagship), but because of the steady improvement in personal computer graphics and the corresponding ascendancy of graphical games over textual ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mystery House&lt;/head&gt;
    &lt;p&gt;The first graphical adventure game actually appeared several months before Zork: On-Line Systems’ Mystery House, created by Ken and Roberta Williams. Unlike Scott Adams and most of the early personal computer hobbyists, Ken Williams got into computers for money, not love. Raised in greater Los Angeles in an unhappy home, he was a driven and impatient young man, and graduated high school at just sixteen. Roberta Heuer, a dreamy young woman whom Williams met through a double date, was impressed enough by his intelligence and ambition to give in to his insistence that they marry in 1972, while they were both still teenagers.&lt;/p&gt;
    &lt;p&gt;With the expectation of children to come, Ken abandoned his physics program at Cal Poly Pomona for a more immediately lucrative career in data processing. His father-in-law helped him get a loan to attend Control Data Corporation’s training school (the Control Data Institute), and from there he went on to a series of positions working on “big iron” batch-processing systems, constantly bouncing from job to job and home to home in search of better opportunities and a fatter pay check. He and Roberta wanted a bigger house and more creature comforts, but most of all they dreamed of an early retirement to a life out-of-doors, far from the city.[23]&lt;/p&gt;
    &lt;p&gt;The Williamses took no notice of microcomputers until Ken and one of his co-workers, Robert Leff, concocted a way to make money off of them: selling fellow programmers a microcomputer implementation of FORTRAN, one of the most popular data processing languages. Not only could this venture make him and Roberta still richer (always a key consideration), it could free them to finally move away from the traffic and grind of Los Angeles and to live out their dream of rural life. Initially Ken planned to write FORTRAN for the TRS-80, but he redirected his energies to the more capable Apple II after he and Roberta got themselves one for a mutual Christmas present.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Roberta had gotten hooked on adventure games. Ken had an electromechanical teletype terminal in their home for one of his consulting jobs, and connected it to a computer with the Crowther-Woods Adventure available to play. He showed the game off to Roberta. For Ken it was a curiosity, but for Roberta it became an obsession: she would not quit until she had beaten the game, weeks later. Ken brought home a borrowed TRS-80 and cassette tapes for the Scott Adams adventure series, and she flew through those, too. Soon she had an idea for a game of her own: instead of a treasure hunt, it would be a murder mystery; a mix of Clue and Ten Little Indians set in a creepy old Victorian house.&lt;/p&gt;
    &lt;p&gt;She insisted that Ken help her create it, and, after putting her off several times, he finally relented. Roberta wanted to add pictures of each room as a way to make this new game better than what came before, taking advantage of the Apple II’s 280×192 pixel high-resolution graphics mode. Because storing dozens of bitmapped images on a floppy disk would be impossible, Ken bought a VersaWriter accessory, a tablet with a movable arm that let Roberta capture the (x, y) position of each line endpoint in her pictures and store them into the computer. He wrote code to re-create the pictures from these coordinates by drawing the lines at runtime.[24]&lt;/p&gt;
    &lt;p&gt;Like Crowther and Adams, Ken split the data tables apart from the code that interpreted them. This allowed Roberta to work out all of the information about the rooms in the game, the items they contain, and the actions the player can perform, without needing to write any code. This division of labor between programming and design, quite novel to computer game software, came about from the accident of Roberta’s limited technical skills (she had worked briefly as COBOL programmer, at Ken’s insistence) and Ken’s lack of interest in the game: he was still focused on launching Apple FORTRAN.[25]&lt;/p&gt;
    &lt;p&gt;Then, while visiting local computer stores to pitch his computer language, Ken demoed an early version of Roberta’s game and everyone in the store gathered around to see it. The owners asked when they could have copies to sell. Ken realized he was backing the wrong horse: it was Roberta’s side project that would make them rich, not FORTRAN. Moreover, rather than give up a cut to a publisher like Programma International, they would take all the revenue for themselves, by publishing the game through the company name he had already registered for his never-to-be-released FORTRAN, On-Line Systems. On top of that, they could make even more money by distributing games into the stores they were already visiting on behalf of other software authors, like Scott Adams’ Florida-based Adventure International. Eventually unable to manage both publishing and distribution, he convinced his former colleague and erstwhile FORTRAN partner, Robert Leff, to buy out the distribution business, which grew into the industry behemoth Softsel.[26]&lt;/p&gt;
    &lt;p&gt;After a month of development on nights and weekends (Ken’s pace was manic: in his memoir he writes that he always strove to be a “Triple-A” player, and his brother called him a “chronic workaholic”), the Williams’ started selling Mystery House in May 1980. It required forty-eight kilobytes of memory, but with chip prices falling continuously, this was not so stringent a requirement as it had been even a year before.&lt;/p&gt;
    &lt;p&gt;The game’s simplistic “mystery” ends with the player gunning down the de facto murderer: the only living character to be found in a houseful of victims. The puzzles are among the more poorly clued and arbitrary to be found in a genre full of such frustrations. But for adventure-starved gamers of the time it was enchanting: not only could they witness the virtual world which they were navigating, it actually changed in response to their actions (picking up an object, like the taunting note that kicked off the murders, would remove it from the scene). Roberta’s drawings, crude and child-like as they certainly are, gave the game a visual appeal that drew in new buyers, and more than justified its price of $24.95.[27]&lt;/p&gt;
    &lt;p&gt;That summer Ken and Roberta were pulling in $30,000 a month and shopping for a house far from Los Angeles, in Coarsegold, California, nestled in the foothills of the Sierra Nevada near Yosemite National Park. On-Line Systems became Sierra On-Line. A few months later a second “High-Res Adventure” followed, The Wizard and the Princess, which added visually-stunning color to Mystery House’s line drawings: Ken used dithering techniques to make the six colors available in high-res mode appear like twenty-one. Roberta’s King’s Quest series, which I encountered on my Apple II, did not begin until 1984. It became Sierra’s best seller: by 1987, the first three installments of the series had sold a combined 500,000 copies, at least according to Sierra’s own marketing.[28]&lt;/p&gt;
    &lt;p&gt;It stands out, in a story populated almost entirely with male characters, that two of the earliest adventure game designers (Alexis Adams and Roberta Williams) were women. The scope of Alexis’ contributions aren’t entirely clear, but Roberta was arguably the most successful adventure game designer of all time. There was an appeal in the adventure game genre, which had more in common with a mystery novel or a logic puzzle than an arcade game and typically eschewed violence (the summary execution of Mystery House’s killer notwithstanding), that attracted some women to an otherwise almost entirely masculine industry.[29]&lt;/p&gt;
    &lt;p&gt;In a world where multiple discovery and parallel invention are the norm, it is also remarkable that all of the games we have discussed (and indeed all the computer adventure games ever made) can trace their ancestry to the Crowther-Woods Adventure. In the meantime, though, many other computer game authorshad drawn inspiration from Dungeons and Dragons, spawning an entirely different genre of computer games, more in tune with D&amp;amp;D’s wargaming roots.&lt;/p&gt;
    &lt;p&gt;[1] Programma International, “Spring 1980 Catalog” (Spring 1980), 3-5 (https://ia903201.us.archive.org/12/items/Programma_Catalog_Spring_1980_for_APPLE_II/Programma_Catalog_Spring_1980_for_APPLE_II.pdf).&lt;/p&gt;
    &lt;p&gt;[2] J.J. Anderson, “Dave tells Ahl—The History of Creative Computing,”Creative Computing (November 1984), 72.&lt;/p&gt;
    &lt;p&gt;[3] “A Star Trek Product,” BYTE (July 1976), 92; “Scelbi Software,” BYTE (July 1976), 17.&lt;/p&gt;
    &lt;p&gt;[4] Alexander Smith, They Create Worlds: The Story of the People and Companies That Shaped the Video Game Industry, Vol. I: 1971-1982 (Boca Raton: CRC Press, 2020), 366-368; Jimmy Maher “Adventureland, Part 2,” The Digital Antiquarian (June 24, 2011) (https://www.filfre.net/2011/06/adventureland-part-2); David H. Ahl, “The First Decade of Personal Computing,” Creative Computing (November 1984), 30.&lt;/p&gt;
    &lt;p&gt;[5] Smith, They Create Worlds, 266-267; David H. Ahl, ed., 101 BASIC Computer Games: Microcomputer Edition (New York: Workman Publishing, 1977).&lt;/p&gt;
    &lt;p&gt;[6] The Wargaming Scribe, “The beginning of home computer gaming: the VDM-1 and the SOL-20” (August 16, 2023) (https://zeitgame.net/archives/10450); “Cromemco Dazzler Games” (Mountain View: Cromemco, 1977); Steve North, “Two Space Games (With Graphics!) For Your Home Computer,” Creative Computing (July/August 1977) 43-44; “Spacewar Available for the Cromemco Dazzler,” Cromemco News (January 1977).&lt;/p&gt;
    &lt;p&gt;[7] Smith, They Create Worlds, 383-384; Katie Hafner, “Will Crowther Interview,” (March 1994), 1-5 (https://archive.org/details/WillCrowtherInterview/mode/1up). I read into&lt;/p&gt;
    &lt;p&gt;[8] Quoted in Dale Peterson, Genesis II: Creation and Recreation with Computers (Reston: Prentice-Hall, 1983), 188.&lt;/p&gt;
    &lt;p&gt;[9] Smith, They Create Worlds, 384-385; Dennis G. Jerz, “Somewhere Nearby is Colossal Cave,” (2007), 83 (https://jerz.setonhill.edu/resources/preprint/SNiCC.pdf&lt;/p&gt;
    &lt;p&gt;[10] Smith, They Create Worlds, 384-385; Jerz, “Somewhere Nearby is Colossal Cave,” 13; Jimmy Maher, “The Completed Adventure, Part 1” The Digital Antiquarian (June 2, 2011) (https://www.filfre.net/2011/06/the-completed-adventure-part-1/); Tracy Kidder, The Soul of A New Machine (New York: Little, Brown, 2000 [1981]), 86-89.&lt;/p&gt;
    &lt;p&gt;[11] IF Archive Adventure zip (https://unbox.ifarchive.org/?url=/if-archive/games/source/adv350-pdp10.tar.gz);”AdventureLand map (https://www.solutionarchive.com/file/id%2C3/); Maher, “AdventureLand, Part 1,” The Digital Antiquarian (June 22, 2011) (https://www.filfre.net/2011/06/adventureland-part-1).&lt;/p&gt;
    &lt;p&gt;[12] Robert Levering, Michael Katz, and Milton Moskowitz, The Computer Entrepreneurs: Who’s Making It Big and How in America’s Upstart Industry (New York: NAL Books, 1984), 114-118; Smith, They Create Worlds, 388.&lt;/p&gt;
    &lt;p&gt;[13] Jimmy Maher, “Adventureland, Part 1,” The Digital Antiquarian (June 22, 2011) (https://www.filfre.net/2011/06/adventureland-part-1).&lt;/p&gt;
    &lt;p&gt;[14] Bob Liddil, “On the Road to Adventure,” BYTE (December 1980), 170.&lt;/p&gt;
    &lt;p&gt;[15] The 1990 LucasArts adventure, Loom, for example, though it is an artistic masterpiece, was criticized by reviewers for being too short and too easy. Scorpia, “Scorpion’s View: ‘Conquests of Camelot’ and ‘Loom’,” Computer Gaming World (July-August 1990), 51, 63, Simply making the games larger, with more puzzles was technically infeasible in the early years (we have already seen that Adventureland had to be much smaller than Adventure to fit on a microcomputer); later, as the costs of game production went up, it became financially infeasible. There is an expert dissection of the sins of one early adventure game in Jimmy Maher, “The Wizard and the Princess, Part 2,” The Digital Antiquarian (October 21, 2011) (https://www.filfre.net/2011/10/the-wizard-and-the-princess-part-2).&lt;/p&gt;
    &lt;p&gt;[16] Bob Liddil, “On the Road to Adventure,” BYTE (December 1980), 162.&lt;/p&gt;
    &lt;p&gt;[17] Jimmy Maher, “The Roots of Infocom,” Digital Antiquarian (January 1, 2012) (https://www.filfre.net/2012/01/the-roots-of-infocom); Jimmy Maher, “Zork on the PDP-10,” Digital Antiquarian (January 3, 2012) (https://www.filfre.net/2012/01/zork-on-the-pdp-10); Stephen Granade and Philip Jong, “David Lebling Interview,” Brass Lantern (undated, ca. 2000) (http://brasslantern.org/community/interviews/lebling.html); Nick Montfort, Twisty Little Passage: An Approach to Interactive Fiction (Cambridge: MIT Press, 2003), 86. Eric Roberts, Crowther and Lebling’s dungeon master, ran a variant of D&amp;amp;D he called Mirkwood Tales. Jon Peterson, Playing at the World (San Diego: Unreason Press, 2012), 617-618, 622.&lt;/p&gt;
    &lt;p&gt;[18] P. David Lebling, “Zork and the Future of Computerized Fantasy Simulations,” BYTE (December 1980), 172-182.&lt;/p&gt;
    &lt;p&gt;[19] Jimmy Maher, “ZIL and the Z-Machine,” The Digital Antiquarian (https://www.filfre.net/2012/01/zil-and-the-z-machine); Maya Posch, “Zork And The Z-Machine: Bringing The Mainframe To 8-bit Home Computers,” Hackaday (May 22, 2019) (https://hackaday.com/2019/05/22/zork-and-the-z-machine-bringing-the-mainframe-to-8-bit-home-computers); Scott Adams “Pirate’s Adventure,” BYTE (December 1980), 212. Virtual memory was a well-established technique in minicomputer and mainframe operating systems, but no widely used personal computer OS offered virtual memory until the release of Windows 3.0 in 1990.&lt;/p&gt;
    &lt;p&gt;[20] “InfoCom Shipments By Title and Year” (https://www.flickr.com/photos/textfiles/2419969220); Bob Liddil, “Zork, The Great Underground Empire,” Byte (February 1981), 262.&lt;/p&gt;
    &lt;p&gt;[21] Liddil, “Zork, The Great Underground Empire,” 262.&lt;/p&gt;
    &lt;p&gt;[22] Jimmy Maher, “Parser Games,” Digital Antiquarian (January 16, 2012) (https://www.filfre.net/2012/01/parser-games).&lt;/p&gt;
    &lt;p&gt;[23] Levy, Hackers, 293-297, 302-303; Ken Williams, Not All Fairy Tales Have Happy Endings: The Rise and Fall of Sierra On-Line (Ken Williams, 2020), 12-24, 22-24; Jimmy Maher, “Ken and Roberta,” The Digital Antiquarian (October 2, 2011) (https://www.filfre.net/2011/10/ken-and-roberta).&lt;/p&gt;
    &lt;p&gt;[24] Williams, Not All Fairy Tales, 55-56, 66-68, 88; Levy, Hackers, 303-304; Ken Wiliams, “Introduction to The Roberta Williams Anthology” (1996) (https://wiki.sierrahelp.com/index.php/Introduction_to_The_Roberta_Williams_Anthology). The account in the previous paragraphs is interpolated from the above sources, which are partially contradictory. All differ about who got the Apple II and why. Levy never mentions the TRS-80 or any adventure games besides Adventure, and has Roberta finishing that game after the time the Apple II was purchased, implying she never played any other adventure games before deciding to write Mystery House: the timeline would simply be too tight. I believe this is wrong, and either an intentional elision or a false interpolation by Levy. It is unlikely that the Williamses would later entirely hallucinate having brought home and played the whole series of Scott Adams games. The accounts also differ on whose idea it was to add pictures to the game. I’m inclined to believe it was Roberta, to whom the game idea and all the passion for it belonged.&lt;/p&gt;
    &lt;p&gt;[25] Williams, Not All Fairy Tales, 69-73.&lt;/p&gt;
    &lt;p&gt;[26] Levy, Hackers, 308-310; Williams, Not All Fairy Tales, 73; Ken Williams, “A Message From the President,” Sierra News Magazine (Summer 1990), 35].&lt;/p&gt;
    &lt;p&gt;[27] John Williams, “Sierra’s First Ten Years,” Sierra News Magazine (Summer 1990), 6; Williams, Not All Fairy Tales, 79; Jimmy Maher, “Mystery House, Part 2,” Digital Antiquarian (October 9, 2011) (https://www.filfre.net/2011/10/mystery-house-part-2); “Game 57: Mystery House,” Data-Driven Gamer (April 22, 2019) (https://datadrivengamer.blogspot.com/2019/04/game-57-mystery-house.html).&lt;/p&gt;
    &lt;p&gt;[28] Jimmy Maher, “The Wizard and the Princess, Part 1,” Digital Antiquarian (October 20, 2011) (https://www.filfre.net/2011/10/the-wizard-and-the-princess-part-1); “On-Line Systems Presents: Hi-Res Adventure,” Softline (September 1981), 16; Levy, Hackers, 310-311; “Sales Data,” King’s Quest Omnipedia (https://kingsquest.fandom.com/wiki/Sales_data#King’s_Quest_Original).&lt;/p&gt;
    &lt;p&gt;[29] In later years, Sierra On-Line would employ several more women as designers—Lori Cole (the Quest for Glory series), Christy Marx (Conquests of Camelot and Conquests of the Longbow), and Jane Jensen (the Gabriel Knight series), while Amy Briggs created Plundered Hearts at Infocom. It is hard to get any reliable numbers on the audience for adventure games: in 1989, Sierra estimated that 35-40% of the players of King’s Quest IV were women, which surely was well above average for a computer game. Patricia Cignarella, “Girls Just Want To Have Fun,” Sierra News Magazine (Autumn 1989), 25.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://technicshistory.com/2025/12/13/the-rise-of-computer-games-part-i-adventure/"/><published>2025-12-13T20:19:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46257607</id><title>I fed 24 years of my blog posts to a Markov model</title><updated>2025-12-14T00:57:16.848634+00:00</updated><content>&lt;doc fingerprint="2518939f24803e80"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Fed 24 Years of My Blog Posts to a Markov Model&lt;/head&gt;
    &lt;p&gt;Yesterday I shared a little program called Mark V. Shaney Junior at github.com/susam/mvs. It is a minimal implementation of a Markov text generator inspired by the legendary Mark V. Shaney program from the 1980s. If you don't know about Mark V. Shaney, read more about it on the Wikipedia article Mark V. Shaney.&lt;/p&gt;
    &lt;p&gt;It is a very small program with only about 30 lines of Python (see mvs.py) that favours simplicity over efficiency. As a hobby, I often engage in exploratory programming where I write computer programs not to solve a specific problem but simply to explore a particular idea or topic for the sole purpose of recreation. I must have written small programs to explore Markov chains for various kinds of state spaces over a dozen times by now. Every time, I just pick my last experimental code and edit it to encode the new state space I am exploring. That's usually my general approach to such one-off programs. I have hundreds of tiny little experimental programs lying on my disk at any given time.&lt;/p&gt;
    &lt;p&gt; Once in a while, I get the itch to take one of those exploratory programs, give it some finishing touches, wrap it up in a nice Git repo along with a &lt;code&gt;README.md&lt;/code&gt;, &lt;code&gt;CHANGES.md&lt;/code&gt;
  and the whole shebang and share it on
  github.com/susam and
  codeberg.org/susam.  The
  Mark V. Shaney Junior program that I shared yesterday happened to be
  one such exercise.
&lt;/p&gt;
    &lt;p&gt;If you scroll down the README of this project, you'll find some nice examples of the gibberish produced by this program. The first few examples there are the result of training the model on A Christmas Carol by Charles Dickens, one of my favourite authors. It is often said that Dickens never used fewer words when more would suffice. So I thought there couldn't be a better piece of text when it comes to testing out my tiny Markov model. I'll not reproduce the generated text examples here for the sake of brevity. If you are interested to take a look, just head over to the Gibberish section of the README.&lt;/p&gt;
    &lt;p&gt;Soon after sharing the project, I wondered what kind of gibberish it would produce if I fed all 24 years of my blog posts and pages into the program. Well, here's one of the results:&lt;/p&gt;
    &lt;quote&gt;$ python3 mvs.py &amp;lt; susam.txt while a query replace operation is approved by the user. The above variable defines the build job. It can be incredibly useful while working on assembly language and machine code. In fact, all internal resources like the result to refine the search prompt changes from bck-i-search: to fwd-i-search:. Now type C-SPC (i.e. ctrl+space) to set a mark causes Emacs to use 32-bit registers like EBP, ESP, etc. Thus the behaviour is undefined. Such code may behave differently when compiled with the readily available GNU tools like the shape of 8. Flipping "P" horizontally makes it a proper quine: cat $0&lt;/quote&gt;
    &lt;p&gt;This is the text that comes out after the program consumes over 200 posts consisting of about 200,000 words. My blog also has a comments section with over 500 comments consisting of about 40,000 words. All comments were excluded while training the model. Here is another output example:&lt;/p&gt;
    &lt;quote&gt;$ python3 mvs.py &amp;lt; susam.txt enjoy asking "what happens if" and then type M-x zap-up-to-char RET b. The buffer for this specific video, the actual fare for 8.3 km and 11 are all written from scratch. No prior knowledge is expected to slow down in future. For now, I will add a statement like x = 0.999... , the inner corner square as discussed in the code segment into the REPL window. Unlike Slimv, Vlime can work with and debug executable files, it can be more convenient. M-x: Execute Extended Command The key sequence M-q invokes the command cat and type TAB to indent the&lt;/quote&gt;
    &lt;p&gt;Here is a particularly incoherent but amusing one:&lt;/p&gt;
    &lt;quote&gt;$ python3 mvs.py &amp;lt; susam.txt Then open a new Lisp source file and the exact answer could harm students' self-esteem. Scientists have arbitrarily assumed that an integral domain. However, the string and comment text. To demonstrate how a build job can trigger itself, pass input to standard output or standard error), Eshell automatically runs the following command in Vim and Emacs will copy the message length limit of 512 characters, etc. For example, while learning to play the game between normal mode to move the point is on an old dictionary lying around our house and that is moving to the small and supportive community&lt;/quote&gt;
    &lt;p&gt;No, I have never said anywhere that opening a Lisp source file could harm anyone's self-esteem. The text generator has picked up the 'Lisp source file' phrase from my Lisp in Vim post and the 'self-esteem' bit from the From Perl to Pi post.&lt;/p&gt;
    &lt;p&gt;By default, this program looks at trigrams (all sequences of three adjacent words) and creates a map where the first two words of the trigram are inserted as the key and the third word is appended to its list value. This map is the model. In this way, the model captures each pair of adjacent words along with the words that immediately follow each pair. The text generator then chooses a key (a pair of words) at random and looks for a word which follows. If there are multiple followers, it picks one at random. That is pretty much the whole algorithm. There isn't much more to it. It is as simple as it gets. For that reason, I often describe a simple Markov model like this as the 'hello, world' for language models.&lt;/p&gt;
    &lt;p&gt;Of course, in 2025, given the overwhelming popularity of large language models (LLMs), Markov models like this look unimpressive. Unlike LLMs, a simple Markov model cannot capture global structure or long-range dependencies within the text. It relies entirely on local word transition statistics. Also, these days, one hardly needs a Markov model to generate gibberish; social media provides an ample supply. Nevertheless, I think the simplicity of its design and implementation serves as a good entry point into language models.&lt;/p&gt;
    &lt;p&gt;In my implementation, the number of words in the key of the map can be set via command line arguments. By default, it is 2 as described above. This value is also known as the order of the model. So by default the order is 2. If we increase it to, say, 3 or 4, the generated text becomes a little more coherent. Here is one such example:&lt;/p&gt;
    &lt;quote&gt;$ python3 mvs.py 4 &amp;lt; susam.txt It is also possible to search for channels by channel names. For example, on Libera Chat, to search for all channels with 'python' in its name, enter the IRC command: /msg alis list python. Although I have used Libera Chat in the examples above, there are plenty of infinite fields, so they must all be integral domains too. Consider the field of rational numbers Q . Another quick way to arrive at this fact is to observe that when one knight is placed on a type D square, only two positions for the second knight such that the two knights&lt;/quote&gt;
    &lt;p&gt;Except for a couple of abrupt transitions, the text is mostly coherent. We need to be careful about not increasing the order too much. In fact, if we increase the order of the model to 5, the generated text becomes very dry and factual because it begins to quote large portions of the blog posts verbatim. Not much fun can be had like that.&lt;/p&gt;
    &lt;p&gt;Before I end this post, let me present one final example where I ask it to generate text from an initial prompt:&lt;/p&gt;
    &lt;quote&gt;$ python3 mvs.py 2 100 'Finally we' Finally we divide this number by a feed aggregrator for Emacs-related blogs. The following complete key sequences describe the effects of previous evaluations shall have taken a simple and small to contain bad content. This provides an interactive byte-compiled Lisp function in MATLAB and GNU bash 5.1.4 on Debian is easily reproducible in Windows XP. Older versions might be able to run that server for me it played a significant burden on me as soon as possible. C-u F: Visit the marked files or directories in the sense that it was already initiated and we were to complete the proof.&lt;/quote&gt;
    &lt;p&gt;Apparently, this is how I would sound if I ever took up speaking gibberish!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://susam.net/fed-24-years-of-posts-to-markov-model.html"/><published>2025-12-13T20:19:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46257714</id><title>Why Twilio Segment moved from microservices back to a monolith</title><updated>2025-12-14T00:57:16.545306+00:00</updated><content>&lt;doc fingerprint="36d908270fc18be2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Goodbye Microservices: From 100s of problem children to 1 superstar&lt;/head&gt;
    &lt;p&gt;Time to read:&lt;/p&gt;
    &lt;head rend="h2"&gt;Goodbye Microservices: From 100s of problem children to 1 superstar&lt;/head&gt;
    &lt;p&gt;Microservices is a service-oriented software architecture in which server-side applications are constructed by combining many single-purpose, low-footprint network services. The touted benefits are improved modularity, reduced testing burden, better functional composition, environmental isolation, and development team autonomy. The opposite is a Monolithic architecture, where a large amount of functionality lives in a single service which is tested, deployed, and scaled as a single unit.&lt;/p&gt;
    &lt;p&gt;Twilio Segment adopted this as a best practice early-on, which served us well in some cases, and, as you’ll soon learn, not so well in others.&lt;/p&gt;
    &lt;p&gt;In the early days of Twilio Segment, we reached a tipping point with a core piece of Twilio Segment’s product. It seemed as if we were falling from the microservices tree, hitting every branch on the way down. Instead of enabling us to move faster, the small team found themselves mired in exploding complexity. Essential benefits of this architecture became burdens. As our velocity plummeted, our defect rate exploded.&lt;/p&gt;
    &lt;p&gt;Eventually, the team found themselves unable to make headway, with 3 full-time engineers spending most of their time just keeping the system alive. Something had to change. This post is the story of how we took a step back and embraced an approach that aligned well with our product requirements and needs of the team.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Microservices worked&lt;/head&gt;
    &lt;p&gt;Twilio Segment’s customer data infrastructure ingests hundreds of thousands of events per second and forwards them to partner APIs, what we refer to as server-side destinations. There are over one hundred types of these destinations, such as Google Analytics, Optimizely, or a custom webhook.&lt;/p&gt;
    &lt;p&gt;Years back, when the product initially launched, the architecture was simple. There was an API that ingested events and forwarded them to a distributed message queue. An event, in this case, is a JSON object generated by a web or mobile app containing information about users and their actions. A sample payload looks like the following:&lt;/p&gt;
    &lt;p&gt;As events were consumed from the queue, customer-managed settings were checked to decide which destinations should receive the event. The event was then sent to each destination’s API, one after another, which was useful because developers only need to send their event to a single endpoint, Twilio Segment’s API, instead of building potentially dozens of integrations. Twilio Segment handles making the request to every destination endpoint.&lt;/p&gt;
    &lt;p&gt;If one of the requests to a destination fails, sometimes we’ll try sending that event again at a later time. Some failures are safe to retry while others are not. Retry-able errors are those that could potentially be accepted by the destination with no changes. For example, HTTP 500s, rate limits, and timeouts. Non-retry-able errors are requests that we can be sure will never be accepted by the destination. For example, requests which have invalid credentials or are missing required fields.&lt;/p&gt;
    &lt;p&gt;At this point, a single queue contained both the newest events as well as those which may have had several retry attempts, across all destinations, which resulted in head-of-line blocking. Meaning in this particular case, if one destination slowed or went down, retries would flood the queue, resulting in delays across all our destinations.&lt;/p&gt;
    &lt;p&gt;Imagine destination X is experiencing a temporary issue and every request errors with a timeout. Now, not only does this create a large backlog of requests which have yet to reach destination X, but also every failed event is put back to retry in the queue. While our systems would automatically scale in response to increased load, the sudden increase in queue depth would outpace our ability to scale up, resulting in delays for the newest events. Delivery times for all destinations would increase because destination X had a momentary outage. Customers rely on the timeliness of this delivery, so we can’t afford increases in wait times anywhere in our pipeline.&lt;/p&gt;
    &lt;p&gt;To solve the head-of-line blocking problem, the team created a separate service and queue for each destination. This new architecture consisted of an additional router process that receives the inbound events and distributes a copy of the event to each selected destination. Now if one destination experienced problems, only it’s queue would back up and no other destinations would be impacted. This microservice-style architecture isolated the destinations from one another, which was crucial when one destination experienced issues as they often do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Case for Individual Repos&lt;/head&gt;
    &lt;p&gt;Each destination API uses a different request format, requiring custom code to translate the event to match this format. A basic example is destination X requires sending birthday as traits.dob in the payload whereas our API accepts it as traits.birthday. The transformation code in destination X would look something like this:&lt;/p&gt;
    &lt;p&gt;Many modern destination endpoints have adopted Twilio Segment’s request format making some transforms relatively simple. However, these transforms can be very complex depending on the structure of the destination’s API. For example, for some of the older and most sprawling destinations, we find ourselves shoving values into hand-crafted XML payloads.&lt;/p&gt;
    &lt;p&gt;Initially, when the destinations were divided into separate services, all of the code lived in one repo. A huge point of frustration was that a single broken test caused tests to fail across all destinations. When we wanted to deploy a change, we had to spend time fixing the broken test even if the changes had nothing to do with the initial change. In response to this problem, it was decided to break out the code for each destination into their own repos. All the destinations were already broken out into their own service, so the transition was natural.&lt;/p&gt;
    &lt;p&gt;The split to separate repos allowed us to isolate the destination test suites easily. This isolation allowed the development team to move quickly when maintaining destinations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scaling Microservices and Repos&lt;/head&gt;
    &lt;p&gt;As time went on, we added over 50 new destinations, and that meant 50 new repos. To ease the burden of developing and maintaining these codebases, we created shared libraries to make common transforms and functionality, such as HTTP request handling, across our destinations easier and more uniform.&lt;/p&gt;
    &lt;p&gt;For example, if we want the name of a user from an event, event.name() can be called in any destination’s code. The shared library checks the event for the property key name and Name. If those don’t exist, it checks for a first name, checking the properties firstName, first_name, and FirstName. It does the same for the last name, checking the cases and combining the two to form the full name.&lt;/p&gt;
    &lt;p&gt;The shared libraries made building new destinations quick. The familiarity brought by a uniform set of shared functionality made maintenance less of a headache.&lt;/p&gt;
    &lt;p&gt;However, a new problem began to arise. Testing and deploying changes to these shared libraries impacted all of our destinations. It began to require considerable time and effort to maintain. Making changes to improve our libraries, knowing we’d have to test and deploy dozens of services, was a risky proposition. When pressed for time, engineers would only include the updated versions of these libraries on a single destination’s codebase.&lt;/p&gt;
    &lt;p&gt;Over time, the versions of these shared libraries began to diverge across the different destination codebases. The great benefit we once had of reduced customization between each destination codebase started to reverse. Eventually, all of them were using different versions of these shared libraries. We could’ve built tools to automate rolling out changes, but at this point, not only was developer productivity suffering but we began to encounter other issues with the microservice architecture.&lt;/p&gt;
    &lt;p&gt;The additional problem is that each service had a distinct load pattern. Some services would handle a handful of events per day while others handled thousands of events per second. For destinations that handled a small number of events, an operator would have to manually scale the service up to meet demand whenever there was an unexpected spike in load.&lt;/p&gt;
    &lt;p&gt;While we did have auto-scaling implemented, each service had a distinct blend of required CPU and memory resources, which made tuning the auto-scaling configuration more art than science.&lt;/p&gt;
    &lt;p&gt;The number of destinations continued to grow rapidly, with the team adding three destinations per month on average, which meant more repos, more queues, and more services. With our microservice architecture, our operational overhead increased linearly with each added destination. Therefore, we decided to take a step back and rethink the entire pipeline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ditching Microservices and Queues&lt;/head&gt;
    &lt;p&gt;The first item on the list was to consolidate the now over 140 services into a single service. The overhead from managing all of these services was a huge tax on our team. We were literally losing sleep over it since it was common for the on-call engineer to get paged to deal with load spikes.&lt;/p&gt;
    &lt;p&gt;However, the architecture at the time would have made moving to a single service challenging. With a separate queue per destination, each worker would have to check every queue for work, which would have added a layer of complexity to the destination service with which we weren’t comfortable. This was the main inspiration for Centrifuge. Centrifuge would replace all our individual queues and be responsible for sending events to the single monolithic service. (Note that Centrifuge became the back-end infrastructure for Connections.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving to a Monorepo&lt;/head&gt;
    &lt;p&gt;Given that there would only be one service, it made sense to move all the destination code into one repo, which meant merging all the different dependencies and tests into a single repo. We knew this was going to be messy.&lt;/p&gt;
    &lt;p&gt;For each of the 120 unique dependencies, we committed to having one version for all our destinations. As we moved destinations over, we’d check the dependencies it was using and update them to the latest versions. We fixed anything in the destinations that broke with the newer versions.&lt;/p&gt;
    &lt;p&gt;With this transition, we no longer needed to keep track of the differences between dependency versions. All our destinations were using the same version, which significantly reduced the complexity across the codebase. Maintaining destinations now became less time consuming and less risky.&lt;/p&gt;
    &lt;p&gt;We also wanted a test suite that allowed us to quickly and easily run all our destination tests. Running all the tests was one of the main blockers when making updates to the shared libraries we discussed earlier.&lt;/p&gt;
    &lt;p&gt;Fortunately, the destination tests all had a similar structure. They had basic unit tests to verify our custom transform logic was correct and would execute HTTP requests to the partner’s endpoint to verify that events showed up in the destination as expected.&lt;/p&gt;
    &lt;p&gt;Recall that the original motivation for separating each destination codebase into its own repo was to isolate test failures. However, it turned out this was a false advantage. Tests that made HTTP requests were still failing with some frequency. With destinations separated into their own repos, there was little motivation to clean up failing tests. This poor hygiene led to a constant source of frustrating technical debt. Often a small change that should have only taken an hour or two would end up requiring a couple of days to a week to complete.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building a Resilient Test Suite&lt;/head&gt;
    &lt;p&gt;The outbound HTTP requests to destination endpoints during the test run was the primary cause of failing tests. Unrelated issues like expired credentials shouldn’t fail tests. We also knew from experience that some destination endpoints were much slower than others. Some destinations took up to 5 minutes to run their tests. With over 140 destinations, our test suite could take up to an hour to run.&lt;/p&gt;
    &lt;p&gt;To solve for both of these, we created Traffic Recorder. Traffic Recorder is built on top of yakbak, and is responsible for recording and saving destinations’ test traffic. Whenever a test runs for the first time, any requests and their corresponding responses are recorded to a file. On subsequent test runs, the request and response in the file is played back instead requesting the destination’s endpoint. These files are checked into the repo so that the tests are consistent across every change. Now that the test suite is no longer dependent on these HTTP requests over the internet, our tests became significantly more resilient, a must-have for the migration to a single repo.&lt;/p&gt;
    &lt;p&gt;It took milliseconds to complete running the tests for all 140+ of our destinations after we integrated Traffic Recorder. In the past, just one destination could have taken a couple of minutes to complete. It felt like magic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why a Monolith works&lt;/head&gt;
    &lt;p&gt;Once the code for all destinations lived in a single repo, they could be merged into a single service. With every destination living in one service, our developer productivity substantially improved. We no longer had to deploy 140+ services for a change to one of the shared libraries. One engineer can deploy the service in a matter of minutes.&lt;/p&gt;
    &lt;p&gt;The proof was in the improved velocity. When our microservice architecture was still in place, we made 32 improvements to our shared libraries. One year later, we’ve made 46 improvements.&lt;/p&gt;
    &lt;p&gt;The change also benefited our operational story. With every destination living in one service, we had a good mix of CPU and memory-intense destinations, which made scaling the service to meet demand significantly easier. The large worker pool can absorb spikes in load, so we no longer get paged for destinations that process small amounts of load.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trade Offs&lt;/head&gt;
    &lt;p&gt;Moving from our microservice architecture to a monolith overall was huge improvement, however, there are trade-offs:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Fault isolation is difficult. With everything running in a monolith, if a bug is introduced in one destination that causes the service to crash, the service will crash for all destinations. We have comprehensive automated testing in place, but tests can only get you so far. We are currently working on a much more robust way to prevent one destination from taking down the entire service while still keeping all the destinations in a monolith.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In-memory caching is less effective. Previously, with one service per destination, our low traffic destinations only had a handful of processes, which meant their in-memory caches of control plane data would stay hot. Now that cache is spread thinly across 3000+ processes so it’s much less likely to be hit. We could use something like Redis to solve for this, but then that’s another point of scaling for which we’d have to account. In the end, we accepted this loss of efficiency given the substantial operational benefits.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Updating the version of a dependency may break multiple destinations. While moving everything to one repo solved the previous dependency mess we were in, it means that if we want to use the newest version of a library, we’ll potentially have to update other destinations to work with the newer version. In our opinion though, the simplicity of this approach is worth the trade-off. And with our comprehensive automated test suite, we can quickly see what breaks with a newer dependency version.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Our initial microservice architecture worked for a time, solving the immediate performance issues in our pipeline by isolating the destinations from each other. However, we weren’t set up to scale. We lacked the proper tooling for testing and deploying the microservices when bulk updates were needed. As a result, our developer productivity quickly declined.&lt;/p&gt;
    &lt;p&gt;Moving to a monolith allowed us to rid our pipeline of operational issues while significantly increasing developer productivity. We didn’t make this transition lightly though and knew there were things we had to consider if it was going to work.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We needed a rock solid testing suite to put everything into one repo. Without this, we would have been in the same situation as when we originally decided to break them apart. Constant failing tests hurt our productivity in the past, and we didn’t want that happening again.&lt;/item&gt;
      &lt;item&gt;We accepted the trade-offs inherent in a monolithic architecture and made sure we had a good story around each. We had to be comfortable with some of the sacrifices that came with this change.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When deciding between microservices or a monolith, there are different factors to consider with each. In some parts of our infrastructure, microservices work well but our server-side destinations were a perfect example of how this popular trend can actually hurt productivity and performance. It turns out, the solution for us was a monolith.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;The transition to a monolith was made possible by Stephen Mathieson, Rick Branson, Achille Roussel, Tom Holmes, and many more.&lt;/p&gt;
    &lt;p&gt;Special thanks to Rick Branson for helping review and edit this post at every stage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to see what Twilio Segment can do for you?&lt;/head&gt;
    &lt;head rend="h2"&gt;The Customer Data Platform Report 2025&lt;/head&gt;
    &lt;p&gt;Drawing on anonymized insights from thousands of Twilio customers, the Customer Data Platform report explores how companies are using CDPs to unlock the power of their data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related Posts&lt;/head&gt;
    &lt;head rend="h2"&gt;Related Resources&lt;/head&gt;
    &lt;p&gt;Twilio Docs&lt;/p&gt;
    &lt;p&gt;From APIs to SDKs to sample apps&lt;/p&gt;
    &lt;p&gt;API reference documentation, SDKs, helper libraries, quickstarts, and tutorials for your language and platform.&lt;/p&gt;
    &lt;p&gt;Resource Center&lt;/p&gt;
    &lt;p&gt;The latest ebooks, industry reports, and webinars&lt;/p&gt;
    &lt;p&gt;Learn from customer engagement experts to improve your own communication.&lt;/p&gt;
    &lt;p&gt;Ahoy&lt;/p&gt;
    &lt;p&gt;Twilio's developer community hub&lt;/p&gt;
    &lt;p&gt;Best practices, code samples, and inspiration to build communications and digital engagement experiences.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.twilio.com/en-us/blog/developers/best-practices/goodbye-microservices"/><published>2025-12-13T20:30:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46257871</id><title>Want to sway an election? Here’s how much fake online accounts cost</title><updated>2025-12-14T00:57:16.050565+00:00</updated><content/><link href="https://www.science.org/content/article/want-sway-election-here-s-how-much-fake-online-accounts-cost"/><published>2025-12-13T20:48:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46257939</id><title>Purdue University Approves New AI Requirement for All Undergrads</title><updated>2025-12-14T00:57:15.932066+00:00</updated><content/><link href="https://www.forbes.com/sites/michaeltnietzel/2025/12/13/purdue-university-approves-new-ai-requirement-for-all-undergrads/"/><published>2025-12-13T20:54:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46258163</id><title>Recovering Anthony Bourdain's (really) lost Li.st's</title><updated>2025-12-14T00:57:15.778982+00:00</updated><content>&lt;doc fingerprint="3b699835a09e1a8c"&gt;
  &lt;main&gt;
    &lt;p&gt;🍇 At least 1 day ago&lt;/p&gt;
    &lt;p&gt;Loved reading through GReg TeChnoLogY Anthony Bourdain’s Lost Li.st’s and seeing the list of lost Anthony Bourdain li.st’s made me think on whether at least some of them we can recover.&lt;/p&gt;
    &lt;p&gt;Having worked in security and crawling space for majority of my career—I don’t have the access nor permission to use the proprietary storages—I thought we might be able to find something from publicly available crawl archives.&lt;/p&gt;
    &lt;p&gt;All of the code and examples lead to the source git repository.&lt;/p&gt;
    &lt;p&gt;If Internet Archive had the partial list that Greg published, what about the Common Crawl? Reading through their documentation, it seems straightforward enough to get prefix index for Tony’s lists and grep for any sub-paths.&lt;/p&gt;
    &lt;p&gt; Putting something up with help of Claude to prove my theory, we have &lt;code&gt;commoncrawl_search.py&lt;/code&gt; that makes a single index request to a specific dataset and if any hits discovered, retrieve them from the public s3 bucket—since they are small straight-up HTML documents, seems even more feasible than I had initially thought.
&lt;/p&gt;
    &lt;p&gt; Simply have a python version around 3.14.2 and install the dependencies from &lt;code&gt;requirements.txt&lt;/code&gt;. Run the below and we are in business. Now, below, you’ll find the command I ran and then some manual archeological effort to prettify the findings.
&lt;/p&gt;
    &lt;code&gt;python commoncrawl_search.py "https://li.st/Bourdain*" --all --download&lt;/code&gt;
    &lt;p&gt; Any and all emphasis, missing punctuation, cool grammar is all by Anthony Bourdain. The only modifications I have made is to the layout, to represent &lt;code&gt;li.st&lt;/code&gt; as closely as possible with no changes to the content.
&lt;/p&gt;
    &lt;p&gt; From Greg’s page, let’s go and try each entry one by one, I’ll put the table of what I wasn’t able to find in Common Crawl, but I would assume exists elsewhere—I’d be happy to take another look. And no, none of this above has been written by AI, only the code since I don’t really care about &lt;code&gt;warcio&lt;/code&gt; encoding or writing the same python requests method for the Nth time. Enjoy!
&lt;/p&gt;
    &lt;p&gt;Cocaine&lt;/p&gt;
    &lt;p&gt;True Detective&lt;/p&gt;
    &lt;p&gt;Scripps Howard&lt;/p&gt;
    &lt;p&gt;Dinners where it takes the waiter longer to describe my food than it takes me to eat it.&lt;/p&gt;
    &lt;p&gt;Beer nerds&lt;/p&gt;
    &lt;p&gt;I admit it: my life doesn’t suck. Some recent views I’ve enjoyed&lt;/p&gt;
    &lt;p&gt;Montana at sunset : There’s pheasant cooking behind the camera somewhere. To the best of my recollection some very nice bourbon. And it IS a big sky .&lt;/p&gt;
    &lt;p&gt;Puerto Rico: Thank you Jose Andres for inviting me to this beautiful beach!&lt;/p&gt;
    &lt;p&gt;Naxos: drinking ouzo and looking at this. Not a bad day at the office .&lt;/p&gt;
    &lt;p&gt;LA: My chosen final resting place . Exact coordinates .&lt;/p&gt;
    &lt;p&gt;Istanbul: raki and grilled lamb and this ..&lt;/p&gt;
    &lt;p&gt;Borneo: The air is thick with hints of durian, sambal, coconut..&lt;/p&gt;
    &lt;p&gt;Chicago: up early to go train #Redzovic&lt;/p&gt;
    &lt;p&gt;The Wire&lt;/p&gt;
    &lt;p&gt;Tinker, Tailor, Soldier, Spy (and its sequel : Smiley’s People)&lt;/p&gt;
    &lt;p&gt;Edge of Darkness (with Bob Peck and Joe Don Baker )&lt;/p&gt;
    &lt;p&gt;Dreamcasting across time with the living and the dead, this untitled, yet to be written masterwork of cinema, shot, no doubt, by Christopher Doyle, lives only in my imagination.&lt;/p&gt;
    &lt;p&gt;This guy&lt;/p&gt;
    &lt;p&gt;And this guy&lt;/p&gt;
    &lt;p&gt;All great films need:&lt;/p&gt;
    &lt;p&gt;The Oscar goes to..&lt;/p&gt;
    &lt;p&gt;And&lt;/p&gt;
    &lt;p&gt;If you bought these vinyls from an emaciated looking dude with an eager, somewhat distracted expression on his face somewhere on upper Broadway sometime in the mid 80’s, that was me . I’d like them back. In a sentimental mood.&lt;/p&gt;
    &lt;p&gt;material things I feel a strange, possibly unnatural attraction to and will buy (if I can) if I stumble across them in my travels. I am not a paid spokesperson for any of this stuff .&lt;/p&gt;
    &lt;p&gt;Vintage Persol sunglasses : This is pretty obvious. I wear them a lot. I collect them when I can. Even my production team have taken to wearing them.&lt;/p&gt;
    &lt;p&gt;19th century trepanning instruments: I don’t know what explains my fascination with these devices, designed to drill drain-sized holes into the skull often for purposes of relieving "pressure" or "bad humours". But I can’t get enough of them. Tip: don’t get a prolonged headache around me and ask if I have anything for it. I do.&lt;/p&gt;
    &lt;p&gt;Montagnard bracelets: I only have one of these but the few that find their way onto the market have so much history. Often given to the indigenous mountain people ’s Special Forces advisors during the very early days of America’s involvement in Vietnam .&lt;/p&gt;
    &lt;p&gt;Jiu Jitsi Gi’s: Yeah. When it comes to high end BJJ wear, I am a total whore. You know those people who collect limited edition Nikes ? I’m like that but with Shoyoroll . In my defense, I don’t keep them in plastic bags in a display case. I wear that shit.&lt;/p&gt;
    &lt;p&gt;Voiture: You know those old school, silver plated (or solid silver) blimp like carts they roll out into the dining room to carve and serve your roast? No. Probably not. So few places do that anymore. House of Prime Rib does it. Danny Bowein does it at Mission Chinese. I don’t have one of these. And I likely never will. But I can dream.&lt;/p&gt;
    &lt;p&gt;Kramer knives: I don’t own one. I can’t afford one . And I’d likely have to wait for years even if I could afford one. There’s a long waiting list for these individually hand crafted beauties. But I want one. Badly. http://www.kramerknives.com/gallery/&lt;/p&gt;
    &lt;p&gt;R. CRUMB : All of it. The collected works. These Taschen volumes to start. I wanted to draw brilliant, beautiful, filthy comix like Crumb until I was 13 or 14 and it became clear that I just didn’t have that kind of talent. As a responsible father of an 8 year old girl, I just can’t have this stuff in the house. Too dark, hateful, twisted. Sigh...&lt;/p&gt;
    &lt;p&gt;THE MAGNIFICENT AMBERSONS : THE UNCUT, ORIGINAL ORSON WELLES VERSION: It doesn’t exist. Which is why I want it. The Holy Grail for film nerds, Welles’ follow up to CITIZEN KANE shoulda, coulda been an even greater masterpiece . But the studio butchered it and re-shot a bullshit ending. I want the original. I also want a magical pony.&lt;/p&gt;
    &lt;p&gt;I like good spy novels. I prefer them to be realistic . I prefer them to be written by real spies. If the main character carries a gun, I’m already losing interest. Spy novels should be about betrayal.&lt;/p&gt;
    &lt;p&gt; Ashenden–Somerset Maugham&lt;lb/&gt;Somerset wrote this bleak, darkly funny, deeply cynical novel in the early part of the 20th century. It was apparently close enough to the reality of his espionage career that MI6 insisted on major excisions. Remarkably ahead of its time in its atmosphere of futility and betrayal. &lt;/p&gt;
    &lt;p&gt; The Man Who Lost the War–WT Tyler&lt;lb/&gt;WT Tyler is a pseudonym for a former "foreign service" officer who could really really write. This one takes place in post-war Berlin and elsewhere and was, in my opinion, wildly under appreciated. See also his Ants of God. &lt;/p&gt;
    &lt;p&gt; The Human Factor–Graham Greene&lt;lb/&gt;Was Greene thinking of his old colleague Kim Philby when he wrote this? Maybe. Probably. See also Our Man In Havana. &lt;/p&gt;
    &lt;p&gt; The Tears of Autumn -Charles McCarry&lt;lb/&gt;A clever take on the JFK assassination with a Vietnamese angle. See also The Miernik Dossier and The Last Supper &lt;/p&gt;
    &lt;p&gt; Agents of Innocence–David Ignatius&lt;lb/&gt;Ignatius is a journalist not a spook, but this one, set in Beirut, hewed all too closely to still not officially acknowledged events. Great stuff. &lt;/p&gt;
    &lt;p&gt;I wake up in a lot of hotels, so I am fiercely loyal to the ones I love. A hotel where I know immediately wher I am when I open my eyes in the morning is a rare joy. Here are some of my favorites&lt;/p&gt;
    &lt;p&gt;CHATEAU MARMONT ( LA) : if I have to die in a hotel room, let it be here. I will work in LA just to stay at the Chateau.&lt;/p&gt;
    &lt;p&gt;CHILTERN FIREHOUSE (London): Same owner as the Chateau. An amazing Victorian firehouse turned hotel. Pretty much perfection&lt;/p&gt;
    &lt;p&gt;THE RALEIGH (Miami): The pool. The pool!&lt;/p&gt;
    &lt;p&gt;LE CONTINENTAL (Saigon): For the history.&lt;/p&gt;
    &lt;p&gt;HOTEL OLOFSSON (Port au Prince): Sagging, creaky and leaky but awesome .&lt;/p&gt;
    &lt;p&gt;PARK HYATT (Tokyo): Because I’m a film geek.&lt;/p&gt;
    &lt;p&gt;EDGEWATER INN (Seattle): kind of a lumber theme going on...ships slide right by your window. And the Led Zep "Mudshark incident".&lt;/p&gt;
    &lt;p&gt;THE METROPOLE (Hanoi): there’s a theme developing: if Graham Greene stayed at a hotel, chances are I will too.&lt;/p&gt;
    &lt;p&gt;GRAND HOTEL D'ANGKOR (Siem Reap): I’m a sucker for grand, colonial era hotels in Asia.&lt;/p&gt;
    &lt;p&gt;THE MURRAY (Livingston,Montana): You want the Peckinpah suite&lt;/p&gt;
    &lt;p&gt;from my phone&lt;/p&gt;
    &lt;p&gt;Bun Bo Hue&lt;/p&gt;
    &lt;p&gt;Kuching Laksa&lt;/p&gt;
    &lt;p&gt;Pot au Feu&lt;/p&gt;
    &lt;p&gt;Jamon&lt;/p&gt;
    &lt;p&gt;Linguine&lt;/p&gt;
    &lt;p&gt;Meat&lt;/p&gt;
    &lt;p&gt;Dessert&lt;/p&gt;
    &lt;p&gt;Light Lunch&lt;/p&gt;
    &lt;p&gt;Meat on a Stick&lt;/p&gt;
    &lt;p&gt;Oily Little Fish&lt;/p&gt;
    &lt;p&gt;Snack&lt;/p&gt;
    &lt;p&gt;Soup&lt;/p&gt;
    &lt;p&gt;Homage&lt;/p&gt;
    &lt;p&gt;Not TOO random&lt;/p&gt;
    &lt;p&gt;Madeline&lt;/p&gt;
    &lt;p&gt;Beirut&lt;/p&gt;
    &lt;p&gt;Musubi&lt;/p&gt;
    &lt;p&gt;BudaeJiggae&lt;/p&gt;
    &lt;p&gt;Dinner&lt;/p&gt;
    &lt;p&gt;Bootsy Collins&lt;/p&gt;
    &lt;p&gt;Bill Murray&lt;/p&gt;
    &lt;p&gt;Spaghetti a la bottarga . I would really, really like some of this. Al dente, lots of chili flakes&lt;/p&gt;
    &lt;p&gt;A big, greasy double cheeseburger. No lettuce. No tomato. Potato bun.&lt;/p&gt;
    &lt;p&gt;A street fair sausage and pepper hero would be nice. Though shitting like a mink is an inevitable and near immediate outcome&lt;/p&gt;
    &lt;p&gt;Some uni. Fuck it. I’ll smear it on an English muffin at this point.&lt;/p&gt;
    &lt;p&gt;I wonder if that cheese is still good?&lt;/p&gt;
    &lt;p&gt;In which my Greek idyll is Suddenly invaded by professional nudists&lt;/p&gt;
    &lt;p&gt;Endemic FUPA. Apparently a prerequisite for joining this outfit.&lt;/p&gt;
    &lt;p&gt;Pistachio dick&lt;/p&gt;
    &lt;p&gt;70’s bush&lt;/p&gt;
    &lt;p&gt;T-shirt and no pants. Leading one to the obvious question : why bother?&lt;/p&gt;
    &lt;p&gt;Popeye’s Mac and Cheese&lt;/p&gt;
    &lt;p&gt;The cheesy crust on the side of the bowl of Onion Soup Gratinee&lt;/p&gt;
    &lt;p&gt;Macaroons . Not macarons . Macaroons&lt;/p&gt;
    &lt;p&gt;Captain Crunch&lt;/p&gt;
    &lt;p&gt;Double Double Animal Style&lt;/p&gt;
    &lt;p&gt;Spam Musubi&lt;/p&gt;
    &lt;p&gt;Aerosmith&lt;/p&gt;
    &lt;p&gt;Before he died, Warren Zevon dropped this wisdom bomb: "Enjoy every sandwich". These are a few locals I’ve particularly enjoyed:&lt;/p&gt;
    &lt;p&gt;PASTRAMI QUEEN: (1125 Lexington Ave. ) Pastrami Sandwich. Also the turkey with Russian dressing is not bad. Also the brisket.&lt;/p&gt;
    &lt;p&gt;EISENBERG'S SANDWICH SHOP: ( 174 5th Ave.) Tuna salad on white with lettuce. I’d suggest drinking a lime Rickey or an Arnold Palmer with that.&lt;/p&gt;
    &lt;p&gt;THE JOHN DORY OYSTER BAR: (1196 Broadway) the Carta di Musica with Bottarga and Chili is amazing. Is it a sandwich? Yes. Yes it is.&lt;/p&gt;
    &lt;p&gt;RANDOM STREET FAIRS: (Anywhere tube socks and stale spices are sold. ) New York street fairs suck. The same dreary vendors, same bad food. But those nasty sausage and pepper hero sandwiches are a siren song, luring me, always towards the rocks. Shitting like a mink almost immediately after is guaranteed but who cares?&lt;/p&gt;
    &lt;p&gt;BARNEY GREENGRASS : ( 541 Amsterdam Ave.) Chopped Liver on rye. The best chopped liver in NYC.&lt;/p&gt;
    &lt;p&gt;A work in progress&lt;/p&gt;
    &lt;p&gt;SIBERIA in any of its iterations. The one on the subway being the best&lt;/p&gt;
    &lt;p&gt;LADY ANNES FULL MOON SALOON a bar so nasty I’d bring out of town visitors there just to scare them&lt;/p&gt;
    &lt;p&gt;THE LION'S HEAD old school newspaper hang out&lt;/p&gt;
    &lt;p&gt;KELLY'S on 43rd and Lex. Notable for 25 cent drafts and regularly and reliably serving me when I was 15&lt;/p&gt;
    &lt;p&gt;THE TERMINAL BAR legendary dive across from port authority&lt;/p&gt;
    &lt;p&gt;BILLY'S TOPLESS (later, Billy’s Stopless) an atmospheric, working class place, perfect for late afternoon drinking where nobody hustled you for money and everybody knew everybody. Great all-hair metal jukebox . Naked breasts were not really the point.&lt;/p&gt;
    &lt;p&gt;THE BAR AT HAWAII KAI. tucked away in a giant tiki themed nightclub in Times Square with a midget doorman and a floor show. Best place to drop acid EVER.&lt;/p&gt;
    &lt;p&gt;THE NURSERY after hours bar decorated like a pediatrician’s office. Only the nursery rhyme characters were punk rockers of the day.&lt;/p&gt;
    &lt;p&gt;It was surprising to see that only one page was not recoverable from the common crawl.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;David Bowie Related&lt;/cell&gt;
        &lt;cell&gt;1/14/2016&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I’ve enjoyed this little project tremendously—a little archeology project. Can we declare victory for at least this endeavor? Hopefully, we would be able to find images, but that’s a little tougher, since that era’s cloudfront is fully gone.&lt;/p&gt;
    &lt;p&gt;What else can we work on restoring and setting up some sort of a public archive to store them? I made this a git repository for the sole purpose so that anyone interested can contribute their interest and passion for these kinds of projects.&lt;/p&gt;
    &lt;p&gt;Thank you and until next time! ◼︎&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sandyuraz.com/blogs/bourdain/"/><published>2025-12-13T21:18:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46258529</id><title>Some surprising things about DuckDuckGo</title><updated>2025-12-14T00:57:15.521011+00:00</updated><content>&lt;doc fingerprint="1f5ad8f558d63f3a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Some surprising things about DuckDuckGo you probably don't know&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;We have hundreds of easter-egg logos (featuring our friendly mascot Dax Brown) that surface when you make certain queries on our search engine. Our subreddit is trying to catch ‘em all. They’ve certainly caught a lot, currently 504, but we keep adding more so it’s a moving target. The total as of this post is 594. I’m the one personally adding them in my spare time just for fun and I recently did a Duck Tales episode (our new podcast) with more details on the process. This incarnation of specialty logos is relatively new, so if you are a long-term user and haven’t noticed them, that’s probably why (aside from of course that you’d have to search one of these queries and notice the subtle change in logo). And, no promises, but I am taking requests.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;There is a rumor continuously circulating that we’re owned by Google, which of course couldn’t be farther from the truth. I was actually a witness in the U.S. v. Google trial for the DOJ. I think this rumor started because Google used to own the domain duck.com and was pointing it at Google search for several years. After my public and private complaining for those same years, in 2018 we finally convinced Google to give us the duck.com domain, which we now use for our email protection service, but the rumor still persists.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We’ve been blocked in China since 2014, and are on-and-off blocked in several other countries too like Indonesia and India because we don’t censor search results.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We’ve been an independent company since our founding in 2008 and been working on our own search indexes for as many years. For over fifteen years now (that whole time) we’ve been doing our own knowledge graph index (like answers from Wikipedia), over ten years for local and other instant-answer indexes (like businesses), and in the past few years we’ve been ramping up our wider web index to support our Search Assist and Duck.ai features. DuckDuckGo began with me crawling the web in my basement, and in the early days, the FBI actually showed up at my front door since I had crawled one of their honeypots.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The plurality of our search traffic now comes from our own browsers. Yes, we have our own browsers with our search engine built in along with a ton of other protections. How do they compare to other popular browsers and extensions, you ask? We made a comparison page so you can see the differences. Our mobile browsers on iOS &amp;amp; Android launched back in 2018 (wow, that’s seven years ago), and our desktop browsers on Mac and Windows in 2022/23. Our iOS browser market share continues to climb and we’re now #3 in the U.S. (behind Safari and Chrome) and #4 on Android (behind Chrome, Samsung, and Firefox). People appreciate all the protections and the front-and-center (now customizable) fire button that quickly clears tabs and data in an (also customizable) animation of fire.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;About 13% of U.S. adults self-report as a “current user” of DuckDuckGo. That’s way more than most people think. Our search market share is lower since all of those users don’t use us on all of their devices, especially on Android where Google makes it especially hard. Once you realize that then it is less surprising that we have the highest search market share on Mac at about 4% in the U.S., followed by iOS at about 3%. I’m talking about the U.S. here since about 44% of our searches are from the U.S., and no other country is double digits, but rounding out the top ten countries are Germany, the United Kingdom, France, Canada, India, the Netherlands, Indonesia, Australia, and Japan.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Our approach to AI differs from most other companies trying to shove it down your throat in that we are dedicated to making all AI features private, useful, and optional. If you like AI, we offer private AI search answers at duckduckgo.com and private chat at duck.ai, which are built-into our browsers. If you don’t like or don’t want AI, that’s cool with us too. You can easily turn all of these features off. In fact, we made a noai.duckduckgo.com search domain that automatically sets those settings for you, including a recent setting we added that allows you to hide many AI-generated images within image search. Another related thing you might find surprising is search traffic has continued to grow steadily even since the rise of ChatGPT (with Duck.ai traffic growing even faster).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you didn’t know we have a browser, you probably also don’t know we have a DuckDuckGo Subscription (launched last year), that includes our VPN, more advanced AI models in Duck.ai, and in the U.S., Personal Information Removal and Identity Theft Restoration. It’s now available in 30 countries with a similar VPN footprint and our VPN is run by us (see latest security audit and free trials).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Speaking of lots of countries, our team has been completely distributed from the beginning, now at over 300 across about 30 countries as well, with less than half in the U.S. And we’re still hiring. We have a unique work culture that, among other things, avoids standing meetings on Wednesdays and Thursdays. We get the whole company together for a week once a year.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We played a critical role in the Global Privacy Control standard and the creation of search preference menus. I have a graduate degree in Technology and Public Policy and so we’ve done more of this kind of thing than one might expect, even going so far to draft our own Do Not Track legislation before we got GPC going. We also donate yearly to like-minded organizations (here’s our 2025 announcement), with our cumulative donations now at over $8 million. Check our donations page for details going back to 2011. We can do this since we’ve been profitable for about that long, and more recently have even started investing in related startups as well.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If this hodge-podge of stuff makes you think of anything, please let me know. I’m not only taking requests for easter-egg logo ideas, but also for stuff to write about.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gabrielweinberg.com/p/some-surprising-things-about-duckduckgo"/><published>2025-12-13T21:57:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46258906</id><title>Flat-pack washing machine spins a fairer future</title><updated>2025-12-14T00:57:14.961169+00:00</updated><content>&lt;doc fingerprint="ec0349190ebcc67a"&gt;
  &lt;main&gt;
    &lt;p&gt;A former Dyson engineer is rolling out a revolution for household chores in deprived communities after inventing an off-grid, flat-packable washing machine&lt;/p&gt;
    &lt;p&gt;Some five billion people in remote and developing regions still wash their clothes by hand. It’s a task that unfairly burdens women and young girls, who can spend up to 20 hours a week on the chore.&lt;/p&gt;
    &lt;p&gt;Enter Navjot Sawhney, who founded the UK-based social enterprise The Washing Machine Project (TWMP) to tackle this, and has now shipped almost 500 of his hand-crank Divya machines to 13 countries, including Mexico, Ghana, Iraq and the US.&lt;/p&gt;
    &lt;p&gt;The Divya washing machine, made up of an outer drum and an inner one which rotates, operates a 30-minute wash cycle where it completes a 5kg load needing only a few minutes of manual turning.&lt;/p&gt;
    &lt;p&gt;It works like this: after loading the clothes, detergent and water, and letting it sit for 10-15 minutes, users can close the lid and turn the handle for two minutes, repeating this twice more after ten minutes of letting the clothes sit in between spins. And voila — the machine can then be drained using the tap at the front.&lt;/p&gt;
    &lt;p&gt;This saves up to 75% of time for its user, and halves water consumption. “The machine takes a task that is exhausting and time-consuming and transforms it into something simple, easier to manage, and time saving,” said Sawhney.&lt;/p&gt;
    &lt;p&gt;The Divya project’s development didn’t end with its invention. “We went back to the drawing board and really listened to the people we were designing for, for the context in which they lived. That research changed everything,” said Laura Tuck, the organisation’s R&amp;amp;D Lead.&lt;/p&gt;
    &lt;p&gt;One crucial consideration was making sure Divyas were fit for the locations where they would be used. For example, in Uganda, machines were delivered to a small island on Lake Victoria using a fishing boat. Repairs or replacements could not get there easily, so the TWMP team needed to rethink how the originally complex gear-system machine could work in these conditions. The solution? Designing a product that was simpler, more intuitive, and repairable locally using the skills and infrastructure available.&lt;/p&gt;
    &lt;p&gt;Guided by feedback from real users during workshops and focus groups, TWMP improved the machine’s durability, physical strain, and usability, with the team introducing robust metal frames, simplified workflows, and improved seals and taps.&lt;/p&gt;
    &lt;p&gt;The innovation has already impacted the lives of almost 50,000 people – and Sawhney is just getting started.&lt;/p&gt;
    &lt;p&gt;TWMP hopes to reach 1,000,000 people by 2030, but says it cannot do this alone; it is building a network of partners including NGOs, UN agencies, and local communities, including the Whirlpool Foundation, the charity wing of the US-based home appliance firm.&lt;/p&gt;
    &lt;p&gt;Localised production will begin in early 2026, manufacturing a new generation of machines in India, closer to those who use them. The project is also piloting ‘Hubs’, where machines can be assembled and distributed, but also offering training, workshops, and educational activities, extending the impact of the time saved by Divya machines.&lt;/p&gt;
    &lt;p&gt;It is also seeking policy engagement to embed laundry access in wider strategies around water, sanitation, hygiene, and gender equality.&lt;/p&gt;
    &lt;p&gt;Images: The Washing Machine Project&lt;/p&gt;
    &lt;head rend="h3"&gt;Be part of the solution&lt;/head&gt;
    &lt;p&gt;At Positive News, we’re not chasing clicks or profits for media moguls – we’re here to serve you and have a positive social impact. We can’t do this unless enough people like you choose to support our journalism.&lt;/p&gt;
    &lt;p&gt;Give once from just £1, or join 1,700+ others who contribute an average of £3 or more per month. Together, we can build a healthier form of media – one that focuses on solutions, progress and possibilities, and empowers people to create positive change.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.positive.news/society/flat-pack-washing-machine-spins-a-fairer-future/"/><published>2025-12-13T22:38:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46259064</id><title>Linux Sandboxes and Fil-C</title><updated>2025-12-14T00:57:14.597477+00:00</updated><content>&lt;doc fingerprint="ea639991fc2930e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linux Sandboxes And Fil-C&lt;/head&gt;
    &lt;p&gt;Memory safety and sandboxing are two different things. It's reasonable to think of them as orthogonal: you could have memory safety but not be sandboxed, or you could be sandboxed but not memory safe.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Example of memory safe but not sandboxed: a pure Java program that opens files on the filesystem for reading and writing and accepts filenames from the user. The OS will allow this program to overwrite any file that the user has access to. This program can be quite dangerous even if it is memory safe. Worse, imagine that the program didn't have any code to open files for reading and writing, but also had no sandbox to prevent those syscalls from working. If there was a bug in the memory safety enforcement of this program (say, because of a bug in the Java implementation), then an attacker could cause this program to overwrite any file if they succeeded at achieving code execution via weird state.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Example of sandboxed but not memory safe: a program written in assembly that starts by requesting that the OS revoke all of its capabilities beyond just pure compute. If the program did want to open a file or write to it, then the kernel will kill the process, based on the earlier request to have this capability revoked. This program could have lots of memory safety bugs (because it's written in assembly), but even if it did, then the attacker cannot make this program overwrite any file unless they find some way to bypass the sandbox.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In practice, sandboxes have holes by design. A typical sandbox allows the program to send and receive messages to broker processes that have higher privileges. So, an attacker may first use a memory safety bug to make the sandboxed process send malicious messages, and then use those malicious messages to break into the brokers.&lt;/p&gt;
    &lt;p&gt;The best kind of defense is to have both a sandbox and memory safety. This document describes how to combine sandboxing and Fil-C's memory safety by explaining what it takes to port OpenSSH's seccomp-based Linux sandbox code to Fil-C.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Fil-C is a memory safe implementation of C and C++ and this site has a lot of documentation about it. Unlike most memory safe languages, Fil-C enforces safety down to where your code meets Linux syscalls and the Fil-C runtime is robust enough that it's possible to use it in low-level system components like &lt;code&gt;init&lt;/code&gt; and &lt;code&gt;udevd&lt;/code&gt;. Lots of programs work in Fil-C, including OpenSSH, which makes use of seccomp-BPF sandboxing.&lt;/p&gt;
    &lt;p&gt;This document focuses on how OpenSSH uses seccomp and other technologies on Linux to build a sandbox around its unprivileged &lt;code&gt;sshd-session&lt;/code&gt; process. Let's review what tools Linux gives us that OpenSSH uses:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;chroot&lt;/code&gt;to restrict the process's view of the filesystem.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running the process with the&lt;/p&gt;&lt;code&gt;sshd&lt;/code&gt;user and group, and giving that user/group no privileges.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setrlimit&lt;/code&gt;to prevent opening files, starting processes, or writing to files.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;seccomp-BPF syscall filter to reduce the attack surface by allowlisting only the set of syscalls that are legitimate for the unprivileged process. Syscalls not in the allowlist will crash the process with&lt;/p&gt;&lt;code&gt;SIGSYS&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Chromium developers and the Mozilla developers both have excellent notes about how to do sandboxing on Linux using seccomp. Seccomp-BPF is a well-documented kernel feature that can be used as part of a larger sandboxing story.&lt;/p&gt;
    &lt;p&gt;Fil-C makes it easy to use &lt;code&gt;chroot&lt;/code&gt; and different users and groups. The syscalls that are used for that part of the sandbox are trivially allowed by Fil-C and no special care is required to use them.&lt;/p&gt;
    &lt;p&gt;Both &lt;code&gt;setrlimit&lt;/code&gt; and seccomp-BPF require special care because the Fil-C runtime starts threads, allocates memory, and performs synchronization. This document describes what you need to know to make effective use of those sandboxing technologies in Fil-C. First, I describe how to build a sandbox that prevents thread creation without breaking Fil-C's use of threads. Then, I describe what tweaks I had to make to OpenSSH's seccomp filter. Finally, I describe how the Fil-C runtime implements the syscalls used to install seccomp filters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preventing Thread Creation Without Breaking The Fil-C Runtime&lt;/head&gt;
    &lt;p&gt;The Fil-C runtime uses multiple background threads for garbage collection and has the ability to automatically shut those threads down when they are not in use. If the program wakes up and starts allocating memory again, then those threads are automatically restarted.&lt;/p&gt;
    &lt;p&gt;Starting threads violates the "no new processes" rule that OpenSSH's &lt;code&gt;setrlimit&lt;/code&gt; sandbox tries to achieve (since threads are just lightweight processes on Linux). It also relies on syscalls like &lt;code&gt;clone3&lt;/code&gt; that are not part of OpenSSH's seccomp filter allowlist.&lt;/p&gt;
    &lt;p&gt;It would be a regression to the sandbox to allow process creation just because the Fil-C runtime relies on it. Instead, I added a new API to &lt;code&gt;&amp;lt;stdfil.h&amp;gt;&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;void zlock_runtime_threads(void);
&lt;/code&gt;
    &lt;p&gt;This forces the runtime to immediately create whatever threads it needs, and to disable shutting them down on demand. Then, I added a call to &lt;code&gt;zlock_runtime_threads()&lt;/code&gt; in OpenSSH's &lt;code&gt;ssh_sandbox_child&lt;/code&gt; function before either the &lt;code&gt;setrlimit&lt;/code&gt; or seccomp-BPF sandbox calls happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tweaks To The OpenSSH Sandbox&lt;/head&gt;
    &lt;p&gt;Because the use of &lt;code&gt;zlock_runtime_threads()&lt;/code&gt; prevents subsequent thread creation from happening, most of the OpenSSH sandbox just works. I did not have to change how OpenSSH uses &lt;code&gt;setrlimit&lt;/code&gt;. I did change the following about the seccomp filter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Failure results in&lt;/p&gt;&lt;code&gt;SECCOMP_RET_KILL_PROCESS&lt;/code&gt;rather than&lt;code&gt;SECCOMP_RET_KILL&lt;/code&gt;. This ensures that Fil-C's background threads are also killed if a sandbox violation occurs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MAP_NORESERVE&lt;/code&gt;is added to the&lt;code&gt;mmap&lt;/code&gt;allowlist, since the Fil-C allocator uses it. This is not a meaningful regression to the filter, since&lt;code&gt;MAP_NORESERVE&lt;/code&gt;is not a meaningful capability for an attacker to have.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sched_yield&lt;/code&gt;is allowed. This is not a dangerous syscall (it's semantically a no-op). The Fil-C runtime uses it as part of its lock implementation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nothing else had to change, since the filter already allowed all of the &lt;code&gt;futex&lt;/code&gt; syscalls that Fil-C uses for synchronization.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Fil-C Implements &lt;code&gt;prctl&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The OpenSSH seccomp filter is installed using two &lt;code&gt;prctl&lt;/code&gt; calls. First, we &lt;code&gt;PR_SET_NO_NEW_PRIVS&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) == -1) {
        debug("%s: prctl(PR_SET_NO_NEW_PRIVS): %s",
            __func__, strerror(errno));
        nnp_failed = 1;
}
&lt;/code&gt;
    &lt;p&gt;This prevents additional privileges from being acquired via &lt;code&gt;execve&lt;/code&gt;. It's required that unprivileged processes that install seccomp filters first set the &lt;code&gt;no_new_privs&lt;/code&gt; bit.&lt;/p&gt;
    &lt;p&gt;Next, we &lt;code&gt;PR_SET_SECCOMP, SECCOMP_MODE_FILTER&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;if (prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, &amp;amp;preauth_program) == -1)
        debug("%s: prctl(PR_SET_SECCOMP): %s",
            __func__, strerror(errno));
else if (nnp_failed)
        fatal("%s: SECCOMP_MODE_FILTER activated but "
            "PR_SET_NO_NEW_PRIVS failed", __func__);
&lt;/code&gt;
    &lt;p&gt;This installs the seccomp filter in &lt;code&gt;preauth_program&lt;/code&gt;. Note that this will fail in the kernel if the &lt;code&gt;no_new_privs&lt;/code&gt; bit is not set, so the fact that OpenSSH reports a fatal error if the filter is installed without &lt;code&gt;no_new_privs&lt;/code&gt; is just healthy paranoia on the part of the OpenSSH authors.&lt;/p&gt;
    &lt;p&gt;The trouble with both syscalls is that they affect the calling thread, not all threads in the process. Without special care, Fil-C runtime's background threads would not have the &lt;code&gt;no_new_privs&lt;/code&gt; bit set and would not have the filter installed. This would mean that if an attacker busted through Fil-C's memory safety protections (in the unlikely event that they found a bug in Fil-C itself!), then they could use those other threads to execute syscalls that bypass the filter!&lt;/p&gt;
    &lt;p&gt;To prevent even this unlikely escape, the Fil-C runtime's wrapper for &lt;code&gt;prctl&lt;/code&gt; implements &lt;code&gt;PR_SET_NO_NEW_PRIVS&lt;/code&gt; and &lt;code&gt;PR_SET_SECCOMP&lt;/code&gt; by handshaking all runtime threads using this internal API:&lt;/p&gt;
    &lt;code&gt;/* Calls the callback from every runtime thread. */
PAS_API void filc_runtime_threads_handshake(void (*callback)(void* arg), void* arg);
&lt;/code&gt;
    &lt;p&gt;The callback performs the requested &lt;code&gt;prctl&lt;/code&gt; from each runtime thread. This ensures that the &lt;code&gt;no_new_privs&lt;/code&gt; bit and the filter are installed on all threads in the Fil-C process.&lt;/p&gt;
    &lt;p&gt;Additionally, because of ambiguity about what to do if the process has multiple user threads, these two &lt;code&gt;prctl&lt;/code&gt; commands will trigger a Fil-C safety error if the program has multiple user threads.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The best kind of protection if you're serious about security is to combine memory safety with sandboxing. This document shows how to achieve this using Fil-C and the sandbox technologies available on Linux, all without regressing the level of protection that those sandboxes enforce or the memory safety guarantees of Fil-C.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fil-c.org/seccomp"/><published>2025-12-13T22:58:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46259095</id><title>RemoveWindowsAI</title><updated>2025-12-14T00:57:14.001322+00:00</updated><content>&lt;doc fingerprint="188fb96ee857a74d"&gt;
  &lt;main&gt;
    &lt;p&gt;The current 25H2 build of Windows 11 and future builds will include increasingly more AI features and components. This script aims to remove ALL of these features to improve user experience, privacy and security.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable Registry Keys &lt;list rend="ul"&gt;&lt;item&gt;Disable Copilot&lt;/item&gt;&lt;item&gt;Disable Recall&lt;/item&gt;&lt;item&gt;Disable Input Insights and typing data harvesting&lt;/item&gt;&lt;item&gt;Copilot in Edge&lt;/item&gt;&lt;item&gt;Image Creator in Paint&lt;/item&gt;&lt;item&gt;Remove AI Fabric Service&lt;/item&gt;&lt;item&gt;Disable AI Actions&lt;/item&gt;&lt;item&gt;Disable AI in Paint&lt;/item&gt;&lt;item&gt;Disable Voice Access&lt;/item&gt;&lt;item&gt;Disable AI Voice Effects&lt;/item&gt;&lt;item&gt;Disable AI in Settings Search&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Prevent Reinstall of AI Packages &lt;list rend="ul"&gt;&lt;item&gt;Installs custom Windows Update package to prevent reinstall of AI packages in the CBS (Component-Based Servicing) store&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Disable Copilot policies &lt;list rend="ul"&gt;&lt;item&gt;Disables policies related to Copilot and Recall in IntegratedServicesRegionPolicySet.json&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Remove AI Appx Packages &lt;list rend="ul"&gt;&lt;item&gt;Removes all AI appx packages including &lt;code&gt;Nonremovable&lt;/code&gt;packages and WindowsWorkload&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Removes all AI appx packages including &lt;/item&gt;
      &lt;item&gt;Remove Recall Optional Feature&lt;/item&gt;
      &lt;item&gt;Remove AI Packages in CBS &lt;list rend="ul"&gt;&lt;item&gt;This will remove hidden and locked AI packages in the CBS (Component-Based Servicing) store&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Remove AI Files &lt;list rend="ul"&gt;&lt;item&gt;This will do a full system cleanup removing all remaining AI installers, registry keys, and package files&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Hide AI Components &lt;list rend="ul"&gt;&lt;item&gt;This will hide the settings page &lt;code&gt;AI Components&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;This will hide the settings page &lt;/item&gt;
      &lt;item&gt;Disable Rewrite AI Feature in Notepad&lt;/item&gt;
      &lt;item&gt;Remove Recall Tasks &lt;list rend="ul"&gt;&lt;item&gt;Forceably removes all instances of Recall's scheduled tasks&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unfourtently, not all features and settings can be disabled via a script. This guide will show additional AI features to disable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;Some third party anti-viruses will falsely detect the script as malicious, obviously this is a false positive and the anti-virus will need to be temporarily disabled or set the script as an exclusion.&lt;/p&gt;
    &lt;p&gt;Due to the nature of making advanced changes to the system many debloat tools/scripts will be falsely detected as malware... if you are unsure about the script I always recommend testing any software in a virtual machine first&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;Running the script with PowerShell 7 can cause issues, to avoid this ensure you are running Windows PowerShell (5.1)&lt;/p&gt;
    &lt;code&gt;&amp;amp; ([scriptblock]::Create((irm "https://raw.githubusercontent.com/zoicware/RemoveWindowsAI/main/RemoveWindowsAi.ps1")))&lt;/code&gt;
    &lt;head rend="h5"&gt;Link shortened using open source link shortener: https://kutt.it/&lt;/head&gt;
    &lt;code&gt;&amp;amp; ([scriptblock]::Create((irm 'https://kutt.it/RWAI')))&lt;/code&gt;
    &lt;p&gt;Run in Non-Interactive Mode with All Options&lt;/p&gt;
    &lt;code&gt;&amp;amp; ([scriptblock]::Create((irm "https://raw.githubusercontent.com/zoicware/RemoveWindowsAI/main/RemoveWindowsAi.ps1"))) -nonInteractive -AllOptions&lt;/code&gt;
    &lt;p&gt;Run with Specific Options Example&lt;/p&gt;
    &lt;code&gt;&amp;amp; ([scriptblock]::Create((irm "https://raw.githubusercontent.com/zoicware/RemoveWindowsAI/main/RemoveWindowsAi.ps1"))) -nonInteractive -Options DisableRegKeys,RemoveNudgesKeys,RemoveAppxPackages&lt;/code&gt;
    &lt;p&gt;All Possible Options:&lt;/p&gt;
    &lt;code&gt;DisableRegKeys          
PreventAIPackageReinstall     
DisableCopilotPolicies       
RemoveAppxPackages        
RemoveRecallFeature 
RemoveCBSPackages         
RemoveAIFiles               
HideAIComponents            
DisableRewrite      
RemoveRecallTasks
&lt;/code&gt;
    &lt;p&gt;Run with Backup Mode Enabled&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Backup Mode needs to be enabled to be able to fully revert&lt;/p&gt;
    &lt;code&gt;&amp;amp; ([scriptblock]::Create((irm "https://raw.githubusercontent.com/zoicware/RemoveWindowsAI/main/RemoveWindowsAi.ps1"))) -nonInteractive -backupMode -AllOptions&lt;/code&gt;
    &lt;p&gt;Revert Changes&lt;/p&gt;
    &lt;code&gt;&amp;amp; ([scriptblock]::Create((irm "https://raw.githubusercontent.com/zoicware/RemoveWindowsAI/main/RemoveWindowsAi.ps1"))) -nonInteractive -revertMode -AllOptions&lt;/code&gt;
    &lt;p&gt;Given that Microsoft are continually updating and adding new AI features this script will attempt to stay updated for the newest stable build.&lt;/p&gt;
    &lt;p&gt;You can view the newest updates to the script here: https://github.com/zoicware/RemoveWindowsAI/commits/main/&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Any feature added to an insider build will not be added to this script till it's added to the latest stable release&lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Submitting An AI Feature&lt;/p&gt;
    &lt;p&gt;If you find an AI feature or registry key that is not currently removed or disabled by the script submit an issue with as much information as possible and I will add it to the script.&lt;/p&gt;
    &lt;p&gt;If you would like to support my work consider donating :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/zoicware/RemoveWindowsAI"/><published>2025-12-13T23:04:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46259334</id><title>Closures as Win32 Window Procedures</title><updated>2025-12-14T00:57:13.796525+00:00</updated><content>&lt;doc fingerprint="a6c91f3926ae8748"&gt;
  &lt;main&gt;
    &lt;p&gt; nullprogram.com/blog/2025/12/12/ &lt;/p&gt;
    &lt;p&gt;Back in 2017 I wrote about a technique for creating closures in C using JIT-compiled wrapper. It’s neat, though rarely necessary in real programs, so I don’t think about it often. I applied it to &lt;code&gt;qsort&lt;/code&gt;,
which sadly accepts no context pointer. More practical would be
working around insufficient custom allocator interfaces, to
create allocation functions at run-time bound to a particular allocation
region. I’ve learned a lot since I last wrote about this subject, and a
recent article had me thinking about it again, and how I could do
better than before. In this article I will enhance Win32 window procedure
callbacks with a fifth argument, allowing us to more directly pass extra
context. I’m using w64devkit on x64, but the everything here should
work out-of-the-box with any x64 toolchain that speaks GNU assembly.&lt;/p&gt;
    &lt;p&gt;A window procedure has this prototype:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;LRESULT Wndproc(
  HWND hWnd,
  UINT Msg,
  WPARAM wParam,
  LPARAM lParam,
);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To create a window we must first register a class with &lt;code&gt;RegisterClass&lt;/code&gt;,
which accepts a set of properties describing a window class, including a
pointer to one of these functions.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    MyState *state = ...;

    RegisterClassA(&amp;amp;(WNDCLASSA){
        // ...
        .lpfnWndProc   = my_wndproc,
        .lpszClassName = "my_class",
        // ...
    });

    HWND hwnd = CreateWindowExA("my_class", ..., state);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The thread drives a message pump with events from the operating system, dispatching them to this procedure, which then manipulates the program state in response:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    for (MSG msg; GetMessageW(&amp;amp;msg, 0, 0, 0);) {
        TranslateMessage(&amp;amp;msg);
        DispatchMessageW(&amp;amp;msg);  // calls the window procedure
    }
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;All four &lt;code&gt;WNDPROC&lt;/code&gt; parameters are determined by Win32. There is no context
pointer argument. So how does this procedure access the program state? We
generally have two options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Global variables. Yucky but easy. Frequently seen in tutorials.&lt;/item&gt;
      &lt;item&gt;A &lt;code&gt;GWLP_USERDATA&lt;/code&gt; pointer attached to the window.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The second option takes some setup. Win32 passes the last &lt;code&gt;CreateWindowEx&lt;/code&gt;
argument to the window procedure when the window created, via &lt;code&gt;WM_CREATE&lt;/code&gt;.
The procedure attaches the pointer to its window as &lt;code&gt;GWLP_USERDATA&lt;/code&gt;. This
pointer is passed indirectly, through a &lt;code&gt;CREATESTRUCT&lt;/code&gt;. So ultimately it
looks like this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    case WM_CREATE:
        CREATESTRUCT *cs = (CREATESTRUCT *)lParam;
        void *arg = (struct state *)cs-&amp;gt;lpCreateParams;
        SetWindowLongPtr(hwnd, GWLP_USERDATA, (LONG_PTR)arg);
        // ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;In future messages we can retrieve it with &lt;code&gt;GetWindowLongPtr&lt;/code&gt;. Every time
I go through this I wish there was a better way. What if there was a fifth
window procedure parameter though which we could pass a context?&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef LRESULT Wndproc5(HWND, UINT, WPARAM, LPARAM, void *);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;We’ll build just this as a trampoline. The x64 calling convention passes the first four arguments in registers, and the rest are pushed on the stack, including this new parameter. Our trampoline cannot just stuff the extra parameter in the register, but will actually have to build a stack frame. Slightly more complicated, but barely so.&lt;/p&gt;
    &lt;head rend="h3"&gt;Allocating executable memory&lt;/head&gt;
    &lt;p&gt;In previous articles, and in the programs where I’ve applied techniques like this, I’ve allocated executable memory with &lt;code&gt;VirtualAlloc&lt;/code&gt; (or &lt;code&gt;mmap&lt;/code&gt;
elsewhere). This introduces a small challenge for solving the problem
generally: Allocations may be arbitrarily far from our code and data, out
of reach of relative addressing. If they’re further than 2G apart, we need
to encode absolute addresses, and in the simple case would just assume
they’re always too far apart.&lt;/p&gt;
    &lt;p&gt;These days I’ve more experience with executable formats, and allocation, and I immediately see a better solution: Request a block of writable, executable memory from the loader, then allocate our trampolines from it. Other than being executable, this memory isn’t special, and allocation works the usual way, using functions unaware it’s executable. By allocating through the loader, this memory will be part of our loaded image, guaranteed to be close to our other code and data, allowing our JIT compiler to assume a small code model.&lt;/p&gt;
    &lt;p&gt;There are a number of ways to do this, and here’s one way to do it with GNU-styled toolchains targeting COFF:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;        .section .exebuf,"bwx"
        .globl exebuf
exebuf:	.space 1&amp;lt;&amp;lt;21
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;This assembly program defines a new section named &lt;code&gt;.exebuf&lt;/code&gt; containing 2M
of writable (&lt;code&gt;"w"&lt;/code&gt;), executable (&lt;code&gt;"x"&lt;/code&gt;) memory, allocated at run time just
like &lt;code&gt;.bss&lt;/code&gt; (&lt;code&gt;"b"&lt;/code&gt;). We’ll treat this like an arena out of which we can
allocate all trampolines we’ll probably ever need. With careful use of
&lt;code&gt;.pushsection&lt;/code&gt; this could be basic inline assembly, but I’ve left it as a
separate source. On the C side I retrieve this like so:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef struct {
    char *beg;
    char *end;
} Arena;

Arena get_exebuf()
{
    extern char exebuf[1&amp;lt;&amp;lt;21];
    Arena r = {exebuf, exebuf+sizeof(exebuf)};
    return r;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Unfortunately I have to repeat myself on the size. There are different ways to deal with this, but this is simple enough for now. I would have loved to define the array in C with the GCC &lt;code&gt;section&lt;/code&gt; attribute,
but as is usually the case with this attribute, it’s not up to the task,
lacking the ability to set section flags. Besides, by not relying on the
attribute, any C compiler could compile this source, and we only need a
GNU-style toolchain to create the tiny COFF object containing &lt;code&gt;exebuf&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;While we’re at it, a reminder of some other basic definitions we’ll need:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;#define S(s)            (Str){s, sizeof(s)-1}
#define new(a, n, t)    (t *)alloc(a, n, sizeof(t), _Alignof(t))

typedef struct {
    char     *data;
    ptrdiff_t len;
} Str;

Str clone(Arena *a, Str s)
{
    Str r = s;
    r.data = new(a, r.len, char);
    memcpy(r.data, s.data, (size_t)r.len);
    return r;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Which have been discussed at length in previous articles.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trampoline compiler&lt;/head&gt;
    &lt;p&gt;From here the plan is to create a function that accepts a &lt;code&gt;Wndproc5&lt;/code&gt; and a
context pointer to bind, and returns a classic &lt;code&gt;WNDPROC&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;WNDPROC make_wndproc(Arena *, Wndproc5, void *arg);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Our window procedure now gets a fifth argument with the program state:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;LRESULT my_wndproc(HWND, UINT, WPARAM, LPARAM, void *arg)
{
    MyState *state = arg;
    // ...
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;When registering the class we wrap it in a trampoline compatible with &lt;code&gt;RegisterClass&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    RegisterClassA(&amp;amp;(WNDCLASSA){
        // ...
        .lpfnWndProc   = make_wndproc(a, my_wndproc, state),
        .lpszClassName = "my_class",
        // ...
    });
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;All windows using this class will readily have access to this state object through their fifth parameter. It turns out setting up &lt;code&gt;exebuf&lt;/code&gt; was the
more complicated part, and &lt;code&gt;make_wndproc&lt;/code&gt; is quite simple!&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;WNDPROC make_wndproc(Arena *a, Wndproc5 proc, void *arg)
{
    Str thunk = S(
        "\x48\x83\xec\x28"      // sub   $40, %rsp
        "\x48\xb8........"      // movq  $arg, %rax
        "\x48\x89\x44\x24\x20"  // mov   %rax, 32(%rsp)
        "\xe8...."              // call  proc
        "\x48\x83\xc4\x28"      // add   $40, %rsp
        "\xc3"                  // ret
    );
    Str r   = clone(a, thunk);
    int rel = (int)((uintptr_t)proc - (uintptr_t)(r.data + 24));
    memcpy(r.data+ 6, &amp;amp;arg, sizeof(arg));
    memcpy(r.data+20, &amp;amp;rel, sizeof(rel));
    return (WNDPROC)r.data;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The assembly allocates a new stack frame, with callee shadow space, and with room for the new argument, which also happens to re-align the stack. It stores the new argument for the &lt;code&gt;Wndproc5&lt;/code&gt; just above the shadow space.
Then calls into the &lt;code&gt;Wndproc5&lt;/code&gt; without touching other parameters. There
are two “patches” to fill out, which I’ve initially filled with dots: the
context pointer itself, and a 32-bit signed relative address for the call.
It’s going to be very near the callee. The only thing I don’t like about
this function is that I’ve manually worked out the patch offsets.&lt;/p&gt;
    &lt;p&gt;It’s probably not useful, but it’s easy to update the context pointer at any time if hold onto the trampoline pointer:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;void set_wndproc_arg(WNDPROC p, void *arg)
{
    memcpy((char *)p+6, &amp;amp;arg, sizeof(arg));
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;So, for instance:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    MyState *state[2] = ...;  // multiple states
    WNDPROC proc = make_wndproc(a, my_wndproc, state[0]);
    // ...
    set_wndproc_arg(proc, state[1]);  // switch states
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Though I expect the most common case is just creating multiple procedures:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    WNDPROC procs[] = {
        make_wndproc(a, my_wndproc, state[0]),
        make_wndproc(a, my_wndproc, state[1]),
    };
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To my slight surprise these trampolines still work with an active Control Flow Guard system policy. Trampolines do not have stack unwind entries, and I thought Windows might refuse to pass control to them.&lt;/p&gt;
    &lt;p&gt;Here’s a complete, runnable example if you’d like to try it yourself: &lt;code&gt;main.c&lt;/code&gt; and &lt;code&gt;exebuf.s&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Better cases&lt;/head&gt;
    &lt;p&gt;This is more work than going through &lt;code&gt;GWLP_USERDATA&lt;/code&gt;, and real programs
have a small, fixed number of window procedures — typically one — so this
isn’t the best example, but I wanted to illustrate with a real interface.
Again, perhaps the best real use is a library with a weak custom allocator
interface:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef struct {
    void *(*malloc)(size_t);   // no context pointer!
    void  (*free)(void *);     // "
} Allocator;

void *arena_malloc(size_t, Arena *);

// ...

    Allocator perm_allocator = {
        .malloc = make_trampoline(exearena, arena_malloc, perm);
        .free   = noop_free,
    };
    Allocator scratch_allocator = {
        .malloc = make_trampoline(exearena, arena_malloc, scratch);
        .free   = noop_free,
    };
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Something to keep in my back pocket for the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nullprogram.com/blog/2025/12/12/"/><published>2025-12-13T23:39:48+00:00</published></entry></feed>