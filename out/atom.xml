<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-18T22:10:12.577945+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46666085</id><title>Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)</title><updated>2026-01-18T22:10:19.901364+00:00</updated><content>&lt;doc fingerprint="177a599341632ef6"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Command-line Tools can be 235x Faster than your Hadoop Cluster&lt;/head&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;As I was browsing the web and catching up on some sites I visit periodically, I found a cool article from Tom Hayden about using Amazon Elastic Map Reduce (EMR) and mrjob in order to compute some statistics on win/loss ratios for chess games he downloaded from the millionbase archive, and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec).&lt;/p&gt;
    &lt;p&gt;After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-called Big Data (tm) tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques.&lt;/p&gt;
    &lt;p&gt;One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your own Storm cluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands between them. You can pretty easily construct a stream processing pipeline with basic commands that will have extremely good performance compared to many modern Big Data (tm) tools.&lt;/p&gt;
    &lt;p&gt;An additional point is the batch versus streaming analysis approach. Tom mentions in the beginning of the piece that after loading 10000 games and doing the analysis locally, that he gets a bit short on memory. This is because all game data is loaded into RAM for the analysis. However, considering the problem for a bit, it can be easily solved with streaming analysis that requires basically no memory at all. The resulting stream processing pipeline we will create will be over 235 times faster than the Hadoop implementation and use virtually no memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learn about the data&lt;/head&gt;
    &lt;p&gt;The first step in the pipeline is to get the data out of the PGN files. Since I had no idea what kind of format this was, I checked it out on Wikipedia.&lt;/p&gt;
    &lt;code&gt;[Event "F/S Return Match"]
[Site "Belgrade, Serbia Yugoslavia|JUG"]
[Date "1992.11.04"]
[Round "29"]
[White "Fischer, Robert J."]
[Black "Spassky, Boris V."]
[Result "1/2-1/2"]
(moves from the game follow...)
&lt;/code&gt;
    &lt;p&gt;We are only interested in the results of the game, which only have 3 real outcomes. The 1-0 case means that white won, the 0-1 case means that black won, and the 1/2-1/2 case means the game was a draw. There is also a - case meaning the game is ongoing or cannot be scored, but we ignore that for our purposes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acquire sample data&lt;/head&gt;
    &lt;p&gt;The first thing to do is get a lot of game data. This proved more difficult than I thought it would be, but after some looking around online I found a git repository on GitHub from rozim that had plenty of games. I used this to compile a set of 3.46GB of data, which is about twice what Tom used in his test. The next step is to get all that data into our pipeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build a processing pipeline&lt;/head&gt;
    &lt;p&gt;If you are following along and timing your processing, don‚Äôt forget to clear your OS page cache as otherwise you won‚Äôt get valid processing times.&lt;/p&gt;
    &lt;p&gt;Shell commands are great for data processing pipelines because you get parallelism for free. For proof, try a simple example in your terminal.&lt;/p&gt;
    &lt;code&gt;sleep 3 | echo "Hello world."
&lt;/code&gt;
    &lt;p&gt;Intuitively it may seem that the above will sleep for 3 seconds and then print &lt;code&gt;Hello world&lt;/code&gt; but in fact both steps are done at the same time. This basic fact is what can offer such great speedups for simple non-IO-bound processing systems capable of running on a single machine.&lt;/p&gt;
    &lt;p&gt;Before starting the analysis pipeline, it is good to get a reference for how fast it could be and for this we can simply dump the data to &lt;code&gt;/dev/null&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cat *.pgn &amp;gt; /dev/null
&lt;/code&gt;
    &lt;p&gt;In this case, it takes about 13 seconds to go through the 3.46GB, which is about 272MB/sec. This would be a kind of upper-bound on how quickly data could be processed on this system due to IO constraints.&lt;/p&gt;
    &lt;p&gt;Now we can start on the analysis pipeline, the first step of which is using &lt;code&gt;cat&lt;/code&gt; to generate the stream of data.&lt;/p&gt;
    &lt;code&gt;cat *.pgn
&lt;/code&gt;
    &lt;p&gt;Since only the result lines in the files are interesting, we can simply scan through all the data files, and pick out the lines containing ‚ÄòResults‚Äô with &lt;code&gt;grep&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result"
&lt;/code&gt;
    &lt;p&gt;This will give us only the &lt;code&gt;Result&lt;/code&gt; lines from the files. Now if we want, we can simply use the &lt;code&gt;sort&lt;/code&gt; and &lt;code&gt;uniq&lt;/code&gt; commands in order to get a list of all the unique items in the file along with their counts.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result" | sort | uniq -c
&lt;/code&gt;
    &lt;p&gt;This is a very straightforward analysis pipeline, and gives us the results in about 70 seconds. While we can certainly do better, assuming linear scaling this would have taken the Hadoop cluster approximately 52 minutes to process.&lt;/p&gt;
    &lt;p&gt;In order to reduce the speed further, we can take out the &lt;code&gt;sort | uniq&lt;/code&gt; steps from the pipeline, and replace them with AWK, which is a wonderful tool/language for event-based data processing.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result" | awk '{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This will take each result record, split it on the hyphen, and take the character immediately to the left, which will be a 0 in the case of a win for black, a 1 in the case of a win for white, or a 2 in the case of a draw. Note that &lt;code&gt;$0&lt;/code&gt; is a built-in variable that represents the entire record.&lt;/p&gt;
    &lt;p&gt;This reduces the running time to approximately 65 seconds, and since we‚Äôre processing twice as much data this is a speedup of around 47 times.&lt;/p&gt;
    &lt;p&gt;So even at this point we already have a speedup of around 47 with a naive local solution. Additionally, the memory usage is effectively zero since the only data stored is the actual counts, and incrementing 3 integers is almost free in memory space terms. However, looking at &lt;code&gt;htop&lt;/code&gt; while this is running shows that &lt;code&gt;grep&lt;/code&gt; is currently the bottleneck with full usage of a single CPU core.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallelize the bottlenecks&lt;/head&gt;
    &lt;p&gt;This problem of unused cores can be fixed with the wonderful &lt;code&gt;xargs&lt;/code&gt; command, which will allow us to parallelize the &lt;code&gt;grep&lt;/code&gt;. Since &lt;code&gt;xargs&lt;/code&gt; expects input in a certain way, it is safer and easier to use &lt;code&gt;find&lt;/code&gt; with the &lt;code&gt;-print0&lt;/code&gt; argument in order to make sure that each file name being passed to &lt;code&gt;xargs&lt;/code&gt; is null-terminated. The corresponding &lt;code&gt;-0&lt;/code&gt; tells &lt;code&gt;xargs&lt;/code&gt; to expected null-terminated input. Additionally, the &lt;code&gt;-n&lt;/code&gt; how many inputs to give each process and the &lt;code&gt;-P&lt;/code&gt; indicates the number of processes to run in parallel. Also important to be aware of is that such a parallel pipeline doesn‚Äôt guarantee delivery order, but this isn‚Äôt a problem if you are used to dealing with distributed processing systems. The &lt;code&gt;-F&lt;/code&gt; for &lt;code&gt;grep&lt;/code&gt; indicates that we are only matching on fixed strings and not doing any fancy regex, and can offer a small speedup, which I did not notice in my testing.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n1 -P4 grep -F "Result" | gawk '{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print NR, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This results in a run time of about 38 seconds, which is an additional 40% or so reduction in processing time from parallelizing the &lt;code&gt;grep&lt;/code&gt; step in our pipeline. This gets us up to approximately 77 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;p&gt;Although we have improved the performance dramatically by parallelizing the &lt;code&gt;grep&lt;/code&gt; step in our pipeline, we can actually remove this entirely by having &lt;code&gt;awk&lt;/code&gt; filter the input records (lines in this case) and only operate on those containing the string ‚ÄúResult‚Äù.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n1 -P4 awk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;You may think that would be the correct solution, but this will output the results of each file individually, when we want to aggregate them all together. The resulting correct implementation is conceptually very similar to what the MapReduce implementation would be.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n4 -P4 awk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }' | awk '{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;By adding the second awk step at the end, we obtain the aggregated game information as desired.&lt;/p&gt;
    &lt;p&gt;This further improves the speed dramatically, achieving a running time of about 18 seconds, or about 174 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;p&gt;However, we can make it a bit faster still by using mawk, which is often a drop-in replacement for &lt;code&gt;gawk&lt;/code&gt; and can offer better performance.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n4 -P4 mawk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }' | mawk '{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This &lt;code&gt;find | xargs mawk | mawk&lt;/code&gt; pipeline gets us down to a runtime of about 12 seconds, or about 270MB/sec, which is around 235 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Hopefully this has illustrated some points about using and abusing tools like Hadoop for data processing tasks that can better be accomplished on a single machine with simple shell commands and tools. If you have a huge amount of data or really need distributed processing, then tools like Hadoop may be required, but more often than not these days I see Hadoop used where a traditional relational database or other solutions would be far better in terms of performance, cost of implementation, and ongoing maintenance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html"/><published>2026-01-18T08:58:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666288</id><title>A free and open-source rootkit for Linux</title><updated>2026-01-18T22:10:19.622502+00:00</updated><content>&lt;doc fingerprint="765bd95e21258ee5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A free and open-source rootkit for Linux&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While there are several rootkits that target Linux, they have so far not fully embraced the open-source ethos typical of Linux software. Luckily, Matheus Alves has been working to remedy this lack by creating an open-source rootkit called Singularity for Linux systems. Users who feel their computers are too secure can install the Singularity kernel module in order to allow remote code execution, disable security features, and hide files and processes from normal administrative tools. Despite its many features, Singularity is not currently known to be in use in the wild ‚Äî instead, it provides security researchers with a testbed to investigate new detection and evasion techniques.&lt;/p&gt;
    &lt;p&gt; Alves is quite emphatic about the research nature of Singularity, saying that its main purpose is to help drive security research forward by demonstrating what is currently possible. He calls for anyone using the software to "&lt;quote&gt;be a researcher, not a criminal&lt;/quote&gt;", and to test it only on systems where they have explicit permission to test. If one did wish to use Singularity for nefarious purposes, however, the code is MIT licensed and freely available ‚Äî using it in that way would only be a crime, not an instance of copyright infringement. &lt;/p&gt;
    &lt;head rend="h4"&gt;Getting its hooks into the kernel&lt;/head&gt;
    &lt;p&gt;The whole problem of how to obtain root permissions on a system and go about installing a kernel module is out of scope for Singularity; its focus is on how to maintain an undetected presence in the kernel once things have already been compromised. In order to do this, Singularity goes to a lot of trouble to present the illusion that the system hasn't been modified at all. It uses the kernel's existing Ftrace mechanism to hook into the functions that handle many system calls and change their responses to hide any sign of its presence.&lt;/p&gt;
    &lt;p&gt;Using Ftrace offers several advantages to the rootkit; most importantly, it means that the rootkit doesn't need to change the CPU trap-handling vector for system calls, which was one of the ways that some rootkits have been identified historically. It also avoids having to patch the kernel's functions directly ‚Äî kernel functions already have hooks for Ftrace, so the rootkit doesn't need to perform its own ad-hoc modifications to the kernel's machine code, which might be detected. The Ftrace mechanism can be disabled at run time, of course ‚Äî so Singularity helpfully enables it automatically and blocks any attempts to turn it off.&lt;/p&gt;
    &lt;p&gt;Singularity is concerned with hiding four classes of things: its own presence, the existence of attacker-controlled processes, network communication with those processes, and the files that those processes use. Hiding its own presence is actually fairly straightforward: when the kernel module is loaded, it resets the kernel's taint marker and removes itself from the list of active kernel modules. This also means that Singularity cannot be unloaded, since it doesn't appear in the normal interfaces that are used for unloading kernel modules. It also blocks the loading of subsequent kernel modules (although they will appear to load ‚Äî they'll just silently fail). Consequently, Alves recommends experimenting with Singularity in a virtual machine.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding processes&lt;/head&gt;
    &lt;p&gt;Hiding processes, on the other hand, is more complicated. The mechanism that Singularity uses starts by identifying and remembering which processes are supposed to be hidden. Singularity uses a single 32-entry array of process IDs to track attacker-controlled processes; this is because a more sophisticated data structure would introduce more opportunities for the rootkit to be caught, either by adding additional memory allocations that could be noticed, or by introducing delays whenever one of its hooked functions needs to check the list of hidden process IDs.&lt;/p&gt;
    &lt;p&gt;Singularity supports two ways to add processes to the list: by sending an unused signal, or by setting a special environment variable and launching a new process. To implement the former, it hooks the kill() system call to detect an unused signal (number 59, by default), quashes the signal, adds the target process to its internal list, and gives the process root permissions in the global namespace. This means that attacker-controlled processes can be added from inside containers, and automatically escape the container using their new root privileges. To handle the environment variable, the execve() system call is hooked in a similar way.&lt;/p&gt;
    &lt;p&gt;Once a process is in the list, attempts to send signal 0 (to check whether the process exists) are also intercepted, as are other system calls that could refer to the process, such as getpgid(), sched_getaffinity(), and others. The total number of processes on the system, as reported by sysinfo() is also decremented to keep things consistent. The process's files in /proc are hidden by Singularity's file-hiding code. That code is probably the trickiest part of the whole rootkit. The basic idea is to filter out hidden directory entries such that the filesystem appears to remain in a consistent state, but filesystem code is difficult to get right at the best of times.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding files&lt;/head&gt;
    &lt;p&gt;When a program calls getdents(), the kernel fills the provided buffer with directory entries as normal. Then, Singularity's hook copies the buffer back from user memory, removes the hidden entries, puts the modified buffer back in user memory, and changes the return value of the system call to reflect the smaller number of directory entries returned. This slightly complicated process is because the kernel doesn't provide a good place for Singularity to inject a hook before the directory entries are written to user memory the first time. So, one potential way to identify the rootkit is to have another thread race with the attempt to read directory entries, trying to spot any that were removed.&lt;/p&gt;
    &lt;p&gt;Changing the number of returned directory entries alone would make the system appear to be in an inconsistent state, however. Directories in Linux filesystems are supposed to track the number of references to them; this includes the ".." references inside child directories. So, when hiding a directory, Singularity also needs to intercept calls to stat() in order to adjust the number of visible links to its parent directory.&lt;/p&gt;
    &lt;p&gt;Direct access to hidden directories, in the form of openat() and related system calls, is also made to fail. readlink() poses a special challenge because it resolves symbolic links without actually opening them; it has to be handled separately. In addition to the procfs files of hidden processes, Singularity also hides any directories matching a set of user-supplied patterns. By default, it hides things named "singularity", but the project's documentation suggests changing this in the build configuration, since otherwise detecting the rootkit becomes straightforward.&lt;/p&gt;
    &lt;p&gt;Despite this sophisticated file-hiding machinery, Singularity doesn't help against forensic examinations of a hard disk from another computer. If it isn't installed in the running kernel, it can't hide anything. Therefore, the documentation also recommends putting as many hidden files as possible onto temporary filesystems stored in RAM, so that they don't show up after the system is rebooted.&lt;/p&gt;
    &lt;p&gt;Another problem for the rootkit is files that contain traces of its presence, but that would raise eyebrows if they disappeared entirely. This includes things like the system log, but also files in procfs like kallsyms or enabled_functions that expose which kernel functions have had Ftrace probes attached. For those files, Singularity doesn't hide them at the filesystem level, but it does filter calls to read() to hide incriminating information.&lt;/p&gt;
    &lt;p&gt;Deciding which log lines are incriminating isn't a completely solved problem, though. Right now, Singularity relies on matching a set of known strings. This is another place where users will have to customize the build to avoid simple detection methods.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding network activity&lt;/head&gt;
    &lt;p&gt;Even once an attacker's processes can hide themselves and their files, it is still usually desirable to communicate information back to a command-and-control server. Singularity will work to hide network connections using a specific TCP port (8081, by default), and hide packets sent to and from that port from packet captures. It supports both IPv4 and IPv6. Hiding the connections from tools like netstat uses the same filesystem-hiding code as before. Hiding things from packet captures requires hooking into the kernel's packet-receiving code.&lt;/p&gt;
    &lt;p&gt;On the other hand, this is another place where Singularity can't control the observations of uncompromised computers: if one is running a network tap on another computer, the packets to and from Singularity's hidden port will be totally visible.&lt;/p&gt;
    &lt;head rend="h4"&gt;The importance of compatibility&lt;/head&gt;
    &lt;p&gt;Singularity only supports x86 and x86_64, but it does support both 64-bit and 32-bit system call interfaces. This is important, because otherwise a 32-bit application running on top of a 64-bit kernel could potentially see different results, which would be suspicious. To avoid this, Singularity inserts all of the aforementioned Ftrace hooks twice, once on the 32-bit system call and once on the 64-bit system call. A generic wrapper function converts from the 32-bit calling convention to the 64-bit calling convention before forwarding to the actual implementation of the hook.&lt;/p&gt;
    &lt;p&gt;Singularity has been tested on a variety of 6.x kernels, including some versions shipped by Ubuntu, CentOS Stream, Debian, and Fedora. Since the tool primarily uses the Ftrace interface, it should be supported on most kernels ‚Äî although since it interfaces with internal details of the kernel, there is always the chance that an update will break things.&lt;/p&gt;
    &lt;p&gt;The tool also comes bundled with a set of utility scripts for cleaning up evidence that it was installed in the first place. These include a script that mimics normal log-rotation behavior, except that it silently truncates the logs to hinder analysis; a script that securely shreds a source-code checkout in case the module was compiled locally; and a script that automatically configures the rootkit's module to be loaded on boot.&lt;/p&gt;
    &lt;p&gt;Overall, Singularity is remarkably sneaky. If someone didn't know what to look for, they would probably have trouble identifying that anything was amiss. The rootkit's biggest tell is probably the way that it prevents Ftrace from being disabled; if one writes "0" to /proc/sys/kernel/ftrace_enabled and the content of the file remains "1", that's a pretty clear sign that something is going on.&lt;/p&gt;
    &lt;p&gt;Readers interested in fixing that limitation are welcome to submit a pull request to the project; Alves is interested in receiving bug fixes, suggestions for new evasion techniques, and reports of working detection methods. The code itself is simple and modular, so it is relatively easy to adapt Singularity for one's own purposes. Perhaps having such a vivid demonstration of what is possible to do with a rootkit will inspire new, better detection or prevention methods.&lt;/p&gt;
    &lt;p&gt; Posted Jan 16, 2026 18:29 UTC (Fri) by tux3 (subscriber, #101245) [Link] (3 responses) Slightly more seriously, I'm a little surprised that it blocks modules and eBPF as an anti-detection feature. On one hand, some sort of antivirus might be able to find the suspicious hooks by loading a module or filter. If the EDR calls home to the admin dashboard and says it failed to talk to its module, the user's device can fail some enterprise posture compliance thing and the machine won't be allowed to log in to the VPN or corporate SSO. Posted Jan 16, 2026 23:15 UTC (Fri) by notriddle (subscriber, #130608) [Link] Posted Jan 17, 2026 6:05 UTC (Sat) by wtarreau (subscriber, #51152) [Link] Also, I was thinking that the code that deals with FS operation might have a tough work detecting accesses it needs to hide, and I suspect that such functions might be visible in "perf top" during heavy I/O. It's not to say that it would reveal it to the unsuspecting user, but those aware of these names might recognize the pattern. In any case it's really nice to provide such a playground to demonstrate what can really happen and that intrusions are not science fiction. Posted Jan 17, 2026 22:39 UTC (Sat) by matheuz (subscriber, #181907) [Link] Another point is that previously there was only a hook on finit and init_module to prevent other rootkit scanners that look for gaps in kernel memory from detecting it. In practice, they still fail to detect it. Even so, I will further improve module hiding using a technique that also avoids detection by LKM-based rootkit scanners. The blocking of new modules is temporary, and this hook will be removed soon. The same applies to blocking certain eBPF operations. This is also temporary. Once I have more time to work on Singularity, eBPF operations that attempt to detect hidden processes or files will be bypassed as well. That said, there will no longer be any behavioral changes related to these two modules. Additionally, Singularity can bypass EDRs such as CrowdStrike Falcon, which is eBPF-based, Trend Micro EDR, which is LKM-based, Kaspersky, also LKM-based, Elastic Security (there is an article in the Singularity README explaining how to bypass it), and some other EDRs that I tested in my virtual machine. Posted Jan 16, 2026 21:22 UTC (Fri) by dud225 (subscriber, #114210) [Link] (3 responses) Posted Jan 16, 2026 22:52 UTC (Fri) by daroc (editor, #160859) [Link] Posted Jan 17, 2026 22:39 UTC (Sat) by matheuz (subscriber, #181907) [Link] This mechanism implements a fake disable of ftrace. From user space, ftrace appears to be properly disabled, since reading ftrace_enabled returns the expected value and no abnormal behavior is observed. Internally, however, ftrace remains fully operational. The internal state is determined by the intercepted write and tracked via internal flags, rather than relying on the real kernel ftrace toggle. Additionally, when ftrace is in this fake-disabled state, access to tracing interfaces such as trace, trace_pipe, enabled_functions, and touched_functions is carefully controlled. Reads from trace return only static header information and no new events, while reads from trace_pipe block indefinitely without emitting trace data. This behavior closely matches that of a legitimately disabled ftrace subsystem and prevents the leakage of partial or suspicious output. As a result, common detection techniques that rely on inconsistencies in ftrace_enabled, or on monitoring trace and trace_pipe for unexpected activity, are ineffective. The overall behavior remains coherent and indistinguishable from a normal ftrace disable operation, despite ftrace continuing to function internally. Posted Jan 18, 2026 4:35 UTC (Sun) by alison (subscriber, #63752) [Link] Another common test would be to check open ports on the host from a remote with nmap. That test would inevitably show that port 8081 is open. Posted Jan 18, 2026 4:49 UTC (Sun) by alison (subscriber, #63752) [Link] https://martus.org/overview.html In other words, might this sneaky rootkit be repurposed into a system which helps journalists and dissidents with life-or-death secrets to hide to conceal them on their system? Most of the needed pieces appear to be present, although a Martus-like system should also report the total storage capacity to be smaller than the actual amount. A security system for dissidents and journalists could reuse many of the components, but allow the user to deploy and control them. &lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;lb/&gt; But it looks like the init_module hook just returns -ENOEXEC, that's bound to raise some alarms, too.&lt;lb/&gt; Or for desktop users, you will have a black screen after loading nvidia.ko... and actually you probably wouldn't suspect anything. Never mind, the stealth works in this case.&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;quote&gt;if one writes "0" to /proc/sys/kernel/ftrace_enabled and the content of the file remains "1", that's a pretty clear sign that something is going on.&lt;/quote&gt; Naive suggestion : why not leveraging the same technique than for hidden files by catching read and write calls to that file and returning modified results? &lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;Might dissidents also find Singularity valuable?&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1053099/19c2e8180aeb0438/"/><published>2026-01-18T09:36:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666574</id><title>Echo Chess: The Quest for Solvability (2023)</title><updated>2026-01-18T22:10:18.841458+00:00</updated><content>&lt;doc fingerprint="2f2ce397292800db"&gt;
  &lt;main&gt;
    &lt;p&gt;This story has stirred some great conversations on Hacker News. Many have reached out with great ideas for a v2. If you still want to chime in, just drop me a note.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prologue&lt;/head&gt;
    &lt;p&gt;Let‚Äôs make Chess more fun in Single-Player. How hard could it be? ‚ò†Ô∏è&lt;/p&gt;
    &lt;p&gt;This is the story of venturing too deep, head-first, into the unknown. It all started with a doodle on a piece of paper. This doodle to be exact.&lt;/p&gt;
    &lt;p&gt;I first conceived of this game on a whim as part of many strategy and puzzle games I‚Äôd been designing for fun. I was musing with the idea of a chess-inspired Turn-Based Strategy (TBS) game that no one would recognize as chess-based. My first attempts purposefully kept steering the theme away from chess. Why go vanilla when there are so many wilder thematic flavors and bolder mechanics to explore?&lt;/p&gt;
    &lt;p&gt;Asymmetric rewards, alternating movement rules, stochastic obstacles, stealth, morphing, etc. A wise friend and fellow strategy game nerd* then said to me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The game‚Äôs dope. But what‚Äôs wrong with people associating it with Chess? No need to innovate that kernel away - their mental load would be taken up by re-learning how to move. Chess pieces are a universal language. They help them overcome the activation energy and figure out what‚Äôs going on.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So I re-designed the rules, mechanics, and first few levels with &lt;del&gt;a sharpie and a whiteboard&lt;/del&gt;&lt;del&gt;Figma and a LLaMA&lt;/del&gt; sandwich paper and a pencil. Then I started testing it with friends by setting up this old wooden board I found in storage. Soon ‚Äúthe game‚Äù looked more like this.&lt;/p&gt;
    &lt;p&gt;Something surprising started happening quickly. Anyone who tried the game got hooked. People would keep coming back trying to beat levels they couldn‚Äôt solve. They‚Äôd ask me to manually reset the board to specific checkpoints in a puzzle so they could try again.&lt;/p&gt;
    &lt;p&gt;Chess masters and n00bs alike would describe feeling a rush of excitement every time they‚Äôd reach the ‚ÄòAha‚Äô moment of a maze. Then people wanted to play ‚Äúthe game‚Äù at home and show it to their friends. I‚Äôd send them photos of my scribbled level designs so they could reproduce them on their own with a physical chess board or something like lichess.&lt;/p&gt;
    &lt;p&gt;One day I just figured this was getting comically unsustainable. So I decided to build ‚Äúthe game‚Äù into a proper thing online and call it Echo Chess.&lt;/p&gt;
    &lt;p&gt;And thus began a deep, deep rabbit hole‚Ä¶üêá&lt;/p&gt;
    &lt;head rend="h2"&gt;Echo Chess ‚ÄúClassic‚Äù&lt;/head&gt;
    &lt;head rend="h3"&gt;Gameplay Design&lt;/head&gt;
    &lt;p&gt;Three simple rules of Echo Chess ‚Äî still relevant from v0 to this day:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You are playing White, there is no opponent. You must capture all pieces to win.&lt;/p&gt;
      &lt;p&gt;You become the "echo" of any piece you capture. Captured a bishop? Become that bishop.&lt;/p&gt;
      &lt;p&gt;You can‚Äôt pass through red obstacles. Find the best move order to clear the board.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And so it was that Echo Chess graduated from a sandwich paper to a Mechanical Turk model to a minimalist web app, where you just hit a link and solve the maze. No downloads, no installs, no load screens, no tutorials, no handholding. Just tap the link, figure out the puzzle. An extreme experiment in ‚Äòshow, don‚Äôt tell‚Äô design.&lt;/p&gt;
    &lt;p&gt;What‚Äôs the catch? Because you‚Äôre transformed with every capture, the maze is effectively changing as you play.&lt;/p&gt;
    &lt;p&gt;In later levels, you unlock squad-like gameplay where you get to move and coordinate multiple pieces ‚Äî or even cannibalize one of your own to clear the way, if it comes down to it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Game Mechanics Implications&lt;/head&gt;
    &lt;p&gt;I think one of the reasons Echo Chess is intriguing to so many diverse player personas is that it‚Äôs (laughably) easy to learn, yet (frustratingly) hard to master.&lt;/p&gt;
    &lt;p&gt;To give you but a glimpse as to why that‚Äôs the case:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The moment you enjoy any capture whatsoever, you lose all your existing abilities. If you were a knight galavanting around obstacles, you just lost all that by capturing a rook.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Landed yourself a powerful queen? Great, you can only use it once before it‚Äôs gone. Think you should‚Äôve saved capturing it for later when you‚Äôre in a pinch? Who says the option would still be there if you take another path? Capturing order matters. A lot.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Need to clear the passage to cross to the other side of the board? Don‚Äôt forget you‚Äôll become whatever blocker you‚Äôre clearing. Pawns = fewer degrees of freedom. Keep them till the end, or better get them over with quickly?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh you think it‚Äôd be much easier if you only had more white pieces on your side? Why don‚Äôt you try solving, say, Level 13 below? (jk pls don‚Äôt actually try it cold turkey. It‚Äôs pretty much impossible.)&lt;/p&gt;
    &lt;p&gt;Very quickly you‚Äôll realize Echo Chess is a puzzle game that‚Äôs deceivingly immune to brute force. Click-spamming won‚Äôt get you far here.&lt;/p&gt;
    &lt;p&gt;I actually had to add a ‚ÄòGive Up‚Äô button as an early termination state for those who get stumped but want to save their high score (in some ways, this makes puzzle games different from the arcade genre because you can never just‚Ä¶ die).&lt;/p&gt;
    &lt;head rend="h3"&gt;Balancing, Levels and Game Design&lt;/head&gt;
    &lt;p&gt;Traditional game design tells us we should aim to reproduce, both as strictly and as loosely as possible, this kind of sentiment when setting the difficulty of a game:&lt;/p&gt;
    &lt;p&gt;A good game feels like it‚Äôs getting easier as you start mastering its mechanics, until a new mechanic/challenge/twist is introduced, and the difficulty shoots up again. You try applying the old strategies you‚Äôve perfected but it‚Äôs no use. Then you get it: you need to snap out of the comfort zone and learn some new SKilLz it‚Äôs pushing you to learn. So you actually #gitgud and the game feels easier again, up until you hit the next bump. Rinse and repeat.&lt;/p&gt;
    &lt;p&gt;In theory, this is the recipe for a truly fun game where players enter the coveted ‚ÄòFlow‚Äô state of gaming.&lt;/p&gt;
    &lt;p&gt;In practice, this kinda works a tiny bit, until it doesn‚Äôt. Players come with all sorts of prior skills (having played many games of the same genre, or just being wired a certain way), a wide spectrum of (im)patience, and all kinds of expectations as to what qualifies as a difficulty spike, or where they would‚Äôve drawn the line between offensively easy and unfairly brutal. You somehow end up both scaring the n00bs and boring the veterans.&lt;/p&gt;
    &lt;p&gt;What‚Äôs the answer, then? Well, if you‚Äôre thinking of making a game that‚Äôs ‚Äòeasy to learn, hard to master‚Äô, I think it goes one of two ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;If you‚Äôre reading this after the year&lt;/p&gt;&lt;del rend="overstrike"&gt;2020&lt;/del&gt;&lt;del rend="overstrike"&gt;2010&lt;/del&gt;2000, don‚Äôt make this game. Seriously, don‚Äôt. No one will play it. Social media and OF culture have somehow managed to turn strategy games into trash TV. You can‚Äôt compete with that. Find some other thing instead. You‚Äôve been warned.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you‚Äôre someone who‚Äôs 100% immune to good advice and you still insist on making this game, then the gods be with you. At the very least, though, please do these simple things:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;(1) let people skip to any level they want, whenever they want. Let them gauge their own spice tolerance.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;(2) if you really need to sprinkle new mechanics here and there, add them in some micro-saw-tooth way.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;(3) make your macro difficulty ramp a slow-building exponential curve instead, closer to this next one. Newcomers will Dunning‚ÄìKruger their way to the early fun part, and ambitious players will power through to reach the really fun stuff.&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;(Optional) offer enough replayability and variability for each difficulty tier so that newbies can thrive in cuteness land forever, while GMs and PhDs play a math meta of their own.&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cool. Let‚Äôs bring it back to Echo Chess. Friends who know me well will attest that, when it comes to any form of creative expression, I ascribe to the (more-controversial-than-it-has-to-be) No Spoilers school of thought.&lt;/p&gt;
    &lt;p&gt;As a service to fellow nospoilerists out there, I‚Äôll only break down one (fictional) example in detail here. Then I‚Äôll let you enjoy the actual levels yourself.&lt;/p&gt;
    &lt;p&gt;Consider the 4x4 board above. You‚Äôre playing King, and need to capture two pawns, a knight, and a rook. There are 2 impassable obstacles. Which piece do you capture first? And is this a good tutorial level?&lt;/p&gt;
    &lt;p&gt;If you‚Äôre new to Echo Chess, it means you fall in one of two camps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;(1) You see a cute small level. Only 4 pieces. You brute-force your way into capturing every piece to see what happens. All hail the #yolo king.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(2) You‚Äôve binge-watched The Queen's Gambit and have been slogging through Chess.com, because Covid. Or maybe you‚Äôre an actual Chess player. So you know better. You plan your 1st, 2nd, Nth move in your head and assign values to certain squares.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem if you‚Äôre a type-2 person is that in Chess, you already have some built-in intuition as to what generally makes a move good vs bad. You‚Äôve spent so many years practicing advanced tactics and openings that using your intuition as a heuristic in your ‚Äòforward pass‚Äô exploration makes it less brute-forcy than for type-1s. But in Echo Chess, your heuristics can be easily deceived. Is r &amp;gt; n or r &amp;lt; n?&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how an Echo Chess player would typically solve the board above using a ‚Äòbackward pass‚Äô instead (obviously there‚Äôs more than one ‚Äúright‚Äù way to do these, but FWIW):&lt;/p&gt;
    &lt;p&gt;In reality, an ‚ÄòEcho Chess player‚Äô (wtv that is) would instinctively have spotted the 2 pawns as finishers from the first glance, and likely computed Steps 3-5 pretty quickly. I‚Äôd say the L-jumps of Step 6 are the only cryptic bit in this maze, assuming the right intuition has already been developed. Is it a good tutorial level? Nope.&lt;/p&gt;
    &lt;p&gt;The onus of nurturing this type of game-mechanic-specific intuition is on the game designer, not the player. It should be (and is) experienced in a prior maze to this one. There are at least a dozen similar heuristics to what we just saw that you‚Äôre bound to pick up while playing Echo Chess. I‚Äôve designed each level to hopefully convey the importance of a new one. If you guess some of them, please drop me a note.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre curious about the more challenging ‚ÄòAha‚Äôs, try levels 11-15 on echochess.com. You can switch to any of them anytime by clicking the Level dropdown at the top.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scoring System&lt;/head&gt;
    &lt;p&gt;Let‚Äôs talk about scores. I‚Äôve always believed that a well-designed scoring system should be optimized to properly incentivize better gameplay, not cheesing for high scores or APM. So naturally I baked that into the design of Echo Chess.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You get points for each capture (higher per piece type, higher overall for harder levels).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gameplay-wise, there‚Äôs (almost) no difference between a Q and a K. Why? Because single-player. If it‚Äôs always your turn, space-time gets bent. K * moves = Q.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; 1function updateScoreForCapture(pieceType) { 2  var pieceScores = { 3    'p': 10, 4    'n': 30, 5    'b': 30, 6    'r': 50, 7    'q': 80, 8    'k': 100 9  }10  runningScore += currentLevel * pieceScores[pieceType]11}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You get a ‚Äúlevel bonus‚Äù every time you solve a maze. This increases quadratically as levels go up.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The level bonus is penalized (in a compounding way) for every move you make. So move efficiency matters a ton ‚Äîespecially in higher levels where the compound penalty hurts a bigger base. You‚Äôd still always get some positive bonus, nbd.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;1function updateScoreForLevelBonus(numMoves) {2  var levelBase = 200 * Math.pow(currentLevel, 2)3  // penalize by compounding -2% for every move used in this level4  var levelBonus = Math.floor(levelBase * Math.pow(0.98, numMoves))5  levelBonus = Math.round(levelBonus / 5) * 56  runningScore += levelBonus7}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Jumping levels using the dropdown is allowed at any time, but the score resets automatically to avoid cheating the live leaderboard.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You get a ‚Äútime bonus‚Äù when finishing the game based on how long it took you to beat the whole thing. The most you could ever score (even at hyperspace speed) is capped at a multiple of base, then it starts dropping as a reciprocal of time spent.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;1function updateScoreForGameCompletion() {2  // time bonus drops reciprocally but is always in [0, +30%]3  var cappedMultiplier = 0.3 / (1 + 0.005 * secondsElapsed)4  timeBonus = Math.floor(cappedMultiplier * runningScore)5  runningScore += timeBonus6}&lt;/code&gt;
    &lt;p&gt;Basically the scoring function rewards maze-solving first, move efficiency second, and speed of completion last.&lt;/p&gt;
    &lt;p&gt;This incentivizes players to embrace the joy of adventuring and actually play the maze until they truly ‚Äòget it‚Äô without premature optimization. Then they can go back and search for more creative solution paths (which brings a joy of its own).&lt;/p&gt;
    &lt;p&gt;Players always behave in ways the game reinforces them to, whether they realize it or not. And a meta will emerge from any scoring system (LeetCode, anyone?). This follows directly from evolutionary psychology ‚Äîor from RL if you prefer robo-speak. Oh and speedrunners gonna speedrun no matter what you do.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúMake sure the winning strategy is also the funnest one to play.‚Äù -Mark Rosewater, strategy game designer and puzzlemaker.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;&lt;del rend="overstrike"&gt;Game&lt;/del&gt; Full Stack Development&lt;/head&gt;
    &lt;p&gt;So how did Echo Chess go from wooden boards to pixels?&lt;/p&gt;
    &lt;p&gt;The frontend is, believe it or not, good old Javascript, jQuery, HTML, CSS. The backend is a Flask server hooked into a ReplitDB. Async calls are made with good old AJAX. Echo Chess is retro through and through.&lt;/p&gt;
    &lt;p&gt;Two open source libraries helped quite a bit for the earliest prototype‚Äôs client-side plumbing: &lt;code&gt;chess.js&lt;/code&gt; and &lt;code&gt;chessboard.js&lt;/code&gt;. Of course, an enormous amount of work went in to go from a barebone chess moves validator to a fully fledged chess variant puzzle game.&lt;/p&gt;
    &lt;p&gt;In fact, a ton of code in these libraries had to be overwritten and extended to incorporate chess variants mechanics. Here are but a few examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;King movements, checks, checkmate&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Relative pins, absolute pins, immobilized pieces&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Restrictions on kings per player (&amp;lt; or &amp;gt; 1)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Piece promotions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Castling rights&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Player turns, or lack thereof&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Half-moves, full moves, draws&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pawn double-step disabling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;En-passant dynamics&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Squares having obstacles or boundaries&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Movement pathing with obstacles&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture-based transformation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Preventing transformation reversion&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Self-sacrifice dynamics&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(you get the point)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The client side keeps track of the game state, level state, moves efficiency, scoring, timing and consistency across level jumps, retries, restarts, switching game modes, etc.&lt;/p&gt;
    &lt;p&gt;The server handles saving and retrieving scores, live leaderboards, level data validation and prediction (more on that later on). The latest saved high scores get cached on the client and only get updated by the server when relevant.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take a look at a simple implementation example from Echo Chess to illustrate how a traditional chess engine can be used as a building block for chess variants.&lt;/p&gt;
    &lt;p&gt;Here we‚Äôd like to highlight in green all the squares a piece can move to, taking into account the concept of obstacles we‚Äôve defined for this game. We start by formalizing how an obstacle can block stuff.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs write some simple code to capture this.&lt;/p&gt;
    &lt;code&gt; 1function obstacleOnPath(direction, source, target, obstacle) { 2  let [i1, j1] = getRankAndFile(source) 3  let [i2, j2] = getRankAndFile(target) 4  let [ik, jk] = getRankAndFile(obstacle) 5  if (direction == 'h') { 6    // horizontal block 7    if (i1 == i2 &amp;amp;&amp;amp; i1 == ik &amp;amp;&amp;amp; isBetween(jk, j1, j2)) { 8      return true 9    }10  }11  else if (direction == 'v') {12    // vertical block13    if (j1 == j2 &amp;amp;&amp;amp; j1 == jk &amp;amp;&amp;amp; isBetween(ik, i1, i2)) {14      return true15    }16  }17  else if (direction == 'd') {18    // diagonal block19	if (Math.abs(i2 - i1) == Math.abs(j2 - j1) &amp;amp;&amp;amp; 20		Math.abs(ik - i1) == Math.abs(jk - j1) &amp;amp;&amp;amp; 21		Math.abs(i2 - ik) == Math.abs(j2 - jk) &amp;amp;&amp;amp;22		isBetween(ik, i1, i2) &amp;amp;&amp;amp; isBetween(jk, j1, j2)) {23		return true24	}25  }26  return false27}&lt;/code&gt;
    &lt;p&gt;Now we can combine these concepts with the usage of the chess engine to write a simple highlighting function for possible moves.&lt;/p&gt;
    &lt;code&gt; 1import { Chess } from 'chess.js' 2const chess = new Chess() 3&lt;/code&gt;
    &lt;p&gt;From then on, we can call the &lt;code&gt;colorPossibleMoves&lt;/code&gt; function every time we pick up a white piece from a given square, and it will color all the corresponding squares in green.&lt;/p&gt;
    &lt;code&gt; 1function onDragStart(source, piece) { 2  // player can only pick up white pieces 3  if (piece.search(/^b/) != -1) { 4    $("#wrong-piece")[0].play(); 5    return false 6  } 7  colorPossibleMoves(source) 8  ... 9}10&lt;/code&gt;
    &lt;p&gt;Fast forward several thousand lines of code, and soon enough, Echo Chess ‚ÄòClassic‚Äô was live! Friends were able to enjoy it on the go.&lt;/p&gt;
    &lt;p&gt;Since I had already meticulously crafted 15 puzzle levels, there was enough in there to keep everyone busy. For a while, the hardest puzzles remained unsolved. Dozens would try, day in, day out, but very few would be able to reach the very end. Weeks passed. And then it happened. People started finishing the game.&lt;/p&gt;
    &lt;p&gt;Like a rush of emotions after binge-watching the last season of a favorite show, they celebrated, sighed, stared into the abyss, then cracked their knuckles, and came back asking for more. MOAR LEVELS. ASAP.&lt;/p&gt;
    &lt;head rend="h3"&gt;Echo Chess ‚ÄúENDLESS‚Äù&lt;/head&gt;
    &lt;p&gt;Once again, I was faced with the bittersweet realization that the users‚Äô appetite for Echo Chess had exceeded my ability to manually keep up with ‚ÄúDM-ing‚Äù it. I knew I needed to automate level creation to make it, once and for all, entirely independent from my involvement as a mammal.&lt;/p&gt;
    &lt;head rend="h3"&gt;Procedural Generation&lt;/head&gt;
    &lt;p&gt;And so was born the new ‚ÄòEndless‚Äô mode for Echo Chess! A true ‚ÄúTetris‚Äù-like puzzle game, completely separate from ‚ÄòClassic Mode‚Äô, togglable in url params.&lt;/p&gt;
    &lt;p&gt;Infinite levels, all randomly generated, all in real time. Anytime you finish a level, you carry over your winning piece to the next, and the board of the upcoming puzzle pivots around your current piece‚Äôs position.&lt;/p&gt;
    &lt;p&gt;So what do we really mean by ‚ÄòEndless‚Äô?&lt;/p&gt;
    &lt;p&gt;We randomly generate each level from parametrized distributions of pieces, obstacles and boundaries (1). This gives us full control over adjusting the variety and difficulty of generations (2), and lets us tailor the beginning of each level to coincide with the ending of the prior one (3).&lt;/p&gt;
    &lt;p&gt;That brings us to the time mechanic. How do we converge an infinite level progression? We make it a countdown race:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;100 seconds. Time's running out! (4) Wanna stay alive? Keep solving new levels! You get more time back for solving bigger ones. (5)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Coincidentally, with this new fast-paced playing style comes a shift from a pure strategy/puzzle solving to a full-on arcade mode. The scoring function gets adjusted accordingly with bonuses scaling up or down with level sizes and difficulty ‚Äîand we get a new leaderboard that‚Äôs kept fully separate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Encoding, Decoding&lt;/head&gt;
    &lt;p&gt;Now that we‚Äôre dealing with so many levels, we‚Äôll need an efficient and lossless way to conveniently read, write, encode, decode, send, receive, store, retrieve, transform, analyze, augment, and render any level configuration.&lt;/p&gt;
    &lt;p&gt;Chess players reading this are probably already thinking of multiple such viable systems, like PGN, SAN, or UCI. These are all great for chess moves encoding and decoding, and could possibly be adjusted for our needs. But there‚Äôs actually a better system we could reuse for the purpose of game states in particular: the elegant Forsyth‚ÄìEdwards Notation (FEN).&lt;/p&gt;
    &lt;p&gt;The easiest way to understand a FEN string is as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Read it left to right, one row at a time, starting from the top row&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lowercase letters = black chess pieces&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Uppercase letters = white chess pieces&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Digits = number of consecutive empty squares (ignore their color)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;King‚ôö, Queen‚ôõ, Rook‚ôú, Bishop‚ôù, Pawn‚ôü, kNight‚ôû&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;The King already called dibs on ‚ÄòK‚Äô (royal greed has no limits) so use ‚ÄòN‚Äô for the&lt;/p&gt;&lt;del rend="overstrike"&gt;peasants&lt;/del&gt;knights insteadüê¥üåù&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First order of business: repurpose and expand the FEN encoding system for Echo Chess. In addition to everything else FEN does, I needed a version that takes into account things like the locations of obstacles, the size and shape of a level, where the boundary squares are, and so on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Developing a New Formal Notation&lt;/head&gt;
    &lt;p&gt;So I came up with a new encoding that expands on FEN, which I‚Äôm offering up here for anyone who might find it valuable for their work. I call it the ‚ÄúcompoundFEN‚Äù.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs consider the following example from Echo Chess to see how the &lt;code&gt;compoundFEN&lt;/code&gt; would be derived. To better visualize this, we‚Äôll overlay a grid over the level to make the canonical 8x8 chess board pop.&lt;/p&gt;
    &lt;p&gt;We go through each row, top to bottom, starting from the 0th to the 7th row ‚Äî sure, we could instead count from 1 to 8 like sapiens commoners, but what would the robots say?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;(0) Nothing but a board boundary in this row showing the confines of this particular level size. Can‚Äôt really think of these squares as ‚Äòempty‚Äô because no movement is allowed on them. Let‚Äôs call each of these boundary squares an ‚ÄòX‚Äô. The royal K hasn‚Äôt expropriated the X letter yet so we should be good to go.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;We‚Äôll encode this row in compoundFEN as ‚ÄòXXXXXXXX‚Äô.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(1) Okay so we start with an ‚ÄòX‚Äô for a boundary. Then we have an actual empty square, so that‚Äôs a 1. Now a bunch of obstacles. They‚Äôre technically similar to boundaries in that they block the player‚Äôs motion, but they have a slightly different use. Let‚Äôs call these obstacle squares lowercase ‚Äòx‚Äô just in case. Okay so we have 5 of those ‚Äòx‚Äô, followed by another ‚ÄòX‚Äô.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Great, we can encode this as ‚ÄòX1xxxxxX‚Äô.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(2) This should be easier. We have ‚ÄòX‚Äô followed by an ‚Äòx‚Äô, followed by a bunch of black pieces (so all lowercase: q, p, b, k, and r), then a final ‚ÄòX‚Äô, with 0 proper ‚Äòempty‚Äô squares to be noted anywhere.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Boom. ‚ÄòXxqpbkrX‚Äô.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(3) Same thing here: we start with an ‚ÄòX‚Äô followed by an ‚Äòx‚Äô, then we get a white piece (so it‚Äôs uppercase) and it‚Äôs an ‚ÄòN‚Äô, not a ‚ÄòK‚Äô, because knights can‚Äôt even keep their initials in an absolute monarchy. Then we continue as before with the alternating black pawns ‚Äòp‚Äô and obstacles ‚Äòx‚Äô until hitting the last boundary ‚ÄòX‚Äô.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Easy. ‚ÄòXxNpxpxX‚Äô.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(4) Straightforward row: ‚ÄòX‚Äô then ‚Äòx‚Äô, then a black king ‚Äòk‚Äô, then 4 ‚Äòx‚Äô, and a final ‚ÄòX‚Äô.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Super chill. ‚ÄòXxkxxxxX‚Äô.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(5) Interesting, a slightly different one. ‚ÄòX‚Äô and 2 ‚Äòx‚Äô, sure. Then 3 actual empty squares. Okay so we‚Äôll represent these as ‚Äò3‚Äô according to the base FEN convention. Then a black knight ‚Äòn‚Äô and a final ‚ÄòX‚Äô.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Cool. ‚ÄòXxx3nX‚Äô.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(6) Back to familiar ones, we‚Äôve got this. ‚ÄòX1xbnrxX‚Äô.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(7) We already know this one, nothing but boundaries. ‚ÄòXXXXXXXX‚Äô&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now we put all these rows together, separated by ‚Äò/‚Äô, to get the full ‚Äòpiece placement‚Äô portion of the &lt;code&gt;compoundFEN&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Let‚Äôs test this to see if our compoundFEN for Level 7 looks right. Here‚Äôs a quick decoding function you can use to convert from compoundFEN to a 2D board.&lt;/p&gt;
    &lt;code&gt; 1def fen_to_board(compound_fen): 2    board = np.empty((8, 8), dtype=str) 3    board.fill(' ')    4    # grab 'piece placement' rows, ignore the rest 5    fen_parts = compound_fen.split(' ')   6    ranks = fen_parts[0].split('/')    7    rank_index = 0 8    file_index = 0 9    for i in range(len(ranks)):10        rank = ranks[i]11        for j in range(len(rank)):12            char = rank[j]13            if char.isdigit():14                # consecutive empty squares15                file_index += int(char)16            else:17                board[rank_index][file_index] = char18                file_index += 119        rank_index += 120        file_index = 0   21    return board&lt;/code&gt;
    &lt;code&gt;1compound_fen = 'XXXXXXXX/X1xxxxxX/XxqpbkrX/XxNpxpxX/XxkxxxxX/Xxx3nX/X1xbnrxX/XXXXXXXX w - - 0 1'2board = fen_to_board(compound_fen)3print(board)&lt;/code&gt;
    &lt;code&gt;1[['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']2 ['X' ' ' 'x' 'x' 'x' 'x' 'x' 'X']3 ['X' 'x' 'q' 'p' 'b' 'k' 'r' 'X']4 ['X' 'x' 'N' 'p' 'x' 'p' 'x' 'X']5 ['X' 'x' 'k' 'x' 'x' 'x' 'x' 'X']6 ['X' 'x' 'x' ' ' ' ' ' ' 'n' 'X']7 ['X' ' ' 'x' 'b' 'n' 'r' 'x' 'X']8 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]&lt;/code&gt;
    &lt;p&gt;Looks great. Go ahead and compare every cell of this board decoded from our compoundFEN to the Level 7 screenshot above. Remember: lowercase ‚Äòx‚Äô refers to obstacles, and uppercase ‚ÄòX‚Äô refers to boundary squares.&lt;/p&gt;
    &lt;p&gt;And now here‚Äôs an encoding function to automate the conversion of boards to compoundFENs moving forward.&lt;/p&gt;
    &lt;code&gt; 1def board_to_fen(board): 2    compound_fen = '' 3    for i in range(len(board)): 4        row = board[i] 5        empty_count = 0 6        for j in range(len(row)): 7            piece = row[j] 8            # count consecutive empty squares 9            if piece == ' ':10                empty_count += 111            else:12                # add preceding empty squares13                if empty_count &amp;gt; 0:14                    compound_fen += str(empty_count)15                    empty_count = 016                # add this square's content17                compound_fen += piece18        # add row's trailing empty squares19        if empty_count &amp;gt; 0:20            compound_fen += str(empty_count)21        if i &amp;lt; len(board) - 1:22            compound_fen += '/'23    compound_fen += ' w - - 0 1'24    return compound_fen&lt;/code&gt;
    &lt;code&gt;1print(board_to_fen(board))2print(compound_fen == board_to_fen(board))&lt;/code&gt;
    &lt;code&gt;1XXXXXXXX/X1xxxxxX/XxqpbkrX/XxNpxpxX/XxkxxxxX/Xxx3nX/X1xbnrxX/XXXXXXXX w - - 0 12True&lt;/code&gt;
    &lt;p&gt;If instead you wanted to check the basic ‚Äòstandard‚Äô FEN encoding for this same Level 7 example, you could simply ignore all ‚Äòx‚Äô or ‚ÄòX‚Äô codes in the compoundFEN and assume they‚Äôre part of the consecutive empty spaces. That‚Äôs because standard FENs consider anything that‚Äôs not a chess piece to be empty. This is what you‚Äôd get: &lt;code&gt;8/8/2qpbkr1/2Np1p2/2k5/6n1/3bnr2/8 w - - 0 1&lt;/code&gt;, which you can conveniently verify with this online chess board (just keep in mind that it won‚Äôt look as recognizable without the Echo Chess parts).&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem of Solvability&lt;/head&gt;
    &lt;p&gt;So now we have an efficient, reliable way to procedurally generate random levels, manipulate them, and serve them to our players. But one thing‚Äôs for sure: we don‚Äôt want to be serving unsolvable mazes. Tough puzzles can be thrilling, broken puzzles are just trash.&lt;/p&gt;
    &lt;p&gt;How do we check that for a given generated maze? Is it even theoretically possible to force randomly generated Echo Chess mazes to be solvable? To understand what we‚Äôre up against, let‚Äôs start by taking a look at why solving an Echo Chess maze is so tricky to start with.&lt;/p&gt;
    &lt;p&gt;Each of these trivial examples illustrates a simple configuration where the maze is completely unsolvable.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs look at some less-trivial examples of unsolvable configurations. See if you can spot the issue before looking at the answers.&lt;/p&gt;
    &lt;p&gt;At this stage, you may be starting to develop some intuition for what made these mazes unsolvable. And yet, here‚Äôs an example of a level that feels unsolvable the first dozen times you try it, but that does actually have known solutions.&lt;/p&gt;
    &lt;p&gt;You might be asking yourself: okay, so solving a maze on the fly is not that straightforward, but given enough prep time, what‚Äôs the big deal? Instead of generating completely random levels in real time, why doesn‚Äôt this guy just serve a random level from a pre-generated list of solvable levels that he‚Äôs already curated beforehand?&lt;/p&gt;
    &lt;p&gt;Good question ‚Äîthat was my initial attempt. The reason why that turns out to be not such a good idea is that we have no control over which carry-over piece and on which carry-over square the player would be starting any given level, seeing that it depends entirely on their prior solution path. Still, you might argue, you could either:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;(1) make sure each level has only one possible end point&lt;/p&gt;&lt;lb/&gt;=&amp;gt; Nope, that means we‚Äôd be back to manual designs, beating the whole point of the Endless mode&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(2) pre-generate and curate levels for every combination of piece type and square the player might end up in, just in case&lt;/p&gt;
        &lt;p&gt;=&amp;gt; Really? You want me to manually test and curate 8 x 8 x 6 = 384 combinations for every level?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;or (3) abandon the whole concept of a carry-over piece and simply serve levels from a curated pregen list regardless of how the prior level ended&lt;/p&gt;
        &lt;p&gt;=&amp;gt; That would actually ruin the core vibe of Endless that all users currently love: it really fees like one continuous, coherent, infinite level being played. To be fair, manually editing a pregen list is still much faster than manually designing from scratch (MidJourney, anyone?) so this could still come in handy for extra help on future Classic mode levels design, but it won‚Äôt give us what we need for Endless.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In any case, this still does not solve the primary matter at hand, which is finding an automated method for generating mazes that have solutions.&lt;/p&gt;
    &lt;p&gt;One way to approach this would be to try using Pathfinding Algorithms like Depth-First Search (DFS) with Backtracking, or variants of A* and heuristic-based approaches.&lt;/p&gt;
    &lt;p&gt;But before we hit another iceberg of delusion, let‚Äôs pause and think a bit about what we‚Äôre really attempting here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;To solve an Echo Chess level, we have to clear the full board.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Clearing the board means capturing every piece.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To capture a piece, we have to move to its location.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Once a piece is captured, we can‚Äôt capture it again.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Therefore, to solve an Echo Chess level, we have to ‚Äòvisit‚Äô each piece exactly once.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Does this ring a bell? Bad news, you guys. It looks like the problem of Echo Chess Solvability (ECS) could be mappable to the Hamiltonian Path Problem (HPP) of visiting each city exactly once in a graph.&lt;/p&gt;
    &lt;p&gt;In case you remember your Thoeretical Computer Science class from back in the day, the HPP is an NP-complete problem.&lt;/p&gt;
    &lt;p&gt;And if you also want to account for Echo Chess‚Äôs move efficiency mechanic, like finding the shortest Hamiltonian path visiting all cities, then you‚Äôre even risking NP-hard territory.&lt;/p&gt;
    &lt;p&gt;If you don‚Äôt recall the HPP, don‚Äôt worry about it - you may be more familiar with its close cousin TSP since the Traveling Salesman Problem is such a notorious one. Any way you look at it, you should probably be un-pumped that we‚Äôre dealing with these beasts.&lt;/p&gt;
    &lt;p&gt;Granted, with enough compute, we could likely attempt some approaches like Monte Carlo Tree Search, alpha-beta pruning, or others, but there are other aspects of the Echo Chess solvability problem that can be non-trivial to deal with ‚Äî even if we developed a reproducible mapping from a level‚Äôs starting compoundFEN+movement rules to a graph theory representation.&lt;/p&gt;
    &lt;p&gt;Things like disappearing ‚Äòcities‚Äô (due to captures), ‚Äòroads‚Äô changing upon every city visit (due to the echoing mechanism), a morphing topology as the graph is being explored: we would need to account for each of these meticulously.&lt;/p&gt;
    &lt;p&gt;Yes, some heuristic-based backtracking algorithm with clever pruning may put up a decent fight with all of this. But at the end of the day, let‚Äôs not lose track of what we actually care about.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ultimately, the main goal we‚Äôre after here is answering this simple question: is this level we‚Äôre serving to the player likely to be ‚Äúsolvable‚Äù or ‚Äúnot solvable‚Äù?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Remember: we‚Äôre not really looking for any actual solutions to a maze, that would be our players‚Äô job ‚Äîthey can keep all the fun to themselves. We just need a simple boolean to tell us YES or NO for solvability. So at the very core of it, we are dealing with a Classification problem!&lt;/p&gt;
    &lt;p&gt;And that‚Äôs something we should be able to, in theory, deal much better with, assuming we set things up correctly. Why don‚Äôt we give that a shot? üåö&lt;/p&gt;
    &lt;head rend="h3"&gt;Data mining&lt;/head&gt;
    &lt;p&gt;The thing that comes to mind right away when we think of any supervised learning approach is data. Do we have any, can we get any, what shape does it come in, will we get it labeled, will it be clean, will we have enough, you know the drill.&lt;/p&gt;
    &lt;p&gt;Thankfully, the initial Classic mode of Echo Chess has already achieved a very humble level of traction and engagement, entirely thanks to word-of-mouth from players enjoying the game around the world.&lt;/p&gt;
    &lt;p&gt;Okay so we have some real users and we can serve them some fresh levels data, that‚Äôs great. Let‚Äôs assume for a second that we can keep getting similar traction and engagement for Endless mode as we got so far from Classic. How will we get labeled data out of this?&lt;/p&gt;
    &lt;p&gt;Well if someone gets to the next level, that means they must‚Äôve found a correct solution. Corollary: that level they were playing must be solvable. Just by winning at the game, they‚Äôre labeling it for us! Now we just need to get our Genki Dama donors to keep playing and solving while having fun.&lt;/p&gt;
    &lt;p&gt;Wait ‚Äîisn‚Äôt that a catch-22? How are we going to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;(a) convince enough people to play that game mode so we can&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(b) crowdsource labeled data, which we‚Äôll use to&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(c) train a model for predicting solvability, which will allow us to&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(d) guess which generated levels are solvable, so that we can&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(e) make something that‚Äôs fun to play, which is key in order to&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(f) convince enough people to play that game mode?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The subtle trick is in (e). If we can make something that‚Äôs just enough fun to play to kickstart the first cycle, we‚Äôll have our prime mover. We‚Äôll need it to be playable without the user reaching a dead end anytime an unsolvable level shows up. That would kill the whole vibe.&lt;/p&gt;
    &lt;p&gt;Say hello to the SHUFFLE button: your infinite Get-Out-of-Jail-Free card to shuffle those pesky dead ends away!&lt;/p&gt;
    &lt;p&gt;Okay, I guess we could acknowledge upfront there are imperfections and dead ends and hope someone will still give the janky prototype a try? Nah, we can do insanely better than that. We make it a feature, not a bug! Remember Minesweeper from the 90s?&lt;/p&gt;
    &lt;p&gt;A pothole is a tragic accident. A mine is a strategic trap.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Introducing ENDLESS Mode. Feeling stuck at any level? Try again, or simply 'SHUFFLE' it away and move on to the next one üíÅüèª‚ôÄÔ∏è&lt;/p&gt;
      &lt;p&gt;Watch out! Some levels are purposefully sprinkled in there as unsolvable traps to be SHUFFLED away üëª&lt;/p&gt;
      &lt;p&gt;Hone the skill of directly spotting dead-ends or you'll get stuck in a treacherous maze üß®&lt;/p&gt;
      &lt;p&gt;Don't be shy with SHUFFLING! You have unlimited shuffles - just keep an eye on the countdown or you'll be racing to Game Over‚è±Ô∏èüòâ&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To offer that same adrenaline rush old-school arcade games are famous for, we also cap the extra time that‚Äôs winnable from successfully solving levels back at the countdown‚Äôs starting 100sec. This lets our players keep their guard up as they progress ‚Äî and keeps our levels being diligently &lt;del&gt;solved&lt;/del&gt; labeled.&lt;/p&gt;
    &lt;p&gt;And the best part, no data whatsoever about any user is needed. Echo Chess simply tracks its own generated level configs, and tags whether they were solved or not. All data collection is 100% anonymous, not even 'anonymized'. Proud to be supporting Plausible.io, a true privacy-obsessed, cookie-less, GDPR-native, open source analytics platform.&lt;/p&gt;
    &lt;p&gt;Next, I needed to GTM with this crowdsourcing Trojan Horse. A savvy marketing friend recommended I post a promo video on Instagram (or, God forbid, Tiktok) to reach newcomers from the casual gaming scene.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pros: more users = labeled level data = better ML modelsü§ì&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cons: must download insta and learn what kids are into these days&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not too proud of what happened next but, hey, data ain‚Äôt cheap ngl.&lt;/p&gt;
    &lt;p&gt;Believe it or not, that lame promo + some trending song really hit it off with the social gods. Soon enough, Echo Chess grew to 1000s of WAUs entirely through word of mouth. I started getting tagged in Twitter conversations from strangers in other languages.&lt;/p&gt;
    &lt;p&gt;So that was great ‚Äî it meant I now had real users (happily) playtesting the randomly generated mazes for free, trying their best to beat each one, and automatically tagging a new maze as ‚Äòsolvable‚Äô in the background every time they leveled up. Win-win-win. Boom.&lt;/p&gt;
    &lt;p&gt;Obviously though, the challenge remained with identifying all the ‚Äòunsolvable‚Äô levels. Trying to crowdsource the labeling of these ones is extremely likely to lead to tons of false negative labels that dirty the data, so it‚Äôs a bad idea to delegate it. Just because someone out there couldn‚Äôt solve a level doesn‚Äôt mean it‚Äôs unsolvable. Only the opposite is true.&lt;/p&gt;
    &lt;p&gt;What‚Äôs the best way to guarantee your data is clean? Collect it yourself.&lt;/p&gt;
    &lt;p&gt;To get over the hump of initial bootstrapping, I went ahead and manually tested+tagged 500 different level generations. Yep, 500. Manually.&lt;/p&gt;
    &lt;p&gt;Was it time-consuming? Agree to disagree. Did my head hurt? Ice cream helped. Did I enjoy it? You bet. I mean I‚Äôm literally solving a puzzle game, it's not like I'm labeling dry paint.&lt;/p&gt;
    &lt;p&gt;And I‚Äôll let you in on a little secret, but please don‚Äôt try this in prod. I was &lt;del&gt;too lazy&lt;/del&gt; concerned about getting carpal tunnel from clicking around 100s of levels on my local dev env with a desktop mouse. So I pushed an easter-egg &lt;code&gt;div&lt;/code&gt; to the live mobile version of Echo Chess and I used it to tag all ‚Äòunsolvables‚Äô by tapping an invisible pixel on the screen while on the go ‚Äîthe ‚Äòsolvables‚Äô continued being tracked automatically upon leveling up.&lt;/p&gt;
    &lt;p&gt;Which brings us to today. Here‚Äôs the data mining pipeline as it stands: client app -&amp;gt; Flask server -&amp;gt; ReplitDB -&amp;gt; export to csv -&amp;gt; Jupyter nb for analysis + model training.&lt;/p&gt;
    &lt;p&gt;From here on out, once we reach some models we feel ‚Äògood enough‚Äô about, we can export them, import them into our Flask server, and handle the Inference stage from there through regular client-server communication.&lt;/p&gt;
    &lt;p&gt;But let‚Äôs not get ahead of ourselves yet. We continue our post-data-mining journey on the Jupyter side.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finally playing with some data&lt;/head&gt;
    &lt;p&gt;We start by pulling in the labeled levels data into a Jupyter notebook. I‚Äôm using Kaggle here, but we can easily switch it up with Colab, our own box, or literally anything.&lt;/p&gt;
    &lt;code&gt;1import pandas as pd2df = pd.read_csv('levels-dataset.csv')3df&lt;/code&gt;
    &lt;p&gt;That‚Äôs 5,548 labeled, unique, procedurally generated levels. Every single one of them manually played and certified by a human player as a solvable or unsolvable maze. Now we‚Äôre talkingü§ò&lt;/p&gt;
    &lt;p&gt;We seem to have ~40 levels (0.7%) that got mined with an &lt;code&gt;undefined&lt;/code&gt; compoundFEN. Let‚Äôs just get rid of those to keep things clean. No tears shed.&lt;/p&gt;
    &lt;code&gt;1df = df[~(df['compoundFen'] == 'undefined')]2df.reset_index(drop=True, inplace=True)3print(len(df))&lt;/code&gt;
    &lt;code&gt;15508&lt;/code&gt;
    &lt;p&gt;Done. 5.5k. That‚Äôs a nice round number.&lt;/p&gt;
    &lt;head rend="h3"&gt;Train-Test Splits&lt;/head&gt;
    &lt;p&gt;Before we do anything else, we should randomly split the data into three separate sets: Training, Validation and Test.&lt;/p&gt;
    &lt;p&gt;Remember taking the SAT? Our friend Bill here does and he‚Äôll take us down memory lane.&lt;/p&gt;
    &lt;p&gt;It only took Bill a few tries to nail those practice sheets that come with answers in the back. Sure, his first two attempts were abysmal. But that‚Äôs because the test format is weird. Once Bill saw the correct answers he directly got what the fuss is about. Soon he was acing every single question. Bill‚Äôs mom was proud. Her Billy was always such a fast learner.&lt;/p&gt;
    &lt;p&gt;Then Bill tried a ‚Äòrealistic‚Äô test. Downloaded a fresh test bank, sight-unseen ‚Äîeven timed himself and all. But Bill didn‚Äôt get so lucky this time around. No sweat, he‚Äôd been there before. Better flunk these than the actual exam. At least Bill was now used to the ‚Äòreal‚Äô conditions and the importance of guessing when getting unfamiliar questions.&lt;/p&gt;
    &lt;p&gt;By his tenth test, Bill had really perfected his guessing game. He taught his friends tips and tricks, preaching about multiple-choice patterns and Bayesian probabilities of B‚Äôs and C‚Äôs in sections following alternating A‚Äôs and D‚Äôs. He showed them charts he‚Äôd been plotting of test bank Q&amp;amp;As. Bill had finally conquered the SAT.&lt;/p&gt;
    &lt;p&gt;When Bill finally sat down for the real thing on D-Day, he couldn‚Äôt shake that weird feeling that the questions he was getting seemed a bit‚Ä¶ different. Still, Bill had an unfair advantage: his genius ‚Äòprobablistic guessing‚Äô technique. He could‚Äôve sworn it‚Äôs infallible ‚Äîeven thought about patenting it. Yet Bill never bombed so hard as he did that day.&lt;/p&gt;
    &lt;p&gt;Could it be that Bill was studying for the wrong version all along?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You, my friend, were majorly overfitting. You got good at gaming ‚Äòa‚Äô system, just not the one that mattered.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Instead of focusing his learning on universal concepts that generalize widely, Bill just wasted his time coming up with two-bit parlor tricks to impress a handful of practice tests. And the saddest part: if he hadn‚Äôt gone through the three sobering phases, Bill would‚Äôve never even known he sucked.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs not repeat Bill‚Äôs mistake. We want our models to learn actually useful stuff that‚Äôs applicable to data they‚Äôve never seen before. The whole point is to test how well we can predict solvability for new unkown levels. So let‚Äôs be sure to set some aside that we‚Äôll keep 100% untouched.&lt;/p&gt;
    &lt;code&gt;1# Split 10% into Test, preserving the `solvability` distribution2df, tst_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['solvability'])3&lt;/code&gt;
    &lt;code&gt;1print(len(trn_df), len(val_df), len(tst_df))&lt;/code&gt;
    &lt;code&gt;13855 1102 551&lt;/code&gt;
    &lt;p&gt;Great. We‚Äôve now randomly split our levels into:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;70% Training Set in&lt;/p&gt;&lt;code&gt;trn_df&lt;/code&gt;(3,855/5,508)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;20% Validation Set in&lt;/p&gt;&lt;code&gt;val_df&lt;/code&gt;(1,102/5,508)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;10% Test Set in&lt;/p&gt;&lt;code&gt;tst_df&lt;/code&gt;(551/5,508)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Data Preprocessing and Feature Engineering&lt;/head&gt;
    &lt;p&gt;Given the knowledge of EchoChess mechanics, we already know there are many features of a level that we could analyze and which could provide some solvability predictive power: things like walls, starting pieces, number of queens, and so on. All this information can be extracted from a level's &lt;code&gt;compoundFen&lt;/code&gt;, which means new features can be easily engineered and populated for every row.&lt;/p&gt;
    &lt;p&gt;Without boring you with the deets, I wrote a preprocessing function that takes in a pandas df, and returns it with all the extra features that may be of interest. This function is then called like this.&lt;/p&gt;
    &lt;code&gt;1proc_data(trn_df)2proc_data(val_df)3proc_data(tst_df)&lt;/code&gt;
    &lt;p&gt;Here are some of the new columns added by &lt;code&gt;proc_data&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;levelSize&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;levelGeoShape&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;levelOrientation&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numObstacleSquares&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numConnectedWalls&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;lengthOfLongestWall&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;startingWhitePiece&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;isStartingPieceBlocked&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numEndStates&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numBlackKnights&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numBlackBishops&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numBlackRooks&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numBlackQueens&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numBlackKings&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;numEmptySpaces&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;emptySquaresProportion&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;blackPiecesProportion&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;obstacleSquaresProportion&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;+to encode a certain understanding of the relative spatial positioning of every board state, we generate the following 64 features (for each of the squares in the 8x8 board):&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;[ content of cell&lt;/p&gt;&lt;code&gt;square_xy&lt;/code&gt;for every&lt;code&gt;xy&lt;/code&gt;position on the board ]&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a sample of what our Training set &lt;code&gt;trn_df&lt;/code&gt; looks like right now.&lt;/p&gt;
    &lt;p&gt;I‚Äôll only call out the implementation of a few interesting features we added that I think showcase well the power and flexibility of the &lt;code&gt;compoundFen&lt;/code&gt; format.&lt;/p&gt;
    &lt;code&gt;1def get_starting_white_piece(fen):2    return next((c for c in fen if c.isupper() and c != 'X'), '')&lt;/code&gt;
    &lt;code&gt;1def count_black_pieces(fen):2    excluded_chars = ['x', 'w']3    return sum(1 for c in fen if c.islower() and c not in excluded_chars)&lt;/code&gt;
    &lt;code&gt;1def count_empty_spaces(fen):2    piece_placement = fen.split(' ')[0]3    digits = [int(char) for char in piece_placement if char.isdigit()]4    return sum(digits)&lt;/code&gt;
    &lt;code&gt; 1def hasValidMove(rank, file, board): 2    piece = board[rank][file].lower() 3    if piece not in ['r', 'b', 'n', 'k', 'q']: 4        return 0	 5    def outOfBounds(i, j): 6        return (i &amp;lt; 0) or (i &amp;gt;= 8) or (j &amp;lt; 0) or (j &amp;gt;= 8) 7    def blockedMove(dx, dy): 8        if outOfBounds(rank+dx, file+dy): 9            return 110		# check if it hits an obstacle or boundary11        return (board[rank+dx][file+dy].lower() == 'x')12    blockedCross = all(blockedMove(dx, dy) for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)])13    blockedDiagonal = all(blockedMove(dx, dy) for dx, dy in [(1, 1), (-1, 1), (1, -1), (-1, -1)])14    l_jumps = [(2, 1), (2, -1), (-2, 1), (-2, -1), (1, 2), (1, -2), (-1, 2), (-1, -2)]15    blockedLJump = all(blockedMove(dx, dy) for dx, dy in l_jumps)   16    if piece == 'r' and blockedCross:17        return 018    elif piece == 'b' and blockedDiagonal:19        return 020    elif piece == 'n' and blockedLJump:21        return 022    elif (piece == 'k' or  piece == 'q') and blockedCross and blockedDiagonal:23        return 0     24    return 1&lt;/code&gt;
    &lt;code&gt;1def blocked_starting_piece(fen):2    board = fen_to_board(fen)3    white_pieces = ['R', 'B', 'N', 'K', 'Q']4    for rank in range(8):5        for file in range(8):6            if board[rank][file] in white_pieces and not hasValidMove(rank, file, board):7                return 18    return 0&lt;/code&gt;
    &lt;code&gt;1def numEndStates(fen):2    board = fen_to_board(fen)3    black_pieces = ['r', 'b', 'n', 'k', 'q']4    blocked_count = 05    for rank in range(8):6        for file in range(8):7            if board[rank][file] in black_pieces and not hasValidMove(rank, file, board):8                blocked_count += 19    return blocked_count&lt;/code&gt;
    &lt;p&gt;Each one of the numerous functions like these is then easily applied to the individual rows of the dataframe to create new features. For instance:&lt;/p&gt;
    &lt;code&gt;1df['numEndStates'] = df['compoundFen'].apply(numEndStates)2df['blackPieces'] = df['compoundFen'].apply(getBlackPieces)3df['numRooks'] = df['blackPieces'].apply(lambda x: countSpecificPiece(x, 'r'))&lt;/code&gt;
    &lt;p&gt;And so on and so forth.&lt;/p&gt;
    &lt;p&gt;In order to simplify our analysis down the line, let‚Äôs also make a distinction, within each of the three 90/20/10 datasets&lt;code&gt;trn_df&lt;/code&gt;, &lt;code&gt;val_df&lt;/code&gt;, and &lt;code&gt;tst_df&lt;/code&gt;, between the independent variables (the X‚Äôs) and the dependent variable (the Y). Y is the column we‚Äôre trying to predict, i.e. &lt;code&gt;solvability&lt;/code&gt;. X‚Äôs are all the columns we‚Äôre using to predict Y.&lt;/p&gt;
    &lt;p&gt;Note that this is a column split, not a row split like train-val-test. So the amount of data in each set won‚Äôt change.&lt;/p&gt;
    &lt;code&gt;1def xs_y(df):2    ...3	# `cats`, `conts` are names of categorical &amp;amp; continuous independent variables4    xs = df[cats+conts].copy()5    return xs,df['solvability'] if 'solvability' in df else None&lt;/code&gt;
    &lt;code&gt;1trn_xs, trn_y = xs_y(trn_df)2val_xs, val_y = xs_y(val_df)3tst_xs, tst_y = xs_y(tst_df)&lt;/code&gt;
    &lt;p&gt;Cool, that was easy. From here, anytime we want to analyze, model or predict the relationship between the X‚Äôs and the Y for any of the three train/val/test sets, we can simply use the corresponding pair.&lt;/p&gt;
    &lt;head rend="h3"&gt;Exploratory Data Analysis&lt;/head&gt;
    &lt;p&gt;We can now perform some preliminary EDA on the training set. As you already guessed, we‚Äôll be using &lt;code&gt;trn_xs&lt;/code&gt; and &lt;code&gt;trn_y&lt;/code&gt; for these plots.&lt;/p&gt;
    &lt;head rend="h2"&gt;Machine Learning&lt;/head&gt;
    &lt;p&gt;Based on these preliminary EDA observations, let's set up a dummy classifier to get a super quick baseline. We‚Äôre just eye-balling the above &lt;code&gt;trn_df&lt;/code&gt; plots for now.&lt;/p&gt;
    &lt;p&gt;Remember, we train a classifier on &lt;code&gt;trn_df&lt;/code&gt; ‚Äîor more precisely in this case, we ‚Äòeye-ball‚Äô a dummy classifier on &lt;code&gt;trn_df&lt;/code&gt;, then we validate our classifier‚Äôs predictive power on &lt;code&gt;val_df&lt;/code&gt;. The test set &lt;code&gt;tst_df&lt;/code&gt;is purposefully never touched! We‚Äôll only come back to it after we‚Äôve selected the winning model using &lt;code&gt;val_df&lt;/code&gt; and we‚Äôre done with absolutely everything.&lt;/p&gt;
    &lt;code&gt; 1def naive_predict(df): 2    predictions = [] 3    for _, row in df.iterrows(): 4        # having queens on the board helps 5        if row['numQueens'] &amp;gt; 0: 6            predictions.append(True) 7        # having fewer obstacles helps 8        elif row['obstaclesPortion'] &amp;lt; 0.25: 9            predictions.append(True)10        # assume everything else unsolvable11        else:12            predictions.append(False)13    return predictions&lt;/code&gt;
    &lt;code&gt;1naive_preds = naive_predict(val_xs)2print(mean_absolute_error(val_y, naive_preds))&lt;/code&gt;
    &lt;code&gt;0.14065335753176045&lt;/code&gt;
    &lt;p&gt;Here‚Äôs how to interpret this information:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;‚ÄòMean Absolute Error‚Äô (MAE) = how far off are we, on average, from correctly predicting the solvability (&lt;/p&gt;&lt;code&gt;val_y&lt;/code&gt;) of each of the 1,102 Echo Chess levels in&lt;code&gt;val_xs&lt;/code&gt;. We got MAE=0.14 here.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;‚ÄòConfusion Matrix‚Äô = fancy term to mean this 2x2 table above which gives a sense of how ‚Äòconfused‚Äô our classifier was in its predictions. For every group of solvability actuals (vertical axis), it shows us what our predictions were (horizontal axis).&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;We correctly predicted ‚Äòunsolvable‚Äô for 27 true unsolvables (True Negatives, TN). Nice.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;We incorrectly predicted ‚Äòunsolvable‚Äô for 60 levels that were actually solvable (False Negatives, FN). Not so nice.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;We correctly predicted ‚Äòsolvable‚Äô for 920 true solvables (True Positives, TP). Good.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;We incorrectly predicted ‚Äòsolvable‚Äô for 95 levels that were actually unsolvable (False Positives, FP). Not good.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our goal now is to hopefully improve on this naive predictor which will be used as a mininmum baseline for performance.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs first compare these results to a proper, but simple, Decision Tree to check whether our EDA-eye-balling predictor is that far off. Instead of us splitting hairs on where the nice-looking groups should be separated on visual plots, a tree model will recursively partition our levels based on the features that best separate them into distinct groups ‚Äîmaximizing both heterogeneity across the groups and homogeneity within each.&lt;/p&gt;
    &lt;code&gt; 1from sklearn.tree import DecisionTreeClassifier, export_graphviz 2import graphviz 3&lt;/code&gt;
    &lt;p&gt;Okay, so it‚Äôs first picking up on the proportion of obstacles to board size, just like we did. It has a more precise cut-off ‚Äîsure, we kinda winged that one. Then it‚Äôs looking at the number of capturable pieces on the board, and finally the type of the starting piece. That makes a lot of sense, the EDA plots for these were pretty interesting. Not too bad.&lt;/p&gt;
    &lt;code&gt;1print(mean_absolute_error(val_y, m.predict(val_xs)))&lt;/code&gt;
    &lt;code&gt;10.11070780399274047&lt;/code&gt;
    &lt;p&gt;MAE at 0.11. Some improvement already. Maybe we'll see even better results with a bigger tree that has more leaves. Feels seasonal enough.&lt;/p&gt;
    &lt;code&gt;1m = DecisionTreeClassifier(max_leaf_nodes=16)2m.fit(trn_xs, trn_y)3draw_tree(m, trn_xs, size=16)&lt;/code&gt;
    &lt;code&gt;1print(mean_absolute_error(val_y, m.predict(val_xs)))&lt;/code&gt;
    &lt;code&gt;10.1161524500907441&lt;/code&gt;
    &lt;p&gt;Nope, our average error actually increased a tiny bit. Let‚Äôs see if we get improvements with a full-on Random Forest instead.&lt;/p&gt;
    &lt;p&gt;Think of an RF as a team of individual Decision Trees working together to make better decisions. Each one takes a random chunk of our levels (rows) and a random subset of their features (columns) and tries to come up with some clever prediction. Then they get together and combine their efforts. It‚Äôs like traveling in big groups, only actually enjoyable, not entirely useless, and where decisions do get made.&lt;/p&gt;
    &lt;code&gt;1from sklearn.ensemble import RandomForestClassifier2&lt;/code&gt;
    &lt;code&gt;10.10435571687840291&lt;/code&gt;
    &lt;code&gt;1confusion_matrix(val_y, rf_preds)&lt;/code&gt;
    &lt;p&gt;Interesting. MAE looks better at 0.10, but we now seem to almost always be predicting the 'solvable' class, leading to a ridiculous amount of False Positives: 111 unsolvables incorrectly classified as solvable ü§¶üèª‚ôÇÔ∏è.&lt;/p&gt;
    &lt;p&gt;Remember when we data mined all those labeled levels, and how crowdsourcing through gaming was only letting us confidently gather solvables?&lt;/p&gt;
    &lt;p&gt;Well this was bound to happen, and we saw it coming, but now we‚Äôve ended up with an imbalanced level dataset where unsolvables are much less common (~1:8 ratio).&lt;/p&gt;
    &lt;p&gt;Might as well mindlessly predict ‚Äòsolvable‚Äô across the board.&lt;/p&gt;
    &lt;p&gt;Maybe we can try selecting a threshold higher than 50% for the model to predict &lt;code&gt;True&lt;/code&gt;. That‚Äôd be, loosely speaking, like asking it to increase its confidence in a level's solvability before saying yes.&lt;/p&gt;
    &lt;p&gt;Hmm. What do we choose as a threshold: 60%, 75%, 90%‚Ä¶ even higher? They say ML is an iterative science. Let‚Äôs &lt;del&gt;throw things at the wall&lt;/del&gt; start with an educated guess to get a feel for the impact.&lt;/p&gt;
    &lt;code&gt;1threshold = 0.752raw_rf_preds = rf.predict_proba(val_xs)[:, 1]3rf_preds = (raw_rf_preds &amp;gt;= threshold).astype(int)4&lt;/code&gt;
    &lt;code&gt;10.14791288566243194&lt;/code&gt;
    &lt;code&gt;1confusion_matrix(val_y, rf_preds)&lt;/code&gt;
    &lt;p&gt;Look at us ping-ponging. We now have a slightly worse MAE, but the amount of False Positives has significantly improved (almost halved) because we are now demanding a higher confidence threshold.&lt;/p&gt;
    &lt;p&gt;I feel like there‚Äôs a missing part.&lt;/p&gt;
    &lt;head rend="h3"&gt;Eval Metrics&lt;/head&gt;
    &lt;p&gt;Before we go any further with ML model iterations to see what works best, we need a crystal clear definition of what it means to be ‚Äòbest‚Äô for our specific use case, or we might end up optimizing the wrong thing right passed it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúIf you aim at nothing, you‚Äôll hit it every time.‚Äù ‚Äîsomeone who aimed at sounding deep.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It‚Äôs clear there are trade-offs here for improving some of the metrics at the expense of others. But is there a smarter and more systematic way to tune this threshold hyperparameter? Sure thing, we‚Äôll need to choose a clever point on the Precision-Recall and AUC-ROC curves, as we‚Äôll see below. Defining that point is trickier than it feels.&lt;/p&gt;
    &lt;p&gt;Precision vs Recall&lt;/p&gt;
    &lt;p&gt;Given that the ultimate goal of all this is to minimize unsolvable levels being served to the user, and given that the procedural generation of new levels is fairly efficient and 'cheap' in some sense, one argument could be to maximize ‚ÄòPrecision‚Äô (i.e. the proportion of levels classified as 'solvable' when they are, indeed, solvable in reality).&lt;/p&gt;
    &lt;p&gt;You can think of aiming for high precision as requiring a high confidence in the levels we are judging to be 'solvable', even if we happen to also be overconservatively discarding other levels that were borderline solvable but that we preferred classifying as 'unsolvable' just in case.&lt;/p&gt;
    &lt;p&gt;In other words, we‚Äôd be willing to optimize for high precision even at the expense of a potential drop in ‚ÄòRecall‚Äô (i.e. even if many potentially valid levels get missed out on through misclassification in our overzealous quest for purity of 'solvables').&lt;/p&gt;
    &lt;p&gt;FPR vs F1&lt;/p&gt;
    &lt;p&gt;Similarly, we also certainly want to minimize the "False Positives Rate" or FPR ratio (i.e. proportion of 'unsolvable' levels that mistakenly get classified as 'solvable', thus ruining our purity of 'solvables'). We can either plot FPR directly for every threshold of our binary classifier, or use the Receiver Operating Characteristic (ROC) curve to analyze the FPR vs TPR (True Positive Rate) trade-off. We‚Äôll do that shortly below.&lt;/p&gt;
    &lt;p&gt;However, given the class imbalance and that the majority of our dataset comes with positive labels ('solvable' levels), we can likely expect Precision in this dataset to be slightly less sensitive than FPR to an 'unsolvable' level being misclassified as 'solvable'. This is because precision (and recall) would reflect mostly the ability of prediction of the positive class (solvables), not the negative class (unsolvables) which will naturally be harder to detect due to the smaller number of samples.&lt;/p&gt;
    &lt;p&gt;In this case, we can expect FPR to be more sensitive than Precision to the model's actual performance given that the negative class is the minority one in the dataset.&lt;/p&gt;
    &lt;p&gt;Therefore, we will look for the optimal threshold range that minimizes FPR, and maximize the F1 Score within that range.&lt;/p&gt;
    &lt;p&gt;Why F1, for that secondary goal? Trying to maximize Precision even further within an already minimized FPR range could lead to undesirable extremes like incredibly low Recall and/or TPR values. Conversely, it could be tempting to search within that range for a 'reasonable-recall' point along the Precision-Recall curve, so that fewer procedurally generated solvables get unnecessarily tossed out. But that's pretty much what F1 is doing for us under the hood either way, so F1 gets us two birds, one stone.&lt;/p&gt;
    &lt;p&gt;We then define a max % FPR we are willing to accept for our use case, say, 5%. Remember, this would represent how many of the generated levels that we predict to be solvable - all else equal - end up being, unfortunately, unsolvable. We‚Äôll still have some leeway in how to make use of these generated levels and their predictions in prod, but for now this seems like a reasonable starting point.&lt;/p&gt;
    &lt;p&gt;To recap our approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;For evaluating models:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;focus on&lt;/p&gt;&lt;code&gt;FPR&lt;/code&gt;(rightside ratio in our confusion matrix)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To select prediction thresholds:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;sweep along the thresholds that are within 5pp of min&lt;/p&gt;
            &lt;code&gt;FPR&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;select the one that maximizes&lt;/p&gt;&lt;code&gt;F1&lt;/code&gt;among these&lt;/item&gt;
          &lt;item&gt;&lt;p&gt;set that&lt;/p&gt;&lt;code&gt;min_fpr_max_f1_threshold&lt;/code&gt;as a hyperparameter&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs the code we‚Äôll need to implement it ‚Äîminus minutiae for plots and the like. We‚Äôll be plotting a bunch of cool curves for different models but it‚Äôs all based on the kernel below.&lt;/p&gt;
    &lt;code&gt; 1from sklearn.metrics import roc_curve, precision_recall_curve 2&lt;/code&gt;
    &lt;head rend="h3"&gt;Model Selection and Tuning&lt;/head&gt;
    &lt;p&gt;Great. So now we have data, features, metrics, and a proper way to select suitable models. Let‚Äôs spin up some proper tabular models and see what‚Äôs up.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tabular models&lt;/head&gt;
    &lt;p&gt;Random Forest&lt;/p&gt;
    &lt;p&gt;Remember our &lt;code&gt;rf&lt;/code&gt;? Let‚Äôs retune this model to improve the metrics we actually care about.&lt;/p&gt;
    &lt;code&gt;1rf_targets = threshold_tune_binary_classifier(val_xs, val_y, rf)&lt;/code&gt;
    &lt;code&gt;1Threshold: 0.95056854256854252Precision: 98.63%3Recall: 43.98%4F1 Score: 60.83%5True Positive Rate: 43.98%6False Positive Rate: 4.92%&lt;/code&gt;
    &lt;p&gt;And I‚Äôve got some snazzy plots for this &lt;code&gt;rf&lt;/code&gt; for us to look at. Don‚Äôt worry if things look a bit confusing, we‚Äôll go through them together in a sec.&lt;/p&gt;
    &lt;p&gt;What does this all mean?&lt;/p&gt;
    &lt;p&gt;It means that for this model and dataset, if we set an overly conservative prediction threshold (i.e. has to be &amp;gt;0.95 to be considered solvable, everything else is assumed not), we can drop the False Positive rate down to 4.9% and get a pretty high Precision of 98.6%. In other words, we‚Äôd be pretty confident that the solvable levels we choose are actually solvable.&lt;/p&gt;
    &lt;p&gt;Evidently this comes at a cost. We‚Äôd only be getting a True Positive rate of 44%, so we‚Äôd be throwing out 1 of every ~2.2 actually solvable levels, say half of them. But that‚Äôs the best that this model can give us for our data given the metrics we care to optimize. Not too shabby tbh.&lt;/p&gt;
    &lt;p&gt;All in all, this looks promising. We‚Äôll probably end up using multiple models that can pick up on different things and ensemble them together. Let‚Äôs add this model to our &lt;code&gt;models_predictions&lt;/code&gt; ensembling dict.&lt;/p&gt;
    &lt;code&gt;1raw_preds = rf.predict_proba(val_xs)[:, 1]¬†2rf_threshold_target = rf_targets['threshold']3predictions = (raw_preds &amp;gt;= rf_threshold_target).astype(int)4models_predictions['Random-Forest-tuned'] = {5¬†¬†¬†¬†'tuned_threshold': rf_threshold_target,6¬†¬†¬†¬†'raw_predictions': raw_preds,7¬†¬†¬†¬†'class_predictions': predictions8}&lt;/code&gt;
    &lt;p&gt;Balanced Random Forest&lt;/p&gt;
    &lt;p&gt;Another approach is to use a Balanced Random Forest (BRF) which could also help reduce the rate of false positives caused by our imbalanced dataset.&lt;/p&gt;
    &lt;code&gt;1from imblearn.ensemble import BalancedRandomForestClassifier2&lt;/code&gt;
    &lt;p&gt;This has much less FPs than the PRE-tuning unbalanced Random Forest, but it does not beat the threshold-tuned version of our unbalanced rf, given our metrics goals. Let's see if we can combine both approaches by threshold-tuning the BRF.&lt;/p&gt;
    &lt;code&gt;1brf_threshold_target = threshold_tune_binary_classifier(val_xs, val_y, brf, max_fpr_tolerance)['threshold']&lt;/code&gt;
    &lt;code&gt;1Threshold: 0.69888915333032992Precision: 98.67%3Recall: 45.51%4F1 Score: 62.29%5True Positive Rate: 45.51%6False Positive Rate: 4.92%&lt;/code&gt;
    &lt;p&gt;This tuned-BRF has very comparable FPR and Precision to the unbalanced tuned-RF, while also having slightly TPR and F1. We‚Äôll add it to the ensemble as well ‚Äîsame code with &lt;code&gt;brf&lt;/code&gt; instead of &lt;code&gt;rf&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;XGBoost (eXtreme Gradient Boosting)&lt;/p&gt;
    &lt;p&gt;I‚Äôm thinking it‚Äôs worth trying to train an XGBoost model instead ‚ÄîXGBoost tends to handle unbalanced datasets slightly better, so it could be a good fit in this case.&lt;/p&gt;
    &lt;p&gt;Remember the team analogy we touched on for random forests? XGBoost is like that smart kid who added some structure around the decision-making team. You‚Äôre still creating a bunch of trees, but not entirely randomly ‚Äîinstead you try to fix the mistakes of the previous trees as you go, paying extra attention to the errors and adjusting each new tree accordingly. Hence the boosting name.&lt;/p&gt;
    &lt;code&gt;1import xgboost as xgb2&lt;/code&gt;
    &lt;p&gt;This first attempt of XGBoost seems to have a fairly high FPR. Let's try threshold-tuning it see if it can fit our needs better.&lt;/p&gt;
    &lt;code&gt;1xgb_threshold_target = threshold_tune_binary_classifier(val_xs, val_y, xgb_model, max_fpr_tolerance)['threshold']&lt;/code&gt;
    &lt;code&gt;1Threshold: 0.95943218469619752Precision: 98.92%3Recall: 46.94%4F1 Score: 63.67%5True Positive Rate: 46.94%6False Positive Rate: 4.10%&lt;/code&gt;
    &lt;p&gt;Look at that gorgeous baby. Better vitals across the board: lower FPR, higher Precision, higher Recall/TPR, higher F1. Sold to the ensemble in the back.&lt;/p&gt;
    &lt;code&gt;1raw_preds = xgb_model.predict_proba(val_xs)[:, 1]2predictions = (raw_preds &amp;gt;= xgb_threshold_target).astype(int)3models_predictions['XGBoost-tuned'] = {4    'tuned_threshold': xgb_threshold_target,5    'raw_predictions': raw_preds,6    'class_predictions': predictions7}&lt;/code&gt;
    &lt;p&gt;Feature Importance&lt;/p&gt;
    &lt;p&gt;Another interesting approach we can try is to only select the most important features according to the random forest, and then train our model only on these features, disregarding all others. The reason this works in many cases is because it can help reduce noise, multicollinearity with less relevant features, and make the model less prone to overfitting.&lt;/p&gt;
    &lt;p&gt;Random Forests are usually good at identifying feature importance in a dataset. Let‚Äôs take a look using our &lt;code&gt;rf&lt;/code&gt; model.&lt;/p&gt;
    &lt;code&gt; 1def get_top_features_importance(model, trn_xs, min_imp=0, num_top_features=40): 2    plt.figure(figsize=(10, 8)) 3    feature_importances = model.feature_importances_     4    df = pd.DataFrame(dict(cols=trn_xs.columns, imp=feature_importances)) 5    df = df[df['imp'] &amp;gt; min_imp].sort_values(by='imp').tail(num_top_features) 6    ax = df.plot('cols', 'imp', kind='barh', legend=False, xlabel='', ylabel='') 7    plt.show() 8    return df.sort_values(by='imp', ascending=False) 9&lt;/code&gt;
    &lt;p&gt;Now let‚Äôs retrain our random forest only on these top features. We‚Äôll make sure to threshold-tune it as well.&lt;/p&gt;
    &lt;code&gt;1imp_trn_xs = trn_xs[rf_top_features.cols]2imp_val_xs = val_xs[rf_top_features.cols]3&lt;/code&gt;
    &lt;code&gt;1Threshold: 0.96142932067932062Precision: 98.72%3Recall: 47.14%4F1 Score: 63.81%5True Positive Rate: 47.14%6False Positive Rate: 4.92%&lt;/code&gt;
    &lt;p&gt;Anoter model giving great results. Throw it in the ensemble bag. You know the drill.&lt;/p&gt;
    &lt;p&gt;Alternatively, we could check if there‚Äôs a better feature selection subset according to the feature importance interpretation of an XGBoost model as opposed to the Random Forest. In general, these should be close enough but it doesn‚Äôt hurt to check for our data.&lt;/p&gt;
    &lt;code&gt; 1def xgb_top_features_importance(xgb_model, importance_type, min_imp=0, num_top_features=40): 2    xgb.plot_importance(xgb_model, importance_type=importance_type, max_num_features=num_top_features, 3                        title=f"Feature {importance_type} importance", xlabel=f"{importance_type}s") 4    xgb_imps_dict = xgb_model.get_booster().get_score(importance_type=importance_type) 5    xgb_imps_dict = dict(sorted(xgb_imps_dict.items(), key=lambda item: item[1], reverse=True)) 6    df = pd.DataFrame(list(xgb_imps_dict.items()), columns=['features', f"{importance_type}_importance"]).head(num_top_features) 7    df = df[ df[f"{importance_type}_importance"] &amp;gt;= min_imp ] 8    return df 9	10xgb_new = xgb.XGBClassifier(n_estimators=100, min_child_weight=5, random_state=42, scale_pos_weight=sum(trn_y==0)/sum(trn_y==1))11xgb_new.fit(trn_xs, trn_y)12xgb_top_features = xgb_top_features_importance(xgb_new, importance_type="gain", num_top_features=15)13x_imp_trn_xs = trn_xs[xgb_top_features.features]14x_imp_val_xs = val_xs[xgb_top_features.features]&lt;/code&gt;
    &lt;p&gt;Interestingly, when we retrain the XGB model on &lt;code&gt;xgb_top_features&lt;/code&gt; instead of &lt;code&gt;rf_top_features&lt;/code&gt;, we get slightly worse results. Even varying the &lt;code&gt;importance_type&lt;/code&gt; XGB uses to select the top features (across gain, cover or weight) doesn‚Äôt improve so much over the good old &lt;code&gt;rf_top_features&lt;/code&gt; approach. We‚Äôll just keep it simple and stick to the original appraoch we had.&lt;/p&gt;
    &lt;p&gt;I‚Äôd say we have a decent start so far with these tabular models. There‚Äôs something much more intriguing we might be missing out on, though. I‚Äôm sure some of you reading this might have been waiting for it from the moment we uttered the word ‚Äòclassification‚Äô.&lt;/p&gt;
    &lt;head rend="h3"&gt;CNNs and image classification&lt;/head&gt;
    &lt;p&gt;CNN classifier using chess board arrays&lt;/p&gt;
    &lt;p&gt;Taking inspiration from image recognition approaches, since 2D chess boards are very structured and always come in the same form with a cell containing one of a predefined set of classes, we can train a Convolutional Neural Network (CNN) on the 2D chess board states extracted from every level's FEN.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;(1) We‚Äôd need 13 categories for the content of a cell given that pawns are excluded here (K, Q, B, N, R, k, q, b, n, r, _, x, X).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(2) We then one-hot-encode each of these 13 to avoid any implicit relative ranking between them.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(3) Every one-hot-encoding of the 13 can now be considered a separate channel for the CNN.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;(4) So our input tensor becomes (numSamples, 13, 8, 8) for every 8x8 chess board. Note that since the biggest levels in Endless mode are of size 6x6, we could also preconvert all 2D boards to 6x6 and make the input tensor (numSamples, 13, 6, 6) if necessary.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach might even capture the essence of the spatial data and relative positioning of pieces in a level in a better way than img classification of the rendered level from the Echo Chess app, because only the 'substance' of a level's config is being trained on, without all the noise and variability linked to unnecessary visual elements.&lt;/p&gt;
    &lt;p&gt;On the flip side, though, this may also make it challenging to simply fine-tune a pretrained SOTA CNN or img classifier from the web since the input format could become substantially different from what is expected.&lt;/p&gt;
    &lt;p&gt;In any case, we can see how well a basic neural net performs in this approach, and then take it from there. We‚Äôll need to prepare the input data accordingly.&lt;/p&gt;
    &lt;code&gt;1trn_X = np.array([fen_to_board(x) for x in trn_df['compoundFen'].values])2one_hot_trn_X = np.array([one_hot_encode_2D_array(x, char_to_index) for x in trn_X])3trn_X_tensors = torch.tensor(one_hot_trn_X)&lt;/code&gt;
    &lt;p&gt;We‚Äôll also do the same for &lt;code&gt;val_X&lt;/code&gt;, &lt;code&gt;trn_y&lt;/code&gt;, and &lt;code&gt;val_y&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To choose a CNN architecture, let‚Äôs first start with a simple baseline and we can iterate from there. In general, most CNNs can be thought of as pretty much some conv layers, normalization, and activation functions, rinse and repeat. We‚Äôll use the following simple arch as a start.&lt;/p&gt;
    &lt;code&gt; 1class ChessCNN(nn.Module): 2    def __init__(self): 3        super(ChessCNN, self).__init__() 4        # Convolutional Layer 1 5        self.conv1 = nn.Conv2d(in_channels=13, out_channels=16, kernel_size=3, stride=1) 6        # Max Pooling Layer 1 7        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) 8        # Convolutional Layer 2 9        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1)10        # Fully Connected Layer 111        self.fc1 = nn.Linear(in_features=32, out_features=64)12        # Fully Connected Layer 2 (Output Layer)13        self.fc2 = nn.Linear(in_features=64, out_features=1)14&lt;/code&gt;
    &lt;p&gt;This is what our basic &lt;code&gt;cnn&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;1ChessCNN(2  (conv1): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1))3  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)4  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))5  (fc1): Linear(in_features=32, out_features=64, bias=True)6  (fc2): Linear(in_features=64, out_features=1, bias=True)7)&lt;/code&gt;
    &lt;p&gt;We‚Äôll train this &lt;code&gt;cnn&lt;/code&gt; model for 8 epochs with the Binary Cross-Entropy (BCE) as the Loss Function, Adam as the optimizer, and a Learning Rate of 0.002. Why these choices? That‚Äôs what worked best in my experiments on this data.&lt;/p&gt;
    &lt;code&gt; 1cnn = ChessCNN() 2criterion = nn.BCELoss() 3optimizer = optim.Adam(cnn.parameters(), lr=0.002) 4num_epochs = 8 5batch_size = 8 6for epoch in range(num_epochs): 7    cnn.train() 8    for i in range(0, len(trn_X_tensors), batch_size): 9        inputs = trn_X_tensors[i:i+batch_size].float()10        labels = torch.tensor(trn_y[i:i+batch_size], dtype=torch.float32).view(-1, 1)11        optimizer.zero_grad()12        outputs = cnn(inputs)13        loss = criterion(outputs, labels)14        loss.backward()15        optimizer.step()&lt;/code&gt;
    &lt;p&gt;Now that we have a neural net classifier spun up, let‚Äôs tune its prediction threshold like we did for the other classifer models.&lt;/p&gt;
    &lt;code&gt; 1def get_cnn_raw_preds(cnn, X_train, X_val): 2    cnn.eval() 3    with torch.no_grad(): 4        trn_raw_predictions = cnn(X_train.float()) 5        trn_raw_predictions = trn_raw_predictions.squeeze().numpy() 6        val_raw_predictions = cnn(X_val.float()) 7        val_raw_predictions = val_raw_predictions.squeeze().numpy() 8    raw_preds_dict = { 9        "trn_raw_preds": trn_raw_predictions,10        "val_raw_preds": val_raw_predictions11    }12    return raw_preds_dict13	14trn_raw_predictions = get_cnn_raw_preds(cnn, trn_X_tensors, val_X_tensors)['trn_raw_preds']15val_raw_predictions = get_cnn_raw_preds(cnn, trn_X_tensors, val_X_tensors)['val_raw_preds']&lt;/code&gt;
    &lt;code&gt;1cnn_threshold_target = threshold_tune_binary_classifier(val_X_tensors, val_y, cnn, max_fpr_tolerance, val_raw_preds=val_raw_predictions)['threshold']&lt;/code&gt;
    &lt;p&gt;Okay, now we‚Äôre talking. But before we rush and add this model to our ensemble, it‚Äôs always good to check our curves to see if anything looks off.&lt;/p&gt;
    &lt;p&gt;Hmm. This model's curves for FPR, Precision and (especially) F1 are incredibly steep in the vicinity of the threshold target. I don‚Äôt feel so great about this news.&lt;/p&gt;
    &lt;p&gt;It means that the model could be extremely sensitive to noise or slight variations in the data. And it‚Äôs especially the case for the FPR value that could fluctuate easily between 0 and 40% if the test data has enough variation compared to our training+validation sets. It will be important to keep this in mind when deciding whether/how to incorporate this model's predictions in the ensemble.&lt;/p&gt;
    &lt;p&gt;With all this experimentation we‚Äôve been doing to deal with our imbalanced data, we still haven‚Äôt tried adding the secret sauce. Let‚Äôs rectify this next.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resampling to balance class distribution&lt;/head&gt;
    &lt;p&gt;Given that only a small proportion of levels in the data are of the ‚Äòunsolvable‚Äô class, what if we just tried to‚Ä¶ oversample from this minority class to get a more balanced dataset?&lt;/p&gt;
    &lt;p&gt;If we believe the unsolvable levels we currently have fairly represent the underlying distribution of unsolvables, let‚Äôs sample more of them! This has the potential to improve model performance by tackling the imbalanced data issue at the root.&lt;/p&gt;
    &lt;p&gt;We proceed to trying various resampling methods like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;RandomOverSampler&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SMOTE&lt;/code&gt;(Synthetic Minority Oversampling Technique)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;BorderlineSMOTE&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;ADASYN&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So now we go back to the very beginning, oversample, retrain, retune, and re-evaluate. For each resampling method and each model. Fun fun fun üôÉ Don‚Äôt worry I‚Äôll fast-forward you to the interesting stuff.&lt;/p&gt;
    &lt;p&gt;From all the experimentation I did, the resampling method that led to the best performance was the regular random oversampling without interpolation using the &lt;code&gt;RandomOverSampler&lt;/code&gt;. SMOTE and its variant resampling methods were disappointingly not helpful in improving results on this dataset. In fact, performance actually suffered when trying out fancier resampling methods on this data.&lt;/p&gt;
    &lt;code&gt;1trn_xs_rnd_resampled, trn_y_rnd_resampled = resample_to_balance('RandomOverSampler', trn_xs,trn_y)&lt;/code&gt;
    &lt;p&gt;Oversampling with tuned-RF&lt;/p&gt;
    &lt;p&gt;Now that we‚Äôve updated our training set with oversampling, we can retrain (and re-tune) our promising random forest model. Notice we‚Äôre using the new versions here: &lt;code&gt;trn_xs_rnd_resampled&lt;/code&gt; and &lt;code&gt;trn_y_rnd_resampled&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;1rf_rnd_rs = RandomForestClassifier(100, min_samples_leaf=5, random_state=42)2rf_rnd_rs.fit(trn_xs_rnd_resampled, trn_y_rnd_resampled)3rf_rnd_rs_threshold_target = threshold_tune_binary_classifier(val_xs, val_y, rf_rnd_rs, max_fpr_tolerance)['threshold']&lt;/code&gt;
    &lt;code&gt;1Threshold: 0.89031130634071832Precision: 98.60%3Recall: 43.06%4F1 Score: 59.94%5True Positive Rate: 43.06%6False Positive Rate: 4.92%&lt;/code&gt;
    &lt;p&gt;Sure, not bad, I‚Äôll take it. Dump it in our ensemble.&lt;/p&gt;
    &lt;code&gt;1models_predictions['RandomOversampling-Random-Forest-tuned'] = {2    'tuned_threshold': rf_rnd_rs_threshold_target,3    'raw_predictions': rf_rnd_rs.predict_proba(val_xs)[:, 1] ,4    'class_predictions': (rf_rnd_rs.predict_proba(val_xs)[:, 1] &amp;gt;= threshold).astype(int)5}&lt;/code&gt;
    &lt;p&gt;Oversampling with tuned-BalancedRF&lt;/p&gt;
    &lt;code&gt;1brf_rnd_rs = BalancedRandomForestClassifier(n_estimators=100, min_samples_leaf=5, random_state=42)2brf_rnd_rs.fit(trn_xs_rnd_resampled, trn_y_rnd_resampled)3brf_rnd_rs_threshold_target = threshold_tune_binary_classifier(val_xs, val_y, brf_rnd_rs, max_fpr_tolerance)['threshold']&lt;/code&gt;
    &lt;code&gt;1Threshold: 0.89994041660953432Precision: 98.50%3Recall: 40.10%4F1 Score: 57.00%5True Positive Rate: 40.10%6False Positive Rate: 4.92%&lt;/code&gt;
    &lt;p&gt;Okay, maybe I‚Äôm getting a little too picky given the stellar options we have, but this one might be pushing it a bit.&lt;/p&gt;
    &lt;p&gt;I mean we‚Äôre already oversampling, and using a balanced RF to handle imbalanced data, and tuning a threshold to be super conservative (all at the risk of generalizability issues). At least we should expect some crazy performance. I think we pass on this one.&lt;/p&gt;
    &lt;p&gt;Oversampling with tuned-XGBoost&lt;/p&gt;
    &lt;p&gt;I‚Äôll spare you the same lines of code. This one also got better with oversampling. Guess where it went? Into the ensemble.&lt;/p&gt;
    &lt;p&gt;I think this was actually our best model so far ‚Äîhard to keep up with all these nice goodies. Real-talk, though, next time I‚Äôm going down such a deep rabbit hole, I‚Äôm obviously using an MLOps platform like Weights &amp;amp; Biases to keep track of all these model iterations. IYKYK. You live and you learn.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ensembling all the promising models&lt;/head&gt;
    &lt;p&gt;So far we‚Äôve been adding each promising model to a &lt;code&gt;models_predictions&lt;/code&gt; dict of dicts. Let‚Äôs convert it to a dataframe to work with it easily.&lt;/p&gt;
    &lt;code&gt;1models_predictions_df = pd.DataFrame.from_dict(models_predictions, orient='index')2print(models_predictions_df.index.values)&lt;/code&gt;
    &lt;code&gt;1['Random-Forest-tuned' 'Balanced-Random-Forest-tuned' 'XGBoost-tuned' 'Top-features-RF-tuned' 'CNN-tuned' 'RandomOversampling-Random-Forest-tuned' 'RandomOversampling-XGBoost-tuned']&lt;/code&gt;
    &lt;p&gt;We can now experiment with different approaches of ensembling using the Validation set to pick the winning one.&lt;/p&gt;
    &lt;p&gt;For instance, we could get the mean raw predictions across the ensemble and then use our existing &lt;code&gt;threshold_tune_binary_classifier&lt;/code&gt; function to select a good &lt;code&gt;ensemble_raw_threshold_target&lt;/code&gt; prediction threshold for the avg raw preds that minimizes FPR and maximizes F1 scores within our 5% tolerance.&lt;/p&gt;
    &lt;code&gt;1avg_models_raw_preds = models_predictions_df['raw_predictions'].mean()2&lt;/code&gt;
    &lt;p&gt;Similarly, we could ensemble by averaging the class prediction themselves (instead of raw preds), then selecting a good &lt;code&gt;ensemble_classes_threshold_target&lt;/code&gt; for the avg of the class preds themselves.&lt;/p&gt;
    &lt;code&gt;1avg_models_class_preds = models_predictions_df['class_predictions'].mean()2&lt;/code&gt;
    &lt;p&gt;Or we could require that both these approaches agree (relative to each one‚Äôs tuned threshold respectively), if we wanted to be even more overconservative on solvability.&lt;/p&gt;
    &lt;code&gt;1comb_preds = (avg_raw_to_tuned_preds * avg_class_to_tuned_preds &amp;gt; 0).astype(int)&lt;/code&gt;
    &lt;p&gt;But in general, each of these approaches risks getting us into dangerous overfitting territory as we squeeze greedily more of the tuning juice out of the validation set. Interestingly, the simpler, non-tuned approach, ends up being as performant too: ensembling through majority voting of the class predictions.&lt;/p&gt;
    &lt;code&gt;1majority_needed = math.ceil(num_models_in_ensemble / 2)2majority_class_preds = (num_positive_class_preds &amp;gt;= majority_needed).astype(int)&lt;/code&gt;
    &lt;p&gt;Here are the final results, side-by-side, for each of these 4 ensembling techniques, post-threshold-tuning (if applicable) on the validation set:&lt;/p&gt;
    &lt;p&gt;Looking at the TPs and FPs of each, no matter which ensembling method is chosen, when judging a generated level as solvable, we‚Äôd still guess right ~99% of the time. For this and the reasons above, we‚Äôll go with the simple majority approach as our ensembling technique.&lt;/p&gt;
    &lt;p&gt;More specifically, our goal when productizing this will be to pick the ‚Äúmost-promisingly solvable‚Äù level from a candidate list of randomly generated levels. So we‚Äôll sort first by class majority vote predictions, then by avg raw preds within each class ‚Äîthis way even if no solvables are found, at least we‚Äôd be serving the most promising level among the unsolvable ones.&lt;/p&gt;
    &lt;code&gt;1# Example usage for inference2ensemble_preds = predict_solvability(level_df, 'models')3most_promising_levels = ensemble_preds.sort_values(by=['ensemble_preds', 'avg_raw_preds'], ascending=[False, False])&lt;/code&gt;
    &lt;head rend="h3"&gt;Inference on the External Test&lt;/head&gt;
    &lt;p&gt;Remember that 10% of data we completely stashed away in &lt;code&gt;tst_df&lt;/code&gt; ages ago? We can now finally check how our ensemble is doing on it. If it sucks, we‚Äôre kind of back to square one like our friend Bill. Moment of truth.&lt;/p&gt;
    &lt;p&gt;Boom! And that's it! Extremely promising results on both the validation and test sets. When we‚Äôre ready to push this to prod, we can load these best-performing models into the server and cache them, grab their pre-tuned thresholds, generate individual predictions at the inference stage, and ensemble them together through majority voting.&lt;/p&gt;
    &lt;p&gt;Great stuff. 6 classical ML + 1 DL model. All straightforward architectures. Preprocessing, feature engineering, hyperparameter tuning and some usage of oversampling here and there to handle the imbalanced dataset. A few other minor things but all in all just a vintage ML modeling routine.&lt;/p&gt;
    &lt;p&gt;But wait! We still have two final tricks up our sleeve to improve this further. Chess players will like these.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trick #1: Data Augmentation&lt;/head&gt;
    &lt;p&gt;Taking inspiration from other domain areas like image processing, we can quickly notice an opportunity to significantly augment our dataset by creating new valid, labeled samples from our existing labeled ones:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Every&lt;/p&gt;&lt;code&gt;rectangle&lt;/code&gt;level can be mirrored either horizontally or vertically on the opposing side (depending on its shape) to create one new, effectively identical, level for classification.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Similarly, every&lt;/p&gt;&lt;code&gt;quad&lt;/code&gt;level can be mirrored across the 3 opposing corners (either horizontally, vertically, or both horizontally and vertically) to create 3 new, effectively identical, levels.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of these 4 levels above are pretty much the same and should command the same &lt;code&gt;solvability&lt;/code&gt; label, yet they each have a different &lt;code&gt;compoundFEN&lt;/code&gt; input. Importantly, they also accurately represent the type of levels that get generated by our ground truth distribution ‚Äîwhich we can be sure of, given that it‚Äôs a parametrized distribution we invented ourselves for the random level generator.&lt;/p&gt;
    &lt;p&gt;Good data augmentation opportunity + easy low hanging fruits. Let‚Äôs implement them quickly.&lt;/p&gt;
    &lt;code&gt; 1def mirror(compoundFen, direction): 2    table = fenToTable(compoundFen) 3    if direction == 'h': 4        return tableToFen(table[[0, 4, 5, 6, 1, 2, 3, 7]]) 5    elif direction == 'v': 6        return tableToFen(table[:, [0, 4, 5, 6, 1, 2, 3, 7]]) 7		 8def augment(rows, mirroring_direction): 9    augmented_rows = rows.copy()10    augmented_rows['compoundFen'] = augmented_rows['compoundFen'].apply(mirror, args=(mirroring_direction,))                      11    augmented_rows['levelOrientation'] = augmented_rows['levelOrientation'].apply(switch_orientation, args=(mirroring_direction,))12    return augmented_rows13&lt;/code&gt;
    &lt;p&gt;And here is a simple example in action.&lt;/p&gt;
    &lt;code&gt;1fen_test = "XXXXXXXX/XqbxBb1X/Xx1xnxrX/X1n1kq1X/XXXXXXXX/XXXXXXXX/XXXXXXXX/XXXXXXXX w - - 0 1"2print(fen_to_board(fen_test))&lt;/code&gt;
    &lt;code&gt;1[['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']2 ['X' 'q' 'b' 'x' 'B' 'b' ' ' 'X']3 ['X' 'x' ' ' 'x' 'n' 'x' 'r' 'X']4 ['X' ' ' 'n' ' ' 'k' 'q' ' ' 'X']5 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']6 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']7 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']8 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]&lt;/code&gt;
    &lt;code&gt;1mirrored_fen = mirror(fen_test,'h')2print(fen_to_board(mirrored_fen))&lt;/code&gt;
    &lt;code&gt;1[['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']2 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']3 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']4 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']5 ['X' 'q' 'b' 'x' 'B' 'b' ' ' 'X']6 ['X' 'x' ' ' 'x' 'n' 'x' 'r' 'X']7 ['X' ' ' 'n' ' ' 'k' 'q' ' ' 'X']8 ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]&lt;/code&gt;
    &lt;p&gt;It may look silly, but this stuff actually allows us to 2X our 'rectangle' row samples and 4X our 'quad's, significantly increasing the size of our dataset.&lt;/p&gt;
    &lt;code&gt;1augmented_trn_df = mirror_augmentation(trn_df)2print(len(trn_df), len(augmented_trn_df))&lt;/code&gt;
    &lt;code&gt;13855, 7877&lt;/code&gt;
    &lt;p&gt;That‚Äôs more than a 2X increase in the size of our data! 7,877 Echo Chess levels. I wonder how long it‚Äôd take for someone to play them all.&lt;/p&gt;
    &lt;p&gt;It‚Äôs worth mentioning too that we could similarly implement further data augmentations through simple 90-degree rotations, as well as internal reflections, of any level to generate new FENs out of every &lt;code&gt;quad&lt;/code&gt;, &lt;code&gt;rectangle&lt;/code&gt;, or even &lt;code&gt;full&lt;/code&gt; level. It‚Äôs not shown in the current version but I‚Äôll just leave you with this for inspiration:&lt;/p&gt;
    &lt;head rend="h3"&gt;Trick #2: Pre-predicting ‚Äòguaranteed unsolvables‚Äô&lt;/head&gt;
    &lt;p&gt;We can again use our knowledge of the Echo Chess game mechanics to directly filter out any ‚Äòguaranteed‚Äô unsolvable levels before even passing the baton to the ensemble model - for example any level with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;No valid first move&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No black pieces on board&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;More ‚Äòend-state-pieces‚Äô, or black pieces with no valid moves, than starting white pieces&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;‚ÄòUntethered‚Äô starting bishops ‚Äîi.e. starting with a bishop on a white square when all black pieces are on a black square, or vice versa&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;And so on‚Ä¶&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We already saw how to check for many of these in the Feature Engineering section, so I won‚Äôt bore you with the implementation again. The key thing here is that when we add all the little tweaks like these, as well as the data augmentation part, the oversampling, feature engineering and feature selection, and retrain our ensemble model on the augmented set, the improvements quickly add up even further.&lt;/p&gt;
    &lt;p&gt;We‚Äôll see next how this is all faring today in prod on the ‚≠êÔ∏èLIVE‚≠êÔ∏è app on echochess.com.&lt;/p&gt;
    &lt;head rend="h2"&gt;The FINAL Outcome on the LIVE APP&lt;/head&gt;
    &lt;p&gt;I can‚Äôt believe I‚Äôm saying this‚Ä¶ But we are FINALLY at the stage where we can put all this in prod. It‚Äôs here. It‚Äôs live. People are using it. With ML.&lt;/p&gt;
    &lt;p&gt;I know you‚Äôre dying to find out how this all ends, so I‚Äôll spare you the details. This is the gist of it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The live Echo Chess Endless mode in prod generates a solvable maze 99%+ of the time. You can test it yourself ü´≥üé§&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How? Every single time we need to serve a new level to the player, 50 (yes, FIFTY) different candidate levels get procedurally generated using parametrized random distributions.&lt;/p&gt;
    &lt;p&gt;On the server, we generate predictions for each of these 50 random gens using each of the tuned ML/DL models we‚Äôve selected. We then ensemble them through majority voting, plus keep track of each level‚Äôs average raw predictions from every model.&lt;/p&gt;
    &lt;p&gt;We then sort the 50 candidate gens by majority class first, and avg raw preds second. The ‚Äúmost-promisingly solvable‚Äù level from these is then sent to the client, rendered and served to be played. The whole thing happens in a split-second while the level transition sound effect is playing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work, what didn‚Äôt make the cut&lt;/head&gt;
    &lt;p&gt;I would be remiss if I didn‚Äôt mention some of the crazy ideas I wanted to try with Echo Chess before I realized this rabbit hole was getting a tad bit too deep.&lt;/p&gt;
    &lt;p&gt;If any of these resonates with you (or especially if you have better ideas than these), drop me a note and maybe I‚Äôll try it out in the upcoming version.&lt;/p&gt;
    &lt;head rend="h3"&gt;RE:Enhancing Predictions&lt;/head&gt;
    &lt;p&gt;Transformer-based NLP classification of compoundFENs&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The encoded&lt;/p&gt;&lt;code&gt;compoundFEN&lt;/code&gt;is just a string. NLP it. Fine-tune a pre-trained Transformer model from HuggingFace like UCL‚Äôs ChessGPT (a 2.8B language model pretrained on Chess).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;LLM k-shot in-context learning&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Instead of using a task-specific classifier, feed in all the labeled FENs from the training set as few-shot examples in a prompt to an LLM, and provide it with the unlabeled validation+test sets to make&lt;/p&gt;&lt;code&gt;solvability&lt;/code&gt;predictions on (can do the inference one FEN at a time or in bulk through the LLM provider‚Äôs API or a wrapper like LangChain).&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Given the size of the training sets, we‚Äôd certainly need to use a large-context-window LLM like Anthropic's Claude2 100k ‚Äì and possibly even some creative version of Retrieval Augmented Generation (RAG) and vector dbs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;JUMBO data augmentation&lt;/p&gt;
    &lt;p&gt;After EVERY player move (which leads to a new board state), save each interim &lt;code&gt;compoundFEN&lt;/code&gt; as its own unique ‚Äònew level‚Äô.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;When the actual level is solved (or is tagged as unsolvable), assign the same solvability for all these unique interim FENs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That‚Äôs because if a path exists from start to end, then definitionally all the interim states of that path can reach the end, if they follow that same path.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Synthetic generation of Unsolvables&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;For every level without a valid first move, generate N similar levels with the same&lt;/p&gt;&lt;code&gt;startingWhitePiece&lt;/code&gt;on that same starting square, but with ALL other NON-obstacle squares replaced with random combinations of {black piece, obstacle square, empty square}. They all won‚Äôt have valid first moves either.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Do the same for every level with more ‚Äúend-state‚Äù pieces than white pieces. Even shuffle the white pieces themselves in type and location, but keep their same count. All these levels will be unsolvable too.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Actual Image classification of levels&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Generate jpg screenshots of every level using its&lt;/p&gt;&lt;code&gt;compoundFEN&lt;/code&gt;, then train an image classifier on the labeled images&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Can fine-tune a pretrained SOTA classifier on this img dataset to get better performance than training from scratch. And given that chess boards probably don‚Äôt look as similar to what ImageNet deals with usually as something like ‚Äòcats‚Äô vs ‚Äòdogs‚Äô, we‚Äôd likely pick one of the architectures from the PyTorch Image Models (timm) that‚Äôs more suitable to datasets that are not very ‚ÄòImageNet-like‚Äô.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;fast.ai has done some great analysis on such models and datasets.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Letting AI actually play using Reinforcement Learning&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Instead of thinking of the problem as a classification problem, approaching it instead from an RL lens √† la AlphaZero. Tons of fun could be had exploring this direction.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From a ‚Äòbusiness‚Äô use case and user-centered design standpoint, we‚Äôre solid. We‚Äôve also pretty much saturated the returns from improvements in solvability prediction tbh. So let‚Äôs turn our eyes instead to overall Product and Game Design.&lt;/p&gt;
    &lt;head rend="h3"&gt;RE:Enhancing the Product experience&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Adjust difficulty curve as player solves more procedurally-generated levels (via varying distribution parametrization, classification thresholds, selected percentile from the sorted preds, etc.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Let players decide what gens look like using an LLM interface: ‚ÄòI want more knights, 4-square walls, pawns with promotion, but keep it easy‚Äô&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Player can unlock power-ups like:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Call reinforcements to add extra random white pieces on the board (start with 0 available, win extra call every X levels)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Upgrade your white piece to Queen at any time (start with 2 instant promotions available, win extra one every Y levels)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Trigger regional explosions by landing on a target highlighted square that shows up every X levels&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Portal squares to teleport the player‚Äôs current piece to another part of the board&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A community-driven level maker for Echo Chess. Anyone could design their own crazy level and upload it to the community to try out.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôm sure there are tons of other interesting ideas as well. Curious what others will find.&lt;/p&gt;
    &lt;head rend="h2"&gt;Epilogue&lt;/head&gt;
    &lt;p&gt;The game is live, it‚Äôs being played by 1000s, both Classic and Endless modes are a hit in their unique ways. ML really saved the day for Endless. And I‚Äôll always enjoy manually crafting hardcore puzzles for Classic.&lt;/p&gt;
    &lt;p&gt;So just go ahead and try it out already üôÇ&lt;/p&gt;
    &lt;p&gt;For all you chess experts looking for a challenge, try starting with levels 8+ in Classic. And for especially ambitious chess veterans, see how many you can solve starting 11+ (heads-up: it gets really difficult, really quickly!)&lt;/p&gt;
    &lt;p&gt;Hopefully this little rabbit hole of a story inspires other builders to create more with chess, games, ML, or all of the above. If you come across anything like that that piques your interest, please drop a link below. Would love to try it out.&lt;/p&gt;
    &lt;p&gt;And if you made it to the end and actually enjoyed any of this (or even if you didn‚Äôt), please let me know by sharing your thoughts or angry rants. Feedback is always welcome anytime.&lt;/p&gt;
    &lt;p&gt;Happy puzzling üïπÔ∏è&lt;/p&gt;
    &lt;p&gt;-Sami&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://web.archive.org/web/20230920164939/https://samiramly.com/chess"/><published>2026-01-18T10:24:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666650</id><title>Overlapping Markup</title><updated>2026-01-18T22:10:18.542033+00:00</updated><content>&lt;doc fingerprint="7eff01cb9371a2d0"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Overlapping markup&lt;/head&gt;&lt;p&gt;In markup languages and the digital humanities, overlap occurs when a document has two or more structures that interact in a non-hierarchical manner. A document with overlapping markup cannot be represented as a tree. This is also known as concurrent markup. Overlap happens, for instance, in poetry, where there may be a metrical structure of feet and lines; a linguistic structure of sentences and quotations; and a physical structure of volumes and pages and editorial annotations.[1][2]&lt;/p&gt;&lt;head rend="h2"&gt;History&lt;/head&gt;[edit]&lt;p&gt;The problem of non-hierarchical structures in documents has been recognised since 1988; resolving it against the dominant paradigm of text as a single hierarchy (an ordered hierarchy of content objects or OHCO) was initially thought to be merely a technical issue, but has, in fact, proven much more difficult.[4] In 2008, Jeni Tennison identified markup overlap as "the main remaining problem area for markup technologists".[5] Markup overlap continues to be a primary issue in the digital study of theological texts in 2019, and is a major reason for the field retaining specialised markup formats‚Äîthe Open Scripture Information Standard and the Theological Markup Language‚Äîrather than the inter-operable Text Encoding Initiative-based formats common to the rest of the digital humanities.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Properties and types&lt;/head&gt;[edit]&lt;p&gt;A distinction exists between schemes that allow non-contiguous overlap, and those that allow only contiguous overlap. Often, 'markup overlap' strictly means the latter. Contiguous overlap can always be represented as a linear document with milestones (typically co-indexed start- and end-markers), without the need for fragmenting a (logical) component into multiple physical ones. Non-contiguous overlap may require document fragmentation. Another distinction in overlapping markup schemes is whether elements can overlap with other elements of the same kind (self-overlap).[2]&lt;/p&gt;&lt;p&gt;A scheme may have a privileged hierarchy. Some XML-based schemes, for example, represent one hierarchy directly in the XML document tree, and represent other, overlapping, structures by another means; these are said to be non-privileged.&lt;/p&gt;&lt;p&gt;Schmidt (2012) identifies a tripartite classification of instances of overlap: 1. "Variation of content and structure", 2. "Overlay of multiple perspectives or markup sets", and 3. "Overlap of individual start and end tags within a single markup perspective"; additionally, some apparent instances of overlap are in fact schema definition problems, which can be resolved hierarchically. He contends that type 1 is best resolved by a system of multiple documents external to the markup, but types 2 and 3 require dealing with internally.&lt;/p&gt;&lt;head rend="h2"&gt;Approaches and implementations&lt;/head&gt;[edit]&lt;p&gt;DeRose (2004, Evaluation criteria) identifies several criteria for judging solutions to the overlap problem:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;readability and maintainability,&lt;/item&gt;&lt;item&gt;tool support and compatibility with XML,&lt;/item&gt;&lt;item&gt;possible validation schemes, and&lt;/item&gt;&lt;item&gt;ease of processing.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Tag soup is, strictly speaking, not overlapping markup‚Äîit is malformed HTML, which is a non-overlapping language, and may be ill-defined. Some web browsers attempted to represent overlapping start and end tags with non-hierarchical Document Object Models (DOM), but this was not standardised across all browsers and was incompatible with the innately hierarchical nature of the DOM.[7][8] HTML5 defines how processors should deal with such mis-nested markup in the HTML syntax and turn it into a single hierarchy.[9] With XHTML and SGML-based HTML, however, mis-nested markup is a strict error and makes processing by standards-compliant systems impossible.[10] The HTML standard defines a paragraph concept which can cause overlap with other elements and can be non-contiguous.[11]&lt;/p&gt;&lt;p&gt;SGML, which early versions of HTML were based on, has a feature called CONCUR that allows multiple independent hierarchies to co-exist without privileging any. DTD validation is only defined for each individual hierarchy with CONCUR. Validation across hierarchies is not defined by the standard. CONCUR cannot support self-overlap, and it interacts poorly with some of SGML's abbreviatory features. This feature has been poorly supported by tools and has seen very little actual use; using CONCUR to represent document overlap was not a recommended use case, according to a commentary by the standard's editor.[12][13]&lt;/p&gt;&lt;head rend="h3"&gt;Within hierarchical languages&lt;/head&gt;[edit]&lt;p&gt;There are several approaches to representing overlap in a non-overlapping language.[14] The Text Encoding Initiative, as an XML-based markup scheme, cannot directly represent overlapping markup. All four of the below approaches are suggested.[15] The Open Scripture Information Standard is another XML-based scheme, designed to mark up the Bible. It uses empty milestone elements to encode non-privileged components.[16]&lt;/p&gt;&lt;p&gt;To illustrate these approaches, marking up the sentences and lines of a fragment of Richard III by William Shakespeare will be used as a running example. Where there is a privileged hierarchy, the lines will be used.&lt;/p&gt;&lt;head rend="h4"&gt;Multiple documents&lt;/head&gt;[edit]&lt;p&gt;Multiple documents can each provide different internally consistent hierarchies. The advantage of this approach is that each document is simple and can be processed with existing tools, but requires maintenance of redundant content and it can be difficult to cross-reference between different views.[17] With multiple documents, the overlap can be analysed with data comparison and delta encoding techniques, and, in an XML context, specific XML tree differencing algorithms are available.[18][19]&lt;/p&gt;&lt;p&gt;Schmidt (2012, 3.5 Variation) recommends this approach for encoding multiple variants of a single text and to accept the duplication of the parts which do not vary, rather than attempting to create a structure that represents all of the variation present; further, he suggests that this alignment be performed automatically, and that misalignment is rare in practice.[20]&lt;/p&gt;&lt;p&gt;Example, with lines marked up:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;Who prays continually for Richmond's good.&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;So much for that.‚ÄîThe silent hours steal on,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;And flaky darkness breaks within the east.&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;p&gt;With sentences marked up:&lt;/p&gt;&lt;code&gt;  &amp;lt;sentence&amp;gt;I, by attorney, bless thee from thy mother,
  Who prays continually for Richmond's good.&amp;lt;/sentence&amp;gt;
  &amp;lt;sentence&amp;gt;So much for that.&amp;lt;/sentence&amp;gt;&amp;lt;sentence&amp;gt;‚ÄîThe silent hours steal on,
  And flaky darkness breaks within the east.&amp;lt;/sentence&amp;gt;
&lt;/code&gt;&lt;head rend="h4"&gt;Milestones&lt;/head&gt;[edit]&lt;p&gt;Milestones are empty elements that mark the beginning and end of a component, typically using the XML ID mechanism to indicate which "begin" element goes with which "end" element. Milestones can be used to embed a non-privileged structure within a hierarchical language, In their basic form they can only represent contiguous overlap. Generic XML can of course parse the milestone elements, but do not understand their special meaning and so cannot easily process or validate the non-privileged structure.[21][22]&lt;/p&gt;&lt;p&gt;Milestone have the advantage that the markup for overlapping elements is located right at the relevant boundaries, like other markup. This is an advantage for maintainability and readability.[23] CLIX (DeRose 2004) is an example of such an approach.&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;&amp;lt;sentence-start /&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;Who prays continually for Richmond's good.&amp;lt;sentence-end /&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence-start /&amp;gt;So much for that.&amp;lt;sentence-end /&amp;gt;&amp;lt;sentence-start /&amp;gt;‚ÄîThe silent hours steal on,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;And flaky darkness breaks within the east.&amp;lt;sentence-end /&amp;gt;&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;p&gt;Punctuation and spaces have been identified as a type of milestone-style 'crypto-overlap' or 'pseudo-markup', as the boundaries of words, clauses, sentences and the like do not necessarily align with the formal markup boundaries hierarchically.[24][25]&lt;/p&gt;&lt;p&gt;It is also possible to use more complex milestones to represent non-contiguous structures. For example, TAGML's "suspend" and "resume" semantic[26] can be expressed using milestones, for example by adding an attribute to indicate whether each milestone represents a start, suspend, resume, or end point. Re-ordering and even self-overlap can be achieved similarly, by annotating each milestone with a "next chunk" reference.&lt;/p&gt;&lt;head rend="h4"&gt;Joins&lt;/head&gt;[edit]&lt;p&gt;Joins are pointers within a privileged hierarchy to other components of the privileged hierarchy, which may be used to reconstruct a non-privileged component akin to following a linked list. A single non-privileged element is segmented into several partial elements within the privileged hierarchy; the partial elements themselves do not represent a single unit in the non-privileged hierarchy, which can be misleading and make processing difficult.[27][28] While this approach can support some discontiguous structures, it is not able to re-order elements.[29] A slightly different approach can, however, express re-ordering by expressing the join away from the content, at the cost of directness and maintainability.[30]&lt;/p&gt;&lt;p&gt;Join-based representations can introduce the possibility of cycles between elements; detecting and rejecting these adds complexity to implementations.[31]&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;&amp;lt;sentence id="a"&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence continues="a"&amp;gt;Who prays continually for Richmond's good.&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence id="b"&amp;gt;So much for that.&amp;lt;/sentence&amp;gt;&amp;lt;sentence id="c"&amp;gt;‚ÄîThe silent hours steal on,&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence continues="c"&amp;gt;And flaky darkness breaks within the east.&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;head rend="h4"&gt;Stand-off markup&lt;/head&gt;[edit]&lt;p&gt;Stand-off markup is similar to using joins, except that there may be no privileged hierarchy: each part of the document is given a label (or might be referred to by an offset), and the document structure is expressed by pointing to the content from markup that 'stands off' from the content (possibly in an entirely different file), and might contain no content itself. The TEI guidelines identify the unity of the elements as a primary advantage of stand-off markup over joins, in addition to the ability to produce and distribute annotations separately from the text, possibly even by different authors applying markup to a read-only document,[32] allowing collaborative approaches to markup by a divide and conquer strategy.[33]&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;span id="a"&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/span&amp;gt;
  &amp;lt;span id="b"&amp;gt;Who prays continually for Richmond's good.&amp;lt;/span&amp;gt;
  &amp;lt;span id="c"&amp;gt;So much for that.&amp;lt;/span&amp;gt;&amp;lt;span id="d"&amp;gt;‚ÄîThe silent hours steal on,&amp;lt;/span&amp;gt;
  &amp;lt;span id="e"&amp;gt;And flaky darkness breaks within the east.&amp;lt;/span&amp;gt;
  ...
  &amp;lt;line contents="a" /&amp;gt;
  &amp;lt;line contents="b" /&amp;gt;
  &amp;lt;line contents="c d" /&amp;gt;
  &amp;lt;line contents="e" /&amp;gt;
  &amp;lt;sentence contents="a b" /&amp;gt;
  &amp;lt;sentence contents="c" /&amp;gt;
  &amp;lt;sentence contents="d e" /&amp;gt;
&lt;/code&gt;&lt;p&gt;It has been claimed that separating markup and text can result in overall simplification and increased maintainability,[34] and by 2017, "[t]he current state of the art to [represent] (...) linguistically annotated data is to use a graph-based representation serialized as standoff XML as a pivot format",[35] i.e., that standoff was the most widely accepted approach to address the overlapping markup challenge.&lt;/p&gt;&lt;p&gt;Standoff formalisms have been the basis for an ISO standard for linguistic annotation,[36] they have been successfully applied for developing corpus management systems,[37] and (as of April 2020) they are actively being developed in the TEI.[38] One published example of a successful stand-off annotation scheme was developed as part of a bitext natural language documentation project focused on the preservation of low-resource or endangered languages.[39]&lt;/p&gt;&lt;head rend="h4"&gt;Challenges&lt;/head&gt;[edit]&lt;p&gt;Representing overlapping markup within hierarchical languages is challenging, for reasons of redundancy and/or complexity. In the 2000s to 2010s, standoff formalisms were generally accepted as the most promising approach here,[35] but a disadvantage of standoff is that validation is very challenging.[40] Standoff formalisms are not natively supported by database management systems, so that (by 2017) it was suggested to "use ... standoff XML as a pivot format (...) and relational data bases for querying."[35] In practical applications, this requires complicated architectures and/or labor-intense transformation between pivot format and internal representation. As a result, maintenance is problematic.[41] This has been a motivation to develop corpus management systems on the basis of graph data bases and for using established graph-based formalisms as pivot formats.&lt;/p&gt;&lt;head rend="h3"&gt;Special-purpose languages&lt;/head&gt;[edit]&lt;p&gt;For implementing the above-mentioned strategies, either existing markup languages (such as the TEI) can be extended or special-purpose languages can be designed.&lt;/p&gt;&lt;head rend="h4"&gt;Historical formalisms&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;LMNL is a non-hierarchical markup language first described in 2002 by Jeni Tennison and Wendell Piez, annotating ranges of a document with properties and allowing self-overlap. CLIX, which originally stood for 'Canonical LMNL In XML', provides a method for representing any LMNL document in a milestone-style XML document.[42] It also has another XML serialisation, xLMNL.[43]&lt;/item&gt;&lt;item&gt;MECS was developed by the University of Bergen's Wittgenstein Archive. However, it had several problems: it allowed some non-sensical documents of overlapping elements, it could not support self-overlap, and it did not have the capacity to define a DTD-like grammar.[44] The theory of General Ordered-Descendant Directed Acyclic Graphs (GODDAGs), while not strictly a markup language itself, is a general data model for non-hierarchical markup. Restricted GODDAGs were designed specifically to match the semantics of MECS; general GODDAGs may be non-contiguous and need a more powerful language.[45] TexMECS is a successor to MECS, which has a formal grammar and is designed to represent every GODDAG and nothing that is not a GODDAG.[46]&lt;/item&gt;&lt;item&gt;XCONCUR (previously MuLaX) is a melding-together of XML and SGML's CONCUR, and also contains a validation language, XCONCUR-CL, and a SAX-like API.[47][48][49]&lt;/item&gt;&lt;item&gt;Marinelli, Vitali and Zacchiroli provide algorithms to convert between restricted GODDAGs, ECLIX, LMNL, parallel documents in XML, contiguous stand-off markup and TexMECS.[50]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;None of these formalisms seem to be maintained anymore. Consensus community seems to be to employ standoff XML or graph-based formalisms.&lt;/p&gt;&lt;head rend="h4"&gt;Actively maintained standoff XML languages&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;GrAF-XML,[51] standoff-XML serialization of the Linguistic Annotation Framework (LAF),[36] used, e.g., for the American National Corpus[52]&lt;/item&gt;&lt;item&gt;PAULA-XML,[53] standoff-XML serialization of the data model underlying the corpus management system ANNIS and the converter suite SALT[54]&lt;/item&gt;&lt;item&gt;NAF (NLP Annotation Format / Newsreader Annotation Format),[55] standoff XML format originally developed in the NewsReader project (FP7, 2013-2015[56]), currently used by NLP tools such as FreeLing[57] (with support for English, Spanish, Portuguese, Italian, French, German, Russian, Catalan, Galician, Croatian, Slovene, etc.), and EusTagger[58] (with support for Basque, English, Spanish).&lt;/item&gt;&lt;item&gt;The Charles Harpur Critical Archive is encoded using 'multi-version documents' (MVD) to represent the variant versions of documents and as a means of indicating additions, deletions and revisions using a tactical combination of multiple documents and stand-off ranges within an underlying graph-based model. MVD is presented as an application file format, requiring specialised tools to view or edit.[59]&lt;/item&gt;&lt;item&gt;A standoff XML scheme was developed by the Odin, Intent, and XigtEdit collaboration, which is focused on a large dataset of Interlinear Glossed Text (IGT) for supporting natural language resource and documentation projects.[39]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Standoff approaches have two parts, commonly called the "content" and the "annotations." These can be expressed in unrelated representations. Simple standoff annotations per se, involve no more than a list of (location, type) pairs. Thus, in a few applications[example needed] standoff annotations are expressed in CSV, JSON(-LD, or other representations. (e.g., Web Annotation[60]) or graph formalisms grounded in string URIs (see below). However, representing and validating content in such representations is much more difficult and much less common.&lt;/p&gt;&lt;head rend="h3"&gt;Graph-based formalisms&lt;/head&gt;[edit]&lt;p&gt;Standoff markup employs a data model based on directed graphs,[61] thus complicating its representation when grounding markup information in a tree. Representing overlapping hierarchies in a graph eliminates this challenge. Standoff annotations can thus be more adequately represented as generalised directed multigraphs and use formalisms and technologies developed for this purpose, most notably those based on the Resource Description Framework (RDF).[62][63] EARMARK is an early RDF/OWL representation that encompasses General Ordered-Descendant Directed Acyclic Graphs (GODDAGs).[14] The theory of GODDAGs, while not strictly a markup language itself, is a general data model for non-hierarchical markup.&lt;/p&gt;&lt;p&gt;RDF is a semantic data model that is linearization-independent, and it provides different linearisations, including an XML format (RDF/XML) that can be modeled to mirror standoff XML, a linearisation that lets RDF be expressed in XML attributes (RDFa), a JSON format (JSON-LD), and binary formats designed to facilitate querying or processing (RDF-HDT,[64] RDF-Thrift[65]). RDF is semantically equivalent to graph-based data models underlying standoff markup; it does not require special-purpose technology for storing, parsing and querying. Multiple interlinked RDF files representing a document or a corpus constitute an example of Linguistic Linked Open Data.&lt;/p&gt;&lt;p&gt;An established technique to link arbitrary graphs with an annotated document is to use URI fragment identifiers to refer to parts of a text and/or document, see overview under Web annotation. The Web Annotation standard provides format-specific 'selectors' as an additional means, e.g., offset-, string-match- or XPath-based selectors.[66]&lt;/p&gt;&lt;p&gt;Native RDF vocabularies capable to represent linguistic annotations include:[67]&lt;/p&gt;&lt;p&gt;Related vocabularies include&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;POWLA, an OWL2/DL serialization of PAULA-XML[71]&lt;/item&gt;&lt;item&gt;RDF-NAF, an RDF serialization of the NLP Annotation Format[72]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In early 2020, W3C Community Group LD4LT has launched an initiative to harmonize these vocabularies and to develop a consolidated RDF vocabulary for linguistic annotations on the web.[73]&lt;/p&gt;&lt;head rend="h2"&gt;Notes&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Text Encoding Initiative.&lt;/item&gt;&lt;item&gt;^ a b DeRose 2004, The problem types.&lt;/item&gt;&lt;item&gt;^ Piez 2014.&lt;/item&gt;&lt;item&gt;^ Renear, Mylonas &amp;amp; Durand 1993.&lt;/item&gt;&lt;item&gt;^ Tennison 2008.&lt;/item&gt;&lt;item&gt;^ MoChridhe 2019.&lt;/item&gt;&lt;item&gt;^ Hickson 2002.&lt;/item&gt;&lt;item&gt;^ Sivonen 2003.&lt;/item&gt;&lt;item&gt;^ HTML, ¬ß 8.2.8 An introduction to error handling and strange cases in the parser.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.1. Non-SGML Notations.&lt;/item&gt;&lt;item&gt;^ HTML, ¬ß 3.2.5.4 Paragraphs.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.2. CONCUR.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, SGML CONCUR.&lt;/item&gt;&lt;item&gt;^ a b Di Iorio, Peroni &amp;amp; Vitali 2009.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, ¬ß 20 Non-hierarchical Structures.&lt;/item&gt;&lt;item&gt;^ Durusau 2006.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, ¬ß 20.1 Multiple Encodings of the Same Information.&lt;/item&gt;&lt;item&gt;^ Schmidt 2009.&lt;/item&gt;&lt;item&gt;^ La Fontaine 2016.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 4.1 Automating Variation.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, ¬ß 20.2 Boundary Marking with Empty Elements.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.4. Milestones.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, TEI-style milestones.&lt;/item&gt;&lt;item&gt;^ Birnbaum &amp;amp; Thorsen 2015.&lt;/item&gt;&lt;item&gt;^ Haentjens Dekker &amp;amp; Birnbaum 2017.&lt;/item&gt;&lt;item&gt;^ Dekker 2018.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, ¬ß 20.3 Fragmentation and Reconstitution of Virtual Elements.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Segmentation.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.5. Fragmentation.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Joins.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 3.4 Interlinking.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, ¬ß 20.4 Stand-off Markup.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 4.2 Markup Outside the Text.&lt;/item&gt;&lt;item&gt;^ Eggert &amp;amp; Schmidt 2019, Conclusion.&lt;/item&gt;&lt;item&gt;^ a b c Ide et al. 2017, p.99.&lt;/item&gt;&lt;item&gt;^ a b "ISO 24612:2012". ISO.&lt;/item&gt;&lt;item&gt;^ Chiarcos et al. 2008.&lt;/item&gt;&lt;item&gt;^ "Standoff: Annotation microstructure ¬∑ Issue #1745 ¬∑ TEIC/TEI". GitHub.&lt;/item&gt;&lt;item&gt;^ a b Xia, F., Lewis, W.D., Goodman, M.W. et al. Enriching a massively multilingual database of interlinear glossed text. Lang Resources &amp;amp; Evaluation 50, 321‚Äì349 (2016). https://doi.org/10.1007/s10579-015-9325-4&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.6. Standoff Markup.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Standoff markup.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, CLIX and LMNL.&lt;/item&gt;&lt;item&gt;^ Piez 2012.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.7. MECS.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000.&lt;/item&gt;&lt;item&gt;^ Huitfeldt &amp;amp; Sperberg-McQueen 2003.&lt;/item&gt;&lt;item&gt;^ Hilbert, Schonefeld &amp;amp; Witt 2005.&lt;/item&gt;&lt;item&gt;^ Witt et al. 2007.&lt;/item&gt;&lt;item&gt;^ Schonefeld 2008.&lt;/item&gt;&lt;item&gt;^ Marinelli, Vitali &amp;amp; Zacchiroli 2008.&lt;/item&gt;&lt;item&gt;^ "ISO GrAF". 7 March 2015.&lt;/item&gt;&lt;item&gt;^ "Home". anc.org.&lt;/item&gt;&lt;item&gt;^ "PAULA XML: Interchange Format for Linguistic Annotations". Archived from the original on 2020-08-17.&lt;/item&gt;&lt;item&gt;^ Zipser, Florian (2016-11-18). "Salt". corpus-tools.org. doi:10.5281/zenodo.17557. Retrieved 2022-09-11.&lt;/item&gt;&lt;item&gt;^ "NAF". GitHub. 30 June 2021.&lt;/item&gt;&lt;item&gt;^ "Building structured event indexes of large volumes of financial and economic data for decision making". Community Research and Development Information Service (CORDIS).&lt;/item&gt;&lt;item&gt;^ "Home - FreeLing Home Page". Archived from the original on 2012-04-29. Retrieved 2020-04-06.&lt;/item&gt;&lt;item&gt;^ "Text Analysis | HiTZ Zentroa".&lt;/item&gt;&lt;item&gt;^ Eggert &amp;amp; Schmidt 2019.&lt;/item&gt;&lt;item&gt;^ "Web Annotation Data Model". 23 February 2017.&lt;/item&gt;&lt;item&gt;^ Ide &amp;amp; Suderman 2007.&lt;/item&gt;&lt;item&gt;^ Cassidy 2010, cassidy.&lt;/item&gt;&lt;item&gt;^ Chiarcos 2012, POWLA.&lt;/item&gt;&lt;item&gt;^ "Home". rdfhdt.org.&lt;/item&gt;&lt;item&gt;^ "RDF Binary using Apache Thrift". afs.github.io.&lt;/item&gt;&lt;item&gt;^ "Selectors and States". 23 February 2017.&lt;/item&gt;&lt;item&gt;^ Cimiano, Philipp; Chiarcos, Christian; McCrae, John P.; Gracia, Jorge (2020). Linguistic Linked Data. Representation, Generation and Applications. Cham: Springer.&lt;/item&gt;&lt;item&gt;^ Verspoor, Karin; Livingston, Kevin (2012). "Towards Adaptation of Linguistic Annotations to Scholarly Annotation Formalisms on the Semantic Web". Proceedings of the Sixth Linguistic Annotation Workshop, Jeju, Republic of Korea: 75‚Äì84. Retrieved 6 April 2020.&lt;/item&gt;&lt;item&gt;^ "NLP Interchange Format (NIF) 2.0 - Overview and Documentation".&lt;/item&gt;&lt;item&gt;^ "LIF Overview".&lt;/item&gt;&lt;item&gt;^ "POWLA". January 2022.&lt;/item&gt;&lt;item&gt;^ "NLP Annotation Format | Background information on NAF".&lt;/item&gt;&lt;item&gt;^ "Towards a consolidated LOD vocabulary for linguistic annotations". GitHub. 7 September 2021.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Birnbaum, David J; Thorsen, Elise (2015). "Markup and meter: Using XML tools to teach a computer to think about versification". Proceedings of Balisage: The Markup Conference 2015. Balisage: The Markup Conference 2015. Vol. 15. Montr√©al. doi:10.4242/BalisageVol15.Birnbaum01. ISBN 978-1-935958-11-6.&lt;/item&gt;&lt;item&gt;Cassidy, Steve (2010). An RDF realisation of LAF in the DADA annotation server (PDF). Proceedings of ISA-5. Hong Kong. CiteSeerX 10.1.1.454.9146. Archived from the original (PDF) on 2016-03-12. Retrieved 2016-05-24.&lt;/item&gt;&lt;item&gt;Chiarcos, Christian (2012). "POWLA: Modeling linguistic corpora in OWL/DL" (PDF). The Semantic Web: Research and Applications. Proceedings of the 9th Extended Semantic Web Conference (ESWC 2012, Heraklion, Crete; LNCS 7295). Lecture Notes in Computer Science. Vol. 7295. pp. 225‚Äì239. doi:10.1007/978-3-642-30284-8_22. ISBN 978-3-642-30283-1. Retrieved 2016-05-24.[dead link]&lt;/item&gt;&lt;item&gt;Chiarcos, Christian; Dipper, Stefanie; G√∂tze, Michael; Leser, Ulf; L√ºdeling, Anke; Ritz, Julia; Stede, Manfred (2008). "A flexible framework for integrating annotations from different tools and tagsets". Traitement Automatique des Langues. 49 (2): 271‚Äì293. Archived from the original on 2020-07-18. Retrieved 2020-04-06.&lt;/item&gt;&lt;item&gt;Dekker, Ronald Haentjens; Bleeker, Elli; Buitendijk, Bram; Kulsdom, Astrid; Birnbaum, David J (2018). "TAGML: A markup language of many dimensions". Proceedings of Balisage: The Markup Conference 2018. Balisage: The Markup Conference 2018. Vol. 21. Rockville, MD. doi:10.4242/BalisageVol21.HaentjensDekker01. ISBN 978-1-935958-18-5.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;DeRose, Steven (2004). Markup Overlap: A Review and a Horse. Extreme Markup Languages 2004. Montr√©al. CiteSeerX 10.1.1.108.9959. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Di Iorio, Angelo; Peroni, Silvio; Vitali, Fabio (August 2009). "Towards markup support for full GODDAGs and beyond: the EARMARK approach". Proceedings of Balisage: The Markup Conference 2009. Balisage: The Markup Conference 2009. Vol. 3. Montr√©al. doi:10.4242/BalisageVol3.Peroni01. ISBN 978-0-9824344-2-0.&lt;/item&gt;&lt;item&gt;Eggert, Paul; Schmidt, Desmond A (2019). "The Charles Harpur Critical Archive: A History and Technical Report". International Journal of Digital Humanities. 1 (1). Retrieved 2019-03-25.&lt;/item&gt;&lt;item&gt;Haentjens Dekker, Ronald; Birnbaum, David J (2017). "It's more than just overlap: Text As Graph". Proceedings of Balisage: The Markup Conference 2017. Balisage: The Markup Conference 2017. Vol. 19. Montr√©al. doi:10.4242/BalisageVol19.Dekker01. ISBN 978-1-935958-15-4.&lt;/item&gt;&lt;item&gt;Durusau, Patrick (2006). OSIS Users Manual (OSIS Schema 2.1.1) (PDF). Archived (PDF) from the original on 2014-10-23. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Ian Hickson (2002-11-21). "Tag Soup: How UAs handle &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;/x&amp;gt; &amp;lt;/y&amp;gt;". Retrieved 2017-11-05.&lt;/item&gt;&lt;item&gt;Hilbert, Mirco; Schonefeld, Oliver; Witt, Andreas (2005). Making CONCUR work. Extreme Markup Languages 2005. Montr√©al. CiteSeerX 10.1.1.104.634. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Huitfeldt, Claus; Sperberg-McQueen, C M (2003). "TexMECS: An experimental markup meta-language for complex documents". Archived from the original on 2017-02-27. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Ide, Nancy; Chiarcos, Christian; Stede, Manfred; Cassidy, Steve (2017). "Designing Annotation Schemes: From Model to Representation". In Ide, Nancy; Pustejovsky, James (eds.). Handbook of Linguistic Annotation. Dordrecht: Springer. p. 99. doi:10.1007/978-94-024-0881-2_3. ISBN 978-94-024-0879-9.&lt;/item&gt;&lt;item&gt;La Fontaine, Robin (2016). "Representing Overlapping Hierarchy as Change in XML". Proceedings of Balisage: The Markup Conference 2016. Balisage: The Markup Conference 2016. Vol. 17. Montr√©al. doi:10.4242/BalisageVol17.LaFontaine01. ISBN 978-1-935958-13-0.&lt;/item&gt;&lt;item&gt;Marinelli, Paolo; Vitali, Fabio; Zacchiroli, Stefano (January 2008). "Towards the unification of formats for overlapping markup" (PDF). New Review of Hypermedia and Multimedia. 14 (1): 57‚Äì94. CiteSeerX 10.1.1.383.1636. doi:10.1080/13614560802316145. ISSN 1361-4568. S2CID 16909224. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;MoChridhe, Race J (2019-04-24). "Twenty Years of Theological Markup Languages: A Retro- and Prospective". Theological Librarianship. 12 (1). doi:10.31046/tl.v12i1.523. ISSN 1937-8904. S2CID 171582852. Archived from the original on 2019-07-15. Retrieved 2019-07-15.&lt;/item&gt;&lt;item&gt;Piez, Wendell (August 2012). "Luminescent: parsing LMNL by XSLT upconversion". Proceedings of Balisage: The Markup Conference 2012. Balisage: The Markup Conference 2012. Vol. 8. Montr√©al. doi:10.4242/BalisageVol8.Piez01. ISBN 978-1-935958-04-8. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Piez, Wendell (2014). Hierarchies within range space: From LMNL to OHCO. Balisage: The Markup Conference 2014. Montr√©al. doi:10.4242/BalisageVol13.Piez01.&lt;/item&gt;&lt;item&gt;Renear, Allen; Mylonas, Elli; Durand, David (1993-01-06). "Refining our Notion of What Text Really Is: The Problem of Overlapping Hierarchies". CiteSeerX 10.1.1.172.9017. hdl:2142/9407. Archived from the original on 2021-03-23. Retrieved 2016-10-02.&lt;/item&gt;&lt;item&gt;Schonefeld, Oliver (August 2008). A Simple API for XCONCUR: Processing concurrent markup using an event-centric API. Balisage: The Markup Conference 2008. Montr√©al. doi:10.4242/BalisageVol1.Schonefeld01. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Sperberg-McQueen, C M; Huitfeldt, Claus (2004). "GODDAG: A Data Structure for Overlapping Hierarchies". Digital Documents: Systems and Principles. Lecture Notes in Computer Science. Vol. 2023. pp. 139‚Äì160. doi:10.1007/978-3-540-39916-2_12. ISBN 978-3-540-21070-2. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Schmidt, Desmond (2009). "Merging Multi-Version Texts: A Generic Solution to the Overlap Problem". Merging Multi-Version Texts: a General Solution to the Overlap Problem. Balisage: The Markup Conference 2009. Proceedings of Balisage: The Markup Conference 2009. Vol. 3. Montr√©al. doi:10.4242/BalisageVol3.Schmidt01. ISBN 978-0-9824344-2-0.&lt;/item&gt;&lt;item&gt;Schmidt, Desmond (2012). "The role of markup in the digital humanities". Historical Social Research. 27 (3): 125‚Äì146. doi:10.12759/hsr.37.2012.3.125-146.&lt;/item&gt;&lt;item&gt;Henri Sivonen (2003-08-16). "Tag Soup: How Mac IE 5 and Safari handle &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;/x&amp;gt; &amp;lt;/y&amp;gt;". Retrieved 2017-11-05.&lt;/item&gt;&lt;item&gt;Ide, Nancy; Suderman, Keith (2007). GrAF: A graph-based format for linguistic annotations (PDF). Proceedings of the First Linguistic Annotation Workshop (LAW-2007, Prague, Czech Republic). pp. 1‚Äì8. CiteSeerX 10.1.1.146.4543.&lt;/item&gt;&lt;item&gt;Tennison, Jenni (2008-12-06). "Overlap, Containment and Dominance". Retrieved 2016-10-02.&lt;/item&gt;&lt;item&gt;Witt, Andreas; Schonefeld, Oliver; Rehm, Georg; Khoo, Jonathan; Evang, Kilian (2007). On the Lossless Transformation of Single-File, Multi-Layer Annotations into Multi-Rooted Trees. Extreme Markup Languages 2007. Montr√©al. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Text Encoding Initiative Consortium (16 September 2014). "Guidelines for Electronic Text Encoding and Interchange" (5 ed.). Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;WHATWG. "HTML Living Standard". Retrieved 2019-03-25.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Overlapping_markup"/><published>2026-01-18T10:37:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666661</id><title>Show HN: Xenia ‚Äì A monospaced font built with a custom Python engine</title><updated>2026-01-18T22:10:18.131875+00:00</updated><content>&lt;doc fingerprint="3e089154df2e396c"&gt;
  &lt;main&gt;
    &lt;p&gt;I made this because monofont shouldn't have to be fugly. I use it myself for coding and I always forget its mono.&lt;/p&gt;
    &lt;p&gt;I will release more weights if I get enough interest...&lt;/p&gt;
    &lt;p&gt;If you want to support an indie coder, buy me a coffee or just download and use for free... enjoy :)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;download &lt;code&gt;xenia_regular.ttf&lt;/code&gt;from this repository&lt;/item&gt;
      &lt;item&gt;open the file and click Install&lt;/item&gt;
      &lt;item&gt;in your editor (Sublime Text, VS Code, etc.) or Terminal, set your font face to &lt;code&gt;xenia&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;700+ glyphs: deep support for symbols and math.&lt;/item&gt;
      &lt;item&gt;non_ambiguous: distinctive &lt;code&gt;1&lt;/code&gt;,&lt;code&gt;l&lt;/code&gt;,&lt;code&gt;I&lt;/code&gt;,&lt;code&gt;0&lt;/code&gt;,&lt;code&gt;O&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;clean geometry: no "yucko" lowercase &lt;code&gt;a&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;python generated: built with a custom procedural engine&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Loretta1982/xenia"/><published>2026-01-18T10:39:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666963</id><title>Starting from scratch: Training a 30M Topological Transformer</title><updated>2026-01-18T22:10:17.845731+00:00</updated><content>&lt;doc fingerprint="141d03d4f13c1c9b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Training a 30M parameters Topological Transformer&lt;/head&gt;
    &lt;p&gt;Tauformer is a topological transformer (see paper) that replaces dot‚Äëproduct attention with a Laplacian-derived scalar (taumode) per token/head, then attends using distances in that scalar space. Below is a post-style overview of the idea and the first training signals from a 30M-parameter run.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tauformer in one idea&lt;/head&gt;
    &lt;p&gt;Tauformer‚Äôs goal is to inject domain structure directly into attention by using a Graph Laplacian built from a domain embedding space (a ‚Äúdomain memory‚Äù) as a persistent reference. Instead of ranking keys by \(Q\cdot K\), Tauformer ranks them by how similar their Laplacian-derived taumode scalars are, which is intended to bias attention toward domain-relevant relations rather than generic geometric similarity.&lt;/p&gt;
    &lt;p&gt;At the implementation level, Tauformer keeps the familiar Q/K/V projections, RoPE, causal masking, and stable softmax/value aggregation pipeline, but changes how attention logits are computed. Each head vector is compressed into a scalar \(\lambda\) using a bounded Rayleigh-quotient energy computed with a feature-space Laplacian \(L\), then logits are computed as a negative distance \(-|\lambda_q-\lambda_k|/\text{temperature}\).&lt;/p&gt;
    &lt;p&gt;Key building blocks (as implemented):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Taumode scalar: compute \(E_{\text{raw}}=(x^\top L x)/(x^\top x+\varepsilon)\), then bound it as \(E_{\text{raw}}/(E_{\text{raw}}+\tau)\) to produce \(\lambda\in[0,1)\).&lt;/item&gt;
      &lt;item&gt;Logits: \(\text{att}_{ij} = -\|\lambda^Q_i - \lambda^K_j\|/\text{temperature}\), then reuse causal mask \(‚Üí\) subtract row max \(‚Üí\) softmax \(‚Üí\) multiply by \(V\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why it can be cheaper&lt;/head&gt;
    &lt;p&gt;Because scoring no longer needs full key vectors, Tauformer‚Äôs KV-cache can store values plus a compact key-side scalar stream rather than both K and V tensors. Concretely, the cache payload is \((V,\lambda_k)\) (not \((K,V)\)), which yields an approximate ~50% per-layer cache reduction for typical head dimensions (small overhead for storing the extra scalar).&lt;/p&gt;
    &lt;p&gt;The design also anticipates using a sparse Laplacian from a precomputed domain manifold so computing \(\lambda\) can depend on Laplacian sparsity (nnz) rather than dense \(D^2\) multiplication. It exchanges the long preliminary adjustment of weights with a pre-training shorter phase in which a Laplacian is built using &lt;code&gt;arrowspace&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run setup (what was trained)&lt;/head&gt;
    &lt;p&gt;This run trains a 30M-class TauGPT. Training uses AdamW with base LR \(5\times10^{-4}\) and a warmup of 100 steps, then keeps the base LR constant unless the plateau logic scales it down. Data comes from a local JSONL file (&lt;code&gt;train.jsonl&lt;/code&gt;) streamed through an IterableDataset, with a routed split where every 20th batch is used for validation (\(‚âà5%\)).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Setting&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Class / size&lt;/cell&gt;
        &lt;cell&gt;TauGPT ~30M parameters (GPT2-inspired)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Layers (&lt;code&gt;n_layer&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Heads (&lt;code&gt;n_head&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Embedding size (&lt;code&gt;n_embd&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;384&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Sequence length (&lt;code&gt;seq_len&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;1024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Vocabulary size (&lt;code&gt;vocab_size&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;30522&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;AdamW&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;Base learning rate&lt;/cell&gt;
        &lt;cell&gt;5e-4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LR schedule&lt;/cell&gt;
        &lt;cell&gt;Warmup&lt;/cell&gt;
        &lt;cell&gt;100 steps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LR schedule&lt;/cell&gt;
        &lt;cell&gt;Post-warmup behavior&lt;/cell&gt;
        &lt;cell&gt;Constant LR (no decay unless manually/externally adjusted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;Source file&lt;/cell&gt;
        &lt;cell&gt;Local JSONL file &lt;code&gt;train.jsonl&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;Loading mode&lt;/cell&gt;
        &lt;cell&gt;Streamed via an IterableDataset-style pipeline (no shuffle in DataLoader)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Validation&lt;/cell&gt;
        &lt;cell&gt;Split rule&lt;/cell&gt;
        &lt;cell&gt;Routed split where every 20th batch is used for validation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Validation&lt;/cell&gt;
        &lt;cell&gt;Approx. validation fraction&lt;/cell&gt;
        &lt;cell&gt;About 5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Results at a glance&lt;/head&gt;
    &lt;p&gt;At step 100 the run reports train loss 4.6772 and val loss 4.9255 (PPL 107.47), and by step 2000 it reaches val loss 2.3585 (Perplexity 6.59). The best validation point in the log is step 4500 with &lt;code&gt;val_loss=1.9146&lt;/code&gt;, after which validation regresses to &lt;code&gt;2.3746&lt;/code&gt; by step 5000.
The final run summary records &lt;code&gt;step=5000&lt;/code&gt;, &lt;code&gt;best_val_loss=1.914555&lt;/code&gt;, &lt;code&gt;current_lr_scale=0.03125&lt;/code&gt;, and &lt;code&gt;total_tokens=655360000&lt;/code&gt;. That is a good result for \(~2\) hours of training on this smallest model (at an average of ~60K Tokens Per Second).&lt;/p&gt;
    &lt;p&gt;The early phase is strong: validation drops from 4.93 at step 100 to ~2.36 by step 2000, showing that the model and pipeline learn effectively at this scale. After that, validation becomes noisy (e.g., rising back to 2.92 at step 2100 and peaking near 2.95 at step 4200) before the late ‚Äúlucky break‚Äù to 1.91 at step 4500. Throughout, the run holds a fixed taumode value which means the attention geometry is not being updated as weights evolve as this will be take place in the next iterations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Baseline: Closing note&lt;/head&gt;
    &lt;p&gt;All the model‚Äôs files, data, training settings and logs will be published with a permissive license once the results are consolidated and tests will move to a larger scale model.&lt;/p&gt;
    &lt;p&gt;This baseline run kept taumode fixed throughout, while using a simple validation loop and plateau-triggered LR scaling, and it still converged quickly in the early-to-mid training window.&lt;/p&gt;
    &lt;p&gt;Because the later part of the run shows volatility and regression after the best checkpoint, the next experiments focus on ‚Äúadaptive‚Äù taumode strategies where taumode is recalibrated at intervals (including the ‚Äúgradient‚Äù strategy that detects energy drift and gates recalibration by performance of the gradient in the previous steps) plus more sophisticated validation behaviors already implemented in the training loop.&lt;/p&gt;
    &lt;p&gt;Considering the small model size and the short training horizon (5,000 steps total, lowest loss at 4600), these results support the architecture as promising, with broader evaluation and scaled tests planned next‚Äîespecially at 100M parameters.&lt;/p&gt;
    &lt;p&gt;A very interesting question has been raised by this test: what is the correlation between cross-entropy and taumode? Model convergence brings the loss down but at the same time recalibrating the taumode used on the learned weights brings down the taumode.&lt;/p&gt;
    &lt;head rend="h2"&gt;What may be correlated (and why)&lt;/head&gt;
    &lt;p&gt;Cross-entropy and taumode are likely correlated because Tauformer‚Äôs attention kernel is built from Laplacian-derived scalar energies (Œª/taumode) rather than dot-product similarity, so changes in the Œª distribution change attention behavior and therefore training dynamics. In the current training loop, the observed ‚Äútaumode convergence‚Äù is also mechanically explained by how taumode is recalibrated: on (re)start, the code can compute a median energy from block0 key (K) vectors produced by the current weights and then set that median as the global taumode.&lt;/p&gt;
    &lt;head rend="h2"&gt;What ‚Äúconverging taumode‚Äù means here&lt;/head&gt;
    &lt;p&gt;The calibration is effectively computing a Rayleigh-style energy statistic on K vectors under a Laplacian (numerator/denominator), and then taking a median over the batch to set a single scalar taumode. In the reference implementation, taumode/Œª is based on a bounded Rayleigh quotient: \(E_{\text{raw}}(x) = \frac{x^\top L x}{x^\top x + \varepsilon}\) and then \(\lambda_\tau(x)=\frac{E_{\text{raw}}}{E_{\text{raw}}+\tau}\), which maps energies into \([0,1)\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Why taumode can drift downward as loss improves&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Healthy interpretation: as training progresses, the model may learn K representations that are ‚Äúsmoother‚Äù (lower-energy, so closer) with respect to the domain/manifold Laplacian, pushing the median energy down while also improving next-token prediction (lower cross-entropy).&lt;/item&gt;
      &lt;item&gt;Unhealthy interpretation (collapse risk): median energy can also drop if K vectors collapse toward low-variance or less-discriminative configurations, which can reduce contrast in Œª-distance logits even if loss continues improving short-term.&lt;/item&gt;
      &lt;item&gt;Key confound: if taumode is recalibrated on resume, then taumode changes are not purely a passive ‚Äúmeasurement of convergence‚Äù; they can act like a mid-training hyperparameter change, so correlation with loss does not automatically imply causality in the direction ‚Äúlower taumode \(‚áí\) lower loss‚Äù.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A strong explanation for ‚Äúconverging taumode‚Äù (as a property of learned representations, not an artifact) is: as weights converge, the distribution of per-token energies \(x^\top L x\) stabilizes, so repeated measurements (median, p50) across batches and checkpoints become consistent and typically shift toward lower-energy manifold-aligned directions. To validate that, it helps to separate (1) the fixed constant used by attention from (2) a purely diagnostic ‚Äúcurrent batch median energy‚Äù, and track not just the median but also the spread (p05/p95), because collapse would show shrinking spread even when the median looks lower.&lt;/p&gt;
    &lt;p&gt;‚Äúlower loss \(‚áí\) lower taumode‚Äù is a plausible causal direction in Tauformer, because the cross-entropy gradient flows through the Tauformer attention path that depends on Laplacian-energy-derived scalars computed from Q/K (and in your calibration code, specifically from block0 K vectors). As the model improves next-token prediction, it can simultaneously learn representations whose Laplacian Rayleigh energy is lower, so any ‚Äúrecalibrate taumode from learned weights‚Äù procedure will tend to output a smaller median. If this it true, where is the optimal stopping state?&lt;/p&gt;
    &lt;head rend="h2"&gt;Further readings&lt;/head&gt;
    &lt;p&gt;Some shift is happening in understanding information thanks to large scale learning machines!&lt;/p&gt;
    &lt;p&gt;In this recent paper, MDL refers to the ‚Äúminimum description length principle‚Äù, which says the best explanation/model is the one that minimizes the total code length needed to describe (1) the model and (2) the data given the model. Epiplexity \(ST(X)\) is defined as the program length of the compute-feasible model \(P\) that minimizes time-bounded MDL, while time-bounded entropy HT(X) is the expected code length of the data under that model. Operationally, the paper proposes practical estimators based on neural-network training dynamics (e.g., prequential ‚Äúarea under the loss curve above final loss‚Äù) to approximate how much structure a bounded learner actually absorbs from data&lt;/p&gt;
    &lt;p&gt;Qualitatively, &lt;code&gt;arrowspace&lt;/code&gt;, &lt;code&gt;taumode&lt;/code&gt; and &lt;code&gt;tau-attention&lt;/code&gt; are exactly the kind of deterministic computations that can increase usable/learnable structure for bounded learners, which is one of the central motivations for epiplexity.
Through the epiplexity lens, the operations carried on by &lt;code&gt;arrowspace&lt;/code&gt; and Tauformer (converts each head vector into a bounded scalar ŒªœÑ using a Rayleigh-quotient-style energy followed by a bounding map) is a deterministic compression that can re-factor information into a form that is cheaper for downstream computation to exploit, potentially increasing the amount of structure a bounded observer can learn from the same underlying signal.&lt;/p&gt;
    &lt;p&gt;I am happy I have somehow anticipated this switch in point of view in &lt;code&gt;arrowspace&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknoledgements&lt;/head&gt;
    &lt;p&gt;I gratefully acknowledge Enverge Labs for kindly providing the computation time used to run these experiments on their H100 GPU cluster powered by clean and cheap energy, this aligns perfectly with the topological tranformer objective to provide cheaper computation for Transformers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer"/><published>2026-01-18T11:39:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46667101</id><title>Keystone (YC S25) Is Hiring</title><updated>2026-01-18T22:10:17.550548+00:00</updated><content>&lt;doc fingerprint="8d21d67548473058"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Keystone builds infrastructure for autonomous coding agents. We give agents sandboxed environments that mirror production, event-based triggers (Sentry, Linear, GitHub), and verification workflows so they can ship code end-to-end‚Äî not just write it. We're hiring a founding engineer to work directly with me (solo founder) on core product. Stack is TypeScript, React (Next.js), Python, Postgres, Redis, AWS.&lt;/p&gt;
      &lt;p&gt;In-person in SoMa. $150K-$350K + 0.5-3% equity.&lt;/p&gt;
      &lt;p&gt;https://www.workatastartup.com/jobs/88801&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46667101"/><published>2026-01-18T12:00:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46667572</id><title>Software engineers can no longer neglect their soft skills</title><updated>2026-01-18T22:10:17.424359+00:00</updated><content>&lt;doc fingerprint="bcc5561f2e328e5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Software engineers can no longer neglect their soft skills&lt;/head&gt;
    &lt;p&gt;January 6, 2026&lt;/p&gt;
    &lt;p&gt;Starting in 2026, communication has become the most important skill for software engineers.&lt;/p&gt;
    &lt;p&gt;It's not writing code, system designs, or having estoric knowledge of a programming language (i.e., Rust).&lt;/p&gt;
    &lt;p&gt;AI coding agents have gotten very, very good. A year ago, I'd reach out to Cursor hesitantly for MVPs or quick fixes. Today, I use Claude Code for almost all non-trivial programming tasks and have spent $500+ on it just last December.&lt;/p&gt;
    &lt;p&gt;AI talks online revolve much around the hard skils. Initially it was prompt tricks to accomplish X, then the best MCPs for Y, and so on. But with Opus 4.5, using vanilla Claude Code gets you 80% there. Even in the age of AI, the 80/20 rule still applies. So, what should engineers focus on?&lt;/p&gt;
    &lt;p&gt;One thing with coding agents is that the better the spec, the more in line they will be with the technical and business requirements. But getting a good spec is hard.&lt;/p&gt;
    &lt;p&gt;In real life, tickets rarely contain all the requirements. To do so, you might need to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ask questions that reveal assumptions people didn't know they had&lt;/item&gt;
      &lt;item&gt;Facilite trade-off discussions&lt;/item&gt;
      &lt;item&gt;Push back on scope without burning bridges&lt;/item&gt;
      &lt;item&gt;Make calls on things nobody thought to specify&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Doing these things well used to be optional for individual contributors. Certain teams would enable engineers to thrive being an average communicator but excellent coder. Now, the non-coding parts are becoming a non-negotiable.&lt;/p&gt;
    &lt;p&gt;Software engineers are problem solvers. We believe that every problem has a solution, a "best practice". But working with people is messy.&lt;/p&gt;
    &lt;p&gt;&lt;del&gt;Un&lt;/del&gt;fortunately, we won't be able to AI our way into better communication skills. Good communication requires empathy, and we can all use a little more of that in today's landscape.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qu8n.com/posts/most-important-software-engineering-skill-2026"/><published>2026-01-18T13:14:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46667675</id><title>What is Plan 9?</title><updated>2026-01-18T22:10:16.714364+00:00</updated><content>&lt;doc fingerprint="9718595c0aab0be5"&gt;
  &lt;main&gt;
    &lt;p&gt;Plan 9 is a research operating system from the same group who created UNIX at Bell Labs Computing Sciences Research Center (CSRC). It emerged in the late 1980s, and its early development coincided with continuing development of the later versions of Research UNIX. Plan 9 can be seen as an attempt to push some of the same ideas that informed UNIX even further into the era of networking and graphics. Rob Pike has described Plan 9 as "an argument" for simplicity and clarity, while others have described it as "UNIX, only moreso."&lt;/p&gt;
    &lt;p&gt;From The Use of Name Spaces in Plan 9:&lt;/p&gt;
    &lt;p&gt;Plan 9 argues that given a few carefully implemented abstractions it is possible to produce a small operating system that provides support for the largest systems on a variety of architectures and networks.&lt;/p&gt;
    &lt;p&gt;From the intro(1) man page:&lt;/p&gt;
    &lt;p&gt;Plan 9 is a distributed computing environment assembled from separate machines acting as terminals, CPU servers, and file servers. A user works at a terminal, running a window system on a raster display. Some windows are connected to CPU servers; the intent is that heavy computing should be done in those windows but it is also possible to compute on the terminal. A separate file server provides file storage for terminals and CPU servers alike.&lt;/p&gt;
    &lt;p&gt;The two most important ideas in Plan 9 are:&lt;/p&gt;
    &lt;p&gt;Most everything else in the system falls out of these two basic ideas.&lt;/p&gt;
    &lt;p&gt;Read: intro(1); Plan 9 from Bell Labs; Designing Plan 9, originally delivered at the UKUUG Conference in London, July 1990; and FQA 7 - System Management; for a more detailed overview of Plan 9's design.&lt;/p&gt;
    &lt;p&gt;Today, Plan 9 continues in its original form, as well as in several derivatives and forks.&lt;/p&gt;
    &lt;p&gt;The United States of Plan 9&lt;/p&gt;
    &lt;p&gt;We hold these truths to be self-evident, that Plan 9 from Bell Labs was dead as in gratis, that a working VGA driver was rejected because VESA, and that people just want to run Plan 9 on their computers.&lt;/p&gt;
    &lt;p&gt;Plan 9 from Bell Labs ‚Äî The original Plan 9. Effectively dead, all the developers have been run out of the Labs and/or are on display at Google.&lt;/p&gt;
    &lt;p&gt;Plan 9 from User Space ‚Äî Plan 9 userspace ported/imitated for UNIX (specifically OS X).&lt;/p&gt;
    &lt;p&gt;9legacy ‚Äî David du Colombier's collection of patches to Bell Labs Plan 9. (It is not a fork, but is treated as the continuation of Labs Plan 9 by people mad about 9front.)&lt;/p&gt;
    &lt;p&gt;9atom ‚Äî Erik Quanstrom's fork of Plan 9, previously maintained to Erik's needs and occasionally pilfered by 9front. Destiny unclear.&lt;/p&gt;
    &lt;p&gt;9front ‚Äî (that's us) (we rule (we're the tunnel snakes))&lt;/p&gt;
    &lt;p&gt;NIX ‚Äî High performance cloud computing is NIX ‚Äî imploded in the past, but survived and adapted to 9front.&lt;/p&gt;
    &lt;p&gt;NxM ‚Äî A kernel for manycore systems ‚Äî never spotted in the wild.&lt;/p&gt;
    &lt;p&gt;Clive ‚Äî A new operating system from Francisco J. Ballesteros, declared inert in 2022.&lt;/p&gt;
    &lt;p&gt;Akaros ‚Äî Akaros is an open source, GPL-licensed operating system for manycore architectures. Has no bearing on anything but has attracted grant money.&lt;/p&gt;
    &lt;p&gt;Harvey ‚Äî Harvey is an effort to get the Plan 9 code working with gcc and clang. Flatlined in 2023, succeeded by r9.&lt;/p&gt;
    &lt;p&gt;Inferno ‚Äî Inferno is a distributed operating system started at Bell Labs, but is now developed and maintained by Vita Nuova Holdings as free software. Just kidding! It is not developed or maintained.&lt;/p&gt;
    &lt;p&gt;ANTS ‚Äî Advanced Namespace Tools for Plan 9. ANTS is a collection of modifications and additional software which adds new namespace manipulation capabilities to Plan 9.&lt;/p&gt;
    &lt;p&gt;Jehanne ‚Äî Jehanne:Harvey::William King Harvey:J. Edgar Hoover&lt;/p&gt;
    &lt;p&gt;r9 ‚Äî A project based on the idea that Plan 9 needs a ground-up rewrite in a language that can't be compiled on Plan 9. Everyone expects big things.&lt;/p&gt;
    &lt;p&gt;Plan 9 Foundation ‚Äî Now offering downloads of historical Plan 9 releases.&lt;/p&gt;
    &lt;p&gt;In the words of the Bell Labs Plan 9 wiki:&lt;/p&gt;
    &lt;p&gt;Plan 9 is not Unix. If you think of it as Unix, you may become frustrated when something doesn't exist or when it works differently than you expected. If you think of it as Plan 9, however, you'll find that most of it works very smoothly, and that there are some truly neat ideas that make things much cleaner than you have seen before.&lt;/p&gt;
    &lt;p&gt;Confusion is compounded by the fact that many UNIX commands exist on Plan 9 and behave in similar ways. In fact, some of Plan 9's userland (such as the upas mail interface, the sam text editor, and the rc shell) are carried over directly from Research UNIX 10th Edition. Further investigation reveals that many ideas found in Plan 9 were explored in more primitive form in the later editions of Research UNIX.&lt;/p&gt;
    &lt;p&gt;However, Plan 9 is a completely new operating system that makes no attempt to conform to past prejudices. The point of the exercise (circa the late 1980s) was to avoid past problems and explore new territory. Plan 9 is not UNIX for a reason.&lt;/p&gt;
    &lt;p&gt;Read: UNIX to Plan 9 translation (wiki.9front.org), UNIX to Plan 9 command translation (9p.io), UNIX Style, or cat -v Considered Harmful&lt;/p&gt;
    &lt;p&gt;Plan 9 from User Space (also known as plan9port or p9p) is a port of many Plan 9 from Bell Labs libraries and applications to UNIX-like operating systems. Currently it has been tested on a variety of operating systems including: Linux, Mac OS X, FreeBSD, NetBSD, OpenBSD, Solaris and SunOS.&lt;/p&gt;
    &lt;p&gt;Plan9port consists of a combination of mostly unaltered Plan 9 userland utilities packaged alongside various attempts to imitate Plan 9's kernel intefaces using miscellaneous available UNIX programs and commands. Some of the imitations are more successful than others. In all, plan9port does not accurately represent the experience of using actual Plan 9, but does provide enough functionality to make some users content with running acme on their Macbooks.&lt;/p&gt;
    &lt;p&gt;It is now being slowly ported to the Go programming language.&lt;/p&gt;
    &lt;p&gt;Inferno is a distributed operating system also created at Bell Labs, but which is now developed and maintained by Vita Nuova Holdings as free software. It employs many ideas from Plan 9 (Inferno does share some compatible interfaces with Plan 9, including the 9P/Styx protocol), but is a completely different OS. Many users new to Plan 9 find out about Inferno and immediately decide to abandon Plan 9 and its opaque user interface for this obviously "more advanced" sibling, but usually abandon that, too, upon first contact with Inferno's Tk GUI. Notable exceptions duly noted.&lt;/p&gt;
    &lt;p&gt;See: http://www.xs4all.nl/~mechiel/inferno/&lt;/p&gt;
    &lt;p&gt;Path: utzoo!utgpu!water!watmath!clyde!bellcore!faline!thumper!ulysses!smb&lt;/p&gt;
    &lt;p&gt;From: s...@ulysses.homer.nj.att.com (Steven Bellovin)&lt;/p&gt;
    &lt;p&gt;Newsgroups: comp.unix.wizards&lt;/p&gt;
    &lt;p&gt;Subject: Re: Plan 9? (+ others)&lt;/p&gt;
    &lt;p&gt;Message-ID: &amp;lt;10533@ulysses.homer.nj.att.com&amp;gt;&lt;/p&gt;
    &lt;p&gt;Date: 23 Aug 88 16:19:40 GMT&lt;/p&gt;
    &lt;p&gt;References: &amp;lt;846@yunexus.UUCP&amp;gt; &amp;lt;282@umbio.MIAMI.EDU&amp;gt; &amp;lt;848@yunexus.UUCP&amp;gt;&lt;/p&gt;
    &lt;p&gt;Organization: AT&amp;amp;T Bell Laboratories, Murray Hill&lt;/p&gt;
    &lt;p&gt;Lines: 33&lt;/p&gt;
    &lt;p&gt;``Plan 9'' is not a product, and is not intended to be. It is research --&lt;/p&gt;
    &lt;p&gt;an experimental investigation into a different way of computing. The&lt;/p&gt;
    &lt;p&gt;developers started from several basic assumptions: that CPUs are very&lt;/p&gt;
    &lt;p&gt;cheap but that we don't really know how to combine them effectively; that&lt;/p&gt;
    &lt;p&gt;*good* networking is very important; that an intelligent user interface&lt;/p&gt;
    &lt;p&gt;(complete with dot-mapped display and mouse) is a Right Decision; that&lt;/p&gt;
    &lt;p&gt;existing systems with networks, mice, etc., are not the correct way to&lt;/p&gt;
    &lt;p&gt;do things, and in particular that today's workstations are not the way to&lt;/p&gt;
    &lt;p&gt;go. (No, I won't bother to explain all their reasoning; that's a long&lt;/p&gt;
    &lt;p&gt;and separate article.) Finally, the UNIX system per se is dead as a&lt;/p&gt;
    &lt;p&gt;vehicle for serious research into operating system structure; it has grown&lt;/p&gt;
    &lt;p&gt;too large, and is too constrained by 15+ years of history.&lt;/p&gt;
    &lt;p&gt;Now -- given those assumptions, they decided to throw away what we have&lt;/p&gt;
    &lt;p&gt;today and design a new system. Compatibility isn't an issue -- they are&lt;/p&gt;
    &lt;p&gt;not in the product-building business. (Nor are they in the ``let's make&lt;/p&gt;
    &lt;p&gt;another clever hack'' business.) Of course aspects of Plan 9 resemble&lt;/p&gt;
    &lt;p&gt;the UNIX system quite strongly -- is it any surprise that Pike, Thompson,&lt;/p&gt;
    &lt;p&gt;et al., think that that's a decent model to follow? But Plan 9 isn't,&lt;/p&gt;
    &lt;p&gt;and is not meant to be, a re-implementation of the UNIX system. If you&lt;/p&gt;
    &lt;p&gt;want, call it a UNIX-like system.&lt;/p&gt;
    &lt;p&gt;Will Plan 9 ever be released? I have no idea. Will it remain buried?&lt;/p&gt;
    &lt;p&gt;I hope not. Large companies do not sponsor large research organizations&lt;/p&gt;
    &lt;p&gt;just for the prestige; they hope for an (eventual) concrete return in the&lt;/p&gt;
    &lt;p&gt;form of concepts that can be made into (or incorporated into) products.&lt;/p&gt;
    &lt;p&gt;--Steve Bellovin&lt;/p&gt;
    &lt;p&gt;Disclaimer: this article is not, of course, an official statement from AT&amp;amp;T.&lt;/p&gt;
    &lt;p&gt;Nor is it an official statement of the reasoning behind Plan 9. I do think&lt;/p&gt;
    &lt;p&gt;it's accurate, though, and I'm sure I'll be told if I'm wrong...&lt;/p&gt;
    &lt;p&gt;From: kalona.ayeliski@fastmail.us&lt;/p&gt;
    &lt;p&gt;To: 9fans &amp;lt;9fans@9fans.net&amp;gt;&lt;/p&gt;
    &lt;p&gt;Subject: Re: [9fans] Where can I find active Plan 9 communities for support&lt;/p&gt;
    &lt;p&gt;and collaboration?&lt;/p&gt;
    &lt;p&gt;Date: Sun, 4 Aug 2024 14:27:58 -0400&lt;/p&gt;
    &lt;p&gt;Reply-To: 9fans &amp;lt;9fans@9fans.net&amp;gt;&lt;/p&gt;
    &lt;p&gt;From a newcomer's perspective, it feels like dealing with a cult run&lt;/p&gt;
    &lt;p&gt;by scam artists. It seems someone wants to profit from me by selling&lt;/p&gt;
    &lt;p&gt;books on Amazon, like a multi-level marketing group. People say&lt;/p&gt;
    &lt;p&gt;others here are on a spectrum, but it feels more like psychosis, with&lt;/p&gt;
    &lt;p&gt;a loss of contact with reality. I really feel like I'm being&lt;/p&gt;
    &lt;p&gt;gaslighted. I might seem like a troll, but you don't understand how&lt;/p&gt;
    &lt;p&gt;you appear to others.&lt;/p&gt;
    &lt;p&gt;I am looking for a Plan 9 group that doesn't behave this way. If&lt;/p&gt;
    &lt;p&gt;anyone is interested, let's form a group that isn't cult-like, that&lt;/p&gt;
    &lt;p&gt;just wants to help newcomers and not prey on them.&lt;/p&gt;
    &lt;p&gt;Let's be perfectly honest. Many features that today's "computer experts" consider to be essential to computing (JavaScript, CSS, HTML5, etc.) either did not exist when Plan 9 was abandoned by its creators, or were purposely left out of the operating system. You might find this to be an unacceptable obstacle to adopting Plan 9 into your daily workflow. If you cannot imagine a use for a computer that does not involve a web browser, Plan 9 may not be for you.&lt;/p&gt;
    &lt;p&gt;See: http://harmful.cat-v.org/software/&lt;/p&gt;
    &lt;p&gt;On the other hand, the roaring 2020s have seen Plan 9 sprout a substantial presence on social media, so if you're here for that, YMMV.&lt;/p&gt;
    &lt;p&gt;You may ask yourself, well, how did I get here? In the words of Plan 9 contributor Russ Cox:&lt;/p&gt;
    &lt;p&gt;Why Plan 9 indeed. Isn't Plan 9 just another Unix clone? Who cares?&lt;/p&gt;
    &lt;p&gt;Plan 9 presents a consistent and easy to use interface. Once you've settled in, there are very few surprises here. After I switched to Linux from Windows 3.1, I noticed all manner of inconsistent behavior in Windows 3.1 that Linux did not have. Switching to Plan 9 from Linux highlighted just as much in Linux.&lt;/p&gt;
    &lt;p&gt;One reason Plan 9 can do this is that the Plan 9 group has had the luxury of having an entire system, so problems can be fixed and features added where they belong, rather than where they can be. For example, there is no tty driver in the kernel. The window system handles the nuances of terminal input.&lt;/p&gt;
    &lt;p&gt;If Plan 9 was just a really clean Unix clone, it might be worth using, or it might not. The neat things start happening with user-level file servers and per-process namespace. In Unix, /dev/tty refers to the current window's output device, and means different things to different processes. This is a special hack enabled by the kernel for a single file. Plan 9 provides full-blown per-process namespaces. In Plan 9 /dev/cons also refers to the current window's output device, and means different things to different processes, but the window system (or telnet daemon, or ssh daemon, or whatever) arranges this, and does the same for /dev/mouse, /dev/text (the contents of the current window), etc.&lt;/p&gt;
    &lt;p&gt;Since pieces of file tree can be provided by user-level servers, the kernel need not know about things like DOS's FAT file system or GNU/Linux's EXT2 file system or NFS, etc. Instead, user-level servers provide this functionality when desired. In Plan 9, even FTP is provided as a file server: you run ftpfs and the files on the server appear in /n/ftp.&lt;/p&gt;
    &lt;p&gt;We need not stop at physical file systems, though. Other file servers synthesize files that represent other resources. For example, upas/fs presents your mail box as a file tree at /mail/fs/mbox. This models the recursive structure of MIME messages especially well.&lt;/p&gt;
    &lt;p&gt;As another example, cdfs presents an audio or data CD as a file system, one file per track. If it's a writable CD, copying new files into the /mnt/cd/wa or /mnt/cd/wd directories does create new audio or data tracks. Want to fixate the CD as audio or data? Remove one of the directories.&lt;/p&gt;
    &lt;p&gt;Plan 9 fits well with a networked environment, files and directory trees can be imported from other machines, and all resources are files or directory trees, it's easy to share resources. Want to use a different machine's sound card? Import its /dev/audio. Want to debug processes that run on another machine? Import its /proc. Want to use a network interface on another machine? Import its /net. And so on.&lt;/p&gt;
    &lt;p&gt;Russ Cox&lt;/p&gt;
    &lt;p&gt;Descriptive testimony by long time Plan 9 users Charles Forsyth, Anthony Sorace and Geoff Collyer:&lt;/p&gt;
    &lt;p&gt;https://9p.io/wiki/plan9/what_do_people_like_about_plan_9/index.html&lt;/p&gt;
    &lt;p&gt;Computing.&lt;/p&gt;
    &lt;p&gt;Read: How I Switched To Plan 9&lt;/p&gt;
    &lt;p&gt;See: FQA 8 - Using 9front&lt;/p&gt;
    &lt;p&gt;John Floren provides a humorous(?) overview of a typical new user's reactions to Plan 9:&lt;/p&gt;
    &lt;p&gt;Hi! I'm new to Plan 9. I'm really excited to work with this new Linux system.&lt;/p&gt;
    &lt;p&gt;I hit some questions.&lt;/p&gt;
    &lt;p&gt;1 How do I run X11?&lt;/p&gt;
    &lt;p&gt;2 Where is Emacs?&lt;/p&gt;
    &lt;p&gt;3 The code is weird. It doesn't look like GNU C at all. Did the people who wrote Plan 9 know about C?&lt;/p&gt;
    &lt;p&gt;4 I tried to run mozilla but it did not work. How come?&lt;/p&gt;
    &lt;p&gt;Is this guy you?&lt;/p&gt;
    &lt;p&gt;Related: http://9front.org/buds.html&lt;/p&gt;
    &lt;p&gt;A summary of common features you may have been expecting that are missing from Plan 9:&lt;/p&gt;
    &lt;p&gt;http://c2.com/cgi/wiki?WhatIsNotInPlanNine&lt;/p&gt;
    &lt;p&gt;All of the people who worked on Plan 9 have moved on from Bell Labs and/or no longer work on Plan 9. Various reasons have been articulated by various people.&lt;/p&gt;
    &lt;p&gt;I ran Plan 9 from Bell Labs as my day to day work environment until around 2002. By then two facts were painfully clear. First, the Internet was here to stay; and second, Plan 9 had no hope of keeping up with web browsers. Porting Mozilla to Plan 9 was far too much work, so instead I ported almost all the Plan 9 user level software to FreeBSD, Linux, and OS X.&lt;/p&gt;
    &lt;p&gt;Russ Cox (again):&lt;/p&gt;
    &lt;p&gt;The standard set up for a Plan 9 aficionado here seems to be a Mac or Linux machine running Plan 9 from User Space to get at sam, acme, and the other tools. Rob, Ken, Dave, and I use Macs as our desktop machines, but we're a bit of an exception. Most Google engineers use Linux machines, and I know of quite a few ex-Bell Labs people who are happy to be using sam or acme on those machines. My own setup is two screens. The first is a standard Mac desktop with non-Plan 9 apps and a handful of 9terms, and the second is a full-screen acme for getting work done. On Linux I do the same but the first screen is a Linux desktop running rio (formerly dhog's 8¬Ω).&lt;/p&gt;
    &lt;p&gt;More broadly, every few months I tend to get an email from someone who is happy to have just discovered that sam is still maintained and available for modern systems. A lot of the time these are people who only used sam on Unix, never on Plan 9. The plan9port.tgz file was downloaded from 2,522 unique IP addresses in 2009, which I suspect is many more than Plan 9 itself. In that sense, it's really nice to see the tools getting a much wider exposure than they used to.&lt;/p&gt;
    &lt;p&gt;I haven't logged into a real Plan 9 system in many years, but I use 9vx occasionally when I want to remind myself how a real Plan 9 tool worked. It's always nice to be back, however briefly.&lt;/p&gt;
    &lt;p&gt;Russ&lt;/p&gt;
    &lt;p&gt;Russ Cox continues:&lt;/p&gt;
    &lt;p&gt;&amp;gt; Can you briefly tell us why you (Russ, Rob, Ken and Dave)&lt;/p&gt;
    &lt;p&gt;&amp;gt; no longer use Plan9 ?&lt;/p&gt;
    &lt;p&gt;&amp;gt; Because of missing apps or because of missing driver for your hardware ?&lt;/p&gt;
    &lt;p&gt;&amp;gt; And do you still use venti ?&lt;/p&gt;
    &lt;p&gt;Operating systems and programming languages have strong network effects: it helps to use the same system that everyone around you is using. In my group at MIT, that meant FreeBSD and C++. I ran Plan 9 for the first few years I was at MIT but gave up, because the lack of a shared system made it too hard to collaborate. When I switched to FreeBSD, I ported all the Plan 9 libraries and tools so I could keep the rest of the user experience.&lt;/p&gt;
    &lt;p&gt;I still use venti, in that I still maintain the venti server that takes care of backups for my old group at MIT. It uses the plan9port venti, vbackup, and vnfs, all running on FreeBSD. The venti server itself was my last real Plan 9 installation. It's Coraid hardware, but I stripped the software and had installed my own Plan 9 kernel to run venti on it directly. But before I left MIT, the last thing I did was reinstall the machine using FreeBSD so that others could help keep it up to date.&lt;/p&gt;
    &lt;p&gt;If I wasn't interacting with anyone else it'd be nice to keep using Plan 9. But it's also nice to be able to use off the shelf software instead of reinventing wheels (9fans runs on Linux) and to have good hardware support done by other people (I can shut my laptop and it goes to sleep, and even better, when I open it again, it wakes up!). Being able to get those things and still keep most of the Plan 9 user experience by running Plan 9 from User Space is a compromise, but one that works well for me.&lt;/p&gt;
    &lt;p&gt;Russ&lt;/p&gt;
    &lt;p&gt;What Russ says is true but for me it was simpler. I used Plan 9 as my local operating system for a year or so after joining Google, but it was just too inconvenient to live on a machine without a C++ compiler, without good NFS and SSH support, and especially without a web browser. I switched to Linux but found it very buggy (the main problem was most likely a bad graphics board and/or driver, but still) and my main collaborator (Robert Griesemer) had done the ground work to get a Mac working as a primary machine inside Google, and Russ had plan9port up, so I pushed plan9port onto the Mac and have been there ever since, quite happily. Nowadays Apples are officially supported so it's become easy, workwise.&lt;/p&gt;
    &lt;p&gt;I miss a lot of what Plan 9 did for me, but the concerns at work override that.&lt;/p&gt;
    &lt;p&gt;-rob&lt;/p&gt;
    &lt;p&gt;They probably have their reasons.&lt;/p&gt;
    &lt;p&gt;Someone tried to find out:&lt;/p&gt;
    &lt;p&gt;https://www.muckrock.com/foi/united-states-of-america-10/foia-cia-plan-9-from-bell-labs-82547/&lt;/p&gt;
    &lt;p&gt;From: Rob Pike &amp;lt;robpike@gmail.com&amp;gt;&lt;/p&gt;
    &lt;p&gt;Date: Wed, 22 Jun 2022 12:58:15 +1000&lt;/p&gt;
    &lt;p&gt;Subject: [TUHS] Re: forgotten versions&lt;/p&gt;
    &lt;p&gt;The Plan 9 CD-ROM needed about 100MB for the full distribution, if that. We&lt;/p&gt;
    &lt;p&gt;hatched a plan to fill up the rest with encoded music and include the&lt;/p&gt;
    &lt;p&gt;software to decode it. (We wanted to get the encoder out too, but lawyers&lt;/p&gt;
    &lt;p&gt;stood in the way. Keep reading.) Using connections I had with folks in the&lt;/p&gt;
    &lt;p&gt;area, and some very helpful friends in the music business, I got permission&lt;/p&gt;
    &lt;p&gt;to distribute several hours of existing recorded stuff from groups like the&lt;/p&gt;
    &lt;p&gt;Residents and Wire. Lou Reed gave a couple of pieces too - he was very&lt;/p&gt;
    &lt;p&gt;interested in Ken and Sean's work (which, it should be noted, was built on&lt;/p&gt;
    &lt;p&gt;groundbreaking work done in the acoustics center at Bell Labs) and visited&lt;/p&gt;
    &lt;p&gt;us to check it out. Debby Harry even recorded an original song for us in&lt;/p&gt;
    &lt;p&gt;the studio.&lt;/p&gt;
    &lt;p&gt;We had permission for all this of course, and releases from everyone&lt;/p&gt;
    &lt;p&gt;involved. It was very exciting.&lt;/p&gt;
    &lt;p&gt;So naturally, just before release, an asshole (I am being kind) lawyer at&lt;/p&gt;
    &lt;p&gt;AT&amp;amp;T headquarters in Manhattan stopped the project cold. In a phone call&lt;/p&gt;
    &lt;p&gt;that treated me as shabbily as I have ever been, he said he didn't know who&lt;/p&gt;
    &lt;p&gt;these "assholes" (again, but this time his term) were and therefore the&lt;/p&gt;
    &lt;p&gt;releases were meaningless because anyone could have written them.&lt;/p&gt;
    &lt;p&gt;And that, my friends, is why MP-3 took off instead of the far better&lt;/p&gt;
    &lt;p&gt;follow-on system we were on the cusp of getting out the door.&lt;/p&gt;
    &lt;p&gt;-rob&lt;/p&gt;
    &lt;p&gt;P.S. No, I don't have the music any more. Too sad to keep.&lt;/p&gt;
    &lt;p&gt;Over the years Plan 9 has been released under various licenses, to the consternation of many.&lt;/p&gt;
    &lt;p&gt;The first edition, released in 1992, was made available only to universities. The process for acquiring the software was convoluted and prone to clerical error. Many potential users had trouble obtaining it within a reasonable time frame and many complaints were voiced on the eventual Plan 9 Internet mailing list.&lt;/p&gt;
    &lt;p&gt;The second edition, released in 1995 in book-and-CD form under a relatively standard commercial license, was available via mailorder as well as through a special telephone number for a price of approximately $350 USD. It was certainly easier to acquire than the first edition, but many potential users still complained that the price was too high and that the license was too restrictive.&lt;/p&gt;
    &lt;p&gt;In the year 2000, the third edition of Plan 9 was finally released under a custom "open source" license, the Plan 9 License. Richard Stallman was not impressed:&lt;/p&gt;
    &lt;p&gt;When I saw the announcement that the Plan Nine software had been released as "open source", I wondered whether it might be free software as well. After studying the license, my conclusion was that it is not free; the license contains several restrictions that are totally unacceptable for the Free Software Movement. (See http://www.gnu.org/philosophy/free-sw.html&lt;/p&gt;
    &lt;p&gt;Read more here:&lt;/p&gt;
    &lt;p&gt;http://www.linuxtoday.com/developer/2000070200704OPLFSW&lt;/p&gt;
    &lt;p&gt;In the year 2002, the fourth edition of Plan 9 was released under the Lucent Public License. This time, Theo de Raadt was not impressed:&lt;/p&gt;
    &lt;p&gt;The new license is utterly unacceptable for use in a BSD project.&lt;/p&gt;
    &lt;p&gt;Actually, I am astounded that the OSI would declare such a license acceptable.&lt;/p&gt;
    &lt;p&gt;That is not a license which makes it free. It is a *contract* with consequences; let me be clear -- it is a contract with consequences that I am unwilling to accept.&lt;/p&gt;
    &lt;p&gt;Read more here:&lt;/p&gt;
    &lt;p&gt;http://9fans.net/archive/2003/06/270&lt;/p&gt;
    &lt;p&gt;In 2014, portions of the Plan 9 source code were again re-licensed, this time under the GPLv2, for distribution with the University of California, Berkeley's Akaros operating system. Predictably, various parties were not impressed.&lt;/p&gt;
    &lt;p&gt;Russ Cox tried to make sense of the situation by commenting in a Hacker News thread:&lt;/p&gt;
    &lt;p&gt;When you ask "why did big company X make strange choice Y regarding licensing or IP", 99 times out of 100 the answer is "lawyers". If the Plan 9 group had had its way, Plan 9 would have been released for free under a trivial MIT-like license (the one used for other pieces of code, like the one true awk) in 2003 instead of creating the Lucent Public License. Or in 2000 instead of creating the "Plan 9 License". Or in 1995 instead of as a $350 book+CD that came with a license for use by an entire "organization". Or in 1992 instead of being a limited academic release.&lt;/p&gt;
    &lt;p&gt;Thankfully I am not at Lucent anymore and am not privy to the tortured negotiations that ended up at the obviously inelegant compromise of "The University of California, Berkeley, has been authorised by Alcatel-Lucent to release all Plan 9 software previously governed by the Lucent Public License, Version 1.02 under the GNU General Public License, Version 2." But the odds are overwhelming that the one-word answer is "lawyers".&lt;/p&gt;
    &lt;p&gt;Some have suggested that confusion about licensing may have contributed to Plan 9's failure to supplant UNIX in the wider computing world.&lt;/p&gt;
    &lt;p&gt;Any additions or changes (as recorded in git history) made by 9front are provided under the terms of the MIT License, reproduced in the file /lib/legal/mit, unless otherwise indicated.&lt;/p&gt;
    &lt;p&gt;Read: /lib/legal/NOTICE.&lt;/p&gt;
    &lt;p&gt;In 2021, the Plan 9 Foundation (aka P9F‚Äîno relation) convinced Nokia to re-license all historical editions of the Plan9 source code under the MIT Public License.&lt;/p&gt;
    &lt;p&gt;As a consequence, all of 9front is now provided under the MIT License unless otherwise indicated.&lt;/p&gt;
    &lt;p&gt;Re-read: /lib/legal/mit&lt;/p&gt;
    &lt;p&gt;-Plan 9 is a trademark of Lucent Technologies Inc.&lt;/p&gt;
    &lt;p&gt;+Plan 9 is a registered trademark of SouthSuite Inc.&lt;/p&gt;
    &lt;p&gt;https://github.com/plan9foundation/plan9/commit/9db62717612a49f78a83b26ff5a176971c6cdd18.diff&lt;/p&gt;
    &lt;p&gt;Brantley Coile noticed the Plan 9 trademark expired and bought it on November 25, 2020. It was before the transfer of the Plan 9 copyrights to the Plan 9 Foundation (March 23, 2021). ‚Äî 0intro, https://news.ycombinator.com/item?id=26946830&lt;/p&gt;
    &lt;p&gt;Academic papers that describe the Plan 9 operating system are available here:&lt;/p&gt;
    &lt;p&gt;Section (1) for general publicly accessible commands.&lt;/p&gt;
    &lt;p&gt;Section (2) for library functions, including system calls.&lt;/p&gt;
    &lt;p&gt;Section (3) for kernel devices (accessed via bind(1)).&lt;/p&gt;
    &lt;p&gt;Section (4) for file services (accessed via mount).&lt;/p&gt;
    &lt;p&gt;Section (5) for the Plan 9 file protocol.&lt;/p&gt;
    &lt;p&gt;Section (6) for file formats.&lt;/p&gt;
    &lt;p&gt;Section (7) for databases and database access programs.&lt;/p&gt;
    &lt;p&gt;Section (8) for things related to administering Plan 9.&lt;/p&gt;
    &lt;p&gt;The official website for the Plan 9 project is located at: https://9p.io/wiki/plan9&lt;/p&gt;
    &lt;p&gt;The official website for the Plan 9 Foundation is located at: http://p9f.org&lt;/p&gt;
    &lt;p&gt;The 9front fork of Plan 9 (that's us): http://9front.org&lt;/p&gt;
    &lt;p&gt;A community wiki setup by 9front users: http://wiki.9front.org&lt;/p&gt;
    &lt;p&gt;Much other valuable information can be found at http://cat-v.org regarding aspects of UNIX, Plan 9, and software in general.&lt;/p&gt;
    &lt;p&gt;Introduction to OS Abstractions Using Plan 9 From Bell Labs, by Francisco J Ballesteros (nemo)&lt;/p&gt;
    &lt;p&gt;Notes on the Plan 9 3rd Edition Kernel, by Francisco J Ballesteros (nemo)&lt;/p&gt;
    &lt;p&gt;The UNIX Programming Environment, by Brian W. Kernighan (bwk) and Rob Pike (rob) (this book is the most clear, concise and eloquent expression of the Unix and 'tool' philosophies to date)&lt;/p&gt;
    &lt;p&gt;9FRONT DASH 1 (the document you are reading right now, but in book form)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fqa.9front.org/fqa0.html#0.1"/><published>2026-01-18T13:32:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46668021</id><title>Predicting OpenAI's ad strategy</title><updated>2026-01-18T22:10:16.581664+00:00</updated><content>&lt;doc fingerprint="281eb85881276e3e"&gt;
  &lt;main&gt;
    &lt;p&gt;The World is Ads&lt;/p&gt;
    &lt;p&gt;Here we go again, the tech press is having another AI doom cycle.&lt;/p&gt;
    &lt;p&gt;I've primarily written this as a response to an NYT analyst painting a completely unsubstantiated, baseless, speculative, outrageous, EGREGIOUS, preposterous "grim picture" on OpenAI going bust.&lt;/p&gt;
    &lt;p&gt;Mate come on. OpenAI is not dying, they're not running out of money. Yes, they're creating possibly the craziest circular economy and defying every economics law since Adam Smith published 'The Wealth of Nations'. $1T in commitments is genuinely insane. But I doubt they're looking to be acquired; honestly by who? you don't raise $40 BILLION at $260 BILLION VALUATION to get acquired. It's all for the $1T IPO.&lt;/p&gt;
    &lt;p&gt;But it seems that the pinnacle of human intelligence: the greatest, smartest, brightest minds have all come together to... build us another ad engine. What happened to superintelligence and AGI?&lt;/p&gt;
    &lt;p&gt;See if OpenAI was not a direct threat to the current ad giants would Google be advertising Gemini every chance they get? Don't forget they're also capitalising on their brand new high-intent ad funnel by launching ads on Gemini and AI overview.&lt;/p&gt;
    &lt;p&gt;Let's crunch the numbers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Recap of OpenAI's 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;March: Closed $40B funding round at $260B valuation, the largest raise by a private tech company on record.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;June: Hit $10B ARR.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;July: First $1B revenue month, doubled from $500M monthly in January.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;November: Sam Altman says OpenAI expects $20B ARR for 2025.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Reached 800M WAU, ~190M DAU, 35M paying subscribers, 1M business customers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 2026: "Both our Weekly Active User (WAU) and Daily Active User (DAU) figures continue to produce all-time-highs (Jan 14 was the highest, Jan 13 was the second highest, etc.)"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 16, 2026: Announced ads in ChatGPT free and Go tiers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, OpenAI is burning $8-12B in 2025. Compute infrastructure is obviously not cheap when serving 190M people daily.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting OpenAI's Ad Strategy&lt;/head&gt;
    &lt;p&gt;So let's try to model their expected ARPU (annual revenue per user) by understanding what OpenAI is actually building and how it compares to existing ad platforms.&lt;/p&gt;
    &lt;p&gt;The ad products they've confirmed thus far:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ads at bottom of answers when there's a relevant sponsored product or service based on your current conversation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rollout:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Q1 2026: Limited beta with select advertisers&lt;/item&gt;
      &lt;item&gt;Q2-Q3 2026: Expanded to ChatGPT Search for free-tier users&lt;/item&gt;
      &lt;item&gt;Q4 2026: Sidebar sponsored content + affiliate features&lt;/item&gt;
      &lt;item&gt;2027: Full international expansion, self-serve platform&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testing starts "in the coming weeks" for logged-in adults in the U.S. on free and Go tiers. Ads will be "clearly labeled and separated from the organic answer." Users can learn why they're seeing an ad or dismiss it.&lt;/p&gt;
    &lt;p&gt;Their principles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answer independence: Ads don't influence ChatGPT's answers&lt;/item&gt;
      &lt;item&gt;Conversation privacy: Conversations stay private from advertisers, data never sold&lt;/item&gt;
      &lt;item&gt;Choice and control: Users can turn off personalization and clear ad data&lt;/item&gt;
      &lt;item&gt;Plus, Pro, Business, and Enterprise tiers won't have ads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They also mentioned a possibility of conversational ads where you can ask follow-up questions about products directly.&lt;/p&gt;
    &lt;p&gt;Revenue targets: Reports suggest OpenAI is targeting $1B in ad revenue for 2026, scaling to $25B by 2029, though OpenAI hasn't confirmed these numbers publicly. We can use these as the conservative benchmark, but knowing the sheer product talent at OpenAI, the funding and hunger. I think they're blow past this.&lt;/p&gt;
    &lt;p&gt;Personal speculations on integration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Self-serve platform: Advertisers bid for placements, super super super likely, exactly what Google does, probably their biggest revenue stream.&lt;/item&gt;
      &lt;item&gt;Affiliate commissions: Built-in checkouts so users can buy products inside ChatGPT, OpenAI takes commission, similar to their Shopify collab.&lt;/item&gt;
      &lt;item&gt;Sidebar sponsored content: When users ask about topics with market potential, sponsored info appears in a sidebar marked "Sponsored"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now let's compare this to existing ad platforms:&lt;/p&gt;
    &lt;head rend="h3"&gt;Google: Intent + Vertical Integration = Highest Revenue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based system where advertisers bid on keywords. Ads appear in search results based on bid + quality score.&lt;/item&gt;
      &lt;item&gt;Why it works: High intent (search queries) + owns the entire vertical stack (ad tech, auction system, targeting, decades of optimization)&lt;/item&gt;
      &lt;item&gt;Ad revenue: [$212.4B in ad revenue in the first 3 quarters of 2025]https://www.demandsage.com/google-ads-statistics/ (8.4% growth from 2024's $273.4B)&lt;/item&gt;
      &lt;item&gt;Google doesn't report ARPU so we need to calculate it: ARPU = $296.2B (projected) √∑ 5.01B = $59.12 per user annually.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Meta: No Intent + Vertical Integration = High ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Newsfeed ads delivered via auction. Meta's Andromeda AI evaluates bid + predicted action rate + ad quality to determine placement.&lt;/item&gt;
      &lt;item&gt;Why it works: Passive scrolling = low purchase intent, but on a massive scale + owns targeting infrastructure + Andromeda AI&lt;/item&gt;
      &lt;item&gt;ARPU: $68.44 in North America, $49.63 globally (Q1 2025)&lt;/item&gt;
      &lt;item&gt;Revenue: $160B in 2024 (97.3% of total revenue)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Twitter/X: Engagement + No Vertical Stack = Low ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based promoted tweets in timeline. Advertisers only pay when users complete actions (click, follow, engage).&lt;/item&gt;
      &lt;item&gt;Why it works: Timeline engagement, CPC ~$0.18, but doesn't own vertical stack and does it on a smaller scale&lt;/item&gt;
      &lt;item&gt;ARPU: ~$5.54 ($2.3B revenue √∑ 415M MAU)&lt;/item&gt;
      &lt;item&gt;Revenue: ~$2.3B in 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ChatGPT: High Intent + No Vertical Stack = Where Does It Sit?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intent level: High. 2.5B prompts daily includes product research, recommendations, comparisons. More intent than Meta's passive scrolling, comparable to Google search.&lt;/item&gt;
      &lt;item&gt;Vertical integration: None. Yet.&lt;/item&gt;
      &lt;item&gt;Scale: 1B WAU by Feb 2026, but free users only (~950M at 95% free tier).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So where should ChatGPT's ARPU sit?&lt;/p&gt;
    &lt;p&gt;It sits with Search, not Social.&lt;/p&gt;
    &lt;p&gt;Which puts it between X ($5.54) and Meta ($49.63). OpenAI has better intent than Meta but worse infrastructure. They have more scale than X but no vertical integration. When a user asks ChatGPT "Help me plan a 5-day trip to Kyoto" or "Best CRM for small business," that is High Intent. That is a Google-level query, not a Facebook-level scroll.&lt;/p&gt;
    &lt;p&gt;We already have a benchmark for this: Perplexity.&lt;/p&gt;
    &lt;p&gt;In late 2024/2025, reports confirmed Perplexity was charging CPMs exceeding $50. This is comparable to premium video or high-end search, and miles above the ~$2-6 CPMs seen on social feeds.&lt;/p&gt;
    &lt;p&gt;If Perplexity can command $50+ CPMs with a smaller user base, OpenAI‚Äôs "High Agency" product team will likely floor their pricing there.&lt;/p&gt;
    &lt;p&gt;Super Bullish Target ARPU Trajectory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: $5.50 (The "Perplexity Floor") - Even with a clumsy beta and low fill rate, high-intent queries command premium pricing. If they serve just one ad every 20 queries at a Perplexity-level CPM, they hit this number effortlessly.&lt;/item&gt;
      &lt;item&gt;2027: $18.00 - The launch of a self-serve ad manager (like Meta/Google) allows millions of SMBs to bid. Competition drives price.&lt;/item&gt;
      &lt;item&gt;2028: $30.00 - This is where "Ads" become "Actions." OpenAI won't just show an ad for a flight; they will book it. Taking a cut of the transaction (CPA model) yields 10x the revenue of showing a banner.&lt;/item&gt;
      &lt;item&gt;2029: $50.00 (Suuuuuuuper bullish case) - Approaching Google‚Äôs ~$60 ARPU. By now, the infrastructure is mature, and "Conversational Commerce" is the standard. This is what Softbank is praying will happen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And we're forgetting that OpenAI have a serious serious product team, I don't doubt for once they'll be fully capable of building out the stack and integrating ads til they occupy your entire subconscious.&lt;/p&gt;
    &lt;p&gt;In fact they hired Fidji Simo as their "CEO of Applications", a newly created role that puts her in charge of their entire revenue engine. Fidji is a Meta powerhouse who spent a decade at Facebook working on the Facebook App and... ads:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Leading Monetization of the Facebook App, with a focus on mobile advertising that represents the vast majority of Facebook's revenue. Launched new ad products such as Video Ads, Lead Ads, Instant Experiences, Carousel ads, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Launched and grew video advertising to be a large portion of Facebook's revenue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Being Realistic About Competition&lt;/head&gt;
    &lt;p&gt;ChatGPT will hit 1B WAU by February 2026.&lt;/p&gt;
    &lt;p&gt;But 1.5-1.8B free users by 2028? That assumes zero competition impact from anyone, certainly not the looming giant Gemini. Unrealistic.&lt;/p&gt;
    &lt;p&gt;Let's estimate growth super conservatively accounting for competition:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: 950M free users (1B WAU √ó 95% free tier)&lt;/item&gt;
      &lt;item&gt;2027: 1.1B free users (slower growth as market saturates)&lt;/item&gt;
      &lt;item&gt;2028: 1.2-1.3B free users (competition from Google, Claude)&lt;/item&gt;
      &lt;item&gt;2029: 1.4B free users (mature market, multi-player landscape)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The main revenue growth comes from ARPU scaling not just user growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting 2026&lt;/head&gt;
    &lt;p&gt;Crunching all the numbers from "High Intent" model, 2026 looks different.&lt;/p&gt;
    &lt;p&gt;Base revenue from subscriptions + enterprise + API: $25-30B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;35M paying subscribers: $8.4B minimum (conservatively assuming all at $20/mo Plus tier)&lt;/item&gt;
      &lt;item&gt;Definitely higher with Pro ($200/mo) and Enterprise (custom pricing)&lt;/item&gt;
      &lt;item&gt;Enterprise/API: $2.3B in 2025 ‚Üí $17.4B by mid-2027&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ad revenue (year 1): ~$5.2B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;950M free users x $5.50 ARPU&lt;/item&gt;
      &lt;item&gt;ChatGPT does 2.5B prompts daily this is what advertisers would class as both higher engagement and higher intent than passive scrolling (although you can fit more ads in a scroll than a chat)&lt;/item&gt;
      &lt;item&gt;Reality Check: This assumes they monetise typical search queries at rates Perplexity has already proven possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total 2026 Revenue: ~$30-35B.&lt;/p&gt;
    &lt;head rend="h2"&gt;Projecting 2027-2029&lt;/head&gt;
    &lt;p&gt;These projections use futuresearch.ai's base forecast ($39B median for mid-2027, no ads) + advertising overlay from internal OpenAI docs + conservative user growth.&lt;/p&gt;
    &lt;p&gt;2027:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $39B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $19.8B (1.1B free users √ó $18 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $58.8B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2028:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $55-60B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $36-39B (1.2-1.3B free users √ó $30 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $91-99B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2029:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $70-80B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $70B (1.4B free users √ó $50 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $140-150B&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The World is Ads&lt;/head&gt;
    &lt;p&gt;Ads were the key to unlocking profitability, you must've seen it coming, thanks to you not skipping that 3 minute health insurance ad - you, yes you helped us achieve AGI!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Mission alignment: Our mission is to ensure AGI benefits all of humanity; our pursuit of advertising is always in support of that mission and making AI more accessible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The A in AGI stands for Ads! It's all ads!! Ads that you can't even block because they are BAKED into the streamed probabilistic word selector purposefully skewed to output the highest bidder's marketing copy.&lt;/p&gt;
    &lt;p&gt;Look on the bright side, if they're turning to ads it likely means AGI is not on the horizon. Your job is safe!&lt;/p&gt;
    &lt;p&gt;It's 4:41AM in London, I'm knackered. Idek if I'm gonna post this because I love AI and do agree that some things are a necessary evil to achieve a greater goal (AGI). Nevertheless, if you have any questions or comments, shout me -&amp;gt; ossamachaib.cs@gmail.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ossa-ma.github.io/blog/openads"/><published>2026-01-18T14:25:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46669663</id><title>Sins of the Children (Adrian Tchaikovsky)</title><updated>2026-01-18T22:10:16.068838+00:00</updated><content>&lt;doc fingerprint="e67013d6963124f4"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The thing that came down right beside us was three meters high with a massive articulated body. A bug, really, Chelicer style. Eight crooked legs out from a central hub like all the mobile life here had, but most of what we‚Äôd seen was gracile, delicate, and came up to your waist. Even the Farmers ‚Äî which we‚Äôd pegged as the most advanced species around ‚Äî were only a meter and a half tall, and most of that was stilting limbs. This thing was not gracile. Every segment and joint of it was ridgy, armored, and spiky. It was dun and khaki like the planet‚Äôs dust, but too big to have hidden anywhere nearby, towering over the scrub. There were spread vanes like sails projecting from its back, but it couldn‚Äôt have flown under organic power. It must have weighed five tons.&lt;/p&gt;
      &lt;p&gt;We just stared. In that moment, when we could have run or called for help, we goggled at it. The stalked globes of its eyes looked back, devoid of living connection. A vast armored monster, airdropped from nowhere.&lt;/p&gt;
      &lt;p&gt;I saw the motion, off on a neighboring hillside. There was a second monster out there, surprisingly hard to spot. It hunkered down, drawing its limbs in.&lt;/p&gt;
      &lt;p&gt;Chunk! That same sound. The thing on the hillside was gone.&lt;/p&gt;
      &lt;p&gt;A second later it was on us, coming down right in front of Merrit. I thought of mechanical advantage, the tricks you could do with a rigid exoskeleton. I thought of fleas, but on an absurd macro scale. It jumped and came down on eight legs that must have been shock absorbers par excellence.&lt;/p&gt;
      &lt;p&gt;Chelicer life doesn‚Äôt quite have a front or a back, built around that hub of legs. The mouth is on the underside and that‚Äôs what this thing tilted at Merrit. &lt;/p&gt;
      &lt;p&gt;I‚Äôd dissected some of the Farmers and they had an arrangement like eight knuckly stumps to mumble over their food with. These new arrivals had a setup like a sphincter made of scissor blades and nutcrackers, more an industrial process than biology. We‚Äôd seen what those tools had done to the weather station already. Right then we were more concerned with what it did to Merrit. &lt;/p&gt;
      &lt;p&gt;He was just crouched there, midway through sifting the wreckage. The monster took instant offense. Its mouthparts extended out and just ‚Ä¶ macerated him. Chopped and crushed so that in a heartbeat there was nothing left that looked remotely human, just a wadded bloody ball of flesh and splintered bone and rags of suit. &lt;/p&gt;
      &lt;p&gt;Greffin and I started shooting. Our guns were so badly printed you could see the mold lines. They chewed up their own mass for ammo in a spray of flechettes. True to our miserly resource budget, most of that barrage just slanted off the things‚Äô carapaces, and I knew we were both going to follow Merrit into extinction, carved up and spat out with alien contempt. Except then Greffin hit a joint, and one monster was suddenly down a leg. That, apparently, was enough. We watched them ratchet down for takeoff, still shrugging off our fire, then ping upward. I recorded the flight of one, desperately trying to keep it in my field of vision. Without that, who‚Äôd believe us? Alien mega-fleas utilizing sheer mechanical tension to jump a half kilometer at a time.&lt;/p&gt;
      &lt;p&gt;We bagged what was left of Merrit. And I grabbed the leg when the skimmer came for evac. Because it was proof that here be monsters.&lt;/p&gt;
      &lt;p&gt;The Farmers had been the tipping point, the reason to establish a human presence planetside. Yes, 14d was a unique world, unknown alien ecosphere, all that. But if it hadn‚Äôt held anything useful then the Garveneer would have focused elsewhere in the Chelicer system. And if the world had only offered mineral wealth, we‚Äôd have a robot mining operation stripping the place instead. All that unique ecosphere would have been flensed from the planet‚Äôs surface as an incidental side effect of our efforts. But on this world, the valuable thing was the biology, which needed more finesse. A human presence on the ground. Meaning a whole team of us thawed off the shelves and given this chance to justify our existence on the payroll.&lt;/p&gt;
      &lt;p&gt;Which was now under threat, as were we. We evacuated back to the farms with our grisly souvenirs.&lt;/p&gt;
      &lt;p&gt;The Concerns that have spearheaded humanity‚Äôs expansion from star to star have refined an efficient system for exploiting exoplants. When a Concern builds farms, that means a continent‚Äôs span of identical fields, robot tended. Everything growing and being harvested at an accelerated rate, processed and dried for minimal weight in transit. Turned into the Ship‚Äôs Reconstitute we‚Äôre all thoroughly sick of eating. The stuff from the Chelicer farms can look mighty good in comparison, which is a shame because a bite would kill you stone dead. But then they‚Äôre not our farms. They‚Äôre a thing the locals were doing long before we arrived.&lt;/p&gt;
      &lt;p&gt;The locals ‚Äî Species 11 ‚Äî are like spiders only ganglier. Four stilty legs interspersed with four spindly arms, and a hub of a body in the middle, high enough to come up to your waist. We called them Farmers from the start because it‚Äôs what they do: tend great stretches of this one crop. Not even a very exciting-looking crop, sort of a warty purple potato-looking thing, except it turns out to be superefficient at concentrating the elements in the crappy soil they‚Äôve got here. Many of which elements are useful to us, for our superconductors and our computational substructures and all that good stuff. When we discovered that, you can be damn sure we moved in and took possession double time. Built our processing plant and started making off with a big chunk of the crop. &lt;/p&gt;
      &lt;p&gt;What did the locals think of this? My professional xenobiologist‚Äôs opinion was they didn‚Äôt think a damn thing. They didn‚Äôt react at all. The whole farming schtick they had going was just instinct, like ants, only they didn‚Äôt even defend anything. When they got in the way of the machines, they got chewed up. We thought at the time they‚Äôd evolved with no natural predators. &lt;/p&gt;
      &lt;p&gt;We sure as hell were wrong about that.&lt;/p&gt;
      &lt;p&gt;Greffin and I made our reports. The dozen on-planet crew came to commiserate, meaning get the gory details. We told everyone to carry a gun and know the emergency drill. Chelicer had an apex predator we hadn‚Äôt known about. After which cautionary tales, I was left facing up to the mission‚Äôs biggest pain in my ass, namely FenJuan.&lt;/p&gt;
      &lt;p&gt;FenJuan had screwed up royally on some past previous assignments, was my guess. They‚Äôd been something senior, and something had gone south in expensive ways. Meaning FenJuan slumming it on our team was an invisible mark against every one of us, because their personnel file came with baggage. Worse, they were my immediate colleague in biosciences, the two of us responsible for figuring out the local biochemistry.&lt;/p&gt;
      &lt;p&gt;‚ÄúStort,‚Äù they addressed me, frosty as always.&lt;/p&gt;
      &lt;p&gt;‚ÄúFen,‚Äù I replied with just as much love.&lt;/p&gt;
      &lt;p&gt;‚ÄúMy samples?‚Äù they said. Because they didn‚Äôt do fieldwork, just like they didn‚Äôt do basic human interaction, just sat at base camp and bitched.&lt;/p&gt;
      &lt;p&gt;And I‚Äôd given them samples previously. I‚Äôd cut a chunk out of a dozen critters on four other excursions and brought them back. And I‚Äôd just seen a work colleague turned to paste by some local monster-bug neither my nor FenJuan‚Äôs science had accounted for. But in the Concerns you don‚Äôt get time off for inefficient foibles like grief or trauma, so I made do with snarling at FenJuan that they‚Äôd had all the damn samples they were getting from me and if that wasn‚Äôt good enough then maybe they were the problem.&lt;/p&gt;
      &lt;p&gt;‚ÄúWhen I say, ‚ÄòGet me a selection so I can run comparative studies,‚Äô‚Äù they snapped, ‚ÄúI do not mean just go snip bits off the Farmers and call the job done. A man is dead because we don‚Äôt understand the world here.‚Äù&lt;/p&gt;
      &lt;p&gt;Which was turning it back on me, making it my fault. And which wasn‚Äôt true to boot. I told them that if they were having difficulty distinguishing between samples maybe they didn‚Äôt have the basic analytical skills required for the task. The structures that they‚Äôd pegged as the local equivalent of a genome were probably just some essential organelle that every damn beastie possessed, and the real genome-equivalent had gone completely under FenJuan‚Äôs radar. &lt;/p&gt;
      &lt;p&gt;‚ÄúYou want a sample?‚Äù I asked FenJuan. ‚ÄúFor real? Cut your own out of this. You can be absolutely sure it doesn‚Äôt come from a Farmer.‚Äù And I pointed them at the leg, the one we‚Äôd shot off the big bouncing bastard.&lt;/p&gt;
      &lt;p&gt;Shouting at people works, when you‚Äôre not allowed time off to process death. Works remarkably well, if it‚Äôs the only outlet you‚Äôve got. Just as well. There would be plenty of both shouting and death in everyone‚Äôs future.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;p&gt;I liked to sit outside to complete my reports. Chelicer has good sun, if you‚Äôve had the treatments to ward off skin damage. I wrote up my thoughts on our giant killer flea problem, watching the Farmers pick their way across the vast fields of ‚ÄúSpecies 13 Resource‚Äù as per official Concern designation, or the Chelicetato as our vulgar parlance had it. They groped over each tuber in turn, then pissed out the right chemicals to help the things grow. The Farmers were a remarkable find. We‚Äôd have gone way over budget making robot gardeners even half as efficient. Worth fighting off a few giant bugs for.&lt;/p&gt;
      &lt;p&gt;Past the processing plants, the elevator cable stretched into forever. Up there was the Garveneer, our home away from home, taking every processed tuber we could hoik out of the ground. And our little outpost here was just the beginning. There were tens of millions of Farmers all over the planet, wherever the conditions suited their crop, all ready to become part of the industrial agriculture of the Concerns. We‚Äôd struck the jackpot when we surveyed Chelicer 14d.&lt;/p&gt;
      &lt;p&gt;Greffin had been going through recent survey images, looking for monsters. She sent me what she found. Holes like burrows I could have driven a ground-car into, written off as geological because nothing we‚Äôd seen could have made them. Now we knew better. Maybe the monster fleas had emerged only recently. Maybe there was a cicada thing going on, killer flea season. We made some recommendations for the next security meeting, and I did a tour of the turret guns that had been put in with the processing plant and never needed since.&lt;/p&gt;
      &lt;p&gt;Doing that put me in FenJuan‚Äôs orbit and I braced myself for the sandpaper of their company. They were deep in analyzing the giant leg, though, or thin-sliced samples thereof. They had a few dead Farmers too ‚Äî there were plenty of aimless ones not working, now we‚Äôd harvested their plots. &lt;/p&gt;
      &lt;p&gt;‚ÄúStort,‚Äù they said, not the usual bark, but thoughtful. On the screens was a variety of different views of microscopic-scale Chelicer cell structure. The spiral-walled cones that FenJuan reckoned were hereditary information, and that they‚Äôd been unspooling and trying to decode. Sections had been flagged up on each, identical one to another.&lt;/p&gt;
      &lt;p&gt;‚ÄúJunk DNA,‚Äù I said, and waited for their usual invective. It didn‚Äôt come, though. FenJuan actually nodded a little, a tiny iota of acknowledgment I‚Äôd said something that wasn‚Äôt stupid. And Earth life accumulates a certain amount of genetic junk, right? Stuff in the genome that‚Äôs been switched off, acquired from bacteria, or from benign transcription errors carried on down through the generations. But FenJuan reckoned something like 90 percent of any given beastie‚Äôs hereditary was this unused junk. &lt;/p&gt;
      &lt;p&gt;I wanted to say they were imagining things. I wanted to say it was a crap planet with crap aliens who had crap hereditary code, and us coming along to exploit them was the best thing that could have happened. That was how my encounters with FenJuan generally went. It was basically entertainment for the rest of the team.&lt;/p&gt;
      &lt;p&gt;I didn‚Äôt say any of that. FenJuan and I looked at each other, not quite ready to bury the hatchet, but maybe agreeing there was a bigger problem out there to save that mutual hatchet for.&lt;/p&gt;
      &lt;p&gt;The attack came the next day, and we weren‚Äôt prepared.&lt;/p&gt;
      &lt;p&gt;I heard the sound, distant, echoing across flat farmland from the dry hills. Chunk. For two whole seconds I was thinking some piece of machinery had gone wrong and how that was someone else‚Äôs problem. And then the first of them came down, just like before. Crashing onto the roof of the processing plant hard enough to buckle the plastic composite. Leering over the edge like a gargoyle. I swear it was twice the size of the one that killed Merrit. &lt;/p&gt;
      &lt;p&gt;I was shouting. Most of us were shouting, but I still caught a rapid heavy drumroll underneath the human noise. Chunkchunkchunkchunkchunkchunkchunk‚Ä¶&lt;/p&gt;
      &lt;p&gt;They started dropping down all round us. We were running for the plant, because it was the most reinforced building and that was the emergency drill. Someone got word to the guns that their services were needed, and they started running friend-or-foe algorithms as a dozen human beings fled frantically into their arcs of fire. &lt;/p&gt;
      &lt;p&gt;One of the death-fleas crashed down in front of me, outspread sails barely slowing it. The articulation of its legs popped and twisted, absorbing the force of impact. A gun hammered chips out of its carapace. It lunged forward and snipped someone ‚Äî one of the resources team I think ‚Äî right in half with its scissor-blade face. I screamed and just about ducked through the shadow of its wings, not knowing if I‚Äôd get killed by its jaws or our own turrets.&lt;/p&gt;
      &lt;p&gt;Most of us got inside. They didn‚Äôt break in after us, but only because they didn‚Äôt try. Maybe object permanence isn‚Äôt a big thing on Chelicer: Once we were out of sight they seemed to forget us, through they chewed up all the guns.&lt;/p&gt;
      &lt;p&gt;Through our cameras, we got to see all the rest of what they did.&lt;/p&gt;
      &lt;p&gt;The Farmers, it turned out, had natural predators. Or they did in death-flea season. The monsters went to town, mostly on the Farmers that didn‚Äôt have anything left to farm, because they were just milling about. It was a massacre. And though they were weird alien spider guys, and you can‚Äôt really anthropomorphize that, we were all surprisingly cut up. It wasn‚Äôt that they were getting slaughtered out there. It was that they were ours. Our livelihood, our profit, the injection of resources that was earning us our wage-worth.&lt;/p&gt;
      &lt;p&gt;The massacre was monopolizing our attention, so the real damage went almost unnoticed until the earthquakelike convulsion that cracked every wall and trashed the processor floor. For a moment the problem was so big I couldn‚Äôt work out what had happened.&lt;/p&gt;
      &lt;p&gt;The elevator cable. Something about it ‚Äî maybe just that it was the biggest thing around ‚Äî had drawn their ire. A half dozen of the bastards had jumped to it, and those mouthparts had sawn through the supertensile material like it was string.&lt;/p&gt;
      &lt;p&gt;That took a long while to clear up. The actual cable was, after all, a long weighted strand that stretched a good way out of atmosphere and into space, and our actual ship was tethered at the halfway point. The Garveneer decoupled sharpish, you can be sure, and the vast length of the cable, cut free at its anchor, just vanished upward and sideways like the blade of God‚Äôs own scythe, on its way toward the outer reaches of the system. &lt;/p&gt;
      &lt;p&gt;We were stuck on-planet for some time, and we‚Äôd just had it demonstrated to us that the death-fleas were more than capable of carving their way into our compromised fortress if they wanted. Yes, our lords and masters in the Concern could shuttle us back to orbit, but that would require circumstances to fall into a very narrow gap indeed. That (1) it wasn‚Äôt worth continuing work on Chelicer 14d, and (2) it was actually worth retrieving us, rather than writing us off. &lt;/p&gt;
      &lt;p&gt;You can imagine the mood on the ground as we waited for their decision. We all gathered in the surviving common space and tried to convince ourselves we weren‚Äôt screwed. All except FenJuan, who didn‚Äôt do social graces, but just kept on studying the samples, which our remaining instruments couldn‚Äôt tell apart.&lt;/p&gt;
      &lt;p&gt;In the end, after they‚Äôd left us hanging for five days, there was a meeting. A handful of us on a staticky link to the chief director safe aboard the Garveneer. We were ready to be bawled out for a colossal loss of resources. That was the very best we thought we‚Äôd get. Instead, though, the Great Man was onside. The harvest from Chelicer had been very good indeed, solving a variety of rare elements shortages none of us knew the Concern had. This world we had worked on was the new hope of further human expansion. If only we could solve our little pest problem.&lt;/p&gt;
      &lt;p&gt;‚ÄúWe need to keep you folks safe,‚Äù said the director heartily. I looked over the recommendations. What they actually wanted to keep safe was the harvest, of course, which meant the Farmers. By then we had images from all over the planet of sporadic attacks on Farmer colonies. Death-fleas picking off the weak. Nothing as sustained as we‚Äôd seen at our base camp, but plainly a part of the circle of life in these parts.&lt;/p&gt;
      &lt;p&gt;‚ÄúOur engineers up here are working on a new cable,‚Äù the Great Man told us. ‚ÄúBut drones, too. Hunter drones. A whole fleet of them. We can justify the cost, given the potential resource revenue you‚Äôve demonstrated. We‚Äôre proposing a global initiative to wipe out these things.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúWipe out the species, Director?‚Äù FenJuan clarified. &lt;/p&gt;
      &lt;p&gt;‚ÄúGiven the losses we‚Äôve sustained and the clear threat to productivity, it‚Äôs the leading proposal. But I‚Äôm here for your thoughts.‚Äù That cheery smile of his. ‚ÄúStort?‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúWe‚Äôre obviously still adjusting our picture of the ecosphere to incorporate these things,‚Äù I said. ‚ÄúGiven the low species count on-world, having an apex predator that only emerges sporadically makes some sense. What happens if we remove it? We can‚Äôt know. If this was a matter of wanting to preserve a working natural ecosystem I‚Äôd say there would be too many potential imbalances generated by cropping the top of the food chain. But.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúBut,‚Äù the director agreed. Because we were not, after all, interested in preserving the ecosystem. Just that part of it that worked for us.&lt;/p&gt;
      &lt;p&gt;FenJuan‚Äôs eyes were boring into me; I didn‚Äôt meet them. ‚ÄúHistorically,‚Äù I said, ‚Äúin a managed agricultural paradigm, removal of the top predators has been accomplished very profitably. Wolves, sheep, so on. It‚Äôs not as though we‚Äôre going to have a problem with some Farmer population explosion. If some other species booms, we can manage the consequences. I say do it.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúDirector,‚Äù FenJuan put in, unasked. ‚ÄúI have yet to come to any understanding of the biology or relationships involved here. There‚Äôs a commonality between species I can‚Äôt account for. This world plainly went through some severe ecological crisis that left a depauperate web of interdependence. We don‚Äôt know‚Äî‚Äù&lt;/p&gt;
      &lt;p&gt;On our screens, the director settled back in his big chair. ‚ÄúWe know all we need to. What this world could be worth to us. How much damage those beasts are capable of doing. An elevator cable! We‚Äôll conduct a localized culling in your region first. Barring any obvious consequence, we can roll it out to the rest of the world and follow up with plant and personnel wherever these Farmer creatures are to be found.‚Äù His smile was genuinely pleased, a man who‚Äôs going to see a nice bonus. ‚ÄúWell done, all. I know it‚Äôs been tough, but you‚Äôre heroes.‚Äù&lt;/p&gt;
      &lt;p&gt;The local cull, when it first happened, was something to watch. Drone footage wheeling and spinning as our machines found and chased the fleas. Killed them as they leapt through the air, as they landed thunderously on the ground, as they emerged from their burrows. Wiping them out within 200 klicks of the processing plant. &lt;/p&gt;
      &lt;p&gt;And nothing broke. The Farmers kept on farming. The crops grew. The crops that, at a cellular level, seemed weirdly indistinguishable from the things that tended them. FenJuan was raising issues every day, by then. Desperate to communicate how weird their results were. Not doing their job, because their job was solely and specifically to identify aspects of the local biochemistry that could be profitably exploited. Instead of which, they were going nuts about how every critter just seemed to have this enormous bolus of unused genetic-equivalent information, with a huge overlap between species. And I think they‚Äôd just about worked it out, except by then they‚Äôd made such a nuisance of themselves that FenJuan was the very last person our bosses up in orbit wanted to hear from. Did this discovery open up new vistas of planetary exploitation for our already profitable operation? No? Then pipe down and stop using up comms resources.&lt;/p&gt;
      &lt;p&gt;The people the director did want to hear from were designing and deploying the hunter-killers. Our expanded drone fleet was greenlit: hundreds of machines shipped downwell and let loose across the globe. Wherever they found the fleas, they destroyed them. We felt we were liberators. Whole populations of Farmers could live without those monstrous shadows falling on them. Yes, we were making a species extinct, but it wasn‚Äôt a nice species. We were already on the next phase of occupation, a 10-year building plan where we‚Äôd fill the planet with farms and processing plants, replicating our first outpost over and over until there wasn‚Äôt an inch of the world that wasn‚Äôt working for us.&lt;/p&gt;
      &lt;p&gt;A couple of years into our agricultural expansion, the cacti disappeared. Not cacti, obviously. Species 43 in the Concern bestiary, but cactus enough that the name had stuck. We had a look one morning and there just wasn‚Äôt any more of it left. I suggested maybe it had been living off some sort of death-flea by-products, though the timing seemed unusually lethargic for that kind of interaction. I ended up working alongside FenJuan, and we found drone footage of the cacti stuff getting up and running around, so that Species 43 turned out to be the larval-or-something form of Species 22, and we had to recalibrate the records. &lt;/p&gt;
      &lt;p&gt;At around the same time, the little hairy critters that were Species 38 rooted down and grew long spires with puffballs on them, making them actually Species 17. Half a year later our existing Species 11s lost their poles and became another sort of thing we‚Äôd already seen, and so on and so on. To most of us it was a curiosity. To FenJuan it was a crawling horror that I was starting to share. All their snapping, bitching at me for not seeing, and I‚Äôd just written it all off as someone pissed their Concern work record was full of demerits. Except they‚Äôd been right and I‚Äôd been wrong.&lt;/p&gt;
      &lt;p&gt;There were no more cacti. That was what scared FenJuan. We watched a wave of transformations. Each form turned into something else, but none of it turned into the cactuslike Species 43. Then it was something else, where our current batch just metamorphosed and there were no new ones. None at all, anywhere on Chelicer. The dry country became less and less inhabited as species after species vanished away.&lt;/p&gt;
      &lt;p&gt;Or not species. That was what FenJuan had been trying to understand. Developmental stages. Not a circle of life, but a life cycle.&lt;/p&gt;
      &lt;p&gt;Our prized cheliceratos, which had been putting out runners and new tubers happily for over a decade, were suddenly ambulatory one morning, sprouting a thicket of spindly legs and just giving up their life of being agricultural produce. That got people‚Äôs attention. Around the same time one weird round critter rooted down where the Farmers were and became the new Chelicetato crop, and the dumbest of our colleagues reckoned that was all OK then. FenJuan and I had stopped trying to raise the alarm, by then, because it obviously wasn‚Äôt going to help. Soon after, some buried fungal-looking thing we‚Äôd found no use for sprouted legs and became new Farmers. And the old farmers ‚Ä¶ died off. Wore out, natural causes. Leaving only the least dregs we‚Äôd left of their crop. From which a handful of stunted things crawled, devouring their own left-behind husks and the last corpses of their tenders. They were tiny, but we recognized them even as they began to wearily dig down into the parched, lifeless soil. Nascent fleas, entering that dormant part of their cycle from which they would emerge, at some future date, into a world devoid of anything that could sustain them. Behind them, the whole ecosystem of life stages had been rolled up. There was nothing left of it. They were the last.&lt;/p&gt;
      &lt;p&gt; As we had harvested and plundered, we had been watching a decade-long series of transformations. One that had definitively ended. Life on Chelicer vanished. Plant forms, bug forms, just about every macrobiological creature dying off one at a time and not being replaced by a new generation. As though death had asked them to form an orderly queue. &lt;/p&gt;
      &lt;p&gt;There had been a mass extinction in Chelicer‚Äôs past, FenJuan and I reckoned. Something that had killed off everything except a hardy species that inherited an utterly impoverished planetary biome. Colder at the poles, warmer at the equator, but barren, desperate. So, over the ages, that species had developed to exploit every last opportunity that the world had left to it, not through speciation but through adaption of its life cycle. Gathering the meager resources of the world, concentrating them in living forms that could be harvested in turn. Sedentary stages, mobile stages, squeezing every possible niche of everything that could be gained and then transforming into the next phase of its long and complex chain of shapes. A desperate ecosystem of one, harvesting and gathering and recycling, each stage into the next, surviving everything thrown at it. Except us, who came and severed a single link utterly and irrevocably. Cut one thread and watched the whole unravel over a mere decade.&lt;/p&gt;
      &lt;p&gt;FenJuan and I were last off the planet, on the final elevator car along with the last salvage from our farming operations. It was on us, we had been told. We were the biologists, and we should have seen it coming. And they were right; we should. But all that would have done was salve our professional pride. I don‚Äôt believe for a moment they‚Äôd have listened to us if we‚Äôd said Stop. Stop isn‚Äôt the way of the Concerns. Stop doesn‚Äôt meet quotas or hit targets.&lt;/p&gt;
      &lt;p&gt;We stepped into the elevator car, FenJuan and I. We looked back over a world unrelieved by messy, complicated stuff, such as life. A failed commercial opportunity, as the report would say. &lt;/p&gt;
      &lt;p&gt;I wanted to say something. Possibly You were right. But what good would it do? We were both going to be back on ice when we reached the ship, with personnel files so dire they‚Äôll probably never thaw us out again. But, like the life of Chelicer, we‚Äôre not important, compared to the bigger picture of the Concerns and their expansion. We humans go on, world to world, star to star, making the universe our own. But on Chelicer there will only ever be dust.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://asteriskmag.com/issues/07/sins-of-the-children"/><published>2026-01-18T17:08:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670024</id><title>Gaussian Splatting ‚Äì A$AP Rocky "Helicopter" music video</title><updated>2026-01-18T22:10:15.804224+00:00</updated><content>&lt;doc fingerprint="2ff1eb95032d70b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Michael Rubloff&lt;/p&gt;
    &lt;p&gt;Jan 13, 2026&lt;/p&gt;
    &lt;p&gt;Believe it or not, A$AP Rocky is a huge fan of radiance fields.&lt;/p&gt;
    &lt;p&gt;Yesterday, when A$AP Rocky released the music video for Helicopter, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. What√¢s easier to miss, unless you know what you√¢re looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.&lt;/p&gt;
    &lt;p&gt;I spoke with Evercoast, the team responsible for capturing the performances, as well as Chris Rutledge, the project√¢s CG Supervisor at Grin Machine, and Wilfred Driscoll of WildCapture and Fits√Ö¬´.ai, to understand how Helicopter came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.&lt;/p&gt;
    &lt;p&gt;The decision to shoot Helicopter volumetrically wasn√¢t driven by technology for technology√¢s sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.&lt;/p&gt;
    &lt;p&gt;Chris told me he√¢d been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply weren√¢t possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a √¢someday√¢ workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.&lt;/p&gt;
    &lt;p&gt;The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.&lt;/p&gt;
    &lt;p&gt;Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoast√¢s system. It√¢s all real performance, preserved spatially.&lt;/p&gt;
    &lt;p&gt;This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for Shittin√¢ Me featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.&lt;/p&gt;
    &lt;p&gt;The primary shoot for Helicopter took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.&lt;/p&gt;
    &lt;p&gt;Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.&lt;/p&gt;
    &lt;p&gt;Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.&lt;/p&gt;
    &lt;p&gt;That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOY√¢s OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.&lt;/p&gt;
    &lt;p&gt;One of the more powerful aspects of the workflow was Evercoast√¢s ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through Evercoast√¢s web player before downloading massive PLY sequences for Houdini.&lt;/p&gt;
    &lt;p&gt;In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. It√¢s a workflow that more closely resembles simulation than traditional filming.&lt;/p&gt;
    &lt;p&gt;Chris also discovered that Octane√¢s Houdini integration had matured, and that Octane√¢s early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional √¢3D video√¢ look was a major reason the final aesthetic lands the way it does.&lt;/p&gt;
    &lt;p&gt;The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCapture√¢s internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow Houdini√¢s simulation toolset to handle rigid body, soft body, and more physically grounded interactions.&lt;/p&gt;
    &lt;p&gt;One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldn√¢t be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You aren√¢t limited by the camera√¢s composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply can√¢t.&lt;/p&gt;
    &lt;p&gt;In other words, radiance field technology isn√¢t replacing reality. It√¢s preserving everything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting"/><published>2026-01-18T17:40:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670181</id><title>Show HN: Lume 0.2 ‚Äì Build and Run macOS VMs with unattended setup</title><updated>2026-01-18T22:10:15.468108+00:00</updated><content>&lt;doc fingerprint="25b849d06d0791ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What is Lume?&lt;/head&gt;
    &lt;p&gt;Introduction to Lume - the macOS VM CLI and framework&lt;/p&gt;
    &lt;p&gt;Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon.&lt;/p&gt;
    &lt;p&gt;MIT License&lt;/p&gt;
    &lt;p&gt;Lume is open-source and MIT licensed. If you find it useful, we'd appreciate a star on GitHub!&lt;/p&gt;
    &lt;p&gt;Cloud macOS Sandboxes&lt;/p&gt;
    &lt;p&gt;We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads. Book a demo if you're interested.&lt;/p&gt;
    &lt;p&gt;A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;You can use Lume directly via CLI, or run &lt;code&gt;lume serve&lt;/code&gt; to expose an HTTP API for programmatic access. The Computer SDK uses this API to automate macOS interactions.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Lume is a thin layer over Apple's Virtualization Framework, which provides hardware-accelerated virtualization on Apple Silicon. This gives you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native speed ‚Äî CPU instructions execute directly via hardware virtualization&lt;/item&gt;
      &lt;item&gt;Paravirtualized graphics ‚Äî Basic GPU support via Apple's virtualization layer (limited to GPU Family 5)&lt;/item&gt;
      &lt;item&gt;Efficient storage ‚Äî Sparse disk files only consume actual usage, not allocated size&lt;/item&gt;
      &lt;item&gt;Rosetta 2 support ‚Äî Run x86 Linux binaries in ARM Linux VMs&lt;/item&gt;
      &lt;item&gt;Automated golden images ‚Äî Go from IPSW to fully configured macOS VM without manual intervention&lt;/item&gt;
      &lt;item&gt;Registry support ‚Äî Pull and push VM images from GHCR or GCS registries&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;When to use Lume&lt;/head&gt;
    &lt;p&gt;Testing across macOS versions ‚Äî Spin up a VM with a specific macOS version, test your software, tear it down. No need to maintain multiple physical machines.&lt;/p&gt;
    &lt;p&gt;Automating macOS tasks ‚Äî Combine Lume with Unattended Setup to create pre-configured VMs. The setup automation uses VNC and OCR to click through the Setup Assistant without manual intervention.&lt;/p&gt;
    &lt;p&gt;Running CI/CD locally ‚Äî Test your macOS builds in isolated VMs before pushing to remote CI. The &lt;code&gt;--no-display&lt;/code&gt; flag runs VMs headlessly.&lt;/p&gt;
    &lt;p&gt;Sandboxing risky operations ‚Äî Need to test untrusted software or destructive scripts? Run them in a VM, then delete it. Clone a known-good VM to reset to a clean state instantly.&lt;/p&gt;
    &lt;p&gt;Building AI agents ‚Äî Lume powers the Cua Computer SDK, providing VMs that AI models can interact with through screenshots and input simulation.&lt;/p&gt;
    &lt;p&gt;Used by Anthropic&lt;/p&gt;
    &lt;p&gt;Apple's Virtualization Framework‚Äîthe same technology Lume is built on‚Äîpowers Claude Cowork, Anthropic's sandboxed environment for Claude Code. It downloads a Linux root filesystem and boots it in an isolated VM where Claude can safely execute commands without access to your broader system.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Lume doesn't do&lt;/head&gt;
    &lt;p&gt;Lume requires Apple Silicon‚Äîit won't work on Intel Macs or other platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Ready to try it? Install Lume and create your first VM in the Quickstart.&lt;/p&gt;
    &lt;p&gt;Was this page helpful?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cua.ai/docs/lume/guide/getting-started/introduction"/><published>2026-01-18T17:53:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670279</id><title>Flux 2 Klein pure C inference</title><updated>2026-01-18T22:10:14.928627+00:00</updated><content>&lt;doc fingerprint="2e5c8a38193e3888"&gt;
  &lt;main&gt;
    &lt;p&gt;This program generates images from text prompts (and optionally from other images) using the FLUX.2-klein-4B model from Black Forest Labs. It can be used as a library as well, and is implemented entirely in C, with zero external dependencies beyond the C standard library. MPS and BLAS acceleration are optional but recommended.&lt;/p&gt;
    &lt;p&gt;I (the human here, Salvatore) wanted to test code generation with a more ambitious task, over the weekend. This is the result. It is my first open source project where I wrote zero lines of code. I believe that inference systems not using the Python stack (which I do not appreciate) are a way to free open models usage and make AI more accessible. There is already a project doing the inference of diffusion models in C / C++ that supports multiple models, and is based on GGML. I wanted to see if, with the assistance of modern AI, I could reproduce this work in a more concise way, from scratch, in a weekend. Looks like it is possible.&lt;/p&gt;
    &lt;p&gt;This code base was written with Claude Code, using the Claude Max plan, the small one of ~80 euros per month. I almost reached the limits but this plan was definitely sufficient for such a large task, which was surprising. In order to simplify the usage of this software, no quantization is used, nor do you need to convert the model. It runs directly with the safetensors model as input, using floats.&lt;/p&gt;
    &lt;p&gt;Even if the code was generated using AI, my help in steering towards the right design, implementation choices, and correctness has been vital during the development. I learned quite a few things about working with non trivial projects and AI.&lt;/p&gt;
    &lt;code&gt;# Build (choose your backend)
make mps       # Apple Silicon (fastest)
# or: make blas    # Intel Mac / Linux with OpenBLAS
# or: make generic # Pure C, no dependencies

# Download the model (~16GB)
pip install huggingface_hub
python download_model.py

# Generate an image
./flux -d flux-klein-model -p "A woman wearing sunglasses" -o output.png&lt;/code&gt;
    &lt;p&gt;That's it. No Python runtime, no PyTorch, no CUDA toolkit required at inference time.&lt;/p&gt;
    &lt;p&gt;Generated with: &lt;code&gt;./flux -d flux-klein-model -p "A picture of a woman in 1960 America. Sunglasses. ASA 400 film. Black and White." -W 250 -H 250 -o /tmp/woman.png&lt;/code&gt;, and later processed with image to image generation via &lt;code&gt;./flux -d flux-klein-model -i /tmp/woman.png -o /tmp/woman2.png -p "oil painting of woman with sunglasses" -v -H 256 -W 256&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero dependencies: Pure C implementation, works standalone. BLAS optional for ~30x speedup (Apple Accelerate on macOS, OpenBLAS on Linux)&lt;/item&gt;
      &lt;item&gt;Metal GPU acceleration: Automatic on Apple Silicon Macs&lt;/item&gt;
      &lt;item&gt;Text-to-image: Generate images from text prompts&lt;/item&gt;
      &lt;item&gt;Image-to-image: Transform existing images guided by prompts&lt;/item&gt;
      &lt;item&gt;Integrated text encoder: Qwen3-4B encoder built-in, no external embedding computation needed&lt;/item&gt;
      &lt;item&gt;Memory efficient: Automatic encoder release after encoding (~8GB freed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "A fluffy orange cat sitting on a windowsill" -o cat.png&lt;/code&gt;
    &lt;p&gt;Transform an existing image based on a prompt:&lt;/p&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "oil painting style" -i photo.png -o painting.png -t 0.7&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-t&lt;/code&gt; (strength) parameter controls how much the image changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.0&lt;/code&gt;= no change (output equals input)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1.0&lt;/code&gt;= full generation (input only provides composition hint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;= good balance for style transfer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required:&lt;/p&gt;
    &lt;code&gt;-d, --dir PATH        Path to model directory
-p, --prompt TEXT     Text prompt for generation
-o, --output PATH     Output image path (.png or .ppm)
&lt;/code&gt;
    &lt;p&gt;Generation options:&lt;/p&gt;
    &lt;code&gt;-W, --width N         Output width in pixels (default: 256)
-H, --height N        Output height in pixels (default: 256)
-s, --steps N         Sampling steps (default: 4)
-S, --seed N          Random seed for reproducibility
&lt;/code&gt;
    &lt;p&gt;Image-to-image options:&lt;/p&gt;
    &lt;code&gt;-i, --input PATH      Input image for img2img
-t, --strength N      How much to change the image, 0.0-1.0 (default: 0.75)
&lt;/code&gt;
    &lt;p&gt;Output options:&lt;/p&gt;
    &lt;code&gt;-q, --quiet           Silent mode, no output
-v, --verbose         Show detailed config and timing info
&lt;/code&gt;
    &lt;p&gt;Other options:&lt;/p&gt;
    &lt;code&gt;-e, --embeddings PATH Load pre-computed text embeddings (advanced)
-h, --help            Show help
&lt;/code&gt;
    &lt;p&gt;The seed is always printed to stderr, even when random:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png
Seed: 1705612345
out.png
&lt;/code&gt;
    &lt;p&gt;To reproduce the same image, use the printed seed:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png -S 1705612345
&lt;/code&gt;
    &lt;p&gt;Choose a backend when building:&lt;/p&gt;
    &lt;code&gt;make            # Show available backends
make generic    # Pure C, no dependencies (slow)
make blas       # BLAS acceleration (~30x faster)
make mps        # Apple Silicon Metal GPU (fastest, macOS only)&lt;/code&gt;
    &lt;p&gt;Recommended:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS Apple Silicon: &lt;code&gt;make mps&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macOS Intel: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux with OpenBLAS: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux without OpenBLAS: &lt;code&gt;make generic&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;code&gt;make blas&lt;/code&gt; on Linux, install OpenBLAS first:&lt;/p&gt;
    &lt;code&gt;# Ubuntu/Debian
sudo apt install libopenblas-dev

# Fedora
sudo dnf install openblas-devel&lt;/code&gt;
    &lt;p&gt;Other targets:&lt;/p&gt;
    &lt;code&gt;make clean      # Clean build artifacts
make info       # Show available backends for this platform
make test       # Run reference image test&lt;/code&gt;
    &lt;p&gt;The model weights are downloaded from HuggingFace:&lt;/p&gt;
    &lt;code&gt;pip install huggingface_hub
python download_model.py&lt;/code&gt;
    &lt;p&gt;This downloads approximately 16GB to &lt;code&gt;./flux-klein-model&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VAE (~300MB)&lt;/item&gt;
      &lt;item&gt;Transformer (~4GB)&lt;/item&gt;
      &lt;item&gt;Qwen3-4B Text Encoder (~8GB)&lt;/item&gt;
      &lt;item&gt;Tokenizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FLUX.2-klein-4B is a rectified flow transformer optimized for fast inference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Architecture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Transformer&lt;/cell&gt;
        &lt;cell&gt;5 double blocks + 20 single blocks, 3072 hidden dim, 24 attention heads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;VAE&lt;/cell&gt;
        &lt;cell&gt;AutoencoderKL, 128 latent channels, 8x spatial compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Text Encoder&lt;/cell&gt;
        &lt;cell&gt;Qwen3-4B, 36 layers, 2560 hidden dim&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Inference steps: This is a distilled model that produces good results with exactly 4 sampling steps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Phase&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text encoding&lt;/cell&gt;
        &lt;cell&gt;~8GB (encoder weights)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Diffusion&lt;/cell&gt;
        &lt;cell&gt;~8GB (transformer ~4GB + VAE ~300MB + activations)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Peak&lt;/cell&gt;
        &lt;cell&gt;~16GB (if encoder not released)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The text encoder is automatically released after encoding, reducing peak memory during diffusion. If you generate multiple images with different prompts, the encoder reloads automatically.&lt;/p&gt;
    &lt;p&gt;Maximum resolution: 1024x1024 pixels. Higher resolutions require prohibitive memory for the attention mechanisms.&lt;/p&gt;
    &lt;p&gt;Minimum resolution: 64x64 pixels.&lt;/p&gt;
    &lt;p&gt;Dimensions should be multiples of 16 (the VAE downsampling factor).&lt;/p&gt;
    &lt;p&gt;The library can be integrated into your own C/C++ projects. Link against &lt;code&gt;libflux.a&lt;/code&gt; and include &lt;code&gt;flux.h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's a complete program that generates an image from a text prompt:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    /* Load the model. This loads VAE, transformer, and text encoder. */
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) {
        fprintf(stderr, "Failed to load model: %s\n", flux_get_error());
        return 1;
    }

    /* Configure generation parameters. Start with defaults and customize. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.width = 512;
    params.height = 512;
    params.seed = 42;  /* Use -1 for random seed */

    /* Generate the image. This handles text encoding, diffusion, and VAE decode. */
    flux_image *img = flux_generate(ctx, "A fluffy orange cat in a sunbeam", &amp;amp;params);
    if (!img) {
        fprintf(stderr, "Generation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    /* Save to file. Format is determined by extension (.png or .ppm). */
    flux_image_save(img, "cat.png");
    printf("Saved cat.png (%dx%d)\n", img-&amp;gt;width, img-&amp;gt;height);

    /* Clean up */
    flux_image_free(img);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Compile with:&lt;/p&gt;
    &lt;code&gt;gcc -o myapp myapp.c -L. -lflux -lm -framework Accelerate  # macOS
gcc -o myapp myapp.c -L. -lflux -lm -lopenblas              # Linux&lt;/code&gt;
    &lt;p&gt;Transform an existing image guided by a text prompt. The &lt;code&gt;strength&lt;/code&gt; parameter controls how much the image changes:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) return 1;

    /* Load the input image */
    flux_image *photo = flux_image_load("photo.png");
    if (!photo) {
        fprintf(stderr, "Failed to load image\n");
        flux_free(ctx);
        return 1;
    }

    /* Set up parameters. Output size defaults to input size. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.strength = 0.7;  /* 0.0 = no change, 1.0 = full regeneration */
    params.seed = 123;

    /* Transform the image */
    flux_image *painting = flux_img2img(ctx, "oil painting, impressionist style",
                                         photo, &amp;amp;params);
    flux_image_free(photo);  /* Done with input */

    if (!painting) {
        fprintf(stderr, "Transformation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    flux_image_save(painting, "painting.png");
    printf("Saved painting.png\n");

    flux_image_free(painting);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Strength values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.3&lt;/code&gt;- Subtle style transfer, preserves most details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.5&lt;/code&gt;- Moderate transformation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;- Strong transformation, good for style transfer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.9&lt;/code&gt;- Almost complete regeneration, keeps only composition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When generating multiple images with different seeds but the same prompt, you can avoid reloading the text encoder:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("flux-klein-model");
flux_params params = FLUX_PARAMS_DEFAULT;
params.width = 256;
params.height = 256;

/* Generate 5 variations with different seeds */
for (int i = 0; i &amp;lt; 5; i++) {
    flux_set_seed(1000 + i);

    flux_image *img = flux_generate(ctx, "A mountain landscape at sunset", &amp;amp;params);

    char filename[64];
    snprintf(filename, sizeof(filename), "landscape_%d.png", i);
    flux_image_save(img, filename);
    flux_image_free(img);
}

flux_free(ctx);&lt;/code&gt;
    &lt;p&gt;Note: The text encoder (~8GB) is automatically released after the first generation to save memory. It reloads automatically if you use a different prompt.&lt;/p&gt;
    &lt;p&gt;All functions that can fail return NULL on error. Use &lt;code&gt;flux_get_error()&lt;/code&gt; to get a description:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("nonexistent-model");
if (!ctx) {
    fprintf(stderr, "Error: %s\n", flux_get_error());
    /* Prints something like: "Failed to load VAE - cannot generate images" */
    return 1;
}&lt;/code&gt;
    &lt;p&gt;Core functions:&lt;/p&gt;
    &lt;code&gt;flux_ctx *flux_load_dir(const char *model_dir);   /* Load model, returns NULL on error */
void flux_free(flux_ctx *ctx);                     /* Free all resources */

flux_image *flux_generate(flux_ctx *ctx, const char *prompt, const flux_params *params);
flux_image *flux_img2img(flux_ctx *ctx, const char *prompt, const flux_image *input,
                          const flux_params *params);&lt;/code&gt;
    &lt;p&gt;Image handling:&lt;/p&gt;
    &lt;code&gt;flux_image *flux_image_load(const char *path);     /* Load PNG or PPM */
int flux_image_save(const flux_image *img, const char *path);  /* 0=success, -1=error */
flux_image *flux_image_resize(const flux_image *img, int new_w, int new_h);
void flux_image_free(flux_image *img);&lt;/code&gt;
    &lt;p&gt;Utilities:&lt;/p&gt;
    &lt;code&gt;void flux_set_seed(int64_t seed);                  /* Set RNG seed for reproducibility */
const char *flux_get_error(void);                  /* Get last error message */
void flux_release_text_encoder(flux_ctx *ctx);     /* Manually free ~8GB (optional) */&lt;/code&gt;
    &lt;code&gt;typedef struct {
    int width;              /* Output width in pixels (default: 256) */
    int height;             /* Output height in pixels (default: 256) */
    int num_steps;          /* Denoising steps, use 4 for klein (default: 4) */
    float guidance_scale;   /* CFG scale, use 1.0 for klein (default: 1.0) */
    int64_t seed;           /* Random seed, -1 for random (default: -1) */
    float strength;         /* img2img only: 0.0-1.0 (default: 0.75) */
} flux_params;

/* Initialize with sensible defaults */
#define FLUX_PARAMS_DEFAULT { 256, 256, 4, 1.0f, -1, 0.75f }&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/antirez/flux2.c"/><published>2026-01-18T18:01:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670290</id><title>Show HN: HTTP:COLON ‚Äì A quick HTTP header/directive inspector and reference</title><updated>2026-01-18T22:10:14.728026+00:00</updated><content>&lt;doc fingerprint="b5099b1f4f445c15"&gt;
  &lt;main&gt;
    &lt;p&gt;HTTP:DOCS&lt;/p&gt;
    &lt;head rend="h1"&gt;What are HTTP Headers?&lt;/head&gt;
    &lt;p&gt;HTTP headers are a fundamental component of the HTTP protocol, which is the backbone of the internet. These headers contain important information about the request and response, such as content type, caching instructions, authentication tokens, and more. By understanding how to read and manipulate HTTP headers, developers can optimize their web applications for performance, security, and functionality. Moreover, HTTP headers play a critical role in API integrations, allowing developers to communicate with external services and systems. In short, HTTP headers are an essential tool in the web developer's arsenal, and any developer serious about building high-quality web applications should invest the time to learn and master them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://httpcolon.dev/"/><published>2026-01-18T18:03:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671174</id><title>Breaking the Zimmermann Telegram (2018)</title><updated>2026-01-18T22:10:14.531033+00:00</updated><content>&lt;doc fingerprint="f7b4d05db0a2feeb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Breaking the Zimmermann Telegram&lt;/head&gt;
    &lt;p&gt;Just over one hundred years ago, the British carried out one of the most audacious acts in the history of codebreaking. So audacious, in fact, that they had to convince the Americans they hadn‚Äôt done it at all‚Ä¶&lt;/p&gt;
    &lt;head rend="h3"&gt;The Admiralty&lt;/head&gt;
    &lt;p&gt;Running, Lieutenant Nigel De Grey decided as he narrowly avoided colliding with another paper-laden trolley, was not something that the corridors of the Admiralty Old Building had been designed for.&lt;/p&gt;
    &lt;p&gt;Nor was it something that the Royal Navy approved of from its junior officers, apparently. This was clear from the angry shouts of the people he dodged as he raced down the building‚Äôs narrow back corridors.&lt;/p&gt;
    &lt;p&gt;Right now though De Grey didn‚Äôt care. It was 17th January 1917 and Europe had been locked in a bloody stalemate for almost three years, but the scrap of paper he held in his hand might well change the outcome of the Great War.&lt;/p&gt;
    &lt;head rend="h3"&gt;The dormouse&lt;/head&gt;
    &lt;p&gt;Although he now spent his days in London, was more than familiar with the horrors happening on the Western Front. The son of a reverend, De Grey had worked at a publishing company before the war where he‚Äôd been nicknamed ‚Äúdormouse‚Äù by his colleagues due to his shyness. At the same time he had been a member of the Royal Naval Reserve. He was called up early and, as a result, had been in combat in Belgium during the early days of the war.&lt;/p&gt;
    &lt;p&gt;In 1915, however, De Grey‚Äôs fluency in both German and French, his quick mind and his love of a good puzzle had been noticed by the powers that be. Without warning, he was ordered back to London to join a mysterious Naval department known as ‚ÄòRoom 40.‚Äô&lt;/p&gt;
    &lt;head rend="h3"&gt;The Room&lt;/head&gt;
    &lt;p&gt;Room 40 had only existed for a few short months when De Grey joined, although plans had existed for such an organisation should war break out since 1911. That it existed at all was because the world of warfare ‚Äî or more importantly the way that people communicated in war ‚Äî was changing. Radio, telegraph and telephony were now viable forms of communication, and so were also potentially vital sources of intelligence too. The arrival of war brought with it a myriad of opportunities for such intelligence gathering. In August 1914, for example, a Russian attach√© gave the Admiralty a copy of a German codebook taken from the beached German cruiser SS Magdeburg. In a spare room (you can guess the number) at the back of the old Admiralty building, a small group of officers and civilians were given a new job ‚Äî break and read German communications. De Grey joined soon after. It was here that he discovered what he would later describe as his ‚Äòhigher calling‚Äô ‚Äî he became a codebreaker.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Research Group&lt;/head&gt;
    &lt;p&gt;In fact, De Grey was soon assigned to an even smaller, more mysterious team within Room 40 ‚Äî the ‚ÄòResearch Group‚Äô. A secret department within a secret department, its innocuous name was cover for work which was anything but. For whilst trying to read your enemy‚Äôs message traffic was considered acceptable (if unsporting) behaviour during wartime, doing the same thing to neutral powers was seriously frowned upon. Yet this was exactly what the Research Group had been created to do.&lt;/p&gt;
    &lt;p&gt;That such an opportunity existed was due to the way transatlantic communication worked at the time. Radio was getting more advanced and powerful, but it was not yet good enough to provide worldwide coverage. This meant that most diplomatic traffic still circulated in telegraph form, sent across vast distances by cable.&lt;/p&gt;
    &lt;p&gt;For the Entente powers in the First World War this wasn‚Äôt really a problem. Britain and France were both at the height of their imperial power and their telegraph networks spanned the globe. Germany, however, did not have that luxury. Its cables ‚Äî particularly those stretching across the Atlantic ‚Äî lay well outside its zone of military control.&lt;/p&gt;
    &lt;p&gt;This situation was not lost on the Entente. Almost as soon as war was declared, much of Germany‚Äôs overseas cable network went dark. It didn‚Äôt take an expert to know why ‚Äî the Royal Navy had cut most of the cables, and Germany realised those that which remained suspiciously uncut should probably be considered compromised.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Neutral&lt;/head&gt;
    &lt;p&gt;Robbed of the ability to communicate with their embassies throughout the world, the Germans protested. They complained that this was as an outrageous violation of diplomatic protocol ‚Äî even during war.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, their complaints fell on deaf ears within the Entente itself. Luckily for the Germans, however, there was one major power who agreed with them ‚Äî the United States of America. America was staunchly neutral at the time, the only ‚Äògreat power‚Äô not involved in the war and its President, Woodrow Wilson, believed that if the the US were to have any hope of mediating an end to the war in Europe, then German diplomats in the US and beyond needed to be able to talk freely to their government.&lt;/p&gt;
    &lt;p&gt;It was a noble goal, and so to further it the US State Department granted Germany permission to use the American transatlantic cable, via Copenhagen, for diplomatic telegraph traffic.&lt;/p&gt;
    &lt;p&gt;Both Germany and the US believed these messages to be entirely secure. German intelligence had sufficiently penetrated the State Department to know that the Americans weren‚Äôt interested in breaking Germany‚Äôs codes. More importantly though, both powers believed that the British would not tap into US traffic ‚Äî to do so would cause an enormous diplomatic incident. Not that it mattered anyway ‚Äî even if they were tempted, the Germans thought they were safe. They understood that the US cable was entirely submarine, and thus safe from tampering.&lt;/p&gt;
    &lt;p&gt;The Germans were right on the first account, Unfortunately were wrong on both the latter.&lt;/p&gt;
    &lt;head rend="h3"&gt;The interception&lt;/head&gt;
    &lt;p&gt;Whatever the thoughts of the British Foreign Office might be, the Admiralty had its own opinions on what was, and wasn‚Äôt, fair game when it came to intelligence gathering. If the Americans were going to transmit coded German messages for them, then as far as Captain Reginald ‚ÄúBlinker‚Äù Hall, the Director of Naval Intelligence and ultimate head of Room 40 was concerned, American diplomatic traffic was absolutely fair game.&lt;/p&gt;
    &lt;p&gt;Again, had the Germans been correct about the American submarine cable then this still wouldn‚Äôt have been a problem, but they weren‚Äôt. In fact, US telegraph traffic came ashore on Britain via a relay station just north of Newcastle and then travelled across the country to Cornwall. From there it was then transmitted onward to Washington. This presented multiple opportunities for the messages to be intercepted by the British, and the Research Group was born. Every day they would receive copies of the traffic sent across the line. Their job was to crack the codes and read every diplomatic message the Americans and the Germans sent.&lt;/p&gt;
    &lt;p&gt;It was a decrypt of one of those diplomatic messages that De Grey now clutched in his hand as he raced down the Admiralty‚Äôs narrow oak halls. Sent the night before, it was pure luck that it had been decrypted so quickly. It was only a short message, which had been sent by Arthur Zimmermann, the German Foreign Minister, to the German ambassador in Mexico. As such, it was considered low-level diplomatic traffic and had been marked as low-priority for breaking and decryption. By chance, however, when it had arrived at Room 40 the pneumatic Tube system had dumped it on the desk of one of the department‚Äôs other rising stars, Alfred Dillwyn Knox.&lt;/p&gt;
    &lt;head rend="h3"&gt;The genius&lt;/head&gt;
    &lt;p&gt;A Classics scholar and papyrologist at Cambridge before the war, ‚ÄúDilly‚Äù had joined Room 40 in 1914. There he swiftly demonstrated an unquestionable genius for codebreaking. Indeed Dilly Knox remains one of the greatest codebreakers Britain has ever produced. After the end of the First World War, he would become one of the founding fathers of the Government Code and Cypher School ‚Äî GCHQ, which remains Britain‚Äôs primary cryptographic line of defence to this day. Nor does his influence end there. In 1925 in Vienna, he became the first British Intelligence officer to acquire an Enigma machine. Then in Warsaw, in 1938, it was to Dilly that the Poles were prepared to turnover their own Enigma codebreaking efforts. It was also Dilly who oversaw the transfer of that information ‚Äî and a number of Polish codebreakers who managed to escape the Nazi invasion of Poland ‚Äî to a new codebreaking institution he had helped set up back in Britain ‚Äî Bletchley Park.&lt;/p&gt;
    &lt;p&gt;What many people don‚Äôt realise is that ‚ÄòEnigma‚Äô wasn‚Äôt one code ‚Äî it was many. The most complex of these (thanks to an extra rotor on the machine) was the German Naval code. The honour for breaking that rightly belongs to Alan Turing, but he was not the only man working on Enigmas. Dilly himself broke not one, but three of the other key codes ‚Äî those of Spanish Intelligence, the German Army and the Italian Navy. To take full advantage of these, he then fought for the right to form a unique codebreaking outfit at Bletchley ‚Äî ‚ÄúIntelligence Service Knox‚Äù (ISK). Under Knox, ISK became the only codebreaking department at Bletchley entirely staffed by women.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ‚ÄòDilly Girls‚Äô&lt;/head&gt;
    &lt;p&gt;Dilly had spotted that whilst women were considered a vital cog in the Bletchley codebreaking machine, they were almost exclusively confined to ‚Äòsupport‚Äô roles ‚Äî Bombe operators, transcribers, translators and beyond. Dilly saw this as a waste of good minds, based solely on flawed preconceptions about gender, at a time when Britain needed good minds the most.&lt;/p&gt;
    &lt;p&gt;The formation of ISK was not without controversy. Rumours soon circulated that Dilly had wandered round the huts pointing at the prettiest girls for his ‚Äòeastern harem‚Äô, and they were soon being referred to by the derogatory nickname ‚ÄòDilly‚Äôs Girls‚Äô.&lt;/p&gt;
    &lt;p&gt;Nothing could have been further from the truth. Once permission had been given to form ISK, Dilly had immediately approached the head of the Women‚Äôs section, who interviewed all of the female staff sent to Bletchley and managed them once they‚Äôd arrived. He asked her to reassign those she considered most wasted in their current roles to ISK and the results soon spoke for themselves. ISK became one of the most successful codebreaking teams at Bletchley, contributing critical decryptions that would help win the naval war in the Mediterranean and ensure the success of the D-Day landings. Indeed ISK‚Äôs contributions outlived Dilly himself (who died suddenly of cancer in 1943), with the department proudly adopting and subverting the ‚ÄòDilly‚Äôs Girls‚Äô moniker until the end of the war.&lt;/p&gt;
    &lt;head rend="h3"&gt;The revelation&lt;/head&gt;
    &lt;p&gt;In 1917, of course, all this was in the future. Right now Dilly‚Äôs efforts were focused firmly on finding new ways into German naval codes. Unusually, Dilly was not particularly mathematical. What he was good at, however, was spotting patterns and looking at things from unusual angles, in part the result of his experience rebuilding and translating Greek manuscripts from mere fragments before the war. He also had a near-uncanny ability to put himself in the mind of the people at the other end of the line. In 1915 he had broken the German Admiralty‚Äôs flag code by spotting ‚Äî and exploiting ‚Äî one particular German telegraph operator‚Äôs love of romantic poetry. These efforts had put Dilly on the Research Group‚Äôs radar, and though he was not officially a member of the team he had been quietly called in to help with their work from time to time.&lt;/p&gt;
    &lt;p&gt;Indeed this was perhaps why this particular intercept had dropped from the Admiralty‚Äôs pneumatic tube system onto Dilly‚Äôs desk on the night of the 16th January. With the rest of the Research Group busy that night, it might have been that Dilly was seen as an overflow for the low-level traffic. Whatever the reason, something about this particular message caught Dilly‚Äôs eye. Rather than leaving it at the bottom of his pile, he worked on trying to break it right through the night.&lt;/p&gt;
    &lt;p&gt;By morning, he had begun to make inroads into the telegram. Dilly didn‚Äôt speak German, but he recognised words such as ‚ÄúSubmarine‚Äù, ‚ÄúMexico‚Äù and ‚ÄúArizona‚Äù. He became increasingly convinced that the telegram was important and so, when De Grey arrived at work the next morning, Dilly roped him in to help. The two men had worked as a decryption team before with considerable success ‚Äî De Grey‚Äôs fluent German and experience as an editor meshing well with Dilly‚Äôs own skills. Together they worked on the telegram right through the morning. The more they decrypted, the more both men became astonished at what they were reading ‚Äî indeed they could barely believe it. By lunchtime, however, they had decrypted enough to know that they weren‚Äôt wrong. They agreed the Captain needed to see this immediately.&lt;/p&gt;
    &lt;p&gt;Normally athletics wouldn‚Äôt have been necessary. Officially, everyone in Room 40 reported to Sir Alfred Ewing, who himself then reported to ‚ÄúBlinker‚Äù Hall. Sometime before, however, the Captain himself had quietly pulled De Grey and the other men of the Research Group aside. Ewing, Hall told them, was a bit of a chatterbox in the corridors of power and Hall didn‚Äôt trust him to keep a really big secret. If the Research Group‚Äôs work ever yielded something particularly sensitive or explosive, then they were ordered to bypass Ewing completely and only reveal what they had found to Hall himself. So this was where De Grey was headed.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Captain&lt;/head&gt;
    &lt;p&gt;De Grey entered the Captain‚Äôs outer office at a sprint, bursting into the Hall‚Äôs office before his personal secretary could object. Luckily, the Captain was in.&lt;/p&gt;
    &lt;p&gt;‚ÄúDo you want to bring America into the war sir?‚Äù De Grey burst out breathlessly.&lt;/p&gt;
    &lt;p&gt;‚ÄúYes, why?‚Äù Replied the slightly bemused Hall. He had long since stopped expecting any semblance of military decorum or normality from his codebreakers.&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôve got a telegram that will bring them in if you give it to them.‚Äù De Grey blurted out, thrusting the results of his own and Dilly‚Äôs efforts towards the Captain.&lt;/p&gt;
    &lt;p&gt;Hall took the decrypt and read it, silently, as De Grey explained who it was from, for and how they had broken it. For the very first time, a senior member of British Intelligence held in his hands a copy of what would become known to history as the ‚ÄòZimmermann Telegram.‚Äô&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We intend to begin on the first of February unrestricted submarine warfare. We shall endeavour in spite of this to keep the United States of America neutral. In the event of this not succeeding, we make Mexico a proposal of alliance on the following basis: make war together, make peace together, generous financial support and an understanding on our part that Mexico is to reconquer the lost territory in Texas, New Mexico, and Arizona. The settlement in detail is left to you. You will inform the President of the above most secretly as soon as the outbreak of war with the United States of America is certain and add the suggestion that he should, on his own initiative, invite Japan to immediate adherence and at the same time mediate between Japan and ourselves. Please call the President‚Äôs attention to the fact that the ruthless employment of our submarines now offers the prospect of compelling England in a few months to make peace.&lt;/p&gt;
      &lt;p&gt;Signed, ZIMMERMANN&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;The telegram&lt;/head&gt;
    &lt;p&gt;Hall listened patiently as De Grey outlined both what they new for certain and what were guesses at length. By the time De Grey had finished, Hall was happy to accept what he was saying was true. At this stage, they had not fully decrypted the message (the above is the full, final text), but it was more than enough for Captain Hall to grasp that De Grey wasn‚Äôt exaggerating. This wasn‚Äôt just confirmation that Germany were preparing to conduct unrestricted submarine warfare ‚Äî it was incitement to Mexico to declare war on the United States.&lt;/p&gt;
    &lt;p&gt;Whilst Zimmermann has been cast in history as something of a naive operator, the truth is anything but. Zimmermann was one of the architects of Germany‚Äôs successful policy of funnelling money and support to rebellions and rivals of the Entente powers. This had caused enormous problems for them, forcing them to spread their forces thinner across the world. Indeed at that very moment this approach was yielding enormous results in Russia, who would be forced out of the war entirely before the year was out.&lt;/p&gt;
    &lt;p&gt;Zimmermann‚Äôs telegram was intended to lay the groundwork for the same approach to be taken across the Atlantic, in the event that the declaration of unrestricted submarine warfare be enough to tip the balance of US government into intervening.&lt;/p&gt;
    &lt;p&gt;Not only were the Germans suggesting Mexico declare war on the United States (with German backing) but, even more incredibly, they were using the using US State Department‚Äôs own telegraph network to do it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The problem&lt;/head&gt;
    &lt;p&gt;If unrestricted submarine warfare itself didn‚Äôt drag the US into the war, then Hall realised that De Grey and Dilly were right ‚Äî this telegram (and the outrageous way it had been sent) could well be enough to do so.&lt;/p&gt;
    &lt;p&gt;Hall, however, was fully aware that he had a problem. Indeed the mother of all intelligence problems. One of the regular problems with good intelligence was working out how to use it without ‚Äòburning‚Äô the source ‚Äî because revealing it might inadvertently reveal to the enemy how you got it, cutting you off from all future intelligence by the same method.&lt;/p&gt;
    &lt;p&gt;Hall‚Äôs problem here was even worse. Not only would revealing the existence of the telegram burn the source, as the Germans would know the US cable was compromised, but that source was, effectively, the US State Department itself.&lt;/p&gt;
    &lt;p&gt;‚ÄúHello chaps, we‚Äôve been reading your mail, and there‚Äôs some things in here you really should see‚Ä¶‚Äù Was a line that was hardly likely to go over well with the Americans. Indeed they may be more than outraged enough about that to eclipse any horror at the telegram itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hall‚Äôs solution&lt;/head&gt;
    &lt;p&gt;Recognising the explosiveness of the situation, Hall and De Grey briefly discussed their options. Realising that whatever he did, he should probably lock things down until they had a plan.&lt;/p&gt;
    &lt;p&gt;Claude Serocold, Hall‚Äôs personal assistant was inducted into the secret and the men then pitched around more ideas as to how they could get the telegram into the hands of the Americans without blowing the source. In the end, it was Hall himself who had the brainwave that led to the solution.&lt;/p&gt;
    &lt;p&gt;Looking at the intercept, he realised that although the final destination of the telegram was the German Ambassador in Mexico, it hadn‚Äôt been sent to him directly. It was routed via Johann Heinrich von Bernstorff, the German Ambassador to the US. Although the British didn‚Äôt know it at the time, this was because the arrangement between the US State Department and the German Foreign Office was that they could send diplomatic communications down the main US cable to Washington but no further. At that point, the Germans would have to make their own arrangements for onward transmission.&lt;/p&gt;
    &lt;p&gt;Whatever the reasons, Hall realised that this presented an opportunity. Von Bernstorff would have to retransmit the message at the American end. The German Embassy, Room 40 knew, had a commercial relationship with Western Union in the United States, so this was likely how von Bernstorff would do it.&lt;/p&gt;
    &lt;p&gt;Room 40 also knew that he would also have to decrypt and then re-encrypt the message before doing so, as the Germans never used their own, high-level codes on commercial networks. Doing so risked opening them up too much to codebreaking efforts. Based on previous experience, the men posited that the whole process of receipt in New York, handover from the State Department to the Germans, decryption, re-encryption and transmission over Western Union would take about five days.&lt;/p&gt;
    &lt;p&gt;Hall realised this whole process offered an opportunity they could exploit. The Western Union message would be in a lower code, transcribed by the Germans themselves. If they could get hold of that, at the Mexican end, then they could claim this was the source instead.&lt;/p&gt;
    &lt;p&gt;The Mexican connection&lt;/p&gt;
    &lt;p&gt;Until now, Room 40 had generally ignored the Western Union traffic as a potential source of high-value intelligence. Any kind of operation across the Atlantic would have involved not just stepping on American toes but smashing a large boot down on them repeatedly. Given the perceived low value of the traffic, it simply wasn‚Äôt worth the risk.&lt;/p&gt;
    &lt;p&gt;Hall pointed out though that right now they didn‚Äôt need everything that Germany was sending over Western Union. They didn‚Äôt even need a tap on the line. They just needed a copy of this specific telegram. They knew who it was going to, who it was from and ‚Äî roughly ‚Äî when it was likely to be sent. They just needed someone who could get hold of a copy from the Western Union office in Mexico City, no questions asked.&lt;/p&gt;
    &lt;p&gt;Hall made discreet inquiries with the British Embassy in Mexico. They confirmed that they had a source in the Western Union office in Mexico City ‚Äî a clerk who, for the right price, would occasionally lift telegrams for them from Western Union‚Äôs files. Hall told them what to watch out for and when, although he refused to tell them why. Nonetheless, they agreed that they would try.&lt;/p&gt;
    &lt;p&gt;It was an inspired idea. A few days later, courtesy of the British Embassy in Mexico, a copy of the telegram, lifted directly from the files of the Mexico City office of Western Union, was delivered to Captain Hall‚Äôs desk by the Foreign Office.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ambassador&lt;/head&gt;
    &lt;p&gt;On 19th February 1917, Captain Hall found himself standing in the offices of the US Ambassador to Britain in the heart of London.&lt;/p&gt;
    &lt;p&gt;19 days before, on the exact day indicated in the Zimmermann Telegram had indicated, Germany had begun waging unrestricted submarine warfare in the Atlantic. It had caused outrage and the breaking of diplomatic relations between Germany and the United States. Yet the US had remained neutral.&lt;/p&gt;
    &lt;p&gt;On 5th February ‚Äî two weeks after De Grey and Dilly had first decrypted it ‚Äî ‚ÄòBlinker‚Äô Hall finally informed the British Foreign Office that the Zimmermann Telegram existed.&lt;/p&gt;
    &lt;p&gt;As Hall expected, the Foreign Office demanded to know the source. Hall was able to present them with the Western Union telegram, describing ‚Äî with a straight face ‚Äî how the message had been a ‚Äòlucky intercept‚Äô in Mexico, that had fallen into the hands of the British Embassy. They‚Äôd suspected it was significant, so had passed it on to Room 40, where it had been decrypted.&lt;/p&gt;
    &lt;p&gt;This was actually a lie on both accounts. Whilst it was clearly the same telegram as the one in the original ‚Äúhigh‚Äù code that they had broken, somewhat ironically the ‚Äúlesser‚Äù code that von Bernstorff had used (Diplomatic Code 13040) was one which the British hadn‚Äôt previously bothered trying to break. Luckily, another of the Room 40 codebreakers had spotted that it was similar to another naval code that they had broken elsewhere, and this had led to a partial decryption. Enough, at least, to fill in the gaps left in Dilly‚Äôs work on the original interception and confirm beyond a doubt that they were the same message.&lt;/p&gt;
    &lt;p&gt;On 18th February 1917, the Foreign Office had discreetly informed the US Ambassador, Walter Hines Page of the telegram‚Äôs existence, but Page was naturally suspicious. Whatever the state of US / German relations, he found it hard to believe that such an incredible telegram existed, let alone that the British would somehow have managed to obtain a copy. He told his personal secretary, Edward Bell, that he wanted more proof. Only then would he present this information to President Wilson.&lt;/p&gt;
    &lt;p&gt;This was why Captain Hall was standing in front of Edward Bell in the US Embassy now. He had been dispatched by the Foreign Office to meet with Bell and satisfy the Ambassador‚Äôs demands. The two men chatted cordially and the Captain told Bell the Mexico story and offered up his copy of the Western Union telegram as evidence. Bell agreed that it was compelling, but he still wanted more.&lt;/p&gt;
    &lt;p&gt;‚ÄúI want to see it decrypted. In person.‚Äù Bell told the Captain.&lt;/p&gt;
    &lt;p&gt;Captain Hall smiled and sent for Nigel De Grey.&lt;/p&gt;
    &lt;head rend="h3"&gt;The final bluff&lt;/head&gt;
    &lt;p&gt;De Grey arrived soon after, clutching his notes on Diplomatic Code 13040. Captain Hall introduced him to Edward Bell and, with a relaxed smile, told De Grey what he was to do ‚Äî decrypt the telegram while Bell watched.&lt;/p&gt;
    &lt;p&gt;On his part, De Grey couldn‚Äôt understand why the Captain was so relaxed, because internally De Grey himself was screaming. Hall had made an uncharacteristic mistake ‚Äî he seemed to have forgotten that they hadn‚Äôt solved the Mexican version of the telegram. They only had a partial decrypt, largely based off the naval code it had been a close match for. Worse, De Grey hadn‚Äôt even bothered to write down all of the keys they had discovered in his own notes. There hadn‚Äôt seemed to be much point once they‚Äôd done enough to fill in the gaps on the original.&lt;/p&gt;
    &lt;p&gt;As he began to decode the telegram, under Bell‚Äôs watchful eye, De Grey realised he was going to have to improvise.&lt;/p&gt;
    &lt;p&gt;‚ÄúIf I stopped and fetched another book,‚Äù De Grey said later, ‚Äúhe would suspect at once that we‚Äôd faked it up for his benefit. If I let him see that I was writing it down out of my head, he would not believe me. If he did not believe me, we should fail and lose the greatest opportunity ever presented to us. Several seconds of bloody sweat. Then I bluffed. I showed him all the groups when they had been written in my book and passed quickly over those that were not, writing the words into the copy of the telegram by heart.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúEdward Bell, the most charming man, was thoroughly convinced ‚Äî the more easily I think in that he wanted to be convinced anyhow and regarded the whole thing as black magic.‚Äù&lt;/p&gt;
    &lt;p&gt;On the 20th February 1917, Bell handed over Hall‚Äôs copy of the Zimmermann telegram to Ambassador Page, telling him he agreed it was genuine, and suggesting they get Western Union to confirm that it was genuine. By the end of the month, the company had done so and a copy of Room 40‚Äôs decrypted version was in the hands of the President. On the 28th February 1917, Wilson handed it over to the American press.&lt;/p&gt;
    &lt;head rend="h3"&gt;The result&lt;/head&gt;
    &lt;p&gt;The United States of America declared war on Germany on the 5th April 1917, just over a month after the Zimmermann telegram had been handed over to the US Government. It is possible that unrestricted submarine warfare would have been enough to tip the US into intervention, eventually. The Zimmermann telegram, however, almost certainly made that inevitable. Few documents, in the entire history of information warfare, can be said to have had such an impact world history.&lt;/p&gt;
    &lt;p&gt;For the men of Room 40, it was a spectacular triumph, albeit one that none of the key players could talk about for considerable time to come. Indeed so good was Captain Hall‚Äôs cover story that it remained, for a long time, the official version of events. This suited ‚ÄòBlinker‚Äô very well indeed. The Admiralty continued to read US Diplomatic traffic right up to ‚Äî and indeed beyond ‚Äî the end of the First World War.&lt;/p&gt;
    &lt;p&gt;‚ÄúHe was a perfectly marvellous person‚Äù Edward Bell later said of Captain Hall, ‚Äúbut the coldest-hearted proposition that ever was ‚Äî he‚Äôd eat a man‚Äôs heart and hand it back to him.‚Äù&lt;/p&gt;
    &lt;p&gt;Both Dilly and De Grey were happy to keep the secret. They were codebreakers, and accepted that public acknowledgement rarely came with the job.&lt;/p&gt;
    &lt;p&gt;One of ‚ÄòDilly‚Äôs Girls‚Äô would later recall that, having been told the real story from the man himself at Bletchley, she asked him if either he, or De Grey, had received any kind of recognition.&lt;/p&gt;
    &lt;p&gt;‚ÄúGosh no!‚Äù Dilly replied, with a laugh. ‚ÄúBut I believe Nigel did get an official telling off for running in the corridor!‚Äù&lt;/p&gt;
    &lt;p&gt;Like what I write? Then help me do more of it. Back London Reconnections, my transport site on Patreon. Every little helps tell a story.&lt;/p&gt;
    &lt;p&gt;Want a thorough and detailed account of Room 40 and its impact? Then buy Inside Room 40 by Paul Gannon.&lt;/p&gt;
    &lt;p&gt;Update!!!!&lt;/p&gt;
    &lt;p&gt;To everyone who said they wanted to know more about Dilly and the ‚ÄòDilly Girls‚Äô in WW2 ‚Äî if we reach our Patreon target, then I will write up the remarkable tale of how Dilly, the women of the ISK, Prince Philip, a golfing British Admiral and an amorous Italian Ambassador all played a part in the last, great naval battle in the history of warfare.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://medium.com/lapsed-historian/breaking-the-zimmermann-telegram-b34ed1d73614"/><published>2026-01-18T19:19:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671191</id><title>Evolution Unleashed (2018)</title><updated>2026-01-18T22:10:14.342338+00:00</updated><content>&lt;doc fingerprint="a710924c9c358151"&gt;
  &lt;main&gt;
    &lt;p&gt;When researchers at Emory University in Atlanta trained mice to fear the smell of almonds (by pairing it with electric shocks), they found, to their consternation, that both the children and grandchildren of these mice were spontaneously afraid of the same smell. That is not supposed to happen. Generations of schoolchildren have been taught that the inheritance of acquired characteristics is impossible. A mouse should not be born with something its parents have learned during their lifetimes, any more than a mouse that loses its tail in an accident should give birth to tailless mice.&lt;/p&gt;
    &lt;p&gt;If you are not a biologist, you‚Äôd be forgiven for being confused about the state of evolutionary science. Modern evolutionary biology dates back to a synthesis that emerged around the 1940s-60s, which married Charles Darwin‚Äôs mechanism of natural selection with Gregor Mendel‚Äôs discoveries of how genes are inherited. The traditional, and still dominant, view is that adaptations ‚Äì from the human brain to the peacock‚Äôs tail ‚Äì are fully and satisfactorily explained by natural selection (and subsequent inheritance). Yet as novel ideas flood in from genomics, epigenetics and developmental biology, most evolutionists agree that their field is in flux. Much of the data implies that evolution is more complex than we once assumed.&lt;/p&gt;
    &lt;p&gt;Some evolutionary biologists, myself included, are calling for a broader characterisation of evolutionary theory, known as the extended evolutionary synthesis (EES). A central issue is whether what happens to organisms during their lifetime ‚Äì their development ‚Äì can play important and previously unanticipated roles in evolution. The orthodox view has been that developmental processes are largely irrelevant to evolution, but the EES views them as pivotal. Protagonists with authoritative credentials square up on both sides of this debate, with big-shot professors at Ivy League universities and members of national academies going head-to-head over the mechanisms of evolution. Some people are even starting to wonder if a revolution is on the cards.&lt;/p&gt;
    &lt;p&gt;In his book On Human Nature (1978), the evolutionary biologist Edward O Wilson claimed that human culture is held on a genetic leash. The metaphor was contentious for two reasons. First, as we‚Äôll see, it‚Äôs no less true that culture holds genes on a leash. Second, while there must be a genetic propensity for cultural learning, few cultural differences can be explained by underlying genetic differences.&lt;/p&gt;
    &lt;p&gt;Nonetheless, the phrase has explanatory potential. Imagine a dog-walker (the genes) struggling to retain control of a brawny mastiff (human culture). The pair‚Äôs trajectory (the pathway of evolution) reflects the outcome of the struggle. Now imagine the same dog-walker struggling with multiple dogs, on leashes of varied lengths, with each dog tugging in different directions. All these tugs represent the influence of developmental factors, including epigenetics, antibodies and hormones passed on by parents, as well as the ecological legacies and culture they bequeath.&lt;/p&gt;
    &lt;p&gt;The struggling dog-walker is a good metaphor for how EES views the adaptive process. Does this require a revolution in evolution? Before we can answer this question, we need to examine how science works. The best authorities here are not biologists but philosophers and historians of science. Thomas Kuhn‚Äôs book The Structure of Scientific Revolutions (1962) popularised the idea that sciences change through revolutions in understanding. These ‚Äòparadigm shifts‚Äô were thought to follow a crisis of confidence in the old theory that arose through the accumulation of conflicting data.&lt;/p&gt;
    &lt;p&gt;Then there‚Äôs Karl Popper, and his conjecture that scientific theories can‚Äôt be proven but can be falsified. Consider the hypothesis: ‚ÄòAll sheep are white.‚Äô Popper maintained that no amount of positive findings consistent with this hypothesis could prove it to be correct, since one could never rule out the chance that a conflicting data-point might arise in the future; conversely, the observation of a single black sheep would decisively prove the hypothesis to be false. He maintained that scientists should strive to carry out critical experiments that could potentially falsify their theories.&lt;/p&gt;
    &lt;p&gt;Everything from diet to air pollution to parental behaviour can influence gene expression&lt;/p&gt;
    &lt;p&gt;While Kuhn and Popper‚Äôs ideas are well-known, they remain disputed and contentious in the eyes of philosophers and historians. Contemporary thinking in these fields is better captured by the Hungarian philosopher Imre Lakatos in The Methodology of Scientific Research Programmes (1978):&lt;/p&gt;
    &lt;quote&gt;The history of science refutes both Popper and Kuhn: on close inspection both Popperian crucial experiments and Kuhnian revolutions turn out to be myths.&lt;/quote&gt;
    &lt;p&gt;Popper‚Äôs arguments might make logical sense, but they don‚Äôt quite map on to how science works in the real world. Scientific observations are susceptible to errors of measurement; scientists are human beings and get attached to their theories; and scientific ideas can be fiendishly complex ‚Äì all of which makes evaluating scientific hypotheses a messy business. Rather than accepting that our hypotheses might be wrong, we challenge the methodology (‚ÄòThat sheep‚Äôs not black ‚Äì your instruments are faulty‚Äô), dispute the interpretation (‚ÄòThe sheep‚Äôs just dirty‚Äô), or come up with tweaks to our hypotheses (‚ÄòI meant domesticated breeds, not wild mouflon‚Äô). Lakatos called such fixes and fudges ‚Äòauxiliary hypotheses‚Äô; scientists propose them to ‚Äòprotect‚Äô their core ideas, so that they need not be rejected.&lt;/p&gt;
    &lt;p&gt;This sort of behaviour is clearly manifest in scientific debates over evolution. Take the idea that new features acquired by an organism during its life can be passed on to the next generation. This hypothesis was brought to prominence in the early 1800s by the French biologist Jean-Baptiste Lamarck, who used it to explain how species evolved. However, it has long been regarded as discredited by experiment ‚Äì to the point that the term ‚ÄòLamarckian‚Äô has a derogatory connotation in evolutionary circles, and any researchers expressing sympathy for the idea effectively brand themselves ‚Äòeccentric‚Äô. The received wisdom is that parental experiences can‚Äôt affect the characters of their offspring.&lt;/p&gt;
    &lt;p&gt;Except they do. The way that genes are expressed to produce an organism‚Äôs phenotype ‚Äì the actual characteristics it ends up with ‚Äì is affected by chemicals that attach to them. Everything from diet to air pollution to parental behaviour can influence the addition or removal of these chemical marks, which switches genes on or off. Usually these so-called ‚Äòepigenetic‚Äô attachments are removed during the production of sperm and eggs cells, but it turns out that some escape the resetting process and are passed on to the next generation, along with the genes. This is known as ‚Äòepigenetic inheritance‚Äô, and more and more studies are confirming that it really happens.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs return to the almond-fearing mice. The inheritance of an epigenetic mark transmitted in the sperm is what led the mice‚Äôs offspring to acquire an inherited fear. In 2011, another extraordinary study reported that worms responded to exposure to a nasty virus by producing virus-silencing factors ‚Äì chemicals that shut down the virus ‚Äì but, remarkably, subsequent generations epigenetically inherited these chemicals via regulatory molecules (known as ‚Äòsmall RNAs‚Äô). There are now hundreds of such studies, many published in the most prominent and prestigious journals. Biologists dispute whether epigenetic inheritance is truly Lamarckian or only superficially resembles it, but there is no getting away from the fact that the inheritance of acquired characteristics really does happen.&lt;/p&gt;
    &lt;p&gt;By Popper‚Äôs reasoning, a single experimental demonstration of epigenetic inheritance ‚Äì like a single black sheep ‚Äì should suffice to convince evolutionary biologists that it‚Äôs possible. Yet, by and large, evolutionary biologists have not rushed to change their theories. Rather, as Lakatos anticipated, we have come up with auxiliary hypotheses that allow us to retain our long-held beliefs (ie, that inheritance is pretty much explained by the transmission of genes across generations). These include the ideas that epigenetic inheritance is rare, that it does not affect functionally important traits, that it is under genetic control, and that it is too unstable to underpin the spread of traits through selection.&lt;/p&gt;
    &lt;p&gt;Unfortunately for the traditionalists, none of these attempts to bracket epigenetic inheritance look credible. It is now known to be widespread in nature, with more and more examples appearing every day. It affects functionally important features such as fruit size, flowering time and root growth in plants ‚Äì and while only a fraction of epigenetic variants are adaptive, that‚Äôs no less true of genetic variation, so it‚Äôs hardly grounds for dismissal. In some systems where rates of epigenetic change have been measured carefully, such as the plant Arabidopsis thaliana, the pace has been found to be low enough to be selected and lead to cumulative evolution. Mathematical models have shown that systems with epigenetic inheritance evolve differently from those solely reliant on genetic inheritance ‚Äì for instance, selection on epigenetic marks can cause changes in gene frequencies. There‚Äôs no longer any doubt that epigenetic inheritance pushes us to think about evolution in a different way.&lt;/p&gt;
    &lt;p&gt;Epigenetics is only part of the story. Through culture and society, all of us inherit knowledge and skills acquired by our parents. Evolutionary biologists have accepted this for at least a century, but until recently it was considered to be restricted to humans. That‚Äôs no longer tenable: creatures across the animal kingdom learn socially about diet, feeding techniques, predator avoidance, communication, migration, and mate and breeding-site choices. Hundreds of experimental studies have demonstrated social learning in mammals, birds, fish and insects.&lt;/p&gt;
    &lt;p&gt;In a single mating season, ‚Äòfads‚Äô can develop in the qualities that individuals find attractive in their partners&lt;/p&gt;
    &lt;p&gt;Among the most compelling data are studies that cross-fostered great tits and blue tits. When raised by the other species, these birds shifted numerous aspects of their behaviour towards the behaviour of their foster parent (including the height in trees at which they foraged, their choice of prey, foraging method, calls and songs, and even their choice of mate). Everyone had assumed that the behavioural differences between these two species were genetic, but it turns out that many are cultural traditions.&lt;/p&gt;
    &lt;p&gt;Animal cultures can be sustained for surprisingly long periods. Archaeological remains show that chimpanzees have used stone tools to crack open nuts for at least 4,300 years. However, as for epigenetic inheritance, it would be a mistake to assume that animal culture must exhibit gene-like stability to be evolutionarily important. In the course of a single mating season, ‚Äòfads‚Äô can develop in the qualities that individuals find attractive in their partners; the process has been experimentally demonstrated in fruit flies, fish, birds and mammals, and mathematical models show that such ‚Äòmate-choice copying‚Äô can strongly affect sexual selection.&lt;/p&gt;
    &lt;p&gt;Another illustration comes from studies of birdsong. When young male birds learn their songs (usually from nearby adult males), they modify the natural-selection pressures of genes that affect how songs are acquired (in males) and which songs are preferred (in females). The cultural transmission of song is known to promote the evolution of brood parasitism ‚Äì where birds, such as cuckoos, don‚Äôt make nests but lay eggs in other birds‚Äô nests ‚Äì as some brood parasites rely on cultural learning to figure out whom to mate with. It also facilitates speciation, since preferences for particular birdsong ‚Äòdialects‚Äô help to maintain genetic differences between populations.&lt;/p&gt;
    &lt;p&gt;Likewise, the diverse, culturally learned foraging traditions of orcas ‚Äì where different groups specialise in particular types of fish, seals or dolphins ‚Äì is thought to be driving them to split into several species. Of course, culture reaches its zenith in our own species, where it is now well-established that our cultural habits have been a major source of natural selection on our genes. Dairy farming and milk consumption generated selection for a genetic variant that increased lactase (the enzyme that metabolises dairy products), while starchy agricultural diets favoured increased amylase (the corresponding enzyme that breaks down starch).&lt;/p&gt;
    &lt;p&gt;All this complexity can‚Äôt be reconciled with a strictly genetic currency for adaptive evolution, as many biologists now acknowledge. Rather, it points to an evolutionary process in which genomes (over hundreds to thousands of generations), epigenetic modifications and inherited cultural factors (over several, perhaps tens or hundreds of generations), and parental effects (over single-generation timespans) collectively inform how organisms adapt. These extra-genetic kinds of inheritance give organisms the flexibility to make rapid adjustments to environmental challenges, dragging genetic change in their wake ‚Äì much like a rowdy pack of dogs.&lt;/p&gt;
    &lt;p&gt;Despite the excitement of all the new data, it‚Äôs unlikely to trigger an evolution revolution for the simple reason that science doesn‚Äôt work that way ‚Äì at least, not evolutionary science. Kuhnian paradigm shifts, like Popper‚Äôs critical experiments, are closer to myths than reality. Look back at the history of evolutionary biology, and you will see nothing that resembles a revolution. Even Charles Darwin‚Äôs theory of evolution through natural selection took approximately 70 years to become widely accepted by the scientific community, and at the turn of the 20th century was viewed with considerable skepticism. Over the following decades, new ideas appeared, they were critically evaluated by the scientific community, and gradually became integrated with pre-existing knowledge. By and large, evolutionary biology was updated without experiencing great periods of ‚Äòcrisis‚Äô.&lt;/p&gt;
    &lt;p&gt;The same holds for the present. Epigenetic inheritance does not disprove genetic inheritance, but shows it to be just one of several mechanisms through which traits are inherited. I know of no biologist who wants to rip up the textbooks, or throw out natural selection. The debate in evolutionary biology concerns whether we want to extend our understanding of the causes of evolution, and whether that changes how we think about the process as a whole. In this respect, what is going on is ‚Äònormal science‚Äô.&lt;/p&gt;
    &lt;p&gt;Why, then, are traditionally minded evolutionary biologists complaining about the misguided evolutionary radicals that lobby for paradigm shift? Why are journalists writing articles about scientists calling for a ‚Äòrevolution‚Äô in evolutionary biology? If nobody actually wants a revolution, and scientific revolutions rarely happen anyway, what‚Äôs all the fuss about? The answer to these questions provides a fascinating insight into the sociology of evolutionary biology.&lt;/p&gt;
    &lt;p&gt;Revolution in evolution is a misattribution ‚Äì a myth propagated by an unlikely alliance of conservative-minded evolutionists, creationists and the press. I don‚Äôt doubt that there are a small number of genuine, revolutionarily minded evolutionary radicals out there, but the vast majority of researchers working towards an extended evolutionary synthesis are simply ordinary, hardworking evolutionary biologists.&lt;/p&gt;
    &lt;p&gt;We all know that sensationalism sells newspapers, and articles that portend a major upheaval make for better copy. Creationists and advocates of ‚Äòintelligent design‚Äô also feed this impression, with propaganda that exaggerates differences of opinion among evolutionists and gives a false impression that the field of evolutionary biology is in turmoil. What‚Äôs more surprising is how commonly conservative-minded biologists play the ‚ÄòWe‚Äôre under attack!‚Äô card against their fellow evolutionists. Portraying intellectual opponents as extremist, and telling people that they are being attacked, are age-old rhetorical tricks to win debate or allegiance.&lt;/p&gt;
    &lt;p&gt;I had always associated such games with politics, not science, but now realise I was naive. Some of the behind-the-scenes shenanigans I have witnessed, seemingly designed to prevent new ideas from spreading by fair means or foul, have truly shocked me, and are out of kilter with practice in other fields that I know. Scientists, too, have careers and legacies at stake, as well as struggles for funding, power and influence. I worry that the traditionalists‚Äô rhetoric is backfiring, creating confusion and inadvertently fuelling creationism by exaggerating division. Too many reputable scientists feel the need for change in evolutionary biology for all to be credibly dismissed as fringe elements.&lt;/p&gt;
    &lt;p&gt;If the extended evolutionary synthesis is not a call for revolution in evolution, then what is it, and why do we need it? To answer these questions, we need to recognise what Kuhn got right ‚Äì namely, that every scientific field possesses shared ways of thinking, or ‚Äòconceptual frameworks‚Äô. Evolutionary biology is no different, and our shared values and assumptions influence what data is collected, how that data is interpreted, and what factors are built into explanations for how evolution works.&lt;/p&gt;
    &lt;p&gt;That is why pluralism in science is healthy. Lakatos stressed that alternative conceptual frameworks ‚Äì what he called different ‚Äòresearch programmes‚Äô ‚Äì can be valuable to the extent that they encourage new hypotheses to be generated and tested, or lead to novel insights. That is the primary function of the EES: to nurture, or even open up, new lines of enquiry, and new productive ways of thinking.&lt;/p&gt;
    &lt;p&gt;What if some ways of building a fish are just more probable than others?&lt;/p&gt;
    &lt;p&gt;A good example concerns what‚Äôs known as ‚Äòdevelopmental bias‚Äô. Consider the intriguing cichlid fishes of East Africa. For tens, perhaps hundreds, of the cichlid species in Lake Malawi, there exists an independently evolved, ‚Äòduplicate‚Äô species in Lake Tanganyika, with a strikingly similar body shape and way of feeding. Such likenesses are usually explained through convergent evolution: random genetic variation has bubbled up as usual, but similar environmental conditions have selected the genes to produce equivalent results. The way that organisms grow and develop might limit which traits arise, but the variation itself is assumed to be essentially random.&lt;/p&gt;
    &lt;p&gt;However, the extraordinary level of parallel evolution seen in these two lakes suggests that something else might be going on. What if some ways of building a fish are just more probable than others? What if trait variation skews towards certain solutions? Selection would still be part of the explanation, but parallel evolution would be much more likely.&lt;/p&gt;
    &lt;p&gt;Cheek teeth (molars) in mammals provide some of the most convincing data for bias. Studies show that it‚Äôs possible to use a mathematical model, based on laboratory mice, to predict the size and number of teeth in a sample of 29 other rodent species. Rather than being free to make any shape or number of teeth, it appears that natural selection is pushing species along a highly specific pathway created by the mechanisms of development. The existence of exceptions ‚Äì rodents such as voles with different ratios of teeth ‚Äì demonstrates that the old way of thinking (that developmental ‚Äòconstraints‚Äô restrict selection) isn‚Äôt quite right. The effect of development is both more subtle and more interesting: developmental mechanisms bias the landscape for selection, and help to determine which features evolve.&lt;/p&gt;
    &lt;p&gt;Such studies are exciting as they help to make evolutionary biology a more predictive science. Why, then, have these ideas received comparatively little attention until recently? We come back to conceptual frameworks. Historically, evolutionary biologists have treated bias in phenotypic variation as a ‚Äòconstraint‚Äô ‚Äì an explanation for why evolution or adaptation has not occurred. The way that organisms grow restricts what sorts of features it is possible or adaptive to possess. Traditionally minded evolutionists have been far more reticent to embrace a positive role for development as a cause of evolutionary direction and change.&lt;/p&gt;
    &lt;p&gt;It took a different perspective (in this instance, that of evolutionary developmental biology, so-called ‚Äòevo devo‚Äô), to motivate this kind of experimentation. From the evo-devo perspective, bias partly explains what evolution and adaptation has occurred. Rodents‚Äô teeth and fishes‚Äô bodies look the way they do because the way that creatures grow makes those characteristics more likely to arise. Bias thus becomes a much more significant concept in evolutionary explanation. By bringing the phenomenon to the fore, the EES hopes it will be investigated.&lt;/p&gt;
    &lt;p&gt;The EES, at least as my collaborators and I frame it, is best viewed as an alternative research programme for evolutionary biology. Inspired by recent findings emerging within evolutionary biology and adjacent fields, the EES starts from the assumption that developmental processes play important roles as causes of novel (and potentially beneficial) phenotypic variation, causes of differences in fitness of those variants, and causes of inheritance. In contrast to how evolution has traditionally been conceived, in the EES the burden of creativity in evolution does not rest on natural selection alone. This alternative way of thinking is being used to generate fresh hypotheses and establish new research agendas. It‚Äôs early days, but there are already signs that this research is starting to yield dividends.&lt;/p&gt;
    &lt;p&gt;If evolution is not to be explained solely in terms of changes in gene frequencies; if previously rejected mechanisms such as the inheritance of acquired characteristics turn out to be important after all; and if organisms are acknowledged to bias evolution through development, learning and other forms of plasticity ‚Äì does all this mean a radically different and profoundly richer account of evolution is emerging? No one knows: but from the perspective of our adapting dog-walker, evolution is looking less like a gentle genetic stroll, and more like a frantic struggle by genes to keep up with strident developmental processes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aeon.co/essays/science-in-flux-is-a-revolution-brewing-in-evolutionary-theory"/><published>2026-01-18T19:21:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671731</id><title>Dead Internet Theory</title><updated>2026-01-18T22:10:13.646398+00:00</updated><content>&lt;doc fingerprint="32e0135d28e5b5bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dead Internet Theory&lt;/head&gt;
    &lt;p&gt;The other day I was browsing my one-and-only social network ‚Äî which is not a social network, but I‚Äôm tired of arguing with people online about it ‚Äî HackerNews. It‚Äôs like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-troll, like to lurk. I like HackerNews. It helps me stay up-to-date about recent tech news (like Cloudflare acquiring Astro which makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); it mostly avoids politics; and it‚Äôs not a social network.&lt;/p&gt;
    &lt;p&gt;And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project. It‚Äôs great to see people work on their projects and decide to show them to the world. I think people underestimate the fear of actually shipping stuff, which involves sharing it with the world.&lt;/p&gt;
    &lt;p&gt;Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated. I grabbed my popcorn, and started to follow this thread. More accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc. And at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it. But I think it‚Äôs fair to disclose the use of AI, especially in open-source software. People on the internet are, mostly, anonymous, and it‚Äôs not always possible to verify the claims or expertise of particular individuals. But as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it‚Äôs impossible to verify every piece of code we are going to use. So it‚Äôs fair to know, I think, if some project is AI generated and to what extent. In the end, LLMs are just probabilistic next-token generators. And while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code).&lt;/p&gt;
    &lt;p&gt;As I was following this thread, I stared to see a pattern: the comments of the author looked AI generated too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The use of em-dashes, which on most keyboard require a special key-combination that most people don‚Äôt know, and while in markdown two dashes will render as em-dash, this is not true of HackerNews (hence, you often see &lt;code&gt;--&lt;/code&gt;in HackerNews comments, where the author is probably used to Markdown renderer turning it into em-dash)&lt;/item&gt;
      &lt;item&gt;The notorious ‚Äúyou are absolutely right‚Äù, which no living human ever used before, at least not that I know of&lt;/item&gt;
      &lt;item&gt;The other notorious ‚Äúlet me know if you want to [do that thing] or [explore this other thing]‚Äù at the end of the sentence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all. Honestly, I was thinking I was going insane. Am I wrong to suspect them? What if people DO USE em-dashes in real life? What if English is not their native language and in their native language it‚Äôs fine to use phrases like ‚Äúyou are absolutely right‚Äù? Is this even a real person? Are the people who are commenting real?&lt;/p&gt;
    &lt;p&gt;And then it hit me. We have reached the Dead Internet. The Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff).&lt;/p&gt;
    &lt;p&gt;I‚Äôm &lt;del&gt;ashamed&lt;/del&gt; proud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me. Back in the early 2000s, there were barely bots on the internet. The average non-tech human didn‚Äôt know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there. I spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now). I‚Äôm basically a graduate of the Internet University. Back then, nobody had doubts that they were talking to a human-being. Sure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real!&lt;/p&gt;
    &lt;p&gt;But today, I no longer know what is real. I saw a picture on LinkedIn, from a real tech company, posting about their ‚Äúoffice vibes‚Äù and their happy employees. And then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts). It was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality. Hell, maybe the people on the picture do not even exist!&lt;/p&gt;
    &lt;p&gt;And these are mild examples. I don‚Äôt use social networks (and no, HackerNews is not a social network), but I hear horror stories about AI generated content on Facebook, Xitter, TikTok, ranging from photos of giants that built the pyramids in Egypt, all the way to short videos of pretty girls saying that the EU is bad for Poland.&lt;/p&gt;
    &lt;p&gt;I honestly got sad that day. Hopeless, if I could say. AI is easily available to the masses, which allow them to generate shitload of AI-slop. People no longer need to write comments or code, they can just feed this to AI agents who will generate the next ‚Äúyou are absolutely right‚Äù masterpiece.&lt;/p&gt;
    &lt;p&gt;I like technology. I like software engineering, and the concept of the internet where people could share knowledge and create communities. Were there malicious actors back then on the internet? For sure. But what I am seeing today, makes me question whether the future we are headed to is a future where technology is useful anymore. Or, rather, it‚Äôs a future where bots talk with bots, and human knowledge just gets recycled and repackaged into ‚Äú10 step to fix your [daily problem] you are having‚Äù for the sake of selling you more stuff.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kudmitry.com/articles/dead-internet-theory/"/><published>2026-01-18T20:19:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671982</id><title>Stirling Cycle Machine Analysis</title><updated>2026-01-18T22:10:13.211752+00:00</updated><content>&lt;doc fingerprint="13d6d2d304ba8708"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;College&lt;/head&gt;
    &lt;p&gt;Russ College of Engineering and Technology&lt;/p&gt;
    &lt;head rend="h2"&gt;Files&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Dedicated to William T. Beale (1928 - 2016), inventor of the Free Piston Stirling Engine, Mentor and Frien.&lt;/p&gt;
    &lt;p&gt;This web resource is intended to be totally self contained learning resource for the analysis and development of computer simulation of single phase, piston/cylinder Stirling cycle machines. It includes thermodynamic, heat transfer and fluid flow friction analysis, and until 2012 it was used as resource material for an advanced course for Mechanical Engineering majors. The course structure was based on the book by I.Urieli &amp;amp; D.M.Berchowitz 'Stirling Cycle Engine Analysis' (Adam Hilger, 1984). The computer simulation program modules (originally written in FORTRAN) have all been updated and rewritten in MATLAB, a convenient interactive language which allows direct graphical output - essential for Stirling cycle analysis. A complete set of all the m-files are developed and provided, and they can be augmented and adapted as needed for specific engine/refrigerator configurations.&lt;/p&gt;
    &lt;p&gt;It is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license and as such is freely available. Comments and constructive criticism are welcomed by the author.&lt;/p&gt;
    &lt;p&gt;Chapter 1: Background and Introduction&lt;/p&gt;
    &lt;p&gt;Chapter 2: Basic Engine Configurations&lt;/p&gt;
    &lt;p&gt;Chapter 3: Ideal Isothermal Analysis&lt;/p&gt;
    &lt;p&gt;We define and analyze the Ideal Isothermal model of a Stirling engine, including the Schmidt Analysis, and discuss its limitations. One obviously incorrect conclusion of this analysis is that all three heat exchangers are redundant, and only contribute dead space, since all required heat transfer processes occur in the isothermal compression and expansion spaces. Nevertheless we can obtain a better understanding of a specific design, particularly when we augment the solution with Allan Organ's particle mass flow analysis.&lt;/p&gt;
    &lt;p&gt;Chapter 4: Ideal Adiabatic Analysis&lt;/p&gt;
    &lt;p&gt;We find that the Ideal Isothermal analysis predicts that the heat exchangers of a Stirling engine are redundant, thus we cannot seriously use this model to predict the ideal performance of an actual machine. We thus turn to an alternative model in which the compression and expansion spaces are adiabatic. We find that there is no closed form solution to this model and we have to resort to computer simulation. We gain various insights from using this model in particular with regards to the importance of the regenerator, which was not understood for a significant period.&lt;/p&gt;
    &lt;p&gt;Chapter 5: Simple Analysis&lt;/p&gt;
    &lt;p&gt;This analysis approach uses the Ideal Adiabatic model as a basis to predict the real performance of the three heat exchanger sections, particularly with regards to heat transfer and pressure drop. The name Simple Analysis is to indicate that this is a simplification of the actual non-steady flow heat exchange, however it enables a parametric analysis of a specific machine.&lt;/p&gt;
    &lt;p&gt;This learning resource includes a set of tutorial MATLAB computer program modules for simulating specific Stirling engine configurations. The complete set of m-files can be downloaded in compressed format sea.zip (sea = stirling engine analysis). These modules can be augmented and adapted as required to simulate a specific engine design. Currently the engine modules are for Alpha machines, including a Sinusoidal drive, a Ross Yoke-drive and a Ross Rocker-V engine. The heat exchanger types include tubular, annular gap, and slot heat exchangers, and the regenerator matrix types include screen mesh and rolled foil matrices. Working gas types include air, helium, and hydrogen.&lt;/p&gt;
    &lt;p&gt;Note that the purpose of this learning resource is to develop an appreciation and understanding of the complexity of practical Stirling cycle machine performance simulation, mainly due to the heat transfer processes. It is not intended as an alternative to the Sage Software for engineering modeling and optimization of Stirling cycle machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Publication Date&lt;/head&gt;
    &lt;p&gt;12-12-2020&lt;/p&gt;
    &lt;head rend="h2"&gt;Publisher&lt;/head&gt;
    &lt;p&gt;Israel Urieli&lt;/p&gt;
    &lt;head rend="h2"&gt;City&lt;/head&gt;
    &lt;p&gt;Athens&lt;/p&gt;
    &lt;head rend="h2"&gt;Keywords&lt;/head&gt;
    &lt;p&gt;Stirling engines, thermal analysis&lt;/p&gt;
    &lt;head rend="h2"&gt;Disciplines&lt;/head&gt;
    &lt;p&gt;Engineering&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;This repository version of Stirling Cycle Machine Analysis by Israel Urieli is a PDF version of the original website (https://people.ohio.edu/trembly/mechanical/stirling/) which may not be accessible. Please visit the Internet Archive's Wayback Machine for archived versions of the website.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creative Commons License&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-Share Alike 4.0 International License.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommended Citation&lt;/head&gt;
    &lt;p&gt; Urieli, Israel, "Stirling Cycle Machine Analysis" (2020). OHIO Open Faculty Textbooks. 9. &lt;lb/&gt; https://ohioopen.library.ohio.edu/opentextbooks/9 &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ohioopen.library.ohio.edu/opentextbooks/9/"/><published>2026-01-18T20:45:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46672150</id><title>Police Invested Millions in Shadowy Phone-Tracking Software Won't Say How Used</title><updated>2026-01-18T22:10:12.953116+00:00</updated><content/><link href="https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/"/><published>2026-01-18T21:05:14+00:00</published></entry></feed>