<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-05T19:32:51.129337+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46157594</id><title>UniFi 5G</title><updated>2025-12-05T19:32:57.855668+00:00</updated><content>&lt;doc fingerprint="3e818a12e14516a1"&gt;
  &lt;main&gt;
    &lt;p&gt;The UniFi 5G Max lineup was created with a clear goal in mind: deliver a sleek, versatile, and exceptionally powerful 5G internet experience that works effortlessly in any environment.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G Max lineup was created with a clear goal in mind: deliver a sleek, versatile, and exceptionally powerful 5G internet experience that works effortlessly in any environment.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G Max makes deployment easy, whether installed locally or at a remote site. Plug it into any PoE port and it instantly appears as a ready to use WAN interface, no matter whether plugged directly into your UniFi gateway or into your office switch. No new cable runs needed! It sits neatly on a desk, but you can reposition it for the best possible signal using the included wall or window mount.&lt;/p&gt;
    &lt;p&gt;The 5G Max delivers downlink speeds up to 2 Gbps with ultra low latency that makes it reliable as a primary connection and seamless as a backup WAN. UniFi routing policies and SLAs let you choose exactly how and when 5G is used, and for which clients and VLANs. Easily set per-SIM usage limits to avoid overage costs with just a few clicks.&lt;/p&gt;
    &lt;p&gt;For tougher environments or deployments with poor indoor cellular coverage, the outdoor model maintains the same high performance cellular connectivity with improved antenna performance in a durable IP67 rated enclosure. It is built for rooftop installs, off site locations, and mobile deployments where reliability is critical. Just like its indoor counterpart, you can also connect it via any PoE port, anywhere on your network, greatly simplifying cabling requirements.&lt;/p&gt;
    &lt;p&gt;If you want everything UniFi in one device, the DreamRouter 5G Max combines 5G connectivity with WiFi 7, local storage, and full UniFi OS application support. Deploy it anywhere 5G is available and run an entire high-performance and scalable network stack instantly.&lt;/p&gt;
    &lt;p&gt;Every device in the UniFi 5G lineup supports both physical SIMs and eSIM, giving you the freedom to choose your carrier and switch whenever needed with zero friction. All are equipped with dual SIM slots, with one SIM replaceable by eSIM, and are fully unlocked: any major carrier, any type of deployment, with one piece of hardware.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G lineup brings sleek design, powerful performance, easy installation, and genuine WAN flexibility to every deployment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.ui.com/article/introducing-unifi-5g"/><published>2025-12-05T07:06:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46160315</id><title>Netflix to Acquire Warner Bros</title><updated>2025-12-05T19:32:57.393385+00:00</updated><content>&lt;doc fingerprint="7a920551567ba73c"&gt;
  &lt;main&gt;&lt;p&gt;Business&lt;/p&gt;05 December 2025&lt;p&gt;Transaction Unites Warner Bros.’ Iconic Franchises and Storied Libraries with Netflix’s Leading Entertainment Service, Creating an Extraordinary Offering for Consumers&lt;/p&gt;&lt;p&gt;Netflix to Maintain Warner Bros.’ Current Operations&lt;/p&gt;&lt;p&gt;Combination Will Offer More Choice and Greater Value for Consumers, Create More Opportunities for the Creative Community and Generate Shareholder Value&lt;/p&gt;&lt;p&gt;Acquisition Will Strengthen the Entertainment Industry&lt;/p&gt;&lt;p&gt;HOLLYWOOD, Calif., Dec. 5, 2025 -- Today, Netflix, Inc. (the Company) and Warner Bros. Discovery, Inc. (WBD) announced they have entered into a definitive agreement under which Netflix will acquire Warner Bros., including its film and television studios, HBO Max and HBO.&lt;/p&gt;&lt;p&gt;The cash and stock transaction is valued at $27.75 per WBD share (subject to a collar as detailed below), with a total enterprise value of approximately $82.7 billion (equity value of $72.0 billion). The transaction is expected to close after the previously announced separation of WBD’s Global Networks division, Discovery Global, into a new publicly-traded company, which is now expected to be completed in Q3 2026.&lt;/p&gt;&lt;p&gt;This acquisition brings together two pioneering entertainment businesses, combining Netflix’s innovation, global reach and best-in-class streaming service with Warner Bros.’ century-long legacy of world-class storytelling. Beloved franchises, shows and movies such as The Big Bang Theory, The Sopranos, Game of Thrones, The Wizard of Oz and the DC Universe will join Netflix’s extensive portfolio including Wednesday, Money Heist, Bridgerton, Adolescence and Extraction, creating an extraordinary entertainment offering for audiences worldwide.&lt;/p&gt;&lt;p&gt;“Our mission has always been to entertain the world,” said Ted Sarandos, co-CEO of Netflix. “By combining Warner Bros.’ incredible library of shows and movies—from timeless classics like Casablanca and Citizen Kane to modern favorites like Harry Potter and Friends—with our culture-defining titles like Stranger Things, KPop Demon Hunters and Squid Game, we'll be able to do that even better. Together, we can give audiences more of what they love and help define the next century of storytelling.”&lt;/p&gt;&lt;p&gt;“This acquisition will improve our offering and accelerate our business for decades to come,” continued Greg Peters, co-CEO of Netflix. “Warner Bros. has helped define entertainment for more than a century and continues to do so with phenomenal creative executives and production capabilities. With our global reach and proven business model, we can introduce a broader audience to the worlds they create—giving our members more options, attracting more fans to our best-in-class streaming service, strengthening the entire entertainment industry and creating more value for shareholders.”&lt;/p&gt;&lt;p&gt;“Today’s announcement combines two of the greatest storytelling companies in the world to bring to even more people the entertainment they love to watch the most,” said David Zaslav, President and CEO of Warner Bros. Discovery. “For more than a century, Warner Bros. has thrilled audiences, captured the world’s attention, and shaped our culture. By coming together with Netflix, we will ensure people everywhere will continue to enjoy the world’s most resonant stories for generations to come.”&lt;/p&gt;&lt;p&gt;Combination Will Offer More Choice, More Opportunities, More Value&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Complementary strengths and assets: Warner Bros.’ studios are world-class, with Warner Bros. recognized as a leading supplier of television titles and filmed entertainment. HBO and HBO Max also provide a compelling, complementary offering for consumers. Netflix expects to maintain Warner Bros.’ current operations and build on its strengths, including theatrical releases for films.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;More choice and greater value for consumers: By adding the deep film and TV libraries and HBO and HBO Max programming, Netflix members will have even more high-quality titles from which to choose. This also allows Netflix to optimize its plans for consumers, enhancing viewing options and expanding access to content.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;A stronger entertainment industry: This acquisition will enhance Netflix’s studio capabilities, allowing the Company to significantly expand U.S. production capacity and continue to grow investment in original content over the long term which will create jobs and strengthen the entertainment industry.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;More opportunities for the creative community: By uniting Netflix’s member experience and global reach with Warner Bros.’ renowned franchises and extensive library, the Company will create greater value for talent—offering more opportunities to work with beloved intellectual property, tell new stories and connect with a wider audience than ever before.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;More value for shareholders: By offering members a wider selection of quality series and films, Netflix expects to attract and retain more members, drive more engagement and generate incremental revenue and operating income. The Company also expects to realize at least $2-3 billion of cost savings per year by the third year and expects the transaction to be accretive to GAAP earnings per share by year two.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Transaction Details and Timing&lt;/p&gt;&lt;p&gt;Under the terms of the agreement, each WBD shareholder will receive $23.25 in cash and $4.501 in shares of Netflix common stock for each share of WBD common stock outstanding at the closing of the transaction. The transaction values Warner Bros. Discovery at $27.75 per share, implying a total equity value of approximately $72.0 billion and an enterprise value of approximately $82.7 billion&lt;/p&gt;&lt;p&gt;In June 2025, WBD announced plans to separate its Streaming &amp;amp; Studios and Global Networks divisions into two separate publicly traded companies. This separation is now expected to be completed in Q3 2026, prior to the closing of this transaction. The newly separated publicly traded company holding the Global Networks division, Discovery Global, will include premier entertainment, sports and news television brands around the world including CNN, TNT Sports in the U.S., and Discovery, free-to-air channels across Europe, and digital products such as Discovery+ and Bleacher Report.&lt;/p&gt;&lt;p&gt;The stock component is subject to a collar under which WBD shareholders will receive Netflix stock valued at $4.50 per share, provided the 15-day volume weighted average price (“VWAP”) of Netflix stock price (measured three trading days prior to closing) falls between $97.91 and $119.67. If the VWAP is below $97.91, WBD shareholders will receive 0.0460 Netflix shares for each WBD share. If the VWAP is above $119.67, WBD shareholders will receive 0.0376 Netflix shares for each WBD share.&lt;/p&gt;&lt;p&gt;The transaction was unanimously approved by the Boards of Directors of both Netflix and WBD. In addition to the completion of the separation of Discovery Global (WBD’s Global Networks business), completion of the transaction is subject to required regulatory approvals, approval of WBD shareholders and other customary closing conditions. The transaction is expected to close in 12-18 months.&lt;/p&gt;&lt;p&gt;Moelis &amp;amp; Company LLC is acting as Netflix’s financial advisor and Skadden, Arps, Slate, Meagher &amp;amp; Flom LLP is serving as legal counsel. Wells Fargo is acting as an additional financial advisor and, along with BNP and HSBC, is providing committed debt financing related to the transaction.&lt;/p&gt;&lt;p&gt;Allen &amp;amp; Company, J.P. Morgan and Evercore are serving as financial advisors to Warner Bros. Discovery and Wachtell Lipton, Rosen &amp;amp; Katz and Debevoise &amp;amp; Plimpton LLP are serving as legal counsel.&lt;/p&gt;&lt;p&gt;1 Reflects a 10% symmetrical collar.&lt;/p&gt;&lt;p&gt;Webcast&lt;/p&gt;&lt;p&gt;Netflix will conduct a conference call today at 5:00am PT/8:00am ET to discuss the contents of this release. A link to the live webcast of the conference call will be available at https://ir.netflix.net/.&lt;/p&gt;&lt;p&gt;Contacts&lt;/p&gt;&lt;p&gt;Netflix&lt;/p&gt;&lt;p&gt;Lowell Singer&lt;/p&gt;&lt;p&gt;VP, Investor Relations&lt;/p&gt;&lt;p&gt;(818) 434-2141&lt;/p&gt;&lt;p&gt;Emily Feingold&lt;/p&gt;&lt;p&gt;VP, Communications&lt;/p&gt;&lt;p&gt;(323) 287-0756&lt;/p&gt;&lt;p&gt;Warner Bros. Discovery&lt;/p&gt;&lt;p&gt;Andrew Slabin&lt;/p&gt;&lt;p&gt;Investor Relations&lt;/p&gt;&lt;p&gt;(212) 548-5544&lt;/p&gt;&lt;p&gt;andrew.slabin@wbd.com&lt;/p&gt;&lt;p&gt;Peter Lee&lt;/p&gt;&lt;p&gt;Investor Relations&lt;/p&gt;&lt;p&gt;(212) 548-5907&lt;/p&gt;&lt;p&gt;peter.lee@wbd.com&lt;/p&gt;&lt;p&gt;Robert Gibbs&lt;/p&gt;&lt;p&gt;Press Contact&lt;/p&gt;&lt;p&gt;(347) 268-3017&lt;/p&gt;&lt;p&gt;IMPORTANT INFORMATION AND WHERE TO FIND IT&lt;/p&gt;&lt;p&gt;In connection with the proposed transaction (the “Merger”) between Netflix, Inc. (“Netflix”) and Warner Bros. Discovery, Inc. (“WBD”), Netflix intends to file with the U.S. Securities and Exchange Commission (the “SEC”) a registration statement on Form S-4 (the “Registration Statement”), which will include a prospectus with respect to the shares of Netflix’s common stock to be issued in the Merger and a proxy statement for WBD’s stockholders (the “Proxy Statement/Prospectus”), and WBD intends to file with the SEC the proxy statement. The definitive proxy statement (if and when available) will be mailed to stockholders of WBD. WBD also intends to file a registration statement for a newly formed subsidiary (“Discovery Global”), which is contemplated to own certain assets and businesses of WBD not being acquired by Netflix in connection with the Merger. Each of Netflix and WBD may also file with or furnish to the SEC other relevant documents regarding the Merger. This communication is not a substitute for the Registration Statement, the Proxy Statement/Prospectus or any other document that Netflix or WBD may file with the SEC or mail to WBD’s stockholders in connection with the Merger.&lt;/p&gt;&lt;p&gt;INVESTORS AND SECURITY HOLDERS OF NETFLIX AND WBD ARE URGED TO READ THE REGISTRATION STATEMENT AND THE PROXY STATEMENT/PROSPECTUS INCLUDED WITHIN THE REGISTRATION STATEMENT WHEN THEY BECOME AVAILABLE, AS WELL AS ANY OTHER RELEVANT DOCUMENTS FILED WITH THE SEC IN CONNECTION WITH THE MERGER OR INCORPORATED BY REFERENCE INTO THE REGISTRATION STATEMENT AND THE PROXY STATEMENT/PROSPECTUS (INCLUDING ANY AMENDMENTS OR SUPPLEMENTS THERETO), BECAUSE THEY WILL CONTAIN IMPORTANT INFORMATION REGARDING NETFLIX, WBD, THE MERGER AND RELATED MATTERS.&lt;/p&gt;&lt;p&gt;The documents filed by Netflix with the SEC also may be obtained free of charge at Netflix’s website at https://ir.netflix.net/home/default.aspx. The documents filed by WBD with the SEC also may be obtained free of charge at WBD’s website at https://ir.wbd.com.&lt;/p&gt;&lt;p&gt;PARTICIPANTS IN THE SOLICITATION&lt;/p&gt;&lt;p&gt;Netflix, WBD and certain of their respective directors and executive officers may be deemed to be participants in the solicitation of proxies from the stockholders of WBD in connection with the Merger under the rules of the SEC.&lt;/p&gt;&lt;p&gt;Information about the interests of the directors and executive officers of Netflix and WBD and other persons who may be deemed to be participants in the solicitation of stockholders of WBD in connection with the Merger and a description of their direct and indirect interests, by security holdings or otherwise, will be included in the Proxy Statement/Prospectus, which will be filed with the SEC.&lt;/p&gt;&lt;p&gt;Information about WBD’s directors and executive officers is set forth in WBD’s proxy statement for its 2025 Annual Meeting of Stockholders on Schedule 14A filed with the SEC on April 23, 2025, WBD’s Annual Report on Form 10-K for the year ended December 31, 2024, and any subsequent filings with the SEC. Information about Netflix’s directors and executive officers is set forth in Netflix’s proxy statement for its 2025 Annual Meeting of Stockholders on Schedule 14A filed with the SEC on April 17, 2025, and any subsequent filings with the SEC. Additional information regarding the direct and indirect interests of those persons and other persons who may be deemed participants in the Merger may be obtained by reading the Proxy Statement/Prospectus regarding the Merger when it becomes available. Free copies of these documents may be obtained as described above.&lt;/p&gt;&lt;p&gt;NO OFFER OR SOLICITATION&lt;/p&gt;&lt;p&gt;This communication is for informational purposes only and does not constitute, or form a part of, an offer to sell or the solicitation of an offer to buy any securities or a solicitation of any vote or approval, nor shall there be any sale of securities in any jurisdiction in which such offer, solicitation or sale would be unlawful prior to registration or qualification under the securities laws of any such jurisdiction. No offer of securities shall be made except by means of a prospectus meeting the requirements of Section 10 of the Securities Act of 1933, as amended, and otherwise in accordance with applicable law.&lt;/p&gt;&lt;p&gt;CAUTIONARY NOTE REGARDING FORWARD LOOKING STATEMENTS&lt;/p&gt;&lt;p&gt;This document contains “forward-looking statements” within the meaning of the federal securities laws, including Section 27A of the U.S. Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934, as amended. These forward-looking statements are based on Netflix’s and WBD’s current expectations, estimates and projections about the expected date of closing of the Merger and the potential benefits thereof, their respective businesses and industries, management’s beliefs and certain assumptions made by Netflix and WBD, all of which are subject to change. All forward-looking statements by their nature address matters that involve risks and uncertainties, many of which are beyond our control and are not guarantees of future results, such as statements about the consummation of the Merger and the anticipated benefits thereof. These and other forward-looking statements, including the failure to consummate the Merger or to make or take any filing or other action required to consummate the transaction on a timely matter or at all, are not guarantees of future results and are subject to risks, uncertainties and assumptions that could cause actual results to differ materially from those expressed in any forward-looking statements. Accordingly, there are or will be important factors that could cause actual results to differ materially from those indicated in such statements and, therefore, you should not place undue reliance on any such statements and caution must be exercised in relying on forward-looking statements. Important risk factors that may cause such a difference include, but are not limited to: (i) the completion of the Merger on anticipated terms and timing, including obtaining stockholder and regulatory approvals, completing the separation of WBD’s Global Networks business and Streaming and Studios business, anticipated tax treatment, unforeseen liabilities, future capital expenditures, revenues, expenses, earnings, synergies, economic performance, indebtedness, financial condition, losses, future prospects, business and management strategies, expansion and growth of WBD’s and Netflix’s businesses and other conditions to the completion of the Merger; (ii) failure to realize the anticipated benefits of the Merger, including as a result of delay in completing the transaction or integrating the businesses of Netflix and WBD; (iii) Netflix’s and WBD’s ability to implement their business strategies; (iv) consumer viewing trends; (v) potential litigation relating to the Merger that could be instituted against Netflix, WBD or their respective directors; (vi) the risk that disruptions from the Merger will harm Netflix’s or WBD’s business, including current plans and operations; (vii) the ability of Netflix or WBD to retain and hire key personnel; (viii) potential adverse reactions or changes to business relationships resulting from the announcement, pendency or completion of the Merger; (ix) uncertainty as to the long-term value of Netflix’s common stock; (x) legislative, regulatory and economic developments affecting Netflix’s and WBD’s businesses; (xi) general economic and market developments and conditions; (xii) the evolving legal, regulatory and tax regimes under which Netflix and WBD operate; (xiii) potential business uncertainty, including changes to existing business relationships, during the pendency of the Merger that could affect Netflix’s or WBD’s financial performance; (xiv) restrictions during the pendency of the Merger that may impact Netflix’s or WBD’s ability to pursue certain business opportunities or strategic transactions; and (xv) failure to receive the approval of the stockholders of WBD. These risks, as well as other risks associated with the Merger, will be more fully discussed in the Registration Statement and Proxy Statement/Prospectus to be filed with the SEC in connection with the Merger and the registration statement to be filed with the SEC in connection with the separation. While the list of factors presented here is, and the list of factors presented in the Registration Statement and Proxy Statement/Prospectus will be, considered representative, no such list should be considered to be a complete statement of all potential risks and uncertainties. Unlisted factors may present significant additional obstacles to the realization of forward-looking statements. Consequences of material differences in results as compared with those anticipated in the forward-looking statements could include, among other things, business disruption, operational problems, financial loss, legal liability to third parties and similar risks, any of which could have a material adverse effect on Netflix’s or WBD’s consolidated financial condition, results of operations or liquidity. The forward-looking statements included in this communication are made only as of the date hereof. Neither Netflix nor WBD assumes any obligation to publicly provide revisions or updates to any forward-looking statements, whether as a result of new information, future developments or otherwise, should circumstances change, except as otherwise required by securities and other applicable laws.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://about.netflix.com/en/news/netflix-to-acquire-warner-bros"/><published>2025-12-05T12:21:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46160698</id><title>Making RSS More Fun</title><updated>2025-12-05T19:32:56.803049+00:00</updated><content>&lt;doc fingerprint="2b4893113586319f"&gt;
  &lt;main&gt;
    &lt;p&gt;I don't like RSS readers. I know, this is blasphemous especially on a website where I'm actively encouraging you to subscribe through RSS. As someone writing stuff, RSS is great for me. I don't have to think about it, the requests are pretty light weight, I don't need to think about your personal data or what client you are using. So as a protocol RSS is great, no notes.&lt;/p&gt;
    &lt;p&gt;However as something I'm going to consume, it's frankly a giant chore. I feel pressured by RSS readers, where there is this endlessly growing backlog of things I haven't read. I rarely want to read all of a websites content from beginning to end, instead I like to jump between them. I also don't really care if the content is chronological, like an old post about something interesting isn't less compelling to me than a newer post.&lt;/p&gt;
    &lt;p&gt;What I want, as a user experience, is something akin to TikTok. The whole appeal of TikTok, for those who haven't wasted hours of their lives on it, is that I get served content based on an algorithm that determines what I might think is useful or fun. However what I would like is to go through content from random small websites. I want to sit somewhere and passively consume random small creators content, then upvote some of that content and the service should show that more often to other users. That's it. No advertising, no collecting tons of user data about me, just a very simple "I have 15 minutes to kill before the next meeting, show me some random stuff."&lt;/p&gt;
    &lt;p&gt;In this case the "algorithm" is pretty simple: if more people like a thing, more people see it. But with Google on its way to replacing search results with LLM generated content, I just wanted to have something that let me play around with the small web the way that I used to.&lt;/p&gt;
    &lt;p&gt;There actually used to be a service like this called StumbleUpon which was more focused on pushing users towards popular sites. It has been taken down, presumably because there was no money in a browser plugin that sent users to other websites whose advertising you didn't control.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;You can go download the Firefox extension now and try this out and skip the rest of this if you want. https://timewasterpro.xyz/ If you hate it or find problems, let me know on Mastodon. https://c.im/@matdevdug&lt;/p&gt;
    &lt;head rend="h3"&gt;Functionality&lt;/head&gt;
    &lt;p&gt;So I wanted to do something pretty basic. You hit a button, get served a new website. If you like the website, upvote it, otherwise downvote it. If you think it has objectionable content then hit report. You have to make an account (because I couldn't think of another way to do it) and then if you submit links and other people like it, you climb a Leaderboard.&lt;/p&gt;
    &lt;p&gt;On the backend I want to (very slowly so I don't cost anyone a bunch of money) crawl a bunch of RSS feeds, stick the pages in a database and then serve them up to users. Then I want to track what sites get upvotes and return those more often to other users so that "high quality" content shows up more often. "High quality" would be defined by the community or just me if I'm the only user.&lt;/p&gt;
    &lt;p&gt;It's pretty basic stuff, most of it copied from tutorials scattered around the Internet. However I really want to drive home to users that this is not a Serious Thing. I'm not a company, this isn't a new social media network, there are no plans to "grow" this concept beyond the original idea unless people smarter than me ping with me ideas. So I found this amazing CSS library: https://sakofchit.github.io/system.css/&lt;/p&gt;
    &lt;p&gt;The Apple's System OS design from the late-80s to the early 90s was one of my personal favorites and I think would send a strong signal to a user that this is not a professional, modern service.&lt;/p&gt;
    &lt;p&gt;Great, the basic layout works. Let's move on!&lt;/p&gt;
    &lt;head rend="h3"&gt;Backend&lt;/head&gt;
    &lt;p&gt;So I ended up doing FastAPI because it's very easy to write. I didn't want to spend a ton of time writing the API because I doubt I nailed the API design on the first round. I use sqlalchemy for the database. The basic API layout is as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;admin - mostly just generating read-only reports of like "how many websites are there"&lt;/item&gt;
      &lt;item&gt;leaderboard - So this is my first attempt at trying to get users involved. Submit a website that other people like? Get points, climb leaderboard.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The source for the RSS feeds came from the (very cool) Kagi small web Github. https://github.com/kagisearch/smallweb. Basically I assume that websites that have submitted their RSS feeds here are cool with me (very rarely) checking for new posts and adding them to my database. If you want the same thing as this does, but as an iFrame, that's the Kagi small web service.&lt;/p&gt;
    &lt;p&gt;The scraping work is straightforward. We make a background worker, they grab 5 feeds every 600 seconds, they check for new content on each feed and then wait until the 600 seconds has elapsed to grab 5 more from the smallweb list of RSS feeds. Since we have a lot of feeds, this ends up look like we're checking for new content less than once a day which is the interval that I want.&lt;/p&gt;
    &lt;p&gt;Then we write it out to a sqlite database and basically track "has this URL been reported", if so, put it into a review queue and then how many times this URL has been liked or disliked. I considered a "real" database but honestly sqlite is getting more and more scalable every day and its impossible to beat the immediate start up and functionality. Plus very easy to back up to encrypted object storage which is super nice for a hobby project where you might wipe the prod database at any moment.&lt;/p&gt;
    &lt;p&gt;In terms of user onboarding I ended up doing the "make an account with an email, I send a link to verify the email". I actually hate this flow and I don't really want to know a users email. I never need to contact you and there's not a lot associated with your account, which makes this especially silly. I have a ton of email addresses and no real "purpose" in having them. I'd switch to Login with Apple, which is great from a security perspective but not everybody has an Apple ID.&lt;/p&gt;
    &lt;p&gt;I also did a passkey version, which worked fine but the OSS passkey handling was pretty rough still and most people seem to be using a commercial service that handled the "do you have the passkey? Great, if not, fall back to email" flow. I don't really want to do a big commercial login service for a hobby application.&lt;/p&gt;
    &lt;p&gt;Auth is a JWT, which actually was a pain and I regret doing it. I don't know why I keep reaching for JWTs, they're a bad user experience and I should stop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I just have the source code?&lt;/head&gt;
    &lt;p&gt;I'm more than happy to release the source code once I feel like the product is in a somewhat stable shape. I'm still ripping down and rewriting relatively large chunks of it as I find weird behavior I don't like or just decide to do things a different way.&lt;/p&gt;
    &lt;p&gt;In the end it does seem to do whats on the label. We have over 600,000 individual pages indexed.&lt;/p&gt;
    &lt;head rend="h3"&gt;So how is it to use?&lt;/head&gt;
    &lt;p&gt;Honestly I've been pretty pleased. But there are some problems.&lt;/p&gt;
    &lt;p&gt;First I couldn't find a reliable way of switching the keyboard shortcuts to be Mac/Windows specific. I found some options for querying platform but they didn't seem to work, so I ended up just hardcoding them as Alt which is not great.&lt;/p&gt;
    &lt;p&gt;The other issue is that when you are making an extension, you spend a long time working with these manifests.json. The specific part I really wasn't sure about was:&lt;/p&gt;
    &lt;code&gt;"browser_specific_settings": {
    "gecko": {
      "id": "[email protected]",
      "strict_min_version": "80.0",
      "data_collection_permissions": {
        "required": ["authenticationInfo"]
      }
    }
  }&lt;/code&gt;
    &lt;p&gt;I'm not entirely sure if that's all I'm doing? I think so from reading the docs.&lt;/p&gt;
    &lt;p&gt;Anyway I built this mostly for me. I have no idea if anybody else will enjoy it. But if you are bored I encourage you to give it a try. It should be pretty light weight and straight-forward if you crack open the extension and look at it. I'm not loading any analytics into the extension so basically until people complain about it, I don't really know if its going well or not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Future stuff&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I need to sort stuff into categories so that you get more stuff in genres you like. I don't 100% know how to do that, maybe there is a way to scan a website to determine the "types" of content that is on there with machine learning? I'm still looking into it.&lt;/item&gt;
      &lt;item&gt;There's a lot of junk in there. I think if we reach a certain number of downvotes I might put it into a special "queue".&lt;/item&gt;
      &lt;item&gt;I want to ensure new users see the "best stuff" early on but there isn't enough data to determine "best vs worst".&lt;/item&gt;
      &lt;item&gt;I wish there were more independent photography and science websites. Also more crafts. That's not really a "future thing", just me putting a hope out into the universe. Non-technical beta testers get overwhelmed by technical content.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://matduggan.com/making-rss-more-fun/"/><published>2025-12-05T13:00:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46160773</id><title>Most technical problems are people problems</title><updated>2025-12-05T19:32:56.445336+00:00</updated><content>&lt;doc fingerprint="2ed1d053672c65da"&gt;
  &lt;main&gt;
    &lt;p&gt;I once worked at a company which had an enormous amount of technical debt - millions of lines of code, no unit tests, based on frameworks that were well over a decade out of date. On one specific project, we had a market need to get some Windows-only modules running on Linux, and rather than cross-compiling, another team had simply copied &amp;amp; pasted a few hundred thousand lines of code, swapping Windows-specific components for Linux-specific.&lt;/p&gt;
    &lt;p&gt;For the non-technical reader, this is an enormous problem because now two versions of the code exist. So, all features &amp;amp; bug fixes must be solved in two separate codebases that will grow apart over time. When I heard about this, a young &amp;amp; naive version of me set out to fix the situation....&lt;/p&gt;
    &lt;head rend="h2"&gt;People Problems&lt;/head&gt;
    &lt;p&gt;Tech debt projects are always a hard sell to management, because even if everything goes flawlessly, the code just does roughly what it did before. This project was no exception, and the optics weren't great. I did as many engineers do and "ignored the politics", put my head down, and got it done. But, the project went long, and I lost a lot of clout in the process.&lt;/p&gt;
    &lt;p&gt;I realized I was essentially trying to solve a people problem with a technical solution. Most of the developers at this company were happy doing the same thing today that they did yesterday...and five years ago. As Andrew Harmel-Law points out, code tends to follow the personalities of the people that wrote it. The code was calcified because the developers were also. Personality types who dislike change tend not to design their code with future change in mind.&lt;/p&gt;
    &lt;p&gt;Most technical problems are really people problems. Think about it. Why does technical debt exist? Because requirements weren't properly clarified before work began. Because a salesperson promised an unrealistic deadline to a customer. Because a developer chose an outdated technology because it was comfortable. Because management was too reactive and cancelled a project mid-flight. Because someone's ego wouldn't let them see a better way of doing things.&lt;/p&gt;
    &lt;p&gt;The core issue with the project was that admitting the need for refactoring was also to admit that the way the company was building software was broken and that individual skillsets were sorely out of date. My small team was trying to fix one module of many, while other developers were writing code as they had been for decades. I had one developer openly tell me, "I don't want to learn anything new." I realized that you'll never clean up tech debt faster than others create it. It is like triage in an emergency room, you must stop the bleeding first, then you can fix whatever is broken.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Ideal World&lt;/head&gt;
    &lt;p&gt;The project also disabused me of the engineer's ideal of a world in which engineering problems can be solved in a vacuum - staying out of "politics" and letting the work speak for itself - a world where deadlines don't exist...and let's be honest, neither do customers. This ideal world rarely exists. The vast majority of projects have non-technical stakeholders, and telling them "just trust me; we're working on it" doesn't cut it. I realized that the perception that your team is getting a lot done is just as important as getting a lot done.&lt;/p&gt;
    &lt;p&gt;Non-technical people do not intuitively understand the level of effort required or the need for tech debt cleanup; it must be communicated effectively by engineering - in both initial estimates &amp;amp; project updates. Unless leadership has an engineering background, the value of the technical debt work likely needs to be quantified and shown as business value.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heads Up&lt;/head&gt;
    &lt;p&gt;Perhaps these are the lessons that prep one for more senior positions. In my opinion, anyone above senior engineer level needs to know how to collaborate cross-functionally, regardless of whether they choose a technical or management track. Schools teach Computer Science, not navigating personalities, egos, and personal blindspots.&lt;/p&gt;
    &lt;p&gt;I have worked with some incredible engineers, better than myself - the type that have deep technical knowledge on just about any technology you bring up. When I was younger, I wanted to be that engineer - the "engineer's engineer". But I realize now, that is not my personality. I'm too ADD for that. :)&lt;/p&gt;
    &lt;p&gt;For all of their (considerable) strengths, more often than not, those engineers shy away from the interpersonal. The tragedy is that they are incredibly productive ICs, but may fail with bigger initiatives because they are only one person - a single processor core can only go so fast. Perhaps equally valuable is the "heads up coder" - the person who is deeply technical, but also able to pick their head up &amp;amp; see project risks coming (technical &amp;amp; otherwise) and steer the team around them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.joeschrag.com/2023/11/most-technical-problems-are-really.html"/><published>2025-12-05T13:07:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46160824</id><title>Show HN: Pbnj – A minimal, self-hosted pastebin you can deploy in 60 seconds</title><updated>2025-12-05T19:32:56.071932+00:00</updated><content>&lt;doc fingerprint="aa4b2d5ddfbc6a08"&gt;
  &lt;main&gt;
    &lt;code&gt;# API Reference

## Authentication

All write operations require a Bearer token:
```
Authorization: Bearer YOUR_AUTH_KEY
```

## Endpoints

### Create Paste

POST /api

#### JSON Request
```bash
curl &lt;/code&gt;
    &lt;p&gt; 05_api.md &lt;/p&gt;
    &lt;p&gt; 13 hours ago &lt;/p&gt;
    &lt;code&gt;# CLI Reference

## Installation

```bash
npm install -g @pbnjs/cli
```

## Configuration

Run the setup wizard:
```bash
pbnj --init
```

This creates ~/.pbnj with your configuration:
```
PBNJ_HOST=ht&lt;/code&gt;
    &lt;p&gt; 04_cli.md &lt;/p&gt;
    &lt;p&gt; 13 hours ago &lt;/p&gt;
    &lt;code&gt;# Cost Breakdown

"This is deployed on Cloudflare, they might charge us eventually!"

Don't worry. Let's do the math.

## Cloudflare D1 Free Tier

- 500 MB storage
- 5 million reads/day
- 100,000 writ&lt;/code&gt;
    &lt;p&gt; 03_cost.md &lt;/p&gt;
    &lt;p&gt; 13 hours ago &lt;/p&gt;
    &lt;code&gt;# Deployment Guide

## One-Click Deploy (Recommended)

Click the "Deploy to Cloudflare" button on the GitHub repo â that's it!

The deploy button automatically:
- Forks the repo to your GitHub account&lt;/code&gt;
    &lt;p&gt; 02_deployment.md &lt;/p&gt;
    &lt;p&gt; 13 hours ago &lt;/p&gt;
    &lt;code&gt;# Welcome to pbnj

pbnj is a simple, minimal self-hosted pastebin solution.

## What is pbnj?

pbnj lets you share code snippets and text files with a simple URL.
No accounts, no bloat - just paste an&lt;/code&gt;
    &lt;p&gt; 01_welcome.md &lt;/p&gt;
    &lt;p&gt; 13 hours ago &lt;/p&gt;
    &lt;code&gt;# Web Interface

pbnj includes a web interface for creating and managing pastes directly from your browser.

## Authentication

The web interface uses the same `AUTH_KEY` as the CLI and API. Authentic&lt;/code&gt;
    &lt;p&gt; 07_web_interface.md &lt;/p&gt;
    &lt;p&gt; 12 hours ago &lt;/p&gt;
    &lt;code&gt;# Configuration

pbnj is configured through a single `pbnj.config.js` file in the project root.

## Default Configuration

```js
export default {
  name: 'pbnj',
  logo: '/logo.png',
  idStyle: 'sandw&lt;/code&gt;
    &lt;p&gt; 06_configuration.md &lt;/p&gt;
    &lt;p&gt; 12 hours ago &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pbnj.sh/"/><published>2025-12-05T13:13:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46161125</id><title>Influential study on glyphosate safety retracted 25 years after publication</title><updated>2025-12-05T19:32:55.778116+00:00</updated><content>&lt;doc fingerprint="364059ff18954fcc"&gt;
  &lt;main&gt;
    &lt;p&gt;A quarter-century after its publication, one of the most influential research articles on the potential carcinogenicity of glyphosate has been retracted for "several critical issues that are considered to undermine the academic integrity of this article and its conclusions." In a retraction notice dated Friday, November 28, the journal Regulatory Toxicology and Pharmacology announced that the study, published in April 2000 and concluding the herbicide was safe, has been removed from its archives. The disavowal comes 25 years after publication and eight years after thousands of internal Monsanto documents were made public during US court proceedings (the "Monsanto Papers"), revealing that the actual authors of the article were not the listed scientists – Gary M. Williams (New York Medical College), Robert Kroes (Ritox, Utrecht University, Netherlands), and Ian C. Munro (Intertek Cantox, Canada) – but rather Monsanto employees.&lt;/p&gt;
    &lt;p&gt;Known as "ghostwriting," this practice is considered a form of scientific fraud. It involves companies paying researchers to sign their names to research articles they did not write. The motivation is clear: When a study supports the safety of a pesticide or drug, it appears far more credible if not authored by scientists employed by the company marketing the product.&lt;/p&gt;
    &lt;p&gt;You have 73.89% of this article left to read. The rest is for subscribers only.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.lemonde.fr/en/environment/article/2025/12/03/influential-study-on-glyphosate-safety-retracted-25-years-after-publication_6748114_114.html"/><published>2025-12-05T13:39:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46162656</id><title>Cloudflare outage on December 5, 2025</title><updated>2025-12-05T19:32:55.174761+00:00</updated><content>&lt;doc fingerprint="e418bb5fc591a8cb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;On December 5, 2025, at 08:47 UTC (all times in this blog are UTC), a portion of Cloudflareâs network began experiencing significant failures. The incident was resolved at 09:12 (~25 minutes total impact), when all services were fully restored.&lt;/p&gt;
      &lt;p&gt;A subset of customers were impacted, accounting for approximately 28% of all HTTP traffic served by Cloudflare. Several factors needed to combine for an individual customer to be affected as described below.&lt;/p&gt;
      &lt;p&gt;The issue was not caused, directly or indirectly, by a cyber attack on Cloudflareâs systems or malicious activity of any kind. Instead, it was triggered by changes being made to our body parsing logic while attempting to detect and mitigate an industry-wide vulnerability disclosed this week in React Server Components.&lt;/p&gt;
      &lt;p&gt;Any outage of our systems is unacceptable, and we know we have let the Internet down again following the incident on November 18. We will be publishing details next week about the work we are doing to stop these types of incidents from occurring.&lt;/p&gt;
      &lt;p&gt;The graph below shows HTTP 500 errors served by our network during the incident timeframe (red line at the bottom), compared to unaffected total Cloudflare traffic (green line at the top).&lt;/p&gt;
      &lt;p&gt;Cloudflare's Web Application Firewall (WAF) provides customers with protection against malicious payloads, allowing them to be detected and blocked. To do this, Cloudflareâs proxy buffers HTTP request body content in memory for analysis. Before today, the buffer size was set to 128KB.&lt;/p&gt;
      &lt;p&gt;As part of our ongoing work to protect customers who use React against a critical vulnerability, CVE-2025-55182, we started rolling out an increase to our buffer size to 1MB, the default limit allowed by Next.js applications, to make sure as many customers as possible were protected.&lt;/p&gt;
      &lt;p&gt;This first change was being rolled out using our gradual deployment system. During rollout, we noticed that our internal WAF testing tool did not support the increased buffer size. As this internal test tool was not needed at that time and had no effect on customer traffic, we made a second change to turn it off.&lt;/p&gt;
      &lt;p&gt;This second change of turning off our WAF testing tool was implemented using our global configuration system. This system does not perform gradual rollouts, but rather propagates changes within seconds to the entire fleet of servers in our network and is under review following the outage we experienced on November 18.Â &lt;/p&gt;
      &lt;p&gt;Unfortunately, in our FL1 version of our proxy, under certain circumstances, the second change of turning off our WAF rule testing tool caused an error state that resulted in 500 HTTP error codes to be served from our network.&lt;/p&gt;
      &lt;p&gt;As soon as the change propagated to our network, code execution in our FL1 proxy reached a bug in our rules module which led to the following Lua exception: &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;[lua] Failed to run module rulesets callback late_routing: /usr/local/nginx-fl/lua/modules/init.lua:314: attempt to index field 'execute' (a nil value)&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;resulting in HTTP code 500 errors being issued.&lt;/p&gt;
      &lt;p&gt;The issue was identified shortly after the change was applied, and was reverted at 09:12, after which all traffic was served correctly.&lt;/p&gt;
      &lt;p&gt;Customers that have their web assets served by our older FL1 proxy AND had the Cloudflare Managed Ruleset deployed were impacted. All requests for websites in this state returned an HTTP 500 error, with the small exception of some test endpoints such as &lt;code&gt;/cdn-cgi/trace&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;Customers that did not have the configuration above applied were not impacted. Customer traffic served by our China network was also not impacted.&lt;/p&gt;
      &lt;p&gt;Cloudflareâs rulesets system consists of sets of rules which are evaluated for each request entering our system. A rule consists of a filter, which selects some traffic, and an action which applies an effect to that traffic. Typical actions are â&lt;code&gt;block&lt;/code&gt;â, â&lt;code&gt;log&lt;/code&gt;â, or â&lt;code&gt;skip&lt;/code&gt;â. Another type of action is â&lt;code&gt;execute&lt;/code&gt;â, which is used to trigger evaluation of another ruleset.&lt;/p&gt;
      &lt;p&gt;Our internal logging system uses this feature to evaluate new rules before we make them available to the public. A top level ruleset will execute another ruleset containing test rules. It was these test rules that we were attempting to disable.&lt;/p&gt;
      &lt;p&gt;We have a killswitch subsystem as part of the rulesets system which is intended to allow a rule which is misbehaving to be disabled quickly. This killswitch system receives information from our global configuration system mentioned in the prior sections. We have used this killswitch system on a number of occasions in the past to mitigate incidents and have a well-defined Standard Operating Procedure, which was followed in this incident.&lt;/p&gt;
      &lt;p&gt;However, we have never before applied a killswitch to a rule with an action of â&lt;code&gt;execute&lt;/code&gt;â. When the killswitch was applied, the code correctly skipped the evaluation of the execute action, and didnât evaluate the sub-ruleset pointed to by it. However, an error was then encountered while processing the overall results of evaluating the ruleset:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;if rule_result.action == "execute" then
  rule_result.execute.results = ruleset_results[tonumber(rule_result.execute.results_index)]
end&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This code expects that, if the ruleset has action=âexecuteâ, the ârule_result.executeâ object will exist. However, because the rule had been skipped, the rule_result.execute object did not exist, and Lua returned an error due to attempting to look up a value in a nil value.&lt;/p&gt;
      &lt;p&gt;This is a straightforward error in the code, which had existed undetected for many years. This type of code error is prevented by languages with strong type systems. In our replacement for this code in our new FL2 proxy, which is written in Rust, the error did not occur.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;What about the changes being made after the incident on November 18, 2025?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We made an unrelated change that caused a similar, longer availability incident two weeks ago on November 18, 2025. In both cases, a deployment to help mitigate a security issue for our customers propagated to our entire network and led to errors for nearly all of our customer base.&lt;/p&gt;
      &lt;p&gt;We have spoken directly with hundreds of customers following that incident and shared our plans to make changes to prevent single updates from causing widespread impact like this. We believe these changes would have helped prevent the impact of todayâs incident but, unfortunately, we have not finished deploying them yet.&lt;/p&gt;
      &lt;p&gt;We know it is disappointing that this work has not been completed yet. It remains our first priority across the organization. In particular, the projects outlined below should help contain the impact of these kinds of changes:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Enhanced Rollouts &amp;amp; Versioning: Similar to how we slowly deploy software with strict health validation, data used for rapid threat response and general configuration needs to have the same safety and blast mitigation features. This includes health validation and quick rollback capabilities among other things.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Streamlined break glass capabilities: Ensure that critical operations can still be achieved in the face of additional types of failures. This applies to internal services as well as all standard methods of interaction with the Cloudflare control plane used by all Cloudflare customers.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;"Fail-Open" Error Handling: As part of the resilience effort, we are replacing the incorrectly applied hard-fail logic across all critical Cloudflare data-plane components. If a configuration file is corrupt or out-of-range (e.g., exceeding feature caps), the system will log the error and default to a known-good state or pass traffic without scoring, rather than dropping requests. Some services will likely give the customer the option to fail open or closed in certain scenarios. This will include drift-prevention capabilities to ensure this is enforced continuously.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Before the end of next week we will publish a detailed breakdown of all the resiliency projects underway, including the ones listed above. While that work is underway, we are locking down all changes to our network in order to ensure we have better mitigation and rollback systems before we begin again.&lt;/p&gt;
      &lt;p&gt;These kinds of incidents, and how closely they are clustered together, are not acceptable for a network like ours. On behalf of the team at Cloudflare we want to apologize for the impact and pain this has caused again to our customers and the Internet as a whole.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Time (UTC)&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Status&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Description&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;08:47&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;INCIDENT start&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Configuration change deployed and propagated to the network&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;08:48&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Full impact&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Change fully propagated&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;08:50&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;INCIDENT declared&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Automated alerts&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;09:11&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Change reverted&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Configuration change reverted and propagation start&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;09:12&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;INCIDENT end&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Revert fully propagated, all traffic restored&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/5-december-2025-outage/"/><published>2025-12-05T15:35:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46162872</id><title>Framework Laptop 13 gets ARM processor with 12 cores via upgrade kit</title><updated>2025-12-05T19:32:54.982804+00:00</updated><content>&lt;doc fingerprint="f36d19f305b9d557"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Framework Laptop 13 gets ARM processor with 12 cores via upgrade kit&lt;/head&gt;
    &lt;p&gt;The Qualcomm Snapdragon X Plus and Snapdragon X Elite have proven that ARM processors have earned a place in the laptop market, as devices like the Lenovo IdeaPad Slim 5 stand out with their long battery life and an affordable price point.&lt;/p&gt;
    &lt;p&gt;MetaComputing is now offering an alternative to Intel, AMD and the Snapdragon X series. Specifically, the company has introduced a mainboard that can be installe in the Framework Laptop 13 or in a mini PC case. This mainboard is equipped with a CIX CP8180 ARM chipset, which is also found inside the Minisforum MS-R1. This processor has a total of eight ARM Cortex-A720 performance cores, the two fastest can hit boost clock speeds of up to 2.6 GHz. Moreover, there are four Cortex-A520 efficiency cores.&lt;/p&gt;
    &lt;p&gt;Additionally, there’s an ARM Immortalis-G720 GPU with ten cores and an AI accelerator with a performance of 30 TOPS. This chipset is likely slower than the Snapdragon X Elite or a current flagship smartphone chip, but it should still provide enough performance for many everyday tasks. Either way, this mainboard upgrade might only be interesting for developers for the most part, because early tests show that the SoC already draws about 16 watts at idle, which means battery life will likely be fairly short when combined with the 55Wh battery of the Framework Laptop 13.&lt;/p&gt;
    &lt;head rend="h2"&gt;Price and availability&lt;/head&gt;
    &lt;p&gt;The MetaComputing ARM AI PC Kit is available now at the manufacturer’s official online shop. The base model with 16GB RAM, 1TB SSD and a mini PC case costs $549. The mainboard can be installed in a previously purchased Framework Laptop 13. Users who don’t own a Framework Laptop can order a bundle including the notebook for $999. MetaComputing charges an additional $100 for 32GB RAM. Shipping is free worldwide, but these list prices do not include import fees or taxes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Source(s)&lt;/head&gt;
    &lt;p&gt;MetaComputing (press release | product page)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.notebookcheck.net/Framework-Laptop-13-gets-ARM-processor-with-12-cores-via-upgrade-kit.1177930.0.html"/><published>2025-12-05T15:49:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163121</id><title>I'm Peter Roberts, immigration attorney who does work for YC and startups. AMA</title><updated>2025-12-05T19:32:54.513850+00:00</updated><content>&lt;doc fingerprint="d376f89b34beef3b"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;As usual, there are countless immigration topics and I'll be guided by whatever you're concerned with. Please remember that I can't provide legal advice on specific cases for obvious liability reasons because I won't have access to all the facts. Please stick to a factual discussion in your questions and comments and I'll do the same in my answers!&lt;/p&gt;
      &lt;p&gt;Previous threads we've done: https://news.ycombinator.com/submitted?id=proberts.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46163121"/><published>2025-12-05T16:04:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163308</id><title>Gemini 3 Pro: the frontier of vision AI</title><updated>2025-12-05T19:32:54.261978+00:00</updated><content>&lt;doc fingerprint="78df88171d739a97"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Gemini 3 Pro: the frontier of vision AI&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro represents a generational leap from simple recognition to true visual and spatial reasoning. It is our most capable multimodal model ever, delivering state-of-the-art performance across document, spatial, screen and video understanding.&lt;/p&gt;
    &lt;p&gt;This model sets new highs on vision benchmarks such as MMMU Pro and Video MMMU for complex visual reasoning, as well as use-case-specific benchmarks across document, spatial, screen and long video understanding.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Document understanding&lt;/head&gt;
    &lt;p&gt;Real-world documents are messy, unstructured, and difficult to parse — often filled with interleaved images, illegible handwritten text, nested tables, complex mathematical notation and non-linear layouts. Gemini 3 Pro represents a major leap forward in this domain, excelling across the entire document processing pipeline — from highly accurate Optical Character Recognition (OCR) to complex visual reasoning.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intelligent perception&lt;/head&gt;
    &lt;p&gt;To truly understand a document, a model must accurately detect and recognize text, tables, math formulas, figures and charts regardless of noise or format.&lt;/p&gt;
    &lt;p&gt;A fundamental capability is "derendering" — the ability to reverse-engineer a visual document back into structured code (HTML, LaTeX, Markdown) that would recreate it. As illustrated below, Gemini 3 demonstrates accurate perception across diverse modalities including converting an 18th-century merchant log into a complex table, or transforming a raw image with mathematical annotation into precise LaTeX code.&lt;/p&gt;
    &lt;p&gt;Example 1: Handwritten Complex Table from 18th century Albany Merchant’s Handbook (HTML transcription)&lt;/p&gt;
    &lt;p&gt;Example 2: Reconstructing equations from an image&lt;/p&gt;
    &lt;p&gt;Example 3: Reconstructing Florence Nightingale's original Polar Area Diagram into an interactive chart (with a toggle!)&lt;/p&gt;
    &lt;head rend="h3"&gt;Sophisticated reasoning&lt;/head&gt;
    &lt;p&gt;Users can rely on Gemini 3 to perform complex, multi-step reasoning across tables and charts — even in long reports. In fact, the model notably outperforms the human baseline on the CharXiv Reasoning benchmark (80.5%).&lt;/p&gt;
    &lt;p&gt;To illustrate this, imagine a user analyzing the 62-page U.S. Census Bureau "Income in the United States: 2022" report with the following prompt: “Compare the 2021–2022 percent change in the Gini index for "Money Income" versus "Post-Tax Income", and what caused the divergence in the post-tax measure, and in terms of "Money Income", does it show the lowest quintile's share rising or falling?”&lt;/p&gt;
    &lt;p&gt;Swipe through the images below to see the model's step-by-step reasoning.&lt;/p&gt;
    &lt;p&gt;Visual Extraction: To answer the Gini Index Comparison question, Gemini located and cross-referenced this info in Figure 3 about “Money Income decreased by 1.2 percent” and in Table B-3 about “Post-Tax Income increased by 3.2 percent”&lt;/p&gt;
    &lt;p&gt;Causal Logic: Crucially, Gemini 3 does not stop at the numbers; it correlates this gap with the text’s policy analysis, correctly identifying Lapse of ARPA Policies and the end of Stimulus Payments are the main causes.&lt;/p&gt;
    &lt;p&gt;Numerical Comparison: To compare the lowest quantile’s share rising or falling, Gemini3 looked at table A-3, and compared the number of 2.9 and 3.0, and concluded that “the share of aggregate household income held by the lowest quintile was rising.”&lt;/p&gt;
    &lt;p&gt;Final Model Answer&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Spatial understanding&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro is our strongest spatial understanding model so far. Combined with its strong reasoning, this enables the model to make sense of the physical world.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pointing capability: Gemini 3 has the ability to point at specific locations in images by outputting pixel-precise coordinates. Sequences of 2D points can be strung together to perform complex tasks, such as estimating human poses or reflecting trajectories over time.&lt;/item&gt;
      &lt;item&gt;Open vocabulary references: Gemini 3 identifies objects and their intent using an open vocabulary. The most direct application is robotics: the user can ask a robot to generate spatially grounded plans like, “Given this messy table, come up with a plan on how to sort the trash.” This also extends to AR/XR devices, where the user can request an AI assistant to “Point to the screw according to the user manual.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;3. Screen understanding&lt;/head&gt;
    &lt;p&gt;Gemini 3.0 Pro’s spatial understanding really shines through its screen understanding of desktop and mobile OS screens. This reliability helps make computer use agents robust enough to automate repetitive tasks. UI understanding capabilities can also enable tasks like QA testing, user onboarding and UX analytics. The following computer use demo shows the model perceiving and clicking with high precision.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Video understanding&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro takes a massive leap forward in how AI understands video, the most complex data format we interact with. It is dense, dynamic, multimodal and rich with context.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;High frame rate understanding: We have optimized the model to be much stronger at understanding fast-paced actions when sampling at &amp;gt;1 frames-per-second. Gemini 3 Pro can capture rapid details — vital for tasks like analyzing golf swing mechanics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By processing video at 10 FPS—10x the default speed—Gemini 3 Pro catches every swing and shift in weight, unlocking deep insights into player mechanics.&lt;/p&gt;
    &lt;p&gt;2. Video reasoning with “thinking” mode: We upgraded "thinking" mode to go beyond object recognition toward true video reasoning. The model can now better trace complex cause-and-effect relationships over time. Instead of just identifying what is happening, it understands why it is happening.&lt;/p&gt;
    &lt;p&gt;3. Turning long videos into action: Gemini 3 Pro bridges the gap between video and code. It can extract knowledge from long-form content and immediately translate it into functioning apps or structured code.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Real-world applications&lt;/head&gt;
    &lt;p&gt;Here are a few ways we think various fields will benefit from Gemini 3’s capabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Education&lt;/head&gt;
    &lt;p&gt;Gemini 3.0 Pro’s enhanced vision capabilities drive significant gains in the education field, particularly for diagram-heavy questions central to math and science. It successfully tackles the full spectrum of multimodal reasoning problems found from middle school through post-secondary curriculums. This includes visual reasoning puzzles (like Math Kangaroo) and complex chemistry and physics diagrams.&lt;/p&gt;
    &lt;p&gt;Gemini 3’s visual intelligence also powers the generative capabilities of Nano Banana Pro. By combining advanced reasoning with precise generation, the model, for example, can help users identify exactly where they went wrong in a homework problem.&lt;/p&gt;
    &lt;p&gt;Prompt: “Here is a photo of my homework attempt. Please check my steps and tell me where I went wrong. Instead of explaining in text, show me visually on my image.” (Note: Student work is shown in blue; model corrections are shown in red). [See prompt in Google AI Studio]&lt;/p&gt;
    &lt;head rend="h3"&gt;Medical and biomedical imaging&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro 1 stands as our most capable general model for medical and biomedical imagery understanding, achieving state-of-the-art performance across major public benchmarks in MedXpertQA-MM (a difficult expert-level medical reasoning exam), VQA-RAD (radiology imagery Q&amp;amp;A) and MicroVQA (multimodal reasoning benchmarks for microscopy based biological research).&lt;/p&gt;
    &lt;p&gt;Input image from MicroVQA - a benchmark for microscopy-based biological research&lt;/p&gt;
    &lt;head rend="h3"&gt;Law and finance&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro’s enhanced document understanding helps professionals in finance and law tackle highly complex workflows. Finance platforms can seamlessly analyze dense reports filled with charts and tables, while legal platforms benefit from the model's sophisticated document reasoning.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Media resolution control&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro improves the way it processes visual inputs by preserving the native aspect ratio of images. This drives significant quality improvements across the board.&lt;lb/&gt;Additionally, developers gain granular control over performance and cost via the new media_resolution parameter. This allows you to tune visual token usage to balance fidelity against consumption:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High resolution: Maximizes fidelity for tasks requiring fine detail, such as dense OCR or complex document understanding.&lt;/item&gt;
      &lt;item&gt;Low resolution: Optimizes for cost and latency on simpler tasks, such as general scene recognition or long-context tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For specific recommendations, refer to our Gemini 3.0 Documentation Guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;Build with Gemini 3 Pro&lt;/head&gt;
    &lt;p&gt;We are excited to see what you build with these new capabilities. To get started, check out our developer documentation or play with the model in Google AI Studio today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/developers/gemini-3-pro-vision/"/><published>2025-12-05T16:15:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163609</id><title>Patterns for Defensive Programming in Rust</title><updated>2025-12-05T19:32:53.912981+00:00</updated><content>&lt;doc fingerprint="2c79df8a6ea533d8"&gt;
  &lt;main&gt;
    &lt;p&gt;I have a hobby.&lt;/p&gt;
    &lt;p&gt;Whenever I see the comment &lt;code&gt;// this should never happen&lt;/code&gt; in code, I try to find out the exact conditions under which it could happen.
And in 90% of cases, I find a way to do just that.
More often than not, the developer just hasn’t considered all edge cases or future code changes.&lt;/p&gt;
    &lt;p&gt;In fact, the reason why I like this comment so much is that it often marks the exact spot where strong guarantees fall apart. Often, violating implicit invariants that aren’t enforced by the compiler are the root cause.&lt;/p&gt;
    &lt;p&gt;Yes, the compiler prevents memory safety issues, and the standard library is best-in-class. But even the standard library has its warts and bugs in business logic can still happen.&lt;/p&gt;
    &lt;p&gt;All we can work with are hard-learned patterns to write more defensive Rust code, learned throughout years of shipping Rust code to production. I’m not talking about design patterns here, but rather small idioms, which are rarely documented, but make a big difference in the overall code quality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: Indexing Into a Vector&lt;/head&gt;
    &lt;p&gt;Here’s some innocent-looking code:&lt;/p&gt;
    &lt;code&gt;if !matching_users.is_empty   
&lt;/code&gt;
    &lt;p&gt;What if you refactor it and forget to keep the &lt;code&gt;is_empty()&lt;/code&gt; check?
The problem is that the vector indexing is decoupled from checking the length.
So &lt;code&gt;matching_users[0]&lt;/code&gt; can panic at runtime if the vector is empty.&lt;/p&gt;
    &lt;p&gt;Checking the length and indexing are two separate operations, which can be changed independently. That’s our first implicit invariant that’s not enforced by the compiler.&lt;/p&gt;
    &lt;p&gt;If we use slice pattern matching instead, we’ll only get access to the element if the correct &lt;code&gt;match&lt;/code&gt; arm is executed.&lt;/p&gt;
    &lt;code&gt;match matching_users.as_slice   
&lt;/code&gt;
    &lt;p&gt;Note how this automatically uncovered one more edge case: what if the list is empty? We hadn’t explicitly considered this case before. The compiler-enforced pattern matching requires us to think about all possible states! This is a common pattern in all robust Rust code: putting the compiler in charge of enforcing invariants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: Lazy use of &lt;code&gt;Default&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;When initializing an object with many fields, it’s tempting to use &lt;code&gt;..Default::default()&lt;/code&gt; to fill in the rest.
In practice, this is a common source of bugs.
You might forget to explicitly set a new field later when you add it to the struct (thus using the default value instead, which might not be what you want), or you might not be aware of all the fields that are being set to default values.&lt;/p&gt;
    &lt;p&gt;Instead of this:&lt;/p&gt;
    &lt;code&gt;let foo = Foo ;
&lt;/code&gt;
    &lt;p&gt;Do this:&lt;/p&gt;
    &lt;code&gt;let foo = Foo ;
&lt;/code&gt;
    &lt;p&gt;Yes, it’s slightly more verbose, but what you gain is that the compiler will force you to handle all fields explicitly. Now when you add a new field to &lt;code&gt;Foo&lt;/code&gt;, the compiler will remind you to set it here as well and reflect on which value makes sense.&lt;/p&gt;
    &lt;p&gt;If you still prefer to use &lt;code&gt;Default&lt;/code&gt; but don’t want to lose compiler checks, you can also destructure the default instance:&lt;/p&gt;
    &lt;code&gt;let Foo   =  default;
&lt;/code&gt;
    &lt;p&gt;This way, you get all the default values assigned to local variables and you can still override what you need:&lt;/p&gt;
    &lt;code&gt;let foo = Foo ;
&lt;/code&gt;
    &lt;p&gt;This pattern gives you the best of both worlds:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You get default values without duplicating default logic&lt;/item&gt;
      &lt;item&gt;The compiler will complain when new fields are added to the struct&lt;/item&gt;
      &lt;item&gt;Your code automatically adapts when default values change&lt;/item&gt;
      &lt;item&gt;It’s clear which fields use defaults and which have custom values&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Code Smell: Fragile Trait Implementations&lt;/head&gt;
    &lt;p&gt;Completely destructuring a struct into its components can also be a defensive strategy for API adherence. For example, let’s say you’re building a pizza ordering system and have an order type like this:&lt;/p&gt;
    &lt;p&gt;For your order tracking system, you want to compare orders based on what’s actually on the pizza - the &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;toppings&lt;/code&gt;, and &lt;code&gt;crust_type&lt;/code&gt;. The &lt;code&gt;ordered_at&lt;/code&gt; timestamp shouldn’t affect whether two orders are considered the same.&lt;/p&gt;
    &lt;p&gt;Here’s the problem with the obvious approach:&lt;/p&gt;
    &lt;p&gt;Now imagine your team adds a field for customization options:&lt;/p&gt;
    &lt;p&gt;Your &lt;code&gt;PartialEq&lt;/code&gt; implementation still compiles, but is it correct?
Should &lt;code&gt;extra_cheese&lt;/code&gt; be part of the equality check?
Probably yes - a pizza with extra cheese is a different order!
But you’ll never know because the compiler won’t remind you to think about it.&lt;/p&gt;
    &lt;p&gt;Here’s the defensive approach using destructuring:&lt;/p&gt;
    &lt;p&gt;Now when someone adds the &lt;code&gt;extra_cheese&lt;/code&gt; field, this code won’t compile anymore.
The compiler forces you to decide: should &lt;code&gt;extra_cheese&lt;/code&gt; be included in the comparison or explicitly ignored with &lt;code&gt;extra_cheese: _&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;This pattern works for any trait implementation where you need to handle struct fields: &lt;code&gt;Hash&lt;/code&gt;, &lt;code&gt;Debug&lt;/code&gt;, &lt;code&gt;Clone&lt;/code&gt;, etc.
It’s especially valuable in codebases where structs evolve frequently as requirements change.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: &lt;code&gt;From&lt;/code&gt; Impls That Are Really &lt;code&gt;TryFrom&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Sometimes there’s no conversion that will work 100% of the time. That’s fine. When that’s the case, resist the temptation to offer a &lt;code&gt;From&lt;/code&gt; implementation out of habit; use &lt;code&gt;TryFrom&lt;/code&gt; instead.&lt;/p&gt;
    &lt;p&gt;Here’s an example of &lt;code&gt;TryFrom&lt;/code&gt; in disguise:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;unwrap_or_else&lt;/code&gt; is a hint that this conversion can fail in some way.
We set a default value instead, but is it really the right thing to do for all callers?
This should be a &lt;code&gt;TryFrom&lt;/code&gt; implementation instead, making the fallible nature explicit.
We fail fast instead of continuing with a potentially flawed business logic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: Non-Exhaustive Matches&lt;/head&gt;
    &lt;p&gt;It’s tempting to use &lt;code&gt;match&lt;/code&gt; in combination with a catch-all pattern like &lt;code&gt;_ =&amp;gt; {}&lt;/code&gt;, but this can haunt you later.
The problem is that you might forget to handle a new case that was added later.&lt;/p&gt;
    &lt;p&gt;Instead of:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;p&gt;Use:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;p&gt;By spelling out all variants explicitly, the compiler will warn you when a new variant is added, forcing you to handle it. Another case of putting the compiler to work.&lt;/p&gt;
    &lt;p&gt;If the code for two variants is the same, you can group them:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;head rend="h2"&gt;Code Smell: &lt;code&gt;_&lt;/code&gt; Placeholders for Unused Variables&lt;/head&gt;
    &lt;p&gt;Using &lt;code&gt;_&lt;/code&gt; as a placeholder for unused variables can lead to confusion.
For example, you might get confused about which variable was skipped.
That’s especially true for boolean flags:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;p&gt;In the above example, it’s not clear which variables were skipped and why. Better to use descriptive names for the variables that are not used:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;p&gt;Even if you don’t use the variables, it’s clear what they represent and the code becomes more readable and easier to review without inline type hints.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern: Temporary Mutability&lt;/head&gt;
    &lt;p&gt;If you only want your data to be mutable temporarily, make that explicit.&lt;/p&gt;
    &lt;code&gt;let mut data = get_vec;
data.sort;
let data = data;  // Shadow to make immutable

// Here `data` is immutable.
&lt;/code&gt;
    &lt;p&gt;This pattern is often called “temporary mutability” and helps prevent accidental modifications after initialization. See the Rust unofficial patterns book for more details.&lt;/p&gt;
    &lt;p&gt;You can go one step further and do the initialization part in a scope block:&lt;/p&gt;
    &lt;code&gt;let data = ;
// Here `data` is immutable
&lt;/code&gt;
    &lt;p&gt;This way, the mutable variable is confined to the inner scope, making it clear that it’s only used for initialization. In case you use any temporary variables during initialization, they won’t leak into the outer scope. In our case above, there were none, but imagine if we had a temporary vector to hold intermediate results:&lt;/p&gt;
    &lt;code&gt;let data = ;
&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;temp&lt;/code&gt; is only accessible within the inner scope, which prevents it from accidental use later on.&lt;/p&gt;
    &lt;p&gt;This is especially useful when you have multiple temporary variables during initialization that you don’t want accessible in the rest of the function. The scope makes it crystal clear that these variables are only meant for initialization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern: Defensively Handle Constructors&lt;/head&gt;
    &lt;p&gt;Tip for libraries&lt;/p&gt;
    &lt;p&gt;The following pattern is only truly helpful for libraries and APIs that need to be robust against future changes. In such a case, you want to ensure that all instances of a type are created through a constructor function that enforces validation logic. Because without that, future refactorings can easily lead to invalid states.&lt;/p&gt;
    &lt;p&gt;For application code, it’s probably best to keep things simple. You typically have all the call sites under control and can ensure that validation logic is always called.&lt;/p&gt;
    &lt;p&gt;Let’s say you have a simple type like the following:&lt;/p&gt;
    &lt;p&gt;Now you want to add validation logic to ensure invalid states are never created. One pattern is to return a &lt;code&gt;Result&lt;/code&gt; from the constructor:&lt;/p&gt;
    &lt;p&gt;But nothing stops someone from bypassing your validation by creating an instance directly:&lt;/p&gt;
    &lt;code&gt;let s = S ;
&lt;/code&gt;
    &lt;p&gt;This should not be possible! It is our implicit invariant that’s not enforced by the compiler: the validation logic is decoupled from struct construction. These are two separate operations, which can be changed independently and the compiler won’t complain.&lt;/p&gt;
    &lt;p&gt;To force external code to go through your constructor, add a private field:&lt;/p&gt;
    &lt;p&gt;Now code outside your module cannot construct &lt;code&gt;S&lt;/code&gt; directly because it cannot access the &lt;code&gt;_private&lt;/code&gt; field.
The compiler enforces that all construction must go through your &lt;code&gt;new()&lt;/code&gt; method, which includes your validation logic!&lt;/p&gt;
    &lt;p&gt;Why the underscore in &lt;code&gt;_private&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Note that the underscore prefix is just a naming convention to indicate the field is intentionally unused; it’s the lack of &lt;code&gt;pub&lt;/code&gt; that makes it private and prevents external construction.&lt;/p&gt;
    &lt;p&gt;For libraries that need to evolve over time, you can also use the &lt;code&gt;#[non_exhaustive]&lt;/code&gt; attribute instead:&lt;/p&gt;
    &lt;p&gt;This has the same effect of preventing construction outside your crate, but also signals to users that you might add more fields in the future. The compiler will prevent them from using struct literal syntax, forcing them to use your constructor.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Should you use #[non_exhaustive]&lt;/code&gt; or &lt;code&gt;_private&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;There’s a big difference between these two approaches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;#[non_exhaustive]&lt;/code&gt;only works across crate boundaries. It prevents construction outside your crate.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;_private&lt;/code&gt;works at the module boundary. It prevents construction outside the module, but within the same crate.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On top of that, some developers find &lt;code&gt;_private: ()&lt;/code&gt; more explicit about intent: “this struct has a private field that prevents construction.”&lt;/p&gt;
    &lt;p&gt;With &lt;code&gt;#[non_exhaustive]&lt;/code&gt;, the primary intent is signaling that fields might be added in the future, and preventing construction is more of a side effect.&lt;/p&gt;
    &lt;p&gt;But what about code within the same module? With the patterns above, code in the same module can still bypass your validation:&lt;/p&gt;
    &lt;code&gt;// Still compiles in the same module!
let s = S ;
&lt;/code&gt;
    &lt;p&gt;Rust’s privacy works at the module level, not the type level. Anything in the same module can access private items.&lt;/p&gt;
    &lt;p&gt;If you need to enforce constructor usage even within your own module, you need a more defensive approach using nested private modules:&lt;/p&gt;
    &lt;code&gt; 

// Re-export for public use
pub use  S;
&lt;/code&gt;
    &lt;p&gt;Now even code in your outer module cannot construct &lt;code&gt;S&lt;/code&gt; directly because &lt;code&gt;Seal&lt;/code&gt; is trapped in the private &lt;code&gt;inner&lt;/code&gt; module.
Only the &lt;code&gt;new()&lt;/code&gt; method, which lives in the same module as &lt;code&gt;Seal&lt;/code&gt;, can construct it.
The compiler guarantees that all construction, even internal construction, goes through your validation logic.&lt;/p&gt;
    &lt;p&gt;You could still access the public fields directly, though.&lt;/p&gt;
    &lt;code&gt;let s =  new.unwrap;
s.field1 = "".to_string; // Still possible to mutate fields directly
&lt;/code&gt;
    &lt;p&gt;To prevent that, you can make the fields private and provide getter methods instead:&lt;/p&gt;
    &lt;p&gt;Now the only way to create an instance of &lt;code&gt;S&lt;/code&gt; is through the &lt;code&gt;new()&lt;/code&gt; method, and the only way to access its fields is through the getter methods.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to Use Each&lt;/head&gt;
    &lt;p&gt;To enforce validation through constructors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For external code: Add a private field like &lt;code&gt;_private: ()&lt;/code&gt;or use&lt;code&gt;#[non_exhaustive]&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;For internal code: Use nested private modules with a private “seal” type&lt;/item&gt;
      &lt;item&gt;Choose based on your needs: Most code only needs to prevent external construction; forcing internal construction is more defensive but also more complex&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key insight is that by making construction impossible without access to a private type, you turn your validation logic from a convention into a guarantee enforced by the compiler. So let’s put that compiler to work!&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern: Use &lt;code&gt;#[must_use]&lt;/code&gt; on Important Types&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;#[must_use]&lt;/code&gt; attribute is often neglected.
That’s sad, because it’s such a simple yet powerful mechanism to prevent callers from accidentally ignoring important return values.&lt;/p&gt;
    &lt;p&gt;Now if someone creates a &lt;code&gt;Config&lt;/code&gt; but forgets to use it, the compiler will warn them
(even with a custom message!):&lt;/p&gt;
    &lt;code&gt;let config =  new;
// Warning: Configuration must be applied to take effect
config.with_timeout; 

// Correct usage:
let config =  new 
    .with_timeout;
apply_config;
&lt;/code&gt;
    &lt;p&gt;This is especially useful for guard types that need to be held for their lifetime and results from operations that must be checked. The standard library uses this extensively. For example, &lt;code&gt;Result&lt;/code&gt; is marked with &lt;code&gt;#[must_use]&lt;/code&gt;, which is why you get warnings if you don’t handle errors.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: Boolean Parameters&lt;/head&gt;
    &lt;p&gt;Boolean parameters make code hard to read at the call site and are error-prone. We all know the scenario where we’re sure this will be the last boolean parameter we’ll ever add to a function.&lt;/p&gt;
    &lt;code&gt;// Too many boolean parameters
 

// At the call site, what do these booleans mean?
process_data;  // What does this do?
&lt;/code&gt;
    &lt;p&gt;It’s impossible to understand what this code does without looking at the function signature. Even worse, it’s easy to accidentally swap the boolean values.&lt;/p&gt;
    &lt;p&gt;Instead, use enums to make the intent explicit:&lt;/p&gt;
    &lt;code&gt; 

 

 

 

// Now the call site is self-documenting
process_data;
&lt;/code&gt;
    &lt;p&gt;This is much more readable and the compiler will catch mistakes if you pass the wrong enum type. You will notice that the enum variants can be more descriptive than just &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;.
And more often than not, there are more than two meaningful options; especially for programs which grow over time.&lt;/p&gt;
    &lt;p&gt;For functions with many options, you can configure them using a parameter struct:&lt;/p&gt;
    &lt;code&gt; 

 

 

// Usage with preset configurations
process_data;

// Or customize for specific needs
process_data;
&lt;/code&gt;
    &lt;p&gt;This approach scales much better as your function evolves. Adding new parameters doesn’t break existing call sites, and you can easily add defaults or make certain fields optional. The preset methods also document common use cases and make it easy to use the right configuration for different scenarios.&lt;/p&gt;
    &lt;p&gt;Rust is often criticized for not having named parameters, but using a parameter struct is arguably even better for larger functions with many options.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clippy Lints for Defensive Programming&lt;/head&gt;
    &lt;p&gt;Many of these patterns can be enforced automatically using Clippy lints. Here are the most relevant ones:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Lint&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::indexing_slicing&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Prevents direct indexing into slices and vectors&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::fallible_impl_from&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Warns about &lt;code&gt;From&lt;/code&gt; implementations that can panic and should be &lt;code&gt;TryFrom&lt;/code&gt; instead.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::wildcard_enum_match_arm&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disallows wildcard &lt;code&gt;_&lt;/code&gt; patterns.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::unneeded_field_pattern&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Identifies when you’re ignoring too many struct fields with &lt;code&gt;..&lt;/code&gt; unnecessarily.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::fn_params_excessive_bools&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Warns when a function has too many boolean parameters (4 or more by default).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::must_use_candidate&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Suggests adding &lt;code&gt;#[must_use]&lt;/code&gt; to types that are good candidates for it.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You can enable these in your project by adding them at the top of your crate, e.g.&lt;/p&gt;
    &lt;p&gt;Or in your &lt;code&gt;Cargo.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[]
= "deny"
  = "deny"
  = "deny"
  = "deny"
  = "deny"
  = "deny"
  &lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Defensive programming in Rust is about leveraging the type system and compiler to catch bugs before they happen. By following these patterns, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make implicit invariants explicit and compiler-checked&lt;/item&gt;
      &lt;item&gt;Future-proof your code against refactoring mistakes&lt;/item&gt;
      &lt;item&gt;Reduce the surface area for bugs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s a skill that doesn’t come naturally and it’s not covered in most Rust books, but knowing these patterns can make the difference between code that works but is brittle, and code that is robust and maintainable for years to come.&lt;/p&gt;
    &lt;p&gt;Remember: if you find yourself writing &lt;code&gt;// this should never happen&lt;/code&gt;, take a step back and ask how the compiler could enforce that invariant for you instead.
The best bug is the one that never compiles in the first place.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://corrode.dev/blog/defensive-programming/"/><published>2025-12-05T16:34:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163671</id><title>Synadia and TigerBeetle Pledge $512,000 to the Zig Software Foundation</title><updated>2025-12-05T19:32:53.686006+00:00</updated><content>&lt;doc fingerprint="bf70d8bf88a3ab96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Synadia and TigerBeetle Pledge $512,000 to the Zig Software Foundation&lt;/head&gt;
    &lt;p&gt;Synadia and TigerBeetle have together pledged $512,000 to the Zig Software Foundation over the next two years in support of the language, leadership, and communities building the future of simpler systems software.&lt;/p&gt;
    &lt;p&gt;I first saw Zig in 2018, seven years ago. Two years later, I chose Zig over C or Rust for TigerBeetle.&lt;/p&gt;
    &lt;p&gt;In 2020, I was following Rust closely. At the time, Rustâs default memory philosophy was to crash when out of memory (OOM). However, for TigerBeetle, I wanted explicit static allocation, following NASAâs Power of Ten Rules for Safety-Critical Code, which would become a part of TigerStyle, a methodology for creating safer software in less time.&lt;/p&gt;
    &lt;p&gt;What I learned is that if you could centralize resource allocation in time and space (the dimensions that prove tricky for humans writing software) then this could not only simplify memory management, to design away some of the need for a borrow checker in the first place, but, more importantly, also be a forcing function for propagating good design, to encourage teams to think through the explicit limits or physics of the software (you have no choice).&lt;/p&gt;
    &lt;p&gt;From a performance perspective, I didnât want TigerBeetle to be fearlessly multithreaded. Transaction processing workloads tend to have inherent contention, even to the point of power law, precluding partitioning and necessitating a single-threaded architecture. Therefore, Rustâs borrow checker, while a phenomenal tool for the class of problems it targets, made less sense for TigerBeetle. TigerBeetle never frees memory and never runs multithreaded, instead using explicit submission/completion queue interfaces by design.&lt;/p&gt;
    &lt;p&gt;Finally, while the borrow checker could achieve local memory safety, TigerBeetle needed more correctness properties. TigerBeetle needed to be always correct, and across literally thousands of invariants. As matklad would say, this is a harder problem! I had also spent enough time in memory safe languages to know that local memory safety is no guarantee of local correctness, let alone distributed system correctness. Per systems thinking, I believe that total correctness is a design problem, not a language problem. Language is valuable. But no human language can guarantee the next Hemingway or Nabokov. For this you need philosophy. Even then itâs not a guarantee but a probability.&lt;/p&gt;
    &lt;p&gt;With Rust off the table, the choice fell to C or Zig. A language of the past or future?&lt;/p&gt;
    &lt;p&gt;Zig was early, which gave me pause, but I felt that the quality of Andrew Kelleyâs design decisions in the language, the standard library (e.g. the unmanaged hashmap interface) and the cross-compilation toolchain, even five years ago, was already exceptional.&lt;/p&gt;
    &lt;p&gt;Andrewsâs philosophy resonated with what I wanted to explore in TigerStyle. No hidden memory allocations. No hidden control flow. No preprocessor. No macros. And then you get things like comptime, reducing the grammar and dimensionality of the language, while simultaneously multiplying its power. The primary benefit of Zig is the favorable ratio of expressivity to language complexity.&lt;/p&gt;
    &lt;p&gt;As a replacement for C, Zig fixed not only the small cuts, such as explicit alignment in the type system for Direct I/O, or safer casts, but the showstoppers of spatial memory safety through bounds checking, and, to a lesser degree (but not guarantee), temporal memory safety through the debug allocator.&lt;/p&gt;
    &lt;p&gt;Zig also enabled checked arithmetic by default in safe builds, which is something I believe only Ada and Swift do (remarkably, Rust disables checked arithmetic by default in safe buildsâa default I would love to see changed). TigerBeetle separates the data plane from the control plane by design, through batching, so the runtime cost of these safety checks was not material, being amortized in the data plane across bigger buffers. While a borrow checker or static allocation can simplify memory management, getting logic and arithmetic correct remains hard. Of course, you can enable checked arithmetic in other languages, but I appreciated Andrewâs concern for checked arithmetic and stricter operands by default.&lt;/p&gt;
    &lt;p&gt;In all these things, what impressed me most was Zigâs approach to safety when working with the metal. Not in terms of an on/off decision, but as a spectrum. Not aiming for 100% guarantees across 1 or 2 categories, but 90% and then across more categories. Not eliminating classes of bugs, but downgrading their probability. All while preserving the power-to-weight ratio of the language, to keep the language beautifully simple.&lt;/p&gt;
    &lt;p&gt;Many languages start simple and grow complex as features are added. Zigâs simplicity is unusual in that it comes from a subtractive discipline (e.g. no private fields) rather than a deferred complexity; minimizing surface area is part of the ethos of the language. The simplicity of Zig meant that we could hire great programmers from any language backgroundâthey could pick up Zig in a weekend. Indeed, Iâve never had to talk to a new hire about learning Zig.&lt;/p&gt;
    &lt;p&gt;Finally, there was the timing. Recognizing that TigerBeetle would take time to reach production (we shipped production in 2024, after 3.5 years of development), giving Zig time to mature, for our trajectories to intersect.&lt;/p&gt;
    &lt;p&gt;Investing in creating a database like TigerBeetle is a long term effort. Databases tend to have a long half life (e.g. Postgres is 30 years old). And so, while Zig being early in 2020 did give me pause, nevertheless Zigâs quality, philosophy and simplicity made sense for a multi-decade horizon.&lt;/p&gt;
    &lt;p&gt;How has the decision for Zig panned out?&lt;/p&gt;
    &lt;p&gt;TigerBeetle is tested end-to-end under some pretty extreme fuzzing. We did have three bugs that would have been prevented by the borrow checker, but these were caught by our fuzzers and online verification. We run a fuzzing fleet of 1,000 dedicated CPU cores 24/7. We invest in deterministic simulation testing (e.g. VOPR), as well as non-deterministic fault-injection harnesses (e.g. VÃ¶rtex). We engaged Kyle Kingsbury in one of the longest Jepsen audits to dateâfour times the typical duration. Through all this, Zigâs quality held up flawlessly.&lt;/p&gt;
    &lt;p&gt;Zig has also been a huge part of our success as a company. TigerBeetle is only 5 years old but is already migrating some of the largest brokerages, exchanges and wealth managements in their respective jurisdictions. Several of our key enterprise contracts were thanks to the CTOs and even CEOs of these companies also following Zig and seeing the quality we wanted to achieve with it. I donât think we could have written TigerBeetle as it is, in any other language, at least not to the same tight tolerances, let alone with the same velocity.&lt;/p&gt;
    &lt;p&gt;Zigâs language specification will only reach 1.0 when all experimental areas of the language (e.g. async I/O) are finally done. For TigerBeetle, we care only about the stable language features we use, testing our binaries end to end, as we would for any language. Nevertheless, upgrading to new versions, even with breaking changes, has only been a pleasure for us as a team. The upgrade work is usually fully paid for by compilation time reduction. For example, the upgrade from Zig 0.14.1 to Zig 0.15.2 (with the native x86_64 backend) makes debug builds 2x faster, and even LLVM release builds become 1.6x faster. With each release, you can literally feel the sheer amount of effort that the entire Zig core team put into making Zig the worldâs most powerful programming languageâand toolchain.&lt;/p&gt;
    &lt;p&gt;Back in 2020, from a production perspective, Zig was more or less a frontend to LLVM, the same compiler used by Rust, Swift and other languages. However, by not shying away from also investing in its own independent compiler backends and toolchain, by appreciating the value of replacing LLVM long term, Zig is becoming well positioned to gain a low-level precision and compilation speed that generic LLVM wonât always be able to match.&lt;/p&gt;
    &lt;p&gt;We want Andrew to take his time, to get these things right for the long term. Fred Brooks once said that conceptual integrity is âthe most important considerationâ in system design, that the design must proceed from one mind.&lt;/p&gt;
    &lt;p&gt;In this spirit, I am grateful for Andrewâs remarkably strong leadership (and taste) in the design of the language and toolchain. There can be thankless pressure on an open source project to give in to the tragedy of the commons. But if anything, in hindsight I think this is what Iâve most appreciated about choosing Zig for TigerBeetle, that Zig has a strong BDFL.&lt;/p&gt;
    &lt;p&gt;Of course, some may hear âBDFLâ and see governance risk. But I fear the opposite: conceptual risk, the harder problem. Brooks was rightâconceptual integrity is almost certainly doomed by committee. Whereas governance is easier solved: put it in the hands, not of the corporates, but of the people. The individuals who choose each day to continue to donate.&lt;/p&gt;
    &lt;p&gt;This is why our pledge today, along with all other ZSF donors, is a simple donation with no strings attached. The Zig Software Foundation is well managed, transparent and independent. We want it to remain this way. The last thing we want is some kind of foundation âseatâ. Andrew is Chef. We want to let him cook, and pay his core team sustainably (e.g. 92% percent of budget goes to directly paying contributors).&lt;/p&gt;
    &lt;p&gt;If cooking is one metaphor, then surfing is another. I believe that technology moves in waves. The art is not in paddling to the wave with a thousand surfers on it. But in spotting the swell before it breaks. And then enjoying the ride with the early adopters who did the same. River, Ghostty, Bun, Mach and many fellow surfers.&lt;/p&gt;
    &lt;p&gt;In fact, it was through Zig that I met Derek Collison, who like me had been sponsoring the language in his personal capacity since 2018. As a former CTO at VMware, Derek was responsible for backing antirez to work full time on Redis. Derek later went on to create NATS, founding Synadia.&lt;/p&gt;
    &lt;p&gt;As we were about to increase TigerBeetleâs yearly donation to Zig, I reached out to Derek, and we decided to do a joint announcement, following Mitchell Hashimotoâs lead. For each of our companies to donate $256,000 in monthly installments over the next two years, with Synadia matching TigerBeetle, for a total of $512,000âthe first installment already made.&lt;/p&gt;
    &lt;p&gt;Please consider donating or increasing your donation if you can. And if you are a CEO or CTO, please team up with another company to outmatch us! Thanks Andrew for creating something special, and to all who code for the joy of the craft:&lt;/p&gt;
    &lt;p&gt;Together we serve the users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tigerbeetle.com/blog/2025-10-25-synadia-and-tigerbeetle-pledge-512k-to-the-zig-software-foundation/"/><published>2025-12-05T16:38:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163804</id><title>Jony Ive's OpenAI Device Barred from Using 'Io' Name</title><updated>2025-12-05T19:32:53.556875+00:00</updated><content>&lt;doc fingerprint="b4064b0d10aa71f0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Jony Ive's OpenAI Device Barred From Using 'io' Name&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;A U.S. appeals court has upheld a temporary restraining order that prevents OpenAI and Jony Ive's new hardware venture from using the name "io" for products similar to those planned by AI audio startup iyO, Bloomberg Law reports.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;iyO sued OpenAI earlier this year after the latter announced its partnership with Ive's new firm, arguing that OpenAI's planned "io" branding was too close to its own name and related to similar AI-driven hardware. Court filings later showed that Ive and Sam Altman chose the name io in mid-2023, and that iyO CEO Jason Rugolo had approached Altman in early 2025 seeking funding for a project about "the future of human-computer interface." Altman declined, saying he was already working on "something competitive."&lt;/p&gt;
          &lt;p&gt;OpenAI countered that io's first product would not be a wearable device, and that Rugolo had voluntarily disclosed details about iyO while suggesting OpenAI acquire his company for $200 million. Despite this, a district court issued a temporary restraining order blocking OpenAI, Altman, Ive, and IO Products, Inc. from using the io mark in connection with products deemed sufficiently similar to iyO's planned AI-audio computer. OpenAI removed its io branding shortly after.&lt;/p&gt;
          &lt;p&gt;The Ninth Circuit affirmed the order earlier this week. The court agreed there was a likelihood of confusion between "IO" and "iyO," that reverse confusion was a significant risk given OpenAI's size, and that iyO could face irreparable harm to its brand and fundraising. However, the ruling does not bar all uses of the io name, only marketing and selling hardware similar to iyO's.&lt;/p&gt;
          &lt;p&gt;The case now returns to the district court for a preliminary injunction hearing in April 2026, with the broader litigation expected to extend into 2027 and 2028. OpenAI's first hardware device is expected to launch next year.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;We're getting closer to the launch of the final major iOS update of the year, with Apple set to release iOS 26.2 in December. We've had three betas so far and are expecting a fourth beta or a release candidate this week, so a launch could follow as soon as next week. Past Launch Dates Apple's past iOS x.2 updates from the last few years have all happened right around the middle of the...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple is encouraging iPhone users who are still running iOS 18 to upgrade to iOS 26 by making the iOS 26 software upgrade option more prominent. Since iOS 26 launched in September, it has been displayed as an optional upgrade at the bottom of the Software Update interface in the Settings app. iOS 18 has been the default operating system option, and users running iOS 18 have seen iOS 18...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple is expected to launch a new foldable iPhone next year, based on multiple rumors and credible sources. The long-awaited device has been rumored for years now, but signs increasingly suggest that 2026 could indeed be the year that Apple releases its first foldable device. Subscribe to the MacRumors YouTube channel for more videos. Below, we've collated an updated set of key details that ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today seeded the release candidate versions of upcoming iOS 26.2 and iPadOS 26.2 updates to developers and public beta testers, with the software coming two weeks after Apple seeded the third betas. The release candidates represent the final versions of iOS 26.2 and iPadOS 26.2 that will be provided to the public if no further bugs are found during this final week of testing....&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;In a statement shared with Bloomberg on Wednesday, Apple confirmed that its software design chief Alan Dye will be leaving. Apple said Dye will be succeeded by Stephen Lemay, who has been a software designer at the company since 1999. Meta CEO Mark Zuckerberg announced that Dye will lead a new creative studio within the company's AR/VR division Reality Labs. On his blog Daring Fireball,...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple's iPhone 17 lineup is selling well enough that Apple is on track to ship more than 247.4 million total iPhones in 2025, according to a new report from IDC. Total 2025 shipments are forecast to grow 6.1 percent year over year due to iPhone 17 demand and increased sales in China, a major market for Apple. Overall worldwide smartphone shipments across Android and iOS are forecast to...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;2026 could be a bumper year for Apple's Mac lineup, with the company expected to announce as many as four separate MacBook launches. Rumors suggest Apple will court both ends of the consumer spectrum, with more affordable options for students and feature-rich premium lines for users that seek the highest specifications from a laptop. Below is a breakdown of what we're expecting over the next ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;The iPhone Air has recorded the steepest early resale value drop of any iPhone model in years, with new data showing that several configurations have lost almost 50% of their value within ten weeks of launch. According to a ten-week analysis published by SellCell, Apple's latest lineup is showing a pronounced split in resale performance between the iPhone 17 models and the iPhone Air....&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;iPhone 17 Pro models, it turns out, can't take photos in Night mode when Portrait mode is selected in the Camera app – a capability that's been available on Apple's Pro devices since the iPhone 12 Pro in 2020. If you're an iPhone 17 Pro or iPhone 17 Pro Max owner, try it for yourself: Open the Camera app with Photo selected in the carousel, then cover the rear lenses with your hand to...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple's iPhone development roadmap runs several years into the future and the company is continually working with suppliers on several successive iPhone models at the same time, which is why we often get rumored features months ahead of launch. The iPhone 18 series is no different, and we already have a good idea of what to expect for the iPhone 18 Pro and iPhone 18 Pro Max. One thing worth...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.macrumors.com/2025/12/05/openai-device-barred-from-io-name/"/><published>2025-12-05T16:48:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163977</id><title>Onlook (YC W25) the Cursor for Designers Is Hiring a Founding Fullstack Engineer</title><updated>2025-12-05T19:32:53.254785+00:00</updated><content>&lt;doc fingerprint="be519058de7ce427"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hey HN! I'm Daniel, building Onlook, the Cursor for Designers. We built an open-source collaborative canvas for code that lets designers and developers craft incredible web experiences together.&lt;/p&gt;
      &lt;p&gt;Since launching, Onlook hit #1 on Hacker News, was the #1 trending repo on GitHub—above DeepSeek + Anthropic—and has earned over 23,000 GitHub stars. We’re looking to bring on Onlook’s first Founding Engineers.&lt;/p&gt;
      &lt;p&gt;This role requires autonomy - you’ll be setting standards for one of the fastest-growing open source projects backed by YC ever. You’ll help design and build an uncompromising visual IDE loved by tens of thousands of designers and engineers around the world, and you'll have a heavy influence on the direction of where we take the company.&lt;/p&gt;
      &lt;p&gt;You’re a full-stack engineer based in the U.S. who is ultra comfortable in Typescript, NextJS, React, and Tailwind, and ready to jump-in and build.&lt;/p&gt;
      &lt;p&gt;The most important things we look for:&lt;/p&gt;
      &lt;p&gt;• Olympic-level dedication – you want to be the best in the world at what you do.&lt;/p&gt;
      &lt;p&gt;• Ownership – you like autonomy and control over the destiny of the company.&lt;/p&gt;
      &lt;p&gt;• Speed – you’re comfortable shipping and iterating quickly with feedback.&lt;/p&gt;
      &lt;p&gt;• Craft – you’re opinionated and are willing to defend your opinions.&lt;/p&gt;
      &lt;p&gt;Ideally, you:&lt;/p&gt;
      &lt;p&gt;• Are looking for a fast-paced, early startup environment.&lt;/p&gt;
      &lt;p&gt;• Are willing to put in long hours and go the extra mile.&lt;/p&gt;
      &lt;p&gt;• Are comfortable with any part of the stack, front-end, back-end, or database.&lt;/p&gt;
      &lt;p&gt;• Believe in open source and are ok with your work being very public.&lt;/p&gt;
      &lt;p&gt;The comp range for this role is $130k-200k, 1-5% equity, great healthcare + other perks, and an awesome office if you happen to be in SF. We're open to remote / hybrid candidates.&lt;/p&gt;
      &lt;p&gt;If you’d like to stand out, please share a project or piece of work that you’re most proud of. We love seeing people’s work. If you have a personal website, please include that as well.&lt;/p&gt;
      &lt;p&gt;If you're interested, email daniel@onlook.com with your Github / LinkedIn / Website or work samples and why you'd be a great addition to the team, or apply here: https://www.ycombinator.com/companies/onlook/jobs/e4gHv1n-fo...&lt;/p&gt;
      &lt;p&gt;Excited to meet, and build alongside you!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46163977"/><published>2025-12-05T17:00:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46164646</id><title>Shingles vaccination prevented or delayed dementia</title><updated>2025-12-05T19:32:52.932224+00:00</updated><content/><link href="https://www.cell.com/cell/fulltext/S0092-8674(25)01256-5"/><published>2025-12-05T17:47:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46164952</id><title>WikiFlix: Full Movies Hosted on Wikimedia Commons</title><updated>2025-12-05T19:32:52.657956+00:00</updated><content>&lt;doc fingerprint="88eeb4d9d542a71f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;User:Spinster/WikiFlix&lt;/head&gt;&lt;p&gt;🎞 🛋 🍿 WikiFlix on Wikimedia Commons - Watch free movies 🍿 🛋 🎞&lt;/p&gt;&lt;p&gt;This is a prototype for browsing and watching full-length, public domain or Creative Commons-licensed films on Wikimedia Commons. The lists of films here are powered by Wikidata.&lt;/p&gt;&lt;p&gt;For a much nicer and slightly different browsing and viewing experience, check out this standalone application on Wikimedia's tool server Toolforge, created by Magnus Manske: WikiFlix (help).&lt;/p&gt;&lt;p&gt;By decade and year:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;1870s&lt;/item&gt;&lt;item&gt;1880s&lt;/item&gt;&lt;item&gt;1890s&lt;/item&gt;&lt;item&gt;1900s&lt;/item&gt;&lt;item&gt;1910s&lt;/item&gt;&lt;item&gt;1920s – 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929&lt;/item&gt;&lt;item&gt;1930s&lt;/item&gt;&lt;item&gt;1940s&lt;/item&gt;&lt;item&gt;1950s&lt;/item&gt;&lt;item&gt;1960s&lt;/item&gt;&lt;item&gt;1970s&lt;/item&gt;&lt;item&gt;1980s&lt;/item&gt;&lt;item&gt;1990s&lt;/item&gt;&lt;item&gt;2000s&lt;/item&gt;&lt;item&gt;2010s&lt;/item&gt;&lt;item&gt;2020s&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Genres: Abstract, Animated, Anime, Documentary, Fantasy, Science fiction&lt;/p&gt;&lt;p&gt;Countries: 🇧🇪 Belgium - 🇫🇷 France - 🇩🇪 Germany - 🇮🇳 India - 🇮🇹 Italy - 🇯🇵 Japan - 🇬🇧 United Kingdom - 🇺🇸 United States&lt;/p&gt;&lt;p&gt;Directed by: Walt Disney 📽 Sergei Eisenstein&lt;/p&gt;&lt;p&gt;With actors: Charlie Chaplin&lt;/p&gt;&lt;p&gt;With characters: Mickey Mouse 🐾 Minnie Mouse&lt;/p&gt;&lt;p&gt;🌟 And a few special lists: 🌟 Films by female directors 🌟 Films by directors who identify/ied as LGBT+ 🇺🇸 National Film Registry&lt;/p&gt;&lt;head rend="h2"&gt;Filming locations&lt;/head&gt;[edit]&lt;head rend="h2"&gt;Films that have won awards&lt;/head&gt;[edit]&lt;p&gt;Here are some award-winning movies:&lt;/p&gt;&lt;p/&gt;&lt;p&gt;This list is periodically updated by a bot. Manual changes to the list will be removed on the next update!&lt;/p&gt;WQS | PetScan | Find images | Recent Changes&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Actor&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Actress&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Animated Short Film&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Art Direction, Black and White&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Cinematography&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Director&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Documentary (Short Subject)&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Documentary Feature Film&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Picture&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Award for Best Story&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Academy Honorary Award&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Annecy Cristal for a Feature Film&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Filmfare Award for Best Film&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Filmfare Award for Best Music Director&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Hundred Flowers Award for Best Animation&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Mussolini Cup&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;National Board of Review: Top Ten Films&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;National Film Award for Best Bengali Feature Film&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;National Film Award for Best Feature Film&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Palme d'Or&lt;/head&gt;[edit]&lt;p/&gt;&lt;head rend="h2"&gt;Silver Bear for Best Director&lt;/head&gt;[edit]&lt;p/&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://commons.wikimedia.org/wiki/User:Spinster/WikiFlix"/><published>2025-12-05T18:08:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46165122</id><title>Wall Street races to protect itself from AI bubble</title><updated>2025-12-05T19:32:52.351380+00:00</updated><content>&lt;doc fingerprint="7d14b2d9c3b680a0"&gt;
  &lt;main&gt;
    &lt;p&gt;Wall Street finds itself in an unusual position as it prepares to lend staggering amounts to artificial intelligence companies. Even as banks facilitate what may become the largest borrowing binge in technology history, they are simultaneously deploying an arsenal of financial tools to protect themselves from the very bubble their money might be inflating.&lt;/p&gt;
    &lt;p&gt;The anxiety permeating credit markets tells the story. The cost of insuring Oracle debt against default through derivatives has climbed to levels not seen since the Global Financial Crisis. Morgan Stanley has explored using specialized insurance mechanisms to reduce exposure to its tech borrowers. Across trading desks, lenders are quietly hedging positions even as they publicly champion the transformative potential of artificial intelligence.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unprecedented Wall Street lending to technology giants&lt;/head&gt;
    &lt;p&gt;Mega offerings from Oracle, Meta Platforms and Alphabet have pushed global bond issuance past $6.46 trillion in 2025. These hyperscalers, alongside electric utilities and related firms, are expected to spend at least $5 trillion racing to build data centers and infrastructure for technology promising to revolutionize the global economy.&lt;/p&gt;
    &lt;p&gt;The scale is so immense that issuers must tap virtually every major debt market, according to JPMorgan Chase analysis. These technology investments could take years to generate returns, assuming they deliver profits at all. The frenzied pace has left some lenders dangerously overexposed, prompting them to use credit derivatives, sophisticated bonds and newer financial products to shift underwriting risk to other investors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technology that may not translate to profits&lt;/head&gt;
    &lt;p&gt;Steven Grey, chief investment officer at Grey Value Management, emphasized that impressive technology does not automatically guarantee profitability. Those risks became tangible last week when a major outage halted trading at CME Group and served as a stark reminder that data center customers can abandon providers after repeated breakdowns. Following that incident, Goldman Sachs paused a planned $1.3 billion mortgage bond sale for CyrusOne, a data center operator.&lt;/p&gt;
    &lt;p&gt;Banks have turned aggressively to credit derivatives markets to reduce exposure. Trading of Oracle credit default swaps exploded to roughly $8 billion over the nine weeks ended November 28, according to analysis of trade repository data by Barclays credit strategist Jigar Patel. That compares to just $350 million during the same period last year.&lt;/p&gt;
    &lt;p&gt;Banks are providing the bulk of massive construction loans for data centers where Oracle serves as the intended tenant, likely driving much of this hedging activity, according to recent Morgan Stanley research. These include a $38 billion loan package and an $18 billion loan to build multiple new data center facilities in Texas, Wisconsin and New Mexico.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hedging costs climb across the sector&lt;/head&gt;
    &lt;p&gt;Prices for protection have risen sharply across the board. A five year credit default swap agreement to protect $10 million of Microsoft debt from default would cost approximately $34,000 annually, or 34 basis points, as of Thursday. In mid October, that same protection cost closer to $20,000 yearly.&lt;/p&gt;
    &lt;p&gt;Andrew Weinberg, a portfolio manager at Saba Capital Management, noted that the spread on Microsoft default swaps appears remarkably wide for a company rated AAA. The hedge fund has been selling protection on the tech giant. By comparison, protection on Johnson &amp;amp; Johnson, the only other American company with a AAA rating, cost about 19 basis points annually on Thursday.&lt;/p&gt;
    &lt;p&gt;Weinberg suggested that selling protection on Microsoft at levels more than 50% wider than fellow AAA rated Johnson &amp;amp; Johnson represents a remarkable opportunity. Microsoft, which has not issued debt this year, declined to comment. Similar opportunities exist with Oracle, Meta and Alphabet, according to Weinberg. Despite their large debt raises, their credit default swaps trade at high spreads relative to actual default risk, making selling protection sensible. Even if these companies face downgrades, the positions should perform well because they already incorporate substantial potential bad news.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sophisticated tools to Wall Street shift risk&lt;/head&gt;
    &lt;p&gt;Morgan Stanley, a key player in financing the artificial intelligence race, has considered offloading some data center exposure through a transaction known as a significant risk transfer. These deals can provide banks with default protection for between 5% and 15% of a designated loan portfolio. Such transfers often involve selling bonds called credit linked notes, which can have credit derivatives tied to companies or loan portfolios embedded within them. If borrowers default, the bank receives a payout covering its loss.&lt;/p&gt;
    &lt;p&gt;Morgan Stanley held preliminary talks with potential investors about a significant risk transfer tied to a portfolio of loans to businesses involved in AI infrastructure, Bloomberg reported Wednesday. Mark Clegg, a senior fixed income trader at Allspring Global Investments, observed that banks remain fully aware of recent market concerns about possible overinvestment and overvaluation. He suggested it should surprise no one that they might explore hedging or risk transfer mechanisms.&lt;/p&gt;
    &lt;p&gt;Private capital firms including Ares Management have been positioning themselves to absorb some bank exposure through significant risk transfers tied to data centers. The massive scale of recent debt offerings adds urgency to these efforts. Not long ago, a $10 billion deal in the American high grade market qualified as big. Now, with multi trillion dollar market capitalization companies and funding needs in the hundreds of billions, Teddy Hodgson, global co head of investment grade debt capital markets at Morgan Stanley, suggested that $10 billion represents merely a drop in the bucket. He noted that Morgan Stanley raised $30 billion for Meta in a drive by financing executed in a single day, an event not historically commonplace. Investors will need to adjust to bigger deals from hyperscalers given how much these companies have grown and how expensive capturing this opportunity will prove.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rollingout.com/2025/12/05/wall-street-protects-itself-ai-bubble/"/><published>2025-12-05T18:21:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46165249</id><title>Why we built Lightpanda in Zig</title><updated>2025-12-05T19:32:52.004921+00:00</updated><content>&lt;doc fingerprint="a778501a8d23ae78"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why We Built Lightpanda in Zig&lt;/head&gt;
    &lt;p&gt;Because We're Not Smart Enough for C++ or Rust&lt;/p&gt;
    &lt;head rend="h3"&gt;Francis Bouvier&lt;/head&gt;
    &lt;head rend="h4"&gt;Cofounder &amp;amp; CEO&lt;/head&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;To be honest, when I began working on Lightpanda, I chose Zig because I’m not smart enough to build a big project in C++ or Rust.&lt;/p&gt;
    &lt;p&gt;I like simple languages. I like Zig for the same reasons I like Go, C, and the KISS principle. Not just because I believe in this philosophy, but because I’m not capable of handling complicated abstractions at scale.&lt;/p&gt;
    &lt;p&gt;Before Lightpanda, I was doing a lot of Go. But building a web browser from scratch requires a low-level systems programming language to ensure great performance, so Go wasn’t an option. And for a project like this, I wanted more safety and modern tooling than C.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why We Built Lightpanda in Zig&lt;/head&gt;
    &lt;p&gt;Our requirements were performance, simplicity, and modern tooling. Zig seemed like the perfect balance: simpler than C++ and Rust, top-tier performance, and better tooling and safety than C.&lt;/p&gt;
    &lt;p&gt;As we built the first iterations of the browser and dug deeper into the language, we came to appreciate features where Zig particularly shines: comptime metaprogramming, explicit memory allocators, and best-in-class C interoperability. Not to mention the ongoing work on compilation times.&lt;/p&gt;
    &lt;p&gt;Of course it’s a big bet. Zig is a relatively new language with a small ecosystem. It’s pre-1.0 with regular breaking changes. But we’re very bullish on this language, and we’re not the only ones: Ghostty , Bun , TigerBeetle , and ZML are all building with Zig. And with Anthropic’s recent acquisition of Bun , big tech is taking notice.&lt;/p&gt;
    &lt;p&gt;Here’s what we’ve learned.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Lightpanda Needs from a Language&lt;/head&gt;
    &lt;p&gt;Before diving into specifics, let’s talk about what building a browser for web automation requires.&lt;/p&gt;
    &lt;p&gt;First, we needed a JavaScript engine. Without one, a browser only sees static HTML: no client-side rendering and no dynamic content. We chose V8, Chrome’s JavaScript engine, because it’s state of the art, widely used (Node.js , Deno ), and relatively easy to embed.&lt;/p&gt;
    &lt;p&gt;V8 is written in C++, and doesn’t have a C API, which means any language integrating with it must handle C++ boundaries. Zig doesn’t interoperate directly with C++, but it has first-class C interop, and C remains the lingua franca of systems programming. We use C headers generated primarily from rusty_v8 , part of the Deno project , to bridge between V8’s C++ API and our Zig code.&lt;/p&gt;
    &lt;p&gt;Beyond integration, performance and memory control were essential. When you’re crawling thousands of pages or running automation at scale, every millisecond counts. We also needed precise control over short-lived allocations like DOM trees, JavaScript objects, and parsing buffers. Zig’s explicit allocator model fits that need perfectly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Not C++?&lt;/head&gt;
    &lt;p&gt;C++ was the obvious option: it powers virtually every major browser engine. But here’s what gave us pause.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Four decades of features: C++ has accumulated enormous complexity over the years. There are multiple ways to do almost everything: template metaprogramming, multiple inheritance patterns, various initialization syntaxes. We wanted a language with one clear way to do things.&lt;/item&gt;
      &lt;item&gt;Memory management: Control comes with constant vigilance. Use-after-free bugs, memory leaks, and dangling pointers are real risks. Smart pointers help, but they add complexity and runtime overhead. Zig’s approach of passing allocators explicitly makes memory management clearer and enables patterns like arenas more naturally.&lt;/item&gt;
      &lt;item&gt;Build systems: Anyone who’s fought with CMake or dealt with header file dependencies knows this pain. For a small team trying to move quickly, we didn’t want to waste time debugging build configuration issues.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’re not saying C++ is bad. It powers incredible software. But for a small team starting from scratch, we wanted something simpler.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why not Rust?&lt;/head&gt;
    &lt;p&gt;Many people ask this next. It’s a fair challenge. Rust is a more mature language than Zig, offers memory safety guarantees, has excellent tooling, and a growing ecosystem.&lt;/p&gt;
    &lt;p&gt;Rust would have been a viable choice. But for Lightpanda’s specific needs (and honestly, for our team’s experience level) it introduced friction we didn’t want.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Unsafe Rust Problem&lt;/head&gt;
    &lt;p&gt;When you need to do things the borrow checker doesn’t like, you end up writing unsafe Rust, which is surprisingly hard. Zack from Bun explores this in depth in his article When Zig is safer and faster than Rust .&lt;/p&gt;
    &lt;p&gt;Browser engines and garbage-collected runtimes are classic examples of code that fights the borrow checker. You’re constantly juggling different memory regions: per-page arenas, shared caches, temporary buffers, objects with complex interdependencies. These patterns don’t map cleanly to Rust’s ownership model. You end up either paying performance costs (using indices instead of pointers, unnecessary clones) or diving into unsafe code where raw pointer ergonomics are poor and Miri becomes your constant companion.&lt;/p&gt;
    &lt;p&gt;Zig takes a different approach. Rather than trying to enforce safety through the type system and then providing an escape hatch, Zig is designed for scenarios where you’re doing memory-unsafe things. It gives you tools to make that experience better: non-null pointers by default, the GeneralPurposeAllocator that catches use-after-free bugs in debug mode, and pointer types with good ergonomics.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Zig Works for Lightpanda&lt;/head&gt;
    &lt;p&gt;Zig sits in an interesting space. It’s a simple language that’s easy to learn, where everything is explicit: no hidden control flow, no hidden allocations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Explicit Memory Management with Allocators&lt;/head&gt;
    &lt;p&gt;Zig makes you choose how memory is managed through allocators. Every allocation requires you to specify which allocator to use. This might sound tedious at first, but it gives you precise control.&lt;/p&gt;
    &lt;p&gt;Here’s what this looks like in practice, using an arena allocator:&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
 
pub fn loadPage(_allocator: std.mem.Allocator, url: []const u8) !void {
    // Create an arena allocator for this page load
    var arena = std.heap.ArenaAllocator.init(_allocator);
    defer arena.deinit(); // Everything gets freed here
 
    const allocator = arena.allocator();
 
    // All these allocations use the arena
    const dom_tree = try parseDom(allocator, url);
    const css_rules = try parseStyles(allocator, dom_tree);
    const js_context = try createJsContext(allocator);
 
    // Execute page, render, extract data...
    try executePage(js_context, dom_tree, css_rules);
 
    // Arena.deinit() frees everything at once, no leaks possible
}&lt;/code&gt;
    &lt;p&gt;This pattern matches browser workloads perfectly. Each page load gets its own arena. When the page is done, we throw away the entire memory chunk. No tracking individual allocations, no reference counting overhead, no garbage collection pauses. (Though we’re learning that single pages can grow large in memory, so we’re also exploring mid-lifecycle cleanup strategies). And you can chain arenas, to create short-lived objects inside a page lifecycle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compile-Time Metaprogramming&lt;/head&gt;
    &lt;p&gt;Zig’s comptime feature lets you write code that runs during compilation. We use this extensively to reduce boilerplate when bridging Zig and JavaScript.&lt;/p&gt;
    &lt;p&gt;When integrating V8, you need to expose native types to JavaScript. In most languages, this requires glue code for each type. To generate this glue you need some code generation, usually through Macros (Rust, C, C++). Macros are a completely different language, which has a lot of downsides. Zig’s comptime lets us automate this:&lt;/p&gt;
    &lt;code&gt;const Point = struct {
    x: i32,
    y: i32,
 
    pub fn moveUp(self: *Point) void {
        self.y += 1;
    }
 
    pub fn moveDown(self: *Point) void {
        self.y -= 1;
    }
};
 
// Our runtime can introspect this at compile time and generate bindings
 
runtime.registerType(Point, "Point");&lt;/code&gt;
    &lt;p&gt;The registerType function uses comptime reflection to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Find all public methods on Point&lt;/item&gt;
      &lt;item&gt;Generate JavaScript wrapper functions&lt;/item&gt;
      &lt;item&gt;Create property getters/setters for x and y&lt;/item&gt;
      &lt;item&gt;Handle type conversions automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This eliminates manual binding code and makes adding new types simple by using the same language at compile time and runtime.&lt;/p&gt;
    &lt;head rend="h3"&gt;C Interop That Just Works&lt;/head&gt;
    &lt;p&gt;Zig’s C interop is a first-class feature: you can directly import C header files and call C functions without wrapper libraries.&lt;/p&gt;
    &lt;p&gt;For example, we use cURL as our HTTP library. We can just import libcurl C headers in Zig and use the C functions directly:&lt;/p&gt;
    &lt;code&gt;pub const c = @cImport({
    @cInclude("curl/curl.h");
});
 
pub fn init() !Http {
    try errorCheck(c.curl_global_init(c.CURL_GLOBAL_SSL));
    errdefer c.curl_global_cleanup();
    // business logic ...
}&lt;/code&gt;
    &lt;p&gt;It feels as simple as using C, except you are programming in Zig.&lt;/p&gt;
    &lt;p&gt;And with the build system it’s also very simple to add the C sources to build everything together (your zig code and the C libraries):&lt;/p&gt;
    &lt;code&gt;fn buildCurl(b: *Build, m: *Build.Module) !void {
    const curl = b.addLibrary(.{
        .name = "curl",
        .root_module = m,
    });
 
    const root = "vendor/curl/";
 
    curl.addIncludePath(b.path(root ++ "lib"));
    curl.addIncludePath(b.path(root ++ "include"));
    curl.addCSourceFiles(.{
        .flags = &amp;amp;.{
            // list of compilation flags (optional)
        },
        .files = &amp;amp;.{
            // list of C source files
    }});
}&lt;/code&gt;
    &lt;p&gt;This simplicity of importing C mitigates the fact that the Zig ecosystem is still small, as you can use all the existing C libraries.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Build System Advantage&lt;/head&gt;
    &lt;p&gt;Zig includes its own build system written in Zig itself. This might sound unremarkable, but compared to CMake, it’s refreshingly straightforward. Adding dependencies, configuring compilation flags, and managing cross-compilation all happen in one place with clear semantics. Runtime, comptime, build system: everything is in Zig, which makes things easier.&lt;/p&gt;
    &lt;p&gt;Cross-compilation in particular is usually a difficult topic, but it’s very easy with Zig. Some projects like Uber use Zig mainly as a build system and toolchain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compile times matter&lt;/head&gt;
    &lt;p&gt;Zig compiles fast. Our full rebuild takes under a minute. Not as fast as Go or an interpreted language, but enough to have a feedback loop that makes development feel responsive. In that regard, Zig is considerably faster than Rust or C++.&lt;/p&gt;
    &lt;p&gt;This is a strong focus of the Zig team. They are also a small team and they need fast compilation for the development of the language, as Zig is written in Zig (self-hosted). For that purpose, they are developing native compiler backends (i.e. not using LLVM), which is very ambitious and yet successful: it’s already the default backend for x86 in debug mode, with a significant improvement in build times (3.5x faster for the Zig project itself ). And incremental compilation is on its way.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We’ve Learned&lt;/head&gt;
    &lt;p&gt;After months of building Lightpanda in Zig, here’s what stands out.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The learning curve is manageable. Zig’s simplicity means you can understand the entire language in a few weeks. Compared to Rust or C++, this makes a real difference.&lt;/item&gt;
      &lt;item&gt;The allocator model pays off. Being able to create arena allocators per page load, per request, or per task gives us fine-grained memory control without tracking individual allocations.&lt;/item&gt;
      &lt;item&gt;The community is small but helpful. Zig is still growing. The Discord community and ziggit.dev are active, and the language is simple enough that you can often figure things out by reading the standard library source.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Lightpanda wouldn’t exist without the work of the Zig Foundation and the community behind it. Zig has made it possible to build something as complex as a browser with a small team and a clear mental model, without sacrificing performance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you’re curious about Zig’s design philosophy or want to see how its compiler and allocator model work, the official documentation is the best place to start.&lt;/item&gt;
      &lt;item&gt;You can also explore the Lightpanda source code and follow the project on GitHub&lt;/item&gt;
      &lt;item&gt;Sign up to test the cloud version&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;head rend="h3"&gt;Is Zig stable enough for production use?&lt;/head&gt;
    &lt;p&gt;Zig is still pre-1.0, which means breaking changes can happen between versions. That said, we’ve found it stable enough for our production use, especially since the ecosystem has largely standardized on tracking the latest tagged releases rather than main. The language itself is well-designed, and most changes between versions are improvements that are worth adapting to. Just be prepared to update code when upgrading Zig versions.&lt;/p&gt;
    &lt;head rend="h3"&gt;What’s the hardest part about learning Zig?&lt;/head&gt;
    &lt;p&gt;The allocator model takes adjustment if you’re coming from garbage-collected languages. You need to think about where memory comes from and when it gets freed. But compared to Rust’s borrow checker or C++‘s memory management, it’s relatively straightforward once you understand the patterns.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can Zig really replace C++ for browser development?&lt;/head&gt;
    &lt;p&gt;For building a focused browser like Lightpanda, yes. For replacing Chromium or Firefox, that’s unlikely: those projects have millions of lines of C++ and decades of optimization. We’re more likely to see Rust complementing C++ in those projects over time, for example how Firefox is leveraging Servo . But for new projects where you control the codebase, Zig is absolutely viable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where can I learn more about Zig?&lt;/head&gt;
    &lt;p&gt;Start with the official Zig documentation . The Zig Learn site provides practical tutorials. And join the community on Discord or ziggit.dev where developers actively help newcomers. The language is simple enough that reading standard library source code is also a viable learning approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Francis Bouvier&lt;/head&gt;
    &lt;head rend="h4"&gt;Cofounder &amp;amp; CEO&lt;/head&gt;
    &lt;p&gt;Francis previously cofounded BlueBoard, an ecommerce analytics platform acquired by ChannelAdvisor in 2020. While running large automation systems he saw how limited existing browsers were for this kind of work. Lightpanda grew from his wish to give developers a faster and more reliable way to automate the web.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lightpanda.io/blog/posts/why-we-built-lightpanda-in-zig"/><published>2025-12-05T18:29:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46165251</id><title>Show HN: SerpApi MCP Server</title><updated>2025-12-05T19:32:51.519731+00:00</updated><content>&lt;doc fingerprint="1f42f8624f8caf58"&gt;
  &lt;main&gt;
    &lt;p&gt;A Model Context Protocol (MCP) server implementation that integrates with SerpApi for comprehensive search engine results and data extraction.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-Engine Search: Google, Bing, Yahoo, DuckDuckGo, YouTube, eBay, and more&lt;/item&gt;
      &lt;item&gt;Real-time Weather Data: Location-based weather with forecasts via search queries&lt;/item&gt;
      &lt;item&gt;Stock Market Data: Company financials and market data through search integration&lt;/item&gt;
      &lt;item&gt;Dynamic Result Processing: Automatically detects and formats different result types&lt;/item&gt;
      &lt;item&gt;Flexible Response Modes: Complete or compact JSON responses&lt;/item&gt;
      &lt;item&gt;JSON Responses: Structured JSON output with complete or compact modes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SerpApi MCP Server is available as a hosted service at mcp.serpapi.com. In order to connect to it, you need to provide an API key. You can find your API key on your SerpApi dashboard.&lt;/p&gt;
    &lt;p&gt;You can configure Claude Desktop to use the hosted server:&lt;/p&gt;
    &lt;code&gt;{
  "mcpServers": {
    "serpapi": {
      "url": "https://mcp.serpapi.com/YOUR_SERPAPI_API_KEY/mcp"
    }
  }
}&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/serpapi/serpapi-mcp.git
cd serpapi-mcp
uv sync &amp;amp;&amp;amp; uv run src/server.py&lt;/code&gt;
    &lt;p&gt;Configure Claude Desktop:&lt;/p&gt;
    &lt;code&gt;{
  "mcpServers": {
    "serpapi": {
      "url": "http://localhost:8000/YOUR_SERPAPI_API_KEY/mcp"
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Get your API key: serpapi.com/manage-api-key&lt;/p&gt;
    &lt;p&gt;Two methods are supported:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Path-based: &lt;code&gt;/YOUR_API_KEY/mcp&lt;/code&gt;(recommended)&lt;/item&gt;
      &lt;item&gt;Header-based: &lt;code&gt;Authorization: Bearer YOUR_API_KEY&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Path-based
curl "https://mcp.serpapi.com/your_key/mcp" -d '...'

# Header-based  
curl "https://mcp.serpapi.com/mcp" -H "Authorization: Bearer your_key" -d '...'&lt;/code&gt;
    &lt;p&gt;The MCP server has one main Search Tool that supports all SerpApi engines and result types. You can find all available parameters on the SerpApi API reference.&lt;/p&gt;
    &lt;p&gt;The parameters you can provide are specific for each API engine. Some sample parameters are provided below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;params.q&lt;/code&gt;(required): Search query&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;params.engine&lt;/code&gt;: Search engine (default: "google_light")&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;params.location&lt;/code&gt;: Geographic filter&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mode&lt;/code&gt;: Response mode - "complete" (default) or "compact"&lt;/item&gt;
      &lt;item&gt;...see other parameters on the SerpApi API reference&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;{"name": "search", "arguments": {"params": {"q": "coffee shops", "location": "Austin, TX"}}}
{"name": "search", "arguments": {"params": {"q": "weather in London"}}}
{"name": "search", "arguments": {"params": {"q": "AAPL stock"}}}
{"name": "search", "arguments": {"params": {"q": "news"}, "mode": "compact"}}
{"name": "search", "arguments": {"params": {"q": "detailed search"}, "mode": "complete"}}&lt;/code&gt;
    &lt;p&gt;Supported Engines: Google, Bing, Yahoo, DuckDuckGo, YouTube, eBay, and more.&lt;/p&gt;
    &lt;p&gt;Result Types: Answer boxes, organic results, news, images, shopping - automatically detected and formatted.&lt;/p&gt;
    &lt;code&gt;# Local development
uv sync &amp;amp;&amp;amp; uv run src/server.py

# Docker
docker build -t serpapi-mcp . &amp;amp;&amp;amp; docker run -p 8000:8000 serpapi-mcp

# Testing with MCP Inspector
npx @modelcontextprotocol/inspector
# Configure: URL mcp.serpapi.com/YOUR_KEY/mcp, Transport "Streamable HTTP transport"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Missing API key": Include key in URL path &lt;code&gt;/{YOUR_KEY}/mcp&lt;/code&gt;or header&lt;code&gt;Bearer YOUR_KEY&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;"Invalid key": Verify at serpapi.com/dashboard&lt;/item&gt;
      &lt;item&gt;"Rate limit exceeded": Wait or upgrade your SerpApi plan&lt;/item&gt;
      &lt;item&gt;"No results": Try different query or engine&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create your feature branch: &lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install dependencies: &lt;code&gt;uv install&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Make your changes&lt;/item&gt;
      &lt;item&gt;Commit changes: &lt;code&gt;git commit -m 'Add amazing feature'&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Push to branch: &lt;code&gt;git push origin feature/amazing-feature&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License - see LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/serpapi/serpapi-mcp"/><published>2025-12-05T18:30:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46165513</id><title>SpaceX in Talks for Share Sale That Would Boost Valuation to $800B</title><updated>2025-12-05T19:32:51.336544+00:00</updated><content/><link href="https://www.wsj.com/business/spacex-in-talks-for-share-sale-that-would-boost-valuation-to-800-billion-b2852191"/><published>2025-12-05T18:49:07+00:00</published></entry></feed>