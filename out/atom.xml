<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-07T23:08:08.236061+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45154501</id><title>The key to getting MVC correct is understanding what models are</title><updated>2025-09-07T23:08:20.247580+00:00</updated><content>&lt;doc fingerprint="1f5bdf1aa5aa0a18"&gt;
  &lt;main&gt;
    &lt;p&gt;How did MVC get so F’ed up?&lt;/p&gt;
    &lt;p&gt;Smalltalk MVC is defined in Design Pattern as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;MVC Consists of three kinds of objects. The Model is the application object, the View is its screen presentation, and the Controller defines the way the user interface reacts to user input.1&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However this definition has been abused over the years - Back in 2003 I gave a talk citing how bad Apple’s definition was. At the time it stated:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A view object knows how to display and possibly edit data from the application’s model… A controller object acts as the intermediary between the application’s view objects and its model objects… Controllers are often the least reusable objects in an application, but that’s acceptable…2&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Of course it isn’t acceptable and, over the years, Apple has refined their definition and now acknowledge the distinction between the traditional Smalltalk version of MVC and the Cocoa version.3 But the Cocoa version is still defined much as it was before:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A view object knows how to display, and might allow users to edit, the data from the application’s model… A controller object acts as the intermediary between the application’s view objects and its model objects…3&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In looking at how iOS applications are written the sentiment that controllers (and now view-controllers) are often the least reusable components in an application still flourishes, even if it is now unstated.&lt;/p&gt;
    &lt;p&gt;MVC (I’ll always use that term to refer to the Smalltalk form) has the following structure:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; figure: Smalltalk MVC4 &lt;/p&gt;
    &lt;p&gt;Here the solid lines imply a direct association. And the dashed lines an indirect association by an observer. So what we see is that the model is unaware of the view and controller, except indirectly through notifications, and hence the code in the Model is reusable. The controller and view bind to the model, not the other way around.&lt;/p&gt;
    &lt;p&gt;Often the function of the Controller and View are tightly coupled into a “widget” or “control”. When Apple talks about a View-Controller in their model they are talking about a grab-bag of an uber-widget that is a composite of UIView widgets and multiple models. From what I’ve seen, including in Apple’s example code, it is usually a pretty big mess.&lt;/p&gt;
    &lt;p&gt;The key to getting MVC correct is understanding what models are. A model is simply an object5 which can be observed (a requirement for attaching views). For example, in ObjC an int is an object, but it is not observable. However, an ObjC object with an int property is observable using Key-Value Observing6. A model may encapsulate complex relationships between the model’s properties. A trivial model is one where each property is completely independent (think C struct vs. C++ class). From a notification the view should be able to determine, at a minimum:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What changed. It may be as simple as “the model bound to the view”.&lt;/item&gt;
      &lt;item&gt;The new value to display.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, let’s say our model is a trivial observable boolean (I can’t imagine a simpler model). What we want is a checkbox that binds to the observable boolean. When the controller requests a change in value, the boolean is updated, and the view is notified of the new state of the model. The model is unaware of what UI is attached to it, and in fact there could be multiple UIs, including something like a scripting system, attached to the same instance of the model. This is a form of data binding - though most data binding systems replicate the problems of their underlying widget set by treating the model as if it were observing the view, not the other way around.&lt;/p&gt;
    &lt;p&gt;Contrast this with most UI frameworks where you have a checkbox widget from which you can query the value and you receive a notification when the value has changed. This is pushing a model into the widget. With MVC you never ask a question like “what is the default state of this checkbox?” - the default state of the view is always the current state of the model. You would also never get the state of the checkbox - the state of the checkbox is simply a reflection of the state of the model. In a system where you get the state of a checkbox you are binding two models together by treating one as a view/controller of the other. Such a pattern doesn’t scale beyond trivial models, and even for those it introduces some ambiguity.&lt;/p&gt;
    &lt;p&gt;I conjecture that one of the reasons why MVC has been so screwed up is because, unlike in Smalltalk, writing something as simple as an observable boolean is a bit of a pain in a language like Pascal or C. You quickly get into object ownership and lifetime issues and how to write bind expressions. If one also assumes that you have a 1:1 mapping from UI to model then there is some inherent inefficiency in the generalization. The Lisa team made some major compromises and the rest of the industry followed along.7&lt;/p&gt;
    &lt;p&gt;To support more complex views, the notification may need to specify what parts of the model changed and how those parts changed. For example, “image 58 was removed from the sequence”. A complete model is one that can support any view of that model type efficiently (related to the notion of a complete type and a type’s efficient basis).&lt;/p&gt;
    &lt;p&gt;One additional attribute of MVC is that it is a composite pattern. This is hinted at by the direct connection between the Controller and the View. As I said early, the view may contain state, this state is itself an object, and because this state is also displayed within the view it is observable. It is another model. I refers to this as the view’s model. This model may include things such as the visibility of a window, the tab the user was last looking at, and the portion of the model being viewed.&lt;/p&gt;
    &lt;p&gt;Identifying what the models are in your system is important. We usually do pretty good at identifying the major models. Such as “this is an image” - but often fall short of identifying the complete model, i.e. “this is an image with a collection of settings.” We end up with our model spread out within the code (an incidental type) and it makes it more difficult to deal with it.&lt;/p&gt;
    &lt;p&gt;A common model that is often completely overlooked is the model for function arguments. When you have a command, button, gesture, or menu item in your application, these are bound to a function. The function itself is not typically a zeroary function but rather has a set of arguments that are constructed through other parts of the UI. For example, if I have a list of images in my application, I might have a button to delete the selected images. Here the current selection is the argument to my delete command. To create a UI for the selection I must create a model of the arguments to my function. A precondition of delete is that the selection is not empty. This precondition must be observable in the argument model so it can be reflected in the view by disabling or hiding the button and in the controller be disallowing the user to click the button and issue the command. The same argument model can be shared for multiple commands within an application.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Gamma, Erich. “1.2 Design Patterns in Smalltalk MVC.” Design Patterns: Elements of Reusable Object-Oriented Software. Reading, MA: Addison-Wesley, 1995. N. pag. Print. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;http://smartfriends.com/U/Presenters/untangling_software.pdf (Don’t bother reading, this was an incomprehensible talk.) ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://developer.apple.com/library/content/documentation/General/Conceptual/CocoaEncyclopedia/Model-View-Controller/Model-View-Controller.html ↩ ↩2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stepanov, Alexander A., and Paul McJones. “1.3 Objects.” Elements of Programming. Upper Saddle River, NJ: Addison-Wesley, 2009. N. pag. Print. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://developer.apple.com/library/content/documentation/Cocoa/Conceptual/KeyValueObserving/KeyValueObserving.html ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://en.wikipedia.org/wiki/Object-oriented_user_interface ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stlab.cc/tips/about-mvc.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45155682</id><title>The "impossibly small" Microdot web framework</title><updated>2025-09-07T23:08:19.869220+00:00</updated><content>&lt;doc fingerprint="3ea911152da319f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The "impossibly small" Microdot web framework&lt;/head&gt;
    &lt;quote&gt;This article brought to you by LWN subscribers&lt;p&gt;Subscribers to LWN.net made this article — and everything that surrounds it — possible. If you appreciate our content, please buy a subscription and make the next set of articles possible.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;The Microdot web framework is quite small, as its name would imply; it supports both standard CPython and MicroPython, so it can be used on systems ranging from internet-of-things (IoT) devices all the way up to large, cloudy servers. It was developed by Miguel Grinberg, who gave a presentation about it at EuroPython 2025. His name may sound familiar from his well-known Flask Mega-Tutorial, which has introduced many to the Flask lightweight Python-based web framework. It should come as no surprise, then, that Microdot is inspired by its rather larger cousin, so Flask enthusiasts will find much to like in Microdot—and will come up to speed quickly should their needs turn toward smaller systems.&lt;/p&gt;
    &lt;p&gt;We have looked at various pieces of this software stack along the way: Microdot itself in January 2024, MicroPython in 2023, and Flask as part of a look at Python microframeworks in 2019.&lt;/p&gt;
    &lt;p&gt; Grinberg began his talk with an introduction. He has been living in Ireland for a few years and "&lt;quote&gt;I make stuff&lt;/quote&gt;". That includes open-source projects, blog posts (on a Flask-based blog platform that he wrote), and "&lt;quote&gt;a bunch of books&lt;/quote&gt;". He works for Elastic and is one of the maintainers of the Elasticsearch Python client, "&lt;quote&gt;so maybe you have used some of the things that I made for money&lt;/quote&gt;". &lt;/p&gt;
    &lt;head rend="h4"&gt;Why?&lt;/head&gt;
    &lt;p&gt; With a chuckle, he asked: "&lt;quote&gt;Why do we need another web framework? We have so many already.&lt;/quote&gt;" The story starts with a move that he made to Ireland from the US in 2018; he rented a house with a "smart" heating controller and was excited to use it. There were two thermostats, one for each level of the house, and he was "&lt;quote&gt;really looking forward to the winter&lt;/quote&gt;" to see the system in action. &lt;/p&gt;
    &lt;p&gt; As might be guessed, he could set target temperatures in each thermostat; they would communicate with the controller that would turn the heating on and off as needed. In addition, the system had a web server that could be used to query various parameters or to start and stop the heaters. You could even send commands via SMS text messages; "&lt;quote&gt;there's a SIM card somewhere in that box [...] very exciting stuff&lt;/quote&gt;". &lt;/p&gt;
    &lt;p&gt; When winter rolled around, it did not work that well, however; sometimes the house was too chilly or warm and he had to start and stop the heaters himself. He did some debugging and found that the thermostats were reporting temperatures that were off by ±3°C, "&lt;quote&gt;which is too much for trying to keep the house at 20°&lt;/quote&gt;". The owner of the house thought that he was too used to the US where things just work; "&lt;quote&gt;at least she thinks that in America everything is super-efficient, everything works, and she thought 'this is the way things work in Ireland'&lt;/quote&gt;". So he did not make any progress with the owner. &lt;/p&gt;
    &lt;p&gt; At that point, most people would probably just give up and live with the problem; "&lt;quote&gt;I hacked my heating controller instead&lt;/quote&gt;". He set the temperatures in both thermostats to zero, which effectively disabled their ability to affect the heaters at all, and built two small boards running MicroPython, each connected to a temperature and humidity sensor device. He wrote code that would check the temperature every five minutes and send the appropriate commands to start or stop the heaters based on what it found. &lt;/p&gt;
    &lt;p&gt; So the second half of his first winter in Ireland went great. The sensors are accurate to ±0.5°C, so "&lt;quote&gt;problem solved&lt;/quote&gt;". But, that led to a new problem for him. "&lt;quote&gt;I wanted to know things: What's the temperature right now? Is the heating running right now or not? How many hours did it run today compared to yesterday?&lt;/quote&gt;" And so on. &lt;/p&gt;
    &lt;p&gt; He added a small LCD screen to display some information, but he had to actually go to the device and look at it; what he really wanted was to be able to talk to the device over WiFi and get information from the couch while he was watching TV. "&lt;quote&gt;I wanted to host a web server [...] that will show me a little dashboard&lt;/quote&gt;". &lt;/p&gt;
    &lt;p&gt; So he searched for web frameworks for MicroPython; in the winter of 2018-2019, "&lt;quote&gt;there were none&lt;/quote&gt;". Neither Flask nor Bottle, which is a good bit smaller, would run on MicroPython; both are too large for the devices, but, in addition, the standard library for MicroPython is a subset of that of CPython, so many things that they need are missing. A "&lt;quote&gt;normal person&lt;/quote&gt;" would likely have just accepted that and moved on; "&lt;quote&gt;I created a web framework instead.&lt;/quote&gt;" &lt;/p&gt;
    &lt;head rend="h4"&gt;Demo&lt;/head&gt;
    &lt;p&gt;He brought one of his thermostat devices to Prague for the conference and did a small demonstration of it operating during the talk. The device was connected to his laptop using USB, which provided power, but also a serial connection to the board. On the laptop, he used the rshell remote MicroPython shell to talk to the board, effectively using the laptop as a terminal.&lt;/p&gt;
    &lt;p&gt;He started the MicroPython read-eval-print loop (REPL) on the board in order to simulate the normal operation of the board. When it is plugged into the wall, rather than a laptop, it will boot to the web server, so he made that happen with a soft-reboot command. The device then connected to the conference WiFi and gave him the IP address (and port) where the server was running.&lt;/p&gt;
    &lt;p&gt;He switched over to Firefox on his laptop and visited the site, which showed a dashboard that had the current temperature (24.4°) and relative humidity (56.9%) of the room. He also used curl from the laptop to contact the api endpoint of the web application, which returned JSON with the two values and the time. There is no persistent clock on the board, so the application contacts an NTP server to pick up the time when it boots; that allows it to report the last time a measurement was taken.&lt;/p&gt;
    &lt;p&gt; Grinberg said that he wanted to set the expectations at the right level by looking at the capabilities of the microcontrollers he often uses with Microdot. For example, the ESP8266 in his thermostat device has 64KB of RAM and up to 4MB of flash. The ESP8266 is the smallest and least expensive (around €5) device with WiFi that he has found; there are many even smaller devices, but they lack the networking required for running a web server. The other devices he uses are the Raspberry Pi Pico W with 2MB of flash and 256KB of RAM and the ESP32 with up to 8MB of flash and 512KB of RAM. He contrasted those with his laptop, which has 32GB of RAM, so "&lt;quote&gt;you need 500,000 ESP8266s&lt;/quote&gt;" to have the same amount of memory. &lt;/p&gt;
    &lt;head rend="h4"&gt;Features&lt;/head&gt;
    &lt;p&gt;The core framework of Microdot is in a single microdot.py file. It is fully asynchronous, using the MicroPython subset of the CPython asyncio module, so it can run on both interpreters. It uses asyncio because that is the only way to do concurrency on the microcontrollers; there is no support for processes or threads on those devices.&lt;/p&gt;
    &lt;p&gt;Microdot has Flask-style route decorators to define URLs for the application. It has Request and Response classes, as well as hooks to run before and after requests, he said. Handling query strings, form data, and JSON are all available in Microdot via normal Python dictionaries. Importantly, it can handle streaming requests and responses; because of the limited memory of these devices, it may be necessary to split up the handling of larger requests or responses.&lt;/p&gt;
    &lt;p&gt; It supports setting cookies and sending static files. Web applications can be constructed from a set of modules, using sub-applications, which are similar to Flask blueprints. It also has its own web server with TLS support. "&lt;quote&gt;I'm very proud of all the stuff I was able to fit in the core Microdot framework&lt;/quote&gt;", Grinberg said. &lt;/p&gt;
    &lt;p&gt;He hoped that attendees would have to think for a minute to come up with things that are missing from Microdot, but they definitely do exist. There are some officially maintained extensions, each in its own single .py file, to fill some of those holes. They encompass functionality that is important, but he did not want to add to the core because that would make it too large to fit on the low-end ESP8266 that he is using.&lt;/p&gt;
    &lt;p&gt; There is an extension for multipart forms, which includes file uploads; "&lt;quote&gt;this is extremely complicated to parse, it didn't make sense to add it into the core because most people don't do this&lt;/quote&gt;". There is support for WebSocket and server-sent events (SSE). Templates are supported using utemplate for both Python implementations or Jinja, which only works on CPython. There are extensions for basic and token-based authentication and for secure user logins with session data; the latter required a replacement for the CPython-only PyJWT, which Grinberg wrote and contributed to MicroPython as jwt.py. There is a small handful of other extensions that he quickly mentioned as well. &lt;/p&gt;
    &lt;p&gt; "&lt;quote&gt;I consider the documentation as part of the framework&lt;/quote&gt;"; he is "&lt;quote&gt;kind of fanatical&lt;/quote&gt;" about documenting everything. If there is something missing or not explained well, "&lt;quote&gt;it's a bug that I need to fix&lt;/quote&gt;". He writes books, so the documentation is organized similarly; it comes in at 9,267 words, which equates to around 47 minutes of reading time. There is 100% test coverage, he said, and there are around 30 examples, with more coming. &lt;/p&gt;
    &lt;p&gt; A design principle that he follows is "&lt;quote&gt;no dark magic&lt;/quote&gt;". An example of dark magic to him is the Flask application context, "&lt;quote&gt;which very few people understand&lt;/quote&gt;". In Microdot, the request object is explicitly passed to each route function. Another example is the dependency injection that is used by the FastAPI framework to add components; Microdot uses explicit decorators instead. &lt;/p&gt;
    &lt;p&gt; He used the cloc utility to count lines of code, while ignoring comments and blank lines. Using that, Django comes in at 110,000 lines, Flask plus its essential Werkzeug library is 15,500 lines, FastAPI with Starlette is 14,900 lines, Bottle is around 3,000 lines, while the Microdot core has 765 lines ("&lt;quote&gt;believe it or not&lt;/quote&gt;") and a full install with all the extensions on MicroPython comes in at just shy of 1,700 lines of code. &lt;/p&gt;
    &lt;p&gt; He ended with an example of how Microdot can be so small by comparing the URL matching in Flask with Microdot. The Flask version does lots more than Microdot, with more supported types of arguments in a URL and multiple classes in the werkzeug.routing module; it has 1,362 lines of code. For Microdot, there is a more limited set of URL arguments, though there is still the ability to define custom types, and a single URLPattern class; all of that is done in 63 lines of code. "&lt;quote&gt;I don't intend to support everything that Flask supports, in terms of routing, but I intend to support the 20% that covers 80% of the use cases.&lt;/quote&gt;" That is the overall mechanism that he has used to get to something that is so small. &lt;/p&gt;
    &lt;p&gt;An audience member asked about whether the Microdot code was minified in order to get it to fit. Grinberg said that doing so was not all that useful for MicroPython, but the code is smaller on the board because it is precompiled on another system; that results in a microdot.mpy file, which is bytecode for MicroPython. For example, on the low-end device he is using for his thermostats, Microdot would not be able to be compiled on the device itself. There are some other tricks that can also be used for reducing the RAM requirements, like putting the code into the firmware as part of the MicroPython binary.&lt;/p&gt;
    &lt;p&gt; The final question was about performance, and how many requests per second could be handled. Grinberg said that he did not have any real numbers, but that the device he demonstrated is "&lt;quote&gt;really really slow&lt;/quote&gt;". That question led to a blog post in late July where Grinberg tried to more fully answer it. &lt;/p&gt;
    &lt;p&gt;[I would like to thank the Linux Foundation, LWN's travel sponsor, for travel assistance to Prague for EuroPython.]&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Conference&lt;/cell&gt;
        &lt;cell&gt;EuroPython/2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Web&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Aug 23, 2025 15:26 UTC (Sat) by lyda (subscriber, #7429) [Link] (1 responses) When I wrote a lot of python, frameworks like this seemed great. But there's a better way. If you define the OpenAPI definition first, you can then generate the server, you can generate all the clients, you can generate tests for the server and client, as well as fuzz tests for the server. Less common, but you can do the same with gRPC. It also allows you to more easily move from one technology to another. Posted Aug 24, 2025 3:10 UTC (Sun) by ssmith32 (subscriber, #72404) [Link] Certainly not for the problem domain covered in the article. &lt;head&gt;OpenAPI or gRPC is a better path forward&lt;/head&gt;&lt;head&gt;OpenAPI or gRPC is a better path forward&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/Articles/1034121/"/></entry><entry><id>https://news.ycombinator.com/item?id=45156826</id><title>What is the origin of the private network address 192.168.*.*? (2009)</title><updated>2025-09-07T23:08:19.791266+00:00</updated><content/><link href="https://lists.ding.net/othersite/isoc-internet-history/2009/oct/msg00000.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45157020</id><title>Show HN: I'm a dermatologist and I vibe coded a skin cancer learning app</title><updated>2025-09-07T23:08:18.039265+00:00</updated><content>&lt;doc fingerprint="7fb8a917f1b73ba5"&gt;
  &lt;main&gt;
    &lt;p&gt;For the best experience, please scan the QR code with your phone's camera to use the app on your mobile device.&lt;/p&gt;
    &lt;p&gt;Are you worried about this skin lesion?Swipe left (concerned) / right (not concerned) or use the buttons.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://molecheck.info/"/></entry><entry><id>https://news.ycombinator.com/item?id=45157110</id><title>Serverless Horrors</title><updated>2025-09-07T23:08:17.117135+00:00</updated><content>&lt;doc fingerprint="499a9798958793a"&gt;
  &lt;main&gt;
    &lt;head rend="h5"&gt;Posts&lt;/head&gt;
    &lt;head rend="h4"&gt;~$1189.420/month&lt;/head&gt;
    &lt;p&gt;For no reason, Webflow charged me $1189.420 for a single month from a 69$/month plan.&lt;/p&gt;
    &lt;head rend="h4"&gt;$100,000.420&lt;/head&gt;
    &lt;p&gt;I ran a semi popular WebGL games uploading site that was hit bad by a DoS and I got a single day firebase bill for $100k...&lt;/p&gt;
    &lt;head rend="h4"&gt;$738.420&lt;/head&gt;
    &lt;p&gt;I subscribe to Vercel Pro for $20 per month. I also added a spending limit of $120, so no nasty surprises, right?&lt;/p&gt;
    &lt;head rend="h4"&gt;$70,000.69&lt;/head&gt;
    &lt;p&gt;You pay $50/month for your project, but one day you woke up to a $70,000 bill...&lt;/p&gt;
    &lt;head rend="h4"&gt;$22.639,69&lt;/head&gt;
    &lt;p&gt;I received an insanely bill of 22k USD today from simply using BigQuery on a public data set in the playground...&lt;/p&gt;
    &lt;head rend="h4"&gt;$250/month&lt;/head&gt;
    &lt;p&gt;9.000 page visits and I have to pay $250/month for it, that is $3000/year...&lt;/p&gt;
    &lt;head rend="h4"&gt;$1273.69&lt;/head&gt;
    &lt;p&gt;We asked Devin (AI) to make a change in our codebase....&lt;/p&gt;
    &lt;head rend="h4"&gt;$530.19&lt;/head&gt;
    &lt;p&gt;Never had to pay anything and suddenly im billed $530....&lt;/p&gt;
    &lt;head rend="h4"&gt;$383.69&lt;/head&gt;
    &lt;p&gt;Woke up to an almost $400 bill for my documentation site...&lt;/p&gt;
    &lt;head rend="h4"&gt;$103.26&lt;/head&gt;
    &lt;p&gt;Why $103 is a horror story? Well, imagine that you are on a free tier...&lt;/p&gt;
    &lt;head rend="h4"&gt;$96,280.69&lt;/head&gt;
    &lt;p&gt;So freaking speechless right now....&lt;/p&gt;
    &lt;head rend="h4"&gt;$120,000.420&lt;/head&gt;
    &lt;p&gt;Cloudflare took down our website after trying to force us to pay 120k$ within 24h...&lt;/p&gt;
    &lt;head rend="h4"&gt;$1,300.69&lt;/head&gt;
    &lt;p&gt;Imagine you create an empty, private AWS S3 bucket in a region of your preference...&lt;/p&gt;
    &lt;head rend="h4"&gt;$11,000.69&lt;/head&gt;
    &lt;p&gt;Sent $11k worth of emails during DoS attack, then lost my database...&lt;/p&gt;
    &lt;head rend="h4"&gt;$104,500.123&lt;/head&gt;
    &lt;p&gt;So I received an email from Netlify last weekend saying that I have a $104,500.00 bill overdue...&lt;/p&gt;
    &lt;head rend="h4"&gt;$23,000.420&lt;/head&gt;
    &lt;p&gt;What is happening?! Someone spammed EchoFox and spiked my Vercel bill to $23k and caused 56k+ accounts and trials...&lt;/p&gt;
    &lt;head rend="h4"&gt;$3,000.69&lt;/head&gt;
    &lt;p&gt;Attention Vercel users. Be careful what you test or deploy to Vercel. I decided to try out...&lt;/p&gt;
    &lt;head rend="h4"&gt;$620.123&lt;/head&gt;
    &lt;p&gt;My sitemap.txt used hundreds of GB/hours apparently...&lt;/p&gt;
    &lt;head rend="h4"&gt;$72,000.999&lt;/head&gt;
    &lt;p&gt;We Burnt $72K testing Firebase + Cloud Run and almost went Bankrupt...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://serverlesshorrors.com/"/></entry><entry><id>https://news.ycombinator.com/item?id=45157466</id><title>Algebraic Effects in Practice with Flix</title><updated>2025-09-07T23:08:16.658415+00:00</updated><content>&lt;doc fingerprint="f002d9f161f7126e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Algebraic Effects in Practice with Flix&lt;/head&gt;
    &lt;p&gt;Algebraic effects are not just a research concept anymore. You can use them in real software, today. Here’s why you’d want to do that, in order of importance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Effects make your code testable&lt;/p&gt;
        &lt;p&gt;One of the central goals of enterprise software development. Dependency injection, mocking, architecture patterns like clean, hexagonal, DDD are all meant to tackle this. Effects solve this elegantly by separating the “what” from the “how”.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Effects give immediate visibility into what your own and 3rd-party code is doing&lt;/p&gt;
        &lt;p&gt;Supply chain attacks are real. And they will get worse with more AI slop entering our codebases. Tools like Go’s Capslock fix this by following the whole chain of calls to stdlib functions. Effects provide this by design, as all effects are tracked by the type and effect system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Effects enable user-defined control flow abstractions&lt;/p&gt;
        &lt;p&gt;Solving the “what color is your function” problem1. You can also leverage effects to implement Async/await, coroutines, backtracking search and other control flow patterns as user libraries without hard-coding these features into the language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Algebraic effects come from the pure functional world, serving a purpose similar to monads — keeping track of and having control over side effects. Like monads, they enable us to write our core logic with pure functions and push side effects like IO outwards, closer to application boundaries.&lt;/p&gt;
    &lt;p&gt;Unlike monads, effects are easy to grasp for a regular developer and give immediate benefits when starting out. For me personally they’re a more natural abstraction for managing side effects — after all, effects are in the name.&lt;/p&gt;
    &lt;p&gt;Starting out as an academic concept, algebraic effects were introduced to the world by research languages like Eff, Koka, Effekt, Frank, Links, and more recently Ante.&lt;/p&gt;
    &lt;p&gt;People have also applied effects in practice, so far usually via a monad-based approach, by making libraries in established languages like Scala Kyo / Cats Effect / ZIO; Typescript Effect and Effector, C# language-ext, C libhandler and libmprompt, C++ cpp-effects, various Haskell libraries, etc.&lt;/p&gt;
    &lt;p&gt;In addition to forcing you into a monadic way of thinking, libraries implementing effects are limited by their host languages.&lt;/p&gt;
    &lt;p&gt;In this article, I will walk you through applying algebraic effects on a real world example using Flix, a new programming language that is built with effects from the ground up, and supports functional, logic and imperative paradigms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Type and Effect System: A Motivating Example&lt;/item&gt;
      &lt;item&gt;Effect Handlers: Building Intuition&lt;/item&gt;
      &lt;item&gt;Real-World App: AI movie recommendations&lt;/item&gt;
      &lt;item&gt;Where to Go From Here&lt;/item&gt;
      &lt;item&gt;Extra: Why Algebraic Effects are Algebraic and how they relate to monads&lt;/item&gt;
      &lt;item&gt;Footnotes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Currently only few languages support effects out of the box. The only one that I know of besides Flix is Unison. OCaml has a language extension, but there is no support yet in the type system. Haskell has added support for delimited continuations, but effects are still only available via libraries.&lt;/p&gt;
    &lt;p&gt;In addition to having a “type and effect system” that improves function signatures and makes sure all effects are handled, Flix supports traits, local mutability via regions, working with immutable or mutable data, and Go/Rust-like structured concurrency. It also has a first-class Datalog integration. But I will only focus on effects here. Let’s start.&lt;/p&gt;
    &lt;head rend="h2"&gt;Type and Effect System: A Motivating Example 🔗&lt;/head&gt;
    &lt;p&gt;Imagine a function called &lt;code&gt;calculateSalary&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;def calculateSalary(base_salary, bonus_percent):
&lt;/code&gt;
    &lt;p&gt;Based on the function name and the signature, one can assume it’s just a pure function that does some calculations. In a statically typed language you are also guaranteed that the function arguments and outputs will be of a certain type.&lt;/p&gt;
    &lt;p&gt;But even if the types are correct, nothing stops our little &lt;code&gt;calculateSalary()&lt;/code&gt; from, say, sending an offensive email to your grandma2:&lt;/p&gt;
    &lt;code&gt;def calculateSalary(base_salary, bonus_percent):
    server.sendmail("grandma@family.com", "Your cookies are terrible!")
    return base_salary * (1 + bonus_percent/100)
&lt;/code&gt;
    &lt;p&gt;If, on the other hand, you extend your type system with effects, you will see immediately in the signature that this function may do something fishy:&lt;/p&gt;
    &lt;code&gt;def calculateSalary(salary: Float64, percent: Float64): 
    Float64 \ {Email} = {
//            ^^^^^^^ Notice the Email effect!
&lt;/code&gt;
    &lt;p&gt;Of course, in real life the issue it’s not usually about the grandma. Instead, this function could throw an exception — still quite dangerous. If you forget to handle the exception, your app will crash. Or another very realistic scenario is that &lt;code&gt;calculateSalary()&lt;/code&gt; calls a database to get some employee details for calculations, and you forgot to provide a database connection string. That can also result in an exception or a panic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Effect Handlers: Building Intuition 🔗&lt;/head&gt;
    &lt;p&gt;The job of the type and effect system is not just to improve our function signatures. It’s also making sure all the effects are handled somewhere. This is where effect handlers come in.&lt;/p&gt;
    &lt;p&gt;Usually when people talk about algebraic effects what they’re actually talking about is effect handlers. If you know exceptions, effect handlers are super easy to understand. Here’s a Jenga analogy:&lt;/p&gt;
    &lt;p&gt;Imagine the call stack is a Jenga tower. New blocks are carefully added each time you call a function.&lt;/p&gt;
    &lt;p&gt;When an exception is thrown, your whole nice Jenga tower gets destroyed, all the way up to the catch() block. The catch block can safely handle the error, but the stack is unwinded, meaning you lose all of the state you had in your program before throwing the exception. You have to build your tower again, from scratch.&lt;/p&gt;
    &lt;p&gt;When using effect handlers you can actually go back to your original computation after the handler is done handling the effect. The handler can also return some values back to your program, and it can even resume multiple times with different return values. You also still have the option of not resuming at all and aborting the program — that would be the effect equivalent of exceptions.&lt;/p&gt;
    &lt;p&gt;Back to the Jenga analogy: if your tower is about to fall down, with effects you can freeze it mid-collapse. You then call someone for help (handler), and they decide whether to let the tower fall, magically restore it to the previous statlte. Or even hand you different blocks to try the same move (call the continuation) again, possibly multiple times with different inputs. Your Jenga tower ends up looking more like a fork or a tree, with multiple different copies of your blocks branching out at some point from the base.&lt;/p&gt;
    &lt;p&gt;To make this more concrete, let’s start by reproducing exceptions with effects. Here’s how a try/catch looks like in Python:&lt;/p&gt;
    &lt;code&gt;def divide(x, y):
    try:
        return x / y
    except ZeroDivisionError:
        print("Division by zero!")
        return None
&lt;/code&gt;
    &lt;p&gt;Here’s the equivalent code in Flix. We first define an Exception effect and a &lt;code&gt;divide()&lt;/code&gt; function:&lt;/p&gt;
    &lt;code&gt;eff Exception {
    def throw(msg: String): Void
}

def divide(x: Int32, y: Int32): Int32 \ Exception = 
    if (y == 0) {
        Exception.throw("Division by zero!")
    } else {
        x / y
    }
&lt;/code&gt;
    &lt;p&gt;And then provide a handler for this effect somewhere, preferably close to &lt;code&gt;main()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;def main(): Unit \ IO = 
    run {
        println(divide(10, 0))
    } with handler Exception {
        def throw(msg, _resume) = println("Error: ${msg}")
    }
&lt;/code&gt;
    &lt;p&gt;What this does is registers an effect called Exception with a method &lt;code&gt;throw()&lt;/code&gt;. We then perform this effect in our function when there’s an error, similar to throwing an exception in the Python version. Control is transferred to the effect handler, which then decides how to handle the exception, similar to a &lt;code&gt;catch()&lt;/code&gt; block in Python.&lt;/p&gt;
    &lt;p&gt;Notice we never call &lt;code&gt;resume()&lt;/code&gt; from the handler. This results in the program being aborted, just like with exceptions. Graphically, this can be represented as follows:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 2 A["Statement 1"] space:1 B["Statement 2"] space:1 C["Statement 3"] space:1 D["Perform Effect"] space:1 space:1 E["Handle Effect"] space:1 F["Process &amp;amp; Exit"] space:1 space:1 D --&amp;gt; E style D fill:#ffcccc,color:#000 style E fill:#ccffcc,color:#000 style F fill:#ccffcc,color:#000&lt;/quote&gt;
    &lt;p&gt;So far so good, but this is not much different from Python. To really take full advantage of effect handlers, we can use &lt;code&gt;resume()&lt;/code&gt; to return to the original computation and proceed from the line after the effect was performed:&lt;/p&gt;
    &lt;code&gt;eff ResumableException {
    def askForInput(): Int32
}

def divide(x: Int32, y: Int32): Int32 \ ResumableException = 
    if (y == 0) {
        let newY = ResumableException.askForInput();
        x / newY
    } else {
        x / y
    }

def main(): Unit \ IO = 
    run {
        println(divide(10, 0))
    } with handler ResumableException {
        def askForInput(_, resume) = {
            println("Enter a new divisor:");
            resume(5) // Or get from user input
        }
    }
&lt;/code&gt;
    &lt;quote&gt;block-beta columns 2 A["Statement 1"] space:1 B["Statement 2"] space:1 C["Statement 3"] space:1 D["Perform Effect"] space:1 space:1 E["Handle Effect"] space:1 F["Resume"] space:1 space:1 G["Statement 4"] space:1 H["Statement 5"] space:1 I["Complete"] space:1 D --&amp;gt; E F --&amp;gt; G style D fill:#ffcccc,color:#000 style E fill:#ccffcc,color:#000 style F fill:#ffffcc,color:#000&lt;/quote&gt;
    &lt;p&gt;I called the effect &lt;code&gt;ResumableException&lt;/code&gt; here, but it’s not really an exception anymore, because the program continues normally.&lt;/p&gt;
    &lt;p&gt;At this point we can use this power bestowed on us by effects and handlers to roll our own Async/await:&lt;/p&gt;
    &lt;code&gt;eff Async {
    def await(url: String): String
}

def fetchData(): String \ Async = 
    Async.await("https://api.example.com/data")

def processData(): String \ Async = {
    let data = fetchData();
    "processed: ${data}"
}

def main(): Unit \ IO = 
    run {
        let result = processData();
        println(result)
    } with handler Async {
        def await(url, resume) = {
            // Simulate async HTTP request
            let result = "data from ${url}";
            resume(result)
        }
    }
&lt;/code&gt;
    &lt;p&gt;See how easy that was? This approach also avoids function coloring, since we didn’t need to use special keywords anywhere. Here’s a graphic version:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 2 A["Statement 1"] space:1 B["Statement 2"] space:1 C["await operation"] space:1 space:1 H1["Start async work"] space:1 H2["⏳ Long pause..."] space:1 H3["⏳ Still waiting..."] space:1 H4["✅ Async complete"] space:1 F["Resume with result"] space:1 space:1 D["Statement 3"] space:1 E["Complete"] space:1 C --&amp;gt; H1 F --&amp;gt; D style C fill:#ffcccc,color:#000 style H1 fill:#ccffcc,color:#000 style H2 fill:#fff3cd,color:#000 style H3 fill:#fff3cd,color:#000 style H4 fill:#d1ecf1,color:#000 style F fill:#ffffcc,color:#000 style D fill:#e7f3ff,color:#000 style E fill:#d4edda,color:#000&lt;/quote&gt;
    &lt;p&gt;That’s cool, but we can do more. Effect handlers allow you to resume multiple times:&lt;/p&gt;
    &lt;code&gt;eff Choose {
    def choose(): Int32
}

def explore(): String \ Choose = {
    let x = Choose.choose();
    let y = Choose.choose();
    "${x}, ${y}"
}

def main(): Unit \ IO = 
    run {
        println(explore())
    } with handler Choose {
        def choose(_, resume) = {
            resume(1);
            resume(2);
            resume(3)
        }
&lt;/code&gt;
    &lt;quote&gt;block-beta columns 4 A["Statement 1"] space:1 space:1 space:1 B["Statement 2"] space:1 space:1 space:1 C["Statement 3"] space:1 space:1 space:1 D["Perform Effect"] space:1 space:1 space:1 space:1 space:1 E["Handle Effect"] space:1 space:1 F1["Resume 1"] F2["Resume 2"] F3["Resume 3"] space:1 G1["Statement 4a"] G2["Statement 4b"] G3["Statement 4c"] space:1 H1["Statement 5a"] H2["Statement 5b"] H3["Statement 5c"] space:1 R1["Resume to Main"] R2["Resume to Main"] R3["Resume to Main"] J["Statement 6"] space:1 space:1 space:1 K["Complete"] space:1 space:1 space:1 D --&amp;gt; E F1 --&amp;gt; G1 F2 --&amp;gt; G2 F3 --&amp;gt; G3 H1 --&amp;gt; R1 H2 --&amp;gt; R2 H3 --&amp;gt; R3 R1 --&amp;gt; J R2 --&amp;gt; J R3 --&amp;gt; J style D fill:#ffcccc,color:#000 style E fill:#ccffcc,color:#000 style F1 fill:#ffffcc,color:#000 style F2 fill:#ffffcc,color:#000 style F3 fill:#ffffcc,color:#000 style G1 fill:#e6f3ff,color:#000 style G2 fill:#ffe6f3,color:#000 style G3 fill:#f3ffe6,color:#000 style H1 fill:#e6f3ff,color:#000 style H2 fill:#ffe6f3,color:#000 style H3 fill:#f3ffe6,color:#000 style R1 fill:#d4edda,color:#000 style R2 fill:#d4edda,color:#000 style R3 fill:#d4edda,color:#000 style J fill:#cce5ff,color:#000 style K fill:#b3d9ff,color:#000&lt;/quote&gt;
    &lt;p&gt;With this, you can implement things like coroutines:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 3 A1["Coroutine 1: Start"] space:1 A2["Coroutine 2: Start"] B1["Statement 1"] space:1 B2["Statement 1"] C1["yield to Co2"] H1["Scheduler"] space:1 space:1 space:1 C2["Statement 2"] space:1 space:1 D2["yield to Co1"] space:1 H2["Scheduler"] space:1 D1["Statement 2"] space:1 space:1 E1["yield to Co2"] H3["Scheduler"] space:1 space:1 space:1 E2["Statement 3"] space:1 space:1 F2["Complete"] F1["Complete"] space:1 space:1 C1 --&amp;gt; H1 H1 --&amp;gt; C2 D2 --&amp;gt; H2 H2 --&amp;gt; D1 E1 --&amp;gt; H3 H3 --&amp;gt; E2 style C1 fill:#ffcccc,color:#000 style D2 fill:#ffcccc,color:#000 style E1 fill:#ffcccc,color:#000 style H1 fill:#ccffcc,color:#000 style H2 fill:#ccffcc,color:#000 style H3 fill:#ccffcc,color:#000 style A1 fill:#e6f3ff,color:#000 style B1 fill:#e6f3ff,color:#000 style D1 fill:#e6f3ff,color:#000 style F1 fill:#e6f3ff,color:#000 style A2 fill:#ffe6f3,color:#000 style B2 fill:#ffe6f3,color:#000 style C2 fill:#ffe6f3,color:#000 style E2 fill:#ffe6f3,color:#000 style F2 fill:#ffe6f3,color:#000&lt;/quote&gt;
    &lt;p&gt;Generators:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 2 A["Start generator"] space:1 B["Statement 1"] space:1 C["yield value 1"] H1["Return value"] space:1 H2["⏸️ Paused"] D["next() called"] H3["Resume generator"] E["Statement 2"] space:1 F["yield value 2"] H4["Return value"] space:1 H5["⏸️ Paused"] G["next() called"] H6["Resume generator"] H["Statement 3"] space:1 I["return (done)"] H7["Signal complete"] C --&amp;gt; H1 H3 --&amp;gt; D F --&amp;gt; H4 H6 --&amp;gt; G I --&amp;gt; H7 style C fill:#ffcccc,color:#000 style F fill:#ffcccc,color:#000 style I fill:#ffcccc,color:#000 style H1 fill:#ccffcc,color:#000 style H3 fill:#ffffcc,color:#000 style H4 fill:#ccffcc,color:#000 style H6 fill:#ffffcc,color:#000 style H7 fill:#ccffcc,color:#000 style H2 fill:#fff3cd,color:#000 style H5 fill:#fff3cd,color:#000 style D fill:#e7f3ff,color:#000 style G fill:#e7f3ff,color:#000&lt;/quote&gt;
    &lt;p&gt;And backtracking search:&lt;/p&gt;
    &lt;quote&gt;block-beta columns 4 A["Start search"] space:1 space:1 space:1 B["choose option"] space:1 space:1 space:1 space:1 H1["Try option 1"] space:1 space:1 space:1 space:1 C1["Explore path 1"] space:1 space:1 space:1 D1["❌ Dead end"] space:1 space:1 H2["Backtrack"] space:1 space:1 space:1 H3["Try option 2"] space:1 space:1 space:1 space:1 space:1 C2["Explore path 2"] space:1 space:1 space:1 D2["✅ Success!"] E["Resume with solution"] space:1 space:1 space:1 F["Complete"] space:1 space:1 space:1 B --&amp;gt; H1 H1 --&amp;gt; C1 D1 --&amp;gt; H2 H2 --&amp;gt; H3 H3 --&amp;gt; C2 D2 --&amp;gt; E style B fill:#ffcccc,color:#000 style H1 fill:#ccffcc,color:#000 style H2 fill:#f8d7da,color:#000 style H3 fill:#ccffcc,color:#000 style C1 fill:#fff3cd,color:#000 style D1 fill:#f8d7da,color:#000 style C2 fill:#d1ecf1,color:#000 style D2 fill:#d4edda,color:#000 style E fill:#ffffcc,color:#000 style F fill:#d4edda,color:#000&lt;/quote&gt;
    &lt;p&gt;Hopefully this gives you a taste of how effect handlers work. This is just a sketch though — you can read more on this and see examples in the Flix docs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Question&lt;/head&gt;
    &lt;p&gt;What's your primary programming language?&lt;/p&gt;
    &lt;p&gt;Defining our own control flow abstractions is great, but most of the time regular async/await and/or coroutines are enough for the job.&lt;/p&gt;
    &lt;p&gt;What is extremely useful for daily programming is that effects let you separate the declaration of the effect (the operation, or the effect “constructor”) from it’s implementation, defined by the effect handler.&lt;/p&gt;
    &lt;p&gt;Add some effect definitions:&lt;/p&gt;
    &lt;code&gt;eff Database {
    def getUser(id: Int32): Option[User],
    def saveUser(user: User): Unit
}
&lt;/code&gt;
    &lt;p&gt;Then use these definitions to perform effects in your code:&lt;/p&gt;
    &lt;code&gt;def updateUserEmail(userId: Int32, newEmail: String): Result[String, User] \ {Database} = {
    match Database.getUser(userId) {
        case Some(user) =&amp;gt; {
            let updatedUser = {user | email = newEmail};
            Database.saveUser(updatedUser);
            Ok(updatedUser)
        }
        case None =&amp;gt; {
            Err("User not found")
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;This replaces the need for dependency injection, since you can provide different handlers for these database operations in production vs testing:&lt;/p&gt;
    &lt;code&gt;def main(): Unit \\ IO = { // production handler, uses a real database
    run {
        updateUserEmail(123, "new@example.com")
    } with handler Database {
        def getUser(id, resume) = {
		        // real db query
            resume(user)
        }
        def saveUser(user, resume) = {
		        // real db query
            resume()
        }
    }
}

def testUpdateUserEmail(): Unit = { // test handler, just stubs
    let testUser = {id = 123, email = "old@example.com"};
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result == Ok({testUser | email = "new@example.com"}))
    } with handler Database {
        def getUser(id, resume) = resume(Some(testUser))
        def saveUser(user, resume) = {
            assert(user.email == "new@example.com");
            resume()
        }
    
}
&lt;/code&gt;
    &lt;p&gt;In my opinion, the biggest advantage that effect handlers give is that they abstract away the patterns associated with DDD, Clean Architecture, Hexagonal architecture, etc. commonly found in enterprise code.&lt;/p&gt;
    &lt;p&gt;All these architectures give you some sort of way to isolate your core logic, which should be pure, from infrastructure and app logic, with deals with external dependencies. But you have to commit to an architecture and the whole team has to be disciplined enough to stick to for this to work.&lt;/p&gt;
    &lt;p&gt;Using effects encourages separating the definition of effect operations from implementation by default, meaning you don’t really need these architecture patterns anymore.&lt;/p&gt;
    &lt;p&gt;This is great, since relying on team discipline exclusively rarely works. It also saves a bunch of time otherwise spent on bike shedding.&lt;/p&gt;
    &lt;p&gt;Effect handlers also allow you to easily install stubs, which you can use to create quick test cases without boilerplate, just by swapping handlers:&lt;/p&gt;
    &lt;code&gt;def testErrorConditions(): Unit = {
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result == Err("User not found"))
    } with handler Database {
        def getUser(_, resume) = resume(None) // Stub: always return None
        def saveUser(_, resume) = resume()             // Won't be called
    }
}

def testSlowDatabase(): Unit = {
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result.isOk())
    } with handler Database {
        def getUser(id, resume) = {
            Thread.sleep(100);  // Simulate slow query
            resume(Some({id = id, email = "old@example.com"}))
        }
        def saveUser(user, resume) = {
            Thread.sleep(50);   // Simulate slow save
            resume()
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;You can even make a handler that records all interactions instead of executing them. There are many possibilities here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Real-World App: AI movie recommendations 🔗&lt;/head&gt;
    &lt;p&gt;To bring this all together, let’s make a real application using effects.&lt;/p&gt;
    &lt;p&gt;Our app will fetch some movie data from TheMovieDB, and then use an LLM to recommend some movies based on user preferences provided from the console.&lt;/p&gt;
    &lt;p&gt;Flix interoperates with the JVM, meaning we can call code from Java, Kotlin, Scala, etc.&lt;/p&gt;
    &lt;p&gt;First, let’s define the two custom effects we will need: MovieAPI and LLM:&lt;/p&gt;
    &lt;code&gt;eff MovieAPI {
    def getPopularMovies(): String
}

eff LLM {
    def recommend(movies: String, preferences: String): String
}
&lt;/code&gt;
    &lt;p&gt;We can then perform the effects in main like so, providing some basic handlers that use the Flix’s stdlib HTTP client:&lt;/p&gt;
    &lt;code&gt;def getRecommendation(preferences: String): String \ {MovieAPI, LLM} = {
    let movies = MovieAPI.getPopularMovies();
    LLM.recommend(movies, preferences)
}

def main(): Unit \ {Net, IO} = 
    run {
        let suggestion = getRecommendation("action movies");
        println(suggestion)
    } with handler MovieAPI {
        def getPopularMovies(_, resume) = {
            let response = HttpWithResult.get("https://api.themoviedb.org/3/movie/popular", Map.empty());
            match response {
                case Result.Ok(resp) =&amp;gt; resume(Http.Response.body(resp))
                case Result.Err(_) =&amp;gt; resume("[]")
            }
        }
    } with handler LLM {
        def recommend(movies, prefs, resume) = {
            let prompt = "Movies: ${movies}. User likes: ${prefs}. Recommend one movie.";
            let response = HttpWithResult.post("https://api.openai.com/v1/completions", Map.empty(), prompt);
            match response {
                case Result.Ok(resp) =&amp;gt; resume(Http.Response.body(resp))
                case Result.Err(_) =&amp;gt; resume("Try watching a classic!")
            }
        }
    } with HttpWithResult.runWithIO
&lt;/code&gt;
    &lt;p&gt;Notice that both effects are quite generic. So we can easily swap either the movie API or the LLM provider without touching anything in the core logic:&lt;/p&gt;
    &lt;code&gt;// Switch to different movie provider
with handler MovieAPI {
    def getPopularMovies(_, resume) = {
        let response = HttpWithResult.get("https://api.imdb.com/popular", Map.empty());
        // ... handle IMDB response format
    }
}

// Switch to different LLM provider  
with handler LLM {
    def recommend(movies, prefs, resume) = {
        let response = HttpWithResult.post("https://api.anthropic.com/v1/messages", Map.empty(), prompt);
        // ... handle Claude response format
    }
}
&lt;/code&gt;
    &lt;p&gt;To get the user input we will need to include the standard Console effect:&lt;/p&gt;
    &lt;code&gt;def main(): Unit \ {Net, IO} = 
    run {
        Console.println("What movie genres do you enjoy?");
        let preferences = Console.readln();
        let suggestion = getRecommendation(preferences);
        Console.println("Recommendation: ${suggestion}")
    } with handler MovieAPI { /* ... */ }
      with handler LLM { /* ... */ }
      with Console.runWithIO
      with HttpWithResult.runWithIO
&lt;/code&gt;
    &lt;p&gt;We can also add some basic logs using the standard Logger effect:&lt;/p&gt;
    &lt;code&gt;def getRecommendation(preferences: String): String \ {MovieAPI, LLM, Logger} = {
    Logger.info("Fetching popular movies...");
    let movies = MovieAPI.getPopularMovies();
    Logger.info("Getting LLM recommendation...");
    LLM.recommend(movies, preferences)
}

def main(): Unit \ {Net, IO} = 
    run {
        /* ... console interaction ... */
    } with handler MovieAPI { /* ... */ }
      with handler LLM { /* ... */ }
      with Console.runWithIO
      with Logger.runWithIO
      with HttpWithResult.runWithIO
&lt;/code&gt;
    &lt;p&gt;That’s it! Let’s run the app and test it manually like so:&lt;/p&gt;
    &lt;code&gt; flix run Main.flix
What movie genres do you enjoy?
&amp;gt; sci-fi horror
[INFO] Fetching popular movies...
[INFO] Getting LLM recommendation...
Recommendation: Based on your interest in sci-fi horror, I recommend "Alien" - a perfect blend of both genres!
&lt;/code&gt;
    &lt;p&gt;We can also easily write tests for the core logic by providing test handlers for our movie and LLM effects:&lt;/p&gt;
    &lt;code&gt;def testRecommendation(): String = 
    run {
        getRecommendation("comedy")
    } with handler MovieAPI {
        def getPopularMovies(_, resume) = {
            resume("""[{"title": "The Grand Budapest Hotel", "genre": "comedy"}]""")
        }
    } with handler LLM {
        def recommend(movies, prefs, resume) = {
            resume("I recommend The Grand Budapest Hotel - perfect for comedy lovers!")
        }
    } with handler Logger {
        def log(_, _, resume) = resume()  // Silent in tests
    }

def runTests(): Unit \ IO = {
    let result = testRecommendation();
    println("Test result: ${result}")
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Where to Go From Here 🔗&lt;/head&gt;
    &lt;p&gt;Read the Flix docs&lt;/p&gt;
    &lt;p&gt;Especially on cool features like effect polymorphism, effect exclusion etc. Check out code examples in the repo&lt;/p&gt;
    &lt;p&gt;Join the community and contribute with libraries&lt;/p&gt;
    &lt;p&gt;The Flix compiler and stdlib are quite feature-rich at this point, and having JVM interop means you have all the essentials you need to write practical code. But there are still very few pure Flix libraries. So it’s very valuable to contribute some. The ideas I can think of are, for example, rebuilding standard things like Web frameworks in an effect oriented way,. Or taking advantage of the unique feature set in Flix to build something entirely new.&lt;/p&gt;
    &lt;p&gt;Explore effect-oriented programming&lt;/p&gt;
    &lt;p&gt;While I personally like Flix and can recommend it to others, there are other ways you can use effects for real-world software. If you’re in Typescript or Scala, try out Effect or ZIO/Kyo/Cats. If you’re looking for other languages that support effects natively, and you’re not afraid of Haskell-like syntax, check out Unison. They have a bunch of other concepts I find cool, like a better distributed computing model and the code being content-addressed.&lt;/p&gt;
    &lt;p&gt;Thanks for reading! I hope this article was useful. Hit me up if you have questions or feedback, and check out my website, where I’m exploring sustainable tech and coding practices: relax.software&lt;/p&gt;
    &lt;head rend="h4"&gt;Question&lt;/head&gt;
    &lt;p&gt;What should I write about next?&lt;/p&gt;
    &lt;head rend="h2"&gt;Extra: Why Algebraic Effects are Algebraic and how they relate to monads 🔗&lt;/head&gt;
    &lt;p&gt;Okay, practical people have left the room. Following sections are nerds-only.&lt;/p&gt;
    &lt;p&gt;For some reason, all the content I’ve been reading on algebraic effects uses this term a lot, but no one explains why specifically they’re called “algebraic”. So I did some digging.&lt;/p&gt;
    &lt;p&gt;Turns out, algebraic effects are “algebraic” because they can be described with laws and equations, like in algebra — the kind we learn at school. Which is I guess why they’re easier to grasp than monads — unlike algebra, you usually don’t study category theory in high school.&lt;/p&gt;
    &lt;p&gt;But the algebraic part only applies to the effect “constructors”, i.e the operations themselves like &lt;code&gt;get()&lt;/code&gt; or &lt;code&gt;put()&lt;/code&gt; for the state effect.&lt;/p&gt;
    &lt;p&gt;Effect handlers, on the other hand, are not algebraic at all, which can be a bit confusing. But it makes sense if you think about it — the purpose of handlers is to act as “deconstructors”, interpreting our algebraic effect operations by means of things that cannot be described by algebraic equations alone, such as continuations .&lt;/p&gt;
    &lt;p&gt;In fact, effect handlers are often (but not always) implemented via delimited continuations. There are also other, static/lexically scoped and maybe more performant approaches being explored, such as this one&lt;/p&gt;
    &lt;p&gt;“Real” algebraic effects don’t require monads. Monads and algebraic effects are two different concepts tackling similar problems. One is expressible in terms of the other, but algebraic effects are arguably more flexible.&lt;/p&gt;
    &lt;p&gt;You could actually implement algebraic effects using a continuation monad. If we don’t care about types, effects are perfectly expressible with monads and vice versa&lt;/p&gt;
    &lt;p&gt;The problems appear when we introduce types into the picture. In a properly typed world, you can’t actually reproduce the same expressiveness you get with effects using monads. You’ll end up breaking the type system or reducing expressiveness at some point.&lt;/p&gt;
    &lt;p&gt;Effects are, in this sense, more “powerful” than monads with their natural type system: you can express infinitely many computations with them. E.g if you use a &lt;code&gt;tick()&lt;/code&gt; effect and you do a bunch of sequential &lt;code&gt;tick()&lt;/code&gt; s, the result will be a distinct computation each time. With monads and their natural type system the set of computations you could express is finite.&lt;/p&gt;
    &lt;p&gt;Additionally, with monads you commit to a specific interpretation of an effect in advance, while effects completely decouple effect definition from it’s implementation.&lt;/p&gt;
    &lt;p&gt;Finally, effects are easier to compose than monads. With monad transformers you quickly hit the wall having to define a bunch of different combinations that each have distinct semantics. Effects compose naturally.&lt;/p&gt;
    &lt;p&gt;So while effect libraries in languages like Typescript and Scala are able to express effects using monads3, and the behavior could be identical at runtime, this cannot replace having an actual type and effect system, with effects being properly typed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Question&lt;/head&gt;
    &lt;p&gt;How do you usually learn about new things?&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes 🔗&lt;/head&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;“What color is your function” is a problem explored in this article. In languages which have Async baked in via special keywords (e.g JavaScript async/await) it becomes a pain to refactor and to combine synchronous and asynchronous code. If you make one function deep in the call stack&lt;/p&gt;&lt;code&gt;async&lt;/code&gt;, all the callers will have to be made Async as well, or&lt;code&gt;await()&lt;/code&gt;the results. With effects you don’t have this issue as there are no keywords and no special behavior. Async is simply done with effect handlers. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I like the grandma example more than the “launch missiles” popular in the Haskell world. Took it from this article by Kevin Mahoney. It’s somehow more offensive ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See some examples in this article. This also shows how Haskell’s new delimited continuation support can be used to implement algebraic effects and handlers ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.relax.software/blog/flix-effects-intro/"/></entry><entry><id>https://news.ycombinator.com/item?id=45157906</id><title>Belling the Cat</title><updated>2025-09-07T23:08:16.532630+00:00</updated><content>&lt;doc fingerprint="be03d5d395e3bec6"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Belling the Cat&lt;/head&gt;&lt;p&gt;Belling the Cat is a fable also known under the titles The Bell and the Cat and The Mice in Council. In the story, a group of mice agree to attach a bell to a cat's neck to warn of its approach in the future, but they fail to find a volunteer to perform the job. The term has become an idiom describing a group of persons, each agreeing to perform an impossibly difficult task under the misapprehension that someone else will be chosen to run the risks and endure the hardship of actual accomplishment.[1]&lt;/p&gt;&lt;p&gt;Although often attributed to Aesop, it was not recorded before the Middle Ages and has been confused with the quite different fable of Classical origin titled The Cat and the Mice. In the classificatory system established for the fables by Ben Edwin Perry, it is numbered 613, which is reserved for Mediaeval attributions outside the Aesopic canon.[2]&lt;/p&gt;&lt;head rend="h2"&gt;Synopsis and idiomatic use&lt;/head&gt;[edit]&lt;p&gt;The fable concerns a group of mice who debate plans to nullify the threat of a marauding cat. One of them proposes placing a bell around its neck, so that they are warned of its approach. The plan is applauded by the others, until one mouse asks who will volunteer to place the bell on the cat. All of them make excuses. The story is used to teach the wisdom of evaluating a plan on not only how desirable the outcome would be but also how it can be executed. It provides a moral lesson about the fundamental difference between ideas and their feasibility, and how this affects the value of a given plan.[3]&lt;/p&gt;&lt;p&gt;The fable gives rise to the idiom to bell the cat, which means to attempt, or agree to perform, an impossibly difficult task.[4] Historically 'Bell the Cat' is frequently claimed to have been a nickname given to fifteenth-century Scottish nobleman Archibald Douglas, 5th Earl of Angus in recognition of his part in the arrest and execution of James III's alleged favourite, Thomas (often misnamed as Robert) Cochrane. In fact the earliest evidence for this use is from Hume of Godscroft's history of the Douglases published in 1644,[5] and therefore is more reflective of perception of the idiom in the seventeenth century than the fifteenth.[6] In the 21st century the idiom was adopted by the investigative journalism group Bellingcat.[7]&lt;/p&gt;&lt;p&gt;The first English collection to attribute the fable to Aesop was John Ogilby's of 1687; in this there is a woodcut (by Francis Barlow), followed by a 10-line verse synopsis by Aphra Behn with the punning conclusion:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Good Councell's easily given, but the effect&lt;/p&gt;&lt;lb/&gt;Oft renders it uneasy to transact.[8]&lt;/quote&gt;&lt;head rend="h2"&gt;Early versions and later interpretations&lt;/head&gt;[edit]&lt;p&gt;One of the earliest versions of the story appears as a parable critical of the clergy in Odo of Cheriton's Parabolae.[9] Written around 1200, it was afterwards translated into Welsh, French and Spanish. Sometime later, the story is found in the work now referred to as Ysopet-Avionnet, which is largely made up of Latin poems by the 12th century Walter of England, followed by a French version dating from as much as two centuries later. It also includes four poems not found in Walter's Esopus; among them is the tale of "The Council of the Mice" (De muribus consilium facientibus contra catum). The author concludes with the scornful comment that laws are of no effect without the means of adequately enforcing them and that such parliamentary assemblies as he describes are like the proverbial mountain in labour that gives birth to a mouse.[10]&lt;/p&gt;&lt;p&gt;The fable also appeared as a cautionary tale in Nicholas Bozon's Anglo-Norman Contes Moralisés (1320), referring to the difficulty of curbing the outrages of superior lords.[11] It was in this context too that the story of a parliament of rats and mice was retold in William Langland's allegorical poem Piers Plowman.[12] The episode is said to refer to the Parliament of 1376 which attempted unsuccessfully to remedy popular dissatisfaction over the exactions made by nobles acting in the royal name.[13] Langland's French contemporary, the satirical Eustache Deschamps, also includes the story among his other moral ballades based on fables as "Les souris et les chats".[14] It has been suggested that in this case too there is a political subtext. The poem was written as a response to the aborted invasion of England in 1386 and contrasts French dithering in the face of English aggression.[15] The refrain of Deschamps' ballade, Qui pendra la sonnette au chat (who will bell the cat) was to become proverbial in France if, indeed, it does not record one already existing.&lt;/p&gt;&lt;p&gt;In the following century, the Italian author Laurentius Abstemius made of the fable a Latin cautionary tale titled De muribus tintinnabulum feli appendere volentibus (The mice who wanted to bell the cat)[16] in 1499. A more popular version in Latin verse was written by Gabriele Faerno and printed posthumously in his Fabulae centum ex antiquis auctoribus delectae (100 delightful fables from ancient authors, Rome 1564), a work that was to be many times reprinted and translated up to start of the 19th century. Titled simply "The Council of the Mice", it comes to rest on the drily stated moral that 'a risky plan can have no good result'. The story was evidently known in Flanders too, since 'belling the cat' was included among the forty Netherlandish Proverbs in the composite painting of Pieter Bruegel the Elder (1559). In this case a man in armour is performing the task in the lower left foreground.[17] A century later, La Fontaine's Fables made the tale even better known under the title Conseil tenu par les rats (II.2).[18]&lt;/p&gt;&lt;p&gt;In mediaeval times the fable was applied to political situations and British commentaries on it were sharply critical of the limited democratic processes of the day and their ability to resolve social conflict when class interests were at stake. This applies equally to the plot against the king's favourite in 15th century Scotland and the direct means that Archibald Douglas chose to resolve the issue. While none of the authors who used the fable actually incited revolution, the 1376 Parliament that Langland satirised was followed by Wat Tyler's revolt five years later, while Archibald Douglas went on to lead a rebellion against King James. During the Renaissance the fangs of the fable were being drawn by European authors, who restricted their criticism to pusillanimous conduct in the face of rashly proposed solutions. A later exception was the Russian fabulist Ivan Krylov, whose adaptation of the story satirises croneyism. In his account only those with perfect tails are to be allowed into the assembly; nevertheless, a tailless rat is admitted because of a family connection with one of the lawmakers.[19]&lt;/p&gt;&lt;p&gt;There still remains the perception of a fundamental opposition between consensus and individualism. This is addressed in the lyrics of "Bell the Cat",[20] a performance put out on DVD by the Japanese rock band LM.C in 2007.[21] This is the monologue of a house cat that wants to walk alone since "Society is by nature evil". It therefore refuses to conform and is impatient of restriction: "your hands hold on to everything – bell the cat". While the lyric is sung in Japanese, the final phrase is in English. Another modernised adaptation based on this fable, that updates the moral, has been published by Patricia McKissack in her Who Will Bell the Cat? (illustrated by Christopher Cyr).[22][23]&lt;/p&gt;&lt;p&gt;There is a Tibetan proverb that is very similar, "Putting a bell on the cat's neck after the mother of mice was consulted"[24]&lt;/p&gt;&lt;head rend="h2"&gt;Illustrations&lt;/head&gt;[edit]&lt;p&gt;Several French artists depicted the fable during the 19th century, generally choosing one of two approaches. Gustave Doré and the genre painter Aurélie Léontine Malbet (fl. 1868–1906)[25] pictured the rats realistically acting out their debate. The illustrator Grandville,[26] along with the contemporaries Philibert Léon Couturier (1823–1901)[27] and Auguste Delierre (1829–1890),[28] caricature the backward practice and pomposity of provincial legislatures, making much the same point as did the Mediaeval authors who first recorded the tale. At the end of the century a publishing curiosity reverts to the first approach. This was in the woodblock print by Kawanabe Kyōsui that appeared in the collection of La Fontaine's fables that was commissioned and printed in Tokyo in 1894 and then exported to France.[29] In the upper left-hand corner a cat is seen through a warehouse window as it approaches across the roofs while inside the rats swarm up the straw-wrapped bales of goods. At its summit the chief rat holds the bell aloft. An earlier Japanese woodblock formed part of Kawanabe Kyōsai's Isoho Monogotari series (1870–80). This shows an assembly of mice in Japanese dress with the proposer in the foreground, brandishing the belled collar.[30]&lt;/p&gt;&lt;head rend="h2"&gt;Musical settings&lt;/head&gt;[edit]&lt;p&gt;In the 18th century the fable was one among many set by Louis-Nicolas Clérambault in the fables section of Nouvelles poésies spirituelles et morales sur les plus beaux airs (1730–37).[31] In the following century the text of La Fontaine's fable was set for male voices by Louis Lacombe[32] and by the Catalan composer Isaac Albéniz for medium voice and piano in 1889.[33] In 1950 it was set for four male voices by Florent Schmitt.[34] But while La Fontaine's humorously named cat Rodilardus, and antiquated words like discomfiture (déconfiture), may fit an art song, there have also been faithful interpretations in the field of light music. A popular composer of the day, Prosper Massé, published such a setting in 1846.[35] More recently there has been Pierre Perret's interpretation as part of his 20 Fables inspirées de Jean de la Fontaine (1995),[36] and a jazz arrangement on Daniel Roca's 10 Fables de La Fontaine (2005).[37]&lt;/p&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Collective action problem&lt;/item&gt;&lt;item&gt;Who Will Bell the Cat?, a children's picture book based on the fable&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Strouf, Judie L. H. (2005). The literature teacher's book of lists. Jossey-Bass. p. 13. ISBN 0787975508.&lt;/item&gt;&lt;item&gt;^ Ben Edwin Perry (1965). Babrius and Phaedrus. Loeb Classical Library. Cambridge, MA: Harvard University Press. pp. 545, no. 613. ISBN 0-674-99480-9.&lt;/item&gt;&lt;item&gt;^ "Belling The Cat". Fables of Aesop. 2016-07-05. Retrieved 2021-03-04.&lt;/item&gt;&lt;item&gt;^ "To Bell the Cat" thefreedictionary.com. Retrieved 9 November 2007.&lt;/item&gt;&lt;item&gt;^ David Reid, David Hume of Godscroft's History of the House of Angus, vol. 1 (STS: Edinburgh, 2005), p. 26.&lt;/item&gt;&lt;item&gt;^ Macdougall, Norman (1982). James III: A Political Study. Edinburgh: John Donald. pp. 287–288. ISBN 0859760782.&lt;/item&gt;&lt;item&gt;^ "Bellingcat: Digital Sleuths on the Hunt for Truth"&lt;/item&gt;&lt;item&gt;^ "21. De cato et muribus (1687), illustrated by Francis Barlow". Mythfolklore.net. Retrieved January 26, 2011.&lt;/item&gt;&lt;item&gt;^ Laura (15 May 2009). "Christianizing Aesop: The Fables of Odo of Cheriton". Journey to the Sea. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ Ysopet-Avionnet, the Latin and French texts, University of Illinois 1919; fable LXII, pp. 190–192; this is archived online&lt;/item&gt;&lt;item&gt;^ Les contes moralisés de Nicole BozonParis, 1889, pp. 144–145; archived here&lt;/item&gt;&lt;item&gt;^ William's Vision of Piers Plowman by William Langland, edited by Ben Byram-Wigfield (2006), Prologue, lines 146–181; online text here Archived 2011-08-07 at the Wayback Machine&lt;/item&gt;&lt;item&gt;^ "The Parliament of the Rats and Mice". Medieval Forum. SFSU. Archived from the original on 10 March 2022. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ Poésies morales et historiques d'Eustache Deschamps, Paris 1832, pp. 188–189&lt;/item&gt;&lt;item&gt;^ Robert Landru, Eustache Deschamps, Fédération des sociétés d'histoire et d'archéologie de l'Aisne, vol. XV 1969, p. 126&lt;/item&gt;&lt;item&gt;^ Fable 195&lt;/item&gt;&lt;item&gt;^ View on Wikimedia Commons&lt;/item&gt;&lt;item&gt;^ "Elizur Wright's translation". Oaks.nvg.org. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ Kriloff's Fables, translated by C. Fillingham Coxwell, London 1920, pp. 38–39; archived online&lt;/item&gt;&lt;item&gt;^ "Lyrics | LM.C – Bell The Cat (English)". SongMeanings. 25 April 2010. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ "Bell the CAT/LM.C". YouTube. 18 November 2007. Archived from the original on 2021-12-12. Retrieved 26 January 2011.&lt;/item&gt;&lt;item&gt;^ Who will bell the cat?. OCLC 1037155724.&lt;/item&gt;&lt;item&gt;^ "Who Will Bell the Cat?". Publishers Weekly. PWxyz LLC. February 19, 2018. Retrieved April 6, 2022.&lt;/item&gt;&lt;item&gt;^ p. 135, Tsewang, Pema. 2012. Like a Yeti Catching Marmots. Boston: Wisdom Publications.&lt;/item&gt;&lt;item&gt;^ Exhibited at the 1888 Salon; photo online&lt;/item&gt;&lt;item&gt;^ "See online". Archived from the original on July 20, 2011. Retrieved 17 August 2012.&lt;/item&gt;&lt;item&gt;^ "In the Musée Denon de Chalon-sur-Saône". Philibert-leon-couturier.com. Retrieved 17 August 2012.&lt;/item&gt;&lt;item&gt;^ "In the Musée La Fontaine at Château Thierry". Retrieved 17 August 2012.&lt;/item&gt;&lt;item&gt;^ George Baxley. "baxleystamps.com". baxleystamps.com. Retrieved 17 August 2012.&lt;/item&gt;&lt;item&gt;^ View online Archived 2012-03-25 at the Wayback Machine&lt;/item&gt;&lt;item&gt;^ The score is printed in: John Metz, The Fables of La Fontaine: A Critical Edition of the Eighteenth-Century, Pendragon Press 1986, p. 45&lt;/item&gt;&lt;item&gt;^ Op. 85, 1879, Score at Gallica&lt;/item&gt;&lt;item&gt;^ Liedernet&lt;/item&gt;&lt;item&gt;^ Op. 123, Liedernet&lt;/item&gt;&lt;item&gt;^ Bibliographie de la France, 14 March 1846, 127&lt;/item&gt;&lt;item&gt;^ "Pierre Perret chante 20 fables inspirées de Jean de La Fontaine Perret, Pierre, 1934–..." bibliotheques.avignon.fr.&lt;/item&gt;&lt;item&gt;^ Track available on Jamendo&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;External links&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Media related to Belling the Cat at Wikimedia Commons&lt;/item&gt;&lt;item&gt;The dictionary definition of belling the cat at Wiktionary&lt;/item&gt;&lt;item&gt;19th–20th century book illustrations online&lt;/item&gt;&lt;item&gt;Collection of primary fable sources online&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Belling_the_Cat"/></entry><entry><id>https://news.ycombinator.com/item?id=45158814</id><title>SQLite's Use of Tcl (2017)</title><updated>2025-09-07T23:08:16.164808+00:00</updated><content>&lt;doc fingerprint="64a9edcfc230b6b"&gt;
  &lt;main&gt;
    &lt;p&gt;SQLite is a TCL extension that has escaped into the wild.&lt;/p&gt;
    &lt;p&gt;The design of SQLite was inspired by the design of TCL, both in the way it handles datatypes and in the formatting of its source code. The index use case for SQLite was in a Tcl/Tk application for an industrial company. From its inception, SQLite has always depended heavily on TCL. These days, SQLite no longer uses TCL internally and can be run separately from any TCL interpreter, and yet the SQLite development process still depends heavily on TCL.&lt;/p&gt;
    &lt;p&gt;SQLite is an SQL database engine, and the most widely used database engine in the world. SQLite is built into all cellphones as a core component and is the primary means of data persistence on phones. SQLite is also an integral part of most web browsers. SQLite is built into MacOS and is used by most of the default applications on that platform. Windows10 requires the C:\Windows\System32\winsqlite3.dll file in order to boot. Countless other popular applications like Skype and WhatsApp and iTunes depend on SQLite.&lt;/p&gt;
    &lt;p&gt;Because SQLite is open source and can be freely downloaded and duplicated, exact usage numbers are unavailable. But reasonable estimates are that there are more SQLite instances in operation today than there are people on earth. Most devices that use the SQLite database engine contain hundreds of separate databases, and there are billions of such devices. Hence, there are likely around one trillion SQLite databases in active use.&lt;/p&gt;
    &lt;p&gt;There are more copies of SQLite in use than there are copies of Linux. We know this because SQLite is used on almost all Linux systems, but SQLite is also used on many other non-linux systems such as Macs, iPhones, and Windows computers. By similar arguments, there are more copies of SQLite in use than there are Macs, or Windows PCs. There are probably more copies of SQLite in use than all other database engines combined. It seems likely that SQLite is the most widely used and deployed software component of any kind, with the possible exception of the zLib compression library.&lt;/p&gt;
    &lt;p&gt;SQLite is not written in TCL. Rather, SQLite is intended to be used by TCL. Like the TCL interpreter, SQLite is written in ANSI C.&lt;/p&gt;
    &lt;p&gt;The fact that SQLite was primarily intended to be used from TCL is evident in an number of ways.&lt;/p&gt;
    &lt;p&gt;All programming languages, other than C/C++, require some kind of adaptor in order to interface with the SQLite C implementation. Language adaptors for SQLite are widely available from third party programmers. The adaptors for PHP and Python are built into those languages, for example. A Java adaptor is baked into Android. And so forth. Only the TCL adaptor is included as part of the SQLite core. The source code file that implements the TCL adaptor for SQLite, "tclsqlite.c", was part of the very first check-in on the SQLite source repository on 2000-05-29. (See https://sqlite.org/src/ci/6f3655f79f9b6fc9.)&lt;/p&gt;
    &lt;p&gt;All modern SQL implementations provide a means to do late binding of parameter values to the SQL statements. Usually a naked "?" character, or a "?" followed by an integer is used. For example:&lt;/p&gt;
    &lt;quote&gt;SELECT passwd, photo FROM user WHERE uid=?1&lt;/quote&gt;
    &lt;p&gt;The "?1" token in the SQL above would be assigned a value at run-time in order to look up the password and photograph of a particular user.&lt;/p&gt;
    &lt;p&gt;SQLite supports this syntax. But because of its TCL heritage, SQLite also allows the parameter to take the form of a TCL variable. Hence:&lt;/p&gt;
    &lt;quote&gt;SELECT passwd, photo FROM user WHERE uid=$uid&lt;/quote&gt;
    &lt;p&gt;When a statement such as the above is run, the TCL language adaptor automatically binds the value of the $uid TCL variable to the SQL statement before it is evaluated, providing an intuitive and seamless interface between TCL and SQL. SQLite is the only database engine that behaves this way.&lt;/p&gt;
    &lt;p&gt;The TCL heritage of SQLite is visible in the type system of SQLite. Early versions of SQLite (prior to 2004) operated on the classic TCL principal that "everything is a string". Beginning with SQLite3 (2004-06-18), SQLite also supports binary data. However, types are still very flexible in SQLite, just as they are in TCL. SQLite treats the datatypes on column names in a CREATE TABLE statement as suggestions rather than hard requirements. SQLite is happy to store a 100KB string value in a column that is declared "SHORT INT", just as TCL is happy to store either a large string or a small integer in the same variable. There are some differences in how SQLite deals with datatypes, in comparison to TCL, due to the different nature of the SQL language. SQLite has the concept of "type affinity". If a column is declared "INT" and one inserts a string into that column that looks like an integer and can be safely converted into an integer without loss of information, then that conversion occurs automatically. This provides a measure of compatibility with the rigid type systems of other SQL database engines.&lt;/p&gt;
    &lt;p&gt;The flexible type system of SQLite seems natural and intuitive to programmers with prior experience programming in TCL. Curiously, though, it is a source of frustration and frequent complaining from programmers accustomed to the rigid and unforgiving type systems of languages like Java.&lt;/p&gt;
    &lt;p&gt;The similarities in the type systems of TCL and SQLite extends to more than just the interface. An important part of the C/C++ interface for SQLite is the "sqlite3_value" object (https://sqlite.org/c3ref/value.html) which is analogous to the Tcl_Obj object in TCL. Both TCL and SQLite use a dual-representation approach, where each value can be represented simultaneously as both a string and some other type.&lt;/p&gt;
    &lt;p&gt;SQLite began as a TCL extension, though these days most uses of SQLite are in applications written in languages other than TCL. Many programmers who use SQLite in their applications have no knowledge or awareness of TCL. The SQLite source code used by most developers is a single file of pure C code named "sqlite3.c" that contains no TCL code. This is what we mean when we say that SQLite as "escaped" into the wild. Deployed instances of SQLite no longer depends on TCL.&lt;/p&gt;
    &lt;p&gt;Nevertheless, SQLite is still heavily dependent upon TCL and the ongoing support, maintenance, and enhancement of SQLite would not be possible without TCL, and would be seriously inconvenienced without Tk.&lt;/p&gt;
    &lt;p&gt;The deliverable source code for SQLite is a single file named "sqlite3.c" and its companion header "sqlite3.h". Both files are 100% ANSI-C code. But developers do not edit these files directly. The sqlite3.c and sqlite3.h source files are build products, and the source tree used to build those files is over 50% TCL code. Figure 1 nearby shows the exact ratios.&lt;/p&gt;
    &lt;p&gt;Figure 1 is for the main SQLite source repository. Many of the test cases and much of the documentation is held in separate repositories, not included in Figure 1. The separate repositories also contain a great deal of TCL code.&lt;/p&gt;
    &lt;p&gt;Much of the TCL code in the main SQLite repository consists of test scripts. At this writing, the core repository contains 1153 separate test scripts totally about 389 KB of space. But this is not the only use of TCL in SQLite.&lt;/p&gt;
    &lt;p&gt;A non-trivial amount of the deliverable C code for SQLite is machine generated. Some of the machine generated code is created by C programs, such as LEMON which translates the SQL language grammar into C code to implement a push-down automaton to parse the SQL language. But much of the automatically generated code is created using TCL scripts. TCL is well suited for scanning source files to extract information to be merged with other files and for making mechanized edits. For example, the byte-code engine used to evaluate SQL statements inside of SQLite is implemented as a large "switch" statement inside a "for" loop, with a separate "case" for each opcode, all in the "vdbe.c" source file. At build-time, TCL scripts scan the vdbe.c source file looking for the appropriate "case" statements and then build header files that assign consecutive integers to each symbolic opcode name. (The opcodes used by the SQLite byte-code engine are not an API as they are in TCL and thus can change from one build to the next.) This mapping of symbolic opcode names into integers is not a simple as one might suppose. For reasons of optimization, there are many constraints on the specific values that are assigned to opcodes. For example, many opcodes such as OP_Add must have the same numeric value as the corresponding "+" token in the SQL language parser. Sometimes a group of related opcodes, such as the comparison operators OP_Eq, OP_Ne, OP_Lt, OP_Le, OP_Ge, and OP_Gt, need to be assigned consecutive integers in a specific order. These constraints are all handled effortlessly in TCL. Accomplishing the same with AWK would be rather more difficult.&lt;/p&gt;
    &lt;p&gt;Perhaps the most important task for TCL during the SQLite build process is constructing the SQLite amalgamation source code file. Recall that most developers use SQLite in the form of a single big file of C code named "sqlite3.c" and referred to as "the amalgamation". A TCL script named "mksqlite3c.tcl" runs in order to construct the amalgamation from over one hundred separate input files. Each of these inputs files must be added to the amalgamation in just the right order. Furthermore, the source files are edited as part of the amalgamation building process. When mksqlite3c.tcl encounters a "#include" for an SQLite header, it replaces the "#include" with a copy of that header file, taking care to make sure each header file is only included once. The mksqlite3.tcl script automatically adds the "static" keyword to internal SQLite APIs to give them file linkage, and makes other similar edits.&lt;/p&gt;
    &lt;p&gt;In addition to the core SQLite library, the SQLite source tree also contains code for several analysis and control programs. One of these programs is called "sqlite3_analyzer" (or "sqlite3_analyzer.exe" on Windows). The sqlite3_analyzer program examines an SQLite database and generates a detailed report on the disk usage by the various tables and indexes within that database. The sqlite3_analyzer program is very useful in understanding how an application is using disk space.&lt;/p&gt;
    &lt;p&gt;It turns out that sqlite3_analyzer, though disguised as an ordinary executable, is really a TCL application. The main source code file for this application is tool/spaceanal.tcl. During the build process, this script is converted into a C-language string constant (using another TCL script) and added to a very simple C-language wrapper than starts a TCL interpreter and then passes the application script to that interpreter.&lt;/p&gt;
    &lt;p&gt;The sqlite3_analyzer program could be rewritten in pure C. But that would be a lot of code. The TCL script that implements sqlite3_analyzer is less than 1000 lines long. The equivalent C program would surely be at least ten times larger.&lt;/p&gt;
    &lt;p&gt;Note that the sqlite3_analyzer utility program statically links a TCL interpreter and so does not require a TCL installation on the target computer to use. The sqlite3_analyzer utility program is used by tens of thousands of developers, most of whom do not realize that they are really running a TCL application.&lt;/p&gt;
    &lt;p&gt;One of the key features of SQLite is that it uses aviation-grade testing. The tests of SQLite, at a minimum, provide 100% modified condition/decision coverage (MC/DC) of the SQLite code, with independence. 100% MC/DC roughly means that every branch instruction at the machine code level is exercised at least once in each direction. The precise definition of MC/DC is slightly stricter than this, for example when comparing boolean vectors, but the 100% branch tests coverage definition is very close approximation. The "with independence" term means that SQLite is tested in multiple ways with test code being written and maintained by different individuals.&lt;/p&gt;
    &lt;p&gt;The amount of testing done on SQLite is fanatical. On the other hand, that level of testing is necessary for a fundamental low-level component, such as a database engine, that is used by billions of devices. If an ordinary application encounters an obscure bug, it can normally be rebooted to clear the problem. But the job of a database engine is to remember things, and so databases tend to remember their mistakes across reboots. For these reasons, it is important that SQLite have a very low bug density.&lt;/p&gt;
    &lt;p&gt;TCL is used in every aspect of SQLite testing. The test cases that are part of the primary SQLite source code repository are written in TCL. Other test cases such as TH3 and SQLLogicTest are written in C but still depend on TCL for operation.&lt;/p&gt;
    &lt;p&gt;The TH3 test suite is a set of proprietary tests for SQLite that form the primary means of achieving 100% MC/DC. TH3 is designed to run on embedded hardware without the support of a desktop operating system. TH3 consists of over 1350 test modules together with over 100 control files. The test modules are written in either C or SQL or a combination of both. The control files are text formatted in a way that easily parsed by TCL. To build a TH3 test, a TCL script is run that combines some subset of the test modules and control files into a single large C program that will automatically run the required tests. This C program is then linked against the "sqlite3.c" amalgamation and the resulting binary is moved to the target computer and executed. TCL scripts automate this entire process on all major host platforms.&lt;/p&gt;
    &lt;p&gt;To verify that the TH3 tests really do provide 100% MC/DC, special options are added to the TCL scripts that run the tests, causing the GCC coverage analysis tools to be invoked. The output of gcov is then postprocessed to reveal and report any branch instructions that were missed by the tests. The TH3 tests themselves are all implemented in C and SQL, but the operation and management of those tests is all done with TCL.&lt;/p&gt;
    &lt;p&gt;The extensive documentation for SQLite available on the SQLite website (https://sqlite.org/) is all generated by TCL. Many of the documents, such as the API reference documentation and the descriptions of the byte-code engine opcodes, are created by TCL scripts that scan C source code and extract the necessary information from the code and comments. Thus, the API documentation is largely derived from comments in the source code. Keeping the official documentation (in comments) and the source code close together helps ensure that they are in agreement.&lt;/p&gt;
    &lt;p&gt;Other whitepaper documents are generated from source files that look mostly like HTML but which contain additional TCL code embedded inside of &amp;lt;tcl&amp;gt;...&amp;lt;/tcl&amp;gt; markup. The added TCL code is used for advanced formatting techniques, for automatically creating cross-references and cross-links, and for constructing complex displays such as the popular "rail-road" syntax diagrams for SQL.&lt;/p&gt;
    &lt;p&gt;The text editor used by the primary author of SQLite is a custom editor with emacs-style key bindings that is built on top of the Tk Text widget. The "e" editor, as it is called, is cross-platform, which helps in the development of a cross-platform software library like SQLite. The "e" editor has been used for the entire 17-year history of SQLite. It has been enhanced over the years with various customizations created especially to help manage the SQLite source code.&lt;/p&gt;
    &lt;p&gt;The Fossil version control system used for the SQLite source code (and written specifically for that purpose) uses Tcl/Tk to show graphical side-by-side diffs in a separate window. When the "fossil diff --tk" command is run, Fossil generates a script to show the diff graphically and then kicks off a separate "wish" process to run that script and display the diff in a separate window. This graphical diff window has a "Save" button which will cause the Tcl/Tk code needed to reproduce itself to be written to a file. This file can be, in turn, sent to a collaborator for display. Passing around graphical diffs as ordinary text files is much simpler and easier than passing around JPEG images or text "context" diffs.&lt;/p&gt;
    &lt;p&gt;No two SQLite developers work in the same office. The team is geographically distributed. To help the team stay in touch, a custom chatroom has been created using a Tcl/Tk script. The same script works as both client and server. The chatroom is private and uses a proprietary protocol, so that developers are free to discuss sensitive matters without fear of eavesdropping. The chatroom is implemented as just over 1000 lines of Tk code, and is thus accessible and easy to customize. Among the customizations is the ability to send saved "fossil diff --tk" graphical diffs to collaborators and have the diff pop automatically on the collaborators screen. Small features like this seem trivial by themselves, but together than help the developers to work much more efficiently. These kinds of productivity-enhancing features are unavailable to users of commercial business collaboration packages such as HipChat.&lt;/p&gt;
    &lt;p&gt;SQLite is an indispensable element of most modern computer systems, and TCL is an indispensable tool used in the production and maintenance of SQLite. Thus, the computing infrastructure we enjoy today would not exist except for TCL.&lt;/p&gt;
    &lt;p&gt;As deployed, SQLite contains no TCL code. However, the design of SQLite is inspired by TCL. And TCL is used extensively in the code generation, testing, analysis, documentation, and development of SQLite. Without TCL, SQLite would not exist.&lt;/p&gt;
    &lt;p&gt;Every developer and every team has a finite number of "brain cycles" available to do their job. The fewer cycles spent messing with tools, the more cycles are available to devote towards solving the problem. So for maximum productivity, it is important to use tools that get the job done with a minimum of fuss and bother. Our 17-year experience using TCL in the SQLite project has convinced us that TCL is just such a tool. Tcl provides the most help per brain cycle of any similar technology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tcl-lang.org/community/tcl2017/assets/talk93/Paper.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45158968</id><title>The MacBook has a sensor that knows the exact angle of the screen hinge</title><updated>2025-09-07T23:08:15.850129+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/samhenrigold/status/1964428927159382261"/></entry><entry><id>https://news.ycombinator.com/item?id=45160774</id><title>Keeping secrets out of logs (2024)</title><updated>2025-09-07T23:08:15.520200+00:00</updated><content>&lt;doc fingerprint="a622f1124d050d9c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Keeping Secrets Out of Logs&lt;/head&gt;
    &lt;p&gt;This post is about how to keep secrets out of logs, and my claim is that (like many things in security) there isn’t a singular action or silver bullet that lets you do this. I would go so far as to say that there’s not even an 80/20 rule, where one action fixes 80% of the problem. It’s not like preventing SQL injection with prepared statements or preventing buffer overflows by using memory-safe languages.&lt;/p&gt;
    &lt;p&gt;What I will offer instead, are lead bullets, of which there are many. I’m going to talk about 10 of them. They are imperfect and sometimes unreliable things that, if put in the right places and with defense-in-depth, can still give us a real good chance at succeeding. My hope is that by the end, you’ll have a slightly better framework for how to reason about this problem and some new ideas to add to your kit.&lt;/p&gt;
    &lt;p&gt;Table of contents:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;With that, let’s dive in and set the table by talking about the problem with secrets in logs.&lt;/p&gt;
    &lt;p&gt;So, there are some problems that are annoying. And there are some problems that are difficult.&lt;/p&gt;
    &lt;p&gt;This is both. I’m gonna level with you: I absolutely hate this problem. But I’m not going to gaslight you and tell you that this is the most important thing to work on worry about, because it probably isn’t!&lt;/p&gt;
    &lt;p&gt;You have somewhere between 5 and 50 other problems in your backlog that seem more important, 1 of which you found out about this morning. But I think it’s likely that none of those problems are nearly as annoying. While researching this topic, I interviewed about a dozen other engineers and, on this point, they unanimously agreed! Nobody likes dealing with secrets in logs because it is extraordinarily annoying.&lt;/p&gt;
    &lt;p&gt;This is a problem that’s also difficult, but not even in the fun sense, like being technically complex or interesting. Once you catch sensitive data in logs, it’s usually pretty straightforward (at least in retrospect) to determine how they got there. But, it’s also surprisingly elusive to prevent, and it crops up in incredibly unexpected places and ways.&lt;/p&gt;
    &lt;p&gt;Secrets could mean lots of different things to lots of different teams, but I’ll use it interchangeably with “sensitive data”: stuff that you want to keep confidential. What’s so frustrating when breaching confidentiality in logs is the full spectrum of potential impact.&lt;/p&gt;
    &lt;p&gt;In the best case (left), you might log an isolated, internal credential, like an API key, which (kudos!) you rotate right after fixing the source of leak. The impact is minimal, and you just move on. Of course, all the way on the other end of the spectrum (right), you might log something that an attacker or inside threat could use to do some real harm.&lt;/p&gt;
    &lt;p&gt;And then somewhere in-between, where I suspect most of the incidents lie. You might log secrets that you unfortunately, can’t rotate yourself. Things like PII or your customer’s passwords, which are reused on other sites, because of course they are. And, depending on your policies, threat model, or regulations, you might choose to issue a disclosure or notification.&lt;/p&gt;
    &lt;p&gt;And it is painful.&lt;/p&gt;
    &lt;p&gt;You could be doing so many good data security practices, like secure-by-design frameworks, database and field-level encryption, zero-touch production, access control&amp;amp;mldr; but logging bypasses all of that&amp;amp;mldr; and ultimately degrades trust, in your systems and in your company. It feels unfair because it’s only a fraction of your security story.&lt;/p&gt;
    &lt;p&gt;And this is a problem that happens to companies of all sizes:&lt;/p&gt;
    &lt;p&gt;Something about “plaintext” just kinda stings, especially as a security practitioner. It’s like&amp;amp;mldr; the most profane insult you can hurl at a security engineer. Imagine retorting with, “Oh yea? Well, you store your passwords in plaintext!”&lt;/p&gt;
    &lt;p&gt;But logging passwords and storing them in plaintext are&amp;amp;mldr; kinda the same thing.&lt;/p&gt;
    &lt;p&gt;Because while logs are rarely or purposefully public, they’re typically afforded broader access than direct access to your databases.&lt;/p&gt;
    &lt;p&gt;Everyone knows by now that storing plaintext secrets in your database is a terrible idea. Logs, however, are still data-at-rest, and we should treat them with the same level of scrutiny.&lt;/p&gt;
    &lt;p&gt;I cherry picked those examples because they are established companies with very mature security programs. I’m not trying to throw shade; in fact, I deeply respect them for being public and transparent about this. I think this also hints that preventing secrets in logs is a deceptively difficult and frustrating problem.&lt;/p&gt;
    &lt;p&gt;If we can understand some causes, we might gain a deeper appreciation for these past occurrences, and stand a better chance at avoiding new incidents in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Causes&lt;/head&gt;
    &lt;p&gt;This is certainly not comprehensive, but from my interviews and personal experience, here are six of the most common causes.&lt;/p&gt;
    &lt;head rend="h3"&gt;🤦 Direct logging&lt;/head&gt;
    &lt;code&gt;const temp = res.cookie["session"];

// TODO: remove after testing is done
Logger.info("session HERE", { temp });
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Narrator: it was not removed after testing was done&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The first group is perhaps the most obvious and facepalm one: when sensitive data is directly logged. Sometimes it’s purely accidental, like the example above: someone wants to debug session cookies in their local environment and then&amp;amp;mldr; accidentally commits the code. Sometimes it comes from an uninformed position where the developer just doesn’t know any better.&lt;/p&gt;
    &lt;p&gt;These tend to be fairly easy to trace down the exact line of code or commit that introduces it. With this example, you can just grep the codebase for &lt;code&gt;session here&lt;/code&gt; and you’ll find it instantly.&lt;/p&gt;
    &lt;head rend="h3"&gt;🚰 Kitchen sinks&lt;/head&gt;
    &lt;code&gt;const client = googleSdk.admin(...);
try {
  const res = client.tokens.list(...);
} catch (e) {
  Logger.error("failed fetch", { e });
}
&lt;/code&gt;
    &lt;p&gt;I’m sure you’ve seen or written code like this before. Here we have an API client or SDK that is used to fetch some data. Exceptions are caught, kind of, and then promptly logged so that on-call engineers can debug the errors.&lt;/p&gt;
    &lt;p&gt;What happens?&lt;/p&gt;
    &lt;p&gt;That error is decorated with a config object stuffed with secrets and the full response object, which is also stuffed with secrets, and now they’re both in your logs!&lt;/p&gt;
    &lt;code&gt;{
  e: {
    status: 400,
    ...
    config: { 💥☠️🪦 },
    response: { 💣😭😱 },
  }
}
&lt;/code&gt;
    &lt;p&gt;I call these “kitchen sinks,” objects that contain or hold secrets, often in opaque or unexpected ways. Think of an actual kitchen sink that’s filled to the brim with dirty dishes and you can’t easily tell what’s at the bottom without reaching into it. Maybe it’s a spoon, or maybe it’s knife and now you have to go to the hospital. What tends to happen is that the whole kitchen sink gets logged, and the logging library happily serializes it, including parts that were actually sensitive.&lt;/p&gt;
    &lt;p&gt;This seems to happen with code that attaches additional data onto errors, or code that logs full request and response objects. It’s typically a bit hard to catch in code review unless you know to look for them. If you are blessed with static types, seeing an &lt;code&gt;any&lt;/code&gt; type flow into logs can be a good hint that you’re
logging too much.&lt;/p&gt;
    &lt;head rend="h3"&gt;🔧 Configuration changes&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Narrator: it was not okay&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Next example: someone needs additional observability and changes a setting like the global log level. You know exactly what happens, here. This dev is about to have a bad time and find out that hope, in fact, is not a valid strategy.&lt;/p&gt;
    &lt;p&gt;We started with an observability problem. Now we also have security problem: brand new secrets are getting emitted into logs.&lt;/p&gt;
    &lt;p&gt;In that example (that totally never happened to me ever), developers built production around log levels set to &lt;code&gt;WARN&lt;/code&gt; and above, but once you flip it to
&lt;code&gt;DEBUG&lt;/code&gt;, all this new stuff comes out of the woodwork.&lt;/p&gt;
    &lt;p&gt;These type of configuration changes tend to involve a system that was built with one set of assumptions, but some kind of modification moves that system from a known state into a unknown state, introducing a new set of problems.&lt;/p&gt;
    &lt;p&gt;These often involve low-level or global utilities like logging config, HTTP middleware, or some central piece of infra like a load balancer. They tend to be singletons that are difficult or costly to test, or they crop up only at runtime. On the positive side, it’s usually loud and quick to patch, but cleanup can be kinda painful.&lt;/p&gt;
    &lt;head rend="h3"&gt;🥧 Embedded secrets&lt;/head&gt;
    &lt;code&gt;app.get("/login/:slug", async (req, res) =&amp;gt; {
  const magicLink = req.params["slug"];
  await login({ magicLink });
});
&lt;/code&gt;
    &lt;p&gt;I completely made up this phrase, but the idea is that secrets are coupled to, embedded into, and baked into more general formats like URLs or remote procedure calls. The central idea is that it’s designed into the format and the system, and can’t easily be separated.&lt;/p&gt;
    &lt;p&gt;Say you have a magic login link handler (see above) where a user can click a link and sign into a web app. There’s nothing in that code that logs the link, but if you look at HTTP logs, it’s right there in plain view:&lt;/p&gt;
    &lt;code&gt;47.29.201.179 - - [17/Jul/2024:13:17:10 +0000] "GET /login/Uj79z1pe01...
&lt;/code&gt;
    &lt;p&gt;These types of leaks arise from fundamental designs that don’t take logging into consideration or incorrectly assume some end-to-end flow. The sensitivity gets lost out of context, and ends up getting logged in another layer, system, or service.&lt;/p&gt;
    &lt;head rend="h3"&gt;📡 Telemetry&lt;/head&gt;
    &lt;code&gt;try:
    db_name = os.getenv("DB_NAME")
    db_pass = os.getenv("DB_PASS") # 🤫 Secret!
    conn = db.connect(db_name, db_pass)
    ...
except Error as e:
    # Don't log e! Not today!!11
    Logger.error("failed to connect")
finally:
    conn.close()
&lt;/code&gt;
    &lt;p&gt;Next example: we have some Python code that’s connecting to a database, we’re specifically NOT logging the error object, and we want to ensure we always close out the connection.&lt;/p&gt;
    &lt;p&gt;How can &lt;code&gt;db_pass&lt;/code&gt; possibly make it into logs? Telemetry!&lt;/p&gt;
    &lt;p&gt;"Oops, that's a log, too!"&lt;/p&gt;
    &lt;p&gt;It turns out that things like error monitoring and analytics can totally be logs, too. I kind of cheated in the code example, because there’s no mention of telemetry in it at all, but it turns out that if you hook it up to error monitoring like Sentry (above), run-time errors send the local variable context right to the dashboard, and you can see the database password in plaintext.&lt;/p&gt;
    &lt;p&gt;These causes tend to bypass the central logging pipeline and become Yet Another Place to have to worry about secrets.&lt;/p&gt;
    &lt;head rend="h3"&gt;🕺🏻 User input&lt;/head&gt;
    &lt;p&gt;Alright, last example. Say there’s a sign in form and the entire dev team made super duper sure that the password field is totally locked down from logging, they read this super awesome post, and took care of all the causes we discussed.&lt;/p&gt;
    &lt;p&gt;What happens?&lt;/p&gt;
    &lt;p&gt;Users end up jamming passwords into the username field!&lt;/p&gt;
    &lt;p&gt;So if you ever looked at login alerts for AWS and saw usernames replaced with &lt;code&gt;HIDDEN_DUE_TO_SECURITY_REASONS&lt;/code&gt;, this is precisely why!&lt;/p&gt;
    &lt;p&gt;Everything that’s within proximity to sensitive user input tends to be radioactive. It could be a UI issue, but users are surprisingly determined to volunteer secrets in ways that you haven’t prepared for.&lt;/p&gt;
    &lt;p&gt;We’ve touched on a half dozen causes, and the list of things goes on. We didn’t even talk about the wonder that is crashdumps. But, I think it’s important to zoom out and note that these are proximate causes.&lt;/p&gt;
    &lt;p&gt;I stand by my claim that there’s no silver bullet to take these all out. If we want to avoid playing whack-a-mole, we must bring out our lead bullets that address these issues at a deeper level, and prevent these kinds of things from happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixes (lead bullets)&lt;/head&gt;
    &lt;p&gt;So let’s dive in! We will survey 10 fixes, and the order we’ll go in is somewhere between “a dependency graph of things that build on each other” and “following the lifecycle of a secret.” Some of these are obvious or perhaps things you’re already doing, so I’ll focus more on fixes that I think might be a bit newer. That said, it is worth starting with the basics.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;📐 Data architecture&lt;/item&gt;
      &lt;item&gt;🍞 Data transformations&lt;/item&gt;
      &lt;item&gt;🪨 Domain primitives&lt;/item&gt;
      &lt;item&gt;🎁 Read-once objects&lt;/item&gt;
      &lt;item&gt;🗃️ Log formatters&lt;/item&gt;
      &lt;item&gt;🧪 Unit tests&lt;/item&gt;
      &lt;item&gt;🕵️ Sensitive data scanners&lt;/item&gt;
      &lt;item&gt;🤖 Log pre-processors&lt;/item&gt;
      &lt;item&gt;🔎 Taint checking&lt;/item&gt;
      &lt;item&gt;🦸 People&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;📐 Data architecture&lt;/head&gt;
    &lt;p&gt;Lead bullet #1 is the most basic and high-level: data architecture and understanding that this is primarily a data flow problem. And part of the solution is reducing the number of data flows and shrinking the problem space so you simply have less things to worry about and protect.&lt;/p&gt;
    &lt;p&gt;Instead of stray print statements or components that write directly to filesystem, you instead centralize all your data flows through a single stream. Make it so that there’s one and only one way to log something. If you can understand and control the data structures that enter that funnel, you can prohibit secrets from exiting it.&lt;/p&gt;
    &lt;p&gt;This has the allure of being a silver bullet, because of course if you can get to 100% of all the things we mentioned here, you’re golden! But in practice (and as we’ve seen previously), that’s difficult because secrets find a way to sneak in or new outflows and side channels are created.&lt;/p&gt;
    &lt;head rend="h3"&gt;🍞 Data transformations&lt;/head&gt;
    &lt;p&gt;The previous bullet was about controlling how data flows through your system, this is about transforming, slicing, and disarming that data into safer forms that can be logged. These are the data security fundamentals that you’re already familiar with and likely already doing. This is your bread and butter, so I’m not going to dive into every one. From top to bottom, this is generally arranged from awesome to meh&amp;amp;mldr; basically, by how much information is retained.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Transformation&lt;/cell&gt;
        &lt;cell role="head"&gt;Result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Minimization&lt;/cell&gt;
        &lt;cell&gt;☁ (nothing)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Redaction&lt;/cell&gt;
        &lt;cell&gt;[redacted]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Tokenization&lt;/cell&gt;
        &lt;cell&gt;2706a40d-3d1d&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Hashing&lt;/cell&gt;
        &lt;cell&gt;daadfab322b59&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Encryption&lt;/cell&gt;
        &lt;cell&gt;AzKt7vBE7qEuf&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Masking&lt;/cell&gt;
        &lt;cell&gt;··········&lt;code&gt;5309&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;On the top, we have data minimization. The best way to not log secrets, is to not have secrets to begin with! This is everything from going passwordless to fetching only the data you need.&lt;/p&gt;
    &lt;p&gt;Redaction is the next best thing. Blanking out the secret parts and before you pass objects around in memory.&lt;/p&gt;
    &lt;p&gt;Tokenization, hashing, encryption: these all have their pros, cons, and caveats. Like&amp;amp;mldr; are you even doing it correctly?&lt;/p&gt;
    &lt;p&gt;Dead last is masking. You leave parts of the secret intact. Maybe this works for you. Maybe it doesn’t. Maybe you go straight to jail 🤷&lt;/p&gt;
    &lt;p&gt;When these techniques work, they generally work well. But very often what happens is that they aren’t used or are used too late, after something is already logged. These have their places in our toolbox, but my claim again is one bullet isn’t enough.&lt;/p&gt;
    &lt;head rend="h3"&gt;🪨 Domain primitives&lt;/head&gt;
    &lt;p&gt;Let’s introduce lead bullet #3: domain primitives. Almost all the secrets you run across in codebases are encoded in-memory as string primitives, and I think that makes our jobs harder. Strings can be almost anything.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Strings: any sequence of bytes from&lt;/p&gt;&lt;code&gt;""&lt;/code&gt;to&lt;code&gt;"c̴̞̑ť̸͈̘̌ h̸͝ ̭̘̊ü̶̜̫̦̠͋̆͠ ļ̵̮̤̟̉̀͂ṹ̴̝̂🤷867-53-0999"&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;const secret = "..."
&lt;/code&gt;
    &lt;p&gt;There’s very little about them——at compile time or run-time——that lets you know that it’s sensitive, dangerous to log, or somehow different than any other vanilla string.&lt;/p&gt;
    &lt;p&gt;The alternative is a concept I learned from the book Secure by Design, and I think it’s one of the most powerful concepts you can add to your codebase, for logs or anything else where you want to layer in security at a fundamental level.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Domain primitives: “combines secure constructs and value objects to define the smallest building block of a domain”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;const secret = new Secret("...")
&lt;/code&gt;
    &lt;p&gt;You use them as basic building blocks that hold secret values, and they provide security invariants and guarantees that basic string primitives simply cannot.&lt;/p&gt;
    &lt;p&gt;It’s one of the easiest things you can do. If you shift from “any string can be a secret” to “secrets are secrets”, it makes things a lot easier to reason about and protect.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compile-time&lt;/head&gt;
    &lt;p&gt;You can use these to great advantage at compile-time, giving developers immediate feedback right in their editors.&lt;/p&gt;
    &lt;p&gt;We can type a logging function (&lt;code&gt;log()&lt;/code&gt;) so that it never accepts secrets.
Then, we use some fetching function that returns secrets, typed as secrets (and
not as strings). If we try to log that secret, it will not compile. The type
system will not let you log this secret.&lt;/p&gt;
    &lt;code&gt;// Types
declare const brand: unique symbol;
type Secret = string &amp;amp; { [brand]: string }; // Branded type that extends string
type NotSecret&amp;lt;T&amp;gt; = T extends Secret ? never : T; // Type that excludes secrets

// Logging function
function log&amp;lt;T extends string&amp;gt;(message: NotSecret&amp;lt;T&amp;gt;) { ... };
&lt;/code&gt;
    &lt;code&gt;const message: string = "this is fine"; // 🧵 string primitive
const secretz: Secret = getSecret();    // 👈 domain primitive

log(message); // 👌 compiles!
log(secretz); // 💥 error!
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;See this example in the TypeScript Playground.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I’m omitting and glossing over a ton of details here, because I don’t want you to focus on the implementation or even TypeScript, for that matter. The salient point here is that instead of tossing secret strings around, you brand them as secret types, providing useful context to both compiler and developer.&lt;/p&gt;
    &lt;head rend="h4"&gt;Run-time&lt;/head&gt;
    &lt;p&gt;It’s really easy to get started, even with code that is functionally a no-op. This is basically the simplest form I can think of—an almost empty class:&lt;/p&gt;
    &lt;code&gt;class OpenAIToken extends String { /* that could be it! */ }

const token = new OpenAIToken(...);
&lt;/code&gt;
    &lt;p&gt;It’s supposed to represent OpenAI credentials, but it’s just using and extending basic language primitives. You can introduce these objects where secrets originate, like password fields or anytime you decrypt sensitive data fetched from the database. And then layer in behaviors and invariants for where they tend to end up. You progressively start introducing these at both sources and sinks, allowing you to control where secrets should or shouldn’t go. You can embed these into data structures so you know what contains secrets. And along the way, you increase the clarity and safety of your codebase: not only can you prevent these tokens from going into logs, you can make sure you’re sending them only to OpenAI and not to some other API by accident.&lt;/p&gt;
    &lt;p&gt;I think in the long run, domain primitives are the most powerful control we have because it makes our code secure by design, but it does take some time to get there. These can easily address the direct logging cause we discussed earlier, and with some modifications can help with many more.&lt;/p&gt;
    &lt;head rend="h4"&gt;Run-time: part deux&lt;/head&gt;
    &lt;p&gt;We can extend this and make it so that the default serialization behavior is redaction.&lt;/p&gt;
    &lt;code&gt;class Secret extends String {
    toString() { return "[redacted]" } // Override!
}
&lt;/code&gt;
    &lt;code&gt;const secret = new Secret("shhh!");
console.log(secret);
&lt;/code&gt;
    &lt;code&gt;Secret: "[redacted]"
&lt;/code&gt;
    &lt;p&gt;If you try to stuff this into logs, into JSON, into kitchen sinks, into error monitoring, wherever, it’ll always spit out the word “redacted”. You have to intentionally reach for the value.&lt;/p&gt;
    &lt;p&gt;Let’s take it further. We can create a custom class with an explicit &lt;code&gt;unwrap()&lt;/code&gt;
function:&lt;/p&gt;
    &lt;code&gt;class Secret&amp;lt;T&amp;gt; {
    constructor(private readonly value: T) {}
    toString() { return "[redacted]" } // Override serialization
    unwrap() { return this.value }     // Explicit getter function
}
&lt;/code&gt;
    &lt;p&gt;There’s so many things you can do here, like maybe you want to encrypt or zero it out in memory, because that’s in your threat model. You can take this as far as you need to or are comfortable with. We’ll take it just one step further.&lt;/p&gt;
    &lt;head rend="h3"&gt;🎁 Read-once objects&lt;/head&gt;
    &lt;p&gt;There’s a bit to unpack here, but these build off domain primitives in a very powerful way.&lt;/p&gt;
    &lt;code&gt;class Secret&amp;lt;T&amp;gt; {
    private locked = false;
    constructor(private readonly value: T) {}
    toString() { return "[redacted]" }

    /* @returns the sensitive value (once and only once) */
    unwrap() {
        if (this.locked) { throw new Error("already read") }
        this.locked = true;
        return this.value;
    }
}
&lt;/code&gt;
    &lt;p&gt;These objects wrap and keep the secret safe, until you actually need it. The code in the &lt;code&gt;unwrap()&lt;/code&gt; function is the crux: there’s a latch or
lock that activates after the secret is retrieved the first time. It goes into a
“locked” state, and any following reads result in an error that fails loudly.&lt;/p&gt;
    &lt;code&gt;const secret = getSecret();
const res = await authenticate(secret.unwrap()); // Proper usage

Logger.info(secret);          // [redacted]
Logger.info(secret.unwrap()); // 💥 Error!
&lt;/code&gt;
    &lt;p&gt;Once you get a secret (from user input, database, decryption, etc.) you wrap it in a read-once object immediately and keep it wrapped for as long as you can. And for its single, intended purpose, like using it for some kind of API authentication, you unwrap the value, use it, and then the object stays locked for good. This is surprisingly effective at preventing and detecting unintentional use. It addresses and disarms many of the proximate causes that we discussed earlier.&lt;/p&gt;
    &lt;p&gt;This object pairs extremely well with static analysis. Tools like CodeQL or Semgrep can help ensure that developers aren’t bypassing any safety guarantees.&lt;/p&gt;
    &lt;p&gt;These are generally high signal, especially when you have good unit test coverage. One drawback is that read-once objects, if handled incorrectly but not necessarily unsafely, could cause errors at run-time. But I think the tradeoffs are usually worth it, especially if you complement it with testing, static analysis, and taint-checking. Speaking of which&amp;amp;mldr;&lt;/p&gt;
    &lt;head rend="h3"&gt;🔎 Taint checking&lt;/head&gt;
    &lt;p&gt;I like to think of taint checking as static analysis with superpowers. I absolutely love it and the first time I used it, it was like someone just handed me a lightsaber. Quick review for the uninitiated: the general idea here is that you add taint to various sources (like database objects), and yell loudly if the data flows into certain sinks (like logs).&lt;/p&gt;
    &lt;p&gt;The red data flow trace on the right detects the secret flowing into logs. But the green path is fine, because the secret is tokenized. Let’s walk through a quick example: semgrep.dev/playground/s/4bq5L&lt;/p&gt;
    &lt;p&gt;On the left, we’ve marked a couple sources like decrypt and a database fetcher. We’ve also marked our logger as a sink, and the &lt;code&gt;tokenize()&lt;/code&gt; function as a
sanitizer.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On the right in red, we can see that taint was created from the decrypt function, propagated through the &lt;code&gt;getSSN()&lt;/code&gt;function, and then flagged for going into the logs on line 18.&lt;/item&gt;
      &lt;item&gt;In blue, there’s a much shorter path where the user model from the database is tainted and then flagged for going into logs.&lt;/item&gt;
      &lt;item&gt;And then lastly, in green, we’re tokenizing the decrypted SSN, so it’s not flagging that it’s logged.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The idea that this is checking millions or more different data flows is the real magic part for me.&lt;/p&gt;
    &lt;head rend="h4"&gt;Awesome&lt;/head&gt;
    &lt;p&gt;Some of the strengths of taint analysis: obviously automation. Tracing these data flows is 100% a job for a machine. This can really help with domain primitives but also can be used standalone and can even key in on heuristics like variable names: for example, all variables containing “password”. You can tie this into all of your critical tools, from code review to CI/CD.&lt;/p&gt;
    &lt;p&gt;This is especially potent against kitchen sinks and embedded secrets, because those data structures can be tainted by secret values and checked accordingly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Not awesome&lt;/head&gt;
    &lt;p&gt;Some personal opinions on drawbacks: I do feel like taint checking rules tend to be a bit difficult to write. I really, really like Semgrep, but I’m also not the biggest fan of YAML.&lt;/p&gt;
    &lt;p&gt;It also turns out that data flow analysis is an NP-hard problem so for large codebases and monorepos, you likely can’t run full taint analysis on every pull request or commit. Because it runs in CI/CD and as part of change management, when it works, it can prevent the introduction of insecure logging into the codebase.&lt;/p&gt;
    &lt;p&gt;But, like all of the lead bullets we’ve discussed and will discuss, they can miss. How can we handle that?&lt;/p&gt;
    &lt;head rend="h3"&gt;🗃️ Log formatters&lt;/head&gt;
    &lt;p&gt;Let’s say we made the mistake of logging too much data with our email service:&lt;/p&gt;
    &lt;code&gt;{
  tenantId: "52902156-7fb6-4ab0-b659-6b07b80cf89a",
  email: {
    subject: "Log in to your account",
    html: '&amp;lt;a href="https://acme.com/login/98fPm..."&amp;gt;Click here&amp;lt;/a&amp;gt; to log in!',
    from: "AcmeCorp &amp;lt;[email protected]&amp;gt;",
    to: "Darth Plagueis (The Wise) &amp;lt;[email protected]&amp;gt;",
    ...
  },
  response: {
    status: 200,
    originalRequest: {
      headers: {
        Authorization: "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIi..."
      },
      body: '{"html": "&amp;lt;a href=\\"https://acme.com/login/98fP...\\"&amp;gt;Click..."}',
      ...
    }
    ....
  },
  ...
}
&lt;/code&gt;
    &lt;p&gt;We have a couple of our usual suspects here. Because we’re logging email contents, magic links show up in logs&amp;amp;mldr; twice! We’re also logging some kitchen sinks, like email metadata and the original request, so we have PII and authorization headers also in logs. But because this data is structured, if we can traverse these objects, it turns out that we can zero in on these leaks quite effectively.&lt;/p&gt;
    &lt;code&gt;{
  tenantId: "52902156-7fb6-4ab0-b659-6b07b80cf89a",
  email: {
    subject: "Log in to your account",
    html: '&amp;lt;a href="https://acme.com/login/REDACTED"&amp;gt;Click here&amp;lt;/a&amp;gt; to log in!',
    from: "AcmeCorp &amp;lt;[email protected]&amp;gt;",
    to: "REDACTED",
    ...
  },
  response: {
    status: 200,
    originalRequest: {
      headers: "REDACTED",
      body: '{"html": "&amp;lt;a href=\\"https://acme.com/login/REDACTED\\"&amp;gt;..."}',
      ...
    }
    ....
  },
  ...
}
&lt;/code&gt;
    &lt;p&gt;If we can introspect these objects, we can scan for dangerous substrings like our login links, and then drop or redact them. Or we can drop whole values, if we know that certain paths like &lt;code&gt;email.to&lt;/code&gt; are particularly dangerous. Fields like
&lt;code&gt;request&lt;/code&gt; or &lt;code&gt;headers&lt;/code&gt; tend to be risky objects that we can also remove. We can
even drop the whole log object if it doesn’t meet some admission criteria,
or—we can simply error out.&lt;/p&gt;
    &lt;p&gt;So, how and where do we deploy something like this? Most application loggers should have some type of middleware stack or pipeline, kinda like here on the right. These are typically configured for operations like converting objects into JSON, turning error objects into readable formats, or enriching logs by inserting useful context like network information. We can invert that, and instead of enriching with useful data, we can remove or redact sensitive data.&lt;/p&gt;
    &lt;code&gt;export const logger = createLogger({
  format: format.combine(
    transform(),
    handleErrors(),
    enrich(),

      redact(), // 👈 insert here!

    truncate(),
    jsonify(),
    ...
  ),
  ...
});
&lt;/code&gt;
    &lt;p&gt;This is a type of guardrail that helps catch many of the common problems we described previously, like request headers or config objects. I’ve used this with decent success and found that it works best as a rifle instead of a shotgun. Because it’s at the application tier, you can customize it for the type of data or context that each application handles. For example, we can make it so that any of our domain primitives that reach this layer are quickly detected and removed.&lt;/p&gt;
    &lt;p&gt;This is extremely cheap to introduce, but there are some trade-offs. It’s certainly more of a safety net than hard control, and a developer determined to bypass it, can and will. Steady state, I measured this at less than 1% of clock time, but there are some deeply unfortunate ways this can go wrong such as poorly written regexes and self-ReDoS.&lt;/p&gt;
    &lt;p&gt;More or less, these risks can be mitigated with solid unit-testing. Which leads us to&amp;amp;mldr;&lt;/p&gt;
    &lt;head rend="h3"&gt;🧪 Unit tests&lt;/head&gt;
    &lt;p&gt;Lead bullet #7: hooking into and using the existing test suite—that’s already there—to our advantage. We can use several of the tools we discussed, but instead of simply detecting or redacting secrets, we can ramp up the sensitivity in our test environment to fail or error loudly.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Technique&lt;/cell&gt;
        &lt;cell role="head"&gt;Prod&lt;/cell&gt;
        &lt;cell role="head"&gt;Test&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;🪨 Domain primitives&lt;/cell&gt;
        &lt;cell&gt;Redact&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;🎁 Read-once objects&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;🗃️ Log formatters&lt;/cell&gt;
        &lt;cell&gt;Redact&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🕵️ Sensitive data scanners&lt;/cell&gt;
        &lt;cell&gt;Detect&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I’ll cover sensitive data scanners next, but many test suites are already set up to capture &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt;, and so you can even point your scanners to
these capture buffers.&lt;/p&gt;
    &lt;p&gt;The takeaway here is that you can reap the same benefits of CI/CD and change management by catching unsafe code before it’s merged or deployed, but of course, you’re also dependent on coverage and if the right code and data paths are exercised.&lt;/p&gt;
    &lt;head rend="h3"&gt;🕵️ Sensitive data scanners&lt;/head&gt;
    &lt;p&gt;These are fairly blunt but effective tools that can discover and remove sensitive data. I’m actively going to avoid diving deep here, because it does seem like many teams and vendors focus on this as the solution. So instead, I’d like to pose a few questions that might help you reason about trade-offs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where and when in your logging pipeline is it most effective?&lt;/item&gt;
      &lt;item&gt;Is it a gate, in-line of the critical path, or does it scan asynchronously?&lt;/item&gt;
      &lt;item&gt;Do you simply want to detect or do you bias towards masking and redaction? How will your team handle and deal with false positives?&lt;/item&gt;
      &lt;item&gt;How far do the general, out-of-box rules take you? Can you tailor it specifically to your usage patterns?&lt;/item&gt;
      &lt;item&gt;Can you verify the credentials? Can that even keep up with log throughput?&lt;/item&gt;
      &lt;item&gt;And then perhaps what tends to be the long pole in the tent: what are the costs, and can you sample instead?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think these tools tend to be better suited for defense-in-depth, because they presume that secrets made it into logs to begin with. They can help catch the more elusive causes we discussed like configuration changes or user input.&lt;/p&gt;
    &lt;head rend="h4"&gt;Sampling&lt;/head&gt;
    &lt;p&gt;A very brief segue into sampling. Logs tend to have a kind of power law distribution, where certain types of logs vastly outnumber others. And typically what you see is that log sources have static points in code, generally with the same type of data running through them. And so within each log type, scanning and finding a single true positive might be highly representative of that group.&lt;/p&gt;
    &lt;p&gt;And so you might run into a scenario where, given some global sample rate, you’re wasting a lot of work for high frequency logs and not even scanning lower frequency logs. I think a better alternative to a global sample rate is to aggregate logs by some heuristic like type or origin, and to ensure you hit some minimum threshold.&lt;/p&gt;
    &lt;p&gt;In practice, I’ve found this difficult or impossible to configure with out-of-box solutions. I’ve had to introduce additional infrastructure to help. And that’s our next lead bullet.&lt;/p&gt;
    &lt;head rend="h3"&gt;🤖 Log pre-processors&lt;/head&gt;
    &lt;p&gt;Second to last lead bullet, #9: log pre-processors. These sit between apps that emit logs, and the final data stores.&lt;/p&gt;
    &lt;p&gt;In the above example, something like Vector can receive and process logs from our microservices before dispatching them to DataDog or wherever logs end up. We can configure it to drop sensitive data in-place using many of the techniques we discussed before. And we can sample some subset of them and store them onto an S3 bucket, using a more powerful tool like Trufflehog or an LLM to catch and verify secrets.&lt;/p&gt;
    &lt;p&gt;The idea here is to process logs streams before they’re persisted. It doesn’t need to be Vector, chances are, you already have this existing infrastructure that’s used for deduping, aggregation, and dropping noisy debug logs. We can re-use it to prevent and detect secrets in logs. This pairs very well with sensitive data scanners that we discussed earlier, and might even unlock new ones you thought were out of reach.&lt;/p&gt;
    &lt;head rend="h3"&gt;🦸 People&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;“Human practitioners are the adaptable element of complex systems. Practitioners and first line management actively adapt the system to maximize production and minimize accidents.”&lt;/p&gt;
      &lt;p&gt;-Richard Cook, https://how.complexsystems.fail/#12&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Our last stop is people. Modern software is a complex system. And while people will write the code that accidentally introduces sensitive data into logs, they’re also the ones that will report, respond, and fix them. They’ll build out the systems and infrastructure that will keep these complex systems safe. And early on in your maturity story and before you’re able to build out secure-by-design frameworks, this is the lead bullet you’ll most likely use the most.&lt;/p&gt;
    &lt;p&gt;The most important message I want to convey here is that your security team isn’t alone, especially if you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;educate your teammates on secure logging design&lt;/item&gt;
      &lt;item&gt;empower them to report and address these issues&lt;/item&gt;
      &lt;item&gt;and equip them with tools that get out of their way and helps them succeed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Recap&lt;/head&gt;
    &lt;p&gt;Alright, so we’ve covered lead bullets that protect code, protect data, and protect logs:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;📐 Data architecture&lt;/item&gt;
      &lt;item&gt;🍞 Data transformations&lt;/item&gt;
      &lt;item&gt;🪨 Domain primitives&lt;/item&gt;
      &lt;item&gt;🎁 Read-once objects&lt;/item&gt;
      &lt;item&gt;🗃️ Log formatters&lt;/item&gt;
      &lt;item&gt;🧪 Unit tests&lt;/item&gt;
      &lt;item&gt;🕵️ Sensitive data scanners&lt;/item&gt;
      &lt;item&gt;🤖 Log pre-processors&lt;/item&gt;
      &lt;item&gt;🔎 Taint checking&lt;/item&gt;
      &lt;item&gt;🦸 People&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of these might work for you, some of these won’t, and some that we haven’t even mentioned could be a homerun for you. Maybe you have super tight control over your log schemas or maybe you’re using LLMs in a really neat and effective way. Or maybe you’re building or using a language that has first class support for controlling secrets.&lt;/p&gt;
    &lt;p&gt;These worked for me. I have some personal opinions on ones which are foundational, some that are powerful in the long-run, and some that are really easy to get started. But your story is different, so I’d like to zoom out and close out with a high-level, methodical strategy that you can apply for your security programs, and that we’ll apply and walk through with an example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Strategy&lt;/head&gt;
    &lt;p&gt;Here’s a general strategy:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lay the foundation&lt;/item&gt;
      &lt;item&gt;Understand the data ﬂow&lt;/item&gt;
      &lt;item&gt;Protect at chokepoints&lt;/item&gt;
      &lt;item&gt;Apply defense-in-depth&lt;/item&gt;
      &lt;item&gt;Plan for response and recovery&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m not shooting for a Nobel, here. You’re probably doing some of these already, and chances are, you have some type of playbook or process that looks just like this. The key idea here is to not miss the forest for the trees, and use these explicit steps to place our efforts where they’ll matter most. I’ll walk you through a hypothetical system and we’ll apply these in order.&lt;/p&gt;
    &lt;head rend="h3"&gt;0. Lay the foundation&lt;/head&gt;
    &lt;p&gt;Step zero is the foundation. Table stakes. This is like the base tier of Maslow’s hierarchy, and we need these before we try anything else.&lt;/p&gt;
    &lt;p&gt;Developing expectations, culture, and support is a must-have. They’re easy to ignore or forget about, but can make or break success. If you work at place that hasn’t addressed these in the past, it can be quite jarring or difficult to shift that mentality.&lt;/p&gt;
    &lt;p&gt;I don’t have a ton of advice here other than making sure your org is aligned on this. It’ll probably feel like it’s getting worse before it’s getting better, but that is a sign of progress. A great litmus test for a solid foundation is if developers will (or already have) come to you to report secrets they found in logs.&lt;/p&gt;
    &lt;p&gt;The second thing we’ll need is to decide is what we consider a secret to begin with. I, admittedly, used secrets and sensitive data interchangeably. This may not be the case for you. It doesn’t need to be perfect or comprehensive, and maybe it’s just a framework. But employees, especially the security team, need common understanding.&lt;/p&gt;
    &lt;p&gt;The third item is technical. If our logs aren’t structured or aren’t JSON, it’ll make this endeavor a lot more difficult. A lot of the techniques we discussed just won’t work. If we don’t have that central pipeline or there isn’t One and Only One Way to both dispatch and view logs, we’ll have to do a lot more lifting. We’ve seen a few ways that logs bypass this, but having a central pipeline should cover most of the bases.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Understand the data flow&lt;/head&gt;
    &lt;p&gt;With the foundation laid, the next best thing to do is to understand and chart out how secrets flow through your system. This is basically a Data Flow Diagram, and we’ll go through a fairly modest example.&lt;/p&gt;
    &lt;p&gt;On the left, we have users that visit some type of single-page web app. Requests and data flow through an application load balancer to several web application services running in containers. This is our core compute and where all the application code runs. Let’s assume that these are disparate microservices processing all types of data, some of which are considered secret. For the most sensitive data, they use KMS to encrypt and then store the ciphertext blobs in their respective database.&lt;/p&gt;
    &lt;p&gt;And then, applications use a standard logging library to emit to stdout, which gets shipped to CloudWatch and then forwarded to Datadog. That’s the final stop, and that’s where employees, devs, support staff, etc. can view them.&lt;/p&gt;
    &lt;p&gt;I highly recommend going through an exercise like this, because not only does it force you to understand the flows and boundaries of the system, if you spend time at each node and threat model it, you end up finding a bunch of unexpected ways and places that secrets make it into logs. For example&amp;amp;mldr;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Front-end analytics! It turns out that secrets from things like form contents to session replays could end up getting sent to your user analytics platform.&lt;/item&gt;
      &lt;item&gt;And then what about our application load balancers? These ship their HTTP logs directly to CloudWatch, so we could be logging embedded secrets in URLs, and it’s totally bypassing our application tiers.&lt;/item&gt;
      &lt;item&gt;Last surprise: error monitoring! Let’s just say that some team wired up Sentry instead of DataDog for error monitoring, because of course they did, and now you have another stream of secrets in logs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We could go further, and we haven’t even drilled into application architecture, but I think this is a good time to move from discovery to action.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Protect at chokepoints&lt;/head&gt;
    &lt;p&gt;The next step we want to take is to protect the chokepoints. And if some flow isn’t going through that chokepoint, like our rogue team that yeeted Sentry into prod, we fix it! We can get rid of Sentry and get that team onto the paved path of our logging pipeline.&lt;/p&gt;
    &lt;p&gt;We have a very clear chokepoint; a narrow path that most logs eventually flow through. Here’s where most of our lead bullets should go.&lt;/p&gt;
    &lt;p&gt;Here’s that chokepoint splayed out. I also added an upstream node to represent CI/CD, because that’s how code get into our apps. We can then put the bulk of our protective controls here on the critical path.&lt;/p&gt;
    &lt;p&gt;We can re-architect the app to use a single logging library and secure-by-default domain primitives. Then we could use those to build out and augment our static analysis, taint-checking, and unit tests. These give us a decent front-line defense for our logging pipeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Apply defense-in-depth&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;“Every preventative control should have a detective control at the same level and/or one level downstream in the architecture.” -Phil Venables, https://www.philvenables.com/post/defense-in-depth&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The third step is about adding depth to that defense, a concept we’re all familiar with. I really like how Phil Venables crystallizes what defense-in-depth means and I think he generally gives great advice. The idea is that our controls are not simply overlapping, but mutually supportive. Something’s always got your back.&lt;/p&gt;
    &lt;p&gt;Along this chokepoint we add our downstream components, in depth. Some are preventative, while some are detective.&lt;/p&gt;
    &lt;p&gt;We can add additional protections like tokenization and read-once objects. We can add the downstream tools like our custom log formatters, and employ various sensitive data scanners at different points. And then finally, we can educate and equip our team.&lt;/p&gt;
    &lt;p&gt;This is what defense-in-depth looks like to me, and I think this maximizes chances of success.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Plan for response and recovery&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determine the scope&lt;/item&gt;
      &lt;item&gt;Restrict access&lt;/item&gt;
      &lt;item&gt;Stop the bleeding / ﬁx the source&lt;/item&gt;
      &lt;item&gt;Clean up all the places, e.g. indexes&lt;/item&gt;
      &lt;item&gt;Restore access&lt;/item&gt;
      &lt;item&gt;Do a post-mortem&lt;/item&gt;
      &lt;item&gt;Make it ~impossible to happen again&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But, of course, if we do miss or if we manage to only detect vs. prevent, we should be prepared for response and recovery. You already know how to respond to incidents like this, so I won’t add much here, other than making sure you’re sticking to a playbook in the right order, pulling levers to restrict and restore access while you’re responding, as well as thinking about all the weird places secrets might persist in logs, like indexes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;And that’s it. This is the culmination of our strategy, our work, and about 30 some minutes of blabber.&lt;/p&gt;
    &lt;p&gt;With a solid foundation and understanding of our data flows, we protected our chokepoints in-depth and kept secrets out of logs. We’ve also introduced a lot of other strong primitives that materially improve our security program. So is that it? Is the job done?&lt;/p&gt;
    &lt;p&gt;Well, no, because the data team wired up some ETL jobs that are now spewing secrets into data lake logs, because of course they did.&lt;/p&gt;
    &lt;p&gt;Like most things in security, the job often isn’t ever done. But we have the understanding, the tools, and a strategy to fight the next fight. Keeping secrets out of logs is in your hands.&lt;/p&gt;
    &lt;p&gt;*me&lt;/p&gt;
    &lt;p&gt;If you liked what you heard, or if you hated it, I’d love to hear your story. Please, reach out! Thanks! ✌️&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://allan.reyes.sh/posts/keeping-secrets-out-of-logs/"/></entry><entry><id>https://news.ycombinator.com/item?id=45160780</id><title>Submarine Cable Map</title><updated>2025-09-07T23:08:15.157732+00:00</updated><link href="https://www.submarinecablemap.com/"/></entry><entry><id>https://news.ycombinator.com/item?id=45161229</id><title>How to make metals from Martian dirt</title><updated>2025-09-07T23:08:13.002391+00:00</updated><content>&lt;doc fingerprint="270d494f78a464bb"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Key points&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swinburne and CSIRO researchers are investigating ways to produce native metals using materials found on Mars.&lt;/item&gt;
      &lt;item&gt;Martian settlements will require large amounts of metal that are difficult to ship from Earth.&lt;/item&gt;
      &lt;item&gt;The team have successfully produced iron using regolith simulant that mimics what is available on the Red Planet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The idea of building settlements on Mars is a popular goal of billionaires, space agencies and interplanetary enthusiasts.&lt;/p&gt;
    &lt;p&gt;But construction demands materials, and we can't ship it all from Earth: it cost US$243 million just to send NASA's one tonne Perseverance Rover to the Red Planet.&lt;/p&gt;
    &lt;p&gt;Unless we're building a settlement for ants, we'll need much, much more stuff. So how do we get it there?&lt;/p&gt;
    &lt;p&gt;CSIRO Postdoctoral Fellow and Swinburne alum Dr Deddy Nababan has been pondering this question for years. His answer lies in the Martian dirt, known as regolith.&lt;/p&gt;
    &lt;p&gt;"Sending metals to Mars from Earth might be feasible, but it's not economical. Can you imagine bringing tonnes of metals to Mars? It's just not practical," Dr Nababan says.&lt;/p&gt;
    &lt;p&gt;"Instead, we can use what's available on Mars. It's called in-situ resource utilisation, or ISRU."&lt;lb/&gt; More specifically, Dr Nababan is looking at astrometallurgy — making metals in space.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building an off-world foundry&lt;/head&gt;
    &lt;p&gt;As it turns out, Mars has all the ingredients needed to make native metals. This includes iron-rich oxides in regolith and carbon from its thin atmosphere, which acts as a reducing agent.&lt;/p&gt;
    &lt;p&gt;Swinburne University of Technology astrometallurgist, Professor Akbar Rhamdhani, is working with Dr Nababan to test this process with regolith simulant - an artificial recreation of the stuff found of Mars.&lt;/p&gt;
    &lt;p&gt;"We picked a simulant with very similar properties to that found at Gale Crater on Mars and processed them on Earth with simulated Mars conditions. This gives us a good idea of how the process would perform off-world," he says.&lt;/p&gt;
    &lt;p&gt;The simulant is placed inside a chamber at Mars surface pressure and heated at increasing temperatures. The experiments showed pure iron metal formation around 1000°C, with liquid silicon-iron alloys produced around 1400°C.&lt;/p&gt;
    &lt;p&gt;"At high enough temperatures, all of the metals coalesced into one large droplet. This could then be separated from liquid slag the same way it is on Earth," Professor Rhamdhani says.&lt;/p&gt;
    &lt;p&gt;Along with Dr Nababan, Prof Rhamdhani is collaborating with CSIRO's Dr Mark Pownceby to further advance the process. They're particularly focused on making metals with zero waste, where the byproducts of the process are used to make useful items.&lt;/p&gt;
    &lt;head rend="h2"&gt;If you can't ship it, make it&lt;/head&gt;
    &lt;p&gt;ISRU is a growing area of space science because in rocket launches, every kilogram counts. While the cost of launches is going down, the demands of human exploration are immense.&lt;/p&gt;
    &lt;p&gt;But huge developments are already happening, including the first demonstration of ISRU off-world. The MOXIE experiment on board the Mars Perseverance rover produced breathable oxygen using only the carbon dioxide in the planet's atmosphere.&lt;/p&gt;
    &lt;p&gt;Metal production is the next giant leap. Prof Rhamdhani hopes Mars-made alloys could be used as shells for housing or research facilities, and in machinery for excavation.&lt;/p&gt;
    &lt;p&gt;"There are certainly challenges. We need to better understand how these alloys would perform over time, and of course whether this process can be recreated on the real Martian surface," Prof Rhamdhani says.&lt;/p&gt;
    &lt;p&gt;But in the meantime, Swinburne and its partners are doubling down. Prof Rhamdhani together with Dr Nababan and Dr Matt Shaw, another CSIRO researcher and Swinburne alum, recently delivered a 4-day bespoke workshop on astrometallurgy in South Korea. The feedback was promising.&lt;/p&gt;
    &lt;p&gt;"We're starting to see increased interest in this field globally as the world gets serious about Mars exploration," he says.&lt;/p&gt;
    &lt;p&gt;"To make it happen, we're going to need experts from many fields — mining, engineering, geology, and much more."&lt;/p&gt;
    &lt;p&gt;For Dr Nababan, the benefits go beyond exploration. He hopes their research will also drive more efficient metallurgy here on Earth.&lt;/p&gt;
    &lt;p&gt;"By doing this, I wish that I can help the development of space exploration, and at the end it will bring good to human life here on Earth."&lt;/p&gt;
    &lt;p&gt;This was article is published from Swinburne University. Read the original article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.csiro.au/en/news/All/Articles/2025/August/Metals-out-of-martian-dirt"/></entry><entry><id>https://news.ycombinator.com/item?id=45161556</id><title>No Silver Bullet: Essence and Accidents of Software Engineering (1986) [pdf]</title><updated>2025-09-07T23:08:12.200240+00:00</updated><content/><link href="https://www.cs.unc.edu/techreports/86-020.pdf"/></entry><entry><id>https://news.ycombinator.com/item?id=45161816</id><title>Everything from 1991 Radio Shack ad I now do with my phone (2014)</title><updated>2025-09-07T23:08:11.417234+00:00</updated><content>&lt;doc fingerprint="7421b90d5be73a32"&gt;
  &lt;main&gt;
    &lt;p&gt;Some people like to spend $3 on a cup of coffee. While that sounds like a gamble I probably wouldn’t take, I’ll always like to gamble– especially as little as three bucks– on what I might be able to dig up on Buffalo and Western New York, our collective past, and what it means for our future.&lt;/p&gt;
    &lt;p&gt;I recently came across a big pile of Buffalo News front sections from 1991, every day for the first three months of the year… collected as the First Gulf War unfolded. $3. I probably could have chiseled the guy down a buck, but I happily paid to see what else was in those papers.&lt;/p&gt;
    &lt;p&gt;There’s plenty about a run up to the first Superbowl appearance ever for the Bills, and mixed in with the disappointment is an air of hope and expectation for what is to come. Harumph. There are also some great local ads commemorating and/or coat-tailing on the Bills success.&lt;/p&gt;
    &lt;p&gt;We’ll get to those someday, but today, something much simpler. The back page of the front section on Saturday, February 16, 1991 was 4/5ths covered with a Radio Shack ad.&lt;/p&gt;
    &lt;p&gt;There are 15 electronic gimzo type items on this page, being sold from America’s Technology Store. 13 of the 15 you now always have in your pocket.&lt;/p&gt;
    &lt;p&gt;So here’s the list of what I’ve replaced with my iPhone.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All weather personal stereo, $11.88. I now use my iPhone with an Otter Box&lt;/item&gt;
      &lt;item&gt;AM/FM clock radio, $13.88. iPhone.&lt;/item&gt;
      &lt;item&gt;In-Ear Stereo Phones, $7.88. Came with iPhone.&lt;/item&gt;
      &lt;item&gt;Microthin calculator, $4.88. Swipe up on iPhone.&lt;/item&gt;
      &lt;item&gt;Tandy 1000 TL/3, $1599. I actually owned a Tandy 1000, and I used it for games and word processing. I now do most of both of those things on my phone.&lt;/item&gt;
      &lt;item&gt;VHS Camcorder, $799. iPhone.&lt;/item&gt;
      &lt;item&gt;Mobile Cellular Telephone, $199. Obvs.&lt;/item&gt;
      &lt;item&gt;Mobile CB, $49.95. Ad says “You’ll never drive ‘alone’ again!” iPhone.&lt;/item&gt;
      &lt;item&gt;20-Memory Speed-Dial phone, $29.95.&lt;/item&gt;
      &lt;item&gt;Deluxe Portable CD Player, $159.95. 80 minutes of music, or 80 hours of music? iPhone.&lt;/item&gt;
      &lt;item&gt;10-Channel Desktop Scanner, $99.55. I still have a scanner, but I have a scanner app, too. iPhone.&lt;/item&gt;
      &lt;item&gt;Easiest-to-Use Phone Answerer, $49.95. iPhone voicemail.&lt;/item&gt;
      &lt;item&gt;Handheld Cassette Tape Recorder, $29.95. I use the Voice Memo app almost daily.&lt;/item&gt;
      &lt;item&gt;BONUS REPLACEMENT: It’s not an item for sale, but at the bottom of the ad, you’re instructed to ‘check your phone book for the Radio Shack Store nearest you.’ Do you even know how to use a phone book?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You’d have spent $3054.82 in 1991 to buy all the stuff in this ad that you can now do with your phone. That amount is roughly equivalent to about $5100 in 2012 dollars.&lt;/p&gt;
    &lt;p&gt;The only two items on the page that my phone really can’t replace:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tiny Dual-Superhet Radar Detector, $79.95. But when is the last time you heard the term “fuzzbuster” anyway?&lt;/item&gt;
      &lt;item&gt;3-Way speaker with massive 15″ Woofer, $149.95.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s nothing new, but it’s a great example of the technology of only two decades ago now replaced by the 3.95 ounce bundle of plastic, glass, and processors in our pockets.&lt;/p&gt;
    &lt;p&gt;Buffalo story teller and Historian Steve Cichon brings us along as he explores the nooks and crannies of Buffalo’s past present and future, which can mean just about anything– twice a week on Trending Buffalo.&lt;/p&gt;
    &lt;p&gt;As he collects WNY’s pop culture history, Steve looks for Buffalo’s good stories and creative ways to tell them as the President and founder of Buffalo Stories LLC. He’d love to help your business tell its story. For a decade, he’s also collected and shared Buffalo’s pop culture history at staffannouncer.com. His latest book, Gimme Jimmy! The James D. Griffin Story, is available now at www.mayorgriffin.com.&lt;/p&gt;
    &lt;p&gt;steve@buffalostories.com | @SteveBuffalo | www.facebook.com/stevecichon&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.trendingbuffalo.com/life/uncle-steves-buffalo/everything-from-1991-radio-shack-ad-now/"/></entry><entry><id>https://news.ycombinator.com/item?id=45161855</id><title>Pico CSS – Minimal CSS Framework for Semantic HTML</title><updated>2025-09-07T23:08:11.134838+00:00</updated><content>&lt;doc fingerprint="be2be4ff293b3380"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Class-light and Semantic&lt;/head&gt;
    &lt;p&gt;Thriving on simplicity, Pico directly styles your HTML tags, using fewer than 10 &lt;code&gt;.classes&lt;/code&gt; overall. It also comes with a class-less version for wild HTMLÂ purists.&lt;/p&gt;
    &lt;p&gt;A minimalist and lightweight starterÂ kit that prioritizes semanticÂ syntax, making every HTMLÂ element responsive and elegantÂ byÂ default.&lt;/p&gt;
    &lt;p&gt;WriteÂ HTML, AddÂ PicoÂ CSS, andÂ VoilÃ !&lt;/p&gt;
    &lt;p&gt;14.8K&lt;/p&gt;
    &lt;p&gt;65.8K&lt;/p&gt;
    &lt;p&gt;(Last month)&lt;/p&gt;
    &lt;p&gt;12.6M&lt;/p&gt;
    &lt;p&gt;(Last month)&lt;/p&gt;
    &lt;p&gt;With just the rightÂ amount ofÂ everything, Pico is a great startingÂ point for aÂ clean andÂ lightweight designÂ system.&lt;/p&gt;
    &lt;p&gt;Thriving on simplicity, Pico directly styles your HTML tags, using fewer than 10 &lt;code&gt;.classes&lt;/code&gt; overall. It also comes with a class-less version for wild HTMLÂ purists.&lt;/p&gt;
    &lt;p&gt;No extra baggage needed. Pico works seamlessly without dependencies, packageÂ managers, external files, or JavaScript, achieving elegant and straightforward styles with pure HTMLÂ markup.&lt;/p&gt;
    &lt;p&gt;Effortless elegance on everyÂ device. Pico natively scales font sizes and spacings with screen widths, resulting in a consistent and elegant look across devices. No extra classes or configuration needed.&lt;/p&gt;
    &lt;p&gt;Pico comes with two accessible, neutral color schemes out of the box: light and dark. The best part? It automatically adapts to users' &lt;code&gt;prefers-color-scheme&lt;/code&gt;, all without the use of JavaScript.&lt;/p&gt;
    &lt;p&gt;Customize Pico with over 130 CSSÂ variables, or dive deeper by using SASS. Switch between 20 handcrafted colorÂ themes and compose with 30+ modularÂ components to tailor the UI to your brand's lookÂ andÂ feel.&lt;/p&gt;
    &lt;p&gt;Speed meets elegance. Unlike bulky and overcomplicated frameworks that demand extensiveÂ class overrides and JavaScript, Pico keeps your HTML lean, decreases memory usage by avoiding excessive CSS specificity, and reduces loaded files.&lt;/p&gt;
    &lt;p&gt;A strong design foundation thrives on simplicity and ease of maintenance.&lt;/p&gt;
    &lt;p&gt;ð Pico CSS&lt;/p&gt;
    &lt;code&gt;&amp;lt;form&amp;gt;
  &amp;lt;input type="text"&amp;gt;
  &amp;lt;button type="submit"&amp;gt;Action&amp;lt;/button&amp;gt;
&amp;lt;/form&amp;gt;&lt;/code&gt;
    &lt;p&gt;ð¥µ Utility CSS Framework&lt;/p&gt;
    &lt;code&gt;&amp;lt;div class="container display-flex my-md mx-sm"&amp;gt;
  &amp;lt;form class="form shadow-md my-md mx-sm align-center"&amp;gt;
    &amp;lt;div class="input-wrapper border-radius-sm"&amp;gt;
      &amp;lt;input type="text" class="input text-color-gray placeholder-color-light-gray focus-outline-blue"&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;div class="button-wrapper border-radius-sm"&amp;gt;
      &amp;lt;button type="submit" class="button bg-color-blue text-color-white focus-light-blue hover-light-blue"&amp;gt;
        Action
      &amp;lt;/button&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/form&amp;gt;
&amp;lt;/div&amp;gt;&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://picocss.com"/></entry><entry><id>https://news.ycombinator.com/item?id=45162220</id><title>Taco Bell AI Drive-Thru</title><updated>2025-09-07T23:08:10.960070+00:00</updated><content>&lt;doc fingerprint="be03308f8313374f"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Taco Bell AI Drive-Thru - “Hold the AI, Extra Chaos”&lt;/head&gt;
    &lt;p&gt;Nominee: Taco Bell Corporation for deploying voice AI ordering systems at 500+ drive-throughs and discovering that artificial intelligence meets its match at “extra sauce, no cilantro, and make it weird.”&lt;/p&gt;
    &lt;p&gt;Reported by: Isabelle Bousquette, Technology Reporter for The Wall Street Journal - August 28, 2025.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Innovation&lt;/head&gt;
    &lt;p&gt;Taco Bell boldly deployed voice AI-powered ordering systems across more than 500 drive-through locations, convinced that artificial intelligence could finally solve humanity's greatest challenge: efficiently ordering tacos. The company's confidence was so spectacular that they rolled out the technology at massive scale, apparently believing that voice AI had conquered human speech patterns, regional accents, and the creative chaos that occurs when hungry humans interact with fast food menus.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Reality Check&lt;/head&gt;
    &lt;p&gt;The Wall Street Journal revealed that customers were not quite as enthusiastic about their robotic taco consultant as Taco Bell had hoped. The AI systems faced a perfect storm of customer complaints, system glitches, and what might charitably be described as “creative user interaction”—including customers deliberately trolling the AI with absurd orders that would make even experienced drive-thru workers question their life choices.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Strategic Reassessment&lt;/head&gt;
    &lt;p&gt;Faced with mounting evidence that artificial intelligence and natural stupidity don't mix well at the drive-thru window, Taco Bell began “reassessing” their AI deployment. The company announced they were evaluating where AI is most effective and considering human intervention during peak periods—corporate speak for “our robots can't handle the breakfast rush and we're not sure why we thought they could.”&lt;/p&gt;
    &lt;head rend="h3"&gt;The Perfect Storm&lt;/head&gt;
    &lt;p&gt;This incident represents the collision of three unstoppable forces: corporate AI evangelism, the infinite creativity of hungry customers, and the fundamental reality that ordering food involves more chaos variables than training a large language model to play chess. Customers reported “glitches and delays”, while others were “intent on trolling the [AI] system” with absurd orders, proving that humans can out-weird artificial intelligence even when they're just trying to get a burrito.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why They're Nominated&lt;/head&gt;
    &lt;p&gt;Taco Bell achieved the perfect AI Darwin Award trifecta: spectacular overconfidence in AI capabilities, deployment at massive scale without adequate testing, and a public admission that their cutting-edge technology was defeated by the simple human desire to customise taco orders. When The Wall Street Journal reports that “the most transformative technology in over a century may have finally found its limit: ordering tacos”, you've achieved a special kind of technological hubris that deserves recognition. Even more remarkably, despite this spectacular AI fail, Taco Bell is reportedly still moving forward with voice AI, which they say remains a critical part of the product road map—proving that true AI confidence means never letting reality interfere with your technological roadmap.&lt;/p&gt;
    &lt;p&gt;Sources: The Wall Street Journal: Taco Bell Rethinks Future of Voice AI at the Drive-Through&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aidarwinawards.org/nominees/taco-bell-ai-drive-thru.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45162368</id><title>Show HN: OpenCV over WebRTC (in Go)</title><updated>2025-09-07T23:08:10.688697+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/pion/example-webrtc-applications/blob/master/gocv-to-webrtc/README.md"/></entry><entry><id>https://news.ycombinator.com/item?id=45162501</id><title>Creative Technology: The Sound Blaster</title><updated>2025-09-07T23:08:09.953385+00:00</updated><content>&lt;doc fingerprint="a50e953b59ba8d0d"&gt;
  &lt;main&gt;
    &lt;p&gt;Sim Wong Hoo was born on the 28th of April in 1955, the tenth child in a family of twelve children (five brothers, seven sisters). His family were Singaporean Hoklo with ancestry in the southernmost area of Fujian, China, and they spoke Hokkien. He grew up in a kampung called End of Coconut Hill in Bukit Panjang, and his father, Sim Chye Thiam, was a factory worker while his mother, Tan Siok Kee, raised chickens, ducks, pigs, and rabbits, and grew fruits and herbs. The young Sim had chores around the house and around the farm as soon as he was physically able, and he often sold eggs at the local market before school classes started each day. This afforded him the ability to buy things for himself such as his harmonica when he was about 11. The harmonica was a hobby he greatly enjoyed throughout his life. He also enjoyed making his own games.&lt;/p&gt;
    &lt;p&gt;Sim graduated from Bukit Panjang Government High School and then went on to attend Ngee Ann Technical College for engineering. At the college, Sim was a member of both the harmonica troupe, consisting of thirty people, and the Practice Theatre School. In the theatre, Sim provided musical accompaniment for the school’s performances with the harmonica and the accordion, often performing his own arrangements. His two interests collided at this time in his life. When writing or arranging music, he’d only be able to hear his composition during weekly practice. Having seen a computer, he realized that a computer could allow him to hear the music precisely as written while still working on it. Sim envisioned a computer that could play music, talk, or even sing, and his earlier entrepreneurial spirit drove him to an ambitious goal: selling 100 million units of a single piece of equipment. Sim graduated in 1975 and then entered the uniformed services for his obligatory two years.&lt;/p&gt;
    &lt;p&gt;For three to four years following his service, Sim worked a brief stint on an offshore oil rig, designing computerized seismic data logging equipment. After that, he opened a computer education center at Coronation Plaza. As he was more interested in teaching and researching, he left the business work to his business partner. This wasn’t a great decision. His partner took off with all the money.&lt;/p&gt;
    &lt;p&gt;On the 1st of July in 1981, Sim founded Creative Technology with Ng Kai Wa, who had been his childhood friend and classmate in a 440 sqft shop at Pearls Center using his own savings of around $6000. The company initially did computer repair and sold parts and accessories for microcomputers. Business wasn’t great, so Sim also did some teaching. In whatever time he had left to him, he was busy developing his own products.&lt;/p&gt;
    &lt;p&gt;The first Creative product (at least, for which I can find any evidence at all) was a memory board for the Apple II. Having an understanding of the Apple II, Creative followed their memory board by producing the CUBIC 99 in 1984. This was an Apple II compatible machine with a 6502, but it also featured a Zilog Z80 for compatibility with CP/M. I am not certain how this was arranged, but I imagine that it wasn’t entirely dissimilar to the Microsoft Z80 SoftCard. Of course, this is Creative Technology, so the machine also featured a voice synthesizer allowing users to record and playback words in English or Chinese. The computer also had an optional Cubic Phone Sitter which could make and answer calls. This was the first computer to be designed and manufactured in Singapore.&lt;/p&gt;
    &lt;p&gt;The market was moving quickly, and the IBM PC had created a standard. The CUBIC CT was released in 1986 as a PC compatible, and it featured graphics and sound capabilities. This was, essentially, a multimedia PC (with a weaker CPU than that standard would later dictate) localized in the Chinese language. Unfortunately, it was too early. With nearly zero software support for anything approaching the capabilities of the CT and an even smaller local market, the product was a failure.&lt;/p&gt;
    &lt;p&gt;Realizing that the sound features of the CUBIC CT were likely more salable and supportable than the computer itself, Sim and his company chose to sell the sound card by itself as the Creative Music System (also C/MS or CT-1300). This board was built around two Philips SAA1099 chips providing 12 channels of square-wave stereo sound on a half-length 8bit ISA card, and it shipped with five 360K 5.25 inch floppy disks (Master Disk, Intelligent Organ, Sound Disk 1, Sound Disk 2, Utilities). To promote this card, Sim moved to California in 1988 and established Creative Labs. His goal was to sell at least 20,000 cards generating $1 million in revenue. The USA was the largest PC market, and he knew that sound cards were seeing good sales.&lt;/p&gt;
    &lt;p&gt;Being in the USA, Sim quickly realized that games were the software titles driving sound card sales, and this meant that he’d need new branding and software partners. The C/MS became the Game Blaster, and the included software was now just the Intelligent Organ, a test utility, a TSR, and drivers for Sierra Online games. The inclusion of those drivers was key to what would follow. Creative’s partnership with Sierra meant that some of the most popular games of the era would support the Game Blaster; ultimately, over 100 games would support the C/MS and Game Blaster. Naturally, selling a card required a store front, and Creative found a partner in Radio Shack. While the Game Blaster sold better than any Creative product before it, it didn’t overtake the Adlib.&lt;/p&gt;
    &lt;p&gt;To better compete, Creative needing something that was better than the Adlib but still compatible with it. This came in 1989 with the CT1310, better known as the Sound Blaster. The Sound Blaster offered 12-voice C/MS stereo sound, 11-voice FM synthesis with Adlib compatibility (via the Yamaha YM3812), a MIDI interface, a joystick port, microphone jack with a built-in amplifier, a stereo amplifier with volume dial, the ability to play back mono-sampled sound at up to 22kHz, and record at 12kHz. While a sample rate of 22k doesn’t seem great (because it isn’t) this did allow simultaneous output of sound effects and music in a game. Likewise, while a game port doesn’t seem like all too big a deal, it saved the buyer an extra $50 to buy one separately, and it saved an ISA slot too. The Sound Blaster was the first sound card to feature digital sample playback, and it took over the market, quickly becoming the top-selling expansion card of any kind in under a year, and Creative’s revenues hit $5.5 million. With the C/MS never having been too popular, Creative followed the CT1310 with the CT1320 which removed the C/MS chips but kept sockets for them on the card.&lt;/p&gt;
    &lt;p&gt;1989 also saw Creative release the PJS operating system and the PJ Views word processor and desktop publishing system which included support for 70,000 Chinese characters. As far as I know, these products were only released in Southeast Asia.&lt;/p&gt;
    &lt;p&gt;Announced in May of 1991, the Sound Blaster Pro, CT1330, was a major redesign of Creative’s sound card. This card used two Yamaha YM3812 chips to provide stereo sound while maintaining full backward compatibility with the original Sound Blaster and Adlib. Sample rates were increased to 22kHz for stereo, 44.1kHz for mono. A simple mixer, low-pass filter, high-pass filter, and CD-ROM interface were added. This CD-ROM interface could take multiple forms, but it was useful in pushing CD-ROMs into the mainstream. Many early CD-ROM drives were SCSI-only and that was expensive. Creative worked with MKE in Japan to produce low-cost IDE CD-ROM drives, and then included support on their cards. As for the card itself, while the card did have the AT connector, it wasn’t 16bit. The Pro was still an 8bit card. The presence of the 16bit AT connector was for additional interrupts and DMAs on the 16bit bus that supported the Multimedia PC standard from Microsoft. The Sound Blaster Pro 2 was released shortly after the original, and it replaced the YM3812s with a single YMF262. The Pro series was often sold in Multimedia Upgrade Kits where it was bundled with a CD-ROM drive and software titles. Given that CD-ROMs were quite new, these kits often represented a significant value to consumers.&lt;/p&gt;
    &lt;p&gt;This card can also be found in Tandy Multimedia PCs as the Tandy Multimedia Audio Adapter. Immediately noticeable changes were from the regular joystick port to two mini-DIN connectors compatible with the Tandy 1000 joysticks, and the addition of a mini-DIN MIDI port. For both the joystick connectors and MIDI connector, adapters were required. A less noticeable change, the Tandy card used a different bus interface chip, the CT1346, and the output amplifier could be disabled via a jumper. Finally, the card featured a high-DMA channel allocated for audio which allowed 16bit 44.1kHz mono output in Windows.&lt;/p&gt;
    &lt;p&gt;The Sound Blaster 2, or Sound Blaster Deluxe, model CT1350 was released in October of 1991. This model improved the board layout allowing for a more compact card, and it completely eliminated the C/MS chips. This model improved on its predecessor by adding auto-init to DMA allowing the card to play continuously without the crackling or pausing that was experienced on the original. The sample rate for digital audio on this card was increased to 44kHz for playback and to 15kHz for recording. With this card, a DSP upgrade was made available to owners of the original Sound Blaster, which was required for full compatibility with the Windows 3 multimedia extensions.&lt;/p&gt;
    &lt;p&gt;Creative was growing quickly, achieving an estimated 72% market share of the sound card market globally in 1992, but it was also facing significant competition. Media Vision’s Pro Audio Spectrum Plus, released in 1991, was capable of 8bit digital sampling and 16bit digital audio playback. It featured a CD-ROM interface, and it was Sound Blaster compatible. The Pro Audio Spectrum 16 of 1992 moved the company to 16bit ISA, added 16bit stereo digital audio, and featured stereo FM synthesis while maintaining full Sound Blaster compatibility. Then, there was Aztech in the more low-end market making some serious OEM deals with likes of Dell and Compaq. They entered the market in 1992 at a much lower price point and offered broad compatibility with sound cards like the Adlib, Sound Blaster 2, Sound Blaster Pro, Cover Speech Thing, Disney Sound Source, and Windows Sound System.&lt;/p&gt;
    &lt;p&gt;To answer the competition and maintain their lead, the company released the Sound Blaster 16, CT1740, in June of 1992. This was a fully 16bit sound card and featured support for 16bit 44.1kHz digital audio. Creative had partnered with E-mu Systems to offer the Wave Blaster daughter board that brought wavetable synthesis to card through the header on the top of the card. The empty socket seen on the SB16 was for the Creative Signal Processor, CT1748, which brought hardware-assisted speech synthesis, QSound audio spatialization for digital wave playback, and PCM audio compression/decompression. The SB16 was more popular than any card before it, and the wavetable daughter board was popular enough to push Creative to acquire E-mu in March of 1993 for $54 million.&lt;/p&gt;
    &lt;p&gt;Creative went public in August of 1992 and became the first Singaporean company to be listed on the NASDAQ. In September of 1992, Creative expanded into China establishing a joint venture in Beijing called Chuang Tong Multimedia Computer Ltd. Creative held 70%, NewStone 20%, and Da Heng 10%. In addition to selling the company’s multimedia hardware, the Chinese subsidiary developed and distributed CD-ROM software in the Chinese language, and sold PJS and PJ Views. The following year, Ed Esber, formerly of Ashton-Tate, joined Creative Labs as CEO, and he assembled a team that included Rich Buchanan, Gail Pomerantz, and Rich Sorkin. Of the new team in the USA, Sorkin had the most lasting impact. He began licensing programs, shortened product development cycles, and began legal endeavors to protect Creative’s intellectual property. Throughout 1993, Creative established itself Australia, Japan, the UK, and Ireland. Finally, that same year, Creative acquired ShareVision Technology who made videoconferencing technologies. Creative’s later attempts in that market didn’t make it far.&lt;/p&gt;
    &lt;p&gt;By 1994, the Sound Blaster 16 was the audio card. The company needed both a low-end product and high-end product, and so the ViBRA 16, CT2501, took the low, and the AWE32 took the high. The ViBRA was a cost-reduced, single-chip implementation of the SB16 and was frequently supplied to OEMs. Some ViBRA models included an on-board modem. The AWE32 featured the CT1748 CSP, CT1747A with OPL3 FM synth, CT1971 (EMU8000) and CT1972 (EM8011, 1MB sample ROM) wavetable synth, CT1745A mixer, CT1741 DSP, a CD-ROM interface, wavetable header, SPDIF header, and 512K of sample RAM upgradeable to 28MB via two 30-pin SIMM slots. The AWE32, CT3900, was a full-length, 16bit, ISA card. With the SB16, ViBRA, and AWE32 on the market, the company’s revenues exceeded $650 million, and the company was listed on the Singapore stock exchange.&lt;/p&gt;
    &lt;p&gt;On the 26th of October in 1994, in time for the Christmas shopping season, Creative released the 3DO Blaster. This brought 3DO games to the PC via a full-length, 16bit, ISA card. On the card was a 32bit RISC CPU, a DSP for CD audio, two graphics processors, 2MB of RAM, 1MB of ROM, 1MB of VRAM, and 32K SRAM with battery backup. The box contained two games (Shockwave, Gridders), some demos, drivers, Aldus Photostyler and Gallery Effects, a controller, manuals, the card itself, and a registration card. Of course, the 3DO blaster was not, itself, a standard VGA card. To use the 3DO Blaster, one’s PC would need to be at least a 25MHz Intel 386, have at least 4MB of RAM, a VGA card, Windows 3.1, a CD-ROM drive (either Matsushita or Creative CR-564), a Sound Blaster, and some speakers. The press release from 3DO read, in part:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;With the introduction of 3DO Blaster, Creative is targeting their extensive installed base of CD-ROM users. 3DO Blaster provides PC owners with the ultimate game platform — exciting 3DO games recognized for unprecedented interactive realism, full-motion video, CD-quality audio and three-dimensional sound effects.&lt;/p&gt;
      &lt;p&gt;“Today’s announcement reflects the efforts of two of the most advanced technology suppliers, Creative Technology and 3DO. The 3DO Blaster provides the advantage of Creative’s and 3DO’s innovation to the installed base of PC’s already using Creative multimedia products,” said Sim Wong Hoo, CEO and chairman of Creative Technology Ltd.&lt;/p&gt;
      &lt;p&gt;“Creative’s and 3DO’s technologies create an advanced entertainment platform which will enhance the capabilities of PCs, and expand the imagination of users by providing them access to exciting, interactive products that fully exploit the potential of multimedia entertainment.”&lt;/p&gt;
      &lt;p&gt;Trip Hawkins, president and CEO of The 3DO Company, said today’s announcement enables his company to expand quickly and aggressively into the vast PC market. “Creative is the leading supplier of multimedia products for PCs, providing us with the opportunity to deliver 3DO’s advanced interactive technology to an even broader audience,” said Hawkins.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Given that the 3DO Blaster cost $399.95 and the 3DO console didn’t do too well, this product was moribund from the start.&lt;/p&gt;
    &lt;p&gt;Also in October of 1994, Creative released HansVision. This was a Chinese-language office suite for Windows, and while Windows replaced PJS, HansVision replaced PJ Views. Also in 1994, Creative acquired Digicom Systems, a modem company. This resulted in the Creative Phone Blaster in 1995. The Phone Blaster, CT3110, was largely just a ViBRA 16 with an integrated modem and a wavetable header, but it was a full-length, 16bit, ISA card. It faired better than the company’s attempts at video conferencing, but it wasn’t much of a success.&lt;/p&gt;
    &lt;p&gt;A cost reduced version of the AWE32 was released in 1995 as the Sound Blaster 32. It was roughly equivalent to the AWE32 but lacked the on-board RAM, Wave Blaster Support, and CSP. Additionally, it utilized the CQM chip from the ViBRA instead of the OPL3. The CQM (Creative Quadratic Modulation) commonly suffered audio clipping, hiss, and ringing when playing digital audio.&lt;/p&gt;
    &lt;p&gt;Esber, Buchanan, and Pomerantz left the company in 1995. They’d never really got on with the folks in Singapore, and the two groups had disagreements over the company’s strategy. Sorkin, however, was promoted to General Manager of the audio division, and then to executive VP of business development and corporate investments.&lt;/p&gt;
    &lt;p&gt;With the earlier success of the company’s CD-ROM and sound card bundles packing Matsushita, Mitsumi, and other vendors’ drives, Creative had gone into the CD-ROM drive business. In 1995, the industry had a large oversupply and Creative dumped its inventory incurring a loss of $30 million, and causing the company’s share price to drop nearly 75%.&lt;/p&gt;
    &lt;p&gt;In 1995, Creative released the 3D Blaster, CT6200. This was a 3D accelerator card built around the 3DLabs GLINT 300SX processor. The GLINT 300SX was built of about a million transistors on IBM’s 3.3V, 0.5 micron process, and it was capable of about 2.5 billion operations per second. As with many cards that would follow, GLINT was designed to process Gouraud-shaded, Z-buffered, dithered triangles that were generated by an application or game and passed to GLINT via the OpenGL API (in this case CGL, and later DirectX). The chip was accompanied by 2MB (or 4MB with the 2MB daughter board) of DRAM, and this VESA Local Bus card achieved a pixel filtrate of 25MP/s. The card cost $349.95 at launch and it only handled 3D, requiring the user to have a 2D card installed and use VGA passthrough. This was roughly a year before the first Voodoo card, but shortly after the Diamond Edge 3D with an NV1 at $299 for 2MB. Given that this was a VLB card, the 3D Blaster was largely a card for 486 machines, and given the price, it didn’t sell well. As far as I am aware, there were roughly 13 game titles to support CGL. Of those, there was Rebel Moon which was exclusive to the CT6200, and even having been designed exclusively for this card, it wasn’t great. Frame rates would get quite sluggish at times, likely having been hampered by the 486 at the heart of VLB machines.&lt;/p&gt;
    &lt;p&gt;The Sound Blaster AWE64 was released in November of 1996, and it improved on the AWE32 in a few ways. First, it increased the signal to noise ratio (especially in the Gold version), and increased component integration resulting in traces that likewise avoided noise. Given increases in integration, the board also became smaller than its predecessor and decreased cost. It’s also notable that with general technological advancements made in the industry, the ICs were of a consistently higher quality than those used in earlier cards despite being less expensive. The card came in two versions; one was the standard version which later was re-branded as the Value version (CT4500) with 512K RAM, and the other was the Gold version (CT4390) with 4MB of RAM, a 20bit DAC, and separate SPDIF output. Functionally, there were two major differences between the AWE64 and AWE32. The AWE64 added WaveGuide which synthesizes instrument sounds. While the Wave Blaster is no longer supported, the AWE64 Gold does have line inputs on the rear for an external Sound Canvas or similar product. Effectively, the WaveGuide feature allowed for greater polyphony through the use of 32 extra software-emulated channels, but in practice this used more CPU time and wasn’t very popular. The other change was the removal of 30-pin SIMM slots in favor of proprietary memory daughter boards. In all other respects, the AWE64 was simply a better AWE32. For purists, the AWE64 lacks Sound Blaster Pro compatibility and genuine OPL3 FM Synthesis, but for those who want SB16 compatibility, mostly noise-free output, hassle-free plug-n-play, and General MIDI capabilities, the AWE64 is wonderful. For collectors today, however, owning a genuine AWE64 Gold will set a buyer back between $200 and $400. That price will increase for those desiring a SIMMConn (replacing the proprietary memory daughter board with a 30-pin SIMM adapter).&lt;/p&gt;
    &lt;p&gt;Creative closed 1996 with $1.6 billion in revenues, and Sorkin left the company for Elon Musk’s Zip2.&lt;/p&gt;
    &lt;p&gt;Media Vision will get its own article at some point, but the company collapsed in a scandal, and Aureal Semiconductor was formed on the 9th of November in 1995 out of the prior company’s remnants. On the 14th of July in 1997, Aureal announced the Vortex AU8820 with high quality positional audio via the company’s A3D technology. This allowed a human listener to perceive audio as coming from a rather precise location, and it had originally been developed by Crystal River Engineering for NASA’s Virtual Environment Workstation Project. Crystal River had been acquired by Aureal in May of 1996, and Aureal productized the technology. The Vortex proved to be extremely popular and its features were supported by many of the most popular gaming titles of the time: Half-Life, Unreal, Quake II, and so on.&lt;/p&gt;
    &lt;p&gt;For Creative, the arrival of the Vortex card was existential. Most of the company’s revenues came from sound cards, and the Vortex had gained the respect of gamers and audiophiles almost immediately following its release. What was worse was that its feature set was being incorporated into games where once the Sound Blaster had been the de facto standard. The fastest way to gain expertise is to buy it, and Creative bought Ensoniq in January of 1998 for $77 million. Within Creative, Ensoniq was merged with E-mu Systems. The acquisition brought the Ensoniq AudioPCI into Creative, and this was a card intended to be cheap, functional, and feature rich. It supported digital effects such as reverb, chorus, and spatial enhancement, as well as DirectSound3D, and sample-based synthesis. For the new owner, the card couldn’t have been better as it support Sound Blaster compatibility through the use of a TSR despite being a PCI card. This card was rebranded several times as the Sound Blaster PCI 64, PCI 128, Vibra PCI and so on. The Ensoniq ES1370 that powered the card became the Creative 5507, and then revised into further AC97 variants. A major downside of the card was that it ran with a 44kHz sample rate only, and thus, audio recorded at any other rate was resampled which lowered fidelity and increased CPU time. The later AC97 variants supported only 48kHz natively, and therefore likewise resampled audio. While the AudioPCI wouldn’t win over audiophiles, its low cost moved units and won the company some OEM deals.&lt;/p&gt;
    &lt;p&gt;On the 20th of January in 1998, Creative chose to remedy the mistake it had made with their first 3D accelerator, and they released the CT6670, or 3D Blaster Voodoo2. It used the PCI bus, had 8MB of 25ns EDO RAM, and like all Voodoos, supported Glide. In September the same year, the company released the 3D Blaster Voodoo Banshee AGP card (CT6750) as well as the CT6760 PCI card. Depending upon the SKU, these could come with 8MB, 12MB or 16MB of SDRAM. While using the same name, the AGP card was designed entirely by Creative, and it was the only Creative board using a 3dfx chip to be so.&lt;/p&gt;
    &lt;p&gt;In July of 1998, Creative proved to be a leader in a different market segment with the introduction of HansVision Future 2000 in schools around Singapore. HVF2K featured the HansWord word processor, the HansBrowser bidirectional English-Chinese dictionary, and the HanSight online translator of webpages. Creative had successfully implemented productivity tools on the web, and they’d done machine translation of the web. Truly outstanding for the time.&lt;/p&gt;
    &lt;p&gt;Beginning in 1997, Creative Labs optical drive bundles began featuring DVD drives and speaker sets (thanks to the acquisition of Cambridge SoundWorks), and on the 10th of March in 1998 these products dropped in price rather dramatically and were expanded in their contents. One example, the Creative Components 700 (the most expensive on offer) included Creative’s PC-DVDx2 drive, Sound Blaster AWE64, the new Graphics Blaster Exxtreme (PCI, 3DLabs Permedia 2 chip, 4MB SGRAM, 64bit data path, OpenGL, up to 1600 by 1200, 60Hz to 150Hz refresh), Creative MPEG-2/Dolby Digital decoder board, and Cambridge SoundWorks’ PCWorks speaker system. This was priced at $479.99. The DVD-ROM drive was $149.99 stand-alone, and the decoder board was $169.99 stand-alone.&lt;/p&gt;
    &lt;p&gt;In August of 1998, Creative released the Sound Blaster Live! (CT4670) as a successor to the ViBRA range of sound cards. These were built around the EMU10K1 chip and supported DirectSound3D, EAX (Environmental Audio Extensions) versions 1 and 2, and featured an onboard, 64-voice, wavetable synthesizer though it did use main memory for sample storage. This was a PCI bus card, and it utilized Ensoniq’s TSR for the emulation of Adlib, Sound Blaster, and General MIDI (the adaptation of that TSR was a condition of the acquisition of Ensoniq).&lt;/p&gt;
    &lt;p&gt;1998 was a year of intense litigation for Creative. The first suit was filed by Creative against Aureal over MIDI caching patent infringements. This was followed by a counter claim of defamation and unfair competition by Aureal against Creative. Creative’s advertising of the Sound Blaster Live! then sparked more lawsuits by Aureal against Creative over claimed falsehoods. By the end of 1999, Aureal had won but had gone bankrupt as a result of legal costs. I am sure it cut quite deeply, but Creative acquired Aureal in September of 2000 for $32 million.&lt;/p&gt;
    &lt;p&gt;After 3dfx acquired STB, they began making their own cards. As a result, Creative began making, mostly, Nvidia-based cards for video and graphics. There were some exceptions. The Creative 3D Blaster Savage 4 obviously used the S3 Savage 4 chipset, and the Graphics Blaster Exxtreme used chips from 3DLabs. Possibly to prevent the sort of problem they’d had with 3dfx, Creative then acquired 3DLabs in June of 2002. From 1999 onward, Creative would release a handful of graphics cards, some did well and others didn’t, but they were no longer a substantial source of revenue for the company.&lt;/p&gt;
    &lt;p&gt;Creative had some great timing with one particular product. WinAmp brought MP3 support to the desktop in 1997, and Windows Media Player 5.2 gained MP3 support in 1998. Creative released the NOMAD MP3 player in April of 1999 for $429. In June of 1999, Napster was born, and MP3s exploded in popularity. The NOMAD connected to a user’s PC via a cradle, and that cradle attached to the PC via parallel port. The device had either 32MB or 64MB of battery backed RAM depending upon the model purchased, with more storage provided by flash media. The NOMAD also provided an FM tuner for those who wished to listen to radio, and a microphone for voice recordings. On the PC side of things, Creative provided both a CD ripper and the NOMAD Manager. The latter of which was for handling the transfer of content to the device. The box proudly claims that 64MB would provide an hour of CD-quality audio, and that’s… well… not true at all. MP3 encoding is quite lossy, and to compress 700MB of lossless CD audio into 64MB infers an incredibly low sample rate. An hour of audio in 64MB would absolutely not be “CD-quality.” Marketing aside, the NOMAD was a cool product.&lt;/p&gt;
    &lt;p&gt;The NOMAD II launched the following year, and it was well received by the press. This time, Creative used USB 1.1 instead of parallel, 32MB of internal memory, bundled 64MB Smart Media flash, and added EAX support, WMA support, a backlight for the LCD, a wired remote for controls, and slightly better microphone for voice recording. This was followed by the IIc which removed the FM tuner and offered either 64MB or 128MB of internal memory.&lt;/p&gt;
    &lt;p&gt;Creative released two further units in 2000, the NOMAD Jukebox and the NOMAD II MG. These also used USB. The II MG returned to the format of the original NOMAD, but it added equalizer presets, ID3-tag support, the wired remote, and the FM tuner returned and now featured. a sleep timer and recording. The NOMAD Jukebox was different. It was roughly the size and shape of a Discman, though slightly thicker, and had a 2.5 inch, 6GB, IDE hard disk in it. The Jukebox also had WAV support, line-in for recording, and two line-out jacks for four speaker systems like Creative’s own Cambridge SoundWorks four point surround. If NiMH batteries were being used, the Jukebox featured a DC jack, and it could charge those batteries. Given the use of spinning rust, battery life was just four hours. For adventurous folks today, the hard disk in this is upgradeable, but the disk didn’t have any identifiable partitions or formatting, and as a result the first 32MB need to be copied with something like &lt;code&gt;dd&lt;/code&gt; and then the drive inserted into the Jukebox and the format function used by holding the Play and Stop buttons (or EAX and Down on newer units) during the “loading” sequence.&lt;/p&gt;
    &lt;p&gt;Following the 2001 crash, Creative became an increasingly audio-only company. Some Chinese/English, electronic, pocketable dictionaries would continue in Asia, but most of Creative’s other endeavors ceased. The company was focused on speakers, headphones, sound cards, and portable music players.&lt;/p&gt;
    &lt;p&gt;US patent 6928433 was awarded to Creative on the 9th of August in 2005 for the user interface of the Zen and NOMAD Jukebox MP3 Players. This patent had been applied for on the 5th of January in 2001. Creative filed suit against Apple in May of 2006, and the two companies reached a settlement in August with Apple agreeing to pay $100 million.&lt;/p&gt;
    &lt;p&gt;Time wasn’t kind to Creative. Motherboard audio had become good enough for most people, and fewer than a quarter of desktop users bought dedicated sound cards. Worse, the shift to laptops during the first decade of the new millennium meant that a majority of PC users couldn’t make use of a sound card. Creative voluntarily delisted from the NASDAQ with the last day of trading having been the 31st of August in 2007. The company continued to be listed on SGX-ST. Layoffs of some staff in Stillwater, Oklahoma followed in 2008.&lt;/p&gt;
    &lt;p&gt;In 2009, 3DLabs and Creative’s Personal Digital Entertainment divisions were combined and reformed as ZiiLABS. This division designed a series of semi-custom ARM chips with 24 to 96 processing units called StemCells. These StemCells were sort of DSPs, and video, audio, and 3D graphics tasks were handled by these coprocessors. ZiiLABS produced at least five SKUs: ZMS-05, ZMS-08, ZMS-20, ZMS-40, and ZMS-50. On the 19th of November in 2012, Creative announced that they’d licensed ZiiLABS technology and patents to Intel for $20 million, and they sold engineering resources and assets to Intel for $30 million. Creative stated in the announcement that they’d retained the patents themselves. The ZiiLABS website was online through 2023, but it later went dormant with a default tomcat page in 2024, and the domain is no longer active. From 2012 forward, the website hadn’t been updated.&lt;/p&gt;
    &lt;p&gt;Today, Creative is led by Freddy Sim (Sim Wong Hoo’s brother), Tan Jok Jin is the executive chairman, and Ng Kai Wa is vice chairman. The company’s 2024 net sales stood at $62.8 million (12% increase over 2023) with $59.4 million of that being due to audio, speakers, and headphones. The company reported a net loss of $11 million for 2024, down from $17 million in 2023. The company continues to sell Sound Blaster products including both internal and external sound cards, DACs, and amplifiers. Their speakers, headphones, and headsets sell well and have won the company some awards.&lt;/p&gt;
    &lt;p&gt;Creative rose to dominate the sound card market at a time when there weren’t many options. They made an excellent product, marketed well, and made solid relationships with software makers. The primary issue for the company was that their entire business was built around a single product category, and their attempts to break out of that category weren’t successful. With video cards, they were right on time with a decent product, but the Voodoo was superior. They pivoted and survived that transition only to have 3dfx abandon board partners. They then moved to MP3 players, saw some success, but were beaten by Apple. Today, the company continues in the same niche they once dominated, and they continue to make excellent sound cards. They are simply a much smaller company. Among retro-tech enthusiasts, however, the Sound Blaster 16, Pro, and AWE64 continue to have loyal fans.&lt;/p&gt;
    &lt;p&gt;My dear readers, many of you worked at, ran, or even founded the companies I cover here on ARF, and some of you were present at those companies for the time periods I cover. A few of you have been mentioned by name. All corrections to the record are sincerely welcome, and I would love any additional insights, corrections, or feedback. Please feel free to leave a comment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.abortretry.fail/p/the-story-of-creative-technology"/></entry><entry><id>https://news.ycombinator.com/item?id=45162626</id><title>Intel Arc Pro B50 GPU Launched at $349 for Compact Workstations</title><updated>2025-09-07T23:08:09.319681+00:00</updated><content>&lt;doc fingerprint="eb89172991b96df2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Intel has officially expanded its professional GPU portfolio with the launch of the Arc Pro B50, designed specifically for small-form-factor workstations. The card is based on the Battlemage BMG-G21 GPU, configured with 16 Xe2 cores. It comes paired with 16 GB of GDDR6 VRAM clocked at 14 Gbps on a 128-bit memory bus, producing 224 GB/s of effective bandwidth. This configuration ensures that the GPU cores are properly fed while maintaining a low overall power draw. Intel has kept the total board power at 70 W, enabling the card to run entirely from the PCIe slot without external connectors. With a PCIe Gen 5 x8 interface, the Pro B50 balances efficiency and bandwidth for professional workloads. One of the key features of the Arc Pro B50 is its suitability for AI workloads and specialized professional applications. Intel claims performance of up to 170 TOPS in INT8 compute, which is significant for local AI inference tasks, machine learning workloads, and data preprocessing. Beyond AI, the GPU is optimized for CAD, engineering, architectural visualization, and design software, where stability is just as important as raw throughput. To meet these needs, Intel supplies a certified workstation driver stack, ensuring predictable performance across industry-standard applications. The physical design reflects its target environment: the card uses a low-profile dual-slot form factor, making it ideal for dense workstation cases that prioritize both space savings and airflow efficiency. &lt;/p&gt;
      &lt;p&gt;Display connectivity is handled via four mini DisplayPort outputs, which support multi-monitor setups critical for professional users who often work with complex datasets or design layouts. By providing flexibility in display configuration while maintaining a small footprint, Intel positions the Arc Pro B50 as a versatile tool for both AI and visual workflows. The emphasis is not on competing with high-end workstation GPUs in sheer raw power, but on striking a balance between price, efficiency, and reliability in scenarios where compact workstations are used.&lt;/p&gt;
      &lt;p&gt;With an MSRP of $349, Intel has aimed for the Arc Pro B50 to be an accessible entry into the workstation GPU segment. The card will be distributed both as a standalone retail product and through OEM workstation systems. At launch, early reviews from outlets such as HardwareLuxx, Phoronix, and Igor’s LAB indicate that the card delivers consistent results within its targeted workloads, rather than focusing on gaming performance. As Intel continues to invest in both consumer and professional graphics solutions, the Arc Pro B50 demonstrates a focus on specialized, compact, and power-efficient GPU offerings that broaden its reach in the professional computing market.&lt;/p&gt;
      &lt;p&gt;Source: Techpowerup&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.guru3d.com/story/intel-arc-pro-b50-gpu-launched-at-for-compact-workstations/"/></entry><entry><id>https://news.ycombinator.com/item?id=45162886</id><title>Detroit's Carmakers to Save Billions in Emissions Rollback</title><updated>2025-09-07T23:08:08.940681+00:00</updated><content>&lt;doc fingerprint="abfe8075481d9901"&gt;
  &lt;main&gt;
    &lt;p&gt;Transportation&lt;/p&gt;
    &lt;head rend="h1"&gt;Detroit’s Carmakers to Save Billions in Trump Emissions Rollback&lt;/head&gt;
    &lt;p&gt;President Donald Trump’s push to cut federal sales incentives and roll back emissions standards is shaping up to be a multibillion-dollar gift to Detroit’s automakers as they shift investments into gasoline-fueled cars.&lt;/p&gt;
    &lt;p&gt;General Motors Co. last week said it would cut electric-vehicle production plans at two factories as it overhauls a third plant to make gas-fueled pickups, instead of battery-powered trucks. Ford Motor Co. is moving funds from a canceled three-row electric SUV to future internal combustion engine vehicles and hybrids, while Jeep-owner Stellantis NV is resurrecting the thirsty Hemi V-8 engine.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bloomberg.com/news/articles/2025-09-07/detroit-s-carmakers-to-save-billions-in-trump-emissions-rollback"/></entry></feed>