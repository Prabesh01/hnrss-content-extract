<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-20T22:09:17.814132+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45640594</id><title>DeepSeek OCR</title><updated>2025-10-20T22:09:27.411356+00:00</updated><content>&lt;doc fingerprint="d0978f309aa0d982"&gt;
  &lt;main&gt;
    &lt;p&gt;üì• Model Download | üìÑ Paper Link | üìÑ Arxiv Paper Link |&lt;/p&gt;
    &lt;p&gt;Explore the boundaries of visual-text compression.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[2025/x/x]üöÄüöÄüöÄ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Our environment is cuda11.8+torch2.6.0.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone this repository and navigate to the DeepSeek-OCR folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/deepseek-ai/DeepSeek-OCR.git&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Conda&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;conda create -n deepseek-ocr python=3.12.9 -y
conda activate deepseek-ocr&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Packages&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;download the vllm-0.8.5 whl&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
pip install -r requirements.txt
pip install flash-attn==2.7.3 --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Note: if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers&amp;gt;=4.51.1&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VLLM:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-vllm&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;image: streaming output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_image.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;pdf: concurrency ~2500tokens/s(an A100-40G)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_pdf.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;batch eval for benchmarks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_eval_batch.py&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transformers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;from transformers import AutoModel, AutoTokenizer
import torch
import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
model_name = 'deepseek-ai/DeepSeek-OCR'

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)
model = model.eval().cuda().to(torch.bfloat16)

# prompt = "&amp;lt;image&amp;gt;\nFree OCR. "
prompt = "&amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown. "
image_file = 'your_image.jpg'
output_path = 'your/output/dir'

res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)&lt;/code&gt;
    &lt;p&gt;or you can&lt;/p&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-hf
python run_dpsk_ocr.py&lt;/code&gt;
    &lt;p&gt;The current open-source model supports the following modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native resolution: &lt;list rend="ul"&gt;&lt;item&gt;Tiny: 512√ó512 Ôºà64 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Small: 640√ó640 Ôºà100 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Base: 1024√ó1024 Ôºà256 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Large: 1280√ó1280 Ôºà400 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Dynamic resolution &lt;list rend="ul"&gt;&lt;item&gt;Gundam: n√ó640√ó640 + 1√ó1024√ó1024 ‚úÖ&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# document: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown.
# other image: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;OCR this image.
# without layouts: &amp;lt;image&amp;gt;\nFree OCR.
# figures in document: &amp;lt;image&amp;gt;\nParse the figure.
# general: &amp;lt;image&amp;gt;\nDescribe this image in detail.
# rec: &amp;lt;image&amp;gt;\nLocate &amp;lt;|ref|&amp;gt;xxxx&amp;lt;|/ref|&amp;gt; in the image.
# 'ÂÖàÂ§©‰∏ã‰πãÂøßËÄåÂøß'&lt;/code&gt;
    &lt;p&gt;We would like to thank Vary, GOT-OCR2.0, MinerU, PaddleOCR, OneChart, Slow Perception for their valuable models and ideas.&lt;/p&gt;
    &lt;p&gt;We also appreciate the benchmarks: Fox, OminiDocBench.&lt;/p&gt;
    &lt;p&gt;coming soonÔºÅ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/deepseek-ai/DeepSeek-OCR"/><published>2025-10-20T06:26:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640838</id><title>AWS Multiple Services Down in us-east-1</title><updated>2025-10-20T22:09:27.263092+00:00</updated><link href="https://health.aws.amazon.com/health/status?ts=20251020"/><published>2025-10-20T07:22:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640877</id><title>Docker Systems Status: Full Service Disruption</title><updated>2025-10-20T22:09:27.104356+00:00</updated><content>&lt;doc fingerprint="3bbcb2c31d7475f1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h5"&gt;Issues accessing Registry, Hub, Scout, DBC, DHIOperational&lt;/head&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;Docker Hub Registry, Docker Authentication, Docker Hub Web Services, Docker Billing, Docker Hub Automated Builds, Docker Hub Security Scanning, Docker Scout, Docker Build Cloud, Testcontainers Cloud, Docker Cloud, Docker Hardened Images&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 03:05 PDT&lt;lb/&gt;October 20, 2025 10:05 UTC&lt;/p&gt;
        &lt;p&gt;[Resolved] This incident is resolved. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 02:43 PDT&lt;lb/&gt;October 20, 2025 09:43 UTC&lt;/p&gt;
        &lt;p&gt;[Monitoring] We are seeing error rates recovering across our SaaS services. We continue to monitor as we process our backlog. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 01:22 PDT&lt;lb/&gt;October 20, 2025 08:22 UTC&lt;/p&gt;
        &lt;p&gt;[Identified] We have identified the underlying issue with one of our cloud service providers. We are monitoring the situation and prepare our systems for when the issues with our service provider resolve. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 00:16 PDT&lt;lb/&gt;October 20, 2025 07:16 UTC&lt;/p&gt;
        &lt;p&gt;[Investigating] We are seeing issues accessing and using our services across many of our products. We are currently investigating and will report back as soon as possible.. &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dockerstatus.com/pages/incident/533c6539221ae15e3f000031/68f5e1c741c825463df7486c"/><published>2025-10-20T07:31:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45642911</id><title>Show HN: Playwright Skill for Claude Code ‚Äì Less context than playwright-MCP</title><updated>2025-10-20T22:09:26.504393+00:00</updated><content>&lt;doc fingerprint="9f451e9e0fdfe3ea"&gt;
  &lt;main&gt;
    &lt;p&gt;General-purpose browser automation as a Claude Skill&lt;/p&gt;
    &lt;p&gt;A Claude Skill that enables Claude to write and execute any Playwright automation on-the-fly - from simple page tests to complex multi-step flows. Packaged as a Claude Code Plugin for easy installation and distribution.&lt;/p&gt;
    &lt;p&gt;Claude autonomously decides when to use this skill based on your browser automation needs, loading only the minimal information required for your specific task.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any Automation Task - Claude writes custom code for your specific request, not limited to pre-built scripts&lt;/item&gt;
      &lt;item&gt;Visible Browser by Default - See automation in real-time with &lt;code&gt;headless: false&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Zero Module Resolution Errors - Universal executor ensures proper module access&lt;/item&gt;
      &lt;item&gt;Progressive Disclosure - Concise SKILL.md with full API reference loaded only when needed&lt;/item&gt;
      &lt;item&gt;Safe Cleanup - Smart temp file management without race conditions&lt;/item&gt;
      &lt;item&gt;Comprehensive Helpers - Optional utility functions for common tasks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This skill can be installed via the Claude Code plugin system or manually.&lt;/p&gt;
    &lt;code&gt;# Add this repository as a marketplace
/plugin marketplace add lackeyjb/playwright-skill

# Install the plugin
/plugin install playwright-skill@playwright-skill

# Navigate to the skill directory and run setup
cd ~/.claude/plugins/marketplaces/playwright-skill/skills/playwright-skill
npm run setup&lt;/code&gt;
    &lt;p&gt;Verify installation by running &lt;code&gt;/help&lt;/code&gt; to confirm the skill is available.&lt;/p&gt;
    &lt;p&gt;Install directly from GitHub to your skills directory:&lt;/p&gt;
    &lt;p&gt;Global Installation (Available Everywhere):&lt;/p&gt;
    &lt;code&gt;# Navigate to your Claude skills directory
cd ~/.claude/skills

# Clone the skill
git clone https://github.com/lackeyjb/playwright-skill.git

# Navigate into the skill directory (note the nested structure)
cd playwright-skill/skills/playwright-skill

# Install dependencies and Chromium browser
npm run setup&lt;/code&gt;
    &lt;p&gt;Project-Specific Installation:&lt;/p&gt;
    &lt;code&gt;# Install in a specific project
cd /path/to/your/project
mkdir -p .claude/skills
cd .claude/skills
git clone https://github.com/lackeyjb/playwright-skill.git
cd playwright-skill/skills/playwright-skill
npm run setup&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest release from GitHub Releases&lt;/item&gt;
      &lt;item&gt;Extract to: &lt;list rend="ul"&gt;&lt;item&gt;Global: &lt;code&gt;~/.claude/skills/playwright-skill&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Project: &lt;code&gt;/path/to/your/project/.claude/skills/playwright-skill&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Global: &lt;/item&gt;
      &lt;item&gt;Navigate to the skill directory and run setup: &lt;code&gt;cd playwright-skill/skills/playwright-skill npm run setup&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run &lt;code&gt;/help&lt;/code&gt; to confirm the skill is loaded, then ask Claude to perform a simple browser task like "Test if google.com loads".&lt;/p&gt;
    &lt;p&gt;After installation, simply ask Claude to test or automate any browser task. Claude will write custom Playwright code, execute it, and return results with screenshots and console output.&lt;/p&gt;
    &lt;code&gt;"Test the homepage"
"Check if the contact form works"
"Verify the signup flow"
&lt;/code&gt;
    &lt;code&gt;"Take screenshots of the dashboard in mobile and desktop"
"Test responsive design across different viewports"
&lt;/code&gt;
    &lt;code&gt;"Fill out the registration form and submit it"
"Click through the main navigation"
"Test the search functionality"
&lt;/code&gt;
    &lt;code&gt;"Check for broken links"
"Verify all images load"
"Test form validation"
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Describe what you want to test or automate&lt;/item&gt;
      &lt;item&gt;Claude writes custom Playwright code for the task&lt;/item&gt;
      &lt;item&gt;The universal executor (run.js) runs it with proper module resolution&lt;/item&gt;
      &lt;item&gt;Browser opens (visible by default) and automation executes&lt;/item&gt;
      &lt;item&gt;Results are displayed with console output and screenshots&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Default settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Headless: &lt;code&gt;false&lt;/code&gt;(browser visible unless explicitly requested otherwise)&lt;/item&gt;
      &lt;item&gt;Slow Motion: &lt;code&gt;100ms&lt;/code&gt;for visibility&lt;/item&gt;
      &lt;item&gt;Timeout: &lt;code&gt;30s&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Screenshots: Saved to &lt;code&gt;/tmp/&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;playwright-skill/
‚îú‚îÄ‚îÄ .claude-plugin/
‚îÇ   ‚îú‚îÄ‚îÄ plugin.json          # Plugin metadata for distribution
‚îÇ   ‚îî‚îÄ‚îÄ marketplace.json     # Marketplace configuration
‚îú‚îÄ‚îÄ skills/
‚îÇ   ‚îî‚îÄ‚îÄ playwright-skill/    # The actual skill (Claude discovers this)
‚îÇ       ‚îú‚îÄ‚îÄ SKILL.md         # What Claude reads (314 lines)
‚îÇ       ‚îú‚îÄ‚îÄ run.js           # Universal executor (proper module resolution)
‚îÇ       ‚îú‚îÄ‚îÄ package.json     # Dependencies &amp;amp; setup scripts
‚îÇ       ‚îî‚îÄ‚îÄ lib/
‚îÇ           ‚îî‚îÄ‚îÄ helpers.js   # Optional utility functions
‚îú‚îÄ‚îÄ API_REFERENCE.md         # Full Playwright API reference (630 lines)
‚îú‚îÄ‚îÄ README.md                # This file - user documentation
‚îú‚îÄ‚îÄ CONTRIBUTING.md          # Contribution guidelines
‚îî‚îÄ‚îÄ LICENSE                  # MIT License
&lt;/code&gt;
    &lt;p&gt;Claude will automatically load &lt;code&gt;API_REFERENCE.md&lt;/code&gt; when needed for comprehensive documentation on selectors, network interception, authentication, visual regression testing, mobile emulation, performance testing, and debugging.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js &amp;gt;= 14.0.0&lt;/item&gt;
      &lt;item&gt;Playwright ^1.48.0 (installed via &lt;code&gt;npm run setup&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Chromium (installed via &lt;code&gt;npm run setup&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Playwright not installed? Navigate to the skill directory and run &lt;code&gt;npm run setup&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Module not found errors? Ensure automation runs via &lt;code&gt;run.js&lt;/code&gt;, which handles module resolution.&lt;/p&gt;
    &lt;p&gt;Browser doesn't open? Verify &lt;code&gt;headless: false&lt;/code&gt; is set. The skill defaults to visible browser unless headless mode is requested.&lt;/p&gt;
    &lt;p&gt;Install all browsers? Run &lt;code&gt;npm run install-all-browsers&lt;/code&gt; from the skill directory.&lt;/p&gt;
    &lt;p&gt;Skills are modular capabilities that extend Claude's functionality. Unlike slash commands that you invoke manually, skills are model-invoked‚ÄîClaude autonomously decides when to use them based on your request.&lt;/p&gt;
    &lt;p&gt;When you ask Claude to test a webpage or automate browser interactions, Claude discovers this skill, loads the necessary instructions, executes custom Playwright code, and returns results with screenshots and console output.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome. Fork the repository, create a feature branch, make your changes, and submit a pull request. See CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Skills - Official announcement from Anthropic&lt;/item&gt;
      &lt;item&gt;Claude Code Skills Documentation&lt;/item&gt;
      &lt;item&gt;Claude Code Plugins Documentation&lt;/item&gt;
      &lt;item&gt;Plugin Marketplaces&lt;/item&gt;
      &lt;item&gt;API_REFERENCE.md - Full Playwright documentation&lt;/item&gt;
      &lt;item&gt;GitHub Issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License - see LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/lackeyjb/playwright-skill"/><published>2025-10-20T11:58:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45643163</id><title>Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system</title><updated>2025-10-20T22:09:26.359295+00:00</updated><content>&lt;doc fingerprint="69989c3d8f16dad1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system‚Äî up to 9x increase in output lets 213 GPUs perform like 1,192&lt;/head&gt;
    &lt;p&gt;A paper presented at SOSP 2025 details how token-level scheduling helped one GPU serve multiple LLMs, reducing demand from 1,192 to 213 H20s.&lt;/p&gt;
    &lt;p&gt;Alibaba Cloud claims its new Aegaeon pooling system reduces the number of Nvidia GPUs required to serve large language models by 82% during a multi-month beta test inside its Model Studio marketplace. The result, published in a peer-reviewed paper presented at the 2025 ACM Symposium on Operating Systems (SOSP) in Seoul, suggests that cloud providers may be able to extract significantly more inference capacity from existing silicon, especially in constrained markets like China, where the supply of Nvidia's latest H20s remains limited.&lt;/p&gt;
    &lt;p&gt;Unlike training-time breakthroughs that chase model quality or speed, Aegaeon is an inference-time scheduler designed to maximize GPU utilization across many models with bursty or unpredictable demand. Instead of pinning one accelerator to one model, Aegaeon virtualizes GPU access at the token level, allowing it to schedule tiny slices of work across a shared pool. This means one H20 could serve several different models simultaneously, with system-wide ‚Äúgoodput‚Äù ‚Äî a measure of effective output ‚Äî rising by as much as nine times compared to older serverless systems.&lt;/p&gt;
    &lt;p&gt;The system was tested in production over several months, according to the paper, which lists authors from both Peking University and Alibaba‚Äôs infrastructure division, including CTO Jingren Zhou. During that window, the number of GPUs needed to support dozens of different LLMs ‚Äî ranging in size up to 72 billion parameters ‚Äî fell from 1,192 to just 213.&lt;/p&gt;
    &lt;p&gt;While the paper does not break down which models contributed most to the savings, reporting by the South China Morning Post says the tests were conducted using Nvidia‚Äôs H20, one of the few accelerators still legally available to Chinese buyers under current U.S. export controls.&lt;/p&gt;
    &lt;p&gt;Alibaba says the gains came from two main techniques: Packing multiple models per GPU, and using a token-level autoscaler to dynamically allocate compute as output is generated, rather than reserving resources at the request level. In benchmarks, Aegaeon beat the goodput of ServerlessLLM and MuxServe by margins ranging from 1.5 times to 9 times.&lt;/p&gt;
    &lt;p&gt;Whether those savings translate outside Alibaba‚Äôs stack remains to be seen. Alibaba Cloud‚Äôs paper does not specify the exact network fabric used in the beta test, but we know the company offers its own eRDMA elastic RDMA network and has a record of building highly‚Äëintegrated GPU serving stacks, suggesting the results may depend on an optimized, vertically integrated environment.&lt;/p&gt;
    &lt;p&gt;Regardless, the result is likely to attract interest from other hyperscalers looking to stretch scarce accelerator fleets as inference demand continues to spike.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Luke James is a freelance writer and journalist. Although his background is in legal, he has a personal interest in all things tech, especially hardware and microelectronics, and anything regulatory.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/semiconductors/alibaba-says-new-pooling-system-cut-nvidia-gpu-use-by-82-percent"/><published>2025-10-20T12:31:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45643357</id><title>Servo v0.0.1</title><updated>2025-10-20T22:09:25.742933+00:00</updated><content>&lt;doc fingerprint="946e1b5c97a1d4"&gt;
  &lt;main&gt;
    &lt;p&gt;Servo is a prototype web browser engine written in the Rust language. It is currently developed on 64-bit macOS, 64-bit Linux, 64-bit Windows, 64-bit OpenHarmony, and Android.&lt;/p&gt;
    &lt;p&gt;Servo welcomes contribution from everyone. Check out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Servo Book for documentation&lt;/item&gt;
      &lt;item&gt;servo.org for news and guides&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Coordination of Servo development happens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Here in the Github Issues&lt;/item&gt;
      &lt;item&gt;On the Servo Zulip&lt;/item&gt;
      &lt;item&gt;In video calls advertised in the Servo Project repo.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more detailed build instructions, see the Servo book under Setting up your environment, Building Servo, Building for Android and Building for OpenHarmony.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download and install Xcode and &lt;code&gt;brew&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;uv&lt;/code&gt;:&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;rustup&lt;/code&gt;:&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install &lt;code&gt;curl&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;Arch: &lt;code&gt;sudo pacman -S --needed curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Debian, Ubuntu: &lt;code&gt;sudo apt install curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Fedora: &lt;code&gt;sudo dnf install curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Gentoo: &lt;code&gt;sudo emerge net-misc/curl&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Arch: &lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;uv&lt;/code&gt;:&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;rustup&lt;/code&gt;:&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download &lt;code&gt;uv&lt;/code&gt;,&lt;code&gt;choco&lt;/code&gt;, and&lt;code&gt;rustup&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Be sure to select Quick install via the Visual Studio Community installer&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;In the Visual Studio Installer, ensure the following components are installed: &lt;list rend="ul"&gt;&lt;item&gt;Windows 10/11 SDK (anything &amp;gt;= 10.0.19041.0) (&lt;code&gt;Microsoft.VisualStudio.Component.Windows{10, 11}SDK.{&amp;gt;=19041}&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;MSVC v143 - VS 2022 C++ x64/x86 build tools (Latest) (&lt;code&gt;Microsoft.VisualStudio.Component.VC.Tools.x86.x64&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;C++ ATL for latest v143 build tools (x86 &amp;amp; x64) (&lt;code&gt;Microsoft.VisualStudio.Component.VC.ATL&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Windows 10/11 SDK (anything &amp;gt;= 10.0.19041.0) (&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;.\mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;.\mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ensure that the following environment variables are set: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;ANDROID_NDK_ROOT&lt;/code&gt;:&lt;code&gt;$ANDROID_SDK_ROOT/ndk/28.2.13676358/&lt;/code&gt;&lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt;can be any directory (such as&lt;code&gt;~/android-sdk&lt;/code&gt;). All of the Android build dependencies will be installed there.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Install the latest version of the Android command-line tools to &lt;code&gt;$ANDROID_SDK_ROOT/cmdline-tools/latest&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Run the following command to install the necessary components: &lt;quote&gt;sudo $ANDROID_SDK_ROOT/cmdline-tools/latest/bin/sdkmanager --install \ "build-tools;34.0.0" \ "emulator" \ "ndk;28.2.13676358" \ "platform-tools" \ "platforms;android-33" \ "system-images;android-33;google_apis;x86_64"&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Follow the instructions above for the platform you are building on&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow the instructions above for the platform you are building on to prepare the environment.&lt;/item&gt;
      &lt;item&gt;Depending on the target distribution (e.g. &lt;code&gt;HarmonyOS NEXT&lt;/code&gt;vs pure&lt;code&gt;OpenHarmony&lt;/code&gt;) the build configuration will differ slightly.&lt;/item&gt;
      &lt;item&gt;Ensure that the following environment variables are set &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;DEVECO_SDK_HOME&lt;/code&gt;(Required when targeting&lt;code&gt;HarmonyOS NEXT&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;OHOS_BASE_SDK_HOME&lt;/code&gt;(Required when targeting&lt;code&gt;OpenHarmony&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;OHOS_SDK_NATIVE&lt;/code&gt;(e.g.&lt;code&gt;${DEVECO_SDK_HOME}/default/openharmony/native&lt;/code&gt;or&lt;code&gt;${OHOS_BASE_SDK_HOME}/${API_VERSION}/native&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;SERVO_OHOS_SIGNING_CONFIG&lt;/code&gt;: Path to json file containing a valid signing configuration for the demo app.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Review the detailed instructions at Building for OpenHarmony.&lt;/item&gt;
      &lt;item&gt;The target distribution can be modified by passing &lt;code&gt;--flavor=&amp;lt;default|harmonyos&amp;gt;&lt;/code&gt;to&lt;code&gt;mach &amp;lt;build|package|install&amp;gt;&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/servo/servo"/><published>2025-10-20T12:55:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45644328</id><title>BERT is just a single text diffusion step</title><updated>2025-10-20T22:09:25.607407+00:00</updated><content>&lt;doc fingerprint="35106edd82bc1c52"&gt;
  &lt;main&gt;
    &lt;p&gt;A while back, Google DeepMind unveiled Gemini Diffusion, an experimental language model that generates text using diffusion. Unlike traditional GPT-style models that generate one word at a time, Gemini Diffusion creates whole blocks of text by refining random noise step-by-step.&lt;/p&gt;
    &lt;p&gt;I read the paper Large Language Diffusion Models and was surprised to find that discrete language diffusion is just a generalization of masked language modeling (MLM), something we‚Äôve been doing since 2018. The first thought I had was, ‚Äúcan we finetune a BERT-like model to do text generation?‚Äù I decided to try a quick proof of concept out of curiosity.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NOTE: After I wrote the article I stumbled upon the paper DiffusionBERT which does essentially the same thing but with more rigorous testing! Check it out if this post interested you.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;A Short History of Transformers#&lt;/head&gt;
    &lt;p&gt;The original Transformer architecture, introduced in 2017, was an encoder-decoder model. In 2018, researchers realized that the encoder and decoder components of the model could be separated (with the advent of BERT and GPT), and two distinct families of models were created:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Encoder-only models (BERT-style, bidirectional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encoder models used masked language modeling (MLM) as a training objective: randomly mask out a subset of tokens of each input and train the encoder to reconstruct the missing tokens (fill in the blanks). The model sees the entire (partially masked) context at once and learns bidirectional representations. This architecture excelled at tasks requiring a full‚Äêsentence (or paragraph) representation (e.g., classification and retrieval).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Decoder-only models (GPT-style, autoregressive)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Decoder models used next‚Äêtoken prediction as a training objective: at each position $t$, predict the token at position $t + 1$ given all tokens up to $t$ as context. Only the left context is used to predict future values (unidirectional). This architecture excelled at generative tasks where you produce text one token at a time, such as open‚Äêended generation, summarization, and translation.&lt;/p&gt;
    &lt;p&gt;Originally, BERT saw immediate use in tasks such as classification, whereas GPT-style models didn‚Äôt become popular until later (due to initial limited capabilities). Eventually, the generation capabilities of autoregressive (decoder) transformers vastly improved. The general training objective of ‚Äúnext token prediction‚Äù means a much larger space of use cases when compared to encoder models.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discrete Language Diffusion Models#&lt;/head&gt;
    &lt;p&gt;Diffusion models were first popularized in image generation. In image generation, diffusion models gradually add Gaussian noise to an image (forward process) and then train a neural network to iteratively denoise it (reverse process). A high‚Äêlevel summary of continuous diffusion with images is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Forward process: Start from a clean image x‚ÇÄ, then add small amounts of (usually Gaussian) noise at each timestep until you end up with near‚Äêpure noise.&lt;/item&gt;
      &lt;item&gt;Reverse process: Train a model (often a U‚ÄêNet) to predict the noise at each timestep, gradually recovering the original image in discrete denoising steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Applying this idea to language means we need a way to add noise to text and then remove it in stages. The simplest way to do this is a masking‚Äêbased noise process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Forward (masking) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;At timestep t = 0, you have a fully uncorrupted text sequence.&lt;/item&gt;
          &lt;item&gt;At each subsequent timestep t &amp;gt; 0, randomly replace a fraction of tokens with a special &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;token according to a pre‚Äêdefined schedule (e.g., gradually increasing the masked proportion from 0% to 100%).&lt;/item&gt;
          &lt;item&gt;By the final timestep T, the entire sequence may be masked (all tokens are &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reverse (denoising) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Train a model (often a standard Transformer encoder) to predict the original token IDs given a partially masked sequence at timestep t.&lt;/item&gt;
          &lt;item&gt;This is akin to performing masked language modeling at varying mask rates: at early timesteps, only a few tokens are masked (easy to predict); at later timesteps, many tokens are masked (harder).&lt;/item&gt;
          &lt;item&gt;By chaining together predictions from high‚Äêmask‚Äêrate back down to zero, you can recover (or generate) a full sequence.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this discrete text diffusion framework, the model learns a likelihood bound on the data distribution by optimizing a sum of denoising losses over all timesteps, rather than a single MLM objective at a fixed mask probability.&lt;/p&gt;
    &lt;p&gt;As we can see, BERT‚Äôs masked language modeling objective is the same training objective as text diffusion, but just for a subset of masking rates. By introducing variable masking rates (from 0 to 1) and a scheduled sequence of denoising steps (inspired by diffusion theory), we can transform BERT‚Äôs masked language modeling objective into a full generative procedure.&lt;/p&gt;
    &lt;head rend="h2"&gt;RoBERTa Diffusion#&lt;/head&gt;
    &lt;p&gt;In 2019, RoBERTa was released. It was essentially just an enhancement of the original BERT model, with better hyperparameters, data training size, and a more simple training objective (MLM only, removed next sentence prediction).&lt;/p&gt;
    &lt;p&gt;Here we use the HuggingFace &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;dataset&lt;/code&gt; libraries to pull in the original RoBERTa weights, tokenizer, and the Trainer class to easily finetune the model on the WikiText dataset.
The main code (full code here) looks like this below:&lt;/p&gt;
    &lt;code&gt;# Load and tokenize dataset and instantiate the model
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")
model = RobertaForMaskedLM.from_pretrained("roberta-base")

# Create the training args and Trainer instance
training_args = TrainingArguments(
    output_dir="finetuned-roberta-diffusion",
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    save_strategy="epoch",
    save_total_limit=1,
    logging_steps=200,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    data_collator=diffusion_collator, # custom implementation
    tokenizer=tokenizer,
)

# Train &amp;amp; save
trainer.train()
trainer.save_model("finetuned-roberta-diffusion")&lt;/code&gt;
    &lt;p&gt;Currently we have 10 diffusion steps, so we randomly sample a percentage $p$ out of &lt;code&gt;mask_probs&lt;/code&gt; (1.0, 0.9, 0.9, &amp;amp;mldr;, 0.1) and mask that percent of the tokens each batch.
The custom &lt;code&gt;diffusion_collator&lt;/code&gt; function (see code here) samples one mask-probability &lt;code&gt;p&lt;/code&gt; from &lt;code&gt;mask_probs&lt;/code&gt; per batch and sets each token to &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; with &lt;code&gt;p&lt;/code&gt; probability.&lt;/p&gt;
    &lt;p&gt;To be able to condition the generation on a ‚Äúprompt‚Äù, we currently never mask the first 16 tokens. That means that during training, each step will always have the first 16 tokens as context for generation.&lt;/p&gt;
    &lt;p&gt;Simplified code for the &lt;code&gt;diffusion_collator&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;  def diffusion_collator(examples):
      batch = tokenizer.pad(examples, return_tensors="pt")

      # Randomly select masking probability for this batch
      mask_prob = random.choice([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])

      # Never mask the first PREFIX_LEN tokens (preserved context)
      maskable_positions = batch.input_ids[:, PREFIX_LEN:]

      # Create random mask for the chosen probability
      mask = torch.rand(maskable_positions.shape) &amp;lt; mask_prob

      # Apply masking
      batch.input_ids[:, PREFIX_LEN:][mask] = tokenizer.mask_token_id
      batch.labels = batch.input_ids.clone()

      return batch&lt;/code&gt;
    &lt;p&gt;For inference, we start with an input which is a tensor of size 256 (since we are generating blocks of 256 tokens). The first 16 positions are the token ids that correspond to the prompt, and the last 240 are just &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens. We iterate through the denoising schedule and each step, we generate a prediction and then remask the sequence again. The process looks like this:&lt;/p&gt;
    &lt;code&gt;Step 0: [PREFIX] &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; ...     (100% masked)
Step 1: [PREFIX] will &amp;lt;mask&amp;gt; over &amp;lt;mask&amp;gt; control ...        (90% masked)
Step 2: [PREFIX] will begin &amp;lt;mask&amp;gt; greater control ...      (80% masked)
...
Step 10: [PREFIX] will begin to assert greater control ...  (0% masked - DONE)&lt;/code&gt;
    &lt;p&gt;Simplified code for generation looks like:&lt;/p&gt;
    &lt;code&gt;# Generate text through iterative denoising
for step, mask_prob in enumerate(mask_probs):
    # Forward pass: predict masked tokens
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = outputs.logits  # shape: (1, MAX_LEN, vocab_size)

    # For each masked position, sample from top-k/top-p filtered distribution
    for pos in range(PREFIX_LEN, MAX_LEN):
        if input_ids[0, pos] == tokenizer.mask_token_id:
            logits = predictions[0, pos, :]
            # Apply top-k and top-p filtering
            filtered_logits = top_k_top_p_filtering(logits, top_k=TOP_K, top_p=TOP_P)
            probs = F.softmax(filtered_logits, dim=-1)
            # Sample token
            sampled_token = torch.multinomial(probs, 1)
            input_ids[0, pos] = sampled_token

    # Re-mask a portion of non-prefix tokens for next iteration
    if mask_prob &amp;gt; 0:
        mask_indices = torch.rand(MAX_LEN - PREFIX_LEN) &amp;lt; mask_prob
        input_ids[0, PREFIX_LEN:][mask_indices] = tokenizer.mask_token_id&lt;/code&gt;
    &lt;p&gt;Here is an example output generation of the fine-tuned model after training on an H200 for 30 minutes (the first line is the initial prompt):&lt;/p&gt;
    &lt;code&gt;Following their victory in the French and Indian War, Britain began to assert
greater...

...dominion over Europe beginning about the early 19th. There conflict took
place on the island, between British and Irish Ireland. British officials 
administered British Ireland, a Celtic empire under the control of the Irish 
nationalist authorities, defined as a dominion of Britain. As the newly Fortic 
states acquired independent and powerful status, many former English colonies
played their part in this new, British @-@ controlled colonial system. Following
this period the Non @-@ Parliamentaryist Party won its influence in Britain in 
1890, led by the support of settlers from the Irish colonies. Looking inwards, 
Sinclair, Lewis questioned, and debated the need to describe " The New Britain "&lt;/code&gt;
    &lt;p&gt;The output looks surprisingly coherent! Most of the quirks present are actually just quirks from the formatting of WikiText (spaces around punctuation &lt;code&gt;"&lt;/code&gt;, turning hyphens &lt;code&gt;-&lt;/code&gt; into &lt;code&gt;@-@&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Below is a comparison between our diffusion model and GPT-2:&lt;/p&gt;
    &lt;p&gt;We see GPT-2‚Äôs output is more coherent and slightly faster (~9 seconds vs ~13) but I‚Äôm pleasantly surprised with how good my simple implementation was. It is a good proof of concept, and with new approaches like AR-Diffusion and Skip-Step Diffusion (and a more optimized implementation), the quality and speed can be drastically improved.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;We‚Äôve seen that masked language models like RoBERTa, originally designed for fill-in-the-blank tasks, can be repurposed into fully generative engines by interpreting variable-rate masking as a discrete diffusion process. By gradually corrupting text with &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens and training the model to iteratively denoise at increasing mask intensities, we effectively turn the standard MLM objective into a step-by-step generation procedure.&lt;/p&gt;
    &lt;p&gt;Even without architectural changes, a fine-tuned RoBERTa can generate coherent looking text after slightly modifying the training objective, validating the idea that BERT-style models are essentially just text diffusion models trained on one masking rate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nathan.rs/posts/roberta-diffusion/"/><published>2025-10-20T14:31:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45645120</id><title>Show HN: I created a cross-platform GUI for the JJ VCS (Git compatible)</title><updated>2025-10-20T22:09:25.537981+00:00</updated><content>&lt;doc fingerprint="10bd79eb74913531"&gt;
  &lt;main&gt;
    &lt;p&gt;Releases Roadmap JUDO The full-featured GUI for JJ VCS (works with Git repos too!) macOS Download for macOS Windows Download for Windows Linux (Ubuntu/Debian) Download for Linux Restore your repo to any point in time with the Operation Log. Undo and redo any change. View combined diffs of multiple commits, or the diff between commits Apply or revert hunks of any diff, files, commits, or even multiple commits at once Use custom revsets to select which commits are shown. Filter by descriptions, authors, ancestry, and more. Drag and drop rebase Duplicate, split, abandon, revert, absorb, squash, and more Keep your bookmarks managed&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://judojj.com"/><published>2025-10-20T15:35:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45645172</id><title>Postman which I thought worked locally on my computer, is down</title><updated>2025-10-20T22:09:24.904690+00:00</updated><content>&lt;doc fingerprint="8a5fb823f5f65ce2"&gt;
  &lt;main&gt;
    &lt;p&gt;Update - We have seen significant recovery of the features. We are continuing to monitor for any further issues. Oct 20, 2025 - 08:20 PDT&lt;/p&gt;
    &lt;p&gt;Monitoring - Majority of the services have recovered. We are continuing to monitor. Oct 20, 2025 - 06:21 PDT&lt;/p&gt;
    &lt;p&gt;Update - We are seeing significant recovery and are continuing to monitor. Oct 20, 2025 - 05:56 PDT&lt;/p&gt;
    &lt;p&gt;Update - We are seeing significant recovery and are continuing to monitor. Oct 20, 2025 - 05:52 PDT&lt;/p&gt;
    &lt;p&gt;Identified - We are currently experiencing significantly increased error rates which is impacting functionality on Postman. There is a major issue with our underlying cloud provider and we are working with them to restore full access as quickly as possible. Oct 20, 2025 - 05:39 PDT&lt;/p&gt;
    &lt;p&gt;US Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;99.99 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Platform on Desktop ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;99.95 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Platform on Browser ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;99.95 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Login ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;99.96 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Monitors ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Mocks ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman API ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman API Network (API Explore) ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Run Button ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Search ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Public Collection Documentation ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Newman ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Learning Center and Documentation ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Support and Community Forum ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Integrations ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Interceptor Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Marketing Website ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postbot ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman VS Code extension ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;API Builder ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;API Specifications ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Collection Runner Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman CLI Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Europe Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Platform on Desktop ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Platform on Browser ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Login ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Mocks ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman API ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Postman Search ? Operational&lt;/p&gt;
    &lt;p&gt;90 days ago&lt;/p&gt;
    &lt;p&gt;100.0 % uptime&lt;/p&gt;
    &lt;p&gt;Today&lt;/p&gt;
    &lt;p&gt;Operational&lt;/p&gt;
    &lt;p&gt;Degraded Performance&lt;/p&gt;
    &lt;p&gt;Partial Outage&lt;/p&gt;
    &lt;p&gt;Major Outage&lt;/p&gt;
    &lt;p&gt;Maintenance&lt;/p&gt;
    &lt;p&gt;Major outage&lt;/p&gt;
    &lt;p&gt;Partial outage&lt;/p&gt;
    &lt;p&gt;No downtime recorded on this day.&lt;/p&gt;
    &lt;p&gt;No data exists for this day.&lt;/p&gt;
    &lt;p&gt;had a major outage.&lt;/p&gt;
    &lt;p&gt;had a partial outage.&lt;/p&gt;
    &lt;p&gt;Related&lt;/p&gt;
    &lt;p&gt;No incidents or maintenance related to this downtime.&lt;/p&gt;
    &lt;p&gt;Completed - The scheduled maintenance has been completed. Oct 12, 08:52 PDT&lt;/p&gt;
    &lt;p&gt;Verifying - Verification is currently underway for the maintenance items. Oct 12, 08:49 PDT&lt;/p&gt;
    &lt;p&gt;In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary. Oct 12, 08:00 PDT&lt;/p&gt;
    &lt;p&gt;Scheduled - We will be performing a planned database upgrade in the US data centre to ensure the latest version and security patches are applied. During this time, users with data in US region may experience temporary service disruptions while using Postman across all platforms.&lt;/p&gt;
    &lt;p&gt;Maintenance is expected to last approximately 2 hours and we will provide regular updates on the progress. Please note, this downtime only affects users with data in US region. Services for users in our EU data centre will remain unaffected.&lt;/p&gt;
    &lt;p&gt;If you have any questions, please don't hesitate to reach out to our support team at support@postman.com. We appreciate your patience and understanding during this maintenance window. Oct 9, 06:30 PDT&lt;/p&gt;
    &lt;p&gt;Completed - Scheduled database maintenance has been successfully completed. Oct 10, 21:40 PDT&lt;/p&gt;
    &lt;p&gt;Verifying - We are in the process of verifying the maintenance items. Oct 10, 21:23 PDT&lt;/p&gt;
    &lt;p&gt;In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary. Oct 10, 20:30 PDT&lt;/p&gt;
    &lt;p&gt;Scheduled - We have a scheduled database maintenance coming up to enable upgrades to the service. Users may experience intermittent service disruptions when doing collection runs via Collection Runner or Postman CLI. Postman Monitors and Scheduled Collections will remain unaffected. We expect to complete maintenance within two hours and will provide frequent updates throughout the process.&lt;/p&gt;
    &lt;p&gt;Contact our support team at support@postman.com with any questions. We appreciate your patience during this maintenance window. Oct 7, 09:15 PDT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://status.postman.com"/><published>2025-10-20T15:40:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45645349</id><title>Production RAG: what I learned from processing 5M+ documents</title><updated>2025-10-20T22:09:24.606291+00:00</updated><content>&lt;doc fingerprint="9bf95f134d9a6771"&gt;
  &lt;main&gt;
    &lt;p&gt;I've spent the last 8 months in the RAG trenches, I want to share what actually worked vs. wasted our time. We built RAG for Usul AI (9M pages) and an unnamed legal AI enterprise (4M pages).&lt;/p&gt;
    &lt;head rend="h2"&gt;Langchain + Llamaindex&lt;/head&gt;
    &lt;p&gt;We started out with youtube tutorials. First Langchain ‚Üí Llamaindex. Got to a working prototype in a couple of days and were optimistic with the progress. We run tests on subset of the data (100 documents) and the results looked great. We spent the next few days running the pipeline on the production dataset and got everything working in a week ‚Äî incredible.&lt;/p&gt;
    &lt;p&gt;Except it wasn't, the results were subpar and only the end users could tell. We spent the following few months rewriting pieces of the system, one at a time, until the performance was at the level we wanted. Here are things we did ranked by ROI.&lt;/p&gt;
    &lt;head rend="h2"&gt;What moved the needle&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query Generation: not all context can be captured by the user's last query. We had an LLM review the thread and generate a number of semantic + keyword queries. We processed all of those queries in parallel, and passed them to a reranker. This made us cover a larger surface area and not be dependent on a computed score for hybrid search.&lt;/item&gt;
      &lt;item&gt;Reranking: the highest value 5 lines of code you'll add. The chunk ranking shifted a lot. More than you'd expect. Reranking can many times make up for a bad setup if you pass in enough chunks. We found the ideal reranker set-up to be 50 chunk input -&amp;gt; 15 output.&lt;/item&gt;
      &lt;item&gt;Chunking Strategy: this takes a lot of effort, you'll probably be spending most of your time on it. We built a custom flow for both enterprises, make sure to understand the data, review the chunks, and check that a) chunks are not getting cut mid-word or sentence b) ~each chunk is a logical unit and captures information on its own&lt;/item&gt;
      &lt;item&gt;Metadata to LLM: we started by passing the chunk text to the LLM, we ran an experiment and found that injecting relevant metadata as well (title, author, etc.) improves context and answers by a lot.&lt;/item&gt;
      &lt;item&gt;Query routing: many users asked questions that can't be answered by RAG (e.g. summarize the article, who wrote this). We created a small router that detects these questions and answers them using an API call + LLM instead of the full-blown RAG set-ups.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our stack&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vector database: Azure -&amp;gt; Pinecone -&amp;gt; Turbopuffer (cheap, supports keyword search natively)&lt;/item&gt;
      &lt;item&gt;Document Extraction: Custom&lt;/item&gt;
      &lt;item&gt;Chunking: Unstructured.io by default, custom for enterprises (heard that Chonkie is good)&lt;/item&gt;
      &lt;item&gt;Embedding: text-embedding-large-3, haven't tested others&lt;/item&gt;
      &lt;item&gt;Reranker: None -&amp;gt; Cohere 3.5 -&amp;gt; Zerank (less known but actually good)&lt;/item&gt;
      &lt;item&gt;LLM: GPT 4.1 -&amp;gt; GPT 5 -&amp;gt; GPT 4.1, covered by Azure credits&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Going Open-source&lt;/head&gt;
    &lt;p&gt;We put all our learning into an open-source project: agentset-ai/agentset under an MIT license. Feel free to reach out if you have any questions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.abdellatif.io/production-rag-processing-5m-documents"/><published>2025-10-20T15:55:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45646691</id><title>TernFS ‚Äì an exabyte scale, multi-region distributed filesystem</title><updated>2025-10-20T22:09:24.315267+00:00</updated><content>&lt;doc fingerprint="65ba0c171fb742d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TernFS ‚Äî an exabyte scale, multi-region distributed filesystem&lt;/head&gt;
    &lt;p&gt;September 2025&lt;/p&gt;
    &lt;p&gt;XTX is an algorithmic trading firm: it builds statistical models that produce price forecasts for over 50,000 financial instruments worldwide. We use those forecasts to make trades. As XTX's research efforts to build better models ramped up, the demand for resources kept increasing.&lt;/p&gt;
    &lt;p&gt;The firm started out with a couple of desktops and an NFS server, and 10 years later ended up with tens of thousands of high-end GPUs, hundreds of thousands of CPUs, and hundreds of petabytes of storage.&lt;/p&gt;
    &lt;p&gt;As compute grew, storage struggled to keep up. We rapidly outgrew NFS first and existing open-source and commercial filesystems later. After evaluating a variety of third-party solutions, we made the decision to implement our own filesystem, which we called TernFS[1].&lt;/p&gt;
    &lt;p&gt;We have decided to open source our efforts: TernFS is available as free software on our public GitHub. This post motivates TernFS, explains its high-level architecture, and then explores some key implementation details. If you just want to spin up a local TernFS cluster, head to the README.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another filesystem?&lt;/head&gt;
    &lt;p&gt;There's a reason why every major tech company has developed its own distributed filesystem ‚Äî they're crucial to running large-scale compute efforts, and liable to cause intense disruption if they malfunction. [2]&lt;/p&gt;
    &lt;p&gt;XTX was in the same position, so we designed TernFS to be a one-stop solution for most of our storage needs, going from relatively 'cold' storage of raw market data to short-lived random-access data used to communicate between GPU jobs running on our cluster.&lt;/p&gt;
    &lt;p&gt;TernFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is designed to scale up to tens of exabytes, trillions of files, millions of concurrent clients.&lt;/item&gt;
      &lt;item&gt;Stores file contents redundantly to protect against drive failures.&lt;/item&gt;
      &lt;item&gt;Has no single point of failure in its metadata services.&lt;/item&gt;
      &lt;item&gt;Supports file snapshot to protect against accidental file deletion.&lt;/item&gt;
      &lt;item&gt;Can span across multiple regions.&lt;/item&gt;
      &lt;item&gt;Is hardware agnostic and uses TCP/IP to communicate.&lt;/item&gt;
      &lt;item&gt;Utilizes different types of storage (such as flash vs. hard disks) cost effectively.&lt;/item&gt;
      &lt;item&gt;Exposes read/write access through its own API over TCP and UDP, and a Linux kernel filesystem module.&lt;/item&gt;
      &lt;item&gt;Requires no external service and has a minimal set of build dependencies. [3]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Specifically, C++ and Go are needed to build the various TernFS components.&lt;/p&gt;
    &lt;p&gt;The C++ and Go processes depend on a handful of vendored libraries, most notably RocksDB for C++.&lt;/p&gt;
    &lt;p&gt;Naturally, there are some limitations, the main ones being:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Files are immutable ‚Äî once they're written they can't be modified.&lt;/item&gt;
      &lt;item&gt;TernFS should not be used for tiny files ‚Äî our median file size is 2MB.&lt;/item&gt;
      &lt;item&gt;The throughput of directory creation and removal is significantly constrained compared to other operations.&lt;/item&gt;
      &lt;item&gt;TernFS is permissionless, deferring that responsibility to other services.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We started designing TernFS in early 2022 and began putting it into production in summer 2023. By mid-2024 all of our machine learning efforts were driven out of TernFS, and we're migrating the rest of the firm's storage needs onto it as well.&lt;/p&gt;
    &lt;p&gt;As of September 2025, our TernFS deployment stores more than 500PB across 30,000 disks, 10,000 flash drives, and three data centres. At peak we serve multiple terabytes per second. To this day, we haven't lost a single byte.&lt;/p&gt;
    &lt;head rend="h2"&gt;High-level overview&lt;/head&gt;
    &lt;p&gt;Now that the stage is set, we're ready to explain the various components that make up TernFS. TernFS' core API is implemented by four services:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Metadata shards store the directory structure and file metadata.&lt;/item&gt;
      &lt;item&gt;The cross-directory coordinator (or CDC) executes cross-shard transactions.&lt;/item&gt;
      &lt;item&gt;Block services store file contents.&lt;/item&gt;
      &lt;item&gt;The registry stores information about all the other services and monitors them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
 A ‚îÄ‚îÄ‚ñ∫ B means "A sends requests to B" 
                                       
                                       
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    
 ‚îÇ Metadata Shard ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          
 ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ          
   ‚îÇ    ‚îÇ                   ‚îÇ          
   ‚îÇ    ‚îÇ                   ‚îÇ          
   ‚îÇ ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê                ‚îÇ          
   ‚îÇ ‚îÇ CDC ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ          
   ‚îÇ ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò          ‚îÇ     ‚îÇ          
   ‚îÇ    ‚îÇ             ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê     
   ‚îÇ    ‚îÇ             ‚îî‚îÄ‚î§        ‚îÇ     
 ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ Client ‚îÇ     
 ‚îÇ Registry  ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§        ‚îÇ     
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     
        ‚îÇ                 ‚îÇ            
        ‚îÇ                 ‚îÇ            
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ            
 ‚îÇ Block Service ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

&lt;/code&gt;
    &lt;p&gt;In the next few sections, we'll describe the high-level design of each service and then give more background on other relevant implementation details.[4]&lt;/p&gt;
    &lt;p&gt;Note that TernFS' multi-region capabilities are orthogonal to much of its high-level design, and they're therefore explained separately.&lt;/p&gt;
    &lt;head rend="h3"&gt;Metadata&lt;/head&gt;
    &lt;p&gt;To talk about metadata, we first need to explain what metadata is in TernFS. The short answer is: 'everything that is not file contents.' The slightly longer answer is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Directory entries, including all files and directory names.&lt;/item&gt;
      &lt;item&gt;File metadata including creation/modification/access time, logical file size, and so on.&lt;/item&gt;
      &lt;item&gt;The mapping between files and the blocks containing their contents.&lt;/item&gt;
      &lt;item&gt;Other ancillary data structures to facilitate maintenance operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TernFS' metadata is split into 256 logical shards. Shards never communicate with each other. This is a general principle in TernFS: each service is disaggregated from the others, deferring to the clients to communicate with each service directly.[5]&lt;/p&gt;
    &lt;p&gt;There are some exceptions ‚Äî most notably the shards execute requests from the CDC, and all services check into the registry.&lt;/p&gt;
    &lt;p&gt;A logical shard is further split into five physical instances, one leader and four followers, in a typical distributed consensus setup. The distributed consensus engine is provided by a purpose-built Raft-like implementation, which we call LogsDB, while RocksDB is used to implement read/write capabilities within a shard instance.&lt;/p&gt;
    &lt;p&gt;Currently all reads and writes go through the leader, but it would be trivial to allow clients to read from followers, and with a bit more effort to switch to a write-write setup.&lt;/p&gt;
    &lt;code&gt;    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê 
    ‚îÇ Shard 0 ‚îÇ ‚îÇ Shard 1 ‚îÇ  ...  ‚îÇ Shard 255 ‚îÇ 
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò 
            ‚îå‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê 
            ‚îÇ                                 ‚îÇ 
            ‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 
            ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ Replica 0  ‚îÇ ‚îÇ 
            ‚îÇ ‚îÇ           ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ (follower) ‚îÇ ‚îÇ 
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ Replica 3 ‚óÑ‚îÄ‚îÄ‚îê ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ 
 ‚îÇ Client ‚îú‚îÄ‚îº‚îÄ‚ñ∫ (leader)  ‚óÑ‚îÄ‚îê‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ           ‚óÑ‚îê‚îÇ‚îî‚îÄ‚ñ∫ Replica 1  ‚îÇ ‚îÇ 
            ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ‚îÇ  ‚îÇ (follower) ‚îÇ ‚îÇ 
            ‚îÇ              ‚îÇ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ 
            ‚îÇ              ‚îÇ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 
            ‚îÇ              ‚îÇ‚îî‚îÄ‚îÄ‚ñ∫ Replica 2  ‚îÇ ‚îÇ 
            ‚îÇ              ‚îÇ   ‚îÇ (follower) ‚îÇ ‚îÇ 
            ‚îÇ              ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ 
            ‚îÇ              ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 
            ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚ñ∫ Replica 4  ‚îÇ ‚îÇ 
            ‚îÇ                  ‚îÇ (follower) ‚îÇ ‚îÇ 
            ‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ 
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò 
&lt;/code&gt;
    &lt;p&gt;Splitting the metadata into 256 shards from the get-go simplifies the design, given that horizontal scaling of metadata requires no rebalancing, just the addition of more metadata servers.&lt;/p&gt;
    &lt;p&gt;For instance, our current deployment can serve hundreds of petabytes and more than 100,000 compute nodes with just 10 metadata servers per data centre, with each server housing roughly 25 shard leaders and 100 shard followers.&lt;/p&gt;
    &lt;p&gt;Given that the metadata servers are totally decoupled from one another, this means that we can scale metadata performance by 25√ó trivially, and by 100√ó if we were to start offloading metadata requests to followers.&lt;/p&gt;
    &lt;p&gt;TernFS shards metadata by assigning each directory to a single shard. This is done in a simple round-robin fashion by the cross-directory coordinator. Once a directory is created, all its directory entries and the files in it are housed in the same shard.&lt;/p&gt;
    &lt;p&gt;This design decision has downsides: TernFS assumes that the load will be spread across the 256 logical shards naturally. This is not a problem in large deployments, given that they will contain many directories, but it is something to keep in mind.[6]&lt;/p&gt;
    &lt;head rend="h3"&gt;Cross-directory transactions&lt;/head&gt;
    &lt;p&gt;Most of the metadata activity is contained within a single shard:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File creation, same-directory renames, and deletion.&lt;/item&gt;
      &lt;item&gt;Listing directory contents.&lt;/item&gt;
      &lt;item&gt;Getting attributes of files or directories.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, some operations do require coordination between shards, namely directory creation, directory removal, and moving directory entries across different directories.&lt;/p&gt;
    &lt;p&gt;The cross-directory coordinator (CDC) performs these distributed transactions using a privileged metadata shard API. The CDC transactions are stateful, and therefore the CDC uses RocksDB and LogsDB much like the metadata shards themselves to persist its state safely.&lt;/p&gt;
    &lt;code&gt; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê 
 ‚îÇ Client ‚îú‚îÄ‚îê  ‚îÇ Shard 32 ‚îÇ ‚îÇ Shard 103 ‚îÇ 
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îò ‚îî‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò 
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îê         
 ‚îÇ CDC ‚îÇ  ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ     ‚îÇ ‚îÇ         
 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ Leader ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         
 ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îò            ‚îÇ         
 ‚îÇ              ‚îÇ               ‚îÇ         
 ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ         
 ‚îÇ       ‚îÇ              ‚îÇ       ‚îÇ         
 ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ         
 ‚îÇ ‚îÇ Follower ‚îÇ .. ‚îÇ Follower ‚îÇ ‚îÇ         
 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   
&lt;/code&gt;
    &lt;p&gt;The CDC executes transactions in parallel, which increases throughput considerably, but it is still a bottleneck when it comes to creating, removing, or moving directories. This means that TernFS has a relatively low throughput when it comes to CDC operations.[7]&lt;/p&gt;
    &lt;head rend="h3"&gt;Block services, or file contents&lt;/head&gt;
    &lt;p&gt;In TernFS, files are split into chunks of data called blocks. Blocks are read and written to by block services. A block service is typically a single drive (be it a hard disk or a flash drive) storing blocks. At XTX a typical storage server will contain around 100 hard disks or 25 flash drives ‚Äî or in TernFS parlance 100 or 25 block services.[8]&lt;/p&gt;
    &lt;p&gt;Read/write access to the block service is provided using a simple TCP API currently implemented by a Go process. This process is hardware agnostic and uses the Go standard library to read and write blocks to a conventional local file system. We originally planned to rewrite the Go process in C++, and possibly write to block devices directly, but the idiomatic Go implementation has proven performant enough for our needs so far.&lt;/p&gt;
    &lt;head rend="h3"&gt;The registry&lt;/head&gt;
    &lt;p&gt;The final piece of the TernFS puzzle is the registry. The registry stores the location of each instance of service (be it a metadata shard, the CDC, or a block storage node). A client only needs to know the address of the registry to mount TernFS ‚Äî it'll then gather the locations of the other services from it.&lt;/p&gt;
    &lt;p&gt;In TernFS all locations are IPv4 addresses. Working with IPv4 directly simplifies the kernel module considerably, since DNS lookups are quite awkward in the Linux kernel. The exception to this rule is addressing the registry itself, for which DNS is used.&lt;/p&gt;
    &lt;p&gt;The registry also stores additional information, such as the capacity and available size of each drive, who is a follower or a leader in LogsDB clusters, and so on.&lt;/p&gt;
    &lt;p&gt;Predictably, the registry itself is a RocksDB and LogsDB C++ process, given its statefulness.&lt;/p&gt;
    &lt;head rend="h3"&gt;Going global&lt;/head&gt;
    &lt;p&gt;TernFS tries very hard not to lose data, by storing both metadata and file contents on many different drives and servers. However, we also want to be resilient to the temporary or even permanent loss of one entire data centre. Therefore, TernFS can transparently scale across multiple locations.&lt;/p&gt;
    &lt;p&gt;The intended use for TernFS locations is for each location to converge to the same dataset. This means that each location will have to be provisioned with roughly equal resources.[9] Both metadata and file contents replication are asynchronous. In general, we judge the event of losing an entire data centre rare enough to tolerate a time window where data is not fully replicated across locations.&lt;/p&gt;
    &lt;p&gt;Metadata replication is set up so that one location is the metadata primary. Write operations in non-primary locations pay a latency price since they are acknowledged only after they are written to the primary location, replicated, and applied in the originating location. In practice this hasn't been an issue since metadata write latencies are generally overshadowed by writing file contents.&lt;/p&gt;
    &lt;p&gt;There is no automated procedure to migrate off a metadata primary location ‚Äî again, we deem it a rare enough occurrence to tolerate manual intervention. In the future we plan to move from the current protocol to a multi-master protocol where each location can commit writes independently, which would reduce write latencies on secondary locations and remove the privileged status of the primary location.&lt;/p&gt;
    &lt;p&gt;File contents, unlike metadata, are written locally to the location the client is writing from. Replication to other locations happens in two ways: proactively and on-demand. Proactive replication is performed by tailing the metadata log and replicating new file contents. On-demand replication happens when a client requests file content which has not been replicated yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Important Details&lt;/head&gt;
    &lt;p&gt;Now that we've laid down the high-level design of TernFS, we can talk about several key implementation details that make TernFS safer, more performant, and more flexible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Talking to TernFS&lt;/head&gt;
    &lt;head rend="h4"&gt;Speaking TernFS' language&lt;/head&gt;
    &lt;p&gt;The most direct way to talk to TernFS is by using its own API. All TernFS messages are defined using a custom serialization format we call bincode. We chose to develop a custom serialization format since we needed it to work within the confines of the Linux kernel and to be easily chopped into UDP packets.&lt;/p&gt;
    &lt;p&gt;We intentionally kept the TernFS API stateless, in the sense that each request executes without regard to previous requests made by the same client. This is in contrast to protocols like NFS, whereby each connection is very stateful, holding resources such as open files, locks, and so on.&lt;/p&gt;
    &lt;p&gt;A stateless API dramatically simplifies the state machines that make up the TernFS core services, therefore simplifying their testing. It also forces each request to be idempotent, or in any case have clear retry semantics, since they might have to be replayed, which facilitates testing further.&lt;/p&gt;
    &lt;p&gt;It also allows the metadata shards and CDC API to be based on UDP rather than TCP, which makes the server and clients (especially the kernel module) simpler, due to doing away with the need for keeping TCP connections. The block service API is TCP based, since it is used to stream large amounts of contiguous data, and any UDP implementation would have to re-implement a reliable stream protocol. The registry API is also TCP-based, given that it is rarely used by clients, and occasionally needs to return large amounts of data.&lt;/p&gt;
    &lt;p&gt;While the TernFS API is simple out-of-the-box, we provide a permissively licensed Go library implementing common tasks that clients might want to perform, such as caching directory policies and retrying requests. This library is used to implement many TernFS processes that are not part of the core TernFS services, such as scrubbing, garbage collection, migrations, and the web UI.&lt;/p&gt;
    &lt;head rend="h4"&gt;Making TernFS POSIX-shaped&lt;/head&gt;
    &lt;p&gt;While the Go library is used for most ancillary tasks, some with high performance requirements, the main way to access TernFS at XTX is through its Linux kernel module.&lt;/p&gt;
    &lt;p&gt;This is because, when migrating our machine learning workflows to TernFS, we needed to support a vast codebase working with files directly. This not only meant that we needed to expose TernFS as a normal filesystem, but also that said normal filesystem API needed to be robust and performant enough for our machine learning needs.[10]&lt;/p&gt;
    &lt;p&gt;For this reason, we opted to work with Linux directly, rather than using FUSE. Working directly with the Linux kernel not only gave us the confidence that we could achieve our performance requirements but also allowed us to bend the POSIX API to our needs, something that would have been more difficult if we had used FUSE.[11]&lt;/p&gt;
    &lt;p&gt;The main obstacle when exposing TernFS as a 'normal' filesystem is that TernFS files are immutable. More specifically, TernFS files are fully written before being 'linked' into the filesystem as a directory entry. This is intentional: it lets us cleanly separate the API for 'under construction' files and 'completed files', and it means that half-written files are not visible.&lt;/p&gt;
    &lt;p&gt;However this design is essentially incompatible with POSIX, which endows the user with near-absolute freedom when it comes to manipulating a file. Therefore, the TernFS kernel module is not POSIX-compliant, but rather exposes enough POSIX to allow many programs to work without modifications, but not all.&lt;/p&gt;
    &lt;p&gt;In practice this means that programs which write files left-to-right and never modify the files' contents will work out-of-the-box. While this might seem very restrictive, we found that a surprising number of programs worked just fine.[12] Programs that did not follow this pattern were modified to first write to a temporary file and then copy the finished file to TernFS.&lt;/p&gt;
    &lt;p&gt;While we feel that writing our own kernel module was the right approach, it proved to be the trickiest part of TernFS, and we would not have been able to implement it without some important safety checks in the TernFS core services.[13]&lt;/p&gt;
    &lt;head rend="h4"&gt;S3 gateway&lt;/head&gt;
    &lt;p&gt;Almost all the storage-related activity at XTX is due to our machine-learning efforts, and for those purposes the TernFS' kernel module has served us well. However, as TernFS proved itself there, we started to look into offering TernFS to the broader firm.&lt;/p&gt;
    &lt;p&gt;Doing so through the kernel module presented multiple challenges. For starters installing a custom kernel module on every machine that needed to reach TernFS is operationally cumbersome. Moreover, while all machine-learning happens in clusters housed in the same data centre as TernFS itself, we wanted to expose TernFS in a way that's more amenable to less local networks, for instance by removing the need for UDP. Finally, TernFS does not have any built-in support for permissions or authentication, which is a requirement in multi-tenant scenarios.&lt;/p&gt;
    &lt;p&gt;To solve all these problems, we implemented a gateway for TernFS, which exposes a TernFS subtree using the S3 API. The gateway is a simple Go process turning S3 calls into TernFS API calls. The S3 gateway is not currently open sourced since it is coupled to authentication services internal to XTX, but we have open sourced a minimal S3 gateway to serve as a starting point for third-party contributors to build their own.&lt;/p&gt;
    &lt;p&gt;We've also planned an NFS gateway to TernFS, but we haven't had a pressing enough need yet to complete it.&lt;/p&gt;
    &lt;head rend="h4"&gt;The web UI and the JSON interface&lt;/head&gt;
    &lt;p&gt;Finally, a view of TernFS is provided by its web UI. The web UI is a stateless Go program which exposes most of the state of TernFS in an easy-to-use interface. This state includes the full filesystem contents (both metadata and file contents), the status of each service including information about decommissioned block services, and so on.&lt;/p&gt;
    &lt;p&gt;Moreover, the web UI also exposes the direct TernFS API in JSON form, which is very useful for small scripts and curl-style automation that does not warrant a full-blown Go program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directory Policies&lt;/head&gt;
    &lt;p&gt;To implement some of the functionality we'll describe below, TernFS adopts a system of per-directory policies.&lt;/p&gt;
    &lt;p&gt;Policies are used for all sorts of decisions, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to redundantly store files.&lt;/item&gt;
      &lt;item&gt;On which type of drive to store files.&lt;/item&gt;
      &lt;item&gt;How long to keep files around after deletion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of the topics above (and a few more we haven't mentioned) correspond to a certain policy tag. The body of the policies are stored in the metadata together with the other directory attributes.&lt;/p&gt;
    &lt;p&gt;Policies are inherited: if a directory does not contain a certain policy tag, it transitively inherits from the parent directory. TernFS clients store a cache of policies to allow for traversal-free policy lookup for most directories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping blocks in check&lt;/head&gt;
    &lt;p&gt;A filesystem is no good if it loses, leaks, corrupts, or otherwise messes up its data. TernFS deploys a host of measures to minimize the chance of anything going wrong. So far, these have worked: we've never lost data in our production deployment of TernFS. This section focuses on the measures in place to specifically safeguard files' blocks.&lt;/p&gt;
    &lt;head rend="h4"&gt;Against bitrot, or CRC32-C&lt;/head&gt;
    &lt;p&gt;The first and possibly most obvious measure consists of aggressively checksumming all TernFS' data. The metadata is automatically checksummed by RocksDB, and every block is stored in a format interleaving 4KiB pages with 4byte CRC32-C checksums.&lt;/p&gt;
    &lt;p&gt;CRC32-C was picked since it is a high-quality checksum and implemented on most modern silicon.[14] It also exhibits some desirable properties when used together with Reed-Solomon coding.&lt;/p&gt;
    &lt;p&gt;Peter Cawley's fast-crc32 repository provides a general framework to compute CRC32-C quickly, together with state-of-the-art implementations for x86 and aarch64 architectures.&lt;/p&gt;
    &lt;p&gt;4KiB was picked since it is the read boundary used by Linux filesystems and is fine-grained while still being large enough to render the storage overhead of the 4byte checksums negligible.&lt;/p&gt;
    &lt;p&gt;Interleaving the CRCs with the block contents does not add any safety, but it does improve operations in two important ways. First, it allows for safe partial reads: clients can demand only a few pages from a block which is many megabytes in size and still check the reads against its checksum. Second, it allows scrubbing files locally on the server which hosts the blocks, without communicating with other services at all.&lt;/p&gt;
    &lt;head rend="h4"&gt;Storing files redundantly, or Reed-Solomon codes&lt;/head&gt;
    &lt;p&gt;We've been talking about files being split into blocks, but we haven't really explained how files become blocks.&lt;/p&gt;
    &lt;p&gt;The first thing we do to a file is split it into spans. Spans are at most 100MiB and are present just to divide files into sections of a manageable size.&lt;/p&gt;
    &lt;p&gt;Then each span is divided into D data blocks, and P parity blocks. D and P are determined by the corresponding directory policy in which the file is created. When D is 1, the entire contents of the span become a single block, and that block is stored D+P times. This scheme is equivalent to a simple mirroring scheme and allows it to lose up to P blocks before losing file data.&lt;/p&gt;
    &lt;p&gt;While wasteful, mirroring the entire contents of the file can be useful for very hot files, since TernFS clients will pick a block at random to read from, thereby sharing the read load across many block services. And naturally files which we do not care much for can be stored with D = 1 and P = 0, without any redundancy.&lt;/p&gt;
    &lt;p&gt;That said, most files will not be stored using mirroring but rather using Reed-Solomon coding. Other resources can be consulted to understand the high-level idea and the low-level details of Reed-Solomon coding, but the gist is it allows us to split a span into D equally sized blocks (some padding might be necessary), and then generate P blocks of equal size such that up to any P blocks can be lost while retaining the ability to reconstruct all the other blocks.&lt;/p&gt;
    &lt;p&gt;As mentioned, D and P are fully configurable, but at XTX we tend to use D = 10 and P = 4, which allows us to lose up to any four drives for any file.&lt;/p&gt;
    &lt;head rend="h4"&gt;Drive type picking&lt;/head&gt;
    &lt;p&gt;We now know how to split files into a bunch of blocks. The next question is: which drives to pick to store the blocks on. The first decision is which kind of drive to use. At XTX we separate drives into two broad categories for this purpose ‚Äî flash and spinning disks.&lt;/p&gt;
    &lt;p&gt;When picking between these two, we want to balance two needs: minimizing the cost of hardware by utilizing hard disks if we can [15], and maximizing hard disk productivity by having them reading data most of the time, rather than seeking.&lt;/p&gt;
    &lt;p&gt;To achieve that, directory policies offer a way to tune how large each block will be, and to tune which drives will be picked based on block size. This allows us to configure TernFS so that larger files that can be read sequentially are stored on hard disks, while random-access or small files are stored on flash. [16]&lt;/p&gt;
    &lt;p&gt;Currently this system is not adaptive, but we found that in practice it's easy to carve out sections of the filesystem which are not read sequentially. We have a default configuration which assumes sequential reads and then uses hard disks down to roughly 2.5MB blocks, below which hard disks stop being productive enough and blocks start needing to be written to flash.&lt;/p&gt;
    &lt;head rend="h4"&gt;Block service picking&lt;/head&gt;
    &lt;p&gt;OK, we now know what type of drive to select for our files, but we still have tens of thousands of individual drives to pick from. Picking the 'right' individual drive requires some sophistication.&lt;/p&gt;
    &lt;p&gt;The first thing to note is that drive failures or unavailability are often correlated. For instance, at XTX a single server handles 102 spinning disks. If the server is down, faulty, or needs to be decommissioned, it'll render its 102 disks temporarily or permanently unavailable.&lt;/p&gt;
    &lt;p&gt;It's therefore wise to spread a file's blocks across many servers. To achieve this, each TernFS block service (which generally corresponds to a single drive) has a failure domain. When picking block services in which to store the blocks for a given file, TernFS will make sure that each block is in a separate failure domain. In our TernFS deployment a failure domain corresponds to a server, but other users might wish to tie it to some other factor as appropriate.&lt;/p&gt;
    &lt;p&gt;TernFS also tries hard to avoid write bottlenecks by spreading the current write load across many disks. Moreover, since new drives can be added at any time, it tries to converge to a situation where each drive is roughly equally filled by assigning writing more to drives with more available space.&lt;/p&gt;
    &lt;p&gt;Mechanically this is achieved by having each shard periodically request a set of block services to use for writing from the registry. When handing out block services to shards, the registry selects block services according to several constraints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It never gives block services from the same failure domain to the same shard&lt;/item&gt;
      &lt;item&gt;It minimizes the variance in how many shards each block service is currently assigned to&lt;/item&gt;
      &lt;item&gt;It prioritizes block services which have more available space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then when a client wants to write a new span, requiring D+P blocks, the shard simply selects D+P block services randomly amongst the ones it last received from the registry.&lt;/p&gt;
    &lt;p&gt;One concept currently absent from TernFS is what is often known as 'copyset replication'. When assigning disks to files at random (even with the caveat of failure domains) the probability of rendering at least one file unreadable quickly becomes a certainty as more and more drives fail:&lt;/p&gt;
    &lt;p&gt;Copysets reduce the likelihood of data loss occurring by choosing blocks out of a limited number of sets of drives, as opposed to picking the drives randomly. This dramatically reduces the probability of data loss[17]. They are generally a good idea, but we haven't found them to be worthwhile, for a few reasons.&lt;/p&gt;
    &lt;p&gt;Note that while copysets reduce the failure probability, they do not (and cannot) reduce the expected amount of data loss. That is, instead of a large probability of a relatively small amount of data loss we have a very small probability of a catastrophic loss.&lt;/p&gt;
    &lt;p&gt;First, evacuating a 20TB drive takes just a few minutes, and in the presence of multiple failed drives the migrator process evacuates first the files which are present in multiple failed drives to get ahead of possible data loss. This means that for TernFS to lose data within a single data centre tens of drives would have to fail within a matter of seconds.&lt;/p&gt;
    &lt;p&gt;More importantly, our TernFS deployment is replicated across three data centres. This replication eliminates the chance of losing data due to 'independent' drive failures ‚Äî thousands of drives would need to fail at once. Obviously, data centre wide events can cause a large proportion of the drives within it to fail, but having such an event in three data centres at once is exceedingly unlikely.&lt;/p&gt;
    &lt;p&gt;Finally, copysets are not without drawbacks or complications. Assigning drives at random is an optimal strategy when it comes to evacuating drives quickly, since the files with blocks in the drives to be evacuated will be evenly spread over the rest of the filesystem, and since we only ever need to replace the failed blocks given that we're not constrained by fitting the new set of blocks in predetermined copysets. This means that the evacuation procedure will not be bottlenecked by drive throughput, which is what enables evacuation to finish in a matter of minutes. Moreover, the algorithm to distribute drives to shards is significantly simpler and more flexible than if it needed to care about copysets.&lt;/p&gt;
    &lt;p&gt;However, users that wish to deploy TernFS within a single data centre might wish to implement some form of copyset replication. Such a change would be entirely contained to the registry and would not change any other component.&lt;/p&gt;
    &lt;head rend="h4"&gt;Block Proofs&lt;/head&gt;
    &lt;p&gt;We now have a solid scheme to store files redundantly (thanks to Reed-Solomon codes) and protect against bitrot (thanks to the checksums). However, said schemes are only as good as their implementation.&lt;/p&gt;
    &lt;p&gt;As previously mentioned, TernFS clients communicate their intention to write a file to metadata servers, the metadata servers select block services that the blocks should be written to, and the clients then write the blocks to block services independently of the metadata services. The same happens when a client wants to erase blocks: the client first communicates its intentions to delete the blocks to the right metadata shard and then performs the erasing itself.&lt;/p&gt;
    &lt;p&gt;This poses a challenge. While verifying the correctness of the core TernFS services is feasible, verifying all clients is not, but we'd still like to prevent buggy clients from breaking key invariants of the filesystem.&lt;/p&gt;
    &lt;p&gt;Buggy clients can wreak havoc in several ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They can leak data by writing blocks to block services that are not referenced anywhere in the metadata.&lt;/item&gt;
      &lt;item&gt;They can lose data by erasing blocks which are still referenced in metadata.&lt;/item&gt;
      &lt;item&gt;They can corrupt data by telling the metadata services they'll write something and then writing something else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We address all these points by using what we call block proofs. To illustrate how block proofs work, it's helpful to go through the steps required to write new data to a file.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When a client is creating a file, it'll do so by adding its file spans one-by-one. For each span the client wants to add it sends an 'initiate span creation' request to the right metadata shard. This request contains both the overall checksum of the span, and the checksum of each block in it (including parity blocks).&lt;/item&gt;
      &lt;item&gt;The metadata shard checks the consistency of the checksum of the span and of its blocks, something it can do thanks to some desirable mathematical properties of CRCs.&lt;/item&gt;
      &lt;item&gt;The shard picks block services for the blocks to be written in and returns this information to the client together with a signature for each 'block write' instruction.&lt;/item&gt;
      &lt;item&gt;The client forwards this signature to the block services, which will refuse to write the block without it. Crucially, the cryptographic signature ranges over a unique identity for the block (ensuring we only write the block we mean to write), together with its checksum, ensuring we don't write the wrong data.[18]&lt;/item&gt;
      &lt;item&gt;After committing the block to disk, the block service returns a 'block written' signature to the client.&lt;/item&gt;
      &lt;item&gt;Finally, the client forwards the block written signature back to the shard, which certifies that the span has been written only when it has received the signatures for all the blocks that make up the span. [19]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This kind of scheme was described more generally in a separate blog post.&lt;/p&gt;
    &lt;p&gt;Similarly, when a client wants to delete a span, it first asks the metadata shard to start doing so. The metadata shard marks the span as 'in deletion' and returns a bunch of 'block erase' signatures to the client. The client then forwards the signatures to the block services that hold the blocks, which delete the blocks, and return a 'block erased' signature. The clients forward these signatures back to the metadata shards, which can then forget about the span entirely.&lt;/p&gt;
    &lt;p&gt;We use AES to generate the signatures for simplicity but note that the goal here is not protecting ourselves from malicious clients ‚Äî just buggy ones. The keys used for the signature are not kept secret, and CRC32-C is not a secure checksum. That said, we've found this scheme enormously valuable in the presence of complex clients. We spent considerable efforts making the core services very simple so we could then take more implementation risks in the clients, with the knowledge that we would have a very low chance of corrupting the filesystem itself.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scrubbing&lt;/head&gt;
    &lt;p&gt;Finally, if things go wrong, we need to notice. The most common failure mode for a drive is for it to fail entirely, in which case our internal hardware monitoring system will pick it up and migrate from it automatically. The more insidious (and still very common) case is a single sector failing in a drive, which will only be noticed when we try to read the block involving that sector.&lt;/p&gt;
    &lt;p&gt;This is acceptable for files which are read frequently, but some files might be very 'cold' but still very important.&lt;/p&gt;
    &lt;p&gt;Consider the case of raw market data taps which are immediately converted to some processed, lossy format. While we generally will use the file containing the processed data, it's paramount to store the raw market data forever so that if we ever want to include more information from the original market data, we can. So important cold files might go months or even years without anyone reading them, and in the meantime, we might find that enough blocks have been corrupted to render them unreadable.[20]&lt;/p&gt;
    &lt;p&gt;To make sure this does not happen, a process called the scrubber continuously reads every block that TernFS stores, and replaces blocks with bad sectors before they can cause too much damage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Snapshots and garbage collection&lt;/head&gt;
    &lt;p&gt;We've talked at length about what TernFS does to try to prevent data loss due to hardware failure or bugs in clients. However, the most common type of data loss is due to human error ‚Äî the &lt;code&gt;rm ‚Äîrf / home/alice/notes.txt&lt;/code&gt; scenario.&lt;/p&gt;
    &lt;p&gt;To protect against these scenarios, TernFS implements a lightweight snapshotting system. When files or directories are deleted, their contents aren't actually deleted. Instead, a weak reference to them is created. We call such weak references snapshot directory entries.&lt;/p&gt;
    &lt;p&gt;Snapshot entries are not be visible through the kernel module or the S3 gateway, but are visible through the direct API, and at XTX we have developed internal tooling to easily recover deleted files through it.[21] Deleted files are also visible through the TernFS web UI.&lt;/p&gt;
    &lt;p&gt;Given that 'normal' file operations do not delete files, but rather make them a snapshot, the task of freeing up space is delegated to an external Go process, the garbage collector. The garbage collector traverses the filesystem and removes expired snapshots, which involves deleting their blocks permanently. Snapshot expiry is predictably regulated by directory policies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping TernFS healthy&lt;/head&gt;
    &lt;p&gt;This last section covers how we (humans of XTX) notice problems in TernFS, and how TernFS self-heals when things go wrong ‚Äî both key topics if we want to ensure no data loss and notice performance problems early.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance metrics&lt;/head&gt;
    &lt;p&gt;TernFS exposes a plethora of performance metrics through the HTTP InfluxDB line protocol. While connecting TernFS to a service which ingests these metrics is optional, it is highly recommended for any production service.&lt;/p&gt;
    &lt;p&gt;Moreover, the kernel module exposes many performance metrics itself through DebugFS.&lt;/p&gt;
    &lt;p&gt;Both types of metrics, especially when used in tandem, have proved invaluable to resolve performance problems quickly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Logging and alerts&lt;/head&gt;
    &lt;p&gt;TernFS services log their output to files in a simple line-based format. The internal logging API is extremely simple and includes support for syslog levels out-of-the-box. At XTX we run TernFS as normal systemd services and use journalctl to view logs.&lt;/p&gt;
    &lt;p&gt;As with metrics, the kernel module includes various logging facilities as well. The first type of logging is just through dmesg, but the kernel module also includes numerous tracepoints for low-overhead opt-in logging of many operations.&lt;/p&gt;
    &lt;p&gt;TernFS is also integrated with XTX's internal alerting system, called XMon, to page on call developers when things go wrong. XMon is not open source, but all the alerts are also rendered as error lines in logs. [22] We plan to eventually move to having alerts feed off performance metrics, which would make them independent from XMon, although we don't have plans to do so in the short-term.&lt;/p&gt;
    &lt;head rend="h4"&gt;Migrations&lt;/head&gt;
    &lt;p&gt;Finally, there's the question of what to do when drives die ‚Äî and they will die, frequently, when you have 50,000 of them. While drives dying is not surprising, we've been surprised at the variety of different drive failures. [23] A malfunctioning drive might:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Produce IO errors when reading specific files. This is probably due to a single bad sector.&lt;/item&gt;
      &lt;item&gt;Produce IO errors when reading or writing anything. This might happen because enough bad sectors have gone bad and the drive cannot remap them, or for a variety of other reasons.&lt;/item&gt;
      &lt;item&gt;Return wrong data. This is usually caught by the built-in error correction codes in the hard drives, but not always.&lt;/item&gt;
      &lt;item&gt;Lie about data being successfully persisted. This can manifest in a variety of ways: file size being wrong on open, file contents being partially zero'd out, and so on.&lt;/item&gt;
      &lt;item&gt;Disappear from the mount list, only to reappear when the machine is rebooted, but missing some data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When clients fail to read from a drive, they'll automatically fall back on other drives to reconstruct the missing data, which is extremely effective in hiding failures from the end-user. That said, something needs to be done about the bad drives, and done quickly to avoid permanent data loss.&lt;/p&gt;
    &lt;p&gt;The TernFS registry allows marking drives as faulty. Faulty drives are then picked up by the migrator, a Go process which waits for bad drives and then stores all its blocks onto freshly picked block services.&lt;/p&gt;
    &lt;p&gt;TernFS also tries to mark drives as bad automatically using a simple heuristic based on the rate of IO errors the drive is experiencing. The number of drives automatically marked as faulty is throttled to avoid having this check go awry and mark the whole cluster as faulty, which would not be catastrophic but would still be messy to deal with.&lt;/p&gt;
    &lt;p&gt;Moreover, drives that are faulty in subtle ways might not be picked up by the heuristics, which means that occasionally a sysadmin will need to mark a drive as faulty manually, after which the migrator will evacuate them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing thoughts&lt;/head&gt;
    &lt;p&gt;At XTX we feel strongly about utilizing our resources efficiently. When it comes to software, this means having software that gets close to some theoretical optimum when it comes to total cost of ownership. This culture was borne out by competing hard for technological excellence when doing on-exchange trading at first, and by our ever-growing hardware costs as our business has grown later.&lt;/p&gt;
    &lt;p&gt;Such idealized tools might not exist or be available yet, in which case we're happy to be the tool makers. TernFS is a perfect example of this and we're excited to open source this component of our business for the community.&lt;/p&gt;
    &lt;p&gt;Crucially, the cost of implementation of a new solution is often overblown compared to the cost of tying yourself to an ill-fitting, expensive third-party solution. Designing and implementing a solution serving exactly your needs allows for much greater simplicity. If the requirements do change, as often happens, changes can be implemented very quickly, again only catering to your needs.&lt;/p&gt;
    &lt;p&gt;That said, we believe that TernFS' set of trade-offs are widely shared across many organizations dealing with large-scale storage workloads, and we hope we'll contribute to at least slowing down the seemingly constant stream of new filesystems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.xtxmarkets.com/tech/2025-ternfs/#posix-shaped"/><published>2025-10-20T17:36:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45646958</id><title>x86-64 Playground ‚Äì An online assembly editor and GDB-like debugger</title><updated>2025-10-20T22:09:23.854185+00:00</updated><content>&lt;doc fingerprint="aefedf1ae8255dda"&gt;
  &lt;main&gt;
    &lt;p&gt;x86-64 Playground is a web app for experimenting and learning x86-64 assembly.&lt;/p&gt;
    &lt;p&gt;The Playground web app provides an online code editor where you can write, compile, and share assembly code for a wide range of popular assemblers such as GNU As, Fasm and Nasm.&lt;/p&gt;
    &lt;p&gt;Unlike traditional onlide editors, this playground allows you to follow the execution of your program step by step, inspecting memory and registers of the running process from a GDB-like interface.&lt;/p&gt;
    &lt;p&gt;You can bring your own programs! Drag and drop into the app any x86-64-Linux static executable to run and debug it in the same sandboxed environment, without having to install anything.&lt;/p&gt;
    &lt;p&gt;The app is for anyone that wants to run amd64 assembly snippets or inspect the inner workings of simple Linux ELF files.&lt;/p&gt;
    &lt;p&gt;It has been designed with the academic world of binary exploitation in mind; The debugger interface offers visualizations similar to the GDB+PwnGDB debugger plugin, and all the controls are labelled with the respective GDB commands.&lt;/p&gt;
    &lt;p&gt;Combined with Compiler Explorer, this app provides a noise-free environment to learn the basics behind the inner workings of a Linux process. When you are ready, it includes the guides and resources necessary to keep experimenting on your own linux environment, with the actual GDB debugger.&lt;/p&gt;
    &lt;p&gt;Have you ever seen a responsive debugger? The app places the mobile experience at the center of its design, and can be embedded in any web page to add interactivity to technical tutorials or documentations.&lt;/p&gt;
    &lt;p&gt;Follow the guide to embed in your website both the asm editor and debugger.&lt;/p&gt;
    &lt;p&gt;The app is open-source, and available on Github. It's powered by the Blink Emulator, which emulates an x86-64-Linux environment entirely client side in your browser. This means that all the code you write, or the excutables you debug are never sent to the server.&lt;/p&gt;
    &lt;p&gt;everything runs in your browser, and once the Web App loads it will work without an internet connection.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://x64.halb.it/"/><published>2025-10-20T17:55:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45647133</id><title>Peanut allergies have plummeted in children</title><updated>2025-10-20T22:09:23.807994+00:00</updated><content/><link href="https://www.nytimes.com/2025/10/20/well/peanut-allergy-drop.html"/><published>2025-10-20T18:09:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45647166</id><title>Claude Code on the web</title><updated>2025-10-20T22:09:23.612313+00:00</updated><content>&lt;doc fingerprint="7f0c0efeffb1c2ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Claude Code on the web&lt;/head&gt;
    &lt;p&gt;Today, we're introducing Claude Code on the web, a new way to delegate coding tasks directly from your browser.&lt;/p&gt;
    &lt;p&gt;Now in beta as a research preview, you can assign multiple coding tasks to Claude that run on Anthropic-managed cloud infrastructure, perfect for tackling bug backlogs, routine fixes, or parallel development work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run coding tasks in parallel&lt;/head&gt;
    &lt;p&gt;Claude Code on the web lets you kick off coding sessions without opening your terminal. Connect your GitHub repositories, describe what you need, and Claude handles the implementation.&lt;/p&gt;
    &lt;p&gt;Each session runs in its own isolated environment with real-time progress tracking, and you can actively steer Claude to adjust course as it‚Äôs working through tasks.&lt;/p&gt;
    &lt;p&gt;With Claude Code running in the cloud, you can now run multiple tasks in parallel across different repositories from a single interface and ship faster with automatic PR creation and clear change summaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;Flexible for every workflow&lt;/head&gt;
    &lt;p&gt;The web interface complements your existing Claude Code workflow. Running tasks in the cloud is especially effective for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answering questions about how projects work and how repositories are mapped&lt;/item&gt;
      &lt;item&gt;Bugfixes and routine, well-defined tasks&lt;/item&gt;
      &lt;item&gt;Backend changes, where Claude Code can use test-driven development to verify changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also use Claude Code on mobile. As part of this research preview, we‚Äôre making Claude Code available on our iOS app so developers can explore coding with Claude on the go. It‚Äôs an early preview, and we hope to quickly refine the mobile experience based on your feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security-first cloud execution&lt;/head&gt;
    &lt;p&gt;Every Claude Code task runs in an isolated sandbox environment with network and filesystem restrictions. Git interactions are handled through a secure proxy service that ensures Claude can only access authorized repositories‚Äîhelping keep your code and credentials protected throughout the entire workflow.&lt;/p&gt;
    &lt;p&gt;You can also add custom network configuration to choose what domains Claude Code can connect to from its sandbox. For example, you can allow Claude to download npm packages over the internet so that it can run tests and validate changes.&lt;/p&gt;
    &lt;p&gt;Read our engineering blog and documentation for a deep dive on Claude Code‚Äôs sandboxing approach.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;Claude Code on the web is available now in research preview for Pro and Max users. Visit claude.com/code to connect your first repository and start delegating tasks.&lt;/p&gt;
    &lt;p&gt;Cloud-based sessions share rate limits with all other Claude Code usage. Explore our documentation to learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-code-on-the-web"/><published>2025-10-20T18:12:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45647853</id><title>First Self-Propagating Worm Using Invisible Code Hits OpenVSX and VS Code</title><updated>2025-10-20T22:09:23.321467+00:00</updated><content>&lt;doc fingerprint="953315118a238643"&gt;
  &lt;main&gt;
    &lt;p&gt;A month after Shai Hulud became the first self-propagating worm in the npm ecosystem, we just discovered the world's first worm targeting VS Code extensions on OpenVSX marketplace.&lt;/p&gt;
    &lt;p&gt;But GlassWorm isn't just another supply chain attack. It's using stealth techniques we've never seen before in the wild - invisible Unicode characters that make malicious code literally disappear from code editors. Combine that with blockchain-based C2 infrastructure that can't be taken down, Google Calendar as a backup command server, and a full remote access trojan that turns every infected developer into a criminal proxy node.&lt;/p&gt;
    &lt;p&gt;This is one of the most sophisticated supply chain attacks we've ever analyzed. And it's spreading right now.&lt;/p&gt;
    &lt;p&gt;What GlassWorm does to infected systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Harvests NPM, GitHub, and Git credentials for supply chain propagation&lt;/item&gt;
      &lt;item&gt;Targets 49 different cryptocurrency wallet extensions to drain funds&lt;/item&gt;
      &lt;item&gt;Deploys SOCKS proxy servers, turning developer machines into criminal infrastructure&lt;/item&gt;
      &lt;item&gt;Installs hidden VNC servers for complete remote access&lt;/item&gt;
      &lt;item&gt;Uses stolen credentials to compromise additional packages and extensions, spreading the worm further&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The current state: Seven OpenVSX extensions compromised on October 17, 2025. Total downloads -35,800. Ten extensions still actively distributing malware as you read this. The attacker's C2 infrastructure is fully operational - payload servers are responding, and stolen credentials are being used to compromise additional packages.&lt;/p&gt;
    &lt;p&gt;Update (Oct 19, 2025): A new infected extension detected in Microsoft's VSCode marketplace - still active.&lt;/p&gt;
    &lt;p&gt;The attack went live yesterday. The infrastructure is active. The worm is spreading.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Our Risk Engine Detected&lt;/head&gt;
    &lt;p&gt;Here's how this whole thing started. Our risk engine at Koi flagged an OpenVSX extension called CodeJoy when version 1.8.3 introduced some suspicious behavioral changes. When our researchers dug into it - like we do with any malware our risk engine flags - what we found was very disturbing.&lt;/p&gt;
    &lt;p&gt;CodeJoy looked legitimate. A developer productivity tool with hundreds of downloads, regular updates, seemingly normal code. But our risk engine caught something that human code review would miss entirely: suspicious network connections and credential access patterns that had nothing to do with the extension's productivity features&lt;/p&gt;
    &lt;p&gt;So we opened up the source code to take a closer look.&lt;/p&gt;
    &lt;p&gt;And that's when we saw it. Or rather, didn't see it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Invisible Attack: Unicode Stealth Technique&lt;/head&gt;
    &lt;p&gt;Look at this screenshot of the CodeJoy extension's source code:&lt;/p&gt;
    &lt;p&gt;See that massive gap between lines 2 and 7? That's not empty space. That's malicious code. Encoded in unprintable Unicode characters that literally don't render in your code editor.&lt;/p&gt;
    &lt;p&gt;Let me say that again: the malware is invisible. Not obfuscated. Not hidden in a minified file. Actually invisible to the human eye.&lt;/p&gt;
    &lt;p&gt;The attacker used Unicode variation selectors - special characters that are part of the Unicode specification but don't produce any visual output. To a developer doing code review, it looks like blank lines or whitespace. To static analysis tools scanning for suspicious code, it looks like nothing at all. But to the JavaScript interpreter? It's executable code.&lt;/p&gt;
    &lt;p&gt;This is why we call it GlassWorm. Like glass, it's completely transparent. You can stare right at it and see nothing. The developer whose account got compromised probably looked at this file, saw what appeared to be their legitimate code, and had no idea they were about to distribute malware to hundreds of users.&lt;/p&gt;
    &lt;p&gt;Here's the thing - this technique completely breaks traditional code review. You can't spot what you can't see. GitHub's diff view? Shows nothing suspicious. Your IDE's syntax highlighting? All clear. Manual code inspection? Everything looks normal.&lt;/p&gt;
    &lt;p&gt;The invisible code technique isn't just clever - it's a fundamental break in our security model. We've built entire systems around the assumption that humans can review code. GlassWorm just proved that assumption wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stage 2: The Unkillable C2 - Solana Blockchain&lt;/head&gt;
    &lt;p&gt;So we decoded the invisible Unicode characters. What do we find inside? Another stage of sophistication that honestly made our jaws drop.&lt;/p&gt;
    &lt;p&gt;The malware uses the Solana blockchain as its command and control infrastructure.&lt;/p&gt;
    &lt;p&gt;Read that again. The attacker is using a public blockchain - immutable, decentralized, impossible to take down - as their C2 server.&lt;/p&gt;
    &lt;p&gt;Here's how it works:&lt;/p&gt;
    &lt;p&gt;The malware searches the Solana blockchain for transactions from the hardcoded wallet address. When it finds a transaction, it reads the memo field - a place where you can attach arbitrary text to blockchain transactions. Inside that memo? A JSON object with a base64-encoded link to download the next stage.&lt;/p&gt;
    &lt;p&gt;Look at that screenshot. That's a real Solana transaction from October 15, 2025 - three days ago. The instruction data contains: &lt;code&gt;{"link":"aHR0cDovLzIxNy42OS4zLjIxOC9xUUQlMkZKb2kzV0NXU2s4Z2dHSGlTdg=="}&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;That base64 string decodes to: &lt;code&gt;http://217.69.3.218/qQD%2FJoi3WCWSk8ggGHiTdg%3D%3D&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;And just like that, the malware knows where to download its next payload.&lt;/p&gt;
    &lt;p&gt;Why this is absolutely brilliant (and terrifying):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;√¢Immutable: Once a transaction is on the blockchain, it can't be modified or deleted. Ever. No takedown requests. No domain seizures. It's there forever.√¢&lt;/item&gt;
      &lt;item&gt;Anonymous: Crypto wallets are pseudonymous. Good luck tracing this back to a real person.√¢&lt;/item&gt;
      &lt;item&gt;Censorship-resistant: There's no hosting provider to contact, no registrar to pressure, no infrastructure to shut down. The Solana blockchain just... exists.√¢&lt;/item&gt;
      &lt;item&gt;Legitimate traffic: Connections to Solana RPC nodes look completely normal. Security tools won't flag it.√¢&lt;/item&gt;
      &lt;item&gt;Dynamic and cheap: Want to update your payload? Just post a new transaction. Cost? 0.000005 SOL - less than a penny. The attacker can rotate infrastructure as often as they want for pocket change.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even if you identify and block the payload URL (&lt;code&gt;217.69.3.218&lt;/code&gt; in this case), the attacker just posts a new transaction with a different URL, and all infected extensions automatically fetch the new location. You're playing whack-a-mole with an opponent who has infinite moles and infinite mallets.&lt;/p&gt;
    &lt;p&gt;This isn't some theoretical attack vector. This is a real-world, production-ready C2 infrastructure that's actively serving malware right now. And there's literally no way to take it down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stage 3: The Credential Harvest&lt;/head&gt;
    &lt;p&gt;The Solana transaction points to an IP address: &lt;code&gt;217.69.3.218&lt;/code&gt;. We fetch the URL and get back a massive base64 payload. But it's encrypted. AES-256-CBC encryption with a key I don't have.&lt;/p&gt;
    &lt;p&gt;So where's the decryption key?&lt;/p&gt;
    &lt;p&gt;In the HTTP response headers.&lt;/p&gt;
    &lt;p&gt;The attacker is dynamically generating encryption keys per request and passing them in custom HTTP headers. Smart - even if you intercept the encrypted payload, you need to make a fresh request to get the current keys.&lt;/p&gt;
    &lt;p&gt;We decrypted the payload and started analyzing what it does. This is where GlassWorm's true purpose becomes clear.&lt;/p&gt;
    &lt;p&gt;The malware is hunting for credentials:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NPM authentication tokens - to publish malicious packages&lt;/item&gt;
      &lt;item&gt;GitHub tokens - to compromise repositories&lt;/item&gt;
      &lt;item&gt;OpenVSX credentials - to inject more extensions&lt;/item&gt;
      &lt;item&gt;Git credentials - to push malicious code&lt;/item&gt;
      &lt;item&gt;49 different cryptocurrency wallet extensions - targeting MetaMask, Phantom, Coinbase Wallet, and dozens more&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But wait, there's more. Buried in the code, we found something else: a Google Calendar link.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;https://calendar.app.google/M2ZCvM8ULL56PD1d6&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The malware reaches out to this Google Calendar event as a backup C2 mechanism. And guess what's in the event title? Another base64-encoded URL pointing to yet another encrypted payload.&lt;/p&gt;
    &lt;p&gt;The attacker created a Google Calendar event with the title: &lt;code&gt;aHR0cDovLzIxNy42OS4zLjIxOC9nZXRfem9tYmlfcGF5bG9hZC9xUUQlMkZKb2kzV0NXU2s4Z2dHSGlUdg==&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;That decodes to: &lt;code&gt;http://217.69.3.218/get_zombi_payload/qQD%2FJoi3WCWSk8ggGHiTdg%3D%3D&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Notice the path: &lt;code&gt;/get_zombi_payload/&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Yeah. "Zombi" as in zombie botnet. The attacker is literally naming their endpoints after what they're turning victims into.&lt;/p&gt;
    &lt;p&gt;Why use Google Calendar as backup C2?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Free and legitimate (no one's blocking Google Calendar)&lt;/item&gt;
      &lt;item&gt;Can be updated anytime by editing the event&lt;/item&gt;
      &lt;item&gt;Completely bypasses security controls&lt;/item&gt;
      &lt;item&gt;Another unkillable infrastructure piece&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So now we have a triple-layer C2 system:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solana blockchain (primary, immutable)&lt;/item&gt;
      &lt;item&gt;Direct IP connection (217.69.3.218)&lt;/item&gt;
      &lt;item&gt;Google Calendar (backup, legitimate service)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If one gets blocked, the others keep working. And all three are nearly impossible to take down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stage 4: ZOMBI - The Nightmare Reveal&lt;/head&gt;
    &lt;p&gt;We fetch the "zombi_payload" URL, capture the encryption keys from the headers, decrypt it, and start deobfuscating what turns out to be a massively obfuscated JavaScript payload.&lt;/p&gt;
    &lt;p&gt;And that's when we realized: this isn't just credential theft. This is a full-spectrum remote access trojan.&lt;/p&gt;
    &lt;p&gt;GlassWorm's final stage - the ZOMBI module - transforms every infected developer workstation into a node in a criminal infrastructure network. Let me break down what this thing can do, because it's honestly one of the most sophisticated pieces of malware we've analyzed.&lt;/p&gt;
    &lt;head rend="h3"&gt;SOCKS Proxy - Your Machine Becomes Criminal Infrastructure&lt;/head&gt;
    &lt;p&gt;The ZOMBI module can turn your computer into a SOCKS proxy server. Here's the code:&lt;/p&gt;
    &lt;p&gt;Your developer workstation - the one sitting inside your corporate network, behind all your firewalls and security controls - just became a proxy node for criminal activity.&lt;/p&gt;
    &lt;p&gt;Why this is devastating:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Corporate network access: Your machine can reach internal systems that external attackers can't&lt;/item&gt;
      &lt;item&gt;Attack anonymization: Attackers route their traffic through your IP, not theirs&lt;/item&gt;
      &lt;item&gt;Firewall bypass: Internal machines can access resources external proxies can't reach&lt;/item&gt;
      &lt;item&gt;Free infrastructure: Why pay for proxy servers when victims provide them?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every single infected developer becomes a node in a global proxy network. And you won't even know it's happening.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebRTC P2P - Direct Peer-to-Peer Control&lt;/head&gt;
    &lt;p&gt;ZOMBI downloads and deploys WebRTC modules for peer-to-peer communication:&lt;/p&gt;
    &lt;p&gt;WebRTC enables direct peer-to-peer connections that bypass traditional firewalls through NAT traversal. The attacker can establish real-time, direct control channels to infected machines without going through any central server.&lt;/p&gt;
    &lt;head rend="h3"&gt;BitTorrent DHT - Decentralized Command Distribution&lt;/head&gt;
    &lt;p&gt;ZOMBI uses BitTorrent's Distributed Hash Table (DHT) network for command distribution:&lt;/p&gt;
    &lt;p&gt;Commands are distributed through the BitTorrent DHT network - the same decentralized system that makes torrent tracking impossible to shut down. There's no central C2 server to take offline. Commands propagate through a distributed network of millions of nodes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hidden VNC (HVNC) - Complete Invisible Remote Control&lt;/head&gt;
    &lt;p&gt;And here's the truly terrifying part - HVNC (Hidden Virtual Network Computing):&lt;/p&gt;
    &lt;p&gt;HVNC gives the attacker complete remote desktop access to your machine - but it's hidden. It runs in a virtual desktop that doesn't appear in Task Manager, doesn't show any windows on your screen, and operates completely invisibly.&lt;/p&gt;
    &lt;p&gt;The attacker can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use your browser with your logged-in sessions&lt;/item&gt;
      &lt;item&gt;Access your email, Slack, internal tools&lt;/item&gt;
      &lt;item&gt;Read your source code&lt;/item&gt;
      &lt;item&gt;Steal additional credentials&lt;/item&gt;
      &lt;item&gt;Pivot to other systems on your network&lt;/item&gt;
      &lt;item&gt;Do literally anything you could do - but you'll never see it happening&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The Full Picture&lt;/head&gt;
    &lt;p&gt;ZOMBI isn't just malware. It's a complete remote access and network penetration toolkit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SOCKS proxy for routing attacks through victim networks&lt;/item&gt;
      &lt;item&gt;WebRTC P2P for direct, firewall-bypassing control&lt;/item&gt;
      &lt;item&gt;BitTorrent DHT for unkillable command distribution&lt;/item&gt;
      &lt;item&gt;HVNC for invisible remote desktop access&lt;/item&gt;
      &lt;item&gt;Automatic restart on any failure (it won't go away)&lt;/item&gt;
      &lt;item&gt;Modular architecture supporting dynamic capability updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For enterprises, this is a nightmare scenario. An infected developer workstation becomes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An internal network access point&lt;/item&gt;
      &lt;item&gt;A persistent backdoor&lt;/item&gt;
      &lt;item&gt;A proxy for attacking other internal systems&lt;/item&gt;
      &lt;item&gt;An exfiltration channel for sensitive data&lt;/item&gt;
      &lt;item&gt;A command and control relay point&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And it all started with an invisible Unicode character in a VS Code extension.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Worm Spreads: Self-Propagation Through Stolen Credentials&lt;/head&gt;
    &lt;p&gt;Here's where GlassWorm earns the "Worm" part of its name.&lt;/p&gt;
    &lt;p&gt;Remember all those credentials it's stealing? NPM tokens, GitHub credentials, OpenVSX access? Those aren't just for data theft. They're for propagation.&lt;/p&gt;
    &lt;p&gt;The self-replication cycle:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Initial infection - Compromised developer account pushes malicious code to legitimate extension&lt;/item&gt;
      &lt;item&gt;Invisible payload - Unicode-hidden malware executes on victim machines&lt;/item&gt;
      &lt;item&gt;Credential harvest - Steals NPM, GitHub, OpenVSX, Git credentials&lt;/item&gt;
      &lt;item&gt;Automated spread - Uses stolen credentials to compromise MORE packages and extensions&lt;/item&gt;
      &lt;item&gt;Exponential growth - Each new victim becomes an infection vector&lt;/item&gt;
      &lt;item&gt;Repeat - The cycle continues automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This isn't a one-off supply chain attack. It's a worm designed to spread through the developer ecosystem like wildfire.&lt;/p&gt;
    &lt;p&gt;Just one month ago, the security community witnessed Shai Hulud - the first successful self-propagating worm in the npm ecosystem. That campaign compromised over 100 packages by stealing npm tokens and automatically publishing malicious versions.&lt;/p&gt;
    &lt;p&gt;GlassWorm brings this same technique to OpenVSX, but with terrifying evolutions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Invisible code injection that bypasses all code review&lt;/item&gt;
      &lt;item&gt;Blockchain-based C2 that can't be taken down&lt;/item&gt;
      &lt;item&gt;Full RAT capabilities turning victims into criminal infrastructure&lt;/item&gt;
      &lt;item&gt;Multi-layered redundancy across three different C2 mechanisms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The pattern is clear. Attackers have figured out how to make supply chain malware self-sustaining. They're not just compromising individual packages anymore - they're building worms that can spread autonomously through the entire software development ecosystem.&lt;/p&gt;
    &lt;p&gt;With traditional supply chain attacks, you compromise one package and that's your blast radius. With worms like Shai Hulud and GlassWorm, each infection is a new launching point for dozens more. It's exponential growth. And we're just starting to see what that looks like.&lt;/p&gt;
    &lt;head rend="h2"&gt;Impact: 35,800 Victims, Active RIGHT NOW&lt;/head&gt;
    &lt;p&gt;Let's talk about the current state of this infection. Because this isn't some theoretical attack or historical incident. GlassWorm is active right now.&lt;/p&gt;
    &lt;p&gt;Attack Timeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;October 17, 2025: Seven OpenVSX extensions compromised (yesterday)&lt;/item&gt;
      &lt;item&gt;October 18, 2025: We detected and began analysis (today)&lt;/item&gt;
      &lt;item&gt;October 19, 2025: More compromised extensions detected in OpenVSX and VSCode marketplaces&lt;/item&gt;
      &lt;item&gt;Current status: Five extensions still actively distributing malware&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total impact: 35,800 installations&lt;/p&gt;
    &lt;p&gt;Here's what makes this particularly urgent: VS Code extensions auto-update. When CodeJoy pushed version 1.8.3 with invisible malware, everyone with CodeJoy installed got automatically updated to the infected version. No user interaction. No warning. Just silent, automatic infection.&lt;/p&gt;
    &lt;p&gt;And since the malware is invisible, the original developers whose accounts were compromised probably had no idea. They might have even reviewed the "empty" lines in their code and seen nothing wrong.&lt;/p&gt;
    &lt;p&gt;What's happening right now to infected systems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Credential theft in progress - NPM tokens, GitHub credentials, Git credentials being harvested&lt;/item&gt;
      &lt;item&gt;Cryptocurrency wallets being drained - 49 different wallet extensions targeted&lt;/item&gt;
      &lt;item&gt;SOCKS proxies deploying - Turning developer workstations into criminal infrastructure&lt;/item&gt;
      &lt;item&gt;HVNC installation - Hidden remote access being established&lt;/item&gt;
      &lt;item&gt;Network reconnaissance - Infected machines mapping internal corporate networks&lt;/item&gt;
      &lt;item&gt;Preparation for spread - Stolen credentials being validated for additional compromises&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The C2 infrastructure is fully operational:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;217.69.3.218 - Responding and serving encrypted payloads&lt;/item&gt;
      &lt;item&gt;Solana blockchain - Transaction active, pointing to payload servers&lt;/item&gt;
      &lt;item&gt;Google Calendar event - Live and accessible&lt;/item&gt;
      &lt;item&gt;Exfiltration server (140.82.52.31) - Collecting stolen data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is an active, ongoing compromise. Not a case study. Not a war story. This is happening right now, as you read this sentence.&lt;/p&gt;
    &lt;p&gt;If you have any of the infected extensions installed, you're compromised. Your credentials are likely stolen. Your crypto wallets may be drained. Your machine might already be serving as a SOCKS proxy for criminal activity. And you probably have no idea any of this is happening.&lt;/p&gt;
    &lt;p&gt;Two developers managed to push clean updates (vscode-theme-seti-folder and git-worktree-menu), suggesting they either regained access to their accounts or noticed something was wrong. But five extensions are still infected. Five developers who either don't know they're compromised or can't regain control of their accounts.&lt;/p&gt;
    &lt;p&gt;And remember: this is just what we've found so far. GlassWorm is designed to spread. Those stolen credentials are being used right now to compromise additional packages and extensions. The real victim count could be much higher.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;This writeup was authored by the research team at Koi Security, with a healthy dose of paranoia and hope for a safer open-source ecosystem.&lt;/p&gt;
    &lt;p&gt;GlassWorm shows how easy it is for malicious extensions to slip past marketplace security and compromise sensitive data. With Koi, security teams gain visibility, risk scoring, and governance across binary &amp;amp; non-binary software before it ever hits production.&lt;/p&gt;
    &lt;p&gt;Book a demo to see how Koi closes the gap that legacy tools miss.&lt;/p&gt;
    &lt;p&gt;For too long, the use of untrusted third-party code, often running with the highest privileges has flown under the radar for both enterprises and attackers. That era is ending. The tide is shifting. Just last month we uncovered another campaign of 18 featured and verified extensions that turned malicious and affected millions of users.&lt;/p&gt;
    &lt;p&gt;We√¢ve built Koi to meet this moment; for practitioners and enterprises alike. Our platform helps discover, assess, and govern everything your teams pull from marketplaces like the Chrome Web Store, VSCode, Hugging Face, Homebrew, GitHub, and beyond.&lt;/p&gt;
    &lt;p&gt;Trusted by Fortune 50 organizations, BFSIs and some of the largest tech companies in the world, Koi automates the security processes needed to gain visibility, establish governance, and proactively reduce risk across this sprawling attack surface.&lt;/p&gt;
    &lt;p&gt;Because in a world where malware can be literally invisible, paranoia isn't a bug - it's a feature.&lt;/p&gt;
    &lt;p&gt;Stay safe out there.&lt;/p&gt;
    &lt;head rend="h2"&gt;IOCs&lt;/head&gt;
    &lt;head rend="h3"&gt;Compromised Extensions&lt;/head&gt;
    &lt;p&gt;OpenVSX Extensions (with malicious versions):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;codejoy.codejoy-vscode-extension@1.8.3&lt;/item&gt;
      &lt;item&gt;codejoy.codejoy-vscode-extension@1.8.4&lt;/item&gt;
      &lt;item&gt;l-igh-t.vscode-theme-seti-folder@1.2.3&lt;/item&gt;
      &lt;item&gt;kleinesfilmroellchen.serenity-dsl-syntaxhighlight@0.3.2&lt;/item&gt;
      &lt;item&gt;JScearcy.rust-doc-viewer@4.2.1&lt;/item&gt;
      &lt;item&gt;SIRILMP.dark-theme-sm@3.11.4&lt;/item&gt;
      &lt;item&gt;CodeInKlingon.git-worktree-menu@1.0.9&lt;/item&gt;
      &lt;item&gt;CodeInKlingon.git-worktree-menu@1.0.91&lt;/item&gt;
      &lt;item&gt;ginfuru.better-nunjucks@0.3.2&lt;/item&gt;
      &lt;item&gt;ellacrity.recoil@0.7.4&lt;/item&gt;
      &lt;item&gt;grrrck.positron-plus-1-e@0.0.71&lt;/item&gt;
      &lt;item&gt;jeronimoekerdt.color-picker-universal@2.8.91&lt;/item&gt;
      &lt;item&gt;srcery-colors.srcery-colors@0.3.9&lt;/item&gt;
      &lt;item&gt;sissel.shopify-liquid@4.0.1&lt;/item&gt;
      &lt;item&gt;TretinV3.forts-api-extention@0.3.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;√¢Microsoft VSCode Extensions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;cline-ai-main.cline-ai-agent@3.1.3&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;√¢&lt;/p&gt;
    &lt;head rend="h3"&gt;Infrastructure&lt;/head&gt;
    &lt;p&gt;Command &amp;amp; Control:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;217.69.3.218&lt;/code&gt;(primary C2 server)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;140.82.52.31:80/wall&lt;/code&gt;(exfiltration endpoint)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Blockchain Infrastructure:&lt;/p&gt;
    &lt;p&gt;Solana Wallet: &lt;code&gt;28PKnu7RzizxBzFPoLp69HLXp9bJL3JFtT2s5QzHsEA2&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Transaction: &lt;code&gt;49CDiVWZpuSW1b2HpzweMgePNg15dckgmqrrmpihYXJMYRsZvumVtFsDim1keESPCrKcW2CzYjN3nSQDGG14KKFM&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Google Calendar C2:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;https://calendar.app.google/M2ZCvM8ULL56PD1d6&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Organizer: &lt;code&gt;uhjdclolkdn@gmail.com&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Payload URLs:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;http://217.69.3.218/qQD%2FJoi3WCWSk8ggGHiTdg%3D%3D&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;http://217.69.3.218/get_arhive_npm/&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;http://217.69.3.218/get_zombi_payload/qQD%2FJoi3WCWSk8ggGHiTdg%3D%3D&lt;/code&gt;
    &lt;/p&gt;
    &lt;head rend="h3"&gt;Registry Indicators&lt;/head&gt;
    &lt;p&gt;Persistence Mechanisms:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;HKCU\Software\Microsoft\Windows\CurrentVersion\Run&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;HKLM\Software\Microsoft\Windows\CurrentVersion\Run&lt;/code&gt;
    &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.koi.ai/blog/glassworm-first-self-propagating-worm-using-invisible-code-hits-openvsx-marketplace"/><published>2025-10-20T19:05:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45648249</id><title>When a stadium adds AI to everything, it's worse experience for everyone</title><updated>2025-10-20T22:09:19.625963+00:00</updated><content>&lt;doc fingerprint="73c993cedba5ceca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;BMO stadium in LA added AI to everything and what they got was a worse experience for everyone&lt;/head&gt;
    &lt;p&gt;I just got back from a 24hr trip to Los Angeles to catch my favorite Portland Thorns team, watching them clinch their playoff spot in a match at BMO stadium in downtown Los Angeles.&lt;/p&gt;
    &lt;p&gt;In May of 2024, I did the same trip to catch a match on Mother's Day, but I accidentally chose bad seats in the sun and it was hot and uncomfortable. Ultimately, it partially inspired my wife and I's book reviewing every NWSL soccer stadium so other fans wouldn't suffer the same fate when flying across the country to catch their favorite team.&lt;/p&gt;
    &lt;p&gt;This year, I got better seats in the shade and enjoyed the game. But overall? The experience of being in the stadium was worse a year later. After thinking about it on the flight home, I think the reason was the stadium's rush to automation and AI in several places.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spoiler alert: deploying camera/AI recognition for everything isn't great&lt;/head&gt;
    &lt;p&gt;Every concession stand, including the ones that didn't even serve hot food, used the apparatus in the photo above to control all checkouts. I assume these are expensive units, because most places that used to have several checkout lanes only had one of them, requiring everyone to checkout through a single location.&lt;/p&gt;
    &lt;p&gt;Here's how they worked in the stadium yesterday: You place all your items on the white shelf with some space between them. Although they were clearly designed to be a self-checkout experience, the stadium had a staff member rearrange your items, then for about 30 seconds the kiosk would be thinking. After, it would pop up all items on the menu, and the staff member would have to tap to confirm what each item was. Then another 30 seconds to calculate and move the purchase to a point of sale/tap on the side, then you'd pay.&lt;/p&gt;
    &lt;p&gt;Overall, this added at least one, if not two full minutes to every transaction that didn't normally have those delays. Lines were unbearably long, and it was a hot day in LA yesterday, at 87¬∫F/30¬∫C. I bought food and drinks several times over the the course of the day and had to endure the process multiple times.&lt;/p&gt;
    &lt;head rend="h2"&gt;When you add object recognition, you're incentivized to reduce choices&lt;/head&gt;
    &lt;p&gt;Here's an unintended consequence of moving all your concession stand checkouts to computer vision: it's easier if you have less things on offer.&lt;/p&gt;
    &lt;p&gt;Case in point: Let's talk about my favorite concession stand at BMO last year, a place that served rotisserie chicken with waffle fries and chicken sandwiches. Here's our meal from 2024, it was well-seasoned, came with great sauces, and was one of the best meals I had at a stadium in my entire nationwide tour, which is why I remembered it.&lt;/p&gt;
    &lt;p&gt;I returned to the same concession stand yesterday and here's their new menu:&lt;/p&gt;
    &lt;p&gt;When your checkout stand relies on computer vision, it's probably confusing to have half a dozen different menu items that fans can enjoy. But if you could condense it to just chicken tenders, fries, a hot dog, and boxes of candy, your computer vision-based checkout system will probably work faster since it has to do less work with the obvious shapes of each of those items.&lt;/p&gt;
    &lt;p&gt;Looking through my photos from my 2024 visit, I saw a variety of food options including smashburgers and a Korean BBQ rice bowl I also tried, pictured above.&lt;/p&gt;
    &lt;p&gt;If foods are difficult for computer vision to decipher, why not get rid of most options? Walking around the stadium yesterday, the menus were basically all hot dogs, pizza, nachos, and chicken tenders.&lt;/p&gt;
    &lt;head rend="h2"&gt;Even quick service options sucked&lt;/head&gt;
    &lt;p&gt;As I said, it was a hot day, I was constantly parched, and I ended up drinking four bottles of water over the course of three hours. Each time, I had to go through the automated checkout gauntlet, and each time it required a long wait in a line, while I missed bits of the match.&lt;/p&gt;
    &lt;p&gt;Late in the game, I wanted to get water quickly and they had these "vending kiosks" that were fully automated. You'd tap your phone on the locked door, it would unlock, you'd grab items, then close the door. Next, you had to stand there for about 2 minutes while it said "calculating checkout" before showing you a receipt on the screen.&lt;/p&gt;
    &lt;p&gt;What was supposed to be fast was very slow. The person in front of me bought two items and saw she got charged for three. Since there were no paper receipts, she took a photo of the machine before going to the guest services to complain. I missed ten minutes of the game getting water.&lt;/p&gt;
    &lt;p&gt;This was a quick service "market" style place and last year, you'd just grab stuff off a shelf, and checkout quickly from staff at multiple registers. This year, it had a long line snaking all over because of the slow AI/camera checkout kiosks.&lt;/p&gt;
    &lt;p&gt;It was a busy game, being the last home match for the fans and I would guess there were around 17,000-18,000 people in attendance. When it's nearly 90¬∫F/30¬∫C, heat exhaustion becomes a problem for crowds. When it takes people ten minutes to buy a bottle of water (I didn't see automated water fillers at the restrooms), the embrace of slow AI/Camera-based checkout systems starts to become a health and safety issue for the crowd.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Mrs. Lincoln‚Äîbesides the obvious‚Äîhow was the play?&lt;/head&gt;
    &lt;p&gt;A year later visiting the same stadium, I got worse food, slower service, and a worse overall experience. On the bright side, the billionaire stadium owners probably got to reduce their staff in the process while maybe increasing profits.&lt;/p&gt;
    &lt;p&gt;The company behind the kiosks claims they are 400% faster than human checkers and result in a 25% increase in profits. After experiencing it in person yesterday, I think those numbers are bullshit. Human checkers are clearly faster and smoother, and I bet they sold more food and drinks when people could get them quickly.&lt;/p&gt;
    &lt;p&gt;And the portions? They were so small!&lt;/p&gt;
    &lt;head rend="h3"&gt;Subscribe to get new posts in your inbox&lt;/head&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://a.wholelottanothing.org/bmo-stadium-in-la-added-ai-to-everything-and-what-they-got-was-a-worse-experience-for-everyone/"/><published>2025-10-20T19:38:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45648258</id><title>J.P. Morgan's OpenAI loan is strange</title><updated>2025-10-20T22:09:18.886906+00:00</updated><content>&lt;doc fingerprint="605191958864efb9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;J.P. Morgan's OpenAI loan is strange&lt;/head&gt;&lt;p&gt;In October, OpenAI secured a 4 billion dollar revolving credit facility from J.P. Morgan and several other banks. I was surprised when I heard this because OpenAI is a young company with no earnings. Shouldn't all their capital come from investors? Let's run some numbers.&lt;/p&gt;&lt;head rend="h2"&gt;From first principles&lt;/head&gt;&lt;p&gt;Let's do an Expected Value (EV) calculation, first from the perspective of an investor and then from the perspective of a lender. We'll pick some arbitrary parameters first, then refine.&lt;/p&gt;&lt;p&gt;Putting our investor hat on, the possible returns for investing $1,000 into OpenAI look like this:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Cost: $1,000&lt;/item&gt;&lt;item&gt;Case 1 (90%): OpenAI goes bankrupt. Return: $0&lt;/item&gt;&lt;item&gt;Case 2 (9%): OpenAI becomes a big successful company and goes 10x. Return: $10,000&lt;/item&gt;&lt;item&gt;Case 3 (1%): OpenAI becomes the big new thing and goes 100x. Return: $100,000&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Our expected value is:&lt;/p&gt;\[\begin{align} EV &amp;amp;= -1000 + 0.9 \times 0 + 0.09 \times 10000 + 0.01 \times 100000\\ EV &amp;amp;= -1000 + 0 + 900 + 1000\\ EV &amp;amp;= 900 \end{align}\]&lt;p&gt;The EV is positive, so this is a good investment. Obviously, there's a 90% chance of it going to zero, so if this were our only investment, it would be an insanely risky one. But provided we can do many investments like this and provided their failure cases aren't correlated, this would be a profitable strategy.&lt;/p&gt;&lt;p&gt;What happens if we instead put our lender hat on? Using the same probabilities as above, the possible returns for lending $1,000 to OpenAI at 5% interest look like this:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Cost: $1,000&lt;/item&gt;&lt;item&gt;Case 1 (90%): OpenAI goes bankrupt. Return: $0&lt;/item&gt;&lt;item&gt;Case 2 (9%): OpenAI becomes a big successful company and goes 10x. Return: $1,000 + 5% interest = $1,050&lt;/item&gt;&lt;item&gt;Case 3 (1%): OpenAI becomes the big new thing and goes 100x. Return: $1,000 + 5% interest = $1,050&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Lenders don't benefit directly from the success of the company. Whether it barely scrapes by but manages to repay the loan or becomes the greatest company ever and easily repays the loan, it's all the same to a lender. So, we can merge cases 2 and 3 into:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Case 2+3 (10%): OpenAI doesn't go bankrupt. Return: $1,000 + 5% interest = $1,050&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This makes our EV in the lending case:&lt;/p&gt;\[\begin{align} EV &amp;amp;= -1000 + 0.9 \times 0 + 0.1 \times 1050\\ EV &amp;amp;= -1000 + 0 + 105\\ EV &amp;amp;= -895 \end{align}\]&lt;p&gt;The EV is negative, so we'd end up losing most of our money on average. Lending on these terms doesn't make sense.&lt;/p&gt;&lt;p&gt;There are two numbers we made up in the above calculation: the probability of bankruptcy and the interest rate. Let's leave the interest rate fixed at 5% and see what the probability \(p\) would have to be for us to break even.&lt;/p&gt;\[\begin{align} EV &amp;amp;= -1000 + p \times 0 + (1 - p) \times 1050\\ EV &amp;amp;= -1000 + 1050 - p \times 1050 \\ EV &amp;amp;= 50 - p \times 1050 \\[0.5cm] &amp;amp; \text{Set EV to 0} \\[0.5cm] 0 &amp;amp;= 50 - p \times 1050 \\ p &amp;amp;= \frac{50}{1050} \\ p &amp;amp;= 0.0476 \end{align}\]&lt;p&gt;So, we'd break even if the probability of OpenAI going bankrupt was only about 5%. In practice, we'd want it to be lower than that so that we made a profit and so that we had a margin of safety in case our assumptions were wrong.&lt;/p&gt;&lt;p&gt;This 5% failure rate seems very optimistic to me, but this scenario is basically the one the consortium of banks got into. Concrete details on the deal are sparse, but this CIO Leaders article claims the interest rate was "SOFR + 100 basis points". The overnight SOFR rate is about 4.1% in October, so this puts OpenAI's interest at about 5%.&lt;/p&gt;&lt;head rend="h2"&gt;From market data&lt;/head&gt;&lt;p&gt;The problem with the above expected value calculation is that it's very idealized. The shape of it is correct, but the real world is too messy to be accurately represented by just a couple of parameters. I think it would be very difficult to build a model with enough predictive accuracy to be useful and I suspect there just isn't enough publicly available data to plug into it to make it work.&lt;/p&gt;&lt;p&gt;Luckily for us, banks exist! We know the banks have the better model and the non-public data and we know they came up with about 5% interest. So, let's work back from that and see what we can learn.&lt;/p&gt;&lt;p&gt;We're talking about a loan here and that's very similar to issuing bonds. So, we should be able to look at the bond market and find companies in similar financial health (from the perspective of a creditor). One problem is that we only know the overnight rate for OpenAI of about 5%, but bonds on the market will have longer maturities. We need to calculate what what yield a longer maturity loan would require and we can do that by looking at US treasuries.&lt;/p&gt;&lt;p&gt;According to Bloomberg, the three month treasuries have a yield of 3.94%. One year ones have a yield of 3.58%.&lt;/p&gt;&lt;p&gt;One way of thinking about corporate bonds is that they're basically treasury bonds plus some premium to account for the risk of default. This default spread seems to be about \(5\% - 3.94\% \approx 1\%\) in OpenAI's case. By this logic, OpenAI's one year debt would have a yield of about 4.6%.&lt;/p&gt;&lt;p&gt;Can we find some one year bonds with a yield of 4.6%?&lt;/p&gt;&lt;p&gt;Some bonds in the vicinity of what we're looking for are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;4.99%: HCA Inc. (US healthcare provider with credit rating BBB),&lt;/item&gt;&lt;item&gt;4.73%: Ziraat Katilim (Turkish bank with credit rating B+), and&lt;/item&gt;&lt;item&gt;4.24%: Citigroup (US bank with credit rating A).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In fact, scanning the sample above, it's mostly banks with BBB and A ratings. So, the consortium of big banks seems to have lent money to OpenAI at the kind of rates they themselves are borrowing at.&lt;/p&gt;&lt;p&gt;Looking at just a few bonds is interesting, but anecdotal. It would be better if we had some statistics across the whole bond market. Helpfully, Prof Damodaran goes through the exercise of calculating just such statistics (archive link) every year, most recently this January.&lt;/p&gt;&lt;p&gt;Looking up OpenAI's default spread of 1% in that table, we see it's at the level we'd expect for an A- or BBB firm (same as with the anecdotal search earlier). This normally corresponds to an interest coverage ratio of 3.00-4.24. However, OpenAI's actual interest coverage ratio is negative because their earnings before interest are negative.&lt;/p&gt;&lt;p&gt;This doesn't make sense: any way we look at it, OpenAI is getting the kind of interest rates only much more established and profitable firms would be getting. So, my initial surprise at hearing about this is justified, but there must be an explanation because the big banks wouldn't make such an obvious mistake.&lt;/p&gt;&lt;head rend="h2"&gt;Making this make sense&lt;/head&gt;&lt;p&gt;OpenAI is not a profitable company. It's also a private company, so we don't get to see audited financials, but we still know some things. This Reuters article claims OpenAI is going to generate $3.6 billion in revenue this year, but the costs will lead to a loss of more than $5 billion.&lt;/p&gt;&lt;p&gt;There's also speculation that their revenue next year will jump to $11.6 billion. However, there's no speculation about what their earnings will be because they're currently selling their services below cost and there isn't really any story as to how they'll turn this profitable.&lt;/p&gt;&lt;p&gt;The banks are lenders in this scenario, so they don't really care about how many users OpenAI gets or how huge their revenue becomes. As lenders, all they care about is getting paid back and it really doesn't seem like OpenAI will have the earnings to do that. But maybe earnings aren't what matters here.&lt;/p&gt;&lt;p&gt;If OpenAI can't pay its debts, it goes bankrupt and the creditors seize the company. Importantly, they seize it from the equity holders. Who are these equity holders? According to this Digital Information World article, the owners are Microsoft (28%), OpenAI non-profit and employees (52%), and other investors (20%).&lt;/p&gt;&lt;p&gt;So, the hypothetical is OpenAI runs out of money. They have revenue, but since their costs are higher, they don't actually have anything left over. They can't make interest payments on their debt, so they go bankrupt, and the banks seize the company from Microsoft. I don't think Microsoft will allow this to happen. Microsoft's earnings for last year were $88 billion, so I think Microsoft will just pay off OpenAI's $4 billion debt in this scenario. And I think the banks know all this.&lt;/p&gt;&lt;p&gt;So, the banks loaning money to OpenAI at an A- interest rate doesn't make sense, but effectively loaning the same to Microsoft with its AAA rating does, and that's what's actually happening here.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://marketunpack.com/j-p-morgans-openai-loan-is-strange/"/><published>2025-10-20T19:38:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45648266</id><title>iOS 26.1 lets users control Liquid Glass transparency</title><updated>2025-10-20T22:09:18.757027+00:00</updated><content>&lt;doc fingerprint="ffec0d95c92609c1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;iOS 26.1 Beta 4 Lets Users Control Liquid Glass Transparency with New Toggle&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;With the fourth betas of iOS 26.1, iPadOS 26.1, and macOS 26.1, Apple has introduced a new setting that's designed to allow users to customize the look of Liquid Glass.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;The toggle lets users select from a clear look for Liquid Glass, or a tinted look. Clear is the current Liquid Glass design, which is more transparent and shows the background underneath buttons, bars, and menus, while tinted increases the opacity of Liquid Glass and adds more contrast.&lt;/p&gt;
          &lt;p&gt;The new setting can be found on iOS and iPadOS by going to Settings &amp;gt; Display and Brightness, or System Settings &amp;gt; Appearance on the Mac.&lt;/p&gt;
          &lt;p&gt;Apple says that the new toggle was added because during the beta testing period over the summer, user feedback suggested that some people would prefer to have a more opaque option for Liquid Glass. The added setting provides additional customization in iOS 26.1, iPadOS 26.1, and macOS Tahoe 26.1. &lt;/p&gt;
          &lt;p&gt;Increasing opacity and adding contrast applies to Liquid Glass throughout the operating system, including in apps and Lock Screen notifications.&lt;/p&gt;
          &lt;p&gt;There are multiple other new features in iOS 26.1, including a new slide to stop feature for alarms and timers, new Apple Intelligence languages, a redesigned Apple TV app icon, changes to the Settings app, and more, with a full list of features available in our iOS 26.1 feature guide.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple plans to cut production of the iPhone Air amid underwhelming sales performance, Japan's Mizuho Securities believes (via The Elec). The Japanese investment banking and securities firm claims that the iPhone 17 Pro and iPhone 17 Pro Max are seeing higher sales than their predecessors during the same period last year, while the standard iPhone 17 is a major success, performing...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;iOS 26 was released last month, but the software train never stops, and iOS 26.1 beta testing is already underway. So far, iOS 26.1 makes both Apple Intelligence and Live Translation on compatible AirPods available in additional languages, and it includes some other minor changes across the Apple Music, Calendar, Photos, Clock, and Safari apps. More features and changes will follow in future ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple's software engineers continue to internally test iOS 26.0.2, according to MacRumors logs, which have been a reliable indicator of upcoming iOS versions. iOS 26.0.2 will be a minor update that addresses bugs and/or security vulnerabilities, but we do not know any specific details yet. The update will likely be released by the end of next week. Last month, Apple released iOS 26.0.1,...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple on Wednesday updated the 14-inch MacBook Pro, iPad Pro, and Vision Pro with its next-generation M5 chip, but previous rumors have indicated that the company still plans to announce at least a few additional products before the end of the year. The following Apple products have at one point been rumored to be updated in 2025, although it is unclear if the timeframe for any of them has...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;iOS 26.4 is expected to introduce a revamped version of Siri powered by Apple Intelligence, but not everyone is satisfied with how well it works. In his Power On newsletter today, Bloomberg's Mark Gurman said some of Apple's software engineers have "concerns" about the overhauled Siri's performance. However, he did not provide any specific details about the shortcomings. iOS 26.4 will...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;While the new iPad Pro's headline feature is the M5 chip, the device has some other changes, including N1 and C1X chips, faster storage speeds, and more. With the M5 chip, the new iPad Pro has up to a 20% faster CPU and up to a 40% faster GPU compared to the previous model with the M4 chip, according to Geekbench 6 results. Keep in mind that 256GB and 512GB configurations have a 9-core CPU,...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;With the fourth betas of iOS 26.1, iPadOS 26.1, and macOS 26.1, Apple has introduced a new setting that's designed to allow users to customize the look of Liquid Glass. The toggle lets users select from a clear look for Liquid Glass, or a tinted look. Clear is the current Liquid Glass design, which is more transparent and shows the background underneath buttons, bars, and menus, while tinted ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple on Wednesday updated the 14-inch MacBook Pro base model with an M5 chip, and there are two key storage-related upgrades beyond that chip bump. First, Apple says the new 14-inch MacBook Pro offers up to 2√ó faster SSD performance than the equivalent previous-generation model, so read and write speeds should get a significant boost. Apple says it is using "the latest storage technology," ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple plans to launch MacBook Air models equipped with the new M5 chip in spring 2026, according to Bloomberg's Mark Gurman. Apple is also working on M5 Pro and M5 Max MacBook Pro models that will come early in the year. Neither the MacBook Pro models nor the MacBook Air models are expected to get design changes, with Apple focusing on simple chip upgrades. In the case of the MacBook Pro, a m...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/"/><published>2025-10-20T19:39:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45648726</id><title>It Kind of Seems Like Peter Thiel Is Losing It</title><updated>2025-10-20T22:09:18.615873+00:00</updated><content>&lt;doc fingerprint="1b15c35b08972bd7"&gt;
  &lt;main&gt;
    &lt;p&gt;PayPal and Palantir cofounder Peter Thiel has long held outrageous ideological views ‚Äî and imposed them on the world using his billions of dollars in wealth to do things like build ICE‚Äôs massive surveillance panopticon, support the rise of the ‚ÄúNew Right,‚Äù and destroying Gawker Media by secretly funding Hulk Hogan‚Äôs lawsuit against it.&lt;/p&gt;
    &lt;p&gt;His beliefs during that period ‚Äî an explosive mixture of race science, libertarianism, and rugged capitalism ‚Äî might strike many people as noxious, but at least they followed the internal logic of an extremely powerful tech billionaire. Now, though, leaked information is making it sound as though he‚Äôs actually losing his grip.&lt;/p&gt;
    &lt;p&gt;For years, rumors have swirled that Thiel has developed some kind of obsession with the ‚ÄúAntichrist,‚Äù an entity referenced in the Bible‚Äôs Book of John and Thessalonians who will supposedly arise during the end times and claim to be God, before ultimately being defeated by Jesus.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre wondering whether Thiel is looking at the legend of the Antichrist as some kind of high-minded metaphor for his political convictions, The Guardian obtained audio this month of Thiel talking about the topic during a recent private appearance ‚Äî and unfortunately, it‚Äôs immediately clear that he believes that this mythical figure from the pages of an ancient holy book is a literal entity who will walk the Earth during some kind of hellish apocalypse.&lt;/p&gt;
    &lt;p&gt;‚ÄúSome people think of [the Antichrist] as a type of very bad person,‚Äù Thiel clarified during his remarks. ‚ÄúSometimes it‚Äôs used more generally as a spiritual descriptor of the forces of evil. What I will focus on is the most common and most dramatic interpretation of Antichrist: an evil king or tyrant or anti-messiah who appears in the end times.‚Äù&lt;/p&gt;
    &lt;p&gt;In fact, Thiel said during the leaked lecture that he‚Äôs suspicious the Antichrist is already among us. He even mentioned some possible suspects: it could be someone like climate activist Greta Thunberg, he suggested, or AI critic Eliezer Yudkowsky ‚Äî both of whom just happen to be his ideological opponents.&lt;/p&gt;
    &lt;p&gt;There‚Äôs something mortifying about this spectacle: a grown man, supposedly a paragon of rational thinking, scratching around in primeval religious texts to terrify himself with superstitious prophecies, which the people around him entertain because he‚Äôs wealthy and powerful. There‚Äôs an old expression, often credited to the Newbery Medal-winning children‚Äôs author Ellen Raskin: ‚Äúthe poor are crazy, the rich just eccentric.‚Äù&lt;/p&gt;
    &lt;p&gt;There are even moments when it sounds as though Thiel might be indulging in something akin to the disordered thinking associated with serious mental illness, making wild associations between numbers and concepts that defy logical thinking ‚Äî outbursts that would prompt talk of psychiatric intervention if a friend or loved one were to express them. (Preoccupation with the Antichrist can also be a symptom of severe mental illness.)&lt;/p&gt;
    &lt;p&gt;‚ÄúChrist only lived to age 33 and became history‚Äôs greatest man,‚Äù Thiel pondered at one point during the leaked audio. ‚ÄúThe Antichrist has to somehow outdo this. I don‚Äôt want to be way too literal on the 33 number ‚Äî I‚Äôd rather stress the Antichrist will be a youthful conqueror; maybe in our gerontocracy, 66 is the new 33. But something like these numbers do occur almost mystically through a number of different contexts.‚Äù&lt;/p&gt;
    &lt;p&gt;In a sense, it‚Äôs easy to empathize with someone like Thiel: able to afford any material pleasure he wants, likely long isolated from any real friends, and surrounded by employees effectively paid to agree that everything he utters is genius, even if it makes no sense at all.&lt;/p&gt;
    &lt;p&gt;Thiel, though, happens to have immense sway over the lives of people around the world via his various foundations, corporations, and political connections. In that sense, whatever‚Äôs going on with him isn‚Äôt just sad ‚Äî for someone as powerful as he is, it‚Äôs scary.&lt;/p&gt;
    &lt;p&gt;Perhaps the most ludicrous part of Thiel‚Äôs strange talk about the Antichrist is that there‚Äôs an obvious rejoinder to it: wouldn‚Äôt an actual Antichrist who was bent on world domination eagerly embrace the surveillance and military tech that Thiel is so heavily invested in? For that matter, doesn‚Äôt the Book of Revelations say that the enemy of God will seek to control commerce, much like Thiel made his fortune doing at PayPal?&lt;/p&gt;
    &lt;p&gt;In an agonizing exchange early this summer, the New York Times‚Äò Ross Douthat posed that exact question.&lt;/p&gt;
    &lt;p&gt;‚ÄúYou‚Äôre an investor in AI,‚Äù Douthat says. ‚ÄúYou‚Äôre deeply invested in Palantir, in military technology, in technologies of surveillance and technologies of warfare and so on. And it just seems to me that when you tell me a story about the Antichrist coming to power and using the fear of technological change to impose order on the world, I feel like that Antichrist would maybe be using the tools that you are building‚Ä¶ Isn‚Äôt that a concern? Wouldn‚Äôt that be the irony of history, that the man publicly worrying about the Antichrist accidentally hastens his or her arrival?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúLook, there are all these different scenarios,‚Äù Thiel sputtered, seemingly caught off guard by the question. ‚ÄúI obviously don‚Äôt think that that‚Äôs what I‚Äôm doing.‚Äù&lt;/p&gt;
    &lt;p&gt;But of course, that‚Äôs exactly what an Antichrist would say.&lt;/p&gt;
    &lt;p&gt;More on Peter Thiel: Tech Billionaires Accused of Quietly Working to Implement ‚ÄúCorporate Dictatorship‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://futurism.com/future-society/peter-thiel-antichrist-lectures"/><published>2025-10-20T20:14:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45649178</id><title>Today is when the Amazon brain drain sent AWS down the spout</title><updated>2025-10-20T22:09:18.376000+00:00</updated><content>&lt;doc fingerprint="3d28b35914248ecf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Today is when the Amazon brain drain finally sent AWS down the spout&lt;/head&gt;
    &lt;head rend="h2"&gt;When your best engineers log off for good, don‚Äôt be surprised when the cloud forgets how DNS works&lt;/head&gt;
    &lt;p&gt;column "It's always DNS" is a long-standing sysadmin saw, and with good reason: a disproportionate number of outages are at their heart DNS issues. And so today, as AWS is still repairing its downed cloud as this article goes to press, it becomes clear that the culprit is once again DNS. But if you or I know this, AWS certainly does.&lt;/p&gt;
    &lt;p&gt;And so, a quiet suspicion starts to circulate: where have the senior AWS engineers who've been to this dance before gone? And the answer increasingly is that they've left the building ‚Äî taking decades of hard-won institutional knowledge about how AWS's systems work at scale right along with them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What happened?&lt;/head&gt;
    &lt;p&gt;AWS reports that on October 20, at 12:11 AM PDT, it began investigating ‚Äúincreased error rates and latencies for multiple AWS services in the US-EAST-1 Region.‚Äù About an hour later, at 1:26 AM, the company confirmed ‚Äúsignificant error rates for requests made to the DynamoDB endpoint‚Äù in that region. By 2:01 AM, engineers had identified DNS resolution of the DynamoDB API endpoint for US-EAST-1 as the likely root cause, which led to cascading failures for most other things in that region. DynamoDB is a "foundational service" upon which a whole mess of other AWS services rely, so the blast radius for an outage touching this thing can be huge.&lt;/p&gt;
    &lt;p&gt;As a result, much of the internet stopped working: banking, gaming, social media, government services, buying things I don't need on Amazon.com itself, etc.&lt;/p&gt;
    &lt;p&gt;AWS has given increasing levels of detail, as is their tradition, when outages strike, and as new information comes to light. Reading through it, one really gets the sense that it took them 75 minutes to go from "things are breaking" to "we've narrowed it down to a single service endpoint, but are still researching," which is something of a bitter pill to swallow. To be clear: I've seen zero signs that this stems from a lack of transparency, and every indication that they legitimately did not know what was breaking for a patently absurd length of time.&lt;/p&gt;
    &lt;p&gt;Note that for those 75 minutes, visitors to the AWS status page (reasonably wondering why their websites and other workloads had just burned down and crashed into the sea) were met with an "all is well!" default response. Ah well, it's not as if AWS had previously called out slow outage notification times as an area for improvement. Multiple times even. We can keep doing this if you'd like.&lt;/p&gt;
    &lt;head rend="h3"&gt;The prophecy&lt;/head&gt;
    &lt;p&gt;AWS is very, very good at infrastructure. You can tell this is a true statement by the fact that a single one of their 38 regions going down (albeit a very important region!) causes this kind of attention, as opposed to it being "just another Monday outage." At AWS's scale, all of their issues are complex; this isn't going to be a simple issue that someone should have caught, just because they've already hit similar issues years ago and ironed out the kinks in their resilience story.&lt;/p&gt;
    &lt;p&gt;Once you reach a certain point of scale, there are no simple problems left. What's more concerning to me is the way it seems AWS has been flailing all day trying to run this one to ground. Suddenly, I'm reminded of something I had tried very hard to forget.&lt;/p&gt;
    &lt;p&gt;At the end of 2023, Justin Garrison left AWS and roasted them on his way out the door. He stated that AWS had seen an increase in Large Scale Events (or LSEs), and predicted significant outages in 2024. It would seem that he discounted the power of inertia, but the pace of senior AWS departures certainly hasn't slowed ‚Äî and now, with an outage like this, one is forced to wonder whether those departures are themselves a contributing factor.&lt;/p&gt;
    &lt;p&gt;You can hire a bunch of very smart people who will explain how DNS works at a deep technical level (or you can hire me, who will incorrect you by explaining that it's a database), but the one thing you can't hire for is the person who remembers that when DNS starts getting wonky, check that seemingly unrelated system in the corner, because it's historically played a contributing role to some outages of yesteryear.&lt;/p&gt;
    &lt;p&gt;When that tribal knowledge departs, you're left having to reinvent an awful lot of in-house expertise that didn't want to participate in your RTO games, or play Layoff Roulette yet again this cycle. This doesn't impact your service reliability ‚Äî until one day it very much does, in spectacular fashion. I suspect that day is today.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AWS outage exposes Achilles heel: central control plane&lt;/item&gt;
      &lt;item&gt;Major AWS outage across US-East region breaks half the internet&lt;/item&gt;
      &lt;item&gt;Amazon spills plan to nuke Washington...with X-Energy mini-reactors&lt;/item&gt;
      &lt;item&gt;Amazon turns James Bond into the Man Without the Golden Gun&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The talent drain evidence&lt;/head&gt;
    &lt;p&gt;This is The Register, a respected journalistic outlet. As a result, I know that if I publish this piece as it stands now, an AWS PR flak will appear as if by magic, waving their hands, insisting that "there is no talent exodus at AWS," a la Baghdad Bob. Therefore, let me forestall that time-wasting enterprise with some data.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is a fact that there have been 27,000+ Amazonians impacted by layoffs between 2022 and 2024, continuing into 2025. It's hard to know how many of these were AWS versus other parts of its Amazon parent, because the company is notoriously tight-lipped about staffing issues.&lt;/item&gt;
      &lt;item&gt;Internal documents reportedly say that Amazon suffers from 69 percent to 81 percent regretted attrition across all employment levels. In other words, "people quitting who we wish didn't."&lt;/item&gt;
      &lt;item&gt;The internet is full of anecdata of senior Amazonians lamenting the hamfisted approach of their Return to Office initiative; experts have weighed in citing similar concerns.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you were one of the early employees who built these systems, the world is your oyster. There's little reason to remain at a company that increasingly demonstrates apparent disdain for your expertise.&lt;/p&gt;
    &lt;head rend="h3"&gt;My take&lt;/head&gt;
    &lt;p&gt;This is a tipping point moment. Increasingly, it seems that the talent who understood the deep failure modes is gone. The new, leaner, presumably less expensive teams lack the institutional knowledge needed to, if not prevent these outages in the first place, significantly reduce the time to detection and recovery. Remember, there was a time when Amazon's "Frugality" leadership principle meant doing more with less, not doing everything with basically nothing. AWS's operational strength was built on redundant, experienced people, and when you cut to the bone, basic things start breaking.&lt;/p&gt;
    &lt;p&gt;I want to be very clear on one last point. This isn't about the technology being old. It's about the people maintaining it being new. If I had to guess what happens next, the market will forgive AWS this time, but the pattern will continue.&lt;/p&gt;
    &lt;p&gt;AWS will almost certainly say this was an "isolated incident," but when you've hollowed out your engineering ranks, every incident becomes more likely. The next outage is already brewing. It's just a matter of which understaffed team trips over which edge case first, because the chickens are coming home to roost. ¬Æ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/"/><published>2025-10-20T20:50:03+00:00</published></entry></feed>